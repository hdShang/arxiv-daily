---
layout: default
title: Dual Language Models: Balancing Training Efficiency and Overfitting Resilience
---

# Dual Language Models: Balancing Training Efficiency and Overfitting Resilience

**arXiv**: [2512.14549v1](https://arxiv.org/abs/2512.14549) | [PDF](https://arxiv.org/pdf/2512.14549.pdf)

**ä½œè€…**: David Samuel, Lucas Georges Gabriel Charpentier

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒç›®æ ‡è®­ç»ƒæ–¹æ³•ä»¥å¹³è¡¡è¯­è¨€æ¨¡åž‹çš„è®­ç»ƒæ•ˆçŽ‡ä¸Žè¿‡æ‹Ÿåˆé²æ£’æ€§**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `è¯­è¨€æ¨¡åž‹` `åŒç›®æ ‡è®­ç»ƒ` `è‡ªå›žå½’å»ºæ¨¡` `æŽ©ç æ‰©æ•£æ¨¡åž‹` `è®­ç»ƒæ•ˆçŽ‡` `è¿‡æ‹Ÿåˆé²æ£’æ€§` `ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½` `æŸå¤±å‡½æ•°ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šè‡ªå›žå½’æ¨¡åž‹è®­ç»ƒæ•ˆçŽ‡é«˜ä½†æ˜“è¿‡æ‹Ÿåˆï¼ŒæŽ©ç æ‰©æ•£æ¨¡åž‹é²æ£’æ€§å¼ºä½†è®­ç»ƒæ•ˆçŽ‡ä½Žï¼ŒçŽ°æœ‰å•ç›®æ ‡æ–¹æ³•éš¾ä»¥å¹³è¡¡ä¸¤è€…ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šç»“åˆè‡ªå›žå½’å’ŒæŽ©ç æ‰©æ•£è®­ç»ƒç›®æ ‡ï¼Œé€šè¿‡åŒç›®æ ‡è®­ç»ƒæå‡æ¨¡åž‹çµæ´»æ€§å’Œæ€§èƒ½ï¼Œæ— éœ€æž¶æž„æ”¹åŠ¨ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨50ä¸ªæ¨¡åž‹å®žéªŒä¸­ï¼ŒåŒç›®æ ‡è®­ç»ƒåœ¨æ‰€æœ‰è®¾ç½®ä¸‹å‡æœ€ä¼˜ï¼Œæœ€ä¼˜æ¯”ä¾‹ç¨³å®šï¼Œæ˜¾è‘—æå‡ä¸‹æ¸¸ä»»åŠ¡è¡¨çŽ°ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç»“åˆè‡ªå›žå½’å’ŒæŽ©ç æ‰©æ•£è®­ç»ƒç›®æ ‡ï¼Œæ— éœ€ä¿®æ”¹æž¶æž„ï¼Œæž„å»ºäº†çµæ´»çš„è¯­è¨€æ¨¡åž‹ï¼Œå…¶æ€§èƒ½ä¼˜äºŽå•ç›®æ ‡æ¨¡åž‹ã€‚è‡ªå›žå½’å»ºæ¨¡å› å…¶è®­ç»ƒæ•ˆçŽ‡é«˜è€Œæµè¡Œï¼Œä½†ä»£ä»·æ˜¯å¯¹è¿‡æ‹Ÿåˆæ•æ„Ÿï¼›è€ŒæŽ©ç æ‰©æ•£æ¨¡åž‹è®­ç»ƒæ•ˆçŽ‡è¾ƒä½Žï¼Œä½†è¿‡æ‹Ÿåˆé²æ£’æ€§æ›´å¼ºã€‚æœ¬ç ”ç©¶è¯æ˜ŽåŒç›®æ ‡è®­ç»ƒèƒ½å…¼é¡¾ä¸¤è€…ä¼˜åŠ¿ã€‚ä¸ºç¡®å®šä¸¤ä¸ªç›®æ ‡ä¹‹é—´çš„æœ€ä¼˜æ¯”ä¾‹ï¼Œæˆ‘ä»¬åœ¨ä¸åŒæ•°æ®é‡å¤æ°´å¹³ä¸‹è®­ç»ƒå’Œè¯„ä¼°äº†50ä¸ªè¯­è¨€æ¨¡åž‹ã€‚ç»“æžœè¡¨æ˜Žï¼Œåœ¨æ‰€æœ‰è¯„ä¼°è®¾ç½®ä¸‹ï¼Œç»“åˆä¸¤ä¸ªç›®æ ‡æ˜¯æœ€ä¼˜çš„ï¼Œä¸”æ— è®ºé’ˆå¯¹è‡ªå›žå½’è¿˜æ˜¯æŽ©ç æ‰©æ•£ä¸‹æ¸¸æ€§èƒ½ï¼Œæœ€ä¼˜æ¯”ä¾‹ç›¸ä¼¼ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è¯­è¨€æ¨¡åž‹è®­ç»ƒä¸­æ•ˆçŽ‡ä¸Žé²æ£’æ€§çš„æƒè¡¡é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•ä¸­ï¼Œè‡ªå›žå½’æ¨¡åž‹è®­ç»ƒå¿«ä½†æ˜“è¿‡æ‹Ÿåˆï¼ŒæŽ©ç æ‰©æ•£æ¨¡åž‹é²æ£’æ€§å¼ºä½†è®­ç»ƒæ…¢ï¼Œè¿™é™åˆ¶äº†æ¨¡åž‹åœ¨å®žé™…åº”ç”¨ä¸­çš„çµæ´»æ€§å’Œæ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºç»“åˆè‡ªå›žå½’å’ŒæŽ©ç æ‰©æ•£è®­ç»ƒç›®æ ‡ï¼Œé€šè¿‡åŒç›®æ ‡è®­ç»ƒæ¥å¹³è¡¡æ•ˆçŽ‡å’Œé²æ£’æ€§ã€‚è¿™æ ·è®¾è®¡æ˜¯å› ä¸ºä¸¤ä¸ªç›®æ ‡äº’è¡¥ï¼šè‡ªå›žå½’ç›®æ ‡ä¿ƒè¿›é«˜æ•ˆå­¦ä¹ ï¼ŒæŽ©ç æ‰©æ•£ç›®æ ‡å¢žå¼ºæ³›åŒ–èƒ½åŠ›ï¼Œä»Žè€Œåœ¨ä¸å¢žåŠ æž¶æž„å¤æ‚åº¦çš„æƒ…å†µä¸‹æå‡æ¨¡åž‹æ•´ä½“è¡¨çŽ°ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬æ¨¡åž‹åˆå§‹åŒ–ã€åŒç›®æ ‡è®­ç»ƒå’Œè¯„ä¼°é˜¶æ®µã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ ‡å‡†è¯­è¨€æ¨¡åž‹æž¶æž„ï¼ˆå¦‚Transformerï¼‰ï¼Œè®­ç»ƒæ—¶åŒæ—¶è®¡ç®—è‡ªå›žå½’æŸå¤±å’ŒæŽ©ç æ‰©æ•£æŸå¤±ï¼Œé€šè¿‡åŠ æƒæ±‚å’Œå½¢æˆæ€»æŸå¤±ï¼Œç„¶åŽè¿›è¡Œåå‘ä¼ æ’­ä¼˜åŒ–ã€‚è¯„ä¼°é˜¶æ®µæµ‹è¯•æ¨¡åž‹åœ¨ä¸åŒæ•°æ®é‡å¤æ°´å¹³ä¸‹çš„ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°æ˜¯æ— éœ€ä¿®æ”¹æ¨¡åž‹æž¶æž„ï¼Œä»…é€šè¿‡è®­ç»ƒç›®æ ‡ç»„åˆå®žçŽ°æ€§èƒ½æå‡ã€‚ä¸ŽçŽ°æœ‰å•ç›®æ ‡æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬è´¨åŒºåˆ«åœ¨äºŽåŒæ—¶åˆ©ç”¨ä¸¤ä¸ªç›®æ ‡çš„ä¼˜åŠ¿ï¼Œé¿å…äº†å•ç‹¬ä½¿ç”¨æ—¶çš„ç¼ºé™·ï¼Œä»Žè€Œåœ¨æ•ˆçŽ‡å’Œé²æ£’æ€§é—´å–å¾—æ›´å¥½å¹³è¡¡ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®å‚æ•°åŒ…æ‹¬è‡ªå›žå½’å’ŒæŽ©ç æ‰©æ•£æŸå¤±çš„æƒé‡æ¯”ä¾‹ï¼Œè®ºæ–‡é€šè¿‡å®žéªŒç¡®å®šæœ€ä¼˜æ¯”ä¾‹ï¼›æŸå¤±å‡½æ•°ä¸ºåŠ æƒå’Œï¼šæ€»æŸå¤± = Î± * è‡ªå›žå½’æŸå¤± + (1-Î±) * æŽ©ç æ‰©æ•£æŸå¤±ï¼Œå…¶ä¸­Î±é€šè¿‡ç½‘æ ¼æœç´¢ä¼˜åŒ–ï¼›ç½‘ç»œç»“æž„ä¿æŒæ ‡å‡†ï¼Œæœªå¼•å…¥é¢å¤–å±‚æˆ–ç»„ä»¶ï¼Œç¡®ä¿æ–¹æ³•é€šç”¨æ€§ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒè®­ç»ƒäº†50ä¸ªè¯­è¨€æ¨¡åž‹ï¼Œåœ¨ä¸åŒæ•°æ®é‡å¤æ°´å¹³ä¸‹è¯„ä¼°ã€‚ç»“æžœæ˜¾ç¤ºï¼ŒåŒç›®æ ‡è®­ç»ƒåœ¨æ‰€æœ‰è®¾ç½®ä¸‹å‡ä¼˜äºŽå•ç›®æ ‡æ¨¡åž‹ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼›æœ€ä¼˜æŸå¤±æƒé‡æ¯”ä¾‹åœ¨ä¸åŒä¸‹æ¸¸ä»»åŠ¡ï¼ˆè‡ªå›žå½’å’ŒæŽ©ç æ‰©æ•£ï¼‰ä¸­ç›¸ä¼¼ï¼Œè¡¨æ˜Žæ–¹æ³•å…·æœ‰ç¨³å®šæ€§å’Œé€šç”¨æ€§ã€‚å…·ä½“æ•°æ®æœªæä¾›ï¼Œä½†è®ºæ–‡å¼ºè°ƒåŒç›®æ ‡æ¨¡åž‹åœ¨æ•ˆçŽ‡å’Œé²æ£’æ€§æ–¹é¢å‡è¾¾åˆ°æœ€ä½³å¹³è¡¡ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå…·æœ‰å¹¿æ³›æ½œåœ¨åº”ç”¨ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿï¼Œé€šè¿‡æå‡æ¨¡åž‹è®­ç»ƒæ•ˆçŽ‡å’Œé²æ£’æ€§ï¼Œå¯é™ä½Žè®¡ç®—æˆæœ¬å¹¶æé«˜éƒ¨ç½²ç¨³å®šæ€§ã€‚æœªæ¥å¯èƒ½æŽ¨åŠ¨æ›´é«˜æ•ˆã€æ³›åŒ–èƒ½åŠ›å¼ºçš„è¯­è¨€æ¨¡åž‹å‘å±•ï¼Œä¿ƒè¿›AIåœ¨å®žé™…åœºæ™¯ä¸­çš„è½åœ°ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper combines autoregressive and masked-diffusion training objectives without any architectural modifications, resulting in flexible language models that outperform single-objective models. Autoregressive modeling has been a popular approach, partly because of its training efficiency; however, that comes at the cost of sensitivity to overfitting. On the other hand, masked-diffusion models are less efficient to train while being more resilient to overfitting. In this work, we demonstrate that dual-objective training achieves the best of both worlds. To derive the optimal ratio between both objectives, we train and evaluate 50 language models under varying levels of data repetition. We show that it is optimal to combine both objectives under all evaluated settings and that the optimal ratio is similar whether targeting autoregressive or masked-diffusion downstream performance.

