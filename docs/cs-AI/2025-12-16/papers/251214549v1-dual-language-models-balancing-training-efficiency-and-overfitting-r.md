---
layout: default
title: Dual Language Models: Balancing Training Efficiency and Overfitting Resilience
---

# Dual Language Models: Balancing Training Efficiency and Overfitting Resilience

**arXiv**: [2512.14549v1](https://arxiv.org/abs/2512.14549) | [PDF](https://arxiv.org/pdf/2512.14549.pdf)

**ä½œè€…**: David Samuel, Lucas Georges Gabriel Charpentier

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒç›®æ ‡è®­ç»ƒæ–¹æ³•ä»¥å¹³è¡¡è¯­è¨€æ¨¡åž‹è®­ç»ƒæ•ˆçŽ‡ä¸Žè¿‡æ‹Ÿåˆé²æ£’æ€§**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `è¯­è¨€æ¨¡åž‹è®­ç»ƒ` `è‡ªå›žå½’å»ºæ¨¡` `æŽ©ç æ‰©æ•£æ¨¡åž‹` `åŒç›®æ ‡ä¼˜åŒ–` `è¿‡æ‹Ÿåˆé²æ£’æ€§` `è®­ç»ƒæ•ˆçŽ‡` `ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½` `æŸå¤±æƒé‡æ¯”ä¾‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è‡ªå›žå½’æ¨¡åž‹è®­ç»ƒæ•ˆçŽ‡é«˜ä½†æ˜“è¿‡æ‹Ÿåˆï¼ŒæŽ©ç æ‰©æ•£æ¨¡åž‹é²æ£’æ€§å¼ºä½†è®­ç»ƒæ…¢ï¼ŒçŽ°æœ‰å•ç›®æ ‡æ–¹æ³•éš¾ä»¥å¹³è¡¡æ•ˆçŽ‡ä¸Žé²æ£’æ€§ã€‚
2. æå‡ºåŒç›®æ ‡è®­ç»ƒæ¡†æž¶ï¼Œç»“åˆè‡ªå›žå½’å’ŒæŽ©ç æ‰©æ•£ç›®æ ‡ï¼Œé€šè¿‡ä¼˜åŒ–æ¯”ä¾‹å®žçŽ°é«˜æ•ˆä¸”é²æ£’çš„è¯­è¨€æ¨¡åž‹è®­ç»ƒã€‚
3. å®žéªŒæ˜¾ç¤ºåŒç›®æ ‡æ¨¡åž‹åœ¨æ‰€æœ‰è®¾ç½®ä¸‹å‡ä¼˜äºŽå•ç›®æ ‡æ¨¡åž‹ï¼Œæœ€ä¼˜æ¯”ä¾‹ç¨³å®šï¼Œæ˜¾è‘—æå‡ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç»“åˆè‡ªå›žå½’å’ŒæŽ©ç æ‰©æ•£è®­ç»ƒç›®æ ‡ï¼Œæ— éœ€ä¿®æ”¹æ¨¡åž‹æž¶æž„ï¼Œå®žçŽ°äº†çµæ´»çš„è¯­è¨€æ¨¡åž‹ï¼Œå…¶æ€§èƒ½ä¼˜äºŽå•ç›®æ ‡æ¨¡åž‹ã€‚è‡ªå›žå½’å»ºæ¨¡å› å…¶è®­ç»ƒæ•ˆçŽ‡é«˜è€Œå¹¿å—æ¬¢è¿Žï¼Œä½†ä»£ä»·æ˜¯å¯¹è¿‡æ‹Ÿåˆæ•æ„Ÿï¼›è€ŒæŽ©ç æ‰©æ•£æ¨¡åž‹è®­ç»ƒæ•ˆçŽ‡è¾ƒä½Žï¼Œä½†å¯¹è¿‡æ‹Ÿåˆæ›´å…·é²æ£’æ€§ã€‚æœ¬ç ”ç©¶è¯æ˜Žï¼ŒåŒç›®æ ‡è®­ç»ƒèƒ½å…¼é¡¾ä¸¤è€…ä¼˜åŠ¿ã€‚ä¸ºç¡®å®šä¸¤ä¸ªç›®æ ‡ä¹‹é—´çš„æœ€ä¼˜æ¯”ä¾‹ï¼Œæˆ‘ä»¬åœ¨ä¸åŒæ•°æ®é‡å¤æ°´å¹³ä¸‹è®­ç»ƒå’Œè¯„ä¼°äº†50ä¸ªè¯­è¨€æ¨¡åž‹ã€‚ç»“æžœè¡¨æ˜Žï¼Œåœ¨æ‰€æœ‰è¯„ä¼°è®¾ç½®ä¸‹ï¼Œç»“åˆä¸¤ä¸ªç›®æ ‡æ˜¯æœ€ä¼˜çš„ï¼Œä¸”æ— è®ºé’ˆå¯¹è‡ªå›žå½’è¿˜æ˜¯æŽ©ç æ‰©æ•£çš„ä¸‹æ¸¸æ€§èƒ½ï¼Œæœ€ä¼˜æ¯”ä¾‹éƒ½ç›¸ä¼¼ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

è®ºæ–‡æå‡ºä¸€ç§åŒç›®æ ‡è®­ç»ƒæ¡†æž¶ï¼Œæ ¸å¿ƒæ–¹æ³•æ˜¯åœ¨ä¸æ”¹å˜æ¨¡åž‹æž¶æž„çš„å‰æä¸‹ï¼ŒåŒæ—¶ç»“åˆè‡ªå›žå½’å’ŒæŽ©ç æ‰©æ•£è®­ç»ƒç›®æ ‡ã€‚å…³é”®åˆ›æ–°ç‚¹åœ¨äºŽé€šè¿‡åŠ¨æ€è°ƒæ•´ä¸¤ä¸ªç›®æ ‡çš„æŸå¤±æƒé‡æ¯”ä¾‹ï¼Œå®žçŽ°è®­ç»ƒæ•ˆçŽ‡ä¸Žè¿‡æ‹Ÿåˆé²æ£’æ€§çš„å¹³è¡¡ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼Œå®ƒé¿å…äº†å•ä¸€ç›®æ ‡è®­ç»ƒçš„å±€é™æ€§ï¼Œæ— éœ€å¤æ‚æž¶æž„ä¿®æ”¹ï¼Œè€Œæ˜¯é€šè¿‡ç›®æ ‡ç»„åˆä¼˜åŒ–æ¨¡åž‹æ€§èƒ½ï¼Œæä¾›äº†ä¸€ç§çµæ´»ä¸”é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨50ä¸ªè¯­è¨€æ¨¡åž‹çš„å®žéªŒä¸­ï¼ŒåŒç›®æ ‡è®­ç»ƒåœ¨æ‰€æœ‰æ•°æ®é‡å¤æ°´å¹³ä¸‹å‡ä¼˜äºŽå•ç›®æ ‡æ¨¡åž‹ï¼Œæœ€ä¼˜æ¯”ä¾‹ç¨³å®šï¼Œæ˜¾è‘—æå‡ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ï¼ŒéªŒè¯äº†æ–¹æ³•åœ¨å¹³è¡¡æ•ˆçŽ‡ä¸Žé²æ£’æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯åº”ç”¨äºŽè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿï¼Œé€šè¿‡æå‡è¯­è¨€æ¨¡åž‹çš„è®­ç»ƒæ•ˆçŽ‡å’Œé²æ£’æ€§ï¼Œé™ä½Žè¿‡æ‹Ÿåˆé£Žé™©ï¼Œé€‚ç”¨äºŽæ•°æ®æœ‰é™æˆ–é‡å¤æ€§é«˜çš„åœºæ™¯ï¼Œå…·æœ‰å®žé™…éƒ¨ç½²ä»·å€¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper combines autoregressive and masked-diffusion training objectives without any architectural modifications, resulting in flexible language models that outperform single-objective models. Autoregressive modeling has been a popular approach, partly because of its training efficiency; however, that comes at the cost of sensitivity to overfitting. On the other hand, masked-diffusion models are less efficient to train while being more resilient to overfitting. In this work, we demonstrate that dual-objective training achieves the best of both worlds. To derive the optimal ratio between both objectives, we train and evaluate 50 language models under varying levels of data repetition. We show that it is optimal to combine both objectives under all evaluated settings and that the optimal ratio is similar whether targeting autoregressive or masked-diffusion downstream performance.

