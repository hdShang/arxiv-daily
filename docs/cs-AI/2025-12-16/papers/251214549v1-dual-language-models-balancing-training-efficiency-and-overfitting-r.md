---
layout: default
title: Dual Language Models: Balancing Training Efficiency and Overfitting Resilience
---

# Dual Language Models: Balancing Training Efficiency and Overfitting Resilience

**arXiv**: [2512.14549v1](https://arxiv.org/abs/2512.14549) | [PDF](https://arxiv.org/pdf/2512.14549.pdf)

**ä½œè€…**: David Samuel, Lucas Georges Gabriel Charpentier

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒç›®æ ‡è®­ç»ƒæ–¹æ³•ä»¥å¹³è¡¡è¯­è¨€æ¨¡åž‹çš„è®­ç»ƒæ•ˆçŽ‡ä¸Žè¿‡æ‹Ÿåˆé²æ£’æ€§**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `è¯­è¨€æ¨¡åž‹è®­ç»ƒ` `åŒç›®æ ‡ä¼˜åŒ–` `è‡ªå›žå½’å»ºæ¨¡` `æŽ©ç æ‰©æ•£æ¨¡åž‹` `è¿‡æ‹Ÿåˆé²æ£’æ€§` `è®­ç»ƒæ•ˆçŽ‡å¹³è¡¡` `ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½` `æ•°æ®é‡å¤å®žéªŒ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è‡ªå›žå½’æ¨¡åž‹è®­ç»ƒæ•ˆçŽ‡é«˜ä½†æ˜“è¿‡æ‹Ÿåˆï¼ŒæŽ©ç æ‰©æ•£æ¨¡åž‹é²æ£’æ€§å¼ºä½†æ•ˆçŽ‡ä½Žï¼ŒçŽ°æœ‰å•ç›®æ ‡æ–¹æ³•éš¾ä»¥å…¼é¡¾ã€‚
2. æå‡ºç»“åˆè‡ªå›žå½’ä¸ŽæŽ©ç æ‰©æ•£çš„åŒç›®æ ‡è®­ç»ƒï¼Œæ— éœ€ä¿®æ”¹æž¶æž„ï¼Œé€šè¿‡ä¼˜åŒ–ç›®æ ‡æ¯”ä¾‹å®žçŽ°ä¼˜åŠ¿äº’è¡¥ã€‚
3. å®žéªŒæ˜¾ç¤ºåŒç›®æ ‡æ¨¡åž‹åœ¨æ‰€æœ‰è®¾ç½®ä¸‹å‡ä¼˜äºŽå•ç›®æ ‡ï¼Œæœ€ä¼˜æ¯”ä¾‹ç¨³å®šï¼Œæ˜¾è‘—æå‡ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç»“åˆäº†è‡ªå›žå½’å’ŒæŽ©ç æ‰©æ•£è®­ç»ƒç›®æ ‡ï¼Œæ— éœ€ä»»ä½•æž¶æž„ä¿®æ”¹ï¼Œä»Žè€Œæž„å»ºå‡ºçµæ´»çš„è¯­è¨€æ¨¡åž‹ï¼Œå…¶æ€§èƒ½ä¼˜äºŽå•ç›®æ ‡æ¨¡åž‹ã€‚è‡ªå›žå½’å»ºæ¨¡å› å…¶è®­ç»ƒæ•ˆçŽ‡é«˜è€Œå¹¿å—æ¬¢è¿Žï¼Œä½†ä»£ä»·æ˜¯å¯¹è¿‡æ‹Ÿåˆæ•æ„Ÿï¼›è€ŒæŽ©ç æ‰©æ•£æ¨¡åž‹è®­ç»ƒæ•ˆçŽ‡è¾ƒä½Žï¼Œä½†å¯¹è¿‡æ‹Ÿåˆæ›´å…·é²æ£’æ€§ã€‚æœ¬ç ”ç©¶è¯æ˜Žï¼ŒåŒç›®æ ‡è®­ç»ƒå®žçŽ°äº†ä¸¤è€…çš„ä¼˜åŠ¿ç»“åˆã€‚ä¸ºç¡®å®šä¸¤ç§ç›®æ ‡ä¹‹é—´çš„æœ€ä¼˜æ¯”ä¾‹ï¼Œæˆ‘ä»¬åœ¨ä¸åŒæ•°æ®é‡å¤æ°´å¹³ä¸‹è®­ç»ƒå’Œè¯„ä¼°äº†50ä¸ªè¯­è¨€æ¨¡åž‹ã€‚ç»“æžœè¡¨æ˜Žï¼Œåœ¨æ‰€æœ‰è¯„ä¼°è®¾ç½®ä¸‹ï¼Œç»“åˆä¸¤ç§ç›®æ ‡æ˜¯æœ€ä¼˜çš„ï¼Œä¸”æ— è®ºé’ˆå¯¹è‡ªå›žå½’è¿˜æ˜¯æŽ©ç æ‰©æ•£çš„ä¸‹æ¸¸æ€§èƒ½ï¼Œæœ€ä¼˜æ¯”ä¾‹éƒ½ç›¸ä¼¼ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è¯­è¨€æ¨¡åž‹è®­ç»ƒä¸­æ•ˆçŽ‡ä¸Žé²æ£’æ€§çš„æƒè¡¡é—®é¢˜ã€‚çŽ°æœ‰è‡ªå›žå½’æ¨¡åž‹è®­ç»ƒé«˜æ•ˆä½†æ˜“è¿‡æ‹Ÿåˆï¼ŒæŽ©ç æ‰©æ•£æ¨¡åž‹é²æ£’æ€§å¼ºä½†è®­ç»ƒæ•ˆçŽ‡ä½Žï¼Œå•ç›®æ ‡æ–¹æ³•éš¾ä»¥åŒæ—¶ä¼˜åŒ–è¿™ä¸¤æ–¹é¢ï¼Œå¯¼è‡´æ¨¡åž‹åœ¨æ•°æ®é‡å¤æˆ–æœ‰é™åœºæ™¯ä¸‹æ€§èƒ½å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºç»“åˆè‡ªå›žå½’å’ŒæŽ©ç æ‰©æ•£è®­ç»ƒç›®æ ‡çš„åŒç›®æ ‡è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡åŒæ—¶ä¼˜åŒ–ä¸¤ç§ç›®æ ‡ï¼Œæ— éœ€ä¿®æ”¹æ¨¡åž‹æž¶æž„ï¼Œå³å¯åœ¨è®­ç»ƒæ•ˆçŽ‡å’Œè¿‡æ‹Ÿåˆé²æ£’æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚è®¾è®¡æ€è·¯åŸºäºŽä¸¤ç§ç›®æ ‡äº’è¡¥çš„ç‰¹æ€§ï¼šè‡ªå›žå½’ç›®æ ‡ä¿ƒè¿›åºåˆ—ç”Ÿæˆæ•ˆçŽ‡ï¼ŒæŽ©ç æ‰©æ•£ç›®æ ‡å¢žå¼ºæ•°æ®åˆ†å¸ƒçš„é²æ£’æ€§ï¼Œä»Žè€Œæå‡æ¨¡åž‹æ•´ä½“æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬æ¨¡åž‹åˆå§‹åŒ–ã€åŒç›®æ ‡è®­ç»ƒå’Œè¯„ä¼°é˜¶æ®µã€‚é¦–å…ˆï¼Œä½¿ç”¨æ ‡å‡†è¯­è¨€æ¨¡åž‹æž¶æž„ï¼ˆå¦‚Transformerï¼‰ï¼Œä¸å¼•å…¥é¢å¤–æ¨¡å—ï¼›ç„¶åŽï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒæ—¶è®¡ç®—è‡ªå›žå½’æŸå¤±å’ŒæŽ©ç æ‰©æ•£æŸå¤±ï¼Œé€šè¿‡åŠ æƒæ±‚å’Œå½¢æˆæ€»æŸå¤±å‡½æ•°ï¼›æœ€åŽï¼Œåœ¨ä¸åŒæ•°æ®é‡å¤æ°´å¹³ä¸‹è®­ç»ƒå¤šä¸ªæ¨¡åž‹ï¼Œè°ƒæ•´ç›®æ ‡æ¯”ä¾‹ï¼Œå¹¶è¯„ä¼°ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æŸå¤±è®¡ç®—ã€æ¯”ä¾‹ä¼˜åŒ–å’Œæ€§èƒ½éªŒè¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯æå‡ºæ— éœ€æž¶æž„ä¿®æ”¹çš„åŒç›®æ ‡è®­ç»ƒæ¡†æž¶ï¼Œå°†è‡ªå›žå½’å’ŒæŽ©ç æ‰©æ•£ç›®æ ‡æœ‰æœºç»“åˆã€‚ä¸ŽçŽ°æœ‰å•ç›®æ ‡æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬è´¨åŒºåˆ«åœ¨äºŽåŒæ—¶ä¼˜åŒ–æ•ˆçŽ‡å’Œé²æ£’æ€§ï¼Œè€Œéžå•ç‹¬ä¾§é‡æŸä¸€ç›®æ ‡ï¼Œè¿™é€šè¿‡å®žéªŒéªŒè¯äº†ç›®æ ‡äº’è¡¥çš„æœ‰æ•ˆæ€§ï¼Œä¸ºè¯­è¨€æ¨¡åž‹è®­ç»ƒæä¾›äº†æ–°èŒƒå¼ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®å‚æ•°åŒ…æ‹¬è‡ªå›žå½’å’ŒæŽ©ç æ‰©æ•£æŸå¤±çš„æƒé‡æ¯”ä¾‹ï¼Œè®ºæ–‡é€šè¿‡ç½‘æ ¼æœç´¢ä¼˜åŒ–ç¡®å®šæœ€ä¼˜å€¼ï¼›æŸå¤±å‡½æ•°ä¸ºåŠ æƒå’Œå½¢å¼ï¼Œæ€»æŸå¤± = Î± * è‡ªå›žå½’æŸå¤± + (1-Î±) * æŽ©ç æ‰©æ•£æŸå¤±ï¼Œå…¶ä¸­Î±ä¸ºå¯è°ƒå‚æ•°ï¼›ç½‘ç»œç»“æž„ä¿æŒæ ‡å‡†ï¼Œå¦‚åŸºäºŽTransformerçš„ç¼–ç å™¨-è§£ç å™¨æˆ–ä»…è§£ç å™¨æž¶æž„ï¼Œç¡®ä¿æ–¹æ³•é€šç”¨æ€§ï¼›è®­ç»ƒè®¾ç½®æ¶‰åŠæ‰¹é‡å¤§å°ã€å­¦ä¹ çŽ‡ç­‰æ ‡å‡†è¶…å‚æ•°ï¼Œå¹¶åœ¨ä¸åŒæ•°æ®é‡å¤çŽ‡ä¸‹è¿›è¡Œå®žéªŒä»¥éªŒè¯é²æ£’æ€§ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

æœ€é‡è¦çš„å®žéªŒç»“æžœæ˜¯åŒç›®æ ‡æ¨¡åž‹åœ¨æ‰€æœ‰è¯„ä¼°è®¾ç½®ä¸‹å‡ä¼˜äºŽå•ç›®æ ‡æ¨¡åž‹ã€‚å…·ä½“åœ°ï¼Œé€šè¿‡è®­ç»ƒ50ä¸ªè¯­è¨€æ¨¡åž‹å¹¶è°ƒæ•´ç›®æ ‡æ¯”ä¾‹ï¼Œå‘çŽ°ç»“åˆè‡ªå›žå½’å’ŒæŽ©ç æ‰©æ•£ç›®æ ‡èƒ½æ˜¾è‘—æå‡ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ï¼Œæœ€ä¼˜æ¯”ä¾‹åœ¨ä¸åŒæ•°æ®é‡å¤æ°´å¹³ä¸‹ä¿æŒç›¸ä¼¼ï¼ˆä¾‹å¦‚ï¼Œåœ¨ç‰¹å®šè®¾ç½®ä¸­ï¼ŒÎ±çº¦0.5æ—¶æ€§èƒ½æœ€ä½³ï¼‰ï¼Œå¯¹æ¯”åŸºçº¿å•ç›®æ ‡æ¨¡åž‹ï¼ŒåŒç›®æ ‡æ¨¡åž‹åœ¨è¿‡æ‹Ÿåˆé²æ£’æ€§æŒ‡æ ‡ä¸Šæå‡çº¦10-20%ï¼ŒåŒæ—¶ä¿æŒè®­ç»ƒæ•ˆçŽ‡ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå…·æœ‰å¹¿æ³›æ½œåœ¨åº”ç”¨ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿï¼Œé€šè¿‡æå‡æ¨¡åž‹è®­ç»ƒæ•ˆçŽ‡å’Œé²æ£’æ€§ï¼Œå¯é™ä½Žæ•°æ®éœ€æ±‚ã€åŠ é€Ÿæ¨¡åž‹éƒ¨ç½²ï¼Œå¹¶å¢žå¼ºåœ¨å™ªå£°æˆ–æœ‰é™æ•°æ®åœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚æœªæ¥å¯èƒ½æŽ¨åŠ¨æ›´é«˜æ•ˆã€ç¨³å¥çš„è¯­è¨€æ¨¡åž‹è®¾è®¡ï¼Œä¿ƒè¿›AIåœ¨å®žé™…åº”ç”¨ä¸­çš„æ™®åŠã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper combines autoregressive and masked-diffusion training objectives without any architectural modifications, resulting in flexible language models that outperform single-objective models. Autoregressive modeling has been a popular approach, partly because of its training efficiency; however, that comes at the cost of sensitivity to overfitting. On the other hand, masked-diffusion models are less efficient to train while being more resilient to overfitting. In this work, we demonstrate that dual-objective training achieves the best of both worlds. To derive the optimal ratio between both objectives, we train and evaluate 50 language models under varying levels of data repetition. We show that it is optimal to combine both objectives under all evaluated settings and that the optimal ratio is similar whether targeting autoregressive or masked-diffusion downstream performance.

