---
layout: default
title: CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer
---

# CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer

**arXiv**: [2512.14560v1](https://arxiv.org/abs/2512.14560) | [PDF](https://arxiv.org/pdf/2512.14560.pdf)

**ä½œè€…**: Xianwei Cao, Dou Quan, Shuang Wang, Ning Huyan, Wei Wang, Yunan Li, Licheng Jiao

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: 16 pages, 6 figures

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCLNetæ¡†æž¶ï¼Œé€šè¿‡æ˜¾å¼è·¨è§†å›¾å¯¹åº”å…³ç³»è§£å†³å›¾åƒæ£€ç´¢å¼è·¨è§†è§’åœ°ç†å®šä½é—®é¢˜ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **è§†è§‰é‡Œç¨‹è®¡**

**å…³é”®è¯**: `è·¨è§†è§’åœ°ç†å®šä½` `å›¾åƒæ£€ç´¢` `ç‰¹å¾å¯¹é½` `ç¥žç»å¯¹åº”å›¾` `éžçº¿æ€§åµŒå…¥è½¬æ¢` `å…¨å±€ç‰¹å¾é‡æ ¡å‡†` `è¯­ä¹‰å‡ ä½•å¯¹åº”` `å¤šè§†å›¾åŒ¹é…`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•ä¾èµ–å…¨å±€è¡¨ç¤ºæˆ–éšå¼å¯¹é½ï¼Œéš¾ä»¥å»ºæ¨¡è·¨è§†å›¾çš„æ˜¾å¼ç©ºé—´å¯¹åº”å…³ç³»ï¼Œå¯¼è‡´åœ°ç†å®šä½ç²¾åº¦å—é™ã€‚
2. CLNeté€šè¿‡ç¥žç»å¯¹åº”å›¾ã€éžçº¿æ€§åµŒå…¥è½¬æ¢å™¨å’Œå…¨å±€ç‰¹å¾é‡æ ¡å‡†æ¨¡å—ï¼Œæ˜¾å¼å­¦ä¹ è¯­ä¹‰å’Œå‡ ä½•å¯¹åº”ï¼Œæå‡ç‰¹å¾å¯¹é½èƒ½åŠ›ã€‚
3. åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šï¼ŒCLNetè¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œå¹¶å±•çŽ°å‡ºæ›´å¥½çš„å¯è§£é‡Šæ€§å’Œæ³›åŒ–æ€§ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºŽå›¾åƒæ£€ç´¢çš„è·¨è§†è§’åœ°ç†å®šä½ï¼ˆIRCVGLï¼‰æ—¨åœ¨åŒ¹é…ä»Žæ˜¾è‘—ä¸åŒè§†è§’ï¼ˆå¦‚å«æ˜Ÿå’Œè¡—æ™¯å›¾åƒï¼‰æ•èŽ·çš„å›¾åƒã€‚çŽ°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å­¦ä¹ é²æ£’çš„å…¨å±€è¡¨ç¤ºæˆ–éšå¼ç‰¹å¾å¯¹é½ï¼Œå¾€å¾€æ— æ³•å»ºæ¨¡å¯¹ç²¾ç¡®å®šä½è‡³å…³é‡è¦çš„æ˜¾å¼ç©ºé—´å¯¹åº”å…³ç³»ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„å¯¹åº”æ„ŸçŸ¥ç‰¹å¾ç»†åŒ–æ¡†æž¶ï¼Œç§°ä¸ºCLNetï¼Œå®ƒæ˜¾å¼åœ°æ¡¥æŽ¥ä¸åŒè§†å›¾ä¹‹é—´çš„è¯­ä¹‰å’Œå‡ ä½•å·®è·ã€‚CLNetå°†è§†å›¾å¯¹é½è¿‡ç¨‹åˆ†è§£ä¸ºä¸‰ä¸ªå¯å­¦ä¹ ä¸”äº’è¡¥çš„æ¨¡å—ï¼šç¥žç»å¯¹åº”å›¾ï¼ˆNCMï¼‰ï¼Œé€šè¿‡æ½œåœ¨å¯¹åº”åœºåœ¨ç©ºé—´ä¸Šå¯¹é½è·¨è§†å›¾ç‰¹å¾ï¼›éžçº¿æ€§åµŒå…¥è½¬æ¢å™¨ï¼ˆNECï¼‰ï¼Œä½¿ç”¨åŸºäºŽMLPçš„å˜æ¢è·¨è§†è§’é‡æ–°æ˜ å°„ç‰¹å¾ï¼›ä»¥åŠå…¨å±€ç‰¹å¾é‡æ ¡å‡†ï¼ˆGFRï¼‰æ¨¡å—ï¼Œé€šè¿‡å­¦ä¹ åˆ°çš„ç©ºé—´çº¿ç´¢å¼•å¯¼é‡æ–°åŠ æƒä¿¡æ¯ä¸°å¯Œçš„ç‰¹å¾é€šé“ã€‚æ‰€æå‡ºçš„CLNetèƒ½å¤Ÿè”åˆæ•èŽ·é«˜çº§è¯­ä¹‰å’Œç»†ç²’åº¦å¯¹é½ã€‚åœ¨å››ä¸ªå…¬å…±åŸºå‡†æ•°æ®é›†ï¼ˆCVUSAã€CVACTã€VIGORå’ŒUniversity-1652ï¼‰ä¸Šçš„å¹¿æ³›å®žéªŒè¡¨æ˜Žï¼ŒCLNetå®žçŽ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶æä¾›äº†æ›´å¥½çš„å¯è§£é‡Šæ€§å’Œæ³›åŒ–æ€§ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

CLNetæ˜¯ä¸€ä¸ªå¯¹åº”æ„ŸçŸ¥ç‰¹å¾ç»†åŒ–æ¡†æž¶ï¼Œæ•´ä½“æž¶æž„åŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šç¥žç»å¯¹åº”å›¾ï¼ˆNCMï¼‰é€šè¿‡æ½œåœ¨å¯¹åº”åœºå®žçŽ°è·¨è§†å›¾ç‰¹å¾çš„ç©ºé—´å¯¹é½ï¼›éžçº¿æ€§åµŒå…¥è½¬æ¢å™¨ï¼ˆNECï¼‰ä½¿ç”¨MLPå˜æ¢è·¨è§†è§’é‡æ–°æ˜ å°„ç‰¹å¾ï¼›å…¨å±€ç‰¹å¾é‡æ ¡å‡†ï¼ˆGFRï¼‰åŸºäºŽå­¦ä¹ åˆ°çš„ç©ºé—´çº¿ç´¢é‡æ–°åŠ æƒç‰¹å¾é€šé“ã€‚å…³é”®åˆ›æ–°åœ¨äºŽæ˜¾å¼å»ºæ¨¡è·¨è§†å›¾å¯¹åº”å…³ç³»ï¼Œè€Œéžä¾èµ–éšå¼å¯¹é½ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽå°†è§†å›¾å¯¹é½åˆ†è§£ä¸ºå¯å­¦ä¹ çš„äº’è¡¥æ¨¡å—ï¼Œè”åˆæ•èŽ·é«˜çº§è¯­ä¹‰å’Œç»†ç²’åº¦å‡ ä½•ä¿¡æ¯ï¼Œä»Žè€Œæ›´æœ‰æ•ˆåœ°æ¡¥æŽ¥è¯­ä¹‰å’Œå‡ ä½•å·®è·ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨CVUSAã€CVACTã€VIGORå’ŒUniversity-1652å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šï¼ŒCLNetå‡è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†è·¨è§†è§’åœ°ç†å®šä½çš„å‡†ç¡®çŽ‡ï¼ŒåŒæ—¶å®žéªŒéªŒè¯äº†å…¶æ›´å¥½çš„å¯è§£é‡Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯åº”ç”¨äºŽè‡ªåŠ¨é©¾é©¶ã€æ— äººæœºå¯¼èˆªå’Œå¢žå¼ºçŽ°å®žç­‰é¢†åŸŸï¼Œé€šè¿‡è·¨è§†è§’å›¾åƒåŒ¹é…å®žçŽ°ç²¾ç¡®çš„åœ°ç†å®šä½ï¼Œæå‡ä½ç½®æ„ŸçŸ¥ç³»ç»Ÿçš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Image retrieval-based cross-view geo-localization (IRCVGL) aims to match images captured from significantly different viewpoints, such as satellite and street-level images. Existing methods predominantly rely on learning robust global representations or implicit feature alignment, which often fail to model explicit spatial correspondences crucial for accurate localization. In this work, we propose a novel correspondence-aware feature refinement framework, termed CLNet, that explicitly bridges the semantic and geometric gaps between different views. CLNet decomposes the view alignment process into three learnable and complementary modules: a Neural Correspondence Map (NCM) that spatially aligns cross-view features via latent correspondence fields; a Nonlinear Embedding Converter (NEC) that remaps features across perspectives using an MLP-based transformation; and a Global Feature Recalibration (GFR) module that reweights informative feature channels guided by learned spatial cues. The proposed CLNet can jointly capture both high-level semantics and fine-grained alignments. Extensive experiments on four public benchmarks, CVUSA, CVACT, VIGOR, and University-1652, demonstrate that our proposed CLNet achieves state-of-the-art performance while offering better interpretability and generalizability.

