---
layout: default
title: PerfCoder: Large Language Models for Interpretable Code Performance Optimization
---

# PerfCoder: Large Language Models for Interpretable Code Performance Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14018" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14018v1</a>
  <a href="https://arxiv.org/pdf/2512.14018.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14018v1" onclick="toggleFavorite(this, '2512.14018v1', 'PerfCoder: Large Language Models for Interpretable Code Performance Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiuding Yang, Shengyao Lu, Hongxuan Liu, Shayan Shirahmad Gale Bagi, Zahra Fazel, Tomasz Czajkowski, Di Niu

**åˆ†ç±»**: cs.SE, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**PerfCoderï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šä»£ç æ€§èƒ½ä¼˜åŒ–**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä»£ç ä¼˜åŒ–` `å¤§è¯­è¨€æ¨¡å‹` `æ€§èƒ½æå‡` `å¼ºåŒ–å­¦ä¹ ` `ä»£ç ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆé«˜æ€§èƒ½ä»£ç æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹æŒ‡å¯¼å¯è§£é‡Šå’Œæœ‰æ•ˆæ€§èƒ½æ”¹è¿›çš„ç›‘ç£ã€‚
2. PerfCoderé€šè¿‡åœ¨ä¼˜åŒ–è½¨è¿¹ä¸Šå¾®è°ƒLLMï¼Œå¹¶ä½¿ç”¨è¿è¡Œæ—¶æµ‹é‡è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œå®ç°å¯è§£é‡Šçš„ä»£ç æ€§èƒ½ä¼˜åŒ–ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPerfCoderåœ¨ä»£ç æ€§èƒ½åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šç°æœ‰æ¨¡å‹ï¼Œå¹¶èƒ½æå‡æ›´å¤§è§„æ¨¡LLMçš„ä¼˜åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨ä»£ç ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶ç”Ÿæˆé«˜æ€§èƒ½ä»£ç çš„èƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œè¿™åœ¨å®é™…è½¯ä»¶ç³»ç»Ÿä¸­è‡³å…³é‡è¦ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œå½“å‰LLMçš„ä¸è¶³ä¸ä»…åœ¨äºæ•°æ®ç¨€ç¼ºï¼Œæ›´é‡è¦çš„æ˜¯ç¼ºä¹æŒ‡å¯¼å¯è§£é‡Šå’Œæœ‰æ•ˆæ€§èƒ½æ”¹è¿›çš„ç›‘ç£ã€‚æœ¬æ–‡æå‡ºäº†PerfCoderï¼Œä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºé€šè¿‡å¯è§£é‡Šçš„ã€å®šåˆ¶çš„ä¼˜åŒ–ä»æºä»£ç ç”Ÿæˆæ€§èƒ½å¢å¼ºä»£ç çš„LLMå®¶æ—ã€‚PerfCoderåœ¨ä¸€ä¸ªç²¾å¿ƒç­–åˆ’çš„ã€å¸¦æœ‰å¯è¯»æ³¨é‡Šçš„çœŸå®ä¼˜åŒ–è½¨è¿¹é›†åˆä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¹¶é€šè¿‡ä½¿ç”¨è¿è¡Œæ—¶æµ‹é‡çš„å¼ºåŒ–å¾®è°ƒè¿›è¡Œåå¥½å¯¹é½ï¼Œä½¿å…¶èƒ½å¤Ÿæå‡ºç‰¹å®šäºè¾“å…¥çš„æ”¹è¿›ç­–ç•¥å¹¶ç›´æ¥åº”ç”¨å®ƒä»¬ï¼Œè€Œæ— éœ€ä¾èµ–è¿­ä»£æ”¹è¿›ã€‚åœ¨PIEä»£ç æ€§èƒ½åŸºå‡†æµ‹è¯•ä¸­ï¼ŒPerfCoderåœ¨è¿è¡Œæ—¶åŠ é€Ÿå’Œæœ‰æ•ˆä¼˜åŒ–ç‡æ–¹é¢å‡è¶…è¿‡äº†æ‰€æœ‰ç°æœ‰æ¨¡å‹ï¼Œè¡¨æ˜æ€§èƒ½ä¼˜åŒ–ä¸èƒ½ä»…é è§„æ¨¡æ¥å®ç°ï¼Œè¿˜éœ€è¦ä¼˜åŒ–ç­–ç•¥æ„è¯†ã€‚æ­¤å¤–ï¼ŒPerfCoderå¯ä»¥ç”Ÿæˆå…³äºæºä»£ç çš„å¯è§£é‡Šåé¦ˆï¼Œå½“åœ¨è§„åˆ’å™¨-ä¼˜åŒ–å™¨ååŒå·¥ä½œæµç¨‹ä¸­ä½œä¸ºè¾ƒå¤§LLMçš„è¾“å…¥æä¾›æ—¶ï¼Œå¯ä»¥è¿›ä¸€æ­¥æ”¹å–„ç»“æœã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬æå‡äº†32Bæ¨¡å‹å’ŒGPT-5åœ¨ä»£ç ä¼˜åŒ–æ–¹é¢çš„æ€§èƒ½è‡³æ–°çš„æ°´å¹³ï¼Œå¤§å¤§è¶…è¿‡äº†å®ƒä»¬åŸæ¥çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆé«˜æ€§èƒ½ä»£ç æ–¹é¢çš„ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•ç”Ÿæˆçš„ä»£ç æ€§èƒ½ä¸é«˜ï¼Œç¼ºä¹å¯è§£é‡Šçš„ä¼˜åŒ–ç­–ç•¥ï¼Œéš¾ä»¥æ»¡è¶³å®é™…è½¯ä»¶ç³»ç»Ÿçš„éœ€æ±‚ã€‚ç°æœ‰LLMç¼ºä¹æœ‰æ•ˆçš„ç›‘ç£ä¿¡å·ï¼Œæ— æ³•æŒ‡å¯¼å…¶è¿›è¡Œå¯è§£é‡Šå’Œæœ‰æ•ˆçš„æ€§èƒ½æ”¹è¿›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®­ç»ƒä¸€ä¸ªä¸“é—¨ç”¨äºä»£ç æ€§èƒ½ä¼˜åŒ–çš„LLMï¼ˆPerfCoderï¼‰ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆå¯è§£é‡Šçš„ä¼˜åŒ–ç­–ç•¥å¹¶ç›´æ¥åº”ç”¨ã€‚é€šè¿‡åœ¨çœŸå®çš„ä¼˜åŒ–è½¨è¿¹ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¹¶ä½¿ç”¨è¿è¡Œæ—¶æµ‹é‡è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ŒPerfCoderèƒ½å¤Ÿå­¦ä¹ åˆ°æœ‰æ•ˆçš„ä¼˜åŒ–æ¨¡å¼ï¼Œå¹¶ç”Ÿæˆé«˜æ€§èƒ½çš„ä»£ç ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPerfCoderçš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š1) æ•°æ®æ”¶é›†ï¼šæ”¶é›†çœŸå®ä¸–ç•Œä¸­çš„ä»£ç ä¼˜åŒ–è½¨è¿¹ï¼Œå¹¶è¿›è¡Œäººå·¥æ ‡æ³¨ï¼Œæä¾›å¯è§£é‡Šçš„ä¼˜åŒ–ä¿¡æ¯ã€‚2) æ¨¡å‹å¾®è°ƒï¼šåœ¨æ”¶é›†åˆ°çš„ä¼˜åŒ–è½¨è¿¹æ•°æ®ä¸Šå¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ åˆ°ä»£ç ä¼˜åŒ–çš„çŸ¥è¯†ã€‚3) å¼ºåŒ–å­¦ä¹ ï¼šä½¿ç”¨è¿è¡Œæ—¶æµ‹é‡ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œå¯¹æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆæ›´é«˜æ€§èƒ½çš„ä»£ç ã€‚4) è§„åˆ’å™¨-ä¼˜åŒ–å™¨ååŒï¼šå°†PerfCoderç”Ÿæˆçš„ä¼˜åŒ–å»ºè®®æä¾›ç»™æ›´å¤§çš„LLMï¼Œä»¥è¿›ä¸€æ­¥æå‡ä»£ç æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†PerfCoderï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºä»£ç æ€§èƒ½ä¼˜åŒ–çš„LLMã€‚2) ä½¿ç”¨ä¼˜åŒ–è½¨è¿¹å’Œè¿è¡Œæ—¶æµ‹é‡è¿›è¡Œç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°æœ‰æ•ˆçš„ä¼˜åŒ–ç­–ç•¥ã€‚3) æå‡ºäº†è§„åˆ’å™¨-ä¼˜åŒ–å™¨ååŒæ¡†æ¶ï¼Œå°†PerfCoderä¸æ›´å¤§çš„LLMç»“åˆï¼Œè¿›ä¸€æ­¥æå‡ä»£ç æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šPerfCoderçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä¼˜åŒ–è½¨è¿¹çš„æ”¶é›†å’Œæ ‡æ³¨ï¼šç¡®ä¿ä¼˜åŒ–è½¨è¿¹çš„è´¨é‡å’Œå¯è§£é‡Šæ€§ã€‚2) å¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±å‡½æ•°è®¾è®¡ï¼šä½¿ç”¨è¿è¡Œæ—¶æµ‹é‡ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆæ›´é«˜æ€§èƒ½çš„ä»£ç ã€‚3) æ¨¡å‹æ¶æ„çš„é€‰æ‹©ï¼šé€‰æ‹©åˆé€‚çš„LLMæ¶æ„ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œå¹¶è¿›è¡Œå¾®è°ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

PerfCoderåœ¨PIEä»£ç æ€§èƒ½åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†æ‰€æœ‰ç°æœ‰æ¨¡å‹ï¼Œåœ¨è¿è¡Œæ—¶åŠ é€Ÿå’Œæœ‰æ•ˆä¼˜åŒ–ç‡æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚é€šè¿‡ä¸æ›´å¤§çš„LLMï¼ˆå¦‚32Bæ¨¡å‹å’ŒGPT-5ï¼‰ååŒå·¥ä½œï¼ŒPerfCoderèƒ½å¤Ÿè¿›ä¸€æ­¥æå‡å®ƒä»¬çš„æ€§èƒ½ï¼Œè¾¾åˆ°æ–°çš„æ°´å¹³ï¼Œå¤§å¹…è¶…è¿‡å…¶åŸå§‹æ€§èƒ½ã€‚è¿™è¡¨æ˜PerfCoderä¸ä»…è‡ªèº«å…·æœ‰å¼ºå¤§çš„ä¼˜åŒ–èƒ½åŠ›ï¼Œè¿˜èƒ½æœ‰æ•ˆæå‡å…¶ä»–LLMçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PerfCoderå¯åº”ç”¨äºå„ç§è½¯ä»¶å¼€å‘åœºæ™¯ï¼Œä¾‹å¦‚è‡ªåŠ¨ä»£ç ä¼˜åŒ–ã€ç¼–è¯‘å™¨ä¼˜åŒ–ã€æ€§èƒ½åˆ†æå’Œè°ƒè¯•ç­‰ã€‚å®ƒå¯ä»¥å¸®åŠ©å¼€å‘è€…å¿«é€Ÿç”Ÿæˆé«˜æ€§èƒ½çš„ä»£ç ï¼Œæé«˜è½¯ä»¶ç³»ç»Ÿçš„æ•ˆç‡å’Œå¯é æ€§ã€‚æœªæ¥ï¼ŒPerfCoderæœ‰æœ›æˆä¸ºè½¯ä»¶å¼€å‘å·¥å…·é“¾ä¸­çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œæ¨åŠ¨è½¯ä»¶å·¥ç¨‹çš„è‡ªåŠ¨åŒ–å’Œæ™ºèƒ½åŒ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have achieved remarkable progress in automatic code generation, yet their ability to produce high-performance code remains limited--a critical requirement in real-world software systems. We argue that current LLMs struggle not only due to data scarcity but, more importantly, because they lack supervision that guides interpretable and effective performance improvements. In this work, we introduce PerfCoder, a family of LLMs specifically designed to generate performance-enhanced code from source code via interpretable, customized optimizations. PerfCoder is fine-tuned on a curated collection of real-world optimization trajectories with human-readable annotations, and preference-aligned by reinforcement fine-tuning using runtime measurements, enabling it to propose input-specific improvement strategies and apply them directly without relying on iterative refinement. On the PIE code performance benchmark, PerfCoder surpasses all existing models in both runtime speedup and effective optimization rate, demonstrating that performance optimization cannot be achieved by scale alone but requires optimization stratetgy awareness. In addition, PerfCoder can generate interpretable feedback about the source code, which, when provided as input to a larger LLM in a planner-and-optimizer cooperative workflow, can further improve outcomes. Specifically, we elevate the performance of 32B models and GPT-5 to new levels on code optimization, substantially surpassing their original performance.

