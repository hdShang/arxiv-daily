---
layout: default
title: PerfCoder: Large Language Models for Interpretable Code Performance Optimization
---

# PerfCoder: Large Language Models for Interpretable Code Performance Optimization

**arXiv**: [2512.14018v1](https://arxiv.org/abs/2512.14018) | [PDF](https://arxiv.org/pdf/2512.14018.pdf)

**ä½œè€…**: Jiuding Yang, Shengyao Lu, Hongxuan Liu, Shayan Shirahmad Gale Bagi, Zahra Fazel, Tomasz Czajkowski, Di Niu

**åˆ†ç±»**: cs.SE, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPerfCoderæ¨¡åž‹ï¼Œé€šè¿‡å¯è§£é‡Šçš„å®šåˆ¶åŒ–ä¼˜åŒ–è§£å†³å¤§è¯­è¨€æ¨¡åž‹ç”Ÿæˆé«˜æ€§èƒ½ä»£ç çš„éš¾é¢˜**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `ä»£ç æ€§èƒ½ä¼˜åŒ–` `å¤§è¯­è¨€æ¨¡åž‹` `å¯è§£é‡ŠAI` `å¼ºåŒ–å¾®è°ƒ` `è½¯ä»¶å·¥ç¨‹` `è‡ªåŠ¨ä»£ç ç”Ÿæˆ` `ä¼˜åŒ–ç­–ç•¥` `Transformeræž¶æž„`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å¤§è¯­è¨€æ¨¡åž‹åœ¨ç”Ÿæˆé«˜æ€§èƒ½ä»£ç æ–¹é¢å—é™ï¼Œä¸»è¦å› ç¼ºä¹å¯è§£é‡Šçš„ä¼˜åŒ–ç›‘ç£ï¼Œå¯¼è‡´ä»£ç æ€§èƒ½ä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºPerfCoderæ¨¡åž‹ï¼Œé€šè¿‡å¾®è°ƒçœŸå®žä¼˜åŒ–è½¨è¿¹å’Œå¼ºåŒ–å­¦ä¹ å¯¹é½åå¥½ï¼Œå®žçŽ°è¾“å…¥ç‰¹å®šçš„å¯è§£é‡Šæ€§èƒ½ä¼˜åŒ–ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨PIEåŸºå‡†ä¸Šè¶…è¶Šæ‰€æœ‰çŽ°æœ‰æ¨¡åž‹ï¼Œæ˜¾è‘—æå‡è¿è¡Œæ—¶é€Ÿåº¦å’Œä¼˜åŒ–çŽ‡ï¼Œå¹¶å¢žå¼ºå¤§æ¨¡åž‹ä»£ç ä¼˜åŒ–èƒ½åŠ›ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨ä»£ç ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶ç”Ÿæˆé«˜æ€§èƒ½ä»£ç çš„èƒ½åŠ›ä»ç„¶æœ‰é™â€”â€”è¿™æ˜¯çŽ°å®žä¸–ç•Œè½¯ä»¶ç³»ç»Ÿä¸­çš„å…³é”®éœ€æ±‚ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œå½“å‰LLMsçš„å›°éš¾ä¸ä»…æºäºŽæ•°æ®ç¨€ç¼ºï¼Œæ›´é‡è¦çš„æ˜¯ç¼ºä¹æŒ‡å¯¼å¯è§£é‡Šä¸”æœ‰æ•ˆæ€§èƒ½æ”¹è¿›çš„ç›‘ç£ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†PerfCoderï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºŽé€šè¿‡å¯è§£é‡Šçš„å®šåˆ¶åŒ–ä¼˜åŒ–ä»Žæºä»£ç ç”Ÿæˆæ€§èƒ½å¢žå¼ºä»£ç çš„LLMså®¶æ—ã€‚PerfCoderåœ¨ç²¾å¿ƒç­–åˆ’çš„çœŸå®žä¸–ç•Œä¼˜åŒ–è½¨è¿¹é›†åˆä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œè¿™äº›è½¨è¿¹å¸¦æœ‰å¯è¯»çš„äººç±»æ³¨é‡Šï¼Œå¹¶é€šè¿‡ä½¿ç”¨è¿è¡Œæ—¶æµ‹é‡çš„å¼ºåŒ–å¾®è°ƒè¿›è¡Œåå¥½å¯¹é½ï¼Œä½¿å…¶èƒ½å¤Ÿæå‡ºè¾“å…¥ç‰¹å®šçš„æ”¹è¿›ç­–ç•¥å¹¶ç›´æŽ¥åº”ç”¨ï¼Œè€Œä¸ä¾èµ–äºŽè¿­ä»£ä¼˜åŒ–ã€‚åœ¨PIEä»£ç æ€§èƒ½åŸºå‡†æµ‹è¯•ä¸­ï¼ŒPerfCoderåœ¨è¿è¡Œæ—¶åŠ é€Ÿå’Œæœ‰æ•ˆä¼˜åŒ–çŽ‡æ–¹é¢å‡è¶…è¶Šäº†æ‰€æœ‰çŽ°æœ‰æ¨¡åž‹ï¼Œè¡¨æ˜Žæ€§èƒ½ä¼˜åŒ–ä¸èƒ½ä»…é€šè¿‡è§„æ¨¡å®žçŽ°ï¼Œè€Œéœ€è¦ä¼˜åŒ–ç­–ç•¥æ„è¯†ã€‚æ­¤å¤–ï¼ŒPerfCoderå¯ä»¥ç”Ÿæˆå…³äºŽæºä»£ç çš„å¯è§£é‡Šåé¦ˆï¼Œå½“åœ¨è§„åˆ’å™¨ä¸Žä¼˜åŒ–å™¨åä½œå·¥ä½œæµä¸­ä½œä¸ºè¾“å…¥æä¾›ç»™æ›´å¤§çš„LLMæ—¶ï¼Œå¯ä»¥è¿›ä¸€æ­¥æ”¹å–„ç»“æžœã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†32Bæ¨¡åž‹å’ŒGPT-5åœ¨ä»£ç ä¼˜åŒ–æ–¹é¢çš„æ€§èƒ½æå‡åˆ°æ–°æ°´å¹³ï¼Œå¤§å¹…è¶…è¶Šäº†å®ƒä»¬çš„åŽŸå§‹æ€§èƒ½ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨ä»£ç ç”Ÿæˆä¸­éš¾ä»¥ç”Ÿæˆé«˜æ€§èƒ½ä»£ç çš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•çš„ç—›ç‚¹åŒ…æ‹¬ï¼šæ•°æ®ç¨€ç¼ºå¯¼è‡´æ¨¡åž‹ç¼ºä¹ä¼˜åŒ–ç¤ºä¾‹ï¼Œæ›´é‡è¦çš„æ˜¯ç¼ºä¹å¯è§£é‡Šçš„ç›‘ç£ä¿¡å·ï¼Œä½¿å¾—æ¨¡åž‹æ— æ³•æœ‰æ•ˆæŒ‡å¯¼æ€§èƒ½æ”¹è¿›ï¼Œå¾€å¾€ä¾èµ–è¿­ä»£ä¼˜åŒ–æˆ–è§„æ¨¡æ‰©å±•ï¼Œæ•ˆçŽ‡ä½Žä¸‹ä¸”æ•ˆæžœæœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥å¯è§£é‡Šçš„ä¼˜åŒ–ç›‘ç£ï¼Œè®­ç»ƒä¸“é—¨çš„å¤§è¯­è¨€æ¨¡åž‹æ¥ç”Ÿæˆæ€§èƒ½å¢žå¼ºçš„ä»£ç ã€‚è®¾è®¡åŸºäºŽçœŸå®žä¸–ç•Œçš„ä¼˜åŒ–è½¨è¿¹ï¼Œç»“åˆäººç±»å¯è¯»æ³¨é‡Šå’Œè¿è¡Œæ—¶æµ‹é‡ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿå­¦ä¹ è¾“å…¥ç‰¹å®šçš„ä¼˜åŒ–ç­–ç•¥ï¼Œå¹¶ç›´æŽ¥åº”ç”¨ï¼Œé¿å…è¿­ä»£è¿‡ç¨‹ï¼Œä»Žè€Œæé«˜ä¼˜åŒ–æ•ˆçŽ‡å’Œæ•ˆæžœã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æž¶æž„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œåœ¨ç²¾å¿ƒç­–åˆ’çš„ä¼˜åŒ–è½¨è¿¹æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œè¿™äº›æ•°æ®åŒ…å«æºä»£ç ã€ä¼˜åŒ–ç‰ˆæœ¬å’Œäººç±»æ³¨é‡Šï¼›å…¶æ¬¡ï¼Œé€šè¿‡å¼ºåŒ–å¾®è°ƒï¼ˆå¦‚ä½¿ç”¨è¿è¡Œæ—¶æµ‹é‡ä½œä¸ºå¥–åŠ±ï¼‰å¯¹é½æ¨¡åž‹åå¥½ï¼Œç¡®ä¿ç”Ÿæˆçš„ä»£ç åœ¨æ€§èƒ½ä¸Šæ›´ä¼˜ã€‚æ¨¡åž‹é‡‡ç”¨æ ‡å‡†Transformeræž¶æž„ï¼Œä½†è®­ç»ƒæµç¨‹ä¸“é—¨é’ˆå¯¹æ€§èƒ½ä¼˜åŒ–ä»»åŠ¡å®šåˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹æ˜¯ç»“åˆäº†å¯è§£é‡Šçš„ä¼˜åŒ–ç›‘ç£å’Œå¼ºåŒ–å­¦ä¹ åå¥½å¯¹é½ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒPerfCoderä¸ä»…ä¾èµ–å¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œè€Œæ˜¯é€šè¿‡ç»“æž„åŒ–ä¼˜åŒ–æ•°æ®å¾®è°ƒï¼Œä½¿æ¨¡åž‹å…·å¤‡ä¼˜åŒ–ç­–ç•¥æ„è¯†ï¼Œèƒ½å¤Ÿç”Ÿæˆå®šåˆ¶åŒ–ã€å¯è§£é‡Šçš„æ”¹è¿›ï¼Œè€Œéžä»…é è§„æ¨¡æˆ–è¿­ä»£ä¼˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼šä½¿ç”¨çœŸå®žä¸–ç•Œä¼˜åŒ–è½¨è¿¹æ•°æ®é›†è¿›è¡Œç›‘ç£å¾®è°ƒï¼ŒæŸå¤±å‡½æ•°åŸºäºŽä»£ç ç”Ÿæˆå’Œæ³¨é‡Šé¢„æµ‹ï¼›å¼ºåŒ–å¾®è°ƒé˜¶æ®µé‡‡ç”¨è¿è¡Œæ—¶é€Ÿåº¦ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œé€šè¿‡ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¼˜åŒ–æ¨¡åž‹è¾“å‡ºï¼›æ¨¡åž‹æž¶æž„åŸºäºŽçŽ°æœ‰LLMsï¼ˆå¦‚GPTç³»åˆ—ï¼‰ï¼Œä½†è®­ç»ƒæ•°æ®å’Œæ–¹æ³•ä¸“é—¨é’ˆå¯¹æ€§èƒ½ä¼˜åŒ–ï¼Œå‚æ•°è®¾ç½®å¯èƒ½æ¶‰åŠå¤šä»»åŠ¡å­¦ä¹ ä»¥å¹³è¡¡ä»£ç ç”Ÿæˆå’Œè§£é‡Šç”Ÿæˆã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨PIEä»£ç æ€§èƒ½åŸºå‡†æµ‹è¯•ä¸­ï¼ŒPerfCoderåœ¨è¿è¡Œæ—¶åŠ é€Ÿå’Œæœ‰æ•ˆä¼˜åŒ–çŽ‡æ–¹é¢å‡è¶…è¶Šæ‰€æœ‰çŽ°æœ‰æ¨¡åž‹ï¼Œå…·ä½“æ•°æ®æœªåœ¨æ‘˜è¦ä¸­æä¾›ï¼Œä½†è¡¨æ˜Žæ€§èƒ½æ˜¾è‘—æå‡ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯è§£é‡Šåé¦ˆä¸Žæ›´å¤§LLMï¼ˆå¦‚32Bæ¨¡åž‹å’ŒGPT-5ï¼‰åä½œï¼Œè¿›ä¸€æ­¥æå‡äº†ä»£ç ä¼˜åŒ–æ•ˆæžœï¼Œå¤§å¹…è¶…è¶ŠåŽŸå§‹æ€§èƒ½ï¼ŒéªŒè¯äº†ä¼˜åŒ–ç­–ç•¥æ„è¯†çš„é‡è¦æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è½¯ä»¶å·¥ç¨‹å’Œç³»ç»Ÿä¼˜åŒ–é¢†åŸŸå…·æœ‰å¹¿æ³›æ½œåœ¨åº”ç”¨ï¼Œå¯ç”¨äºŽè‡ªåŠ¨ä»£ç æ€§èƒ½æå‡ã€ç¼–è¯‘å™¨ä¼˜åŒ–è¾…åŠ©ã€å®žæ—¶ç³»ç»Ÿä»£ç ç”Ÿæˆç­‰åœºæ™¯ã€‚å®žé™…ä»·å€¼åœ¨äºŽæé«˜è½¯ä»¶å¼€å‘æ•ˆçŽ‡ï¼Œå‡å°‘äººå·¥ä¼˜åŒ–æˆæœ¬ï¼Œå¹¶å¢žå¼ºä»£ç çš„å¯ç»´æŠ¤æ€§å’Œæ€§èƒ½ã€‚æœªæ¥å¯èƒ½å½±å“AIè¾…åŠ©ç¼–ç¨‹å·¥å…·çš„å‘å±•ï¼ŒæŽ¨åŠ¨å¤§è¯­è¨€æ¨¡åž‹åœ¨ä¸“ä¸šé¢†åŸŸï¼ˆå¦‚é«˜æ€§èƒ½è®¡ç®—ã€åµŒå…¥å¼ç³»ç»Ÿï¼‰çš„æ·±å…¥åº”ç”¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models (LLMs) have achieved remarkable progress in automatic code generation, yet their ability to produce high-performance code remains limited--a critical requirement in real-world software systems. We argue that current LLMs struggle not only due to data scarcity but, more importantly, because they lack supervision that guides interpretable and effective performance improvements. In this work, we introduce PerfCoder, a family of LLMs specifically designed to generate performance-enhanced code from source code via interpretable, customized optimizations. PerfCoder is fine-tuned on a curated collection of real-world optimization trajectories with human-readable annotations, and preference-aligned by reinforcement fine-tuning using runtime measurements, enabling it to propose input-specific improvement strategies and apply them directly without relying on iterative refinement. On the PIE code performance benchmark, PerfCoder surpasses all existing models in both runtime speedup and effective optimization rate, demonstrating that performance optimization cannot be achieved by scale alone but requires optimization stratetgy awareness. In addition, PerfCoder can generate interpretable feedback about the source code, which, when provided as input to a larger LLM in a planner-and-optimizer cooperative workflow, can further improve outcomes. Specifically, we elevate the performance of 32B models and GPT-5 to new levels on code optimization, substantially surpassing their original performance.

