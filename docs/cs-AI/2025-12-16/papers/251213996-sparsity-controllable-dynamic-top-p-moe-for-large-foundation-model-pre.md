---
layout: default
title: Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training
---

# Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.13996" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.13996</a>
  <a href="https://arxiv.org/pdf/2512.13996.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.13996" onclick="toggleFavorite(this, '2512.13996', 'Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Can Jin, Hongwu Peng, Mingcan Xiang, Qixin Zhang, Xiangchi Yuan, Amit Hasan, Ohiremen Dibua, Yifan Gong, Yan Kang, Dimitris N. Metaxas

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDTop-p MoEï¼Œå®ç°ç¨€ç–åº¦å¯æ§çš„åŠ¨æ€Top-pè·¯ç”±ï¼Œæå‡å¤§æ¨¡å‹é¢„è®­ç»ƒæ•ˆæœã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ··åˆä¸“å®¶æ¨¡å‹` `MoE` `ç¨€ç–è·¯ç”±` `Top-pè·¯ç”±` `åŠ¨æ€é˜ˆå€¼` `PIæ§åˆ¶å™¨` `å¤§æ¨¡å‹é¢„è®­ç»ƒ` `è®¡ç®—æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸTop-kè·¯ç”±åœ¨MoEä¸­å¼ºåˆ¶ç»Ÿä¸€ç¨€ç–æ€§ï¼Œå¿½ç•¥äº†ä¸åŒtokençš„éš¾åº¦å·®å¼‚ï¼Œè€Œå›ºå®šé˜ˆå€¼çš„Top-pè·¯ç”±éš¾ä»¥æ§åˆ¶è®¡ç®—æˆæœ¬ã€‚
2. DTop-p MoEåˆ©ç”¨PIæ§åˆ¶å™¨åŠ¨æ€è°ƒæ•´Top-pé˜ˆå€¼ï¼Œä½¿æ¿€æ´»ä¸“å®¶æ•°é‡ä¸ç›®æ ‡ç¨€ç–åº¦å¯¹é½ï¼Œå®ç°ç¨€ç–åº¦å¯æ§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDTop-påœ¨LLMå’ŒDiffusion Transformerä¸Šä¼˜äºTop-kå’Œå›ºå®šé˜ˆå€¼Top-pï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„æ‰©å±•æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¨€ç–æ··åˆä¸“å®¶(MoE)æ¶æ„é€šè¿‡ä»…æ¿€æ´»æ¯ä¸ªè¾“å…¥tokençš„ä¸“å®¶å­é›†æ¥æœ‰æ•ˆåœ°æ‰©å±•æ¨¡å‹å®¹é‡ã€‚ç„¶è€Œï¼Œæ ‡å‡†çš„Top-kè·¯ç”±ç­–ç•¥æ–½åŠ äº†ä¸€ç§ç»Ÿä¸€çš„ç¨€ç–æ¨¡å¼ï¼Œå¿½ç•¥äº†tokenéš¾åº¦çš„å˜åŒ–ã€‚è™½ç„¶Top-pè·¯ç”±æä¾›äº†ä¸€ç§çµæ´»çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç°æœ‰çš„å®ç°é€šå¸¸ä¾èµ–äºå›ºå®šçš„å…¨å±€æ¦‚ç‡é˜ˆå€¼ï¼Œè¿™å¯¼è‡´äº†ä¸å¯æ§çš„è®¡ç®—æˆæœ¬å’Œå¯¹è¶…å‚æ•°é€‰æ‹©çš„æ•æ„Ÿæ€§ã€‚æœ¬æ–‡æå‡ºäº†DTop-p MoEï¼Œä¸€ç§ç¨€ç–åº¦å¯æ§çš„åŠ¨æ€Top-pè·¯ç”±æœºåˆ¶ã€‚ä¸ºäº†è§£å†³ä¼˜åŒ–ä¸å¯å¾®é˜ˆå€¼çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åˆ©ç”¨æ¯”ä¾‹-ç§¯åˆ†(PI)æ§åˆ¶å™¨åŠ¨æ€è°ƒæ•´æ¦‚ç‡é˜ˆå€¼ï¼Œä½¿è¿è¡Œæ¿€æ´»çš„ä¸“å®¶ç¨€ç–åº¦ä¸æŒ‡å®šçš„targetå¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŠ¨æ€è·¯ç”±å½’ä¸€åŒ–æœºåˆ¶ï¼Œè¯¥æœºåˆ¶è°ƒæ•´å±‚çº§çš„è·¯ç”±logitsï¼Œå…è®¸ä¸åŒçš„å±‚å­¦ä¹ ä¸åŒçš„ä¸“å®¶é€‰æ‹©æ¨¡å¼ï¼ŒåŒæ—¶ä½¿ç”¨å…¨å±€æ¦‚ç‡é˜ˆå€¼ã€‚åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£Transformerä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDTop-på§‹ç»ˆä¼˜äºTop-kå’Œå›ºå®šé˜ˆå€¼Top-påŸºçº¿ã€‚æˆ‘ä»¬çš„åˆ†æè¯å®ï¼ŒDTop-pä¿æŒå¯¹æ¿€æ´»ä¸“å®¶æ•°é‡çš„ç²¾ç¡®æ§åˆ¶ï¼ŒåŒæ—¶è‡ªé€‚åº”åœ°åœ¨ä¸åŒçš„tokenå’Œå±‚ä¹‹é—´åˆ†é…èµ„æºã€‚æ­¤å¤–ï¼ŒDTop-påœ¨ä¸“å®¶ç²’åº¦ã€ä¸“å®¶å®¹é‡ã€æ¨¡å‹å¤§å°å’Œæ•°æ®é›†å¤§å°æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„ç¼©æ”¾ç‰¹æ€§ï¼Œä¸ºå¤§è§„æ¨¡MoEé¢„è®­ç»ƒæä¾›äº†ä¸€ä¸ªé²æ£’çš„æ¡†æ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰MoEæ¨¡å‹ä¸­çš„Top-kè·¯ç”±ç­–ç•¥å¯¹æ‰€æœ‰tokené‡‡ç”¨ç›¸åŒçš„ç¨€ç–åº¦ï¼Œæ— æ³•æ ¹æ®tokençš„éš¾æ˜“ç¨‹åº¦è‡ªé€‚åº”åœ°åˆ†é…è®¡ç®—èµ„æºã€‚è€ŒTop-pè·¯ç”±è™½ç„¶å¯ä»¥è‡ªé€‚åº”åœ°é€‰æ‹©ä¸“å®¶ï¼Œä½†ä¾èµ–äºå›ºå®šçš„å…¨å±€æ¦‚ç‡é˜ˆå€¼ï¼Œéš¾ä»¥æ§åˆ¶è®¡ç®—æˆæœ¬ï¼Œä¸”å¯¹è¶…å‚æ•°æ•æ„Ÿã€‚è¿™é™åˆ¶äº†MoEæ¨¡å‹åœ¨å¤§è§„æ¨¡é¢„è®­ç»ƒä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDTop-p MoEçš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥ä¸€ä¸ªåŠ¨æ€è°ƒæ•´çš„Top-pé˜ˆå€¼ï¼Œè¯¥é˜ˆå€¼ç”±ä¸€ä¸ªæ¯”ä¾‹-ç§¯åˆ†(PI)æ§åˆ¶å™¨æ§åˆ¶ã€‚PIæ§åˆ¶å™¨æ ¹æ®å½“å‰æ¿€æ´»çš„ä¸“å®¶æ•°é‡ä¸ç›®æ ‡ç¨€ç–åº¦ä¹‹é—´çš„å·®å¼‚ï¼ŒåŠ¨æ€åœ°è°ƒæ•´æ¦‚ç‡é˜ˆå€¼ï¼Œä»è€Œå®ç°å¯¹æ¿€æ´»ä¸“å®¶æ•°é‡çš„ç²¾ç¡®æ§åˆ¶ã€‚åŒæ—¶ï¼Œå¼•å…¥åŠ¨æ€è·¯ç”±å½’ä¸€åŒ–æœºåˆ¶ï¼Œå…è®¸ä¸åŒå±‚å­¦ä¹ ä¸åŒçš„ä¸“å®¶é€‰æ‹©æ¨¡å¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDTop-p MoEçš„æ•´ä½“æ¡†æ¶ä¸æ ‡å‡†çš„MoEæ¨¡å‹ç±»ä¼¼ï¼Œä¸»è¦åŒºåˆ«åœ¨äºè·¯ç”±æœºåˆ¶ã€‚å¯¹äºæ¯ä¸ªè¾“å…¥tokenï¼Œé¦–å…ˆè®¡ç®—è·¯ç”±logitsã€‚ç„¶åï¼Œé€šè¿‡åŠ¨æ€Top-pè·¯ç”±é€‰æ‹©æ¿€æ´»çš„ä¸“å®¶ã€‚PIæ§åˆ¶å™¨æ ¹æ®æ¿€æ´»çš„ä¸“å®¶æ•°é‡ä¸ç›®æ ‡ç¨€ç–åº¦ä¹‹é—´çš„å·®å¼‚ï¼ŒåŠ¨æ€è°ƒæ•´Top-pé˜ˆå€¼ã€‚æœ€åï¼Œå°†tokenåˆ†é…ç»™é€‰å®šçš„ä¸“å®¶è¿›è¡Œå¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šDTop-p MoEçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ç§ç¨€ç–åº¦å¯æ§çš„åŠ¨æ€Top-pè·¯ç”±æœºåˆ¶ï¼Œè§£å†³äº†ä¼ ç»ŸTop-pè·¯ç”±éš¾ä»¥æ§åˆ¶è®¡ç®—æˆæœ¬çš„é—®é¢˜ã€‚2) åˆ©ç”¨PIæ§åˆ¶å™¨åŠ¨æ€è°ƒæ•´Top-pé˜ˆå€¼ï¼Œå®ç°äº†å¯¹æ¿€æ´»ä¸“å®¶æ•°é‡çš„ç²¾ç¡®æ§åˆ¶ã€‚3) å¼•å…¥åŠ¨æ€è·¯ç”±å½’ä¸€åŒ–æœºåˆ¶ï¼Œå…è®¸ä¸åŒå±‚å­¦ä¹ ä¸åŒçš„ä¸“å®¶é€‰æ‹©æ¨¡å¼ã€‚

**å…³é”®è®¾è®¡**ï¼šPIæ§åˆ¶å™¨çš„å‚æ•°ï¼ˆæ¯”ä¾‹å¢ç›Šå’Œç§¯åˆ†å¢ç›Šï¼‰éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚åŠ¨æ€è·¯ç”±å½’ä¸€åŒ–æœºåˆ¶é€šè¿‡å­¦ä¹ æ¯ä¸ªå±‚çš„ç¼©æ”¾å› å­æ¥å®ç°ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æ ‡å‡†çš„é¢„è®­ç»ƒæŸå¤±å’Œç”¨äºæ§åˆ¶ç¨€ç–åº¦çš„è¾…åŠ©æŸå¤±ã€‚ç›®æ ‡ç¨€ç–åº¦æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œéœ€è¦æ ¹æ®è®¡ç®—èµ„æºå’Œæ¨¡å‹æ€§èƒ½è¿›è¡Œæƒè¡¡ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.13996/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.13996/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.13996/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDTop-p MoEåœ¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£Transformerä¸Šå‡ä¼˜äºTop-kå’Œå›ºå®šé˜ˆå€¼Top-påŸºçº¿ã€‚ä¾‹å¦‚ï¼Œåœ¨è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸­ï¼ŒDTop-påœ¨ä¿æŒç›¸åŒè®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿè·å¾—æ›´é«˜çš„perplexityã€‚æ­¤å¤–ï¼ŒDTop-påœ¨ä¸åŒä¸“å®¶ç²’åº¦ã€ä¸“å®¶å®¹é‡ã€æ¨¡å‹å¤§å°å’Œæ•°æ®é›†å¤§å°ä¸‹å‡è¡¨ç°å‡ºè‰¯å¥½çš„æ‰©å±•æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DTop-p MoEå¯åº”ç”¨äºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€è§†è§‰æ¨¡å‹ç­‰å„ç§æ·±åº¦å­¦ä¹ æ¨¡å‹çš„é¢„è®­ç»ƒï¼Œå°¤å…¶é€‚ç”¨äºè®¡ç®—èµ„æºæœ‰é™çš„åœºæ™¯ã€‚é€šè¿‡ç²¾ç¡®æ§åˆ¶ç¨€ç–åº¦ï¼Œå¯ä»¥åœ¨ä¿è¯æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œé™ä½è®¡ç®—æˆæœ¬ï¼ŒåŠ é€Ÿæ¨¡å‹è®­ç»ƒã€‚è¯¥æ–¹æ³•æœ‰æœ›æ¨åŠ¨æ›´å¤§è§„æ¨¡ã€æ›´é«˜æ•ˆçš„AIæ¨¡å‹çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sparse Mixture-of-Experts (MoE) architectures effectively scale model capacity by activating only a subset of experts for each input token. However, the standard Top-k routing strategy imposes a uniform sparsity pattern that ignores the varying difficulty of tokens. While Top-p routing offers a flexible alternative, existing implementations typically rely on a fixed global probability threshold, which results in uncontrolled computational costs and sensitivity to hyperparameter selection. In this paper, we propose DTop-p MoE, a sparsity-controllable dynamic Top-p routing mechanism. To resolve the challenge of optimizing a non-differentiable threshold, we utilize a Proportional-Integral (PI) Controller that dynamically adjusts the probability threshold to align the running activated-expert sparsity with a specified target. Furthermore, we introduce a dynamic routing normalization mechanism that adapts layer-wise routing logits, allowing different layers to learn distinct expert-selection patterns while utilizing a global probability threshold. Extensive experiments on Large Language Models and Diffusion Transformers demonstrate that DTop-p consistently outperforms both Top-k and fixed-threshold Top-p baselines. Our analysis confirms that DTop-p maintains precise control over the number of activated experts while adaptively allocating resources across different tokens and layers. Furthermore, DTop-p exhibits strong scaling properties with respect to expert granularity, expert capacity, model size, and dataset size, offering a robust framework for large-scale MoE pre-training.

