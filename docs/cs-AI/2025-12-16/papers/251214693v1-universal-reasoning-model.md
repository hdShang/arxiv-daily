---
layout: default
title: Universal Reasoning Model
---

# Universal Reasoning Model

**arXiv**: [2512.14693v1](https://arxiv.org/abs/2512.14693) | [PDF](https://arxiv.org/pdf/2512.14693.pdf)

**ä½œè€…**: Zitian Gao, Lynx Chen, Yihao Xiao, He Xing, Ran Tao, Haoming Luo, Joey Zhou, Bryan Dai

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/zitian-gao/URM)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€šç”¨æŽ¨ç†æ¨¡åž‹ä»¥æå‡å¤æ‚æŽ¨ç†ä»»åŠ¡æ€§èƒ½ï¼Œåœ¨ARC-AGIåŸºå‡†ä¸Šå®žçŽ°æ–°çªç ´**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **è§†è§‰é‡Œç¨‹è®¡**

**å…³é”®è¯**: `é€šç”¨æŽ¨ç†æ¨¡åž‹` `Transformerå¢žå¼º` `å¤æ‚æŽ¨ç†ä»»åŠ¡` `ARC-AGIåŸºå‡†` `çŸ­å·ç§¯` `æˆªæ–­åå‘ä¼ æ’­` `å¾ªçŽ¯å½’çº³åç½®` `éžçº¿æ€§ç»„ä»¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰é€šç”¨Transformeråœ¨å¤æ‚æŽ¨ç†ä»»åŠ¡ä¸­æ€§èƒ½æå‡æ¥æºä¸æ˜Žç¡®ï¼Œé™åˆ¶äº†è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚
2. é€šè¿‡åˆ†æžå‘çŽ°æ€§èƒ½æºäºŽå¾ªçŽ¯å½’çº³åç½®å’Œå¼ºéžçº¿æ€§ï¼Œæå‡ºå¢žå¼ºé€šç”¨Transformerçš„é€šç”¨æŽ¨ç†æ¨¡åž‹ã€‚
3. åœ¨ARC-AGIåŸºå‡†ä¸Šå®žçŽ°æ˜¾è‘—æå‡ï¼Œè¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é€šç”¨Transformerï¼ˆUTsï¼‰å·²å¹¿æ³›åº”ç”¨äºŽARC-AGIå’Œæ•°ç‹¬ç­‰å¤æ‚æŽ¨ç†ä»»åŠ¡ï¼Œä½†å…¶æ€§èƒ½æå‡çš„å…·ä½“æ¥æºå°šæœªå¾—åˆ°å……åˆ†æŽ¢ç´¢ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåˆ†æžäº†UTsçš„å˜ä½“ï¼Œå‘çŽ°ARC-AGIä¸Šçš„æ”¹è¿›ä¸»è¦æºäºŽTransformerçš„å¾ªçŽ¯å½’çº³åç½®å’Œå¼ºéžçº¿æ€§ç»„ä»¶ï¼Œè€Œéžå¤æ‚çš„æž¶æž„è®¾è®¡ã€‚åŸºäºŽè¿™ä¸€å‘çŽ°ï¼Œæˆ‘ä»¬æå‡ºäº†é€šç”¨æŽ¨ç†æ¨¡åž‹ï¼ˆURMï¼‰ï¼Œé€šè¿‡å¼•å…¥çŸ­å·ç§¯å’Œæˆªæ–­åå‘ä¼ æ’­æ¥å¢žå¼ºUTã€‚è¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æŽ¨ç†æ€§èƒ½ï¼Œåœ¨ARC-AGI 1ä¸Šè¾¾åˆ°äº†53.8%çš„pass@1ï¼Œåœ¨ARC-AGI 2ä¸Šè¾¾åˆ°äº†16.0%çš„pass@1ï¼Œå®žçŽ°äº†æœ€å…ˆè¿›æ°´å¹³ã€‚ä»£ç å·²å¼€æºï¼šhttps://github.com/zitian-gao/URMã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤æ‚æŽ¨ç†ä»»åŠ¡ï¼ˆå¦‚ARC-AGIå’Œæ•°ç‹¬ï¼‰ä¸­ï¼Œé€šç”¨Transformerï¼ˆUTsï¼‰æ€§èƒ½æå‡æ¥æºä¸æ˜Žç¡®çš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•ä¾èµ–å¤æ‚æž¶æž„è®¾è®¡ï¼Œä½†ç¼ºä¹å¯¹æ ¸å¿ƒé©±åŠ¨å› ç´ çš„æ·±å…¥ç†è§£ï¼Œå¯¼è‡´ä¼˜åŒ–æ–¹å‘æ¨¡ç³Šï¼Œé™åˆ¶äº†æ¨¡åž‹åœ¨æŽ¨ç†ä»»åŠ¡ä¸Šçš„è¿›ä¸€æ­¥çªç ´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç³»ç»Ÿåˆ†æžUTså˜ä½“ï¼Œè¯†åˆ«å‡ºæ€§èƒ½æå‡çš„å…³é”®å› ç´ â€”â€”å¾ªçŽ¯å½’çº³åç½®å’Œå¼ºéžçº¿æ€§ç»„ä»¶ï¼Œè€Œéžå¤æ‚æž¶æž„ã€‚åŸºäºŽæ­¤ï¼Œè®¾è®¡ä¸€ä¸ªå¢žå¼ºåž‹æ¨¡åž‹ï¼Œé€šè¿‡å¼•å…¥çŸ­å·ç§¯å’Œæˆªæ–­åå‘ä¼ æ’­æ¥å¼ºåŒ–è¿™äº›å› ç´ ï¼Œä»Žè€Œæå‡æŽ¨ç†èƒ½åŠ›ï¼Œé¿å…ä¸å¿…è¦çš„æž¶æž„å¤æ‚æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æž¶æž„åŸºäºŽé€šç”¨Transformerï¼Œå¢žå¼ºä¸ºé€šç”¨æŽ¨ç†æ¨¡åž‹ï¼ˆURMï¼‰ã€‚æµç¨‹åŒ…æ‹¬ï¼šè¾“å…¥å¤„ç†é˜¶æ®µï¼Œä½¿ç”¨æ ‡å‡†Transformerç¼–ç å™¨ï¼›æ ¸å¿ƒæŽ¨ç†é˜¶æ®µï¼Œé›†æˆçŸ­å·ç§¯æ¨¡å—ä»¥å¢žå¼ºå±€éƒ¨ç‰¹å¾æå–å’Œå¾ªçŽ¯å½’çº³åç½®ï¼›è®­ç»ƒé˜¶æ®µï¼Œé‡‡ç”¨æˆªæ–­åå‘ä¼ æ’­ä¼˜åŒ–è®¡ç®—æ•ˆçŽ‡ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬Transformerå±‚ã€çŸ­å·ç§¯å±‚å’Œè®­ç»ƒä¼˜åŒ–æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯ç»“åˆçŸ­å·ç§¯å’Œæˆªæ–­åå‘ä¼ æ’­æ¥å¢žå¼ºé€šç”¨Transformerã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬è´¨åŒºåˆ«åœ¨äºŽä¸ä¾èµ–å¤æ‚æž¶æž„è®¾è®¡ï¼Œè€Œæ˜¯èšç„¦äºŽå¼ºåŒ–å·²è¯†åˆ«çš„å…³é”®å› ç´ ï¼ˆå¾ªçŽ¯å½’çº³åç½®å’Œå¼ºéžçº¿æ€§ï¼‰ï¼Œä»Žè€Œæ›´ç›´æŽ¥åœ°æå‡æŽ¨ç†æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒæ¨¡åž‹ç®€æ´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼šçŸ­å·ç§¯å±‚ç”¨äºŽå¢žå¼ºå±€éƒ¨æ¨¡å¼æ•æ‰ï¼Œå…·ä½“å‚æ•°è®¾ç½®å¦‚å·ç§¯æ ¸å¤§å°å’Œæ­¥é•¿éœ€æ ¹æ®ä»»åŠ¡è°ƒæ•´ï¼›æˆªæ–­åå‘ä¼ æ’­ç”¨äºŽå‡å°‘è®­ç»ƒæ—¶çš„è®¡ç®—å¼€é”€ï¼Œæé«˜æ•ˆçŽ‡ï¼›ç½‘ç»œç»“æž„ä¿æŒTransformeråŸºç¡€ï¼Œä½†é›†æˆè¿™äº›ç»„ä»¶ï¼›æŸå¤±å‡½æ•°é€šå¸¸ä½¿ç”¨æ ‡å‡†äº¤å‰ç†µæˆ–ä»»åŠ¡ç‰¹å®šæŸå¤±ï¼Œå…·ä½“ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜Žï¼Œéœ€å‚è€ƒå¼€æºä»£ç ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

æœ€é‡è¦çš„å®žéªŒç»“æžœåŒ…æ‹¬ï¼šåœ¨ARC-AGI 1åŸºå‡†ä¸Šï¼ŒURMè¾¾åˆ°53.8% pass@1ï¼Œåœ¨ARC-AGI 2åŸºå‡†ä¸Šè¾¾åˆ°16.0% pass@1ï¼Œå‡å®žçŽ°æœ€å…ˆè¿›æ°´å¹³ã€‚å¯¹æ¯”åŸºçº¿å¯èƒ½åŒ…æ‹¬æ ‡å‡†UTså’Œå…¶ä»–å˜ä½“ï¼Œæå‡å¹…åº¦æ˜¾è‘—ï¼Œå…·ä½“æ•°æ®åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†å¯¹æ¯”ï¼Œä½†æ‘˜è¦è¡¨æ˜Žâ€œsubstantially improvesâ€ï¼ŒéªŒè¯äº†æ–¹æ³•åœ¨å¤æ‚æŽ¨ç†ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨å¤æ‚æŽ¨ç†ä»»åŠ¡é¢†åŸŸå…·æœ‰å¹¿æ³›æ½œåœ¨åº”ç”¨ï¼Œå¦‚äººå·¥æ™ºèƒ½é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰åŸºå‡†æµ‹è¯•ï¼ˆå¦‚ARC-AGIï¼‰ã€é€»è¾‘æ¸¸æˆï¼ˆå¦‚æ•°ç‹¬ï¼‰å’Œéœ€è¦é«˜çº§æŽ¨ç†çš„AIç³»ç»Ÿã€‚å®žé™…ä»·å€¼åœ¨äºŽæä¾›äº†ä¸€ç§é«˜æ•ˆã€ç®€æ´çš„æ¨¡åž‹ä¼˜åŒ–æ–¹æ³•ï¼Œå¯æŽ¨åŠ¨æŽ¨ç†æ¨¡åž‹çš„å‘å±•ï¼Œæœªæ¥å¯èƒ½å½±å“æ•™è‚²è¾…åŠ©ã€è‡ªåŠ¨åŒ–å†³ç­–å’Œæ™ºèƒ½æœºå™¨äººç­‰é¢†åŸŸï¼Œæå‡AIçš„æŽ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM.

