---
layout: default
title: Universal Reasoning Model
---

# Universal Reasoning Model

**arXiv**: [2512.14693v1](https://arxiv.org/abs/2512.14693) | [PDF](https://arxiv.org/pdf/2512.14693.pdf)

**ä½œè€…**: Zitian Gao, Lynx Chen, Yihao Xiao, He Xing, Ran Tao, Haoming Luo, Joey Zhou, Bryan Dai

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/zitian-gao/URM)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€šç”¨æŽ¨ç†æ¨¡åž‹ä»¥æå‡å¤æ‚æŽ¨ç†ä»»åŠ¡æ€§èƒ½ï¼Œåœ¨ARC-AGIåŸºå‡†ä¸Šå®žçŽ°æ–°çªç ´**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **è§†è§‰é‡Œç¨‹è®¡**

**å…³é”®è¯**: `é€šç”¨æŽ¨ç†æ¨¡åž‹` `Transformeræž¶æž„` `å¤æ‚æŽ¨ç†ä»»åŠ¡` `ARC-AGIåŸºå‡†` `çŸ­å·ç§¯` `æˆªæ–­åå‘ä¼ æ’­` `å¾ªçŽ¯å½’çº³åç½®` `éžçº¿æ€§ç»„ä»¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰é€šç”¨Transformeråœ¨å¤æ‚æŽ¨ç†ä»»åŠ¡ä¸­æ€§èƒ½æ¥æºä¸æ˜Žï¼Œé™åˆ¶äº†è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚
2. é€šè¿‡åˆ†æžå‘çŽ°æ€§èƒ½æå‡æºäºŽå¾ªçŽ¯å½’çº³åç½®å’Œå¼ºéžçº¿æ€§ï¼Œæå‡ºå¢žå¼ºé€šç”¨æŽ¨ç†æ¨¡åž‹ã€‚
3. åœ¨ARC-AGIåŸºå‡†ä¸Šå®žçŽ°æ˜¾è‘—æå‡ï¼Œè¾¾åˆ°53.8%å’Œ16.0%çš„pass@1æ–°çºªå½•ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é€šç”¨Transformerï¼ˆUTsï¼‰å·²å¹¿æ³›åº”ç”¨äºŽARC-AGIå’Œæ•°ç‹¬ç­‰å¤æ‚æŽ¨ç†ä»»åŠ¡ï¼Œä½†å…¶æ€§èƒ½æå‡çš„å…·ä½“æ¥æºå°šæœªå¾—åˆ°å……åˆ†æŽ¢ç´¢ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåˆ†æžäº†UTsçš„å˜ä½“ï¼Œå‘çŽ°ARC-AGIä¸Šçš„æ”¹è¿›ä¸»è¦æºäºŽTransformerçš„å¾ªçŽ¯å½’çº³åç½®å’Œå¼ºéžçº¿æ€§ç»„ä»¶ï¼Œè€Œéžå¤æ‚çš„æž¶æž„è®¾è®¡ã€‚åŸºäºŽè¿™ä¸€å‘çŽ°ï¼Œæˆ‘ä»¬æå‡ºäº†é€šç”¨æŽ¨ç†æ¨¡åž‹ï¼ˆURMï¼‰ï¼Œé€šè¿‡å¼•å…¥çŸ­å·ç§¯å’Œæˆªæ–­åå‘ä¼ æ’­æ¥å¢žå¼ºUTã€‚è¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æŽ¨ç†æ€§èƒ½ï¼Œåœ¨ARC-AGI 1ä¸Šè¾¾åˆ°äº†53.8%çš„pass@1ï¼Œåœ¨ARC-AGI 2ä¸Šè¾¾åˆ°äº†16.0%çš„pass@1ï¼Œå®žçŽ°äº†æœ€å…ˆè¿›æ°´å¹³ã€‚ä»£ç å·²å¼€æºï¼šhttps://github.com/zitian-gao/URMã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤æ‚æŽ¨ç†ä»»åŠ¡ï¼ˆå¦‚ARC-AGIå’Œæ•°ç‹¬ï¼‰ä¸­ï¼Œé€šç”¨Transformeræ€§èƒ½æå‡æ¥æºä¸æ˜Žç¡®çš„é—®é¢˜ï¼ŒçŽ°æœ‰æ–¹æ³•ä¾èµ–å¤æ‚æž¶æž„è®¾è®¡ï¼Œä½†å®žé™…æ•ˆæžœå¯èƒ½æºäºŽå…¶ä»–å› ç´ ï¼Œå¯¼è‡´ä¼˜åŒ–æ–¹å‘æ¨¡ç³Šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç³»ç»Ÿåˆ†æžå‘çŽ°æ€§èƒ½æå‡ä¸»è¦æ¥è‡ªTransformerçš„å¾ªçŽ¯å½’çº³åç½®å’Œå¼ºéžçº¿æ€§ç»„ä»¶ï¼Œè€Œéžå¤æ‚æž¶æž„ï¼Œå› æ­¤æå‡ºå¢žå¼ºé€šç”¨æŽ¨ç†æ¨¡åž‹ï¼Œèšç„¦äºŽè¿™äº›æ ¸å¿ƒè¦ç´ ï¼Œä»¥æ›´é«˜æ•ˆåœ°æå‡æŽ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æž¶æž„åŸºäºŽé€šç”¨Transformerï¼Œé€šè¿‡å¼•å…¥çŸ­å·ç§¯æ¨¡å—æ¥å¢žå¼ºå±€éƒ¨ç‰¹å¾æå–ï¼Œå¹¶ç»“åˆæˆªæ–­åå‘ä¼ æ’­æŠ€æœ¯ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œå½¢æˆURMæ¨¡åž‹ï¼Œåº”ç”¨äºŽåºåˆ—æŽ¨ç†ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯ç»“åˆçŸ­å·ç§¯å’Œæˆªæ–­åå‘ä¼ æ’­æ¥å¢žå¼ºé€šç”¨Transformerï¼ŒåŒºåˆ«äºŽçŽ°æœ‰æ–¹æ³•ä¾èµ–å¤æ‚æž¶æž„è®¾è®¡ï¼Œæœ¬è´¨åŒºåˆ«åœ¨äºŽæ›´ç›´æŽ¥åœ°åˆ©ç”¨Transformerçš„å†…åœ¨ä¼˜åŠ¿ï¼Œå®žçŽ°æ€§èƒ½çªç ´ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬çŸ­å·ç§¯å±‚çš„å‚æ•°è®¾ç½®ä»¥æ•æ‰å±€éƒ¨æ¨¡å¼ï¼Œæˆªæ–­åå‘ä¼ æ’­ç”¨äºŽå‡å°‘è®¡ç®—å¼€é”€å’Œæ¢¯åº¦é—®é¢˜ï¼Œç½‘ç»œç»“æž„ä¿æŒTransformeråŸºç¡€ï¼Œä½†é€šè¿‡å¢žå¼ºç»„ä»¶æå‡éžçº¿æ€§è¡¨è¾¾èƒ½åŠ›ï¼ŒæŸå¤±å‡½æ•°åŸºäºŽä»»åŠ¡ç›®æ ‡ï¼ˆå¦‚åˆ†ç±»æˆ–åºåˆ—é¢„æµ‹ï¼‰è¿›è¡Œä¼˜åŒ–ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

æœ€é‡è¦çš„å®žéªŒç»“æžœæ˜¯åœ¨ARC-AGIåŸºå‡†ä¸Šå®žçŽ°æ˜¾è‘—æ€§èƒ½æå‡ï¼šåœ¨ARC-AGI 1ä¸Šè¾¾åˆ°53.8% pass@1ï¼Œåœ¨ARC-AGI 2ä¸Šè¾¾åˆ°16.0% pass@1ï¼Œå¯¹æ¯”åŸºçº¿é€šç”¨Transformerå’Œå…¶ä»–å˜ä½“ï¼Œæå‡å¹…åº¦æ˜Žæ˜¾ï¼Œç¡®ç«‹äº†æœ€å…ˆè¿›æ°´å¹³ï¼ŒéªŒè¯äº†çŸ­å·ç§¯å’Œæˆªæ–­åå‘ä¼ æ’­çš„æœ‰æ•ˆæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨å¤æ‚æŽ¨ç†ä»»åŠ¡å¦‚ARC-AGIå’Œæ•°ç‹¬ä¸­å…·æœ‰ç›´æŽ¥åº”ç”¨ä»·å€¼ï¼Œå¯æŽ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨æŠ½è±¡æŽ¨ç†å’Œé—®é¢˜è§£å†³é¢†åŸŸçš„å‘å±•ã€‚æ½œåœ¨åº”ç”¨åŒ…æ‹¬æ™ºèƒ½æ•™è‚²ç³»ç»Ÿã€è‡ªåŠ¨åŒ–é€»è¾‘æµ‹è¯•å’Œé€šç”¨AIåŸºå‡†è¯„ä¼°ï¼Œæœªæ¥å¯èƒ½å½±å“æ›´å¹¿æ³›çš„è®¤çŸ¥è®¡ç®—å’Œæœºå™¨äººå†³ç­–ç³»ç»Ÿã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM.

