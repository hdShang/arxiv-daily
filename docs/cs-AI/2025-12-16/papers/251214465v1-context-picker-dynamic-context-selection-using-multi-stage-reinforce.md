---
layout: default
title: Context-Picker: Dynamic context selection using multi-stage reinforcement learning
---

# Context-Picker: Dynamic context selection using multi-stage reinforcement learning

**arXiv**: [2512.14465v1](https://arxiv.org/abs/2512.14465) | [PDF](https://arxiv.org/pdf/2512.14465.pdf)

**ä½œè€…**: Siyuan Zhu, Chengdong Xu, Kaiqiang Ke, Chao Yu

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºContext-Pickeræ¡†æž¶ï¼Œé€šè¿‡å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ è§£å†³é•¿ä¸Šä¸‹æ–‡é—®ç­”ä¸­çš„åŠ¨æ€ä¸Šä¸‹æ–‡é€‰æ‹©é—®é¢˜**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡é—®ç­”` `å¼ºåŒ–å­¦ä¹ ` `åŠ¨æ€ä¸Šä¸‹æ–‡é€‰æ‹©` `æœ€å°å……åˆ†å­é›†` `å¤šé˜¶æ®µä¼˜åŒ–` `è¯æ®è’¸é¦` `æ£€ç´¢å¢žå¼ºç”Ÿæˆ` `æŽ¨ç†æ„ŸçŸ¥æ¡†æž¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šé•¿ä¸Šä¸‹æ–‡é—®ç­”ä¸­ï¼Œä¼ ç»Ÿå›ºå®šTop-Kæ£€ç´¢å’Œå•é˜¶æ®µé‡æŽ’åºæ–¹æ³•éš¾ä»¥åŠ¨æ€ç¡®å®šæœ€ä¼˜ä¸Šä¸‹æ–‡é‡ï¼Œå¯¼è‡´ä¿¡æ¯é—æ¼æˆ–å™ªå£°å¼•å…¥ï¼Œå°¤å…¶å½±å“äº‹å®žæ€§é—®é¢˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºContext-Pickeræ¡†æž¶ï¼Œå°†ä¸Šä¸‹æ–‡é€‰æ‹©è§†ä¸ºå†³ç­–è¿‡ç¨‹ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ ï¼ˆå¬å›žå¯¼å‘å’Œç²¾åº¦å¯¼å‘ï¼‰ä¼˜åŒ–æœ€å°å……åˆ†å­é›†é€‰æ‹©ï¼Œå¹¶å¼•å…¥ç¦»çº¿è¯æ®è’¸é¦è§£å†³å¥–åŠ±ç¨€ç–æ€§ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒContext-Pickeræ˜¾è‘—ä¼˜äºŽRAGåŸºçº¿ï¼Œæå‡ç­”æ¡ˆå‡†ç¡®æ€§ï¼ŒåŒæ—¶å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œæ¶ˆèžç ”ç©¶éªŒè¯äº†å…³é”®ç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨é•¿ä¸Šä¸‹æ–‡é—®ç­”ï¼ˆLCQAï¼‰ä¸­ï¼Œç¡®å®šç»™å®šæŸ¥è¯¢çš„æœ€ä½³ä¸Šä¸‹æ–‡é‡æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚åŒ…å«å¤ªå°‘æ®µè½å¯èƒ½é—æ¼å…³é”®ä¿¡æ¯ï¼Œè€ŒåŒ…å«å¤ªå¤šæ®µè½å¯èƒ½å¼•å…¥å™ªå£°å¹¶é™ä½Žç­”æ¡ˆè´¨é‡ã€‚ä¼ ç»Ÿæ–¹æ³•ï¼Œå¦‚å›ºå®šçš„Top-Kæ£€ç´¢å’Œå•é˜¶æ®µé‡æŽ’åºï¼Œé¢ä¸´é€‰æ‹©æ­£ç¡®æ®µè½æ•°é‡çš„å›°å¢ƒã€‚è¿™ä¸ªé—®é¢˜å¯¹äºŽäº‹å®žæ€§é—®é¢˜å°¤å…¶çªå‡ºï¼Œè¿™äº›é—®é¢˜é€šå¸¸åªéœ€è¦å°‘é‡ç‰¹å®šè¯æ®ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Context-Pickerï¼Œè¿™æ˜¯ä¸€ä¸ªæŽ¨ç†æ„ŸçŸ¥æ¡†æž¶ï¼Œå°†èŒƒå¼ä»ŽåŸºäºŽç›¸ä¼¼æ€§çš„æŽ’åºè½¬å‘æœ€å°å……åˆ†å­é›†é€‰æ‹©ã€‚Context-Pickerå°†ä¸Šä¸‹æ–‡é€‰æ‹©è§†ä¸ºä¸€ä¸ªå†³ç­–è¿‡ç¨‹ï¼Œé€šè¿‡äººç±»å¯å‘çš„ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ è®¡åˆ’è¿›è¡Œä¼˜åŒ–ï¼šä¸€ä¸ªé¢å‘å¬å›žçš„é˜¶æ®µï¼Œä¼˜å…ˆè€ƒè™‘æŽ¨ç†é“¾çš„è¦†ç›–ï¼›éšåŽæ˜¯ä¸€ä¸ªé¢å‘ç²¾åº¦çš„é˜¶æ®µï¼Œç§¯æžå‰ªæžå†—ä½™ä»¥æç‚¼ç´§å‡‘çš„è¯æ®é›†ã€‚ä¸ºè§£å†³å¥–åŠ±ç¨€ç–æ€§é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç¦»çº¿è¯æ®è’¸é¦ç®¡é“ï¼Œé€šè¿‡ç•™ä¸€æ³•ï¼ˆLOOï¼‰ç¨‹åºæŒ–æŽ˜â€œæœ€å°å……åˆ†é›†â€ï¼Œæä¾›å¯†é›†ã€ä»»åŠ¡å¯¹é½çš„ç›‘ç£ã€‚åœ¨äº”ä¸ªé•¿ä¸Šä¸‹æ–‡å’Œå¤šè·³é—®ç­”åŸºå‡†ä¸Šçš„å®žéªŒè¡¨æ˜Žï¼ŒContext-Pickeræ˜¾è‘—ä¼˜äºŽå¼ºå¤§çš„RAGåŸºçº¿ï¼Œåœ¨å¯æ¯”æˆ–å‡å°‘çš„ä¸Šä¸‹æ–‡é•¿åº¦ä¸‹å®žçŽ°äº†æ›´ä¼˜çš„ç­”æ¡ˆå‡†ç¡®æ€§ã€‚æ¶ˆèžç ”ç©¶è¡¨æ˜Žï¼Œä»Žç²—åˆ°ç»†çš„ä¼˜åŒ–è®¡åˆ’ã€å†—ä½™æ„ŸçŸ¥çš„å¥–åŠ±å¡‘é€ å’ŒæŽ¨ç†å¼•å¯¼çš„æ ¼å¼éƒ½å¯¹è¿™ä¸€å¢žç›Šæœ‰å®žè´¨æ€§è´¡çŒ®ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è§£å†³é•¿ä¸Šä¸‹æ–‡é—®ç­”ï¼ˆLCQAï¼‰ä¸­çš„åŠ¨æ€ä¸Šä¸‹æ–‡é€‰æ‹©é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•å¦‚å›ºå®šTop-Kæ£€ç´¢å’Œå•é˜¶æ®µé‡æŽ’åºé¢ä¸´å›°å¢ƒï¼šé€‰æ‹©å¤ªå°‘æ®µè½å¯èƒ½é—æ¼å…³é”®ä¿¡æ¯ï¼Œé€‰æ‹©å¤ªå¤šåˆ™å¼•å…¥å™ªå£°ï¼Œé™ä½Žç­”æ¡ˆè´¨é‡ï¼Œå°¤å…¶å¯¹äºŽäº‹å®žæ€§é—®é¢˜ï¼Œè¿™éœ€è¦ç²¾ç¡®çš„æœ€å°è¯æ®é›†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ä¸Šä¸‹æ–‡é€‰æ‹©ä»ŽåŸºäºŽç›¸ä¼¼æ€§çš„æŽ’åºèŒƒå¼è½¬å‘æœ€å°å……åˆ†å­é›†é€‰æ‹©ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å†³ç­–è¿‡ç¨‹ã€‚è®¾è®¡çµæ„Ÿæ¥è‡ªäººç±»æŽ¨ç†ï¼šå…ˆå¹¿æ³›è¦†ç›–å¯èƒ½è¯æ®ï¼ˆå¬å›žï¼‰ï¼Œå†ç²¾ç»†å‰ªæžå†—ä½™ï¼ˆç²¾åº¦ï¼‰ï¼Œä»Žè€ŒåŠ¨æ€ç¡®å®šæœ€ä¼˜ä¸Šä¸‹æ–‡é‡ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æž¶æž„åŒ…æ‹¬ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ è®¡åˆ’ï¼šç¬¬ä¸€é˜¶æ®µä¸ºå¬å›žå¯¼å‘é˜¶æ®µï¼Œä¼˜å…ˆè¦†ç›–æŽ¨ç†é“¾ï¼Œç¡®ä¿ä¸é—æ¼å…³é”®ä¿¡æ¯ï¼›ç¬¬äºŒé˜¶æ®µä¸ºç²¾åº¦å¯¼å‘é˜¶æ®µï¼Œç§¯æžå‰ªæžå†—ä½™ï¼Œæç‚¼ç´§å‡‘è¯æ®é›†ã€‚æ­¤å¤–ï¼Œå¼•å…¥ç¦»çº¿è¯æ®è’¸é¦ç®¡é“ï¼Œé€šè¿‡ç•™ä¸€æ³•ï¼ˆLOOï¼‰æŒ–æŽ˜æœ€å°å……åˆ†é›†ï¼Œæä¾›å¯†é›†ç›‘ç£ä»¥è§£å†³å¥–åŠ±ç¨€ç–æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯æå‡ºæŽ¨ç†æ„ŸçŸ¥çš„å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ æ¡†æž¶ï¼Œå°†ä¸Šä¸‹æ–‡é€‰æ‹©è§†ä¸ºå†³ç­–ä¼˜åŒ–é—®é¢˜ï¼Œè€Œéžé™æ€æŽ’åºã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºŽï¼šä»Žå›ºå®šæ•°é‡é€‰æ‹©è½¬å‘åŠ¨æ€å­é›†é€‰æ‹©ï¼Œç»“åˆäººç±»å¯å‘å¼ä¼˜åŒ–ï¼Œå¹¶é€šè¿‡ç¦»çº¿è’¸é¦å¢žå¼ºè®­ç»ƒæ•ˆçŽ‡ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼šä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ è®¡åˆ’ï¼ˆå¬å›žå’Œç²¾åº¦é˜¶æ®µï¼‰ï¼Œä½¿ç”¨ç‰¹å®šå¥–åŠ±å‡½æ•°ï¼ˆå¦‚è¦†ç›–å¥–åŠ±å’Œå†—ä½™æƒ©ç½šï¼‰ï¼›ç¦»çº¿è¯æ®è’¸é¦ç®¡é“ï¼ŒåŸºäºŽLOOç¨‹åºç”Ÿæˆç›‘ç£ä¿¡å·ï¼›ç½‘ç»œç»“æž„å¯èƒ½æ¶‰åŠç­–ç•¥ç½‘ç»œå’ŒçŽ¯å¢ƒäº¤äº’ï¼Œä½†å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç»†èŠ‚åœ¨æ‘˜è¦ä¸­æœªæ˜Žç¡®ï¼Œéœ€å‚è€ƒè®ºæ–‡æ­£æ–‡ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨äº”ä¸ªé•¿ä¸Šä¸‹æ–‡å’Œå¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼ŒContext-Pickeræ˜¾è‘—ä¼˜äºŽå¼ºå¤§çš„RAGåŸºçº¿ï¼Œå…·ä½“æ€§èƒ½æ•°æ®æœªåœ¨æ‘˜è¦ä¸­æä¾›ï¼Œä½†è¡¨æ˜Žåœ¨å¯æ¯”æˆ–å‡å°‘çš„ä¸Šä¸‹æ–‡é•¿åº¦ä¸‹å®žçŽ°äº†æ›´ä¼˜çš„ç­”æ¡ˆå‡†ç¡®æ€§ã€‚æ¶ˆèžç ”ç©¶éªŒè¯äº†å…³é”®ç»„ä»¶ï¼ˆå¦‚ä¸¤é˜¶æ®µä¼˜åŒ–è®¡åˆ’ã€å†—ä½™æ„ŸçŸ¥å¥–åŠ±å¡‘é€ å’ŒæŽ¨ç†å¼•å¯¼æ ¼å¼ï¼‰çš„å®žè´¨æ€§è´¡çŒ®ï¼Œæå‡äº†æ•´ä½“æ•ˆæžœã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨é•¿ä¸Šä¸‹æ–‡é—®ç­”é¢†åŸŸå…·æœ‰å¹¿æ³›æ½œåœ¨åº”ç”¨ï¼Œå¦‚æ™ºèƒ½å®¢æœã€æ–‡æ¡£åˆ†æžã€æ•™è‚²è¾…åŠ©å’ŒçŸ¥è¯†åº“æ£€ç´¢ç³»ç»Ÿã€‚é€šè¿‡åŠ¨æ€é€‰æ‹©æœ€å°å……åˆ†ä¸Šä¸‹æ–‡ï¼Œèƒ½æå‡ç­”æ¡ˆå‡†ç¡®æ€§ã€å‡å°‘è®¡ç®—å¼€é”€ï¼Œå¹¶å¢žå¼ºç³»ç»Ÿåœ¨å¤æ‚æŸ¥è¯¢ï¼ˆå¦‚å¤šè·³æŽ¨ç†ï¼‰ä¸­çš„æ€§èƒ½ã€‚æœªæ¥å¯èƒ½å½±å“æ£€ç´¢å¢žå¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯çš„å‘å±•ï¼ŒæŽ¨åŠ¨æ›´é«˜æ•ˆã€ç²¾å‡†çš„ä¿¡æ¯æå–æ–¹æ³•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In long-context question answering (LCQA), determining the optimal amount of context for a given query is a significant challenge. Including too few passages may omit critical information, while including too many can introduce noise and reduce the quality of the answer. Traditional approaches, such as fixed Top-$K$ retrieval and single-stage reranking, face the dilemma of selecting the right number of passages. This problem is particularly pronounced for factoid questions, which often require only a few specific pieces of evidence. To address this issue, we introduce \emph{Context-Picker}, a reasoning-aware framework that shifts the paradigm from similarity-based ranking to minimal sufficient subset selection. Context-Picker treats context selection as a decision-making process optimized via a human-inspired, two-stage reinforcement learning schedule: a \emph{recall-oriented} stage that prioritizes the coverage of reasoning chains, followed by a \emph{precision-oriented} stage that aggressively prunes redundancy to distill a compact evidence set. To resolve reward sparsity, we propose an offline evidence distillation pipeline that mines "minimal sufficient sets" via a Leave-One-Out (LOO) procedure, providing dense, task-aligned supervision. Experiments on five long-context and multi-hop QA benchmarks demonstrate that Context-Picker significantly outperforms strong RAG baselines, achieving superior answer accuracy with comparable or reduced context lengths. Ablation studies indicate that the coarse-to-fine optimization schedule, the redundancy-aware reward shaping, and the rationale-guided format all contribute substantially to these gains.

