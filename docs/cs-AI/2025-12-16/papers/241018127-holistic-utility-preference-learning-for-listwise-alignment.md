---
layout: default
title: Holistic Utility Preference Learning for Listwise Alignment
---

# Holistic Utility Preference Learning for Listwise Alignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2410.18127" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2410.18127</a>
  <a href="https://arxiv.org/pdf/2410.18127.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2410.18127" onclick="toggleFavorite(this, '2410.18127', 'Holistic Utility Preference Learning for Listwise Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiacong Zhou, Xianyun Wang, Min Zhang, Jun Yu

**åˆ†ç±»**: cs.IR, cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDRPOä»¥è§£å†³äººç±»åå¥½å¯¹é½ä¸­çš„å…¨å±€æ’åé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äººç±»åå¥½å¯¹é½` `ç›´æ¥æ’ååå¥½ä¼˜åŒ–` `å­¦ä¹ æ’åº` `NDCG` `å“åº”ç”Ÿæˆ` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•ä¸»è¦ä¾èµ–æˆå¯¹æ¯”è¾ƒï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰å¤šä¸ªå“åº”ä¹‹é—´çš„æ•´ä½“æ’åå…³ç³»ï¼Œé™åˆ¶äº†åå¥½ä¿¡æ¯çš„åˆ©ç”¨ã€‚
2. æœ¬æ–‡æå‡ºçš„DRPOæ–¹æ³•å°†äººç±»åå¥½å¯¹é½è§†ä¸ºå­¦ä¹ æ’åºä»»åŠ¡ï¼Œé€šè¿‡è®¡ç®—æ•´ä½“æ•ˆç”¨åˆ†æ•°æ¥ä¼˜åŒ–æ•´ä¸ªå“åº”åˆ—è¡¨çš„åå¥½æ’åã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDRPOåœ¨ç”Ÿæˆå“åº”çš„è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å¯¹äºæé«˜äº¤äº’è´¨é‡å’Œå®‰å…¨æ€§è‡³å…³é‡è¦ã€‚ç°æœ‰çš„ç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æˆå¯¹æ¯”è¾ƒï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰å¤šä¸ªå“åº”ä¹‹é—´çš„æ•´ä½“æ’åå…³ç³»ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ç›´æ¥æ’ååå¥½ä¼˜åŒ–ï¼ˆDRPOï¼‰ï¼Œå°†äººç±»åå¥½å¯¹é½è§†ä¸ºå­¦ä¹ æ’åºä»»åŠ¡ï¼Œé€šè¿‡è®¡ç®—æ•´ä½“æ•ˆç”¨åˆ†æ•°æ¥ä¼˜åŒ–å“åº”åˆ—è¡¨çš„åå¥½æ’åã€‚ä¸ºå®ç°ä¸éå¯å¾®çš„NDCGçš„ç«¯åˆ°ç«¯ä¼˜åŒ–ï¼Œæå‡ºäº†diffNDCGæŸå¤±å‡½æ•°ï¼Œå¹¶å¼•å…¥äº†è‡ªé€‚åº”æ’åç­–ç•¥è¯„åˆ†ä»¥å¢å¼ºç”Ÿæˆå“åº”çš„åŒºåˆ†è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDRPOåœ¨ç”Ÿæˆå“åº”çš„è´¨é‡ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½ä¸­çš„æ•´ä½“æ’åé—®é¢˜ã€‚ç°æœ‰çš„æˆå¯¹æ¯”è¾ƒæ–¹æ³•æ— æ³•å……åˆ†åˆ©ç”¨å¤šä¸ªå“åº”ä¹‹é—´çš„åå¥½ä¿¡æ¯ï¼Œå¯¼è‡´å¯¹é½æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDRPOæ–¹æ³•é€šè¿‡å°†äººç±»åå¥½å¯¹é½è§†ä¸ºå­¦ä¹ æ’åºä»»åŠ¡ï¼Œä¼˜åŒ–æ•´ä¸ªå“åº”åˆ—è¡¨çš„åå¥½æ’åï¼Œè€Œä¸æ˜¯å•ç‹¬æ¯”è¾ƒæˆå¯¹å“åº”ã€‚è¿™æ ·çš„è®¾è®¡èƒ½å¤Ÿæ›´å…¨é¢åœ°æ•æ‰äººç±»çš„åå¥½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDRPOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€å“åº”ç”Ÿæˆã€åå¥½è¯„åˆ†å’Œä¼˜åŒ–å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆæ”¶é›†äººç±»åé¦ˆï¼Œç„¶åç”Ÿæˆå“åº”ï¼Œæ¥ç€è®¡ç®—æ•´ä½“æ•ˆç”¨åˆ†æ•°ï¼Œæœ€åè¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šDRPOçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†diffNDCGæŸå¤±å‡½æ•°ï¼Œä½œä¸ºNDCGçš„å¯å¾®è¿‘ä¼¼ï¼Œä»è€Œå®ç°äº†ä¸éå¯å¾®æŒ‡æ ‡çš„ç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„æˆå¯¹æ¯”è¾ƒæ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨æ•´ä½“åå¥½ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨DRPOä¸­ï¼Œé‡‡ç”¨äº†è‡ªé€‚åº”æ’åç­–ç•¥è¯„åˆ†ï¼Œä»¥å¢å¼ºç”Ÿæˆå“åº”çš„åŒºåˆ†è´¨é‡ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡è€ƒè™‘äº†æ•´ä½“æ•ˆç”¨åˆ†æ•°çš„è®¡ç®—ï¼Œç¡®ä¿äº†ä¼˜åŒ–è¿‡ç¨‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2410.18127/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2410.18127/x2.png" alt="fig_1" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDRPOåœ¨ç”Ÿæˆå“åº”çš„è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„ç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨NDCGæŒ‡æ ‡ä¸Šæå‡äº†çº¦15%ã€‚è¿™ä¸€ç»“æœéªŒè¯äº†DRPOåœ¨å¤„ç†äººç±»åå¥½å¯¹é½ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€æ¨èç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æ›´å¥½åœ°å¯¹é½äººç±»åå¥½ï¼ŒDRPOèƒ½å¤Ÿæå‡ç”Ÿæˆå†…å®¹çš„è´¨é‡å’Œå®‰å…¨æ€§ï¼Œè¿›è€Œæ”¹å–„ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨æ›´å¹¿æ³›çš„AIåº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨äººæœºåä½œçš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Aligning large language models with human preferences is essential for improving interaction quality and safety by ensuring outputs better reflect human values. A promising strategy involves Reinforcement Learning from Human Feedback (RLHF), starting with collecting and ranking responses generated by a supervised fine-tuning model to refine alignment. Existing methods such as Direct Preference Optimization (DPO) focus on pairwise comparisons, categorizing responses into preferred and less preferred pairs and optimizing pairwise margins. However, this pairwise approach cannot capture the holistic ranking relationships among multiple responses or effectively leverage the rich preference information available in list-wise comparisons. To address this challenge, this paper introduces \underline{D}irect \underline{R}anking \underline{P}reference \underline{O}ptimization (DRPO), a novel method that views human preference alignment as a Learning-to-Rank (LTR) task. Unlike pairwise methods, DRPO optimizes the preference ranking of entire response lists by computing holistic utility scores through NDCG, a standard LTR metric. To enable end-to-end optimization with the non-differentiable NDCG, we propose diffNDCG loss, a differentiable approximation facilitated by a sorting network. Furthermore, we introduce a novel margin-based Adaptive Rank Policy Score to enhance the discriminative quality of generated responses. Extensive experiments have shown that DRPO outperforms existing methods, enhancing the quality of the generated responses.

