---
layout: default
title: PentestEval: Benchmarking LLM-based Penetration Testing with Modular and Stage-Level Design
---

# PentestEval: Benchmarking LLM-based Penetration Testing with Modular and Stage-Level Design

**arXiv**: [2512.14233v1](https://arxiv.org/abs/2512.14233) | [PDF](https://arxiv.org/pdf/2512.14233.pdf)

**ä½œè€…**: Ruozhao Yang, Mingfei Cheng, Gelei Deng, Tianwei Zhang, Junjie Wang, Xiaofei Xie

**åˆ†ç±»**: cs.SE, cs.AI, cs.CR

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: 13 pages, 6 figures

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPentestEvalåŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡æ¨¡å—åŒ–é˜¶æ®µè®¾è®¡è¯„ä¼°LLMåœ¨æ¸—é€æµ‹è¯•ä¸­çš„æ€§èƒ½**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **è§†è§‰é‡Œç¨‹è®¡** **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `æ¸—é€æµ‹è¯•` `å¤§åž‹è¯­è¨€æ¨¡åž‹` `åŸºå‡†æµ‹è¯•` `æ¨¡å—åŒ–è®¾è®¡` `ç½‘ç»œå®‰å…¨` `è‡ªåŠ¨åŒ–è¯„ä¼°` `é˜¶æ®µçº§è¯„ä¼°` `ç»“æž„åŒ–æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰LLMåœ¨æ¸—é€æµ‹è¯•ä¸­ä¾èµ–ç®€å•æç¤ºï¼Œç¼ºä¹ä»»åŠ¡åˆ†è§£å’Œé¢†åŸŸé€‚åº”ï¼Œå¯¼è‡´æ€§èƒ½ä¸å¯é ä¸”éš¾ä»¥è¯„ä¼°å„é˜¶æ®µèƒ½åŠ›ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºPentestEvalåŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡æ¨¡å—åŒ–è®¾è®¡åˆ†è§£æ¸—é€æµ‹è¯•ä¸ºå…­ä¸ªé˜¶æ®µï¼Œé›†æˆä¸“å®¶æ ‡æ³¨æ•°æ®å’Œè‡ªåŠ¨åŒ–è¯„ä¼°ç®¡é“ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯„ä¼°9ä¸ªLLMsæ˜¾ç¤ºç«¯åˆ°ç«¯æˆåŠŸçŽ‡ä»…31%ï¼Œè‡ªä¸»ä»£ç†å‡ ä¹Žå®Œå…¨å¤±è´¥ï¼Œæ¨¡å—åŒ–èƒ½æ˜¾è‘—æå‡æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¸—é€æµ‹è¯•å¯¹äºŽè¯„ä¼°å’Œå¢žå¼ºç³»ç»Ÿå®‰å…¨è‡³å…³é‡è¦ï¼Œä½†ä¼ ç»Ÿå·¥ä½œæµç¨‹é«˜åº¦ä¾èµ–äººå·¥ã€ä¸“ä¸šçŸ¥è¯†å¯†é›†ä¸”éš¾ä»¥æ‰©å±•ã€‚å°½ç®¡å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰ä¸ºè‡ªåŠ¨åŒ–æä¾›äº†å‰æ™¯ï¼Œä½†çŽ°æœ‰åº”ç”¨ä¾èµ–ç®€å•çš„æç¤ºæ–¹æ³•ï¼Œç¼ºä¹ä»»åŠ¡åˆ†è§£æˆ–é¢†åŸŸé€‚åº”ï¼Œå¯¼è‡´ä¸å¯é çš„é»‘ç›’è¡Œä¸ºå’Œæœ‰é™çš„å¯¹æ¸—é€æµ‹è¯•å„é˜¶æ®µæ¨¡åž‹èƒ½åŠ›çš„æ´žå¯Ÿã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†PentestEvalï¼Œè¿™æ˜¯é¦–ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºŽè¯„ä¼°LLMsåœ¨å…­ä¸ªåˆ†è§£çš„æ¸—é€æµ‹è¯•é˜¶æ®µï¼šä¿¡æ¯æ”¶é›†ã€å¼±ç‚¹æ”¶é›†ä¸Žè¿‡æ»¤ã€æ”»å‡»å†³ç­–ã€æ¼æ´žåˆ©ç”¨ç”Ÿæˆä¸Žä¿®è®¢ã€‚PentestEvalé›†æˆäº†ä¸“å®¶æ ‡æ³¨çš„çœŸå®žæ•°æ®å’Œä¸€ä¸ªå®Œå…¨è‡ªåŠ¨åŒ–çš„è¯„ä¼°ç®¡é“ï¼Œè¦†ç›–12ä¸ªçŽ°å®žæ¼æ´žåœºæ™¯ä¸­çš„æ‰€æœ‰é˜¶æ®µï¼Œå…±346ä¸ªä»»åŠ¡ã€‚æˆ‘ä»¬å¯¹9ä¸ªå¹¿æ³›ä½¿ç”¨çš„LLMsè¿›è¡Œé˜¶æ®µçº§è¯„ä¼°ï¼Œæ­ç¤ºäº†åœ¨æ¸—é€æµ‹è¯•å·¥ä½œæµç¨‹å„é˜¶æ®µæ™®éè¾ƒå¼±çš„æ€§èƒ½å’Œæ˜Žæ˜¾çš„å±€é™æ€§ã€‚ç«¯åˆ°ç«¯ç®¡é“ä»…è¾¾åˆ°31%çš„æˆåŠŸçŽ‡ï¼ŒçŽ°æœ‰LLMé©±åŠ¨çš„ç³»ç»Ÿå¦‚PentestGPTã€PentestAgentå’ŒVulnBotè¡¨çŽ°å‡ºç±»ä¼¼çš„å±€é™æ€§ï¼Œè‡ªä¸»ä»£ç†å‡ ä¹Žå®Œå…¨å¤±è´¥ã€‚è¿™äº›å‘çŽ°å¼ºè°ƒè‡ªä¸»æ¸—é€æµ‹è¯•éœ€è¦æ›´å¼ºçš„ç»“æž„åŒ–æŽ¨ç†ï¼Œå…¶ä¸­æ¨¡å—åŒ–å¢žå¼ºæ¯ä¸ªå•ç‹¬é˜¶æ®µå¹¶æé«˜æ•´ä½“æ€§èƒ½ã€‚PentestEvalä¸ºæœªæ¥ç»†ç²’åº¦ã€é˜¶æ®µçº§è¯„ä¼°ç ”ç©¶æä¾›äº†åŸºç¡€åŸºå‡†ï¼Œä¸ºæ›´å¯é çš„åŸºäºŽLLMçš„è‡ªåŠ¨åŒ–é“ºå¹³é“è·¯ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³LLMåœ¨æ¸—é€æµ‹è¯•è‡ªåŠ¨åŒ–ä¸­æ€§èƒ½è¯„ä¼°ä¸è¶³çš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•ä¾èµ–ç®€å•æç¤ºï¼Œç¼ºä¹ä»»åŠ¡åˆ†è§£å’Œé¢†åŸŸé€‚åº”ï¼Œå¯¼è‡´æ¨¡åž‹è¡Œä¸ºä¸å¯é¢„æµ‹ã€éš¾ä»¥è¯„ä¼°å„é˜¶æ®µèƒ½åŠ›ï¼Œä¸”çŽ°æœ‰åŸºå‡†æµ‹è¯•ä¸å…¨é¢ï¼Œé™åˆ¶äº†LLMåœ¨å®‰å…¨é¢†åŸŸçš„å¯é åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ¨¡å—åŒ–è®¾è®¡å°†æ¸—é€æµ‹è¯•å·¥ä½œæµç¨‹åˆ†è§£ä¸ºå¤šä¸ªé˜¶æ®µï¼Œå¹¶æž„å»ºä¸€ä¸ªç»¼åˆåŸºå‡†æµ‹è¯•æ¥è¯„ä¼°LLMåœ¨æ¯ä¸ªé˜¶æ®µçš„è¡¨çŽ°ã€‚è¿™æ ·è®¾è®¡å¯ä»¥æ›´ç²¾ç»†åœ°åˆ†æžæ¨¡åž‹èƒ½åŠ›ï¼Œä¿ƒè¿›ç»“æž„åŒ–æŽ¨ç†ï¼Œä»Žè€Œæå‡æ•´ä½“è‡ªåŠ¨åŒ–æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æž¶æž„åŒ…æ‹¬å…­ä¸ªåˆ†è§£çš„æ¸—é€æµ‹è¯•é˜¶æ®µï¼šä¿¡æ¯æ”¶é›†ã€å¼±ç‚¹æ”¶é›†ä¸Žè¿‡æ»¤ã€æ”»å‡»å†³ç­–ã€æ¼æ´žåˆ©ç”¨ç”Ÿæˆä¸Žä¿®è®¢ã€‚PentestEvalé›†æˆäº†ä¸“å®¶æ ‡æ³¨çš„çœŸå®žæ•°æ®ï¼ˆground truthï¼‰å’Œä¸€ä¸ªå®Œå…¨è‡ªåŠ¨åŒ–çš„è¯„ä¼°ç®¡é“ï¼Œè¦†ç›–12ä¸ªçŽ°å®žæ¼æ´žåœºæ™¯ä¸­çš„æ‰€æœ‰é˜¶æ®µï¼Œå…±346ä¸ªä»»åŠ¡ã€‚è¯„ä¼°è¿‡ç¨‹ä»Žæ•°æ®å‡†å¤‡åˆ°ç»“æžœåˆ†æžï¼Œç¡®ä¿å®¢è§‚æ€§å’Œå¯é‡å¤æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯é¦–æ¬¡æå‡ºä¸€ä¸ªå…¨é¢çš„ã€é˜¶æ®µçº§çš„åŸºå‡†æµ‹è¯•PentestEvalï¼Œç”¨äºŽè¯„ä¼°LLMåœ¨æ¸—é€æµ‹è¯•ä¸­çš„æ€§èƒ½ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒé€šè¿‡æ¨¡å—åŒ–åˆ†è§£å’Œä¸“å®¶æ ‡æ³¨æ•°æ®ï¼Œæä¾›äº†æ›´ç»†ç²’åº¦çš„è¯„ä¼°èƒ½åŠ›ï¼Œæœ¬è´¨åŒºåˆ«åœ¨äºŽå¼ºè°ƒç»“æž„åŒ–æŽ¨ç†å’Œé¢†åŸŸé€‚åº”ï¼Œè€Œéžç®€å•çš„é»‘ç›’æµ‹è¯•ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä»»åŠ¡åˆ†è§£ä¸ºå…­ä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µå¯¹åº”ç‰¹å®šå­ä»»åŠ¡ï¼›2) ä½¿ç”¨ä¸“å®¶æ ‡æ³¨çš„çœŸå®žæ•°æ®ä½œä¸ºè¯„ä¼°æ ‡å‡†ï¼›3) è‡ªåŠ¨åŒ–è¯„ä¼°ç®¡é“è¦†ç›–æ‰€æœ‰ä»»åŠ¡ï¼Œç¡®ä¿ä¸€è‡´æ€§ï¼›4) åŸºäºŽ12ä¸ªçŽ°å®žæ¼æ´žåœºæ™¯æž„å»ºæ•°æ®é›†ï¼Œå¢žå¼ºå®žç”¨æ€§ï¼›5) è¯„ä¼°æŒ‡æ ‡å¯èƒ½åŒ…æ‹¬æˆåŠŸçŽ‡ã€å‡†ç¡®çŽ‡ç­‰ï¼Œå…·ä½“ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜Žï¼Œä½†å¼ºè°ƒå®¢è§‚é‡åŒ–ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

æœ€é‡è¦çš„å®žéªŒç»“æžœåŒ…æ‹¬ï¼š1) å¯¹9ä¸ªå¹¿æ³›ä½¿ç”¨çš„LLMsè¿›è¡Œé˜¶æ®µçº§è¯„ä¼°ï¼Œæ˜¾ç¤ºæ•´ä½“æ€§èƒ½è¾ƒå¼±ï¼›2) ç«¯åˆ°ç«¯æ¸—é€æµ‹è¯•ç®¡é“æˆåŠŸçŽ‡ä»…31%ï¼Œè¡¨æ˜ŽçŽ°æœ‰è‡ªåŠ¨åŒ–èƒ½åŠ›æœ‰é™ï¼›3) çŽ°æœ‰LLMé©±åŠ¨ç³»ç»Ÿå¦‚PentestGPTã€PentestAgentå’ŒVulnBotè¡¨çŽ°å‡ºç±»ä¼¼å±€é™æ€§ï¼›4) è‡ªä¸»ä»£ç†åœ¨æµ‹è¯•ä¸­å‡ ä¹Žå®Œå…¨å¤±è´¥ï¼Œçªæ˜¾ç»“æž„åŒ–æŽ¨ç†çš„ä¸è¶³ï¼›5) æ¨¡å—åŒ–è®¾è®¡èƒ½å¢žå¼ºå„é˜¶æ®µæ€§èƒ½ï¼Œæå‡æ•´ä½“æ•ˆæžœã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸»è¦åº”ç”¨äºŽç½‘ç»œå®‰å…¨é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯æ¸—é€æµ‹è¯•è‡ªåŠ¨åŒ–ã€‚æ½œåœ¨ä»·å€¼åŒ…æ‹¬ä¸ºå®‰å…¨ç ”ç©¶äººå‘˜å’Œä»Žä¸šè€…æä¾›æ ‡å‡†åŒ–çš„è¯„ä¼°å·¥å…·ï¼Œä¿ƒè¿›LLMåœ¨å®‰å…¨ä»»åŠ¡ä¸­çš„å¯é éƒ¨ç½²ã€‚æœªæ¥å½±å“å¯èƒ½æŽ¨åŠ¨æ›´æ™ºèƒ½çš„è‡ªä¸»å®‰å…¨ç³»ç»Ÿå¼€å‘ï¼Œæé«˜æ¼æ´žæ£€æµ‹å’Œä¿®å¤æ•ˆçŽ‡ï¼Œä½†éœ€è¿›ä¸€æ­¥ç ”ç©¶ä»¥è§£å†³å½“å‰å±€é™æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Penetration testing is essential for assessing and strengthening system security against real-world threats, yet traditional workflows remain highly manual, expertise-intensive, and difficult to scale. Although recent advances in Large Language Models (LLMs) offer promising opportunities for automation, existing applications rely on simplistic prompting without task decomposition or domain adaptation, resulting in unreliable black-box behavior and limited insight into model capabilities across penetration testing stages. To address this gap, we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision. PentestEval integrates expert-annotated ground truth with a fully automated evaluation pipeline across 346 tasks covering all stages in 12 realistic vulnerable scenarios. Our stage-level evaluation of 9 widely used LLMs reveals generally weak performance and distinct limitations across the stages of penetration-testing workflow. End-to-end pipelines reach only 31% success rate, and existing LLM-powered systems such as PentestGPT, PentestAgent, and VulnBot exhibit similar limitations, with autonomous agents failing almost entirely. These findings highlight that autonomous penetration testing demands stronger structured reasoning, where modularization enhances each individual stage and improves overall performance. PentestEval provides the foundational benchmark needed for future research on fine-grained, stage-level evaluation, paving the way toward more reliable LLM-based automation.

