---
layout: default
title: From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition
---

# From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition

**arXiv**: [2512.14244v1](https://arxiv.org/abs/2512.14244) | [PDF](https://arxiv.org/pdf/2512.14244.pdf)

**ä½œè€…**: Yiqing Zhou, Yu Lei, Shuzheng Si, Qingyan Sun, Wei Wang, Yifei Wu, Hao Wen, Gang Chen, Fanchao Qi, Maosong Sun

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽåŸºæœ¬è¯è¯­å•å…ƒçš„ä¸Šä¸‹æ–‡åŽ‹ç¼©æ¡†æž¶ï¼Œä»¥è§£å†³é•¿æ–‡æ¡£å¤„ç†ä¸­çš„è®¡ç®—æˆæœ¬é«˜å’Œå™ªå£°é—®é¢˜ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡åŽ‹ç¼©` `åŸºæœ¬è¯è¯­å•å…ƒ` `ç»“æž„å…³ç³»æ ‘` `é•¿æ–‡æ¡£å¤„ç†` `å¤§åž‹è¯­è¨€æ¨¡åž‹` `è®¡ç®—æ•ˆçŽ‡` `ä¸‹æ¸¸ä»»åŠ¡å¢žå¼º` `æ˜¾å¼åŽ‹ç¼©æ¡†æž¶`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŽ‹ç¼©æ–¹æ³•ç ´åå±€éƒ¨è¿žè´¯æ€§æˆ–å­˜åœ¨ä½ç½®åå·®ï¼Œå¯¼è‡´é•¿æ–‡æ¡£å¤„ç†æ•ˆçŽ‡ä½Žä¸‹ã€‚
2. æå‡ºåŸºäºŽåŸºæœ¬è¯è¯­å•å…ƒçš„ç»“æž„åŒ–åŽ‹ç¼©æ¡†æž¶ï¼Œé€šè¿‡åˆ†è§£å’Œé€‰æ‹©ä¿ç•™ä¸Šä¸‹æ–‡å®Œæ•´æ€§ã€‚
3. å®žéªŒæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨ç»“æž„é¢„æµ‹å’Œä¸‹æ¸¸ä»»åŠ¡ä¸­ä¼˜äºŽå‰æ²¿æ¨¡åž‹ï¼ŒåŒæ—¶é™ä½Žè®¡ç®—æˆæœ¬ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç®¡ç†å¤§é‡ä¸Šä¸‹æ–‡ä»ç„¶æ˜¯å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰çš„å…³é”®ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿æ–‡æ¡£é—®ç­”å’Œè‡ªä¸»ä»£ç†ç­‰åº”ç”¨ä¸­ï¼Œå†—é•¿çš„è¾“å…¥ä¼šå¯¼è‡´é«˜è®¡ç®—æˆæœ¬å¹¶å¼•å…¥å™ªå£°ã€‚çŽ°æœ‰çš„åŽ‹ç¼©æŠ€æœ¯é€šå¸¸é€šè¿‡ç¦»æ•£æ ‡è®°åˆ é™¤ç ´åå±€éƒ¨è¿žè´¯æ€§ï¼Œæˆ–ä¾èµ–å­˜åœ¨ä½ç½®åå·®ä¸”ä¸Žé—­æºAPIä¸å…¼å®¹çš„éšå¼æ½œåœ¨ç¼–ç ã€‚ä¸ºè§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºŽEDUçš„ä¸Šä¸‹æ–‡åŽ‹ç¼©å™¨ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ˜¾å¼åŽ‹ç¼©æ¡†æž¶ï¼Œæ—¨åœ¨ä¿ç•™å…¨å±€ç»“æž„å’Œç»†ç²’åº¦ç»†èŠ‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†ä¸Šä¸‹æ–‡åŽ‹ç¼©é‡æ–°è¡¨è¿°ä¸ºç»“æž„-ç„¶åŽ-é€‰æ‹©çš„è¿‡ç¨‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬çš„LingoEDUå°†çº¿æ€§æ–‡æœ¬è½¬æ¢ä¸ºåŸºæœ¬è¯è¯­å•å…ƒï¼ˆEDUsï¼‰çš„ç»“æž„å…³ç³»æ ‘ï¼Œè¿™äº›å•å…ƒä¸¥æ ¼é”šå®šåˆ°æºç´¢å¼•ä»¥æ¶ˆé™¤å¹»è§‰ã€‚å…¶æ¬¡ï¼Œä¸€ä¸ªè½»é‡çº§æŽ’åæ¨¡å—é€‰æ‹©ä¸ŽæŸ¥è¯¢ç›¸å…³çš„å­æ ‘è¿›è¡Œçº¿æ€§åŒ–ã€‚ä¸ºäº†ä¸¥æ ¼è¯„ä¼°ç»“æž„ç†è§£ï¼Œæˆ‘ä»¬å‘å¸ƒäº†StructBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«248ä¸ªå¤šæ ·åŒ–æ–‡æ¡£çš„æ‰‹åŠ¨æ ‡æ³¨æ•°æ®é›†ã€‚å®žè¯ç»“æžœè¡¨æ˜Žï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®žçŽ°äº†æœ€å…ˆè¿›çš„ç»“æž„é¢„æµ‹å‡†ç¡®æ€§ï¼Œå¹¶æ˜¾è‘—ä¼˜äºŽå‰æ²¿LLMsï¼ŒåŒæ—¶é™ä½Žäº†æˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç»“æž„æ„ŸçŸ¥åŽ‹ç¼©æ˜¾è‘—æé«˜äº†ä»Žé•¿ä¸Šä¸‹æ–‡ä»»åŠ¡åˆ°å¤æ‚æ·±åº¦æœç´¢åœºæ™¯çš„ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

è®ºæ–‡æå‡ºEDU-based Context Compressoræ¡†æž¶ï¼Œæ•´ä½“é‡‡ç”¨ç»“æž„-ç„¶åŽ-é€‰æ‹©çš„ä¸¤é˜¶æ®µæµç¨‹ã€‚é¦–å…ˆï¼ŒLingoEDUæ¨¡å—å°†çº¿æ€§æ–‡æœ¬åˆ†è§£ä¸ºåŸºæœ¬è¯è¯­å•å…ƒï¼ˆEDUsï¼‰ï¼Œæž„å»ºç»“æž„å…³ç³»æ ‘å¹¶ä¸¥æ ¼é”šå®šæºç´¢å¼•ä»¥é¿å…å¹»è§‰ã€‚å…¶æ¬¡ï¼Œè½»é‡çº§æŽ’åæ¨¡å—åŸºäºŽæŸ¥è¯¢ç›¸å…³æ€§é€‰æ‹©å­æ ‘è¿›è¡Œçº¿æ€§åŒ–è¾“å‡ºã€‚å…³é”®åˆ›æ–°åœ¨äºŽæ˜¾å¼ç»“æž„åŒ–åŽ‹ç¼©ï¼Œé€šè¿‡EDUåˆ†è§£ä¿ç•™å…¨å±€å’Œå±€éƒ¨ä¿¡æ¯ï¼Œä¸ŽçŽ°æœ‰åŸºäºŽç¦»æ•£åˆ é™¤æˆ–éšå¼ç¼–ç çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†ç»“æž„ä¿çœŸåº¦å’Œå…¼å®¹æ€§ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨StructBenchæ•°æ®é›†ä¸Šå®žçŽ°æœ€å…ˆè¿›çš„ç»“æž„é¢„æµ‹å‡†ç¡®æ€§ï¼Œæ˜¾è‘—ä¼˜äºŽå‰æ²¿LLMsï¼›ç»“æž„æ„ŸçŸ¥åŽ‹ç¼©ä½¿ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½å¤§å¹…æå‡ï¼ŒåŒæ—¶å‡å°‘è®¡ç®—æˆæœ¬ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶é€‚ç”¨äºŽé•¿æ–‡æ¡£é—®ç­”ã€è‡ªä¸»ä»£ç†ç³»ç»Ÿã€å¤æ‚æ·±åº¦æœç´¢ç­‰åœºæ™¯ï¼Œèƒ½æœ‰æ•ˆé™ä½ŽLLMsçš„è®¡ç®—å¼€é”€å’Œå™ªå£°å¹²æ‰°ï¼Œæå‡å¤„ç†æ•ˆçŽ‡å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰å®žé™…éƒ¨ç½²ä»·å€¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.

