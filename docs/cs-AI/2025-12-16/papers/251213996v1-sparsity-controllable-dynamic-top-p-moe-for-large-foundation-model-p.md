---
layout: default
title: Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training
---

# Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.13996" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.13996v1</a>
  <a href="https://arxiv.org/pdf/2512.13996.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.13996v1" onclick="toggleFavorite(this, '2512.13996v1', 'Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Can Jin, Hongwu Peng, Mingcan Xiang, Qixin Zhang, Xiangchi Yuan, Amit Hasan, Ohiremen Dibua, Yifan Gong, Yan Kang, Dimitris N. Metaxas

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDTop-p MoEï¼Œå®ç°ç¨€ç–åº¦å¯æ§çš„åŠ¨æ€Top-pè·¯ç”±ï¼Œæå‡å¤§æ¨¡å‹é¢„è®­ç»ƒæ•ˆæœã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ··åˆä¸“å®¶æ¨¡å‹` `MoE` `ç¨€ç–è·¯ç”±` `Top-pè·¯ç”±` `åŠ¨æ€ç¨€ç–åº¦` `PIæ§åˆ¶å™¨` `å¤§æ¨¡å‹é¢„è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰Top-k MoEè·¯ç”±ç­–ç•¥å¯¹æ‰€æœ‰tokené‡‡ç”¨ç»Ÿä¸€ç¨€ç–åº¦ï¼Œå¿½ç•¥äº†tokenéš¾åº¦çš„å·®å¼‚ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½ã€‚
2. DTop-p MoEåˆ©ç”¨PIæ§åˆ¶å™¨åŠ¨æ€è°ƒæ•´Top-pé˜ˆå€¼ï¼Œä½¿æ¿€æ´»ä¸“å®¶ç¨€ç–åº¦ä¸ç›®æ ‡å€¼å¯¹é½ï¼Œå®ç°ç¨€ç–åº¦å¯æ§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDTop-påœ¨LLMå’ŒDiffusion Transformerä¸Šå‡ä¼˜äºTop-kå’Œå›ºå®šé˜ˆå€¼Top-pï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„æ‰©å±•æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¨€ç–æ··åˆä¸“å®¶(MoE)æ¶æ„é€šè¿‡ä»…æ¿€æ´»æ¯ä¸ªè¾“å…¥tokençš„ä¸“å®¶å­é›†æ¥æœ‰æ•ˆåœ°æ‰©å±•æ¨¡å‹å®¹é‡ã€‚ç„¶è€Œï¼Œæ ‡å‡†çš„Top-kè·¯ç”±ç­–ç•¥æ–½åŠ äº†ä¸€ç§ç»Ÿä¸€çš„ç¨€ç–æ¨¡å¼ï¼Œå¿½ç•¥äº†tokençš„ä¸åŒéš¾åº¦ã€‚è™½ç„¶Top-pè·¯ç”±æä¾›äº†ä¸€ç§çµæ´»çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç°æœ‰çš„å®ç°é€šå¸¸ä¾èµ–äºå›ºå®šçš„å…¨å±€æ¦‚ç‡é˜ˆå€¼ï¼Œè¿™å¯¼è‡´äº†ä¸å¯æ§çš„è®¡ç®—æˆæœ¬å’Œå¯¹è¶…å‚æ•°é€‰æ‹©çš„æ•æ„Ÿæ€§ã€‚æœ¬æ–‡æå‡ºäº†DTop-p MoEï¼Œä¸€ç§ç¨€ç–åº¦å¯æ§çš„åŠ¨æ€Top-pè·¯ç”±æœºåˆ¶ã€‚ä¸ºäº†è§£å†³ä¼˜åŒ–ä¸å¯å¾®é˜ˆå€¼çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åˆ©ç”¨æ¯”ä¾‹-ç§¯åˆ†(PI)æ§åˆ¶å™¨åŠ¨æ€è°ƒæ•´æ¦‚ç‡é˜ˆå€¼ï¼Œä»¥ä½¿è¿è¡Œæ¿€æ´»çš„ä¸“å®¶ç¨€ç–åº¦ä¸æŒ‡å®šçš„targetå¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŠ¨æ€è·¯ç”±å½’ä¸€åŒ–æœºåˆ¶ï¼Œè¯¥æœºåˆ¶è‡ªé€‚åº”åœ°è°ƒæ•´å±‚çº§çš„è·¯ç”±logitsï¼Œå…è®¸ä¸åŒçš„å±‚å­¦ä¹ ä¸åŒçš„ä¸“å®¶é€‰æ‹©æ¨¡å¼ï¼ŒåŒæ—¶ä½¿ç”¨å…¨å±€æ¦‚ç‡é˜ˆå€¼ã€‚åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£Transformerä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDTop-på§‹ç»ˆä¼˜äºTop-kå’Œå›ºå®šé˜ˆå€¼Top-påŸºçº¿ã€‚æˆ‘ä»¬çš„åˆ†æè¯å®ï¼ŒDTop-pä¿æŒå¯¹æ¿€æ´»ä¸“å®¶æ•°é‡çš„ç²¾ç¡®æ§åˆ¶ï¼ŒåŒæ—¶è‡ªé€‚åº”åœ°åœ¨ä¸åŒçš„tokenå’Œå±‚ä¹‹é—´åˆ†é…èµ„æºã€‚æ­¤å¤–ï¼ŒDTop-påœ¨ä¸“å®¶ç²’åº¦ã€ä¸“å®¶å®¹é‡ã€æ¨¡å‹å¤§å°å’Œæ•°æ®é›†å¤§å°æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„ç¼©æ”¾ç‰¹æ€§ï¼Œä¸ºå¤§è§„æ¨¡MoEé¢„è®­ç»ƒæä¾›äº†ä¸€ä¸ªé²æ£’çš„æ¡†æ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰Top-k MoEè·¯ç”±ç­–ç•¥é‡‡ç”¨å›ºå®šçš„ç¨€ç–åº¦ï¼Œæ— æ³•æ ¹æ®tokençš„éš¾æ˜“ç¨‹åº¦è‡ªé€‚åº”åœ°åˆ†é…è®¡ç®—èµ„æºã€‚Top-pè·¯ç”±è™½ç„¶å¯ä»¥è‡ªé€‚åº”åœ°é€‰æ‹©ä¸“å®¶ï¼Œä½†ç°æœ‰æ–¹æ³•ä¾èµ–äºå›ºå®šçš„å…¨å±€æ¦‚ç‡é˜ˆå€¼ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬ä¸å¯æ§ï¼Œä¸”å¯¹è¶…å‚æ•°æ•æ„Ÿã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDTop-p MoEçš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥ä¸€ä¸ªåŠ¨æ€è°ƒæ•´çš„Top-pé˜ˆå€¼ï¼Œå¹¶ä½¿ç”¨æ¯”ä¾‹-ç§¯åˆ†(PI)æ§åˆ¶å™¨æ¥æ§åˆ¶è¿™ä¸ªé˜ˆå€¼ï¼Œä½¿å¾—å®é™…æ¿€æ´»çš„ä¸“å®¶æ•°é‡ä¸é¢„è®¾çš„ç›®æ ‡ç¨€ç–åº¦ç›¸åŒ¹é…ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®tokençš„éš¾åº¦è‡ªé€‚åº”åœ°é€‰æ‹©åˆé€‚çš„ä¸“å®¶æ•°é‡ï¼ŒåŒæ—¶ä¿è¯æ•´ä½“çš„è®¡ç®—æˆæœ¬å¯æ§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDTop-p MoEçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è·¯ç”±logitsç”Ÿæˆï¼šä¸ä¼ ç»ŸMoEç±»ä¼¼ï¼Œé¦–å…ˆç”Ÿæˆæ¯ä¸ªtokenåˆ°å„ä¸ªä¸“å®¶çš„è·¯ç”±logitsã€‚2) åŠ¨æ€Top-pé˜ˆå€¼è°ƒæ•´ï¼šä½¿ç”¨PIæ§åˆ¶å™¨æ ¹æ®å½“å‰æ¿€æ´»çš„ä¸“å®¶æ•°é‡ä¸ç›®æ ‡ç¨€ç–åº¦ä¹‹é—´çš„å·®å¼‚ï¼ŒåŠ¨æ€è°ƒæ•´Top-pé˜ˆå€¼ã€‚3) ä¸“å®¶é€‰æ‹©ï¼šæ ¹æ®åŠ¨æ€è°ƒæ•´åçš„Top-pé˜ˆå€¼ï¼Œé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ä¸“å®¶å­é›†ã€‚4) åŠ¨æ€è·¯ç”±å½’ä¸€åŒ–ï¼šè‡ªé€‚åº”åœ°è°ƒæ•´å±‚çº§çš„è·¯ç”±logitsï¼Œå…è®¸ä¸åŒçš„å±‚å­¦ä¹ ä¸åŒçš„ä¸“å®¶é€‰æ‹©æ¨¡å¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šDTop-p MoEçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) å¼•å…¥äº†PIæ§åˆ¶å™¨æ¥åŠ¨æ€è°ƒæ•´Top-pé˜ˆå€¼ï¼Œè§£å†³äº†ä¼˜åŒ–ä¸å¯å¾®é˜ˆå€¼çš„éš¾é¢˜ï¼Œå®ç°äº†ç¨€ç–åº¦å¯æ§ã€‚2) æå‡ºäº†åŠ¨æ€è·¯ç”±å½’ä¸€åŒ–æœºåˆ¶ï¼Œå…è®¸ä¸åŒå±‚å­¦ä¹ ä¸åŒçš„ä¸“å®¶é€‰æ‹©æ¨¡å¼ï¼Œæé«˜äº†æ¨¡å‹çš„çµæ´»æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šPIæ§åˆ¶å™¨çš„å‚æ•°ï¼ˆæ¯”ä¾‹å¢ç›Šå’Œç§¯åˆ†å¢ç›Šï¼‰éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚åŠ¨æ€è·¯ç”±å½’ä¸€åŒ–æœºåˆ¶é€šè¿‡å­¦ä¹ ä¸€ä¸ªç¼©æ”¾å› å­æ¥è°ƒæ•´æ¯ä¸€å±‚çš„è·¯ç”±logitsã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±ä»¥åŠä¸€ä¸ªè¾…åŠ©æŸå¤±ï¼Œç”¨äºé¼“åŠ±PIæ§åˆ¶å™¨ç¨³å®šåœ°æ§åˆ¶ç¨€ç–åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDTop-p MoEåœ¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£Transformerä¸Šå‡ä¼˜äºTop-kå’Œå›ºå®šé˜ˆå€¼Top-påŸºçº¿ã€‚DTop-pèƒ½å¤Ÿç²¾ç¡®æ§åˆ¶æ¿€æ´»çš„ä¸“å®¶æ•°é‡ï¼Œå¹¶åœ¨ä¸åŒçš„tokenå’Œå±‚ä¹‹é—´è‡ªé€‚åº”åœ°åˆ†é…èµ„æºã€‚æ­¤å¤–ï¼ŒDTop-påœ¨ä¸“å®¶ç²’åº¦ã€ä¸“å®¶å®¹é‡ã€æ¨¡å‹å¤§å°å’Œæ•°æ®é›†å¤§å°æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ‰©å±•æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DTop-p MoEé€‚ç”¨äºå¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ã€‚å®ƒå¯ä»¥åº”ç”¨äºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬åˆ†ç±»ç­‰ï¼Œä»¥åŠè®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œå¦‚å›¾åƒç”Ÿæˆã€å›¾åƒåˆ†ç±»ç­‰ã€‚é€šè¿‡è‡ªé€‚åº”åœ°åˆ†é…è®¡ç®—èµ„æºï¼ŒDTop-p MoEå¯ä»¥æé«˜æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sparse Mixture-of-Experts (MoE) architectures effectively scale model capacity by activating only a subset of experts for each input token. However, the standard Top-k routing strategy imposes a uniform sparsity pattern that ignores the varying difficulty of tokens. While Top-p routing offers a flexible alternative, existing implementations typically rely on a fixed global probability threshold, which results in uncontrolled computational costs and sensitivity to hyperparameter selection. In this paper, we propose DTop-p MoE, a sparsity-controllable dynamic Top-p routing mechanism. To resolve the challenge of optimizing a non-differentiable threshold, we utilize a Proportional-Integral (PI) Controller that dynamically adjusts the probability threshold to align the running activated-expert sparsity with a specified target. Furthermore, we introduce a dynamic routing normalization mechanism that adapts layer-wise routing logits, allowing different layers to learn distinct expert-selection patterns while utilizing a global probability threshold. Extensive experiments on Large Language Models and Diffusion Transformers demonstrate that DTop-p consistently outperforms both Top-k and fixed-threshold Top-p baselines. Our analysis confirms that DTop-p maintains precise control over the number of activated experts while adaptively allocating resources across different tokens and layers. Furthermore, DTop-p exhibits strong scaling properties with respect to expert granularity, expert capacity, model size, and dataset size, offering a robust framework for large-scale MoE pre-training.

