---
layout: default
title: TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs
---

# TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs

**arXiv**: [2512.14698v1](https://arxiv.org/abs/2512.14698) | [PDF](https://arxiv.org/pdf/2512.14698.pdf)

**ä½œè€…**: Jun Zhang, Teng Wang, Yuying Ge, Yixiao Ge, Xinhao Li, Ying Shan, Limin Wang

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Project Page: https://timelens-arc-lab.github.io/

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTimeLensåŸºå‡†ä¸Žæ¨¡åž‹ï¼Œé€šè¿‡é«˜è´¨é‡æ•°æ®å’Œç®—æ³•è®¾è®¡æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹çš„è§†é¢‘æ—¶åºå®šä½èƒ½åŠ›**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `è§†é¢‘æ—¶åºå®šä½` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `æ•°æ®è´¨é‡åŸºå‡†` `å¼ºåŒ–å­¦ä¹ è®­ç»ƒ` `æ—¶é—´è¡¨ç¤ºç¼–ç ` `è§†é¢‘ç†è§£` `å¼€æºæ¨¡åž‹` `ç®—æ³•è®¾è®¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VTGåŸºå‡†å­˜åœ¨ä¸¥é‡è´¨é‡é—®é¢˜ï¼Œå¯¼è‡´æ¨¡åž‹è¯„ä¼°ä¸å¯é ï¼Œä¸”è®­ç»ƒæ•°æ®å™ªå£°å¤§ï¼Œé™åˆ¶äº†MLLMsåœ¨è§†é¢‘æ—¶åºå®šä½ä¸­çš„æ€§èƒ½æå‡ã€‚
2. è®ºæ–‡ä»Žæ•°æ®è´¨é‡å’Œç®—æ³•è®¾è®¡åŒç»´åº¦å…¥æ‰‹ï¼Œæž„å»ºé«˜è´¨é‡åŸºå‡†TimeLens-Benchå’Œè®­ç»ƒé›†TimeLens-100Kï¼Œå¹¶è®¾è®¡äº¤æ›¿æ–‡æœ¬ç¼–ç å’ŒRLVRè®­ç»ƒèŒƒå¼ã€‚
3. TimeLensæ¨¡åž‹åœ¨å¼€æºæ¨¡åž‹ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ï¼Œè¶…è¶ŠGPT-5ç­‰ä¸“æœ‰æ¨¡åž‹ï¼Œæ˜¾è‘—æå‡äº†VTGä»»åŠ¡çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡å¹¶æœªæå‡ºå…¨æ–°æ–¹æ³•ï¼Œè€Œæ˜¯ä¸ºè§†é¢‘ç†è§£çš„æ ¸å¿ƒèƒ½åŠ›â€”â€”è§†é¢‘æ—¶åºå®šä½ï¼ˆVTGï¼‰å»ºç«‹äº†ä¸€ä¸ªç›´æŽ¥ã€æ¸è¿›ä½†è‡³å…³é‡è¦çš„åŸºçº¿ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ï¼ˆMLLMsï¼‰åœ¨å¤šç§è§†é¢‘ç†è§£ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œä½†ä¼˜åŒ–å…¶VTGèƒ½åŠ›çš„æ–¹æ¡ˆä»æœªè¢«å……åˆ†æŽ¢ç´¢ã€‚æœ¬æ–‡æå‡ºTimeLensï¼Œä»Žæ•°æ®è´¨é‡å’Œç®—æ³•è®¾è®¡ä¸¤ä¸ªä¸»è¦ç»´åº¦ï¼Œç³»ç»Ÿæ€§åœ°ç ”ç©¶å¦‚ä½•æž„å»ºå…·æœ‰å¼ºå¤§VTGèƒ½åŠ›çš„MLLMsã€‚æˆ‘ä»¬é¦–å…ˆæ­ç¤ºäº†çŽ°æœ‰VTGåŸºå‡†ä¸­çš„å…³é”®è´¨é‡é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†TimeLens-Benchï¼Œå®ƒåŒ…å«ä¸‰ä¸ªæµè¡ŒåŸºå‡†çš„ç²¾å¿ƒé‡æ–°æ ‡æ³¨ç‰ˆæœ¬ï¼Œéµå¾ªä¸¥æ ¼çš„è´¨é‡æ ‡å‡†ã€‚æˆ‘ä»¬çš„åˆ†æžæ˜¾ç¤ºï¼Œä¸Žæ—§åŸºå‡†ç›¸æ¯”ï¼Œæ¨¡åž‹æŽ’åå‘ç”Ÿäº†æ˜¾è‘—å˜åŒ–ï¼Œè¯å®žäº†å…ˆå‰è¯„ä¼°æ ‡å‡†çš„ä¸å¯é æ€§ã€‚æˆ‘ä»¬è¿˜é€šè¿‡è‡ªåŠ¨é‡æ–°æ ‡æ³¨æµç¨‹è§£å†³äº†è®­ç»ƒæ•°æ®ä¸­çš„å™ªå£°é—®é¢˜ï¼Œç”Ÿæˆäº†TimeLens-100Kï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®é›†ã€‚åŸºäºŽæˆ‘ä»¬çš„æ•°æ®åŸºç¡€ï¼Œæˆ‘ä»¬æ·±å…¥æŽ¢ç´¢äº†ç®—æ³•è®¾è®¡åŽŸåˆ™ï¼Œå¾—å‡ºä¸€ç³»åˆ—æœ‰æ„ä¹‰çš„è§è§£å’Œæœ‰æ•ˆä¸”é«˜æ•ˆçš„å®žè·µã€‚è¿™äº›åŒ…æ‹¬ç”¨äºŽæ—¶é—´è¡¨ç¤ºçš„äº¤æ›¿æ–‡æœ¬ç¼–ç ã€ä½œä¸ºè®­ç»ƒèŒƒå¼çš„å…æ€è€ƒå¼ºåŒ–å­¦ä¹ ä¸Žå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ–¹æ³•ï¼Œä»¥åŠç²¾å¿ƒè®¾è®¡çš„RLVRè®­ç»ƒæ–¹æ¡ˆã€‚è¿™äº›åŠªåŠ›æœ€ç»ˆå½¢æˆäº†TimeLensæ¨¡åž‹ç³»åˆ—ï¼Œè¿™æ˜¯ä¸€ç»„åœ¨å¼€æºæ¨¡åž‹ä¸­å…·æœ‰æœ€å…ˆè¿›VTGæ€§èƒ½çš„MLLMsï¼Œç”šè‡³è¶…è¶Šäº†GPT-5å’ŒGemini-2.5-Flashç­‰ä¸“æœ‰æ¨¡åž‹ã€‚æ‰€æœ‰ä»£ç ã€æ•°æ®å’Œæ¨¡åž‹éƒ½å°†å‘å¸ƒä»¥ä¿ƒè¿›æœªæ¥ç ”ç©¶ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†é¢‘æ—¶åºå®šä½ï¼ˆVTGï¼‰ä»»åŠ¡ä¸­ï¼Œç”±äºŽçŽ°æœ‰åŸºå‡†æ•°æ®è´¨é‡ä½Žå’Œè®­ç»ƒæ•°æ®å™ªå£°å¤§ï¼Œå¯¼è‡´å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ï¼ˆMLLMsï¼‰æ€§èƒ½è¯„ä¼°ä¸å¯é ä¸”ä¼˜åŒ–å›°éš¾çš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•çš„ç—›ç‚¹åŒ…æ‹¬æ ‡æ³¨é”™è¯¯ã€è¯„ä¼°æ ‡å‡†ä¸ä¸€è‡´ï¼Œä»¥åŠç¼ºä¹ç³»ç»Ÿæ€§çš„ç®—æ³•è®¾è®¡æŒ‡å¯¼ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æå‡æ•°æ®è´¨é‡å’Œä¼˜åŒ–ç®—æ³•è®¾è®¡ï¼Œç³»ç»Ÿæ€§åœ°æž„å»ºå…·æœ‰å¼ºå¤§VTGèƒ½åŠ›çš„MLLMsã€‚è¿™åŒ…æ‹¬é‡æ–°æ ‡æ³¨åŸºå‡†ä»¥æ¶ˆé™¤å™ªå£°ã€åˆ›å»ºé«˜è´¨é‡è®­ç»ƒé›†ï¼Œå¹¶æŽ¢ç´¢æœ‰æ•ˆçš„ç¼–ç å’Œè®­ç»ƒç­–ç•¥ï¼Œä»¥ç¡®ä¿æ¨¡åž‹èƒ½å‡†ç¡®ç†è§£è§†é¢‘ä¸­çš„æ—¶é—´ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æ¡†æž¶åˆ†ä¸ºæ•°æ®æž„å»ºå’Œç®—æ³•è®¾è®¡ä¸¤é˜¶æ®µã€‚é¦–å…ˆï¼Œé€šè¿‡äººå·¥å®¡æ ¸å’Œè‡ªåŠ¨æµç¨‹ï¼Œç”ŸæˆTimeLens-Benchï¼ˆé«˜è´¨é‡è¯„ä¼°åŸºå‡†ï¼‰å’ŒTimeLens-100Kï¼ˆé«˜è´¨é‡è®­ç»ƒæ•°æ®é›†ï¼‰ã€‚ç„¶åŽï¼ŒåŸºäºŽè¿™äº›æ•°æ®ï¼Œè®¾è®¡MLLMæ¨¡åž‹ï¼Œé‡‡ç”¨äº¤æ›¿æ–‡æœ¬ç¼–ç å¤„ç†æ—¶é—´è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨RLVRä½œä¸ºè®­ç»ƒèŒƒå¼ï¼Œç»“åˆå¯éªŒè¯å¥–åŠ±è¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯æå‡ºäº†TimeLens-Benchå’ŒTimeLens-100Kï¼Œè§£å†³äº†æ•°æ®è´¨é‡é—®é¢˜ï¼›åŒæ—¶ï¼Œå¼•å…¥äº†äº¤æ›¿æ–‡æœ¬ç¼–ç å’ŒRLVRè®­ç»ƒæ–¹æ³•ï¼Œè¿™äº›è®¾è®¡æ˜¾è‘—æå‡äº†VTGæ€§èƒ½ï¼Œä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ›´æ³¨é‡æ•°æ®å¯é æ€§å’Œç®—æ³•æ•ˆçŽ‡ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼šäº¤æ›¿æ–‡æœ¬ç¼–ç å°†æ—¶é—´ä¿¡æ¯åµŒå…¥æ–‡æœ¬åºåˆ—ï¼Œå¢žå¼ºæ—¶é—´æ„ŸçŸ¥ï¼›RLVRè®­ç»ƒèŒƒå¼å…é™¤äº†å¤æ‚æ€è€ƒæ­¥éª¤ï¼Œç›´æŽ¥åŸºäºŽå¯éªŒè¯å¥–åŠ±ï¼ˆå¦‚å®šä½å‡†ç¡®æ€§ï¼‰è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼›è®­ç»ƒæ–¹æ¡ˆä¸­å¯èƒ½æ¶‰åŠå¥–åŠ±å‡½æ•°è®¾è®¡ã€å­¦ä¹ çŽ‡è°ƒåº¦ç­‰è¶…å‚æ•°ä¼˜åŒ–ï¼Œå…·ä½“ç»†èŠ‚éœ€å‚è€ƒè®ºæ–‡ä»£ç ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

TimeLensæ¨¡åž‹åœ¨TimeLens-Benchä¸Šè¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºä¸Žæ—§åŸºå‡†ç›¸æ¯”çš„æ¨¡åž‹æŽ’åå·¨å˜ï¼Œè¯å®žäº†å…ˆå‰æ ‡å‡†çš„ä¸å¯é æ€§ã€‚å…·ä½“æ€§èƒ½ä¸Šï¼ŒTimeLensåœ¨å¼€æºæ¨¡åž‹ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ï¼Œç”šè‡³è¶…è¶Šäº†GPT-5å’ŒGemini-2.5-Flashç­‰ä¸“æœ‰æ¨¡åž‹ï¼Œæå‡äº†VTGä»»åŠ¡çš„å‡†ç¡®çŽ‡ï¼Œå…·ä½“æ•°æ®éœ€å‚è€ƒè®ºæ–‡å®žéªŒéƒ¨åˆ†ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯åº”ç”¨äºŽè§†é¢‘å†…å®¹åˆ†æžã€æ™ºèƒ½ç›‘æŽ§ã€è§†é¢‘æ£€ç´¢å’Œç¼–è¾‘ç­‰é¢†åŸŸï¼Œé€šè¿‡æå‡è§†é¢‘æ—¶åºå®šä½çš„å‡†ç¡®æ€§ï¼ŒåŠ©åŠ›è‡ªåŠ¨åŒ–è§†é¢‘ç†è§£ç³»ç»Ÿçš„å‘å±•ã€‚å…¶é«˜è´¨é‡æ•°æ®å’Œç®—æ³•è®¾è®¡ä¸ºæœªæ¥VTGç ”ç©¶æä¾›äº†å¯é åŸºçº¿ï¼ŒæŽ¨åŠ¨å¤šæ¨¡æ€AIåœ¨çœŸå®žåœºæ™¯ä¸­çš„è½åœ°ï¼Œå¦‚æ•™è‚²ã€å¨±ä¹å’Œå®‰é˜²ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.

