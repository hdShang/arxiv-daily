---
layout: default
title: TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs
---

# TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs

**arXiv**: [2512.14698v1](https://arxiv.org/abs/2512.14698) | [PDF](https://arxiv.org/pdf/2512.14698.pdf)

**ä½œè€…**: Jun Zhang, Teng Wang, Yuying Ge, Yixiao Ge, Xinhao Li, Ying Shan, Limin Wang

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Project Page: https://timelens-arc-lab.github.io/

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTimeLensåŸºå‡†ä¸Žæ¨¡åž‹ï¼Œé€šè¿‡é«˜è´¨é‡æ•°æ®ä¸Žç®—æ³•è®¾è®¡æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹çš„è§†é¢‘æ—¶åºå®šä½èƒ½åŠ›**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `è§†é¢‘æ—¶åºå®šä½` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `æ•°æ®è´¨é‡åŸºå‡†` `å¼ºåŒ–å­¦ä¹ è®­ç»ƒ` `è§†é¢‘ç†è§£` `æ—¶é—´è¡¨ç¤ºç¼–ç ` `å¼€æºæ¨¡åž‹ä¼˜åŒ–` `è‡ªåŠ¨åŒ–æ ‡æ³¨`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†é¢‘æ—¶åºå®šä½åŸºå‡†å­˜åœ¨æ•°æ®è´¨é‡é—®é¢˜ï¼Œå¯¼è‡´æ¨¡åž‹è¯„ä¼°ä¸å¯é ï¼Œä¸”å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨è¯¥ä»»åŠ¡ä¸Šçš„ä¼˜åŒ–æ–¹æ¡ˆç¼ºä¹ç³»ç»ŸæŽ¢ç´¢ã€‚
2. è®ºæ–‡ä»Žæ•°æ®è´¨é‡å’Œç®—æ³•è®¾è®¡ä¸¤ä¸ªç»´åº¦å…¥æ‰‹ï¼Œæž„å»ºé«˜è´¨é‡åŸºå‡†å’Œè®­ç»ƒæ•°æ®é›†ï¼Œå¹¶æŽ¢ç´¢æœ‰æ•ˆçš„è®­ç»ƒèŒƒå¼å¦‚äº¤æ›¿æ–‡æœ¬ç¼–ç å’ŒRLVRæ–¹æ³•ã€‚
3. TimeLensæ¨¡åž‹åœ¨å¼€æºæ¨¡åž‹ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œè¶…è¶ŠGPT-5ç­‰ä¸“æœ‰æ¨¡åž‹ï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘æ—¶åºå®šä½çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡å¹¶æœªæå‡ºå…¨æ–°æ–¹æ³•ï¼Œè€Œæ˜¯ä¸ºè§†é¢‘ç†è§£çš„æ ¸å¿ƒèƒ½åŠ›â€”â€”è§†é¢‘æ—¶åºå®šä½ï¼ˆVTGï¼‰å»ºç«‹äº†ä¸€ä¸ªç›´æŽ¥ã€æ¸è¿›ä½†è‡³å…³é‡è¦çš„åŸºçº¿ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ï¼ˆMLLMsï¼‰åœ¨å¤šç§è§†é¢‘ç†è§£ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œä½†ä¼˜åŒ–å…¶VTGèƒ½åŠ›çš„æ–¹æ¡ˆä»å¾…æŽ¢ç´¢ã€‚æœ¬æ–‡æå‡ºTimeLensï¼Œä»Žæ•°æ®è´¨é‡å’Œç®—æ³•è®¾è®¡ä¸¤ä¸ªä¸»è¦ç»´åº¦ï¼Œç³»ç»Ÿæ€§åœ°ç ”ç©¶å¦‚ä½•æž„å»ºå…·æœ‰å¼ºå¤§VTGèƒ½åŠ›çš„MLLMsã€‚æˆ‘ä»¬é¦–å…ˆæ­ç¤ºäº†çŽ°æœ‰VTGåŸºå‡†ä¸­çš„å…³é”®è´¨é‡é—®é¢˜ï¼Œå¹¶å¼•å…¥TimeLens-Benchï¼Œå®ƒåŒ…å«ä¸‰ä¸ªæµè¡ŒåŸºå‡†çš„ç²¾å¿ƒé‡æ–°æ ‡æ³¨ç‰ˆæœ¬ï¼Œéµå¾ªä¸¥æ ¼çš„è´¨é‡æ ‡å‡†ã€‚æˆ‘ä»¬çš„åˆ†æžæ˜¾ç¤ºï¼Œä¸Žæ—§åŸºå‡†ç›¸æ¯”ï¼Œæ¨¡åž‹æŽ’åå‘ç”Ÿäº†æ˜¾è‘—å˜åŒ–ï¼Œè¯å®žäº†å…ˆå‰è¯„ä¼°æ ‡å‡†çš„ä¸å¯é æ€§ã€‚æˆ‘ä»¬è¿˜é€šè¿‡è‡ªåŠ¨é‡æ–°æ ‡æ³¨æµç¨‹è§£å†³äº†è®­ç»ƒæ•°æ®ä¸­çš„å™ªå£°é—®é¢˜ï¼Œç”Ÿæˆäº†TimeLens-100Kï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®é›†ã€‚åŸºäºŽæˆ‘ä»¬çš„æ•°æ®åŸºç¡€ï¼Œæˆ‘ä»¬æ·±å…¥æŽ¢ç´¢äº†ç®—æ³•è®¾è®¡åŽŸåˆ™ï¼Œå¾—å‡ºä¸€ç³»åˆ—æœ‰æ„ä¹‰çš„è§è§£å’Œé«˜æ•ˆå®žç”¨çš„åšæ³•ã€‚è¿™äº›åŒ…æ‹¬ç”¨äºŽæ—¶é—´è¡¨ç¤ºçš„äº¤æ›¿æ–‡æœ¬ç¼–ç ã€ä½œä¸ºè®­ç»ƒèŒƒå¼çš„å…æ€è€ƒå¼ºåŒ–å­¦ä¹ ä¸Žå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ–¹æ³•ï¼Œä»¥åŠç²¾å¿ƒè®¾è®¡çš„RLVRè®­ç»ƒæ–¹æ¡ˆã€‚è¿™äº›åŠªåŠ›æœ€ç»ˆå½¢æˆäº†TimeLensæ¨¡åž‹ç³»åˆ—ï¼Œè¿™æ˜¯ä¸€ç»„åœ¨å¼€æºæ¨¡åž‹ä¸­å…·æœ‰æœ€å…ˆè¿›VTGæ€§èƒ½çš„MLLMsï¼Œç”šè‡³è¶…è¶Šäº†GPT-5å’ŒGemini-2.5-Flashç­‰ä¸“æœ‰æ¨¡åž‹ã€‚æ‰€æœ‰ä»£ç ã€æ•°æ®å’Œæ¨¡åž‹éƒ½å°†å‘å¸ƒï¼Œä»¥ä¿ƒè¿›æœªæ¥ç ”ç©¶ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†é¢‘æ—¶åºå®šä½ï¼ˆVTGï¼‰ä»»åŠ¡ä¸­ï¼ŒçŽ°æœ‰åŸºå‡†æ•°æ®è´¨é‡å·®å¯¼è‡´æ¨¡åž‹è¯„ä¼°ä¸å¯é ï¼Œä»¥åŠå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ï¼ˆMLLMsï¼‰åœ¨è¯¥ä»»åŠ¡ä¸Šç¼ºä¹ç³»ç»Ÿä¼˜åŒ–æ–¹æ¡ˆçš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•çš„ç—›ç‚¹åŒ…æ‹¬æ ‡æ³¨å™ªå£°ã€è¯„ä¼°æ ‡å‡†ä¸ä¸€è‡´å’Œè®­ç»ƒæ•ˆçŽ‡ä½Žã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æå‡æ•°æ®è´¨é‡å’Œç³»ç»ŸåŒ–ç®—æ³•è®¾è®¡æ¥å¢žå¼ºMLLMsçš„VTGèƒ½åŠ›ã€‚è®¾è®¡åŸºäºŽä¸¤ä¸ªç»´åº¦ï¼šé¦–å…ˆï¼Œé‡æ–°æ ‡æ³¨åŸºå‡†å’Œè®­ç»ƒæ•°æ®ä»¥å‡å°‘å™ªå£°ï¼›å…¶æ¬¡ï¼ŒæŽ¢ç´¢é«˜æ•ˆçš„è®­ç»ƒèŒƒå¼ï¼Œå¦‚äº¤æ›¿æ–‡æœ¬ç¼–ç å’Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥ä¼˜åŒ–æ¨¡åž‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æž¶æž„åŒ…æ‹¬æ•°æ®å‡†å¤‡å’Œæ¨¡åž‹è®­ç»ƒä¸¤ä¸ªé˜¶æ®µã€‚æ•°æ®å‡†å¤‡é˜¶æ®µæ¶‰åŠæž„å»ºTimeLens-Benchï¼ˆé‡æ–°æ ‡æ³¨çš„åŸºå‡†ï¼‰å’ŒTimeLens-100Kï¼ˆé«˜è´¨é‡è®­ç»ƒæ•°æ®é›†ï¼‰ï¼›æ¨¡åž‹è®­ç»ƒé˜¶æ®µé‡‡ç”¨MLLMsåŸºç¡€æž¶æž„ï¼Œé›†æˆäº¤æ›¿æ–‡æœ¬ç¼–ç æ¨¡å—å’ŒRLVRè®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡å¯éªŒè¯å¥–åŠ±æœºåˆ¶ä¼˜åŒ–æ—¶é—´å®šä½ç²¾åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯å¼•å…¥TimeLens-Benchå’ŒTimeLens-100Kæ•°æ®é›†ï¼Œè§£å†³äº†æ•°æ®è´¨é‡é—®é¢˜ï¼›åŒæ—¶ï¼Œæå‡ºRLVRè®­ç»ƒèŒƒå¼ï¼Œç»“åˆå…æ€è€ƒå¼ºåŒ–å­¦ä¹ å’Œå¯éªŒè¯å¥–åŠ±ï¼Œæé«˜äº†è®­ç»ƒæ•ˆçŽ‡å’Œæ¨¡åž‹æ€§èƒ½ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºŽç³»ç»Ÿæ€§åœ°ä»Žæ•°æ®æºå¤´å’Œç®—æ³•è®¾è®¡ä¸¤æ–¹é¢å…¥æ‰‹ï¼Œè€Œéžä»…ä¾èµ–æ¨¡åž‹æž¶æž„æ”¹è¿›ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬äº¤æ›¿æ–‡æœ¬ç¼–ç ç”¨äºŽæ—¶é—´è¡¨ç¤ºï¼Œå°†æ—¶é—´ä¿¡æ¯åµŒå…¥æ–‡æœ¬åºåˆ—ï¼›RLVRæ–¹æ³•ä½¿ç”¨å¯éªŒè¯å¥–åŠ±å‡½æ•°ï¼ˆå¦‚åŸºäºŽæ ‡æ³¨ä¸€è‡´æ€§çš„å¥–åŠ±ï¼‰æ¥æŒ‡å¯¼å¼ºåŒ–å­¦ä¹ ï¼Œé¿å…å¤æ‚æ€è€ƒè¿‡ç¨‹ï¼›è®­ç»ƒå‚æ•°è®¾ç½®å¯èƒ½æ¶‰åŠå¤šé˜¶æ®µä¼˜åŒ–ï¼Œå…·ä½“ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜Žï¼Œä½†å¼ºè°ƒé«˜æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

TimeLensæ¨¡åž‹åœ¨é‡æ–°æ ‡æ³¨çš„TimeLens-Benchä¸Šè¡¨çŽ°å‡ºè‰²ï¼Œåœ¨å¼€æºæ¨¡åž‹ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œå…·ä½“æ•°æ®æœªåœ¨æ‘˜è¦ä¸­æä¾›ï¼Œä½†æåˆ°è¶…è¶Šäº†ä¸“æœ‰æ¨¡åž‹å¦‚GPT-5å’ŒGemini-2.5-Flashã€‚å®žéªŒæ˜¾ç¤ºï¼Œä¸Žæ—§åŸºå‡†ç›¸æ¯”ï¼Œæ¨¡åž‹æŽ’åå‘ç”Ÿæ˜¾è‘—å˜åŒ–ï¼Œè¯å®žäº†é«˜è´¨é‡æ•°æ®å¯¹è¯„ä¼°çš„é‡è¦æ€§ï¼Œæå‡äº†VTGä»»åŠ¡çš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è§†é¢‘ç†è§£é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå¦‚æ™ºèƒ½è§†é¢‘æ£€ç´¢ã€å†…å®¹æ‘˜è¦ç”Ÿæˆã€è‡ªåŠ¨é©¾é©¶åœºæ™¯åˆ†æžå’Œæ•™è‚²è§†é¢‘æ ‡æ³¨ç­‰ã€‚é€šè¿‡æå‡è§†é¢‘æ—¶åºå®šä½çš„å‡†ç¡®æ€§ï¼Œå¯å¢žå¼ºå¤šæ¨¡æ€AIç³»ç»Ÿçš„å®žç”¨æ€§å’Œå¯é æ€§ï¼ŒæŽ¨åŠ¨äººæœºäº¤äº’å’Œè‡ªåŠ¨åŒ–å¤„ç†çš„å‘å±•ï¼Œå¯¹æœªæ¥çš„è§†é¢‘AIæŠ€æœ¯æ ‡å‡†åŒ–å’Œå•†ä¸šåŒ–æœ‰é‡è¦å½±å“ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.

