---
layout: default
title: Step-Tagging: Toward controlling the generation of Language Reasoning Models through step monitoring
---

# Step-Tagging: Toward controlling the generation of Language Reasoning Models through step monitoring

**arXiv**: [2512.14332v1](https://arxiv.org/abs/2512.14332) | [PDF](https://arxiv.org/pdf/2512.14332.pdf)

**ä½œè€…**: Yannis Belkhiter, Seshu Tirupathi, Giulio Zizzo, John D. Kelleher

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºStep-Taggingæ¡†æž¶ï¼Œé€šè¿‡å®žæ—¶ç›‘æŽ§æŽ¨ç†æ­¥éª¤ç±»åž‹æ¥æŽ§åˆ¶è¯­è¨€æŽ¨ç†æ¨¡åž‹çš„ç”Ÿæˆè¿‡ç¨‹**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **è§†è§‰é‡Œç¨‹è®¡** **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `è¯­è¨€æŽ¨ç†æ¨¡åž‹` `æ­¥éª¤ç›‘æŽ§` `æ—©æœŸåœæ­¢` `è½»é‡çº§åˆ†ç±»å™¨` `æŽ¨ç†æ•ˆçŽ‡` `å¯è§£é‡ŠAI` `ä»¤ç‰Œå‡å°‘` `ReasonTypeåˆ†ç±»æ³•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è¯­è¨€æŽ¨ç†æ¨¡åž‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å­˜åœ¨æ•ˆçŽ‡ä½Žä¸‹é—®é¢˜ï¼Œè¿‡åº¦ç”ŸæˆéªŒè¯å’Œåæ€æ­¥éª¤ï¼Œå¯¼è‡´è®¡ç®—èµ„æºæµªè´¹ã€‚
2. æå‡ºStep-Taggingæ¡†æž¶ï¼Œé€šè¿‡è½»é‡çº§å¥å­åˆ†ç±»å™¨å®žæ—¶æ ‡æ³¨æŽ¨ç†æ­¥éª¤ç±»åž‹ï¼Œå¹¶åŸºäºŽReasonTypeåˆ†ç±»æ³•ç›‘æŽ§æŽ¨ç†è¡Œä¸ºã€‚
3. åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®žçŽ°20-50%ä»¤ç‰Œå‡å°‘ï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®æ€§ï¼Œä¸ºLRMç”ŸæˆæŽ§åˆ¶æä¾›æ–°æ–¹æ³•ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¯­è¨€æŽ¨ç†æ¨¡åž‹ï¼ˆLRMsï¼‰é¢†åŸŸè¿‘å¹´æ¥å‘å±•è¿…é€Ÿï¼Œè®­ç»ƒå’ŒæŽ¨ç†æŠ€æœ¯çš„è¿›æ­¥ä½¿LRMsèƒ½å¤Ÿè¿›è¡Œæ›´é•¿ã€æ›´å‡†ç¡®çš„æŽ¨ç†ã€‚ç„¶è€Œï¼Œè¶Šæ¥è¶Šå¤šçš„ç ”ç©¶è¡¨æ˜Žï¼ŒLRMsä»ç„¶æ•ˆçŽ‡ä½Žä¸‹ï¼Œè¿‡åº¦ç”ŸæˆéªŒè¯å’Œåæ€æ­¥éª¤ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Step-Taggingæ¡†æž¶ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„å¥å­åˆ†ç±»å™¨ï¼Œèƒ½å¤Ÿå®žæ—¶æ ‡æ³¨LRMç”Ÿæˆçš„æŽ¨ç†æ­¥éª¤ç±»åž‹ã€‚ä¸ºäº†ç›‘æŽ§æŽ¨ç†è¡Œä¸ºï¼Œæˆ‘ä»¬å¼•å…¥äº†ReasonTypeï¼šä¸€ç§æ–°é¢–çš„æŽ¨ç†æ­¥éª¤åˆ†ç±»æ³•ã€‚åŸºäºŽæ­¤æ¡†æž¶ï¼Œæˆ‘ä»¬è¯æ˜Žäº†åœ¨çº¿ç›‘æŽ§ç‰¹å®šæ­¥éª¤çš„æ•°é‡å¯ä»¥äº§ç”Ÿæœ‰æ•ˆçš„å¯è§£é‡Šæ—©æœŸåœæ­¢æ ‡å‡†ï¼Œç”¨äºŽLRMæŽ¨ç†ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå¼€æºæŽ¨ç†æ¨¡åž‹ä¸Šè¯„ä¼°äº†Step-Taggingæ¡†æž¶ï¼Œä½¿ç”¨æ ‡å‡†åŸºå‡†æ•°æ®é›†ï¼šMATH500ã€GSM8Kã€AIMEä»¥åŠéžæ•°å­¦ä»»åŠ¡ï¼ˆGPQAå’ŒMMLU-Proï¼‰ã€‚æˆ‘ä»¬å®žçŽ°äº†20%åˆ°50%çš„ä»¤ç‰Œå‡å°‘ï¼ŒåŒæ—¶ä¿æŒä¸Žæ ‡å‡†ç”Ÿæˆç›¸å½“çš„å‡†ç¡®æ€§ï¼Œåœ¨è®¡ç®—é‡æ›´å¤§çš„ä»»åŠ¡ä¸Šè§‚å¯Ÿåˆ°æœ€å¤§çš„å¢žç›Šã€‚è¿™é¡¹å·¥ä½œæä¾›äº†ä¸€ç§æ–°é¢–çš„æ–¹å¼æ¥å¢žåŠ å¯¹LRMç”Ÿæˆçš„æŽ§åˆ¶ï¼Œä»¥åŠä¸€ç§ç ”ç©¶LRMè¡Œä¸ºçš„æ–°å·¥å…·ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè¯­è¨€æŽ¨ç†æ¨¡åž‹ï¼ˆLRMsï¼‰åœ¨æŽ¨ç†è¿‡ç¨‹ä¸­å­˜åœ¨æ•ˆçŽ‡ä½Žä¸‹é—®é¢˜ï¼Œå…·ä½“è¡¨çŽ°ä¸ºè¿‡åº¦ç”ŸæˆéªŒè¯å’Œåæ€æ­¥éª¤ï¼Œå¯¼è‡´ä¸å¿…è¦çš„è®¡ç®—å¼€é”€å’Œèµ„æºæµªè´¹ã€‚çŽ°æœ‰æ–¹æ³•ç¼ºä¹å¯¹æŽ¨ç†æ­¥éª¤ç±»åž‹çš„å®žæ—¶ç›‘æŽ§æœºåˆ¶ï¼Œæ— æ³•æœ‰æ•ˆæŽ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ï¼Œä½¿å¾—æ¨¡åž‹åœ¨è¾¾åˆ°æ­£ç¡®ç»“è®ºåŽä»ç»§ç»­ç”Ÿæˆå†—ä½™æ­¥éª¤ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥ä¸€ä¸ªè½»é‡çº§çš„å¥å­åˆ†ç±»å™¨ï¼ˆStep-Taggingæ¡†æž¶ï¼‰ï¼Œå®žæ—¶è¯†åˆ«å’Œæ ‡æ³¨LRMç”Ÿæˆçš„æŽ¨ç†æ­¥éª¤ç±»åž‹ï¼Œä»Žè€Œç›‘æŽ§æŽ¨ç†è¡Œä¸ºã€‚åŸºäºŽæ­¤ï¼Œè®¾è®¡æ—©æœŸåœæ­¢æ ‡å‡†ï¼Œå½“æ£€æµ‹åˆ°ç‰¹å®šæ­¥éª¤ï¼ˆå¦‚éªŒè¯æˆ–åæ€ï¼‰æ•°é‡è¾¾åˆ°é˜ˆå€¼æ—¶ï¼Œæå‰ç»ˆæ­¢ç”Ÿæˆï¼Œä»¥å‡å°‘ä»¤ç‰Œä½¿ç”¨å¹¶æé«˜æ•ˆçŽ‡ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æž¶æž„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œæž„å»ºReasonTypeåˆ†ç±»æ³•ï¼Œå®šä¹‰æŽ¨ç†æ­¥éª¤ç±»åž‹ï¼ˆå¦‚å‰æã€æŽ¨å¯¼ã€éªŒè¯ã€åæ€ç­‰ï¼‰ï¼›å…¶æ¬¡ï¼Œè®­ç»ƒä¸€ä¸ªå¥å­åˆ†ç±»å™¨ï¼Œå¯¹LRMç”Ÿæˆçš„æ¯ä¸ªå¥å­è¿›è¡Œå®žæ—¶åˆ†ç±»ï¼Œæ ‡æ³¨å…¶æ‰€å±žçš„ReasonTypeç±»åˆ«ã€‚åœ¨æŽ¨ç†è¿‡ç¨‹ä¸­ï¼Œç³»ç»Ÿåœ¨çº¿ç»Ÿè®¡å„ç±»æ­¥éª¤çš„æ•°é‡ï¼Œå¹¶æ ¹æ®é¢„è®¾è§„åˆ™ï¼ˆå¦‚éªŒè¯æ­¥éª¤è¿‡å¤šï¼‰è§¦å‘æ—©æœŸåœæ­¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯æå‡ºäº†ReasonTypeåˆ†ç±»æ³•å’ŒåŸºäºŽStep-Taggingçš„å®žæ—¶ç›‘æŽ§æ¡†æž¶ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬è´¨åŒºåˆ«åœ¨äºŽå°†æŽ¨ç†è¿‡ç¨‹åˆ†è§£ä¸ºå¯è§£é‡Šçš„æ­¥éª¤ç±»åž‹ï¼Œå¹¶é€šè¿‡è½»é‡çº§åˆ†ç±»å™¨å®žçŽ°åŠ¨æ€æŽ§åˆ¶ï¼Œè€Œéžä¾èµ–å›ºå®šçš„ç”Ÿæˆé•¿åº¦æˆ–åŽå¤„ç†ä¼˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ReasonTypeåˆ†ç±»æ³•çš„å…·ä½“ç±»åˆ«å®šä¹‰ï¼ˆå¦‚æ•°å­¦æŽ¨ç†ä¸­çš„è®¡ç®—ã€éªŒè¯ï¼Œéžæ•°å­¦ä»»åŠ¡ä¸­çš„åˆ†æžã€æ€»ç»“ï¼‰ï¼Œå¥å­åˆ†ç±»å™¨çš„ç½‘ç»œç»“æž„ï¼ˆå¯èƒ½åŸºäºŽé¢„è®­ç»ƒè¯­è¨€æ¨¡åž‹çš„è½»é‡çº§å¾®è°ƒï¼‰ï¼Œä»¥åŠæ—©æœŸåœæ­¢è§„åˆ™çš„å‚æ•°è®¾ç½®ï¼ˆå¦‚éªŒè¯æ­¥éª¤çš„æœ€å¤§å…è®¸æ•°é‡ï¼‰ã€‚æŸå¤±å‡½æ•°é€šå¸¸ä½¿ç”¨äº¤å‰ç†µæŸå¤±è¿›è¡Œå¤šåˆ†ç±»è®­ç»ƒï¼Œç¡®ä¿åˆ†ç±»å‡†ç¡®æ€§ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨MATH500ã€GSM8Kã€AIMEç­‰æ•°å­¦æ•°æ®é›†ä»¥åŠGPQAå’ŒMMLU-Proéžæ•°å­¦ä»»åŠ¡ä¸Šï¼ŒStep-Taggingæ¡†æž¶å®žçŽ°äº†æ˜¾è‘—çš„ä»¤ç‰Œå‡å°‘ï¼šè¾¾åˆ°20%åˆ°50%ï¼ŒåŒæ—¶ä¿æŒä¸Žæ ‡å‡†ç”Ÿæˆæ–¹æ³•ç›¸å½“çš„å‡†ç¡®æ€§ã€‚å¯¹æ¯”åŸºçº¿ä¸ºæœªä½¿ç”¨æ—©æœŸåœæ­¢çš„æ ‡å‡†LRMç”Ÿæˆï¼Œæå‡å¹…åº¦åœ¨è®¡ç®—é‡æ›´å¤§çš„ä»»åŠ¡ä¸­å°¤ä¸ºæ˜Žæ˜¾ï¼Œä¾‹å¦‚åœ¨å¤æ‚æ•°å­¦æŽ¨ç†ä¸­è§‚å¯Ÿåˆ°æœ€å¤§å¢žç›Šã€‚è¿™è¯æ˜Žäº†æ¡†æž¶åœ¨å‡å°‘å†—ä½™ç”Ÿæˆæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜æ•ˆæŽ¨ç†çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œå¦‚æ•°å­¦é—®é¢˜æ±‚è§£ã€ç§‘å­¦é—®ç­”å’Œå¤æ‚å†³ç­–æ”¯æŒã€‚å®žé™…ä»·å€¼åœ¨äºŽé™ä½Žè®¡ç®—æˆæœ¬ï¼Œæé«˜è¯­è¨€æŽ¨ç†æ¨¡åž‹çš„éƒ¨ç½²æ•ˆçŽ‡ï¼Œé€‚ç”¨äºŽèµ„æºå—é™çŽ¯å¢ƒã€‚æœªæ¥å¯èƒ½å½±å“æ¨¡åž‹ä¼˜åŒ–å’Œå¯è§£é‡Šæ€§ç ”ç©¶ï¼Œä¸ºå¼€å‘æ›´å¯æŽ§çš„AIç³»ç»Ÿæä¾›æ–°å·¥å…·ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The field of Language Reasoning Models (LRMs) has been very active over the past few years with advances in training and inference techniques enabling LRMs to reason longer, and more accurately. However, a growing body of studies show that LRMs are still inefficient, over-generating verification and reflection steps. To address this challenge, we introduce the Step-Tagging framework, a lightweight sentence-classifier enabling real-time annotation of the type of reasoning steps that an LRM is generating. To monitor reasoning behaviors, we introduced ReasonType: a novel taxonomy of reasoning steps. Building on this framework, we demonstrated that online monitoring of the count of specific steps can produce effective interpretable early stopping criteria of LRM inferences. We evaluate the Step-tagging framework on three open-source reasoning models across standard benchmark datasets: MATH500, GSM8K, AIME and non-mathematical tasks (GPQA and MMLU-Pro). We achieve 20 to 50\% token reduction while maintaining comparable accuracy to standard generation, with largest gains observed on more computation-heavy tasks. This work offers a novel way to increase control over the generation of LRMs, and a new tool to study behaviors of LRMs.

