---
layout: default
title: A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in Resource-Constrained Settings
---

# A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in Resource-Constrained Settings

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2504.15610" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2504.15610</a>
  <a href="https://arxiv.org/pdf/2504.15610.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2504.15610" onclick="toggleFavorite(this, '2504.15610', 'A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in Resource-Constrained Settings')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Md Millat Hosen

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºLoRAå¾®è°ƒLLMçš„æ•™è‚²æŒ‡å¯¼æ–¹æ³•ï¼Œé€‚ç”¨äºèµ„æºå—é™åœºæ™¯**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `LoRAå¾®è°ƒ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•™è‚²æŒ‡å¯¼` `èµ„æºå—é™` `å‚æ•°é«˜æ•ˆ` `çŸ¥è¯†è’¸é¦` `å‡ºå›½ç•™å­¦å’¨è¯¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMåœ¨æ•™è‚²æŒ‡å¯¼é¢†åŸŸåº”ç”¨é¢ä¸´è®¡ç®—èµ„æºéœ€æ±‚é«˜å’Œé¢†åŸŸçŸ¥è¯†ä¸è¶³çš„æŒ‘æˆ˜ã€‚
2. åˆ©ç”¨LoRAè¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼Œç»“åˆåˆæˆæ•°æ®å’Œäººå·¥æ ‡æ³¨æ•°æ®ï¼Œæå‡LLMåœ¨ç‰¹å®šé¢†åŸŸçš„æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é™ä½è®­ç»ƒæŸå¤±ã€æé«˜é¢†åŸŸå‡†ç¡®ç‡å’Œæ ¼å¼æ”¯æŒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸”è¿è¡Œæ•ˆç‡é«˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»æµé«˜æ•ˆçš„æ–¹æ³•ï¼Œç”¨äºè°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä»¥é€‚åº”å­¦æœ¯æŒ‡å¯¼ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å‡ºå›½ç•™å­¦èƒŒæ™¯ï¼Œå¹¶åº”ç”¨äºèµ„æºæœ‰é™çš„æ–‡åŒ–é€‚åº”æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¸¦æœ‰ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ–¹æ³•å’Œ4ä½é‡åŒ–æ–¹æ³•çš„Mistral-7B-Instructæ¨¡å‹ï¼Œå¹¶é’ˆå¯¹æœ¬ç ”ç©¶çš„ç›®çš„è¿›è¡Œäº†ä¸¤ä¸ªä¸åŒé˜¶æ®µçš„è®­ç»ƒï¼Œä»¥å¢å¼ºé¢†åŸŸç‰¹å¼‚æ€§ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæ¨¡å‹é€šè¿‡Gemini Pro APIä½¿ç”¨åˆæˆæ•°æ®é›†è¿›è¡Œæ¡ä»¶è®­ç»ƒï¼›åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæ¨¡å‹ä½¿ç”¨StudyAbroadGPTé¡¹ç›®ä¸­æ‰‹åŠ¨ç­–åˆ’çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œä»¥å®ç°å¢å¼ºçš„ã€ä¸Šä¸‹æ–‡ç›¸å…³çš„å“åº”ã€‚æŠ€æœ¯åˆ›æ–°åŒ…æ‹¬å†…å­˜é«˜æ•ˆçš„é‡åŒ–ã€å‚æ•°é«˜æ•ˆçš„é€‚åº”ä»¥åŠé€šè¿‡Weights & Biasesè¿›è¡Œçš„æŒç»­è®­ç»ƒåˆ†æã€‚è®­ç»ƒåï¼Œæœ¬ç ”ç©¶è¡¨æ˜è®­ç»ƒæŸå¤±å‡å°‘äº†52.7%ï¼Œé¢†åŸŸç‰¹å®šæ¨èçš„å‡†ç¡®ç‡è¾¾åˆ°äº†92%ï¼Œå®ç°äº†95%çš„åŸºäºMarkdownçš„æ ¼å¼æ”¯æŒï¼Œå¹¶ä¸”åœ¨ç°æˆçš„GPUè®¾å¤‡ä¸Šå®ç°äº†æ¯ç§’100ä¸ªæ ·æœ¬çš„ä¸­å€¼è¿è¡Œé€Ÿç‡ã€‚è¿™äº›å‘ç°æ”¯æŒäº†instruction-tuned LLMåœ¨æ•™è‚²é¡¾é—®ä¸­çš„æœ‰æ•ˆåº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºæœ‰é™çš„æœºæ„åœºæ™¯ä¸­ã€‚å±€é™æ€§åŒ…æ‹¬æ³›åŒ–èƒ½åŠ›ä¸‹é™å’Œåˆæˆæ•°æ®é›†çš„åº”ç”¨ï¼Œä½†è¯¥æ¡†æ¶å¯æ‰©å±•ï¼Œå¯æ·»åŠ æ–°çš„å¤šè¯­è¨€å¢å¼ºå’Œå®æ—¶å­¦æœ¯æŒ‡å¯¼æµç¨‹ã€‚æœªæ¥çš„æ–¹å‘å¯èƒ½åŒ…æ‹¬æ£€ç´¢å¢å¼ºç”Ÿæˆã€åº”ç”¨åŠ¨æ€é‡åŒ–ç¨‹åºä»¥åŠè¿æ¥åˆ°å®æ—¶å­¦æœ¯æ•°æ®åº“ï¼Œä»¥æé«˜é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸‹ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨äºæ•™è‚²æŒ‡å¯¼ï¼Œç‰¹åˆ«æ˜¯å‡ºå›½ç•™å­¦å’¨è¯¢çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºè¿›è¡Œå…¨å‚æ•°å¾®è°ƒï¼Œå¹¶ä¸”å¯èƒ½ç¼ºä¹ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†ï¼Œå¯¼è‡´æ¨¡å‹æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ–¹æ³•ï¼Œåœ¨é¢„è®­ç»ƒçš„LLMåŸºç¡€ä¸Šè¿›è¡Œå‚æ•°é«˜æ•ˆçš„å¾®è°ƒã€‚LoRAé€šè¿‡å¼•å…¥å°‘é‡å¯è®­ç»ƒçš„å‚æ•°æ¥é€‚åº”ç‰¹å®šä»»åŠ¡ï¼Œä»è€Œæ˜¾è‘—é™ä½è®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜ç»“åˆäº†åˆæˆæ•°æ®å’Œäººå·¥æ ‡æ³¨æ•°æ®ï¼Œä»¥å¢å¼ºæ¨¡å‹åœ¨æ•™è‚²æŒ‡å¯¼é¢†åŸŸçš„çŸ¥è¯†å’Œèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚ç¬¬ä¸€é˜¶æ®µï¼Œä½¿ç”¨Gemini Pro APIç”Ÿæˆåˆæˆæ•°æ®é›†ï¼Œå¯¹æ¨¡å‹è¿›è¡Œåˆæ­¥çš„é¢†åŸŸçŸ¥è¯†è®­ç»ƒã€‚ç¬¬äºŒé˜¶æ®µï¼Œä½¿ç”¨StudyAbroadGPTé¡¹ç›®ä¸­çš„äººå·¥æ ‡æ³¨æ•°æ®é›†ï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹çš„ä¸Šä¸‹æ–‡ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä½¿ç”¨4-bité‡åŒ–æ–¹æ³•ï¼Œä»¥å‡å°‘å†…å­˜å ç”¨ã€‚åŒæ—¶ï¼Œåˆ©ç”¨Weights & Biasesè¿›è¡ŒæŒç»­çš„è®­ç»ƒåˆ†æï¼Œç›‘æ§æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†LoRAæ–¹æ³•ä¸åˆæˆæ•°æ®å’Œäººå·¥æ ‡æ³¨æ•°æ®ç›¸ç»“åˆï¼Œå®ç°äº†ä¸€ç§å‚æ•°é«˜æ•ˆä¸”é¢†åŸŸçŸ¥è¯†ä¸°å¯Œçš„LLMå¾®è°ƒæ–¹æ³•ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸‹ï¼Œæœ‰æ•ˆåœ°æå‡LLMåœ¨æ•™è‚²æŒ‡å¯¼é¢†åŸŸçš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡é‡‡ç”¨äº†Mistral-7B-Instructæ¨¡å‹ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨LoRAè¿›è¡Œå¾®è°ƒã€‚å…·ä½“æ¥è¯´ï¼ŒLoRAåœ¨æ¨¡å‹çš„Transformerå±‚ä¸­æ’å…¥äº†ä½ç§©çŸ©é˜µï¼Œè¿™äº›çŸ©é˜µæ˜¯å”¯ä¸€éœ€è¦è®­ç»ƒçš„å‚æ•°ã€‚è®ºæ–‡è¿˜ä½¿ç”¨äº†4-bité‡åŒ–æ–¹æ³•ï¼Œä»¥å‡å°‘æ¨¡å‹çš„å¤§å°å’Œå†…å­˜å ç”¨ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œè®ºæ–‡å¯èƒ½é‡‡ç”¨äº†æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œç”¨äºè¡¡é‡æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ä¸ç›®æ ‡æ–‡æœ¬ä¹‹é—´çš„å·®å¼‚ã€‚ï¼ˆå…·ä½“æŸå¤±å‡½æ•°ç»†èŠ‚æœªçŸ¥ï¼‰

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2504.15610/fig1_arch.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2504.15610/fig2_loss_p100.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2504.15610/fig3_grad_p100.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—é™ä½è®­ç»ƒæŸå¤±ï¼ˆ52.7%ï¼‰ï¼Œæé«˜é¢†åŸŸç‰¹å®šæ¨èçš„å‡†ç¡®ç‡ï¼ˆ92%ï¼‰ï¼Œå¹¶æ”¯æŒé«˜è´¨é‡çš„Markdownæ ¼å¼è¾“å‡ºï¼ˆ95%ï¼‰ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨æ™®é€šGPUè®¾å¤‡ä¸Šå®ç°äº†æ¯ç§’100ä¸ªæ ·æœ¬çš„æ¨ç†é€Ÿåº¦ï¼ŒéªŒè¯äº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„å®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¼€å‘ä½æˆæœ¬ã€é«˜æ•ˆçš„æ™ºèƒ½æ•™è‚²å’¨è¯¢ç³»ç»Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºåŒ®ä¹çš„å­¦æ ¡æˆ–åœ°åŒºã€‚å®ƒå¯ä»¥ä¸ºå­¦ç”Ÿæä¾›ä¸ªæ€§åŒ–çš„å‡ºå›½ç•™å­¦æŒ‡å¯¼ã€è¯¾ç¨‹é€‰æ‹©å»ºè®®å’Œæ–‡åŒ–é€‚åº”æ”¯æŒï¼Œä»è€Œæé«˜æ•™è‚²å…¬å¹³æ€§å’Œå­¦ç”ŸæˆåŠŸç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–æ•™è‚²é¢†åŸŸï¼Œå¦‚èŒä¸šè§„åˆ’ã€å¿ƒç†è¾…å¯¼ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The current study describes a cost-effective method for adapting large language models (LLMs) for academic advising with study-abroad contexts in mind and for application in low-resource methods for acculturation. With the Mistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and a 4-bit quantization method, the model underwent training in two distinct stages related to this study's purpose to enhance domain specificity while maintaining computational efficiency. In Phase 1, the model was conditioned with a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained with manually curated datasets from the StudyAbroadGPT project to achieve enhanced, contextualized responses. Technical innovations entailed memory-efficient quantization, parameter-efficient adaptation, and continuous training analytics via Weights & Biases. After training, this study demonstrated a reduction in training loss by 52.7%, 92% accuracy in domain-specific recommendations, achieved 95% markdown-based formatting support, and a median run-rate of 100 samples per second on off-the-shelf GPU equipment. These findings support the effective application of instruction-tuned LLMs within educational advisers, especially in low-resource institutional scenarios. Limitations included decreased generalizability and the application of a synthetically generated dataset, but this framework is scalable for adding new multilingual-augmented and real-time academic advising processes. Future directions may include plans for the integration of retrieval-augmented generation, applying dynamic quantization routines, and connecting to real-time academic databases to increase adaptability and accuracy.

