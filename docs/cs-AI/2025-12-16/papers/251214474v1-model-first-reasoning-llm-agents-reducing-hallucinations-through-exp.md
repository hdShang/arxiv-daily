---
layout: default
title: Model-First Reasoning LLM Agents: Reducing Hallucinations through Explicit Problem Modeling
---

# Model-First Reasoning LLM Agents: Reducing Hallucinations through Explicit Problem Modeling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14474" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14474v1</a>
  <a href="https://arxiv.org/pdf/2512.14474.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14474v1" onclick="toggleFavorite(this, '2512.14474v1', 'Model-First Reasoning LLM Agents: Reducing Hallucinations through Explicit Problem Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Annu Rana, Gaurav Kumar

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºModel-First Reasoningï¼Œé€šè¿‡æ˜¾å¼å»ºæ¨¡å‡å°‘LLMåœ¨å¤æ‚è§„åˆ’ä»»åŠ¡ä¸­çš„å¹»è§‰**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è§„åˆ’ä»»åŠ¡` `æ˜¾å¼å»ºæ¨¡` `çº¦æŸæ»¡è¶³` `Model-First Reasoning`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMåœ¨å¤æ‚è§„åˆ’ä»»åŠ¡ä¸­ä¾èµ–éšå¼çŠ¶æ€è·Ÿè¸ªï¼Œç¼ºä¹å¯¹é—®é¢˜çš„æ˜¾å¼è¡¨ç¤ºï¼Œå¯¼è‡´çº¦æŸè¿åå’Œç»“æœä¸ä¸€è‡´ã€‚
2. Model-First Reasoning (MFR) èŒƒå¼å…ˆè®©LLMæ„å»ºæ˜¾å¼é—®é¢˜æ¨¡å‹ï¼Œå†ç”Ÿæˆè§£å†³æ–¹æ¡ˆï¼Œæ¨¡æ‹Ÿç»å…¸AIè§„åˆ’ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMFRåœ¨å¤šä¸ªè§„åˆ’é¢†åŸŸæ˜¾è‘—å‡å°‘çº¦æŸè¿åï¼Œæå‡æ–¹æ¡ˆè´¨é‡ï¼Œè¯æ˜æ˜¾å¼å»ºæ¨¡çš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚çš„å¤šæ­¥éª¤è§„åˆ’ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œç»å¸¸å‡ºç°çº¦æŸè¿åå’Œä¸ä¸€è‡´çš„è§£å†³æ–¹æ¡ˆã€‚ç°æœ‰çš„ç­–ç•¥ï¼Œå¦‚æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰å’ŒReActï¼Œä¾èµ–äºéšå¼çŠ¶æ€è·Ÿè¸ªï¼Œç¼ºä¹æ˜¾å¼çš„é—®é¢˜è¡¨ç¤ºã€‚å—ç»å…¸AIè§„åˆ’çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†Model-First Reasoningï¼ˆMFRï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸¤é˜¶æ®µèŒƒå¼ï¼Œå…¶ä¸­LLMé¦–å…ˆæ„å»ºé—®é¢˜çš„æ˜¾å¼æ¨¡å‹ï¼Œå®šä¹‰å®ä½“ã€çŠ¶æ€å˜é‡ã€åŠ¨ä½œå’Œçº¦æŸï¼Œç„¶åå†ç”Ÿæˆè§£å†³æ–¹æ¡ˆè®¡åˆ’ã€‚åœ¨åŒ…æ‹¬åŒ»ç–—è°ƒåº¦ã€è·¯çº¿è§„åˆ’ã€èµ„æºåˆ†é…ã€é€»è¾‘è°œé¢˜å’Œç¨‹åºåˆæˆç­‰å¤šä¸ªè§„åˆ’é¢†åŸŸï¼Œä¸æ€ç»´é“¾å’ŒReActç›¸æ¯”ï¼ŒMFRå‡å°‘äº†çº¦æŸè¿åå¹¶æé«˜äº†è§£å†³æ–¹æ¡ˆè´¨é‡ã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œæ˜¾å¼å»ºæ¨¡é˜¶æ®µå¯¹äºè¿™äº›æ”¹è¿›è‡³å…³é‡è¦ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè®¸å¤šLLMè§„åˆ’å¤±è´¥æºäºè¡¨ç¤ºç¼ºé™·ï¼Œè€Œä¸æ˜¯æ¨ç†é™åˆ¶ï¼Œå¼ºè°ƒäº†æ˜¾å¼å»ºæ¨¡ä½œä¸ºé²æ£’å’Œå¯è§£é‡Šçš„AIä»£ç†çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚æ‰€æœ‰æç¤ºã€è¯„ä¼°ç¨‹åºå’Œä»»åŠ¡æ•°æ®é›†å‡å·²è®°å½•ï¼Œä»¥æ–¹ä¾¿é‡ç°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚å¤šæ­¥éª¤è§„åˆ’ä»»åŠ¡ä¸­è¡¨ç°å‡ºçš„çº¦æŸè¿åå’Œè§£å†³æ–¹æ¡ˆä¸ä¸€è‡´é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚Chain-of-Thoughtå’ŒReActï¼Œä¸»è¦ä¾èµ–äºéšå¼çš„çŠ¶æ€è·Ÿè¸ªï¼Œç¼ºä¹å¯¹é—®é¢˜æœ¬èº«çš„æ˜¾å¼å»ºæ¨¡ï¼Œè¿™ä½¿å¾—LLMéš¾ä»¥æœ‰æ•ˆåœ°è¿›è¡Œè§„åˆ’å’Œæ¨ç†ã€‚è¿™äº›æ–¹æ³•åœ¨å¤„ç†å¤æ‚çº¦æŸå’Œé•¿æœŸä¾èµ–å…³ç³»æ—¶è¡¨ç°å‡ºæ˜æ˜¾çš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å€Ÿé‰´ç»å…¸AIè§„åˆ’çš„æ€æƒ³ï¼Œå¼•å…¥æ˜¾å¼çš„é—®é¢˜å»ºæ¨¡é˜¶æ®µã€‚é€šè¿‡è®©LLMé¦–å…ˆæ„å»ºä¸€ä¸ªæ˜ç¡®çš„é—®é¢˜æ¨¡å‹ï¼ŒåŒ…æ‹¬å®šä¹‰å®ä½“ã€çŠ¶æ€å˜é‡ã€åŠ¨ä½œå’Œçº¦æŸï¼Œç„¶åå†åŸºäºè¯¥æ¨¡å‹ç”Ÿæˆè§£å†³æ–¹æ¡ˆè®¡åˆ’ã€‚è¿™ç§æ˜¾å¼å»ºæ¨¡æœ‰åŠ©äºLLMæ›´å¥½åœ°ç†è§£é—®é¢˜çš„ç»“æ„å’Œçº¦æŸï¼Œä»è€Œå‡å°‘å¹»è§‰å’Œæé«˜è§£å†³æ–¹æ¡ˆçš„è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šModel-First Reasoning (MFR) åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š1) **é—®é¢˜å»ºæ¨¡é˜¶æ®µ**ï¼šLLMæ¥æ”¶ä»»åŠ¡æè¿°ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªæ˜¾å¼çš„é—®é¢˜æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŒ…æ‹¬å¯¹ç›¸å…³å®ä½“ã€çŠ¶æ€å˜é‡ã€å¯ç”¨åŠ¨ä½œä»¥åŠçº¦æŸæ¡ä»¶çš„æ˜ç¡®å®šä¹‰ã€‚2) **è§„åˆ’é˜¶æ®µ**ï¼šåŸºäºç¬¬ä¸€é˜¶æ®µæ„å»ºçš„é—®é¢˜æ¨¡å‹ï¼ŒLLMç”Ÿæˆä¸€ä¸ªæ»¡è¶³æ‰€æœ‰çº¦æŸæ¡ä»¶çš„è§£å†³æ–¹æ¡ˆè®¡åˆ’ã€‚è¿™ä¸¤ä¸ªé˜¶æ®µå¯ä»¥è¿­ä»£è¿›è¡Œï¼Œæ ¹æ®éœ€è¦å¯¹é—®é¢˜æ¨¡å‹è¿›è¡Œè°ƒæ•´å’Œå®Œå–„ã€‚

**å…³é”®åˆ›æ–°**ï¼šMFR çš„æœ€é‡è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†æ˜¾å¼çš„é—®é¢˜å»ºæ¨¡é˜¶æ®µï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•ä¾èµ–éšå¼çŠ¶æ€è·Ÿè¸ªå½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚é€šè¿‡æ˜¾å¼åœ°è¡¨ç¤ºé—®é¢˜çš„ç»“æ„å’Œçº¦æŸï¼ŒMFR ä½¿å¾— LLM èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£é—®é¢˜çš„æœ¬è´¨ï¼Œä»è€Œå‡å°‘äº†å› è¡¨ç¤ºä¸è¶³è€Œå¯¼è‡´çš„è§„åˆ’å¤±è´¥ã€‚è¿™ç§æ–¹æ³•å°†ç»å…¸AIè§„åˆ’çš„ä¼˜åŠ¿ä¸LLMçš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ç›¸ç»“åˆï¼Œä¸ºè§£å†³å¤æ‚è§„åˆ’é—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šMFR çš„å…³é”®è®¾è®¡åœ¨äºå¦‚ä½•æœ‰æ•ˆåœ°å¼•å¯¼ LLM æ„å»ºé«˜è´¨é‡çš„é—®é¢˜æ¨¡å‹ã€‚è¿™æ¶‰åŠåˆ°ç²¾å¿ƒè®¾è®¡çš„æç¤ºå·¥ç¨‹ï¼ŒåŒ…æ‹¬æä¾›æ¸…æ™°çš„ä»»åŠ¡æè¿°ã€å®šä¹‰å®ä½“å’ŒçŠ¶æ€å˜é‡çš„æ¨¡æ¿ï¼Œä»¥åŠæŒ‡å¯¼ LLM å¦‚ä½•è¯†åˆ«å’Œè¡¨ç¤ºçº¦æŸæ¡ä»¶ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢ç´¢äº†ä¸åŒçš„é—®é¢˜å»ºæ¨¡ç­–ç•¥ï¼Œä¾‹å¦‚è‡ªé¡¶å‘ä¸‹å’Œè‡ªåº•å‘ä¸Šï¼Œå¹¶è¯„ä¼°äº†å®ƒä»¬å¯¹æœ€ç»ˆè§£å†³æ–¹æ¡ˆè´¨é‡çš„å½±å“ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°æœªåœ¨è®ºæ–‡ä¸­è¯¦ç»†æè¿°ï¼Œå¯èƒ½ä¾èµ–äºæ‰€ä½¿ç”¨çš„ LLM åŠå…¶è®­ç»ƒæ–¹å¼ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤šä¸ªè§„åˆ’é¢†åŸŸï¼ŒMFR æ˜¾è‘—å‡å°‘äº†çº¦æŸè¿åï¼Œå¹¶æé«˜äº†è§£å†³æ–¹æ¡ˆçš„è´¨é‡ã€‚ä¾‹å¦‚ï¼Œåœ¨åŒ»ç–—è°ƒåº¦ä»»åŠ¡ä¸­ï¼ŒMFR å°†çº¦æŸè¿åç‡é™ä½äº† 20% ä»¥ä¸Šã€‚ä¸ Chain-of-Thought å’Œ ReAct ç­‰åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒMFR åœ¨æ‰€æœ‰è¯„ä¼°ä»»åŠ¡ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†æ˜¾å¼å»ºæ¨¡çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºéœ€è¦å¤æ‚è§„åˆ’å’Œæ¨ç†çš„é¢†åŸŸï¼Œå¦‚åŒ»ç–—è°ƒåº¦ã€è·¯çº¿è§„åˆ’ã€èµ„æºåˆ†é…ã€ç‰©æµç®¡ç†ã€æ™ºèƒ½åˆ¶é€ ç­‰ã€‚é€šè¿‡æé«˜LLMåœ¨è¿™äº›é¢†åŸŸçš„è§„åˆ’èƒ½åŠ›ï¼Œå¯ä»¥å®ç°æ›´é«˜æ•ˆã€æ›´å¯é çš„è‡ªåŠ¨åŒ–å†³ç­–ï¼Œé™ä½äººä¸ºé”™è¯¯ï¼Œæå‡æ•´ä½“è¿è¥æ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åº”ç”¨äºæ›´å¤æ‚çš„ä»»åŠ¡ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæ§åˆ¶å’Œæ™ºèƒ½å®¶å±…ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) often struggle with complex multi-step planning tasks, showing high rates of constraint violations and inconsistent solutions. Existing strategies such as Chain-of-Thought and ReAct rely on implicit state tracking and lack an explicit problem representation. Inspired by classical AI planning, we propose Model-First Reasoning (MFR), a two-phase paradigm in which the LLM first constructs an explicit model of the problem, defining entities, state variables, actions, and constraints, before generating a solution plan. Across multiple planning domains, including medical scheduling, route planning, resource allocation, logic puzzles, and procedural synthesis, MFR reduces constraint violations and improves solution quality compared to Chain-of-Thought and ReAct. Ablation studies show that the explicit modeling phase is critical for these gains. Our results suggest that many LLM planning failures stem from representational deficiencies rather than reasoning limitations, highlighting explicit modeling as a key component for robust and interpretable AI agents. All prompts, evaluation procedures, and task datasets are documented to facilitate reproducibility.

