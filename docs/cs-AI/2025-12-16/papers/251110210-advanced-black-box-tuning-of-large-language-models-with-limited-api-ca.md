---
layout: default
title: Advanced Black-Box Tuning of Large Language Models with Limited API Calls
---

# Advanced Black-Box Tuning of Large Language Models with Limited API Calls

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.10210" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.10210</a>
  <a href="https://arxiv.org/pdf/2511.10210.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.10210" onclick="toggleFavorite(this, '2511.10210', 'Advanced Black-Box Tuning of Large Language Models with Limited API Calls')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhikang Xie, Weilin Wan, Peizhu Gong, Weizhong Zhang, Cheng Jin

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§é«˜çº§é»‘ç›’è°ƒä¼˜æ–¹æ³•ï¼Œä»¥æœ‰é™APIè°ƒç”¨é«˜æ•ˆä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é»‘ç›’è°ƒä¼˜` `å¤§è¯­è¨€æ¨¡å‹` `é«˜æ–¯è¿‡ç¨‹` `ä»£ç†æ¨¡å‹` `APIè°ƒç”¨` `æ¨¡å‹é€‚é…` `LogitMap Pairs`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é»‘ç›’è°ƒä¼˜æ–¹æ³•åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œè¦ä¹ˆæ•ˆç‡é«˜ä½†æå‡æœ‰é™ï¼Œè¦ä¹ˆæ€§èƒ½å¥½ä½†APIè°ƒç”¨æˆæœ¬è¿‡é«˜ã€‚
2. è¯¥è®ºæ–‡æå‡ºä½¿ç”¨é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰ä»£ç†æ¨¡å‹ï¼Œé€šè¿‡å°‘é‡APIè°ƒç”¨å­¦ä¹ åŸºç¡€æ¨¡å‹çš„è¡Œä¸ºï¼ŒæŒ‡å¯¼ä»£ç†æ¨¡å‹çš„è®­ç»ƒã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—é™ä½APIè°ƒç”¨æ¬¡æ•°çš„åŒæ—¶ï¼Œå°†æ¨¡å‹å‡†ç¡®ç‡ä»55.92%æå‡è‡³86.85%ï¼Œä¼˜äºç¦»çº¿æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é»‘ç›’è°ƒä¼˜æ˜¯ä¸€ç§æ–°å…´èŒƒå¼ï¼Œç”¨äºè°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥æ›´å¥½åœ°å®ç°æœŸæœ›çš„è¡Œä¸ºï¼Œå°¤å…¶æ˜¯åœ¨æ— æ³•ç›´æ¥è®¿é—®æ¨¡å‹å‚æ•°æ—¶ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç­–ç•¥å¸¸å¸¸é¢ä¸´æ¬¡ä¼˜çš„æç«¯å›°å¢ƒï¼šè¦ä¹ˆå•ç‹¬è®­ç»ƒä¸€ä¸ªå°å‹çš„ä»£ç†æ¨¡å‹ï¼Œç„¶åç”¨å®ƒæ¥æ”¹å˜åŸºç¡€æ¨¡å‹çš„é¢„æµ‹ï¼Œè¿™ç§æ–¹æ³•æ•ˆç‡æ˜¾è‘—ï¼Œä½†æ”¹è¿›æœ‰é™ï¼›è¦ä¹ˆåœ¨æ¯ä¸ªè°ƒä¼˜è¿­ä»£ä¸­å¯¹åŸºç¡€æ¨¡å‹è¿›è¡ŒAPIè°ƒç”¨ï¼Œè¿™ä¼šå¸¦æ¥è¿‡é«˜çš„è®¡ç®—æˆæœ¬ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹LLMçš„é«˜çº§é»‘ç›’è°ƒä¼˜æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é™åˆ¶äº†APIè°ƒç”¨æ¬¡æ•°ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒç­–ç•¥åŒ…æ‹¬è®­ç»ƒä¸€ä¸ªé«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰ä»£ç†æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨ä»åŸºç¡€æ¨¡å‹æŸ¥è¯¢è·å¾—çš„â€œLogitMap Pairsâ€ï¼Œè¿™äº›æŸ¥è¯¢åŸºäºä¸€ä¸ªæœ€å°ä½†ä¿¡æ¯é‡æé«˜çš„è®­ç»ƒå­é›†ã€‚è¯¥ä»£ç†æ¨¡å‹å¯ä»¥è¿‘ä¼¼åŸºç¡€æ¨¡å‹çš„è¾“å‡ºï¼Œä»è€ŒæŒ‡å¯¼ä»£ç†æ¨¡å‹çš„è®­ç»ƒï¼Œæœ‰æ•ˆåœ°å‡å°‘äº†å¯¹åŸºç¡€æ¨¡å‹ç›´æ¥æŸ¥è¯¢çš„éœ€æ±‚ã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•å°†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„å‡†ç¡®ç‡ä»55.92%æé«˜åˆ°86.85%ï¼Œå¹¶å°†APIæŸ¥è¯¢é¢‘ç‡é™ä½åˆ°ä»…1.38%ã€‚è¿™æ˜¾è‘—ä¼˜äºå®Œå…¨æ— éœ€APIè®¿é—®çš„ç¦»çº¿æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ˜¾è‘—é™ä½APIæˆæœ¬çš„åŒæ—¶ï¼Œä¹Ÿå®ç°äº†ä¸æŸ¥è¯¢å¯†é›†å‹æ–¹æ³•ç›¸å½“æˆ–æ›´é«˜çš„å‡†ç¡®ç‡ã€‚è¿™ä¸ºè¯­è¨€æ¨¡å‹é€‚é…æä¾›äº†ä¸€ç§ç¨³å¥ä¸”é«˜æ•ˆçš„èŒƒå¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰é»‘ç›’è°ƒä¼˜æ–¹æ³•åœ¨è°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹æ—¶é¢ä¸´æ•ˆç‡å’Œæ€§èƒ½çš„å›°å¢ƒã€‚ç›´æ¥å¯¹åŸºç¡€æ¨¡å‹è¿›è¡ŒAPIè°ƒç”¨æˆæœ¬é«˜æ˜‚ï¼Œè€Œç¦»çº¿è®­ç»ƒä»£ç†æ¨¡å‹æ•ˆæœæœ‰é™ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§åœ¨æœ‰é™APIè°ƒç”¨çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½çš„é»‘ç›’è°ƒä¼˜æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰æ„å»ºä¸€ä¸ªä»£ç†æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿè¿‘ä¼¼åŸºç¡€æ¨¡å‹çš„è¾“å‡ºã€‚é€šè¿‡å°‘é‡ä½†ä¿¡æ¯é‡å¤§çš„APIè°ƒç”¨ï¼Œå­¦ä¹ åŸºç¡€æ¨¡å‹çš„â€œLogitMap Pairsâ€ï¼Œç„¶ååˆ©ç”¨è¿™äº›ä¿¡æ¯è®­ç»ƒGPä»£ç†æ¨¡å‹ã€‚è¯¥ä»£ç†æ¨¡å‹éšåç”¨äºæŒ‡å¯¼ä»£ç†æ¨¡å‹çš„è®­ç»ƒï¼Œä»è€Œå‡å°‘å¯¹åŸºç¡€æ¨¡å‹ç›´æ¥æŸ¥è¯¢çš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) ä½¿ç”¨å°‘é‡APIè°ƒç”¨æŸ¥è¯¢åŸºç¡€æ¨¡å‹ï¼Œè·å–â€œLogitMap Pairsâ€ï¼›2) åˆ©ç”¨â€œLogitMap Pairsâ€è®­ç»ƒé«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰ä»£ç†æ¨¡å‹ï¼›3) ä½¿ç”¨GPä»£ç†æ¨¡å‹æŒ‡å¯¼ä»£ç†æ¨¡å‹çš„è®­ç»ƒï¼›4) ä½¿ç”¨è®­ç»ƒå¥½çš„ä»£ç†æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰ä½œä¸ºä»£ç†æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨â€œLogitMap Pairsâ€è¿›è¡Œè®­ç»ƒã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿåœ¨å°‘é‡APIè°ƒç”¨çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆåœ°å­¦ä¹ åŸºç¡€æ¨¡å‹çš„è¡Œä¸ºï¼Œä»è€ŒæŒ‡å¯¼ä»£ç†æ¨¡å‹çš„è®­ç»ƒã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨APIè°ƒç”¨æˆæœ¬å’Œæ¨¡å‹æ€§èƒ½ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­å…³é”®çš„è®¾è®¡åŒ…æ‹¬ï¼šå¦‚ä½•é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„è®­ç»ƒå­é›†ä»¥å‡å°‘APIè°ƒç”¨æ¬¡æ•°ï¼›å¦‚ä½•æ„å»ºå’Œè®­ç»ƒé«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰ä»£ç†æ¨¡å‹ï¼›ä»¥åŠå¦‚ä½•åˆ©ç”¨GPä»£ç†æ¨¡å‹æŒ‡å¯¼ä»£ç†æ¨¡å‹çš„è®­ç»ƒã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼ˆå…·ä½“æ•°å€¼æœªçŸ¥ï¼‰ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2511.10210/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2511.10210/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2511.10210/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å°†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„å‡†ç¡®ç‡ä»55.92%æå‡åˆ°86.85%çš„åŒæ—¶ï¼Œå°†APIæŸ¥è¯¢é¢‘ç‡é™ä½åˆ°ä»…1.38%ã€‚è¿™æ˜¾è‘—ä¼˜äºå®Œå…¨æ— éœ€APIè®¿é—®çš„ç¦»çº¿æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨APIæˆæœ¬æ˜¾è‘—é™ä½çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†ä¸æŸ¥è¯¢å¯†é›†å‹æ–¹æ³•ç›¸å½“æˆ–æ›´é«˜çš„å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœéªŒè¯äº†è¯¥æ–¹æ³•åœ¨é»‘ç›’è°ƒä¼˜æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå®šåˆ¶åŒ–è°ƒæ•´çš„åœºæ™¯ï¼Œä¾‹å¦‚ç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿä¼˜åŒ–ã€ä»¥åŠæ¨¡å‹è¡Œä¸ºçš„å¹²é¢„å’Œæ§åˆ¶ã€‚è¯¥æ–¹æ³•é™ä½äº†APIè°ƒç”¨æˆæœ¬ï¼Œä½¿å¾—åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸‹ä¹Ÿèƒ½é«˜æ•ˆåœ°è¿›è¡Œæ¨¡å‹è°ƒä¼˜ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œå®é™…ä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„é»‘ç›’æ¨¡å‹è°ƒä¼˜ä»»åŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Black-box tuning is an emerging paradigm for adapting large language models (LLMs) to better achieve desired behaviors, particularly when direct access to model parameters is unavailable. Current strategies, however, often present a dilemma of suboptimal extremes: either separately train a small proxy model and then use it to shift the predictions of the foundation model, offering notable efficiency but often yielding limited improvement; or making API calls in each tuning iteration to the foundation model, which entails prohibitive computational costs. Therefore, we propose a novel advanced black-box tuning method for LLMs with limited API calls. Our core strategy involves training a Gaussian Process (GP) surrogate model with "LogitMap Pairs" derived from querying the foundation model on a minimal but highly informative training subset. This surrogate can approximate the outputs of the foundation model to guide the training of the proxy model, thereby effectively reducing the need for direct queries to the foundation model. Extensive experiments verify that our approach elevates pre-trained language model accuracy from 55.92% to 86.85%, reducing the frequency of API queries to merely 1.38%. This significantly outperforms offline approaches that operate entirely without API access. Notably, our method also achieves comparable or superior accuracy to query-intensive approaches, while significantly reducing API costs. This offers a robust and high-efficiency paradigm for language model adaptation.

