---
layout: default
title: OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving
---

# OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving

**arXiv**: [2512.14044v1](https://arxiv.org/abs/2512.14044) | [PDF](https://arxiv.org/pdf/2512.14044.pdf)

**ä½œè€…**: Zhenguo Zhang, Haohan Zhen, Yishen Wang, Le Xu, Tianchen Deng, Xuefeng Chen, Qu Chen, Bo Zhang, Wuxiong Huang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOmniDrive-R1æ¡†æž¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„äº¤é”™å¤šæ¨¡æ€æ€ç»´é“¾è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­è§†è§‰è¯­è¨€æ¨¡åž‹çš„å¯é æ€§é—®é¢˜**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **è‡ªåŠ¨é©¾é©¶** **è§†è§‰é‡Œç¨‹è®¡** **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `è§†è§‰è¯­è¨€æ¨¡åž‹` `å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€æ€ç»´é“¾` `è§†è§‰æŽ¥åœ°` `ç«¯åˆ°ç«¯ä¼˜åŒ–` `å¯é æ€§æå‡` `è·¨æ¨¡æ€ä¸€è‡´æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•å­˜åœ¨æ„ŸçŸ¥ä¸ŽæŽ¨ç†é˜¶æ®µè§£è€¦çš„é—®é¢˜ï¼Œæ— æ³•å®žçŽ°ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–ï¼Œä¸”ä¾èµ–æ˜‚è´µå¯†é›†çš„å®šä½æ ‡æ³¨ï¼Œé™åˆ¶äº†è‡ªåŠ¨é©¾é©¶ä¸­è§†è§‰è¯­è¨€æ¨¡åž‹çš„å¯é æ€§ã€‚
2. è®ºæ–‡æå‡ºäº¤é”™å¤šæ¨¡æ€æ€ç»´é“¾æœºåˆ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„è§†è§‰æŽ¥åœ°èƒ½åŠ›ï¼Œä½¿æ¨¡åž‹èƒ½è‡ªä¸»èšç„¦å…³é”®åŒºåŸŸï¼Œå®žçŽ°ç»†ç²’åº¦åˆ†æžå’Œå®žæ—¶è·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚
3. åœ¨DriveLMM-o1æ•°æ®é›†ä¸Šï¼ŒOmniDrive-R1ç›¸æ¯”åŸºçº¿æ¨¡åž‹ï¼Œæ•´ä½“æŽ¨ç†åˆ†æ•°æå‡è‡³80.35%ï¼Œæœ€ç»ˆç­”æ¡ˆå‡†ç¡®çŽ‡æå‡è‡³73.62%ï¼Œæ˜¾è‘—æ”¹å–„æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨è‡ªåŠ¨é©¾é©¶ç­‰å®‰å…¨å…³é”®é¢†åŸŸéƒ¨ç½²è§†è§‰è¯­è¨€æ¨¡åž‹æ—¶ï¼Œå¯é æ€§é—®é¢˜ï¼ˆç‰¹åˆ«æ˜¯ç‰©ä½“å¹»è§‰ï¼‰ä¸¥é‡é˜»ç¢äº†å…¶åº”ç”¨ã€‚è¿™ç§å¤±è´¥æºäºŽæ¨¡åž‹ä¾èµ–æœªæŽ¥åœ°çš„ã€åŸºäºŽæ–‡æœ¬çš„æ€ç»´é“¾æŽ¨ç†ã€‚çŽ°æœ‰çš„å¤šæ¨¡æ€æ€ç»´é“¾æ–¹æ³•è™½ç„¶å°è¯•ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†å­˜åœ¨ä¸¤ä¸ªæ ¹æœ¬ç¼ºé™·ï¼šï¼ˆ1ï¼‰è§£è€¦çš„æ„ŸçŸ¥å’ŒæŽ¨ç†é˜¶æ®µé˜»ç¢äº†ç«¯åˆ°ç«¯çš„è”åˆä¼˜åŒ–ï¼›ï¼ˆ2ï¼‰ä¾èµ–æ˜‚è´µä¸”å¯†é›†çš„å®šä½æ ‡æ³¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†OmniDrive-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºè‡ªåŠ¨é©¾é©¶è®¾è®¡çš„ç«¯åˆ°ç«¯è§†è§‰è¯­è¨€æ¨¡åž‹æ¡†æž¶ï¼Œé€šè¿‡äº¤é”™å¤šæ¨¡æ€æ€ç»´é“¾æœºåˆ¶ç»Ÿä¸€äº†æ„ŸçŸ¥å’ŒæŽ¨ç†ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒåˆ›æ–°æ˜¯å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„è§†è§‰æŽ¥åœ°èƒ½åŠ›ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿè‡ªä¸»å¼•å¯¼æ³¨æ„åŠ›å¹¶â€œæ”¾å¤§â€å…³é”®åŒºåŸŸè¿›è¡Œç»†ç²’åº¦åˆ†æžã€‚è¿™ä¸€èƒ½åŠ›é€šè¿‡æˆ‘ä»¬çš„çº¯ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ è®­ç»ƒæµç¨‹å’ŒClip-GRPOç®—æ³•å®žçŽ°ã€‚å…³é”®çš„æ˜¯ï¼ŒClip-GRPOå¼•å…¥äº†æ— éœ€æ ‡æ³¨çš„ã€åŸºäºŽè¿‡ç¨‹çš„æŽ¥åœ°å¥–åŠ±ã€‚è¯¥å¥–åŠ±ä¸ä»…æ¶ˆé™¤äº†å¯¹å¯†é›†æ ‡æ³¨çš„éœ€æ±‚ï¼Œè¿˜é€šè¿‡å¼ºåˆ¶è§†è§‰ç„¦ç‚¹å’Œæ–‡æœ¬æŽ¨ç†ä¹‹é—´çš„å®žæ—¶è·¨æ¨¡æ€ä¸€è‡´æ€§ï¼Œè§„é¿äº†å¤–éƒ¨å·¥å…·è°ƒç”¨çš„ä¸ç¨³å®šæ€§ã€‚åœ¨DriveLMM-o1æ•°æ®é›†ä¸Šçš„å¤§é‡å®žéªŒè¯æ˜Žäº†æˆ‘ä»¬æ¨¡åž‹çš„æ˜¾è‘—æ”¹è¿›ã€‚ä¸ŽåŸºçº¿Qwen2.5VL-7Bç›¸æ¯”ï¼ŒOmniDrive-R1å°†æ•´ä½“æŽ¨ç†åˆ†æ•°ä»Ž51.77%æå‡åˆ°80.35%ï¼Œæœ€ç»ˆç­”æ¡ˆå‡†ç¡®çŽ‡ä»Ž37.81%æå‡åˆ°73.62%ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

OmniDrive-R1æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„è§†è§‰è¯­è¨€æ¨¡åž‹æ¡†æž¶ï¼Œä¸“ä¸ºè‡ªåŠ¨é©¾é©¶è®¾è®¡ã€‚å…¶æ ¸å¿ƒæ˜¯äº¤é”™å¤šæ¨¡æ€æ€ç»´é“¾æœºåˆ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„è§†è§‰æŽ¥åœ°èƒ½åŠ›ç»Ÿä¸€æ„ŸçŸ¥å’ŒæŽ¨ç†ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°åŒ…æ‹¬çº¯ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ è®­ç»ƒæµç¨‹å’ŒClip-GRPOç®—æ³•ï¼ŒåŽè€…å¼•å…¥æ— éœ€æ ‡æ³¨çš„ã€åŸºäºŽè¿‡ç¨‹çš„æŽ¥åœ°å¥–åŠ±ï¼Œå¼ºåˆ¶è§†è§‰ç„¦ç‚¹ä¸Žæ–‡æœ¬æŽ¨ç†çš„å®žæ—¶ä¸€è‡´æ€§ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼šå®ƒé¿å…äº†æ„ŸçŸ¥å’ŒæŽ¨ç†é˜¶æ®µçš„è§£è€¦ï¼Œå®žçŽ°äº†ç«¯åˆ°ç«¯ä¼˜åŒ–ï¼›åŒæ—¶ï¼Œä¸ä¾èµ–å¯†é›†å®šä½æ ‡æ³¨ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è‡ªä¸»å¼•å¯¼æ³¨æ„åŠ›ï¼Œæé«˜äº†æ¨¡åž‹çš„å¯é æ€§å’Œæ•ˆçŽ‡ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨DriveLMM-o1æ•°æ®é›†ä¸Šï¼ŒOmniDrive-R1ç›¸æ¯”åŸºçº¿Qwen2.5VL-7Bï¼Œæ•´ä½“æŽ¨ç†åˆ†æ•°ä»Ž51.77%å¤§å¹…æå‡è‡³80.35%ï¼Œæœ€ç»ˆç­”æ¡ˆå‡†ç¡®çŽ‡ä»Ž37.81%æå‡è‡³73.62%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›å’Œå¯é æ€§å¢žå¼ºã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸»è¦åº”ç”¨äºŽè‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯è§†è§‰è¯­è¨€æ¨¡åž‹çš„éƒ¨ç½²ï¼Œä»¥æé«˜åœ¨å®‰å…¨å…³é”®åœºæ™¯ä¸­çš„å¯é æ€§å’Œå†³ç­–å‡†ç¡®æ€§ã€‚æ½œåœ¨åº”ç”¨åŒ…æ‹¬æ™ºèƒ½è½¦è¾†çš„çŽ¯å¢ƒæ„ŸçŸ¥ã€å®žæ—¶æŽ¨ç†å’Œè‡ªä¸»å¯¼èˆªï¼Œæœ‰åŠ©äºŽæŽ¨åŠ¨è‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„å®žé™…è½åœ°å’Œå®‰å…¨æ€§æå‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The deployment of Vision-Language Models (VLMs) in safety-critical domains like autonomous driving (AD) is critically hindered by reliability failures, most notably object hallucination. This failure stems from their reliance on ungrounded, text-based Chain-of-Thought (CoT) reasoning.While existing multi-modal CoT approaches attempt mitigation, they suffer from two fundamental flaws: (1) decoupled perception and reasoning stages that prevent end-to-end joint optimization, and (2) reliance on expensive, dense localization labels.Thus we introduce OmniDrive-R1, an end-to-end VLM framework designed for autonomous driving, which unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. Our core innovation is an Reinforcement-driven visual grounding capability, enabling the model to autonomously direct its attention and "zoom in" on critical regions for fine-grained analysis. This capability is enabled by our pure two-stage reinforcement learning training pipeline and Clip-GRPO algorithm. Crucially, Clip-GRPO introduces an annotation-free, process-based grounding reward. This reward not only eliminates the need for dense labels but also circumvents the instability of external tool calls by enforcing real-time cross-modal consistency between the visual focus and the textual reasoning. Extensive experiments on DriveLMM-o1 demonstrate our model's significant improvements. Compared to the baseline Qwen2.5VL-7B, OmniDrive-R1 improves the overall reasoning score from 51.77% to 80.35%, and the final answer accuracy from 37.81% to 73.62%.

