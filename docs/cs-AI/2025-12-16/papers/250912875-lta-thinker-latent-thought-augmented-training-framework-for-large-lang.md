---
layout: default
title: LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning
---

# LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.12875" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2509.12875</a>
  <a href="https://arxiv.org/pdf/2509.12875.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.12875" onclick="toggleFavorite(this, '2509.12875', 'LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiaqi Wang, Binquan Ji, Haibo Luo, Yiyang Qi, Ruiting Li, Huiyan Wang, Yuantao Han, Cangyi Yang, jiaxu Zhang, Feiliang Ren

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**LTA-thinkerï¼šæ½œå˜é‡æ€æƒ³å¢å¼ºè®­ç»ƒæ¡†æ¶ï¼Œæå‡å¤§è¯­è¨€æ¨¡å‹å¤æ‚æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¤æ‚æ¨ç†` `æ½œå˜é‡æ¨¡å‹` `å¯¹æ¯”å­¦ä¹ ` `çŸ¥è¯†è’¸é¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨åˆ©ç”¨æ½œå˜é‡ç©ºé—´è¿›è¡Œå¤æ‚æ¨ç†æ—¶ï¼Œé«˜è´¨é‡æ½œå˜é‡æ€æƒ³çš„æœ‰æ•ˆç”Ÿæˆå’Œåˆ©ç”¨ä»ç„¶æ˜¯ç“¶é¢ˆã€‚
2. LTA-Thinkeré€šè¿‡æ„å»ºåŸºäºå¯å­¦ä¹ å…ˆéªŒçš„æ½œå˜é‡ç”Ÿæˆæ¶æ„ï¼Œå¹¶å¼•å…¥åŸºäºåˆ†å¸ƒçš„å®šå‘ä¼˜åŒ–èŒƒå¼ï¼Œæå‡æ½œå˜é‡æ€æƒ³çš„è´¨é‡å’Œåˆ©ç”¨æ•ˆç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLTA-Thinkeråœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºæ›´é«˜çš„æ€§èƒ½ä¸Šé™å’Œæ›´å¥½çš„æ‰©å±•æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLTA-Thinkerçš„æ½œå˜é‡æ€æƒ³å¢å¼ºè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶å—åˆ°SoftCoT++ç†è®ºçš„å¯å‘ï¼Œå³ç”Ÿæˆæ½œå˜é‡æ€æƒ³åˆ†å¸ƒçš„æ›´å¤§æ–¹å·®æ›´æ¥è¿‘çœŸå®åˆ†å¸ƒã€‚LTA-Thinkerä»ä¸¤ä¸ªè§’åº¦å‡ºå‘ï¼Œæé«˜åˆ†å¸ƒæ–¹å·®å¹¶å¢å¼ºæ¨ç†æ€§èƒ½ã€‚é¦–å…ˆï¼Œæ„å»ºäº†ä¸€ä¸ªåŸºäºå¯å­¦ä¹ å…ˆéªŒçš„æ½œå˜é‡æ€æƒ³ç”Ÿæˆæ¶æ„ï¼Œæ—¨åœ¨å¢åŠ ç”Ÿæˆæ½œå˜é‡å‘é‡çš„æ–¹å·®åˆ†å¸ƒï¼Œä»è€Œç®€åŒ–æ•´ä½“ç»“æ„å¹¶æé«˜æ€§èƒ½ä¸Šé™ã€‚å…¶æ¬¡ï¼Œå¼•å…¥äº†ä¸€ç§åŸºäºåˆ†å¸ƒçš„å®šå‘ä¼˜åŒ–èŒƒå¼ï¼Œè”åˆçº¦æŸåˆ†å¸ƒå±€éƒ¨æ€§å’Œåˆ†å¸ƒå°ºåº¦ã€‚è¯¥æœºåˆ¶é€šè¿‡ç»“åˆæ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æŸå¤±ä¸ä¸¤ä¸ªæ–°é¢–çš„æŸå¤±å‡½æ•°ï¼Œå³è¯­ä¹‰å¯¹é½æŸå¤±ï¼ˆåˆ©ç”¨KLæ•£åº¦ç¡®ä¿æ½œå˜é‡æ€æƒ³ä¸é—®é¢˜è¯­ä¹‰é«˜åº¦ç›¸å…³ï¼‰å’Œæ¨ç†ç„¦ç‚¹æŸå¤±ï¼ˆåˆ©ç”¨å¯¹æ¯”å­¦ä¹ æœºåˆ¶å¼•å¯¼æ¨¡å‹å…³æ³¨æœ€å…³é”®çš„æ¨ç†æ­¥éª¤ï¼‰ï¼Œæ¥æé«˜ä¿¡æ¯æ•ˆç‡å’Œè®¡ç®—æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLTA-Thinkeråœ¨å„ç§åŸºçº¿æ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›ï¼ˆSOTAï¼‰çš„æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†æ›´é«˜çš„æ€§èƒ½ä¸Šé™å’Œæ›´å¥½çš„ç¼©æ”¾æ•ˆæœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ï¼Œè™½ç„¶å¯ä»¥é€šè¿‡æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰ç­‰æ–¹æ³•ç¼“è§£è¿‡åº¦æ€è€ƒé—®é¢˜ï¼Œä½†å¦‚ä½•é«˜æ•ˆåœ°ç”Ÿæˆå’Œåˆ©ç”¨é«˜è´¨é‡çš„æ½œå˜é‡æ€æƒ³ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–¹æ³•ï¼Œä¾‹å¦‚Coconutå’ŒSoftCoTï¼Œåœ¨è¿ç»­æ½œå˜é‡ç©ºé—´æ¨ç†æ–¹é¢å–å¾—äº†ä¸€å®šçš„è¿›å±•ï¼Œä½†ä»ç„¶å—é™äºæ½œå˜é‡è´¨é‡ä¸é«˜ä»¥åŠåˆ©ç”¨æ•ˆç‡ä¸è¶³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLTA-Thinkerçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¢å¤§ç”Ÿæˆçš„æ½œå˜é‡æ€æƒ³åˆ†å¸ƒçš„æ–¹å·®æ¥æ›´é€¼è¿‘çœŸå®çš„åˆ†å¸ƒã€‚å€Ÿé‰´SoftCoT++çš„ç†è®ºï¼Œè®¤ä¸ºæ›´å¤§çš„æ–¹å·®èƒ½å¤Ÿæ›´å¥½åœ°è¦†ç›–å¯èƒ½çš„æ¨ç†è·¯å¾„ã€‚å› æ­¤ï¼ŒLTA-Thinkeræ—¨åœ¨æé«˜æ½œå˜é‡çš„ç”Ÿæˆè´¨é‡å’Œåˆ©ç”¨æ•ˆç‡ï¼Œä»è€Œæå‡æ•´ä½“çš„æ¨ç†æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLTA-Thinkeræ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šæ½œå˜é‡æ€æƒ³ç”Ÿæˆæ¶æ„å’ŒåŸºäºåˆ†å¸ƒçš„å®šå‘ä¼˜åŒ–èŒƒå¼ã€‚æ½œå˜é‡æ€æƒ³ç”Ÿæˆæ¶æ„åŸºäºå¯å­¦ä¹ çš„å…ˆéªŒåˆ†å¸ƒï¼Œç”¨äºç”Ÿæˆå…·æœ‰æ›´å¤§æ–¹å·®çš„æ½œå˜é‡å‘é‡ã€‚åŸºäºåˆ†å¸ƒçš„å®šå‘ä¼˜åŒ–èŒƒå¼åˆ™é€šè¿‡è”åˆçº¦æŸåˆ†å¸ƒçš„å±€éƒ¨æ€§å’Œå°ºåº¦ï¼Œæé«˜ä¿¡æ¯æ•ˆç‡å’Œè®¡ç®—æ•ˆç‡ã€‚æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹é‡‡ç”¨å¤šç›®æ ‡ååŒè®­ç»ƒç­–ç•¥ï¼Œç»“åˆæ ‡å‡†çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æŸå¤±ä»¥åŠä¸¤ä¸ªæ–°é¢–çš„æŸå¤±å‡½æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šLTA-Thinkerçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†åŸºäºå¯å­¦ä¹ å…ˆéªŒçš„æ½œå˜é‡æ€æƒ³ç”Ÿæˆæ¶æ„ï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰æ›´å¤§æ–¹å·®çš„æ½œå˜é‡ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›ï¼›2) å¼•å…¥äº†åŸºäºåˆ†å¸ƒçš„å®šå‘ä¼˜åŒ–èŒƒå¼ï¼Œé€šè¿‡è¯­ä¹‰å¯¹é½æŸå¤±å’Œæ¨ç†ç„¦ç‚¹æŸå¤±ï¼Œå¼•å¯¼æ¨¡å‹å­¦ä¹ ä¸é—®é¢˜è¯­ä¹‰ç›¸å…³ä¸”èšç„¦å…³é”®æ¨ç†æ­¥éª¤çš„æ½œå˜é‡è¡¨ç¤ºã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒLTA-Thinkeræ›´åŠ æ³¨é‡æ½œå˜é‡åˆ†å¸ƒçš„è´¨é‡å’Œåˆ©ç”¨æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šLTA-Thinkerçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¯å­¦ä¹ å…ˆéªŒåˆ†å¸ƒçš„è®¾è®¡ï¼Œå…·ä½“å½¢å¼æœªçŸ¥ï¼›2) è¯­ä¹‰å¯¹é½æŸå¤±ï¼Œä½¿ç”¨KLæ•£åº¦æ¥è¡¡é‡ç”Ÿæˆæ½œå˜é‡ä¸é—®é¢˜è¯­ä¹‰ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œç¡®ä¿æ½œå˜é‡ä¸é—®é¢˜ç›¸å…³ï¼›3) æ¨ç†ç„¦ç‚¹æŸå¤±ï¼Œä½¿ç”¨å¯¹æ¯”å­¦ä¹ æœºåˆ¶ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨æœ€å…³é”®çš„æ¨ç†æ­¥éª¤ï¼Œæé«˜æ¨ç†çš„å‡†ç¡®æ€§ã€‚æŸå¤±å‡½æ•°çš„å…·ä½“æƒé‡æ¯”ä¾‹æœªçŸ¥ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://ar5iv.labs.arxiv.org/html/2509.12875/assets/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://ar5iv.labs.arxiv.org/html/2509.12875/assets/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://ar5iv.labs.arxiv.org/html/2509.12875/assets/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

LTA-Thinkeråœ¨å®éªŒä¸­å–å¾—äº†SOTAæ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿æœªçŸ¥ï¼Œä½†æ‘˜è¦å¼ºè°ƒäº†LTA-Thinkerå…·æœ‰æ›´é«˜çš„æ€§èƒ½ä¸Šé™å’Œæ›´å¥½çš„ç¼©æ”¾æ•ˆæœï¼Œæ„å‘³ç€è¯¥æ–¹æ³•åœ¨æ¨¡å‹è§„æ¨¡å¢å¤§æ—¶èƒ½å¤Ÿè·å¾—æ›´å¤§çš„æ€§èƒ½æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLTA-Thinkerèƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LTA-Thinkeræ¡†æ¶å¯ä»¥åº”ç”¨äºå„ç§éœ€è¦å¤æ‚æ¨ç†èƒ½åŠ›çš„åœºæ™¯ï¼Œä¾‹å¦‚é—®ç­”ç³»ç»Ÿã€çŸ¥è¯†å›¾è°±æ¨ç†ã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æé«˜è¿™äº›åº”ç”¨åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„æ€§èƒ½å’Œå¯é æ€§ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œå®é™…ä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–æ¨¡æ€ï¼Œä¾‹å¦‚å›¾åƒå’Œè¯­éŸ³ï¼Œä»¥æ”¯æŒæ›´å¤æ‚çš„è·¨æ¨¡æ€æ¨ç†ä»»åŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Complex Reasoning in Large Language Models can be dynamically optimized using Test-Time Scaling (TTS) to mitigate Overthinking. Methods such as Coconut, SoftCoT and its variant are effective in continuous latent space inference, the core bottleneck still lies in the efficient generation and utilization of high-quality Latent Thought. Drawing from the theory of SoftCoT++ that a larger variance in the generated Latent Thought distribution more closely approximates the golden truth distribution, we propose a Latent Thought-Augmented Training Framework--LTA-Thinker, which improves distributional variance and enhances reasoning performance from two perspectives. First, LTA-Thinker constructs a Latent Thought generation architecture based on a learnable prior. This architecture aims to increase the variance distribution of generated Latent Thought Vectors in order to simplify the overall structure and raise the performance ceiling. Second, LTA-Thinker introduces a distribution-based directional optimization paradigm that jointly constrains both distribution locality and distribution scale. This mechanism improves information efficiency and computational cost through a multi-objective co-training strategy, which combines standard Supervised Fine-Tuning (SFT) loss with two novel losses: Semantic Alignment Loss, which utilizes KL divergence to ensure that the Latent Thought is highly relevant to the semantics of the question; Reasoning Focus Loss, which utilizes a contrastive learning mechanism to guide the model to focus on the most critical reasoning steps. Experiments show that LTA-thinker achieves state-of-the-art (SOTA) performance among various baselines and demonstrates a higher performance ceiling and better scaling effects.

