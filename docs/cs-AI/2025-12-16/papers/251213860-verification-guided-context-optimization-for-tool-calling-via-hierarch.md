---
layout: default
title: Verification-Guided Context Optimization for Tool Calling via Hierarchical LLMs-as-Editors
---

# Verification-Guided Context Optimization for Tool Calling via Hierarchical LLMs-as-Editors

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.13860" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.13860</a>
  <a href="https://arxiv.org/pdf/2512.13860.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.13860" onclick="toggleFavorite(this, '2512.13860', 'Verification-Guided Context Optimization for Tool Calling via Hierarchical LLMs-as-Editors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Henger Li, Shuangjie You, Flavio Di Palo, Yiyue Qian, Ayush Jain

**åˆ†ç±»**: cs.SE, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVGCOæ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚LLMç¼–è¾‘å™¨ä¼˜åŒ–å·¥å…·è°ƒç”¨ä¸Šä¸‹æ–‡ï¼Œæå‡å·¥å…·ä½¿ç”¨æ•ˆæœã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å·¥å…·è°ƒç”¨` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¸Šä¸‹æ–‡ä¼˜åŒ–` `åˆ†å±‚ç¼–è¾‘` `çŸ¥è¯†åº“` `è‡ªåŠ¨åŒ–` `LLMç¼–è¾‘å™¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å·¥å…·è°ƒç”¨æ–¹æ³•ä¾èµ–äºä¸ºäººç±»ç¼–å†™çš„æ–‡æ¡£ï¼Œä¸LLMçš„ç†è§£å­˜åœ¨åå·®ï¼Œå¯¼è‡´å·¥å…·ä½¿ç”¨æ•ˆæœä¸ä½³ã€‚
2. VGCOæ¡†æ¶åˆ©ç”¨LLMä½œä¸ºç¼–è¾‘å™¨ï¼Œé€šè¿‡åˆ†å±‚ç»“æ„å’ŒéªŒè¯å¼•å¯¼ï¼Œè‡ªåŠ¨ä¼˜åŒ–å·¥å…·ç›¸å…³çš„æ–‡æ¡£å’ŒçŸ¥è¯†åº“ä¸Šä¸‹æ–‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒVGCOåœ¨å•è½®å¤§è§„æ¨¡å·¥å…·è°ƒç”¨é—®é¢˜ä¸Šï¼Œæ˜¾è‘—æå‡äº†LLMçš„å‡†ç¡®æ€§ã€é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVerification-Guided Context Optimization (VGCO) çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) ä½œä¸ºç¼–è¾‘å™¨ï¼Œè‡ªåŠ¨ä¼˜åŒ–ä¸å·¥å…·ç›¸å…³çš„æ–‡æ¡£å’ŒçŸ¥è¯†åº“ä¸Šä¸‹æ–‡ï¼Œä»è€Œæå‡å·¥å…·è°ƒç”¨çš„æœ‰æ•ˆæ€§ã€‚VGCO åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œè¯„ä¼°é˜¶æ®µæ”¶é›†çœŸå®ä¸–ç•Œçš„å¤±è´¥æ¡ˆä¾‹ï¼Œè¯†åˆ«å·¥å…·åŠå…¶ä¸Šä¸‹æ–‡ä¹‹é—´çš„ä¸åŒ¹é…ï¼›å…¶æ¬¡ï¼Œä¼˜åŒ–é˜¶æ®µé€šè¿‡ç¦»çº¿å­¦ä¹ ï¼Œåˆ©ç”¨ç»“æ„æ„ŸçŸ¥çš„ä¸Šä¸‹æ–‡ä¼˜åŒ–è¿›è¡Œåˆ†å±‚ç¼–è¾‘ã€‚LLM ç¼–è¾‘å™¨çš„åˆ›æ–°ä¹‹å¤„åœ¨äºï¼šé‡‡ç”¨ä¸å·¥å…·è°ƒç”¨å·¥ä½œæµç¨‹è‡ªç„¶é›†æˆçš„åˆ†å±‚ç»“æ„ï¼›å…·å¤‡çŠ¶æ€æ„ŸçŸ¥ã€åŠ¨ä½œç‰¹å®šå’ŒéªŒè¯å¼•å¯¼çš„ç‰¹æ€§ï¼Œä»è€Œçº¦æŸæœç´¢ç©ºé—´å¹¶å®ç°é«˜æ•ˆã€æœ‰é’ˆå¯¹æ€§çš„æ”¹è¿›ï¼›æ”¯æŒç»æµé«˜æ•ˆçš„å­ä»»åŠ¡ä¸“ä¸šåŒ–ï¼Œå¯ä»¥é€šè¿‡æç¤ºå·¥ç¨‹å¤§å‹ç¼–è¾‘å™¨æ¨¡å‹æˆ–é€šè¿‡åè®­ç»ƒè¾ƒå°çš„ç¼–è¾‘å™¨æ¨¡å‹æ¥å®ç°ã€‚ä¸å¼ºè°ƒå¤šè½®æ¨ç†çš„å…ˆå‰å·¥ä½œä¸åŒï¼ŒVGCO ä¸“æ³¨äºå•è½®ã€å¤§è§„æ¨¡çš„å·¥å…·è°ƒç”¨é—®é¢˜ï¼Œå¹¶åœ¨ LLM çš„å‡†ç¡®æ€§ã€é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å·¥å…·è°ƒç”¨æ–¹æ³•ä¾èµ–äºäººå·¥ç¼–å†™çš„å·¥å…·æ–‡æ¡£å’ŒçŸ¥è¯†åº“ï¼Œè¿™äº›ææ–™é€šå¸¸æ˜¯ä¸ºäººç±»è®¾è®¡çš„ï¼Œä¸LLMç†è§£ä¿¡æ¯çš„æ–¹å¼å­˜åœ¨åå·®ã€‚å°¤å…¶æ˜¯åœ¨å·¥ä¸šç¯å¢ƒä¸­ï¼Œå­˜åœ¨å¤§é‡åŠŸèƒ½é‡å çš„å·¥å…·ï¼Œå¯¼è‡´å¯æ‰©å±•æ€§ã€å˜å¼‚æ€§å’Œæ­§ä¹‰æ€§é—®é¢˜ï¼Œä¸¥é‡å½±å“å·¥å…·è°ƒç”¨çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVGCOçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†LLMä½œä¸ºç¼–è¾‘å™¨ï¼Œè‡ªåŠ¨åœ°å¯¹å·¥å…·ç›¸å…³çš„æ–‡æ¡£å’ŒçŸ¥è¯†åº“ä¸Šä¸‹æ–‡è¿›è¡Œä¼˜åŒ–ã€‚é€šè¿‡æ”¶é›†çœŸå®ä¸–ç•Œçš„å¤±è´¥æ¡ˆä¾‹ï¼Œè¯†åˆ«å·¥å…·åŠå…¶ä¸Šä¸‹æ–‡ä¹‹é—´çš„ä¸åŒ¹é…ï¼Œç„¶ååˆ©ç”¨LLMçš„ç¼–è¾‘èƒ½åŠ›ï¼Œå¯¹è¿™äº›ä¸Šä¸‹æ–‡è¿›è¡Œæ”¹è¿›ï¼Œä½¿å…¶æ›´é€‚åˆLLMçš„ç†è§£å’Œä½¿ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVGCOæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šè¯„ä¼°ï¼ˆEvaluationï¼‰å’Œä¼˜åŒ–ï¼ˆOptimizationï¼‰ã€‚è¯„ä¼°é˜¶æ®µè´Ÿè´£æ”¶é›†çœŸå®ä¸–ç•Œçš„å·¥å…·è°ƒç”¨å¤±è´¥æ¡ˆä¾‹ï¼Œå¹¶åˆ†æå¤±è´¥åŸå› ï¼Œæ‰¾å‡ºå·¥å…·æ–‡æ¡£å’ŒçŸ¥è¯†åº“ä¸­å­˜åœ¨çš„é—®é¢˜ã€‚ä¼˜åŒ–é˜¶æ®µåˆ™åˆ©ç”¨LLMä½œä¸ºç¼–è¾‘å™¨ï¼Œå¯¹è¯„ä¼°é˜¶æ®µå‘ç°çš„é—®é¢˜è¿›è¡Œä¿®å¤å’Œæ”¹è¿›ã€‚ä¼˜åŒ–è¿‡ç¨‹é‡‡ç”¨åˆ†å±‚ç¼–è¾‘çš„æ–¹å¼ï¼Œé¦–å…ˆå¯¹æ–‡æ¡£çš„æ•´ä½“ç»“æ„è¿›è¡Œè°ƒæ•´ï¼Œç„¶åé€æ­¥ç»†åŒ–åˆ°å…·ä½“çš„ç»†èŠ‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šVGCOçš„å…³é”®åˆ›æ–°åœ¨äºå…¶LLMç¼–è¾‘å™¨çš„è®¾è®¡ã€‚è¯¥ç¼–è¾‘å™¨é‡‡ç”¨åˆ†å±‚ç»“æ„ï¼Œèƒ½å¤Ÿè‡ªç„¶åœ°èå…¥åˆ°å·¥å…·è°ƒç”¨çš„å·¥ä½œæµç¨‹ä¸­ã€‚åŒæ—¶ï¼Œç¼–è¾‘å™¨å…·å¤‡çŠ¶æ€æ„ŸçŸ¥ã€åŠ¨ä½œç‰¹å®šå’ŒéªŒè¯å¼•å¯¼çš„ç‰¹æ€§ï¼Œèƒ½å¤Ÿæ ¹æ®å½“å‰çš„çŠ¶æ€å’Œéœ€è¦æ‰§è¡Œçš„åŠ¨ä½œï¼Œæœ‰é’ˆå¯¹æ€§åœ°å¯¹ä¸Šä¸‹æ–‡è¿›è¡Œä¼˜åŒ–ã€‚éªŒè¯å¼•å¯¼åˆ™é€šè¿‡å¯¹ç¼–è¾‘ç»“æœè¿›è¡ŒéªŒè¯ï¼Œç¡®ä¿ä¼˜åŒ–åçš„ä¸Šä¸‹æ–‡èƒ½å¤Ÿæé«˜å·¥å…·è°ƒç”¨çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šVGCOé‡‡ç”¨ç¦»çº¿å­¦ä¹ çš„æ–¹å¼è®­ç»ƒLLMç¼–è¾‘å™¨ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨å¤§é‡çš„å·¥å…·è°ƒç”¨å¤±è´¥æ¡ˆä¾‹ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œé€šè¿‡ä¼˜åŒ–æŸå¤±å‡½æ•°ï¼Œä½¿ç¼–è¾‘å™¨èƒ½å¤Ÿå­¦ä¹ åˆ°å¦‚ä½•æ ¹æ®å¤±è´¥æ¡ˆä¾‹å¯¹ä¸Šä¸‹æ–‡è¿›è¡Œæ”¹è¿›ã€‚æ­¤å¤–ï¼ŒVGCOè¿˜æ”¯æŒé€šè¿‡æç¤ºå·¥ç¨‹æˆ–åè®­ç»ƒçš„æ–¹å¼ï¼Œå¯¹ç¼–è¾‘å™¨è¿›è¡Œå­ä»»åŠ¡ä¸“ä¸šåŒ–ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†ç‰¹å®šç±»å‹çš„å·¥å…·è°ƒç”¨é—®é¢˜ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.13860/aaai2026/framework_diagram.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.13860/aaai2026/accuracy_line_graph_Claude_Sonnet_3.5.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.13860/aaai2026/accuracy_line_graph_Claude_Sonnet_3.7.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

VGCOåœ¨å•è½®å¤§è§„æ¨¡å·¥å…·è°ƒç”¨é—®é¢˜ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVGCOèƒ½å¤Ÿæ˜¾è‘—æé«˜LLMåœ¨å·¥å…·è°ƒç”¨ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§ã€é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“çš„æ•°æ®æå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­ç»™å‡ºï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æœ‰æ˜¾è‘—æé«˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VGCOæ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦å·¥å…·è°ƒç”¨çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–è¿ç»´ã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡è‡ªåŠ¨ä¼˜åŒ–å·¥å…·ç›¸å…³çš„æ–‡æ¡£å’ŒçŸ¥è¯†åº“ä¸Šä¸‹æ–‡ï¼Œå¯ä»¥æ˜¾è‘—æé«˜å·¥å…·è°ƒç”¨çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œé™ä½äººå·¥ç»´æŠ¤æˆæœ¬ï¼Œå¹¶æå‡ç”¨æˆ·ä½“éªŒã€‚è¯¥ç ”ç©¶å¯¹äºæ¨åŠ¨LLMåœ¨å®é™…åº”ç”¨ä¸­çš„è½åœ°å…·æœ‰é‡è¦æ„ä¹‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Tool calling enables large language models (LLMs) to interact with external environments through tool invocation, providing a practical way to overcome the limitations of pretraining. However, the effectiveness of tool use depends heavily on the quality of the associated documentation and knowledge base context. These materials are usually written for human users and are often misaligned with how LLMs interpret information. This problem is even more pronounced in industrial settings, where hundreds of tools with overlapping functionality create challenges in scalability, variability, and ambiguity. We propose Verification-Guided Context Optimization (VGCO), a framework that uses LLMs as editors to automatically refine tool-related documentation and knowledge base context. VGCO works in two stages. First, Evaluation collects real-world failure cases and identifies mismatches between tools and their context. Second, Optimization performs hierarchical editing through offline learning with structure-aware, in-context optimization. The novelty of our LLM editors has three main aspects. First, they use a hierarchical structure that naturally integrates into the tool-calling workflow. Second, they are state-aware, action-specific, and verification-guided, which constrains the search space and enables efficient, targeted improvements. Third, they enable cost-efficient sub-task specialization, either by prompt engineering large editor models or by post-training smaller editor models. Unlike prior work that emphasizes multi-turn reasoning, VGCO focuses on the single-turn, large-scale tool-calling problem and achieves significant improvements in accuracy, robustness, and generalization across LLMs.

