---
layout: default
title: Alignment Revisited: Are Large Language Models Consistent in Stated and Revealed Preferences?
---

# Alignment Revisited: Are Large Language Models Consistent in Stated and Revealed Preferences?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00751" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00751v1</a>
  <a href="https://arxiv.org/pdf/2506.00751.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00751v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00751v1', 'Alignment Revisited: Are Large Language Models Consistent in Stated and Revealed Preferences?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhuojun Gu, Quan Wang, Shuchu Han

**åˆ†ç±»**: cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåå¥½ä¸€è‡´æ€§æµ‹é‡æ–¹æ³•ä»¥è§£å†³LLMè¡Œä¸ºä¸äººç±»ä»·å€¼ä¸ä¸€è‡´é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `åå¥½ä¸€è‡´æ€§` `äººæœºäº¤äº’` `é“å¾·AI` `å†³ç­–é€æ˜æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰ç ”ç©¶æœªèƒ½å……åˆ†æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹çš„é™ˆè¿°åå¥½ä¸æ­ç¤ºåå¥½ä¹‹é—´çš„å·®å¼‚ï¼Œå½±å“å…¶å¯è§£é‡Šæ€§å’Œä¿¡ä»»åº¦ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡è®¾è®¡å¼ºåˆ¶äºŒå…ƒé€‰æ‹©çš„æç¤ºæ¥æµ‹é‡LLMåå¥½åå·®çš„æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨KLæ•£åº¦ç­‰æŒ‡æ ‡è¿›è¡Œé‡åŒ–ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šåœ¨å››ç§ä¸»æµLLMä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæç¤ºæ ¼å¼çš„å¾®å°å˜åŒ–ä¼šæ˜¾è‘—æ”¹å˜æ¨¡å‹çš„åå¥½é€‰æ‹©ï¼Œæ­ç¤ºäº†å†³ç­–è¿‡ç¨‹ä¸­çš„ä¸ä¸€è‡´æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•çªæ˜¾äº†å…¶è¡Œä¸ºä¸äººç±»ä»·å€¼è§‚å¯¹é½çš„å¿…è¦æ€§ã€‚æœ¬æ–‡æ¢è®¨äº†LLMçš„é™ˆè¿°åå¥½ï¼ˆä¸ä¸€èˆ¬åŸåˆ™çš„å¯¹é½ï¼‰ä¸æ­ç¤ºåå¥½ï¼ˆåœ¨ç‰¹å®šæƒ…å¢ƒä¸­çš„å†³ç­–æ¨æ–­ï¼‰ä¹‹é—´çš„æ½œåœ¨å·®å¼‚ã€‚æˆ‘ä»¬æ­£å¼å®šä¹‰å¹¶æå‡ºäº†ä¸€ç§æµ‹é‡è¿™ç§åå¥½åå·®çš„æ–¹æ³•ï¼Œåˆ†æäº†LLMåœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­æ¿€æ´»ä¸åŒæŒ‡å¯¼åŸåˆ™çš„æƒ…å†µã€‚é€šè¿‡è®¾è®¡ä¸€ç³»åˆ—å¼ºåˆ¶äºŒå…ƒé€‰æ‹©çš„æç¤ºï¼Œæ¯”è¾ƒLLMå¯¹ä¸€èˆ¬åŸåˆ™æç¤ºçš„å“åº”ä¸å¯¹æƒ…å¢ƒæç¤ºçš„å“åº”ï¼Œå‘ç°æç¤ºæ ¼å¼çš„å¾®å°å˜åŒ–ä¼šæ˜¾è‘—å½±å“LLMçš„åå¥½é€‰æ‹©ã€‚è¿™ä¸€ç°è±¡å¼ºè°ƒäº†å¯¹LLMå†³ç­–èƒ½åŠ›çš„ç†è§£å’Œæ§åˆ¶çš„ä¸è¶³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é™ˆè¿°åå¥½ä¸æ­ç¤ºåå¥½ä¹‹é—´å¯èƒ½å­˜åœ¨çš„åå·®é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆæµ‹é‡å’Œç†è§£è¿™ç§åå·®ï¼Œå¯¼è‡´LLMsåœ¨é«˜é£é™©åº”ç”¨ä¸­çš„å¯è§£é‡Šæ€§å’Œé“å¾·æ€§å—åˆ°è´¨ç–‘ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡è®¾è®¡ä¸€ç³»åˆ—å¼ºåˆ¶äºŒå…ƒé€‰æ‹©çš„æç¤ºï¼Œæ¥ç³»ç»Ÿæ€§åœ°æµ‹é‡LLMsçš„åå¥½åå·®ã€‚é€šè¿‡æ¯”è¾ƒæ¨¡å‹åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­çš„å“åº”ï¼Œæ­ç¤ºå…¶å†³ç­–è¿‡ç¨‹ä¸­çš„ä¸ä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬æ•°æ®é›†çš„æ„å»ºã€æç¤ºè®¾è®¡ã€æ¨¡å‹å“åº”æ”¶é›†å’Œåå·®é‡åŒ–ã€‚é¦–å…ˆï¼Œæ„å»ºä¸€ä¸ªä¸°å¯Œçš„æç¤ºæ•°æ®é›†ï¼Œç„¶åå°†å…¶åº”ç”¨äºå¤šä¸ªä¸»æµLLMsï¼Œæœ€åä½¿ç”¨KLæ•£åº¦ç­‰æŒ‡æ ‡é‡åŒ–åå·®ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°å®šä¹‰å’Œæµ‹é‡åå¥½åå·®ï¼Œå¹¶é€šè¿‡å¼ºåˆ¶äºŒå…ƒé€‰æ‹©çš„æ–¹å¼æ­ç¤ºLLMsåœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸‹çš„å†³ç­–å˜åŒ–ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºå…³æ³¨åå¥½çš„ä¸€è‡´æ€§è€Œéå•ä¸€çš„è¾“å‡ºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æç¤ºè®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šç§æ ¼å¼å’Œå†…å®¹çš„æç¤ºï¼Œä»¥ç¡®ä¿è¦†ç›–ä¸åŒçš„åå¥½ç±»åˆ«ã€‚å®éªŒä¸­ä½¿ç”¨çš„åº¦é‡æ ‡å‡†åŒ…æ‹¬KLæ•£åº¦ï¼Œä»¥é‡åŒ–ä¸åŒæç¤ºä¸‹çš„åå¥½å˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæç¤ºæ ¼å¼çš„å¾®å°å˜åŒ–èƒ½å¤Ÿæ˜¾è‘—å½±å“LLMsçš„åå¥½é€‰æ‹©ï¼ŒKLæ•£åº¦çš„é‡åŒ–ç»“æœè¡¨æ˜åœ¨ä¸åŒåå¥½ç±»åˆ«ä¸­å­˜åœ¨æ˜æ˜¾çš„åå·®ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†å¯¹LLMå†³ç­–è¿‡ç¨‹çš„æ·±å…¥ç†è§£å’Œæ§åˆ¶çš„å¿…è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬äººæœºäº¤äº’ã€è‡ªåŠ¨åŒ–å†³ç­–ç³»ç»Ÿå’Œé“å¾·AIç­‰ã€‚é€šè¿‡æé«˜å¯¹LLMsåå¥½ä¸€è‡´æ€§çš„ç†è§£ï¼Œå¯ä»¥å¢å¼ºå…¶åœ¨é«˜é£é™©åœºæ™¯ä¸­çš„å¯è§£é‡Šæ€§å’Œä¿¡ä»»åº¦ï¼Œä»è€Œä¿ƒè¿›å…¶åœ¨ç¤¾ä¼šè´£ä»»å’Œä¼¦ç†åº”ç”¨ä¸­çš„éƒ¨ç½²ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in Large Language Models (LLMs) highlight the need to align their behaviors with human values. A critical, yet understudied, issue is the potential divergence between an LLM's stated preferences (its reported alignment with general principles) and its revealed preferences (inferred from decisions in contextualized scenarios). Such deviations raise fundamental concerns for the interpretability, trustworthiness, reasoning transparency, and ethical deployment of LLMs, particularly in high-stakes applications. This work formally defines and proposes a method to measure this preference deviation. We investigate how LLMs may activate different guiding principles in specific contexts, leading to choices that diverge from previously stated general principles. Our approach involves crafting a rich dataset of well-designed prompts as a series of forced binary choices and presenting them to LLMs. We compare LLM responses to general principle prompts stated preference with LLM responses to contextualized prompts revealed preference, using metrics like KL divergence to quantify the deviation. We repeat the analysis across different categories of preferences and on four mainstream LLMs and find that a minor change in prompt format can often pivot the preferred choice regardless of the preference categories and LLMs in the test. This prevalent phenomenon highlights the lack of understanding and control of the LLM decision-making competence. Our study will be crucial for integrating LLMs into services, especially those that interact directly with humans, where morality, fairness, and social responsibilities are crucial dimensions. Furthermore, identifying or being aware of such deviation will be critically important as LLMs are increasingly envisioned for autonomous agentic tasks where continuous human evaluation of all LLMs' intermediary decision-making steps is impossible.

