---
layout: default
title: "CMT-LLM: Contextual Multi-Talker ASR Utilizing Large Language Models"
---

# CMT-LLM: Contextual Multi-Talker ASR Utilizing Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.12059" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.12059v1</a>
  <a href="https://arxiv.org/pdf/2506.12059.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.12059v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.12059v1', 'CMT-LLM: Contextual Multi-Talker ASR Utilizing Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiajun He, Naoki Sawada, Koichi Miyazaki, Tomoki Toda

**åˆ†ç±»**: eess.AS, cs.AI, cs.CL, cs.SD

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31

**å¤‡æ³¨**: Accepted by INTERSPEECH 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCMT-LLMæ¡†æ¶ä»¥è§£å†³å¤šè¯´è¯è€…ASRä¸ä¸Šä¸‹æ–‡åç½®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªåŠ¨è¯­éŸ³è¯†åˆ«` `å¤šè¯´è¯è€…è¯†åˆ«` `ä¸Šä¸‹æ–‡åç½®` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç¨€æœ‰è¯æ±‡è¯†åˆ«`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šè¯´è¯è€…ASRå’Œä¸Šä¸‹æ–‡åç½®æ–¹æ³•åˆ†åˆ«å¤„ç†ï¼Œå¯¼è‡´åœ¨å¤æ‚åœºæ™¯ä¸­çš„æ€§èƒ½ä¸è¶³ã€‚
2. æå‡ºçš„ç»Ÿä¸€æ¡†æ¶å°†å¤šè¯´è¯è€…é‡å è¯­éŸ³è¯†åˆ«ä¸ä¸Šä¸‹æ–‡åç½®ç»“åˆï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è¯­éŸ³ç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨LibriMixå’ŒAMI SDMæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå­—é”™è¯¯ç‡åˆ†åˆ«ä¸º7.9%å’Œ32.9%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿå¿…é¡»å¤„ç†å¤šè¯´è¯è€…çš„é‡å è¯­éŸ³å¹¶è¯†åˆ«ç¨€æœ‰è¯æ±‡ï¼Œå¦‚æŠ€æœ¯æœ¯è¯­ã€‚ä¼ ç»Ÿæ–¹æ³•å°†å¤šè¯´è¯è€…ASRå’Œä¸Šä¸‹æ–‡åç½®åˆ†åˆ«å¤„ç†ï¼Œé™åˆ¶äº†åœ¨å¤æ‚åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå°†å¤šè¯´è¯è€…é‡å è¯­éŸ³è¯†åˆ«å’Œä¸Šä¸‹æ–‡åç½®æ•´åˆä¸ºå•ä¸€ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ASRæ–¹æ³•ç»“åˆäº†é¢„è®­ç»ƒçš„è¯­éŸ³ç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œå¹¶é‡‡ç”¨ä¼˜åŒ–çš„å¾®è°ƒç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§ä¸¤é˜¶æ®µè¿‡æ»¤ç®—æ³•ï¼Œä»¥é«˜æ•ˆè¯†åˆ«æ¥è‡ªå¤§å‹åç½®åˆ—è¡¨çš„ç›¸å…³ç¨€æœ‰è¯æ±‡ï¼Œå¹¶å°†å…¶çº³å…¥LLMçš„æç¤ºè¾“å…¥ä¸­ï¼Œä»è€Œå¢å¼ºç¨€æœ‰è¯æ±‡çš„è¯†åˆ«èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨LibriMixä¸Šè¾¾åˆ°äº†7.9%çš„å­—é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œåœ¨AMI SDMä¸Šè¾¾åˆ°äº†32.9%ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚è¯­éŸ³åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šè¯´è¯è€…ASRç³»ç»Ÿåœ¨å¤„ç†é‡å è¯­éŸ³å’Œç¨€æœ‰è¯æ±‡æ—¶çš„æ€§èƒ½ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•å°†è¿™ä¸¤è€…åˆ†å¼€å¤„ç†ï¼Œå¯¼è‡´åœ¨å¤æ‚åœºæ™¯ä¸­çš„è¯†åˆ«æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„CMT-LLMæ¡†æ¶å°†å¤šè¯´è¯è€…é‡å è¯­éŸ³è¯†åˆ«ä¸ä¸Šä¸‹æ–‡åç½®æ•´åˆä¸ºä¸€ä¸ªç»Ÿä¸€ä»»åŠ¡ï¼Œé€šè¿‡ç»“åˆé¢„è®­ç»ƒçš„è¯­éŸ³ç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹æ¥æå‡è¯†åˆ«ç²¾åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯å¤šè¯´è¯è€…é‡å è¯­éŸ³è¯†åˆ«æ¨¡å—ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è¯­éŸ³ç¼–ç å™¨è¿›è¡Œç‰¹å¾æå–ï¼›å…¶æ¬¡æ˜¯ä¸Šä¸‹æ–‡åç½®æ¨¡å—ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ä¼˜åŒ–è¾“å…¥æç¤ºï¼Œå¢å¼ºç¨€æœ‰è¯æ±‡çš„è¯†åˆ«èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†å¤šè¯´è¯è€…ASRä¸ä¸Šä¸‹æ–‡åç½®æ•´åˆä¸ºä¸€ä¸ªä»»åŠ¡ï¼Œåˆ©ç”¨ä¸¤é˜¶æ®µè¿‡æ»¤ç®—æ³•é«˜æ•ˆè¯†åˆ«ç›¸å…³ç¨€æœ‰è¯æ±‡ï¼Œæ˜¾è‘—æå‡äº†è¯†åˆ«æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ä¼˜åŒ–çš„å¾®è°ƒç­–ç•¥ï¼Œç¡®ä¿é¢„è®­ç»ƒæ¨¡å‹èƒ½å¤Ÿé€‚åº”ç‰¹å®šä»»åŠ¡éœ€æ±‚ã€‚åŒæ—¶ï¼Œè®¾è®¡äº†ä¸¤é˜¶æ®µè¿‡æ»¤ç®—æ³•ï¼Œä»¥å‡å°‘åç½®åˆ—è¡¨çš„è§„æ¨¡ï¼Œæé«˜ç¨€æœ‰è¯æ±‡çš„è¯†åˆ«æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCMT-LLMæ¡†æ¶åœ¨LibriMixæ•°æ®é›†ä¸Šå®ç°äº†7.9%çš„å­—é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œåœ¨AMI SDMæ•°æ®é›†ä¸Šè¾¾åˆ°äº†32.9%ã€‚ä¸ä¼ ç»Ÿä¸Šä¸‹æ–‡åç½®æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†è¯†åˆ«æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚è¯­éŸ³åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¼šè®®è®°å½•ã€å®¢æœç³»ç»Ÿå’Œå¤šåª’ä½“å†…å®¹çš„è‡ªåŠ¨è½¬å½•ç­‰ã€‚é€šè¿‡æå‡å¤šè¯´è¯è€…ç¯å¢ƒä¸‹çš„è¯­éŸ³è¯†åˆ«èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜ä¿¡æ¯è·å–çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In real-world applications, automatic speech recognition (ASR) systems must handle overlapping speech from multiple speakers and recognize rare words like technical terms. Traditional methods address multi-talker ASR and contextual biasing separately, limiting performance in complex scenarios. We propose a unified framework that combines multi-talker overlapping speech recognition and contextual biasing into a single task. Our ASR method integrates pretrained speech encoders and large language models (LLMs), using optimized finetuning strategies. We also introduce a two-stage filtering algorithm to efficiently identify relevant rare words from large biasing lists and incorporate them into the LLM's prompt input, enhancing rare word recognition. Experiments show that our approach outperforms traditional contextual biasing methods, achieving a WER of 7.9% on LibriMix and 32.9% on AMI SDM when the biasing size is 1,000, demonstrating its effectiveness in complex speech scenarios.

