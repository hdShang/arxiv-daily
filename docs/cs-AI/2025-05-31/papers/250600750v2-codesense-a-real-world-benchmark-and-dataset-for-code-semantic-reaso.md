---
layout: default
title: CodeSense: a Real-World Benchmark and Dataset for Code Semantic Reasoning
---

# CodeSense: a Real-World Benchmark and Dataset for Code Semantic Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00750" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00750v2</a>
  <a href="https://arxiv.org/pdf/2506.00750.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00750v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00750v2', 'CodeSense: a Real-World Benchmark and Dataset for Code Semantic Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Monoshi Kumar Roy, Simin Chen, Benjamin Steenhoek, Jinjun Peng, Gail Kaiser, Baishakhi Ray, Wei Le

**åˆ†ç±»**: cs.SE, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31 (æ›´æ–°: 2025-10-02)

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://codesense-bench.github.io/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCodeSenseä»¥è§£å†³ä»£ç è¯­ä¹‰æ¨ç†åŸºå‡†ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä»£ç è¯­ä¹‰æ¨ç†` `åŸºå‡†æµ‹è¯•` `çœŸå®æ•°æ®é›†` `è½¯ä»¶å·¥ç¨‹` `å¤§è¯­è¨€æ¨¡å‹` `ç»†ç²’åº¦æ¨ç†` `æ‰§è¡Œè¿½è¸ª` `æ¨¡å‹è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä»£ç æ¨ç†åŸºå‡†å¤šä¾èµ–åˆæˆæ•°æ®é›†ï¼Œç¼ºä¹çœŸå®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹ä»»åŠ¡çš„æœ‰æ•ˆè¯„ä¼°ã€‚
2. æå‡ºCodeSenseåŸºå‡†ï¼Œæä¾›çœŸå®ä»£ç çš„ç»†ç²’åº¦æ¨ç†ä»»åŠ¡ï¼Œæ„å»ºçœŸå®æ•°æ®é›†ä»¥æ”¯æŒè¯„ä¼°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰LLMsåœ¨ç»†ç²’åº¦æ¨ç†ä»»åŠ¡ä¸Šå­˜åœ¨æ˜æ˜¾æ€§èƒ½å·®è·ï¼Œæç¤ºæŠ€æœ¯è™½æœ‰å¸®åŠ©ä½†ä»å—é™äºä»£ç è¯­ä¹‰ç†è§£ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç†è§£å’Œæ¨ç†ä»£ç è¯­ä¹‰å¯¹äºæå‡ä»£ç å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å®é™…è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­çš„èƒ½åŠ›è‡³å…³é‡è¦ã€‚ç°æœ‰çš„ä»£ç æ¨ç†åŸºå‡†å¤§å¤šä¾èµ–äºåˆæˆæ•°æ®é›†æˆ–æ•™è‚²ç¼–ç é—®é¢˜ï¼Œä¸”ä¸»è¦å…³æ³¨ç²—ç²’åº¦æ¨ç†ä»»åŠ¡ï¼Œå¦‚è¾“å…¥/è¾“å‡ºé¢„æµ‹ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…è½¯ä»¶å·¥ç¨‹èƒŒæ™¯ä¸‹è¯„ä¼°LLMsçš„æœ‰æ•ˆæ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†CodeSenseï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæä¾›ä¸€ç³»åˆ—ä¸çœŸå®ä»£ç çš„è½¯ä»¶å·¥ç¨‹ç›¸å…³çš„ç»†ç²’åº¦ä»£ç æ¨ç†ä»»åŠ¡çš„åŸºå‡†ã€‚æˆ‘ä»¬ä»çœŸå®çš„ä»£ç åº“ä¸­æ”¶é›†äº†Pythonã€Cå’ŒJavaè½¯ä»¶é¡¹ç›®ï¼Œæ‰§è¡Œæµ‹è¯•å¹¶æ”¶é›†æ‰§è¡Œè½¨è¿¹ï¼Œæ„å»ºäº†ç»†ç²’åº¦è¯­ä¹‰æ¨ç†ä»»åŠ¡çš„çœŸå®æ•°æ®é›†ã€‚ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰æ¨¡å‹åœ¨å¤„ç†ç»†ç²’åº¦æ¨ç†ä»»åŠ¡æ—¶å­˜åœ¨æ˜æ˜¾çš„æ€§èƒ½å·®è·ï¼Œå°½ç®¡é“¾å¼æ€ç»´å’Œä¸Šä¸‹æ–‡å­¦ä¹ ç­‰æç¤ºæŠ€æœ¯æœ‰æ‰€å¸®åŠ©ï¼Œä½†LLMsåœ¨ä»£ç è¯­ä¹‰æ–¹é¢çš„ç¼ºä¹æ ¹æœ¬é™åˆ¶äº†å…¶æ¨ç†èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ä»£ç æ¨ç†åŸºå‡†åœ¨çœŸå®è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯ç¼ºä¹ç»†ç²’åº¦æ¨ç†ä»»åŠ¡çš„è¯„ä¼°ã€‚ç°æœ‰æ–¹æ³•å¤šä¾èµ–åˆæˆæ•°æ®é›†ï¼Œæ— æ³•æœ‰æ•ˆåæ˜ å®é™…åº”ç”¨ä¸­çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºCodeSenseåŸºå‡†ï¼Œé€šè¿‡æ”¶é›†çœŸå®ä»£ç åº“ä¸­çš„é¡¹ç›®ï¼Œæ„å»ºç»†ç²’åº¦è¯­ä¹‰æ¨ç†ä»»åŠ¡çš„æ•°æ®é›†ï¼Œæ—¨åœ¨æå‡LLMsåœ¨å®é™…è½¯ä»¶å·¥ç¨‹ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ‰§è¡Œæµ‹è¯•ã€è½¨è¿¹æ”¶é›†å’Œæ•°æ®é›†æ„å»ºå››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆä»çœŸå®ä»£ç åº“ä¸­æå–é¡¹ç›®ï¼Œç„¶åæ‰§è¡Œç›¸å…³æµ‹è¯•ä»¥è·å–æ‰§è¡Œè½¨è¿¹ï¼Œæœ€åæ„å»ºå‡ºç”¨äºç»†ç²’åº¦æ¨ç†çš„çœŸå®æ•°æ®é›†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡æä¾›äº†é’ˆå¯¹çœŸå®ä¸–ç•Œä»£ç çš„ç»†ç²’åº¦æ¨ç†ä»»åŠ¡åŸºå‡†ï¼Œå¡«è¡¥äº†ç°æœ‰åŸºå‡†çš„ç©ºç™½ï¼Œæ¨åŠ¨äº†LLMsåœ¨å®é™…åº”ç”¨ä¸­çš„è¯„ä¼°å’Œæ”¹è¿›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†æ„å»ºè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†å¤šç§ç¼–ç¨‹è¯­è¨€ï¼ˆPythonã€Cã€Javaï¼‰ï¼Œå¹¶è®¾è®¡äº†æ‰§è¡Œè¿½è¸ªæ¡†æ¶å’Œå·¥å…·é›†ï¼Œä»¥ä¾¿äºæ”¶é›†çœŸå®çš„æ‰§è¡Œè½¨è¿¹å’Œæ„å»ºé«˜è´¨é‡çš„åŸºå‡†æ•°æ®é›†ã€‚å®éªŒä¸­ä½¿ç”¨äº†å¤šç§æç¤ºæŠ€æœ¯æ¥è¯„ä¼°å…¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰çš„LLMsåœ¨ç»†ç²’åº¦æ¨ç†ä»»åŠ¡ä¸Šå­˜åœ¨æ˜æ˜¾çš„æ€§èƒ½å·®è·ï¼Œå°½ç®¡é‡‡ç”¨äº†é“¾å¼æ€ç»´å’Œä¸Šä¸‹æ–‡å­¦ä¹ ç­‰æç¤ºæŠ€æœ¯ï¼Œæ¨¡å‹çš„è¡¨ç°ä»ç„¶å—åˆ°ä»£ç è¯­ä¹‰ç†è§£çš„é™åˆ¶ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªæä¾›ï¼Œä½†ç»“æœæ˜¾ç¤ºäº†æ¨¡å‹åœ¨å¤„ç†å¤æ‚ä»£ç æ¨ç†ä»»åŠ¡æ—¶çš„ä¸è¶³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è½¯ä»¶å¼€å‘ã€ä»£ç å®¡æŸ¥å’Œè‡ªåŠ¨åŒ–æµ‹è¯•ç­‰ã€‚é€šè¿‡æä¾›çœŸå®ä¸–ç•Œçš„ä»£ç æ¨ç†åŸºå‡†ï¼ŒCodeSenseå¯ä»¥å¸®åŠ©å¼€å‘è€…å’Œç ”ç©¶è€…æ›´å¥½åœ°è¯„ä¼°å’Œæ”¹è¿›ä»£ç å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œæ¨åŠ¨æ™ºèƒ½ç¼–ç¨‹åŠ©æ‰‹çš„å‘å±•ï¼Œæå‡è½¯ä»¶å·¥ç¨‹çš„æ•ˆç‡å’Œè´¨é‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Understanding and reasoning about code semantics is essential for enhancing code LLMs' abilities to solve real-world software engineering (SE) tasks. Although several code reasoning benchmarks exist, most rely on synthetic datasets or educational coding problems and focus on coarse-grained reasoning tasks such as input/output prediction, limiting their effectiveness in evaluating LLMs in practical SE contexts. To bridge this gap, we propose CodeSense, the first benchmark that makes available a spectrum of fine-grained code reasoning tasks concerned with the software engineering of real-world code. We collected Python, C and Java software projects from real-world repositories. We executed tests from these repositories, collected their execution traces, and constructed a ground truth dataset for fine-grained semantic reasoning tasks. We then performed comprehensive evaluations on state-of-the-art LLMs. Our results show a clear performance gap for the models to handle fine-grained reasoning tasks. Although prompting techniques such as chain-of-thought and in-context learning helped, the lack of code semantics in LLMs fundamentally limit models' capabilities of code reasoning. Besides dataset, benchmark and evaluation, our work produced an execution tracing framework and tool set that make it easy to collect ground truth for fine-grained SE reasoning tasks, offering a strong basis for future benchmark construction and model post training. Our code and data are located at https://codesense-bench.github.io/.

