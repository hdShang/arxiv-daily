---
layout: default
title: Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents
---

# Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00320" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00320v3</a>
  <a href="https://arxiv.org/pdf/2506.00320.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00320v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00320v3', 'Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiao Yu, Baolin Peng, Ruize Xu, Michel Galley, Hao Cheng, Suman Nath, Jianfeng Gao, Zhou Yu

**åˆ†ç±»**: cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31 (æ›´æ–°: 2025-10-10)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDyna-Thinkæ¡†æ¶ä»¥æå‡AIä»£ç†çš„æ¨ç†ä¸è¡ŒåŠ¨èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨ç†èƒ½åŠ›` `ä¸–ç•Œæ¨¡å‹` `åŠ¨æ€è®­ç»ƒ` `æ¨¡ä»¿å­¦ä¹ ` `AIä»£ç†` `é•¿æ—¶é—´ä»»åŠ¡` `å†³ç­–èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­ç¼ºä¹æœ‰æ•ˆçš„è¡Œä¸ºæŒ‡å¯¼ï¼Œå¯¼è‡´AIä»£ç†æ€§èƒ½ä¸è¶³ã€‚
2. Dyna-Thinkæ¡†æ¶æ•´åˆäº†æ¨ç†ã€è¡ŒåŠ¨ä¸ä¸–ç•Œæ¨¡å‹ä»¿çœŸï¼Œæå‡äº†AIä»£ç†çš„å†³ç­–èƒ½åŠ›ã€‚
3. å®éªŒæ˜¾ç¤ºï¼ŒDyna-Thinkåœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼Œç”Ÿæˆçš„tokenæ•°é‡å‡å°‘äº†50%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­ï¼ŒAIä»£ç†çš„æœ‰æ•ˆè¡Œä¸ºä»ä¸æ˜ç¡®ã€‚æœ¬æ–‡æå‡ºDyna-Thinkæ¡†æ¶ï¼Œå°†è§„åˆ’ã€å†…éƒ¨ä¸–ç•Œæ¨¡å‹ã€æ¨ç†ä¸è¡ŒåŠ¨ç›¸ç»“åˆï¼Œä»¥å¢å¼ºAIä»£ç†çš„æ€§èƒ½ã€‚ä¸ºå®ç°Dyna-Thinkï¼Œæå‡ºäº†Dyna-Thinkæ¨¡ä»¿å­¦ä¹ ï¼ˆDITï¼‰å’ŒDyna-ThinkåŠ¨æ€è®­ç»ƒï¼ˆDDTï¼‰ã€‚DITé€šè¿‡é‡å»ºæ€ç»´è¿‡ç¨‹ï¼Œä¸“æ³¨äºä¸è®¡åˆ’è¡ŒåŠ¨ç›¸å…³çš„ä¸–ç•Œæ¨¡å‹ä»¿çœŸï¼Œå¹¶åˆ©ç”¨é‡å»ºæ•°æ®è®­ç»ƒç­–ç•¥ã€‚DDTåˆ™é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œé¦–å…ˆæå‡ä»£ç†çš„ä¸–ç•Œå»ºæ¨¡èƒ½åŠ›ï¼Œç„¶åé€šè¿‡ç­–ç•¥è®­ç»ƒæ”¹å–„è¡ŒåŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDyna-Thinkåœ¨OSWorldå’ŒWindowsAgentArenaä¸Šæ˜¾è‘—æå‡äº†ä»£ç†çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³AIä»£ç†åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­æ¨ç†ä¸è¡ŒåŠ¨èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆæ•´åˆæ¨ç†ä¸ä¸–ç•Œæ¨¡å‹ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDyna-Thinkæ¡†æ¶é€šè¿‡ç»“åˆå†…éƒ¨ä¸–ç•Œæ¨¡å‹ä¸æ¨ç†ã€è¡ŒåŠ¨ï¼Œæå‡AIä»£ç†çš„ç»¼åˆè¡¨ç°ã€‚è®¾è®¡ä¸Šå¼ºè°ƒä¸–ç•Œæ¨¡å‹ä»¿çœŸä¸è®¡åˆ’è¡ŒåŠ¨çš„ç´§å¯†ç»“åˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDyna-ThinkåŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šDyna-Thinkæ¨¡ä»¿å­¦ä¹ ï¼ˆDITï¼‰å’ŒDyna-ThinkåŠ¨æ€è®­ç»ƒï¼ˆDDTï¼‰ã€‚DITé‡å»ºæ€ç»´è¿‡ç¨‹ä»¥è®­ç»ƒç­–ç•¥ï¼ŒDDTåˆ™é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæå‡ä¸–ç•Œå»ºæ¨¡ä¸è¡ŒåŠ¨èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šDyna-Thinkçš„åˆ›æ–°åœ¨äºå°†ä¸–ç•Œæ¨¡å‹ä»¿çœŸä¸æ¨ç†ã€è¡ŒåŠ¨ç›¸ç»“åˆï¼Œå½¢æˆäº†ä¸€ä¸ªæ–°çš„æ€ç»´æ¡†æ¶ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†AIä»£ç†çš„å†³ç­–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨DITä¸­ï¼Œé‡å»ºçš„æ•°æ®ç”¨äºè®­ç»ƒç­–ç•¥ï¼ŒDDTåˆ™é€šè¿‡çŠ¶æ€é¢„æµ‹å’Œæ‰¹è¯„ç”Ÿæˆç­‰ç›®æ ‡æå‡ä¸–ç•Œå»ºæ¨¡èƒ½åŠ›ï¼Œæœ€ç»ˆé€šè¿‡ç­–ç•¥è®­ç»ƒæ”¹å–„è¡ŒåŠ¨æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDyna-Thinkåœ¨OSWorldå’ŒWindowsAgentArenaä¸Šå®ç°äº†ä¸R1ç›¸ä¼¼çš„æœ€ä½³æ€§èƒ½ï¼ŒåŒæ—¶ç”Ÿæˆçš„tokenæ•°é‡å¹³å‡å‡å°‘äº†50%ã€‚æ­¤å¤–ï¼Œä½¿ç”¨æ‰¹è¯„ç”Ÿæˆè¿›è¡Œä¸–ç•Œæ¨¡å‹è®­ç»ƒæ˜¾è‘—æå‡äº†ç­–ç•¥æ€§èƒ½ï¼Œè¡¨æ˜æ›´å¥½çš„ä¸–ç•Œå»ºæ¨¡èƒ½åŠ›ä¸ä»£ç†æ€§èƒ½ä¹‹é—´å­˜åœ¨æ­£ç›¸å…³å…³ç³»ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Dyna-Thinkæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦å¤æ‚å†³ç­–å’Œé•¿æ—¶é—´è§„åˆ’çš„é¢†åŸŸï¼Œå¦‚è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæ§åˆ¶å’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚å…¶é›†æˆçš„æ¨ç†ä¸è¡ŒåŠ¨èƒ½åŠ›å°†æ¨åŠ¨AIä»£ç†åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ï¼Œæå‡äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent progress in reasoning with large language models (LLMs), such as DeepSeek-R1, demonstrates impressive capabilities in domains like mathematics and coding, by exhibiting complex cognitive behaviors such as verification, goal decomposition, and self-reflection. However, it is unclear what behavior is effective and what behavior is missing for long-horizon AI agents tasks. In this work, we propose Dyna-Think, a thinking framework that integrates planning with an internal world model with reasoning and acting to enhance AI agent performance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning (DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with Dyna-Think, DIT reconstructs the thinking process of R1 to focus on performing world model simulation relevant to the proposed (and planned) action, and trains the policy using this reconstructed data. To enhance Dyna-Think, DDT uses a two-stage training process to first improve the agent's world modeling ability via objectives such as state prediction or critique generation, and then improve the agent's action via policy training. We evaluate our methods on OSWorld and WindowsAgentArena, and demonstrate that Dyna-Think improves the agent's in-domain and out-of-domain performance, achieving similar best-of-n performance compared to R1 while generating 2x less tokens on average. Our extensive empirical studies reveal that 1) using critique generation for world model training is effective to improve policy performance; and 2) AI agents with better performance correlate with better world modeling abilities. We believe our results suggest a promising research direction to integrate world model simulation into AI agents to enhance their reasoning, planning, and acting capabilities.

