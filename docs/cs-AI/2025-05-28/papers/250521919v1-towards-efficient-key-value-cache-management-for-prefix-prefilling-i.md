---
layout: default
title: Towards Efficient Key-Value Cache Management for Prefix Prefilling in LLM Inference
---

# Towards Efficient Key-Value Cache Management for Prefix Prefilling in LLM Inference

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21919" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21919v1</a>
  <a href="https://arxiv.org/pdf/2505.21919.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21919v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21919v1', 'Towards Efficient Key-Value Cache Management for Prefix Prefilling in LLM Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yue Zhu, Hao Yu, Chen Wang, Zhuoran Liu, Eun Kyung Lee

**åˆ†ç±»**: cs.ET, cs.AI, cs.DC

**å‘å¸ƒæ—¥æœŸ**: 2025-05-28

**å¤‡æ³¨**: This paper has been accepted at IEEE Cloud 2025 as WIP paper. The final version will appear in IEEE Xplore

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé«˜æ•ˆçš„é”®å€¼ç¼“å­˜ç®¡ç†æ–¹æ¡ˆä»¥ä¼˜åŒ–LLMæ¨ç†æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é”®å€¼ç¼“å­˜` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†ä¼˜åŒ–` `å…ƒæ•°æ®ç®¡ç†` `åˆ†å¸ƒå¼ç³»ç»Ÿ` `æ€§èƒ½è¯„ä¼°` `RDMAæŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é”®å€¼ç¼“å­˜ç®¡ç†æ–¹æ³•æœªèƒ½æœ‰æ•ˆåº”å¯¹å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†ä¸­çš„é«˜ç¼“å­˜é‡ç”¨æ€§ï¼Œå¯¼è‡´æ€§èƒ½ç“¶é¢ˆã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹KVCé¢„å¡«å……çš„é«˜æ•ˆåˆ†å¸ƒå¼ç¼“å­˜ç®¡ç†æ–¹æ¡ˆï¼Œä¼˜åŒ–äº†å…ƒæ•°æ®ç®¡ç†ä»¥æå‡æ¨ç†é€Ÿåº¦ã€‚
3. é€šè¿‡å¯¹æ¯”å®éªŒï¼Œè®ºæ–‡å±•ç¤ºäº†æ–°æ–¹æ³•åœ¨æ¨ç†æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„é”®å€¼å­˜å‚¨æ–¹æ¡ˆï¼Œæå‡å¹…åº¦å¯è§‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ‰©å±•ä¸Šä¸‹æ–‡çª—å£ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œä¼˜åŒ–é”®å€¼ç¼“å­˜ï¼ˆKVCï¼‰ç®¡ç†ä»¥æå‡æ¨ç†æ€§èƒ½å˜å¾—è‡³å…³é‡è¦ã€‚è¯¸å¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œæ™ºèƒ½ä½“ç­‰æ¨ç†å·¥ä½œè´Ÿè½½å±•ç°å‡ºé«˜ç¼“å­˜é‡ç”¨æ€§ï¼Œå› æ­¤é«˜æ•ˆçš„ç¼“å­˜ç®¡ç†å¯¹äºå‡å°‘å†—ä½™å’Œæé«˜é€Ÿåº¦è‡³å…³é‡è¦ã€‚æœ¬æ–‡åˆ†æäº†çœŸå®ä¸–ç•Œçš„KVCè®¿é—®æ¨¡å¼ï¼Œå¹¶è¯„ä¼°äº†å•†ä¸šé”®å€¼å­˜å‚¨ï¼ˆå¦‚Redisï¼‰å’Œæœ€å…ˆè¿›çš„åŸºäºRDMAçš„ç³»ç»Ÿï¼ˆå¦‚CHIMEå’ŒShermanï¼‰åœ¨KVCå…ƒæ•°æ®ç®¡ç†ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰ç¼ºä¹é’ˆå¯¹KVCé¢„å¡«å……çš„å®šåˆ¶å­˜å‚¨è§£å†³æ–¹æ¡ˆï¼Œå¼ºè°ƒäº†ä¸ºLLMå·¥ä½œè´Ÿè½½è®¾è®¡é«˜æ•ˆåˆ†å¸ƒå¼ç¼“å­˜ç³»ç»Ÿå’Œä¼˜åŒ–å…ƒæ•°æ®ç®¡ç†çš„å¿…è¦æ€§ï¼Œå¹¶ä¸ºæ”¹è¿›KVCç®¡ç†ç³»ç»Ÿä»¥å®ç°å¯æ‰©å±•ã€ä½å»¶è¿Ÿçš„æ¨ç†æä¾›äº†è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†ä¸­çš„é”®å€¼ç¼“å­˜ç®¡ç†æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é«˜ç¼“å­˜é‡ç”¨æ€§æ—¶ï¼Œæœªèƒ½æä¾›é’ˆå¯¹æ€§çš„è§£å†³æ–¹æ¡ˆï¼Œå¯¼è‡´æ¨ç†æ€§èƒ½å—åˆ°å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„åˆ†å¸ƒå¼ç¼“å­˜ç®¡ç†ç³»ç»Ÿï¼Œä¸“æ³¨äºä¼˜åŒ–KVCçš„å…ƒæ•°æ®ç®¡ç†ï¼Œä»¥é€‚åº”LLMæ¨ç†çš„ç‰¹å®šéœ€æ±‚ã€‚é€šè¿‡åˆ†æçœŸå®çš„KVCè®¿é—®æ¨¡å¼ï¼Œè®¾è®¡å‡ºæ›´é«˜æ•ˆçš„ç¼“å­˜ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é‡‡é›†æ¨¡å—ã€ç¼“å­˜ç®¡ç†æ¨¡å—å’Œæ€§èƒ½è¯„ä¼°æ¨¡å—ã€‚æ•°æ®é‡‡é›†æ¨¡å—è´Ÿè´£è·å–çœŸå®çš„KVCè®¿é—®æ•°æ®ï¼Œç¼“å­˜ç®¡ç†æ¨¡å—å®ç°é«˜æ•ˆçš„ç¼“å­˜ç­–ç•¥ï¼Œæ€§èƒ½è¯„ä¼°æ¨¡å—åˆ™ç”¨äºéªŒè¯æ–°æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé’ˆå¯¹KVCé¢„å¡«å……è®¾è®¡çš„é«˜æ•ˆåˆ†å¸ƒå¼ç¼“å­˜ç³»ç»Ÿï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å…ƒæ•°æ®ç®¡ç†ä¸Šçš„ä¸è¶³ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†é€Ÿåº¦å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œè®ºæ–‡é€šè¿‡å®éªŒç¡®å®šäº†æœ€ä½³çš„ç¼“å­˜å¤§å°å’Œæ›´æ–°ç­–ç•¥ï¼ŒæŸå¤±å‡½æ•°é‡‡ç”¨äº†é’ˆå¯¹æ¨ç†å»¶è¿Ÿçš„ä¼˜åŒ–ç›®æ ‡ï¼Œç½‘ç»œç»“æ„åˆ™ç»“åˆäº†ç°æœ‰çš„RDMAæŠ€æœ¯ä»¥æé«˜æ•°æ®ä¼ è¾“æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„KVCç®¡ç†æ–¹æ¡ˆåœ¨æ¨ç†é€Ÿåº¦ä¸Šè¾ƒä¼ ç»ŸRediså’ŒRDMAç³»ç»Ÿæœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°30%ä»¥ä¸Šï¼ŒéªŒè¯äº†æ–°æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†ä¼˜åŒ–ã€æ™ºèƒ½ä½“ç³»ç»Ÿçš„å®æ—¶å“åº”ä»¥åŠæ£€ç´¢å¢å¼ºç”Ÿæˆä»»åŠ¡ã€‚é€šè¿‡æé«˜KVCç®¡ç†æ•ˆç‡ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ç›¸å…³åº”ç”¨çš„æ€§èƒ½ï¼Œé™ä½å»¶è¿Ÿï¼Œè¿›è€Œæ¨åŠ¨æ›´å¤æ‚çš„AIç³»ç»Ÿçš„å®é™…åº”ç”¨ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æ›´å¤šéœ€è¦é«˜æ•ˆæ•°æ®è®¿é—®çš„åœºæ™¯ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The increasing adoption of large language models (LLMs) with extended context windows necessitates efficient Key-Value Cache (KVC) management to optimize inference performance. Inference workloads like Retrieval-Augmented Generation (RAG) and agents exhibit high cache reusability, making efficient caching critical to reducing redundancy and improving speed. We analyze real-world KVC access patterns using publicly available traces and evaluate commercial key-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1] and Sherman [2]) for KVC metadata management. Our work demonstrates the lack of tailored storage solution for KVC prefilling, underscores the need for an efficient distributed caching system with optimized metadata management for LLM workloads, and provides insights into designing improved KVC management systems for scalable, low-latency inference.

