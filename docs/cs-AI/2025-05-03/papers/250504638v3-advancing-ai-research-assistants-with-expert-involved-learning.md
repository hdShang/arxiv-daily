---
layout: default
title: Advancing AI Research Assistants with Expert-Involved Learning
---

# Advancing AI Research Assistants with Expert-Involved Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.04638" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.04638v3</a>
  <a href="https://arxiv.org/pdf/2505.04638.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.04638v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.04638v3', 'Advancing AI Research Assistants with Expert-Involved Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianyu Liu, Simeng Han, Hanchen Wang, Xiao Luo, Pan Lu, Biqing Zhu, Yuge Wang, Keyi Li, Jiapeng Chen, Rihao Qu, Yufeng Liu, Xinyue Cui, Aviv Yaish, Yuhang Chen, Minsheng Hao, Chuhan Li, Kexing Li, Arman Cohan, Hua Xu, Mark Gerstein, James Zou, Hongyu Zhao

**åˆ†ç±»**: cs.AI, cs.CL, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2025-05-03 (æ›´æ–°: 2025-12-10)

**å¤‡æ³¨**: 36 pages, 7 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºARIELæ¡†æ¶ä»¥æå‡ç”Ÿç‰©åŒ»å­¦é¢†åŸŸAIç ”ç©¶åŠ©æ‰‹çš„å¯é æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç”Ÿç‰©åŒ»å­¦` `å¤šæ¨¡æ€æ¨¡å‹` `AIç ”ç©¶åŠ©æ‰‹` `ä¸“å®¶å‚ä¸å­¦ä¹ ` `æ–‡æœ¬æ‘˜è¦` `è§†è§‰æ¨ç†` `æ¨¡å‹è¯„ä¼°` `å¼€æºæ¡†æ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€æ¨¡å‹åœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„åº”ç”¨å­˜åœ¨å¯é æ€§ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶åœ¨ç”Ÿæˆæ‘˜è¦å’Œè§†è§‰æ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ã€‚
2. è®ºæ–‡æå‡ºARIELæ¡†æ¶ï¼Œç»“åˆä¸“å®¶å®¡æ ¸çš„ä»»åŠ¡ä¸å¤šæ¨¡æ€ç”Ÿç‰©åŒ»å­¦è¯­æ–™åº“ï¼Œæ—¨åœ¨æå‡AIç ”ç©¶åŠ©æ‰‹çš„æ€§èƒ½å’Œå¯é æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡æç¤ºå·¥ç¨‹å’Œè½»é‡å¾®è°ƒï¼Œæ–‡æœ¬è¦†ç›–ç‡æ˜¾è‘—æé«˜ï¼Œè§†è§‰é—®ç­”èƒ½åŠ›ä¹Ÿå¾—åˆ°äº†å¢å¼ºï¼ŒARIELèƒ½å¤Ÿæå‡ºå¯æµ‹è¯•çš„å‡è®¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨åŠ é€Ÿç”Ÿç‰©åŒ»å­¦å‘ç°æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œä½†å…¶å¯é æ€§å°šä¸æ˜ç¡®ã€‚æˆ‘ä»¬æå‡ºäº†ARIELï¼ˆä¸“å®¶å‚ä¸å­¦ä¹ çš„AIç ”ç©¶åŠ©æ‰‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºè¯„ä¼°å’Œä¼˜åŒ–æ¡†æ¶ï¼Œç»“åˆäº†ç»è¿‡ä¸“å®¶å®¡æ ¸çš„å¤šæ¨¡æ€ç”Ÿç‰©åŒ»å­¦è¯­æ–™åº“å’Œä»»åŠ¡ï¼Œæ¢è®¨äº†å®Œæ•´æ–‡ç« æ‘˜è¦å’Œç»†è‡´å›¾å½¢è§£è¯»ä¸¤é¡¹èƒ½åŠ›ã€‚é€šè¿‡ç»Ÿä¸€çš„åè®®å’Œç›²è¯„ï¼Œæˆ‘ä»¬å‘ç°ç°æœ‰æ¨¡å‹ç”Ÿæˆçš„æ‘˜è¦æµç•…ä½†ä¸å®Œæ•´ï¼Œè€ŒLMMsåœ¨è¯¦ç»†è§†è§‰æ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œæç¤ºå·¥ç¨‹å’Œè½»é‡çº§å¾®è°ƒæ˜¾è‘—æé«˜äº†æ–‡æœ¬è¦†ç›–ç‡ï¼Œè€Œè®¡ç®—è§„æ¨¡æ¨ç†ç­–ç•¥å¢å¼ºäº†è§†è§‰é—®ç­”èƒ½åŠ›ã€‚ARIELä»£ç†æ•´åˆäº†æ–‡æœ¬å’Œè§†è§‰çº¿ç´¢ï¼Œèƒ½å¤Ÿæå‡ºå¯æµ‹è¯•çš„æœºåˆ¶å‡è®¾ï¼Œæ˜ç¡®äº†åŸºç¡€æ¨¡å‹çš„å½“å‰ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œå¹¶ä¸ºæ¨åŠ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸå¯ä¿¡AIæä¾›äº†å¯é‡å¤çš„å¹³å°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€æ¨¡å‹åœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸåº”ç”¨ä¸­çš„å¯é æ€§é—®é¢˜ï¼Œå°¤å…¶æ˜¯ç”Ÿæˆçš„æ‘˜è¦ä¸å®Œæ•´å’Œè§†è§‰æ¨ç†èƒ½åŠ›ä¸è¶³çš„ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šARIELæ¡†æ¶é€šè¿‡æ•´åˆä¸“å®¶å®¡æ ¸çš„ä»»åŠ¡ä¸å¤šæ¨¡æ€è¯­æ–™åº“ï¼Œé‡‡ç”¨ä¸“å®¶å‚ä¸çš„å­¦ä¹ æ–¹å¼ï¼Œæå‡æ¨¡å‹åœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šARIELçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€ä»»åŠ¡è®¾è®¡ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œæ”¶é›†ç»è¿‡ä¸“å®¶å®¡æ ¸çš„å¤šæ¨¡æ€ç”Ÿç‰©åŒ»å­¦æ•°æ®ï¼›å…¶æ¬¡ï¼Œè®¾è®¡é’ˆå¯¹æ€§çš„ä»»åŠ¡ä»¥è¯„ä¼°æ¨¡å‹èƒ½åŠ›ï¼›ç„¶åï¼Œè¿›è¡Œæ¨¡å‹çš„è®­ç»ƒå’Œå¾®è°ƒï¼›æœ€åï¼Œé€šè¿‡ç›²è¯„è¿›è¡Œæ•ˆæœè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šARIELçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ä¸“å®¶å‚ä¸çš„å­¦ä¹ æœºåˆ¶ï¼Œç»“åˆäº†å¤šæ¨¡æ€æ•°æ®å’Œä»»åŠ¡ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„éœ€æ±‚ï¼Œä¸ä¼ ç»Ÿçš„å•ä¸€æ¨¡å‹è®­ç»ƒæ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†è½»é‡çº§å¾®è°ƒç­–ç•¥å’Œè®¡ç®—è§„æ¨¡æ¨ç†ç­–ç•¥ï¼Œä¼˜åŒ–äº†æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥æé«˜æ–‡æœ¬è¦†ç›–ç‡å’Œè§†è§‰é—®ç­”çš„å‡†ç¡®æ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒARIELæ¡†æ¶åœ¨æ–‡æœ¬æ‘˜è¦ç”Ÿæˆä¸­æ˜¾è‘—æé«˜äº†è¦†ç›–ç‡ï¼Œä¸”åœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°30%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶åœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ARIELæ¡†æ¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç”Ÿç‰©åŒ»å­¦ç ”ç©¶ã€ä¸´åºŠå†³ç­–æ”¯æŒå’Œç§‘å­¦æ–‡çŒ®åˆ†æç­‰ã€‚å…¶æä¾›çš„å¯é‡å¤æ€§å¹³å°èƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜æ›´æœ‰æ•ˆåœ°åˆ©ç”¨AIæŠ€æœ¯ï¼ŒåŠ é€Ÿç”Ÿç‰©åŒ»å­¦å‘ç°ï¼Œæ¨åŠ¨ç§‘å­¦ç ”ç©¶çš„è¿›å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) and large multimodal models (LMMs) promise to accelerate biomedical discovery, yet their reliability remains unclear. We introduce ARIEL (AI Research Assistant for Expert-in-the-Loop Learning), an open-source evaluation and optimization framework that pairs a curated multimodal biomedical corpus with expert-vetted tasks to probe two capabilities: full-length article summarization and fine-grained figure interpretation. Using uniform protocols and blinded PhD-level evaluation, we find that state-of-the-art models generate fluent but incomplete summaries, whereas LMMs struggle with detailed visual reasoning. We later observe that prompt engineering and lightweight fine-tuning substantially improve textual coverage, and a compute-scaled inference strategy enhances visual question answering. We build an ARIEL agent that integrates textual and visual cues, and we show it can propose testable mechanistic hypotheses. ARIEL delineates current strengths and limitations of foundation models, and provides a reproducible platform for advancing trustworthy AI in biomedicine.

