---
layout: default
title: Inducing Robustness in a 2 Dimensional Direct Preference Optimization Paradigm
---

# Inducing Robustness in a 2 Dimensional Direct Preference Optimization Paradigm

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.01706" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.01706v1</a>
  <a href="https://arxiv.org/pdf/2505.01706.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.01706v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.01706v1', 'Inducing Robustness in a 2 Dimensional Direct Preference Optimization Paradigm')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sarvesh Shashidhar, Ritik, Nachiketa Patil, Suraj Racha, Ganesh Ramakrishnan

**åˆ†ç±»**: cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-03

**å¤‡æ³¨**: Updated abstract, algorithm and experimental results

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡º2D-DPOä»¥è§£å†³ç›´æ¥åå¥½ä¼˜åŒ–ä¸­çš„è¯„åˆ†ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç›´æ¥åå¥½ä¼˜åŒ–` `äºŒç»´è¯„åˆ†` `é²æ£’æ€§` `å¤§å‹è¯­è¨€æ¨¡å‹` `äººç±»åå¥½` `æœºå™¨å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•æœªèƒ½æœ‰æ•ˆå¤„ç†äººç±»åå¥½çš„ç»†ç²’åº¦è¯„åˆ†ï¼Œå¯¼è‡´å¯¹å“åº”çš„å„ä¸ªéƒ¨åˆ†è¯„åˆ†ä¸å‡è¡¡ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§äºŒç»´è¯„åˆ†çš„å¯¹é½æ–¹æ³•2D-DPOï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥ç»†ç²’åº¦è¯„åˆ†æ¥æ”¹å–„DPOçš„æ•ˆæœã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œ2D-DPOåœ¨å¯¹æŠ—æ ‡ç­¾å™ªå£°æ–¹é¢è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼Œå¹¶é€šè¿‡å®è¯éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»åå¥½çš„æ–¹æ³•ï¼Œå­˜åœ¨è¯„åˆ†ç²’åº¦ä¸è¶³çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º2D-DPOçš„äºŒç»´è¯„åˆ†æ–¹æ³•ï¼Œä»¥è§£å†³DPOåœ¨å¤„ç†äººç±»åå¥½æ—¶çš„ä¸è¶³ã€‚é€šè¿‡å¯¹æ¯”2D-DPOä¸æ ‡å‡†DPOçš„èƒœç‡ï¼Œå‘ç°2D-DPOåœ¨å¯¹æŠ—æ ‡ç­¾å™ªå£°æ–¹é¢è¡¨ç°æ›´ä½³ï¼Œå¹¶æä¾›äº†ç†è®ºæ”¯æŒå’Œå®è¯éªŒè¯ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤„ç†è¯„åˆ†å™ªå£°æ—¶çš„ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰åœ¨å¤„ç†äººç±»åå¥½æ—¶çš„è¯„åˆ†ä¸è¶³é—®é¢˜ã€‚ç°æœ‰DPOæ–¹æ³•æœªèƒ½è€ƒè™‘å“åº”ä¸­å„ä¸ªéƒ¨åˆ†çš„åå¥½å·®å¼‚ï¼Œå¯¼è‡´è¯„åˆ†ä¸å¤Ÿç»†è‡´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäºŒç»´è¯„åˆ†æ–¹æ³•2D-DPOï¼Œé€šè¿‡å¼•å…¥å¯¹å“åº”çš„ç»†ç²’åº¦è¯„åˆ†ï¼Œæ”¹å–„DPOåœ¨å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½æ—¶çš„æ•ˆæœã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ äººç±»çš„çœŸå®åå¥½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼š2D-DPOæ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯å¯¹å“åº”è¿›è¡Œç»†ç²’åº¦è¯„åˆ†ï¼ŒäºŒæ˜¯é€šè¿‡æ¯”è¾ƒä¸åŒè¯„åˆ†çš„èƒœç‡æ¥ä¼˜åŒ–æ¨¡å‹ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†ä¸åŒéƒ¨åˆ†çš„è¯„åˆ†å·®å¼‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†äºŒç»´è¯„åˆ†æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰äººç±»åå¥½çš„å¤æ‚æ€§ã€‚è¿™ä¸€åˆ›æ–°ä¸ä¼ ç»ŸDPOæ–¹æ³•çš„å•ä¸€è¯„åˆ†æœºåˆ¶å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç»†ç²’åº¦è¯„åˆ†çš„å‡†ç¡®æ€§ï¼Œå¹¶å¼•å…¥äº†é’ˆå¯¹è¯„åˆ†å™ªå£°çš„é²æ£’æ€§è®¾è®¡ï¼Œä»¥æé«˜æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„ç¨³å®šæ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œ2D-DPOåœ¨å¯¹æ¯”æ ‡å‡†DPOæ—¶ï¼Œèƒœç‡æ˜¾è‘—æé«˜ï¼Œå°¤å…¶åœ¨å¤„ç†æ ‡ç­¾å™ªå£°æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚å…·ä½“æ•°æ®è¡¨æ˜ï¼Œ2D-DPOåœ¨å¤šä¸ªå¼€æ”¾æºåå¥½æ•°æ®é›†ä¸Šçš„æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°15%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å¯¹è¯ç³»ç»Ÿã€æ¨èç³»ç»Ÿä»¥åŠä»»ä½•éœ€è¦å¯¹äººç±»åå¥½è¿›è¡Œå»ºæ¨¡çš„åœºæ™¯ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ï¼Œ2D-DPOèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æä¾›æ›´ä¼˜è´¨çš„ç”¨æˆ·ä½“éªŒï¼Œæ¨åŠ¨äººæœºäº¤äº’çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Direct Preference Optimisation (DPO) has emerged as a powerful method for aligning Large Language Models (LLMs) with human preferences, offering a stable and efficient alternative to approaches that use Reinforcement learning via Human Feedback. In this work, we investigate the performance of DPO using open-source preference datasets. One of the major drawbacks of DPO is that it doesn't induce granular scoring and treats all the segments of the responses with equal propensity. However, this is not practically true for human preferences since even "good" responses have segments that may not be preferred by the annotator. To resolve this, a 2-dimensional scoring for DPO alignment called 2D-DPO was proposed. We explore the 2D-DPO alignment paradigm and the advantages it provides over the standard DPO by comparing their win rates. It is observed that these methods, even though effective, are not robust to label/score noise. To counter this, we propose an approach of incorporating segment-level score noise robustness to the 2D-DPO algorithm. Along with theoretical backing, we also provide empirical verification in favour of the algorithm and introduce other noise models that can be present.

