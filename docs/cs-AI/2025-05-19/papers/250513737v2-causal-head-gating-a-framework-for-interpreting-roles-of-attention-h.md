---
layout: default
title: Causal Head Gating: A Framework for Interpreting Roles of Attention Heads in Transformers
---

# Causal Head Gating: A Framework for Interpreting Roles of Attention Heads in Transformers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13737" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13737v2</a>
  <a href="https://arxiv.org/pdf/2505.13737.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13737v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13737v2', 'Causal Head Gating: A Framework for Interpreting Roles of Attention Heads in Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Andrew Nam, Henry Conklin, Yukang Yang, Thomas Griffiths, Jonathan Cohen, Sarah-Jane Leslie

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19 (æ›´æ–°: 2025-10-23)

**å¤‡æ³¨**: 10 pages, 5 figures, 2 tables. The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå› æœå¤´é—¨æ§æ–¹æ³•ä»¥è§£æå˜æ¢å™¨ä¸­æ³¨æ„åŠ›å¤´çš„åŠŸèƒ½è§’è‰²**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å› æœæ¨æ–­` `æ³¨æ„åŠ›æœºåˆ¶` `å˜æ¢å™¨æ¨¡å‹` `å¯è§£é‡Šæ€§` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä»»åŠ¡æ€§èƒ½` `è½¯é—¨æ§` `å­ç”µè·¯åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æœºåˆ¶å¯è§£é‡Šæ€§æ–¹æ³•é€šå¸¸ä¾èµ–äºå‡è®¾é©±åŠ¨å’Œç‰¹å®šçš„æç¤ºæ¨¡æ¿ï¼Œé™åˆ¶äº†å…¶é€‚ç”¨æ€§å’Œçµæ´»æ€§ã€‚
2. å› æœå¤´é—¨æ§ï¼ˆCHGï¼‰é€šè¿‡å­¦ä¹ è½¯é—¨æ§å¹¶å¯¹æ³¨æ„åŠ›å¤´è¿›è¡Œå› æœåˆ†ç±»ï¼Œæä¾›äº†ä¸€ç§é€šç”¨çš„è§£ææ–¹æ³•ï¼Œé€‚ç”¨äºå„ç§æ•°æ®é›†ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCHGèƒ½å¤Ÿæ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ç¨€ç–ä»»åŠ¡å……åˆ†å­ç”µè·¯ï¼Œå¹¶éªŒè¯äº†å› æœå…³ç³»çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„æ–¹æ³•â€”â€”å› æœå¤´é—¨æ§ï¼ˆCHGï¼‰ï¼Œç”¨äºè§£æå˜æ¢å™¨æ¨¡å‹ä¸­æ³¨æ„åŠ›å¤´çš„åŠŸèƒ½è§’è‰²ã€‚CHGé€šè¿‡å­¦ä¹ å¤´éƒ¨çš„è½¯é—¨æ§ï¼Œå¹¶æ ¹æ®å…¶å¯¹ä»»åŠ¡æ€§èƒ½çš„å½±å“å°†å…¶åˆ†ä¸ºä¿ƒè¿›ã€å¹²æ‰°æˆ–æ— å…³çš„å› æœåˆ†ç±»ã€‚ä¸ä»¥å¾€çš„æœºåˆ¶å¯è§£é‡Šæ€§æ–¹æ³•ä¸åŒï¼ŒCHGä¸ä¾èµ–äºå‡è®¾é©±åŠ¨æˆ–ç‰¹å®šçš„æç¤ºæ¨¡æ¿ï¼Œè€Œæ˜¯ç›´æ¥åº”ç”¨äºä»»ä½•æ•°æ®é›†ï¼Œä½¿ç”¨æ ‡å‡†çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ ·åŒ–ä»»åŠ¡ä¸Šè¯„ä¼°äº†CHGï¼Œç»“æœè¡¨æ˜CHGå¾—åˆ†æä¾›äº†å› æœè€Œéä»…ä»…æ˜¯ç›¸å…³çš„æ´å¯Ÿï¼Œä¸”é€šè¿‡æ¶ˆèå’Œå› æœä¸­ä»‹åˆ†æå¾—åˆ°äº†éªŒè¯ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†å¯¹æ¯”CHGå˜ä½“ï¼Œä»¥éš”ç¦»ç‰¹å®šä»»åŠ¡ç»„ä»¶çš„å­ç”µè·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¦‚ä½•æœ‰æ•ˆè§£æå˜æ¢å™¨æ¨¡å‹ä¸­æ³¨æ„åŠ›å¤´çš„åŠŸèƒ½è§’è‰²è¿™ä¸€é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå‡è®¾å’Œç‰¹å®šçš„ä»»åŠ¡æ ‡ç­¾ï¼Œé™åˆ¶äº†å…¶é€‚ç”¨èŒƒå›´å’Œçµæ´»æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šå› æœå¤´é—¨æ§ï¼ˆCHGï¼‰é€šè¿‡å­¦ä¹ æ³¨æ„åŠ›å¤´çš„è½¯é—¨æ§ï¼ŒåŸºäºå…¶å¯¹ä»»åŠ¡æ€§èƒ½çš„å½±å“å°†å…¶åˆ†ç±»ä¸ºä¿ƒè¿›ã€å¹²æ‰°æˆ–æ— å…³ï¼Œä»è€Œæä¾›ä¸€ç§é€šç”¨çš„è§£ææ¡†æ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCHGçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤´éƒ¨é—¨æ§å­¦ä¹ ã€å› æœåˆ†ç±»å’Œä»»åŠ¡æ€§èƒ½è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œæ¨¡å‹é€šè¿‡æ ‡å‡†çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹å­¦ä¹ å¤´éƒ¨çš„è½¯é—¨æ§ï¼›ç„¶åï¼ŒåŸºäºè¿™äº›é—¨æ§çš„å½±å“è¿›è¡Œå› æœåˆ†ç±»ï¼›æœ€åï¼Œé€šè¿‡å¤šç§ä»»åŠ¡è¯„ä¼°å…¶æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šCHGçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ä¸ä¾èµ–äºå‡è®¾é©±åŠ¨çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿç›´æ¥åº”ç”¨äºä»»ä½•æ•°æ®é›†ï¼Œå¹¶æä¾›å› æœè€Œéç›¸å…³çš„æ´å¯Ÿã€‚è¿™ä¸€æ–¹æ³•ä¸ä»¥å¾€çš„æœºåˆ¶å¯è§£é‡Šæ€§æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒCHGé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–é—¨æ§å­¦ä¹ ï¼Œå¹¶é€šè¿‡æ¶ˆèå®éªŒå’Œå› æœä¸­ä»‹åˆ†æéªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚æ¨¡å‹ç»“æ„ä¸Šï¼ŒCHGèƒ½å¤Ÿçµæ´»é€‚åº”ä¸åŒçš„ä»»åŠ¡å’Œæ•°æ®é›†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCHGåœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ­ç¤ºæ³¨æ„åŠ›å¤´çš„å› æœè§’è‰²ã€‚é€šè¿‡æ¶ˆèå®éªŒéªŒè¯ï¼ŒCHGå¾—åˆ†æä¾›äº†å› æœæ´å¯Ÿï¼Œä¸”ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæå‡äº†æ¨¡å‹åœ¨è¯­æ³•ã€å¸¸è¯†å’Œæ•°å­¦æ¨ç†ç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆæ›´å¥½åœ°ç†è§£å’Œä¼˜åŒ–å˜æ¢å™¨æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡è§£ææ³¨æ„åŠ›å¤´çš„åŠŸèƒ½è§’è‰²ï¼Œèƒ½å¤Ÿæå‡æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œå¯é æ€§ï¼Œä¿ƒè¿›æ›´æ™ºèƒ½çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present causal head gating (CHG), a scalable method for interpreting the functional roles of attention heads in transformer models. CHG learns soft gates over heads and assigns them a causal taxonomy - facilitating, interfering, or irrelevant - based on their impact on task performance. Unlike prior approaches in mechanistic interpretability, which are hypothesis-driven and require prompt templates or target labels, CHG applies directly to any dataset using standard next-token prediction. We evaluate CHG across multiple large language models (LLMs) in the Llama 3 model family and diverse tasks, including syntax, commonsense, and mathematical reasoning, and show that CHG scores yield causal, not merely correlational, insight validated via ablation and causal mediation analyses. We also introduce contrastive CHG, a variant that isolates sub-circuits for specific task components. Our findings reveal that LLMs contain multiple sparse task-sufficient sub-circuits, that individual head roles depend on interactions with others (low modularity), and that instruction following and in-context learning rely on separable mechanisms.

