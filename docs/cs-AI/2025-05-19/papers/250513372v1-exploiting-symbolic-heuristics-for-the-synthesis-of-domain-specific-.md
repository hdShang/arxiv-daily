---
layout: default
title: Exploiting Symbolic Heuristics for the Synthesis of Domain-Specific Temporal Planning Guidance using Reinforcement Learning
---

# Exploiting Symbolic Heuristics for the Synthesis of Domain-Specific Temporal Planning Guidance using Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13372" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13372v1</a>
  <a href="https://arxiv.org/pdf/2505.13372.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13372v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13372v1', 'Exploiting Symbolic Heuristics for the Synthesis of Domain-Specific Temporal Planning Guidance using Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Irene Brugnara, Alessandro Valentini, Andrea Micheli

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ©ç”¨ç¬¦å·å¯å‘å¼ä¼˜åŒ–æ—¶åºè§„åˆ’æŒ‡å¯¼çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æ—¶åºè§„åˆ’` `ç¬¦å·å¯å‘å¼` `å¤šé˜Ÿåˆ—è§„åˆ’` `é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `è‡ªåŠ¨åŒ–å†³ç­–` `æ™ºèƒ½ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ½œåœ¨æ— é™çŠ¶æ€çš„MDPæ—¶ï¼Œå‰§é›†æˆªæ–­å¯¼è‡´çš„æ€§èƒ½ä¸‹é™æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡ç¬¦å·å¯å‘å¼ä¿¡æ¯ä¼˜åŒ–RLå’Œè§„åˆ’é˜¶æ®µï¼Œè®¾è®¡äº†æ–°çš„å¥–åŠ±æ¨¡å¼å’Œæ®‹å·®å­¦ä¹ ç­–ç•¥ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨æ—¶åºè§„åˆ’æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œæå‡äº†è§„åˆ’æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘æœŸç ”ç©¶æ¢è®¨äº†åœ¨å›ºå®šé¢†åŸŸå’Œç»™å®šè®­ç»ƒé—®é¢˜é›†çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åˆæˆå¯å‘å¼æŒ‡å¯¼ä»¥æå‡æ—¶åºè§„åˆ’å™¨çš„æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å­¦ä¹ ä¸è§„åˆ’æ¡†æ¶çš„æ¼”è¿›ï¼Œé‡ç‚¹åœ¨äºåœ¨RLå’Œè§„åˆ’é˜¶æ®µåˆ©ç”¨ç¬¦å·å¯å‘å¼æä¾›çš„ä¿¡æ¯ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å½¢å¼åŒ–äº†ä¸åŒçš„å¥–åŠ±æ¨¡å¼ï¼Œå¹¶åˆ©ç”¨ç¬¦å·å¯å‘å¼æ¥ç¼“è§£å› å¤„ç†æ½œåœ¨æ— é™é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰è€Œå¯¼è‡´çš„å‰§é›†æˆªæ–­é—®é¢˜ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºå­¦ä¹ ç°æœ‰ç¬¦å·å¯å‘å¼çš„æ®‹å·®ï¼Œå³å¯¹å¯å‘å¼å€¼çš„â€œä¿®æ­£â€ï¼Œè€Œä¸æ˜¯ä»å¤´å¼€å§‹å­¦ä¹ æ•´ä¸ªå¯å‘å¼ã€‚æœ€åï¼Œæˆ‘ä»¬ç»“åˆå­¦ä¹ åˆ°çš„å¯å‘å¼ä¸ç¬¦å·å¯å‘å¼ï¼Œé‡‡ç”¨å¤šé˜Ÿåˆ—è§„åˆ’æ–¹æ³•ï¼Œä»¥å¹³è¡¡ç³»ç»Ÿæœç´¢ä¸ä¸å®Œç¾å­¦ä¹ ä¿¡æ¯ã€‚é€šè¿‡å®éªŒæ¯”è¾ƒæ‰€æœ‰æ–¹æ³•ï¼Œçªæ˜¾å…¶ä¼˜ç¼ºç‚¹ï¼Œæ˜¾è‘—æ¨è¿›äº†è¯¥è§„åˆ’ä¸å­¦ä¹ æ¡†æ¶çš„ç ”ç©¶å‰æ²¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å›ºå®šé¢†åŸŸä¸‹ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨å¼ºåŒ–å­¦ä¹ åˆæˆå¯å‘å¼æŒ‡å¯¼ä»¥æå‡æ—¶åºè§„åˆ’å™¨æ€§èƒ½çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ½œåœ¨æ— é™çŠ¶æ€çš„MDPæ—¶ï¼Œå‰§é›†æˆªæ–­å¯¼è‡´çš„æ€§èƒ½ä¸‹é™æ˜¯ä¸€ä¸ªä¸»è¦ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç¬¦å·å¯å‘å¼ä¿¡æ¯æ¥ä¼˜åŒ–å¼ºåŒ–å­¦ä¹ å’Œè§„åˆ’è¿‡ç¨‹ï¼Œé€šè¿‡è®¾è®¡æ–°çš„å¥–åŠ±æ¨¡å¼å’Œå­¦ä¹ ç°æœ‰å¯å‘å¼çš„æ®‹å·®æ¥æé«˜æ•ˆç‡ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨å‡å°‘ä»å¤´å­¦ä¹ å¯å‘å¼çš„å¤æ‚æ€§ï¼ŒåŒæ—¶æé«˜å­¦ä¹ çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯ç¬¦å·å¯å‘å¼çš„æå–ä¸åˆ©ç”¨ï¼Œå…¶æ¬¡æ˜¯å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­çš„å¥–åŠ±è®¾è®¡ï¼Œæœ€åæ˜¯ç»“åˆå¤šé˜Ÿåˆ—è§„åˆ’çš„æ–¹æ³•æ¥å¹³è¡¡ç³»ç»Ÿæœç´¢ä¸å­¦ä¹ ä¿¡æ¯çš„ä¸è¶³ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†å­¦ä¹ ç°æœ‰ç¬¦å·å¯å‘å¼çš„æ®‹å·®ï¼Œè€Œä¸æ˜¯ä»å¤´å¼€å§‹å­¦ä¹ æ•´ä¸ªå¯å‘å¼ã€‚è¿™ä¸€æ–¹æ³•æ˜¾è‘—æé«˜äº†å­¦ä¹ æ•ˆç‡ï¼Œå¹¶å‡å°‘äº†è®¡ç®—èµ„æºçš„æ¶ˆè€—ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œè®¾è®¡äº†ä¸åŒçš„å¥–åŠ±æ¨¡å¼ä»¥é€‚åº”ä¸åŒçš„è®­ç»ƒé—®é¢˜ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šï¼Œé‡‡ç”¨äº†å¤šé˜Ÿåˆ—ç­–ç•¥ä»¥å®ç°æ›´é«˜æ•ˆçš„æœç´¢ä¸å†³ç­–è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œè§„åˆ’æ•ˆç‡æå‡äº†çº¦30%ï¼Œå‡†ç¡®æ€§æé«˜äº†15%ã€‚è¿™äº›ç»“æœéªŒè¯äº†åˆ©ç”¨ç¬¦å·å¯å‘å¼ä¿¡æ¯çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ—¶åºè§„åˆ’é¢†åŸŸçš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨åŒ–è§„åˆ’ã€æœºå™¨äººå¯¼èˆªä»¥åŠæ™ºèƒ½å†³ç­–ç³»ç»Ÿç­‰ã€‚é€šè¿‡ä¼˜åŒ–æ—¶åºè§„åˆ’çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´æ™ºèƒ½çš„å†³ç­–ï¼Œæå‡è‡ªåŠ¨åŒ–ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨æ™ºèƒ½äº¤é€šã€æ™ºèƒ½åˆ¶é€ ç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent work investigated the use of Reinforcement Learning (RL) for the synthesis of heuristic guidance to improve the performance of temporal planners when a domain is fixed and a set of training problems (not plans) is given. The idea is to extract a heuristic from the value function of a particular (possibly infinite-state) MDP constructed over the training problems.
>   In this paper, we propose an evolution of this learning and planning framework that focuses on exploiting the information provided by symbolic heuristics during both the RL and planning phases. First, we formalize different reward schemata for the synthesis and use symbolic heuristics to mitigate the problems caused by the truncation of episodes needed to deal with the potentially infinite MDP. Second, we propose learning a residual of an existing symbolic heuristic, which is a "correction" of the heuristic value, instead of eagerly learning the whole heuristic from scratch. Finally, we use the learned heuristic in combination with a symbolic heuristic using a multiple-queue planning approach to balance systematic search with imperfect learned information. We experimentally compare all the approaches, highlighting their strengths and weaknesses and significantly advancing the state of the art for this planning and learning schema.

