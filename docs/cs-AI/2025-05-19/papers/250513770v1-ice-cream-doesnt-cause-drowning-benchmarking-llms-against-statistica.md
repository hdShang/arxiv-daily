---
layout: default
title: Ice Cream Doesn't Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference
---

# Ice Cream Doesn't Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13770" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13770v1</a>
  <a href="https://arxiv.org/pdf/2505.13770.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13770v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13770v1', 'Ice Cream Doesn\'t Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jin Du, Li Chen, Xun Xian, An Luo, Fangqiao Tian, Ganghua Wang, Charles Doss, Xiaotong Shen, Jie Ding

**åˆ†ç±»**: cs.AI, cs.CL, cs.LG, stat.ME, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCausalPitfallsåŸºå‡†ä»¥è§£å†³LLMså› æœæ¨æ–­ä¸­çš„ç»Ÿè®¡é™·é˜±é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å› æœæ¨æ–­` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç»Ÿè®¡é™·é˜±` `åŸºå‡†æµ‹è¯•` `æ¨¡å‹è¯„ä¼°` `æœºå™¨å­¦ä¹ ` `æ•°æ®åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å› æœæ¨æ–­åŸºå‡†ä»»åŠ¡è¿‡äºç®€åŒ–ï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°LLMsåœ¨å¤æ‚å› æœæ¨æ–­ä¸­çš„èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºCausalPitfallsåŸºå‡†ï¼Œé€šè¿‡ç»“æ„åŒ–æŒ‘æˆ˜å’Œè¯„åˆ†æ ‡å‡†ï¼Œå…¨é¢è¯„ä¼°LLMsåœ¨å› æœæ¨æ–­ä¸­çš„è¡¨ç°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰LLMsåœ¨ç»Ÿè®¡å› æœæ¨æ–­æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™ï¼Œä¸ºæœªæ¥çš„å› æœæ¨ç†ç³»ç»Ÿæä¾›äº†é‡è¦æŒ‡å¯¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯é çš„å› æœæ¨æ–­åœ¨åŒ»å­¦ã€ç»æµå­¦å’Œå…¬å…±æ”¿ç­–ç­‰é«˜é£é™©é¢†åŸŸè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç›®å‰å°šä¸æ¸…æ¥šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯å¦èƒ½å¤Ÿå¤„ç†ä¸¥æ ¼ä¸”å¯ä¿¡çš„ç»Ÿè®¡å› æœæ¨æ–­ã€‚ç°æœ‰åŸºå‡†é€šå¸¸æ¶‰åŠç®€åŒ–ä»»åŠ¡ï¼Œå¯èƒ½å¿½è§†é‡è¦çš„ç»Ÿè®¡é™·é˜±ï¼Œå¦‚è¾›æ™®æ£®æ‚–è®ºæˆ–é€‰æ‹©åå·®ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†CausalPitfallsï¼Œä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œæ—¨åœ¨ä¸¥æ ¼è¯„ä¼°LLMså…‹æœå¸¸è§å› æœæ¨æ–­é™·é˜±çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«å¤šä¸ªéš¾åº¦çº§åˆ«çš„ç»“æ„åŒ–æŒ‘æˆ˜ï¼Œå¹¶é…æœ‰è¯„åˆ†æ ‡å‡†ï¼Œå…è®¸æˆ‘ä»¬å®šé‡æµ‹é‡å› æœæ¨ç†èƒ½åŠ›å’ŒLLMså“åº”çš„å¯é æ€§ã€‚é€šè¿‡ç›´æ¥æç¤ºå’Œä»£ç è¾…åŠ©æç¤ºä¸¤ç§åè®®è¯„ä¼°æ¨¡å‹ï¼Œç»“æœæ˜¾ç¤ºå½“å‰LLMsåœ¨è¿›è¡Œç»Ÿè®¡å› æœæ¨æ–­æ—¶å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å› æœæ¨æ–­ä¸­é¢ä¸´çš„ç»Ÿè®¡é™·é˜±é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†å¤æ‚çš„ç»Ÿè®¡ç°è±¡ï¼Œå¦‚è¾›æ™®æ£®æ‚–è®ºå’Œé€‰æ‹©åå·®ï¼Œå¯¼è‡´æ¨æ–­ç»“æœä¸å¯é ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCausalPitfallsåŸºå‡†é€šè¿‡è®¾è®¡å¤šå±‚æ¬¡çš„ç»“æ„åŒ–æŒ‘æˆ˜ï¼Œç»“åˆè¯„åˆ†æ ‡å‡†ï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°LLMsåœ¨å› æœæ¨æ–­ä¸­çš„èƒ½åŠ›ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿåº”å¯¹å¤æ‚çš„ç»Ÿè®¡é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥åŸºå‡†åŒ…æ‹¬å¤šä¸ªéš¾åº¦çº§åˆ«çš„ä»»åŠ¡ï¼Œé‡‡ç”¨ä¸¤ç§è¯„ä¼°åè®®ï¼šç›´æ¥æç¤ºå’Œä»£ç è¾…åŠ©æç¤ºã€‚ç›´æ¥æç¤ºè¯„ä¼°æ¨¡å‹çš„å†…åœ¨å› æœæ¨ç†èƒ½åŠ›ï¼Œè€Œä»£ç è¾…åŠ©æç¤ºåˆ™è¦æ±‚æ¨¡å‹ç”Ÿæˆå¯æ‰§è¡Œä»£ç è¿›è¡Œæ˜ç¡®çš„ç»Ÿè®¡åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šCausalPitfallsçš„åˆ›æ–°åœ¨äºå…¶å…¨é¢æ€§å’Œç³»ç»Ÿæ€§ï¼Œèƒ½å¤Ÿé‡åŒ–è¯„ä¼°LLMsåœ¨å› æœæ¨æ–­ä¸­çš„è¡¨ç°ï¼Œå¡«è¡¥äº†ç°æœ‰åŸºå‡†çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåŸºå‡†ä»»åŠ¡è®¾è®¡æ¶µç›–äº†å¤šç§å› æœæ¨æ–­çš„ç»Ÿè®¡é™·é˜±ï¼Œè¯„åˆ†æ ‡å‡†ç»è¿‡éªŒè¯ï¼Œä¸äººç±»ä¸“å®¶çš„è¯„ä¼°ç»“æœç›¸æ¯”è¾ƒï¼Œç¡®ä¿äº†è¯„ä¼°çš„å¯é æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰LLMsåœ¨å¤„ç†å› æœæ¨æ–­æ—¶å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œå°¤å…¶åœ¨é¢å¯¹å¤æ‚ç»Ÿè®¡é™·é˜±æ—¶ï¼Œè¡¨ç°ä¸ä½³ã€‚CausalPitfallsåŸºå‡†çš„å¼•å…¥ä¸ºæœªæ¥å› æœæ¨ç†ç³»ç»Ÿçš„å¼€å‘æä¾›äº†é‡è¦çš„å®šé‡æŒ‡æ ‡å’ŒæŒ‡å¯¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»å­¦ã€ç»æµå­¦å’Œå…¬å…±æ”¿ç­–ç­‰é«˜é£é™©å†³ç­–åœºæ™¯ã€‚é€šè¿‡æå‡LLMsåœ¨å› æœæ¨æ–­ä¸­çš„å¯é æ€§ï¼Œèƒ½å¤Ÿä¸ºæ”¿ç­–åˆ¶å®šå’Œç§‘å­¦ç ”ç©¶æä¾›æ›´ä¸ºå‡†ç¡®çš„æ”¯æŒï¼Œè¿›è€Œå½±å“ç¤¾ä¼šå„ä¸ªå±‚é¢çš„å†³ç­–è¿‡ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reliable causal inference is essential for making decisions in high-stakes areas like medicine, economics, and public policy. However, it remains unclear whether large language models (LLMs) can handle rigorous and trustworthy statistical causal inference. Current benchmarks usually involve simplified tasks. For example, these tasks might only ask LLMs to identify semantic causal relationships or draw conclusions directly from raw data. As a result, models may overlook important statistical pitfalls, such as Simpson's paradox or selection bias. This oversight limits the applicability of LLMs in the real world. To address these limitations, we propose CausalPitfalls, a comprehensive benchmark designed to rigorously evaluate the capability of LLMs in overcoming common causal inference pitfalls. Our benchmark features structured challenges across multiple difficulty levels, each paired with grading rubrics. This approach allows us to quantitatively measure both causal reasoning capabilities and the reliability of LLMs' responses. We evaluate models using two protocols: (1) direct prompting, which assesses intrinsic causal reasoning, and (2) code-assisted prompting, where models generate executable code for explicit statistical analysis. Additionally, we validate the effectiveness of this judge by comparing its scoring with assessments from human experts. Our results reveal significant limitations in current LLMs when performing statistical causal inference. The CausalPitfalls benchmark provides essential guidance and quantitative metrics to advance the development of trustworthy causal reasoning systems.

