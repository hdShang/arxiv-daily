---
layout: default
title: CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition
---

# CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13380" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13380v1</a>
  <a href="https://arxiv.org/pdf/2505.13380.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13380v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13380v1', 'CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nam V. Nguyen, Huy Nguyen, Quang Pham, Van Nguyen, Savitha Ramasamy, Nhat Ho

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19

**å¤‡æ³¨**: 52 pages. This work is an improved version of the previous study at arXiv:2402.02526

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Fsoft-AIC/CompeteSMoE)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCompeteSMoEä»¥è§£å†³ç¨€ç–ä¸“å®¶æ¨¡å‹è®­ç»ƒä¸­çš„è·¯ç”±æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¨€ç–ä¸“å®¶æ¨¡å‹` `ç«äº‰æœºåˆ¶` `è·¯ç”±æ•ˆç‡` `è¯­è¨€æ¨¡å‹` `è§†è§‰æŒ‡ä»¤è°ƒä¼˜` `æ ·æœ¬æ•ˆç‡` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¨€ç–ä¸“å®¶æ¨¡å‹è®­ç»ƒæ–¹æ³•åœ¨è·¯ç”±è¿‡ç¨‹ä¸­æ•ˆç‡ä½ä¸‹ï¼Œå¯¼è‡´è®¡ç®—èµ„æºæµªè´¹ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç«äº‰æœºåˆ¶ï¼Œé€šè¿‡ä¼˜åŒ–ä»¤ç‰Œè·¯ç”±åˆ°å“åº”æœ€å¼ºçš„ä¸“å®¶ï¼Œæé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCompeteSMoEåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè®­ç»ƒå¼€é”€ä½ï¼Œå…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¨€ç–ä¸“å®¶æ¨¡å‹ï¼ˆSMoEï¼‰ä¸ºæå‡æ¨¡å‹å¤æ‚åº¦æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰è®­ç»ƒæ–¹æ³•åœ¨è·¯ç”±è¿‡ç¨‹ä¸­å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç«äº‰æœºåˆ¶ï¼Œé€šè¿‡å°†è¾“å…¥ä»¤ç‰Œè·¯ç”±åˆ°å“åº”æœ€å¼ºçš„ä¸“å®¶ï¼Œæ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚æˆ‘ä»¬å¼€å‘äº†CompeteSMoEç®—æ³•ï¼Œåˆ©ç”¨è·¯ç”±å™¨å­¦ä¹ ç«äº‰ç­–ç•¥ï¼Œåœ¨è§†è§‰æŒ‡ä»¤è°ƒä¼˜å’Œè¯­è¨€é¢„è®­ç»ƒä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”è®­ç»ƒå¼€é”€ä½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCompeteSMoEåœ¨æ€§èƒ½ã€é²æ£’æ€§å’Œå¯æ‰©å±•æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„SMoEç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¨€ç–ä¸“å®¶æ¨¡å‹ï¼ˆSMoEï¼‰è®­ç»ƒä¸­çš„è·¯ç”±æ•ˆç‡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•çš„è·¯ç”±è¿‡ç¨‹æœªèƒ½æœ‰æ•ˆåˆ©ç”¨ä¸“å®¶çš„è®¡ç®—èƒ½åŠ›ï¼Œå¯¼è‡´æ ·æœ¬æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«äº‰æœºåˆ¶ï¼Œé€šè¿‡å°†è¾“å…¥ä»¤ç‰Œè·¯ç”±åˆ°å“åº”æœ€å¼ºçš„ä¸“å®¶ï¼Œä»è€Œæé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¯ä¸ªä¸“å®¶çš„è®¡ç®—èƒ½åŠ›èƒ½å¤Ÿç›´æ¥å½±å“è·¯ç”±å†³ç­–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCompeteSMoEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªè·¯ç”±å™¨æ¨¡å—ï¼Œè¯¥æ¨¡å—å­¦ä¹ ç«äº‰ç­–ç•¥ä»¥ä¼˜åŒ–ä»¤ç‰Œçš„è·¯ç”±è¿‡ç¨‹ã€‚ç®—æ³•é€šè¿‡è®­ç»ƒè¿‡ç¨‹ä¸æ–­è°ƒæ•´è·¯ç”±ç­–ç•¥ï¼Œä»¥å®ç°é«˜æ•ˆçš„ä¸“å®¶é€‰æ‹©ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥ç«äº‰æœºåˆ¶ï¼Œæ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„softmaxè·¯ç”±æ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨ä¸“å®¶çš„è®¡ç®—èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•å®ç°ä¸­ï¼Œæˆ‘ä»¬è®¾ç½®äº†é€‚å½“çš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–è·¯ç”±ç­–ç•¥ï¼Œå¹¶é‡‡ç”¨äº†ç®€å•çš„ç½‘ç»œç»“æ„ä»¥é™ä½è®­ç»ƒå¼€é”€ï¼ŒåŒæ—¶ç¡®ä¿äº†æ¨¡å‹çš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªå®éªŒä¸­ï¼ŒCompeteSMoEåœ¨è§†è§‰æŒ‡ä»¤è°ƒä¼˜å’Œè¯­è¨€é¢„è®­ç»ƒä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºç°æœ‰çš„SMoEç­–ç•¥ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ã€‚æ­¤å¤–ï¼Œè®­ç»ƒå¼€é”€æ˜¾è‘—é™ä½ï¼Œä½¿å¾—å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒæ›´åŠ å¯è¡Œã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CompeteSMoEåœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒã€è§†è§‰æŒ‡ä»¤è°ƒä¼˜ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚å…¶é«˜æ•ˆçš„è·¯ç”±æœºåˆ¶å’Œä½è®­ç»ƒå¼€é”€ä½¿å…¶é€‚ç”¨äºéœ€è¦å¿«é€Ÿå“åº”å’Œé«˜æ€§èƒ½çš„å®é™…åœºæ™¯ï¼Œå¦‚æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨ç¿»è¯‘å’Œå¤šæ¨¡æ€å­¦ä¹ ç­‰ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨åŠ¨æ›´å¤æ‚æ¨¡å‹çš„å¼€å‘ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sparse mixture of experts (SMoE) offers an appealing solution to scale up the model complexity beyond the mean of increasing the network's depth or width. However, we argue that effective SMoE training remains challenging because of the suboptimal routing process where experts that perform computation do not directly contribute to the routing process. In this work, we propose competition, a novel mechanism to route tokens to experts with the highest neural response. Theoretically, we show that the competition mechanism enjoys a better sample efficiency than the traditional softmax routing. Furthermore, we develop CompeteSMoE, a simple yet effective algorithm to train large language models by deploying a router to learn the competition policy, thus enjoying strong performances at a low training overhead. Our extensive empirical evaluations on both the visual instruction tuning and language pre-training tasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE compared to state-of-the-art SMoE strategies. We have made the implementation available at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an improved version of the previous study at arXiv:2402.02526

