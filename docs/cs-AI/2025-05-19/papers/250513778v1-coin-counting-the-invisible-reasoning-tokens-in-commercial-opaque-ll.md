---
layout: default
title: "CoIn: Counting the Invisible Reasoning Tokens in Commercial Opaque LLM APIs"
---

# CoIn: Counting the Invisible Reasoning Tokens in Commercial Opaque LLM APIs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13778" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13778v1</a>
  <a href="https://arxiv.org/pdf/2505.13778.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13778v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13778v1', 'CoIn: Counting the Invisible Reasoning Tokens in Commercial Opaque LLM APIs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guoheng Sun, Ziyao Wang, Bowei Tian, Meng Liu, Zheyu Shen, Shwai He, Yexiao He, Wanghao Ye, Yiting Wang, Ang Li

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/CASE-Lab-UMD/LLM-Auditing-CoIn)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCoInæ¡†æ¶ä»¥è§£å†³å•†ä¸šLLM APIä¸­çš„éšæ€§æ¨ç†ä»¤ç‰Œè®¡æ•°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `è®¡è´¹é€æ˜åº¦` `å®¡è®¡æ¡†æ¶` `å“ˆå¸Œæ ‘` `åµŒå…¥åŒ¹é…` `å•†ä¸šAPI` `ç”¨æˆ·æƒç›Š`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å•†ä¸šLLM APIåœ¨è¿”å›æœ€ç»ˆç­”æ¡ˆæ—¶éšè—æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´ç”¨æˆ·æ— æ³•éªŒè¯ä»¤ç‰Œçš„çœŸå®ä½¿ç”¨æƒ…å†µã€‚
2. CoInæ¡†æ¶é€šè¿‡æ„å»ºå¯éªŒè¯çš„å“ˆå¸Œæ ‘å’ŒåµŒå…¥åŸºç¡€çš„ç›¸å…³æ€§åŒ¹é…ï¼Œå®¡è®¡éšè—ä»¤ç‰Œçš„æ•°é‡å’Œè¯­ä¹‰æœ‰æ•ˆæ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCoInåœ¨æ£€æµ‹ä»¤ç‰Œè®¡æ•°è†¨èƒ€æ–¹é¢çš„æˆåŠŸç‡é«˜è¾¾94.7%ï¼Œæ˜¾è‘—æå‡äº†è®¡è´¹é€æ˜åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€åè®­ç»ƒæŠ€æœ¯çš„å‘å±•ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€æ¸å¢å¼ºäº†ç»“æ„åŒ–çš„å¤šæ­¥éª¤æ¨ç†èƒ½åŠ›ï¼Œé€šå¸¸é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ã€‚è¿™äº›å¢å¼ºæ¨ç†çš„æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸Šä¼˜äºæ ‡å‡†LLMsï¼Œå¹¶æ”¯æ’‘ç€è®¸å¤šå•†ä¸šLLM APIã€‚ç„¶è€Œï¼Œä¸ºäº†ä¿æŠ¤ä¸“æœ‰è¡Œä¸ºå¹¶å‡å°‘å†—é•¿ï¼Œæä¾›å•†é€šå¸¸åœ¨è¿”å›æœ€ç»ˆç­”æ¡ˆæ—¶éšè—æ¨ç†ç—•è¿¹ã€‚è¿™ç§ä¸é€æ˜æ€§å¯¼è‡´äº†å…³é”®çš„é€æ˜åº¦ç¼ºå£ï¼šç”¨æˆ·ä¸ºä¸å¯è§çš„æ¨ç†ä»¤ç‰Œä»˜è´¹ï¼Œè€Œè¿™äº›ä»¤ç‰Œå¾€å¾€å æ®äº†è´¹ç”¨çš„å¤§éƒ¨åˆ†ï¼Œç”¨æˆ·å´æ— æ³•éªŒè¯å…¶çœŸå®æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†CoInï¼Œä¸€ä¸ªéªŒè¯æ¡†æ¶ï¼Œç”¨äºå®¡è®¡éšè—ä»¤ç‰Œçš„æ•°é‡å’Œè¯­ä¹‰æœ‰æ•ˆæ€§ã€‚å®éªŒè¡¨æ˜ï¼Œä½œä¸ºå¯ä¿¡çš„ç¬¬ä¸‰æ–¹å®¡è®¡è€…ï¼ŒCoInèƒ½å¤Ÿæœ‰æ•ˆæ£€æµ‹ä»¤ç‰Œè®¡æ•°è†¨èƒ€ï¼ŒæˆåŠŸç‡é«˜è¾¾94.7%ï¼Œæ˜¾ç¤ºå‡ºåœ¨ä¸é€æ˜çš„LLMæœåŠ¡ä¸­æ¢å¤è®¡è´¹é€æ˜åº¦çš„å¼ºå¤§èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å•†ä¸šLLM APIä¸­éšæ€§æ¨ç†ä»¤ç‰Œè®¡æ•°çš„é€æ˜åº¦é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æ— æ³•éªŒè¯ç”¨æˆ·ä¸ºä¸å¯è§ä»¤ç‰Œæ”¯ä»˜çš„è´¹ç”¨çš„çœŸå®æ€§ï¼Œå¯èƒ½å¯¼è‡´ä»¤ç‰Œè®¡æ•°è†¨èƒ€å’Œä¸å½“æ”¶è´¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCoInæ¡†æ¶çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å»ºå¯éªŒè¯çš„å“ˆå¸Œæ ‘å’ŒåµŒå…¥åŸºç¡€çš„ç›¸å…³æ€§åŒ¹é…ï¼Œæ¥å®¡è®¡å’ŒéªŒè¯éšè—ä»¤ç‰Œçš„æ•°é‡å’Œè¯­ä¹‰æœ‰æ•ˆæ€§ï¼Œä»è€Œæé«˜è®¡è´¹çš„é€æ˜åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoInçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œåˆ©ç”¨ä»¤ç‰ŒåµŒå…¥æŒ‡çº¹æ„å»ºå“ˆå¸Œæ ‘ä»¥æ£€æŸ¥ä»¤ç‰Œè®¡æ•°ï¼›å…¶æ¬¡ï¼Œé€šè¿‡åµŒå…¥åŸºç¡€çš„ç›¸å…³æ€§åŒ¹é…æ¥æ£€æµ‹ä¼ªé€ çš„æ¨ç†å†…å®¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šCoInçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶èƒ½å¤Ÿæœ‰æ•ˆå®¡è®¡éšè—ä»¤ç‰Œçš„æ•°é‡å’Œè¯­ä¹‰ï¼Œå¡«è¡¥äº†ç°æœ‰æ–¹æ³•åœ¨é€æ˜åº¦å’ŒéªŒè¯èƒ½åŠ›ä¸Šçš„ç©ºç™½ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒCoInæä¾›äº†ä¸€ç§æ–°çš„éªŒè¯æœºåˆ¶ï¼Œç¡®ä¿ç”¨æˆ·èƒ½å¤Ÿä¿¡ä»»å…¶æ”¯ä»˜çš„è´¹ç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒCoIné‡‡ç”¨äº†é«˜æ•ˆçš„å“ˆå¸Œç®—æ³•ä»¥æ„å»ºå“ˆå¸Œæ ‘ï¼Œå¹¶ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡ŒåµŒå…¥åŒ¹é…ï¼Œç¡®ä¿äº†é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡åœ¨å®éªŒä¸­ç»è¿‡ä¼˜åŒ–ï¼Œä»¥æé«˜æ£€æµ‹çš„æˆåŠŸç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCoInåœ¨æ£€æµ‹ä»¤ç‰Œè®¡æ•°è†¨èƒ€æ–¹é¢çš„æˆåŠŸç‡é«˜è¾¾94.7%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„å®¡è®¡æ–¹æ³•ã€‚è¿™ä¸€æˆæœå±•ç¤ºäº†CoInåœ¨æ¢å¤å•†ä¸šLLMæœåŠ¡è®¡è´¹é€æ˜åº¦æ–¹é¢çš„å¼ºå¤§èƒ½åŠ›ï¼Œä¸ºç”¨æˆ·æä¾›äº†å¯ä¿¡çš„è´¹ç”¨éªŒè¯æ‰‹æ®µã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CoInæ¡†æ¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å•†ä¸šLLM APIçš„è®¡è´¹é€æ˜åº¦å®¡è®¡ã€äº‘è®¡ç®—æœåŠ¡çš„è´¹ç”¨ç›‘æ§ä»¥åŠä»»ä½•éœ€è¦éªŒè¯éšæ€§æ•°æ®å¤„ç†çš„åœºæ™¯ã€‚å…¶å®é™…ä»·å€¼åœ¨äºä¿æŠ¤ç”¨æˆ·æƒç›Šï¼Œé˜²æ­¢ä¸å½“æ”¶è´¹ï¼Œå¹¶ä¿ƒè¿›LLMæœåŠ¡çš„å…¬å¹³æ€§ä¸é€æ˜åº¦ã€‚æœªæ¥ï¼ŒCoInå¯èƒ½æ¨åŠ¨è¡Œä¸šæ ‡å‡†çš„å»ºç«‹ï¼Œä¿ƒè¿›æ›´å¹¿æ³›çš„æŠ€æœ¯åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As post-training techniques evolve, large language models (LLMs) are increasingly augmented with structured multi-step reasoning abilities, often optimized through reinforcement learning. These reasoning-enhanced models outperform standard LLMs on complex tasks and now underpin many commercial LLM APIs. However, to protect proprietary behavior and reduce verbosity, providers typically conceal the reasoning traces while returning only the final answer. This opacity introduces a critical transparency gap: users are billed for invisible reasoning tokens, which often account for the majority of the cost, yet have no means to verify their authenticity. This opens the door to token count inflation, where providers may overreport token usage or inject synthetic, low-effort tokens to inflate charges. To address this issue, we propose CoIn, a verification framework that audits both the quantity and semantic validity of hidden tokens. CoIn constructs a verifiable hash tree from token embedding fingerprints to check token counts, and uses embedding-based relevance matching to detect fabricated reasoning content. Experiments demonstrate that CoIn, when deployed as a trusted third-party auditor, can effectively detect token count inflation with a success rate reaching up to 94.7%, showing the strong ability to restore billing transparency in opaque LLM services. The dataset and code are available at https://github.com/CASE-Lab-UMD/LLM-Auditing-CoIn.

