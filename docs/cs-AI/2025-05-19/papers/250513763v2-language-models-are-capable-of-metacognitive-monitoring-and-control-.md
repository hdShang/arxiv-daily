---
layout: default
title: Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations
---

# Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13763" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13763v2</a>
  <a href="https://arxiv.org/pdf/2505.13763.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13763v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13763v2', 'Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Li Ji-An, Hua-Dong Xiong, Robert C. Wilson, Marcelo G. Mattar, Marcus K. Benna

**åˆ†ç±»**: cs.AI, cs.CL, q-bio.NC

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19 (æ›´æ–°: 2025-10-24)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¥ç»åé¦ˆèŒƒå¼ä»¥é‡åŒ–è¯­è¨€æ¨¡å‹çš„å…ƒè®¤çŸ¥èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…ƒè®¤çŸ¥` `è¯­è¨€æ¨¡å‹` `ç¥ç»åé¦ˆ` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `AIå®‰å…¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯­è¨€æ¨¡å‹åœ¨è§£å†³ä»»åŠ¡æ—¶ï¼Œå¸¸å¸¸æ— æ³•å‡†ç¡®è¯†åˆ«å’ŒæŠ¥å‘Šå…¶å†…éƒ¨ç­–ç•¥ï¼Œæ˜¾ç¤ºå‡ºå…ƒè®¤çŸ¥èƒ½åŠ›çš„ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç¥ç»åé¦ˆèŒƒå¼ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ æ¥é‡åŒ–è¯­è¨€æ¨¡å‹çš„å…ƒè®¤çŸ¥èƒ½åŠ›ï¼Œæ—¨åœ¨æé«˜å…¶å¯¹å†…éƒ¨æ¿€æ´»æ¨¡å¼çš„ç›‘æ§å’Œæ§åˆ¶èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹çš„å…ƒè®¤çŸ¥èƒ½åŠ›å—å¤šç§å› ç´ å½±å“ï¼Œä¸”å…¶ç›‘æ§èƒ½åŠ›ä»…é™äºç¥ç»æ¿€æ´»çš„ä¸€ä¸ªå°å­é›†ï¼Œå…·æœ‰é‡è¦çš„å®‰å…¨éšæ‚£æç¤ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ‰æ—¶èƒ½å¤ŸæŠ¥å‘Šå…¶è§£å†³ä»»åŠ¡æ‰€ä½¿ç”¨çš„ç­–ç•¥ï¼Œä½†åœ¨å…¶ä»–æƒ…å†µä¸‹ä¼¼ä¹æ— æ³•è¯†åˆ«æ”¯é…å…¶è¡Œä¸ºçš„ç­–ç•¥ã€‚è¿™è¡¨æ˜å®ƒä»¬å…·æœ‰æœ‰é™çš„å…ƒè®¤çŸ¥èƒ½åŠ›ï¼Œå³ç›‘æ§è‡ªèº«è®¤çŸ¥è¿‡ç¨‹çš„èƒ½åŠ›ã€‚å…ƒè®¤çŸ¥å¢å¼ºäº†LLMsåœ¨è§£å†³å¤æ‚ä»»åŠ¡ä¸­çš„èƒ½åŠ›ï¼Œä½†ä¹Ÿå¼•å‘äº†å®‰å…¨éšæ‚£ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§å—ç¥ç»ç§‘å­¦å¯å‘çš„ç¥ç»åé¦ˆèŒƒå¼ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ é‡åŒ–LLMsçš„å…ƒè®¤çŸ¥èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLLMsçš„å…ƒè®¤çŸ¥èƒ½åŠ›ä¾èµ–äºå¤šä¸ªå› ç´ ï¼ŒåŒ…æ‹¬æä¾›çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹æ•°é‡ã€ç¥ç»æ¿€æ´»æ–¹å‘çš„è¯­ä¹‰å¯è§£é‡Šæ€§ä»¥åŠè¯¥æ–¹å‘è§£é‡Šçš„æ–¹å·®ã€‚è¿™äº›æ–¹å‘æ„æˆäº†ä¸€ä¸ªâ€œå…ƒè®¤çŸ¥ç©ºé—´â€ï¼Œå…¶ç»´åº¦è¿œä½äºæ¨¡å‹çš„ç¥ç»ç©ºé—´ï¼Œè¡¨æ˜LLMsåªèƒ½ç›‘æ§å…¶ç¥ç»æ¿€æ´»çš„ä¸€ä¸ªå°å­é›†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»»åŠ¡æ‰§è¡Œä¸­å¯¹è‡ªèº«è®¤çŸ¥ç­–ç•¥çš„ç›‘æ§å’ŒæŠ¥å‘Šèƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆé‡åŒ–å’Œç†è§£æ¨¡å‹çš„å…ƒè®¤çŸ¥èƒ½åŠ›ï¼Œå¯¼è‡´å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç¥ç»åé¦ˆçš„èŒƒå¼ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ æ¥å¸®åŠ©æ¨¡å‹è¯†åˆ«å’Œæ§åˆ¶å…¶å†…éƒ¨æ¿€æ´»æ¨¡å¼ï¼Œä»è€Œå¢å¼ºå…¶å…ƒè®¤çŸ¥èƒ½åŠ›ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’ŒæŠ¥å‘Šå…¶å†³ç­–è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®è¾“å…¥ã€ä¸Šä¸‹æ–‡ç¤ºä¾‹æä¾›ã€ç¥ç»æ¿€æ´»ç›‘æ§å’Œåé¦ˆæœºåˆ¶ç­‰ä¸»è¦æ¨¡å—ã€‚æ¨¡å‹é¦–å…ˆæ¥æ”¶ä»»åŠ¡è¾“å…¥ï¼Œå¹¶é€šè¿‡ä¸Šä¸‹æ–‡ç¤ºä¾‹å­¦ä¹ æ¿€æ´»æ¨¡å¼ï¼Œç„¶åè¿›è¡Œè‡ªæˆ‘ç›‘æ§å’Œè°ƒæ•´ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†â€œå…ƒè®¤çŸ¥ç©ºé—´â€çš„æ¦‚å¿µï¼Œè¯¥ç©ºé—´çš„ç»´åº¦è¿œä½äºæ¨¡å‹çš„ç¥ç»ç©ºé—´ï¼Œè¡¨æ˜æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆç›‘æ§çš„æ¿€æ´»æ¨¡å¼æ˜¯æœ‰é™çš„ã€‚è¿™ä¸€å‘ç°ä¸ºç†è§£æ¨¡å‹çš„å†…éƒ¨æœºåˆ¶æä¾›äº†æ–°çš„è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†ä¸åŒæ•°é‡çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼Œå¹¶è¯„ä¼°äº†æ¿€æ´»æ–¹å‘çš„è¯­ä¹‰å¯è§£é‡Šæ€§å’Œè§£é‡Šæ–¹å·®ã€‚è¿™äº›è®¾è®¡å¸®åŠ©é‡åŒ–æ¨¡å‹çš„å…ƒè®¤çŸ¥èƒ½åŠ›ï¼Œå¹¶æ­ç¤ºäº†å…¶åœ¨å®‰å…¨æ€§æ–¹é¢çš„æ½œåœ¨éšæ‚£ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹çš„å…ƒè®¤çŸ¥èƒ½åŠ›ä¸æä¾›çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹æ•°é‡ã€æ¿€æ´»æ–¹å‘çš„å¯è§£é‡Šæ€§åŠå…¶æ–¹å·®å¯†åˆ‡ç›¸å…³ã€‚å…·ä½“è€Œè¨€ï¼Œå¢åŠ ä¸Šä¸‹æ–‡ç¤ºä¾‹æ•°é‡å¯æ˜¾è‘—æå‡æ¨¡å‹çš„ç›‘æ§èƒ½åŠ›ï¼Œä¸”åœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼Œæ¨¡å‹çš„è¡¨ç°æå‡å¹…åº¦å¯è¾¾20%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–å†³ç­–ç³»ç»Ÿå’Œå®‰å…¨ç›‘æ§ç­‰ã€‚é€šè¿‡å¢å¼ºè¯­è¨€æ¨¡å‹çš„å…ƒè®¤çŸ¥èƒ½åŠ›ï¼Œå¯ä»¥æé«˜å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¹¶é™ä½æ¨¡å‹åœ¨å®‰å…¨æ€§æ–¹é¢çš„é£é™©ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) can sometimes report the strategies they actually use to solve tasks, yet at other times seem unable to recognize those strategies that govern their behavior. This suggests a limited degree of metacognition - the capacity to monitor one's own cognitive processes for subsequent reporting and self-control. Metacognition enhances LLMs' capabilities in solving complex tasks but also raises safety concerns, as models may obfuscate their internal processes to evade neural-activation-based oversight (e.g., safety detector). Given society's increased reliance on these models, it is critical that we understand their metacognitive abilities. To address this, we introduce a neuroscience-inspired neurofeedback paradigm that uses in-context learning to quantify metacognitive abilities of LLMs to report and control their activation patterns. We demonstrate that their abilities depend on several factors: the number of in-context examples provided, the semantic interpretability of the neural activation direction (to be reported/controlled), and the variance explained by that direction. These directions span a "metacognitive space" with dimensionality much lower than the model's neural space, suggesting LLMs can monitor only a small subset of their neural activations. Our paradigm provides empirical evidence to quantify metacognition in LLMs, with significant implications for AI safety (e.g., adversarial attack and defense).

