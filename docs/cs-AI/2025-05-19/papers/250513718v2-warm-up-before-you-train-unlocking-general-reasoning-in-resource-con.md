---
layout: default
title: "Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings"
---

# Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13718" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13718v2</a>
  <a href="https://arxiv.org/pdf/2505.13718.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13718v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13718v2', 'Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Safal Shrestha, Minwu Kim, Aadim Nepal, Anubhav Shrestha, Keith Ross

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19 (æ›´æ–°: 2025-05-26)

**DOI**: [10.18653/v1/2025.emnlp-main.727](https://doi.org/10.18653/v1/2025.emnlp-main.727)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒé˜¶æ®µè®­ç»ƒç­–ç•¥ä»¥è§£å†³æ•°æ®ç¨€ç¼ºä¸‹çš„æ¨ç†èƒ½åŠ›é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨ç†èƒ½åŠ›` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `é•¿é“¾æ€ç»´` `æ ·æœ¬æ•ˆç‡` `æ•°æ®ç¨€ç¼º` `åŒé˜¶æ®µè®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨ç†èƒ½åŠ›æ¨¡å‹è®­ç»ƒæ–¹æ³•ä¾èµ–å¤§é‡é«˜è´¨é‡æ•°æ®ï¼Œå¯¼è‡´åœ¨æ•°æ®ç¨€ç¼ºç¯å¢ƒä¸­é¢ä¸´æŒ‘æˆ˜ã€‚
2. æå‡ºçš„åŒé˜¶æ®µè®­ç»ƒç­–ç•¥é€šè¿‡å…ˆåœ¨ç©å…·é¢†åŸŸè¿›è¡Œçƒ­èº«ï¼Œå†åœ¨ç›®æ ‡é¢†åŸŸè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œæå‡æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œçƒ­èº«é˜¶æ®µæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œä¸”åœ¨å°æ•°æ®é›†ä¸Šçƒ­èº«æ¨¡å‹ä¼˜äºåŸºç¡€æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¾è®¡æœ‰æ•ˆçš„æ¨ç†èƒ½åŠ›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸éœ€è¦ä½¿ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æˆ–ç»è¿‡ç²¾å¿ƒç­–åˆ’çš„é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰è’¸é¦ï¼Œè¿™ä¸¤è€…éƒ½ä¾èµ–å¤§é‡çš„è®­ç»ƒæ•°æ®ã€‚å½“é«˜è´¨é‡è®­ç»ƒæ•°æ®ç¨€ç¼ºæ—¶ï¼Œè¿™å°±æˆä¸ºä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ ·æœ¬é«˜æ•ˆçš„åŒé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä»¥åœ¨æœ‰é™ç›‘ç£ä¸‹å¼€å‘æ¨ç†LLMsã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡ä»ç©å…·é¢†åŸŸï¼ˆéª‘å£«ä¸éª—å­é€»è¾‘è°œé¢˜ï¼‰è’¸é¦é•¿é“¾æ€ç»´æ¥â€œçƒ­èº«â€æ¨¡å‹ï¼Œä»¥è·å–ä¸€èˆ¬æ¨ç†æŠ€èƒ½ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨æœ‰é™çš„ç›®æ ‡é¢†åŸŸç¤ºä¾‹å¯¹çƒ­èº«åçš„æ¨¡å‹åº”ç”¨RLVRã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§ä¸¤é˜¶æ®µæ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨æ•°æ®ç¨€ç¼ºç¯å¢ƒä¸‹ï¼Œå¦‚ä½•æœ‰æ•ˆè®­ç»ƒå…·æœ‰æ¨ç†èƒ½åŠ›çš„LLMsçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚RLVRå’ŒCoTè’¸é¦ä¾èµ–å¤§é‡é«˜è´¨é‡æ•°æ®ï¼Œéš¾ä»¥é€‚åº”æ•°æ®ç¨€ç¼ºçš„åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„åŒé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé¦–å…ˆé€šè¿‡ç©å…·é¢†åŸŸçš„é•¿é“¾æ€ç»´è’¸é¦è¿›è¡Œæ¨¡å‹çƒ­èº«ï¼Œè·å–ä¸€èˆ¬æ¨ç†æŠ€èƒ½ï¼Œéšååœ¨ç›®æ ‡é¢†åŸŸè¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œä»¥æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œæ ·æœ¬æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µä¸ºçƒ­èº«é˜¶æ®µï¼Œé€šè¿‡éª‘å£«ä¸éª—å­é€»è¾‘è°œé¢˜è¿›è¡Œé•¿é“¾æ€ç»´çš„è’¸é¦ï¼›ç¬¬äºŒé˜¶æ®µä¸ºå¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œä½¿ç”¨æœ‰é™çš„ç›®æ ‡é¢†åŸŸç¤ºä¾‹è¿›è¡ŒRLVRè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥çƒ­èº«é˜¶æ®µï¼Œä½¿å¾—æ¨¡å‹åœ¨è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒä¹‹å‰ï¼Œèƒ½å¤Ÿå…ˆè·å¾—ä¸€èˆ¬æ¨ç†èƒ½åŠ›ï¼Œä»è€Œæå‡è·¨é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§è®¾è®¡æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨å°æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨çƒ­èº«é˜¶æ®µï¼Œä½¿ç”¨éª‘å£«ä¸éª—å­é€»è¾‘è°œé¢˜è¿›è¡Œé•¿é“¾æ€ç»´è’¸é¦ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°åŸºæœ¬çš„æ¨ç†è§„åˆ™ï¼›åœ¨å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œé‡‡ç”¨æœ‰é™çš„ç›®æ ‡é¢†åŸŸç¤ºä¾‹è¿›è¡Œè®­ç»ƒï¼Œä¼˜åŒ–æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œæ ·æœ¬æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œçƒ­èº«é˜¶æ®µæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨MATHã€HumanEval$^{+}$å’ŒMMLU-Proç­‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚çƒ­èº«æ¨¡å‹åœ¨ç›¸åŒå°æ•°æ®é›†ï¼ˆâ‰¤100ä¸ªç¤ºä¾‹ï¼‰ä¸Šå§‹ç»ˆä¼˜äºåŸºç¡€æ¨¡å‹ï¼Œä¸”åœ¨RLVRè®­ç»ƒä¸­æé«˜äº†æ•´ä½“æ ·æœ¬æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€æ¸¸æˆè®¾è®¡å’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ï¼Œèƒ½å¤Ÿåœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´å¹¿æ³›çš„æ¨ç†èƒ½åŠ›æ¨¡å‹çš„å¼€å‘ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Designing effective reasoning-capable LLMs typically requires training using Reinforcement Learning with Verifiable Rewards (RLVR) or distillation with carefully curated Long Chain of Thoughts (CoT), both of which depend heavily on extensive training data. This creates a major challenge when the amount of quality training data is scarce. We propose a sample-efficient, two-stage training strategy to develop reasoning LLMs under limited supervision. In the first stage, we "warm up" the model by distilling Long CoTs from a toy domain, namely, Knights \& Knaves (K\&K) logic puzzles to acquire general reasoning skills. In the second stage, we apply RLVR to the warmed-up model using a limited set of target-domain examples. Our experiments demonstrate that this two-phase approach offers several benefits: $(i)$ the warmup phase alone facilitates generalized reasoning, leading to performance improvements across a range of tasks, including MATH, HumanEval$^{+}$, and MMLU-Pro; $(ii)$ When both the base model and the warmed-up model are RLVR trained on the same small dataset ($\leq100$ examples), the warmed-up model consistently outperforms the base model; $(iii)$ Warming up before RLVR training allows a model to maintain cross-domain generalizability even after training on a specific domain; $(iv)$ Introducing warmup in the pipeline improves not only accuracy but also overall sample efficiency during RLVR training. The results in this paper highlight the promise of warmup for building robust reasoning LLMs in data-scarce environments.

