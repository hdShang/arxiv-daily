---
layout: default
title: Safety Alignment Can Be Not Superficial With Explicit Safety Signals
---

# Safety Alignment Can Be Not Superficial With Explicit Safety Signals

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.17072" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.17072v2</a>
  <a href="https://arxiv.org/pdf/2505.17072.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.17072v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.17072v2', 'Safety Alignment Can Be Not Superficial With Explicit Safety Signals')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jianwei Li, Jung-Eun Kim

**åˆ†ç±»**: cs.CR, cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19 (æ›´æ–°: 2025-05-30)

**å¤‡æ³¨**: ICML 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡æ˜¾å¼å®‰å…¨ä¿¡å·æå‡å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨å¯¹é½èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å®‰å…¨å¯¹é½` `å¤§è¯­è¨€æ¨¡å‹` `å¯¹æŠ—æ€§æ”»å‡»` `ç”ŸæˆAI` `æ˜¾å¼ä¿¡å·`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹å®‰å…¨å¯¹é½æ–¹æ³•å¾€å¾€è¡¨é¢åŒ–ï¼Œæ— æ³•æœ‰æ•ˆæŠµå¾¡å¯¹æŠ—æ€§æ”»å‡»ã€‚
2. æœ¬æ–‡é€šè¿‡å¼•å…¥æ˜¾å¼çš„å®‰å…¨äºŒå…ƒåˆ†ç±»ä»»åŠ¡ï¼Œç»“åˆæ³¨æ„åŠ›å’Œè§£ç ç­–ç•¥ï¼Œæå‡æ¨¡å‹çš„å®‰å…¨å“åº”èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å¯¹æŠ—æ€§æ”»å‡»ä¸‹æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„é²æ£’æ€§ï¼Œå…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘æœŸå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å®‰å…¨å¯¹é½çš„ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€è¡¨é¢åŒ–ï¼Œå¯¼è‡´æ¨¡å‹æ˜“å—å„ç§å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ã€‚å°½ç®¡è¿™äº›ç ”ç©¶å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä½†é€šå¸¸æœªèƒ½æä¾›è¶…è¶Šæ•°æ®å¢å¼ºçš„å¯è¡Œè§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡è¯†åˆ«å‡ºè¿™ç§è¡¨é¢åŒ–çš„æ ¹æœ¬åŸå› ï¼šç°æœ‰å¯¹é½æ–¹æ³•å‡è®¾æ¨¡å‹èƒ½å¤Ÿåœ¨å¯¹é½è¿‡ç¨‹ä¸­éšå¼å­¦ä¹ å®‰å…¨ç›¸å…³çš„æ¨ç†ä»»åŠ¡ï¼Œä»è€Œæ‹’ç»æœ‰å®³è¯·æ±‚ã€‚ç„¶è€Œï¼Œå­¦ä¹ åˆ°çš„å®‰å…¨ä¿¡å·å¸¸å¸¸è¢«å…¶ä»–ç«äº‰ç›®æ ‡ç¨€é‡Šï¼Œå¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹å¯¹æŠ—æ€§æ”»å‡»æ—¶éš¾ä»¥åˆ’å®šæ˜ç¡®çš„å®‰å…¨å†³ç­–è¾¹ç•Œã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œæœ¬æ–‡é€šè¿‡æ˜¾å¼å¼•å…¥å®‰å…¨ç›¸å…³çš„äºŒå…ƒåˆ†ç±»ä»»åŠ¡ï¼Œå¹¶å°†å…¶ä¿¡å·ä¸æ³¨æ„åŠ›å’Œè§£ç ç­–ç•¥ç›¸ç»“åˆï¼Œæ¶ˆé™¤äº†è¿™ç§æ¨¡ç³Šæ€§ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´è´Ÿè´£ä»»åœ°å“åº”æ¶æ„æŸ¥è¯¢ã€‚å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•æ˜¾è‘—æé«˜äº†LLMså¯¹å„ç§å¯¹æŠ—æ€§æ”»å‡»çš„æŠµæŠ—åŠ›ï¼Œä¸ºæ›´å¼ºå¤§çš„ç”ŸæˆAIç³»ç»Ÿæä¾›äº†æœ‰å¸Œæœ›çš„è·¯å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨å®‰å…¨å¯¹é½æ–¹é¢çš„è¡¨é¢åŒ–é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å‡è®¾æ¨¡å‹èƒ½å¤Ÿéšå¼å­¦ä¹ å®‰å…¨æ¨ç†ä»»åŠ¡ï¼Œå¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹å¯¹æŠ—æ€§æ”»å‡»æ—¶å†³ç­–è¾¹ç•Œæ¨¡ç³Šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ˜¾å¼å¼•å…¥å®‰å…¨ç›¸å…³çš„äºŒå…ƒåˆ†ç±»ä»»åŠ¡ï¼Œä½¿æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­èƒ½å¤Ÿæ˜ç¡®è¯„ä¼°æŸ¥è¯¢å’Œå…ˆå‰ç”Ÿæˆçš„æ ‡è®°çš„å®‰å…¨æ€§ï¼Œä»è€Œæå‡å…¶å¯¹æ¶æ„è¯·æ±‚çš„å“åº”èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¼•å…¥å®‰å…¨ä¿¡å·çš„äºŒå…ƒåˆ†ç±»æ¨¡å—ã€æ³¨æ„åŠ›æœºåˆ¶å’Œè§£ç ç­–ç•¥ã€‚æ¨¡å‹åœ¨æ¯ä¸ªç”Ÿæˆæ­¥éª¤ä¸­è¯„ä¼°å®‰å…¨æ€§ï¼Œç¡®ä¿ç”Ÿæˆå†…å®¹çš„å®‰å…¨æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæ˜¾å¼å®‰å…¨ä¿¡å·çš„å¼•å…¥ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„éšå¼å­¦ä¹ æœºåˆ¶å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å®‰å…¨å†³ç­–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæœ¬æ–‡è®¾ç½®äº†ä½äº0.2å€çš„å¼€é”€æˆæœ¬ï¼Œç¡®ä¿æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­èƒ½å¤Ÿå®æ—¶è¯„ä¼°å®‰å…¨æ€§ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å®‰å…¨ä¿¡å·çš„å­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨å¯¹æŠ—æ€§æ”»å‡»ä¸‹æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„é²æ£’æ€§ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œå®‰å…¨æ€§è¯„ä¼°çš„å‡†ç¡®ç‡æå‡äº†20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¯¹è¯ç³»ç»Ÿã€å†…å®¹ç”Ÿæˆå’Œè‡ªåŠ¨åŒ–å®¢æœç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡ç”ŸæˆAIç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„AIåº”ç”¨ä¸­å¾—åˆ°æ¨å¹¿ï¼Œå¢å¼ºç”¨æˆ·ä¿¡ä»»å’Œå®‰å…¨æ„Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent studies on the safety alignment of large language models (LLMs) have revealed that existing approaches often operate superficially, leaving models vulnerable to various adversarial attacks. Despite their significance, these studies generally fail to offer actionable solutions beyond data augmentation for achieving more robust safety mechanisms. This paper identifies a fundamental cause of this superficiality: existing alignment approaches often presume that models can implicitly learn a safety-related reasoning task during the alignment process, enabling them to refuse harmful requests. However, the learned safety signals are often diluted by other competing objectives, leading models to struggle with drawing a firm safety-conscious decision boundary when confronted with adversarial attacks. Based on this observation, by explicitly introducing a safety-related binary classification task and integrating its signals with our attention and decoding strategies, we eliminate this ambiguity and allow models to respond more responsibly to malicious queries. We emphasize that, with less than 0.2x overhead cost, our approach enables LLMs to assess the safety of both the query and the previously generated tokens at each necessary generating step. Extensive experiments demonstrate that our method significantly improves the resilience of LLMs against various adversarial attacks, offering a promising pathway toward more robust generative AI systems.

