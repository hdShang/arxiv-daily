---
layout: default
title: "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards"
---

# Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13445" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.13445v1</a>
  <a href="https://arxiv.org/pdf/2505.13445.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13445v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13445v1', 'Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Xiaoyuan Liu, Tian Liang, Zhiwei He, Jiahao Xu, Wenxuan Wang, Pinjia He, Zhaopeng Tu, Haitao Mi, Dong Yu

**ÂàÜÁ±ª**: cs.AI, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-19

**Â§áÊ≥®**: code available at https://github.com/xyliu-cs/RISE

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫RISEÊ°ÜÊû∂‰ª•Ëß£ÂÜ≥Âº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑËá™ÊàëÈ™åËØÅÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `Ëá™ÊàëÈ™åËØÅ` `ÂèØÈ™åËØÅÂ•ñÂä±` `Êï∞Â≠¶Êé®ÁêÜ` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Âú®Á∫øÂ≠¶‰π†` `Ê®°ÂûãËÆ≠ÁªÉ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂú®Ëá™ÊàëÈ™åËØÅÊñπÈù¢Â≠òÂú®‰∏çË∂≥ÔºåÊ®°ÂûãÊó†Ê≥ïÊúâÊïàÂú∞È™åËØÅÂÖ∂ËæìÂá∫ÁöÑÊ≠£Á°ÆÊÄß„ÄÇ
2. RISEÊ°ÜÊû∂ÈÄöËøáÂêåÊó∂ËÆ≠ÁªÉÊ®°ÂûãÁöÑÈóÆÈ¢òËß£ÂÜ≥ÂíåËá™ÊàëÈ™åËØÅËÉΩÂäõÔºåÂà©Áî®ÂèØÈ™åËØÅÂ•ñÂä±Êèê‰æõÂÆûÊó∂ÂèçÈ¶à„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåRISEÂú®Â§ö‰∏™Êï∞Â≠¶Êé®ÁêÜÂü∫ÂáÜ‰∏äÊòæËëóÊèêÈ´ò‰∫ÜËß£ÂÜ≥ÂáÜÁ°ÆÊÄßÔºåÂπ∂Â¢ûÂº∫‰∫ÜËá™ÊàëÈ™åËØÅË°å‰∏∫„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§çÊùÇÊé®ÁêÜ‰∏≠Â±ïÁé∞Âá∫Â∑®Â§ßÊΩúÂäõÔºåËÄåÂèØÈ™åËØÅÂ•ñÂä±ÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLVRÔºâÊòØÂÖ∂ÂÖ≥ÈîÆÂ¢ûÂº∫Á≠ñÁï•„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ïÊôÆÈÅçÂ≠òÂú®‚ÄúË°®Èù¢Ëá™ÊàëÂèçÊÄù‚ÄùÁöÑÈóÆÈ¢òÔºåÊ®°ÂûãÊú™ËÉΩÊúâÊïàÈ™åËØÅËá™Ë∫´ËæìÂá∫„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫RISEÔºàÈÄöËøáËá™ÊàëÈ™åËØÅÂº∫ÂåñÊé®ÁêÜÔºâÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÂú®Á∫øÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÊó®Âú®ÂêåÊó∂ÊèêÂçáÊ®°ÂûãÁöÑÈóÆÈ¢òËß£ÂÜ≥ËÉΩÂäõÂíåËá™ÊàëÈ™åËØÅËÉΩÂäõ„ÄÇRISEÈÄöËøáÁªìÊûúÈ™åËØÅÂô®Êèê‰æõÂèØÈ™åËØÅÂ•ñÂä±ÔºåÂÆûÊó∂ÂèçÈ¶àËß£ÂÜ≥ÊñπÊ°àÁîüÊàêÂíåËá™ÊàëÈ™åËØÅ‰ªªÂä°„ÄÇÂú®ÊØèÊ¨°Ëø≠‰ª£‰∏≠ÔºåÊ®°ÂûãÁîüÊàêËß£ÂÜ≥ÊñπÊ°àÂπ∂Ëá™ÊàëÊâπËØÑÔºå‰∏§‰∏™ËΩ®ËøπÂÖ±Âêå‰øÉËøõÁ≠ñÁï•Êõ¥Êñ∞„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåRISEÂú®Êï∞Â≠¶Êé®ÁêÜÂü∫ÂáÜ‰∏äÊåÅÁª≠ÊèêÈ´òÊ®°ÂûãÁöÑËß£ÂÜ≥ÂáÜÁ°ÆÊÄßÔºåÂêåÊó∂Â¢ûÂº∫Ëá™ÊàëÈ™åËØÅËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ï‰∏≠Ê®°ÂûãËá™ÊàëÈ™åËØÅËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÂ∞§ÂÖ∂ÊòØ‚ÄúË°®Èù¢Ëá™ÊàëÂèçÊÄù‚ÄùÁé∞Ë±°ÔºåÂØºËá¥Ê®°ÂûãÊó†Ê≥ïÊúâÊïàÁ°ÆËÆ§ÂÖ∂ËæìÂá∫ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöRISEÊ°ÜÊû∂ÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÈÄöËøáÂú®Á∫øÂº∫ÂåñÂ≠¶‰π†ËøáÁ®ãÔºåÁªìÂêàÈóÆÈ¢òËß£ÂÜ≥ÂíåËá™ÊàëÈ™åËØÅ‰ªªÂä°ÁöÑËÆ≠ÁªÉÔºåÂà©Áî®ÂèØÈ™åËØÅÂ•ñÂä±Êèê‰æõÂÆûÊó∂ÂèçÈ¶àÔºå‰ªéËÄåÊèêÂçáÊ®°ÂûãÁöÑÊï¥‰ΩìËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöRISEÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöËß£ÂÜ≥ÊñπÊ°àÁîüÊàêÊ®°ÂùóÂíåËá™ÊàëÈ™åËØÅÊ®°Âùó„ÄÇÂú®ÊØèÊ¨°Ëø≠‰ª£‰∏≠ÔºåÊ®°ÂûãÈ¶ñÂÖàÁîüÊàêËß£ÂÜ≥ÊñπÊ°àÔºåÁÑ∂ÂêéÂØπËá™Ë∫´ÁîüÊàêÁöÑËß£ÂÜ≥ÊñπÊ°àËøõË°åÊâπËØÑÔºå‰∏§‰∏™ËøáÁ®ãÁöÑÁªìÊûúÂÖ±ÂêåÁî®‰∫éÊõ¥Êñ∞Á≠ñÁï•„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöRISEÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂ∞ÜËá™ÊàëÈ™åËØÅ‰∏éÈóÆÈ¢òËß£ÂÜ≥ËÉΩÂäõÁöÑËÆ≠ÁªÉÊï¥ÂêàÂú®‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂‰∏≠ÔºåÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑËá™ÊàëÂèçÊÄùËÉΩÂäõÔºå‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºåÂÖ∑ÊúâÊõ¥È´òÁöÑÁÅµÊ¥ªÊÄßÂíåÊúâÊïàÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®RISE‰∏≠ÔºåÂÖ≥ÈîÆÂèÇÊï∞ËÆæÁΩÆÂåÖÊã¨ÂèØÈ™åËØÅÂ•ñÂä±ÁöÑËÆæËÆ°ÂíåÊçüÂ§±ÂáΩÊï∞ÁöÑÈÄâÊã©ÔºåÁ°Æ‰øùÊ®°ÂûãÂú®Ëá™ÊàëÈ™åËØÅËøáÁ®ã‰∏≠ËÉΩÂ§üËé∑ÂæóÊúâÊïàÁöÑÂèçÈ¶à„ÄÇÊ≠§Â§ñÔºåÁΩëÁªúÁªìÊûÑËÆæËÆ°‰∏äÔºåÊ®°ÂûãËÉΩÂ§üÂêåÊó∂Â§ÑÁêÜÁîüÊàêÂíåÈ™åËØÅ‰ªªÂä°Ôºå‰ºòÂåñÊï¥‰ΩìÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®Â§ö‰∏™Êï∞Â≠¶Êé®ÁêÜÂü∫ÂáÜ‰∏äÔºåRISEÊ®°ÂûãÁöÑËß£ÂÜ≥ÂáÜÁ°ÆÊÄßÊòæËëóÊèêÈ´òÔºåÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÁõ∏ËæÉ‰∫éÂü∫Á∫øÊ®°ÂûãÔºåRISEÂú®Ëá™ÊàëÈ™åËØÅË°å‰∏∫‰∏äË°®Áé∞Âá∫Êõ¥È´òÁöÑÈ¢ëÁéáÂíåÂáÜÁ°ÆÊÄßÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄßÂíå‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

RISEÊ°ÜÊû∂ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÁâπÂà´ÊòØÂú®ÈúÄË¶ÅÈ´òÂáÜÁ°ÆÊÄßÂíåËá™ÊàëÈ™åËØÅËÉΩÂäõÁöÑÂ§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠ÔºåÂ¶ÇËá™Âä®ÂåñÂÜ≥Á≠ñÁ≥ªÁªü„ÄÅÊô∫ËÉΩÂä©ÊâãÂíåÊïôËÇ≤ÊäÄÊúØÁ≠âÈ¢ÜÂüü„ÄÇÊú™Êù•ÔºåRISEÂèØËÉΩÊé®Âä®Êõ¥Êô∫ËÉΩÁöÑËá™ÊàëÊÑèËØÜÊé®ÁêÜÁ≥ªÁªüÁöÑÂèëÂ±ïÔºåÊèêÂçá‰∫∫Â∑•Êô∫ËÉΩÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂèØÈù†ÊÄßÂíåÊúâÊïàÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large Language Models (LLMs) show great promise in complex reasoning, with Reinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement strategy. However, a prevalent issue is ``superficial self-reflection'', where models fail to robustly verify their own outputs. We introduce RISE (Reinforcing Reasoning with Self-Verification), a novel online RL framework designed to tackle this. RISE explicitly and simultaneously trains an LLM to improve both its problem-solving and self-verification abilities within a single, integrated RL process. The core mechanism involves leveraging verifiable rewards from an outcome verifier to provide on-the-fly feedback for both solution generation and self-verification tasks. In each iteration, the model generates solutions, then critiques its own on-policy generated solutions, with both trajectories contributing to the policy update. Extensive experiments on diverse mathematical reasoning benchmarks show that RISE consistently improves model's problem-solving accuracy while concurrently fostering strong self-verification skills. Our analyses highlight the advantages of online verification and the benefits of increased verification compute. Additionally, RISE models exhibit more frequent and accurate self-verification behaviors during reasoning. These advantages reinforce RISE as a flexible and effective path towards developing more robust and self-aware reasoners.

