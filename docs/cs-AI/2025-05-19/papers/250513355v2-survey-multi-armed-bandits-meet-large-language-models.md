---
layout: default
title: "Survey: Multi-Armed Bandits Meet Large Language Models"
---

# Survey: Multi-Armed Bandits Meet Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13355" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13355v2</a>
  <a href="https://arxiv.org/pdf/2505.13355.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13355v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13355v2', 'Survey: Multi-Armed Bandits Meet Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Djallel Bouneffouf, Raphael Feraud

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19 (æ›´æ–°: 2025-09-30)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢è®¨å¤šè‡‚è€è™æœºç®—æ³•ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„ååŒæ½œåŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè‡‚è€è™æœºç®—æ³•` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `å†³ç­–ä¼˜åŒ–` `æ¢ç´¢ä¸åˆ©ç”¨` `è‡ªé€‚åº”å­¦ä¹ ` `æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šè‡‚è€è™æœºç®—æ³•ä¸å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹é—´ç¼ºä¹æœ‰æ•ˆçš„ç»“åˆï¼Œå¯¼è‡´å„è‡ªçš„æ½œåŠ›æœªèƒ½å……åˆ†å‘æŒ¥ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å¤šè‡‚è€è™æœºç®—æ³•ä¼˜åŒ–LLMsçš„å¾®è°ƒå’Œå“åº”ç”Ÿæˆï¼Œåˆ©ç”¨æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡æ¥æå‡æ€§èƒ½ã€‚
3. ç ”ç©¶è¡¨æ˜ï¼Œç»“åˆè¿™ä¸¤ç§æŠ€æœ¯å¯ä»¥æ˜¾è‘—æ”¹å–„å†³ç­–è¿‡ç¨‹å’Œè‡ªç„¶è¯­è¨€å¤„ç†çš„æ•ˆæœï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šè‡‚è€è™æœºç®—æ³•å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸä¸­å„è‡ªè§£å†³ä¸åŒä½†äº’è¡¥çš„å†³ç­–å’Œè‡ªç„¶è¯­è¨€å¤„ç†æŒ‘æˆ˜ã€‚æœ¬æ–‡ç»¼è¿°äº†è¿™ä¸¤ä¸ªé¢†åŸŸä¹‹é—´çš„ååŒæ½œåŠ›ï¼Œå¼ºè°ƒäº†å¤šè‡‚è€è™æœºç®—æ³•å¦‚ä½•æå‡LLMsçš„æ€§èƒ½ï¼Œä»¥åŠLLMså¦‚ä½•ä¸ºå¤šè‡‚è€è™æœºå†³ç­–æä¾›æ–°è§è§£ã€‚æˆ‘ä»¬é¦–å…ˆè€ƒå¯Ÿäº†å¤šè‡‚è€è™æœºç®—æ³•åœ¨ä¼˜åŒ–LLMså¾®è°ƒã€æç¤ºå·¥ç¨‹å’Œè‡ªé€‚åº”å“åº”ç”Ÿæˆä¸­çš„ä½œç”¨ï¼Œé‡ç‚¹å…³æ³¨å…¶åœ¨å¤§è§„æ¨¡å­¦ä¹ ä»»åŠ¡ä¸­å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨çš„èƒ½åŠ›ã€‚éšåï¼Œæˆ‘ä»¬æ¢è®¨äº†LLMså¦‚ä½•é€šè¿‡é«˜çº§ä¸Šä¸‹æ–‡ç†è§£ã€åŠ¨æ€é€‚åº”å’Œè‡ªç„¶è¯­è¨€æ¨ç†æ”¹å–„å¤šè‡‚è€è™æœºç®—æ³•çš„ç­–ç•¥é€‰æ‹©ã€‚é€šè¿‡å…¨é¢å›é¡¾ç°æœ‰ç ”ç©¶å¹¶è¯†åˆ«å…³é”®æŒ‘æˆ˜ä¸æœºé‡ï¼Œæœ¬æ–‡æ—¨åœ¨å¼¥åˆå¤šè‡‚è€è™æœºç®—æ³•ä¸LLMsä¹‹é—´çš„å·®è·ï¼Œä¸ºäººå·¥æ™ºèƒ½ä¸­çš„åˆ›æ–°åº”ç”¨å’Œè·¨å­¦ç§‘ç ”ç©¶é“ºå¹³é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šè‡‚è€è™æœºç®—æ³•ä¸å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹é—´çš„ååŒåº”ç”¨é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å†³ç­–ä¼˜åŒ–å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä¸Šå„è‡ªç‹¬ç«‹ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨å½¼æ­¤çš„ä¼˜åŠ¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ¢ç´¢å¤šè‡‚è€è™æœºç®—æ³•å¦‚ä½•é€šè¿‡ä¼˜åŒ–LLMsçš„å¾®è°ƒå’Œå“åº”ç”Ÿæˆè¿‡ç¨‹ï¼Œæå‡å…¶åœ¨å¤§è§„æ¨¡å­¦ä¹ ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œèƒ½å¤Ÿå®ç°æ›´æœ‰æ•ˆçš„æ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯å¤šè‡‚è€è™æœºç®—æ³•åœ¨LLMså¾®è°ƒä¸­çš„åº”ç”¨ï¼ŒäºŒæ˜¯LLMså¯¹å¤šè‡‚è€è™æœºç®—æ³•çš„å¢å¼ºã€‚å‰è€…å…³æ³¨å¦‚ä½•ä¼˜åŒ–æ¨¡å‹å‚æ•°ï¼Œåè€…åˆ™åˆ©ç”¨è‡ªç„¶è¯­è¨€æ¨ç†æ”¹å–„ç­–ç•¥é€‰æ‹©ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†å¤šè‡‚è€è™æœºç®—æ³•ä¸LLMsç»“åˆï¼Œå½¢æˆä¸€ä¸ªäº’è¡¥çš„ç³»ç»Ÿï¼Œèƒ½å¤Ÿåœ¨å†³ç­–è¿‡ç¨‹ä¸­åŠ¨æ€é€‚åº”å¹¶ä¼˜åŒ–ç­–ç•¥é€‰æ‹©ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºåŒå‘çš„ååŒä½œç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬å¤šè‡‚è€è™æœºç®—æ³•çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°çš„é€‰æ‹©ï¼Œä»¥åŠLLMsçš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›çš„å¢å¼ºã€‚è¿™äº›è®¾è®¡ç¡®ä¿äº†ç³»ç»Ÿåœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶çš„çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»“åˆå¤šè‡‚è€è™æœºç®—æ³•ä¸å¤§å‹è¯­è¨€æ¨¡å‹åï¼Œæ¨¡å‹åœ¨å¤šä¸ªè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡æ˜¾è‘—ï¼Œå°¤å…¶æ˜¯åœ¨å“åº”ç”Ÿæˆå’Œç­–ç•¥é€‰æ‹©æ–¹é¢ï¼Œæå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å¯¹è¯ç³»ç»Ÿã€ä¸ªæ€§åŒ–æ¨èå’Œè‡ªé€‚åº”å­¦ä¹ ç­‰ã€‚é€šè¿‡ç»“åˆå¤šè‡‚è€è™æœºç®—æ³•ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å†³ç­–åˆ¶å®šå’Œè‡ªç„¶è¯­è¨€å¤„ç†ä¸Šå®ç°æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Bandit algorithms and Large Language Models (LLMs) have emerged as powerful tools in artificial intelligence, each addressing distinct yet complementary challenges in decision-making and natural language processing. This survey explores the synergistic potential between these two fields, highlighting how bandit algorithms can enhance the performance of LLMs and how LLMs, in turn, can provide novel insights for improving bandit-based decision-making. We first examine the role of bandit algorithms in optimizing LLM fine-tuning, prompt engineering, and adaptive response generation, focusing on their ability to balance exploration and exploitation in large-scale learning tasks. Subsequently, we explore how LLMs can augment bandit algorithms through advanced contextual understanding, dynamic adaptation, and improved policy selection using natural language reasoning. By providing a comprehensive review of existing research and identifying key challenges and opportunities, this survey aims to bridge the gap between bandit algorithms and LLMs, paving the way for innovative applications and interdisciplinary research in AI.

