---
layout: default
title: CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code
---

# CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.01485" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.01485v1</a>
  <a href="https://arxiv.org/pdf/2505.01485.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.01485v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.01485v1', 'CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Tasnim Ahmed, Salimur Choudhury

**ÂàÜÁ±ª**: cs.AI, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-02

**Â§áÊ≥®**: This paper has been accepted for presentation at the 19th Learning and Intelligent Optimization Conference (LION 19)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫CHORUSÊ°ÜÊû∂‰ª•Ëß£ÂÜ≥Á∫øÊÄßËßÑÂàí‰ª£Á†ÅÁîüÊàêÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Á∫øÊÄßËßÑÂàí` `‰ª£Á†ÅÁîüÊàê` `Â¢ûÂº∫Ê£ÄÁ¥¢ÁîüÊàê` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Â±ÇÊ¨°ÂåñÂàÜÂùó` `Ëá™Âä®Âåñ‰ºòÂåñ` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®ÁîüÊàêÁ∫øÊÄßËßÑÂàí‰ª£Á†ÅÊó∂ÔºåÂæÄÂæÄÈúÄË¶ÅËæÉÈ´òÁöÑÈ¢ÜÂüüÁü•ËØÜÂíåÁºñÁ®ãËÉΩÂäõÔºåÈôêÂà∂‰∫ÜÈùû‰∏ìÂÆ∂ÁöÑ‰ΩøÁî®„ÄÇ
2. CHORUSÊ°ÜÊû∂ÈÄöËøáÂ±ÇÊ¨°ÂåñÁöÑÂàÜÂùóÁ≠ñÁï•ÂíåÂ¢ûÂº∫Ê£ÄÁ¥¢ÁîüÊàêÊäÄÊúØÔºåËÉΩÂ§ü‰ªéËá™ÁÑ∂ËØ≠Ë®ÄÈóÆÈ¢ò‰∏≠Ëá™Âä®ÁîüÊàêLP‰ª£Á†Å„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCHORUSÂú®Â§ö‰∏™ÂºÄÊ∫êLLMs‰∏äÊòæËëóÊèêÂçá‰∫Ü‰ª£Á†ÅÁîüÊàêÊÄßËÉΩÔºåÁîöËá≥Ë∂ÖË∂ä‰∫ÜÊõ¥Âº∫ÁöÑÂü∫Á∫øÊ®°ÂûãÂ¶ÇGPT-3.5ÂíåGPT-4„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Á∫øÊÄßËßÑÂàíÔºàLPÔºâÈóÆÈ¢òÊó®Âú®Âú®Á∫¶ÊùüÊù°‰ª∂‰∏ãÂØªÊâæÁõÆÊ†áÁöÑÊúÄ‰ºòËß£„ÄÇËøô‰∫õÈóÆÈ¢òÈÄöÂ∏∏ÈúÄË¶ÅÈ¢ÜÂüüÁü•ËØÜ„ÄÅÊï∞Â≠¶ÊäÄËÉΩÂíåÁºñÁ®ãËÉΩÂäõÔºåÂØπÈùû‰∏ìÂÆ∂ÊûÑÊàêÈáçÂ§ßÊåëÊàò„ÄÇÊú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÁîüÊàêÁâπÂÆöÊ±ÇËß£Âô®LP‰ª£Á†ÅÊñπÈù¢ÁöÑÊïàÁéá„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜCHORUSÔºå‰∏Ä‰∏™Â¢ûÂº∫Ê£ÄÁ¥¢ÁîüÊàêÔºàRAGÔºâÊ°ÜÊû∂ÔºåÁî®‰∫é‰ªéËá™ÁÑ∂ËØ≠Ë®ÄÈóÆÈ¢òÈôàËø∞ÂêàÊàêÂü∫‰∫éGurobiÁöÑLP‰ª£Á†Å„ÄÇCHORUSÈááÁî®‰∫ÜÂ±ÇÊ¨°Ê†ëÁä∂ÂàÜÂùóÁ≠ñÁï•ÔºåÂπ∂ÁîüÊàêÂü∫‰∫éÊñáÊ°£‰ª£Á†ÅÁ§∫‰æãÁöÑÈôÑÂä†ÂÖÉÊï∞ÊçÆÔºå‰ª•‰øÉËøõËá™ÂåÖÂê´„ÄÅËØ≠‰πâËøûË¥ØÁöÑÊ£ÄÁ¥¢„ÄÇCHORUSÁöÑ‰∏§Èò∂ÊÆµÊ£ÄÁ¥¢ÊñπÊ≥ïÂíå‰∫§ÂèâÁºñÁ†ÅÂô®ÈáçÊéíÂ∫èËøõ‰∏ÄÊ≠•Á°Æ‰øù‰∫Ü‰∏ä‰∏ãÊñáÁõ∏ÂÖ≥ÊÄß„ÄÇÁªèËøáÂÆûÈ™åÈ™åËØÅÔºåCHORUSÊòæËëóÊèêÂçá‰∫ÜÂºÄÊ∫êLLMsÁöÑ‰ª£Á†ÅÁîüÊàêÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Á∫øÊÄßËßÑÂàí‰ª£Á†ÅÁîüÊàê‰∏≠ÁöÑÊïàÁéáÂíåÂáÜÁ°ÆÊÄßÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫é‰∏ìÂÆ∂Áü•ËØÜÔºåÈöæ‰ª•Êª°Ë∂≥Èùû‰∏ìÂÆ∂Áî®Êà∑ÁöÑÈúÄÊ±Ç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöCHORUSÊ°ÜÊû∂ÁªìÂêà‰∫ÜÂ¢ûÂº∫Ê£ÄÁ¥¢ÁîüÊàêÔºàRAGÔºâÊäÄÊúØÂíåÂ±ÇÊ¨°ÂåñÂàÜÂùóÁ≠ñÁï•ÔºåÊó®Âú®‰ªéËá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞‰∏≠Ëá™Âä®ÁîüÊàêÈ´òË¥®ÈáèÁöÑLP‰ª£Á†Å„ÄÇÈÄöËøáËøôÁßçËÆæËÆ°ÔºåÁ≥ªÁªüËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ÈóÆÈ¢ò‰∏ä‰∏ãÊñáÂπ∂ÁîüÊàêÁõ∏Â∫î‰ª£Á†Å„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCHORUSÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÈ¶ñÂÖàÊòØÂü∫‰∫éÂ±ÇÊ¨°ÂåñÂàÜÂùóÁöÑÊ£ÄÁ¥¢Ê®°ÂùóÔºåÂÖ∂Ê¨°ÊòØÁîüÊàêÊ®°Âùó„ÄÇÊ£ÄÁ¥¢Ê®°ÂùóË¥üË¥£‰ªéÊñáÊ°£‰∏≠Ëé∑ÂèñÁõ∏ÂÖ≥‰ª£Á†ÅÁ§∫‰æãÔºåËÄåÁîüÊàêÊ®°ÂùóÂàôÂà©Áî®Ëøô‰∫õÁ§∫‰æãÁîüÊàêÊúÄÁªàÁöÑLP‰ª£Á†Å„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöCHORUSÁöÑÂàõÊñ∞‰πãÂ§ÑÂú®‰∫éÂÖ∂Â±ÇÊ¨°ÂåñÊ†ëÁä∂ÂàÜÂùóÁ≠ñÁï•Âíå‰∏§Èò∂ÊÆµÊ£ÄÁ¥¢ÊñπÊ≥ïÔºåËøô‰ΩøÂæóÁîüÊàêÁöÑ‰ª£Á†ÅÂú®ËØ≠‰πâ‰∏äÊõ¥Âä†ËøûË¥Ø‰∏î‰∏ä‰∏ãÊñáÁõ∏ÂÖ≥ÔºåÊòæËëóÊèêÂçá‰∫ÜÁîüÊàêÊïàÊûú„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåCHORUS‰ΩøÁî®‰∫Ü‰∏ìÂÆ∂ÊèêÁ§∫ÂíåÁªìÊûÑÂåñËß£ÊûêÂô®Ôºå‰ª•ÂºïÂØºÊ®°ÂûãÁîüÊàêÊõ¥‰∏∫ÂáÜÁ°ÆÁöÑ‰ª£Á†Å„ÄÇÊ≠§Â§ñÔºå‰∫§ÂèâÁºñÁ†ÅÂô®ÈáçÊéíÂ∫èÊäÄÊúØÁ°Æ‰øù‰∫ÜÊ£ÄÁ¥¢ÁªìÊûúÁöÑÁõ∏ÂÖ≥ÊÄßÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÁîüÊàêÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåCHORUSÂú®NL4Opt-CodeÂü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÊèêÂçá‰∫ÜÂºÄÊ∫êLLMsÁöÑÊÄßËÉΩÔºåÂ∞§ÂÖ∂ÊòØLlama3.1Ôºà8BÔºâ„ÄÅLlama3.3Ôºà70BÔºâÁ≠âÊ®°ÂûãÔºåÁõ∏ËæÉ‰∫éÂü∫Á∫øÂíå‰º†ÁªüRAGÊñπÊ≥ïÔºåÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ÊòæËëóÔºåÁîöËá≥‰∏éGPT-3.5ÂíåGPT-4Áõ∏ÂΩìÔºåÂêåÊó∂ËÆ°ÁÆóËµÑÊ∫êÈúÄÊ±ÇÂ§ßÂπÖÈôç‰Ωé„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨‰ºòÂåñÁÆóÊ≥ïÁöÑËá™Âä®ÂåñÁîüÊàê„ÄÅÊïôËÇ≤È¢ÜÂüüÁöÑÁºñÁ®ãËæÖÂä©Â∑•ÂÖ∑‰ª•ÂèäÂ∑•‰∏öÁïåÁöÑÂÜ≥Á≠ñÊîØÊåÅÁ≥ªÁªü„ÄÇÈÄöËøáÈôç‰ΩéÂØπÈ¢ÜÂüüÁü•ËØÜÁöÑ‰æùËµñÔºåCHORUSÂèØ‰ª•Â∏ÆÂä©Êõ¥Â§öÈùû‰∏ìÂÆ∂Áî®Êà∑Ëß£ÂÜ≥Â§çÊùÇÁöÑÁ∫øÊÄßËßÑÂàíÈóÆÈ¢òÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÂΩ±ÂìçÂäõ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Linear Programming (LP) problems aim to find the optimal solution to an objective under constraints. These problems typically require domain knowledge, mathematical skills, and programming ability, presenting significant challenges for non-experts. This study explores the efficiency of Large Language Models (LLMs) in generating solver-specific LP code. We propose CHORUS, a retrieval-augmented generation (RAG) framework for synthesizing Gurobi-based LP code from natural language problem statements. CHORUS incorporates a hierarchical tree-like chunking strategy for theoretical contents and generates additional metadata based on code examples from documentation to facilitate self-contained, semantically coherent retrieval. Two-stage retrieval approach of CHORUS followed by cross-encoder reranking further ensures contextual relevance. Finally, expertly crafted prompt and structured parser with reasoning steps improve code generation performance significantly. Experiments on the NL4Opt-Code benchmark show that CHORUS improves the performance of open-source LLMs such as Llama3.1 (8B), Llama3.3 (70B), Phi4 (14B), Deepseek-r1 (32B), and Qwen2.5-coder (32B) by a significant margin compared to baseline and conventional RAG. It also allows these open-source LLMs to outperform or match the performance of much stronger baselines-GPT3.5 and GPT4 while requiring far fewer computational resources. Ablation studies further demonstrate the importance of expert prompting, hierarchical chunking, and structured reasoning.

