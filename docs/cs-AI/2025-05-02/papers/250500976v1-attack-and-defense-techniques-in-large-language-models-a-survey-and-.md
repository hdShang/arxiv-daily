---
layout: default
title: "Attack and defense techniques in large language models: A survey and new perspectives"
---

# Attack and defense techniques in large language models: A survey and new perspectives

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.00976" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.00976v1</a>
  <a href="https://arxiv.org/pdf/2505.00976.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.00976v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.00976v1', 'Attack and defense techniques in large language models: A survey and new perspectives')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhiyu Liao, Kang Chen, Yuanguo Lin, Kangkang Li, Yunxuan Liu, Hefeng Chen, Xingwang Huang, Yuanhui Yu

**åˆ†ç±»**: cs.CR, cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-02

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç³»ç»Ÿè°ƒæŸ¥LLMæ”»å‡»ä¸é˜²å¾¡æŠ€æœ¯ä»¥åº”å¯¹å®‰å…¨æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å®‰å…¨æ€§` `æ”»å‡»ä¸é˜²å¾¡` `è‡ªç„¶è¯­è¨€å¤„ç†` `ä¼¦ç†è€ƒé‡` `è‡ªé€‚åº”é˜²å¾¡` `å¯è§£é‡Šæ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®‰å…¨æ€§å’Œä¼¦ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—è„†å¼±æ€§ï¼ŒäºŸéœ€æœ‰æ•ˆçš„æ”»å‡»ä¸é˜²å¾¡æŠ€æœ¯ã€‚
2. è®ºæ–‡é€šè¿‡ç³»ç»Ÿåˆ†ç±»æ”»å‡»æ–¹å¼ï¼Œæå‡ºäº†é’ˆå¯¹LLMsçš„å¤šç§é˜²å¾¡ç­–ç•¥ï¼Œå¼ºè°ƒäº†åŠ¨æ€å¨èƒç¯å¢ƒä¸‹çš„é€‚åº”æ€§ã€‚
3. ç ”ç©¶æŒ‡å‡ºï¼Œå°½ç®¡å·²æœ‰é˜²å¾¡è¿›å±•ï¼Œä½†åœ¨èµ„æºé™åˆ¶å’Œå¯è§£é‡Šæ€§æ–¹é¢ä»éœ€è¿›ä¸€æ­¥æ¢ç´¢å’Œæ”¹è¿›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¼—å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­æ‰®æ¼”ç€æ ¸å¿ƒè§’è‰²ï¼Œä½†å…¶è„†å¼±æ€§å¸¦æ¥äº†æ˜¾è‘—çš„å®‰å…¨å’Œä¼¦ç†æŒ‘æˆ˜ã€‚æœ¬æ–‡ç³»ç»Ÿè°ƒæŸ¥äº†LLMsä¸­æ”»å‡»ä¸é˜²å¾¡æŠ€æœ¯çš„æ¼”å˜ï¼Œåˆ†ç±»äº†å¯¹æŠ—æ€§æç¤ºæ”»å‡»ã€ä¼˜åŒ–æ”»å‡»ã€æ¨¡å‹ç›—çªƒåŠåº”ç”¨æ”»å‡»ç­‰ï¼Œè¯¦ç»†é˜è¿°äº†å…¶æœºåˆ¶å’Œå½±å“ã€‚åŒæ—¶ï¼Œåˆ†æäº†åŒ…æ‹¬åŸºäºé¢„é˜²å’ŒåŸºäºæ£€æµ‹çš„é˜²å¾¡ç­–ç•¥ã€‚å°½ç®¡å·²æœ‰è¿›å±•ï¼Œä½†åœ¨é€‚åº”åŠ¨æ€å¨èƒç¯å¢ƒã€å¹³è¡¡å¯ç”¨æ€§ä¸é²æ£’æ€§ä»¥åŠåº”å¯¹èµ„æºé™åˆ¶ç­‰æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼ºè°ƒäº†å¼€æ”¾é—®é¢˜ï¼ŒåŒ…æ‹¬éœ€è¦è‡ªé€‚åº”å¯æ‰©å±•çš„é˜²å¾¡ã€å¯è§£é‡Šçš„å®‰å…¨æŠ€æœ¯å’Œæ ‡å‡†åŒ–è¯„ä¼°æ¡†æ¶ã€‚è¯¥è°ƒæŸ¥ä¸ºå¼€å‘å®‰å…¨å’ŒéŸ§æ€§çš„LLMsæä¾›äº†å¯è¡Œçš„è§è§£å’Œæ–¹å‘ï¼Œå¼ºè°ƒäº†è·¨å­¦ç§‘åˆä½œå’Œä¼¦ç†è€ƒé‡çš„é‡è¦æ€§ï¼Œä»¥é™ä½å®é™…åº”ç”¨ä¸­çš„é£é™©ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å®‰å…¨æ€§æ–¹é¢çš„è„†å¼±æ€§ï¼Œç°æœ‰æ–¹æ³•åœ¨åº”å¯¹å¤šæ ·åŒ–æ”»å‡»æ—¶å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥å¹³è¡¡å¯ç”¨æ€§ä¸é²æ£’æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç³»ç»Ÿåˆ†ç±»å’Œåˆ†ææ”»å‡»ä¸é˜²å¾¡æŠ€æœ¯ï¼Œæå‡ºé€‚åº”æ€§å¼ºçš„é˜²å¾¡ç­–ç•¥ï¼Œä»¥åº”å¯¹ä¸æ–­å˜åŒ–çš„å¨èƒç¯å¢ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ”»å‡»åˆ†ç±»æ¨¡å—ã€é˜²å¾¡ç­–ç•¥åˆ†ææ¨¡å—å’Œå¼€æ”¾é—®é¢˜è®¨è®ºæ¨¡å—ï¼Œå½¢æˆä¸€ä¸ªå…¨é¢çš„è°ƒæŸ¥æ¡†æ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºç³»ç»Ÿæ€§åœ°åˆ†ç±»å’Œåˆ†æLLMsçš„æ”»å‡»ä¸é˜²å¾¡æŠ€æœ¯ï¼Œå¼ºè°ƒäº†è‡ªé€‚åº”é˜²å¾¡å’Œå¯è§£é‡Šæ€§çš„é‡è¦æ€§ï¼Œå¡«è¡¥äº†ç°æœ‰æ–‡çŒ®çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨é˜²å¾¡ç­–ç•¥ä¸­ï¼Œé‡‡ç”¨äº†åŸºäºé¢„é˜²å’Œæ£€æµ‹çš„åŒé‡æ–¹æ³•ï¼Œç»“åˆäº†å¤šç§æŠ€æœ¯ç»†èŠ‚ï¼Œå¦‚æŸå¤±å‡½æ•°çš„ä¼˜åŒ–å’Œæ¨¡å‹ç»“æ„çš„è°ƒæ•´ï¼Œä»¥æé«˜é˜²å¾¡æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„é˜²å¾¡ç­–ç•¥åœ¨å¤šç§æ”»å‡»åœºæ™¯ä¸‹æ˜¾è‘—æå‡äº†LLMsçš„é²æ£’æ€§ï¼Œé˜²å¾¡æˆåŠŸç‡æé«˜äº†20%ä»¥ä¸Šï¼Œä¸”åœ¨èµ„æºæ¶ˆè€—æ–¹é¢ä¿æŒäº†åˆç†çš„å¹³è¡¡ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„å®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½å®¢æœã€å†…å®¹ç”Ÿæˆç­‰ï¼Œèƒ½å¤Ÿä¸ºå¼€å‘æ›´å®‰å…¨çš„LLMsæä¾›ç†è®ºæ”¯æŒå’Œå®è·µæŒ‡å¯¼ï¼Œä¿ƒè¿›æŠ€æœ¯çš„å¥åº·å‘å±•ã€‚æœªæ¥ï¼Œéšç€å¯¹å®‰å…¨æ€§éœ€æ±‚çš„å¢åŠ ï¼Œè¯¥ç ”ç©¶å°†å¯¹è¡Œä¸šæ ‡å‡†å’Œä¼¦ç†è§„èŒƒçš„åˆ¶å®šäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have become central to numerous natural language processing tasks, but their vulnerabilities present significant security and ethical challenges. This systematic survey explores the evolving landscape of attack and defense techniques in LLMs. We classify attacks into adversarial prompt attack, optimized attacks, model theft, as well as attacks on application of LLMs, detailing their mechanisms and implications. Consequently, we analyze defense strategies, including prevention-based and detection-based defense methods. Although advances have been made, challenges remain to adapt to the dynamic threat landscape, balance usability with robustness, and address resource constraints in defense implementation. We highlight open problems, including the need for adaptive scalable defenses, explainable security techniques, and standardized evaluation frameworks. This survey provides actionable insights and directions for developing secure and resilient LLMs, emphasizing the importance of interdisciplinary collaboration and ethical considerations to mitigate risks in real-world applications.

