---
layout: default
title: AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding
---

# AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16250" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16250v1</a>
  <a href="https://arxiv.org/pdf/2512.16250.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16250v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16250v1', 'AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sanjoy Chowdhury, Karren D. Yang, Xudong Liu, Fartash Faghri, Pavan Kumar Anasosalu Vasu, Oncel Tuzel, Dinesh Manocha, Chun-Liang Li, Raviteja Vemulapalli

**åˆ†ç±»**: cs.AI, cs.MA

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**AMUSEï¼šç”¨äºAgenticå¤šè¯´è¯äººç†è§£çš„éŸ³è§†é¢‘åŸºå‡†å’Œå¯¹é½æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè¯´è¯äººç†è§£` `éŸ³è§†é¢‘åˆ†æ` `Agenticæ¨ç†` `å¤šæ¨¡æ€å­¦ä¹ ` `åŸºå‡†æµ‹è¯•` `å¥–åŠ±ä¼˜åŒ–` `è‡ªè¯„ä¼°` `å¤§è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šè¯´è¯äººå¯¹è¯åœºæ™¯ä¸­ï¼Œç¼ºä¹æœ‰æ•ˆçš„Agenticæ¨ç†èƒ½åŠ›ï¼Œéš¾ä»¥è·Ÿè¸ªè¯´è¯äººã€ç†è§£è§’è‰²å’Œäº‹ä»¶ã€‚
2. è®ºæ–‡æå‡ºAMUSEåŸºå‡†æµ‹è¯•å’ŒRAFTå¯¹é½æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæå‡æ¨¡å‹åœ¨Agenticå¤šè¯´è¯äººç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚
3. RAFTæ¡†æ¶é€šè¿‡å¥–åŠ±ä¼˜åŒ–å’Œå¤šæ¨¡æ€è‡ªè¯„ä¼°ï¼Œåœ¨AMUSEåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é«˜è¾¾39.52%çš„ç›¸å¯¹å‡†ç¡®ç‡æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†AMUSEï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°Agenticå¤šè¯´è¯äººç†è§£èƒ½åŠ›çš„éŸ³è§†é¢‘åŸºå‡†ã€‚ç°æœ‰çš„å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå¦‚GPT-4oå’ŒQwen3-Omniï¼Œåœ¨æ„ŸçŸ¥æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤šè¯´è¯äººã€ä»¥å¯¹è¯ä¸ºä¸­å¿ƒçš„åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œè¿™äº›åœºæ™¯éœ€è¦Agenticæ¨ç†æ¥è·Ÿè¸ªè¯´è¯è€…ã€ç»´æŠ¤è§’è‰²ä»¥åŠç†è§£è·¨æ—¶é—´çš„äº‹ä»¶ã€‚AMUSEå›´ç»•æœ¬è´¨ä¸Šæ˜¯Agenticçš„ä»»åŠ¡è®¾è®¡ï¼Œè¦æ±‚æ¨¡å‹å°†å¤æ‚çš„éŸ³è§†é¢‘äº¤äº’åˆ†è§£ä¸ºè§„åˆ’ã€ç†è§£å’Œåæ€æ­¥éª¤ã€‚å®ƒåœ¨é›¶æ ·æœ¬ã€å¼•å¯¼å’ŒAgenticä¸‰ç§æ¨¡å¼ä»¥åŠå…­ä¸ªä»»åŠ¡æ—ï¼ˆåŒ…æ‹¬æ—¶ç©ºè¯´è¯äººå®šä½å’Œå¤šæ¨¡æ€å¯¹è¯æ‘˜è¦ï¼‰ä¸­è¯„ä¼°MLLMã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰æ¨¡å‹åœ¨éAgenticå’ŒAgenticè¯„ä¼°ä¸‹éƒ½è¡¨ç°å‡ºè¾ƒå¼±çš„å¤šè¯´è¯äººæ¨ç†å’Œä¸ä¸€è‡´çš„è¡Œä¸ºã€‚å—è¿™äº›ä»»åŠ¡çš„Agenticæœ¬è´¨å’ŒLLM Agentæœ€æ–°è¿›å±•çš„æ¨åŠ¨ï¼Œæœ¬æ–‡æå‡ºRAFTï¼Œä¸€ç§æ•°æ®é«˜æ•ˆçš„Agenticå¯¹é½æ¡†æ¶ï¼Œå®ƒå°†å¥–åŠ±ä¼˜åŒ–ä¸å†…åœ¨å¤šæ¨¡æ€è‡ªæˆ‘è¯„ä¼°ï¼ˆä½œä¸ºå¥–åŠ±ï¼‰å’Œé€‰æ‹©æ€§å‚æ•°é€‚åº”ç›¸ç»“åˆï¼Œä»¥å®ç°æ•°æ®å’Œå‚æ•°é«˜æ•ˆçš„æ›´æ–°ã€‚ä½¿ç”¨RAFTï¼Œåœ¨AMUSEåŸºå‡†æµ‹è¯•ä¸­ï¼Œå‡†ç¡®ç‡æé«˜äº†é«˜è¾¾39.52ï¼…ã€‚AMUSEå’ŒRAFTå…±åŒä¸ºæ£€æŸ¥å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„Agenticæ¨ç†å’Œæé«˜å…¶èƒ½åŠ›æä¾›äº†ä¸€ä¸ªå®ç”¨çš„å¹³å°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å¤„ç†å¤šè¯´è¯äººéŸ³è§†é¢‘å¯¹è¯åœºæ™¯æ—¶ï¼Œé¢ä¸´Agenticæ¨ç†çš„æŒ‘æˆ˜ã€‚è¿™äº›æ¨¡å‹éš¾ä»¥å‡†ç¡®è·Ÿè¸ªæ¯ä¸ªè¯´è¯äººçš„èº«ä»½ã€è§’è‰²ï¼Œä»¥åŠç†è§£è·¨è¶Šæ—¶é—´è½´çš„äº‹ä»¶å…³è”ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¹Ÿç¼ºä¹å¯¹Agenticæ¨ç†èƒ½åŠ›çš„é’ˆå¯¹æ€§æµ‹è¯•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å’Œæå‡Agenticå¤šè¯´è¯äººç†è§£èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼ˆAMUSEï¼‰å’Œå¯¹é½æ¡†æ¶ï¼ˆRAFTï¼‰ã€‚AMUSEåŸºå‡†ä¾§é‡äºéœ€è¦è§„åˆ’ã€ç†è§£å’Œåæ€çš„ä»»åŠ¡ï¼Œè€ŒRAFTæ¡†æ¶åˆ™åˆ©ç”¨å¥–åŠ±ä¼˜åŒ–å’Œå¤šæ¨¡æ€è‡ªè¯„ä¼°æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRAFTæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦ç»„æˆéƒ¨åˆ†ï¼š1) å¤šæ¨¡æ€è¾“å…¥ï¼ˆéŸ³è§†é¢‘æ•°æ®ï¼‰ï¼›2) Agenticä»»åŠ¡åˆ†è§£ï¼ˆå°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºè§„åˆ’ã€ç†è§£å’Œåæ€æ­¥éª¤ï¼‰ï¼›3) å¥–åŠ±ä¼˜åŒ–ï¼ˆä½¿ç”¨å¥–åŠ±ä¿¡å·æ¥æŒ‡å¯¼æ¨¡å‹å­¦ä¹ ï¼‰ï¼›4) å¤šæ¨¡æ€è‡ªè¯„ä¼°ï¼ˆæ¨¡å‹è‡ªæˆ‘è¯„ä¼°æ€§èƒ½å¹¶ç”Ÿæˆå¥–åŠ±ä¿¡å·ï¼‰ï¼›5) é€‰æ‹©æ€§å‚æ•°é€‚åº”ï¼ˆä»…æ›´æ–°æ¨¡å‹çš„éƒ¨åˆ†å‚æ•°ï¼Œä»¥æé«˜æ•°æ®æ•ˆç‡ï¼‰ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œæ¨¡å‹æ¥æ”¶éŸ³è§†é¢‘è¾“å…¥ï¼Œæ‰§è¡ŒAgenticä»»åŠ¡ï¼Œè¿›è¡Œè‡ªæˆ‘è¯„ä¼°ï¼Œå¹¶æ ¹æ®å¥–åŠ±ä¿¡å·æ›´æ–°å‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šRAFTæ¡†æ¶çš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ•°æ®é«˜æ•ˆçš„Agenticå¯¹é½æ–¹æ³•ã€‚å®ƒç»“åˆäº†å¥–åŠ±ä¼˜åŒ–å’Œå†…åœ¨å¤šæ¨¡æ€è‡ªè¯„ä¼°ï¼Œæ— éœ€å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®å³å¯æå‡æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé€‰æ‹©æ€§å‚æ•°é€‚åº”è¿›ä¸€æ­¥æé«˜äº†æ•°æ®æ•ˆç‡ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¿«åœ°é€‚åº”æ–°çš„ä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨RAFTæ¡†æ¶ä¸­ï¼Œå¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œå®ƒéœ€è¦èƒ½å¤Ÿå‡†ç¡®åæ˜ æ¨¡å‹åœ¨Agenticä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å¤šæ¨¡æ€è‡ªè¯„ä¼°æ¨¡å—åˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›æ¥ç”Ÿæˆå¥–åŠ±ä¿¡å·ã€‚é€‰æ‹©æ€§å‚æ•°é€‚åº”åˆ™é€šè¿‡æ§åˆ¶å“ªäº›å‚æ•°è¢«æ›´æ–°ï¼Œæ¥å¹³è¡¡å­¦ä¹ é€Ÿåº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼Œä½†æ­¤å¤„æœªçŸ¥ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16250v1/figures/teaser.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16250v1/figures/eval-modes-short.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16250v1/figures/raft-revised.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRAFTæ¡†æ¶åœ¨AMUSEåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸å¯¹å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾39.52%ã€‚è¿™è¡¨æ˜RAFTæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡æ¨¡å‹åœ¨Agenticå¤šè¯´è¯äººç†è§£æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶ä¸”å…·æœ‰è¾ƒé«˜çš„æ•°æ®æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤šç§åœºæ™¯ï¼Œå¦‚æ™ºèƒ½ä¼šè®®åŠ©æ‰‹ã€å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿã€è§†é¢‘å†…å®¹åˆ†æç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹åœ¨å¤šè¯´è¯äººç¯å¢ƒä¸‹çš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥å®ç°æ›´è‡ªç„¶ã€æ›´æ™ºèƒ½çš„äººæœºäº¤äº’ï¼Œå¹¶ä¸ºè§†é¢‘å†…å®¹åˆ†ææä¾›æ›´å‡†ç¡®çš„ä¿¡æ¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.

