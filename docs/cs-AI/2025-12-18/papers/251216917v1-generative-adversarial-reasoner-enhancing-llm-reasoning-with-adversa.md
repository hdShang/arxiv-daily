---
layout: default
title: Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning
---

# Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16917" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16917v1</a>
  <a href="https://arxiv.org/pdf/2512.16917.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16917v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16917v1', 'Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qihao Liu, Luoxin Ye, Wufei Ma, Yu-Cheng Chou, Alan Yuille

**åˆ†ç±»**: cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç”Ÿæˆå¯¹æŠ—æ¨ç†å™¨ï¼Œé€šè¿‡å¯¹æŠ—å¼ºåŒ–å­¦ä¹ æå‡LLMçš„æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶åœ¨æ•°å­¦é—®é¢˜ä¸Šã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `å¯¹æŠ—å­¦ä¹ ` `æ•°å­¦æ¨ç†` `æ¨ç†å™¨` `åˆ¤åˆ«å™¨` `å¥–åŠ±å¡‘é€ ` `åœ¨çº¿å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å…·å¤‡æ˜¾å¼æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä»å­˜åœ¨è¿‡ç¨‹é”™è¯¯ï¼Œå¦‚ä¸æ­£ç¡®çš„è®¡ç®—å’Œè„†å¼±çš„é€»è¾‘ã€‚
2. è®ºæ–‡æå‡ºç”Ÿæˆå¯¹æŠ—æ¨ç†å™¨ï¼Œé€šè¿‡å¯¹æŠ—å¼ºåŒ–å­¦ä¹ ååŒè®­ç»ƒLLMæ¨ç†å™¨å’Œåˆ¤åˆ«å™¨ï¼Œåˆ©ç”¨åˆ¤åˆ«å™¨å¯¹æ¨ç†è¿‡ç¨‹è¿›è¡Œç»†ç²’åº¦è¯„ä¼°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸è¾ƒäºæ ‡å‡†å¼ºåŒ–å­¦ä¹ åè®­ç»ƒçš„åŸºçº¿æ¨¡å‹ï¼Œæ€§èƒ½å¾—åˆ°æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºç”Ÿæˆå¯¹æŠ—æ¨ç†å™¨(Generative Adversarial Reasoner)çš„åœ¨çº¿è”åˆè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¯¹æŠ—å¼ºåŒ–å­¦ä¹ ååŒè¿›åŒ–LLMæ¨ç†å™¨å’ŒåŸºäºLLMçš„åˆ¤åˆ«å™¨ï¼Œä»è€Œå¢å¼ºæ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é‡‡ç”¨è®¡ç®—é«˜æ•ˆçš„å®¡æŸ¥æœºåˆ¶ï¼Œå°†æ¯ä¸ªæ¨ç†é“¾åˆ’åˆ†ä¸ºé€»è¾‘å®Œæ•´çš„ã€é•¿åº¦ç›¸å½“çš„ç‰‡æ®µï¼Œåˆ¤åˆ«å™¨ä½¿ç”¨ç®€æ´ã€ç»“æ„åŒ–çš„ç†ç”±è¯„ä¼°æ¯ä¸ªç‰‡æ®µçš„åˆç†æ€§ã€‚å­¦ä¹ è¿‡ç¨‹è€¦åˆäº†äº’è¡¥ä¿¡å·ï¼šLLMæ¨ç†å™¨å› äº§ç”Ÿé€»è¾‘ä¸€è‡´ä¸”èƒ½å¾—å‡ºæ­£ç¡®ç­”æ¡ˆçš„æ­¥éª¤è€Œè·å¾—å¥–åŠ±ï¼Œè€Œåˆ¤åˆ«å™¨å› æ­£ç¡®æ£€æµ‹åˆ°æ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯æˆ–åŒºåˆ†æ¨ç†è½¨è¿¹è€Œè·å¾—å¥–åŠ±ã€‚è¿™äº§ç”Ÿäº†å¯†é›†çš„ã€è‰¯å¥½æ ¡å‡†çš„ã€åœ¨çº¿çš„æ­¥éª¤çº§åˆ«å¥–åŠ±ï¼Œè¡¥å……äº†ç¨€ç–çš„ç²¾ç¡®åŒ¹é…ä¿¡å·ï¼Œæ”¹å–„äº†ä¿¡ç”¨åˆ†é…ï¼Œæé«˜äº†æ ·æœ¬æ•ˆç‡ï¼Œå¹¶å¢å¼ºäº†LLMçš„æ•´ä½“æ¨ç†è´¨é‡ã€‚åœ¨å„ç§æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•ç›¸å¯¹äºä½¿ç”¨æ ‡å‡†RLåè®­ç»ƒçš„å¼ºå¤§åŸºçº¿ï¼Œå®ç°äº†æŒç»­çš„æ”¶ç›Šã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨AIME24ä¸Šï¼ŒDeepSeek-R1-Distill-Qwen-7Bä»54.0æé«˜åˆ°61.3ï¼ˆ+7.3ï¼‰ï¼ŒDeepSeek-R1-Distill-Llama-8Bä»43.7æé«˜åˆ°53.7ï¼ˆ+10.0ï¼‰ã€‚æ¨¡å—åŒ–åˆ¤åˆ«å™¨è¿˜èƒ½å¤Ÿçµæ´»åœ°è¿›è¡Œå¥–åŠ±å¡‘é€ ï¼Œä»¥å®ç°è¯¸å¦‚æ•™å¸ˆçŸ¥è¯†è’¸é¦ã€åå¥½å¯¹é½å’ŒåŸºäºæ•°å­¦è¯æ˜çš„æ¨ç†ç­‰ç›®æ ‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦æ¨ç†è¿‡ç¨‹ä¸­å‡ºç°çš„é€»è¾‘é”™è¯¯ã€è®¡ç®—é”™è¯¯ç­‰é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºç¨€ç–çš„å¥–åŠ±ä¿¡å·ï¼ˆä¾‹å¦‚ï¼Œç­”æ¡ˆæ˜¯å¦å®Œå…¨æ­£ç¡®ï¼‰ï¼Œéš¾ä»¥å¯¹æ¨ç†è¿‡ç¨‹ä¸­çš„æ¯ä¸€æ­¥è¿›è¡Œæœ‰æ•ˆæŒ‡å¯¼ï¼Œå¯¼è‡´ä¿¡ç”¨åˆ†é…å›°éš¾ï¼Œè®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥ä¸€ä¸ªåˆ¤åˆ«å™¨ï¼Œä¸LLMæ¨ç†å™¨è¿›è¡Œå¯¹æŠ—è®­ç»ƒã€‚åˆ¤åˆ«å™¨è´Ÿè´£è¯„ä¼°æ¨ç†è¿‡ç¨‹ä¸­çš„æ¯ä¸€æ­¥æ˜¯å¦åˆç†ï¼Œå¹¶ç»™å‡ºç»“æ„åŒ–çš„ç†ç”±ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥ä¸ºæ¨ç†å™¨æä¾›æ›´å¯†é›†ã€æ›´ç»†ç²’åº¦çš„å¥–åŠ±ä¿¡å·ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡å’Œæ¨ç†è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šLLMæ¨ç†å™¨å’ŒLLMåˆ¤åˆ«å™¨ã€‚æ¨ç†å™¨è´Ÿè´£ç”Ÿæˆæ¨ç†æ­¥éª¤ï¼Œåˆ¤åˆ«å™¨è´Ÿè´£è¯„ä¼°æ¯ä¸ªæ¨ç†æ­¥éª¤çš„åˆç†æ€§ã€‚è®­ç»ƒè¿‡ç¨‹é‡‡ç”¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œæ¨ç†å™¨æ ¹æ®åˆ¤åˆ«å™¨çš„åé¦ˆè°ƒæ•´ç­–ç•¥ï¼Œåˆ¤åˆ«å™¨æ ¹æ®æ¨ç†å™¨çš„è¡¨ç°è°ƒæ•´è¯„ä¼°æ ‡å‡†ã€‚ä¸€ä¸ªå…³é”®ç»„ä»¶æ˜¯â€œå®¡æŸ¥æœºåˆ¶â€ï¼Œå®ƒå°†æ¨ç†é“¾åˆ†å‰²æˆé€»è¾‘å®Œæ•´çš„ç‰‡æ®µï¼Œä»¥ä¾¿åˆ¤åˆ«å™¨è¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºä½¿ç”¨å¯¹æŠ—å¼ºåŒ–å­¦ä¹ æ¥è®­ç»ƒLLMæ¨ç†å™¨ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæä¾›æ›´å¯†é›†ã€æ›´ç»†ç²’åº¦çš„å¥–åŠ±ä¿¡å·ï¼Œä»è€Œæ›´å¥½åœ°æŒ‡å¯¼æ¨ç†è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæ¨¡å—åŒ–çš„åˆ¤åˆ«å™¨è®¾è®¡ä½¿å¾—å¯ä»¥çµæ´»åœ°è¿›è¡Œå¥–åŠ±å¡‘é€ ï¼Œä»¥é€‚åº”ä¸åŒçš„ç›®æ ‡ï¼Œä¾‹å¦‚çŸ¥è¯†è’¸é¦å’Œåå¥½å¯¹é½ã€‚

**å…³é”®è®¾è®¡**ï¼šå®¡æŸ¥æœºåˆ¶å°†æ¨ç†é“¾åˆ†å‰²æˆé•¿åº¦ç›¸å½“çš„ç‰‡æ®µï¼Œç¡®ä¿æ¯ä¸ªç‰‡æ®µåœ¨é€»è¾‘ä¸Šæ˜¯å®Œæ•´çš„ã€‚åˆ¤åˆ«å™¨è¾“å‡ºç»“æ„åŒ–çš„ç†ç”±ï¼Œè§£é‡Šå…¶è¯„ä¼°ç»“æœã€‚æ¨ç†å™¨å’Œåˆ¤åˆ«å™¨çš„å¥–åŠ±å‡½æ•°è¢«è®¾è®¡ä¸ºç›¸äº’å¯¹æŠ—ï¼Œæ¨ç†å™¨è¯•å›¾ç”Ÿæˆèƒ½å¤Ÿæ¬ºéª—åˆ¤åˆ«å™¨çš„æ¨ç†æ­¥éª¤ï¼Œè€Œåˆ¤åˆ«å™¨è¯•å›¾å‡†ç¡®åœ°è¯†åˆ«æ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16917v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16917v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16917v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨AIME24æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…·ä½“è€Œè¨€ï¼ŒDeepSeek-R1-Distill-Qwen-7Bæ¨¡å‹ä»54.0æé«˜åˆ°61.3ï¼ˆ+7.3ï¼‰ï¼ŒDeepSeek-R1-Distill-Llama-8Bæ¨¡å‹ä»43.7æé«˜åˆ°53.7ï¼ˆ+10.0ï¼‰ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜LLMçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä¼˜äºç°æœ‰çš„å¼ºåŒ–å­¦ä¹ åè®­ç»ƒæ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤æ‚æ¨ç†èƒ½åŠ›çš„åœºæ™¯ï¼Œä¾‹å¦‚æ•°å­¦é—®é¢˜æ±‚è§£ã€ç§‘å­¦ç ”ç©¶ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿç­‰ã€‚é€šè¿‡æé«˜LLMçš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥ä½¿å…¶åœ¨è¿™äº›é¢†åŸŸå‘æŒ¥æ›´å¤§çš„ä½œç”¨ï¼Œå¹¶ä¸ºè‡ªåŠ¨åŒ–æ¨ç†å’Œå†³ç­–æä¾›æ›´å¯é çš„åŸºç¡€ã€‚æœªæ¥çš„å½±å“åŒ…æ‹¬æå‡AIåœ¨å¤æ‚é—®é¢˜è§£å†³æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶å¯èƒ½ä¿ƒè¿›AIåœ¨æ•™è‚²ã€ç§‘ç ”ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.

