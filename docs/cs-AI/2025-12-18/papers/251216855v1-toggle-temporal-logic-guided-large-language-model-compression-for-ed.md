---
layout: default
title: TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge
---

# TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16855" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16855v1</a>
  <a href="https://arxiv.org/pdf/2512.16855.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16855v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16855v1', 'TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Khurram Khalil, Khaza Anuarul Hoque

**åˆ†ç±»**: cs.AI, cs.LO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

**å¤‡æ³¨**: Published in the IEEE ICCAD 2025 conference

**DOI**: [10.1109/ICCAD66269.2025.11240962](https://doi.org/10.1109/ICCAD66269.2025.11240962)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**TOGGLEï¼šæ—¶åºé€»è¾‘å¼•å¯¼çš„å¤§è¯­è¨€æ¨¡å‹è¾¹ç¼˜å‹ç¼©æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹å‹ç¼©` `è¾¹ç¼˜è®¡ç®—` `ä¿¡å·æ—¶åºé€»è¾‘` `å½¢å¼åŒ–æ–¹æ³•` `è´å¶æ–¯ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMå‹ç¼©æ–¹æ³•åœ¨é™ä½è®¡ç®—èµ„æºéœ€æ±‚çš„åŒæ—¶ï¼Œå¾€å¾€ä¼šç‰ºç‰²æ¨¡å‹çš„å…³é”®è¯­è¨€å±æ€§ï¼Œä¸”ç¼ºä¹å¯¹æ¨¡å‹è¡Œä¸ºçš„æ­£å¼ä¿è¯ã€‚
2. TOGGLEåˆ©ç”¨ä¿¡å·æ—¶åºé€»è¾‘ï¼ˆSTLï¼‰æ¥å½¢å¼åŒ–åœ°æŒ‡å®šå’Œæ‰§è¡Œå‹ç¼©è¿‡ç¨‹ä¸­çš„è¯­è¨€å±æ€§ï¼Œç¡®ä¿å‹ç¼©åçš„æ¨¡å‹æ»¡è¶³é¢„å®šä¹‰çš„è¯­è¨€çº¦æŸã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒTOGGLEåœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬å’Œæ¨¡å‹å¤§å°çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒæ¨¡å‹çš„è¯­è¨€å±æ€§ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œé™åˆ¶äº†å…¶åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šçš„éƒ¨ç½²ã€‚ç°æœ‰çš„å‹ç¼©æŠ€æœ¯ï¼Œå¦‚é‡åŒ–å’Œå‰ªæï¼Œé€šå¸¸ä¼šé™ä½å…³é”®çš„è¯­è¨€å±æ€§ï¼Œå¹¶ä¸”ç¼ºä¹å¯¹æ¨¡å‹è¡Œä¸ºä¿æŒçš„æ­£å¼ä¿è¯ã€‚æˆ‘ä»¬æå‡ºäº†æ—¶åºé€»è¾‘å¼•å¯¼çš„å¤§è¯­è¨€æ¨¡å‹å‹ç¼©ï¼ˆTOGGLEï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ä¿¡å·æ—¶åºé€»è¾‘ï¼ˆSTLï¼‰æ¥æ­£å¼åœ°æŒ‡å®šå’Œæ‰§è¡Œå‹ç¼©è¿‡ç¨‹ä¸­çš„è¯­è¨€å±æ€§ã€‚TOGGLEé‡‡ç”¨STLé²æ£’æ€§å¼•å¯¼çš„è´å¶æ–¯ä¼˜åŒ–ï¼Œç³»ç»Ÿåœ°æ¢ç´¢é€å±‚é‡åŒ–å’Œå‰ªæé…ç½®ï¼Œç”Ÿæˆå‹ç¼©æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨ä¸è¿›è¡Œé‡æ–°è®­ç»ƒæˆ–å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œæ­£å¼åœ°æ»¡è¶³æŒ‡å®šçš„è¯­è¨€çº¦æŸã€‚åœ¨å››ä¸ªLLMæ¶æ„ï¼ˆGPT-2ã€DeepSeek-V2 7Bã€LLaMA 3 8Bå’ŒMistral 7Bï¼‰ä¸Šè¯„ä¼°TOGGLEï¼Œæˆ‘ä»¬å®ç°äº†é«˜è¾¾3.3å€çš„è®¡ç®—æˆæœ¬ï¼ˆFLOPsï¼‰é™ä½å’Œé«˜è¾¾68.8%çš„æ¨¡å‹å¤§å°é™ä½ï¼ŒåŒæ—¶æ»¡è¶³æ‰€æœ‰è¯­è¨€å±æ€§ã€‚TOGGLEä»£è¡¨äº†å½¢å¼åŒ–æ–¹æ³•é¦–æ¬¡é›†æˆåˆ°LLMå‹ç¼©ä¸­ï¼Œä»è€Œèƒ½å¤Ÿåœ¨è¾¹ç¼˜ç¡¬ä»¶ä¸Šé«˜æ•ˆã€å¯éªŒè¯åœ°éƒ¨ç½²LLMã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²çš„é—®é¢˜ã€‚ç°æœ‰å‹ç¼©æ–¹æ³•ï¼Œå¦‚é‡åŒ–å’Œå‰ªæï¼Œè™½ç„¶å¯ä»¥å‡å°æ¨¡å‹å¤§å°å’Œè®¡ç®—é‡ï¼Œä½†å¾€å¾€ä¼šæŸå®³æ¨¡å‹çš„è¯­è¨€èƒ½åŠ›ï¼Œå¹¶ä¸”ç¼ºä¹å½¢å¼åŒ–çš„éªŒè¯æ‰‹æ®µæ¥ä¿è¯å‹ç¼©åçš„æ¨¡å‹ä»ç„¶æ»¡è¶³ç‰¹å®šçš„è¯­è¨€å±æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTOGGLEçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¿¡å·æ—¶åºé€»è¾‘ï¼ˆSTLï¼‰æ¥å½¢å¼åŒ–åœ°æè¿°LLMéœ€è¦æ»¡è¶³çš„è¯­è¨€å±æ€§ï¼Œå¹¶åœ¨æ¨¡å‹å‹ç¼©è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡ä¼˜åŒ–ç®—æ³•æ¥å¯»æ‰¾æ»¡è¶³è¿™äº›å±æ€§çš„æœ€ä½³å‹ç¼©é…ç½®ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ä¼ ç»Ÿå‹ç¼©æ–¹æ³•ä¸­å¯¹è¯­è¨€å±æ€§çš„å¿½è§†ï¼Œå¹¶æä¾›äº†å½¢å¼åŒ–çš„ä¿è¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTOGGLEæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä½¿ç”¨STLå½¢å¼åŒ–åœ°å®šä¹‰LLMéœ€è¦æ»¡è¶³çš„è¯­è¨€å±æ€§ï¼›2) ä½¿ç”¨STLé²æ£’æ€§åº¦é‡æ¥è¯„ä¼°æ¨¡å‹æ»¡è¶³è¿™äº›å±æ€§çš„ç¨‹åº¦ï¼›3) ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–ç®—æ³•ï¼Œä»¥STLé²æ£’æ€§ä¸ºç›®æ ‡å‡½æ•°ï¼Œæœç´¢æœ€ä½³çš„é€å±‚é‡åŒ–å’Œå‰ªæé…ç½®ï¼›4) ç”Ÿæˆå‹ç¼©åçš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨æ»¡è¶³æŒ‡å®šçš„è¯­è¨€çº¦æŸçš„åŒæ—¶ï¼Œå…·æœ‰æ›´å°çš„æ¨¡å‹å¤§å°å’Œæ›´ä½çš„è®¡ç®—æˆæœ¬ã€‚

**å…³é”®åˆ›æ–°**ï¼šTOGGLEçš„å…³é”®åˆ›æ–°åœ¨äºå°†å½¢å¼åŒ–æ–¹æ³•ï¼ˆSTLï¼‰å¼•å…¥åˆ°LLMå‹ç¼©ä¸­ã€‚ä¸ä¼ ç»Ÿçš„å‹ç¼©æ–¹æ³•ä¸åŒï¼ŒTOGGLEä¸ä»…å…³æ³¨æ¨¡å‹çš„å¤§å°å’Œè®¡ç®—é‡ï¼Œæ›´å…³æ³¨æ¨¡å‹å‹ç¼©åæ˜¯å¦ä»ç„¶æ»¡è¶³é¢„å®šä¹‰çš„è¯­è¨€å±æ€§ã€‚é€šè¿‡STLé²æ£’æ€§å¼•å¯¼çš„è´å¶æ–¯ä¼˜åŒ–ï¼ŒTOGGLEèƒ½å¤Ÿç³»ç»Ÿåœ°æ¢ç´¢å‹ç¼©é…ç½®ï¼Œå¹¶ç”Ÿæˆæ»¡è¶³è¯­è¨€çº¦æŸçš„å‹ç¼©æ¨¡å‹ã€‚

**å…³é”®è®¾è®¡**ï¼šTOGGLEçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨STLæ¥å½¢å¼åŒ–åœ°æè¿°è¯­è¨€å±æ€§ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨STLæ¥æè¿°æ¨¡å‹åœ¨ç‰¹å®šè¾“å…¥ä¸‹å¿…é¡»äº§ç”Ÿç‰¹å®šè¾“å‡ºçš„çº¦æŸï¼›2) ä½¿ç”¨STLé²æ£’æ€§åº¦é‡æ¥é‡åŒ–æ¨¡å‹æ»¡è¶³è¿™äº›å±æ€§çš„ç¨‹åº¦ï¼Œé²æ£’æ€§è¶Šé«˜ï¼Œè¡¨ç¤ºæ¨¡å‹è¶Šèƒ½æŠµæŠ—å™ªå£°å’Œæ‰°åŠ¨ï¼Œä»è€Œæ›´å¥½åœ°æ»¡è¶³è¯­è¨€å±æ€§ï¼›3) ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–ç®—æ³•æ¥æœç´¢æœ€ä½³çš„é€å±‚é‡åŒ–å’Œå‰ªæé…ç½®ï¼Œè´å¶æ–¯ä¼˜åŒ–èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¢ç´¢æœç´¢ç©ºé—´ï¼Œå¹¶æ‰¾åˆ°æ»¡è¶³è¯­è¨€çº¦æŸçš„æœ€ä½³å‹ç¼©é…ç½®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

TOGGLEåœ¨GPT-2ã€DeepSeek-V2 7Bã€LLaMA 3 8Bå’ŒMistral 7Bå››ä¸ªLLMæ¶æ„ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒTOGGLEèƒ½å¤Ÿå®ç°é«˜è¾¾3.3å€çš„è®¡ç®—æˆæœ¬ï¼ˆFLOPsï¼‰é™ä½å’Œé«˜è¾¾68.8%çš„æ¨¡å‹å¤§å°é™ä½ï¼ŒåŒæ—¶æ»¡è¶³æ‰€æœ‰æŒ‡å®šçš„è¯­è¨€å±æ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒTOGGLEæ˜¯ä¸€ç§æœ‰æ•ˆçš„LLMå‹ç¼©æ–¹æ³•ï¼Œå¯ä»¥åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²é«˜æ€§èƒ½çš„LLMã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TOGGLEçš„åº”ç”¨åœºæ™¯å¹¿æ³›ï¼ŒåŒ…æ‹¬æ™ºèƒ½å®¶å±…ã€è‡ªåŠ¨é©¾é©¶ã€å¯ç©¿æˆ´è®¾å¤‡ç­‰è¾¹ç¼˜è®¡ç®—é¢†åŸŸã€‚é€šè¿‡åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²å‹ç¼©åçš„LLMï¼Œå¯ä»¥å®ç°æœ¬åœ°åŒ–çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼Œæé«˜å“åº”é€Ÿåº¦å’Œæ•°æ®å®‰å…¨æ€§ã€‚æ­¤å¤–ï¼ŒTOGGLEè¿˜å¯ä»¥åº”ç”¨äºå¯¹æ¨¡å‹å®‰å…¨æ€§æœ‰ä¸¥æ ¼è¦æ±‚çš„åœºæ™¯ï¼Œä¾‹å¦‚é‡‘èå’ŒåŒ»ç–—é¢†åŸŸï¼Œç¡®ä¿æ¨¡å‹åœ¨å‹ç¼©åä»ç„¶æ»¡è¶³å…³é”®çš„è¯­è¨€å±æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.

