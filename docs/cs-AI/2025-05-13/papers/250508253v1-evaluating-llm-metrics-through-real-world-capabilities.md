---
layout: default
title: Evaluating LLM Metrics Through Real-World Capabilities
---

# Evaluating LLM Metrics Through Real-World Capabilities

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08253" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08253v1</a>
  <a href="https://arxiv.org/pdf/2505.08253.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08253v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08253v1', 'Evaluating LLM Metrics Through Real-World Capabilities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Justin K Miller, Wenjia Tang

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

**å¤‡æ³¨**: 14 pages main text, 5 pages references, 20 pages appendix; includes 3 figures and 4 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºçœŸå®ä¸–ç•Œèƒ½åŠ›è¯„ä¼°LLMæ€§èƒ½çš„æ–°æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ€§èƒ½è¯„ä¼°` `ç”¨æˆ·æ•ˆç”¨` `çœŸå®ä¸–ç•Œåº”ç”¨` `æ ¸å¿ƒèƒ½åŠ›` `åŸºå‡†æµ‹è¯•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯„ä¼°åŸºå‡†ä¸»è¦é›†ä¸­äºä»£ç ç”Ÿæˆå’Œäº‹å®å›å¿†ï¼Œæœªèƒ½å…¨é¢åæ˜ ç”¨æˆ·åœ¨å®é™…ä»»åŠ¡ä¸­çš„éœ€æ±‚ã€‚
2. è®ºæ–‡é€šè¿‡åˆ†æç”¨æˆ·ä½¿ç”¨æ•°æ®ï¼Œæå‡ºäº†å…­ç§æ ¸å¿ƒèƒ½åŠ›ï¼Œå¹¶åŸºäºè¿™äº›èƒ½åŠ›è¯„ä¼°ç°æœ‰åŸºå‡†çš„è¦†ç›–æƒ…å†µã€‚
3. ç ”ç©¶å‘ç°Google Geminiåœ¨ç”¨æˆ·æ•ˆç”¨æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜äºå…¶ä»–ä¸»æµæ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ—¥ç›Šèå…¥æ—¥å¸¸å·¥ä½œæµç¨‹ï¼Œè¯„ä¼°å…¶æ€§èƒ½çš„æ–¹å¼åº”åæ˜ çœŸå®ä½¿ç”¨æƒ…å†µï¼Œè€ŒéæŠ½è±¡çš„æ™ºèƒ½æ¦‚å¿µã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å…³æ³¨å®é™…æ•ˆç”¨çš„è¯„ä¼°æ–¹æ³•ï¼Œåˆ†æäº†ç”¨æˆ·åœ¨æ—¥å¸¸ä»»åŠ¡ä¸­å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä½¿ç”¨æƒ…å†µã€‚é€šè¿‡å¯¹å¤§è§„æ¨¡è°ƒæŸ¥æ•°æ®å’Œä½¿ç”¨æ—¥å¿—çš„åˆ†æï¼Œè¯†åˆ«å‡ºå…­ç§æ ¸å¿ƒèƒ½åŠ›ï¼ŒåŒ…æ‹¬æ‘˜è¦ã€æŠ€æœ¯æ”¯æŒã€å·¥ä½œå®¡æŸ¥ã€æ•°æ®ç»“æ„åŒ–ã€ç”Ÿæˆå’Œä¿¡æ¯æ£€ç´¢ã€‚ç ”ç©¶å‘ç°ç°æœ‰åŸºå‡†åœ¨è¦†ç›–è¿™äº›èƒ½åŠ›æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå¹¶æå‡ºäº†ä»¥äººæœ¬æ ‡å‡†ä¸ºåŸºç¡€çš„è¯„ä¼°æ¡†æ¶ã€‚æœ€ç»ˆï¼Œç ”ç©¶è¡¨æ˜Google Geminiåœ¨è¿™äº›æ•ˆç”¨å¯¼å‘çš„æŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¯„ä¼°åŸºå‡†æœªèƒ½å……åˆ†åæ˜ å¤§å‹è¯­è¨€æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„æ•ˆç”¨é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ç”¨æˆ·æ—¥å¸¸ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ†æç”¨æˆ·çš„å®é™…ä½¿ç”¨æƒ…å†µï¼Œè¯†åˆ«å‡ºå…­ç§æ ¸å¿ƒèƒ½åŠ›ï¼Œå¹¶è¯„ä¼°ç°æœ‰åŸºå‡†åœ¨è¿™äº›èƒ½åŠ›ä¸Šçš„è¦†ç›–ç¨‹åº¦ï¼Œä»¥æ­¤æå‡ºæ›´ç¬¦åˆç”¨æˆ·éœ€æ±‚çš„è¯„ä¼°æ ‡å‡†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆæ”¶é›†å¤§è§„æ¨¡çš„ç”¨æˆ·è°ƒæŸ¥æ•°æ®å’Œä½¿ç”¨æ—¥å¿—ï¼Œè¯†åˆ«å‡ºå…­ç§æ ¸å¿ƒèƒ½åŠ›ã€‚æ¥ç€ï¼Œè¯„ä¼°ç°æœ‰åŸºå‡†åœ¨è¿™äº›èƒ½åŠ›ä¸Šçš„è¡¨ç°ï¼Œå¹¶æå‡ºäººæœ¬æ ‡å‡†è¿›è¡Œæ¯”è¾ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°åœ¨äºå°†è¯„ä¼°é‡ç‚¹ä»æŠ½è±¡æ™ºèƒ½è½¬å‘å®é™…æ•ˆç”¨ï¼Œè¯†åˆ«å‡ºç”¨æˆ·åœ¨æ—¥å¸¸ä»»åŠ¡ä¸­çœŸæ­£éœ€è¦çš„èƒ½åŠ›ï¼Œå¹¶æå‡ºç›¸åº”çš„è¯„ä¼°æ ‡å‡†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†äº”ä¸ªå®ç”¨æ ‡å‡†ï¼šè¿è´¯æ€§ã€å‡†ç¡®æ€§ã€æ¸…æ™°æ€§ã€ç›¸å…³æ€§å’Œæ•ˆç‡ï¼Œä»¥ç¡®ä¿è¯„ä¼°ç»“æœèƒ½å¤ŸçœŸå®åæ˜ ç”¨æˆ·çš„éœ€æ±‚ã€‚å…·ä½“çš„åŸºå‡†é€‰æ‹©å’Œæ¯”è¾ƒæ–¹æ³•ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿ç»“æœçš„å¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGoogle Geminiåœ¨å…­ç§æ ¸å¿ƒèƒ½åŠ›çš„æ•ˆç”¨è¯„ä¼°ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼ŒåŒ…æ‹¬OpenAIçš„GPTã€xAIçš„Grokã€Metaçš„LLaMAç­‰ã€‚å…·ä½“è€Œè¨€ï¼ŒGoogle Geminiåœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢çš„æå‡å¹…åº¦æ˜¾è‘—ï¼Œè¡¨æ˜å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€å†…å®¹åˆ›ä½œã€æŠ€æœ¯æ”¯æŒç­‰å¤šä¸ªè¡Œä¸šã€‚é€šè¿‡æ›´å‡†ç¡®åœ°è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„å®é™…æ•ˆç”¨ï¼Œèƒ½å¤Ÿå¸®åŠ©ä¼ä¸šå’Œå¼€å‘è€…ä¼˜åŒ–AIå·¥å…·çš„è®¾è®¡ä¸åº”ç”¨ï¼Œæé«˜ç”¨æˆ·ä½“éªŒå’Œå·¥ä½œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´ç¬¦åˆç”¨æˆ·éœ€æ±‚çš„AIè¯„ä¼°æ ‡å‡†çš„å»ºç«‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As generative AI becomes increasingly embedded in everyday workflows, it is important to evaluate its performance in ways that reflect real-world usage rather than abstract notions of intelligence. Unlike many existing benchmarks that assess general intelligence, our approach focuses on real-world utility, evaluating how well models support users in everyday tasks. While current benchmarks emphasize code generation or factual recall, users rely on AI for a much broader range of activities-from writing assistance and summarization to citation formatting and stylistic feedback. In this paper, we analyze large-scale survey data and usage logs to identify six core capabilities that represent how people commonly use Large Language Models (LLMs): Summarization, Technical Assistance, Reviewing Work, Data Structuring, Generation, and Information Retrieval. We then assess the extent to which existing benchmarks cover these capabilities, revealing significant gaps in coverage, efficiency measurement, and interpretability. Drawing on this analysis, we use human-centered criteria to identify gaps in how well current benchmarks reflect common usage that is grounded in five practical criteria: coherence, accuracy, clarity, relevance, and efficiency. For four of the six capabilities, we identify the benchmarks that best align with real-world tasks and use them to compare leading models. We find that Google Gemini outperforms other models-including OpenAI's GPT, xAI's Grok, Meta's LLaMA, Anthropic's Claude, DeepSeek, and Qwen from Alibaba-on these utility-focused metrics.

