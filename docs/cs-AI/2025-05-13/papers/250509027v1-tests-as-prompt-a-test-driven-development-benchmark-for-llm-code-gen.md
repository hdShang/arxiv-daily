---
layout: default
title: Tests as Prompt: A Test-Driven-Development Benchmark for LLM Code Generation
---

# Tests as Prompt: A Test-Driven-Development Benchmark for LLM Code Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.09027" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.09027v1</a>
  <a href="https://arxiv.org/pdf/2505.09027.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.09027v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.09027v1', 'Tests as Prompt: A Test-Driven-Development Benchmark for LLM Code Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yi Cui

**åˆ†ç±»**: cs.SE, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

**å¤‡æ³¨**: arXiv admin note: text overlap with arXiv:2409.05177

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºWebApp1KåŸºå‡†ä»¥è¯„ä¼°LLMåœ¨æµ‹è¯•é©±åŠ¨å¼€å‘ä¸­çš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æµ‹è¯•é©±åŠ¨å¼€å‘` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä»£ç ç”Ÿæˆ` `æ€§èƒ½è¯„ä¼°` `è½¯ä»¶å¼€å‘`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–è‡ªç„¶è¯­è¨€æç¤ºï¼Œéš¾ä»¥æœ‰æ•ˆè¯„ä¼°LLMsåœ¨çœŸå®è½¯ä»¶å¼€å‘ä¸­çš„è¡¨ç°ã€‚
2. æå‡ºWebApp1KåŸºå‡†ï¼Œåˆ©ç”¨æµ‹è¯•ç”¨ä¾‹ä½œä¸ºæç¤ºå’ŒéªŒè¯ï¼Œå¼ºè°ƒLLMsä»æµ‹è¯•ä¸­ç”Ÿæˆä»£ç çš„èƒ½åŠ›ã€‚
3. é€šè¿‡å¯¹19ä¸ªæ¨¡å‹çš„è¯„ä¼°ï¼Œå‘ç°æŒ‡ä»¤éµå¾ªå’Œä¸Šä¸‹æ–‡å­¦ä¹ æ˜¯TDDæˆåŠŸçš„å…³é”®ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿç¼–ç èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬ä»‹ç»äº†WebApp1Kï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æµ‹è¯•é©±åŠ¨å¼€å‘ï¼ˆTDDï¼‰ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå…¶ä¸­æµ‹è¯•ç”¨ä¾‹æ—¢ä½œä¸ºæç¤ºåˆä½œä¸ºä»£ç ç”Ÿæˆçš„éªŒè¯ã€‚ä¸ä¼ ç»Ÿä¾èµ–è‡ªç„¶è¯­è¨€æç¤ºçš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„åŸºå‡†å¼ºè°ƒLLMsç›´æ¥ä»æµ‹è¯•ç”¨ä¾‹ä¸­ç†è§£å’Œå®ç°åŠŸèƒ½çš„èƒ½åŠ›ï¼Œåæ˜ äº†ç°å®è½¯ä»¶å¼€å‘å®è·µã€‚åŸºå‡†åŒ…å«1000ä¸ªå¤šæ ·åŒ–çš„æŒ‘æˆ˜ï¼Œæ¶µç›–20ä¸ªåº”ç”¨é¢†åŸŸï¼Œè¯„ä¼°LLMsåœ¨ä¸Šä¸‹æ–‡é•¿åº¦å’Œå¤šç‰¹å¾å¤æ‚æ€§é™åˆ¶ä¸‹ç”Ÿæˆç´§å‡‘ã€åŠŸèƒ½æ€§ä»£ç çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œéµå¾ªæŒ‡ä»¤å’Œä¸Šä¸‹æ–‡å­¦ä¹ æ˜¯TDDæˆåŠŸçš„å…³é”®èƒ½åŠ›ï¼Œè¶…è¶Šäº†é€šç”¨ç¼–ç èƒ½åŠ›æˆ–é¢„è®­ç»ƒçŸ¥è¯†çš„é‡è¦æ€§ã€‚é€šè¿‡å¯¹19ä¸ªå‰æ²¿æ¨¡å‹çš„å…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬æ­ç¤ºäº†æ€§èƒ½ç“¶é¢ˆï¼Œå¦‚é•¿æç¤ºä¸­çš„æŒ‡ä»¤ä¸¢å¤±ï¼Œå¹¶æä¾›äº†æ¶µç›–å¤šä¸ªæ ¹æœ¬åŸå› çš„è¯¦ç»†é”™è¯¯åˆ†æã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†TDDç‰¹å®šåŸºå‡†çš„å®é™…ä»·å€¼ï¼Œå¹¶ä¸ºæå‡LLMåœ¨ä¸¥æ ¼åº”ç”¨é©±åŠ¨ç¼–ç åœºæ™¯ä¸­çš„èƒ½åŠ›å¥ å®šäº†åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯å¦‚ä½•æœ‰æ•ˆè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æµ‹è¯•é©±åŠ¨å¼€å‘ä¸­çš„èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–è‡ªç„¶è¯­è¨€æç¤ºï¼Œæ— æ³•çœŸå®åæ˜ è½¯ä»¶å¼€å‘ä¸­çš„å®é™…éœ€æ±‚å’ŒæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æµ‹è¯•ç”¨ä¾‹ä½œä¸ºæç¤ºå’ŒéªŒè¯æ‰‹æ®µï¼Œå¼ºè°ƒLLMsä»æµ‹è¯•ç”¨ä¾‹ä¸­ç›´æ¥ç†è§£å’Œå®ç°åŠŸèƒ½çš„èƒ½åŠ›ã€‚è¿™ç§è®¾è®¡æ›´è´´è¿‘çœŸå®å¼€å‘ç¯å¢ƒï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯„ä¼°æ¨¡å‹çš„å®é™…åº”ç”¨èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹è¯„ä¼°å’Œæ€§èƒ½åˆ†æä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é›†åŒ…å«1000ä¸ªå¤šæ ·åŒ–çš„æµ‹è¯•ç”¨ä¾‹ï¼Œæ¨¡å‹è¯„ä¼°é€šè¿‡ç”Ÿæˆä»£ç çš„åŠŸèƒ½æ€§å’Œç´§å‡‘æ€§è¿›è¡Œï¼Œæ€§èƒ½åˆ†æåˆ™å…³æ³¨æ¨¡å‹åœ¨é•¿æç¤ºä¸‹çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†æµ‹è¯•ç”¨ä¾‹ä½œä¸ºæç¤ºå’ŒéªŒè¯çš„åŒé‡è§’è‰²ï¼Œçªç ´äº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™ï¼Œä½¿å¾—è¯„ä¼°æ›´å…·é’ˆå¯¹æ€§å’Œå®ç”¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œè€ƒè™‘äº†ä¸Šä¸‹æ–‡é•¿åº¦å’Œå¤šç‰¹å¾å¤æ‚æ€§ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šå¼ºè°ƒäº†æŒ‡ä»¤éµå¾ªå’Œä¸Šä¸‹æ–‡å­¦ä¹ çš„é‡è¦æ€§ï¼Œç½‘ç»œç»“æ„åˆ™ä¼˜åŒ–äº†ç”Ÿæˆä»£ç çš„ç´§å‡‘æ€§å’ŒåŠŸèƒ½æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œ19ä¸ªå‰æ²¿æ¨¡å‹åœ¨éµå¾ªæŒ‡ä»¤å’Œä¸Šä¸‹æ–‡å­¦ä¹ æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—å·®å¼‚ï¼ŒæŸäº›æ¨¡å‹åœ¨é•¿æç¤ºä¸‹çš„æ€§èƒ½ä¸‹é™è¶…è¿‡20%ã€‚é€šè¿‡è¯¦ç»†çš„é”™è¯¯åˆ†æï¼Œæ­ç¤ºäº†æŒ‡ä»¤ä¸¢å¤±ç­‰å¤šä¸ªæ€§èƒ½ç“¶é¢ˆï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è½¯ä»¶å¼€å‘å·¥å…·ã€è‡ªåŠ¨åŒ–æµ‹è¯•ç”Ÿæˆã€ä»£ç å®¡æŸ¥å’Œæ•™è‚²ç­‰ã€‚é€šè¿‡æä¾›ä¸€ä¸ªé’ˆå¯¹TDDçš„è¯„ä¼°åŸºå‡†ï¼Œèƒ½å¤Ÿå¸®åŠ©å¼€å‘è€…æ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨LLMsï¼Œæé«˜è½¯ä»¶å¼€å‘æ•ˆç‡å’Œä»£ç è´¨é‡ã€‚æœªæ¥ï¼Œè¯¥åŸºå‡†å¯èƒ½æ¨åŠ¨LLMsåœ¨æ›´å¤æ‚çš„åº”ç”¨åœºæ™¯ä¸­çš„åº”ç”¨ï¼Œä¿ƒè¿›æ™ºèƒ½ç¼–ç¨‹åŠ©æ‰‹çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce WebApp1K, a novel benchmark for evaluating large language models (LLMs) in test-driven development (TDD) tasks, where test cases serve as both prompt and verification for code generation. Unlike traditional approaches relying on natural language prompts, our benchmark emphasizes the ability of LLMs to interpret and implement functionality directly from test cases, reflecting real-world software development practices. Comprising 1000 diverse challenges across 20 application domains, the benchmark evaluates LLMs on their ability to generate compact, functional code under the constraints of context length and multi-feature complexity. Our findings highlight instruction following and in-context learning as critical capabilities for TDD success, surpassing the importance of general coding proficiency or pretraining knowledge. Through comprehensive evaluation of 19 frontier models, we reveal performance bottlenecks, such as instruction loss in long prompts, and provide a detailed error analysis spanning multiple root causes. This work underscores the practical value of TDD-specific benchmarks and lays the foundation for advancing LLM capabilities in rigorous, application-driven coding scenarios.

