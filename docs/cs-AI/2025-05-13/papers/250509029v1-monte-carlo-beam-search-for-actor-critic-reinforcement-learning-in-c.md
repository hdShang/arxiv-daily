---
layout: default
title: Monte Carlo Beam Search for Actor-Critic Reinforcement Learning in Continuous Control
---

# Monte Carlo Beam Search for Actor-Critic Reinforcement Learning in Continuous Control

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.09029" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.09029v1</a>
  <a href="https://arxiv.org/pdf/2505.09029.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.09029v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.09029v1', 'Monte Carlo Beam Search for Actor-Critic Reinforcement Learning in Continuous Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hazim Alzorgan, Abolfazl Razi

**åˆ†ç±»**: cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè’™ç‰¹å¡æ´›æŸæœç´¢ä»¥æ”¹å–„è¿ç»­æ§åˆ¶ä¸­çš„ç­–ç•¥å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¼”å‘˜-è¯„è®ºå®¶` `è’™ç‰¹å¡æ´›æ–¹æ³•` `æŸæœç´¢` `å¼ºåŒ–å­¦ä¹ ` `è¿ç»­æ§åˆ¶` `ç­–ç•¥ä¼˜åŒ–` `æ ·æœ¬æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¼”å‘˜-è¯„è®ºå®¶æ–¹æ³•ä¾èµ–äºåŸºäºå™ªå£°çš„æ¢ç´¢ï¼Œå¯¼è‡´ç­–ç•¥æ”¶æ•›æ•ˆæœä¸ä½³ã€‚
2. æå‡ºçš„è’™ç‰¹å¡æ´›æŸæœç´¢é€šè¿‡ç»“åˆæŸæœç´¢å’Œè’™ç‰¹å¡æ´›å›æ»šï¼Œæå‡äº†æ¢ç´¢å’ŒåŠ¨ä½œé€‰æ‹©çš„è´¨é‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMCBSåœ¨å¤šä¸ªç¯å¢ƒä¸­æ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œæ ·æœ¬æ•ˆç‡æ˜¾è‘—æé«˜ï¼Œè¾¾åˆ°90%çš„æœ€å¤§å¯è¾¾å¥–åŠ±æ‰€éœ€æ—¶é—´å‡å°‘äº†50%.

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ··åˆæ–¹æ³•â€”â€”è’™ç‰¹å¡æ´›æŸæœç´¢ï¼ˆMCBSï¼‰ï¼Œç»“åˆäº†æŸæœç´¢å’Œè’™ç‰¹å¡æ´›å›æ»šï¼Œä»¥æ”¹å–„ç°æœ‰çš„æ¼”å‘˜-è¯„è®ºå®¶æ–¹æ³•ï¼ˆå¦‚TD3ï¼‰çš„æ¢ç´¢å’ŒåŠ¨ä½œé€‰æ‹©ã€‚MCBSé€šè¿‡åœ¨ç­–ç•¥è¾“å‡ºå‘¨å›´ç”Ÿæˆå¤šä¸ªå€™é€‰åŠ¨ä½œï¼Œå¹¶é€šè¿‡çŸ­æœŸå›æ»šè¿›è¡Œè¯„ä¼°ï¼Œä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿåšå‡ºæ›´ä¸ºæ˜æ™ºçš„é€‰æ‹©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMCBSåœ¨å¤šä¸ªè¿ç»­æ§åˆ¶åŸºå‡†ä¸Šè¡¨ç°å‡ºæ›´é«˜çš„æ ·æœ¬æ•ˆç‡å’Œæ€§èƒ½ï¼Œç›¸è¾ƒäºæ ‡å‡†TD3åŠå…¶ä»–åŸºçº¿æ–¹æ³•ï¼ˆå¦‚SACã€PPOå’ŒA2Cï¼‰æœ‰æ˜¾è‘—æå‡ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†å…³é”®è¶…å‚æ•°ï¼Œå¦‚æŸå®½å’Œå›æ»šæ·±åº¦ï¼Œå¹¶æ¢è®¨äº†è‡ªé€‚åº”ç­–ç•¥ä»¥ä¼˜åŒ–MCBSåœ¨å¤æ‚æ§åˆ¶ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„æ¼”å‘˜-è¯„è®ºå®¶æ–¹æ³•ï¼ˆå¦‚TD3ï¼‰åœ¨æ¢ç´¢è¿‡ç¨‹ä¸­ä¾èµ–äºåŸºæœ¬çš„å™ªå£°æœºåˆ¶ï¼Œè¿™å¯èƒ½å¯¼è‡´ç­–ç•¥æ”¶æ•›ä¸å¤Ÿç†æƒ³ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨ç¯å¢ƒä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºçš„è’™ç‰¹å¡æ´›æŸæœç´¢ï¼ˆMCBSï¼‰é€šè¿‡ç”Ÿæˆå¤šä¸ªå€™é€‰åŠ¨ä½œå¹¶è¿›è¡ŒçŸ­æœŸå›æ»šè¯„ä¼°ï¼Œå¢å¼ºäº†æ™ºèƒ½ä½“çš„å†³ç­–èƒ½åŠ›ï¼Œä»è€Œæ”¹å–„äº†ç­–ç•¥å­¦ä¹ çš„æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMCBSçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ç”Ÿæˆå€™é€‰åŠ¨ä½œã€æ‰§è¡ŒçŸ­æœŸå›æ»šå’Œé€‰æ‹©æœ€ä½³åŠ¨ä½œä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œåœ¨ç­–ç•¥è¾“å‡ºé™„è¿‘ç”Ÿæˆå¤šä¸ªå€™é€‰åŠ¨ä½œï¼›ç„¶åï¼Œé€šè¿‡çŸ­æœŸå›æ»šè¯„ä¼°è¿™äº›å€™é€‰åŠ¨ä½œçš„æ½œåœ¨æ”¶ç›Šï¼›æœ€åï¼Œé€‰æ‹©æ”¶ç›Šæœ€é«˜çš„åŠ¨ä½œæ‰§è¡Œã€‚

**å…³é”®åˆ›æ–°**ï¼šMCBSçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†æŸæœç´¢ä¸è’™ç‰¹å¡æ´›å›æ»šç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„æ¢ç´¢æœºåˆ¶ï¼Œæ˜¾è‘—æé«˜äº†ç­–ç•¥å­¦ä¹ çš„æ•ˆç‡å’Œæ”¶æ•›é€Ÿåº¦ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”å…·æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨MCBSä¸­ï¼ŒæŸå®½å’Œå›æ»šæ·±åº¦æ˜¯ä¸¤ä¸ªå…³é”®è¶…å‚æ•°ï¼Œå½±å“æ¢ç´¢çš„å¹¿åº¦å’Œæ·±åº¦ã€‚é€šè¿‡å¯¹è¿™äº›å‚æ•°çš„ç»†è‡´è°ƒæ•´ï¼ŒMCBSèƒ½å¤Ÿåœ¨å¤æ‚æ§åˆ¶ä»»åŠ¡ä¸­å®ç°æ›´ä¼˜çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMCBSåœ¨å¤šä¸ªè¿ç»­æ§åˆ¶åŸºå‡†ï¼ˆå¦‚HalfCheetah-v4ã€Walker2d-v5å’ŒSwimmer-v5ï¼‰ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºæ ‡å‡†TD3ï¼ŒMCBSåœ¨è¾¾åˆ°90%æœ€å¤§å¯è¾¾å¥–åŠ±æ—¶æ‰€éœ€çš„æ—¶é—´å‡å°‘äº†50%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ ·æœ¬æ•ˆç‡å’Œæ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰éœ€è¦é«˜æ•ˆå†³ç­–çš„è¿ç»­æ§åˆ¶ä»»åŠ¡ã€‚é€šè¿‡æå‡ç­–ç•¥å­¦ä¹ çš„æ•ˆç‡ï¼ŒMCBSèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­å®ç°æ›´å¿«é€Ÿçš„é€‚åº”å’Œæ›´é«˜çš„æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Actor-critic methods, like Twin Delayed Deep Deterministic Policy Gradient (TD3), depend on basic noise-based exploration, which can result in less than optimal policy convergence. In this study, we introduce Monte Carlo Beam Search (MCBS), a new hybrid method that combines beam search and Monte Carlo rollouts with TD3 to improve exploration and action selection. MCBS produces several candidate actions around the policy's output and assesses them through short-horizon rollouts, enabling the agent to make better-informed choices. We test MCBS across various continuous-control benchmarks, including HalfCheetah-v4, Walker2d-v5, and Swimmer-v5, showing enhanced sample efficiency and performance compared to standard TD3 and other baseline methods like SAC, PPO, and A2C. Our findings emphasize MCBS's capability to enhance policy learning through structured look-ahead search while ensuring computational efficiency. Additionally, we offer a detailed analysis of crucial hyperparameters, such as beam width and rollout depth, and explore adaptive strategies to optimize MCBS for complex control tasks. Our method shows a higher convergence rate across different environments compared to TD3, SAC, PPO, and A2C. For instance, we achieved 90% of the maximum achievable reward within around 200 thousand timesteps compared to 400 thousand timesteps for the second-best method.

