---
layout: default
title: AI-Mediated Code Comment Improvement
---

# AI-Mediated Code Comment Improvement

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.09021" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.09021v1</a>
  <a href="https://arxiv.org/pdf/2505.09021.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.09021v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.09021v1', 'AI-Mediated Code Comment Improvement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Maria Dhakal, Chia-Yi Su, Robert Wallace, Chris Fakhimi, Aakash Bansal, Toby Li, Yu Huang, Collin McMillan

**åˆ†ç±»**: cs.SE, cs.AI, cs.PL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºAIçš„ä»£ç æ³¨é‡Šæ”¹è¿›æ–¹æ³•ä»¥æå‡ä»£ç è´¨é‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä»£ç æ³¨é‡Š` `äººå·¥æ™ºèƒ½` `å¤§å‹è¯­è¨€æ¨¡å‹` `è´¨é‡æå‡` `è½¯ä»¶å¼€å‘` `å¯è¯»æ€§` `æ•°æ®æ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ä»£ç æ³¨é‡Šå¾€å¾€ç¼ºä¹æ¸…æ™°æ€§å’Œä¸€è‡´æ€§ï¼Œå½±å“ä»£ç çš„å¯è¯»æ€§å’Œç»´æŠ¤æ€§ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç¨‹åºï¼Œæ—¨åœ¨æ ¹æ®ç‰¹å®šè´¨é‡ç»´åº¦é‡å†™å’Œä¼˜åŒ–ä»£ç æ³¨é‡Šã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¯¥æ–¹æ³•åï¼Œä»£ç æ³¨é‡Šåœ¨å¤šä¸ªè´¨é‡ç»´åº¦ä¸Šå¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ï¼Œæå‡äº†ä»£ç çš„å¯ç†è§£æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æè¿°äº†ä¸€ç§é€šè¿‡å®šåˆ¶çš„äººå·¥æ™ºèƒ½å·¥å…·æ”¹è¿›ä»£ç æ³¨é‡Šè´¨é‡çš„æ–¹æ³•ã€‚æˆ‘ä»¬è¿›è¡Œäº†å®è¯ç ”ç©¶ï¼Œå¹¶é€šè¿‡æ‰æ ¹ç†è®ºå®šæ€§åˆ†æç¡®å®šéœ€è¦æ”¹è¿›çš„è´¨é‡ç»´åº¦ã€‚éšåï¼Œæå‡ºäº†ä¸€ç§ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é‡å†™ç°æœ‰ä»£ç æ³¨é‡Šçš„ç¨‹åºã€‚æˆ‘ä»¬ä½¿ç”¨GPT-4oå®ç°è¯¥ç¨‹åºï¼Œå¹¶å°†ç»“æœæç‚¼ä¸ºä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹ï¼Œä»¥ä¾¿ç”¨æˆ·èƒ½å¤Ÿä¿æŒæ•°æ®æ§åˆ¶ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä½¿ç”¨GPT-4oå’Œæç‚¼æ¨¡å‹ç‰ˆæœ¬çš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜è¯¥ç¨‹åºåœ¨å¤šä¸ªè´¨é‡ç»´åº¦ä¸Šæœ‰æ•ˆæ”¹å–„äº†ä»£ç æ³¨é‡Šã€‚æ‰€æœ‰æ•°æ®å’Œæºä»£ç å·²åœ¨åœ¨çº¿ä»“åº“ä¸­å‘å¸ƒï¼Œä»¥ç¡®ä¿å¯é‡å¤æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ä»£ç æ³¨é‡Šè´¨é‡ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆæå‡æ³¨é‡Šçš„æ¸…æ™°åº¦å’Œä¸€è‡´æ€§ï¼Œå¯¼è‡´ä»£ç å¯è¯»æ€§å·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¦‚GPT-4oï¼Œé‡å†™ç°æœ‰ä»£ç æ³¨é‡Šï¼Œé’ˆå¯¹ç‰¹å®šçš„è´¨é‡ç»´åº¦è¿›è¡Œä¼˜åŒ–ï¼Œä»¥æé«˜æ³¨é‡Šçš„è´¨é‡å’Œå¯è¯»æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆè¿›è¡Œè´¨é‡ç»´åº¦çš„ç¡®å®šï¼Œå…¶æ¬¡ä½¿ç”¨LLMé‡å†™æ³¨é‡Šï¼Œæœ€åå°†ç»“æœæç‚¼ä¸ºå¯åœ¨æœ¬åœ°è¿è¡Œçš„æ¨¡å‹ï¼Œä»¥ç¡®ä¿ç”¨æˆ·æ•°æ®çš„æ§åˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºç»“åˆäº†å®šæ€§åˆ†æä¸LLMæŠ€æœ¯ï¼Œæå‡ºäº†ä¸€ç§ç³»ç»ŸåŒ–çš„æ³¨é‡Šæ”¹è¿›æ–¹æ³•ï¼Œæ˜¾è‘—ä¸åŒäºä¼ ç»Ÿçš„æ‰‹åŠ¨æ³¨é‡Šæ”¹è¿›æ–¹å¼ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ³¨é‡Šçš„è´¨é‡ï¼Œå¹¶é€šè¿‡å®éªŒç¡®å®šäº†æœ€ä½³çš„å‚æ•°è®¾ç½®ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„æ³¨é‡Šæ—¢å‡†ç¡®åˆæ˜“äºç†è§£ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨GPT-4oé‡å†™çš„ä»£ç æ³¨é‡Šåœ¨æ¸…æ™°åº¦å’Œä¸€è‡´æ€§æ–¹é¢ç›¸æ¯”äºåŸå§‹æ³¨é‡Šæå‡äº†çº¦30%ã€‚æ­¤å¤–ï¼Œæç‚¼åçš„æ¨¡å‹åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½å¤Ÿåœ¨æœ¬åœ°ç¯å¢ƒä¸­è¿è¡Œï¼Œç¡®ä¿äº†æ•°æ®çš„å®‰å…¨æ€§å’Œéšç§æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è½¯ä»¶å¼€å‘ã€ä»£ç å®¡æŸ¥å’Œæ•™è‚²ç­‰ã€‚é€šè¿‡æå‡ä»£ç æ³¨é‡Šçš„è´¨é‡ï¼Œå¼€å‘è€…èƒ½å¤Ÿæ›´é«˜æ•ˆåœ°ç†è§£å’Œç»´æŠ¤ä»£ç ï¼Œè¿›è€Œæé«˜è½¯ä»¶å¼€å‘çš„æ•´ä½“æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæ”¹è¿›çš„æ³¨é‡Šä¹Ÿæœ‰åŠ©äºæ–°æ‰‹å­¦ä¹ ç¼–ç¨‹å’Œç†è§£å¤æ‚ä»£ç ç»“æ„ï¼Œå…·æœ‰é‡è¦çš„æ•™è‚²ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper describes an approach to improve code comments along different quality axes by rewriting those comments with customized Artificial Intelligence (AI)-based tools. We conduct an empirical study followed by grounded theory qualitative analysis to determine the quality axes to improve. Then we propose a procedure using a Large Language Model (LLM) to rewrite existing code comments along the quality axes. We implement our procedure using GPT-4o, then distil the results into a smaller model capable of being run in-house, so users can maintain data custody. We evaluate both our approach using GPT-4o and the distilled model versions. We show in an evaluation how our procedure improves code comments along the quality axes. We release all data and source code in an online repository for reproducibility.

