---
layout: default
title: "Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification"
---

# Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.09031" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.09031v1</a>
  <a href="https://arxiv.org/pdf/2505.09031.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.09031v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.09031v1', 'Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Adarsh Kumar, Hwiyoon Kim, Jawahar Sai Nathani, Neil Roy

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»“åˆCoTä¸RAGç­‰æ–¹æ³•ä»¥å‡å°‘LLMçš„å¹»è§‰ç°è±¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `é“¾å¼æ€ç»´` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `è‡ªä¸€è‡´æ€§` `è‡ªéªŒè¯` `å¹»è§‰ç°è±¡` `å¤šæ­¥éª¤æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶ï¼ŒLLMså¸¸å¸¸äº§ç”Ÿå¹»è§‰ç°è±¡ï¼Œå¯¼è‡´ç”Ÿæˆçš„ä¿¡æ¯ä¸å‡†ç¡®æˆ–æ— å…³ã€‚
2. æœ¬ç ”ç©¶æå‡ºå°†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç›¸ç»“åˆï¼Œå¹¶å¼•å…¥è‡ªä¸€è‡´æ€§å’Œè‡ªéªŒè¯ç­–ç•¥ï¼Œä»¥æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆè¿™äº›æ–¹æ³•åï¼Œæ¨¡å‹åœ¨å‡å°‘å¹»è§‰ç°è±¡çš„åŒæ—¶ï¼Œä¿æŒäº†è‰¯å¥½çš„æµç•…æ€§å’Œæ¨ç†æ·±åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¹»è§‰ç°è±¡ï¼Œå³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆè‡ªä¿¡ä½†é”™è¯¯æˆ–æ— å…³çš„ä¿¡æ¯ï¼Œä»ç„¶æ˜¯å…¶åœ¨å¤æ‚å¼€æ”¾ä»»åŠ¡ä¸­åº”ç”¨çš„ä¸»è¦é™åˆ¶ã€‚é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºæ³•å·²æˆä¸ºæ”¹å–„å¤šæ­¥éª¤æ¨ç†çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†å•ç‹¬ä½¿ç”¨CoTå¹¶ä¸èƒ½å®Œå…¨è§£å†³å¹»è§‰é—®é¢˜ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•å°†CoTä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç›¸ç»“åˆï¼Œå¹¶åº”ç”¨è‡ªä¸€è‡´æ€§å’Œè‡ªéªŒè¯ç­–ç•¥ï¼Œä»¥å‡å°‘å¹»è§‰å¹¶æé«˜äº‹å®å‡†ç¡®æ€§ã€‚é€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥å¤–éƒ¨çŸ¥è¯†æºï¼Œå¹¶ä½¿æ¨¡å‹èƒ½å¤ŸéªŒè¯æˆ–ä¿®æ­£è‡ªèº«è¾“å‡ºï¼Œæˆ‘ä»¬æ—¨åœ¨ç”Ÿæˆæ›´å‡†ç¡®å’Œè¿è´¯çš„å“åº”ã€‚æˆ‘ä»¬å¯¹åŸºçº¿LLMsä¸CoTã€CoT+RAGã€è‡ªä¸€è‡´æ€§å’Œè‡ªéªŒè¯æŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒè¯„ä¼°ï¼Œç»“æœçªæ˜¾äº†æ¯ç§æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯†åˆ«å‡ºæœ€ç¨³å¥çš„å‡å°‘å¹»è§‰çš„æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒæµç•…æ€§å’Œæ¨ç†æ·±åº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­äº§ç”Ÿçš„å¹»è§‰ç°è±¡ï¼Œå³ç”Ÿæˆè‡ªä¿¡ä½†é”™è¯¯çš„ä¿¡æ¯ã€‚ç°æœ‰æ–¹æ³•å¦‚é“¾å¼æ€ç»´ï¼ˆCoTï¼‰è™½ç„¶æœ‰åŠ©äºæ¨ç†ï¼Œä½†ä»æ— æ³•å®Œå…¨æ¶ˆé™¤å¹»è§‰é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡ç»“åˆé“¾å¼æ€ç»´ï¼ˆCoTï¼‰ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ï¼Œå¹¶å¼•å…¥è‡ªä¸€è‡´æ€§å’Œè‡ªéªŒè¯ç­–ç•¥ï¼Œæ¥å‡å°‘å¹»è§‰ç°è±¡å¹¶æé«˜æ¨¡å‹çš„äº‹å®å‡†ç¡®æ€§ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨é€šè¿‡å¤–éƒ¨çŸ¥è¯†æºå’Œæ¨¡å‹è‡ªæˆ‘éªŒè¯æ¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å››ä¸ªä¸»è¦æ¨¡å—ï¼šé“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨¡å—ç”¨äºå¼•å¯¼æ¨ç†è¿‡ç¨‹ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¨¡å—ç”¨äºå¼•å…¥å¤–éƒ¨çŸ¥è¯†ï¼Œè‡ªä¸€è‡´æ€§æ¨¡å—ç”¨äºç¡®ä¿è¾“å‡ºçš„ä¸€è‡´æ€§ï¼Œè‡ªéªŒè¯æ¨¡å—ç”¨äºæ¨¡å‹è‡ªæˆ‘æ£€æŸ¥å’Œä¿®æ­£ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºå°†CoTä¸RAGç›¸ç»“åˆï¼Œå¹¶å¼•å…¥è‡ªä¸€è‡´æ€§å’Œè‡ªéªŒè¯ç­–ç•¥ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å•ä¸€æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡ç”Ÿæˆçš„æµç•…æ€§ä¸å‡†ç¡®æ€§ï¼ŒåŒæ—¶åœ¨è‡ªéªŒè¯é˜¶æ®µå¼•å…¥äº†é˜ˆå€¼è®¾ç½®ï¼Œä»¥ç¡®ä¿æ¨¡å‹è¾“å‡ºçš„å¯ä¿¡åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»“åˆCoTä¸RAGçš„æ–¹æ³•åœ¨å‡å°‘å¹»è§‰ç°è±¡æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œå‡†ç¡®æ€§æå‡äº†çº¦15%ï¼ŒåŒæ—¶ä¿æŒäº†æµç•…æ€§å’Œæ¨ç†æ·±åº¦ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€å¯¹è¯ç”Ÿæˆã€å†…å®¹åˆ›ä½œç­‰ã€‚é€šè¿‡å‡å°‘å¹»è§‰ç°è±¡ï¼Œæå‡æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œèƒ½å¤Ÿåœ¨æ›´å¤æ‚çš„å®é™…åœºæ™¯ä¸­åº”ç”¨LLMsï¼Œå¢å¼ºå…¶å•†ä¸šä»·å€¼å’Œç¤¾ä¼šå½±å“åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Hallucination, where large language models (LLMs) generate confident but incorrect or irrelevant information, remains a key limitation in their application to complex, open-ended tasks. Chain-of-thought (CoT) prompting has emerged as a promising method for improving multistep reasoning by guiding models through intermediate steps. However, CoT alone does not fully address the hallucination problem. In this work, we investigate how combining CoT with retrieval-augmented generation (RAG), as well as applying self-consistency and self-verification strategies, can reduce hallucinations and improve factual accuracy. By incorporating external knowledge sources during reasoning and enabling models to verify or revise their own outputs, we aim to generate more accurate and coherent responses. We present a comparative evaluation of baseline LLMs against CoT, CoT+RAG, self-consistency, and self-verification techniques. Our results highlight the effectiveness of each method and identify the most robust approach for minimizing hallucinations while preserving fluency and reasoning depth.

