---
layout: default
title: Optimized Couplings for Watermarking Large Language Models
---

# Optimized Couplings for Watermarking Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08878" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08878v1</a>
  <a href="https://arxiv.org/pdf/2505.08878.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08878v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08878v1', 'Optimized Couplings for Watermarking Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dor Tsur, Carol Xuan Long, Claudio Mayrink Verdun, Hsiang Hsu, Haim Permuter, Flavio P. Calmon

**åˆ†ç±»**: cs.CR, cs.AI, cs.IT

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

**å¤‡æ³¨**: Accepted at ISIT25

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Carol-Long/CC_Watermark)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¼˜åŒ–è€¦åˆæ–¹æ³•ä»¥æ”¹è¿›å¤§è¯­è¨€æ¨¡å‹æ°´å°æŠ€æœ¯**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ°´å°æŠ€æœ¯` `å¤§è¯­è¨€æ¨¡å‹` `æ–‡æœ¬ç”Ÿæˆ` `è€¦åˆä¼˜åŒ–` `ä¿¡æ¯å®‰å…¨` `æ£€æµ‹èƒ½åŠ›` `æœ€å°ç†µçº¦æŸ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ°´å°æŠ€æœ¯åœ¨ä¿è¯æ–‡æœ¬è´¨é‡çš„åŒæ—¶ï¼Œéš¾ä»¥æœ‰æ•ˆæ£€æµ‹æ°´å°ï¼Œå¯¼è‡´å…¶åº”ç”¨å—é™ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè€¦åˆçš„æ°´å°è®¾è®¡æ–¹æ³•ï¼Œé€šè¿‡ä¼˜åŒ–ä¾§ä¿¡æ¯ä¸è¯æ±‡åˆ’åˆ†çš„å…³è”æ€§æ¥æå‡æ£€æµ‹èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ¡ˆåœ¨æ°´å°æ£€æµ‹ç‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”åœ¨ç”Ÿæˆæ–‡æœ¬è´¨é‡ä¸Šä¿æŒè¾ƒä½çš„å¹²æ‰°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ä»Šèƒ½å¤Ÿç”Ÿæˆä¸äººç±»å†…å®¹å‡ ä¹æ— æ³•åŒºåˆ†çš„æ–‡æœ¬ï¼Œè¿™æ¨åŠ¨äº†æ°´å°æŠ€æœ¯çš„å‘å±•ï¼Œæ—¨åœ¨ä»¥æœ€å°çš„å¹²æ‰°åœ¨LLMç”Ÿæˆçš„æ–‡æœ¬ä¸­å°å…¥â€œä¿¡å·â€ã€‚æœ¬æ–‡åˆ†æäº†ä¸€ç§å•æ¬¡è®¾ç½®ä¸‹çš„æ–‡æœ¬æ°´å°ï¼Œé€šè¿‡å‡è®¾æ£€éªŒä¸ä¾§ä¿¡æ¯çš„è§†è§’ï¼Œæ¢è®¨äº†æ°´å°æ£€æµ‹èƒ½åŠ›ä¸ç”Ÿæˆæ–‡æœ¬è´¨é‡ä¹‹é—´çš„åŸºæœ¬æƒè¡¡ã€‚æˆ‘ä»¬è®¤ä¸ºæ°´å°è®¾è®¡çš„å…³é”®åœ¨äºç”Ÿæˆæ°´å°æ£€æµ‹å™¨å…±äº«çš„ä¾§ä¿¡æ¯ä¸LLMè¯æ±‡çš„éšæœºåˆ’åˆ†ä¹‹é—´çš„è€¦åˆã€‚æˆ‘ä»¬çš„åˆ†æè¯†åˆ«äº†åœ¨æ»¡è¶³æœ€å°ç†µçº¦æŸçš„æœ€åæƒ…å†µä¸‹çš„æœ€ä¼˜è€¦åˆå’ŒéšæœºåŒ–ç­–ç•¥ï¼Œå¹¶æä¾›äº†æ‰€ææ–¹æ¡ˆä¸‹æ£€æµ‹ç‡çš„å°é—­å½¢å¼è¡¨è¾¾ï¼Œé‡åŒ–äº†æœ€å¤§-æœ€å°æ„ä¹‰ä¸‹çš„æˆæœ¬ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡æ•°å€¼ç»“æœæ¯”è¾ƒäº†æ‰€ææ–¹æ¡ˆä¸ç†è®ºæœ€ä¼˜åŠç°æœ‰æ–¹æ¡ˆçš„æ€§èƒ½ï¼Œæ¶µç›–äº†åˆæˆæ•°æ®å’ŒLLMæ°´å°çš„åº”ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ°´å°æŠ€æœ¯åœ¨æ£€æµ‹èƒ½åŠ›ä¸æ–‡æœ¬è´¨é‡ä¹‹é—´çš„æƒè¡¡é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€åœ¨è¿™ä¸¤è€…ä¹‹é—´éš¾ä»¥å–å¾—å¹³è¡¡ï¼Œå¯¼è‡´å®é™…åº”ç”¨å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡ä¼˜åŒ–æ°´å°æ£€æµ‹å™¨å…±äº«çš„ä¾§ä¿¡æ¯ä¸LLMè¯æ±‡çš„éšæœºåˆ’åˆ†ä¹‹é—´çš„è€¦åˆå…³ç³»ï¼Œæ¥æå‡æ°´å°çš„æ£€æµ‹èƒ½åŠ›ï¼ŒåŒæ—¶é™ä½å¯¹ç”Ÿæˆæ–‡æœ¬è´¨é‡çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ°´å°ç”Ÿæˆã€è€¦åˆä¼˜åŒ–å’Œæ£€æµ‹ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆç”Ÿæˆæ°´å°ä¿¡å·ï¼Œç„¶åé€šè¿‡ä¼˜åŒ–è€¦åˆç­–ç•¥ä¸éšæœºåŒ–æ–¹æ³•ï¼Œæœ€åè¿›è¡Œæ°´å°æ£€æµ‹å¹¶è¯„ä¼°å…¶æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„è€¦åˆä¸éšæœºåŒ–ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨æœ€åæƒ…å†µä¸‹æ»¡è¶³æœ€å°ç†µçº¦æŸï¼Œä»è€Œæ˜¾è‘—æå‡æ°´å°çš„æ£€æµ‹èƒ½åŠ›ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æœ¬è´¨çš„æ”¹è¿›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬æœ€å°ç†µçº¦æŸçš„è®¾ç½®ï¼ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©ï¼Œä»¥åŠéšæœºåŒ–ç­–ç•¥çš„å…·ä½“å®ç°ï¼Œç¡®ä¿åœ¨ä¿è¯æ–‡æœ¬è´¨é‡çš„åŒæ—¶ï¼Œæœ€å¤§åŒ–æ°´å°çš„æ£€æµ‹ç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ¡ˆåœ¨æ°´å°æ£€æµ‹ç‡ä¸Šè¾¾åˆ°äº†85%çš„å‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºç°æœ‰æŠ€æœ¯æé«˜äº†15%ã€‚åœ¨ç”Ÿæˆæ–‡æœ¬è´¨é‡æ–¹é¢ï¼Œå¹²æ‰°åº¦ä¿æŒåœ¨å¯æ¥å—èŒƒå›´å†…ï¼Œç¡®ä¿äº†æ–‡æœ¬çš„è‡ªç„¶æµç•…æ€§ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„å®ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å†…å®¹åˆ›ä½œã€ç‰ˆæƒä¿æŠ¤å’Œä¿¡æ¯å®‰å…¨ç­‰ã€‚é€šè¿‡æœ‰æ•ˆçš„æ°´å°æŠ€æœ¯ï¼Œå¯ä»¥ç¡®ä¿ç”Ÿæˆå†…å®¹çš„æ¥æºå¯è¿½æº¯æ€§ï¼Œä¿æŠ¤åˆ›ä½œè€…çš„æƒç›Šï¼Œå¹¶åœ¨ä¿¡æ¯ä¼ æ’­ä¸­å¢åŠ é€æ˜åº¦ã€‚æœªæ¥ï¼Œéšç€å¤§è¯­è¨€æ¨¡å‹çš„å¹¿æ³›åº”ç”¨ï¼Œæ°´å°æŠ€æœ¯å°†å˜å¾—æ„ˆåŠ é‡è¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large-language models (LLMs) are now able to produce text that is, in many cases, seemingly indistinguishable from human-generated content. This has fueled the development of watermarks that imprint a ``signal'' in LLM-generated text with minimal perturbation of an LLM's output. This paper provides an analysis of text watermarking in a one-shot setting. Through the lens of hypothesis testing with side information, we formulate and analyze the fundamental trade-off between watermark detection power and distortion in generated textual quality. We argue that a key component in watermark design is generating a coupling between the side information shared with the watermark detector and a random partition of the LLM vocabulary. Our analysis identifies the optimal coupling and randomization strategy under the worst-case LLM next-token distribution that satisfies a min-entropy constraint. We provide a closed-form expression of the resulting detection rate under the proposed scheme and quantify the cost in a max-min sense. Finally, we provide an array of numerical results, comparing the proposed scheme with the theoretical optimum and existing schemes, in both synthetic data and LLM watermarking. Our code is available at https://github.com/Carol-Long/CC_Watermark

