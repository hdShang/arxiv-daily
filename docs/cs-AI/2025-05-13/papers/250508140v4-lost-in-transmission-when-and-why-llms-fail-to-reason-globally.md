---
layout: default
title: Lost in Transmission: When and Why LLMs Fail to Reason Globally
---

# Lost in Transmission: When and Why LLMs Fail to Reason Globally

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08140" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08140v4</a>
  <a href="https://arxiv.org/pdf/2505.08140.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08140v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08140v4', 'Lost in Transmission: When and Why LLMs Fail to Reason Globally')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tobias Schnabel, Kiran Tomlinson, Adith Swaminathan, Jennifer Neville

**åˆ†ç±»**: cs.AI, cs.FL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13 (æ›´æ–°: 2025-10-24)

**å¤‡æ³¨**: 36 pages; accepted to NeurIPS '25 (spotlight)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBAPOæ¨¡å‹ä»¥è§£å†³LLMsåœ¨å¤æ‚æ¨ç†ä¸­çš„å±€é™æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤æ‚æ¨ç†` `æœ‰ç•Œæ³¨æ„åŠ›` `ä¿¡æ¯æµåŠ¨` `æ€ç»´é“¾` `æ¨ç†é—®é¢˜åˆ†ç±»` `æ¨¡å‹æ¶æ„` `å¸¦å®½é™åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMsåœ¨å¤„ç†éœ€è¦å¤æ‚æ¨ç†çš„ä»»åŠ¡æ—¶ï¼Œå¸¸å¸¸å› ä¸ºä¿¡æ¯æµåŠ¨çš„å®¹é‡é™åˆ¶è€Œå¤±è´¥ã€‚
2. è®ºæ–‡æå‡ºäº†æœ‰ç•Œæ³¨æ„å‰ç¼€Oracleï¼ˆBAPOï¼‰æ¨¡å‹ï¼Œæ—¨åœ¨æ¨¡æ‹ŸLLMsä¸­æ³¨æ„åŠ›å¤´çš„å¸¦å®½çº¦æŸï¼Œä»è€Œè§£å†³æ¨ç†é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸»æµLLMsåœ¨BAPO-easyä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨BAPO-hardä»»åŠ¡ä¸Šå¤±è´¥ï¼ŒéªŒè¯äº†ç†è®ºé¢„æµ‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å˜æ¢å™¨åŸºç¡€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å–å¾—äº†è¯¸å¤šæˆåŠŸï¼Œä½†åœ¨éœ€è¦å¯¹è¾“å…¥çš„è¾ƒå¤§éƒ¨åˆ†è¿›è¡Œå¤æ‚æ¨ç†çš„ä»»åŠ¡ä¸­ä»ç„¶å­˜åœ¨å›°éš¾ã€‚æœ¬æ–‡è®¤ä¸ºï¼Œè¿™äº›å¤±è´¥æºäºLLMså†…éƒ¨ä¿¡æ¯æµåŠ¨çš„å®¹é‡é™åˆ¶ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼•å…¥äº†æœ‰ç•Œæ³¨æ„å‰ç¼€Oracleï¼ˆBAPOï¼‰æ¨¡å‹ï¼Œä½œä¸ºä¸€ç§æ–°çš„è®¡ç®—æ¡†æ¶ï¼Œæ¨¡æ‹Ÿæ³¨æ„åŠ›å¤´çš„å¸¦å®½çº¦æŸã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¤šä¸ªé‡è¦çš„æ¨ç†é—®é¢˜å¦‚å›¾çš„å¯è¾¾æ€§éœ€è¦é«˜é€šä¿¡å¸¦å®½æ‰èƒ½è¢«BAPOsè§£å†³ï¼›è¿™äº›é—®é¢˜è¢«ç§°ä¸ºBAPO-hardã€‚å®éªŒç»“æœéªŒè¯äº†ç†è®ºé¢„æµ‹ï¼Œæ˜¾ç¤ºGPT-4oã€Claudeå’ŒGeminiåœ¨BAPO-easyä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç›¸å¯¹è¾ƒå°çš„BAPO-hardä»»åŠ¡ä¸Šå¤±è´¥ã€‚æ­¤å¤–ï¼ŒBAPOsè¿˜æ­ç¤ºäº†æ€ç»´é“¾ï¼ˆCoTï¼‰çš„å¦ä¸€ä¸ªå¥½å¤„ï¼šé€šè¿‡CoTåˆ†è§£ä»»åŠ¡å¯ä»¥å°†ä»»ä½•BAPO-hardé—®é¢˜è½¬åŒ–ä¸ºBAPO-easyé—®é¢˜ã€‚ç ”ç©¶ç»“æœä¸ºLLMsçš„å…³é”®å¤±è´¥æä¾›äº†åŸåˆ™æ€§è§£é‡Šï¼Œå¹¶å»ºè®®äº†ç¼“è§£å¸¦å®½é™åˆ¶çš„æ¶æ„å’Œæ¨ç†æ–¹æ³•çš„æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯ç”±äºä¿¡æ¯æµåŠ¨çš„å¸¦å®½é™åˆ¶å¯¼è‡´çš„å¤±è´¥ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆå¤„ç†éœ€è¦é«˜é€šä¿¡å¸¦å®½çš„æ¨ç†é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥BAPOæ¨¡å‹ï¼Œé€šè¿‡æ¨¡æ‹Ÿæ³¨æ„åŠ›å¤´çš„å¸¦å®½é™åˆ¶ï¼Œæä¾›ä¸€ä¸ªæ–°çš„è§†è§’æ¥ç†è§£LLMsåœ¨æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¿™æ ·çš„è®¾è®¡æœ‰åŠ©äºè¯†åˆ«å’Œåˆ†ç±»ä¸åŒéš¾åº¦çš„æ¨ç†é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBAPOæ¨¡å‹çš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹æ³¨æ„åŠ›æœºåˆ¶çš„å¸¦å®½çº¦æŸå»ºæ¨¡ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬ä¿¡æ¯æµåŠ¨çš„åˆ†æå’Œæ¨ç†é—®é¢˜çš„åˆ†ç±»ã€‚é€šè¿‡å¯¹æ¯”ä¸åŒä»»åŠ¡çš„å¸¦å®½éœ€æ±‚ï¼Œè¯„ä¼°æ¨¡å‹çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†BAPO-hardå’ŒBAPO-easyçš„åˆ†ç±»ï¼Œæ˜ç¡®äº†ä¸åŒæ¨ç†é—®é¢˜å¯¹å¸¦å®½çš„éœ€æ±‚ã€‚è¿™ä¸€åˆ†ç±»æ–¹æ³•ä¸ç°æœ‰çš„æ¨ç†æ¨¡å‹æœ‰æœ¬è´¨åŒºåˆ«ï¼Œæä¾›äº†æ›´æ·±å…¥çš„ç†è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬æ³¨æ„åŠ›å¤´çš„æ•°é‡å’Œå¸¦å®½é™åˆ¶çš„è®¾å®šã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†æ¨ç†ä»»åŠ¡çš„å¤æ‚æ€§ï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ å’Œé€‚åº”ä¸åŒçš„æ¨ç†åœºæ™¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-4oã€Claudeå’ŒGeminiåœ¨BAPO-easyä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨BAPO-hardä»»åŠ¡ä¸Šå¤±è´¥ï¼ŒéªŒè¯äº†ç†è®ºé¢„æµ‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰å¯ä»¥å°†BAPO-hardé—®é¢˜è½¬åŒ–ä¸ºBAPO-easyé—®é¢˜ï¼Œä»è€Œæ˜¾è‘—æå‡æ¨ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œå¤æ‚å†³ç­–æ”¯æŒç³»ç»Ÿã€‚é€šè¿‡æ”¹è¿›LLMsçš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥åœ¨æ›´å¹¿æ³›çš„åœºæ™¯ä¸­æå‡å…¶åº”ç”¨æ•ˆæœï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤æ‚é€»è¾‘æ¨ç†çš„ä»»åŠ¡ä¸­ã€‚æœªæ¥ï¼ŒBAPOæ¨¡å‹çš„æ€è·¯å¯èƒ½ä¼šå½±å“LLMsçš„æ¶æ„è®¾è®¡å’Œæ¨ç†æ–¹æ³•ï¼Œæ¨åŠ¨æ›´æ™ºèƒ½çš„AIç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite their many successes, transformer-based large language models (LLMs) continue to struggle with tasks that require complex reasoning over large parts of their input. We argue that these failures arise due to capacity limits on the accurate flow of information within LLMs. To formalize this issue, we introduce the bounded attention prefix oracle (BAPO) model, a new computational framework that models bandwidth constraints on attention heads, the mechanism for internal communication in LLMs. We show that several important reasoning problems like graph reachability require high communication bandwidth for BAPOs to solve; we call these problems BAPO-hard. Our experiments corroborate our theoretical predictions: GPT-4o, Claude, and Gemini succeed on BAPO-easy tasks and fail even on relatively small BAPO-hard tasks. BAPOs also reveal another benefit of chain of thought (CoT): we prove that breaking down a task using CoT can turn any BAPO-hard problem into a BAPO-easy one. Our results offer principled explanations for key LLM failures and suggest directions for architectures and inference methods that mitigate bandwidth limits.

