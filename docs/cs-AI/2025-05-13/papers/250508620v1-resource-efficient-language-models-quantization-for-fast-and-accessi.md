---
layout: default
title: Resource-Efficient Language Models: Quantization for Fast and Accessible Inference
---

# Resource-Efficient Language Models: Quantization for Fast and Accessible Inference

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08620" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08620v1</a>
  <a href="https://arxiv.org/pdf/2505.08620.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08620v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08620v1', 'Resource-Efficient Language Models: Quantization for Fast and Accessible Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tollef Emil JÃ¸rgensen

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

**å¤‡æ³¨**: 17 pages, 9 figures, preprint

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåè®­ç»ƒé‡åŒ–æŠ€æœ¯ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åè®­ç»ƒé‡åŒ–` `æ¨ç†æ•ˆç‡` `èµ„æºä¼˜åŒ–` `å¤§è¯­è¨€æ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `èƒ½è€—é™ä½` `ç¡¬ä»¶é€‚åº”æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶å¯¹ç¡¬ä»¶èµ„æºçš„é«˜éœ€æ±‚å’Œèƒ½è€—é—®é¢˜æ˜¯å½“å‰çš„ä¸»è¦æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç³»åˆ—åè®­ç»ƒé‡åŒ–æŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–é‡åŒ–æ–¹æ¡ˆæ¥æé«˜æ¨ç†æ•ˆç‡ã€‚
3. ç ”ç©¶è¡¨æ˜ï¼Œé‡‡ç”¨åè®­ç»ƒé‡åŒ–æŠ€æœ¯å¯ä»¥æ˜¾è‘—é™ä½æ¨¡å‹çš„èµ„æºæ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒæ¨ç†æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶å¯¹ç¡¬ä»¶çš„é«˜è¦æ±‚å’Œèƒ½è€—é—®é¢˜ä¸¥é‡åˆ¶çº¦äº†å…¶æ™®åŠã€‚æœ¬æ–‡å¯¹åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰æŠ€æœ¯è¿›è¡Œäº†æ·±å…¥çš„é«˜å±‚æ¬¡å›é¡¾ï¼Œæ—¨åœ¨ä¼˜åŒ–æœ€ç»ˆç”¨æˆ·çš„æ¨ç†æ•ˆç‡ã€‚æ–‡ç« è¯¦ç»†ä»‹ç»äº†å¤šç§é‡åŒ–æ–¹æ¡ˆã€ç²’åº¦åŠå…¶æƒè¡¡ï¼ŒåŠ›æ±‚åœ¨ç†è®ºä¸åè®­ç»ƒé‡åŒ–çš„åº”ç”¨ä¹‹é—´æä¾›å¹³è¡¡çš„æ¦‚è¿°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯¹ç¡¬ä»¶èµ„æºçš„é«˜éœ€æ±‚å’Œèƒ½è€—é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•åœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶æœ‰æ•ˆé™ä½èµ„æºæ¶ˆè€—ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åè®­ç»ƒé‡åŒ–æŠ€æœ¯ï¼Œä¼˜åŒ–æ¨¡å‹çš„æ¨ç†æ•ˆç‡ã€‚é€šè¿‡å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œé‡åŒ–ï¼Œå‡å°‘è®¡ç®—å’Œå­˜å‚¨éœ€æ±‚ï¼Œä»è€Œä½¿æ¨¡å‹æ›´æ˜“äºéƒ¨ç½²å’Œä½¿ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªé˜¶æ®µï¼šé¦–å…ˆæ˜¯æ¨¡å‹è®­ç»ƒé˜¶æ®µï¼Œç„¶åè¿›è¡Œåè®­ç»ƒé‡åŒ–ï¼Œæœ€åè¯„ä¼°é‡åŒ–æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬é‡åŒ–æ–¹æ¡ˆé€‰æ‹©ã€å‚æ•°è°ƒæ•´å’Œæ€§èƒ½è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†å¤šç§é‡åŒ–æ–¹æ¡ˆå’Œç²’åº¦é€‰æ‹©ï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚è¿›è¡Œçµæ´»é…ç½®ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€é‡åŒ–ç­–ç•¥å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨é‡åŒ–è¿‡ç¨‹ä¸­ï¼Œå…³é”®å‚æ•°è®¾ç½®åŒ…æ‹¬é‡åŒ–ä½å®½ã€é‡åŒ–ç­–ç•¥ï¼ˆå¦‚å¯¹ç§°ä¸éå¯¹ç§°é‡åŒ–ï¼‰ä»¥åŠæŸå¤±å‡½æ•°çš„é€‰æ‹©ã€‚è¿™äº›è®¾è®¡å†³å®šäº†é‡åŒ–åæ¨¡å‹çš„æ€§èƒ½å’Œèµ„æºæ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨åè®­ç»ƒé‡åŒ–æŠ€æœ¯åï¼Œæ¨¡å‹çš„æ¨ç†é€Ÿåº¦æå‡äº†40%ï¼ŒåŒæ—¶å†…å­˜å ç”¨å‡å°‘äº†60%ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œé‡åŒ–æ¨¡å‹åœ¨ä¿æŒç›¸ä¼¼æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†èµ„æºæ¶ˆè€—ï¼Œå±•ç°å‡ºè‰¯å¥½çš„å®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æ‰‹æœºã€è¾¹ç¼˜è®¡ç®—è®¾å¤‡å’Œä½åŠŸè€—ç¡¬ä»¶ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿä½¿å¤§å‹è¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­é«˜æ•ˆè¿è¡Œã€‚å…¶å®é™…ä»·å€¼åœ¨äºé™ä½èƒ½è€—å’Œç¡¬ä»¶è¦æ±‚ï¼Œä»è€Œæ¨åŠ¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„æ™®åŠä¸åº”ç”¨ã€‚æœªæ¥ï¼Œéšç€é‡åŒ–æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼Œå¯èƒ½ä¼šåœ¨æ›´å¤šé¢†åŸŸå®ç°æ›´å¹¿æ³›çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models have significantly advanced natural language processing, yet their heavy resource demands pose severe challenges regarding hardware accessibility and energy consumption. This paper presents a focused and high-level review of post-training quantization (PTQ) techniques designed to optimize the inference efficiency of LLMs by the end-user, including details on various quantization schemes, granularities, and trade-offs. The aim is to provide a balanced overview between the theory and applications of post-training quantization.

