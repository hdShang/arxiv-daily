---
layout: default
title: Improved Algorithms for Differentially Private Language Model Alignment
---

# Improved Algorithms for Differentially Private Language Model Alignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08849" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08849v1</a>
  <a href="https://arxiv.org/pdf/2505.08849.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08849v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08849v1', 'Improved Algorithms for Differentially Private Language Model Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Keyu Chen, Hao Tang, Qinglin Liu, Yizhao Xu

**åˆ†ç±»**: cs.CR, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºéšç§ä¿æŠ¤çš„è¯­è¨€æ¨¡å‹å¯¹é½ç®—æ³•ä»¥è§£å†³ç”¨æˆ·æ•°æ®éšç§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å·®åˆ†éšç§` `è¯­è¨€æ¨¡å‹` `å¯¹é½æŠ€æœ¯` `ç”¨æˆ·éšç§` `æœºå™¨å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `ä¼˜åŒ–ç®—æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯¹é½æ–¹æ³•åœ¨ä¿æŠ¤ç”¨æˆ·éšç§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´éšç§é£é™©è¾ƒé«˜ã€‚
2. æœ¬æ–‡æå‡ºçš„æ–°ç®—æ³•é€šè¿‡å·®åˆ†éšç§æŠ€æœ¯å®ç°éšç§ä¿æŠ¤çš„å¯¹é½ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å¯¹é½è´¨é‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDP-AdamWç®—æ³•åœ¨ä¸­ç­‰éšç§é¢„ç®—ä¸‹ï¼Œæå‡å¯¹é½è´¨é‡è¾¾15%ï¼Œè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¯­è¨€æ¨¡å‹å¯¹é½å¯¹äºç¡®ä¿å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½ä¸€è‡´è‡³å…³é‡è¦ï¼Œä½†é€šå¸¸æ¶‰åŠæ•æ„Ÿç”¨æˆ·æ•°æ®ï¼Œå¸¦æ¥æ˜¾è‘—çš„éšç§é—®é¢˜ã€‚å°½ç®¡ä¹‹å‰çš„ç ”ç©¶å·²å°†å·®åˆ†éšç§ä¸å¯¹é½æŠ€æœ¯ç»“åˆï¼Œä½†å…¶æ€§èƒ½ä»æœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†æ–°é¢–çš„éšç§ä¿æŠ¤å¯¹é½ç®—æ³•ï¼Œå¹¶ä¸¥æ ¼åˆ†æå…¶åœ¨ä¸åŒéšç§é¢„ç®—å’Œæ¨¡å‹ä¸‹çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶å¯åº”ç”¨äºä¸¤ç§è‘—åçš„å¯¹é½æŠ€æœ¯ï¼Œå³ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ã€‚é€šè¿‡å¯¹å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„ç³»ç»Ÿå®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†è¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ç®—æ³•DP-AdamWä¸DPOç»“åˆï¼Œåœ¨ä¸­ç­‰éšç§é¢„ç®—ï¼ˆÎµ=2-5ï¼‰ä¸‹ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œæå‡äº†å¯¹é½è´¨é‡è¾¾15%ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¢è®¨äº†éšç§ä¿éšœã€å¯¹é½æ•ˆæœä¸è®¡ç®—éœ€æ±‚ä¹‹é—´çš„ç›¸äº’å…³ç³»ï¼Œä¸ºä¼˜åŒ–è¿™äº›æƒè¡¡æä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½è¿‡ç¨‹ä¸­ç”¨æˆ·æ•°æ®éšç§ä¿æŠ¤ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨éšç§ä¿éšœä¸å¯¹é½æ•ˆæœä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œå¯¼è‡´éšç§é£é™©è¾ƒé«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ–°ç®—æ³•é€šè¿‡å¼•å…¥å·®åˆ†éšç§æœºåˆ¶ï¼Œç¡®ä¿åœ¨è¿›è¡Œæ¨¡å‹å¯¹é½æ—¶ï¼Œç”¨æˆ·æ•°æ®çš„éšç§å¾—åˆ°æœ‰æ•ˆä¿æŠ¤ï¼ŒåŒæ—¶æå‡å¯¹é½æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼ŒäºŒæ˜¯åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ã€‚åœ¨è¿™ä¸¤ä¸ªæ¨¡å—ä¸­ï¼Œç®—æ³•é€šè¿‡å·®åˆ†éšç§æŠ€æœ¯è¿›è¡Œä¼˜åŒ–ï¼Œç¡®ä¿éšç§ä¿æŠ¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†DP-AdamWç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨ç»“åˆDPOæ—¶ï¼Œæ˜¾è‘—æå‡äº†å¯¹é½è´¨é‡ï¼Œå°¤å…¶æ˜¯åœ¨ä¸­ç­‰éšç§é¢„ç®—ä¸‹ï¼Œè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šç®—æ³•è®¾è®¡ä¸­ï¼Œéšç§é¢„ç®—ï¼ˆÎµï¼‰è®¾ç½®ä¸º2-5ï¼Œé‡‡ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–ç­–ç•¥ï¼Œä»¥ç¡®ä¿åœ¨ä¿è¯éšç§çš„åŒæ—¶ï¼Œæœ€å¤§åŒ–å¯¹é½æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDP-AdamWç®—æ³•åœ¨ä¸­ç­‰éšç§é¢„ç®—ï¼ˆÎµ=2-5ï¼‰ä¸‹ï¼Œæå‡äº†å¯¹é½è´¨é‡è¾¾15%ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¯¹é½æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨éšç§ä¿æŠ¤ä¸æ€§èƒ½æå‡ä¹‹é—´çš„æœ‰æ•ˆå¹³è¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“ã€åœ¨çº¿å®¢æœå’Œä¸ªæ€§åŒ–æ¨èç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿåœ¨ä¿æŠ¤ç”¨æˆ·éšç§çš„å‰æä¸‹ï¼Œæå‡ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æ›´å¤šæ¶‰åŠç”¨æˆ·æ•°æ®çš„åœºæ™¯ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œæ¨åŠ¨éšç§ä¿æŠ¤ä¸äººå·¥æ™ºèƒ½çš„ç»“åˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Language model alignment is crucial for ensuring that large language models (LLMs) align with human preferences, yet it often involves sensitive user data, raising significant privacy concerns. While prior work has integrated differential privacy (DP) with alignment techniques, their performance remains limited. In this paper, we propose novel algorithms for privacy-preserving alignment and rigorously analyze their effectiveness across varying privacy budgets and models. Our framework can be deployed on two celebrated alignment techniques, namely direct preference optimization (DPO) and reinforcement learning from human feedback (RLHF). Through systematic experiments on large-scale language models, we demonstrate that our approach achieves state-of-the-art performance. Notably, one of our algorithms, DP-AdamW, combined with DPO, surpasses existing methods, improving alignment quality by up to 15% under moderate privacy budgets (Îµ=2-5). We further investigate the interplay between privacy guarantees, alignment efficacy, and computational demands, providing practical guidelines for optimizing these trade-offs.

