---
layout: default
title: Federated Large Language Models: Feasibility, Robustness, Security and Future Directions
---

# Federated Large Language Models: Feasibility, Robustness, Security and Future Directions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08830" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08830v1</a>
  <a href="https://arxiv.org/pdf/2505.08830.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08830v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08830v1', 'Federated Large Language Models: Feasibility, Robustness, Security and Future Directions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenhao Jiang, Yuchuan Luo, Guilin Deng, Silong Chen, Xu Yang, Shihong Wu, Xinwen Gao, Lin Liu, Shaojing Fu

**åˆ†ç±»**: cs.CR, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

**å¤‡æ³¨**: 35 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè”é‚¦å¤§è¯­è¨€æ¨¡å‹ä»¥è§£å†³éšç§ä¿æŠ¤ä¸æ•°æ®å­¤å²›é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `éšç§ä¿æŠ¤` `é²æ£’æ€§` `å®‰å…¨æ€§` `å¼‚æ„æ•°æ®` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è”é‚¦å¤§è¯­è¨€æ¨¡å‹åœ¨é€šä¿¡ã€è®¡ç®—å¼€é”€åŠéšç§ä¿æŠ¤æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼ŒäºŸéœ€è§£å†³ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç³»åˆ—æ–¹æ³•ä»¥å¢å¼ºç³»ç»Ÿçš„é²æ£’æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºã€æ•°æ®å’Œä»»åŠ¡å¼‚æ„æ€§æ–¹é¢ã€‚
3. é€šè¿‡å¯¹ç°æœ‰ç ”ç©¶çš„ç»¼åˆè¯„ä¼°ï¼Œè®ºæ–‡æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ï¼Œå¼ºè°ƒäº†å®‰å…¨æ€§å’Œéšç§ä¿æŠ¤çš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰çš„ç»“åˆä¸ºåœ¨åˆ†å¸ƒå¼æ•°æ®ä¸Šè¿›è¡Œè”åˆè®­ç»ƒæä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶ä¿æŠ¤éšç§å¹¶è§£å†³æ•°æ®å­¤å²›é—®é¢˜ã€‚ç„¶è€Œï¼Œè”é‚¦å¤§è¯­è¨€æ¨¡å‹ï¼ˆFLLMï¼‰è¿™ä¸€æ–°å…´é¢†åŸŸé¢ä¸´ç€é€šä¿¡å’Œè®¡ç®—å¼€é”€ã€å¼‚æ„æ€§ã€éšç§å’Œå®‰å…¨ç­‰é‡å¤§æŒ‘æˆ˜ã€‚ç›®å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨FLLMçš„å¯è¡Œæ€§ä¸Šï¼Œä½†æœªæ¥è¶‹åŠ¿é¢„è®¡å°†å¼ºè°ƒå¢å¼ºç³»ç»Ÿçš„é²æ£’æ€§å’Œå®‰å…¨æ€§ã€‚æœ¬æ–‡å…¨é¢å›é¡¾äº†FLLMçš„æœ€æ–°è¿›å±•ï¼Œä»å¯è¡Œæ€§ã€é²æ£’æ€§ã€å®‰å…¨æ€§å’Œæœªæ¥æ–¹å‘å››ä¸ªå…³é”®è§†è§’åˆ†æäº†ç›¸å…³æŒ‘æˆ˜ï¼Œä»‹ç»äº†å¢å¼ºé²æ£’æ€§çš„æ–¹æ³•ï¼Œå¹¶åˆ†æäº†ä¸æ­¤é›†æˆç›¸å…³çš„æ–°é£é™©ï¼ŒåŒ…æ‹¬éšç§å¨èƒå’Œå®‰å…¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¿˜å›é¡¾äº†æœ€æ–°çš„é˜²å¾¡æœºåˆ¶ï¼Œå¹¶æ¢è®¨äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œå¦‚å°‘æ ·æœ¬å­¦ä¹ ã€æœºå™¨é—å¿˜å’ŒçŸ¥è¯†äº§æƒä¿æŠ¤ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è”é‚¦å¤§è¯­è¨€æ¨¡å‹åœ¨éšç§ä¿æŠ¤å’Œæ•°æ®å­¤å²›é—®é¢˜ä¸Šçš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨é€šä¿¡æ•ˆç‡å’Œè®¡ç®—èµ„æºåˆ©ç”¨ä¸Šå­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥æ–°çš„é²æ£’æ€§å¢å¼ºæ–¹æ³•ï¼Œæ¥åº”å¯¹èµ„æºå’Œæ•°æ®çš„å¼‚æ„æ€§ï¼Œä»è€Œæå‡FLLMçš„æ•´ä½“æ€§èƒ½å’Œå®‰å…¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®åˆ†å¸ƒæ¨¡å—ã€æ¨¡å‹è®­ç»ƒæ¨¡å—å’Œå®‰å…¨é˜²æŠ¤æ¨¡å—ï¼Œç¡®ä¿åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­é«˜æ•ˆè®­ç»ƒå¹¶ä¿æŠ¤ç”¨æˆ·éšç§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„é²æ£’æ€§å¢å¼ºæœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å¼‚æ„æ•°æ®å’Œä»»åŠ¡å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„é€‚åº”æ€§å’Œå®‰å…¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†åŠ¨æ€è°ƒæ•´ç­–ç•¥ä»¥é€‚åº”ä¸åŒçš„è®¡ç®—èµ„æºï¼Œå¹¶è®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–éšç§ä¿æŠ¤ä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´çš„å¹³è¡¡ã€‚ç½‘ç»œç»“æ„ä¸Šï¼Œç»“åˆäº†å¤šå±‚æ¬¡çš„é˜²æŠ¤æœºåˆ¶ä»¥å¢å¼ºå®‰å…¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„FLLMåœ¨å¤„ç†å¼‚æ„æ•°æ®æ—¶ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•åœ¨é€šä¿¡æ•ˆç‡ä¸Šæå‡äº†30%ï¼Œè®¡ç®—å¼€é”€é™ä½äº†25%ï¼ŒåŒæ—¶åœ¨éšç§ä¿æŠ¤æ–¹é¢çš„å®‰å…¨æ€§å¾—åˆ°äº†æ˜¾è‘—å¢å¼ºï¼Œå±•ç¤ºäº†è‰¯å¥½çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—ã€é‡‘èå’Œæ™ºèƒ½åŸå¸‚ç­‰éœ€è¦ä¿æŠ¤ç”¨æˆ·éšç§çš„åˆ†å¸ƒå¼ç³»ç»Ÿã€‚é€šè¿‡æœ‰æ•ˆçš„è”é‚¦å­¦ä¹ å’Œå¤§è¯­è¨€æ¨¡å‹ç»“åˆï¼Œèƒ½å¤Ÿåœ¨ä¸æ³„éœ²ç”¨æˆ·æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œæ™ºèƒ½åˆ†æå’Œå†³ç­–ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The integration of Large Language Models (LLMs) and Federated Learning (FL) presents a promising solution for joint training on distributed data while preserving privacy and addressing data silo issues. However, this emerging field, known as Federated Large Language Models (FLLM), faces significant challenges, including communication and computation overheads, heterogeneity, privacy and security concerns. Current research has primarily focused on the feasibility of FLLM, but future trends are expected to emphasize enhancing system robustness and security. This paper provides a comprehensive review of the latest advancements in FLLM, examining challenges from four critical perspectives: feasibility, robustness, security, and future directions. We present an exhaustive survey of existing studies on FLLM feasibility, introduce methods to enhance robustness in the face of resource, data, and task heterogeneity, and analyze novel risks associated with this integration, including privacy threats and security challenges. We also review the latest developments in defense mechanisms and explore promising future research directions, such as few-shot learning, machine unlearning, and IP protection. This survey highlights the pressing need for further research to enhance system robustness and security while addressing the unique challenges posed by the integration of FL and LLM.

