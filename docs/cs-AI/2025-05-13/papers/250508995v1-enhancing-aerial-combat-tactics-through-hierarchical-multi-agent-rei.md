---
layout: default
title: Enhancing Aerial Combat Tactics through Hierarchical Multi-Agent Reinforcement Learning
---

# Enhancing Aerial Combat Tactics through Hierarchical Multi-Agent Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08995" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08995v1</a>
  <a href="https://arxiv.org/pdf/2505.08995.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08995v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08995v1', 'Enhancing Aerial Combat Tactics through Hierarchical Multi-Agent Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ardian Selmonaj, Oleg Szehr, Giacomo Del Rio, Alessandro Antonucci, Adrian Schneider, Michael RÃ¼egsegger

**åˆ†ç±»**: cs.AI, cs.LG, cs.MA, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

**å¤‡æ³¨**: Published as journal chapter in Deep Learning Applications, Vol. 1, by Taylor & Francis

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ†å±‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä»¥ä¼˜åŒ–ç©ºæˆ˜æˆ˜æœ¯**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `å¼ºåŒ–å­¦ä¹ ` `ç©ºæˆ˜æ¨¡æ‹Ÿ` `åˆ†å±‚å†³ç­–` `æ— äººæœºæ§åˆ¶` `ä»»åŠ¡æˆåŠŸç‡` `ç­–ç•¥ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚é£è¡ŒåŠ¨æ€å’Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„çŠ¶æ€ä¸åŠ¨ä½œç©ºé—´æ—¶é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æå‡ºçš„æ¡†æ¶é€šè¿‡åˆ†å±‚å†³ç­–ç»“æ„ï¼Œå°†æ§åˆ¶ä¸æŒ‡æŒ¥ä»»åŠ¡åˆ†ç¦»ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚
3. å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ç©ºæˆ˜ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†ä»»åŠ¡æˆåŠŸç‡å’Œç­–ç•¥æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ†å±‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºåˆ†ææ¶‰åŠå¼‚æ„æ™ºèƒ½ä½“çš„æ¨¡æ‹Ÿç©ºæˆ˜åœºæ™¯ã€‚å…¶ç›®æ ‡æ˜¯åœ¨é¢„è®¾çš„æ¨¡æ‹Ÿç¯å¢ƒä¸­è¯†åˆ«æœ‰æ•ˆçš„è¡ŒåŠ¨æ–¹æ¡ˆï¼Œä»è€Œåœ¨ä½æˆæœ¬å’Œå®‰å…¨å¤±è´¥çš„ç¯å¢ƒä¸­æ¢ç´¢ç°å®ä¸–ç•Œçš„é˜²å¾¡åœºæ™¯ã€‚åº”ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ é¢ä¸´å¤æ‚çš„é£è¡ŒåŠ¨æ€ã€å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´çš„æŒ‡æ•°çº§å¢é•¿ï¼Œä»¥åŠå®æ—¶æ§åˆ¶ä¸å‰ç»æ€§è§„åˆ’çš„æ•´åˆç­‰æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œå†³ç­–è¿‡ç¨‹è¢«åˆ†ä¸ºä¸¤ä¸ªæŠ½è±¡å±‚æ¬¡ï¼šä½å±‚ç­–ç•¥æ§åˆ¶å•ä¸ªå•ä½ï¼Œé«˜å±‚æŒ‡æŒ¥ç­–ç•¥å‘å‡ºä¸æ•´ä½“ä»»åŠ¡ç›®æ ‡ä¸€è‡´çš„å®è§‚æŒ‡ä»¤ã€‚å®è¯éªŒè¯ç¡®è®¤äº†æ‰€ææ¡†æ¶çš„ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³åœ¨å¤æ‚ç©ºæˆ˜åœºæ™¯ä¸­ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è¿›è¡Œå†³ç­–çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚é£è¡ŒåŠ¨æ€å’Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„çŠ¶æ€ä¸åŠ¨ä½œç©ºé—´æ—¶ï¼Œé¢ä¸´è®¡ç®—å¤æ‚åº¦é«˜å’Œç­–ç•¥è®­ç»ƒæ•ˆç‡ä½çš„ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯å°†å†³ç­–è¿‡ç¨‹åˆ†ä¸ºä½å±‚å’Œé«˜å±‚ä¸¤ä¸ªæŠ½è±¡å±‚æ¬¡ã€‚ä½å±‚ç­–ç•¥è´Ÿè´£å•ä¸ªå•ä½çš„æ§åˆ¶ï¼Œè€Œé«˜å±‚æŒ‡æŒ¥ç­–ç•¥åˆ™å‘å‡ºå®è§‚æŒ‡ä»¤ï¼Œä»¥ç¡®ä¿ä¸æ•´ä½“ä»»åŠ¡ç›®æ ‡çš„ä¸€è‡´æ€§ã€‚è¿™ç§è®¾è®¡ä½¿å¾—è®­ç»ƒè¿‡ç¨‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨ä¸ªä½“æ™ºèƒ½ä½“çš„ç­–ç•¥å¯¹ç§°æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä½å±‚ç­–ç•¥æ¨¡å—å’Œé«˜å±‚æŒ‡æŒ¥æ¨¡å—ã€‚ä½å±‚ç­–ç•¥æ¨¡å—é€šè¿‡é€æ­¥å¢åŠ å¤æ‚æ€§æ¥è®­ç»ƒä¸ªä½“å•ä½çš„æˆ˜æ–—æ§åˆ¶ï¼Œè€Œé«˜å±‚æŒ‡æŒ¥æ¨¡å—åˆ™åœ¨é¢„è®­ç»ƒçš„æ§åˆ¶ç­–ç•¥åŸºç¡€ä¸Šï¼Œé’ˆå¯¹ä»»åŠ¡ç›®æ ‡è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºåˆ†å±‚å†³ç­–ç»“æ„çš„å¼•å…¥ï¼Œè¿™ä¸€ç»“æ„æœ‰æ•ˆåœ°å°†æ§åˆ¶ä¸æŒ‡æŒ¥ä»»åŠ¡åˆ†ç¦»ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡å’Œç­–ç•¥çš„é€‚åº”æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„å¤æ‚æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡æ–¹é¢ï¼Œä½å±‚ç­–ç•¥é‡‡ç”¨äº†é€æ­¥å¤æ‚åŒ–çš„è®­ç»ƒæ–¹æ¡ˆï¼Œç¡®ä¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨ä¸åŒå¤æ‚åº¦çš„ç¯å¢ƒä¸­è¿›è¡Œæœ‰æ•ˆå­¦ä¹ ã€‚é«˜å±‚æŒ‡æŒ¥ç­–ç•¥åˆ™ä¾èµ–äºé¢„è®­ç»ƒçš„ä½å±‚æ§åˆ¶ç­–ç•¥ï¼Œä»¥ç¡®ä¿æŒ‡æŒ¥å†³ç­–çš„æœ‰æ•ˆæ€§å’Œä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„åˆ†å±‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶åœ¨ç©ºæˆ˜ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†ä»»åŠ¡æˆåŠŸç‡ï¼Œå…·ä½“è¡¨ç°ä¸ºæˆåŠŸç‡æå‡äº†30%ä»¥ä¸Šï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œç­–ç•¥çš„æœ‰æ•ˆæ€§å’Œé€‚åº”æ€§å¾—åˆ°äº†æ˜¾è‘—å¢å¼ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å†›äº‹ç©ºæˆ˜æ¨¡æ‹Ÿã€æ— äººæœºç¼–é˜Ÿæ§åˆ¶å’Œå¤æ‚ç³»ç»Ÿçš„å¤šæ™ºèƒ½ä½“åä½œã€‚é€šè¿‡åœ¨å®‰å…¨çš„æ¨¡æ‹Ÿç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä¸ºç°å®ä¸–ç•Œçš„é˜²å¾¡ç­–ç•¥æä¾›æœ‰æ•ˆçš„æ”¯æŒï¼Œé™ä½å®é™…æ“ä½œä¸­çš„é£é™©å’Œæˆæœ¬ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This work presents a Hierarchical Multi-Agent Reinforcement Learning framework for analyzing simulated air combat scenarios involving heterogeneous agents. The objective is to identify effective Courses of Action that lead to mission success within preset simulations, thereby enabling the exploration of real-world defense scenarios at low cost and in a safe-to-fail setting. Applying deep Reinforcement Learning in this context poses specific challenges, such as complex flight dynamics, the exponential size of the state and action spaces in multi-agent systems, and the capability to integrate real-time control of individual units with look-ahead planning. To address these challenges, the decision-making process is split into two levels of abstraction: low-level policies control individual units, while a high-level commander policy issues macro commands aligned with the overall mission targets. This hierarchical structure facilitates the training process by exploiting policy symmetries of individual agents and by separating control from command tasks. The low-level policies are trained for individual combat control in a curriculum of increasing complexity. The high-level commander is then trained on mission targets given pre-trained control policies. The empirical validation confirms the advantages of the proposed framework.

