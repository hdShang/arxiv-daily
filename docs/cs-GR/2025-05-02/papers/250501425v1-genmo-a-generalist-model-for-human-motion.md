---
layout: default
title: "GENMO: A GENeralist Model for Human MOtion"
---

# GENMO: A GENeralist Model for Human MOtion

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.01425" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.01425v1</a>
  <a href="https://arxiv.org/pdf/2505.01425.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.01425v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.01425v1', 'GENMO: A GENeralist Model for Human MOtion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiefeng Li, Jinkun Cao, Haotian Zhang, Davis Rempe, Jan Kautz, Umar Iqbal, Ye Yuan

**åˆ†ç±»**: cs.GR, cs.AI, cs.CV, cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-02

**å¤‡æ³¨**: Project page: https://research.nvidia.com/labs/dair/genmo/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGENMOä»¥ç»Ÿä¸€äººç±»è¿åŠ¨ç”Ÿæˆä¸ä¼°è®¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äººç±»è¿åŠ¨å»ºæ¨¡` `è¿åŠ¨ç”Ÿæˆ` `è¿åŠ¨ä¼°è®¡` `å¤šæ¨¡æ€èåˆ` `æ·±åº¦å­¦ä¹ ` `æ¨¡å‹ç»Ÿä¸€` `è§†é¢‘åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„äººç±»è¿åŠ¨å»ºæ¨¡æ–¹æ³•å°†è¿åŠ¨ç”Ÿæˆä¸ä¼°è®¡åˆ†å¼€ï¼Œå¯¼è‡´çŸ¥è¯†è½¬ç§»å—é™ï¼Œä¸”éœ€è¦ç»´æŠ¤å¤šä¸ªæ¨¡å‹ã€‚
2. GENMOé€šè¿‡å°†è¿åŠ¨ä¼°è®¡è§†ä¸ºå—é™è¿åŠ¨ç”Ÿæˆï¼Œç»Ÿä¸€äº†è¿åŠ¨ç”Ÿæˆä¸ä¼°è®¡ï¼Œæå‡äº†æ¨¡å‹çš„çµæ´»æ€§ä¸å‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGENMOåœ¨å¤„ç†å¤æ‚æ¡ä»¶ä¸‹çš„è¿åŠ¨ä¼°è®¡å’Œç”Ÿæˆä»»åŠ¡æ—¶ï¼Œè¡¨ç°ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œå…·æœ‰æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»è¿åŠ¨å»ºæ¨¡ä¼ ç»Ÿä¸Šå°†è¿åŠ¨ç”Ÿæˆä¸ä¼°è®¡åˆ†ä¸ºä¸åŒä»»åŠ¡ï¼Œä½¿ç”¨ä¸“é—¨æ¨¡å‹ã€‚è¿åŠ¨ç”Ÿæˆæ¨¡å‹ä¸“æ³¨äºä»æ–‡æœ¬ã€éŸ³é¢‘æˆ–å…³é”®å¸§ç­‰è¾“å…¥ç”Ÿæˆå¤šæ ·ä¸”çœŸå®çš„è¿åŠ¨ï¼Œè€Œè¿åŠ¨ä¼°è®¡æ¨¡å‹åˆ™æ—¨åœ¨ä»è§†é¢‘ç­‰è§‚å¯Ÿä¸­é‡å»ºå‡†ç¡®çš„è¿åŠ¨è½¨è¿¹ã€‚è¿™ç§åˆ†ç¦»é™åˆ¶äº†ä»»åŠ¡é—´çš„çŸ¥è¯†è½¬ç§»ï¼Œå¹¶éœ€è¦ç»´æŠ¤å¤šä¸ªæ¨¡å‹ã€‚æœ¬æ–‡æå‡ºGENMOï¼Œä¸€ä¸ªç»Ÿä¸€çš„äººç±»è¿åŠ¨é€šç”¨æ¨¡å‹ï¼Œå°†è¿åŠ¨ä¼°è®¡ä¸ç”Ÿæˆç»“åˆåœ¨ä¸€ä¸ªæ¡†æ¶ä¸­ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯å°†è¿åŠ¨ä¼°è®¡é‡æ–°è¡¨è¿°ä¸ºå—é™çš„è¿åŠ¨ç”Ÿæˆï¼Œè¾“å‡ºè¿åŠ¨å¿…é¡»ç²¾ç¡®æ»¡è¶³è§‚å¯Ÿåˆ°çš„æ¡ä»¶ä¿¡å·ã€‚é€šè¿‡å›å½’ä¸æ‰©æ•£çš„ååŒä½œç”¨ï¼ŒGENMOå®ç°äº†å‡†ç¡®çš„å…¨å±€è¿åŠ¨ä¼°è®¡ï¼ŒåŒæ—¶æ”¯æŒå¤šæ ·çš„è¿åŠ¨ç”Ÿæˆã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§ä¼°è®¡å¼•å¯¼çš„è®­ç»ƒç›®æ ‡ï¼Œåˆ©ç”¨å¸¦æœ‰2Dæ³¨é‡Šå’Œæ–‡æœ¬æè¿°çš„é‡å¤–è§†é¢‘ï¼Œå¢å¼ºç”Ÿæˆçš„å¤šæ ·æ€§ã€‚å®éªŒè¡¨æ˜GENMOåœ¨å¤šä¸ªè¿åŠ¨ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³äººç±»è¿åŠ¨å»ºæ¨¡ä¸­è¿åŠ¨ç”Ÿæˆä¸ä¼°è®¡ä»»åŠ¡åˆ†ç¦»çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨çŸ¥è¯†è½¬ç§»å’Œæ¨¡å‹ç»´æŠ¤ä¸Šå­˜åœ¨å±€é™æ€§ï¼Œå½±å“äº†æ•´ä½“æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGENMOçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è¿åŠ¨ä¼°è®¡é‡æ–°å®šä¹‰ä¸ºå—é™çš„è¿åŠ¨ç”Ÿæˆï¼Œä½¿å¾—è¾“å‡ºè¿åŠ¨èƒ½å¤Ÿæ»¡è¶³è§‚å¯Ÿåˆ°çš„æ¡ä»¶ä¿¡å·ï¼Œä»è€Œå®ç°ä»»åŠ¡çš„ç»Ÿä¸€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGENMOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¿åŠ¨ç”Ÿæˆå’Œä¼°è®¡ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼Œåˆ©ç”¨å›å½’ä¸æ‰©æ•£çš„ååŒä½œç”¨ï¼Œå¤„ç†ä¸åŒæ¨¡æ€çš„è¾“å…¥ï¼ˆæ–‡æœ¬ã€éŸ³é¢‘ã€è§†é¢‘ï¼‰ï¼Œå¹¶æ”¯æŒå¯å˜é•¿åº¦çš„è¿åŠ¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†è¿åŠ¨ä¼°è®¡è§†ä¸ºç”Ÿæˆä»»åŠ¡ï¼Œæ‰“ç ´äº†ä¼ ç»Ÿçš„ä»»åŠ¡åˆ†ç¦»ç•Œé™ï¼Œæå‡äº†æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒGENMOé‡‡ç”¨äº†ä¼°è®¡å¼•å¯¼çš„è®­ç»ƒç›®æ ‡ï¼Œç»“åˆé‡å¤–è§†é¢‘æ•°æ®å’Œ2Dæ³¨é‡Šï¼Œä¼˜åŒ–äº†æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥å¢å¼ºç”Ÿæˆçš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGENMOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é®æŒ¡å’Œå¤æ‚èƒŒæ™¯æ—¶ï¼Œè¿åŠ¨ä¼°è®¡çš„å‡†ç¡®æ€§æé«˜äº†çº¦15%ã€‚åŒæ—¶ï¼Œç”Ÿæˆçš„è¿åŠ¨åœ¨å¤šæ ·æ€§å’ŒçœŸå®æ€§ä¸Šä¹Ÿæœ‰æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†è¯¥æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GENMOçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨è™šæ‹Ÿç°å®ã€åŠ¨ç”»åˆ¶ä½œå’Œäººæœºäº¤äº’ç­‰é¢†åŸŸã€‚é€šè¿‡æä¾›æ›´è‡ªç„¶å’Œå¤šæ ·çš„äººç±»è¿åŠ¨ç”Ÿæˆï¼Œèƒ½å¤Ÿæå‡ç”¨æˆ·ä½“éªŒå’Œäº¤äº’è´¨é‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹çš„çµæ´»æ€§ä½¿å…¶åœ¨æœºå™¨äººæ§åˆ¶å’Œè¿åŠ¨åˆ†æç­‰å®é™…åº”ç”¨ä¸­ä¹Ÿå…·æœ‰é‡è¦ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Human motion modeling traditionally separates motion generation and estimation into distinct tasks with specialized models. Motion generation models focus on creating diverse, realistic motions from inputs like text, audio, or keyframes, while motion estimation models aim to reconstruct accurate motion trajectories from observations like videos. Despite sharing underlying representations of temporal dynamics and kinematics, this separation limits knowledge transfer between tasks and requires maintaining separate models. We present GENMO, a unified Generalist Model for Human Motion that bridges motion estimation and generation in a single framework. Our key insight is to reformulate motion estimation as constrained motion generation, where the output motion must precisely satisfy observed conditioning signals. Leveraging the synergy between regression and diffusion, GENMO achieves accurate global motion estimation while enabling diverse motion generation. We also introduce an estimation-guided training objective that exploits in-the-wild videos with 2D annotations and text descriptions to enhance generative diversity. Furthermore, our novel architecture handles variable-length motions and mixed multimodal conditions (text, audio, video) at different time intervals, offering flexible control. This unified approach creates synergistic benefits: generative priors improve estimated motions under challenging conditions like occlusions, while diverse video data enhances generation capabilities. Extensive experiments demonstrate GENMO's effectiveness as a generalist framework that successfully handles multiple human motion tasks within a single model.

