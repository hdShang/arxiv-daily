---
layout: default
title: Can Representation Gaps Be the Key to Enhancing Robustness in Graph-Text Alignment?
---

# Can Representation Gaps Be the Key to Enhancing Robustness in Graph-Text Alignment?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.12087" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.12087v1</a>
  <a href="https://arxiv.org/pdf/2510.12087.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.12087v1" onclick="toggleFavorite(this, '2510.12087v1', 'Can Representation Gaps Be the Key to Enhancing Robustness in Graph-Text Alignment?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Heng Zhang, Tianyi Zhang, Yuling Shi, Xiaodong Gu, Yaomin Shen, Zijian Zhang, Yilei Yuan, Hao Zhang, Jin Huang

**åˆ†ç±»**: cs.GR

**å‘å¸ƒæ—¥æœŸ**: 2025-10-14

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLLM4GTAæ¡†æ¶ï¼Œé€šè¿‡ä¿æŒè¡¨å¾å·®å¼‚æå‡å›¾æ–‡å¯¹é½çš„é²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å›¾æ–‡å¯¹é½` `è¡¨å¾å­¦ä¹ ` `å¯¹æ¯”å­¦ä¹ ` `é²æ£’æ€§` `é›¶æ ·æœ¬å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å›¾æ–‡å¯¹é½æ–¹æ³•è¿‡åº¦å¼ºè°ƒè·¨æ¨¡æ€ç›¸ä¼¼æ€§ï¼Œå¿½ç•¥äº†å›¾ç»“æ„å’Œæ–‡æœ¬è¯­ä¹‰çš„å›ºæœ‰å·®å¼‚ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. LLM4GTAæ¡†æ¶é€šè¿‡è‡ªé€‚åº”åœ°ä¿æŒå›¾æ–‡è¡¨å¾ä¹‹é—´çš„é—´éš™ï¼Œé¿å…è¿‡åº¦å¯¹é½ï¼Œä»è€Œä¿ç•™æ¨¡æ€ç‰¹å®šçŸ¥è¯†ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLLM4GTAåœ¨é›¶æ ·æœ¬å’Œå°æ ·æœ¬åœºæ™¯ä¸‹ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†æ–‡æœ¬å±æ€§å›¾ï¼ˆTAGsï¼‰ä¸Šçš„è¡¨å¾å­¦ä¹ ï¼Œè¯¥æ–¹æ³•å°†ç»“æ„è¿é€šæ€§ä¸ä¸°å¯Œçš„æ–‡æœ¬è¯­ä¹‰ç›¸ç»“åˆï¼Œåº”ç”¨äºå¤šä¸ªé¢†åŸŸã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å¯¹æ¯”å­¦ä¹ æ¥æœ€å¤§åŒ–è·¨æ¨¡æ€ç›¸ä¼¼æ€§ï¼Œè®¤ä¸ºå›¾å’Œæ–‡æœ¬è¡¨å¾ä¹‹é—´æ›´ç´§å¯†çš„è€¦åˆå¯ä»¥æé«˜è¿ç§»æ€§èƒ½ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç»éªŒåˆ†æè¡¨æ˜ï¼Œè‡ªç„¶é—´éš™çš„æ‰©å¤§å’Œå¼ºåˆ¶é—´éš™çš„ç¼©å°éƒ½ä¼šé€šè¿‡ç ´åé¢„è®­ç»ƒçš„çŸ¥è¯†ç»“æ„å’ŒæŸå®³æ³›åŒ–èƒ½åŠ›è€Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚è¿™æ˜¯ç”±äºç¼–ç å™¨ä¹‹é—´çš„å‡ ä½•ä¸å…¼å®¹æ€§é€ æˆçš„ï¼Œå…¶ä¸­å›¾ç¼–ç å™¨æ•è·æ‹“æ‰‘æ¨¡å¼ï¼Œè€Œæ–‡æœ¬ç¼–ç å™¨æ•è·è¯­ä¹‰ç»“æ„ã€‚è¿‡åº¦å¯¹é½å°†è¿™äº›ä¸åŒçš„ç©ºé—´å‹ç¼©åˆ°å…±äº«å­ç©ºé—´ä¸­ï¼Œå¯¼è‡´ç»“æ„å´©æºƒï¼Œä»è€Œå‰Šå¼±äº†æ‹“æ‰‘æ¨ç†å’Œè¯­ä¹‰ç†è§£ã€‚æˆ‘ä»¬æå‡ºäº†LLM4GTAï¼Œä¸€ä¸ªé—´éš™æ„ŸçŸ¥å¯¹é½æ¡†æ¶ï¼Œå®ƒä¿ç•™äº†è¡¨å¾é—´éš™ï¼Œä½œä¸ºä¿æŒæ¨¡æ€ç‰¹å®šçŸ¥è¯†å’Œæé«˜è¿ç§»æ€§èƒ½çš„å‡ ä½•å¿…è¦æ€§ã€‚LLM4GTAåŒ…æ‹¬ä¸€ä¸ªè‡ªé€‚åº”é—´éš™ä¿æŒæ¨¡å—ï¼Œé€šè¿‡ç›‘æ§ç›¸ä¼¼æ€§æ¼”å˜æ¥é˜²æ­¢è¿‡åº¦å¯¹é½ï¼Œä»¥åŠä¸€ä¸ªä½¿ç”¨å›¾ç©ºé—´ä¸­çš„è¾…åŠ©åˆ†ç±»å™¨æ¥æé«˜åˆ¤åˆ«èƒ½åŠ›çš„æ¨¡æ€å†…è¡¥å¿æœºåˆ¶ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼Œåœ¨é›¶æ ·æœ¬å’Œå°æ ·æœ¬åœºæ™¯ä¸­ï¼Œè¯¥æ–¹æ³•æ¯”ç°æœ‰æ–¹æ³•æœ‰æ˜¾è‘—çš„æ”¹è¿›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å›¾æ–‡å¯¹é½æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºå¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•ï¼Œå€¾å‘äºæœ€å¤§åŒ–å›¾å’Œæ–‡æœ¬è¡¨å¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼ŒæœŸæœ›æ›´ç´§å¯†çš„è€¦åˆèƒ½å¸¦æ¥æ›´å¥½çš„è¿ç§»æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ç§åšæ³•å¿½ç•¥äº†å›¾ç»“æ„å’Œæ–‡æœ¬è¯­ä¹‰çš„æœ¬è´¨åŒºåˆ«ã€‚å›¾ç¼–ç å™¨ä¾§é‡äºæ•è·æ‹“æ‰‘æ¨¡å¼ï¼Œè€Œæ–‡æœ¬ç¼–ç å™¨ä¾§é‡äºæ•è·è¯­ä¹‰ç»“æ„ã€‚è¿‡åº¦å¯¹é½ä¼šå¯¼è‡´ä¿¡æ¯å‹ç¼©ï¼ŒæŸå®³æ¨¡å‹å¯¹æ‹“æ‰‘ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯çš„ç†è§£èƒ½åŠ›ï¼Œæœ€ç»ˆé™ä½æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLLM4GTAçš„æ ¸å¿ƒæ€è·¯æ˜¯ä¿æŒå›¾æ–‡è¡¨å¾ä¹‹é—´çš„â€œé—´éš™â€ï¼Œå³ä¸å¼ºè¡Œå°†å®ƒä»¬å¯¹é½åˆ°å®Œå…¨ç›¸åŒçš„ç©ºé—´ã€‚ä½œè€…è®¤ä¸ºï¼Œè¿™ç§é—´éš™æ˜¯å¿…è¦çš„ï¼Œå› ä¸ºå®ƒåæ˜ äº†å›¾å’Œæ–‡æœ¬æ¨¡æ€çš„å›ºæœ‰å·®å¼‚ï¼Œä¿ç•™äº†å„è‡ªæ¨¡æ€çš„ç‰¹å®šçŸ¥è¯†ã€‚é€šè¿‡ç»´æŒé€‚å½“çš„é—´éš™ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°åˆ©ç”¨ä¸¤ç§æ¨¡æ€çš„ä¿¡æ¯ï¼Œä»è€Œæé«˜é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLLM4GTAæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šè‡ªé€‚åº”é—´éš™ä¿æŒæ¨¡å—å’Œæ¨¡æ€å†…è¡¥å¿æœºåˆ¶ã€‚è‡ªé€‚åº”é—´éš™ä¿æŒæ¨¡å—é€šè¿‡ç›‘æ§å›¾æ–‡è¡¨å¾çš„ç›¸ä¼¼æ€§æ¼”å˜ï¼ŒåŠ¨æ€è°ƒæ•´å¯¹é½çš„å¼ºåº¦ï¼Œé˜²æ­¢è¿‡åº¦å¯¹é½ã€‚æ¨¡æ€å†…è¡¥å¿æœºåˆ¶åˆ™é€šè¿‡åœ¨å›¾ç©ºé—´ä¸­å¼•å…¥è¾…åŠ©åˆ†ç±»å™¨ï¼Œå¢å¼ºå›¾è¡¨å¾çš„åˆ¤åˆ«èƒ½åŠ›ï¼Œå¼¥è¡¥å› ä¿æŒé—´éš™è€Œå¯èƒ½å¯¼è‡´çš„æ€§èƒ½æŸå¤±ã€‚æ•´ä½“æµç¨‹æ˜¯å…ˆåˆ†åˆ«å¯¹å›¾å’Œæ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œç„¶åé€šè¿‡è‡ªé€‚åº”é—´éš™ä¿æŒæ¨¡å—è¿›è¡Œå¯¹é½ï¼Œæœ€ååˆ©ç”¨æ¨¡æ€å†…è¡¥å¿æœºåˆ¶å¢å¼ºå›¾è¡¨å¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šLLM4GTAçš„å…³é”®åˆ›æ–°åœ¨äºå…¶â€œé—´éš™æ„ŸçŸ¥â€çš„å¯¹é½ç­–ç•¥ã€‚ä¸ä»¥å¾€è¿½æ±‚æœ€å¤§åŒ–è·¨æ¨¡æ€ç›¸ä¼¼æ€§çš„æ–¹æ³•ä¸åŒï¼ŒLLM4GTAä¸»åŠ¨ä¿æŒå›¾æ–‡è¡¨å¾ä¹‹é—´çš„å·®å¼‚ï¼Œè®¤ä¸ºè¿™ç§å·®å¼‚å¯¹äºä¿ç•™æ¨¡æ€ç‰¹å®šçŸ¥è¯†è‡³å…³é‡è¦ã€‚è¿™ç§åç›´è§‰çš„è®¾è®¡ç†å¿µæ˜¯æœ¬æ–‡æœ€å¤§çš„äº®ç‚¹ã€‚

**å…³é”®è®¾è®¡**ï¼šè‡ªé€‚åº”é—´éš™ä¿æŒæ¨¡å—çš„å…³é”®åœ¨äºå¦‚ä½•è¡¡é‡å’Œæ§åˆ¶å›¾æ–‡è¡¨å¾çš„ç›¸ä¼¼æ€§ã€‚è®ºæ–‡å¯èƒ½ä½¿ç”¨äº†æŸç§ç›¸ä¼¼æ€§åº¦é‡ï¼ˆä¾‹å¦‚ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰æ¥è¯„ä¼°å›¾æ–‡è¡¨å¾çš„æ¥è¿‘ç¨‹åº¦ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæŸå¤±å‡½æ•°æ¥æƒ©ç½šè¿‡åº¦å¯¹é½ã€‚æ¨¡æ€å†…è¡¥å¿æœºåˆ¶çš„å…³é”®åœ¨äºè¾…åŠ©åˆ†ç±»å™¨çš„è®¾è®¡ï¼ŒåŒ…æ‹¬åˆ†ç±»å™¨çš„ç»“æ„ã€è®­ç»ƒæ•°æ®å’ŒæŸå¤±å‡½æ•°ã€‚å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚ï¼ˆä¾‹å¦‚æŸå¤±å‡½æ•°çš„å…·ä½“å½¢å¼ã€è¾…åŠ©åˆ†ç±»å™¨çš„ç»“æ„ï¼‰éœ€è¦å‚è€ƒè®ºæ–‡åŸæ–‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLLM4GTAåœ¨é›¶æ ·æœ¬å’Œå°æ ·æœ¬åœºæ™¯ä¸‹ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„å›¾æ–‡å¯¹é½æ–¹æ³•ã€‚å…·ä½“çš„æ€§èƒ½æå‡å¹…åº¦å–å†³äºæ•°æ®é›†å’Œä»»åŠ¡ï¼Œä½†æ€»ä½“è¶‹åŠ¿æ˜¯LLM4GTAèƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªæ•°æ®é›†ä¸Šï¼ŒLLM4GTAå¯èƒ½æ¯”æœ€ä½³åŸºçº¿æ–¹æ³•æé«˜äº†5-10%çš„å‡†ç¡®ç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LLM4GTAæ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦å›¾æ–‡å¯¹é½çš„åœºæ™¯ï¼Œä¾‹å¦‚çŸ¥è¯†å›¾è°±è¡¥å…¨ã€å›¾æ–‡æ£€ç´¢ã€è§†è§‰é—®ç­”ç­‰ã€‚è¯¥æ–¹æ³•é€šè¿‡æå‡æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥æœ‰æ•ˆåº”å¯¹å®é™…åº”ç”¨ä¸­å­˜åœ¨çš„å™ªå£°å’Œæ•°æ®ç¨€ç–é—®é¢˜ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨åŒ»ç–—ã€é‡‘èç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Representation learning on text-attributed graphs (TAGs) integrates structural connectivity with rich textual semantics, enabling applications in diverse domains. Current methods largely rely on contrastive learning to maximize cross-modal similarity, assuming tighter coupling between graph and text representations improves transfer performance. However, our empirical analysis reveals that both natural gap expansion and forced gap reduction result in performance degradation by disrupting pre-trained knowledge structures and impairing generalization. This arises from the geometric incompatibility between encoders, where graph encoders capture topological patterns, while text encoders capture semantic structures. Over-alignment compresses these distinct spaces into shared subspaces, causing structure collapse that diminishes both topological reasoning and semantic understanding. We propose \textbf{LLM4GTA}, a gap-aware alignment framework that preserves representation gaps as geometric necessities for maintaining modality-specific knowledge and improving transfer performance. LLM4GTA includes an adaptive gap preservation module to prevent over-alignment by monitoring similarity evolution and an intra-modal compensation mechanism that boosts discriminative power using auxiliary classifiers in graph space. Extensive experiments show significant improvements over existing methods in zero-shot and few-shot scenarios.

