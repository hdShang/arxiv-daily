---
layout: default
title: Can Representation Gaps Be the Key to Enhancing Robustness in Graph-Text Alignment?
---

# Can Representation Gaps Be the Key to Enhancing Robustness in Graph-Text Alignment?

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.12087" target="_blank" class="toolbar-btn">arXiv: 2510.12087v1</a>
    <a href="https://arxiv.org/pdf/2510.12087.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.12087v1" 
            onclick="toggleFavorite(this, '2510.12087v1', 'Can Representation Gaps Be the Key to Enhancing Robustness in Graph-Text Alignment?')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Heng Zhang, Tianyi Zhang, Yuling Shi, Xiaodong Gu, Yaomin Shen, Zijian Zhang, Yilei Yuan, Hao Zhang, Jin Huang

**ÂàÜÁ±ª**: cs.GR

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-14

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫LLM4GTAÊ°ÜÊû∂ÔºåÈÄöËøá‰øùÊåÅË°®ÂæÅÂ∑ÆÂºÇÊèêÂçáÂõæÊñáÂØπÈΩêÁöÑÈ≤ÅÊ£íÊÄß**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ÂõæÊñáÂØπÈΩê` `Ë°®ÂæÅÂ≠¶‰π†` `ÂØπÊØîÂ≠¶‰π†` `È≤ÅÊ£íÊÄß` `Èõ∂Ê†∑Êú¨Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂõæÊñáÂØπÈΩêÊñπÊ≥ïËøáÂ∫¶Âº∫Ë∞ÉË∑®Ê®°ÊÄÅÁõ∏‰ººÊÄßÔºåÂøΩÁï•‰∫ÜÂõæÁªìÊûÑÂíåÊñáÊú¨ËØ≠‰πâÁöÑÂõ∫ÊúâÂ∑ÆÂºÇÔºåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇ
2. LLM4GTAÊ°ÜÊû∂ÈÄöËøáËá™ÈÄÇÂ∫îÂú∞‰øùÊåÅÂõæÊñáË°®ÂæÅ‰πãÈó¥ÁöÑÈó¥ÈöôÔºåÈÅøÂÖçËøáÂ∫¶ÂØπÈΩêÔºå‰ªéËÄå‰øùÁïôÊ®°ÊÄÅÁâπÂÆöÁü•ËØÜ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLLM4GTAÂú®Èõ∂Ê†∑Êú¨ÂíåÂ∞èÊ†∑Êú¨Âú∫ÊôØ‰∏ãÔºåÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÁ†îÁ©∂‰∫ÜÊñáÊú¨Â±ûÊÄßÂõæÔºàTAGsÔºâ‰∏äÁöÑË°®ÂæÅÂ≠¶‰π†ÔºåËØ•ÊñπÊ≥ïÂ∞ÜÁªìÊûÑËøûÈÄöÊÄß‰∏é‰∏∞ÂØåÁöÑÊñáÊú¨ËØ≠‰πâÁõ∏ÁªìÂêàÔºåÂ∫îÁî®‰∫éÂ§ö‰∏™È¢ÜÂüü„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶Å‰æùËµñÂØπÊØîÂ≠¶‰π†Êù•ÊúÄÂ§ßÂåñË∑®Ê®°ÊÄÅÁõ∏‰ººÊÄßÔºåËÆ§‰∏∫ÂõæÂíåÊñáÊú¨Ë°®ÂæÅ‰πãÈó¥Êõ¥Á¥ßÂØÜÁöÑËÄ¶ÂêàÂèØ‰ª•ÊèêÈ´òËøÅÁßªÊÄßËÉΩ„ÄÇÁÑ∂ËÄåÔºåÊàë‰ª¨ÁöÑÁªèÈ™åÂàÜÊûêË°®ÊòéÔºåËá™ÁÑ∂Èó¥ÈöôÁöÑÊâ©Â§ßÂíåÂº∫Âà∂Èó¥ÈöôÁöÑÁº©Â∞èÈÉΩ‰ºöÈÄöËøáÁ†¥ÂùèÈ¢ÑËÆ≠ÁªÉÁöÑÁü•ËØÜÁªìÊûÑÂíåÊçüÂÆ≥Ê≥õÂåñËÉΩÂäõËÄåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇËøôÊòØÁî±‰∫éÁºñÁ†ÅÂô®‰πãÈó¥ÁöÑÂá†‰Ωï‰∏çÂÖºÂÆπÊÄßÈÄ†ÊàêÁöÑÔºåÂÖ∂‰∏≠ÂõæÁºñÁ†ÅÂô®ÊçïËé∑ÊãìÊâëÊ®°ÂºèÔºåËÄåÊñáÊú¨ÁºñÁ†ÅÂô®ÊçïËé∑ËØ≠‰πâÁªìÊûÑ„ÄÇËøáÂ∫¶ÂØπÈΩêÂ∞ÜËøô‰∫õ‰∏çÂêåÁöÑÁ©∫Èó¥ÂéãÁº©Âà∞ÂÖ±‰∫´Â≠êÁ©∫Èó¥‰∏≠ÔºåÂØºËá¥ÁªìÊûÑÂ¥©Ê∫ÉÔºå‰ªéËÄåÂâäÂº±‰∫ÜÊãìÊâëÊé®ÁêÜÂíåËØ≠‰πâÁêÜËß£„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜLLM4GTAÔºå‰∏Ä‰∏™Èó¥ÈöôÊÑüÁü•ÂØπÈΩêÊ°ÜÊû∂ÔºåÂÆÉ‰øùÁïô‰∫ÜË°®ÂæÅÈó¥ÈöôÔºå‰Ωú‰∏∫‰øùÊåÅÊ®°ÊÄÅÁâπÂÆöÁü•ËØÜÂíåÊèêÈ´òËøÅÁßªÊÄßËÉΩÁöÑÂá†‰ΩïÂøÖË¶ÅÊÄß„ÄÇLLM4GTAÂåÖÊã¨‰∏Ä‰∏™Ëá™ÈÄÇÂ∫îÈó¥Èöô‰øùÊåÅÊ®°ÂùóÔºåÈÄöËøáÁõëÊéßÁõ∏‰ººÊÄßÊºîÂèòÊù•Èò≤Ê≠¢ËøáÂ∫¶ÂØπÈΩêÔºå‰ª•Âèä‰∏Ä‰∏™‰ΩøÁî®ÂõæÁ©∫Èó¥‰∏≠ÁöÑËæÖÂä©ÂàÜÁ±ªÂô®Êù•ÊèêÈ´òÂà§Âà´ËÉΩÂäõÁöÑÊ®°ÊÄÅÂÜÖË°•ÂÅøÊú∫Âà∂„ÄÇÂ§ßÈáèÁöÑÂÆûÈ™åË°®ÊòéÔºåÂú®Èõ∂Ê†∑Êú¨ÂíåÂ∞èÊ†∑Êú¨Âú∫ÊôØ‰∏≠ÔºåËØ•ÊñπÊ≥ïÊØîÁé∞ÊúâÊñπÊ≥ïÊúâÊòæËëóÁöÑÊîπËøõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂõæÊñáÂØπÈΩêÊñπÊ≥ïÔºåÁâπÂà´ÊòØÂü∫‰∫éÂØπÊØîÂ≠¶‰π†ÁöÑÊñπÊ≥ïÔºåÂÄæÂêë‰∫éÊúÄÂ§ßÂåñÂõæÂíåÊñáÊú¨Ë°®ÂæÅ‰πãÈó¥ÁöÑÁõ∏‰ººÊÄßÔºåÊúüÊúõÊõ¥Á¥ßÂØÜÁöÑËÄ¶ÂêàËÉΩÂ∏¶Êù•Êõ¥Â•ΩÁöÑËøÅÁßªÊÄßËÉΩ„ÄÇÁÑ∂ËÄåÔºåËøôÁßçÂÅöÊ≥ïÂøΩÁï•‰∫ÜÂõæÁªìÊûÑÂíåÊñáÊú¨ËØ≠‰πâÁöÑÊú¨Ë¥®Âå∫Âà´„ÄÇÂõæÁºñÁ†ÅÂô®‰æßÈáç‰∫éÊçïËé∑ÊãìÊâëÊ®°ÂºèÔºåËÄåÊñáÊú¨ÁºñÁ†ÅÂô®‰æßÈáç‰∫éÊçïËé∑ËØ≠‰πâÁªìÊûÑ„ÄÇËøáÂ∫¶ÂØπÈΩê‰ºöÂØºËá¥‰ø°ÊÅØÂéãÁº©ÔºåÊçüÂÆ≥Ê®°ÂûãÂØπÊãìÊâëÁªìÊûÑÂíåËØ≠‰πâ‰ø°ÊÅØÁöÑÁêÜËß£ËÉΩÂäõÔºåÊúÄÁªàÈôç‰ΩéÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöLLM4GTAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØ‰øùÊåÅÂõæÊñáË°®ÂæÅ‰πãÈó¥ÁöÑ‚ÄúÈó¥Èöô‚ÄùÔºåÂç≥‰∏çÂº∫Ë°åÂ∞ÜÂÆÉ‰ª¨ÂØπÈΩêÂà∞ÂÆåÂÖ®Áõ∏ÂêåÁöÑÁ©∫Èó¥„ÄÇ‰ΩúËÄÖËÆ§‰∏∫ÔºåËøôÁßçÈó¥ÈöôÊòØÂøÖË¶ÅÁöÑÔºåÂõ†‰∏∫ÂÆÉÂèçÊò†‰∫ÜÂõæÂíåÊñáÊú¨Ê®°ÊÄÅÁöÑÂõ∫ÊúâÂ∑ÆÂºÇÔºå‰øùÁïô‰∫ÜÂêÑËá™Ê®°ÊÄÅÁöÑÁâπÂÆöÁü•ËØÜ„ÄÇÈÄöËøáÁª¥ÊåÅÈÄÇÂΩìÁöÑÈó¥ÈöôÔºåÊ®°ÂûãÂèØ‰ª•Êõ¥Â•ΩÂú∞Âà©Áî®‰∏§ÁßçÊ®°ÊÄÅÁöÑ‰ø°ÊÅØÔºå‰ªéËÄåÊèêÈ´òÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöLLM4GTAÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™Ê†∏ÂøÉÊ®°ÂùóÔºöËá™ÈÄÇÂ∫îÈó¥Èöô‰øùÊåÅÊ®°ÂùóÂíåÊ®°ÊÄÅÂÜÖË°•ÂÅøÊú∫Âà∂„ÄÇËá™ÈÄÇÂ∫îÈó¥Èöô‰øùÊåÅÊ®°ÂùóÈÄöËøáÁõëÊéßÂõæÊñáË°®ÂæÅÁöÑÁõ∏‰ººÊÄßÊºîÂèòÔºåÂä®ÊÄÅË∞ÉÊï¥ÂØπÈΩêÁöÑÂº∫Â∫¶ÔºåÈò≤Ê≠¢ËøáÂ∫¶ÂØπÈΩê„ÄÇÊ®°ÊÄÅÂÜÖË°•ÂÅøÊú∫Âà∂ÂàôÈÄöËøáÂú®ÂõæÁ©∫Èó¥‰∏≠ÂºïÂÖ•ËæÖÂä©ÂàÜÁ±ªÂô®ÔºåÂ¢ûÂº∫ÂõæË°®ÂæÅÁöÑÂà§Âà´ËÉΩÂäõÔºåÂº•Ë°•Âõ†‰øùÊåÅÈó¥ÈöôËÄåÂèØËÉΩÂØºËá¥ÁöÑÊÄßËÉΩÊçüÂ§±„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÂÖàÂàÜÂà´ÂØπÂõæÂíåÊñáÊú¨ËøõË°åÁºñÁ†ÅÔºåÁÑ∂ÂêéÈÄöËøáËá™ÈÄÇÂ∫îÈó¥Èöô‰øùÊåÅÊ®°ÂùóËøõË°åÂØπÈΩêÔºåÊúÄÂêéÂà©Áî®Ê®°ÊÄÅÂÜÖË°•ÂÅøÊú∫Âà∂Â¢ûÂº∫ÂõæË°®ÂæÅ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöLLM4GTAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂‚ÄúÈó¥ÈöôÊÑüÁü•‚ÄùÁöÑÂØπÈΩêÁ≠ñÁï•„ÄÇ‰∏é‰ª•ÂæÄËøΩÊ±ÇÊúÄÂ§ßÂåñË∑®Ê®°ÊÄÅÁõ∏‰ººÊÄßÁöÑÊñπÊ≥ï‰∏çÂêåÔºåLLM4GTA‰∏ªÂä®‰øùÊåÅÂõæÊñáË°®ÂæÅ‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºåËÆ§‰∏∫ËøôÁßçÂ∑ÆÂºÇÂØπ‰∫é‰øùÁïôÊ®°ÊÄÅÁâπÂÆöÁü•ËØÜËá≥ÂÖ≥ÈáçË¶Å„ÄÇËøôÁßçÂèçÁõ¥ËßâÁöÑËÆæËÆ°ÁêÜÂøµÊòØÊú¨ÊñáÊúÄÂ§ßÁöÑ‰∫ÆÁÇπ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËá™ÈÄÇÂ∫îÈó¥Èöô‰øùÊåÅÊ®°ÂùóÁöÑÂÖ≥ÈîÆÂú®‰∫éÂ¶Ç‰ΩïË°°ÈáèÂíåÊéßÂà∂ÂõæÊñáË°®ÂæÅÁöÑÁõ∏‰ººÊÄß„ÄÇËÆ∫ÊñáÂèØËÉΩ‰ΩøÁî®‰∫ÜÊüêÁßçÁõ∏‰ººÊÄßÂ∫¶ÈáèÔºà‰æãÂ¶Ç‰ΩôÂº¶Áõ∏‰ººÂ∫¶ÔºâÊù•ËØÑ‰º∞ÂõæÊñáË°®ÂæÅÁöÑÊé•ËøëÁ®ãÂ∫¶ÔºåÂπ∂ËÆæËÆ°‰∫Ü‰∏Ä‰∏™ÊçüÂ§±ÂáΩÊï∞Êù•ÊÉ©ÁΩöËøáÂ∫¶ÂØπÈΩê„ÄÇÊ®°ÊÄÅÂÜÖË°•ÂÅøÊú∫Âà∂ÁöÑÂÖ≥ÈîÆÂú®‰∫éËæÖÂä©ÂàÜÁ±ªÂô®ÁöÑËÆæËÆ°ÔºåÂåÖÊã¨ÂàÜÁ±ªÂô®ÁöÑÁªìÊûÑ„ÄÅËÆ≠ÁªÉÊï∞ÊçÆÂíåÊçüÂ§±ÂáΩÊï∞„ÄÇÂÖ∑‰ΩìÁöÑÊäÄÊúØÁªÜËäÇÔºà‰æãÂ¶ÇÊçüÂ§±ÂáΩÊï∞ÁöÑÂÖ∑‰ΩìÂΩ¢Âºè„ÄÅËæÖÂä©ÂàÜÁ±ªÂô®ÁöÑÁªìÊûÑÔºâÈúÄË¶ÅÂèÇËÄÉËÆ∫ÊñáÂéüÊñá„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLLM4GTAÂú®Èõ∂Ê†∑Êú¨ÂíåÂ∞èÊ†∑Êú¨Âú∫ÊôØ‰∏ãÔºåÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑÂõæÊñáÂØπÈΩêÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ÂèñÂÜ≥‰∫éÊï∞ÊçÆÈõÜÂíå‰ªªÂä°Ôºå‰ΩÜÊÄª‰ΩìË∂ãÂäøÊòØLLM4GTAËÉΩÂ§üÊúâÊïàÂú∞ÊèêÈ´òÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ‰æãÂ¶ÇÔºåÂú®Êüê‰∏™Êï∞ÊçÆÈõÜ‰∏äÔºåLLM4GTAÂèØËÉΩÊØîÊúÄ‰Ω≥Âü∫Á∫øÊñπÊ≥ïÊèêÈ´ò‰∫Ü5-10%ÁöÑÂáÜÁ°ÆÁéá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

LLM4GTAÊ°ÜÊû∂ÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÂõæÊñáÂØπÈΩêÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÁü•ËØÜÂõæË∞±Ë°•ÂÖ®„ÄÅÂõæÊñáÊ£ÄÁ¥¢„ÄÅËßÜËßâÈóÆÁ≠îÁ≠â„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊèêÂçáÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõÔºåÂèØ‰ª•ÊúâÊïàÂ∫îÂØπÂÆûÈôÖÂ∫îÁî®‰∏≠Â≠òÂú®ÁöÑÂô™Â£∞ÂíåÊï∞ÊçÆÁ®ÄÁñèÈóÆÈ¢ò„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÊúâÊúõÂú®ÂåªÁñó„ÄÅÈáëËûçÁ≠âÈ¢ÜÂüüÂèëÊå•ÈáçË¶Å‰ΩúÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Representation learning on text-attributed graphs (TAGs) integrates structural connectivity with rich textual semantics, enabling applications in diverse domains. Current methods largely rely on contrastive learning to maximize cross-modal similarity, assuming tighter coupling between graph and text representations improves transfer performance. However, our empirical analysis reveals that both natural gap expansion and forced gap reduction result in performance degradation by disrupting pre-trained knowledge structures and impairing generalization. This arises from the geometric incompatibility between encoders, where graph encoders capture topological patterns, while text encoders capture semantic structures. Over-alignment compresses these distinct spaces into shared subspaces, causing structure collapse that diminishes both topological reasoning and semantic understanding. We propose \textbf{LLM4GTA}, a gap-aware alignment framework that preserves representation gaps as geometric necessities for maintaining modality-specific knowledge and improving transfer performance. LLM4GTA includes an adaptive gap preservation module to prevent over-alignment by monitoring similarity evolution and an intra-modal compensation mechanism that boosts discriminative power using auxiliary classifiers in graph space. Extensive experiments show significant improvements over existing methods in zero-shot and few-shot scenarios.

