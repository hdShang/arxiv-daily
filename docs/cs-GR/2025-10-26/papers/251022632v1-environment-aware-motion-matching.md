---
layout: default
title: Environment-aware Motion Matching
---

# Environment-aware Motion Matching

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.22632" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.22632v1</a>
  <a href="https://arxiv.org/pdf/2510.22632.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22632v1" onclick="toggleFavorite(this, '2510.22632v1', 'Environment-aware Motion Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jose Luis Ponton, Sheldon Andrews, Carlos Andujar, Nuria Pelechano

**åˆ†ç±»**: cs.GR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-26

**å¤‡æ³¨**: Published in ACM TOG and presented in SIGGRAPH ASIA 2025. Project webpage: https://upc-virvig.github.io/Environment-aware-Motion-Matching/

**DOI**: [10.1145/3763334](https://doi.org/10.1145/3763334)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¯å¢ƒæ„ŸçŸ¥è¿åŠ¨åŒ¹é…ï¼Œè§£å†³è§’è‰²ä¸åŠ¨æ€ç¯å¢ƒè‡ªç„¶äº¤äº’çš„éš¾é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `è¿åŠ¨åŒ¹é…` `è§’è‰²åŠ¨ç”»` `ç¯å¢ƒæ„ŸçŸ¥` `äººæœºäº¤äº’` `äººç¾¤åŠ¨ç”»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§’è‰²åŠ¨ç”»æ–¹æ³•éš¾ä»¥å¤„ç†å¤æ‚åŠ¨æ€ç¯å¢ƒäº¤äº’ï¼Œç¼ºä¹è‡ªç„¶æ€§å’Œçµæ´»æ€§ã€‚
2. æå‡ºç¯å¢ƒæ„ŸçŸ¥è¿åŠ¨åŒ¹é…ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´å§¿åŠ¿å’Œè½¨è¿¹ï¼Œä½¿è§’è‰²è‡ªç„¶é€‚åº”ç¯å¢ƒã€‚
3. è¯¥æ–¹æ³•åœ¨è¿è¡Œæ—¶é«˜æ•ˆæœç´¢åŒ¹é…ç”¨æˆ·è¾“å…¥å’Œå½“å‰å§¿åŠ¿ï¼ŒåŒæ—¶é¿å…ä¸ç¯å¢ƒç¢°æ’ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äº¤äº’å¼åº”ç”¨éœ€è¦è§’è‰²èƒ½å¤Ÿå¯¹åŠ¨æ€ç¯å¢ƒåšå‡ºè‡ªç„¶çš„ååº”ã€‚ä¼ ç»Ÿçš„è§’è‰²åŠ¨ç”»æŠ€æœ¯éš¾ä»¥å¤„ç†ä»»æ„æƒ…å†µï¼Œå› æ­¤åŠ¨æ€é€‰æ‹©è¿åŠ¨æ•æ‰åŠ¨ç”»çš„æ–¹æ³•æ—¥ç›Šæµè¡Œï¼Œè¿™ç§æ–¹æ³•åŸºäºé¢„å®šä¹‰çš„ç‰¹å¾ã€‚è¿åŠ¨åŒ¹é…åœ¨å¯¹é½ç›®æ ‡è½¨è¿¹çš„è¿åŠ¨æ–¹é¢å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ï¼Œä½†ç”±äºéœ€è¦è€ƒè™‘å‘¨å›´çš„å…ƒç´ ï¼Œå› æ­¤åŠ¨ç”»ç¯å¢ƒäº¤äº’å’Œäººç¾¤è¡Œä¸ºä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸æ¶‰åŠæ‰‹åŠ¨è®¾ç½®æˆ–ç¼ºä¹è¿åŠ¨æ•æ‰çš„è‡ªç„¶æ€§ã€‚æ­¤å¤–ï¼Œåœ¨äººç¾¤åŠ¨ç”»ä¸­ï¼Œèº«ä½“åŠ¨ç”»é€šå¸¸è¢«è§†ä¸ºä¸è½¨è¿¹è§„åˆ’åˆ†ç¦»çš„è¿‡ç¨‹ï¼Œå¯¼è‡´èº«ä½“å§¿åŠ¿å’Œæ ¹è¿åŠ¨ä¹‹é—´çš„ä¸ä¸€è‡´ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¯å¢ƒæ„ŸçŸ¥è¿åŠ¨åŒ¹é…ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå…¨èº«è§’è‰²åŠ¨ç”»çš„æ–°å‹å®æ—¶ç³»ç»Ÿï¼Œå¯ä»¥åŠ¨æ€é€‚åº”éšœç¢ç‰©å’Œå…¶ä»–æ™ºèƒ½ä½“ï¼Œå¼ºè°ƒå§¿åŠ¿å’Œè½¨è¿¹ä¹‹é—´çš„åŒå‘å…³ç³»ã€‚åœ¨é¢„å¤„ç†æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬ä»è¿åŠ¨æ•æ‰æ•°æ®åº“ä¸­æå–å½¢çŠ¶ã€å§¿åŠ¿å’Œè½¨è¿¹ç‰¹å¾ã€‚åœ¨è¿è¡Œæ—¶ï¼Œæˆ‘ä»¬æ‰§è¡Œé«˜æ•ˆçš„æœç´¢ï¼Œè¯¥æœç´¢åŒ¹é…ç”¨æˆ·è¾“å…¥å’Œå½“å‰å§¿åŠ¿ï¼ŒåŒæ—¶æƒ©ç½šä¸åŠ¨æ€ç¯å¢ƒçš„ç¢°æ’ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…è®¸è§’è‰²è‡ªç„¶åœ°è°ƒæ•´å…¶å§¿åŠ¿å’Œè½¨è¿¹ä»¥åœ¨æ‹¥æŒ¤çš„åœºæ™¯ä¸­å¯¼èˆªã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§’è‰²åŠ¨ç”»æŠ€æœ¯åœ¨å¤„ç†ä¸åŠ¨æ€ç¯å¢ƒçš„äº¤äº’æ—¶å­˜åœ¨å±€é™æ€§ã€‚ä¼ ç»Ÿçš„è¿åŠ¨åŒ¹é…æ–¹æ³•è™½ç„¶åœ¨è§’è‰²è¿åŠ¨æ§åˆ¶æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†éš¾ä»¥è‡ªç„¶åœ°å¤„ç†è§’è‰²ä¸ç¯å¢ƒï¼ˆå¦‚éšœç¢ç‰©ã€å…¶ä»–è§’è‰²ï¼‰çš„äº¤äº’ã€‚æ­¤å¤–ï¼Œäººç¾¤åŠ¨ç”»ä¸­ï¼Œèº«ä½“åŠ¨ç”»å’Œè½¨è¿¹è§„åˆ’é€šå¸¸æ˜¯åˆ†ç¦»çš„ï¼Œå¯¼è‡´åŠ¨ç”»ä¸è‡ªç„¶ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆéœ€è¦å¤§é‡æ‰‹åŠ¨è®¾ç½®ï¼Œè¦ä¹ˆç¼ºä¹è¿åŠ¨æ•æ‰çš„è‡ªç„¶æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å»ºç«‹ä¸€ä¸ªç¯å¢ƒæ„ŸçŸ¥çš„è¿åŠ¨åŒ¹é…ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèƒ½å¤ŸåŒæ—¶è€ƒè™‘è§’è‰²çš„å§¿åŠ¿ã€è½¨è¿¹ä»¥åŠå‘¨å›´ç¯å¢ƒçš„ä¿¡æ¯ã€‚é€šè¿‡åœ¨è¿åŠ¨åŒ¹é…è¿‡ç¨‹ä¸­å¼•å…¥ç¯å¢ƒå› ç´ ï¼Œä½¿è§’è‰²èƒ½å¤Ÿè‡ªç„¶åœ°è°ƒæ•´å§¿åŠ¿å’Œè½¨è¿¹ï¼Œä»è€Œå®ç°ä¸ç¯å¢ƒçš„è‡ªç„¶äº¤äº’ã€‚è¿™ç§æ–¹æ³•å¼ºè°ƒå§¿åŠ¿å’Œè½¨è¿¹ä¹‹é—´çš„åŒå‘å…³ç³»ï¼Œä½¿å¾—è§’è‰²çš„è¿åŠ¨æ›´åŠ çœŸå®å’Œå¯ä¿¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç³»ç»ŸåŒ…å«é¢„å¤„ç†å’Œè¿è¡Œæ—¶ä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚åœ¨é¢„å¤„ç†é˜¶æ®µï¼Œä»è¿åŠ¨æ•æ‰æ•°æ®åº“ä¸­æå–å½¢çŠ¶ã€å§¿åŠ¿å’Œè½¨è¿¹ç‰¹å¾ï¼Œå¹¶æ„å»ºç´¢å¼•ç»“æ„ã€‚åœ¨è¿è¡Œæ—¶é˜¶æ®µï¼Œç³»ç»Ÿæ¥æ”¶ç”¨æˆ·è¾“å…¥å’Œå½“å‰è§’è‰²çŠ¶æ€ï¼Œç„¶åæ‰§è¡Œé«˜æ•ˆçš„æœç´¢ç®—æ³•ï¼Œä»è¿åŠ¨æ•æ‰æ•°æ®åº“ä¸­æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„åŠ¨ç”»ç‰‡æ®µã€‚æœç´¢è¿‡ç¨‹ä¸­ï¼Œç³»ç»Ÿä¼šè€ƒè™‘ç”¨æˆ·è¾“å…¥ã€å½“å‰å§¿åŠ¿ä»¥åŠç¯å¢ƒä¿¡æ¯ï¼Œå¹¶å¯¹ä¸ç¯å¢ƒçš„ç¢°æ’è¿›è¡Œæƒ©ç½šã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†ç¯å¢ƒä¿¡æ¯èå…¥åˆ°è¿åŠ¨åŒ¹é…è¿‡ç¨‹ä¸­ã€‚é€šè¿‡æ˜¾å¼åœ°è€ƒè™‘ç¯å¢ƒå› ç´ ï¼Œç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆæ›´åŠ è‡ªç„¶å’ŒçœŸå®çš„äº¤äº’åŠ¨ç”»ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¼ºè°ƒå§¿åŠ¿å’Œè½¨è¿¹ä¹‹é—´çš„åŒå‘å…³ç³»ï¼Œä½¿å¾—è§’è‰²çš„è¿åŠ¨æ›´åŠ åè°ƒå’Œä¸€è‡´ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨é¢„å¤„ç†é˜¶æ®µï¼Œéœ€è¦é€‰æ‹©åˆé€‚çš„å½¢çŠ¶ã€å§¿åŠ¿å’Œè½¨è¿¹ç‰¹å¾ã€‚åœ¨è¿è¡Œæ—¶é˜¶æ®µï¼Œéœ€è¦è®¾è®¡é«˜æ•ˆçš„æœç´¢ç®—æ³•ï¼Œå¹¶å®šä¹‰åˆé€‚çš„ç¢°æ’æƒ©ç½šå‡½æ•°ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œå‡½æ•°å½¢å¼éœ€è¦æ ¹æ®å…·ä½“çš„åº”ç”¨åœºæ™¯è¿›è¡Œè°ƒæ•´ã€‚ä¾‹å¦‚ï¼Œç¢°æ’æƒ©ç½šå‡½æ•°å¯ä»¥æ ¹æ®ç¢°æ’çš„ä¸¥é‡ç¨‹åº¦è¿›è¡Œè°ƒæ•´ï¼Œä»¥æ§åˆ¶è§’è‰²é¿è®©éšœç¢ç‰©çš„ç¨‹åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡æå‡ºäº†ä¸€ç§ç¯å¢ƒæ„ŸçŸ¥çš„è¿åŠ¨åŒ¹é…ç³»ç»Ÿï¼Œèƒ½å¤Ÿä½¿è§’è‰²è‡ªç„¶åœ°ä¸åŠ¨æ€ç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚é€šè¿‡åœ¨è¿åŠ¨åŒ¹é…è¿‡ç¨‹ä¸­è€ƒè™‘ç¯å¢ƒå› ç´ ï¼Œç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆæ›´åŠ è‡ªç„¶å’ŒçœŸå®çš„äº¤äº’åŠ¨ç”»ã€‚è™½ç„¶è®ºæ–‡ä¸­æ²¡æœ‰ç»™å‡ºå…·ä½“çš„æ€§èƒ½æ•°æ®ï¼Œä½†å…¶æ ¸å¿ƒæ€æƒ³å’Œæ–¹æ³•å…·æœ‰é‡è¦çš„ç†è®ºå’Œå®è·µæ„ä¹‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ¸¸æˆã€è™šæ‹Ÿç°å®ã€æœºå™¨äººç­‰é¢†åŸŸã€‚åœ¨æ¸¸æˆä¸­ï¼Œå¯ä»¥ä½¿æ¸¸æˆè§’è‰²æ›´åŠ æ™ºèƒ½å’Œé€¼çœŸåœ°ä¸ç¯å¢ƒäº’åŠ¨ã€‚åœ¨è™šæ‹Ÿç°å®ä¸­ï¼Œå¯ä»¥å¢å¼ºç”¨æˆ·çš„æ²‰æµ¸æ„Ÿå’Œäº¤äº’ä½“éªŒã€‚åœ¨æœºå™¨äººé¢†åŸŸï¼Œå¯ä»¥ä½¿æœºå™¨äººæ›´åŠ çµæ´»å’Œå®‰å…¨åœ°åœ¨å¤æ‚ç¯å¢ƒä¸­è¿åŠ¨ã€‚è¯¥æŠ€æœ¯æœ‰æœ›æ¨åŠ¨äººæœºäº¤äº’å’Œæ™ºèƒ½ä½“åŠ¨ç”»çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Interactive applications demand believable characters that respond naturally to dynamic environments. Traditional character animation techniques often struggle to handle arbitrary situations, leading to a growing trend of dynamically selecting motion-captured animations based on predefined features. While Motion Matching has proven effective for locomotion by aligning to target trajectories, animating environment interactions and crowd behaviors remains challenging due to the need to consider surrounding elements. Existing approaches often involve manual setup or lack the naturalism of motion capture. Furthermore, in crowd animation, body animation is frequently treated as a separate process from trajectory planning, leading to inconsistencies between body pose and root motion. To address these limitations, we present Environment-aware Motion Matching, a novel real-time system for full-body character animation that dynamically adapts to obstacles and other agents, emphasizing the bidirectional relationship between pose and trajectory. In a preprocessing step, we extract shape, pose, and trajectory features from a motion capture database. At runtime, we perform an efficient search that matches user input and current pose while penalizing collisions with a dynamic environment. Our method allows characters to naturally adjust their pose and trajectory to navigate crowded scenes.

