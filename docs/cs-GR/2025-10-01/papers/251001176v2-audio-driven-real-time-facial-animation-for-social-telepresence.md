---
layout: default
title: Audio Driven Real-Time Facial Animation for Social Telepresence
---

# Audio Driven Real-Time Facial Animation for Social Telepresence

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.01176" target="_blank" class="toolbar-btn">arXiv: 2510.01176v2</a>
    <a href="https://arxiv.org/pdf/2510.01176.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01176v2" 
            onclick="toggleFavorite(this, '2510.01176v2', 'Audio Driven Real-Time Facial Animation for Social Telepresence')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Jiye Lee, Chenghui Li, Linh Tran, Shih-En Wei, Jason Saragih, Alexander Richard, Hanbyul Joo, Shaojie Bai

**åˆ†ç±»**: cs.GR, cs.CV, cs.LG, cs.SD

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01 (æ›´æ–°: 2025-11-01)

**å¤‡æ³¨**: SIGGRAPH Asia 2025. Project page: https://jiyewise.github.io/projects/AudioRTA

**DOI**: [10.1145/3757377.3763854](https://doi.org/10.1145/3757377.3763854)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§åŸºäºéŸ³é¢‘é©±åŠ¨çš„å®æ—¶é¢éƒ¨åŠ¨ç”»ç³»ç»Ÿï¼Œç”¨äºç¤¾äº¤è¿œç¨‹å‘ˆç°ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éŸ³é¢‘é©±åŠ¨` `é¢éƒ¨åŠ¨ç”»` `å®æ—¶æ¸²æŸ“` `æ‰©æ•£æ¨¡å‹` `åœ¨çº¿Transformer` `è’¸é¦å­¦ä¹ ` `ç¤¾äº¤VR`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰éŸ³é¢‘é©±åŠ¨é¢éƒ¨åŠ¨ç”»æ–¹æ³•éš¾ä»¥å…¼é¡¾é«˜çœŸå®åº¦ã€ä½å»¶è¿Ÿå’Œå®æ—¶æ€§ï¼Œé™åˆ¶äº†å…¶åœ¨ç¤¾äº¤VRä¸­çš„åº”ç”¨ã€‚
2. åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œç»“åˆåœ¨çº¿Transformerå’Œè’¸é¦æŠ€æœ¯ï¼Œå®ç°é«˜è´¨é‡ã€ä½å»¶è¿Ÿçš„å®æ—¶é¢éƒ¨åŠ¨ç”»ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é¢éƒ¨åŠ¨ç”»å‡†ç¡®æ€§ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæ¨ç†é€Ÿåº¦æå‡æ˜¾è‘—ï¼Œå¹¶æˆåŠŸåº”ç”¨äºå®æ—¶VRæ¼”ç¤ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§éŸ³é¢‘é©±åŠ¨çš„å®æ—¶ç³»ç»Ÿï¼Œç”¨äºä¸ºè™šæ‹Ÿç°å®ä¸­çš„ç¤¾äº¤äº’åŠ¨ç”Ÿæˆé€¼çœŸçš„3Dé¢éƒ¨å¤´åƒåŠ¨ç”»ï¼Œä¸”å»¶è¿Ÿæä½ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªç¼–ç å™¨æ¨¡å‹ï¼Œèƒ½å¤Ÿå®æ—¶å°†éŸ³é¢‘ä¿¡å·è½¬æ¢ä¸ºæ½œåœ¨çš„é¢éƒ¨è¡¨æƒ…åºåˆ—ï¼Œç„¶åå°†å…¶è§£ç ä¸ºé€¼çœŸçš„3Dé¢éƒ¨å¤´åƒã€‚åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œç³»ç»Ÿèƒ½å¤Ÿæ•æ‰è‡ªç„¶äº¤æµæ‰€éœ€çš„ä¸°å¯Œé¢éƒ¨è¡¨æƒ…ï¼ŒåŒæ—¶å®ç°å®æ—¶æ€§èƒ½ï¼ˆ<15ms GPUæ—¶é—´ï¼‰ã€‚é€šè¿‡åœ¨çº¿Transformerï¼ˆæ¶ˆé™¤äº†å¯¹æœªæ¥è¾“å…¥çš„ä¾èµ–ï¼‰å’Œè’¸é¦æµæ°´çº¿ï¼ˆå°†è¿­ä»£å»å™ªåŠ é€Ÿä¸ºå•æ­¥ï¼‰ï¼Œè¯¥æ¶æ„æœ€å¤§é™åº¦åœ°é™ä½äº†å»¶è¿Ÿã€‚æ­¤å¤–ï¼Œè¿˜è§£å†³äº†å®æ—¶åœºæ™¯ä¸­é€å¸§å¤„ç†è¿ç»­éŸ³é¢‘ä¿¡å·å¹¶ä¿æŒä¸€è‡´åŠ¨ç”»è´¨é‡çš„å…³é”®è®¾è®¡æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶çš„å¤šåŠŸèƒ½æ€§æ‰©å±•åˆ°å¤šæ¨¡æ€åº”ç”¨ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿæ¡ä»¶ç­‰è¯­ä¹‰æ¨¡æ€ä»¥åŠVRå¤´æ˜¾ä¸Šçš„å¤´æˆ´å¼çœ¼åŠ¨ç›¸æœºç­‰ä¼ æ„Ÿå™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„ç¦»çº¿æœ€å…ˆè¿›åŸºçº¿ç›¸æ¯”ï¼Œé¢éƒ¨åŠ¨ç”»çš„å‡†ç¡®æ€§å¾—åˆ°äº†æ˜¾è‘—æé«˜ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†100åˆ°1000å€ã€‚é€šè¿‡å®æ—¶VRæ¼”ç¤ºå’Œå¤šè¯­è¨€è¯­éŸ³ç­‰å„ç§åœºæ™¯éªŒè¯äº†è¯¥æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰éŸ³é¢‘é©±åŠ¨çš„é¢éƒ¨åŠ¨ç”»æ–¹æ³•é€šå¸¸éš¾ä»¥åœ¨çœŸå®æ„Ÿã€å»¶è¿Ÿå’Œå®æ—¶æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ç¦»çº¿æ–¹æ³•è™½ç„¶å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„åŠ¨ç”»ï¼Œä½†å»¶è¿Ÿè¿‡é«˜ï¼Œæ— æ³•æ»¡è¶³å®æ—¶äº¤äº’çš„éœ€æ±‚ã€‚è€Œç°æœ‰çš„å®æ—¶æ–¹æ³•å¾€å¾€ç‰ºç‰²äº†åŠ¨ç”»çš„çœŸå®æ„Ÿå’Œä¸°å¯Œæ€§ã€‚å› æ­¤ï¼Œè¯¥è®ºæ–‡æ—¨åœ¨è§£å†³å¦‚ä½•åœ¨ä¿è¯é¢éƒ¨åŠ¨ç”»çœŸå®æ„Ÿçš„åŒæ—¶ï¼Œå®ç°æä½çš„å»¶è¿Ÿï¼Œä»è€Œæ»¡è¶³ç¤¾äº¤VRç­‰å®æ—¶åº”ç”¨çš„éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›æ¥æ•æ‰ä¸°å¯Œçš„é¢éƒ¨è¡¨æƒ…ï¼Œå¹¶é€šè¿‡åœ¨çº¿Transformerå’Œè’¸é¦æŠ€æœ¯æ¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹ï¼Œä»è€Œå®ç°å®æ—¶æ€§èƒ½ã€‚é€šè¿‡å°†éŸ³é¢‘ä¿¡å·ç¼–ç ä¸ºæ½œåœ¨çš„é¢éƒ¨è¡¨æƒ…åºåˆ—ï¼Œå¹¶ä½¿ç”¨æ‰©æ•£æ¨¡å‹è§£ç ä¸ºé€¼çœŸçš„3Dé¢éƒ¨å¤´åƒï¼Œå¯ä»¥åœ¨ä¿è¯åŠ¨ç”»è´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½å»¶è¿Ÿã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç³»ç»Ÿä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šç¼–ç é˜¶æ®µå’Œè§£ç é˜¶æ®µã€‚åœ¨ç¼–ç é˜¶æ®µï¼Œä½¿ç”¨ä¸€ä¸ªåœ¨çº¿Transformerå°†éŸ³é¢‘ä¿¡å·å®æ—¶è½¬æ¢ä¸ºæ½œåœ¨çš„é¢éƒ¨è¡¨æƒ…åºåˆ—ã€‚åœ¨çº¿Transformerçš„è®¾è®¡é¿å…äº†å¯¹æœªæ¥è¾“å…¥çš„ä¾èµ–ï¼Œä»è€Œé™ä½äº†å»¶è¿Ÿã€‚åœ¨è§£ç é˜¶æ®µï¼Œä½¿ç”¨ä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„è§£ç å™¨å°†æ½œåœ¨çš„é¢éƒ¨è¡¨æƒ…åºåˆ—è§£ç ä¸ºé€¼çœŸçš„3Dé¢éƒ¨å¤´åƒã€‚ä¸ºäº†è¿›ä¸€æ­¥åŠ é€Ÿè§£ç è¿‡ç¨‹ï¼Œé‡‡ç”¨äº†ä¸€ç§è’¸é¦æµæ°´çº¿ï¼Œå°†è¿­ä»£å»å™ªè¿‡ç¨‹åŠ é€Ÿä¸ºå•æ­¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹å‡ ç‚¹ï¼š1) æå‡ºäº†ä¸€ç§åœ¨çº¿Transformerï¼Œæ¶ˆé™¤äº†å¯¹æœªæ¥è¾“å…¥çš„ä¾èµ–ï¼Œä»è€Œé™ä½äº†å»¶è¿Ÿã€‚2) æå‡ºäº†ä¸€ç§è’¸é¦æµæ°´çº¿ï¼Œå°†è¿­ä»£å»å™ªè¿‡ç¨‹åŠ é€Ÿä¸ºå•æ­¥ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦ã€‚3) å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºå®æ—¶é¢éƒ¨åŠ¨ç”»ï¼Œä»è€Œåœ¨ä¿è¯åŠ¨ç”»è´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†å®æ—¶æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨çº¿Transformeré‡‡ç”¨äº†å› æœæ³¨æ„åŠ›æœºåˆ¶ï¼Œç¡®ä¿æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºåªä¾èµ–äºè¿‡å»çš„è¾“å…¥ã€‚è’¸é¦æµæ°´çº¿é€šè¿‡è®­ç»ƒä¸€ä¸ªå•æ­¥æ¨¡å‹æ¥é€¼è¿‘è¿­ä»£å»å™ªè¿‡ç¨‹çš„ç»“æœã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬é‡å»ºæŸå¤±å’Œå¯¹æŠ—æŸå¤±ï¼Œç”¨äºä¿è¯åŠ¨ç”»çš„çœŸå®æ„Ÿå’Œä¸€è‡´æ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é¢éƒ¨åŠ¨ç”»å‡†ç¡®æ€§ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„ç¦»çº¿æœ€å…ˆè¿›åŸºçº¿ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†100åˆ°1000å€ï¼Œå®ç°äº†<15msçš„GPUæ—¶é—´ã€‚é€šè¿‡å®æ—¶VRæ¼”ç¤ºå’Œå¤šè¯­è¨€è¯­éŸ³ç­‰å„ç§åœºæ™¯éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®æ—¶é¢éƒ¨åŠ¨ç”»é¢†åŸŸå…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºç¤¾äº¤VRã€è¿œç¨‹ä¼šè®®ã€è™šæ‹Ÿä¸»æ’­ã€æ¸¸æˆç­‰é¢†åŸŸã€‚é€šè¿‡å®æ—¶æ•æ‰å’Œç”Ÿæˆé€¼çœŸçš„é¢éƒ¨åŠ¨ç”»ï¼Œå¯ä»¥å¢å¼ºè™šæ‹Ÿäº¤äº’çš„æ²‰æµ¸æ„Ÿå’ŒçœŸå®æ„Ÿï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥åº”ç”¨äºä¸ªæ€§åŒ–å¤´åƒå®šåˆ¶ã€æƒ…æ„Ÿè¯†åˆ«å’Œè¡¨è¾¾ç­‰é¢†åŸŸï¼Œå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present an audio-driven real-time system for animating photorealistic 3D facial avatars with minimal latency, designed for social interactions in virtual reality for anyone. Central to our approach is an encoder model that transforms audio signals into latent facial expression sequences in real time, which are then decoded as photorealistic 3D facial avatars. Leveraging the generative capabilities of diffusion models, we capture the rich spectrum of facial expressions necessary for natural communication while achieving real-time performance (<15ms GPU time). Our novel architecture minimizes latency through two key innovations: an online transformer that eliminates dependency on future inputs and a distillation pipeline that accelerates iterative denoising into a single step. We further address critical design challenges in live scenarios for processing continuous audio signals frame-by-frame while maintaining consistent animation quality. The versatility of our framework extends to multimodal applications, including semantic modalities such as emotion conditions and multimodal sensors with head-mounted eye cameras on VR headsets. Experimental results demonstrate significant improvements in facial animation accuracy over existing offline state-of-the-art baselines, achieving 100 to 1000 times faster inference speed. We validate our approach through live VR demonstrations and across various scenarios such as multilingual speeches.

