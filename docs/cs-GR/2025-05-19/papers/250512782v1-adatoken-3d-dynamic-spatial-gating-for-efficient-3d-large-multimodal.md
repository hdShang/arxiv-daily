---
layout: default
title: AdaToken-3D: Dynamic Spatial Gating for Efficient 3D Large Multimodal-Models Reasoning
---

# AdaToken-3D: Dynamic Spatial Gating for Efficient 3D Large Multimodal-Models Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.12782" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.12782v1</a>
  <a href="https://arxiv.org/pdf/2505.12782.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.12782v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.12782v1', 'AdaToken-3D: Dynamic Spatial Gating for Efficient 3D Large Multimodal-Models Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kai Zhang, Xingyu Chen, Xiaofeng Zhang

**åˆ†ç±»**: cs.GR, cs.CV, cs.IR, cs.IT

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAdaToken-3Dä»¥è§£å†³3Då¤šæ¨¡æ€æ¨¡å‹æ¨ç†æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨¡å‹` `3Dåœºæ™¯ç†è§£` `ç©ºé—´æ ‡è®°ä¼˜åŒ–` `åŠ¨æ€ä¿®å‰ª` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„3Då¤šæ¨¡æ€æ¨¡å‹åœ¨æ¨ç†æ—¶é¢ä¸´è®¡ç®—å¼€é”€å¤§å’Œä¿¡æ¯å†—ä½™çš„é—®é¢˜ï¼Œå½±å“äº†æ•ˆç‡ã€‚
2. æå‡ºçš„AdaToken-3Dé€šè¿‡åŠ¨æ€ä¿®å‰ªå†—ä½™ç©ºé—´æ ‡è®°ï¼Œä¼˜åŒ–3D LMMçš„æ¨ç†è¿‡ç¨‹ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒAdaToken-3Dåœ¨æ¨ç†é€Ÿåº¦ä¸Šæå‡21%ï¼ŒFLOPså‡å°‘63%ï¼Œä¸”ä¿æŒäº†ä»»åŠ¡å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨æ·±åº¦å­¦ä¹ ä¸­å·²æˆä¸ºé‡è¦ç ”ç©¶æ–¹å‘ï¼Œå°¤å…¶åœ¨3Dåœºæ™¯ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå½“å‰çš„3D LMMsç”±äºä½¿ç”¨æˆåƒä¸Šä¸‡çš„ç©ºé—´æ ‡è®°è¿›è¡Œå¤šæ¨¡æ€æ¨ç†ï¼Œé¢ä¸´ç€è®¡ç®—å¼€é”€è¿‡å¤§å’Œä¿¡æ¯å†—ä½™çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†AdaToken-3Dï¼Œä¸€ä¸ªè‡ªé€‚åº”ç©ºé—´æ ‡è®°ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡ç©ºé—´è´¡çŒ®åˆ†æåŠ¨æ€ä¿®å‰ªå†—ä½™æ ‡è®°ã€‚è¯¥æ–¹æ³•é€šè¿‡æ³¨æ„åŠ›æ¨¡å¼æŒ–æ˜é‡åŒ–æ ‡è®°çº§ä¿¡æ¯æµï¼Œè‡ªåŠ¨è°ƒæ•´ä¿®å‰ªç­–ç•¥ä»¥é€‚åº”ä¸åŒçš„3D LMMæ¶æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAdaToken-3Dåœ¨ä¿æŒåŸä»»åŠ¡å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå®ç°äº†21%çš„æ¨ç†é€Ÿåº¦æå‡å’Œ63%çš„FLOPså‡å°‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰3Då¤šæ¨¡æ€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”±äºä½¿ç”¨å¤§é‡ç©ºé—´æ ‡è®°è€Œå¯¼è‡´çš„è®¡ç®—æ•ˆç‡ä½ä¸‹å’Œä¿¡æ¯å†—ä½™é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯æ—¶ï¼Œå­˜åœ¨æ¶æ„å†—ä½™ï¼Œå½±å“äº†æ•´ä½“æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAdaToken-3Dçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç©ºé—´è´¡çŒ®åˆ†æåŠ¨æ€ä¿®å‰ªå†—ä½™çš„ç©ºé—´æ ‡è®°ï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡ã€‚è¯¥æ–¹æ³•é€šè¿‡é‡åŒ–æ ‡è®°çº§çš„ä¿¡æ¯æµï¼Œè‡ªåŠ¨è°ƒæ•´ä¿®å‰ªç­–ç•¥ï¼Œä»¥é€‚åº”ä¸åŒçš„3D LMMæ¶æ„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ç©ºé—´æ ‡è®°çš„åŠ¨æ€ä¿®å‰ªæ¨¡å—å’Œæ³¨æ„åŠ›æ¨¡å¼æŒ–æ˜æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡åˆ†æç©ºé—´æ ‡è®°çš„è´¡çŒ®åº¦ï¼Œè¯†åˆ«å†—ä½™æ ‡è®°ï¼›ç„¶åï¼Œæ ¹æ®ä¸åŒæ¨¡å‹çš„éœ€æ±‚ï¼Œè°ƒæ•´ä¿®å‰ªç­–ç•¥ï¼Œæœ€ç»ˆå®ç°é«˜æ•ˆæ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§è‡ªé€‚åº”çš„ç©ºé—´æ ‡è®°ä¼˜åŒ–æ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®æ ‡è®°çš„å®é™…è´¡çŒ®åŠ¨æ€è°ƒæ•´æ ‡è®°çš„ä½¿ç”¨ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„é™æ€æ ‡è®°é€‰æ‹©æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œèƒ½å¤Ÿæ˜¾è‘—å‡å°‘å†—ä½™ä¿¡æ¯æµã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†æ³¨æ„åŠ›æ¨¡å¼æŒ–æ˜æŠ€æœ¯æ¥é‡åŒ–æ ‡è®°çº§ä¿¡æ¯æµï¼Œå¹¶è®¾ç½®äº†ç›¸åº”çš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ ‡è®°ä¿®å‰ªè¿‡ç¨‹ã€‚ç½‘ç»œç»“æ„æ–¹é¢ï¼ŒAdaToken-3Dèƒ½å¤Ÿçµæ´»é€‚åº”ä¸åŒçš„3D LMMæ¶æ„ï¼Œç¡®ä¿é«˜æ•ˆæ€§ä¸å‡†ç¡®æ€§å¹¶å­˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAdaToken-3Dåœ¨LLaVA-3Dï¼ˆä¸€ä¸ª7Bå‚æ•°çš„3D-LMMï¼‰ä¸Šå®ç°äº†21%çš„æ¨ç†é€Ÿåº¦æå‡å’Œ63%çš„FLOPså‡å°‘ï¼ŒåŒæ—¶ä¿æŒäº†åŸæœ‰ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚è¿™ä¸€æ˜¾è‘—çš„æ€§èƒ½æå‡éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰3Dåœºæ™¯ç†è§£ä»»åŠ¡ã€‚é€šè¿‡æé«˜3Då¤šæ¨¡æ€æ¨¡å‹çš„æ¨ç†æ•ˆç‡ï¼ŒAdaToken-3Dèƒ½å¤Ÿåœ¨å®æ—¶åº”ç”¨ä¸­æä¾›æ›´å¿«çš„å“åº”é€Ÿåº¦ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Multimodal Models (LMMs) have become a pivotal research focus in deep learning, demonstrating remarkable capabilities in 3D scene understanding. However, current 3D LMMs employing thousands of spatial tokens for multimodal reasoning suffer from critical inefficiencies: excessive computational overhead and redundant information flows. Unlike 2D VLMs processing single images, 3D LMMs exhibit inherent architectural redundancy due to the heterogeneous mechanisms between spatial tokens and visual tokens. To address this challenge, we propose AdaToken-3D, an adaptive spatial token optimization framework that dynamically prunes redundant tokens through spatial contribution analysis. Our method automatically tailors pruning strategies to different 3D LMM architectures by quantifying token-level information flows via attention pattern mining. Extensive experiments on LLaVA-3D (a 7B parameter 3D-LMM) demonstrate that AdaToken-3D achieves 21\% faster inference speed and 63\% FLOPs reduction while maintaining original task accuracy. Beyond efficiency gains, this work systematically investigates redundancy patterns in multimodal spatial information flows through quantitative token interaction analysis. Our findings reveal that over 60\% of spatial tokens contribute minimally ($<$5\%) to the final predictions, establishing theoretical foundations for efficient 3D multimodal learning.

