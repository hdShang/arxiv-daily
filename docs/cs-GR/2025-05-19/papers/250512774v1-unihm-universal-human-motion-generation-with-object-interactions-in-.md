---
layout: default
title: "UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes"
---

# UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.12774" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.12774v1</a>
  <a href="https://arxiv.org/pdf/2505.12774.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.12774v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.12774v1', 'UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zichen Geng, Zeeshan Hayder, Wei Liu, Ajmal Mian

**ÂàÜÁ±ª**: cs.GR, cs.AI, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-19

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫UniHM‰ª•Ëß£ÂÜ≥Â§çÊùÇÂú∫ÊôØ‰∏ã‰∫∫Á±ªÂä®‰ΩúÁîüÊàêÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±ÂõõÔºöÁîüÊàêÂºèÂä®‰Ωú (Generative Motion)** **ÊîØÊü±‰∫îÔºö‰∫§‰∫í‰∏éÂèçÂ∫î (Interaction & Reaction)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `‰∫∫Á±ªÂä®‰ΩúÂêàÊàê` `Â§öÊ®°ÊÄÅËûçÂêà` `Êâ©Êï£ÁîüÊàê` `ÂèòÂàÜËá™ÁºñÁ†ÅÂô®` `Â§çÊùÇÂú∫ÊôØÁêÜËß£` `ÊñáÊú¨Âà∞Âä®‰Ωú` `‰∫∫Á±ª-Áâ©‰Ωì‰∫§‰∫í`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑËØ≠Ë®ÄÊù°‰ª∂Âä®‰ΩúÊ®°ÂûãÂú®Â§çÊùÇÂú∫ÊôØ‰∏ãÁöÑÂä®‰ΩúÁîüÊàêËÉΩÂäõ‰∏çË∂≥ÔºåÊó†Ê≥ïÊúâÊïàÊçïÊçâ3DËøêÂä®ÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØ„ÄÇ
2. ÊèêÂá∫UniHMÊ°ÜÊû∂ÔºåÁªìÂêàÊâ©Êï£ÁîüÊàêÊñπÊ≥ïÔºåÊîØÊåÅÊñáÊú¨Âà∞Âä®‰ΩúÂíåÊñáÊú¨Âà∞‰∫∫Á±ª-Áâ©‰Ωì‰∫§‰∫íÁöÑÂêàÊàê„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåUniHMÂú®OMOMOÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂπ∂Âú®HumanML3D‰∏äÂÆûÁé∞‰∫ÜÁ´û‰∫âÊÄßÁªìÊûú„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰∫∫Á±ªÂä®‰ΩúÂêàÊàêÂú®Â§çÊùÇÂú∫ÊôØ‰∏≠Èù¢‰∏¥Âü∫Êú¨ÊåëÊàòÔºåË∂ÖË∂ä‰∫Ü‰º†ÁªüÁöÑÊñáÊú¨Âà∞Âä®‰Ωú‰ªªÂä°ÔºåÈúÄË¶ÅÊï¥ÂêàÈùôÊÄÅÁéØÂ¢É„ÄÅÂèØÁßªÂä®Áâ©‰Ωì„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÊèêÁ§∫ÂíåÁ©∫Èó¥Ë∑ØÂæÑÁ≠âÂ§öÁßçÊ®°ÊÄÅ„ÄÇÁé∞ÊúâÁöÑËØ≠Ë®ÄÊù°‰ª∂Âä®‰ΩúÊ®°ÂûãÂú®Âú∫ÊôØÊÑüÁü•Âä®‰ΩúÁîüÊàêÊñπÈù¢Â∏∏Â∏∏ÂèóÈôê‰∫éÂä®‰ΩúÊ†áËÆ∞ÂåñÁöÑÂ±ÄÈôêÔºåÂØºËá¥‰ø°ÊÅØ‰∏¢Â§±ÔºåÊó†Ê≥ïÊçïÊçâ3D‰∫∫Á±ªËøêÂä®ÁöÑËøûÁª≠ÊÄßÂíå‰∏ä‰∏ãÊñá‰æùËµñÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜUniHMÔºå‰∏Ä‰∏™Áªü‰∏ÄÁöÑËøêÂä®ËØ≠Ë®ÄÊ®°ÂûãÔºåÂà©Áî®Âü∫‰∫éÊâ©Êï£ÁöÑÁîüÊàêÊñπÊ≥ïÂêàÊàêÂú∫ÊôØÊÑüÁü•ÁöÑ‰∫∫Á±ªÂä®‰Ωú„ÄÇUniHMÊòØÁ¨¨‰∏Ä‰∏™ÊîØÊåÅÂ§çÊùÇ3DÂú∫ÊôØ‰∏≠ÊñáÊú¨Âà∞Âä®‰ΩúÂíåÊñáÊú¨Âà∞‰∫∫Á±ª-Áâ©‰Ωì‰∫§‰∫íÁöÑÊ°ÜÊû∂„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë¥°ÁåÆÂåÖÊã¨ÔºöÊ∑∑ÂêàËøêÂä®Ë°®Á§∫„ÄÅÂàõÊñ∞ÁöÑÊó†Êü•ÊâæÈáèÂåñÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºàLFQ-VAEÔºâ‰ª•ÂèäÂ¢ûÂº∫ÁöÑLingoÊï∞ÊçÆÈõÜ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â§çÊùÇÂÆ§ÂÜÖÂú∫ÊôØ‰∏ãÁöÑ‰∫∫Á±ªÂä®‰ΩúÁîüÊàêÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Âä®‰ΩúÊ†áËÆ∞ÂåñËøáÁ®ã‰∏≠Â≠òÂú®‰ø°ÊÅØ‰∏¢Â§±ÔºåÊó†Ê≥ïÊúâÊïàÊçïÊçâ3D‰∫∫Á±ªËøêÂä®ÁöÑËøûÁª≠ÊÄßÂíå‰∏ä‰∏ãÊñá‰æùËµñÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊàë‰ª¨ÊèêÂá∫ÁöÑUniHMÊ°ÜÊû∂ÈÄöËøáÂºïÂÖ•Êâ©Êï£ÁîüÊàêÊñπÊ≥ïÔºåÁªìÂêàÊ∑∑ÂêàËøêÂä®Ë°®Á§∫ÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞Êï¥ÂêàÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÁúüÂÆûÁöÑÂä®‰ΩúÂêàÊàê„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöUniHMÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÊ∑∑ÂêàËøêÂä®Ë°®Á§∫Ê®°Âùó„ÄÅLFQ-VAEÊ®°ÂùóÂíåÊï∞ÊçÆÈõÜÂ¢ûÂº∫Ê®°Âùó„ÄÇÊ∑∑ÂêàËøêÂä®Ë°®Á§∫Ê®°ÂùóÂ∞ÜËøûÁª≠ÁöÑ6DoFËøêÂä®‰∏éÁ¶ªÊï£ÁöÑÂ±ÄÈÉ®ËøêÂä®Ê†áËÆ∞ËûçÂêàÔºåLFQ-VAEÊ®°ÂùóÁî®‰∫éÊèêÈ´òÈáçÂª∫Á≤æÂ∫¶ÂíåÁîüÊàêÊÄßËÉΩÔºåÊï∞ÊçÆÈõÜÂ¢ûÂº∫Ê®°ÂùóÂàôÈÄöËøáHumanML3DÊ≥®ÈáäÂ¢ûÂº∫LingoÊï∞ÊçÆÈõÜ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éLFQ-VAEÁöÑÊèêÂá∫ÔºåÂÆÉË∂ÖË∂ä‰∫Ü‰º†ÁªüÁöÑVQ-VAEsÔºåÂú®ÈáçÂª∫Á≤æÂ∫¶ÂíåÁîüÊàêÊÄßËÉΩ‰∏äÂùáË°®Áé∞Âá∫Ëâ≤ÔºåÊòæËëóÊèêÂçá‰∫ÜÂú∫ÊôØÁâπÂÆöÁöÑÂä®‰ΩúÂ≠¶‰π†ËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÊàë‰ª¨ÈááÁî®‰∫ÜÊ∑∑ÂêàËøêÂä®Ë°®Á§∫‰ª•ÊèêÈ´òÂä®‰ΩúÁöÑÁúüÂÆûÊÑüÔºåÂπ∂ÈÄöËøáÊó†Êü•ÊâæÈáèÂåñÊñπÊ≥ï‰ºòÂåñ‰∫ÜÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÁöÑÊÄßËÉΩÔºåÁ°Æ‰øù‰∫ÜÁîüÊàêËøáÁ®ãÁöÑÈ´òÊïàÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇÊàë‰ª¨ËøòÂ¢ûÂº∫‰∫ÜÊï∞ÊçÆÈõÜÔºå‰ª•Êèê‰æõÊõ¥Âº∫ÁöÑÁõëÁù£‰ø°Âè∑„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåUniHMÂú®OMOMOÂü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫Ü‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÂΩìÁöÑÊÄßËÉΩÔºåÂπ∂Âú®HumanML3D‰∏äËé∑Âæó‰∫ÜÁ´û‰∫âÊÄßÁªìÊûúÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®ÊñáÊú¨Âà∞‰∫∫Á±ª-Áâ©‰Ωì‰∫§‰∫íÂêàÊàêÂíå‰∏ÄËà¨ÊñáÊú¨Êù°‰ª∂Âä®‰ΩúÁîüÊàêÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåUniHMÂú®ÁîüÊàêË¥®ÈáèÂíåÂáÜÁ°ÆÊÄß‰∏äÂùáÊúâÊòæËëóÊèêÂçá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ËôöÊãüÁé∞ÂÆû„ÄÅÊ∏∏ÊàèÂºÄÂèëÂíå‰∫∫Êú∫‰∫§‰∫íÁ≠âÂú∫ÊôØÔºåËÉΩÂ§ü‰∏∫Ëøô‰∫õÈ¢ÜÂüüÊèê‰æõÊõ¥Ëá™ÁÑ∂ÂíåÁúüÂÆûÁöÑ‰∫∫Á±ªÂä®‰ΩúÂêàÊàêÔºåÊèêÂçáÁî®Êà∑‰ΩìÈ™å„ÄÇÊú™Êù•ÔºåUniHMÊúâÊúõÊé®Âä®Êô∫ËÉΩÊú∫Âô®‰∫∫ÂíåËá™Âä®ÂåñÁ≥ªÁªüÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÂ∫îÁî®ÔºåÂ¢ûÂº∫ÂÖ∂‰∏é‰∫∫Á±ªÂèäÁâ©‰ΩìÁöÑ‰∫§‰∫íËÉΩÂäõ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Human motion synthesis in complex scenes presents a fundamental challenge, extending beyond conventional Text-to-Motion tasks by requiring the integration of diverse modalities such as static environments, movable objects, natural language prompts, and spatial waypoints. Existing language-conditioned motion models often struggle with scene-aware motion generation due to limitations in motion tokenization, which leads to information loss and fails to capture the continuous, context-dependent nature of 3D human movement. To address these issues, we propose UniHM, a unified motion language model that leverages diffusion-based generation for synthesizing scene-aware human motion. UniHM is the first framework to support both Text-to-Motion and Text-to-Human-Object Interaction (HOI) in complex 3D scenes. Our approach introduces three key contributions: (1) a mixed-motion representation that fuses continuous 6DoF motion with discrete local motion tokens to improve motion realism; (2) a novel Look-Up-Free Quantization VAE (LFQ-VAE) that surpasses traditional VQ-VAEs in both reconstruction accuracy and generative performance; and (3) an enriched version of the Lingo dataset augmented with HumanML3D annotations, providing stronger supervision for scene-specific motion learning. Experimental results demonstrate that UniHM achieves comparative performance on the OMOMO benchmark for text-to-HOI synthesis and yields competitive results on HumanML3D for general text-conditioned motion generation.

