---
layout: default
title: GSsplat: Generalizable Semantic Gaussian Splatting for Novel-view Synthesis in 3D Scenes
---

# GSsplat: Generalizable Semantic Gaussian Splatting for Novel-view Synthesis in 3D Scenes

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.04659" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.04659v1</a>
  <a href="https://arxiv.org/pdf/2505.04659.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.04659v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.04659v1', 'GSsplat: Generalizable Semantic Gaussian Splatting for Novel-view Synthesis in 3D Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Feng Xiao, Hongbin Xu, Wanlin Liang, Wenxiong Kang

**åˆ†ç±»**: cs.GR

**å‘å¸ƒæ—¥æœŸ**: 2025-05-07

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGSsplatä»¥è§£å†³3Dåœºæ™¯ä¸­æ–°è§†è§’åˆæˆçš„æ•ˆç‡ä¸æ€§èƒ½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `3Dåœºæ™¯ç†è§£` `æ–°è§†è§’åˆæˆ` `é«˜æ–¯ç‚¹äº‘` `ç¥ç»è¾å°„åœº` `è¯­ä¹‰ä¿¡æ¯æå–` `å¤šä»»åŠ¡å­¦ä¹ ` `è™šæ‹Ÿç°å®` `è‡ªåŠ¨é©¾é©¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ–°è§†è§’åˆæˆä¸­å­˜åœ¨é€Ÿåº¦æ…¢å’Œåˆ†å‰²æ€§èƒ½å·®çš„æŒ‘æˆ˜ï¼Œé™åˆ¶äº†3Dåœºæ™¯ç†è§£çš„åº”ç”¨ã€‚
2. GSsplaté€šè¿‡ä¸€æ¬¡è¾“å…¥é¢„æµ‹åœºæ™¯è‡ªé€‚åº”é«˜æ–¯åˆ†å¸ƒçš„å±æ€§ï¼Œç®€åŒ–äº†ä¼ ç»Ÿæ–¹æ³•ä¸­çš„ç¨ å¯†åŒ–å’Œä¿®å‰ªè¿‡ç¨‹ã€‚
3. åœ¨å¤šè§†è§’è¾“å…¥çš„è¯„ä¼°ä¸­ï¼ŒGSsplatå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸”é€Ÿåº¦æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨3Dåœºæ™¯ç†è§£ç ”ç©¶ä¸­ï¼Œä»å¤šä¸ªè§†è§’åˆæˆæœªè§åœºæ™¯çš„è¯­ä¹‰ä¿¡æ¯è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡é‡å»ºé€šç”¨ç¥ç»è¾å°„åœºæ¥æ¸²æŸ“æ–°è§†è§’å›¾åƒå’Œè¯­ä¹‰å›¾ï¼Œä½†åœ¨é€Ÿåº¦å’Œåˆ†å‰²æ€§èƒ½ä¸Šå­˜åœ¨å±€é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨çš„è¯­ä¹‰é«˜æ–¯ç‚¹äº‘æ–¹æ³•ï¼ˆGSsplatï¼‰ï¼Œæ—¨åœ¨é«˜æ•ˆåœ°è¿›è¡Œæ–°è§†è§’åˆæˆã€‚è¯¥æ¨¡å‹ä»ä¸€æ¬¡è¾“å…¥ä¸­é¢„æµ‹åœºæ™¯è‡ªé€‚åº”é«˜æ–¯åˆ†å¸ƒçš„ä½ç½®å’Œå±æ€§ï¼Œå–ä»£äº†ä¼ ç»Ÿåœºæ™¯ç‰¹å®šé«˜æ–¯ç‚¹äº‘çš„ç¨ å¯†åŒ–å’Œä¿®å‰ªè¿‡ç¨‹ã€‚é€šè¿‡æ··åˆç½‘ç»œæå–é¢œè‰²å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶é¢„æµ‹é«˜æ–¯å‚æ•°ã€‚ä¸ºå¢å¼ºé«˜æ–¯çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„åç§»å­¦ä¹ æ¨¡å—å’Œç‚¹çº§äº¤äº’æ¨¡å—ã€‚GSsplatåœ¨å¤šè§†è§’è¾“å…¥ä¸‹çš„è¯„ä¼°ä¸­ï¼Œè¾¾åˆ°äº†æœ€å¿«çš„é€Ÿåº¦å’Œæœ€å…ˆè¿›çš„è¯­ä¹‰åˆæˆæ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ–°è§†è§’åˆæˆæ–¹æ³•åœ¨é€Ÿåº¦å’Œåˆ†å‰²æ€§èƒ½ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚3Dåœºæ™¯æ—¶çš„æ•ˆç‡é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGSsplaté€šè¿‡ä¸€æ¬¡æ€§è¾“å…¥é¢„æµ‹åœºæ™¯è‡ªé€‚åº”çš„é«˜æ–¯åˆ†å¸ƒï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­çš„ç¹çç¨ å¯†åŒ–å’Œä¿®å‰ªæ­¥éª¤ï¼Œä»è€Œæé«˜äº†åˆæˆæ•ˆç‡å’Œè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•é‡‡ç”¨å¤šä»»åŠ¡æ¡†æ¶ï¼Œè®¾è®¡äº†ä¸€ä¸ªæ··åˆç½‘ç»œæ¥æå–é¢œè‰²å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶é¢„æµ‹é«˜æ–¯å‚æ•°ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬è¾“å…¥å¤„ç†ã€ç‰¹å¾æå–å’Œé«˜æ–¯å‚æ•°é¢„æµ‹ç­‰æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šGSsplatçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†åç§»å­¦ä¹ æ¨¡å—å’Œç‚¹çº§äº¤äº’æ¨¡å—ï¼Œå¢å¼ºäº†é«˜æ–¯çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å¤„ç†æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼Œé‡‡ç”¨äº†æ··åˆç½‘ç»œæ¶æ„ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†è¯­ä¹‰å’Œé¢œè‰²ä¿¡æ¯çš„èåˆï¼Œç¡®ä¿é«˜æ–¯å‚æ•°çš„å‡†ç¡®é¢„æµ‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

GSsplatåœ¨å¤šè§†è§’è¾“å…¥ä¸‹çš„è¯„ä¼°ä¸­ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¯­ä¹‰åˆæˆæ€§èƒ½ï¼Œé€Ÿåº¦æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®è¡¨æ˜å…¶åœ¨å¤„ç†å¤æ‚åœºæ™¯æ—¶çš„æ•ˆç‡æé«˜äº†XX%ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•è¡¨ç°å‡ºæ˜æ˜¾ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡é«˜æ•ˆçš„3Dåœºæ™¯åˆæˆï¼Œèƒ½å¤Ÿæå‡ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The semantic synthesis of unseen scenes from multiple viewpoints is crucial for research in 3D scene understanding. Current methods are capable of rendering novel-view images and semantic maps by reconstructing generalizable Neural Radiance Fields. However, they often suffer from limitations in speed and segmentation performance. We propose a generalizable semantic Gaussian Splatting method (GSsplat) for efficient novel-view synthesis. Our model predicts the positions and attributes of scene-adaptive Gaussian distributions from once input, replacing the densification and pruning processes of traditional scene-specific Gaussian Splatting. In the multi-task framework, a hybrid network is designed to extract color and semantic information and predict Gaussian parameters. To augment the spatial perception of Gaussians for high-quality rendering, we put forward a novel offset learning module through group-based supervision and a point-level interaction module with spatial unit aggregation. When evaluated with varying numbers of multi-view inputs, GSsplat achieves state-of-the-art performance for semantic synthesis at the fastest speed.

