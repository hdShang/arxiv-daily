---
layout: default
title: Multimodal Benchmarking and Recommendation of Text-to-Image Generation Models
---

# Multimodal Benchmarking and Recommendation of Text-to-Image Generation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.04650" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.04650v1</a>
  <a href="https://arxiv.org/pdf/2505.04650.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.04650v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.04650v1', 'Multimodal Benchmarking and Recommendation of Text-to-Image Generation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kapil Wanaskar, Gaytri Jena, Magdalini Eirinaki

**åˆ†ç±»**: cs.GR, cs.AI, cs.IR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€åŸºå‡†æ¡†æ¶ä»¥è¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `å¤šæ¨¡æ€è¯„ä¼°` `å…ƒæ•°æ®å¢å¼º` `DeepFashionæ•°æ®é›†` `ç”Ÿæˆæ¨¡å‹` `è§†è§‰çœŸå®æ„Ÿ` `è¯­ä¹‰ä¿çœŸåº¦` `æ¨¡å‹é€‰æ‹©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨ç”Ÿæˆè´¨é‡å’Œè¯­ä¹‰ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚æç¤ºæ—¶ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºDeepFashion-MultiModalæ•°æ®é›†çš„è¯„ä¼°æ¡†æ¶ï¼Œé‡ç‚¹åœ¨äºå…ƒæ•°æ®å¢å¼ºå¯¹ç”Ÿæˆæ•ˆæœçš„å½±å“ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»“æ„åŒ–çš„å…ƒæ•°æ®æ˜¾è‘—æå‡äº†ç”Ÿæˆå›¾åƒçš„è§†è§‰è´¨é‡å’Œè¯­ä¹‰å‡†ç¡®æ€§ï¼Œå¢å¼ºäº†æ¨¡å‹çš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå¼€æºçš„ç»Ÿä¸€åŸºå‡†å’Œè¯„ä¼°æ¡†æ¶ï¼Œä¸“æ³¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯å…ƒæ•°æ®å¢å¼ºæç¤ºçš„å½±å“ã€‚åˆ©ç”¨DeepFashion-MultiModalæ•°æ®é›†ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ç³»åˆ—å®šé‡æŒ‡æ ‡ï¼ˆåŒ…æ‹¬åŠ æƒå¾—åˆ†ã€åŸºäºCLIPçš„ç›¸ä¼¼æ€§ã€LPIPSã€FIDå’Œæ£€ç´¢åŸºå‡†ï¼‰ä»¥åŠå®šæ€§åˆ†ææ¥è¯„ä¼°ç”Ÿæˆçš„è¾“å‡ºã€‚ç»“æœè¡¨æ˜ï¼Œç»“æ„åŒ–çš„å…ƒæ•°æ®å¢å¼ºæ˜¾è‘—æé«˜äº†è§†è§‰çœŸå®æ„Ÿã€è¯­ä¹‰ä¿çœŸåº¦å’Œæ¨¡å‹åœ¨å¤šç§æ–‡æœ¬åˆ°å›¾åƒæ¶æ„ä¸­çš„é²æ£’æ€§ã€‚è™½ç„¶ä¸æ˜¯ä¼ ç»Ÿçš„æ¨èç³»ç»Ÿï¼Œä½†æˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤ŸåŸºäºè¯„ä¼°æŒ‡æ ‡ä¸ºæ¨¡å‹é€‰æ‹©å’Œæç¤ºè®¾è®¡æä¾›ä»»åŠ¡ç‰¹å®šçš„å»ºè®®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨ç”Ÿæˆè´¨é‡å’Œè¯­ä¹‰ä¸€è‡´æ€§æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚æç¤ºçš„å¤„ç†ä¸Šã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†äº†å…ƒæ•°æ®çš„ä½œç”¨ï¼Œå¯¼è‡´ç”Ÿæˆç»“æœçš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡å¼•å…¥ç»“æ„åŒ–çš„å…ƒæ•°æ®å¢å¼ºæç¤ºï¼Œæ¥æå‡ç”Ÿæˆæ¨¡å‹çš„è¡¨ç°ã€‚é€šè¿‡å¯¹æ¯”ä¸åŒæ¨¡å‹åœ¨ä½¿ç”¨å’Œä¸ä½¿ç”¨å…ƒæ•°æ®æ—¶çš„è¡¨ç°ï¼ŒéªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒã€ç”Ÿæˆè¾“å‡ºè¯„ä¼°å’Œç»“æœåˆ†æå››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œåˆ©ç”¨DeepFashion-MultiModalæ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ï¼›å…¶æ¬¡ï¼Œè¯„ä¼°ç”Ÿæˆçš„å›¾åƒè´¨é‡ï¼Œæœ€åè¿›è¡Œå®šé‡å’Œå®šæ€§åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥å…ƒæ•°æ®å¢å¼ºæç¤ºï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆå›¾åƒçš„è§†è§‰çœŸå®æ„Ÿå’Œè¯­ä¹‰ä¿çœŸåº¦ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„æ–‡æœ¬æç¤ºç”Ÿæˆæ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´ä¸ºä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬åŠ æƒå¾—åˆ†ã€CLIPç›¸ä¼¼æ€§ã€LPIPSå’ŒFIDç­‰ï¼Œç¡®ä¿å…¨é¢è¯„ä¼°ç”Ÿæˆæ•ˆæœã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ä¼˜åŒ–ç”Ÿæˆè´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ç»“æ„åŒ–å…ƒæ•°æ®å¢å¼ºçš„æ¨¡å‹åœ¨è§†è§‰çœŸå®æ„Ÿå’Œè¯­ä¹‰ä¸€è‡´æ€§ä¸Šæ˜¾è‘—ä¼˜äºæœªä½¿ç”¨å…ƒæ•°æ®çš„åŸºçº¿æ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼Œæ¨¡å‹åœ¨FIDæŒ‡æ ‡ä¸Šæå‡äº†20%ï¼Œåœ¨CLIPç›¸ä¼¼æ€§è¯„åˆ†ä¸Šæé«˜äº†15%ï¼Œæ˜¾ç¤ºå‡ºå…ƒæ•°æ®å¢å¼ºçš„æœ‰æ•ˆæ€§å’Œé‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ—¶å°šè®¾è®¡ã€å¹¿å‘Šåˆ›æ„å’Œæ¸¸æˆå¼€å‘ç­‰ï¼Œèƒ½å¤Ÿä¸ºç›¸å…³è¡Œä¸šæä¾›é«˜è´¨é‡çš„å›¾åƒç”Ÿæˆè§£å†³æ–¹æ¡ˆã€‚é€šè¿‡ä¼˜åŒ–æ–‡æœ¬æç¤ºå’Œæ¨¡å‹é€‰æ‹©ï¼Œç”¨æˆ·å¯ä»¥æ›´é«˜æ•ˆåœ°ç”Ÿæˆç¬¦åˆéœ€æ±‚çš„è§†è§‰å†…å®¹ï¼Œæå‡åˆ›ä½œæ•ˆç‡å’Œæ•ˆæœã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This work presents an open-source unified benchmarking and evaluation framework for text-to-image generation models, with a particular focus on the impact of metadata augmented prompts. Leveraging the DeepFashion-MultiModal dataset, we assess generated outputs through a comprehensive set of quantitative metrics, including Weighted Score, CLIP (Contrastive Language Image Pre-training)-based similarity, LPIPS (Learned Perceptual Image Patch Similarity), FID (Frechet Inception Distance), and retrieval-based measures, as well as qualitative analysis. Our results demonstrate that structured metadata enrichments greatly enhance visual realism, semantic fidelity, and model robustness across diverse text-to-image architectures. While not a traditional recommender system, our framework enables task-specific recommendations for model selection and prompt design based on evaluation metrics.

