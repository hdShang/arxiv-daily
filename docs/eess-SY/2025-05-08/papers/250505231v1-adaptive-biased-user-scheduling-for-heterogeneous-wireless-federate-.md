---
layout: default
title: Adaptive Biased User Scheduling for Heterogeneous Wireless Federate Learning Network
---

# Adaptive Biased User Scheduling for Heterogeneous Wireless Federate Learning Network

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.05231" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.05231v1</a>
  <a href="https://arxiv.org/pdf/2505.05231.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.05231v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.05231v1', 'Adaptive Biased User Scheduling for Heterogeneous Wireless Federate Learning Network')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Changxiang Wu, Yijing Ren, Daniel K. C. So, Jie Tang

**åˆ†ç±»**: eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-05-08

**å¤‡æ³¨**: 13 pages, 9 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”åç½®ç”¨æˆ·è°ƒåº¦ä»¥è§£å†³å¼‚æ„æ— çº¿è”é‚¦å­¦ä¹ ç½‘ç»œä¸­çš„æ”¶æ•›é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `ç”¨æˆ·è°ƒåº¦` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `èµ„æºåˆ†é…` `å¼‚æ„ç½‘ç»œ` `æ”¶æ•›é€Ÿåº¦` `æ‹‰æ ¼æœ—æ—¥åˆ†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¼‚æ„æ— çº¿ç½‘ç»œä¸­è°ƒåº¦ç”¨æˆ·æ—¶ï¼Œéš¾ä»¥å¹³è¡¡å•è½®æ—¶é•¿ä¸ç´¯è®¡è½®æ•°ï¼Œå¯¼è‡´æ”¶æ•›æ•ˆç‡ä½ä¸‹ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡æ·±åº¦å¼ºåŒ–å­¦ä¹ å’Œæ‹‰æ ¼æœ—æ—¥åˆ†è§£ç›¸ç»“åˆçš„æ–¹å¼ï¼Œä¼˜åŒ–ç”¨æˆ·è°ƒåº¦å’Œèµ„æºåˆ†é…ï¼Œä»¥åŠ é€ŸFLçš„æ”¶æ•›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ¡†æ¶åœ¨å¤šç§FLä»»åŠ¡ä¸­æ˜¾è‘—å‡å°‘äº†ä»»åŠ¡æ—¶é—´ï¼Œç›¸è¾ƒäºç°æœ‰åŸºå‡†æœ‰æ˜æ˜¾æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰åœ¨åˆ†å¸ƒå¼ç½‘ç»œä¸­çš„åä½œæ¨¡å‹è®­ç»ƒä¸­å¼•é¢†äº†æ•°æ®éšç§å’Œé€šä¿¡æ•ˆç‡çš„æ–°æ½®æµã€‚æœ¬æ–‡ç ”ç©¶äº†åœ¨æ— çº¿å¼‚æ„ç½‘ç»œä¸­é«˜æ•ˆéƒ¨ç½²FLçš„ç­–ç•¥ï¼Œé‡ç‚¹å…³æ³¨å¦‚ä½•åŠ é€Ÿæ”¶æ•›ï¼Œå°¤å…¶æ˜¯åœ¨å­˜åœ¨å»¶è¿Ÿç”¨æˆ·çš„æƒ…å†µä¸‹ã€‚é€šè¿‡ä¼˜åŒ–ç”¨æˆ·è°ƒåº¦å’Œèµ„æºåˆ†é…ï¼Œæ—¨åœ¨æœ€å°åŒ–é•¿æœŸæ”¶æ•›çš„æ—¶é’Ÿæ—¶é—´ã€‚å°½ç®¡å»¶è¿Ÿç”¨æˆ·å¯èƒ½åœ¨å•è½®ä¸­å¼•å…¥å»¶è¿Ÿï¼Œä½†å…¶åŒ…å«å¯ä»¥åŠ é€Ÿåç»­è½®æ¬¡ï¼Œç‰¹åˆ«æ˜¯å½“å®ƒä»¬æ‹¥æœ‰å…³é”®æ•°æ®æ—¶ã€‚æœ¬æ–‡é‡‡ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å®æ—¶ç³»ç»Ÿå’Œç»Ÿè®¡ä¿¡æ¯æ¥é€‚åº”æ€§é€‰æ‹©ç”¨æˆ·é›†ï¼Œå¹¶é€šè¿‡æ‹‰æ ¼æœ—æ—¥åˆ†è§£ä¼˜åŒ–å±€éƒ¨èµ„æºåˆ©ç”¨ï¼Œæå‡ç³»ç»Ÿæ•ˆç‡ã€‚ä»¿çœŸç»“æœéªŒè¯äº†æ‰€ææ¡†æ¶åœ¨å¤šç§FLä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼‚æ„æ— çº¿ç½‘ç»œä¸­è”é‚¦å­¦ä¹ çš„ç”¨æˆ·è°ƒåº¦å’Œèµ„æºåˆ†é…é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹å»¶è¿Ÿç”¨æˆ·æ—¶ï¼Œéš¾ä»¥æœ‰æ•ˆåˆ©ç”¨å…¶ä¿¡æ¯ï¼Œå¯¼è‡´æ”¶æ•›æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥è‡ªé€‚åº”åç½®è°ƒåº¦ç­–ç•¥ï¼Œç»“åˆæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ŒåŠ¨æ€é€‰æ‹©ç”¨æˆ·é›†ï¼Œä»¥ä¼˜åŒ–æ”¶æ•›é€Ÿåº¦å’Œèµ„æºåˆ©ç”¨ã€‚è¿™æ ·çš„è®¾è®¡èƒ½å¤Ÿåœ¨ä¸åŒçš„ç½‘ç»œæ¡ä»¶ä¸‹çµæ´»è°ƒæ•´ï¼Œæé«˜æ•´ä½“ç³»ç»Ÿæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ç”¨æˆ·è°ƒåº¦æ¨¡å—ã€èµ„æºåˆ†é…æ¨¡å—å’Œæ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å—ã€‚ç”¨æˆ·è°ƒåº¦æ¨¡å—è´Ÿè´£å®æ—¶é€‰æ‹©å‚ä¸è®­ç»ƒçš„ç”¨æˆ·ï¼Œèµ„æºåˆ†é…æ¨¡å—ä¼˜åŒ–æ¯ä¸ªç”¨æˆ·çš„è®¡ç®—èµ„æºï¼Œè€Œæ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å—åˆ™æ ¹æ®å®æ—¶åé¦ˆè°ƒæ•´ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸åç½®è°ƒåº¦ç›¸ç»“åˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å»¶è¿Ÿç”¨æˆ·çš„å½±å“ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†æ”¶æ•›é€Ÿåº¦å’Œç³»ç»Ÿæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†åŸºäºç”¨æˆ·èƒ½é‡çº¦æŸçš„è°ƒåº¦ç­–ç•¥ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†æ”¶æ•›æ—¶é—´å’Œèµ„æºåˆ©ç”¨çš„å¹³è¡¡ï¼Œç½‘ç»œç»“æ„åˆ™åŸºäºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ç®—æ³•è¿›è¡Œè®¾è®¡ï¼Œä»¥å®ç°é«˜æ•ˆçš„ç­–ç•¥æ›´æ–°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ¡†æ¶åœ¨å¤šç§FLä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºç°æœ‰åŸºå‡†ï¼Œä»»åŠ¡æ—¶é—´å‡å°‘äº†çº¦20%-30%ã€‚åœ¨ä¸åŒçš„ç½‘ç»œæ¡ä»¶ä¸‹ï¼Œç³»ç»Ÿçš„æ”¶æ•›é€Ÿåº¦å’Œèµ„æºåˆ©ç”¨æ•ˆç‡å‡æœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½äº¤é€šç³»ç»Ÿã€ç‰©è”ç½‘è®¾å¤‡çš„ååŒå­¦ä¹ ä»¥åŠç§»åŠ¨è¾¹ç¼˜è®¡ç®—ç­‰åœºæ™¯ã€‚é€šè¿‡æé«˜è”é‚¦å­¦ä¹ çš„æ•ˆç‡ï¼Œå¯ä»¥åœ¨ä¿æŠ¤ç”¨æˆ·éšç§çš„åŒæ—¶ï¼Œå®ç°æ›´å¿«é€Ÿçš„æ¨¡å‹è®­ç»ƒï¼Œæ¨åŠ¨æ™ºèƒ½åº”ç”¨çš„å‘å±•ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„å¼‚æ„ç½‘ç»œç¯å¢ƒä¸­å¾—åˆ°åº”ç”¨ï¼Œæå‡æ•´ä½“ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Federated Learning (FL) has revolutionized collaborative model training in distributed networks, prioritizing data privacy and communication efficiency. This paper investigates efficient deployment of FL in wireless heterogeneous networks, focusing on strategies to accelerate convergence despite stragglers. The primary objective is to minimize long-term convergence wall-clock time through optimized user scheduling and resource allocation. While stragglers may introduce delays in a single round, their inclusion can expedite subsequent rounds, particularly when they possess critical information. Moreover, balancing single-round duration with the number of cumulative rounds, compounded by dynamic training and transmission conditions, necessitates a novel approach beyond conventional optimization solutions. To tackle these challenges, convergence analysis with respect to adaptive and biased scheduling is derived. Then, by factoring in real-time system and statistical information, including diverse energy constraints and users' energy harvesting capabilities, a deep reinforcement learning approach, empowered by proximal policy optimization, is employed to adaptively select user sets. For the scheduled users, Lagrangian decomposition is applied to optimize local resource utilization, further enhancing system efficiency. Simulation results validate the effectiveness and robustness of the proposed framework for various FL tasks, demonstrating reduced task time compared to existing benchmarks under various settings.

