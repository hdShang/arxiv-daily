---
layout: default
title: Multi-Objective Reinforcement Learning for Energy-Efficient Industrial Control
---

# Multi-Objective Reinforcement Learning for Energy-Efficient Industrial Control

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07607" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07607v1</a>
  <a href="https://arxiv.org/pdf/2505.07607.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07607v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07607v1', 'Multi-Objective Reinforcement Learning for Energy-Efficient Industrial Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Georg SchÃ¤fer, Raphael Seliger, Jakob Rehrl, Stefan Huber, Simon Hirlaender

**åˆ†ç±»**: eess.SY, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12

**å¤‡æ³¨**: Accepted at DEXA 2025 (AI4IP)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä»¥å®ç°å·¥ä¸šæ§åˆ¶çš„èƒ½æºæ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ ` `èƒ½æºæ•ˆç‡` `å·¥ä¸šæ§åˆ¶` `å¤åˆå¥–åŠ±å‡½æ•°` `è‡ªé€‚åº”ä¼˜åŒ–` `æ™ºèƒ½åˆ¶é€ ` `ç¯å¢ƒå½±å“`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å·¥ä¸šæ§åˆ¶æ–¹æ³•åœ¨èƒ½æºæ•ˆç‡ä¸æ€§èƒ½ä¹‹é—´çš„å¹³è¡¡é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æˆæœ¬å’Œç¯å¢ƒçº¦æŸæ—¥ç›Šä¸¥æ ¼çš„èƒŒæ™¯ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡è®¾è®¡å¤åˆå¥–åŠ±å‡½æ•°æ¥åŒæ—¶ä¼˜åŒ–è·Ÿè¸ªè¯¯å·®å’Œç”µåŠ›æ¶ˆè€—ï¼Œæå‡èƒ½æºæ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè°ƒæ•´èƒ½é‡æƒ©ç½šæƒé‡Î±å¯¹æ§åˆ¶æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼Œå°¤å…¶åœ¨Î±å€¼ä¸º0.0åˆ°0.25ä¹‹é—´ï¼Œè¡¨ç°å‡ºæ˜æ˜¾çš„æ€§èƒ½è½¬å˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å·¥ä¸šè‡ªåŠ¨åŒ–å¯¹èƒ½æºé«˜æ•ˆæ§åˆ¶ç­–ç•¥çš„éœ€æ±‚æ—¥ç›Šå¢åŠ ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ ï¼ˆMORLï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨å¹³è¡¡æ€§èƒ½ã€ç¯å¢ƒå’Œæˆæœ¬çº¦æŸã€‚ç ”ç©¶èšç„¦äºQuanser Aero 2æµ‹è¯•å¹³å°çš„ä¸€è‡ªç”±åº¦é…ç½®ï¼Œè®¾è®¡äº†ä¸€ä¸ªå¤åˆå¥–åŠ±å‡½æ•°ï¼ŒåŒæ—¶æƒ©ç½šè·Ÿè¸ªè¯¯å·®å’Œç”µåŠ›æ¶ˆè€—ã€‚åˆæ­¥å®éªŒæ¢è®¨äº†èƒ½é‡æƒ©ç½šæƒé‡Î±å¯¹ä¿¯ä»°è·Ÿè¸ªä¸èŠ‚èƒ½ä¹‹é—´æƒè¡¡çš„å½±å“ï¼Œç»“æœæ˜¾ç¤ºåœ¨Î±å€¼ä¸º0.0åˆ°0.25ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½è½¬å˜ï¼Œä¸”åœ¨è¾ƒä½çš„Î±å€¼ä¸‹å‡ºç°éå¸•ç´¯æ‰˜æœ€ä¼˜è§£ã€‚æˆ‘ä»¬æ¨æµ‹è¿™äº›ç°è±¡å¯èƒ½ä¸Adamä¼˜åŒ–å™¨çš„è‡ªé€‚åº”è¡Œä¸ºå¼•å…¥çš„ä¼ªå½±æœ‰å…³ï¼Œæœªæ¥å·¥ä½œå°†é›†ä¸­äºé€šè¿‡åŸºäºé«˜æ–¯è¿‡ç¨‹çš„å¸•ç´¯æ‰˜å‰æ²¿å»ºæ¨¡è‡ªåŠ¨é€‰æ‹©Î±ï¼Œå¹¶å°†è¯¥æ–¹æ³•ä»ä»¿çœŸè½¬å‘å®é™…éƒ¨ç½²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å·¥ä¸šæ§åˆ¶ä¸­èƒ½æºæ•ˆç‡ä¸æ€§èƒ½ä¹‹é—´çš„çŸ›ç›¾ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æœªèƒ½æœ‰æ•ˆå…¼é¡¾è¿™ä¸¤è€…ï¼Œå¯¼è‡´èµ„æºæµªè´¹å’Œæˆæœ¬å¢åŠ ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè®¾è®¡å¤åˆå¥–åŠ±å‡½æ•°ï¼Œæ—¢è€ƒè™‘è·Ÿè¸ªç²¾åº¦åˆå…³æ³¨ç”µåŠ›æ¶ˆè€—ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„æ§åˆ¶ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ç¯å¢ƒå»ºæ¨¡ã€å¥–åŠ±å‡½æ•°è®¾è®¡ã€å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚MORLï¼‰å’Œå®éªŒéªŒè¯ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬çŠ¶æ€ç©ºé—´å®šä¹‰ã€åŠ¨ä½œé€‰æ‹©ç­–ç•¥å’Œå¥–åŠ±åé¦ˆæœºåˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¤åˆå¥–åŠ±å‡½æ•°çš„è®¾è®¡ï¼Œä½¿å¾—æ§åˆ¶ç­–ç•¥èƒ½å¤Ÿåœ¨è·Ÿè¸ªç²¾åº¦å’Œèƒ½è€—ä¹‹é—´è¿›è¡Œæœ‰æ•ˆæƒè¡¡ï¼Œä¸ä¼ ç»Ÿå•ç›®æ ‡ä¼˜åŒ–æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œèƒ½é‡æƒ©ç½šæƒé‡Î±çš„è®¾ç½®å¯¹æ€§èƒ½å½±å“æ˜¾è‘—ï¼Œå°¤å…¶åœ¨0.0åˆ°0.25çš„èŒƒå›´å†…ï¼Œæ­¤å¤–ï¼Œä½¿ç”¨Adamä¼˜åŒ–å™¨æ—¶éœ€æ³¨æ„å…¶è‡ªé€‚åº”è¡Œä¸ºå¯èƒ½å¼•å…¥çš„åå·®ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“èƒ½é‡æƒ©ç½šæƒé‡Î±åœ¨0.0åˆ°0.25ä¹‹é—´å˜åŒ–æ—¶ï¼Œæ§åˆ¶æ€§èƒ½å‘ç”Ÿæ˜¾è‘—è½¬å˜ï¼Œå°¤å…¶åœ¨è¾ƒä½Î±å€¼ä¸‹å‡ºç°éå¸•ç´¯æ‰˜æœ€ä¼˜è§£ï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨èƒ½æºæ•ˆç‡ä¸æ€§èƒ½å¹³è¡¡æ–¹é¢å…·æœ‰é‡è¦æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å·¥ä¸šè‡ªåŠ¨åŒ–ã€æœºå™¨äººæ§åˆ¶å’Œæ™ºèƒ½åˆ¶é€ ç­‰ï¼Œèƒ½å¤Ÿä¸ºä¼ä¸šæä¾›æ›´ä¸ºé«˜æ•ˆçš„èƒ½æºç®¡ç†æ–¹æ¡ˆï¼Œé™ä½è¿è¥æˆæœ¬å¹¶å‡å°‘ç¯å¢ƒå½±å“ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶æœ‰æœ›æ¨å¹¿è‡³æ›´å¤æ‚çš„å·¥ä¸šåœºæ™¯ï¼Œå®ç°æ›´å¹¿æ³›çš„åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Industrial automation increasingly demands energy-efficient control strategies to balance performance with environmental and cost constraints. In this work, we present a multi-objective reinforcement learning (MORL) framework for energy-efficient control of the Quanser Aero 2 testbed in its one-degree-of-freedom configuration. We design a composite reward function that simultaneously penalizes tracking error and electrical power consumption. Preliminary experiments explore the influence of varying the Energy penalty weight, alpha, on the trade-off between pitch tracking and energy savings. Our results reveal a marked performance shift for alpha values between 0.0 and 0.25, with non-Pareto optimal solutions emerging at lower alpha values, on both the simulation and the real system. We hypothesize that these effects may be attributed to artifacts introduced by the adaptive behavior of the Adam optimizer, which could bias the learning process and favor bang-bang control strategies. Future work will focus on automating alpha selection through Gaussian Process-based Pareto front modeling and transitioning the approach from simulation to real-world deployment.

