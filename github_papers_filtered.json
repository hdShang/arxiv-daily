{
  "total_found": 945,
  "matched_count": 675,
  "filter_threshold": 2.0,
  "matched_papers": [
    {
      "title": "[2024.01] Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation [IL] [[project](https://mobile-aloha.github.io/)]",
      "arxiv_id": "2401.02117",
      "arxiv_url": "http://arxiv.org/abs/2401.02117",
      "summary": "Imitation learning from human demonstrations has shown impressive performance in robotics. However, most results focus on table-top manipulation, lacking the mobility and dexterity necessary for generally useful tasks. In this work, we develop a system for imitating mobile manipulation tasks that are bimanual and require whole-body control. We first present Mobile ALOHA, a low-cost and whole-body teleoperation system for data collection. It augments the ALOHA system with a mobile base, and a whole-body teleoperation interface. Using data collected with Mobile ALOHA, we then perform supervised behavior cloning and find that co-training with existing static ALOHA datasets boosts performance on mobile manipulation tasks. With 50 demonstrations for each task, co-training can increase success rates by up to 90%, allowing Mobile ALOHA to autonomously complete complex mobile manipulation tasks such as sauteing and serving a piece of shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling and entering an elevator, and lightly rinsing a used pan using a kitchen faucet. Project website: https://mobile-aloha.github.io",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "whole-body control",
            "[T]manipulation",
            "[T]mobile manipulation",
            "[T]bi-manual",
            "[T]teleoperation"
          ],
          "score": 26.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning",
            "behavior cloning"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]Aloha",
            "[T]Mobile Aloha"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 47.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025.03] FLAM: Foundation Model-Based Body Stabilization for Humanoid Locomotion and Manipulation",
      "arxiv_id": "2503.22249",
      "arxiv_url": "https://arxiv.org/pdf/2503.22249",
      "summary": "Humanoid robots have attracted significant attention in recent years. Reinforcement Learning (RL) is one of the main ways to control the whole body of humanoid robots. RL enables agents to complete tasks by learning from environment interactions, guided by task rewards. However, existing RL methods rarely explicitly consider the impact of body stability on humanoid locomotion and manipulation. Achieving high performance in whole-body control remains a challenge for RL methods that rely solely on task rewards. In this paper, we propose a Foundation model-based method for humanoid Locomotion And Manipulation (FLAM for short). FLAM integrates a stabilizing reward function with a basic policy. The stabilizing reward function is designed to encourage the robot to learn stable postures, thereby accelerating the learning process and facilitating task completion. Specifically, the robot pose is first mapped to the 3D virtual human model. Then, the human pose is stabilized and reconstructed through a human motion reconstruction model. Finally, the pose before and after reconstruction is used to compute the stabilizing reward. By combining this stabilizing reward with the task reward, FLAM effectively guides policy learning. Experimental results on a humanoid robot benchmark demonstrate that FLAM outperforms state-of-the-art RL methods, highlighting its effectiveness in improving stability and overall performance.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Manipulation, Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]humanoid locomotion",
            "whole-body control",
            "[T]locomotion",
            "[T]manipulation"
          ],
          "score": 28.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "policy learning"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 40.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction [[project](https://omniretarget.github.io/)]",
      "arxiv_id": "2509.26633",
      "arxiv_url": "https://arxiv.org/pdf/2509.26633",
      "summary": "A dominant paradigm for teaching humanoid robots complex skills is to retarget human motions as kinematic references to train reinforcement learning (RL) policies. However, existing retargeting pipelines often struggle with the significant embodiment gap between humans and robots, producing physically implausible artifacts like foot-skating and penetration. More importantly, common retargeting methods neglect the rich human-object and human-environment interactions essential for expressive locomotion and loco-manipulation. To address this, we introduce OmniRetarget, an interaction-preserving data generation engine based on an interaction mesh that explicitly models and preserves the crucial spatial and contact relationships between an agent, the terrain, and manipulated objects. By minimizing the Laplacian deformation between the human and robot meshes while enforcing kinematic constraints, OmniRetarget generates kinematically feasible trajectories. Moreover, preserving task-relevant interactions enables efficient data augmentation, from a single demonstration to different robot embodiments, terrains, and object configurations. We comprehensively evaluate OmniRetarget by retargeting motions from OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour trajectories that achieve better kinematic constraint satisfaction and contact preservation than widely used baselines. Such high-quality data enables proprioceptive RL policies to successfully execute long-horizon (up to 30 seconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained with only 5 reward terms and simple domain randomization shared by all tasks, without any learning curriculum.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "locomotion",
            "parkour",
            "[T]manipulation",
            "[T]loco-manipulation",
            "domain randomization",
            "Unitree"
          ],
          "score": 28.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "foot skating",
            "penetration"
          ],
          "score": 5.0
        },
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "OMOMO"
          ],
          "score": 2.5
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "interaction mesh"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 40.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "4_motion_diffusion",
        "5_interaction_reaction",
        "7_retargeting"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025.02] Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids [RL]",
      "arxiv_id": "2502.20396",
      "arxiv_url": "https://arxiv.org/abs/2502.20396",
      "summary": "Learning generalizable robot manipulation policies, especially for complex multi-fingered humanoids, remains a significant challenge. Existing approaches primarily rely on extensive data collection and imitation learning, which are expensive, labor-intensive, and difficult to scale. Sim-to-real reinforcement learning (RL) offers a promising alternative, but has mostly succeeded in simpler state-based or single-hand setups. How to effectively extend this to vision-based, contact-rich bimanual manipulation tasks remains an open question. In this paper, we introduce a practical sim-to-real RL recipe that trains a humanoid robot to perform three challenging dexterous manipulation tasks: grasp-and-reach, box lift and bimanual handover. Our method features an automated real-to-sim tuning module, a generalized reward formulation based on contact and object goals, a divide-and-conquer policy distillation framework, and a hybrid object representation strategy with modality-specific augmentation. We demonstrate high success rates on unseen objects and robust, adaptive policy behaviors -- highlighting that vision-based dexterous manipulation via sim-to-real RL is not only viable, but also scalable and broadly applicable to real-world humanoid manipulation tasks.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Manipulation, Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]manipulation",
            "[T]dexterous manipulation",
            "bi-manual",
            "bimanual manipulation",
            "[T]sim-to-real"
          ],
          "score": 30.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "imitation learning",
            "distillation"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 37.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.06] OmniH2O: Universal and Dexterous Human-to-Humanoid Whole-Body Teleoperation and Learning [benchmark] [[project](https://omni.human2humanoid.com/)]",
      "arxiv_id": "2406.08858",
      "arxiv_url": "https://arxiv.org/abs/2406.08858",
      "summary": "We present OmniH2O (Omni Human-to-Humanoid), a learning-based system for whole-body humanoid teleoperation and autonomy. Using kinematic pose as a universal control interface, OmniH2O enables various ways for a human to control a full-sized humanoid with dexterous hands, including using real-time teleoperation through VR headset, verbal instruction, and RGB camera. OmniH2O also enables full autonomy by learning from teleoperated demonstrations or integrating with frontier models such as GPT-4. OmniH2O demonstrates versatility and dexterity in various real-world whole-body tasks through teleoperation or autonomy, such as playing multiple sports, moving and manipulating objects, and interacting with humans. We develop an RL-based sim-to-real pipeline, which involves large-scale retargeting and augmentation of human motion datasets, learning a real-world deployable policy with sparse sensor input by imitating a privileged teacher policy, and reward designs to enhance robustness and stability. We release the first humanoid whole-body control dataset, OmniH2O-6, containing six everyday tasks, and demonstrate humanoid whole-body skill learning from teleoperated datasets.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "https://github.com/LeCAR-Lab/human2humanoid",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "whole-body control",
            "dexterous hand",
            "sim-to-real",
            "[T]teleoperation"
          ],
          "score": 18.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reward design"
          ],
          "score": 1.5
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "[T]human-to-humanoid",
            "[T]OmniH2O"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 37.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "7_retargeting"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [Nvidia] GR00T N1: An Open Foundation Model for Generalist Humanoid Robots",
      "arxiv_id": "2503.14734",
      "arxiv_url": "https://arxiv.org/pdf/2503.14734",
      "summary": "General-purpose robots need a versatile body and an intelligent mind. Recent advancements in humanoid robots have shown great promise as a hardware platform for building generalist autonomy in the human world. A robot foundation model, trained on massive and diverse data sources, is essential for enabling the robots to reason about novel situations, robustly handle real-world variability, and rapidly learn new tasks. To this end, we introduce GR00T N1, an open foundation model for humanoid robots. GR00T N1 is a Vision-Language-Action (VLA) model with a dual-system architecture. The vision-language module (System 2) interprets the environment through vision and language instructions. The subsequent diffusion transformer module (System 1) generates fluid motor actions in real time. Both modules are tightly coupled and jointly trained end-to-end. We train GR00T N1 with a heterogeneous mixture of real-robot trajectories, human videos, and synthetically generated datasets. We show that our generalist robot model GR00T N1 outperforms the state-of-the-art imitation learning baselines on standard simulation benchmarks across multiple robot embodiments. Furthermore, we deploy our model on the Fourier GR-1 humanoid robot for language-conditioned bimanual manipulation tasks, achieving strong performance with high data efficiency.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot",
            "manipulation",
            "bi-manual",
            "bimanual manipulation"
          ],
          "score": 18.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA",
            "[T]foundation model",
            "language conditioned"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 37.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Humanoid Locomotion and Manipulation: Current Progress and Challenges in Control, Planning, and Learning [Survey]",
      "arxiv_id": "2501.02116",
      "arxiv_url": "https://arxiv.org/abs/2501.02116",
      "summary": "Humanoid robots hold great potential to perform various human-level skills, involving unified locomotion and manipulation in real-world settings. Driven by advances in machine learning and the strength of existing model-based approaches, these capabilities have progressed rapidly, but often separately. This survey offers a comprehensive overview of the state-of-the-art in humanoid locomotion and manipulation (HLM), with a focus on control, planning, and learning methods. We first review the model-based methods that have been the backbone of humanoid robotics for the past three decades. We discuss contact planning, motion planning, and whole-body control, highlighting the trade-offs between model fidelity and computational efficiency. Then the focus is shifted to examine emerging learning-based methods, with an emphasis on reinforcement and imitation learning that enhance the robustness and versatility of loco-manipulation skills. Furthermore, we assess the potential of integrating foundation models with humanoid embodiments to enable the development of generalist humanoid agents. This survey also highlights the emerging role of tactile sensing, particularly whole-body tactile feedback, as a crucial modality for handling contact-rich interactions. Finally, we compare the strengths and limitations of model-based and learning-based paradigms from multiple perspectives, such as robustness, computational efficiency, versatility, and generalizability, and suggest potential solutions to existing challenges.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]humanoid locomotion",
            "whole-body control",
            "[T]locomotion",
            "[T]manipulation",
            "loco-manipulation",
            "motion planning"
          ],
          "score": 32.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 36.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Bi-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Dexterous Manipulations",
      "arxiv_id": "2405.06039",
      "arxiv_url": "https://arxiv.org/pdf/2405.06039",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]dexterous manipulation",
            "[T]bi-manual"
          ],
          "score": 18.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 36.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Retargeting Matters: General Motion Retargeting for Humanoid Motion Tracking",
      "arxiv_id": "2510.02252",
      "arxiv_url": "https://arxiv.org/pdf/2510.02252",
      "summary": "Humanoid motion tracking policies are central to building teleoperation pipelines and hierarchical controllers, yet they face a fundamental challenge: the embodiment gap between humans and humanoid robots. Current approaches address this gap by retargeting human motion data to humanoid embodiments and then training reinforcement learning (RL) policies to imitate these reference trajectories. However, artifacts introduced during retargeting, such as foot sliding, self-penetration, and physically infeasible motion are often left in the reference trajectories for the RL policy to correct. While prior work has demonstrated motion tracking abilities, they often require extensive reward engineering and domain randomization to succeed. In this paper, we systematically evaluate how retargeting quality affects policy performance when excessive reward tuning is suppressed. To address issues that we identify with existing retargeting methods, we propose a new retargeting method, General Motion Retargeting (GMR). We evaluate GMR alongside two open-source retargeters, PHC and ProtoMotions, as well as with a high-quality closed-source dataset from Unitree. Using BeyondMimic for policy training, we isolate retargeting effects without reward tuning. Our experiments on a diverse subset of the LAFAN1 dataset reveal that while most motions can be tracked, artifacts in retargeted data significantly reduce policy robustness, particularly for dynamic or long sequences. GMR consistently outperforms existing open-source methods in both tracking performance and faithfulness to the source motion, achieving perceptual fidelity and policy success rates close to the closed-source baseline. Website: https://jaraujo98.github.io/retargeting_matters. Code: https://github.com/YanjieZe/GMR.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/YanjieZe/GMR",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "domain randomization",
            "teleoperation",
            "Unitree"
          ],
          "score": 14.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "penetration"
          ],
          "score": 2.5
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "[T]motion retargeting"
          ],
          "score": 9.0
        },
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "[T]motion tracking",
            "PHC"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 35.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "4_motion_diffusion",
        "7_retargeting",
        "8_physics_animation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2022] Sim-to-Real Learning of Compliant Bipedal Locomotion on Torque Sensor-Less Gear-Driven Humanoid.",
      "arxiv_id": "2204.03897",
      "arxiv_url": "https://arxiv.org/abs/2204.03897",
      "summary": "Sim-to-real is a mainstream method to cope with the large number of trials needed by typical deep reinforcement learning methods. However, transferring a policy trained in simulation to actual hardware remains an open challenge due to the reality gap. In particular, the characteristics of actuators in legged robots have a considerable influence on sim-to-real transfer. There are two challenges: 1) High reduction ratio gears are widely used in actuators, and the reality gap issue becomes especially pronounced when backdrivability is considered in controlling joints compliantly. 2) The difficulty in achieving stable bipedal locomotion causes typical system identification methods to fail to sufficiently transfer the policy. For these two challenges, we propose 1) a new simulation model of gears and 2) a method for system identification that can utilize failed attempts. The method's effectiveness is verified using a biped robot, the ROBOTIS-OP3, and the sim-to-real transferred policy can stabilize the robot under severe disturbances and walk on uneven surfaces without using force and torque sensors.",
      "authors": [],
      "year": "2022",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "legged robot",
            "[T]humanoid",
            "[T]bipedal",
            "[T]biped",
            "[T]locomotion",
            "[T]sim-to-real"
          ],
          "score": 32.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "deep reinforcement learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 35.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Distillation-PPO: A Novel Two-Stage Reinforcement Learning Framework for Humanoid Robot Perceptive Locomotion",
      "arxiv_id": "2503.08299",
      "arxiv_url": "https://arxiv.org/abs/2503.08299",
      "summary": "In recent years, humanoid robots have garnered significant attention from both academia and industry due to their high adaptability to environments and human-like characteristics. With the rapid advancement of reinforcement learning, substantial progress has been made in the walking control of humanoid robots. However, existing methods still face challenges when dealing with complex environments and irregular terrains. In the field of perceptive locomotion, existing approaches are generally divided into two-stage methods and end-to-end methods. Two-stage methods first train a teacher policy in a simulated environment and then use distillation techniques, such as DAgger, to transfer the privileged information learned as latent features or actions to the student policy. End-to-end methods, on the other hand, forgo the learning of privileged information and directly learn policies from a partially observable Markov decision process (POMDP) through reinforcement learning. However, due to the lack of supervision from a teacher policy, end-to-end methods often face difficulties in training and exhibit unstable performance in real-world applications. This paper proposes an innovative two-stage perceptive locomotion framework that combines the advantages of teacher policies learned in a fully observable Markov decision process (MDP) to regularize and supervise the student policy. At the same time, it leverages the characteristics of reinforcement learning to ensure that the student policy can continue to learn in a POMDP, thereby enhancing the model's upper bound. Our experimental results demonstrate that our two-stage training framework achieves higher training efficiency and stability in simulated environments, while also exhibiting better robustness and generalization capabilities in real-world applications.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning, Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot",
            "[T]locomotion"
          ],
          "score": 18.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "[T]PPO",
            "[T]distillation",
            "privileged information"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 33.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2023] Learning Bipedal Walking for Humanoids with Current Feedback.",
      "arxiv_id": "2303.03724",
      "arxiv_url": "https://arxiv.org/pdf/2303.03724.pdf",
      "summary": "Recent advances in deep reinforcement learning (RL) based techniques combined with training in simulation have offered a new approach to developing robust controllers for legged robots. However, the application of such approaches to real hardware has largely been limited to quadrupedal robots with direct-drive actuators and light-weight bipedal robots with low gear-ratio transmission systems. Application to real, life-sized humanoid robots has been less common arguably due to a large sim2real gap. In this paper, we present an approach for effectively overcoming the sim2real gap issue for humanoid robots arising from inaccurate torque-tracking at the actuator level. Our key idea is to utilize the current feedback from the actuators on the real robot, after training the policy in a simulation environment artificially degraded with poor torque-tracking. Our approach successfully trains a unified, end-to-end policy in simulation that can be deployed on a real HRP-5P humanoid robot to achieve bipedal locomotion. Through ablations, we also show that a feedforward policy architecture combined with targeted dynamics randomization is sufficient for zero-shot sim2real success, thus eliminating the need for computationally expensive, memory-based network architectures. Finally, we validate the robustness of the proposed RL policy by comparing its performance against a conventional model-based controller for walking on uneven terrain with the real robot.",
      "authors": [],
      "year": "2023",
      "venue": "arxiv",
      "code_url": "https://github.com/rohanpsingh/LearningHumanoidWalking/tree/topic/omnidirectional-walk",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped",
            "legged robot",
            "[T]humanoid",
            "humanoid robot",
            "[T]bipedal",
            "[T]biped",
            "locomotion",
            "sim2real",
            "dynamics randomization"
          ],
          "score": 30.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "deep reinforcement learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 33.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] 3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary Affordance Detection in 3D Worlds",
      "arxiv_id": "2502.20041",
      "arxiv_url": "https://arxiv.org/pdf/2502.20041",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]open-vocabulary",
            "[T]open vocabulary",
            "[T]affordance",
            "[T]affordance detection"
          ],
          "score": 24.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 33.0,
      "hit_pillars": [
        "3_perception_slam",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.10] DexMimicGen: Automated Data Generation for Bimanual Dexterous Manipulation via Imitation Learning [IL] [[project](https://dexmimicgen.github.io/#)]",
      "arxiv_id": "2410.24185",
      "arxiv_url": "https://arxiv.org/abs/2410.24185",
      "summary": "Imitation learning from human demonstrations is an effective means to teach robots manipulation skills. But data acquisition is a major bottleneck in applying this paradigm more broadly, due to the amount of cost and human effort involved. There has been significant interest in imitation learning for bimanual dexterous robots, like humanoids. Unfortunately, data collection is even more challenging here due to the challenges of simultaneously controlling multiple arms and multi-fingered hands. Automated data generation in simulation is a compelling, scalable alternative to fuel this need for data. To this end, we introduce DexMimicGen, a large-scale automated data generation system that synthesizes trajectories from a handful of human demonstrations for humanoid robots with dexterous hands. We present a collection of simulation environments in the setting of bimanual dexterous manipulation, spanning a range of manipulation behaviors and different requirements for coordination among the two arms. We generate 21K demos across these tasks from just 60 source human demos and study the effect of several data generation and policy learning decisions on agent performance. Finally, we present a real-to-sim-to-real pipeline and deploy it on a real-world humanoid can sorting task. Generated datasets, simulation environments and additional results are at https://dexmimicgen.github.io/",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid",
            "humanoid robot",
            "[T]manipulation",
            "dexterous hand",
            "[T]dexterous manipulation",
            "[T]bi-manual",
            "sim-to-real"
          ],
          "score": 26.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning",
            "[T]imitation learning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 32.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Shake-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Manipulations and Liquid Mixing",
      "arxiv_id": "2501.06919",
      "arxiv_url": "https://arxiv.org/pdf/2501.06919",
      "summary": "This paper introduces Shake-VLA, a Vision-Language-Action (VLA) model-based system designed to enable bimanual robotic manipulation for automated cocktail preparation. The system integrates a vision module for detecting ingredient bottles and reading labels, a speech-to-text module for interpreting user commands, and a language model to generate task-specific robotic instructions. Force Torque (FT) sensors are employed to precisely measure the quantity of liquid poured, ensuring accuracy in ingredient proportions during the mixing process. The system architecture includes a Retrieval-Augmented Generation (RAG) module for accessing and adapting recipes, an anomaly detection mechanism to address ingredient availability issues, and bimanual robotic arms for dexterous manipulation. Experimental evaluations demonstrated a high success rate across system components, with the speech-to-text module achieving a 93% success rate in noisy environments, the vision module attaining a 91% success rate in object and label detection in cluttered environment, the anomaly module successfully identified 95% of discrepancies between detected ingredients and recipe requirements, and the system achieved an overall success rate of 100% in preparing cocktails, from recipe formulation to action generation.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "dexterous manipulation",
            "[T]bi-manual"
          ],
          "score": 14.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 32.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [WBC] BFM-Zero: A Promptable Behavioral Foundation Model for Humanoid Control Using Unsupervised Reinforcement Learning",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid control",
            "[T]WBC"
          ],
          "score": 18.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 31.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation",
      "arxiv_id": "2503.20384",
      "arxiv_url": "https://arxiv.org/pdf/2503.20384",
      "summary": "Multimodal Large Language Models (MLLMs) excel in understanding complex language and visual data, enabling generalist robotic systems to interpret instructions and perform embodied tasks. Nevertheless, their real-world deployment is hindered by substantial computational and storage demands. Recent insights into the homogeneous patterns in the LLM layer have inspired sparsification techniques to address these challenges, such as early exit and token pruning. However, these methods often neglect the critical role of the final layers that encode the semantic information most relevant to downstream robotic tasks. Aligning with the recent breakthrough of the Shallow Brain Hypothesis (SBH) in neuroscience and the mixture of experts in model sparsification, we conceptualize each LLM layer as an expert and propose a Mixture-of-Layers Vision-Language-Action model (MoLe-VLA, or simply MoLe) architecture for dynamic LLM layer activation. We introduce a Spatial-Temporal Aware Router (STAR) for MoLe to selectively activate only parts of the layers based on the robot's current state, mimicking the brain's distinct signal pathways specialized for cognition and causal reasoning. Additionally, to compensate for the cognitive ability of LLMs lost in MoLe, we devise a Cognition Self-Knowledge Distillation (CogKD) framework. CogKD enhances the understanding of task demands and improves the generation of task-relevant action sequences by leveraging cognitive features. Extensive experiments conducted in both RLBench simulation and real-world environments demonstrate the superiority of MoLe-VLA in both efficiency and performance. Specifically, MoLe-VLA achieves an 8% improvement in the mean success rate across ten tasks while reducing computational costs by up to x5.6 compared to standard LLMs.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "distillation"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA",
            "large language model",
            "multimodal"
          ],
          "score": 24.0
        }
      ],
      "relevance_score": 31.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] TACT: Humanoid Whole-body Contact Manipulation through Deep Imitation Learning with Tactile Modality",
      "arxiv_id": "2506.15146",
      "arxiv_url": "https://arxiv.org/pdf/2506.15146",
      "summary": "Manipulation with whole-body contact by humanoid robots offers distinct advantages, including enhanced stability and reduced load. On the other hand, we need to address challenges such as the increased computational cost of motion generation and the difficulty of measuring broad-area contact. We therefore have developed a humanoid control system that allows a humanoid robot equipped with tactile sensors on its upper body to learn a policy for whole-body manipulation through imitation learning based on human teleoperation data. This policy, named tactile-modality extended ACT (TACT), has a feature to take multiple sensor modalities as input, including joint position, vision, and tactile measurements. Furthermore, by integrating this policy with retargeting and locomotion control based on a biped model, we demonstrate that the life-size humanoid robot RHP7 Kaleido is capable of achieving whole-body contact manipulation while maintaining balance and walking. Through detailed experimental verification, we show that inputting both vision and tactile modalities into the policy contributes to improving the robustness of manipulation involving broad and delicate contact.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "humanoid control",
            "biped",
            "locomotion",
            "[T]manipulation",
            "whole-body manipulation",
            "teleoperation"
          ],
          "score": 24.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "motion generation"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 31.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "4_motion_diffusion"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] TD-GRPC: Temporal Difference Learning with Group Relative Policy Constraint for Humanoid Locomotion",
      "arxiv_id": "2505.13549",
      "arxiv_url": "https://arxiv.org/pdf/2505.13549",
      "summary": "Robot learning in high-dimensional control settings, such as humanoid locomotion, presents persistent challenges for reinforcement learning (RL) algorithms due to unstable dynamics, complex contact interactions, and sensitivity to distributional shifts during training. Model-based methods, \\textit{e.g.}, Temporal-Difference Model Predictive Control (TD-MPC), have demonstrated promising results by combining short-horizon planning with value-based learning, enabling efficient solutions for basic locomotion tasks. However, these approaches remain ineffective in addressing policy mismatch and instability introduced by off-policy updates. Thus, in this work, we introduce Temporal-Difference Group Relative Policy Constraint (TD-GRPC), an extension of the TD-MPC framework that unifies Group Relative Policy Optimization (GRPO) with explicit Policy Constraints (PC). TD-GRPC applies a trust-region constraint in the latent policy space to maintain consistency between the planning priors and learned rollouts, while leveraging group-relative ranking to assess and preserve the physical feasibility of candidate trajectories. Unlike prior methods, TD-GRPC achieves robust motions without modifying the underlying planner, enabling flexible planning and policy learning. We validate our method across a locomotion task suite ranging from basic walking to highly dynamic movements on the 26-DoF Unitree H1-2 humanoid robot. Through simulation results, TD-GRPC demonstrates its improvements in stability and policy robustness with sampling efficiency while training for complex humanoid control tasks.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "humanoid control",
            "[T]humanoid locomotion",
            "[T]locomotion",
            "MPC",
            "model predictive control",
            "Unitree"
          ],
          "score": 28.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "policy learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 31.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Adversarial Locomotion and Motion Imitation for Humanoid Policy Learning",
      "arxiv_id": "2504.14305",
      "arxiv_url": "https://arxiv.org/pdf/2504.14305",
      "summary": "Humans exhibit diverse and expressive whole-body movements. However, attaining human-like whole-body coordination in humanoid robots remains challenging, as conventional approaches that mimic whole-body motions often neglect the distinct roles of upper and lower body. This oversight leads to computationally intensive policy learning and frequently causes robot instability and falls during real-world execution. To address these issues, we propose Adversarial Locomotion and Motion Imitation (ALMI), a novel framework that enables adversarial policy learning between upper and lower body. Specifically, the lower body aims to provide robust locomotion capabilities to follow velocity commands while the upper body tracks various motions. Conversely, the upper-body policy ensures effective motion tracking when the robot executes velocity-based movements. Through iterative updates, these policies achieve coordinated whole-body control, which can be extended to loco-manipulation tasks with teleoperation systems. Extensive experiments demonstrate that our method achieves robust locomotion and precise motion tracking in both simulation and on the full-size Unitree H1 robot. Additionally, we release a large-scale whole-body motion control dataset featuring high-quality episodic trajectories from MuJoCo simulations deployable on real robots. The project page is https://almi-humanoid.github.io.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "whole-body control",
            "[T]locomotion",
            "manipulation",
            "loco-manipulation",
            "teleoperation",
            "Unitree"
          ],
          "score": 24.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]policy learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "motion tracking"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 30.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "8_physics_animation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[Humanoid] Deep Imitation Learning for Humanoid Loco-manipulation through Human Teleoperation [imitation] [Tele-Opreation]",
      "arxiv_id": "2309.01952",
      "arxiv_url": "https://arxiv.org/pdf/2309.01952.pdf",
      "summary": "We tackle the problem of developing humanoid loco-manipulation skills with deep imitation learning. The difficulty of collecting task demonstrations and training policies for humanoids with a high degree of freedom presents substantial challenges. We introduce TRILL, a data-efficient framework for training humanoid loco-manipulation policies from human demonstrations. In this framework, we collect human demonstration data through an intuitive Virtual Reality (VR) interface. We employ the whole-body control formulation to transform task-space commands by human operators into the robot's joint-torque actuation while stabilizing its dynamics. By employing high-level action abstractions tailored for humanoid loco-manipulation, our method can efficiently learn complex sensorimotor skills. We demonstrate the effectiveness of TRILL in simulation and on a real-world robot for performing various loco-manipulation tasks. Videos and additional materials can be found on the project page: https://ut-austin-rpl.github.io/TRILL.",
      "authors": [],
      "year": "",
      "venue": "arxiv",
      "code_url": "https://github.com/UT-Austin-RPL/TRILL",
      "source_repo": "Awesome Humanoid Learning, Awesome Humanoid Robot Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "whole-body control",
            "[T]manipulation",
            "[T]loco-manipulation",
            "[T]teleoperation"
          ],
          "score": 26.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 30.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2022] Learning Bipedal Walking On Planned Footsteps For Humanoid Robots.",
      "arxiv_id": "2207.12644",
      "arxiv_url": "https://arxiv.org/pdf/2207.12644.pdf",
      "summary": "Deep reinforcement learning (RL) based controllers for legged robots have demonstrated impressive robustness for walking in different environments for several robot platforms. To enable the application of RL policies for humanoid robots in real-world settings, it is crucial to build a system that can achieve robust walking in any direction, on 2D and 3D terrains, and be controllable by a user-command. In this paper, we tackle this problem by learning a policy to follow a given step sequence. The policy is trained with the help of a set of procedurally generated step sequences (also called footstep plans). We show that simply feeding the upcoming 2 steps to the policy is sufficient to achieve omnidirectional walking, turning in place, standing, and climbing stairs. Our method employs curriculum learning on the complexity of terrains, and circumvents the need for reference motions or pre-trained weights. We demonstrate the application of our proposed method to learn RL policies for 2 new robot platforms - HRP5P and JVRC-1 - in the MuJoCo simulation environment. The code for training and evaluation is available online.",
      "authors": [],
      "year": "2022",
      "venue": "arxiv",
      "code_url": "https://github.com/rohanpsingh/LearningHumanoidWalking",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "legged robot",
            "[T]humanoid",
            "[T]humanoid robot",
            "[T]bipedal",
            "[T]biped"
          ],
          "score": 26.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "deep reinforcement learning",
            "curriculum learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 30.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[Workshop on Whole-body Control and Bimanual Manipulation: Applications in Humanoids and Beyond",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "CORL",
      "code_url": "",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]whole-body control",
            "[T]manipulation",
            "[T]bi-manual",
            "[T]bimanual manipulation"
          ],
          "score": 30.0
        }
      ],
      "relevance_score": 30.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] OmniH2O: Universal and Dexterous Human-to-Humanoid Whole-Body Teleoperation and Learning [[project](https://omni.human2humanoid.com/)]",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]teleoperation"
          ],
          "score": 12.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "[T]human-to-humanoid",
            "[T]OmniH2O"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 30.0,
      "hit_pillars": [
        "1_robot_core",
        "7_retargeting"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2022] NeRF2Real: Sim2real Transfer of Vision-guided Bipedal Motion Skills using Neural Radiance Fields.",
      "arxiv_id": "2210.04932",
      "arxiv_url": "https://arxiv.org/pdf/2210.04932.pdf",
      "summary": "We present a system for applying sim2real approaches to \"in the wild\" scenes with realistic visuals, and to policies which rely on active perception using RGB cameras. Given a short video of a static scene collected using a generic phone, we learn the scene's contact geometry and a function for novel view synthesis using a Neural Radiance Field (NeRF). We augment the NeRF rendering of the static scene by overlaying the rendering of other dynamic objects (e.g. the robot's own body, a ball). A simulation is then created using the rendering engine in a physics simulator which computes contact dynamics from the static scene geometry (estimated from the NeRF volume density) and the dynamic objects' geometry and physical properties (assumed known). We demonstrate that we can use this simulation to learn vision-based whole body navigation and ball pushing policies for a 20 degrees of freedom humanoid robot with an actuated head-mounted RGB camera, and we successfully transfer these policies to a real robot. Project video is available at https://sites.google.com/view/nerf2real/home",
      "authors": [],
      "year": "2022",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid",
            "humanoid robot",
            "[T]bipedal",
            "[T]biped",
            "[T]sim2real"
          ],
          "score": 22.0
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "NeRF",
            "[T]neural radiance field"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 30.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] A Survey on Vision-Language-Action Models for Embodied AI",
      "arxiv_id": "2405.14093",
      "arxiv_url": "https://arxiv.org/abs/2405.14093",
      "summary": "Embodied AI is widely recognized as a key element of artificial general intelligence because it involves controlling embodied agents to perform tasks in the physical world. Building on the success of large language models and vision-language models, a new category of multimodal models -- referred to as vision-language-action models (VLAs) -- has emerged to address language-conditioned robotic tasks in embodied AI by leveraging their distinct ability to generate actions. In recent years, a myriad of VLAs have been developed, making it imperative to capture the rapidly evolving landscape through a comprehensive survey. To this end, we present the first survey on VLAs for embodied AI. This work provides a detailed taxonomy of VLAs, organized into three major lines of research. The first line focuses on individual components of VLAs. The second line is dedicated to developing control policies adept at predicting low-level actions. The third line comprises high-level task planners capable of decomposing long-horizon tasks into a sequence of subtasks, thereby guiding VLAs to follow more general user instructions. Furthermore, we provide an extensive summary of relevant resources, including datasets, simulators, and benchmarks. Finally, we discuss the challenges faced by VLAs and outline promising future directions in embodied AI. We have created a project associated with this survey, which is available at https://github.com/yueen-ma/Awesome-VLA.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]embodied AI",
            "[T]vision-language-action",
            "VLA",
            "large language model",
            "multimodal",
            "language conditioned"
          ],
          "score": 30.0
        }
      ],
      "relevance_score": 30.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.04] Humanoid-Gym: Reinforcement Learning for Humanoid Robot with Zero-Shot Sim2Real Transfer [RL] [benchmark]",
      "arxiv_id": "2404.05695",
      "arxiv_url": "https://arxiv.org/abs/2404.05695",
      "summary": "Humanoid-Gym is an easy-to-use reinforcement learning (RL) framework based on Nvidia Isaac Gym, designed to train locomotion skills for humanoid robots, emphasizing zero-shot transfer from simulation to the real-world environment. Humanoid-Gym also integrates a sim-to-sim framework from Isaac Gym to Mujoco that allows users to verify the trained policies in different physical simulations to ensure the robustness and generalization of the policies. This framework is verified by RobotEra's XBot-S (1.2-meter tall humanoid robot) and XBot-L (1.65-meter tall humanoid robot) in a real-world environment with zero-shot sim-to-real transfer. The project website and source code can be found at: https://sites.google.com/view/humanoid-gym/.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "https://github.com/roboterax/humanoid-gym",
      "source_repo": "Awesome Humanoid Manipulation, Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot",
            "locomotion",
            "sim-to-real",
            "[T]sim2real"
          ],
          "score": 22.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "zero-shot transfer"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 29.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] DPL: Depth-only Perceptive Humanoid Locomotion via Realistic Depth Synthesis and Cross-Attention Terrain Reconstruction",
      "arxiv_id": "2510.07152",
      "arxiv_url": "https://arxiv.org/pdf/2510.07152",
      "summary": "Recent advancements in legged robot perceptive locomotion have shown promising progress. However, terrain-aware humanoid locomotion remains largely constrained to two paradigms: depth image-based end-to-end learning and elevation map-based methods. The former suffers from limited training efficiency and a significant sim-to-real gap in depth perception, while the latter depends heavily on multiple vision sensors and localization systems, resulting in latency and reduced robustness. To overcome these challenges, we propose a novel framework that tightly integrates three key components: (1) Terrain-Aware Locomotion Policy with a Blind Backbone, which leverages pre-trained elevation map-based perception to guide reinforcement learning with minimal visual input; (2) Multi-Modality Cross-Attention Transformer, which reconstructs structured terrain representations from noisy depth images; (3) Realistic Depth Images Synthetic Method, which employs self-occlusion-aware ray casting and noise-aware modeling to synthesize realistic depth observations, achieving over 30\\% reduction in terrain reconstruction error. This combination enables efficient policy training with limited data and hardware resources, while preserving critical terrain features essential for generalization. We validate our framework on a full-sized humanoid robot, demonstrating agile and adaptive locomotion across diverse and challenging terrains.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "legged robot",
            "[T]humanoid",
            "humanoid robot",
            "[T]humanoid locomotion",
            "[T]locomotion",
            "locomotion policy",
            "sim-to-real"
          ],
          "score": 26.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "elevation map"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 29.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[RSS] Learning Memory-Based Control for Human-Scale Bipedal Locomotion. [sim2real]",
      "arxiv_id": "2006.02402",
      "arxiv_url": "https://arxiv.org/abs/2006.02402",
      "summary": "Controlling a non-statically stable biped is a difficult problem largely due to the complex hybrid dynamics involved. Recent work has demonstrated the effectiveness of reinforcement learning (RL) for simulation-based training of neural network controllers that successfully transfer to real bipeds. The existing work, however, has primarily used simple memoryless network architectures, even though more sophisticated architectures, such as those including memory, often yield superior performance in other RL domains. In this work, we consider recurrent neural networks (RNNs) for sim-to-real biped locomotion, allowing for policies that learn to use internal memory to model important physical properties. We show that while RNNs are able to significantly outperform memoryless policies in simulation, they do not exhibit superior behavior on the real biped due to overfitting to the simulation physics unless trained using dynamics randomization to prevent overfitting; this leads to consistently better sim-to-real transfer. We also show that RNNs could use their learned memory states to perform online system identification by encoding parameters of the dynamics into memory.",
      "authors": [],
      "year": "2006",
      "venue": "RSS",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bipedal",
            "[T]biped",
            "[T]locomotion",
            "sim-to-real",
            "[T]sim2real",
            "dynamics randomization"
          ],
          "score": 28.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 29.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration",
      "arxiv_id": "2502.14795",
      "arxiv_url": "https://arxiv.org/pdf/2502.14795",
      "summary": "This paper addresses the limitations of current humanoid robot control frameworks, which primarily rely on reactive mechanisms and lack autonomous interaction capabilities due to data scarcity. We propose Humanoid-VLA, a novel framework that integrates language understanding, egocentric scene perception, and motion control, enabling universal humanoid control. Humanoid-VLA begins with language-motion pre-alignment using non-egocentric human motion datasets paired with textual descriptions, allowing the model to learn universal motion patterns and action semantics. We then incorporate egocentric visual context through a parameter efficient video-conditioned fine-tuning, enabling context-aware motion generation. Furthermore, we introduce a self-supervised data augmentation strategy that automatically generates pseudoannotations directly derived from motion data. This process converts raw motion sequences into informative question-answer pairs, facilitating the effective use of large-scale unlabeled video data. Built upon whole-body control architectures, extensive experiments show that Humanoid-VLA achieves object interaction and environment exploration tasks with enhanced contextual awareness, demonstrating a more human-like capacity for adaptive and intelligent engagement.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]humanoid control",
            "whole-body control"
          ],
          "score": 16.0
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "motion generation"
          ],
          "score": 2.5
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "egocentric"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]VLA"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 29.5,
      "hit_pillars": [
        "1_robot_core",
        "4_motion_diffusion",
        "6_video_extraction",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] A Survey of Behavior Foundation Model: Next-Generation Whole-Body Control System of Humanoid Robots",
      "arxiv_id": "2506.20487",
      "arxiv_url": "https://arxiv.org/abs/2506.20487",
      "summary": "Humanoid robots are drawing significant attention as versatile platforms for complex motor control, human-robot interaction, and general-purpose physical intelligence. However, achieving efficient whole-body control (WBC) in humanoids remains a fundamental challenge due to sophisticated dynamics, underactuation, and diverse task requirements. While learning-based controllers have shown promise for complex tasks, their reliance on labor-intensive and costly retraining for new scenarios limits real-world applicability. To address these limitations, behavior(al) foundation models (BFMs) have emerged as a new paradigm that leverages large-scale pre-training to learn reusable primitive skills and broad behavioral priors, enabling zero-shot or rapid adaptation to a wide range of downstream tasks. In this paper, we present a comprehensive overview of BFMs for humanoid WBC, tracing their development across diverse pre-training pipelines. Furthermore, we discuss real-world applications, current limitations, urgent challenges, and future opportunities, positioning BFMs as a key approach toward scalable and general-purpose humanoid intelligence. Finally, we provide a curated and regularly updated collection of BFM papers and projects to facilitate more subsequent research, which is available at https://github.com/yuanmingqi/awesome-bfm-papers.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot",
            "[T]whole-body control",
            "WBC"
          ],
          "score": 20.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 29.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Robust Humanoid Walking on Compliant and Uneven Terrain with Deep Reinforcement Learning",
      "arxiv_id": "2504.13619",
      "arxiv_url": "https://arxiv.org/pdf/2504.13619",
      "summary": "For the deployment of legged robots in real-world environments, it is essential to develop robust locomotion control methods for challenging terrains that may exhibit unexpected deformability and irregularity. In this paper, we explore the application of sim-to-real deep reinforcement learning (RL) for the design of bipedal locomotion controllers for humanoid robots on compliant and uneven terrains. Our key contribution is to show that a simple training curriculum for exposing the RL agent to randomized terrains in simulation can achieve robust walking on a real humanoid robot using only proprioceptive feedback. We train an end-to-end bipedal locomotion policy using the proposed approach, and show extensive real-robot demonstration on the HRP-5P humanoid over several difficult terrains inside and outside the lab environment. Further, we argue that the robustness of a bipedal walking policy can be improved if the robot is allowed to exhibit aperiodic motion with variable stepping frequency. We propose a new control policy to enable modification of the observed clock signal, leading to adaptive gait frequencies depending on the terrain and command velocity. Through simulation experiments, we show the effectiveness of this policy specifically for walking over challenging terrains by controlling swing and stance durations. The code for training and evaluation is available online at https://github.com/rohanpsingh/LearningHumanoidWalking. Demo video is available at https://www.youtube.com/watch?v=ZgfNzGAkk2Q.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "legged robot",
            "[T]humanoid",
            "humanoid robot",
            "bipedal",
            "biped",
            "locomotion",
            "locomotion policy",
            "sim-to-real"
          ],
          "score": 20.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "[T]deep reinforcement learning"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 29.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model",
      "arxiv_id": "2502.14420",
      "arxiv_url": "https://arxiv.org/pdf/2502.14420",
      "summary": "Humans possess a unified cognitive ability to perceive, comprehend, and interact with the physical world. Why can't large language models replicate this holistic understanding? Through a systematic analysis of existing training paradigms in vision-language-action models (VLA), we identify two key challenges: spurious forgetting, where robot training overwrites crucial visual-text alignments, and task interference, where competing control and understanding tasks degrade performance when trained jointly. To overcome these limitations, we propose ChatVLA, a novel framework featuring Phased Alignment Training, which incrementally integrates multimodal data after initial control mastery, and a Mixture-of-Experts architecture to minimize task interference. ChatVLA demonstrates competitive performance on visual question-answering datasets and significantly surpasses state-of-the-art vision-language-action (VLA) methods on multimodal understanding benchmarks. Notably, it achieves a six times higher performance on MMMU and scores 47.2% on MMStar with a more parameter-efficient design than ECoT. Furthermore, ChatVLA demonstrates superior performance on 25 real-world robot manipulation tasks compared to existing VLA methods like OpenVLA. Our findings highlight the potential of our unified framework for achieving both robust multimodal understanding and effective robot control.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "large language model",
            "[T]multimodal",
            "OpenVLA"
          ],
          "score": 27.0
        }
      ],
      "relevance_score": 29.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": [
        "question answering"
      ]
    },
    {
      "title": "[2025] [CVPR 25] MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation",
      "arxiv_id": "2503.13446",
      "arxiv_url": "https://arxiv.org/pdf/2503.13446",
      "summary": "Mobile manipulation is the fundamental challenge for robotics to assist humans with diverse tasks and environments in everyday life. However, conventional mobile manipulation approaches often struggle to generalize across different tasks and environments because of the lack of large-scale training. In contrast, recent advances in vision-language-action (VLA) models have shown impressive generalization capabilities, but these foundation models are developed for fixed-base manipulation tasks. Therefore, we propose an efficient policy adaptation framework named MoManipVLA to transfer pre-trained VLA models of fix-base manipulation to mobile manipulation, so that high generalization ability across tasks and environments can be achieved in mobile manipulation policy. Specifically, we utilize pre-trained VLA models to generate waypoints of the end-effector with high generalization ability. We design motion planning objectives for the mobile base and the robot arm, which aim at maximizing the physical feasibility of the trajectory. Finally, we present an efficient bi-level objective optimization framework for trajectory generation, where the upper-level optimization predicts waypoints for base movement to enhance the manipulator policy space, and the lower-level optimization selects the optimal end-effector trajectory to complete the manipulation task. In this way, MoManipVLA can adjust the position of the robot base in a zero-shot manner, thus making the waypoints predicted from the fixed-base VLA models feasible. Extensive experimental results on OVMM and the real world demonstrate that MoManipVLA achieves a 4.2% higher success rate than the state-of-the-art mobile manipulation, and only requires 50 training cost for real world deployment due to the strong generalization ability in the pre-trained VLA models.",
      "authors": [],
      "year": "2025",
      "venue": "CVPR",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]mobile manipulation",
            "motion planning"
          ],
          "score": 14.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "foundation model"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 29.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [CVPR 25] CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models",
      "arxiv_id": "2503.22020",
      "arxiv_url": "https://arxiv.org/pdf/2503.22020",
      "summary": "Vision-language-action models (VLAs) have shown potential in leveraging pretrained vision-language models and diverse robot demonstrations for learning generalizable sensorimotor control. While this paradigm effectively utilizes large-scale data from both robotic and non-robotic sources, current VLAs primarily focus on direct input--output mappings, lacking the intermediate reasoning steps crucial for complex manipulation tasks. As a result, existing VLAs lack temporal planning or reasoning capabilities. In this paper, we introduce a method that incorporates explicit visual chain-of-thought (CoT) reasoning into vision-language-action models (VLAs) by predicting future image frames autoregressively as visual goals before generating a short action sequence to achieve these goals. We introduce CoT-VLA, a state-of-the-art 7B VLA that can understand and generate visual and action tokens. Our experimental results demonstrate that CoT-VLA achieves strong performance, outperforming the state-of-the-art VLA model by 17% in real-world manipulation tasks and 6% in simulation benchmarks. Project website: https://cot-vla.github.io/",
      "authors": [],
      "year": "2025",
      "venue": "CVPR",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA",
            "[T]chain-of-thought"
          ],
          "score": 27.0
        }
      ],
      "relevance_score": 29.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] [CoRL 24] OpenVLA: An Open-Source Vision-Language-Action Model",
      "arxiv_id": "2406.09246",
      "arxiv_url": "https://arxiv.org/pdf/2406.09246",
      "summary": "Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, we introduce OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4%. We also explore compute efficiency; as a separate contribution, we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.",
      "authors": [],
      "year": "2024",
      "venue": "CoRL",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning",
            "diffusion policy"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "RT-2",
            "[T]OpenVLA"
          ],
          "score": 24.0
        }
      ],
      "relevance_score": 29.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Deep Reinforcement Learning for Bipedal Locomotion: A Brief Survey",
      "arxiv_id": "2404.17070",
      "arxiv_url": "https://arxiv.org/abs/2404.17070",
      "summary": "Bipedal robots are gaining global recognition due to their potential applications and advancements in artificial intelligence, particularly through Deep Reinforcement Learning (DRL). While DRL has significantly advanced bipedal locomotion, the development of a unified framework capable of handling a wide range of tasks remains an ongoing challenge. This survey systematically categorises, compares, and analyses existing DRL frameworks for bipedal locomotion, organising them into end-to-end and hierarchical control schemes. End-to-end frameworks are evaluated based on their learning approaches, while hierarchical frameworks are examined in terms of layered structures that integrate learning-based or traditional model-based methods. We provide a detailed evaluation of the composition, strengths, limitations, and capabilities of each framework. Additionally, this survey identifies key research gaps and proposes future directions aimed at creating a more integrated and efficient framework for bipedal locomotion, with wide-ranging applications in real-world environments.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bipedal",
            "[T]biped",
            "[T]locomotion"
          ],
          "score": 18.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "[T]deep reinforcement learning",
            "DRL"
          ],
          "score": 10.5
        }
      ],
      "relevance_score": 28.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[ICRA] Reinforcement Learning for Robust Parameterized Locomotion Control of Bipedal Robots. [sim2real]",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "ICRA",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bipedal",
            "[T]biped",
            "[T]locomotion",
            "[T]sim2real"
          ],
          "score": 24.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 28.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [CoRL 25] Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation",
      "arxiv_id": "2508.19958",
      "arxiv_url": "https://arxiv.org/pdf/2508.19958",
      "summary": "Vision-Language-Action (VLA) models have become a cornerstone in robotic policy learning, leveraging large-scale multimodal data for robust and scalable control. However, existing VLA frameworks primarily address short-horizon tasks, and their effectiveness on long-horizon, multi-step robotic manipulation remains limited due to challenges in skill chaining and subtask dependencies. In this work, we introduce Long-VLA, the first end-to-end VLA model specifically designed for long-horizon robotic tasks. Our approach features a novel phase-aware input masking strategy that adaptively segments each subtask into moving and interaction phases, enabling the model to focus on phase-relevant sensory cues and enhancing subtask compatibility. This unified strategy preserves the scalability and data efficiency of VLA training, and our architecture-agnostic module can be seamlessly integrated into existing VLA models. We further propose the L-CALVIN benchmark to systematically evaluate long-horizon manipulation. Extensive experiments on both simulated and real-world tasks demonstrate that Long-VLA significantly outperforms prior state-of-the-art methods, establishing a new baseline for long-horizon robotic control.",
      "authors": [],
      "year": "2025",
      "venue": "CoRL",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA",
            "multimodal"
          ],
          "score": 21.0
        }
      ],
      "relevance_score": 28.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model",
      "arxiv_id": "2510.10274",
      "arxiv_url": "https://arxiv.org/pdf/2510.10274",
      "summary": "Successful generalist Vision-Language-Action (VLA) models rely on effective training across diverse robotic platforms with large-scale, cross-embodiment, heterogeneous datasets. To facilitate and leverage the heterogeneity in rich, diverse robotic data sources, we propose a novel Soft Prompt approach with minimally added parameters, by infusing prompt learning concepts into cross-embodiment robot learning and introducing separate sets of learnable embeddings for each distinct data source. These embeddings serve as embodiment-specific prompts, which in unity empower VLA models with effective exploitation of varying cross-embodiment features. Our new X-VLA, a neat flow-matching-based VLA architecture, relies exclusively on soft-prompted standard Transformer encoders, enjoying both scalability and simplicity. Evaluated across 6 simulations as well as 3 real-world robots, our 0.9B instantiation-X-VLA-0.9B simultaneously achieves SOTA performance over a sweep of benchmarks, demonstrating superior results on a wide axes of capabilities, from flexible dexterity to quick adaptation across embodiments, environments, and tasks. Website: https://thu-air-dream.github.io/X-VLA/",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/2toinf/X-VLA",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "flow matching"
          ],
          "score": 1.5
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "[T]cross-embodiment"
          ],
          "score": 9.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 28.5,
      "hit_pillars": [
        "2_algo_arch",
        "7_retargeting",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [CoRL] GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data [[project](https://pku-epic.github.io/GraspVLA-web/)]",
      "arxiv_id": "2505.03233",
      "arxiv_url": "https://arxiv.org/pdf/2505.03233",
      "summary": "Embodied foundation models are gaining increasing attention for their zero-shot generalization, scalability, and adaptability to new tasks through few-shot post-training. However, existing models rely heavily on real-world data, which is costly and labor-intensive to collect. Synthetic data offers a cost-effective alternative, yet its potential remains largely underexplored. To bridge this gap, we explore the feasibility of training Vision-Language-Action models entirely with large-scale synthetic action data. We curate SynGrasp-1B, a billion-frame robotic grasping dataset generated in simulation with photorealistic rendering and extensive domain randomization. Building on this, we present GraspVLA, a VLA model pretrained on large-scale synthetic action data as a foundational model for grasping tasks. GraspVLA integrates autoregressive perception tasks and flow-matching-based action generation into a unified Chain-of-Thought process, enabling joint training on synthetic action data and Internet semantics data. This design helps mitigate sim-to-real gaps and facilitates the transfer of learned actions to a broader range of Internet-covered objects, achieving open-vocabulary generalization in grasping. Extensive evaluations across real-world and simulation benchmarks demonstrate GraspVLA's advanced zero-shot generalizability and few-shot adaptability to specific human preferences. We will release SynGrasp-1B dataset and pre-trained weights to benefit the community.",
      "authors": [],
      "year": "2025",
      "venue": "CoRL",
      "code_url": "https://github.com/PKU-EPIC/GraspVLA",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "sim-to-real",
            "domain randomization"
          ],
          "score": 4.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "flow matching"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "open-vocabulary",
            "open vocabulary"
          ],
          "score": 4.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA",
            "[T]foundation model",
            "chain-of-thought"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 27.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos",
      "arxiv_id": "2507.12440",
      "arxiv_url": "https://arxiv.org/pdf/2507.12440",
      "summary": "Real robot data collection for imitation learning has led to significant advancements in robotic manipulation. However, the requirement for robot hardware in the process fundamentally constrains the scale of the data. In this paper, we explore training Vision-Language-Action (VLA) models using egocentric human videos. The benefit of using human videos is not only for their scale but more importantly for the richness of scenes and tasks. With a VLA trained on human video that predicts human wrist and hand actions, we can perform Inverse Kinematics and retargeting to convert the human actions to robot actions. We fine-tune the model using a few robot manipulation demonstrations to obtain the robot policy, namely EgoVLA. We propose a simulation benchmark called Ego Humanoid Manipulation Benchmark, where we design diverse bimanual manipulation tasks with demonstrations. We fine-tune and evaluate EgoVLA with Ego Humanoid Manipulation Benchmark and show significant improvements over baselines and ablate the importance of human data. Videos can be found on our website: https://rchalyang.github.io/EgoVLA",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid",
            "manipulation",
            "bi-manual",
            "bimanual manipulation"
          ],
          "score": 8.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "[T]egocentric"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 27.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "6_video_extraction",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation",
      "arxiv_id": "2507.23523",
      "arxiv_url": "https://arxiv.org/pdf/2507.23523v1",
      "summary": "Imitation learning for robotic manipulation faces a fundamental challenge: the scarcity of large-scale, high-quality robot demonstration data. Recent robotic foundation models often pre-train on cross-embodiment robot datasets to increase data scale, while they face significant limitations as the diverse morphologies and action spaces across different robot embodiments make unified training challenging. In this paper, we present H-RDT (Human to Robotics Diffusion Transformer), a novel approach that leverages human manipulation data to enhance robot manipulation capabilities. Our key insight is that large-scale egocentric human manipulation videos with paired 3D hand pose annotations provide rich behavioral priors that capture natural manipulation strategies and can benefit robotic policy learning. We introduce a two-stage training paradigm: (1) pre-training on large-scale egocentric human manipulation data, and (2) cross-embodiment fine-tuning on robot-specific data with modular action encoders and decoders. Built on a diffusion transformer architecture with 2B parameters, H-RDT uses flow matching to model complex action distributions. Extensive evaluations encompassing both simulation and real-world experiments, single-task and multitask scenarios, as well as few-shot learning and robustness assessments, demonstrate that H-RDT outperforms training from scratch and existing state-of-the-art methods, including Pi0 and RDT, achieving significant improvements of 13.9% and 40.5% over training from scratch in simulation and real-world experiments, respectively. The results validate our core hypothesis that human manipulation data can serve as a powerful foundation for learning bimanual robotic manipulation policies.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/HongzheBi/H_RDT",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]bi-manual"
          ],
          "score": 12.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning",
            "imitation learning",
            "flow matching"
          ],
          "score": 4.5
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "egocentric"
          ],
          "score": 2.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "human-to-robot",
            "cross-embodiment"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 27.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "6_video_extraction",
        "7_retargeting",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.10] RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation [IL] [[github](https://github.com/thu-ml/RoboticsDiffusionTransformer)]",
      "arxiv_id": "2410.07864",
      "arxiv_url": "https://arxiv.org/pdf/2410.07864",
      "summary": "Bimanual manipulation is essential in robotics, yet developing foundation models is extremely challenging due to the inherent complexity of coordinating two robot arms (leading to multi-modal action distributions) and the scarcity of training data. In this paper, we present the Robotics Diffusion Transformer (RDT), a pioneering diffusion foundation model for bimanual manipulation. RDT builds on diffusion models to effectively represent multi-modality, with innovative designs of a scalable Transformer to deal with the heterogeneity of multi-modal inputs and to capture the nonlinearity and high frequency of robotic data. To address data scarcity, we further introduce a Physically Interpretable Unified Action Space, which can unify the action representations of various robots while preserving the physical meanings of original actions, facilitating learning transferrable physical knowledge. With these designs, we managed to pre-train RDT on the largest collection of multi-robot datasets to date and scaled it up to 1.2B parameters, which is the largest diffusion-based foundation model for robotic manipulation. We finally fine-tuned RDT on a self-created multi-task bimanual dataset with over 6K+ episodes to refine its manipulation capabilities. Experiments on real robots demonstrate that RDT significantly outperforms existing methods. It exhibits zero-shot generalization to unseen objects and scenes, understands and follows language instructions, learns new skills with just 1~5 demonstrations, and effectively handles complex, dexterous tasks. We refer to https://rdt-robotics.github.io/rdt-robotics/ for the code and videos.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "https://github.com/thu-ml/RoboticsDiffusionTransformer",
      "source_repo": "Awesome Humanoid Manipulation, Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]bi-manual",
            "[T]bimanual manipulation"
          ],
          "score": 18.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 27.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [WBC] Behavior Foundation Model for Humanoid Robots",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot",
            "[T]WBC"
          ],
          "score": 18.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 27.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[CoRL] DiffuseLoco: Real-Time Legged Locomotion Control with Diffusion from Offline Datasets",
      "arxiv_id": "2404.19264",
      "arxiv_url": "https://arxiv.org/abs/2404.19264",
      "summary": "This work introduces DiffuseLoco, a framework for training multi-skill diffusion-based policies for dynamic legged locomotion from offline datasets, enabling real-time control of diverse skills on robots in the real world. Offline learning at scale has led to breakthroughs in computer vision, natural language processing, and robotic manipulation domains. However, scaling up learning for legged robot locomotion, especially with multiple skills in a single policy, presents significant challenges for prior online reinforcement learning methods. To address this challenge, we propose a novel, scalable framework that leverages diffusion models to directly learn from offline multimodal datasets with a diverse set of locomotion skills. With design choices tailored for real-time control in dynamical systems, including receding horizon control and delayed inputs, DiffuseLoco is capable of reproducing multimodality in performing various locomotion skills, zero-shot transfer to real quadrupedal robots, and it can be deployed on edge computing devices. Furthermore, DiffuseLoco demonstrates free transitions between skills and robustness against environmental variations. Through extensive benchmarking in real-world experiments, DiffuseLoco exhibits better stability and velocity tracking performance compared to prior reinforcement learning and non-diffusion-based behavior cloning baselines. The design choices are validated via comprehensive ablation studies. This work opens new possibilities for scaling up learning-based legged locomotion controllers through the scaling of large, expressive models and diverse offline datasets.",
      "authors": [],
      "year": "",
      "venue": "CoRL",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped",
            "legged robot",
            "[T]legged locomotion",
            "[T]locomotion",
            "manipulation"
          ],
          "score": 18.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "behavior cloning"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal",
            "zero-shot transfer"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 27.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] WoCoCo: Learning Whole-Body Humanoid Control with Sequential Contacts [[project](https://lecar-lab.github.io/wococo/)]",
      "arxiv_id": "2406.06005",
      "arxiv_url": "https://arxiv.org/abs/2406.06005",
      "summary": "Humanoid activities involving sequential contacts are crucial for complex robotic interactions and operations in the real world and are traditionally solved by model-based motion planning, which is time-consuming and often relies on simplified dynamics models. Although model-free reinforcement learning (RL) has become a powerful tool for versatile and robust whole-body humanoid control, it still requires tedious task-specific tuning and state machine design and suffers from long-horizon exploration issues in tasks involving contact sequences. In this work, we propose WoCoCo (Whole-Body Control with Sequential Contacts), a unified framework to learn whole-body humanoid control with sequential contacts by naturally decomposing the tasks into separate contact stages. Such decomposition facilitates simple and general policy learning pipelines through task-agnostic reward and sim-to-real designs, requiring only one or two task-related terms to be specified for each task. We demonstrated that end-to-end RL-based controllers trained with WoCoCo enable four challenging whole-body humanoid tasks involving diverse contact sequences in the real world without any motion priors: 1) versatile parkour jumping, 2) box loco-manipulation, 3) dynamic clap-and-tap dancing, and 4) cliffside climbing. We further show that WoCoCo is a general framework beyond humanoid by applying it in 22-DoF dinosaur robot loco-manipulation tasks.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid control",
            "whole-body control",
            "parkour",
            "manipulation",
            "loco-manipulation",
            "sim-to-real",
            "motion planning"
          ],
          "score": 24.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "policy learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 27.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey",
      "arxiv_id": "2508.13073",
      "arxiv_url": "https://arxiv.org/pdf/2508.13073",
      "summary": "Robotic manipulation, a key frontier in robotics and embodied AI, requires precise motor control and multimodal understanding, yet traditional rule-based methods fail to scale or generalize in unstructured, novel environments. In recent years, Vision-Language-Action (VLA) models, built upon Large Vision-Language Models (VLMs) pretrained on vast image-text datasets, have emerged as a transformative paradigm. This survey provides the first systematic, taxonomy-oriented review of large VLM-based VLA models for robotic manipulation. We begin by clearly defining large VLM-based VLA models and delineating two principal architectural paradigms: (1) monolithic models, encompassing single-system and dual-system designs with differing levels of integration; and (2) hierarchical models, which explicitly decouple planning from execution via interpretable intermediate representations. Building on this foundation, we present an in-depth examination of large VLM-based VLA models: (1) integration with advanced domains, including reinforcement learning, training-free optimization, learning from human videos, and world model integration; (2) synthesis of distinctive characteristics, consolidating architectural traits, operational strengths, and the datasets and benchmarks that support their development; (3) identification of promising directions, including memory mechanisms, 4D perception, efficient adaptation, multi-agent cooperation, and other emerging capabilities. This survey consolidates recent advances to resolve inconsistencies in existing taxonomies, mitigate research fragmentation, and fill a critical gap through the systematic integration of studies at the intersection of large VLMs and robotic manipulation. We provide a regularly updated project page to document ongoing progress: https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "world model"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "embodied AI",
            "[T]vision-language-action",
            "VLA",
            "multimodal"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 27.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [Microsoft Research] OmniVLA: Physically-Grounded Multimodal VLA with Unified Multi-Sensor Perception for Robotic Manipulation",
      "arxiv_id": "2511.01210",
      "arxiv_url": "https://arxiv.org/abs/2511.01210",
      "summary": "Vision-language-action (VLA) models have shown strong generalization for robotic action prediction through large-scale vision-language pretraining. However, most existing models rely solely on RGB cameras, limiting their perception and, consequently, manipulation capabilities. We present OmniVLA, an omni-modality VLA model that integrates novel sensing modalities for physically-grounded spatial intelligence beyond RGB perception. The core of our approach is the sensor-masked image, a unified representation that overlays spatially grounded and physically meaningful masks onto the RGB images, derived from sensors including an infrared camera, a mmWave radar, and a microphone array. This image-native unification keeps sensor input close to RGB statistics to facilitate training, provides a uniform interface across sensor hardware, and enables data-efficient learning with lightweight per-sensor projectors. Built on this, we present a multisensory vision-language-action model architecture and train the model based on an RGB-pretrained VLA backbone. We evaluate OmniVLA on challenging real-world tasks where sensor-modality perception guides the robotic manipulation. OmniVLA achieves an average task success rate of 84%, significantly outperforms both RGB-only and raw-sensor-input baseline models by 59% and 28% respectively, meanwhile showing higher learning efficiency and stronger generalization capability.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "[T]VLA",
            "[T]multimodal"
          ],
          "score": 21.0
        }
      ],
      "relevance_score": 27.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution",
      "arxiv_id": "2411.02359",
      "arxiv_url": "https://arxiv.org/pdf/2411.02359",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]VLA",
            "[T]large language model",
            "[T]multimodal"
          ],
          "score": 27.0
        }
      ],
      "relevance_score": 27.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2023] PaLM-E: An Embodied Multimodal Language Model: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
      "arxiv_id": "2303.03378",
      "arxiv_url": "https://arxiv.org/pdf/2303.03378",
      "summary": "",
      "authors": [],
      "year": "2023",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]multimodal",
            "[T]PaLM-E"
          ],
          "score": 27.0
        }
      ],
      "relevance_score": 27.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.12] Mobile-TeleVision: Predictive Motion Priors for Humanoid Whole-Body Control [RL] [[project](https://mobile-tv.github.io/)]",
      "arxiv_id": "2412.07773",
      "arxiv_url": "https://arxiv.org/abs/2412.07773",
      "summary": "Humanoid robots require both robust lower-body locomotion and precise upper-body manipulation. While recent Reinforcement Learning (RL) approaches provide whole-body loco-manipulation policies, they lack precise manipulation with high DoF arms. In this paper, we propose decoupling upper-body control from locomotion, using inverse kinematics (IK) and motion retargeting for precise manipulation, while RL focuses on robust lower-body locomotion. We introduce PMP (Predictive Motion Priors), trained with Conditional Variational Autoencoder (CVAE) to effectively represent upper-body motions. The locomotion policy is trained conditioned on this upper-body motion representation, ensuring that the system remains robust with both manipulation and locomotion. We show that CVAE features are crucial for stability and robustness, and significantly outperforms RL-based whole-body control in precise manipulation. With precise upper-body motion and robust lower-body locomotion control, operators can remotely control the humanoid to walk around and explore different environments, while performing diverse manipulation tasks.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "https://github.com/OpenTeleVision/TeleVision",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]whole-body control",
            "locomotion",
            "locomotion policy",
            "manipulation",
            "loco-manipulation"
          ],
          "score": 22.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "motion retargeting"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 26.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "7_retargeting"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.06] Learning Hybrid Behavior Planning for Autonomous Loco-manipulation [VLM] [[project](https://hy-motion.github.io/)]",
      "arxiv_id": "2406.14655",
      "arxiv_url": "https://arxiv.org/abs/2406.14655v1",
      "summary": "Enabling robots to autonomously perform hybrid motions in diverse environments can be beneficial for long-horizon tasks such as material handling, household chores, and work assistance. This requires extensive exploitation of intrinsic motion capabilities, extraction of affordances from rich environmental information, and planning of physical interaction behaviors. Despite recent progress has demonstrated impressive humanoid whole-body control abilities, they struggle to achieve versatility and adaptability for new tasks. In this work, we propose HYPERmotion, a framework that learns, selects and plans behaviors based on tasks in different scenarios. We combine reinforcement learning with whole-body optimization to generate motion for 38 actuated joints and create a motion library to store the learned skills. We apply the planning and reasoning features of the large language models (LLMs) to complex loco-manipulation tasks, constructing a hierarchical task graph that comprises a series of primitive behaviors to bridge lower-level execution with higher-level planning. By leveraging the interaction of distilled spatial geometry and 2D observation with a visual language model (VLM) to ground knowledge into a robotic morphology selector to choose appropriate actions in single- or dual-arm, legged or wheeled locomotion. Experiments in simulation and real-world show that learned motions can efficiently adapt to new tasks, demonstrating high autonomy from free-text commands in unstructured scenes. Videos and website: hy-motion.github.io/",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Manipulation, Awesome Humanoid Robot Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid",
            "whole-body control",
            "locomotion",
            "[T]manipulation",
            "[T]loco-manipulation",
            "dual-arm"
          ],
          "score": 20.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "affordance"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 26.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending",
      "arxiv_id": "2506.09366",
      "arxiv_url": "https://arxiv.org/abs/2506.09366",
      "summary": "Humanoid robots hold significant potential in accomplishing daily tasks across diverse environments thanks to their flexibility and human-like morphology. Recent works have made significant progress in humanoid whole-body control and loco-manipulation leveraging optimal control or reinforcement learning. However, these methods require tedious task-specific tuning for each task to achieve satisfactory behaviors, limiting their versatility and scalability to diverse tasks in daily scenarios. To that end, we introduce SkillBlender, a novel hierarchical reinforcement learning framework for versatile humanoid loco-manipulation. SkillBlender first pretrains goal-conditioned task-agnostic primitive skills, and then dynamically blends these skills to accomplish complex loco-manipulation tasks with minimal task-specific reward engineering. We also introduce SkillBench, a parallel, cross-embodiment, and diverse simulated benchmark containing three embodiments, four primitive skills, and eight challenging loco-manipulation tasks, accompanied by a set of scientific evaluation metrics balancing accuracy and feasibility. Extensive simulated experiments show that our method significantly outperforms all baselines, while naturally regularizing behaviors to avoid reward hacking, resulting in more accurate and feasible movements for diverse loco-manipulation tasks in our daily scenarios. Our code and benchmark will be open-sourced to the community to facilitate future research. Project page: https://usc-gvl.github.io/SkillBlender-web/.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/Humanoid-SkillBlender/SkillBlender",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "whole-body control",
            "[T]manipulation",
            "[T]loco-manipulation"
          ],
          "score": 22.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "cross-embodiment"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 26.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "7_retargeting"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Learning Perceptive Humanoid Locomotion over Challenging Terrain",
      "arxiv_id": "2503.00692",
      "arxiv_url": "https://arxiv.org/pdf/2503.00692",
      "summary": "Humanoid robots are engineered to navigate terrains akin to those encountered by humans, which necessitates human-like locomotion and perceptual abilities. Currently, the most reliable controllers for humanoid motion rely exclusively on proprioception, a reliance that becomes both dangerous and unreliable when coping with rugged terrain. Although the integration of height maps into perception can enable proactive gait planning, robust utilization of this information remains a significant challenge, especially when exteroceptive perception is noisy. To surmount these challenges, we propose a solution based on a teacher-student distillation framework. In this paradigm, an oracle policy accesses noise-free data to establish an optimal reference policy, while the student policy not only imitates the teacher's actions but also simultaneously trains a world model with a variational information bottleneck for sensor denoising and state estimation. Extensive evaluations demonstrate that our approach markedly enhances performance in scenarios characterized by unreliable terrain estimations. Moreover, we conducted rigorous testing in both challenging urban settings and off-road environments, the model successfully traverse 2 km of varied terrain without external intervention.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]humanoid locomotion",
            "[T]locomotion"
          ],
          "score": 20.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "world model",
            "teacher-student",
            "distillation"
          ],
          "score": 4.5
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "height map"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 26.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Learning Humanoid Locomotion with World Model Reconstruction",
      "arxiv_id": "2502.16230",
      "arxiv_url": "https://arxiv.org/pdf/2502.16230",
      "summary": "Humanoid robots are designed to navigate environments accessible to humans using their legs. However, classical research has primarily focused on controlled laboratory settings, resulting in a gap in developing controllers for navigating complex real-world terrains. This challenge mainly arises from the limitations and noise in sensor data, which hinder the robot's understanding of itself and the environment. In this study, we introduce World Model Reconstruction (WMR), an end-to-end learning-based approach for blind humanoid locomotion across challenging terrains. We propose training an estimator to explicitly reconstruct the world state and utilize it to enhance the locomotion policy. The locomotion policy takes inputs entirely from the reconstructed information. The policy and the estimator are trained jointly; however, the gradient between them is intentionally cut off. This ensures that the estimator focuses solely on world reconstruction, independent of the locomotion policy's updates. We evaluated our model on rough, deformable, and slippery surfaces in real-world scenarios, demonstrating robust adaptability and resistance to interference. The robot successfully completed a 3.2 km hike without any human assistance, mastering terrains covered with ice and snow.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]humanoid locomotion",
            "[T]locomotion",
            "locomotion policy"
          ],
          "score": 22.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 26.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2022] Dynamic Bipedal Maneuvers through Sim-to-Real Reinforcement Learning.",
      "arxiv_id": "2207.07835",
      "arxiv_url": "https://arxiv.org/abs/2207.07835",
      "summary": "For legged robots to match the athletic capabilities of humans and animals, they must not only produce robust periodic walking and running, but also seamlessly switch between nominal locomotion gaits and more specialized transient maneuvers. Despite recent advancements in controls of bipedal robots, there has been little focus on producing highly dynamic behaviors. Recent work utilizing reinforcement learning to produce policies for control of legged robots have demonstrated success in producing robust walking behaviors. However, these learned policies have difficulty expressing a multitude of different behaviors on a single network. Inspired by conventional optimization-based control techniques for legged robots, this work applies a recurrent policy to execute four-step, 90 degree turns trained using reference data generated from optimized single rigid body model trajectories. We present a novel training framework using epilogue terminal rewards for learning specific behaviors from pre-computed trajectory data and demonstrate a successful transfer to hardware on the bipedal robot Cassie.",
      "authors": [],
      "year": "2022",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "legged robot",
            "[T]bipedal",
            "[T]biped",
            "locomotion",
            "[T]sim-to-real"
          ],
          "score": 22.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 26.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit Decoding",
      "arxiv_id": "2506.13725",
      "arxiv_url": "https://arxiv.org/pdf/2506.13725",
      "summary": "In recent years, Vision-Language-Action (VLA) models have become a vital research direction in robotics due to their impressive multimodal understanding and generalization capabilities. Despite the progress, their practical deployment is severely constrained by inference speed bottlenecks, particularly in high-frequency and dexterous manipulation tasks. While recent studies have explored Jacobi decoding as a more efficient alternative to traditional autoregressive decoding, its practical benefits are marginal due to the lengthy iterations. To address it, we introduce consistency distillation training to predict multiple correct action tokens in each iteration, thereby achieving acceleration. Besides, we design mixed-label supervision to mitigate the error accumulation during distillation. Although distillation brings acceptable speedup, we identify that certain inefficient iterations remain a critical bottleneck. To tackle this, we propose an early-exit decoding strategy that moderately relaxes convergence conditions, which further improves average inference efficiency. Experimental results show that the proposed method achieves more than 4 times inference acceleration across different baselines while maintaining high task success rates in both simulated and real-world robot tasks. These experiments validate that our approach provides an efficient and general paradigm for accelerating multimodal decision-making in robotics. Our project page is available at https://irpn-eai.github.io/CEED-VLA/.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "dexterous manipulation"
          ],
          "score": 4.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "distillation"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA",
            "multimodal"
          ],
          "score": 21.0
        }
      ],
      "relevance_score": 26.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Unsupervised Neural Motion Retargeting for Humanoid Teleoperation",
      "arxiv_id": "2406.00727",
      "arxiv_url": "https://arxiv.org/pdf/2406.00727",
      "summary": "This study proposes an approach to human-to-humanoid teleoperation using GAN-based online motion retargeting, which obviates the need for the construction of pairwise datasets to identify the relationship between the human and the humanoid kinematics. Consequently, it can be anticipated that our proposed teleoperation system will reduce the complexity and setup requirements typically associated with humanoid controllers, thereby facilitating the development of more accessible and intuitive teleoperation systems for users without robotics knowledge. The experiments demonstrated the efficacy of the proposed method in retargeting a range of upper-body human motions to humanoid, including a body jab motion and a basketball shoot motion. Moreover, the human-in-the-loop teleoperation performance was evaluated by measuring the end-effector position errors between the human and the retargeted humanoid motions. The results demonstrated that the error was comparable to those of conventional motion retargeting methods that require pairwise motion datasets. Finally, a box pick-and-place task was conducted to demonstrate the usability of the developed humanoid teleoperation system.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid control",
            "[T]teleoperation"
          ],
          "score": 14.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "[T]motion retargeting",
            "human-to-humanoid"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 26.0,
      "hit_pillars": [
        "1_robot_core",
        "7_retargeting"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Expressive Whole-Body Control for Humanoid Robots [imitation] [[project](https://expressive-humanoid.github.io/)]",
      "arxiv_id": "2402.16796",
      "arxiv_url": "https://arxiv.org/abs/2402.16796",
      "summary": "Can we enable humanoid robots to generate rich, diverse, and expressive motions in the real world? We propose to learn a whole-body control policy on a human-sized robot to mimic human motions as realistic as possible. To train such a policy, we leverage the large-scale human motion capture data from the graphics community in a Reinforcement Learning framework. However, directly performing imitation learning with the motion capture dataset would not work on the real humanoid robot, given the large gap in degrees of freedom and physical capabilities. Our method Expressive Whole-Body Control (Exbody) tackles this problem by encouraging the upper humanoid body to imitate a reference motion, while relaxing the imitation constraint on its two legs and only requiring them to follow a given velocity robustly. With training in simulation and Sim2Real transfer, our policy can control a humanoid robot to walk in different styles, shake hands with humans, and even dance with a human in the real world. We conduct extensive studies and comparisons on diverse motions in both simulation and the real world to show the effectiveness of our approach.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning, Awesome Humanoid Robot Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot",
            "[T]whole-body control",
            "sim2real"
          ],
          "score": 20.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "imitation learning"
          ],
          "score": 3.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "ExBody"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 26.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "7_retargeting"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Refined Policy Distillation: From VLA Generalists to RL Experts",
      "arxiv_id": "2503.05833",
      "arxiv_url": "https://arxiv.org/pdf/2503.05833",
      "summary": "Vision-Language-Action Models (VLAs) have demonstrated remarkable generalization capabilities in real-world experiments. However, their success rates are often not on par with expert policies, and they require fine-tuning when the setup changes. In this work, we introduce Refined Policy Distillation (RPD), a novel Reinforcement Learning (RL)-based policy refinement method that bridges this performance gap through a combination of on-policy RL with behavioral cloning. The core idea of RPD is to distill and refine VLAs into compact, high-performing expert policies by guiding the student policy during RL exploration using the actions of a teacher VLA, resulting in increased sample efficiency and faster convergence. We complement our method by fine-tuned versions of Octo and OpenVLA for ManiSkill3 to evaluate RPD in simulation. While this is a key requirement for applying RL, it also yields new insights beyond existing studies on VLA performance in real-world settings. Our experimental results across various manipulation tasks show that RPD enables the RL student to learn expert policies that outperform the VLA teacher in both dense and sparse reward settings, while also achieving faster convergence than the RL baseline. Our approach is even robust to changes in camera perspective and can generalize to task variations that the underlying VLA cannot solve. Our code, dataset, VLA checkpoints, and videos are available at https://refined-policy-distillation.github.io",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "[T]distillation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "[T]VLA",
            "Octo",
            "OpenVLA"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 26.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image Generation",
      "arxiv_id": "2506.01196",
      "arxiv_url": "https://arxiv.org/pdf/2506.01196",
      "summary": "We introduce OG-VLA, a novel architecture and learning framework that combines the generalization strengths of Vision Language Action models (VLAs) with the robustness of 3D-aware policies. We address the challenge of mapping natural language instructions and one or more RGBD observations to quasi-static robot actions. 3D-aware robot policies achieve state-of-the-art performance on precise robot manipulation tasks, but struggle with generalization to unseen instructions, scenes, and objects. On the other hand, VLAs excel at generalizing across instructions and scenes, but can be sensitive to camera and robot pose variations. We leverage prior knowledge embedded in language and vision foundation models to improve generalization of 3D-aware keyframe policies. OG-VLA unprojects input observations from diverse views into a point cloud which is then rendered from canonical orthographic views, ensuring input view invariance and consistency between input and output spaces. These canonical views are processed with a vision backbone, a Large Language Model (LLM), and an image diffusion model to generate images that encode the next position and orientation of the end-effector on the input scene. Evaluations on the Arnold and Colosseum benchmarks demonstrate state-of-the-art generalization to unseen environments, with over 40% relative improvements while maintaining robust performance in seen settings. We also show real-world adaption in 3 to 5 demonstrations along with strong generalization. Videos and resources at https://og-vla.github.io/",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA",
            "large language model",
            "foundation model"
          ],
          "score": 24.0
        }
      ],
      "relevance_score": 26.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": [
        "image generation"
      ]
    },
    {
      "title": "[2024.12] ARMOR: Egocentric Perception for Humanoid Robot Collision Avoidance and Motion Planning [IL] [MP]",
      "arxiv_id": "2412.00396",
      "arxiv_url": "https://arxiv.org/abs/2412.00396",
      "summary": "Humanoid robots have significant gaps in their sensing and perception, making it hard to perform motion planning in dense environments. To address this, we introduce ARMOR, a novel egocentric perception system that integrates both hardware and software, specifically incorporating wearable-like depth sensors for humanoid robots. Our distributed perception approach enhances the robot's spatial awareness, and facilitates more agile motion planning. We also train a transformer-based imitation learning (IL) policy in simulation to perform dynamic collision avoidance, by leveraging around 86 hours worth of human realistic motions from the AMASS dataset. We show that our ARMOR perception is superior against a setup with multiple dense head-mounted, and externally mounted depth cameras, with a 63.7% reduction in collisions, and 78.7% improvement on success rate. We also compare our IL policy against a sampling-based motion planning expert cuRobo, showing 31.6% less collisions, 16.9% higher success rate, and 26x reduction in computational latency. Lastly, we deploy our ARMOR perception on our real-world GR1 humanoid from Fourier Intelligence. We are going to update the link to the source code, HW description, and 3D CAD files in the arXiv version of this text.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Manipulation, Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot",
            "[T]motion planning"
          ],
          "score": 18.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "[T]egocentric"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 25.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "6_video_extraction"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.03] HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation [benchmark] [[project](https://humanoid-bench.github.io/)]",
      "arxiv_id": "2403.10506",
      "arxiv_url": "https://arxiv.org/abs/2403.10506",
      "summary": "Humanoid robots hold great promise in assisting humans in diverse environments and tasks, due to their flexibility and adaptability leveraging human-like morphology. However, research in humanoid robots is often bottlenecked by the costly and fragile hardware setups. To accelerate algorithmic research in humanoid robots, we present a high-dimensional, simulated robot learning benchmark, HumanoidBench, featuring a humanoid robot equipped with dexterous hands and a variety of challenging whole-body manipulation and locomotion tasks. Our findings reveal that state-of-the-art reinforcement learning algorithms struggle with most tasks, whereas a hierarchical learning approach achieves superior performance when supported by robust low-level policies, such as walking or reaching. With HumanoidBench, we provide the robotics community with a platform to identify the challenges arising when solving diverse tasks with humanoid robots, facilitating prompt verification of algorithms and ideas. The open-source code is available at https://humanoid-bench.github.io.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "https://github.com/carlosferrazza/humanoid-bench",
      "source_repo": "Awesome Humanoid Manipulation, Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]locomotion",
            "[T]manipulation",
            "whole-body manipulation",
            "dexterous hand"
          ],
          "score": 24.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 25.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] FALCON: Learning Force-Adaptive Humanoid Loco-Manipulation",
      "arxiv_id": "2505.06776",
      "arxiv_url": "https://arxiv.org/pdf/2505.06776",
      "summary": "Humanoid loco-manipulation holds transformative potential for daily service and industrial tasks, yet achieving precise, robust whole-body control with 3D end-effector force interaction remains a major challenge. Prior approaches are often limited to lightweight tasks or quadrupedal/wheeled platforms. To overcome these limitations, we propose FALCON, a dual-agent reinforcement-learning-based framework for robust force-adaptive humanoid loco-manipulation. FALCON decomposes whole-body control into two specialized agents: (1) a lower-body agent ensuring stable locomotion under external force disturbances, and (2) an upper-body agent precisely tracking end-effector positions with implicit adaptive force compensation. These two agents are jointly trained in simulation with a force curriculum that progressively escalates the magnitude of external force exerted on the end effector while respecting torque limits. Experiments demonstrate that, compared to the baselines, FALCON achieves 2x more accurate upper-body joint tracking, while maintaining robust locomotion under force disturbances and achieving faster training convergence. Moreover, FALCON enables policy training without embodiment-specific reward or curriculum tuning. Using the same training setup, we obtain policies that are deployed across multiple humanoids, enabling forceful loco-manipulation tasks such as transporting payloads (0-20N force), cart-pulling (0-100N), and door-opening (0-40N) in the real world.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped",
            "[T]humanoid",
            "whole-body control",
            "locomotion",
            "[T]manipulation",
            "[T]loco-manipulation"
          ],
          "score": 24.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 25.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] HOMIE: Humanoid Loco-Manipulation with Isomorphic Exoskeleton Cockpit",
      "arxiv_id": "2502.13013",
      "arxiv_url": "https://arxiv.org/abs/2502.13013",
      "summary": "Generalizable humanoid loco-manipulation poses significant challenges, requiring coordinated whole-body control and precise, contact-rich object manipulation. To address this, this paper introduces HOMIE, a semi-autonomous teleoperation system that combines a reinforcement learning policy for body control mapped to a pedal, an isomorphic exoskeleton arm for arm control, and motion-sensing gloves for hand control, forming a unified cockpit to freely operate humanoids and establish a data flywheel. The policy incorporates novel designs, including an upper-body pose curriculum, a height-tracking reward, and symmetry utilization. These features enable the system to perform walking and squatting to specific heights while seamlessly adapting to arbitrary upper-body poses. The exoskeleton, by eliminating the reliance on inverse dynamics, delivers faster and more precise arm control. The gloves utilize Hall sensors instead of servos, allowing even compact devices to achieve 15 or more degrees of freedom and freely adapt to any model of dexterous hands. Compared to previous teleoperation systems, HOMIE stands out for its exceptional efficiency, completing tasks in half the time; its expanded working range, allowing users to freely reach high and low areas as well as interact with any objects; and its affordability, with a price of just $500. The system is fully open-source, demos and code can be found in our https://homietele.github.io/.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "whole-body control",
            "[T]manipulation",
            "[T]loco-manipulation",
            "dexterous hand",
            "teleoperation"
          ],
          "score": 24.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 25.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Learning Smooth Humanoid Locomotion through Lipschitz-Constrained Policies [[project](https://lipschitz-constrained-policy.github.io/)]",
      "arxiv_id": "2410.11825",
      "arxiv_url": "https://arxiv.org/abs/2410.11825",
      "summary": "Reinforcement learning combined with sim-to-real transfer offers a general framework for developing locomotion controllers for legged robots. To facilitate successful deployment in the real world, smoothing techniques, such as low-pass filters and smoothness rewards, are often employed to develop policies with smooth behaviors. However, because these techniques are non-differentiable and usually require tedious tuning of a large set of hyperparameters, they tend to require extensive manual tuning for each robotic platform. To address this challenge and establish a general technique for enforcing smooth behaviors, we propose a simple and effective method that imposes a Lipschitz constraint on a learned policy, which we refer to as Lipschitz-Constrained Policies (LCP). We show that the Lipschitz constraint can be implemented in the form of a gradient penalty, which provides a differentiable objective that can be easily incorporated with automatic differentiation frameworks. We demonstrate that LCP effectively replaces the need for smoothing rewards or low-pass filters and can be easily integrated into training frameworks for many distinct humanoid robots. We extensively evaluate LCP in both simulation and real-world humanoid robots, producing smooth and robust locomotion controllers. All simulation and deployment code, along with complete checkpoints, is available on our project page: https://lipschitz-constrained-policy.github.io.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "https://github.com/zixuan417/smooth-humanoid-locomotion",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "legged robot",
            "[T]humanoid",
            "humanoid robot",
            "[T]humanoid locomotion",
            "[T]locomotion",
            "sim-to-real"
          ],
          "score": 24.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 25.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Hierarchical World Models as Visual Whole-Body Humanoid Controllers",
      "arxiv_id": "2405.18418",
      "arxiv_url": "https://arxiv.org/pdf/2405.18418v1",
      "summary": "Whole-body control for humanoids is challenging due to the high-dimensional nature of the problem, coupled with the inherent instability of a bipedal morphology. Learning from visual observations further exacerbates this difficulty. In this work, we explore highly data-driven approaches to visual whole-body humanoid control based on reinforcement learning, without any simplifying assumptions, reward design, or skill primitives. Specifically, we propose a hierarchical world model in which a high-level agent generates commands based on visual observations for a low-level agent to execute, both of which are trained with rewards. Our approach produces highly performant control policies in 8 tasks with a simulated 56-DoF humanoid, while synthesizing motions that are broadly preferred by humans.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "https://github.com/nicklashansen/puppeteer",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid control",
            "bipedal",
            "biped",
            "whole-body control"
          ],
          "score": 18.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "[T]world model",
            "reward design"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 25.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2023] Learning Vision-Based Bipedal Locomotion for Challenging Terrain.",
      "arxiv_id": "2309.14594",
      "arxiv_url": "http://arxiv.org/abs/2309.14594",
      "summary": "Reinforcement learning (RL) for bipedal locomotion has recently demonstrated robust gaits over moderate terrains using only proprioceptive sensing. However, such blind controllers will fail in environments where robots must anticipate and adapt to local terrain, which requires visual perception. In this paper, we propose a fully-learned system that allows bipedal robots to react to local terrain while maintaining commanded travel speed and direction. Our approach first trains a controller in simulation using a heightmap expressed in the robot's local frame. Next, data is collected in simulation to train a heightmap predictor, whose input is the history of depth images and robot states. We demonstrate that with appropriate domain randomization, this approach allows for successful sim-to-real transfer with no explicit pose estimation and no fine-tuning using real-world data. To the best of our knowledge, this is the first example of sim-to-real learning for vision-based bipedal locomotion over challenging terrains.",
      "authors": [],
      "year": "2023",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bipedal",
            "[T]biped",
            "[T]locomotion",
            "sim-to-real",
            "domain randomization"
          ],
          "score": 22.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "height map"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 25.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models",
      "arxiv_id": "2503.08007",
      "arxiv_url": "https://arxiv.org/pdf/2503.08007",
      "summary": "Developing versatile quadruped robots that can smoothly perform various actions and tasks in real-world environments remains a significant challenge. This paper introduces a novel vision-language-action (VLA) model, mixture of robotic experts (MoRE), for quadruped robots that aim to introduce reinforcement learning (RL) for fine-tuning large-scale VLA models with a large amount of mixed-quality data. MoRE integrates multiple low-rank adaptation modules as distinct experts within a dense multi-modal large language model (MLLM), forming a sparse-activated mixture-of-experts model. This design enables the model to effectively adapt to a wide array of downstream tasks. Moreover, we employ a reinforcement learning-based training objective to train our model as a Q-function after deeply exploring the structural properties of our tasks. Effective learning from automatically collected mixed-quality data enhances data efficiency and model performance. Extensive experiments demonstrate that MoRE outperforms all baselines across six different skills and exhibits superior generalization capabilities in out-of-distribution scenarios. We further validate our method in real-world scenarios, confirming the practicality of our approach and laying a solid foundation for future research on multi-task learning in quadruped robots.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]quadruped"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "large language model"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 25.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning",
      "arxiv_id": "2505.18719",
      "arxiv_url": "https://arxiv.org/pdf/2505.18719",
      "summary": "Recent high-capacity vision-language-action (VLA) models have demonstrated impressive performance on a range of robotic manipulation tasks by imitating human demonstrations. However, exploiting offline data with limited visited states will cause execution failure in out-of-distribution scenarios. Intuitively, an exploration-based method that improves on online collected data at test time could address this limitation. We present VLA-RL, an algorithmic and systematic framework that leverages online reinforcement learning (RL) to improve pretrained auto-regressive VLAs in downstream tasks. Within a unified perspective, we first introduce a trajectory-level RL formulation for auto-regressive VLA training, which models general robotic manipulation trajectory as multi-modal multi-turn conversation. To address the challenge of sparse rewards, we fine-tune a pretrained vision-language model as a robotic process reward model, which is trained on pseudo reward labels annotated on automatically extracted task segments. To scale up, we identify several implementation findings that improve the stability and efficiency including curriculum selection strategy, GPU-balanced vectorized environments, batch decoding, and critic warmup. VLA-RL enables OpenVLA-7B to surpass the strongest finetuned baseline by 4.5% on 40 challenging robotic manipulation tasks in LIBERO, and even matches the performance of advanced commercial models such as $π_0$-FAST. Notably, we observe that VLA-RL benefits from increased test-time optimization, indicating an early spark of inference scaling laws in robotics.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "[T]VLA",
            "OpenVLA"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 25.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation",
      "arxiv_id": "2506.07530",
      "arxiv_url": "https://arxiv.org/pdf/2506.07530",
      "summary": "Vision-Language-Action (VLA) models have shown impressive capabilities across a wide range of robotics manipulation tasks. However, their growing model size poses significant challenges for deployment on resource-constrained robotic systems. While 1-bit pretraining has proven effective for enhancing the inference efficiency of large language models with minimal performance loss, its application to VLA models remains underexplored. In this work, we present BitVLA, the first 1-bit VLA model for robotics manipulation, in which every parameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint of the vision encoder, we propose the distillation-aware training strategy that compresses the full-precision encoder to 1.58-bit weights. During this process, a full-precision encoder serves as a teacher model to better align latent representations. Despite the lack of large-scale robotics pretraining, BitVLA achieves performance comparable to the state-of-the-art model OpenVLA-OFT with 4-bit post-training quantization on the LIBERO benchmark, while consuming only 29.8% of the memory. These results highlight BitVLA's promise for deployment on memory-constrained edge devices. We release the code and model weights in https://github.com/ustcwhy/BitVLA.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "distillation"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "large language model",
            "OpenVLA"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 25.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Vision-Language-Action Instruction Tuning: From Understanding to Manipulation",
      "arxiv_id": "2507.17520",
      "arxiv_url": "https://arxiv.org/pdf/2507.17520",
      "summary": "To operate effectively in the real world, robots must integrate multimodal reasoning with precise action generation. However, existing vision-language-action (VLA) models often sacrifice one for the other, narrow their abilities to task-specific manipulation data, and suffer catastrophic forgetting of pre-trained vision-language capabilities. To bridge this gap, we introduce InstructVLA, an end-to-end VLA model that preserves the flexible reasoning of large vision-language models (VLMs) while delivering leading manipulation performance. InstructVLA introduces a novel training paradigm, Vision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal training with mixture-of-experts adaptation to jointly optimize textual reasoning and action generation on both standard VLM corpora and a curated 650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves 30.5% improvement over SpatialVLA. To evaluate generalization, we introduce SimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and high-level instruction understanding, where it outperforms a fine-tuned OpenVLA by 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA surpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling by leveraging textual reasoning to boost manipulation performance in both simulated and real-world settings. These results demonstrate InstructVLA's potential for bridging intuitive and steerable human-robot interaction with efficient policy learning.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "multimodal",
            "OpenVLA"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 25.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation",
      "arxiv_id": "2509.26642",
      "arxiv_url": "https://arxiv.org/pdf/2509.26642",
      "summary": "Vision-language-action models (VLAs) have shown generalization capabilities in robotic manipulation tasks by inheriting from vision-language models (VLMs) and learning action generation. Most VLA models focus on interpreting vision and language to generate actions, whereas robots must perceive and interact within the spatial-physical world. This gap highlights the need for a comprehensive understanding of robotic-specific multisensory information, which is crucial for achieving complex and contact-rich control. To this end, we introduce a multisensory language-action (MLA) model that collaboratively perceives heterogeneous sensory modalities and predicts future multisensory objectives to facilitate physical world modeling. Specifically, to enhance perceptual representations, we propose an encoder-free multimodal alignment scheme that innovatively repurposes the large language model itself as a perception module, directly interpreting multimodal cues by aligning 2D images, 3D point clouds, and tactile tokens through positional correspondence. To further enhance MLA's understanding of physical dynamics, we design a future multisensory generation post-training strategy that enables MLA to reason about semantic, geometric, and interaction information, providing more robust conditions for action generation. For evaluation, the MLA model outperforms the previous state-of-the-art 2D and 3D VLA methods by 12% and 24% in complex, contact-rich real-world tasks, respectively, while also demonstrating improved generalization to unseen configurations. Project website: https://sites.google.com/view/open-mla",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/ZhuoyangLiu2005/MLA",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "world model"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA",
            "large language model",
            "[T]multimodal"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 25.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] UniTracker: Learning Universal Whole-Body Motion Tracker for Humanoid Robots",
      "arxiv_id": "2507.07356",
      "arxiv_url": "https://arxiv.org/abs/2507.07356",
      "summary": "Achieving expressive and generalizable whole-body motion control is essential for deploying humanoid robots in real-world environments. In this work, we propose UniTracker, a three-stage training framework that enables robust and scalable motion tracking across a wide range of human behaviors. In the first stage, we train a teacher policy with privileged observations to generate high-quality actions. In the second stage, we introduce a Conditional Variational Autoencoder (CVAE) to model a universal student policy that can be deployed directly on real hardware. The CVAE structure allows the policy to learn a global latent representation of motion, enhancing generalization to unseen behaviors and addressing the limitations of standard MLP-based policies under partial observations. Unlike pure MLPs that suffer from drift in global attributes like orientation, our CVAE-student policy incorporates global intent during training by aligning a partial-observation prior to the full-observation encoder. In the third stage, we introduce a fast adaptation module that fine-tunes the universal policy on harder motion sequences that are difficult to track directly. This adaptation can be performed both for single sequences and in batch mode, further showcasing the flexibility and scalability of our approach. We evaluate UniTracker in both simulation and real-world settings using a Unitree G1 humanoid, demonstrating strong performance in motion diversity, tracking accuracy, and deployment robustness.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot",
            "Unitree"
          ],
          "score": 14.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "[T]UniTracker"
          ],
          "score": 9.0
        },
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "motion tracking"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 25.0,
      "hit_pillars": [
        "1_robot_core",
        "7_retargeting",
        "8_physics_animation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[ICRA] Sim-to-Real Learning of All Common Bipedal Gaits via Periodic Reward Composition.",
      "arxiv_id": "2011.01387",
      "arxiv_url": "https://arxiv.org/abs/2011.01387",
      "summary": "We study the problem of realizing the full spectrum of bipedal locomotion on a real robot with sim-to-real reinforcement learning (RL). A key challenge of learning legged locomotion is describing different gaits, via reward functions, in a way that is intuitive for the designer and specific enough to reliably learn the gait across different initial random seeds or hyperparameters. A common approach is to use reference motions (e.g. trajectories of joint positions) to guide learning. However, finding high-quality reference motions can be difficult and the trajectories themselves narrowly constrain the space of learned motion. At the other extreme, reference-free reward functions are often underspecified (e.g. move forward) leading to massive variance in policy behavior, or are the product of significant reward-shaping via trial-and-error, making them exclusive to specific gaits. In this work, we propose a reward-specification framework based on composing simple probabilistic periodic costs on basic forces and velocities. We instantiate this framework to define a parametric reward function with intuitive settings for all common bipedal gaits - standing, walking, hopping, running, and skipping. Using this function we demonstrate successful sim-to-real transfer of the learned gaits to the bipedal robot Cassie, as well as a generic policy that can transition between all of the two-beat gaits.",
      "authors": [],
      "year": "2011",
      "venue": "ICRA",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "legged locomotion",
            "[T]bipedal",
            "[T]biped",
            "locomotion",
            "[T]sim-to-real"
          ],
          "score": 22.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "reward shaping"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 25.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Vision-Language-Action Models: Concepts, Progress, Applications and Challenges",
      "arxiv_id": "2505.04769",
      "arxiv_url": "https://arxiv.org/pdf/2505.04769v1",
      "summary": "Vision-Language-Action (VLA) models mark a transformative advancement in artificial intelligence, aiming to unify perception, natural language understanding, and embodied action within a single computational framework. This foundational review presents a comprehensive synthesis of recent advancements in Vision-Language-Action models, systematically organized across five thematic pillars that structure the landscape of this rapidly evolving field. We begin by establishing the conceptual foundations of VLA systems, tracing their evolution from cross-modal learning architectures to generalist agents that tightly integrate vision-language models (VLMs), action planners, and hierarchical controllers. Our methodology adopts a rigorous literature review framework, covering over 80 VLA models published in the past three years. Key progress areas include architectural innovations, parameter-efficient training strategies, and real-time inference accelerations. We explore diverse application domains such as humanoid robotics, autonomous vehicles, medical and industrial robotics, precision agriculture, and augmented reality navigation. The review further addresses major challenges across real-time control, multimodal action representation, system scalability, generalization to unseen tasks, and ethical deployment risks. Drawing from the state-of-the-art, we propose targeted solutions including agentic AI adaptation, cross-embodiment generalization, and unified neuro-symbolic planning. In our forward-looking discussion, we outline a future roadmap where VLA models, VLMs, and agentic AI converge to power socially aligned, adaptive, and general-purpose embodied agents. This work serves as a foundational reference for advancing intelligent, real-world robotics and artificial general intelligence. &gt;Vision-language-action, Agentic AI, AI Agents, Vision-language Models",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid",
            "humanoid robot"
          ],
          "score": 4.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "cross-embodiment"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "generalist agent",
            "[T]vision-language-action",
            "VLA",
            "multimodal"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 25.0,
      "hit_pillars": [
        "1_robot_core",
        "7_retargeting",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": [
        "medical"
      ]
    },
    {
      "title": "[2025] LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction",
      "arxiv_id": "2506.13751",
      "arxiv_url": "https://arxiv.org/pdf/2506.13751",
      "summary": "Vision-language-action (VLA) models have demonstrated strong semantic understanding and zero-shot generalization, yet most existing systems assume an accurate low-level controller with hand-crafted action \"vocabulary\" such as end-effector pose or root velocity. This assumption confines prior work to quasi-static tasks and precludes the agile, whole-body behaviors required by humanoid whole-body control (WBC) tasks. To capture this gap in the literature, we start by introducing the first sim-to-real-ready, vision-language, closed-loop benchmark for humanoid WBC, comprising over 150 tasks from 10 categories. We then propose LeVERB: Latent Vision-Language-Encoded Robot Behavior, a hierarchical latent instruction-following framework for humanoid vision-language WBC, the first of its kind. At the top level, a vision-language policy learns a latent action vocabulary from synthetically rendered kinematic demonstrations; at the low level, a reinforcement-learned WBC policy consumes these latent verbs to generate dynamics-level commands. In our benchmark, LeVERB can zero-shot attain a 80% success rate on simple visual navigation tasks, and 58.5% success rate overall, outperforming naive hierarchical whole-body VLA implementation by 7.8 times.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]whole-body control",
            "WBC",
            "sim-to-real"
          ],
          "score": 16.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA",
            "instruction following"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 25.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Eva-VLA: Evaluating Vision-Language-Action Models’ Robustness Under Real-World Physical Variations",
      "arxiv_id": "2509.18953",
      "arxiv_url": "https://arxiv.org/pdf/2509.18953",
      "summary": "Vision-Language-Action (VLA) models have emerged as promising solutions for robotic manipulation, yet their robustness to real-world physical variations remains critically underexplored. To bridge this gap, we propose Eva-VLA, the first unified framework that systematically evaluates the robustness of VLA models by transforming discrete physical variations into continuous optimization problems. However, comprehensively assessing VLA robustness presents two key challenges: (1) how to systematically characterize diverse physical variations encountered in real-world deployments while maintaining evaluation reproducibility, and (2) how to discover worst-case scenarios without prohibitive real-world data collection costs efficiently. To address the first challenge, we decompose real-world variations into three critical domains: object 3D transformations that affect spatial reasoning, illumination variations that challenge visual perception, and adversarial patches that disrupt scene understanding. For the second challenge, we introduce a continuous black-box optimization framework that transforms discrete physical variations into parameter optimization, enabling systematic exploration of worst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models across multiple benchmarks reveal alarming vulnerabilities: all variation types trigger failure rates exceeding 60%, with object transformations causing up to 97.8% failure in long-horizon tasks. Our findings expose critical gaps between controlled laboratory success and unpredictable deployment readiness, while the Eva-VLA framework provides a practical pathway for hardening VLA-based robotic manipulation models against real-world deployment challenges.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA",
            "OpenVLA"
          ],
          "score": 21.0
        }
      ],
      "relevance_score": 25.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.07] Bunny-Vision Pro: Real-Time Bimanual Dexterous Teleoperation for Imitation Learning [IL] [[project](https://dingry.github.io/projects/bunny_visionpro)]",
      "arxiv_id": "2407.03162",
      "arxiv_url": "https://arxiv.org/abs/2407.03162",
      "summary": "Teleoperation is a crucial tool for collecting human demonstrations, but controlling robots with bimanual dexterous hands remains a challenge. Existing teleoperation systems struggle to handle the complexity of coordinating two hands for intricate manipulations. We introduce Bunny-VisionPro, a real-time bimanual dexterous teleoperation system that leverages a VR headset. Unlike previous vision-based teleoperation systems, we design novel low-cost devices to provide haptic feedback to the operator, enhancing immersion. Our system prioritizes safety by incorporating collision and singularity avoidance while maintaining real-time performance through innovative designs. Bunny-VisionPro outperforms prior systems on a standard task suite, achieving higher success rates and reduced task completion times. Moreover, the high-quality teleoperation demonstrations improve downstream imitation learning performance, leading to better generalizability. Notably, Bunny-VisionPro enables imitation learning with challenging multi-stage, long-horizon dexterous manipulation tasks, which have rarely been addressed in previous work. Our system's ability to handle bimanual manipulations while prioritizing safety and real-time performance makes it a powerful tool for advancing dexterous manipulation and imitation learning.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "https://github.com/Dingry/BunnyVisionPro",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "dexterous hand",
            "dexterous manipulation",
            "[T]bi-manual",
            "bimanual manipulation",
            "[T]teleoperation"
          ],
          "score": 20.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 24.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion [[project](https://beyondmimic.github.io/)]",
      "arxiv_id": "2508.08241",
      "arxiv_url": "https://arxiv.org/abs/2508.08241",
      "summary": "The human-like form of humanoid robots positions them uniquely to achieve the agility and versatility in motor skills that humans possess. Learning from human demonstrations offers a scalable approach to acquiring these capabilities. However, prior works either produce unnatural motions or rely on motion-specific tuning to achieve satisfactory naturalness. Furthermore, these methods are often motion- or goal-specific, lacking the versatility to compose diverse skills, especially when solving unseen tasks. We present BeyondMimic, a framework that scales to diverse motions and carries the versatility to compose them seamlessly in tackling unseen downstream tasks. At heart, a compact motion-tracking formulation enables mastering a wide range of radically agile behaviors, including aerial cartwheels, spin-kicks, flip-kicks, and sprinting, with a single setup and shared hyperparameters, all while achieving state-of-the-art human-like performance. Moving beyond the mere imitation of existing motions, we propose a unified latent diffusion model that empowers versatile goal specification, seamless task switching, and dynamic composition of these agile behaviors. Leveraging classifier guidance, a diffusion-specific technique for test-time optimization toward novel objectives, our model extends its capability to solve downstream tasks never encountered during training, including motion inpainting, joystick teleoperation, and obstacle avoidance, and transfers these skills zero-shot to real hardware. This work opens new frontiers for humanoid robots by pushing the limits of scalable human-like motor skill acquisition from human motion and advancing seamless motion synthesis that achieves generalization and versatility beyond training setups.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/HybridRobotics/whole_body_tracking",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]humanoid control",
            "teleoperation"
          ],
          "score": 16.0
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "motion synthesis"
          ],
          "score": 2.5
        },
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "[T]motion tracking"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 24.5,
      "hit_pillars": [
        "1_robot_core",
        "4_motion_diffusion",
        "8_physics_animation"
      ],
      "excluded": false,
      "exclusion_keywords": [
        "inpainting"
      ]
    },
    {
      "title": "[2024] Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation",
      "arxiv_id": "2403.04436",
      "arxiv_url": "https://arxiv.org/abs/2403.04436",
      "summary": "We present Human to Humanoid (H2O), a reinforcement learning (RL) based framework that enables real-time whole-body teleoperation of a full-sized humanoid robot with only an RGB camera. To create a large-scale retargeted motion dataset of human movements for humanoid robots, we propose a scalable \"sim-to-data\" process to filter and pick feasible motions using a privileged motion imitator. Afterwards, we train a robust real-time humanoid motion imitator in simulation using these refined motions and transfer it to the real humanoid robot in a zero-shot manner. We successfully achieve teleoperation of dynamic whole-body motions in real-world scenarios, including walking, back jumping, kicking, turning, waving, pushing, boxing, etc. To the best of our knowledge, this is the first demonstration to achieve learning-based real-time whole-body humanoid teleoperation.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning, Awesome Humanoid Robot Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]teleoperation"
          ],
          "score": 14.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "[T]human-to-humanoid"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 24.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "7_retargeting"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] FlowVLA: Thinking in Motion with a Visual Chain of Thought",
      "arxiv_id": "2508.18269",
      "arxiv_url": "https://arxiv.org/pdf/2508.18269",
      "summary": "Many Vision-Language-Action (VLA) models are built upon an internal world model trained via next-frame prediction ``$v_t \\rightarrow v_{t+1}$''. However, this paradigm attempts to predict the future frame's appearance directly, without explicitly reasoning about the underlying dynamics. \\textbf{This lack of an explicit motion reasoning step} often leads to physically implausible visual forecasts and inefficient policy learning. To address this limitation, we introduce the \\textbf{Visual Chain of Thought (Visual CoT)}, a paradigm that compels the model to first reason about \\textbf{motion dynamics} before generating the future frame. We instantiate this paradigm by proposing \\textbf{FlowVLA}, an autoregressive Transformer that explicitly materializes this reasoning process as ``$v_t \\rightarrow f_t \\rightarrow v_{t+1}$'', where $f_t$ is an intermediate optical flow prediction that inherently encodes motion. By forcing the model to first follow the motion plan encoded by $f_t$, this process inherently \\textbf{aligns the pre-training objective of dynamics prediction with the downstream task of action generation.} We conduct experiments on challenging robotics manipulation benchmarks, as well as real-robot evaluations. Our FlowVLA not only generates \\textbf{more coherent and physically plausible visual predictions}, but also achieves state-of-the-art policy performance with \\textbf{substantially improved sample efficiency}, pointing toward a more principled foundation for world modeling in VLAs. Project page: https://irpn-lab.github.io/FlowVLA/",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning",
            "world model"
          ],
          "score": 3.0
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "optical flow"
          ],
          "score": 2.0
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "physically plausible"
          ],
          "score": 2.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA",
            "[T]chain-of-thought"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 24.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam",
        "4_motion_diffusion",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators",
      "arxiv_id": "2510.00406",
      "arxiv_url": "https://arxiv.org/pdf/2510.00406",
      "summary": "Vision-Language-Action (VLA) models enable embodied decision-making but rely heavily on imitation learning, leading to compounding errors and poor robustness under distribution shift. Reinforcement learning (RL) can mitigate these issues yet typically demands costly real-world interactions or suffers from sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning framework that leverages a data-driven world model as a controllable simulator. Trained from real interaction data, the simulator predicts future visual observations conditioned on actions, allowing policy rollouts with dense, trajectory-level rewards derived from goal-achieving references. This design delivers an efficient and action-aligned learning signal, drastically lowering sample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses strong supervised baselines and achieves greater efficiency than simulator-based RL. Moreover, it exhibits strong robustness under perturbed conditions, sustaining stable task execution. Our results establish world-model-based RFT as a practical post-training paradigm to enhance the generalization and robustness of VLA models. For more details, please refer to https://vla-rft.github.io/.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/OpenHelix-Team/VLA-RFT",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "sim-to-real"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "imitation learning",
            "world model"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 24.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search",
      "arxiv_id": "2509.22643",
      "arxiv_url": "https://arxiv.org/pdf/2509.22643",
      "summary": "Vision-Language-Action models (VLAs) achieve strong performance in general robotic manipulation tasks by scaling imitation learning. However, existing VLAs are limited to predicting short-sighted next-action, which struggle with long-horizon trajectory tasks due to incremental deviations. To address this problem, we propose a plug-in framework named VLA-Reasoner that effectively empowers off-the-shelf VLAs with the capability of foreseeing future states via test-time scaling. Specifically, VLA-Reasoner samples and rolls out possible action trajectories where involved actions are rationales to generate future states via a world model, which enables VLA-Reasoner to foresee and reason potential outcomes and search for the optimal actions. We further leverage Monte Carlo Tree Search (MCTS) to improve search efficiency in large action spaces, where stepwise VLA predictions seed the root. Meanwhile, we introduce a confidence sampling mechanism based on Kernel Density Estimation (KDE), to enable efficient exploration in MCTS without redundant VLA queries. We evaluate intermediate states in MCTS via an offline reward shaping strategy, to score predicted futures and correct deviations with long-term feedback. We conducted extensive experiments in both simulators and the real world, demonstrating that our proposed VLA-Reasoner achieves significant improvements over the state-of-the-art VLAs. Our method highlights a potential pathway toward scalable test-time computation of robotic manipulation.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning",
            "world model",
            "reward shaping"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 24.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [WBC] SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]whole-body control",
            "[T]WBC"
          ],
          "score": 18.0
        },
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "[T]motion tracking"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 24.0,
      "hit_pillars": [
        "1_robot_core",
        "8_physics_animation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] One Policy but Many Worlds: A Scalable Unified Policy for Versatile Humanoid Locomotion",
      "arxiv_id": "2505.18780",
      "arxiv_url": "https://arxiv.org/pdf/2505.18780",
      "summary": "Humanoid locomotion faces a critical scalability challenge: traditional reinforcement learning (RL) methods require task-specific rewards and struggle to leverage growing datasets, even as more training terrains are introduced. We propose DreamPolicy, a unified framework that enables a single policy to master diverse terrains and generalize zero-shot to unseen scenarios by systematically integrating offline data and diffusion-driven motion synthesis. At its core, DreamPolicy introduces Humanoid Motion Imagery (HMI) - future state predictions synthesized through an autoregressive terrain-aware diffusion planner curated by aggregating rollouts from specialized policies across various distinct terrains. Unlike human motion datasets requiring laborious retargeting, our data directly captures humanoid kinematics, enabling the diffusion planner to synthesize \"dreamed\" trajectories that encode terrain-specific physical constraints. These trajectories act as dynamic objectives for our HMI-conditioned policy, bypassing manual reward engineering and enabling cross-terrain generalization. DreamPolicy addresses the scalability limitations of prior methods: while traditional RL fails to exploit growing datasets, our framework scales seamlessly with more offline data. As the dataset expands, the diffusion prior learns richer locomotion skills, which the policy leverages to master new terrains without retraining. Experiments demonstrate that DreamPolicy achieves average 90% success rates in training environments and an average of 20% higher success on unseen terrains than the prevalent method. It also generalizes to perturbed and composite scenarios where prior approaches collapse. By unifying offline data, diffusion-based trajectory synthesis, and policy optimization, DreamPolicy overcomes the \"one task, one policy\" bottleneck, establishing a paradigm for scalable, data-driven humanoid control.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid control",
            "[T]humanoid locomotion",
            "[T]locomotion"
          ],
          "score": 20.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "motion synthesis"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 24.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "4_motion_diffusion"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Toward Understanding Key Estimation in Learning Robust Humanoid Locomotion",
      "arxiv_id": "2403.05868",
      "arxiv_url": "https://arxiv.org/abs/2403.05868",
      "summary": "Accurate state estimation plays a critical role in ensuring the robust control of humanoid robots, particularly in the context of learning-based control policies for legged robots. However, there is a notable gap in analytical research concerning estimations. Therefore, we endeavor to further understand how various types of estimations influence the decision-making processes of policies. In this paper, we provide quantitative insight into the effectiveness of learned state estimations, employing saliency analysis to identify key estimation variables and optimize their combination for humanoid locomotion tasks. Evaluations assessing tracking precision and robustness are conducted on comparative groups of policies with varying estimation combinations in both simulated and real-world environments. Results validated that the proposed policy is capable of crossing the sim-to-real gap and demonstrating superior performance relative to alternative policy configurations.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "legged robot",
            "[T]humanoid",
            "humanoid robot",
            "[T]humanoid locomotion",
            "[T]locomotion",
            "sim-to-real"
          ],
          "score": 24.0
        }
      ],
      "relevance_score": 24.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Reinforcement Learning for Versatile, Dynamic, and Robust Bipedal Locomotion Control",
      "arxiv_id": "2401.16889",
      "arxiv_url": "https://arxiv.org/abs/2401.16889",
      "summary": "This paper presents a comprehensive study on using deep reinforcement learning (RL) to create dynamic locomotion controllers for bipedal robots. Going beyond focusing on a single locomotion skill, we develop a general control solution that can be used for a range of dynamic bipedal skills, from periodic walking and running to aperiodic jumping and standing. Our RL-based controller incorporates a novel dual-history architecture, utilizing both a long-term and short-term input/output (I/O) history of the robot. This control architecture, when trained through the proposed end-to-end RL approach, consistently outperforms other methods across a diverse range of skills in both simulation and the real world. The study also delves into the adaptivity and robustness introduced by the proposed RL system in developing locomotion controllers. We demonstrate that the proposed architecture can adapt to both time-invariant dynamics shifts and time-variant changes, such as contact events, by effectively using the robot's I/O history. Additionally, we identify task randomization as another key source of robustness, fostering better task generalization and compliance to disturbances. The resulting control policies can be successfully deployed on Cassie, a torque-controlled human-sized bipedal robot. This work pushes the limits of agility for bipedal robots through extensive real-world experiments. We demonstrate a diverse range of locomotion skills, including: robust standing, versatile walking, fast running with a demonstration of a 400-meter dash, and a diverse set of jumping skills, such as standing long jumps and high jumps.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning, Awesome Humanoid Robot Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bipedal",
            "[T]biped",
            "[T]locomotion"
          ],
          "score": 18.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "deep reinforcement learning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 24.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[ICRA] Sim-to-Real Learning of Footstep-Constrained Bipedal Dynamic Walking.",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2015",
      "venue": "ICRA",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bipedal",
            "[T]biped",
            "[T]dynamic walking",
            "[T]sim-to-real"
          ],
          "score": 24.0
        }
      ],
      "relevance_score": 24.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[ACM GRAPH] ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "[T]simulated character",
            "[T]physically simulated character",
            "[T]ASE",
            "[T]adversarial skill embeddings"
          ],
          "score": 24.0
        }
      ],
      "relevance_score": 24.0,
      "hit_pillars": [
        "8_physics_animation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive Token Caching in Robotic Manipulation",
      "arxiv_id": "2502.02175",
      "arxiv_url": "https://arxiv.org/pdf/2502.02175",
      "summary": "Vision-Language-Action (VLA) models have demonstrated strong multi-modal reasoning capabilities, enabling direct action generation from visual perception and language instructions in an end-to-end manner. However, their substantial computational cost poses a challenge for real-time robotic control, where rapid decision-making is essential. This paper introduces VLA-Cache, a training-free inference acceleration method that reduces computational overhead by adaptively caching and reusing static visual tokens across frames. Exploiting the temporal continuity in robotic manipulation, VLA-Cache identifies minimally changed tokens between adjacent frames and reuses their cached key-value representations, thereby circumventing redundant computations. Additionally, to maintain action precision, VLA-Cache selectively re-computes task-relevant tokens that are environmentally sensitive, ensuring the fidelity of critical visual information. To further optimize efficiency, we introduce a layer adaptive token reusing strategy that dynamically adjusts the reuse ratio based on attention concentration across decoder layers, prioritizing critical tokens for recomputation. Extensive experiments on two simulation platforms (LIBERO and SIMPLER) and a real-world robotic system demonstrate that VLA-Cache achieves up to 1.7x speedup in CUDA latency and a 15% increase in control frequency, with negligible loss on task success rate. The code and videos can be found at our project page: https://vla-cache.github.io.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 24.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions",
      "arxiv_id": "2505.02152",
      "arxiv_url": "https://arxiv.org/pdf/2505.02152",
      "summary": "The rise of foundation models paves the way for generalist robot policies in the physical world. Existing methods relying on text-only instructions often struggle to generalize to unseen scenarios. We argue that interleaved image-text inputs offer richer and less biased context and enable robots to better handle unseen tasks with more versatile human-robot interaction. Building on this insight, Interleave-VLA, the first robot learning paradigm capable of comprehending interleaved image-text instructions and directly generating continuous action sequences in the physical world, is introduced. It offers a natural, flexible, and model-agnostic paradigm that extends state-of-the-art vision-language-action (VLA) models with minimal modifications while achieving strong zero-shot generalization. Interleave-VLA also includes an automatic pipeline that converts text instructions from Open X-Embodiment into interleaved image-text instructions, resulting in a large-scale real-world interleaved embodied dataset with 210k episodes. Comprehensive evaluation in simulation and the real world shows that Interleave-VLA offers two major benefits: (1) improves out-of-domain generalization to unseen objects by 2x compared to text input baselines, (2) supports flexible task interfaces and diverse instructions in a zero-shot manner, such as hand-drawn sketches. We attribute Interleave-VLA's strong zero-shot capability to the use of instruction images, which effectively mitigate hallucinations, and the inclusion of heterogeneous multimodal datasets, enriched with Internet-sourced images, offering potential for scalability. More information is available at https://interleave-vla.github.io/Interleave-VLA-Anonymous/",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "[T]VLA",
            "foundation model",
            "multimodal"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 24.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training",
      "arxiv_id": "2510.06710",
      "arxiv_url": "https://arxiv.org/pdf/2510.06710",
      "summary": "Recent progress in vision and language foundation models has significantly advanced multimodal understanding, reasoning, and generation, inspiring a surge of interest in extending such capabilities to embodied settings through vision-language-action (VLA) models. Yet, most VLA models are still trained with supervised fine-tuning (SFT), which struggles to generalize under distribution shifts due to error accumulation. Reinforcement learning (RL) offers a promising alternative by directly optimizing task performance through interaction, but existing attempts remain fragmented and lack a unified platform for fair and systematic comparison across model architectures and algorithmic designs. To address this gap, we introduce RLinf-VLA, a unified and efficient framework for scalable RL training of VLA models. The system adopts a highly flexible resource allocation design that addresses the challenge of integrating rendering, training, and inference in RL+VLA training. In particular, for GPU-parallelized simulators, RLinf-VLA implements a novel hybrid fine-grained pipeline allocation mode, achieving a 1.61x-1.88x speedup in training. Through a unified interface, RLinf-VLA seamlessly supports diverse VLA architectures (e.g., OpenVLA, OpenVLA-OFT), multiple RL algorithms (e.g., PPO, GRPO), and various simulators (e.g., ManiSkill, LIBERO). In simulation, a unified model achieves 98.11\\% across 130 LIBERO tasks and 97.66\\% across 25 ManiSkill tasks. Beyond empirical performance, our study distills a set of best practices for applying RL to VLA training and sheds light on emerging patterns in this integration. Furthermore, we present preliminary deployment on a real-world Franka robot, where RL-trained policies exhibit stronger generalization than those trained with SFT. We envision RLinf-VLA as a foundation to accelerate and standardize research on embodied intelligence.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/RLinf/RLinf",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "PPO"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "[T]VLA",
            "foundation model",
            "multimodal",
            "OpenVLA"
          ],
          "score": 21.0
        }
      ],
      "relevance_score": 24.0,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies",
      "arxiv_id": "2512.05693",
      "arxiv_url": "https://arxiv.org/abs/2512.05693",
      "summary": "The development of foundation models for embodied intelligence critically depends on access to large-scale, high-quality robot demonstration data. Recent approaches have sought to address this challenge by training on large collections of heterogeneous robotic datasets. However, unlike vision or language data, robotic demonstrations exhibit substantial heterogeneity across embodiments and action spaces as well as other prominent variations such as senor configurations and action control frequencies. The lack of explicit designs for handling such heterogeneity causes existing methods to struggle with integrating diverse factors, thereby limiting their generalization and leading to degraded performance when transferred to new settings. In this paper, we present HiMoE-VLA, a novel vision-language-action (VLA) framework tailored to effectively handle diverse robotic data with heterogeneity. Specifically, we introduce a Hierarchical Mixture-of-Experts (HiMoE) architecture for the action module which adaptively handles multiple sources of heterogeneity across layers and gradually abstracts them into shared knowledge representations. Through extensive experimentation with simulation benchmarks and real-world robotic platforms, HiMoE-VLA demonstrates a consistent performance boost over existing VLA baselines, achieving higher accuracy and robust generalization across diverse robots and action spaces. The code and models are publicly available at https://github.com/ZhiyingDu/HiMoE-VLA.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/ZhiyingDu/HiMoE-VLA",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "cross-embodiment"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA",
            "foundation model"
          ],
          "score": 21.0
        }
      ],
      "relevance_score": 24.0,
      "hit_pillars": [
        "7_retargeting",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos",
      "arxiv_id": "2510.21571",
      "arxiv_url": "https://arxiv.org/pdf/2510.21571",
      "summary": "This paper presents a novel approach for pretraining robotic manipulation Vision-Language-Action (VLA) models using a large corpus of unscripted real-life video recordings of human hand activities. Treating human hand as dexterous robot end-effector, we show that \"in-the-wild\" egocentric human videos without any annotations can be transformed into data formats fully aligned with existing robotic V-L-A training data in terms of task granularity and labels. This is achieved by the development of a fully-automated holistic human activity analysis approach for arbitrary human hand videos. This approach can generate atomic-level hand activity segments and their language descriptions, each accompanied with framewise 3D hand motion and camera motion. We process a large volume of egocentric videos and create a hand-VLA training dataset containing 1M episodes and 26M frames. This training data covers a wide range of objects and concepts, dexterous manipulation tasks, and environment variations in real life, vastly exceeding the coverage of existing robot data. We design a dexterous hand VLA model architecture and pretrain the model on this dataset. The model exhibits strong zero-shot capabilities on completely unseen real-world observations. Additionally, fine-tuning it on a small amount of real robot action data significantly improves task success rates and generalization to novel objects in real robotic experiments. We also demonstrate the appealing scaling behavior of the model's task performance with respect to pretraining data scale. We believe this work lays a solid foundation for scalable VLA pretraining, advancing robots toward truly generalizable embodied intelligence.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/microsoft/VITRA/",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "dexterous hand",
            "dexterous manipulation"
          ],
          "score": 10.0
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "egocentric"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 24.0,
      "hit_pillars": [
        "1_robot_core",
        "6_video_extraction",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] QUAR-VLA: Vision-Language-Action Model for Quadruped Robots",
      "arxiv_id": "2312.14457",
      "arxiv_url": "https://arxiv.org/pdf/2312.14457",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]quadruped"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 24.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] BeamDojo: Learning Agile Humanoid Locomotion on Sparse Footholds",
      "arxiv_id": "2502.10363",
      "arxiv_url": "https://arxiv.org/abs/2502.10363",
      "summary": "Traversing risky terrains with sparse footholds poses a significant challenge for humanoid robots, requiring precise foot placements and stable locomotion. Existing learning-based approaches often struggle on such complex terrains due to sparse foothold rewards and inefficient learning processes. To address these challenges, we introduce BeamDojo, a reinforcement learning (RL) framework designed for enabling agile humanoid locomotion on sparse footholds. BeamDojo begins by introducing a sampling-based foothold reward tailored for polygonal feet, along with a double critic to balancing the learning process between dense locomotion rewards and sparse foothold rewards. To encourage sufficient trial-and-error exploration, BeamDojo incorporates a two-stage RL approach: the first stage relaxes the terrain dynamics by training the humanoid on flat terrain while providing it with task-terrain perceptive observations, and the second stage fine-tunes the policy on the actual task terrain. Moreover, we implement a onboard LiDAR-based elevation map to enable real-world deployment. Extensive simulation and real-world experiments demonstrate that BeamDojo achieves efficient learning in simulation and enables agile locomotion with precise foot placement on sparse footholds in the real world, maintaining a high success rate even under significant external disturbances.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]humanoid locomotion",
            "[T]locomotion"
          ],
          "score": 20.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "elevation map"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 23.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Learning Generic and Dynamic Locomotion of Humanoids Across Discrete Terrains",
      "arxiv_id": "2405.17227",
      "arxiv_url": "https://arxiv.org/abs/2405.17227",
      "summary": "This paper addresses the challenge of terrain-adaptive dynamic locomotion in humanoid robots, a problem traditionally tackled by optimization-based methods or reinforcement learning (RL). Optimization-based methods, such as model-predictive control, excel in finding optimal reaction forces and achieving agile locomotion, especially in quadruped, but struggle with the nonlinear hybrid dynamics of legged systems and the real-time computation of step location, timing, and reaction forces. Conversely, RL-based methods show promise in navigating dynamic and rough terrains but are limited by their extensive data requirements. We introduce a novel locomotion architecture that integrates a neural network policy, trained through RL in simplified environments, with a state-of-the-art motion controller combining model-predictive control (MPC) and whole-body impulse control (WBIC). The policy efficiently learns high-level locomotion strategies, such as gait selection and step positioning, without the need for full dynamics simulations. This control architecture enables humanoid robots to dynamically navigate discrete terrains, making strategic locomotion decisions (e.g., walking, jumping, and leaping) based on ground height maps. Our results demonstrate that this integrated control architecture achieves dynamic locomotion with significantly fewer training samples than conventional RL-based methods and can be transferred to different humanoid platforms without additional training. The control architecture has been extensively tested in dynamic simulations, accomplishing terrain height-based dynamic locomotion for three different robots.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped",
            "[T]humanoid",
            "humanoid robot",
            "[T]locomotion",
            "MPC"
          ],
          "score": 18.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "height map"
          ],
          "score": 2.0
        },
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "PULSE"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 23.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam",
        "8_physics_animation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning",
      "arxiv_id": "2509.09674",
      "arxiv_url": "https://arxiv.org/pdf/2509.09674",
      "summary": "Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $π_0$ on RoboTwin 1.0\\&amp;2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/PRIME-RL/SimpleVLA-RL",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "AMP"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "[T]VLA",
            "OpenVLA"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 23.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "8_physics_animation",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control",
      "arxiv_id": "2505.03738",
      "arxiv_url": "https://arxiv.org/pdf/2505.03738",
      "summary": "Humanoid robots derive much of their dexterity from hyper-dexterous whole-body movements, enabling tasks that require a large operational workspace: such as picking objects off the ground. However, achieving these capabilities on real humanoids remains challenging due to their high degrees of freedom (DoF) and nonlinear dynamics. We propose Adaptive Motion Optimization (AMO), a framework that integrates sim-to-real reinforcement learning (RL) with trajectory optimization for real-time, adaptive whole-body control. To mitigate distribution bias in motion imitation RL, we construct a hybrid AMO dataset and train a network capable of robust, on-demand adaptation to potentially O.O.D. commands. We validate AMO in simulation and on a 29-DoF Unitree G1 humanoid robot, demonstrating superior stability and an expanded workspace compared to strong baselines. Finally, we show that AMO's consistent performance supports autonomous task execution via imitation learning, underscoring the system's versatility and robustness.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]whole-body control",
            "sim-to-real",
            "trajectory optimization",
            "Unitree"
          ],
          "score": 20.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "imitation learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 23.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Embrace Collisions: Humanoid Shadowing for Deployable Contact-Agnostics Motions",
      "arxiv_id": "2502.01465",
      "arxiv_url": "https://arxiv.org/abs/2502.01465",
      "summary": "Previous humanoid robot research works treat the robot as a bipedal mobile manipulation platform, where only the feet and hands contact the environment. However, we humans use all body parts to interact with the world, e.g., we sit in chairs, get up from the ground, or roll on the floor. Contacting the environment using body parts other than feet and hands brings significant challenges in both model-predictive control and reinforcement learning-based methods. An unpredictable contact sequence makes it almost impossible for model-predictive control to plan ahead in real time. The success of the zero-shot sim-to-real reinforcement learning method for humanoids heavily depends on the acceleration of GPU-based rigid-body physical simulator and simplification of the collision detection. Lacking extreme torso movement of the humanoid research makes all other components non-trivial to design, such as termination conditions, motion commands and reward designs. To address these potential challenges, we propose a general humanoid motion framework that takes discrete motion commands and controls the robot's motor action in real time. Using a GPU-accelerated rigid-body simulator, we train a humanoid whole-body control policy that follows the high-level motion command in the real world in real time, even with stochastic contacts and extremely large robot base rotation and not-so-feasible motion command. More details at https://project-instinct.github.io",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "bipedal",
            "biped",
            "whole-body control",
            "manipulation",
            "mobile manipulation",
            "sim-to-real"
          ],
          "score": 20.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "reward design"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 23.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] UniVLA: Unified Vision-Language-Action Model",
      "arxiv_id": "2506.19850",
      "arxiv_url": "https://arxiv.org/abs/2506.19850",
      "summary": "Vision-language-action models (VLAs) have garnered significant attention for their potential in advancing robotic manipulation. However, previous approaches predominantly rely on the general comprehension capabilities of vision-language models (VLMs) to generate action signals, often overlooking the rich temporal and causal structure embedded in visual observations. In this paper, we present UniVLA, a unified and native multimodal VLA model that autoregressively models vision, language, and action signals as discrete token sequences. This formulation enables flexible multimodal tasks learning, particularly from large-scale video data. By incorporating world modeling during post-training, UniVLA captures causal dynamics from videos, facilitating effective transfer to downstream policy learning--especially for long-horizon tasks. Our approach sets new state-of-the-art results across several widely used simulation benchmarks, including CALVIN, LIBERO, and Simplenv-Bridge, significantly surpassing previous methods. For example, UniVLA achieves 95.5% average success rate on LIBERO benchmark, surpassing pi0-FAST's 85.5%. We further demonstrate its broad applicability on real-world ALOHA manipulation and autonomous driving.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/baaivision/UniVLA",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning",
            "world model"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "multimodal",
            "Aloha"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 23.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": [
        "autonomous driving"
      ]
    },
    {
      "title": "[2025] [RSS 25] Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success",
      "arxiv_id": "2502.19645",
      "arxiv_url": "https://arxiv.org/pdf/2502.19645",
      "summary": "Recent vision-language-action models (VLAs) build upon pretrained vision-language models and leverage diverse robot datasets to demonstrate strong task execution, language following ability, and semantic generalization. Despite these successes, VLAs struggle with novel robot setups and require fine-tuning to achieve good performance, yet how to most effectively fine-tune them is unclear given many possible strategies. In this work, we study key VLA adaptation design choices such as different action decoding schemes, action representations, and learning objectives for fine-tuning, using OpenVLA as our representative base model. Our empirical analysis informs an Optimized Fine-Tuning (OFT) recipe that integrates parallel decoding, action chunking, a continuous action representation, and a simple L1 regression-based learning objective to altogether improve inference efficiency, policy performance, and flexibility in the model's input-output specifications. We propose OpenVLA-OFT, an instantiation of this recipe, which sets a new state of the art on the LIBERO simulation benchmark, significantly boosting OpenVLA's average success rate across four task suites from 76.5% to 97.1% while increasing action generation throughput by 26$\\times$. In real-world evaluations, our fine-tuning recipe enables OpenVLA to successfully execute dexterous, high-frequency control tasks on a bimanual ALOHA robot and outperform other VLAs ($π_0$ and RDT-1B) fine-tuned using their default recipes, as well as strong imitation learning policies trained from scratch (Diffusion Policy and ACT) by up to 15% (absolute) in average success rate. We release code for OFT and pretrained model checkpoints at https://openvla-oft.github.io/.",
      "authors": [],
      "year": "2025",
      "venue": "RSS",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "bi-manual"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning",
            "diffusion policy"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "OpenVLA",
            "Aloha"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 23.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] VTLA: Vision-Tactile-Language-Action Model with Preference Learning for Insertion Manipulation",
      "arxiv_id": "2505.09577",
      "arxiv_url": "https://arxiv.org/pdf/2505.09577",
      "summary": "While vision-language models have advanced significantly, their application in language-conditioned robotic manipulation is still underexplored, especially for contact-rich tasks that extend beyond visually dominant pick-and-place scenarios. To bridge this gap, we introduce Vision-Tactile-Language-Action model, a novel framework that enables robust policy generation in contact-intensive scenarios by effectively integrating visual and tactile inputs through cross-modal language grounding. A low-cost, multi-modal dataset has been constructed in a simulation environment, containing vision-tactile-action-instruction pairs specifically designed for the fingertip insertion task. Furthermore, we introduce Direct Preference Optimization (DPO) to offer regression-like supervision for the VTLA model, effectively bridging the gap between classification-based next token prediction loss and continuous robotic tasks. Experimental results show that the VTLA model outperforms traditional imitation learning methods (e.g., diffusion policies) and existing multi-modal baselines (TLA/VLA), achieving over 90% success rates on unseen peg shapes. Finally, we conduct real-world peg-in-hole experiments to demonstrate the exceptional Sim2Real performance of the proposed VTLA model. For supplementary videos and results, please visit our project website: https://sites.google.com/view/vtla",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "sim2real"
          ],
          "score": 8.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning",
            "[T]preference learning",
            "DPO",
            "direct preference optimization"
          ],
          "score": 9.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "VLA",
            "language conditioned"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 23.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation",
      "arxiv_id": "2505.22159",
      "arxiv_url": "https://arxiv.org/pdf/2505.22159",
      "summary": "Vision-Language-Action (VLA) models have advanced general-purpose robotic manipulation by leveraging pretrained visual and linguistic representations. However, they struggle with contact-rich tasks that require fine-grained control involving force, especially under visual occlusion or dynamic uncertainty. To address these limitations, we propose ForceVLA, a novel end-to-end manipulation framework that treats external force sensing as a first-class modality within VLA systems. ForceVLA introduces FVLMoE, a force-aware Mixture-of-Experts fusion module that dynamically integrates pretrained visual-language embeddings with real-time 6-axis force feedback during action decoding. This enables context-aware routing across modality-specific experts, enhancing the robot's ability to adapt to subtle contact dynamics. We also introduce \\textbf{ForceVLA-Data}, a new dataset comprising synchronized vision, proprioception, and force-torque signals across five contact-rich manipulation tasks. ForceVLA improves average task success by 23.2% over strong pi_0-based baselines, achieving up to 80% success in tasks such as plug insertion. Our approach highlights the importance of multimodal integration for dexterous manipulation and sets a new benchmark for physically intelligent robotic control. Code and data will be released at https://sites.google.com/view/forcevla2025.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "dexterous manipulation"
          ],
          "score": 8.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "[T]VLA",
            "multimodal"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 23.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models",
      "arxiv_id": "2508.19257",
      "arxiv_url": "https://arxiv.org/pdf/2508.19257",
      "summary": "Vision-Language-Action (VLA) models process visual inputs independently at each timestep, discarding valuable temporal information inherent in robotic manipulation tasks. This frame-by-frame processing makes models vulnerable to visual noise while ignoring the substantial coherence between consecutive frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a training-free approach that intelligently integrates historical and current visual representations to enhance VLA inference quality. Our method employs dual-dimension detection combining efficient grayscale pixel difference analysis with attention-based semantic relevance assessment, enabling selective temporal token fusion through hard fusion strategies and keyframe anchoring to prevent error accumulation. Comprehensive experiments across LIBERO, SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0 percentage points average on LIBERO (72.4\\% vs 68.4\\% baseline), cross-environment validation on SimplerEnv (4.8\\% relative improvement), and 8.7\\% relative improvement on real robot tasks. Our approach proves model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably, TTF reveals that selective Query matrix reuse in attention mechanisms enhances rather than compromises performance, suggesting promising directions for direct KQV matrix reuse strategies that achieve computational acceleration while improving task success rates.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA",
            "OpenVLA"
          ],
          "score": 21.0
        }
      ],
      "relevance_score": 23.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.10] Learning Diverse Bimanual Dexterous Manipulation Skills From Human [IL] [[project](https://sites.google.com/view/bidexhd)]",
      "arxiv_id": "2410.02477",
      "arxiv_url": "https://arxiv.org/abs/2410.02477",
      "summary": "Bimanual dexterous manipulation is a critical yet underexplored area in robotics. Its high-dimensional action space and inherent task complexity present significant challenges for policy learning, and the limited task diversity in existing benchmarks hinders general-purpose skill development. Existing approaches largely depend on reinforcement learning, often constrained by intricately designed reward functions tailored to a narrow set of tasks. In this work, we present a novel approach for efficiently learning diverse bimanual dexterous skills from abundant human demonstrations. Specifically, we introduce BiDexHD, a framework that unifies task construction from existing bimanual datasets and employs teacher-student policy learning to address all tasks. The teacher learns state-based policies using a general two-stage reward function across tasks with shared behaviors, while the student distills the learned multi-task policies into a vision-based policy. With BiDexHD, scalable learning of numerous bimanual dexterous skills from auto-constructed tasks becomes feasible, offering promising advances toward universal bimanual dexterous manipulation. Our empirical evaluation on the TACO dataset, spanning 141 tasks across six categories, demonstrates a task fulfillment rate of 74.59% on trained tasks and 51.07% on unseen tasks, showcasing the effectiveness and competitive zero-shot generalization capabilities of BiDexHD. For videos and more information, visit our project page https://sites.google.com/view/bidexhd.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]dexterous manipulation",
            "[T]bi-manual"
          ],
          "score": 18.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "policy learning",
            "teacher-student"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 22.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.07] VoxAct-B: Voxel-Based Acting and Stabilizing Policy for Bimanual Manipulation [IL] [VLM] [[project](https://github.com/VoxAct-B/voxactb)]",
      "arxiv_id": "2407.04152",
      "arxiv_url": "https://arxiv.org/abs/2407.04152",
      "summary": "Bimanual manipulation is critical to many robotics applications. In contrast to single-arm manipulation, bimanual manipulation tasks are challenging due to higher-dimensional action spaces. Prior works leverage large amounts of data and primitive actions to address this problem, but may suffer from sample inefficiency and limited generalization across various tasks. To this end, we propose VoxAct-B, a language-conditioned, voxel-based method that leverages Vision Language Models (VLMs) to prioritize key regions within the scene and reconstruct a voxel grid. We provide this voxel grid to our bimanual manipulation policy to learn acting and stabilizing actions. This approach enables more efficient policy learning from voxels and is generalizable to different tasks. In simulation, we show that VoxAct-B outperforms strong baselines on fine-grained bimanual manipulation tasks. Furthermore, we demonstrate VoxAct-B on real-world $\\texttt{Open Drawer}$ and $\\texttt{Open Jar}$ tasks using two UR5s. Code, data, and videos are available at https://voxact-b.github.io.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "https://github.com/VoxAct-B/voxactb",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]bi-manual",
            "[T]bimanual manipulation"
          ],
          "score": 18.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "language conditioned"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 22.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Physically Consistent Humanoid Loco-Manipulation using Latent Diffusion Models",
      "arxiv_id": "2504.16843",
      "arxiv_url": "https://arxiv.org/pdf/2504.16843",
      "summary": "This paper uses the capabilities of latent diffusion models (LDMs) to generate realistic RGB human-object interaction scenes to guide humanoid loco-manipulation planning. To do so, we extract from the generated images both the contact locations and robot configurations that are then used inside a whole-body trajectory optimization (TO) formulation to generate physically consistent trajectories for humanoids. We validate our full pipeline in simulation for different long-horizon loco-manipulation scenarios and perform an extensive analysis of the proposed contact and robot configuration extraction pipeline. Our results show that using the information extracted from LDMs, we can generate physically consistent trajectories that require long-horizon reasoning.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning, Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]manipulation",
            "[T]loco-manipulation",
            "trajectory optimization"
          ],
          "score": 20.0
        },
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "human-object interaction"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 22.5,
      "hit_pillars": [
        "1_robot_core",
        "5_interaction_reaction"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[RSS] Advancing Humanoid Locomotion: Mastering Challenging Terrains with Denoising World Model Learning",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "RSS",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid locomotion",
            "[T]locomotion"
          ],
          "score": 18.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 22.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[IROS] Development of a Whole-body Work Imitation Learning System by a Biped and Bi-armed Humanoid",
      "arxiv_id": "2309.15756",
      "arxiv_url": "https://arxiv.org/abs/2309.15756",
      "summary": "Imitation learning has been actively studied in recent years. In particular, skill acquisition by a robot with a fixed body, whose root link position and posture and camera angle of view do not change, has been realized in many cases. On the other hand, imitation of the behavior of robots with floating links, such as humanoid robots, is still a difficult task. In this study, we develop an imitation learning system using a biped robot with a floating link. There are two main problems in developing such a system. The first is a teleoperation device for humanoids, and the second is a control system that can withstand heavy workloads and long-term data collection. For the first point, we use the whole body control device TABLIS. It can control not only the arms but also the legs and can perform bilateral control with the robot. By connecting this TABLIS with the high-power humanoid robot JAXON, we construct a control system for imitation learning. For the second point, we will build a system that can collect long-term data based on posture optimization, and can simultaneously move the robot's limbs. We combine high-cycle posture generation with posture optimization methods, including whole-body joint torque minimization and contact force optimization. We designed an integrated system with the above two features to achieve various tasks through imitation learning. Finally, we demonstrate the effectiveness of this system by experiments of manipulating flexible fabrics such that not only the hands but also the head and waist move simultaneously, manipulating objects using legs characteristic of humanoids, and lifting heavy objects that require large forces.",
      "authors": [],
      "year": "",
      "venue": "IROS",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]biped",
            "whole-body control",
            "teleoperation"
          ],
          "score": 18.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 22.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[IEEE-RAS] Dynamic Bipedal Turning through Sim-to-Real Reinforcement Learning.",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bipedal",
            "[T]biped",
            "[T]sim-to-real"
          ],
          "score": 18.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 22.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[RSS] Blind Bipedal Stair Traversal via Sim-to-Real Reinforcement Learning.",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2021",
      "venue": "RSS",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bipedal",
            "[T]biped",
            "[T]sim-to-real"
          ],
          "score": 18.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 22.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model",
      "arxiv_id": "2510.27607",
      "arxiv_url": "https://arxiv.org/pdf/2510.27607",
      "summary": "Recently, augmenting vision-language-action models (VLAs) with world-models has shown promise in robotic policy learning. However, it remains challenging to jointly predict next-state observations and action sequences because of the inherent difference between the two modalities. To address this, we propose DUal-STream diffusion (DUST), a world-model augmented VLA framework that handles the modality conflict and enhances the performance of VLAs across diverse tasks. Specifically, we propose a multimodal diffusion transformer architecture that explicitly maintains separate modality streams while enabling cross-modal knowledge sharing. In addition, we propose training techniques such as independent noise perturbations for each modality and a decoupled flow matching loss, which enables the model to learn the joint distribution in a bidirectional manner while avoiding the need for a unified latent space. Furthermore, based on the decoupled training framework, we introduce a sampling method where we sample action and vision tokens asynchronously at different rates, which shows improvement through inference-time scaling. Through experiments on simulated benchmarks such as RoboCasa and GR-1, DUST achieves up to 6% gains over a standard VLA baseline and implicit world-modeling methods, with our inference-time scaling approach providing an additional 2-5% gain on success rate. On real-world tasks with the Franka Research 3, DUST outperforms baselines in success rate by 13%, confirming its effectiveness beyond simulation. Lastly, we demonstrate the effectiveness of DUST in large-scale pretraining with action-free videos from BridgeV2, where DUST leads to significant gain when transferred to the RoboCasa benchmark.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning",
            "flow matching",
            "[T]world model"
          ],
          "score": 7.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "multimodal"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 22.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] 3D-VLA: A 3D Vision-Language-Action Generative World Model",
      "arxiv_id": "2403.09631",
      "arxiv_url": "https://arxiv.org/pdf/2403.09631",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 22.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.03] AnySkill: Learning Open-Vocabulary Physical Skill for Interactive Agents [RL] [[project](https://anyskill.github.io/)]",
      "arxiv_id": "2403.12835",
      "arxiv_url": "https://arxiv.org/abs/2403.12835",
      "summary": "Traditional approaches in physics-based motion generation, centered around imitation learning and reward shaping, often struggle to adapt to new scenarios. To tackle this limitation, we propose AnySkill, a novel hierarchical method that learns physically plausible interactions following open-vocabulary instructions. Our approach begins by developing a set of atomic actions via a low-level controller trained via imitation learning. Upon receiving an open-vocabulary textual instruction, AnySkill employs a high-level policy that selects and integrates these atomic actions to maximize the CLIP similarity between the agent's rendered images and the text. An important feature of our method is the use of image-based rewards for the high-level policy, which allows the agent to learn interactions with objects without manual reward engineering. We demonstrate AnySkill's capability to generate realistic and natural motion sequences in response to unseen instructions of varying lengths, marking it the first method capable of open-vocabulary physical skill learning for interactive humanoid agents.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "https://github.com/jiemingcui/anyskill",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning",
            "reward shaping"
          ],
          "score": 3.0
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]open-vocabulary",
            "[T]open vocabulary"
          ],
          "score": 12.0
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "motion generation",
            "physically plausible"
          ],
          "score": 5.0
        }
      ],
      "relevance_score": 22.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam",
        "4_motion_diffusion"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] VB-Com: Learning Vision-Blind Composite Humanoid Locomotion Against Deficient Perception",
      "arxiv_id": "2502.14814",
      "arxiv_url": "https://arxiv.org/pdf/2502.14814",
      "summary": "The performance of legged locomotion is closely tied to the accuracy and comprehensiveness of state observations. Blind policies, which rely solely on proprioception, are considered highly robust due to the reliability of proprioceptive observations. However, these policies significantly limit locomotion speed and often require collisions with the terrain to adapt. In contrast, Vision policies allows the robot to plan motions in advance and respond proactively to unstructured terrains with an online perception module. However, perception is often compromised by noisy real-world environments, potential sensor failures, and the limitations of current simulations in presenting dynamic or deformable terrains. Humanoid robots, with high degrees of freedom and inherently unstable morphology, are particularly susceptible to misguidance from deficient perception, which can result in falls or termination on challenging dynamic terrains. To leverage the advantages of both vision and blind policies, we propose VB-Com, a composite framework that enables humanoid robots to determine when to rely on the vision policy and when to switch to the blind policy under perceptual deficiency. We demonstrate that VB-Com effectively enables humanoid robots to traverse challenging terrains and obstacles despite perception deficiencies caused by dynamic terrains or perceptual noise.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "legged locomotion",
            "[T]humanoid",
            "humanoid robot",
            "[T]humanoid locomotion",
            "[T]locomotion"
          ],
          "score": 22.0
        }
      ],
      "relevance_score": 22.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[CoRL] One Policy to Run Them All: an End-to-end Learning Approach to Multi-Embodiment Locomotion [XLA]",
      "arxiv_id": "2409.06366",
      "arxiv_url": "https://arxiv.org/abs/2409.06366",
      "summary": "Deep Reinforcement Learning techniques are achieving state-of-the-art results in robust legged locomotion. While there exists a wide variety of legged platforms such as quadruped, humanoids, and hexapods, the field is still missing a single learning framework that can control all these different embodiments easily and effectively and possibly transfer, zero or few-shot, to unseen robot embodiments. We introduce URMA, the Unified Robot Morphology Architecture, to close this gap. Our framework brings the end-to-end Multi-Task Reinforcement Learning approach to the realm of legged robots, enabling the learned policy to control any type of robot morphology. The key idea of our method is to allow the network to learn an abstract locomotion controller that can be seamlessly shared between embodiments thanks to our morphology-agnostic encoders and decoders. This flexible architecture can be seen as a potential first step in building a foundation model for legged robot locomotion. Our experiments show that URMA can learn a locomotion policy on multiple embodiments that can be easily transferred to unseen robot platforms in simulation and the real world.",
      "authors": [],
      "year": "",
      "venue": "CoRL",
      "code_url": "https://github.com/nico-bohlinger/one_policy_to_run_them_all",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped",
            "legged robot",
            "legged locomotion",
            "humanoid",
            "[T]locomotion",
            "locomotion policy"
          ],
          "score": 16.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "deep reinforcement learning"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 22.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control",
      "arxiv_id": "2502.05855",
      "arxiv_url": "https://arxiv.org/pdf/2502.05855",
      "summary": "Enabling robots to perform diverse tasks across varied environments is a central challenge in robot learning. While vision-language-action (VLA) models have shown promise for generalizable robot skills, realizing their full potential requires addressing limitations in action representation and efficient training. Current VLA models often focus on scaling the vision-language model (VLM) component, while the action space representation remains a critical bottleneck. This paper introduces DexVLA, a novel framework designed to enhance the efficiency and generalization capabilities of VLAs for complex, long-horizon tasks across diverse robot embodiments. DexVLA features a novel diffusion-based action expert, scaled to one billion parameters, designed for cross-embodiment learning. A novel embodiment curriculum learning strategy facilitates efficient training: (1) pre-training the diffusion expert that is separable from the VLA on cross-embodiment data, (2) aligning the VLA model to specific embodiments, and (3) post-training for rapid adaptation to new tasks. We conduct comprehensive experiments across multiple embodiments, including single-arm, bimanual, and dexterous hand, demonstrating DexVLA's adaptability to challenging tasks without task-specific adaptation, its ability to learn dexterous skills on novel embodiments with limited data, and its capacity to complete complex, long-horizon tasks using only direct language prompting, such as laundry folding. In all settings, our method demonstrates superior performance compared to state-of-the-art models like Octo, OpenVLA, and Diffusion Policy.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "dexterous hand",
            "bi-manual"
          ],
          "score": 4.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "diffusion policy",
            "curriculum learning"
          ],
          "score": 3.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "cross-embodiment"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA",
            "Octo",
            "OpenVLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 22.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "7_retargeting",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Galaxea Open-World Dataset and G0 Dual-System VLA Model",
      "arxiv_id": "2509.00576",
      "arxiv_url": "https://arxiv.org/pdf/2509.00576",
      "summary": "We present Galaxea Open-World Dataset, a large-scale, diverse collection of robot behaviors recorded in authentic human living and working environments. All demonstrations are gathered using a consistent robotic embodiment, paired with precise subtask-level language annotations to facilitate both training and evaluation. Building on this dataset, we introduce G0, a dual-system framework that couples a Vision-Language Model (VLM) for multimodal planning with a Vision-Language-Action (VLA) model for fine-grained execution. G0 is trained using a three-stage curriculum: cross-embodiment pre-training, single-embodiment pre-training, and task-specific post-training. A comprehensive benchmark spanning tabletop manipulation, few-shot learning, and long-horizon mobile manipulation, demonstrates the effectiveness of our approach. In particular, we find that the single-embodiment pre-training stage, together with the Galaxea Open-World Dataset, plays a critical role in achieving strong performance.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/OpenGalaxea/G0",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "mobile manipulation"
          ],
          "score": 4.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "cross-embodiment"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "[T]VLA",
            "multimodal"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 22.0,
      "hit_pillars": [
        "1_robot_core",
        "7_retargeting",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[Science Robotics] Attention-Based Map Encoding for Learning Generalized Legged Locomotion",
      "arxiv_id": "2506.09588",
      "arxiv_url": "https://arxiv.org/abs/2506.09588",
      "summary": "Dynamic locomotion of legged robots is a critical yet challenging topic in expanding the operational range of mobile robots. It requires precise planning when possible footholds are sparse, robustness against uncertainties and disturbances, and generalizability across diverse terrains. While traditional model-based controllers excel at planning on complex terrains, they struggle with real-world uncertainties. Learning-based controllers offer robustness to such uncertainties but often lack precision on terrains with sparse steppable areas. Hybrid methods achieve enhanced robustness on sparse terrains by combining both methods but are computationally demanding and constrained by the inherent limitations of model-based planners. To achieve generalized legged locomotion on diverse terrains while preserving the robustness of learning-based controllers, this paper proposes to learn an attention-based map encoding conditioned on robot proprioception, which is trained as part of the end-to-end controller using reinforcement learning. We show that the network learns to focus on steppable areas for future footholds when the robot dynamically navigates diverse and challenging terrains. We synthesize behaviors that exhibit robustness against uncertainties while enabling precise and agile traversal of sparse terrains. Additionally, our method offers a way to interpret the topographical perception of a neural network. We have trained two controllers for a 12-DoF quadrupedal robot and a 23-DoF humanoid robot respectively and tested the resulting controllers in the real world under various challenging indoor and outdoor scenarios, including ones unseen during training.",
      "authors": [],
      "year": "",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped",
            "legged robot",
            "[T]legged locomotion",
            "humanoid",
            "humanoid robot",
            "[T]locomotion"
          ],
          "score": 20.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 21.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Kaiwu: A Multimodal Manipulation Dataset and Framework for Robot Learning and Human-Robot Interaction",
      "arxiv_id": "2503.05231",
      "arxiv_url": "https://arxiv.org/pdf/2503.05231",
      "summary": "Cutting-edge robot learning techniques including foundation models and imitation learning from humans all pose huge demands on large-scale and high-quality datasets which constitute one of the bottleneck in the general intelligent robot fields. This paper presents the Kaiwu multimodal dataset to address the missing real-world synchronized multimodal data problems in the sophisticated assembling scenario,especially with dynamics information and its fine-grained labelling. The dataset first provides an integration of human,environment and robot data collection framework with 20 subjects and 30 interaction objects resulting in totally 11,664 instances of integrated actions. For each of the demonstration,hand motions,operation pressures,sounds of the assembling process,multi-view videos, high-precision motion capture information,eye gaze with first-person videos,electromyography signals are all recorded. Fine-grained multi-level annotation based on absolute timestamp,and semantic segmentation labelling are performed. Kaiwu dataset aims to facilitate robot learning,dexterous manipulation,human intention investigation and human-robot collaboration research.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "dexterous manipulation"
          ],
          "score": 8.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model",
            "[T]multimodal"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 21.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities",
      "arxiv_id": "2503.05652",
      "arxiv_url": "https://arxiv.org/pdf/2503.05652",
      "summary": "Real-world household tasks present significant challenges for mobile manipulation robots. An analysis of existing robotics benchmarks reveals that successful task performance hinges on three key whole-body control capabilities: bimanual coordination, stable and precise navigation, and extensive end-effector reachability. Achieving these capabilities requires careful hardware design, but the resulting system complexity further complicates visuomotor policy learning. To address these challenges, we introduce the BEHAVIOR Robot Suite (BRS), a comprehensive framework for whole-body manipulation in diverse household tasks. Built on a bimanual, wheeled robot with a 4-DoF torso, BRS integrates a cost-effective whole-body teleoperation interface for data collection and a novel algorithm for learning whole-body visuomotor policies. We evaluate BRS on five challenging household tasks that not only emphasize the three core capabilities but also introduce additional complexities, such as long-range navigation, interaction with articulated and deformable objects, and manipulation in confined spaces. We believe that BRS's integrated robotic embodiment, data collection interface, and learning framework mark a significant step toward enabling real-world whole-body manipulation for everyday household tasks. BRS is open-sourced at https://behavior-robot-suite.github.io/",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning, Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "whole-body control",
            "[T]manipulation",
            "mobile manipulation",
            "[T]whole-body manipulation",
            "bi-manual",
            "teleoperation"
          ],
          "score": 20.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 21.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Learning Humanoid Locomotion over Challenging Terrain",
      "arxiv_id": "2410.03654",
      "arxiv_url": "https://arxiv.org/pdf/2410.03654",
      "summary": "Humanoid robots can, in principle, use their legs to go almost anywhere. Developing controllers capable of traversing diverse terrains, however, remains a considerable challenge. Classical controllers are hard to generalize broadly while the learning-based methods have primarily focused on gentle terrains. Here, we present a learning-based approach for blind humanoid locomotion capable of traversing challenging natural and man-made terrain. Our method uses a transformer model to predict the next action based on the history of proprioceptive observations and actions. The model is first pre-trained on a dataset of flat-ground trajectories with sequence modeling, and then fine-tuned on uneven terrain using reinforcement learning. We evaluate our model on a real humanoid robot across a variety of terrains, including rough, deformable, and sloped surfaces. The model demonstrates robust performance, in-context adaptation, and emergent terrain representations. In real-world case studies, our humanoid robot successfully traversed over 4 miles of hiking trails in Berkeley and climbed some of the steepest streets in San Francisco.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]humanoid locomotion",
            "[T]locomotion"
          ],
          "score": 20.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 21.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2023] Learning Humanoid Locomotion with Transformers.",
      "arxiv_id": "2303.03381",
      "arxiv_url": "https://arxiv.org/pdf/2303.03381.pdf",
      "summary": "Humanoid robots that can autonomously operate in diverse environments have the potential to help address labour shortages in factories, assist elderly at homes, and colonize new planets. While classical controllers for humanoid robots have shown impressive results in a number of settings, they are challenging to generalize and adapt to new environments. Here, we present a fully learning-based approach for real-world humanoid locomotion. Our controller is a causal transformer that takes the history of proprioceptive observations and actions as input and predicts the next action. We hypothesize that the observation-action history contains useful information about the world that a powerful transformer model can use to adapt its behavior in-context, without updating its weights. We train our model with large-scale model-free reinforcement learning on an ensemble of randomized environments in simulation and deploy it to the real world zero-shot. Our controller can walk over various outdoor terrains, is robust to external disturbances, and can adapt in context.",
      "authors": [],
      "year": "2023",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning, Awesome Humanoid Robot Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]humanoid locomotion",
            "[T]locomotion"
          ],
          "score": 20.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 21.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[ICRA] Learning Task Space Actions for Bipedal Locomotion",
      "arxiv_id": "2011.04741",
      "arxiv_url": "https://arxiv.org/pdf/2011.04741.pdf",
      "summary": "Recent work has demonstrated the success of reinforcement learning (RL) for training bipedal locomotion policies for real robots. This prior work, however, has focused on learning joint-coordination controllers based on an objective of following joint trajectories produced by already available controllers. As such, it is difficult to train these approaches to achieve higher-level goals of legged locomotion, such as simply specifying the desired end-effector foot movement or ground reaction forces. In this work, we propose an approach for integrating knowledge of the robot system into RL to allow for learning at the level of task space actions in terms of feet setpoints. In particular, we integrate learning a task space policy with a model-based inverse dynamics controller, which translates task space actions into joint-level controls. With this natural action space for learning locomotion, the approach is more sample efficient and produces desired task space dynamics compared to learning purely joint space actions. We demonstrate the approach in simulation and also show that the learned policies are able to transfer to the real bipedal robot Cassie. This result encourages further research towards incorporating bipedal control techniques into the structure of the learning process to enable dynamic behaviors.",
      "authors": [],
      "year": "2011",
      "venue": "ICRA",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "legged locomotion",
            "[T]bipedal",
            "[T]biped",
            "[T]locomotion"
          ],
          "score": 20.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 21.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy",
      "arxiv_id": "2502.05450",
      "arxiv_url": "https://arxiv.org/pdf/2502.05450",
      "summary": "Vision-Language-Action (VLA) models have shown substantial potential in real-world robotic manipulation. However, fine-tuning these models through supervised learning struggles to achieve robust performance due to limited, inconsistent demonstrations, especially in contact-rich environments. In this paper, we propose a reinforced fine-tuning approach for VLA models, named ConRFT, which consists of offline and online fine-tuning with a unified consistency-based training objective, to address these challenges. In the offline stage, our method integrates behavior cloning and Q-learning to effectively extract policy from a small set of demonstrations and stabilize value estimating. In the online stage, the VLA model is further fine-tuned via consistency policy, with human interventions to ensure safe exploration and high sample efficiency. We evaluate our approach on eight diverse real-world manipulation tasks. It achieves an average success rate of 96.3% within 45-90 minutes of online fine-tuning, outperforming prior supervised methods with a 144% improvement in success rate and 1.9x shorter episode length. This work highlights the potential of integrating reinforcement learning to enhance the performance of VLA models for real-world robotic applications. Videos and code are available at our project website https://cccedric.github.io/conrft/.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "behavior cloning",
            "[T]consistency policy"
          ],
          "score": 7.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "[T]VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 21.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [RSS 25] CLIP-RT: Learning Language-Conditioned Robotic Policies from Natural Language Supervision",
      "arxiv_id": "2411.00508",
      "arxiv_url": "https://arxiv.org/abs/2411.00508",
      "summary": "Teaching robots desired skills in real-world environments remains challenging, especially for non-experts. A key bottleneck is that collecting robotic data often requires expertise or specialized hardware, limiting accessibility and scalability. We posit that natural language offers an intuitive and accessible interface for robot learning. To this end, we study two aspects: (1) enabling non-experts to collect robotic data through natural language supervision (e.g., \"move the arm to the right\") and (2) training robot policies directly from this supervision. Specifically, we introduce a data collection framework that collects robot demonstrations based on natural language supervision and further augments these demonstrations. We then present CLIP-RT, a new vision-language-action (VLA) model that learns language-conditioned visuomotor policies from this supervision. CLIP-RT adapts the pretrained CLIP model and learns to predict language-based motion primitives via contrastive imitation learning. We train CLIP-RT on the Open X-Embodiment dataset and finetune it on in-domain data collected by our framework. In real-world evaluations, CLIP-RT demonstrates strong capabilities in learning novel manipulation skills, outperforming OpenVLA (7B parameters) by 24% in average success rates, while using 7x fewer parameters (1B). We further assess CLIP-RT's capabilities in few-shot generalization and collaborative scenarios involving large pretrained models or humans. In simulated environments, CLIP-RT also yields strong performance, achieving a 93.1% average success rate on the LIBERO benchmark with an inference throughput of 163 Hz.",
      "authors": [],
      "year": "2025",
      "venue": "RSS",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA",
            "[T]language conditioned",
            "OpenVLA"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 21.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos",
      "arxiv_id": "2507.15597",
      "arxiv_url": "https://arxiv.org/pdf/2507.15597",
      "summary": "We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained on large-scale human videos. Existing VLAs struggle with complex manipulation tasks requiring high dexterity and generalize poorly to novel scenarios and tasks, primarily due to their reliance on synthetic data with significant sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To address this data bottleneck, we propose leveraging human hands as a foundation manipulator, capitalizing on the rich dexterity and scalability present in web data. Our approach centers on physical instruction tuning, a novel training paradigm that combines large-scale VLA pretraining from human videos, physical space alignment for 3D reasoning, and post-training adaptation for robotic tasks. Additionally, we introduce a part-level motion tokenization method which achieves millimeter-level reconstruction accuracy to model precise hand trajectories for action learning. To support our proposed paradigm, we further develop a comprehensive data curation pipeline that integrates heterogeneous sources -- including motion capture, VR, and RGB-only videos -- into a large-scale dataset with millions of motion-based instructional instances. We empirically show the excellence of Being-H0 in hand motion generation and instruction following, and it also scales well with model and data sizes. Importantly, we observe the expected gains of Being-H0 in real-world robotic manipulation as physical instruction tuning is applied. More details are available at https://beingbeyond.github.io/Being-H0.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/BeingBeyond/Being-H0",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "sim-to-real"
          ],
          "score": 4.0
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "motion generation"
          ],
          "score": 2.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "instruction following"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 21.5,
      "hit_pillars": [
        "1_robot_core",
        "4_motion_diffusion",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.04] Large Language Models for Orchestrating Bimanual Robots [LLM]",
      "arxiv_id": "2404.02018",
      "arxiv_url": "https://arxiv.org/abs/2404.02018",
      "summary": "Although there has been rapid progress in endowing robots with the ability to solve complex manipulation tasks, generating control policies for bimanual robots to solve tasks involving two hands is still challenging because of the difficulties in effective temporal and spatial coordination. With emergent abilities in terms of step-by-step reasoning and in-context learning, Large Language Models (LLMs) have demonstrated promising potential in a variety of robotic tasks. However, the nature of language communication via a single sequence of discrete symbols makes LLM-based coordination in continuous space a particular challenge for bimanual tasks. To tackle this challenge, we present LAnguage-model-based Bimanual ORchestration (LABOR), an agent utilizing an LLM to analyze task configurations and devise coordination control policies for addressing long-horizon bimanual tasks. We evaluate our method through simulated experiments involving two classes of long-horizon tasks using the NICOL humanoid robot. Our results demonstrate that our method outperforms the baseline in terms of success rate. Additionally, we thoroughly analyze failure cases, offering insights into LLM-based approaches in bimanual robotic control and revealing future research trends. The project website can be found at http://labor-agent.github.io.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "https://github.com/Kchu/LABOR-Agent",
      "source_repo": "Awesome Humanoid Manipulation, Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid",
            "humanoid robot",
            "manipulation",
            "[T]bi-manual"
          ],
          "score": 12.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 21.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.12] AnyBimanual: Transferring Unimanual Policy for General Bimanual Manipulation [RL]",
      "arxiv_id": "2412.06779",
      "arxiv_url": "https://arxiv.org/abs/2412.06779",
      "summary": "Performing general language-conditioned bimanual manipulation tasks is of great importance for many applications ranging from household service to industrial assembly. However, collecting bimanual manipulation data is expensive due to the high-dimensional action space, which poses challenges for conventional methods to handle general bimanual manipulation tasks. In contrast, unimanual policy has recently demonstrated impressive generalizability across a wide range of tasks because of scaled model parameters and training data, which can provide sharable manipulation knowledge for bimanual systems. To this end, we propose a plug-and-play method named AnyBimanual, which transfers pre-trained unimanual policy to general bimanual manipulation policy with few bimanual demonstrations. Specifically, we first introduce a skill manager to dynamically schedule the skill representations discovered from pre-trained unimanual policy for bimanual manipulation tasks, which linearly combines skill primitives with task-oriented compensation to represent the bimanual manipulation instruction. To mitigate the observation discrepancy between unimanual and bimanual systems, we present a visual aligner to generate soft masks for visual embedding of the workspace, which aims to align visual input of unimanual policy model for each arm with those during pretraining stage. AnyBimanual shows superiority on 12 simulated tasks from RLBench2 with a sizable 12.67% improvement in success rate over previous methods. Experiments on 9 real-world tasks further verify its practicality with an average success rate of 84.62%.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "https://github.com/TengBoYuu/AnyBimanual",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]bi-manual",
            "[T]bimanual manipulation"
          ],
          "score": 18.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "language conditioned"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 21.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.07] PerAct2: Benchmarking and Learning for Robotic Bimanual Manipulation Tasks [IL] [[project](https://bimanual.github.io/)]",
      "arxiv_id": "2407.00278",
      "arxiv_url": "https://arxiv.org/abs/2407.00278",
      "summary": "Bimanual manipulation is challenging due to precise spatial and temporal coordination required between two arms. While there exist several real-world bimanual systems, there is a lack of simulated benchmarks with a large task diversity for systematically studying bimanual capabilities across a wide range of tabletop tasks. This paper addresses the gap by extending RLBench to bimanual manipulation. We open-source our code and benchmark comprising 13 new tasks with 23 unique task variations, each requiring a high degree of coordination and adaptability. To kickstart the benchmark, we extended several state-of-the art methods to bimanual manipulation and also present a language-conditioned behavioral cloning agent -- PerAct2, which enables the learning and execution of bimanual 6-DoF manipulation tasks. Our novel network architecture efficiently integrates language processing with action prediction, allowing robots to understand and perform complex bimanual tasks in response to user-specified goals. Project website with code is available at: http://bimanual.github.io",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "https://github.com/markusgrotz/peract_bimanual",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]bi-manual",
            "[T]bimanual manipulation"
          ],
          "score": 18.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "language conditioned"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 21.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Natural Humanoid Robot Locomotion with Generative Motion Prior",
      "arxiv_id": "2503.09015",
      "arxiv_url": "https://arxiv.org/pdf/2503.09015",
      "summary": "Natural and lifelike locomotion remains a fundamental challenge for humanoid robots to interact with human society. However, previous methods either neglect motion naturalness or rely on unstable and ambiguous style rewards. In this paper, we propose a novel Generative Motion Prior (GMP) that provides fine-grained motion-level supervision for the task of natural humanoid robot locomotion. To leverage natural human motions, we first employ whole-body motion retargeting to effectively transfer them to the robot. Subsequently, we train a generative model offline to predict future natural reference motions for the robot based on a conditional variational auto-encoder. During policy training, the generative motion prior serves as a frozen online motion generator, delivering precise and comprehensive supervision at the trajectory level, including joint angles and keypoint positions. The generative motion prior significantly enhances training stability and improves interpretability by offering detailed and dense guidance throughout the learning process. Experimental results in both simulation and real-world environments demonstrate that our method achieves superior motion naturalness compared to existing approaches. Project page can be found at https://sites.google.com/view/humanoid-gmp",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot",
            "[T]locomotion"
          ],
          "score": 18.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "motion retargeting"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 21.0,
      "hit_pillars": [
        "1_robot_core",
        "7_retargeting"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Zero-Shot Whole-Body Humanoid Control via Behavioral Foundation Models [[project](https://metamotivo.metademolab.com/)] [[github](https://github.com/facebookresearch/metamotivo?tab=readme-ov-file)]",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "",
      "code_url": "https://github.com/facebookresearch/metamotivo?tab=readme-ov-file",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid control"
          ],
          "score": 12.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 21.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[Machines] Deep Reinforcement Learning for Model Predictive Controller Based on Disturbed Single Rigid Body Model of Biped Robots.",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2075",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]biped",
            "[T]model predictive control"
          ],
          "score": 12.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "[T]deep reinforcement learning"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 21.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[IEEE-RAS] Improving Sample Efficiency of Deep Reinforcement Learning for Bipedal Walking.",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "",
      "code_url": "https://github.com/rgalljamov/learn2walk",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bipedal",
            "[T]biped"
          ],
          "score": 12.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "[T]deep reinforcement learning"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 21.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[TCAS-II] Parallel Deep Reinforcement Learning Method for Gait Control of Biped Robot.",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]biped",
            "[T]gait control"
          ],
          "score": 12.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "[T]deep reinforcement learning"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 21.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[ICRA] Deepwalk: Omnidirectional bipedal gait by deep reinforcement learning.",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2021",
      "venue": "ICRA",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bipedal",
            "[T]biped"
          ],
          "score": 12.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "[T]deep reinforcement learning"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 21.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] VLAS: Vision-Language-Action Model with Speech Instructions for Customized Robot Manipulation",
      "arxiv_id": "2502.13508",
      "arxiv_url": "https://arxiv.org/pdf/2502.13508v1",
      "summary": "Vision-language-action models (VLAs) have become increasingly popular in robot manipulation for their end-to-end design and remarkable performance. However, existing VLAs rely heavily on vision-language models (VLMs) that only support text-based instructions, neglecting the more natural speech modality for human-robot interaction. Traditional speech integration methods usually involves a separate speech recognition system, which complicates the model and introduces error propagation. Moreover, the transcription procedure would lose non-semantic information in the raw speech, such as voiceprint, which may be crucial for robots to successfully complete customized tasks. To overcome above challenges, we propose VLAS, a novel end-to-end VLA that integrates speech recognition directly into the robot policy model. VLAS allows the robot to understand spoken commands through inner speech-text alignment and produces corresponding actions to fulfill the task. We also present two new datasets, SQA and CSI, to support a three-stage tuning process for speech instructions, which empowers VLAS with the ability of multimodal interaction across text, image, speech, and robot actions. Taking a step further, a voice retrieval-augmented generation (RAG) paradigm is designed to enable our model to effectively handle tasks that require individual-specific knowledge. Our extensive experiments show that VLAS can effectively accomplish robot manipulation tasks with diverse speech commands, offering a seamless and customized interaction experience.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "multimodal"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 21.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": [
        "speech recognition"
      ]
    },
    {
      "title": "[2025] VLA Model-Expert Collaboration for Bi-directional Manipulation Learning",
      "arxiv_id": "2503.04163",
      "arxiv_url": "https://arxiv.org/pdf/2503.04163",
      "summary": "The emergence of vision-language-action (VLA) models has given rise to foundation models for robot manipulation. Although these models have achieved significant improvements, their generalization in multi-task manipulation remains limited. This study proposes a VLA model-expert collaboration framework that leverages a limited number of expert actions to enhance VLA model performance. This approach reduces expert workload relative to manual operation while simultaneously improving the reliability and generalization of VLA models. Furthermore, manipulation data collected during collaboration can further refine the VLA model, while human participants concurrently enhance their skills. This bi-directional learning loop boosts the overall performance of the collaboration system. Experimental results across various VLA models demonstrate the effectiveness of the proposed system in collaborative manipulation and learning, as evidenced by improved success rates across tasks. Additionally, validation using a brain-computer interface (BCI) indicates that the collaboration system enhances the efficiency of low-speed action systems by involving VLA model during manipulation. These promising results pave the way for advancing human-robot interaction in the era of foundation models for robotics. (Project website: https://aoqunjin.github.io/Expert-VLA/)",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "[T]VLA",
            "foundation model"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 21.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Fast-in-Slow: A Dual-System Foundation Model Unifying Fast Manipulation within Slow Reasoning",
      "arxiv_id": "2506.01953",
      "arxiv_url": "https://arxiv.org/pdf/2506.01953",
      "summary": "Generalized policy and execution efficiency constitute the two critical challenges in robotic manipulation. While recent foundation policies benefit from the common-sense reasoning capabilities of internet-scale pretrained vision-language models (VLMs), they often suffer from low execution frequency. To mitigate this dilemma, dual-system approaches, inspired by Kahneman's theory, have been proposed to leverage a VLM-based System 2 model handling high-level reasoning and a separate System 1 action model ensuring real-time control. However, existing designs maintain both systems as separate models, limiting System 1 from fully leveraging the rich pretrained knowledge from the VLM-based System 2. In this work, we propose Fast-in-Slow (FiS), a unified dual-system vision-language-action (VLA) model that embeds the System 1 execution module within the VLM-based System 2 by partially sharing parameters. This innovative paradigm not only enables high-frequency execution in System 1 but also facilitates coordination between the reasoning and execution components within a single foundation model of System 2. Given their fundamentally distinct roles within FiS-VLA, we design the two systems to incorporate heterogeneous modality inputs alongside asynchronous operating frequencies, enabling both fast and precise manipulation. To enable coordination between the two systems, a dual-aware co-training strategy is proposed that equips System 1 with action generation capabilities while preserving System 2's contextual reasoning representation. For evaluation, FiS-VLA outperforms previous state-of-the-art methods by 8% in simulation and 11% in real-world tasks in terms of average success rate, while achieving a 117.7 Hz control frequency with action chunk set to eight. Project web page: fast-in-slow.github.io.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA",
            "[T]foundation model"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 21.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization",
      "arxiv_id": "2507.09160",
      "arxiv_url": "https://arxiv.org/pdf/2507.09160",
      "summary": "Vision-Language-Action (VLA) models have shown remarkable achievements, driven by the rich implicit knowledge of their vision-language components. However, achieving generalist robotic agents demands precise grounding into physical interactions, especially in contact-rich scenarios where fine-grained force control is essential. We advance VLAs' implicit knowledge beyond identifying what to do, towards guiding how to physically interact with real world. This paper introduces Tactile-VLA, a novel framework that deeply fuses vision, language, action, and tactile sensing. This framework incorporates a hybrid position-force controller to translate the model's intentions into precise physical actions and a reasoning module that allows the robot to adapt its strategy based on tactile feedback. Experiments demonstrate Tactile-VLA's effectiveness and generalizability in three key aspects: (1) enabling tactile-aware instruction following, (2) utilizing tactile-relevant commonsense, and (3) facilitating adaptive tactile-involved reasoning. A key finding is that the VLM's prior knowledge already contains semantic understanding of physical interaction; by connecting it to the robot's tactile sensors with only a few demonstrations, we can activate this prior knowledge to achieve zero-shot generalization in contact-rich tasks.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA",
            "instruction following"
          ],
          "score": 21.0
        }
      ],
      "relevance_score": 21.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning",
      "arxiv_id": "2509.05614",
      "arxiv_url": "https://arxiv.org/pdf/2509.05614",
      "summary": "Pruning accelerates compute-bound models by reducing computation. Recently applied to Vision-Language-Action (VLA) models, existing methods prune tokens using only local info from current action, ignoring global context from prior actions, causing &gt;20% success rate drop and limited speedup. We observe high similarity across consecutive actions and propose leveraging both local (current) and global (past) info for smarter token selection. We introduce SpecPrune-VLA, a training-free method with two-level pruning and heuristic control: (1) Static pruning at action level: uses global history and local context to reduce visual tokens per action; (2) Dynamic pruning at layer level: prunes tokens per layer based on layer-specific importance; (3) Lightweight action-aware controller: classifies actions as coarse/fine-grained (by speed), adjusting pruning aggressiveness since fine-grained actions are pruning-sensitive. Experiments on LIBERO show SpecPrune-VLA achieves 1.46 times speedup on NVIDIA A800 and 1.57 times on NVIDIA GeForce RTX 3090 vs. OpenVLA-OFT, with negligible success rate loss.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA",
            "OpenVLA"
          ],
          "score": 21.0
        }
      ],
      "relevance_score": 21.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] NORA-1.5: A Vision-Language-Action Model Trained using World Model and Action-based Preference Rewards",
      "arxiv_id": "2511.14659",
      "arxiv_url": "https://arxiv.org/pdf/2511.14659",
      "summary": "Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/declare-lab/nora-1.5",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "flow matching",
            "DPO",
            "direct preference optimization",
            "[T]world model"
          ],
          "score": 9.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 21.0,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Semantic Mapping in Indoor Embodied AI - A Comprehensive Survey and Future Directions",
      "arxiv_id": "2501.05750",
      "arxiv_url": "https://arxiv.org/pdf/2501.05750",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]semantic mapping",
            "[T]semantic map"
          ],
          "score": 12.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]embodied AI"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 21.0,
      "hit_pillars": [
        "3_perception_slam",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Humanoid Whole-Body Locomotion on Narrow Terrain via Dynamic Balance and Reinforcement Learning",
      "arxiv_id": "2502.17219",
      "arxiv_url": "https://arxiv.org/pdf/2502.17219",
      "summary": "Humans possess delicate dynamic balance mechanisms that enable them to maintain stability across diverse terrains and under extreme conditions. However, despite significant advances recently, existing locomotion algorithms for humanoid robots are still struggle to traverse extreme environments, especially in cases that lack external perception (e.g., vision or LiDAR). This is because current methods often rely on gait-based or perception-condition rewards, lacking effective mechanisms to handle unobservable obstacles and sudden balance loss. To address this challenge, we propose a novel whole-body locomotion algorithm based on dynamic balance and Reinforcement Learning (RL) that enables humanoid robots to traverse extreme terrains, particularly narrow pathways and unexpected obstacles, using only proprioception. Specifically, we introduce a dynamic balance mechanism by leveraging an extended measure of Zero-Moment Point (ZMP)-driven rewards and task-driven rewards in a whole-body actor-critic framework, aiming to achieve coordinated actions of the upper and lower limbs for robust locomotion. Experiments conducted on a full-sized Unitree H1-2 robot verify the ability of our method to maintain balance on extremely narrow terrains and under external disturbances, demonstrating its effectiveness in enhancing the robot's adaptability to complex environments. The videos are given at https://whole-body-loco.github.io.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]locomotion",
            "Unitree"
          ],
          "score": 16.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 20.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning",
      "arxiv_id": "2503.03480",
      "arxiv_url": "https://arxiv.org/pdf/2503.03480",
      "summary": "Vision-language-action models (VLAs) show potential as generalist robot policies. However, these models pose extreme safety challenges during real-world deployment, including the risk of harm to the environment, the robot itself, and humans. How can safety constraints be explicitly integrated into VLAs? We address this by exploring an integrated safety approach (ISA), systematically modeling safety requirements, then actively eliciting diverse unsafe behaviors, effectively constraining VLA policies via safe reinforcement learning, and rigorously assuring their safety through targeted evaluations. Leveraging the constrained Markov decision process (CMDP) paradigm, ISA optimizes VLAs from a min-max perspective against elicited safety risks. Thus, policies aligned through this comprehensive approach achieve the following key features: (I) effective safety-performance trade-offs, reducing the cumulative cost of safety violations by 83.58% compared to the state-of-the-art method, while also maintaining task success rate (+3.85%). (II) strong safety assurance, with the ability to mitigate long-tail risks and handle extreme failure scenarios. (III) robust generalization of learned safety behaviors to various out-of-distribution perturbations. The effectiveness is evaluated on long-horizon mobile manipulation tasks. Our data, models and newly proposed benchmark environment are available at https://pku-safevla.github.io.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "mobile manipulation"
          ],
          "score": 4.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 20.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.10] OKAMI: Teaching Humanoid Robots Manipulation Skills through Single Video Imitation [IL] [[project](https://ut-austin-rpl.github.io/OKAMI/)]",
      "arxiv_id": "2410.11792",
      "arxiv_url": "http://arxiv.org/abs/2410.11792",
      "summary": "We study the problem of teaching humanoid robots manipulation skills by imitating from single video demonstrations. We introduce OKAMI, a method that generates a manipulation plan from a single RGB-D video and derives a policy for execution. At the heart of our approach is object-aware retargeting, which enables the humanoid robot to mimic the human motions in an RGB-D video while adjusting to different object locations during deployment. OKAMI uses open-world vision models to identify task-relevant objects and retarget the body motions and hand poses separately. Our experiments show that OKAMI achieves strong generalizations across varying visual and spatial conditions, outperforming the state-of-the-art baseline on open-world imitation from observation. Furthermore, OKAMI rollout trajectories are leveraged to train closed-loop visuomotor policies, which achieve an average success rate of 79.2% without the need for labor-intensive teleoperation. More videos can be found on our website https://ut-austin-rpl.github.io/OKAMI/.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Manipulation, Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot",
            "[T]manipulation",
            "teleoperation"
          ],
          "score": 20.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] General Motion Tracking for Humanoid Whole-Body Control [[project](https://gmt-humanoid.github.io/)]",
      "arxiv_id": "2506.14770",
      "arxiv_url": "https://arxiv.org/abs/2506.14770",
      "summary": "The ability to track general whole-body motions in the real world is a useful way to build general-purpose humanoid robots. However, achieving this can be challenging due to the temporal and kinematic diversity of the motions, the policy's capability, and the difficulty of coordination of the upper and lower bodies. To address these issues, we propose GMT, a general and scalable motion-tracking framework that trains a single unified policy to enable humanoid robots to track diverse motions in the real world. GMT is built upon two core components: an Adaptive Sampling strategy and a Motion Mixture-of-Experts (MoE) architecture. The Adaptive Sampling automatically balances easy and difficult motions during training. The MoE ensures better specialization of different regions of the motion manifold. We show through extensive experiments in both simulation and the real world the effectiveness of GMT, achieving state-of-the-art performance across a broad spectrum of motions using a unified general policy. Videos and additional information can be found at https://gmt-humanoid.github.io.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/zixuan417/humanoid-general-motion-tracking",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]whole-body control"
          ],
          "score": 14.0
        },
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "[T]motion tracking"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "1_robot_core",
        "8_physics_animation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] HiFAR: Multi-Stage Curriculum Learning for High-Dynamics Humanoid Fall Recovery",
      "arxiv_id": "2502.20061",
      "arxiv_url": "https://arxiv.org/pdf/2502.20061",
      "summary": "Humanoid robots encounter considerable difficulties in autonomously recovering from falls, especially within dynamic and unstructured environments. Conventional control methodologies are often inadequate in addressing the complexities associated with high-dimensional dynamics and the contact-rich nature of fall recovery. Meanwhile, reinforcement learning techniques are hindered by issues related to sparse rewards, intricate collision scenarios, and discrepancies between simulation and real-world applications. In this study, we introduce a multi-stage curriculum learning framework, termed HiFAR. This framework employs a staged learning approach that progressively incorporates increasingly complex and high-dimensional recovery tasks, thereby facilitating the robot's acquisition of efficient and stable fall recovery strategies. Furthermore, it enables the robot to adapt its policy to effectively manage real-world fall incidents. We assess the efficacy of the proposed method using a real humanoid robot, showcasing its capability to autonomously recover from a diverse range of falls with high success rates, rapid recovery times, robustness, and generalization.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning, Awesome Humanoid Robot Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]fall recovery"
          ],
          "score": 14.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "[T]curriculum learning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Humanoid Locomotion as Next Token Prediction",
      "arxiv_id": "2402.19469",
      "arxiv_url": "https://arxiv.org/abs/2402.19469",
      "summary": "We cast real-world humanoid control as a next token prediction problem, akin to predicting the next word in language. Our model is a causal transformer trained via autoregressive prediction of sensorimotor trajectories. To account for the multi-modal nature of the data, we perform prediction in a modality-aligned way, and for each input token predict the next token from the same modality. This general formulation enables us to leverage data with missing modalities, like video trajectories without actions. We train our model on a collection of simulated trajectories coming from prior neural network policies, model-based controllers, motion capture data, and YouTube videos of humans. We show that our model enables a full-sized humanoid to walk in San Francisco zero-shot. Our model can transfer to the real world even when trained on only 27 hours of walking data, and can generalize to commands not seen during training like walking backward. These findings suggest a promising path toward learning challenging real-world control tasks by generative modeling of sensorimotor trajectories.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid control",
            "[T]humanoid locomotion",
            "[T]locomotion"
          ],
          "score": 20.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[ICMA] Custom Sine Waves Are Enough for Imitation Learning of Bipedal Gaits with Different Styles.",
      "arxiv_id": "2204.04157",
      "arxiv_url": "https://arxiv.org/abs/2204.04157",
      "summary": "Not until recently, robust bipedal locomotion has been achieved through reinforcement learning. However, existing implementations rely heavily on insights and efforts from human experts, which is costly for the iterative design of robot systems. Also, styles of the learned motion are strictly limited to that of the reference. In this paper, we propose a new way to learn bipedal locomotion from a simple sine wave as the reference for foot heights. With the naive human insight that the two feet should be lifted up alternatively and periodically, we experimentally demonstrate on the Cassie robot that, a simple reward function is able to make the robot learn to walk end-to-end and efficiently without any explicit knowledge of the model. With custom sine waves, the learned gait pattern can also have customized styles. Codes are released at github.com/WooQi57/sin-cassie-rl.",
      "authors": [],
      "year": "",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bipedal",
            "[T]biped",
            "locomotion"
          ],
          "score": 14.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "[T]imitation learning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
      "arxiv_id": "2509.19012",
      "arxiv_url": "https://arxiv.org/pdf/2509.19012",
      "summary": "The emergence of Vision Language Action (VLA) models marks a paradigm shift from traditional policy-based control to generalized robotics, reframing Vision Language Models (VLMs) from passive sequence generators into active agents for manipulation and decision-making in complex, dynamic environments. This survey delves into advanced VLA methods, aiming to provide a clear taxonomy and a systematic, comprehensive review of existing research. It presents a comprehensive analysis of VLA applications across different scenarios and classifies VLA approaches into several paradigms: autoregression-based, diffusion-based, reinforcement-based, hybrid, and specialized methods; while examining their motivations, core strategies, and implementations in detail. In addition, foundational datasets, benchmarks, and simulation platforms are introduced. Building on the current VLA landscape, the review further proposes perspectives on key challenges and future directions to advance research in VLA models and generalizable robotics. By synthesizing insights from over three hundred recent studies, this survey maps the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose VLA methods.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [PKU-PsiBot] A Survey on Vision-Language-Action Models: An Action Tokenization Perspective",
      "arxiv_id": "2507.01925",
      "arxiv_url": "https://arxiv.org/pdf/2507.01925",
      "summary": "The remarkable advancements of vision and language foundation models in multimodal understanding, reasoning, and generation has sparked growing efforts to extend such intelligence to the physical world, fueling the flourishing of vision-language-action (VLA) models. Despite seemingly diverse approaches, we observe that current VLA models can be unified under a single framework: vision and language inputs are processed by a series of VLA modules, producing a chain of \\textit{action tokens} that progressively encode more grounded and actionable information, ultimately generating executable actions. We further determine that the primary design choice distinguishing VLA models lies in how action tokens are formulated, which can be categorized into language description, code, affordance, trajectory, goal state, latent representation, raw action, and reasoning. However, there remains a lack of comprehensive understanding regarding action tokens, significantly impeding effective VLA development and obscuring future directions. Therefore, this survey aims to categorize and interpret existing VLA research through the lens of action tokenization, distill the strengths and limitations of each token type, and identify areas for improvement. Through this systematic review and analysis, we offer a synthesized outlook on the broader evolution of VLA models, highlight underexplored yet promising directions, and contribute guidance for future research, hoping to bring the field closer to general-purpose intelligence.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "affordance"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "foundation model",
            "multimodal"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "3_perception_slam",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [ByteDance Seed] GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation",
      "arxiv_id": "2512.01801",
      "arxiv_url": "https://arxiv.org/pdf/2512.01801",
      "summary": "We present GR-RL, a robotic learning framework that turns a generalist vision-language-action (VLA) policy into a highly capable specialist for long-horizon dexterous manipulation. Assuming the optimality of human demonstrations is core to existing VLA policies. However, we claim that in highly dexterous and precise manipulation tasks, human demonstrations are noisy and suboptimal. GR-RL proposes a multi-stage training pipeline that filters, augments, and reinforces the demonstrations by reinforcement learning. First, GR-RL learns a vision-language-conditioned task progress, filters the demonstration trajectories, and only keeps the transitions that contribute positively to the progress. Specifically, we show that by directly applying offline RL with sparse reward, the resulting $Q$-values can be treated as a robust progress function. Next, we introduce morphological symmetry augmentation that greatly improves the generalization and performance of GR-RL. Lastly, to better align the VLA policy with its deployment behaviors for high-precision control, we perform online RL by learning a latent space noise predictor. With this pipeline, GR-RL is, to our knowledge, the first learning-based policy that can autonomously lace up a shoe by threading shoelaces through multiple eyelets with an 83.3% success rate, a task requiring long-horizon reasoning, millimeter-level precision, and compliant soft-body interaction. We hope GR-RL provides a step toward enabling generalist robot foundations models to specialize into reliable real-world experts.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "dexterous manipulation"
          ],
          "score": 8.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "offline RL"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA",
            "language conditioned"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [Physical Intelligence] Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models",
      "arxiv_id": "2502.19417",
      "arxiv_url": "https://arxiv.org/pdf/2502.19417",
      "summary": "Generalist robots that can perform a range of different tasks in open-world settings must be able to not only reason about the steps needed to accomplish their goals, but also process complex instructions, prompts, and even feedback during task execution. Intricate instructions (e.g., \"Could you make me a vegetarian sandwich?\" or \"I don't like that one\") require not just the ability to physically perform the individual steps, but the ability to situate complex commands and feedback in the physical world. In this work, we describe a system that uses vision-language models in a hierarchical structure, first reasoning over complex prompts and user feedback to deduce the most appropriate next step to fulfill the task, and then performing that step with low-level actions. In contrast to direct instruction following methods that can fulfill simple commands (\"pick up the cup\"), our system can reason through complex prompts and incorporate situated feedback during task execution (\"that's not trash\"). We evaluate our system across three robotic platforms, including single-arm, dual-arm, and dual-arm mobile robots, demonstrating its ability to handle tasks such as cleaning messy tables, making sandwiches, and grocery shopping. Videos are available at https://www.pi.website/research/hirobot",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "dual-arm"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]instruction following"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Magma: A Foundation Model for Multimodal AI Agents",
      "arxiv_id": "2502.13130",
      "arxiv_url": "https://arxiv.org/pdf/2502.13130",
      "summary": "We present Magma, a foundation model that serves multimodal AI agentic tasks in both the digital and physical worlds. Magma is a significant extension of vision-language (VL) models in that it not only retains the VL understanding ability (verbal intelligence) of the latter, but is also equipped with the ability to plan and act in the visual-spatial world (spatial-temporal intelligence) and complete agentic tasks ranging from UI navigation to robot manipulation. To endow the agentic capabilities, Magma is pretrained on large amounts of heterogeneous datasets spanning from images, videos to robotics data, where the actionable visual objects (e.g., clickable buttons in GUI) in images are labeled by Set-of-Mark (SoM) for action grounding, and the object movements (e.g., the trace of human hands or robotic arms) in videos are labeled by Trace-of-Mark (ToM) for action planning. Extensive experiments show that SoM and ToM reach great synergy and facilitate the acquisition of spatial-temporal intelligence for our Magma model, which is fundamental to a wide range of tasks as shown in Fig.1. In particular, Magma creates new state-of-the-art results on UI navigation and robotic manipulation tasks, outperforming previous models that are specifically tailored to these tasks. On image and video-related multimodal tasks, Magma also compares favorably to popular large multimodal models that are trained on much larger datasets. We make our model and code public for reproducibility at https://microsoft.github.io/Magma.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model",
            "[T]multimodal"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] TriVLA: A Unified Triple-System-Based Unified Vision-Language-Action Model for General Robot Control",
      "arxiv_id": "2507.01424",
      "arxiv_url": "https://arxiv.org/pdf/2507.01424v1",
      "summary": "Recent advances in vision-language models (VLMs) have enabled robots to follow open-ended instructions and demonstrate impressive commonsense reasoning. However, current vision-language-action (VLA) frameworks primarily rely on static representations and limited temporal context, restricting agents to short-horizon, reactive behaviors and hindering robust generalization in dynamic embodied environments. Inspired by cognitive neuroscience theories of episodic memory, we propose, to our knowledge, one of the first formalized episodic world models in VLA, enabling embodied robots to accumulate, recall, and predict sequential experiences. As an instantiation of this concept, our unified TriVLA realizes the episodic world model through a triple-system architecture: integrating multimodal grounding from a pretrained VLM (System 2) and temporally rich dynamics perception from a video diffusion model (System 3). This enables the agent to accumulate and recall sequential experiences, interpret current contexts, and predict future environmental evolution. Guided by episodic representations that span both the past and anticipated future, the downstream policy (System 1) generates coherent, context-aware action sequences through flow-matching and cross-modal attention mechanisms. Experimental results show that TriVLA operates efficiently at approximately 36 Hz and consistently outperforms baseline models on standard benchmarks and challenging real-world manipulation tasks. It demonstrates strong long-horizon planning and open-ended intent understanding, showcasing the advantages of episodic world model-inspired reasoning for robust, generalizable robot intelligence. Project Page: https://zhenyangliu.github.io/TriVLA/.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "flow matching",
            "world model"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "multimodal"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [ICCV 25] VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers",
      "arxiv_id": "2507.01016",
      "arxiv_url": "https://arxiv.org/pdf/2507.01016",
      "summary": "In this paper, we introduce an innovative vector quantization based action tokenizer built upon the largest-scale action trajectory dataset to date, leveraging over 100 times more data than previous approaches. This extensive dataset enables our tokenizer to capture rich spatiotemporal dynamics, resulting in a model that not only accelerates inference but also generates smoother and more coherent action outputs. Once trained, the tokenizer can be seamlessly adapted to a wide range of downstream tasks in a zero-shot manner, from short-horizon reactive behaviors to long-horizon planning. A key finding of our work is that the domain gap between synthetic and real action trajectories is marginal, allowing us to effectively utilize a vast amount of synthetic data during training without compromising real-world performance. To validate our approach, we conducted extensive experiments in both simulated environments and on real robotic platforms. The results demonstrate that as the volume of synthetic trajectory data increases, the performance of our tokenizer on downstream tasks improves significantly-most notably, achieving up to a 30% higher success rate on two real-world tasks in long-horizon scenarios. These findings highlight the potential of our action tokenizer as a robust and scalable solution for real-time embodied intelligence systems, paving the way for more efficient and reliable robotic control in diverse application domains.Project website: https://xiaoxiao0406.github.io/vqvla.github.io",
      "authors": [],
      "year": "2025",
      "venue": "ICCV",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "spatiotemporal"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "8_physics_animation",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding",
      "arxiv_id": "2507.00416",
      "arxiv_url": "https://arxiv.org/pdf/2507.00416",
      "summary": "Vision-Language-Action (VLA) models have emerged as a promising framework for enabling generalist robots capable of perceiving, reasoning, and acting in the real world. These models usually build upon pretrained Vision-Language Models (VLMs), which excel at semantic understanding due to large-scale image and text pretraining. However, existing VLMs typically lack precise spatial understanding capabilities, as they are primarily tuned on 2D image-text pairs without 3D supervision. To address this limitation, recent approaches have incorporated explicit 3D inputs such as point clouds or depth maps, but this necessitates additional depth sensors or pre-trained depth estimation models, which may yield defective results. In contrast, our work introduces a plug-and-play module that implicitly incorporates 3D geometry features into VLA models by leveraging an off-the-shelf visual geometry foundation model. This integration provides the model with depth-aware visual representations, improving its ability to understand the geometric structure of the scene and the spatial relationships among objects from RGB images alone. We evaluate our method on a set of spatially challenging tasks in both simulation and the real world. Extensive evaluations show that our method significantly improves the performance of state-of-the-art VLA models across diverse scenarios.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "spatial relationship"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "foundation model"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "3_perception_slam",
        "7_retargeting",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation",
      "arxiv_id": "2507.01961",
      "arxiv_url": "https://arxiv.org/pdf/2507.01961",
      "summary": "Recently, mobile manipulation has attracted increasing attention for enabling language-conditioned robotic control in household tasks. However, existing methods still face challenges in coordinating mobile base and manipulator, primarily due to two limitations. On the one hand, they fail to explicitly model the influence of the mobile base on manipulator control, which easily leads to error accumulation under high degrees of freedom. On the other hand, they treat the entire mobile manipulation process with the same visual observation modality (e.g., either all 2D or all 3D), overlooking the distinct multimodal perception requirements at different stages during mobile manipulation. To address this, we propose the Adaptive Coordination Diffusion Transformer (AC-DiT), which enhances mobile base and manipulator coordination for end-to-end mobile manipulation. First, since the motion of the mobile base directly influences the manipulator's actions, we introduce a mobility-to-body conditioning mechanism that guides the model to first extract base motion representations, which are then used as context prior for predicting whole-body actions. This enables whole-body control that accounts for the potential impact of the mobile base's motion. Second, to meet the perception requirements at different stages of mobile manipulation, we design a perception-aware multimodal conditioning strategy that dynamically adjusts the fusion weights between various 2D visual images and 3D point clouds, yielding visual features tailored to the current perceptual needs. This allows the model to, for example, adaptively rely more on 2D inputs when semantic information is crucial for action prediction, while placing greater emphasis on 3D geometric information when precise spatial understanding is required. We validate AC-DiT through extensive experiments on both simulated and real-world mobile manipulation tasks.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "whole-body control",
            "[T]manipulation",
            "[T]mobile manipulation"
          ],
          "score": 14.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal",
            "language conditioned"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback",
      "arxiv_id": "2507.17294",
      "arxiv_url": "https://arxiv.org/pdf/2507.17294",
      "summary": "Tactile feedback is generally recognized to be crucial for effective interaction with the physical world. However, state-of-the-art Vision-Language-Action (VLA) models lack the ability to interpret and use tactile signals, limiting their effectiveness in contact-rich tasks. Incorporating tactile feedback into these systems is challenging due to the absence of large multi-modal datasets. We present VLA-Touch, an approach that enhances generalist robot policies with tactile sensing \\emph{without fine-tuning} the base VLA. Our method introduces two key innovations: (1) a pipeline that leverages a pretrained tactile-language model that provides semantic tactile feedback for high-level task planning, and (2) a diffusion-based controller that refines VLA-generated actions with tactile signals for contact-rich manipulation. Through real-world experiments, we demonstrate that our dual-level integration of tactile feedback improves task planning efficiency while enhancing execution precision. Code is open-sourced at \\href{https://github.com/jxbi1010/VLA-Touch}{this URL}.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/jxbi1010/VLA-Touch",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models",
      "arxiv_id": "2507.23682",
      "arxiv_url": "https://arxiv.org/pdf/2507.23682",
      "summary": "Vision-Language-Action (VLA) models have emerged as a popular paradigm for learning robot manipulation policies that can follow language instructions and generalize to novel scenarios. Recent works have begun to explore the incorporation of latent actions, abstract representations of motion between two frames, into VLA pre-training. In this paper, we introduce villa-X, a novel Vision-Language-Latent-Action (ViLLA) framework that advances latent action modeling for learning generalizable robot manipulation policies. Our approach improves both how latent actions are learned and how they are incorporated into VLA pre-training. We demonstrate that villa-X can generate latent action plans in a zero-shot fashion, even for unseen embodiments and open-vocabulary symbolic understanding. This capability enables villa-X to achieve superior performance across diverse simulation tasks in SIMPLER and on two real-world robotic setups involving both gripper and dexterous hand manipulation. These results establish villa-X as a principled and scalable paradigm for learning generalizable robot manipulation policies. We believe it provides a strong foundation for future research.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/microsoft/villa-x",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "dexterous hand"
          ],
          "score": 4.0
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "open-vocabulary",
            "open vocabulary"
          ],
          "score": 4.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction",
      "arxiv_id": "2509.04018",
      "arxiv_url": "https://arxiv.org/pdf/2509.04018",
      "summary": "Robotic manipulation is a fundamental component of automation. However, traditional perception-planning pipelines often fall short in open-ended tasks due to limited flexibility, while the architecture of a single end-to-end Vision-Language-Action (VLA) offers promising capabilities but lacks crucial mechanisms for anticipating and recovering from failure. To address these challenges, we propose FPC-VLA, a dual-model framework that integrates VLA with a supervisor for failure prediction and correction. The supervisor evaluates action viability through vision-language queries and generates corrective strategies when risks arise, trained efficiently without manual labeling. A dual-stream fusion module further refines actions by leveraging past predictions. Evaluation results on multiple simulation platforms (SIMPLER and LIBERO) and robot embodiments (WidowX, Google Robot, Franka) show that FPC-VLA outperforms state-of-the-art models in both zero-shot and fine-tuned settings. Successful real-world deployments on diverse, long-horizon tasks confirm FPC-VLA's strong generalization and practical utility for building more reliable autonomous systems.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [CoRL 25] TA-VLA: Elucidating the Design Space of Torque-aware Vision-Language-Action Models",
      "arxiv_id": "2509.07962",
      "arxiv_url": "https://arxiv.org/pdf/2509.07962",
      "summary": "Many robotic manipulation tasks require sensing and responding to force signals such as torque to assess whether the task has been successfully completed and to enable closed-loop control. However, current Vision-Language-Action (VLA) models lack the ability to integrate such subtle physical feedback. In this work, we explore Torque-aware VLA models, aiming to bridge this gap by systematically studying the design space for incorporating torque signals into existing VLA architectures. We identify and evaluate several strategies, leading to three key findings. First, introducing torque adapters into the decoder consistently outperforms inserting them into the encoder.Third, inspired by joint prediction and planning paradigms in autonomous driving, we propose predicting torque as an auxiliary output, which further improves performance. This strategy encourages the model to build a physically grounded internal representation of interaction dynamics. Extensive quantitative and qualitative experiments across contact-rich manipulation benchmarks validate our findings.",
      "authors": [],
      "year": "2025",
      "venue": "CoRL",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": [
        "autonomous driving"
      ]
    },
    {
      "title": "[2025] Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning",
      "arxiv_id": "2508.10399",
      "arxiv_url": "https://arxiv.org/pdf/2508.10399",
      "summary": "Embodied AI aims to develop intelligent systems with physical forms capable of perceiving, decision-making, acting, and learning in real-world environments, providing a promising way to Artificial General Intelligence (AGI). Despite decades of explorations, it remains challenging for embodied agents to achieve human-level intelligence for general-purpose tasks in open dynamic environments. Recent breakthroughs in large models have revolutionized embodied AI by enhancing perception, interaction, planning and learning. In this article, we provide a comprehensive survey on large model empowered embodied AI, focusing on autonomous decision-making and embodied learning. We investigate both hierarchical and end-to-end decision-making paradigms, detailing how large models enhance high-level planning, low-level execution, and feedback for hierarchical decision-making, and how large models enhance Vision-Language-Action (VLA) models for end-to-end decision making. For embodied learning, we introduce mainstream learning methodologies, elaborating on how large models enhance imitation learning and reinforcement learning in-depth. For the first time, we integrate world models into the survey of embodied AI, presenting their design methods and critical roles in enhancing decision-making and learning. Though solid advances have been achieved, challenges still exist, which are discussed at the end of this survey, potentially as the further research directions.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "imitation learning",
            "world model"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]embodied AI",
            "vision-language-action",
            "VLA"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 19.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] A0: An Affordance-Aware Hierarchical Model for General Robotic Manipulation",
      "arxiv_id": "2504.12636",
      "arxiv_url": "https://arxiv.org/pdf/2504.12636",
      "summary": "Robotic manipulation faces critical challenges in understanding spatial affordances--the \"where\" and \"how\" of object interactions--essential for complex manipulation tasks like wiping a board or stacking objects. Existing methods, including modular-based and end-to-end approaches, often lack robust spatial reasoning capabilities. Unlike recent point-based and flow-based affordance methods that focus on dense spatial representations or trajectory modeling, we propose A0, a hierarchical affordance-aware diffusion model that decomposes manipulation tasks into high-level spatial affordance understanding and low-level action execution. A0 leverages the Embodiment-Agnostic Affordance Representation, which captures object-centric spatial affordances by predicting contact points and post-contact trajectories. A0 is pre-trained on 1 million contact points data and fine-tuned on annotated trajectories, enabling generalization across platforms. Key components include Position Offset Attention for motion-aware feature extraction and a Spatial Information Aggregation Layer for precise coordinate mapping. The model's output is executed by the action execution module. Experiments on multiple robotic systems (Franka, Kinova, Realman, and Dobot) demonstrate A0's superior performance in complex tasks, showcasing its efficiency, flexibility, and real-world applicability.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]affordance"
          ],
          "score": 6.0
        },
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "[T]affordance-aware"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 19.5,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam",
        "5_interaction_reaction"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies",
      "arxiv_id": "2508.20072",
      "arxiv_url": "https://arxiv.org/pdf/2508.20072",
      "summary": "Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions into robot actions. However, prevailing VLAs either generate actions auto-regressively in a fixed left-to-right order or attach separate MLP or diffusion heads outside the backbone, leading to fragmented information pathways and specialized training requirements that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a unified-transformer policy that models discretized action chunks with discrete diffusion. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary re-masking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pre-trained vision-language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. success rates on LIBERO, 71.2% visual matching on SimplerEnv-Fractal and 54.2% overall on SimplerEnv-Bridge, improving over autoregressive, MLP decoder and continuous diffusion baselines. These findings indicate that discrete-diffusion VLA supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets. Our project page is https://github.com/Liang-ZX/DiscreteDiffusionVLA",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "transformer policy"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 19.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] 3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World Model",
      "arxiv_id": "2506.06199",
      "arxiv_url": "https://arxiv.org/pdf/2506.06199",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "[T]cross-embodiment"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 19.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "7_retargeting"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Towards Autonomous Reinforcement Learning for Real-World Robotic Manipulation with Large Language Models",
      "arxiv_id": "2503.04280",
      "arxiv_url": "https://arxiv.org/pdf/2503.04280",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 19.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.10] Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies [IL] [diffusion] [[project](https://humanoid-manipulation.github.io/)]",
      "arxiv_id": "2410.10803",
      "arxiv_url": "https://arxiv.org/abs/2410.10803",
      "summary": "Humanoid robots capable of autonomous operation in diverse environments have long been a goal for roboticists. However, autonomous manipulation by humanoid robots has largely been restricted to one specific scene, primarily due to the difficulty of acquiring generalizable skills and the expensiveness of in-the-wild humanoid robot data. In this work, we build a real-world robotic system to address this challenging problem. Our system is mainly an integration of 1) a whole-upper-body robotic teleoperation system to acquire human-like robot data, 2) a 25-DoF humanoid robot platform with a height-adjustable cart and a 3D LiDAR sensor, and 3) an improved 3D Diffusion Policy learning algorithm for humanoid robots to learn from noisy human data. We run more than 2000 episodes of policy rollouts on the real robot for rigorous policy evaluation. Empowered by this system, we show that using only data collected in one single scene and with only onboard computing, a full-sized humanoid robot can autonomously perform skills in diverse real-world scenarios. Videos are available at https://humanoid-manipulation.github.io .",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Manipulation, Awesome Humanoid Learning, Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]manipulation",
            "teleoperation"
          ],
          "score": 16.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning",
            "diffusion policy"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 19.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.11] Object-Centric Dexterous Manipulation from Human Motion Data [RL] [[project](https://cypypccpy.github.io/obj-dex.github.io/)]",
      "arxiv_id": "2411.04005",
      "arxiv_url": "https://arxiv.org/abs/2411.04005",
      "summary": "Manipulating objects to achieve desired goal states is a basic but important skill for dexterous manipulation. Human hand motions demonstrate proficient manipulation capability, providing valuable data for training robots with multi-finger hands. Despite this potential, substantial challenges arise due to the embodiment gap between human and robot hands. In this work, we introduce a hierarchical policy learning framework that uses human hand motion data for training object-centric dexterous robot manipulation. At the core of our method is a high-level trajectory generative model, learned with a large-scale human hand motion capture dataset, to synthesize human-like wrist motions conditioned on the desired object goal states. Guided by the generated wrist motions, deep reinforcement learning is further used to train a low-level finger controller that is grounded in the robot's embodiment to physically interact with the object to achieve the goal. Through extensive evaluation across 10 household objects, our approach not only demonstrates superior performance but also showcases generalization capability to novel object geometries and goal states. Furthermore, we transfer the learned policies from simulation to a real-world bimanual dexterous robot system, further demonstrating its applicability in real-world scenarios. Project website: https://cypypccpy.github.io/obj-dex.github.io/.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]dexterous manipulation",
            "bi-manual"
          ],
          "score": 14.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "deep reinforcement learning",
            "policy learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 18.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.10] ALOHA Unleashed: A Simple Recipe for Robot Dexterity [IL] [[project](https://aloha-unleashed.github.io/)]",
      "arxiv_id": "2410.13126",
      "arxiv_url": "https://arxiv.org/abs/2410.13126",
      "summary": "Recent work has shown promising results for learning end-to-end robot policies using imitation learning. In this work we address the question of how far can we push imitation learning for challenging dexterous manipulation tasks. We show that a simple recipe of large scale data collection on the ALOHA 2 platform, combined with expressive models such as Diffusion Policies, can be effective in learning challenging bimanual manipulation tasks involving deformable objects and complex contact rich dynamics. We demonstrate our recipe on 5 challenging real-world and 3 simulated tasks and demonstrate improved performance over state-of-the-art baselines. The project website and videos can be found at aloha-unleashed.github.io.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "dexterous manipulation",
            "bi-manual",
            "bimanual manipulation"
          ],
          "score": 8.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]Aloha"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 18.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [Manipulation] [Dexterous Hands] Dexterous Robotic Piano Playing at Scale",
      "arxiv_id": "2511.02504",
      "arxiv_url": "https://arxiv.org/pdf/2511.02504",
      "summary": "Endowing robot hands with human-level dexterity has been a long-standing goal in robotics. Bimanual robotic piano playing represents a particularly challenging task: it is high-dimensional, contact-rich, and requires fast, precise control. We present OmniPianist, the first agent capable of performing nearly one thousand music pieces via scalable, human-demonstration-free learning. Our approach is built on three core components. First, we introduce an automatic fingering strategy based on Optimal Transport (OT), allowing the agent to autonomously discover efficient piano-playing strategies from scratch without demonstrations. Second, we conduct large-scale Reinforcement Learning (RL) by training more than 2,000 agents, each specialized in distinct music pieces, and aggregate their experience into a dataset named RP1M++, consisting of over one million trajectories for robotic piano playing. Finally, we employ a Flow Matching Transformer to leverage RP1M++ through large-scale imitation learning, resulting in the OmniPianist agent capable of performing a wide range of musical pieces. Extensive experiments and ablation studies highlight the effectiveness and scalability of our approach, advancing dexterous robotic piano playing at scale.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]dexterous hand",
            "bi-manual"
          ],
          "score": 14.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "imitation learning",
            "flow matching"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 18.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] LiPS: Large-Scale Humanoid Robot Reinforcement Learning with Parallel-Series Structures",
      "arxiv_id": "2503.08349",
      "arxiv_url": "https://arxiv.org/abs/2503.08349",
      "summary": "In recent years, research on humanoid robots has garnered significant attention, particularly in reinforcement learning based control algorithms, which have achieved major breakthroughs. Compared to traditional model-based control algorithms, reinforcement learning based algorithms demonstrate substantial advantages in handling complex tasks. Leveraging the large-scale parallel computing capabilities of GPUs, contemporary humanoid robots can undergo extensive parallel training in simulated environments. A physical simulation platform capable of large-scale parallel training is crucial for the development of humanoid robots. As one of the most complex robot forms, humanoid robots typically possess intricate mechanical structures, encompassing numerous series and parallel mechanisms. However, many reinforcement learning based humanoid robot control algorithms currently employ open-loop topologies during training, deferring the conversion to series-parallel structures until the sim2real phase. This approach is primarily due to the limitations of physics engines, as current GPU-based physics engines often only support open-loop topologies or have limited capabilities in simulating multi-rigid-body closed-loop topologies. For enabling reinforcement learning-based humanoid robot control algorithms to train in large-scale parallel environments, we propose a novel training method LiPS. By incorporating multi-rigid-body dynamics modeling in the simulation environment, we significantly reduce the sim2real gap and the difficulty of converting to parallel structures during model deployment, thereby robustly supporting large-scale reinforcement learning for humanoid robots.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning, Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot",
            "sim2real"
          ],
          "score": 14.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 18.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Improving Vision-Language-Action Model with Online Reinforcement Learning",
      "arxiv_id": "2501.16664",
      "arxiv_url": "https://arxiv.org/pdf/2501.16664",
      "summary": "Recent studies have successfully integrated large vision-language models (VLMs) into low-level robotic control by supervised fine-tuning (SFT) with expert robotic datasets, resulting in what we term vision-language-action (VLA) models. Although the VLA models are powerful, how to improve these large models during interaction with environments remains an open question. In this paper, we explore how to further improve these VLA models via Reinforcement Learning (RL), a commonly used fine-tuning technique for large models. However, we find that directly applying online RL to large VLA models presents significant challenges, including training instability that severely impacts the performance of large models, and computing burdens that exceed the capabilities of most local machines. To address these challenges, we propose iRe-VLA framework, which iterates between Reinforcement Learning and Supervised Learning to effectively improve VLA models, leveraging the exploratory benefits of RL while maintaining the stability of supervised learning. Experiments in two simulated benchmarks and a real-world manipulation suite validate the effectiveness of our method.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 18.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models",
      "arxiv_id": "2506.17639",
      "arxiv_url": "https://arxiv.org/pdf/2506.17639",
      "summary": "Vision-Language-Action models (VLA) have demonstrated remarkable capabilities and promising potential in solving complex robotic manipulation tasks. However, their substantial parameter sizes and high inference latency pose significant challenges for real-world deployment, particularly on resource-constrained robotic platforms. To address this issue, we begin by conducting an extensive empirical study to explore the effectiveness of model compression techniques when applied to VLAs. Building on the insights gained from these preliminary experiments, we propose RLRC, a three-stage recovery method for compressed VLAs, including structured pruning, performance recovery based on SFT and RL, and further quantization. RLRC achieves up to an 8x reduction in memory usage and a 2.3x improvement in inference throughput, while maintaining or even surpassing the original VLA's task success rate. Extensive experiments show that RLRC consistently outperforms existing compression baselines, demonstrating strong potential for on-device deployment of VLAs. Project website: https://rlrc-vla.github.io",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 18.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] LLaDA-VLA: Vision Language Diffusion Action Models",
      "arxiv_id": "2509.06932",
      "arxiv_url": "https://arxiv.org/pdf/2509.06932",
      "summary": "The rapid progress of auto-regressive vision-language models (VLMs) has inspired growing interest in vision-language-action models (VLA) for robotic manipulation. Recently, masked diffusion models, a paradigm distinct from autoregressive models, have begun to demonstrate competitive performance in text generation and multimodal applications, leading to the development of a series of diffusion-based VLMs (d-VLMs). However, leveraging such models for robot policy learning remains largely unexplored. In this work, we present LLaDA-VLA, the first Vision-Language-Diffusion-Action model built upon pretrained d-VLMs for robotic manipulation. To effectively adapt d-VLMs to robotic domain, we introduce two key designs: (1) a localized special-token classification strategy that replaces full-vocabulary classification with special action token classification, reducing adaptation difficulty; (2) a hierarchical action-structured decoding strategy that decodes action sequences hierarchically considering the dependencies within and across actions. Extensive experiments demonstrate that LLaDA-VLA significantly outperforms state-of-the-art VLAs on both simulation and real-world robots.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "[T]VLA",
            "multimodal"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 18.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Hold My Beer: Learning Gentle Humanoid Locomotion and End-Effector Stabilization Control",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "",
      "code_url": "https://github.com/LeCAR-Lab/SoFTA",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid locomotion",
            "[T]locomotion"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Visual Imitation Enables Contextual Humanoid Control",
      "arxiv_id": "2505.03729",
      "arxiv_url": "https://arxiv.org/pdf/2505.03729",
      "summary": "How can we teach humanoids to climb staircases and sit on chairs using the surrounding environment context? Arguably, the simplest way is to just show them-casually capture a human motion video and feed it to humanoids. We introduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday videos, jointly reconstructs the humans and the environment, and produces whole-body control policies for humanoid robots that perform the corresponding skills. We demonstrate the results of our pipeline on real humanoid robots, showing robust, repeatable contextual control such as staircase ascents and descents, sitting and standing from chairs and benches, as well as other dynamic whole-body skills-all from a single policy, conditioned on the environment and global root commands. VIDEOMIMIC offers a scalable path towards teaching humanoids to operate in diverse real-world environments.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]humanoid control",
            "whole-body control",
            "sim-to-real"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] EmbodiedVSR: Dynamic Scene Graph-Guided Chain-of-Thought Reasoning for Visual Spatial Tasks",
      "arxiv_id": "2503.11089",
      "arxiv_url": "https://arxiv.org/pdf/2503.11089",
      "summary": "While multimodal large language models (MLLMs) have made groundbreaking progress in embodied intelligence, they still face significant challenges in spatial reasoning for complex long-horizon tasks. To address this gap, we propose EmbodiedVSR (Embodied Visual Spatial Reasoning), a novel framework that integrates dynamic scene graph-guided Chain-of-Thought (CoT) reasoning to enhance spatial understanding for embodied agents. By explicitly constructing structured knowledge representations through dynamic scene graphs, our method enables zero-shot spatial reasoning without task-specific fine-tuning. This approach not only disentangles intricate spatial relationships but also aligns reasoning steps with actionable environmental dynamics. To rigorously evaluate performance, we introduce the eSpatial-Benchmark, a comprehensive dataset including real-world embodied scenarios with fine-grained spatial annotations and adaptive task difficulty levels. Experiments demonstrate that our framework significantly outperforms existing MLLM-based methods in accuracy and reasoning coherence, particularly in long-horizon tasks requiring iterative environment interaction. The results reveal the untapped potential of MLLMs for embodied intelligence when equipped with structured, explainable reasoning mechanisms, paving the way for more reliable deployment in real-world spatial applications. The codes and datasets will be released soon.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning, Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "spatial relationship"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "multimodal",
            "[T]chain-of-thought"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "7_retargeting",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] NuExo: A Wearable Exoskeleton Covering all Upper Limb ROM for Outdoor Data Collection and Teleoperation of Humanoid Robots",
      "arxiv_id": "2503.10554",
      "arxiv_url": "https://arxiv.org/pdf/2503.10554",
      "summary": "The evolution from motion capture and teleoperation to robot skill learning has emerged as a hotspot and critical pathway for advancing embodied intelligence. However, existing systems still face a persistent gap in simultaneously achieving four objectives: accurate tracking of full upper limb movements over extended durations (Accuracy), ergonomic adaptation to human biomechanics (Comfort), versatile data collection (e.g., force data) and compatibility with humanoid robots (Versatility), and lightweight design for outdoor daily use (Convenience). We present a wearable exoskeleton system, incorporating user-friendly immersive teleoperation and multi-modal sensing collection to bridge this gap. Due to the features of a novel shoulder mechanism with synchronized linkage and timing belt transmission, this system can adapt well to compound shoulder movements and replicate 100% coverage of natural upper limb motion ranges. Weighing 5.2 kg, NuExo supports backpack-type use and can be conveniently applied in daily outdoor scenarios. Furthermore, we develop a unified intuitive teleoperation framework and a comprehensive data collection system integrating multi-modal sensing for various humanoid robots. Experiments across distinct humanoid platforms and different users validate our exoskeleton's superiority in motion range and flexibility, while confirming its stability in data collection and teleoperation accuracy in dynamic scenarios.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot",
            "[T]teleoperation"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Learning Getting-Up Policies for Real-World Humanoid Robots",
      "arxiv_id": "2502.12152",
      "arxiv_url": "https://arxiv.org/pdf/2502.12152",
      "summary": "Automatic fall recovery is a crucial prerequisite before humanoid robots can be reliably deployed. Hand-designing controllers for getting up is difficult because of the varied configurations a humanoid can end up in after a fall and the challenging terrains humanoid robots are expected to operate on. This paper develops a learning framework to produce controllers that enable humanoid robots to get up from varying configurations on varying terrains. Unlike previous successful applications of learning to humanoid locomotion, the getting-up task involves complex contact patterns (which necessitates accurately modeling of the collision geometry) and sparser rewards. We address these challenges through a two-phase approach that induces a curriculum. The first stage focuses on discovering a good getting-up trajectory under minimal constraints on smoothness or speed / torque limits. The second stage then refines the discovered motions into deployable (i.e. smooth and slow) motions that are robust to variations in initial configuration and terrains. We find these innovations enable a real-world G1 humanoid robot to get up from two main situations that we considered: a) lying face up and b) lying face down, both tested on flat, deformable, slippery surfaces and slopes (e.g., sloppy grass and snowfield). This is one of the first successful demonstrations of learned getting-up policies for human-sized humanoid robots in the real world.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot",
            "humanoid locomotion",
            "locomotion",
            "fall recovery"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Teleoperation of Humanoid Robots: A Survey",
      "arxiv_id": "2301.04317",
      "arxiv_url": "https://arxiv.org/pdf/2301.04317",
      "summary": "Teleoperation of humanoid robots enables the integration of the cognitive skills and domain expertise of humans with the physical capabilities of humanoid robots. The operational versatility of humanoid robots makes them the ideal platform for a wide range of applications when teleoperating in a remote environment. However, the complexity of humanoid robots imposes challenges for teleoperation, particularly in unstructured dynamic environments with limited communication. Many advancements have been achieved in the last decades in this area, but a comprehensive overview is still missing. This survey paper gives an extensive overview of humanoid robot teleoperation, presenting the general architecture of a teleoperation system and analyzing the different components. We also discuss different aspects of the topic, including technological and methodological advances, as well as potential applications. A web-based version of the paper can be found at https://humanoid-teleoperation.github.io/.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning, Awesome Humanoid Robot Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot",
            "[T]teleoperation"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[ICRA] Optimizing Bipedal Locomotion for The 100m Dash With Comparison to Human Running.",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "ICRA",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bipedal",
            "[T]biped",
            "[T]locomotion"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[ICRA] Benchmarking Potential Based Rewards for Learning Humanoid Locomotion.",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "ICRA",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid locomotion",
            "[T]locomotion"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[PMLR] Towards Real Robot Learning in the Wild: A Case Study in Bipedal Locomotion.",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bipedal",
            "[T]biped",
            "[T]locomotion"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[IEEE-RAL] Learning Natural Locomotion Behaviors for Humanoid Robots Using Human Bias.",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "RAL",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot",
            "[T]locomotion"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[IROS] Sim-to-Real Transfer for Biped Locomotion.",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "IROS",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]biped",
            "[T]locomotion",
            "[T]sim-to-real"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] A Survey on Vision-Language-Action Models for Autonomous Driving",
      "arxiv_id": "2506.24044",
      "arxiv_url": "https://arxiv.org/pdf/2506.24044",
      "summary": "The rapid progress of multimodal large language models (MLLM) has paved the way for Vision-Language-Action (VLA) paradigms, which integrate visual perception, natural language understanding, and control within a single policy. Researchers in autonomous driving are actively adapting these methods to the vehicle domain. Such models promise autonomous vehicles that can interpret high-level instructions, reason about complex traffic scenes, and make their own decisions. However, the literature remains fragmented and is rapidly expanding. This survey offers the first comprehensive overview of VLA for Autonomous Driving (VLA4AD). We (i) formalize the architectural building blocks shared across recent work, (ii) trace the evolution from early explainer to reasoning-centric VLA models, and (iii) compare over 20 representative models according to VLA's progress in the autonomous driving domain. We also consolidate existing datasets and benchmarks, highlighting protocols that jointly measure driving safety, accuracy, and explanation quality. Finally, we detail open challenges - robustness, real-time efficiency, and formal verification - and outline future directions of VLA4AD. This survey provides a concise yet complete reference for advancing interpretable socially aligned autonomous vehicles. Github repo is available at \\href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "large language model",
            "multimodal"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": [
        "autonomous driving"
      ]
    },
    {
      "title": "[2025] Dexterous Manipulation through Imitation Learning: A Survey",
      "arxiv_id": "2504.03515",
      "arxiv_url": "https://arxiv.org/pdf/2504.03515",
      "summary": "Dexterous manipulation, which refers to the ability of a robotic hand or multi-fingered end-effector to skillfully control, reorient, and manipulate objects through precise, coordinated finger movements and adaptive force modulation, enables complex interactions similar to human hand dexterity. With recent advances in robotics and machine learning, there is a growing demand for these systems to operate in complex and unstructured environments. Traditional model-based approaches struggle to generalize across tasks and object variations due to the high dimensionality and complex contact dynamics of dexterous manipulation. Although model-free methods such as reinforcement learning (RL) show promise, they require extensive training, large-scale interaction data, and carefully designed rewards for stability and effectiveness. Imitation learning (IL) offers an alternative by allowing robots to acquire dexterous manipulation skills directly from expert demonstrations, capturing fine-grained coordination and contact dynamics while bypassing the need for explicit modeling and large-scale trial-and-error. This survey provides an overview of dexterous manipulation methods based on imitation learning, details recent advances, and addresses key challenges in the field. Additionally, it explores potential research directions to enhance IL-driven dexterous manipulation. Our goal is to offer researchers and practitioners a comprehensive introduction to this rapidly evolving domain.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]dexterous manipulation"
          ],
          "score": 12.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "[T]imitation learning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [Toyota Research Institute] A Careful Examination of Large Behavior Models for Multitask Dexterous Manipulation",
      "arxiv_id": "2507.05331",
      "arxiv_url": "https://arxiv.org/pdf/2507.05331",
      "summary": "Robot manipulation has seen tremendous progress in recent years, with imitation learning policies enabling successful performance of dexterous and hard-to-model tasks. Concurrently, scaling data and model size has led to the development of capable language and vision foundation models, motivating large-scale efforts to create general-purpose robot foundation models. While these models have garnered significant enthusiasm and investment, meaningful evaluation of real-world performance remains a challenge, limiting both the pace of development and inhibiting a nuanced understanding of current capabilities. In this paper, we rigorously evaluate multitask robot manipulation policies, referred to as Large Behavior Models (LBMs), by extending the Diffusion Policy paradigm across a corpus of simulated and real-world robot data. We propose and validate an evaluation pipeline to rigorously analyze the capabilities of these models with statistical confidence. We compare against single-task baselines through blind, randomized trials in a controlled setting, using both simulation and real-world experiments. We find that multi-task pretraining makes the policies more successful and robust, and enables teaching complex new tasks more quickly, using a fraction of the data when compared to single-task baselines. Moreover, performance predictably increases as pretraining scale and diversity grows. Project page: https://toyotaresearchinstitute.github.io/lbm1/",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]dexterous manipulation"
          ],
          "score": 12.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning",
            "diffusion policy"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] RoboHorizon: An LLM-Assisted Multi-View World Model for Long-Horizon Robotic Manipulation",
      "arxiv_id": "2501.06605",
      "arxiv_url": "https://arxiv.org/pdf/2501.06605",
      "summary": "Efficient control in long-horizon robotic manipulation is challenging due to complex representation and policy learning requirements. Model-based visual reinforcement learning (RL) has shown great potential in addressing these challenges but still faces notable limitations, particularly in handling sparse rewards and complex visual features in long-horizon environments. To address these limitations, we propose the Recognize-Sense-Plan-Act (RSPA) pipeline for long-horizon tasks and further introduce RoboHorizon, an LLM-assisted multi-view world model tailored for long-horizon robotic manipulation. In RoboHorizon, pre-trained LLMs generate dense reward structures for multi-stage sub-tasks based on task language instructions, enabling robots to better recognize long-horizon tasks. Keyframe discovery is then integrated into the multi-view masked autoencoder (MAE) architecture to enhance the robot's ability to sense critical task sequences, strengthening its multi-stage perception of long-horizon processes. Leveraging these dense rewards and multi-view representations, a robotic world model is constructed to efficiently plan long-horizon tasks, enabling the robot to reliably act through RL algorithms. Experiments on two representative benchmarks, RLBench and FurnitureBench, show that RoboHorizon outperforms state-of-the-art visual model-based RL methods, achieving a 23.35% improvement in task success rates on RLBench's 4 short-horizon tasks and a 29.23% improvement on 6 long-horizon tasks from RLBench and 3 furniture assembly tasks from FurnitureBench.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "policy learning",
            "[T]world model",
            "model-based RL",
            "masked autoencoder",
            "MAE"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation",
      "arxiv_id": "2502.05485",
      "arxiv_url": "https://arxiv.org/pdf/2502.05485",
      "summary": "Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is the lack of robotic data, which are typically obtained through expensive on-robot operation. A promising remedy is to leverage cheaper, off-domain data such as action-free videos, hand-drawn sketches or simulation data. In this work, we posit that hierarchical vision-language-action (VLA) models can be more effective in utilizing off-domain data than standard monolithic VLA models that directly finetune vision-language models (VLMs) to predict actions. In particular, we study a class of hierarchical VLA models, where the high-level VLM is finetuned to produce a coarse 2D path indicating the desired robot end-effector trajectory given an RGB image and a task description. The intermediate 2D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Doing so alleviates the high-level VLM from fine-grained action prediction, while reducing the low-level policy's burden on complex task-level reasoning. We show that, with the hierarchical design, the high-level VLM can transfer across significant domain gaps between the off-domain finetuning data and real-robot testing scenarios, including differences on embodiments, dynamics, visual appearances and task semantics, etc. In the real-robot experiments, we observe an average of 20% improvement in success rate across seven different axes of generalization over OpenVLA, representing a 50% relative gain. Visual results, code, and dataset are provided at: https://hamster-robot.github.io/",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA",
            "foundation model",
            "OpenVLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] RoboBERT: An End-to-end Multimodal Robotic Manipulation Model",
      "arxiv_id": "2502.07837",
      "arxiv_url": "https://arxiv.org/pdf/2502.07837",
      "summary": "Embodied intelligence seamlessly integrates vision, language, and action.~However, most multimodal robotic models rely on massive fine-tuning, incurring high time and hardware costs.~To address this, we introduce RoboBERT, an end-to-end multimodal manipulation model built around a novel two-stage training paradigm.~In the first stage, we freeze most of the vision encoder and train with a single \"standard\" instruction phrasing, allowing the model to focus on stable policy learning via a CNN-based diffusion policy.~In the second stage, we unfreeze all modules and inject diverse natural language variants, rapidly aligning varied instructions to the already-learned policy without destabilizing performance.~We further employ systematic data augmentations to enhance robustness against visual perturbations.~Without relying on auxiliary datasets, RoboBERT achieves new state-of-the-art (SOTA) mean episode lengths of 4.52 on the CALVIN ABCD-D benchmark and 3.79 on the ABC-D benchmark using only language-labeled expert demonstrations and a comparatively lightweight architecture.Real-robot trials on a 6-DOF manipulator confirm higher success rates than comparable methods trained on identical data.These results demonstrate that our data-augmentation-enhanced two-stage training paradigm delivers efficient, scalable, and broadly applicable performance for multimodal robotic systems.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning",
            "diffusion policy"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation",
      "arxiv_id": "2501.05014",
      "arxiv_url": "https://arxiv.org/pdf/2501.05014",
      "summary": "The UAV-VLA (Visual-Language-Action) system is a tool designed to facilitate communication with aerial robots. By integrating satellite imagery processing with the Visual Language Model (VLM) and the powerful capabilities of GPT, UAV-VLA enables users to generate general flight paths-and-action plans through simple text requests. This system leverages the rich contextual information provided by satellite images, allowing for enhanced decision-making and mission planning. The combination of visual analysis by VLM and natural language processing by GPT can provide the user with the path-and-action set, making aerial operations more efficient and accessible. The newly developed method showed the difference in the length of the created trajectory in 22% and the mean error in finding the objects of interest on a map in 34.22 m by Euclidean distance in the K-Nearest Neighbors (KNN) approach.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": [
        "UAV",
        "satellite image"
      ]
    },
    {
      "title": "[2025] PointVLA: Injecting the 3D World into Vision-Language-Action Models",
      "arxiv_id": "2503.07511",
      "arxiv_url": "https://arxiv.org/pdf/2503.07511",
      "summary": "Vision-Language-Action (VLA) models excel at robotic tasks by leveraging large-scale 2D vision-language pretraining, but their reliance on RGB images limits spatial reasoning critical for real-world interaction. Retraining these models with 3D data is computationally prohibitive, while discarding existing 2D datasets wastes valuable resources. To bridge this gap, we propose PointVLA, a framework that enhances pre-trained VLAs with point cloud inputs without requiring retraining. Our method freezes the vanilla action expert and injects 3D features via a lightweight modular block. To identify the most effective way of integrating point cloud representations, we conduct a skip-block analysis to pinpoint less useful blocks in the vanilla action expert, ensuring that 3D features are injected only into these blocks--minimizing disruption to pre-trained representations.\n  Extensive experiments demonstrate that PointVLA outperforms state-of-the-art 2D imitation learning methods, such as OpenVLA, Diffusion Policy and DexVLA, across both simulated and real-world robotic tasks. Specifically, we highlight several key advantages of PointVLA enabled by point cloud integration: (1) Few-shot multi-tasking, where PointVLA successfully performs four different tasks using only 20 demonstrations each; (2) Real-vs-photo discrimination, where PointVLA distinguishes real objects from their images, leveraging 3D world knowledge to improve safety and reliability; (3) Height adaptability, Unlike conventional 2D imitation learning methods, PointVLA enables robots to adapt to objects at varying table height that unseen in train data. Furthermore, PointVLA achieves strong performance in long-horizon tasks, such as picking and packing objects from a moving conveyor belt, showcasing its ability to generalize across complex, dynamic environments.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning",
            "diffusion policy"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "OpenVLA"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] LUMOS: Language-Conditioned Imitation Learning with World Models",
      "arxiv_id": "2503.10370",
      "arxiv_url": "https://arxiv.org/pdf/2503.10370",
      "summary": "We introduce LUMOS, a language-conditioned multi-task imitation learning framework for robotics. LUMOS learns skills by practicing them over many long-horizon rollouts in the latent space of a learned world model and transfers these skills zero-shot to a real robot. By learning on-policy in the latent space of the learned world model, our algorithm mitigates policy-induced distribution shift which most offline imitation learning methods suffer from. LUMOS learns from unstructured play data with fewer than 1% hindsight language annotations but is steerable with language commands at test time. We achieve this coherent long-horizon performance by combining latent planning with both image- and language-based hindsight goal relabeling during training, and by optimizing an intrinsic reward defined in the latent space of the world model over multiple time steps, effectively reducing covariate shift. In experiments on the difficult long-horizon CALVIN benchmark, LUMOS outperforms prior learning-based methods with comparable approaches on chained multi-task evaluations. To the best of our knowledge, we are the first to learn a language-conditioned continuous visuomotor control for a real-world robot within an offline world model. Videos, dataset and code are available at http://lumos.cs.uni-freiburg.de.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning",
            "[T]world model"
          ],
          "score": 9.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]language conditioned"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation",
      "arxiv_id": "2505.03912",
      "arxiv_url": "https://arxiv.org/pdf/2505.03912",
      "summary": "Dual-system VLA (Vision-Language-Action) architectures have become a hot topic in embodied intelligence research, but there is a lack of sufficient open-source work for further performance analysis and optimization. To address this problem, this paper will summarize and compare the structural designs of existing dual-system architectures, and conduct systematic empirical evaluations on the core design elements of existing dual-system architectures. Ultimately, it will provide a low-cost open-source model for further exploration. Of course, this project will continue to update with more experimental conclusions and open-source models with improved performance for everyone to choose from. Project page: https://openhelix-robot.github.io/.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "[T]VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification",
      "arxiv_id": "2508.21046",
      "arxiv_url": "https://arxiv.org/pdf/2508.21046",
      "summary": "Recent Vision-Language-Action (VLA) models built on pre-trained Vision-Language Models (VLMs) require extensive post-training, resulting in high computational overhead that limits scalability and deployment.We propose CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages instruction-driven routing and sparsification to improve both efficiency and performance. CogVLA draws inspiration from human multimodal coordination and introduces a 3-stage progressive architecture. 1) Encoder-FiLM based Aggregation Routing (EFA-Routing) injects instruction information into the vision encoder to selectively aggregate and compress dual-stream visual tokens, forming a instruction-aware latent representation. 2) Building upon this compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing) introduces action intent into the language model by pruning instruction-irrelevant visually grounded tokens, thereby achieving token-level sparsity. 3) To ensure that compressed perception inputs can still support accurate and coherent action generation, we introduce V-L-A Coupled Attention (CAtten), which combines causal vision-language attention with bidirectional action parallel decoding. Extensive experiments on the LIBERO benchmark and real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art performance with success rates of 97.4% and 70.0%, respectively, while reducing training costs by 2.5-fold and decreasing inference latency by 2.8-fold compared to OpenVLA. CogVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/CogVLA.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/JiuTian-VL/CogVLA",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "multimodal",
            "OpenVLA"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions",
      "arxiv_id": "2509.06951",
      "arxiv_url": "https://arxiv.org/pdf/2509.06951",
      "summary": "Executing language-conditioned tasks in dynamic visual environments remains a central challenge in embodied AI. Existing Vision-Language-Action (VLA) models predominantly adopt reactive state-to-action mappings, often leading to short-sighted behaviors and poor robustness in dynamic scenes. In this paper, we introduce F1, a pretrained VLA framework which integrates the visual foresight generation into decision-making pipeline. F1 adopts a Mixture-of-Transformer architecture with dedicated modules for perception, foresight generation, and control, thereby bridging understanding, generation, and actions. At its core, F1 employs a next-scale prediction mechanism to synthesize goal-conditioned visual foresight as explicit planning targets. By forecasting plausible future visual states, F1 reformulates action generation as a foresight-guided inverse dynamics problem, enabling actions that implicitly achieve visual goals. To endow F1 with robust and generalizable capabilities, we propose a three-stage training recipe on an extensive dataset comprising over 330k trajectories across 136 diverse tasks. This training scheme enhances modular reasoning and equips the model with transferable visual foresight, which is critical for complex and dynamic environments. Extensive evaluations on real-world tasks and simulation benchmarks demonstrate F1 consistently outperforms existing approaches, achieving substantial gains in both task success rate and generalization ability.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/InternRobotics/F1-VLA",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "embodied AI",
            "[T]vision-language-action",
            "VLA",
            "language conditioned"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model",
      "arxiv_id": "2509.09372",
      "arxiv_url": "https://arxiv.org/pdf/2509.09372",
      "summary": "Vision-Language-Action (VLA) models typically bridge the gap between perceptual and action spaces by pre-training a large-scale Vision-Language Model (VLM) on robotic data. While this approach greatly enhances performance, it also incurs significant training costs. In this paper, we investigate how to effectively bridge vision-language (VL) representations to action (A). We introduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA models on large-scale VLMs and extensive pre-training. To this end, we first systematically analyze the effectiveness of various VL conditions and present key findings on which conditions are essential for bridging perception and action spaces. Based on these insights, we propose a lightweight Policy module with Bridge Attention, which autonomously injects the optimal condition into the action space. In this way, our method achieves high performance using only a 0.5B-parameter backbone, without any robotic data pre-training. Extensive experiments on both simulated and real-world robotic benchmarks demonstrate that VLA-Adapter not only achieves state-of-the-art level performance, but also offers the fast inference speed reported to date. Furthermore, thanks to the proposed advanced bridging paradigm, VLA-Adapter enables the training of a powerful VLA model in just 8 hours on a single consumer-grade GPU, greatly lowering the barrier to deploying the VLA model. Project page: https://vla-adapter.github.io/.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression",
      "arxiv_id": "2412.03293",
      "arxiv_url": "https://arxiv.org/pdf/2412.03293",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]VLA",
            "[T]foundation model"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] RoboNurse-VLA: Robotic Scrub Nurse System based on Vision-Language-Action Model",
      "arxiv_id": "2409.19590",
      "arxiv_url": "https://arxiv.org/pdf/2409.19590",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]VLA"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2023] RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
      "arxiv_id": "2307.15818",
      "arxiv_url": "https://arxiv.org/pdf/2307.15818",
      "summary": "",
      "authors": [],
      "year": "2023",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "[T]RT-2"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs",
      "arxiv_id": "2407.07775",
      "arxiv_url": "https://arxiv.org/pdf/2407.07775",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]VLA",
            "[T]multimodal"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning",
      "arxiv_id": "2503.21860",
      "arxiv_url": "https://arxiv.org/pdf/2503.21860",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]bi-manual",
            "[T]bimanual manipulation"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Learning Coordinated Bimanual Manipulation Policies using State Diffusion and Inverse Dynamics Models",
      "arxiv_id": "2503.23271",
      "arxiv_url": "https://arxiv.org/pdf/2503.23271",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]bi-manual",
            "[T]bimanual manipulation"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Novel Demonstration Generation with Gaussian Splatting Enables Robust One-Shot Manipulation",
      "arxiv_id": "2504.13175",
      "arxiv_url": "https://arxiv.org/pdf/2504.13175",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]gaussian splatting",
            "[T]splatting"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Diffusion-Based Imaginative Coordination for Bimanual Manipulation",
      "arxiv_id": "2507.11296",
      "arxiv_url": "https://arxiv.org/pdf/2507.11296",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]bi-manual",
            "[T]bimanual manipulation"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] AffordGrasp: In-Context Affordance Reasoning for Open-Vocabulary Task-Oriented Grasping in Clutter",
      "arxiv_id": "2503.00778",
      "arxiv_url": "https://arxiv.org/pdf/2503.00778",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]open-vocabulary",
            "[T]open vocabulary",
            "[T]affordance"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for Open-Vocabulary Robotic Manipulation",
      "arxiv_id": "2503.10546",
      "arxiv_url": "https://arxiv.org/pdf/2503.10546",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]open-vocabulary",
            "[T]open vocabulary"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Context-Aware Human Behavior Prediction Using Multimodal Large Language Models: Challenges and Insights",
      "arxiv_id": "2504.00839",
      "arxiv_url": "https://arxiv.org/pdf/2504.00839",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model",
            "[T]multimodal"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Visual Embodied Brain: Let Multimodal Large Language Models See, Think, and Control in Spaces",
      "arxiv_id": "2506.00123",
      "arxiv_url": "https://arxiv.org/pdf/2506.00123",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model",
            "[T]multimodal"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] OpenNav: Open-World Navigation with Multimodal Large Language Models",
      "arxiv_id": "2507.18033",
      "arxiv_url": "https://arxiv.org/pdf/2507.18033",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model",
            "[T]multimodal"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Eureka: Human-Level Reward Design via Coding Large Language Models",
      "arxiv_id": "2310.12931",
      "arxiv_url": "https://arxiv.org/pdf/2310.12931",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reward design",
            "[T]Eureka"
          ],
          "score": 9.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] A Distributional Treatment of Real2Sim2Real for Vision-Driven Deformable Linear Object Manipulation",
      "arxiv_id": "2502.18615",
      "arxiv_url": "https://arxiv.org/pdf/2502.18615",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]sim2real",
            "[T]real2sim"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation",
      "arxiv_id": "2506.18088",
      "arxiv_url": "https://arxiv.org/pdf/2506.18088",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]bi-manual",
            "[T]domain randomization"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control",
      "arxiv_id": "2508.21112",
      "arxiv_url": "https://arxiv.org/pdf/2508.21112",
      "summary": "The human ability to seamlessly perform multimodal reasoning and physical interaction in the open world is a core goal for general-purpose embodied intelligent systems. Recent vision-language-action (VLA) models, which are co-trained on large-scale robot and visual-text data, have demonstrated notable progress in general robot control. However, they still fail to achieve human-level flexibility in interleaved reasoning and interaction. In this work, introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is a unified embodied foundation model that achieves superior performance in multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training. The development of EO-1 is based on two key pillars: (i) a unified architecture that processes multimodal inputs indiscriminately (image, text, video, and action), and (ii) a massive, high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains over 1.5 million samples with emphasis on interleaved vision-text-action comprehension. EO-1 is trained through synergies between auto-regressive decoding and flow matching denoising on EO-Data1.5M, enabling seamless robot action generation and multimodal embodied reasoning. Extensive experiments demonstrate the effectiveness of interleaved vision-text-action learning for open-world understanding and generalization, validated through a variety of long-horizon, dexterous manipulation tasks across multiple embodiments. This paper details the architecture of EO-1, the data construction strategy of EO-Data1.5M, and the training methodology, offering valuable insights for developing advanced embodied foundation models.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/eo-robotics/EO-1",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "dexterous manipulation"
          ],
          "score": 4.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "flow matching"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA",
            "foundation model",
            "multimodal"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 17.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] HuB: Learning Extreme Humanoid Balance",
      "arxiv_id": "2505.07294",
      "arxiv_url": "https://arxiv.org/pdf/2505.07294",
      "summary": "The human body demonstrates exceptional motor capabilities-such as standing steadily on one foot or performing a high kick with the leg raised over 1.5 meters-both requiring precise balance control. While recent research on humanoid control has leveraged reinforcement learning to track human motions for skill acquisition, applying this paradigm to balance-intensive tasks remains challenging. In this work, we identify three key obstacles: instability from reference motion errors, learning difficulties due to morphological mismatch, and the sim-to-real gap caused by sensor noise and unmodeled dynamics. To address these challenges, we propose HuB (Humanoid Balance), a unified framework that integrates reference motion refinement, balance-aware policy learning, and sim-to-real robustness training, with each component targeting a specific challenge. We validate our approach on the Unitree G1 humanoid robot across challenging quasi-static balance tasks, including extreme single-legged poses such as Swallow Balance and Bruce Lee's Kick. Our policy remains stable even under strong physical disturbances-such as a forceful soccer strike-while baseline methods consistently fail to complete these tasks. Project website: https://hub-robot.github.io",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "humanoid control",
            "sim-to-real",
            "Unitree"
          ],
          "score": 14.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "policy learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 17.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] TWIST: Teleoperated Whole-Body Imitation System",
      "arxiv_id": "2505.02833",
      "arxiv_url": "https://arxiv.org/pdf/2505.02833",
      "summary": "Teleoperating humanoid robots in a whole-body manner marks a fundamental step toward developing general-purpose robotic intelligence, with human motion providing an ideal interface for controlling all degrees of freedom. Yet, most current humanoid teleoperation systems fall short of enabling coordinated whole-body behavior, typically limiting themselves to isolated locomotion or manipulation tasks. We present the Teleoperated Whole-Body Imitation System (TWIST), a system for humanoid teleoperation through whole-body motion imitation. We first generate reference motion clips by retargeting human motion capture data to the humanoid robot. We then develop a robust, adaptive, and responsive whole-body controller using a combination of reinforcement learning and behavior cloning (RL+BC). Through systematic analysis, we demonstrate how incorporating privileged future motion frames and real-world motion capture (MoCap) data improves tracking accuracy. TWIST enables real-world humanoid robots to achieve unprecedented, versatile, and coordinated whole-body motor skills--spanning whole-body manipulation, legged manipulation, locomotion, and expressive movement--using a single unified neural network controller. Our project website: https://humanoid-teleop.github.io",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning, Awesome Humanoid Robot Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid",
            "humanoid robot",
            "whole-body control",
            "locomotion",
            "manipulation",
            "whole-body manipulation",
            "teleoperation"
          ],
          "score": 14.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "behavior cloning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 17.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] LangWBC: Language-directed Humanoid Whole-Body Control via End-to-end Learning",
      "arxiv_id": "2504.21738",
      "arxiv_url": "https://arxiv.org/pdf/2504.21738",
      "summary": "General-purpose humanoid robots are expected to interact intuitively with humans, enabling seamless integration into daily life. Natural language provides the most accessible medium for this purpose. However, translating language into humanoid whole-body motion remains a significant challenge, primarily due to the gap between linguistic understanding and physical actions. In this work, we present an end-to-end, language-directed policy for real-world humanoid whole-body control. Our approach combines reinforcement learning with policy distillation, allowing a single neural network to interpret language commands and execute corresponding physical actions directly. To enhance motion diversity and compositionality, we incorporate a Conditional Variational Autoencoder (CVAE) structure. The resulting policy achieves agile and versatile whole-body behaviors conditioned on language inputs, with smooth transitions between various motions, enabling adaptation to linguistic variations and the emergence of novel motions. We validate the efficacy and generalizability of our method through extensive simulations and real-world experiments, demonstrating robust whole-body control. Please see our website at LangWBC.github.io for more information.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]whole-body control"
          ],
          "score": 14.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "distillation"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 17.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Foundation Model Driven Robotics: A Comprehensive Review",
      "arxiv_id": "2507.10087",
      "arxiv_url": "https://arxiv.org/pdf/2507.10087",
      "summary": "The rapid emergence of foundation models, particularly Large Language Models (LLMs) and Vision-Language Models (VLMs), has introduced a transformative paradigm in robotics. These models offer powerful capabilities in semantic understanding, high-level reasoning, and cross-modal generalization, enabling significant advances in perception, planning, control, and human-robot interaction. This critical review provides a structured synthesis of recent developments, categorizing applications across simulation-driven design, open-world execution, sim-to-real transfer, and adaptable robotics. Unlike existing surveys that emphasize isolated capabilities, this work highlights integrated, system-level strategies and evaluates their practical feasibility in real-world environments. Key enabling trends such as procedural scene generation, policy generalization, and multimodal reasoning are discussed alongside core bottlenecks, including limited embodiment, lack of multimodal data, safety risks, and computational constraints. Through this lens, this paper identifies both the architectural strengths and critical limitations of foundation model-based robotics, highlighting open challenges in real-time operation, grounding, resilience, and trust. The review concludes with a roadmap for future research aimed at bridging semantic reasoning and physical intelligence through more robust, interpretable, and embodied models.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "sim-to-real"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "[T]foundation model",
            "multimodal"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 17.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [Nvidia] VLA-0: Building State-of-the-Art VLAs with Zero Modification",
      "arxiv_id": "2510.13054",
      "arxiv_url": "https://arxiv.org/pdf/2510.13054",
      "summary": "Vision-Language-Action models (VLAs) hold immense promise for enabling generalist robot manipulation. However, the best way to build them remains an open question. Current approaches often add complexity, such as modifying the existing vocabulary of a Vision-Language Model (VLM) with action tokens or introducing special action heads. Curiously, the simplest strategy of representing actions directly as text has remained largely unexplored. This work introduces VLA-0 to investigate this idea. We find that VLA-0 is not only effective; it is surprisingly powerful. With the right design, VLA-0 outperforms more involved models. On LIBERO, a popular benchmark for evaluating VLAs, VLA-0 outperforms all existing methods trained on the same robotic data, including $π_0.5$-KI, OpenVLA-OFT and SmolVLA. Furthermore, without large-scale robotics-specific training, it outperforms methods trained on large-scale robotic data, like $π_0.5$-KI, $π_0$, GR00T-N1 and MolmoAct. These findings also translate to the real world, where VLA-0 outperforms SmolVLA, a VLA model pre-trained on large-scale real data. This paper summarizes our unexpected findings and spells out the specific techniques required to unlock the high performance of this simple yet potent VLA design. Visual results, code, and trained models are provided here: https://vla0.github.io/.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/NVlabs/vla0",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "[T]VLA",
            "OpenVLA"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 17.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration",
      "arxiv_id": "2502.19250",
      "arxiv_url": "https://arxiv.org/pdf/2502.19250",
      "summary": "Imitation learning has proven to be highly effective in teaching robots dexterous manipulation skills. However, it typically relies on large amounts of human demonstration data, which limits its scalability and applicability in dynamic, real-world environments. One key challenge in this context is object generalization, where a robot trained to perform a task with one object, such as \"hand over the apple,\" struggles to transfer its skills to a semantically similar but visually different object, such as \"hand over the peach.\" This gap in generalization to new objects beyond those in the same category has yet to be adequately addressed in previous work on end-to-end visuomotor policy learning. In this paper, we present a simple yet effective approach for achieving object generalization through Vision-Language-Action (VLA) models, referred to as \\textbf{ObjectVLA}. Our model enables robots to generalize learned skills to novel objects without requiring explicit human demonstrations for each new target object. By leveraging vision-language pair data, our method provides a lightweight and scalable way to inject knowledge about the target object, establishing an implicit link between the object and the desired action. We evaluate ObjectVLA on a real robotic platform, demonstrating its ability to generalize across 100 novel objects with a 64\\% success rate in selecting objects not seen during training. Furthermore, we propose a more accessible method for enhancing object generalization in VLA models, using a smartphone to capture a few images and fine-tune the pre-trained model. These results highlight the effectiveness of our approach in enabling object-level generalization and reducing the need for extensive human demonstrations, paving the way for more flexible and scalable robotic learning systems.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "dexterous manipulation"
          ],
          "score": 8.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning",
            "imitation learning"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 17.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model",
      "arxiv_id": "2503.10631",
      "arxiv_url": "https://arxiv.org/pdf/2503.10631",
      "summary": "A fundamental objective of manipulation policy design is to endow robots to comprehend human instructions, reason about scene cues, and execute generalized actions in dynamic environments. Recent autoregressive vision-language-action (VLA) methods inherit common-sense reasoning capabilities from vision-language models (VLMs) for next action-token prediction. However, these methods quantize actions into discrete bins, which disrupts the continuity required for precise control. In contrast, existing diffusion-based VLA methods incorporate an additional diffusion head to predict continuous actions solely conditioned on feature representations extracted by the VLM, without fully leveraging the VLM's pretrained reasoning capabilities through token-level generation. To address these limitations, we introduce HybridVLA, a unified framework that absorbs the continuous nature of diffusion-based actions and the contextual reasoning of autoregression within a single large language model. To mitigate interference between the two generation paradigms, we propose a collaborative training recipe that seamlessly incorporates diffusion denoising into the next-token prediction process. With this recipe, we find these two action prediction methods not only reinforce each other but also exhibit varying strength across different tasks. Therefore, we design a collaborative action ensemble mechanism that adaptively fuses both predictions, leading to more robust control. HybridVLA outperforms previous state-of-the-art VLA methods by 14\\% and 19\\% in mean success rate on simulation and real-world tasks, respectively, while demonstrating stable manipulation in unseen configurations.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "large language model"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 17.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] SwitchVLA: Execution-Aware Task Switching for Vision-Language-Action Models",
      "arxiv_id": "2506.03574",
      "arxiv_url": "https://arxiv.org/pdf/2506.03574",
      "summary": "Robots deployed in dynamic environments must be able to not only follow diverse language instructions but flexibly adapt when user intent changes mid-execution. While recent Vision-Language-Action (VLA) models have advanced multi-task learning and instruction following, they typically assume static task intent, failing to respond when new instructions arrive during ongoing execution. This limitation hinders natural and robust interaction in dynamic settings, such as retail or household environments, where real-time intent changes are common. We propose SwitchVLA, a unified, execution-aware framework that enables smooth and reactive task switching without external planners or additional switch-specific data. We model task switching as a behavior modulation problem conditioned on execution state and instruction context. Expert demonstrations are segmented into temporally grounded contact phases, allowing the policy to infer task progress and adjust its behavior accordingly. A multi-behavior conditional policy is then trained to generate flexible action chunks under varying behavior modes through conditioned trajectory modeling. Experiments in both simulation and real-world robotic manipulation demonstrate that SwitchVLA enables robust instruction adherence, fluid task switching, and strong generalization-outperforming prior VLA baselines in both task success rate and interaction naturalness.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "instruction following"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 17.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] SAFE: Multitask Failure Detection for Vision-Language-Action Models",
      "arxiv_id": "2506.09937",
      "arxiv_url": "https://arxiv.org/pdf/2506.09937",
      "summary": "While vision-language-action models (VLAs) have shown promising robotic behaviors across a diverse set of manipulation tasks, they achieve limited success rates when deployed on novel tasks out of the box. To allow these policies to safely interact with their environments, we need a failure detector that gives a timely alert such that the robot can stop, backtrack, or ask for help. However, existing failure detectors are trained and tested only on one or a few specific tasks, while generalist VLAs require the detector to generalize and detect failures also in unseen tasks and novel environments. In this paper, we introduce the multitask failure detection problem and propose SAFE, a failure detector for generalist robot policies such as VLAs. We analyze the VLA feature space and find that VLAs have sufficient high-level knowledge about task success and failure, which is generic across different tasks. Based on this insight, we design SAFE to learn from VLA internal features and predict a single scalar indicating the likelihood of task failure. SAFE is trained on both successful and failed rollouts and is evaluated on unseen tasks. SAFE is compatible with different policy architectures. We test it on OpenVLA, $π_0$, and $π_0$-FAST in both simulated and real-world environments extensively. We compare SAFE with diverse baselines and show that SAFE achieves state-of-the-art failure detection performance and the best trade-off between accuracy and detection time using conformal prediction. More qualitative results and code can be found at the project webpage: https://vla-safe.github.io/",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "OpenVLA"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 17.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting",
      "arxiv_id": "2507.05116",
      "arxiv_url": "https://arxiv.org/pdf/2507.05116v1",
      "summary": "Recent large-scale Vision Language Action (VLA) models have shown superior performance in robotic manipulation tasks guided by natural language. However, current VLA models suffer from two drawbacks: (i) generation of massive tokens leading to high inference latency and increased training cost, and (ii) insufficient utilization of generated actions resulting in potential performance loss. To address these issues, we develop a training framework to finetune VLA models for generating significantly fewer action tokens with high parallelism, effectively reducing inference latency and training cost. Furthermore, we introduce an inference optimization technique with a novel voting-based ensemble strategy to combine current and previous action predictions, improving the utilization of generated actions and overall performance. Our results demonstrate that we achieve superior performance compared with state-of-the-art VLA models, achieving significantly higher success rates and 39$\\times$ faster inference than OpenVLA with 46 Hz throughput on edge platforms, demonstrating practical deployability. The code is available at https://github.com/LukeLIN-web/VOTE.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "OpenVLA"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 17.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver",
      "arxiv_id": "2508.10333",
      "arxiv_url": "https://arxiv.org/pdf/2508.10333",
      "summary": "Recent advances in Vision-Language-Action (VLA) models have enabled robotic agents to integrate multimodal understanding with action execution. However, our empirical analysis reveals that current VLAs struggle to allocate visual attention to target regions. Instead, visual attention is always dispersed. To guide the visual attention grounding on the correct target, we propose ReconVLA, a reconstructive VLA model with an implicit grounding paradigm. Conditioned on the model's visual outputs, a diffusion transformer aims to reconstruct the gaze region of the image, which corresponds to the target manipulated objects. This process prompts the VLA model to learn fine-grained representations and accurately allocate visual attention, thus effectively leveraging task-specific visual information and conducting precise manipulation. Moreover, we curate a large-scale pretraining dataset comprising over 100k trajectories and 2 million data samples from open-source robotic datasets, further boosting the model's generalization in visual reconstruction. Extensive experiments in simulation and the real world demonstrate the superiority of our implicit grounding method, showcasing its capabilities of precise manipulation and generalization. Our project page is https://zionchow.github.io/ReconVLA/.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/Chowzy069/Reconvla",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "multimodal"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 17.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.10] EgoMimic: Scaling Imitation Learning via Egocentric Video [IL] [[project](https://egomimic.github.io/)]",
      "arxiv_id": "2410.24221",
      "arxiv_url": "https://arxiv.org/abs/2410.24221",
      "summary": "The scale and diversity of demonstration data required for imitation learning is a significant challenge. We present EgoMimic, a full-stack framework which scales manipulation via human embodiment data, specifically egocentric human videos paired with 3D hand tracking. EgoMimic achieves this through: (1) a system to capture human embodiment data using the ergonomic Project Aria glasses, (2) a low-cost bimanual manipulator that minimizes the kinematic gap to human data, (3) cross-domain data alignment techniques, and (4) an imitation learning architecture that co-trains on human and robot data. Compared to prior works that only extract high-level intent from human videos, our approach treats human and robot data equally as embodied demonstration data and learns a unified policy from both data sources. EgoMimic achieves significant improvement on a diverse set of long-horizon, single-arm and bimanual manipulation tasks over state-of-the-art imitation learning methods and enables generalization to entirely new scenes. Finally, we show a favorable scaling trend for EgoMimic, where adding 1 hour of additional hand data is significantly more valuable than 1 hour of additional robot data. Videos and additional information can be found at https://egomimic.github.io/",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "https://github.com/SimarKareer/EgoMimic",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "bi-manual",
            "bimanual manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "[T]egocentric"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 16.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "6_video_extraction"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.06] Human-Object Interaction from Human-Level Instructions [LLM] [[project](https://hoifhli.github.io/)]",
      "arxiv_id": "2406.17840",
      "arxiv_url": "https://arxiv.org/abs/2406.17840",
      "summary": "Intelligent agents must autonomously interact with the environments to perform daily tasks based on human-level instructions. They need a foundational understanding of the world to accurately interpret these instructions, along with precise low-level movement and interaction skills to execute the derived actions. In this work, we propose the first complete system for synthesizing physically plausible, long-horizon human-object interactions for object manipulation in contextual environments, driven by human-level instructions. We leverage large language models (LLMs) to interpret the input instructions into detailed execution plans. Unlike prior work, our system is capable of generating detailed finger-object interactions, in seamless coordination with full-body movements. We also train a policy to track generated motions in physics simulation via reinforcement learning (RL) to ensure physical plausibility of the motion. Our experiments demonstrate the effectiveness of our system in synthesizing realistic interactions with diverse objects in complex environments, highlighting its potential for real-world applications.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "physically plausible"
          ],
          "score": 2.5
        },
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "[T]human-object interaction"
          ],
          "score": 7.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 16.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "4_motion_diffusion",
        "5_interaction_reaction",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill",
      "arxiv_id": "2504.04191",
      "arxiv_url": "https://arxiv.org/pdf/2504.04191",
      "summary": "Learning open-vocabulary physical skills for simulated agents presents a significant challenge in artificial intelligence. Current reinforcement learning approaches face critical limitations: manually designed rewards lack scalability across diverse tasks, while demonstration-based methods struggle to generalize beyond their training distribution. We introduce GROVE, a generalized reward framework that enables open-vocabulary physical skill learning without manual engineering or task-specific demonstrations. Our key insight is that Large Language Models(LLMs) and Vision Language Models(VLMs) provide complementary guidance -- LLMs generate precise physical constraints capturing task requirements, while VLMs evaluate motion semantics and naturalness. Through an iterative design process, VLM-based feedback continuously refines LLM-generated constraints, creating a self-improving reward system. To bridge the domain gap between simulation and natural images, we develop Pose2CLIP, a lightweight mapper that efficiently projects agent poses directly into semantic feature space without computationally expensive rendering. Extensive experiments across diverse embodiments and learning paradigms demonstrate GROVE's effectiveness, achieving 22.2% higher motion naturalness and 25.7% better task completion scores while training 8.4x faster than previous methods. These results establish a new foundation for scalable physical skill acquisition in simulated environments.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning, Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]open-vocabulary",
            "[T]open vocabulary"
          ],
          "score": 12.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 16.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models",
      "arxiv_id": "2503.10626",
      "arxiv_url": "https://arxiv.org/pdf/2503.10626",
      "summary": "Acquiring physically plausible motor skills across diverse and unconventional morphologies-including humanoid robots, quadrupeds, and animals-is essential for advancing character simulation and robotics. Traditional methods, such as reinforcement learning (RL) are task- and body-specific, require extensive reward function engineering, and do not generalize well. Imitation learning offers an alternative but relies heavily on high-quality expert demonstrations, which are difficult to obtain for non-human morphologies. Video diffusion models, on the other hand, are capable of generating realistic videos of various morphologies, from humans to ants. Leveraging this capability, we propose a data-independent approach for skill acquisition that learns 3D motor skills from 2D-generated videos, with generalization capability to unconventional and non-human forms. Specifically, we guide the imitation learning process by leveraging vision transformers for video-based comparisons by calculating pair-wise distance between video embeddings. Along with video-encoding distance, we also use a computed similarity between segmented video frames as a guidance reward. We validate our method on locomotion tasks involving unique body configurations. In humanoid robot locomotion tasks, we demonstrate that 'No-data Imitation Learning' (NIL) outperforms baselines trained on 3D motion-capture data. Our results highlight the potential of leveraging generative video models for physically plausible skill learning with diverse morphologies, effectively replacing data collection with data generation for imitation learning.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning, Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped",
            "humanoid",
            "humanoid robot",
            "locomotion"
          ],
          "score": 8.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "[T]imitation learning"
          ],
          "score": 6.0
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "physically plausible"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 16.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "4_motion_diffusion"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Trinity: A Modular Humanoid Robot AI System",
      "arxiv_id": "2503.08338",
      "arxiv_url": "https://arxiv.org/abs/2503.08338",
      "summary": "In recent years, research on humanoid robots has garnered increasing attention. With breakthroughs in various types of artificial intelligence algorithms, embodied intelligence, exemplified by humanoid robots, has been highly anticipated. The advancements in reinforcement learning (RL) algorithms have significantly improved the motion control and generalization capabilities of humanoid robots. Simultaneously, the groundbreaking progress in large language models (LLM) and visual language models (VLM) has brought more possibilities and imagination to humanoid robots. LLM enables humanoid robots to understand complex tasks from language instructions and perform long-term task planning, while VLM greatly enhances the robots' understanding and interaction with their environment. This paper introduces \\textcolor{magenta}{Trinity}, a novel AI system for humanoid robots that integrates RL, LLM, and VLM. By combining these technologies, Trinity enables efficient control of humanoid robots in complex environments. This innovative approach not only enhances the capabilities but also opens new avenues for future research and applications of humanoid robotics.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning, Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot"
          ],
          "score": 12.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 16.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[IROS] Robust Feedback Motion Policy Design Using Reinforcement Learning on a 3D Digit Bipedal Robot.",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "IROS",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bipedal",
            "[T]biped"
          ],
          "score": 12.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 16.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [NeurIPS 25] What Can RL Bring to VLA Generalization? An Empirical Study",
      "arxiv_id": "2505.19789",
      "arxiv_url": "https://arxiv.org/pdf/2505.19789?",
      "summary": "Large Vision-Language Action (VLA) models have shown significant potential for embodied AI. However, their predominant training via supervised fine-tuning (SFT) limits generalization due to susceptibility to compounding errors under distribution shifts. Reinforcement learning (RL) offers a path to overcome these limitations by optimizing for task objectives via trial-and-error, yet a systematic understanding of its specific generalization benefits for VLAs compared to SFT is lacking. To address this, our study introduces a comprehensive benchmark for evaluating VLA generalization and systematically investigates the impact of RL fine-tuning across diverse visual, semantic, and execution dimensions. Our extensive experiments reveal that RL fine-tuning, particularly with PPO, significantly enhances generalization in semantic understanding and execution robustness over SFT, while maintaining comparable visual robustness. We identify PPO as a more effective RL algorithm for VLAs than LLM-derived methods like DPO and GRPO. We also develop a simple recipe for efficient PPO training on VLAs, and demonstrate its practical utility for improving VLA generalization. The project page is at https://rlvla.github.io",
      "authors": [],
      "year": "2025",
      "venue": "NeurIPS",
      "code_url": "https://github.com/gen-robot/RL4VLA",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "PPO",
            "DPO"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "embodied AI",
            "[T]VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 16.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Variable-Friction In-Hand Manipulation for Arbitrary Objects via Diffusion-Based Imitation Learning",
      "arxiv_id": "2503.02738",
      "arxiv_url": "https://arxiv.org/pdf/2503.02738",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]in-hand manipulation"
          ],
          "score": 12.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 16.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [ICRA 25 Best Paper Finalist] UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "ICRA",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]distillation"
          ],
          "score": 4.5
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]affordance"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 16.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] LensDFF: Language-enhanced Sparse Feature Distillation for Efficient Few-Shot Dexterous Manipulation",
      "arxiv_id": "2503.03890",
      "arxiv_url": "https://arxiv.org/pdf/2503.03890",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]dexterous manipulation"
          ],
          "score": 12.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]distillation"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 16.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] InterMimic: Towards Universal Whole-Body Control for Physics-Based Human-Object Interactions",
      "arxiv_id": "2502.20390",
      "arxiv_url": "https://arxiv.org/pdf/2502.20390",
      "summary": "Achieving realistic simulations of humans interacting with a wide range of objects has long been a fundamental goal. Extending physics-based motion imitation to complex human-object interactions (HOIs) is challenging due to intricate human-object coupling, variability in object geometries, and artifacts in motion capture data, such as inaccurate contacts and limited hand detail. We introduce InterMimic, a framework that enables a single policy to robustly learn from hours of imperfect MoCap data covering diverse full-body interactions with dynamic and varied objects. Our key insight is to employ a curriculum strategy -- perfect first, then scale up. We first train subject-specific teacher policies to mimic, retarget, and refine motion capture data. Next, we distill these teachers into a student policy, with the teachers acting as online experts providing direct supervision, as well as high-quality references. Notably, we incorporate RL fine-tuning on the student policy to surpass mere demonstration replication and achieve higher-quality solutions. Our experiments demonstrate that InterMimic produces realistic and diverse interactions across multiple HOI datasets. The learned policy generalizes in a zero-shot manner and seamlessly integrates with kinematic generators, elevating the framework from mere imitation to generative modeling of complex human-object interactions.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]whole-body control"
          ],
          "score": 6.0
        },
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "[T]human-object interaction",
            "HOI"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 16.0,
      "hit_pillars": [
        "1_robot_core",
        "5_interaction_reaction"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Tailoring Solution Accuracy for Fast Whole-body Model Predictive Control of Legged Robots",
      "arxiv_id": "2407.10789",
      "arxiv_url": "https://arxiv.org/abs/2407.10789",
      "summary": "Thanks to recent advancements in accelerating non-linear model predictive control (NMPC), it is now feasible to deploy whole-body NMPC at real-time rates for humanoid robots. However, enforcing inequality constraints in real time for such high-dimensional systems remains challenging due to the need for additional iterations. This paper presents an implementation of whole-body NMPC for legged robots that provides low-accuracy solutions to NMPC with general equality and inequality constraints. Instead of aiming for highly accurate optimal solutions, we leverage the alternating direction method of multipliers to rapidly provide low-accuracy solutions to quadratic programming subproblems. Our extensive simulation results indicate that real robots often cannot benefit from highly accurate solutions due to dynamics discretization errors, inertial modeling errors and delays. We incorporate control barrier functions (CBFs) at the initial timestep of the NMPC for the self-collision constraints, resulting in up to a 26-fold reduction in the number of self-collisions without adding computational burden. The controller is reliably deployed on hardware at 90 Hz for a problem involving 32 timesteps, 2004 variables, and 3768 constraints. The NMPC delivers sufficiently accurate solutions, enabling the MIT Humanoid to plan complex crossed-leg and arm motions that enhance stability when walking and recovering from significant disturbances.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]legged robot",
            "humanoid",
            "humanoid robot",
            "[T]model predictive control"
          ],
          "score": 16.0
        }
      ],
      "relevance_score": 16.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [IJRR 25] Foundation Models in Robotics: Applications, Challenges, and the Future",
      "arxiv_id": "2312.07843",
      "arxiv_url": "https://arxiv.org/pdf/2312.07843",
      "summary": "We survey applications of pretrained foundation models in robotics. Traditional deep learning models in robotics are trained on small datasets tailored for specific tasks, which limits their adaptability across diverse applications. In contrast, foundation models pretrained on internet-scale data appear to have superior generalization capabilities, and in some instances display an emergent ability to find zero-shot solutions to problems that are not present in the training data. Foundation models may hold the potential to enhance various components of the robot autonomy stack, from perception to decision-making and control. For example, large language models can generate code or provide common sense reasoning, while vision-language models enable open-vocabulary visual recognition. However, significant open research challenges remain, particularly around the scarcity of robot-relevant training data, safety guarantees and uncertainty quantification, and real-time execution. In this survey, we study recent papers that have used or built foundation models to solve robotics problems. We explore how foundation models contribute to improving robot capabilities in the domains of perception, decision-making, and control. We discuss the challenges hindering the adoption of foundation models in robot autonomy and provide opportunities and potential pathways for future advancements. The GitHub project corresponding to this paper (Preliminary release. We are committed to further enhancing and updating this work to ensure its quality and relevance) can be found here: https://github.com/robotics-survey/Awesome-Robotics-Foundation-Models",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "open-vocabulary",
            "open vocabulary"
          ],
          "score": 4.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "[T]foundation model"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 16.0,
      "hit_pillars": [
        "3_perception_slam",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision",
      "arxiv_id": "2504.02477",
      "arxiv_url": "https://arxiv.org/pdf/2504.02477",
      "summary": "Robot vision has greatly benefited from advancements in multimodal fusion techniques and vision-language models (VLMs). We adopt a task-oriented perspective to systematically review the applications and advancements of multimodal fusion methods and VLMs in the field of robot vision. For semantic scene understanding tasks, we categorize fusion approaches into encoder-decoder frameworks, attention-based architectures, and graph neural networks. Meanwhile, we also analyze the architectural characteristics and practical implementations of these fusion strategies in key tasks such as simultaneous localization and mapping (SLAM), 3D object detection, navigation, and manipulation. We compare the evolutionary paths and applicability of VLMs based on large language models (LLMs) with traditional multimodal fusion methods.Additionally, we conduct an in-depth analysis of commonly used datasets, evaluating their applicability and challenges in real-world robotic scenarios. Building on this analysis, we identify key challenges in current research, including cross-modal alignment, efficient fusion, real-time deployment, and domain adaptation. We propose future directions such as self-supervised learning for robust multimodal representations, structured spatial memory and environment modeling to enhance spatial intelligence, and the integration of adversarial robustness and human feedback mechanisms to enable ethically aligned system deployment. Through a comprehensive review, comparative analysis, and forward-looking discussion, we provide a valuable reference for advancing multimodal perception and interaction in robotic vision. A comprehensive list of studies in this survey is available at https://github.com/Xiaofeng-Han-Res/MF-RV.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "[T]multimodal"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 16.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [Physical Intelligence] π0.5: A Vision-Language-Action Model with Open-World Generalization",
      "arxiv_id": "2504.16054",
      "arxiv_url": "https://arxiv.org/pdf/2504.16054",
      "summary": "In order for robots to be useful, they must perform practically relevant tasks in the real world, outside of the lab. While vision-language-action (VLA) models have demonstrated impressive results for end-to-end robot control, it remains an open question how far such models can generalize in the wild. We describe $π_{0.5}$, a new model based on $π_{0}$ that uses co-training on heterogeneous tasks to enable broad generalization. $π_{0.5}$\\ uses data from multiple robots, high-level semantic prediction, web data, and other sources to enable broadly generalizable real-world robotic manipulation. Our system uses a combination of co-training and hybrid multi-modal examples that combine image observations, language commands, object detections, semantic subtask prediction, and low-level actions. Our experiments show that this kind of knowledge transfer is essential for effective generalization, and we demonstrate for the first time that an end-to-end learning-enabled robotic system can perform long-horizon and dexterous manipulation skills, such as cleaning a kitchen or bedroom, in entirely new homes.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/Physical-Intelligence/openpi",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "dexterous manipulation"
          ],
          "score": 4.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 16.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning",
      "arxiv_id": "2505.11917",
      "arxiv_url": "https://arxiv.org/pdf/2505.11917",
      "summary": "General-purpose robots capable of performing diverse tasks require synergistic reasoning and acting capabilities. However, recent dual-system approaches, which separate high-level reasoning from low-level acting, often suffer from challenges such as limited mutual understanding of capabilities between systems and latency issues. This paper introduces OneTwoVLA, a single unified vision-language-action model that can perform both acting (System One) and reasoning (System Two). Crucially, OneTwoVLA adaptively switches between two modes: explicitly reasoning at critical moments during task execution, and generating actions based on the most recent reasoning at other times. To further unlock OneTwoVLA's reasoning and generalization capabilities, we design a scalable pipeline for synthesizing embodied reasoning-centric vision-language data, used for co-training with robot data. We validate OneTwoVLA's effectiveness through extensive experiments, highlighting its superior performance across four key capabilities: long-horizon task planning, error detection and recovery, natural human-robot interaction, and generalizable visual grounding, enabling the model to perform long-horizon, highly dexterous manipulation tasks such as making hotpot or mixing cocktails.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "dexterous manipulation"
          ],
          "score": 4.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "visual grounding"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 16.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.06] CooHOI: Learning Cooperative Human-Object Interaction with Manipulated Object Dynamics [RL] [[project](https://gao-jiawei.com/Research/CooHOI/)]",
      "arxiv_id": "2406.14558",
      "arxiv_url": "https://arxiv.org/abs/2406.14558",
      "summary": "Enabling humanoid robots to clean rooms has long been a pursued dream within humanoid research communities. However, many tasks require multi-humanoid collaboration, such as carrying large and heavy furniture together. Given the scarcity of motion capture data on multi-humanoid collaboration and the efficiency challenges associated with multi-agent learning, these tasks cannot be straightforwardly addressed using training paradigms designed for single-agent scenarios. In this paper, we introduce Cooperative Human-Object Interaction (CooHOI), a framework designed to tackle the challenge of multi-humanoid object transportation problem through a two-phase learning paradigm: individual skill learning and subsequent policy transfer. First, a single humanoid character learns to interact with objects through imitation learning from human motion priors. Then, the humanoid learns to collaborate with others by considering the shared dynamics of the manipulated object using centralized training and decentralized execution (CTDE) multi-agent RL algorithms. When one agent interacts with the object, resulting in specific object dynamics changes, the other agents learn to respond appropriately, thereby achieving implicit communication and coordination between teammates. Unlike previous approaches that relied on tracking-based methods for multi-humanoid HOI, CooHOI is inherently efficient, does not depend on motion capture data of multi-humanoid interactions, and can be seamlessly extended to include more participants and a wide range of object types.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid",
            "humanoid robot"
          ],
          "score": 4.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "[T]human-object interaction",
            "HOI"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 15.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "5_interaction_reaction"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] COMPASS: Cross-embOdiment Mobility Policy via ResiduAl RL and Skill Synthesis",
      "arxiv_id": "2502.16372",
      "arxiv_url": "https://arxiv.org/pdf/2502.16372",
      "summary": "As robots are increasingly deployed in diverse application domains, enabling robust mobility across different embodiments has become a critical challenge. Classical mobility stacks, though effective on specific platforms, require extensive per-robot tuning and do not scale easily to new embodiments. Learning-based approaches, such as imitation learning (IL), offer alternatives, but face significant limitations on the need for high-quality demonstrations for each embodiment.\n  To address these challenges, we introduce COMPASS, a unified framework that enables scalable cross-embodiment mobility using expert demonstrations from only a single embodiment. We first pre-train a mobility policy on a single robot using IL, combining a world model with a policy model. We then apply residual reinforcement learning (RL) to efficiently adapt this policy to diverse embodiments through corrective refinements. Finally, we distill specialist policies into a single generalist policy conditioned on an embodiment embedding vector. This design significantly reduces the burden of collecting data while enabling robust generalization across a wide range of robot designs. Our experiments demonstrate that COMPASS scales effectively across diverse robot platforms while maintaining adaptability to various environment configurations, achieving a generalist policy with a success rate approximately 5X higher than the pre-trained IL policy on unseen embodiments, and further demonstrates zero-shot sim-to-real transfer.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning, Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "sim-to-real"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "imitation learning",
            "world model"
          ],
          "score": 4.5
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "[T]cross-embodiment"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "7_retargeting"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] A Survey of Embodied Learning for Object-Centric Robotic Manipulation",
      "arxiv_id": "2408.11537",
      "arxiv_url": "https://arxiv.org/pdf/2408.11537",
      "summary": "Embodied learning for object-centric robotic manipulation is a rapidly developing and challenging area in embodied AI. It is crucial for advancing next-generation intelligent robots and has garnered significant interest recently. Unlike data-driven machine learning methods, embodied learning focuses on robot learning through physical interaction with the environment and perceptual feedback, making it especially suitable for robotic manipulation. In this paper, we provide a comprehensive survey of the latest advancements in this field and categorize the existing work into three main branches: 1) Embodied perceptual learning, which aims to predict object pose and affordance through various data representations; 2) Embodied policy learning, which focuses on generating optimal robotic decisions using methods such as reinforcement learning and imitation learning; 3) Embodied task-oriented learning, designed to optimize the robot's performance based on the characteristics of different tasks in object grasping and manipulation. In addition, we offer an overview and discussion of public datasets, evaluation metrics, representative applications, current challenges, and potential future research directions. A project associated with this survey has been established at https://github.com/RayYoh/OCRM_survey.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "policy learning",
            "imitation learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "affordance"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "embodied AI"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 15.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Motus: A Unified Latent Action World Model",
      "arxiv_id": "2512.13030",
      "arxiv_url": "https://arxiv.org/pdf/2512.13030",
      "summary": "While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level \"delta action\" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/thu-ml/Motus",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "optical flow"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA",
            "multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [RSS 25] Learning to Act Anywhere with Task-centric Latent Actions",
      "arxiv_id": "2505.06111",
      "arxiv_url": "https://arxiv.org/pdf/2505.06111",
      "summary": "A generalist robot should perform effectively across various environments. However, most existing approaches heavily rely on scaling action-annotated data to enhance their capabilities. Consequently, they are often limited to single physical specification and struggle to learn transferable knowledge across different embodiments and environments. To confront these limitations, we propose UniVLA, a new framework for learning cross-embodiment vision-language-action (VLA) policies. Our key innovation is to derive task-centric action representations from videos with a latent action model. This enables us to exploit extensive data across a wide spectrum of embodiments and perspectives. To mitigate the effect of task-irrelevant dynamics, we incorporate language instructions and establish a latent action model within the DINO feature space. Learned from internet-scale videos, the generalist policy can be deployed to various robots through efficient latent action decoding. We obtain state-of-the-art results across multiple manipulation and navigation benchmarks, as well as real-robot deployments. UniVLA achieves superior performance over OpenVLA with less than 1/20 of pretraining compute and 1/10 of downstream data. Continuous performance improvements are observed as heterogeneous data, even including human videos, are incorporated into the training pipeline. The results underscore UniVLA's potential to facilitate scalable and efficient robot policy learning.",
      "authors": [],
      "year": "2025",
      "venue": "RSS",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "cross-embodiment"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA",
            "OpenVLA"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "7_retargeting",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation",
      "arxiv_id": "2509.18865",
      "arxiv_url": "https://arxiv.org/pdf/2509.18865",
      "summary": "We propose Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation (Bi-VLA), a novel framework that extends bilateral control-based imitation learning to handle more than one task within a single model. Conventional bilateral control methods exploit joint angle, velocity, torque, and vision for precise manipulation but require task-specific models, limiting their generality. Bi-VLA overcomes this limitation by utilizing robot joint angle, velocity, and torque data from leader-follower bilateral control with visual features and natural language instructions through SigLIP and FiLM-based fusion. We validated Bi-VLA on two task types: one requiring supplementary language cues and another distinguishable solely by vision. Real-robot experiments showed that Bi-VLA successfully interprets vision-language combinations and improves task success rates compared to conventional bilateral control-based imitation learning. Our Bi-VLA addresses the single-task limitation of prior bilateral approaches and provides empirical evidence that combining vision and language significantly enhances versatility. Experimental results validate the effectiveness of Bi-VLA in real-world tasks. For additional material, please visit the website: https://mertcookimg.github.io/bi-vla/",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]VLA"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025.03] Humanoid Policy ~ Human Policy",
      "arxiv_id": "2503.13441",
      "arxiv_url": "https://arxiv.org/pdf/2503.13441",
      "summary": "Training manipulation policies for humanoid robots with diverse data enhances their robustness and generalization across tasks and platforms. However, learning solely from robot demonstrations is labor-intensive, requiring expensive tele-operated data collection which is difficult to scale. This paper investigates a more scalable data source, egocentric human demonstrations, to serve as cross-embodiment training data for robot learning. We mitigate the embodiment gap between humanoids and humans from both the data and modeling perspectives. We collect an egocentric task-oriented dataset (PH2D) that is directly aligned with humanoid manipulation demonstrations. We then train a human-humanoid behavior policy, which we term Human Action Transformer (HAT). The state-action space of HAT is unified for both humans and humanoid robots and can be differentiably retargeted to robot actions. Co-trained with smaller-scale robot data, HAT directly models humanoid robots and humans as different embodiments without additional supervision. We show that human data improves both generalization and robustness of HAT with significantly better data collection efficiency. Code and data: https://human-as-robot.github.io/",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Manipulation, Awesome Humanoid Learning, Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "manipulation"
          ],
          "score": 10.0
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "egocentric"
          ],
          "score": 2.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "cross-embodiment"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "6_video_extraction",
        "7_retargeting"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.07] BiGym: A Demo-Driven Mobile Bi-Manual Manipulation Benchmark [benchmark] [[project](https://chernyadev.github.io/bigym)]",
      "arxiv_id": "2407.07788",
      "arxiv_url": "https://arxiv.org/abs/2407.07788",
      "summary": "We introduce BiGym, a new benchmark and learning environment for mobile bi-manual demo-driven robotic manipulation. BiGym features 40 diverse tasks set in home environments, ranging from simple target reaching to complex kitchen cleaning. To capture the real-world performance accurately, we provide human-collected demonstrations for each task, reflecting the diverse modalities found in real-world robot trajectories. BiGym supports a variety of observations, including proprioceptive data and visual inputs such as RGB, and depth from 3 camera views. To validate the usability of BiGym, we thoroughly benchmark the state-of-the-art imitation learning algorithms and demo-driven reinforcement learning algorithms within the environment and discuss the future opportunities.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "https://github.com/YanjieZe/Humanoid-Teleoperation",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]bi-manual"
          ],
          "score": 12.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "imitation learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.06] HumanPlus: Humanoid Shadowing and Imitation from Humans [IL] [[project](https://humanoid-ai.github.io/)]",
      "arxiv_id": "2406.10454",
      "arxiv_url": "https://arxiv.org/abs/2406.10454",
      "summary": "One of the key arguments for building robots that have similar form factors to human beings is that we can leverage the massive human data for training. Yet, doing so has remained challenging in practice due to the complexities in humanoid perception and control, lingering physical gaps between humanoids and humans in morphologies and actuation, and lack of a data pipeline for humanoids to learn autonomous skills from egocentric vision. In this paper, we introduce a full-stack system for humanoids to learn motion and autonomous skills from human data. We first train a low-level policy in simulation via reinforcement learning using existing 40-hour human motion datasets. This policy transfers to the real world and allows humanoid robots to follow human body and hand motion in real time using only a RGB camera, i.e. shadowing. Through shadowing, human operators can teleoperate humanoids to collect whole-body data for learning different tasks in the real world. Using the data collected, we then perform supervised behavior cloning to train skill policies using egocentric vision, allowing humanoids to complete different tasks autonomously by imitating human skills. We demonstrate the system on our customized 33-DoF 180cm humanoid, autonomously completing tasks such as wearing a shoe to stand up and walk, unloading objects from warehouse racks, folding a sweatshirt, rearranging objects, typing, and greeting another robot with 60-100% success rates using up to 40 demonstrations. Project website: https://humanoid-ai.github.io/",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "https://github.com/MarkFzp/humanplus",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot"
          ],
          "score": 8.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "behavior cloning"
          ],
          "score": 3.0
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "egocentric",
            "egocentric vision"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "6_video_extraction"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges",
      "arxiv_id": "2512.11362",
      "arxiv_url": "https://arxiv.org/pdf/2512.11362",
      "summary": "Vision-Language-Action (VLA) models are driving a revolution in robotics, enabling machines to understand instructions and interact with the physical world. This field is exploding with new models and datasets, making it both exciting and challenging to keep pace with. This survey offers a clear and structured guide to the VLA landscape. We design it to follow the natural learning path of a researcher: we start with the basic Modules of any VLA model, trace the history through key Milestones, and then dive deep into the core Challenges that define recent research frontier. Our main contribution is a detailed breakdown of the five biggest challenges in: (1) Representation, (2) Execution, (3) Generalization, (4) Safety, and (5) Dataset and Evaluation. This structure mirrors the developmental roadmap of a generalist agent: establishing the fundamental perception-action loop, scaling capabilities across diverse embodiments and environments, and finally ensuring trustworthy deployment-all supported by the essential data infrastructure. For each of them, we review existing approaches and highlight future opportunities. We position this paper as both a foundational guide for newcomers and a strategic roadmap for experienced researchers, with the dual aim of accelerating learning and inspiring new ideas in embodied intelligence. A live version of this survey, with continuous updates, is maintained on our \\href{https://suyuz1.github.io/Survery/}{project page}.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "generalist agent",
            "[T]vision-language-action",
            "VLA"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
      "arxiv_id": "2510.07077",
      "arxiv_url": "https://arxiv.org/pdf/2510.07077",
      "summary": "Amid growing efforts to leverage advances in large language models (LLMs) and vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models have recently gained significant attention. By unifying vision, language, and action data at scale, which have traditionally been studied separately, VLA models aim to learn policies that generalise across diverse tasks, objects, embodiments, and environments. This generalisation capability is expected to enable robots to solve novel downstream tasks with minimal or no additional task-specific data, facilitating more flexible and scalable real-world deployment. Unlike previous surveys that focus narrowly on action representations or high-level model architectures, this work offers a comprehensive, full-stack review, integrating both software and hardware components of VLA systems. In particular, this paper provides a systematic review of VLAs, covering their strategy and architectural transition, architectures and building blocks, modality-specific processing techniques, and learning paradigms. In addition, to support the deployment of VLAs in real-world robotic applications, we also review commonly used robot platforms, data collection strategies, publicly available datasets, data augmentation methods, and evaluation benchmarks. Throughout this comprehensive survey, this paper aims to offer practical guidance for the robotics community in applying VLAs to real-world robotic systems. All references categorized by training approach, evaluation method, modality, and dataset are available in the table on our project website: https://vla-survey.github.io .",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "large language model"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Survey on Vision-Language-Action Models",
      "arxiv_id": "2502.06851",
      "arxiv_url": "https://arxiv.org/pdf/2502.06851",
      "summary": "This paper presents an AI-generated review of Vision-Language-Action (VLA) models, summarizing key methodologies, findings, and future directions. The content is produced using large language models (LLMs) and is intended only for demonstration purposes. This work does not represent original research, but highlights how AI can help automate literature reviews. As AI-generated content becomes more prevalent, ensuring accuracy, reliability, and proper synthesis remains a challenge. Future research will focus on developing a structured framework for AI-assisted literature reviews, exploring techniques to enhance citation accuracy, source credibility, and contextual understanding. By examining the potential and limitations of LLM in academic writing, this study aims to contribute to the broader discussion of integrating AI into research workflows. This work serves as a preliminary step toward establishing systematic approaches for leveraging AI in literature review generation, making academic knowledge synthesis more efficient and scalable.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "large language model"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [AgiBot] EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation",
      "arxiv_id": "2501.01895",
      "arxiv_url": "https://arxiv.org/pdf/2501.01895",
      "summary": "We introduce EnerVerse, a generative robotics foundation model that constructs and interprets embodied spaces. EnerVerse employs a chunk-wise autoregressive video diffusion framework to predict future embodied spaces from instructions, enhanced by a sparse context memory for long-term reasoning. To model the 3D robotics world, we adopt a multi-view video representation, providing rich perspectives to address challenges like motion ambiguity and 3D grounding. Additionally, EnerVerse-D, a data engine pipeline combining generative modeling with 4D Gaussian Splatting, forms a self-reinforcing data loop to reduce the sim-to-real gap. Leveraging these innovations, EnerVerse translates 4D world representations into physical actions via a policy head (EnerVerse-A), achieving state-of-the-art performance in both simulation and real-world tasks. For efficiency, EnerVerse-A reuses features from the first denoising step and predicts action chunks, achieving about 280 ms per 8-step action chunk on a single RTX 4090. Further video demos, dataset samples could be found in our project page.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "sim-to-real"
          ],
          "score": 8.0
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "gaussian splatting",
            "splatting"
          ],
          "score": 4.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] SAM2Act: Integrating Visual Foundation Model with A Memory Architecture for Robotic Manipulation",
      "arxiv_id": "2501.18564",
      "arxiv_url": "https://arxiv.org/pdf/2501.18564",
      "summary": "Robotic manipulation systems operating in diverse, dynamic environments must exhibit three critical abilities: multitask interaction, generalization to unseen scenarios, and spatial memory. While significant progress has been made in robotic manipulation, existing approaches often fall short in generalization to complex environmental variations and addressing memory-dependent tasks. To bridge this gap, we introduce SAM2Act, a multi-view robotic transformer-based policy that leverages multi-resolution upsampling with visual representations from large-scale foundation model. SAM2Act achieves a state-of-the-art average success rate of 86.8% across 18 tasks in the RLBench benchmark, and demonstrates robust generalization on The Colosseum benchmark, with only a 4.3% performance gap under diverse environmental perturbations. Building on this foundation, we propose SAM2Act+, a memory-based architecture inspired by SAM2, which incorporates a memory bank, an encoder, and an attention mechanism to enhance spatial memory. To address the need for evaluating memory-dependent tasks, we introduce MemoryBench, a novel benchmark designed to assess spatial memory and action recall in robotic manipulation. SAM2Act+ achieves an average success rate of 94.3% on memory-based tasks in MemoryBench, significantly outperforming existing approaches and pushing the boundaries of memory-based robotic systems. Project page: sam2act.github.io.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] RaceVLA: VLA-based Racing Drone Navigation with Human-like Behaviour",
      "arxiv_id": "2503.02572",
      "arxiv_url": "https://arxiv.org/pdf/2503.02572",
      "summary": "RaceVLA presents an innovative approach for autonomous racing drone navigation by leveraging Visual-Language-Action (VLA) to emulate human-like behavior. This research explores the integration of advanced algorithms that enable drones to adapt their navigation strategies based on real-time environmental feedback, mimicking the decision-making processes of human pilots. The model, fine-tuned on a collected racing drone dataset, demonstrates strong generalization despite the complexity of drone racing environments. RaceVLA outperforms OpenVLA in motion (75.0 vs 60.0) and semantic generalization (45.5 vs 36.3), benefiting from the dynamic camera and simplified motion tasks. However, visual (79.6 vs 87.0) and physical (50.0 vs 76.7) generalization were slightly reduced due to the challenges of maneuvering in dynamic environments with varying object sizes. RaceVLA also outperforms RT-2 across all axes - visual (79.6 vs 52.0), motion (75.0 vs 55.0), physical (50.0 vs 26.7), and semantic (45.5 vs 38.8), demonstrating its robustness for real-time adjustments in complex environments. Experiments revealed an average velocity of 1.04 m/s, with a maximum speed of 2.02 m/s, and consistent maneuverability, demonstrating RaceVLA's ability to handle high-speed scenarios effectively. These findings highlight the potential of RaceVLA for high-performance navigation in competitive racing contexts. The RaceVLA codebase, pretrained weights, and dataset are available at this http URL: https://racevla.github.io/",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]VLA",
            "RT-2",
            "OpenVLA"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": [
        "drone"
      ]
    },
    {
      "title": "[2025] NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks",
      "arxiv_id": "2504.19854",
      "arxiv_url": "https://arxiv.org/pdf/2504.19854",
      "summary": "Existing Visual-Language-Action (VLA) models have shown promising performance in zero-shot scenarios, demonstrating impressive task execution and reasoning capabilities. However, a significant challenge arises from the limitations of visual encoding, which can result in failures during tasks such as object grasping. Moreover, these models typically suffer from high computational overhead due to their large sizes, often exceeding 7B parameters. While these models excel in reasoning and task planning, the substantial computational overhead they incur makes them impractical for real-time robotic environments, where speed and efficiency are paramount. To address the limitations of existing VLA models, we propose NORA, a 3B-parameter model designed to reduce computational overhead while maintaining strong task performance. NORA adopts the Qwen-2.5-VL-3B multimodal model as its backbone, leveraging its superior visual-semantic understanding to enhance visual reasoning and action grounding. Additionally, our \\model{} is trained on 970k real-world robot demonstrations and equipped with the FAST+ tokenizer for efficient action sequence generation. Experimental results demonstrate that NORA outperforms existing large-scale VLA models, achieving better task performance with significantly reduced computational overhead, making it a more practical solution for real-time robotic autonomy.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "multimodal"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] CrayonRobo: Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation",
      "arxiv_id": "2505.02166",
      "arxiv_url": "https://arxiv.org/pdf/2505.02166",
      "summary": "In robotic, task goals can be conveyed through various modalities, such as language, goal images, and goal videos. However, natural language can be ambiguous, while images or videos may offer overly detailed specifications. To tackle these challenges, we introduce CrayonRobo that leverages comprehensive multi-modal prompts that explicitly convey both low-level actions and high-level planning in a simple manner. Specifically, for each key-frame in the task sequence, our method allows for manual or automatic generation of simple and expressive 2D visual prompts overlaid on RGB images. These prompts represent the required task goals, such as the end-effector pose and the desired movement direction after contact. We develop a training strategy that enables the model to interpret these visual-language prompts and predict the corresponding contact poses and movement directions in SE(3) space. Furthermore, by sequentially executing all key-frame steps, the model can complete long-horizon tasks. This approach not only helps the model explicitly understand the task objectives but also enhances its robustness on unseen tasks by providing easily interpretable prompts. We evaluate our method in both simulated and real-world environments, demonstrating its robust manipulation capabilities.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] SmolVLA: A vision-language-action model for affordable and efficient robotics",
      "arxiv_id": "2506.01844",
      "arxiv_url": "https://arxiv.org/pdf/2506.01844",
      "summary": "Vision-language models (VLMs) pretrained on large-scale multimodal datasets encode rich visual and linguistic knowledge, making them a strong foundation for robotics. Rather than training robotic policies from scratch, recent approaches adapt VLMs into vision-language-action (VLA) models that enable natural language-driven perception and control. However, existing VLAs are typically massive--often with billions of parameters--leading to high training costs and limited real-world deployability. Moreover, they rely on academic and industrial datasets, overlooking the growing availability of community-collected data from affordable robotic platforms. In this work, we present SmolVLA, a small, efficient, and community-driven VLA that drastically reduces both training and inference costs, while retaining competitive performance. SmolVLA is designed to be trained on a single GPU and deployed on consumer-grade GPUs or even CPUs. To further improve responsiveness, we introduce an asynchronous inference stack decoupling perception and action prediction from action execution, allowing higher control rates with chunked action generation. Despite its compact size, SmolVLA achieves performance comparable to VLAs that are 10x larger. We evaluate SmolVLA on a range of both simulated as well as real-world robotic benchmarks and release all code, pretrained models, and training data.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "multimodal"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse",
      "arxiv_id": "2506.07639",
      "arxiv_url": "https://arxiv.org/pdf/2506.07639",
      "summary": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action (VLA) models by improving performance and interpretability through intermediate reasoning steps. However, its sequential autoregressive token generation introduces significant inference latency, limiting real-time deployment. We propose Fast ECoT, an inference-time acceleration method that exploits the structured and repetitive nature of ECoT to (1) cache and reuse high-level reasoning across timesteps and (2) parallelise the generation of modular reasoning steps. Additionally, we introduce an asynchronous scheduler that decouples reasoning from action decoding, further boosting responsiveness. Fast ECoT requires no model changes or additional training and integrates easily into existing VLA pipelines. Experiments in both simulation (LIBERO) and real-world robot tasks show up to a 7.5% reduction in latency with comparable or improved task success rate and reasoning faithfulness, bringing ECoT policies closer to practical real-time deployment.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA",
            "[T]chain-of-thought"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] CronusVLA: Transferring Latent Motion Across Time for Multi-Frame Prediction in Manipulation",
      "arxiv_id": "2506.19816",
      "arxiv_url": "https://arxiv.org/pdf/2506.19816",
      "summary": "Recent vision-language-action (VLA) models built on pretrained vision-language models (VLMs) have demonstrated strong performance in robotic manipulation. However, these models remain constrained by the single-frame image paradigm and fail to fully leverage the temporal information offered by multi-frame histories, as directly feeding multiple frames into VLM backbones incurs substantial computational overhead and inference latency. We propose CronusVLA, a unified framework that extends single-frame VLA models to the multi-frame paradigm. CronusVLA follows a two-stage process: (1) Single-frame pretraining on large-scale embodied datasets with autoregressive prediction of action tokens, establishing an effective embodied vision-language foundation; (2) Multi-frame post-training, which adapts the prediction of the vision-language backbone from discrete tokens to learnable features, and aggregates historical information via feature chunking. CronusVLA effectively addresses the existing challenges of multi-frame modeling while enhancing performance and observational robustness. To evaluate the robustness under temporal and spatial disturbances, we introduce SimplerEnv-OR, a novel benchmark featuring 24 types of observational disturbances and 120 severity levels. Experiments across three embodiments in simulated and real-world environments demonstrate that CronusVLA achieves leading performance and superior robustness, with a 70.9% success rate on SimplerEnv, a 26.8% improvement over OpenVLA on LIBERO, and the highest robustness score on SimplerEnv-OR. These results highlight the potential of efficient multi-frame adaptation in VLA models for more powerful and robust real-world deployment.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA",
            "OpenVLA"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision",
      "arxiv_id": "2509.05578",
      "arxiv_url": "https://arxiv.org/pdf/2509.05578",
      "summary": "Multimodal large language models (MLLMs) have shown strong vision-language reasoning abilities but still lack robust 3D spatial understanding, which is critical for autonomous driving. This limitation stems from two key challenges: (1) the difficulty of constructing accessible yet effective 3D representations without expensive manual annotations, and (2) the loss of fine-grained spatial details in VLMs due to the absence of large-scale 3D vision-language pretraining. To address these challenges, we propose OccVLA, a novel framework that integrates 3D occupancy representations into a unified multimodal reasoning process. Unlike prior approaches that rely on explicit 3D inputs, OccVLA treats dense 3D occupancy as both a predictive output and a supervisory signal, enabling the model to learn fine-grained spatial structures directly from 2D visual inputs. The occupancy predictions are regarded as implicit reasoning processes and can be skipped during inference without performance degradation, thereby adding no extra computational overhead. OccVLA achieves state-of-the-art results on the nuScenes benchmark for trajectory planning and demonstrates superior performance on 3D visual question-answering tasks, offering a scalable, interpretable, and fully vision-based solution for autonomous driving.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "large language model",
            "multimodal"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": [
        "autonomous driving",
        "question answering"
      ]
    },
    {
      "title": "[2025] Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model",
      "arxiv_id": "2510.12276",
      "arxiv_url": "https://arxiv.org/pdf/2510.12276",
      "summary": "Vision-language-action (VLA) models have recently shown strong potential in enabling robots to follow language instructions and execute precise actions. However, most VLAs are built upon vision-language models pretrained solely on 2D data, which lack accurate spatial awareness and hinder their ability to operate in the 3D physical world. Existing solutions attempt to incorporate explicit 3D sensor inputs such as depth maps or point clouds, but these approaches face challenges due to sensor noise, hardware heterogeneity, and incomplete depth coverage in existing datasets. Alternative methods that estimate 3D cues from 2D images also suffer from the limited performance of depth estimators. We propose Spatial Forcing (SF), a simple yet effective alignment strategy that implicitly forces VLA models to develop spatial comprehension capabilities without relying on explicit 3D inputs or depth estimators. SF aligns intermediate visual embeddings of VLAs with geometric representations produced by pretrained 3D foundation models. By enforcing alignment at intermediate layers, SF guides VLAs to encode richer spatial representations that enhance action precision. Extensive experiments in simulation and real-world environments demonstrate that SF achieves state-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF further accelerates training by up to 3.8x and improves data efficiency across diverse robotic tasks. Project page is at https://spatial-forcing.github.io/",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/OpenHelix-Team/Spatial-Forcing",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA",
            "foundation model"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation",
      "arxiv_id": "2409.12514",
      "arxiv_url": "https://arxiv.org/pdf/2409.12514",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation",
      "arxiv_id": "2411.19650",
      "arxiv_url": "https://www.arxiv.org/pdf/2411.19650",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] RoboMM: All-in-One Multimodal Large Model for Robotic Manipulation",
      "arxiv_id": "2412.07215",
      "arxiv_url": "https://arxiv.org/pdf/2412.07215",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2023] VIMA: General Robot Manipulation with Multimodal Prompts",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2023",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [IJRR 25] Multimodal Spatial Language Maps for Robot Navigation and Manipulation",
      "arxiv_id": "2506.06862",
      "arxiv_url": "https://arxiv.org/pdf/2506.06862",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [RSS 25] NaVILA: Legged Robot Vision-Language-Action Model for Navigation",
      "arxiv_id": "2412.04453",
      "arxiv_url": "https://arxiv.org/pdf/2412.04453",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "RSS",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]legged robot"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged Information Guidance",
      "arxiv_id": "2505.08712",
      "arxiv_url": "https://arxiv.org/pdf/2505.08712",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]sim-to-real"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy",
            "[T]privileged information"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [RSS 25] Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation",
      "arxiv_id": "2503.02881",
      "arxiv_url": "https://arxiv.org/pdf/2503.02881",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "RSS",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]policy learning",
            "[T]diffusion policy"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] FlowDreamer: A RGB-D World Model with Flow-based Motion Representations for Robot Manipulation",
      "arxiv_id": "2505.10075",
      "arxiv_url": "https://arxiv.org/pdf/2505.10075",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model",
            "[T]dreamer"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Latent Action Diffusion for Cross-Embodiment Manipulation",
      "arxiv_id": "2506.14608",
      "arxiv_url": "https://arxiv.org/pdf/2506.14608",
      "summary": "End-to-end learning is emerging as a powerful paradigm for robotic manipulation, but its effectiveness is limited by data scarcity and the heterogeneity of action spaces across robot embodiments. In particular, diverse action spaces across different end-effectors create barriers for cross-embodiment learning and skill transfer. We address this challenge through diffusion policies learned in a latent action space that unifies diverse end-effector actions. We first show that we can learn a semantically aligned latent action space for anthropomorphic robotic hands, a human hand, and a parallel jaw gripper using encoders trained with a contrastive loss. Second, we show that by using our proposed latent action space for co-training on manipulation data from different end-effectors, we can utilize a single policy for multi-robot control and obtain up to 25.3% improved manipulation success rates, indicating successful skill transfer despite a significant embodiment gap. Our approach using latent cross-embodiment policies presents a new method to unify different action spaces across embodiments, enabling efficient multi-robot control and data sharing across robot setups. This unified representation significantly reduces the need for extensive data collection for each new robot morphology, accelerates generalization across embodiments, and ultimately facilitates more scalable and efficient robotic learning.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "[T]cross-embodiment"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "7_retargeting"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Learning Generalizable Language-Conditioned Cloth Manipulation from Long Demonstrations",
      "arxiv_id": "2503.04557",
      "arxiv_url": "https://arxiv.org/pdf/2503.04557",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]language conditioned"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Look Before You Leap: Using Serialized State Machine for Language Conditioned Robotic Manipulation",
      "arxiv_id": "2503.05114",
      "arxiv_url": "https://arxiv.org/pdf/2503.05114",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]language conditioned"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] LLM+MAP: Bimanual Robot Task Planning using Large Language Models and Planning Domain Definition Language",
      "arxiv_id": "2503.17309",
      "arxiv_url": "https://arxiv.org/pdf/2503.17309",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bi-manual"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Chain-of-Modality: Learning Manipulation Programs from Multimodal Human Videos with Vision-Language-Models",
      "arxiv_id": "2504.13351",
      "arxiv_url": "https://arxiv.org/pdf/2504.13351",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Task",
      "arxiv_id": "2412.18194",
      "arxiv_url": "https://arxiv.org/pdf/2412.18194",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]language conditioned"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.12] Mimicking-Bench: A Benchmark for Generalizable Humanoid-Scene Interaction Learning via Human Mimicking [Benchmark]  [[project](https://mimicking-bench.github.io/)]",
      "arxiv_id": "2412.17730",
      "arxiv_url": "https://arxiv.org/abs/2412.17730",
      "summary": "Learning generic skills for humanoid robots interacting with 3D scenes by mimicking human data is a key research challenge with significant implications for robotics and real-world applications. However, existing methodologies and benchmarks are constrained by the use of small-scale, manually collected demonstrations, lacking the general dataset and benchmark support necessary to explore scene geometry generalization effectively. To address this gap, we introduce Mimicking-Bench, the first comprehensive benchmark designed for generalizable humanoid-scene interaction learning through mimicking large-scale human animation references. Mimicking-Bench includes six household full-body humanoid-scene interaction tasks, covering 11K diverse object shapes, along with 20K synthetic and 3K real-world human interaction skill references. We construct a complete humanoid skill learning pipeline and benchmark approaches for motion retargeting, motion tracking, imitation learning, and their various combinations. Extensive experiments highlight the value of human mimicking for skill learning, revealing key challenges and research directions.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot"
          ],
          "score": 8.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "motion retargeting"
          ],
          "score": 3.0
        },
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "motion tracking"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 14.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "7_retargeting",
        "8_physics_animation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] GBC: Generalized Behavior-Cloning Framework for Whole-Body Humanoid Imitation",
      "arxiv_id": "2508.09960",
      "arxiv_url": "https://www.arxiv.org/abs/2508.09960",
      "summary": "The creation of human-like humanoid robots is hindered by a fundamental fragmentation: data processing and learning algorithms are rarely universal across different robot morphologies. This paper introduces the Generalized Behavior Cloning (GBC) framework, a comprehensive and unified solution designed to solve this end-to-end challenge. GBC establishes a complete pathway from human motion to robot action through three synergistic innovations. First, an adaptive data pipeline leverages a differentiable IK network to automatically retarget any human MoCap data to any humanoid. Building on this foundation, our novel DAgger-MMPPO algorithm with its MMTransformer architecture learns robust, high-fidelity imitation policies. To complete the ecosystem, the entire framework is delivered as an efficient, open-source platform based on Isaac Lab, empowering the community to deploy the full workflow via simple configuration scripts. We validate the power and generality of GBC by training policies on multiple heterogeneous humanoids, demonstrating excellent performance and transfer to novel motions. This work establishes the first practical and unified pathway for creating truly generalized humanoid controllers.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "humanoid control"
          ],
          "score": 10.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]behavior cloning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 14.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Think on your feet: Seamless Transition between Human-like Locomotion in Response to Changing Commands",
      "arxiv_id": "2502.18901",
      "arxiv_url": "https://arxiv.org/pdf/2502.18901",
      "summary": "While it is relatively easier to train humanoid robots to mimic specific locomotion skills, it is more challenging to learn from various motions and adhere to continuously changing commands. These robots must accurately track motion instructions, seamlessly transition between a variety of movements, and master intermediate motions not present in their reference data. In this work, we propose a novel approach that integrates human-like motion transfer with precise velocity tracking by a series of improvements to classical imitation learning. To enhance generalization, we employ the Wasserstein divergence criterion (WGAN-div). Furthermore, a Hybrid Internal Model provides structured estimates of hidden states and velocity to enhance mobile stability and environment adaptability, while a curiosity bonus fosters exploration. Our comprehensive method promises highly human-like locomotion that adapts to varying velocity requirements, direct generalization to unseen motions and multitasking, as well as zero-shot transfer to the simulator and the real world across different terrains. These advancements are validated through simulations across various robot models and extensive real-world experiments.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid",
            "humanoid robot",
            "[T]locomotion"
          ],
          "score": 10.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "zero-shot transfer"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 14.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.07] GRUtopia: Dream General Robots in a City at Scale [benchmark] [[doc](https://grutopia.github.io/)]",
      "arxiv_id": "2407.10943",
      "arxiv_url": "https://arxiv.org/abs/2407.10943",
      "summary": "Recent works have been exploring the scaling laws in the field of Embodied AI. Given the prohibitive costs of collecting real-world data, we believe the Simulation-to-Real (Sim2Real) paradigm is a crucial step for scaling the learning of embodied models. This paper introduces project GRUtopia, the first simulated interactive 3D society designed for various robots. It features several advancements: (a) The scene dataset, GRScenes, includes 100k interactive, finely annotated scenes, which can be freely combined into city-scale environments. In contrast to previous works mainly focusing on home, GRScenes covers 89 diverse scene categories, bridging the gap of service-oriented environments where general robots would be initially deployed. (b) GRResidents, a Large Language Model (LLM) driven Non-Player Character (NPC) system that is responsible for social interaction, task generation, and task assignment, thus simulating social scenarios for embodied AI applications. (c) The benchmark, GRBench, supports various robots but focuses on legged robots as primary agents and poses moderately challenging tasks involving Object Loco-Navigation, Social Loco-Navigation, and Loco-Manipulation. We hope that this work can alleviate the scarcity of high-quality data in this field and provide a more comprehensive assessment of Embodied AI research. The project is available at https://github.com/OpenRobotLab/GRUtopia.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Manipulation, Awesome Humanoid Robot Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "legged robot",
            "manipulation",
            "loco-manipulation",
            "sim2real"
          ],
          "score": 8.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "embodied AI",
            "large language model"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] GaussGym: An open-source real-to-sim framework for learning locomotion from pixels",
      "arxiv_id": "2510.15352",
      "arxiv_url": "https://arxiv.org/abs/2510.15352",
      "summary": "We present a novel approach for photorealistic robot simulation that integrates 3D Gaussian Splatting as a drop-in renderer within vectorized physics simulators such as IsaacGym. This enables unprecedented speed -- exceeding 100,000 steps per second on consumer GPUs -- while maintaining high visual fidelity, which we showcase across diverse tasks. We additionally demonstrate its applicability in a sim-to-real robotics setting. Beyond depth-based sensing, our results highlight how rich visual semantics improve navigation and decision-making, such as avoiding undesirable regions. We further showcase the ease of incorporating thousands of environments from iPhone scans, large-scale scene datasets (e.g., GrandTour, ARKit), and outputs from generative video models like Veo, enabling rapid creation of realistic training worlds. This work bridges high-throughput simulation and high-fidelity perception, advancing scalable and generalizable robot learning. All code and data will be open-sourced for the community to build upon. Videos, code, and data available at https://escontrela.me/gauss_gym/.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]locomotion",
            "sim-to-real"
          ],
          "score": 8.0
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "gaussian splatting",
            "splatting"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends",
      "arxiv_id": "2506.20966",
      "arxiv_url": "https://arxiv.org/pdf/2506.20966",
      "summary": "Vision-language-action (VLA) models extend vision-language models (VLM) by integrating action generation modules for robotic manipulation. Leveraging strengths of VLM in vision perception and instruction understanding, VLA models exhibit promising generalization across diverse manipulation tasks. However, applications demanding high precision and accuracy reveal performance gaps without further adaptation. Evidence from multiple domains highlights the critical role of post-training to align foundational models with downstream applications, spurring extensive research on post-training VLA models. VLA model post-training aims to address the challenge of improving an embodiment's ability to interact with the environment for the given tasks, analogous to the process of humans motor skills acquisition. Accordingly, this paper reviews post-training strategies for VLA models through the lens of human motor learning, focusing on three dimensions: environments, embodiments, and tasks. A structured taxonomy is introduced aligned with human learning mechanisms: (1) enhancing environmental perception, (2) improving embodiment awareness, (3) deepening task comprehension, and (4) multi-component integration. Finally, key challenges and trends in post-training VLA models are identified, establishing a conceptual framework to guide future research. This work delivers both a comprehensive overview of current VLA model post-training methods from a human motor learning perspective and practical insights for VLA model development. (Project website: https://github.com/AoqunJin/Awesome-VLA-Post-Training)",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "[T]VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [CVPR 25] RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete",
      "arxiv_id": "2502.21257",
      "arxiv_url": "https://arxiv.org/pdf/2502.21257",
      "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have shown remarkable capabilities across various multimodal contexts. However, their application in robotic scenarios, particularly for long-horizon manipulation tasks, reveals significant limitations. These limitations arise from the current MLLMs lacking three essential robotic brain capabilities: Planning Capability, which involves decomposing complex manipulation instructions into manageable sub-tasks; Affordance Perception, the ability to recognize and interpret the affordances of interactive objects; and Trajectory Prediction, the foresight to anticipate the complete manipulation trajectory necessary for successful execution. To enhance the robotic brain's core capabilities from abstract to concrete, we introduce ShareRobot, a high-quality heterogeneous dataset that labels multi-dimensional information such as task planning, object affordance, and end-effector trajectory. ShareRobot's diversity and accuracy have been meticulously refined by three human annotators. Building on this dataset, we developed RoboBrain, an MLLM-based model that combines robotic and general multi-modal data, utilizes a multi-stage training strategy, and incorporates long videos and high-resolution images to improve its robotic manipulation capabilities. Extensive experiments demonstrate that RoboBrain achieves state-of-the-art performance across various robotic tasks, highlighting its potential to advance robotic brain capabilities.",
      "authors": [],
      "year": "2025",
      "venue": "CVPR",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "affordance"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "multimodal"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding",
      "arxiv_id": "2503.02310",
      "arxiv_url": "https://arxiv.org/pdf/2503.02310",
      "summary": "Vision-Language-Action (VLA) models demonstrate remarkable potential for generalizable robotic manipulation. The performance of VLA models can be improved by integrating with action chunking, a critical technique for effective control. However, action chunking linearly scales up action dimensions in VLA models with increased chunking sizes. This reduces the inference efficiency. To tackle this problem, we propose PD-VLA, the first parallel decoding framework for VLA models integrated with action chunking. Our framework reformulates autoregressive decoding as a nonlinear system solved by parallel fixed-point iterations. This approach preserves model performance with mathematical guarantees while significantly improving decoding speed. In addition, it enables training-free acceleration without architectural changes, as well as seamless synergy with existing acceleration techniques. Extensive simulations validate that our PD-VLA maintains competitive success rates while achieving 2.52 times execution frequency on manipulators (with 7 degrees of freedom) compared with the fundamental VLA model. Furthermore, we experimentally identify the most effective settings for acceleration. Finally, real-world experiments validate its high applicability across different tasks.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [RSS 25] Gripper Keypose and Object Pointflow as Interfaces for Bimanual Robotic Manipulation",
      "arxiv_id": "2504.17784",
      "arxiv_url": "https://arxiv.org/pdf/2504.17784",
      "summary": "Bimanual manipulation is a challenging yet crucial robotic capability, demanding precise spatial localization and versatile motion trajectories, which pose significant challenges to existing approaches. Existing approaches fall into two categories: keyframe-based strategies, which predict gripper poses in keyframes and execute them via motion planners, and continuous control methods, which estimate actions sequentially at each timestep. The keyframe-based method lacks inter-frame supervision, struggling to perform consistently or execute curved motions, while the continuous method suffers from weaker spatial perception. To address these issues, this paper introduces an end-to-end framework PPI (keyPose and Pointflow Interface), which integrates the prediction of target gripper poses and object pointflow with the continuous actions estimation. These interfaces enable the model to effectively attend to the target manipulation area, while the overall framework guides diverse and collision-free trajectories. By combining interface predictions with continuous actions estimation, PPI demonstrates superior performance in diverse bimanual manipulation tasks, providing enhanced spatial localization and satisfying flexibility in handling movement restrictions. In extensive evaluations, PPI significantly outperforms prior methods in both simulated and real-world experiments, achieving state-of-the-art performance with a +16.1% improvement on the RLBench2 simulation benchmark and an average of +27.5% gain across four challenging real-world tasks. Notably, PPI exhibits strong stability, high precision, and remarkable generalization capabilities in real-world scenarios. Project page: https://yuyinyang3y.github.io/PPI/",
      "authors": [],
      "year": "2025",
      "venue": "RSS",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]bi-manual",
            "bimanual manipulation"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents",
      "arxiv_id": "2505.23450",
      "arxiv_url": "https://arxiv.org/pdf/2505.23450",
      "summary": "Long-horizon robotic manipulation poses significant challenges for autonomous systems, requiring extended reasoning, precise execution, and robust error recovery across complex sequential tasks. Current approaches, whether based on static planning or end-to-end visuomotor policies, suffer from error accumulation and lack effective verification mechanisms during execution, limiting their reliability in real-world scenarios. We present Agentic Robot, a brain-inspired framework that addresses these limitations through Standardized Action Procedure (SAP)--a novel coordination protocol governing component interactions throughout manipulation tasks. Drawing inspiration from Standardized Operating Procedures (SOPs) in human organizations, SAP establishes structured workflows for planning, execution, and verification phases. Our architecture comprises three specialized components: (1) a large reasoning model that decomposes high-level instructions into semantically coherent subgoals, (2) a vision-language-action executor that generates continuous control commands from real-time visual inputs, and (3) a temporal verifier that enables autonomous progression and error recovery through introspective assessment. This SAP-driven closed-loop design supports dynamic self-verification without external supervision. On the LIBERO benchmark, Agentic Robot achieves state-of-the-art performance with an average success rate of 79.6%, outperforming SpatialVLA by 6.1% and OpenVLA by 7.4% on long-horizon tasks. These results demonstrate that SAP-driven coordination between specialized components enhances both performance and interpretability in sequential manipulation, suggesting significant potential for reliable autonomous systems. Project Github: https://agentic-robot.github.io.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "OpenVLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] RationalVLA: A Rational Vision-Language-Action Model with Dual System",
      "arxiv_id": "2506.10826",
      "arxiv_url": "https://arxiv.org/pdf/2506.10826",
      "summary": "A fundamental requirement for real-world robotic deployment is the ability to understand and respond to natural language instructions. Existing language-conditioned manipulation tasks typically assume that instructions are perfectly aligned with the environment. This assumption limits robustness and generalization in realistic scenarios where instructions may be ambiguous, irrelevant, or infeasible. To address this problem, we introduce RAtional MAnipulation (RAMA), a new benchmark that challenges models with both unseen executable instructions and defective ones that should be rejected. In RAMA, we construct a dataset with over 14,000 samples, including diverse defective instructions spanning six dimensions: visual, physical, semantic, motion, safety, and out-of-context. We further propose the Rational Vision-Language-Action model (RationalVLA). It is a dual system for robotic arms that integrates the high-level vision-language model with the low-level manipulation policy by introducing learnable latent space embeddings. This design enables RationalVLA to reason over instructions, reject infeasible commands, and execute manipulation effectively. Experiments demonstrate that RationalVLA outperforms state-of-the-art baselines on RAMA by a 14.5% higher success rate and 0.94 average task length, while maintaining competitive performance on standard manipulation tasks. Real-world trials further validate its effectiveness and robustness in practical applications. Our project page is https://irpn-eai.github.io/RationalVLA.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "language conditioned"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] MinD: Unified Visual Imagination and Control via Hierarchical World Model",
      "arxiv_id": "2506.18897",
      "arxiv_url": "https://arxiv.org/pdf/2506.18897",
      "summary": "Video Generation Models (VGMs) have become powerful backbones for Vision-Language-Action (VLA) models, leveraging large-scale pretraining for robust dynamics modeling. However, current methods underutilize their distribution modeling capabilities for predicting future states. Two challenges hinder progress: integrating generative processes into feature learning is both technically and conceptually underdeveloped, and naive frame-by-frame video diffusion is computationally inefficient for real-time robotics. To address these, we propose Manipulate in Dream (MinD), a dual-system world model for real-time, risk-aware planning. MinD uses two asynchronous diffusion processes: a low-frequency visual generator (LoDiff) that predicts future scenes and a high-frequency diffusion policy (HiDiff) that outputs actions. Our key insight is that robotic policies do not require fully denoised frames but can rely on low-resolution latents generated in a single denoising step. To connect early predictions to actions, we introduce DiffMatcher, a video-action alignment module with a novel co-training strategy that synchronizes the two diffusion models. MinD achieves a 63% success rate on RL-Bench, 60% on real-world Franka tasks, and operates at 11.3 FPS, demonstrating the efficiency of single-step latent features for control signals. Furthermore, MinD identifies 74% of potential task failures in advance, providing real-time safety signals for monitoring and intervention. This work establishes a new paradigm for efficient and reliable robotic manipulation using generative world models.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "diffusion policy",
            "[T]world model"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] MultiGen: Using Multimodal Generation in Simulation to Learn Multimodal Policies in Real",
      "arxiv_id": "2507.02864",
      "arxiv_url": "https://arxiv.org/pdf/2507.02864",
      "summary": "Robots must integrate multiple sensory modalities to act effectively in the real world. Yet, learning such multimodal policies at scale remains challenging. Simulation offers a viable solution, but while vision has benefited from high-fidelity simulators, other modalities (e.g. sound) can be notoriously difficult to simulate. As a result, sim-to-real transfer has succeeded primarily in vision-based tasks, with multimodal transfer still largely unrealized. In this work, we tackle these challenges by introducing MultiGen, a framework that integrates large-scale generative models into traditional physics simulators, enabling multisensory simulation. We showcase our framework on the dynamic task of robot pouring, which inherently relies on multimodal feedback. By synthesizing realistic audio conditioned on simulation video, our method enables training on rich audiovisual trajectories -- without any real robot data. We demonstrate effective zero-shot transfer to real-world pouring with novel containers and liquids, highlighting the potential of generative modeling to both simulate hard-to-model modalities and close the multimodal sim-to-real gap.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "sim-to-real"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal",
            "zero-shot transfer"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] GeoVLA: Empowering 3D Representations in Vision-Language-Action Models",
      "arxiv_id": "2508.09071",
      "arxiv_url": "https://arxiv.org/pdf/2508.09071",
      "summary": "Vision-Language-Action (VLA) models have emerged as a promising approach for enabling robots to follow language instructions and predict corresponding actions. However, current VLA models mainly rely on 2D visual inputs, neglecting the rich geometric information in the 3D physical world, which limits their spatial awareness and adaptability. In this paper, we present GeoVLA, a novel VLA framework that effectively integrates 3D information to advance robotic manipulation. It uses a vision-language model (VLM) to process images and language instructions,extracting fused vision-language embeddings. In parallel, it converts depth maps into point clouds and employs a customized point encoder, called Point Embedding Network, to generate 3D geometric embeddings independently. These produced embeddings are then concatenated and processed by our proposed spatial-aware action expert, called 3D-enhanced Action Expert, which combines information from different sensor modalities to produce precise action sequences. Through extensive experiments in both simulation and real-world environments, GeoVLA demonstrates superior performance and robustness. It achieves state-of-the-art results in the LIBERO and ManiSkill2 simulation benchmarks and shows remarkable robustness in real-world tasks requiring height adaptability, scale awareness and viewpoint invariance.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy",
      "arxiv_id": "2508.13103",
      "arxiv_url": "https://arxiv.org/pdf/2508.13103",
      "summary": "Vision-Language-Action (VLA) models frequently encounter challenges in generalizing to real-world environments due to inherent discrepancies between observation and action spaces. Although training data are collected from diverse camera perspectives, the models typically predict end-effector poses within the robot base coordinate frame, resulting in spatial inconsistencies. To mitigate this limitation, we introduce the Observation-Centric VLA (OC-VLA) framework, which grounds action predictions directly in the camera observation space. Leveraging the camera's extrinsic calibration matrix, OC-VLA transforms end-effector poses from the robot base coordinate system into the camera coordinate system, thereby unifying prediction targets across heterogeneous viewpoints. This lightweight, plug-and-play strategy ensures robust alignment between perception and action, substantially improving model resilience to camera viewpoint variations. The proposed approach is readily compatible with existing VLA architectures, requiring no substantial modifications. Comprehensive evaluations on both simulated and real-world robotic manipulation tasks demonstrate that OC-VLA accelerates convergence, enhances task success rates, and improves cross-view generalization. The code will be publicly available.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI",
      "arxiv_id": "2510.05684",
      "arxiv_url": "https://arxiv.org/abs/2510.05684",
      "summary": "Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physical trajectory collection. Desktop environments -- particularly gaming -- offer a compelling alternative: they provide rich sensorimotor interactions at scale while maintaining the structured observation-action coupling essential for embodied learning. We present D2E (Desktop to Embodied AI), a framework that demonstrates desktop interactions can serve as an effective pretraining substrate for robotics embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a complete pipeline from scalable desktop data collection to verified transfer in embodied domains. Our framework comprises three components: (1) the OWA Toolkit that unifies diverse desktop interactions into a standardized format with 152x compression, (2) the Generalist-IDM that achieves strong zero-shot generalization across unseen games through timestamp-based event prediction, enabling internet-scale pseudo-labeling, and (3) VAPT that transfers desktop-pretrained representations to physical manipulation and navigation. Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO manipulation and 83.3% on CANVAS navigation benchmarks. This validates that sensorimotor primitives in digital interactions exhibit sufficient invariance to transfer meaningfully to physical embodied tasks, establishing desktop pretraining as a practical paradigm for robotics. We will make all our work public, including the OWA toolkit, datasets of human-collected and pseudo-labeled, and VAPT-trained models available at https://worv-ai.github.io/d2e/",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/worv-ai/D2E",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]embodied AI",
            "large language model"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2023] Diffusion policy: Visuomotor policy learning via action diffusion",
      "arxiv_id": "2303.04137",
      "arxiv_url": "https://arxiv.org/pdf/2303.04137",
      "summary": "This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details is publicly available diffusion-policy.cs.columbia.edu",
      "authors": [],
      "year": "2023",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]policy learning",
            "[T]diffusion policy"
          ],
          "score": 9.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.08] ACE: A Cross-platform Visual-Exoskeletons for Low-Cost Dexterous Teleoperation [teleop] [[project](https://ace-teleop.github.io/)]",
      "arxiv_id": "2408.11805",
      "arxiv_url": "https://arxiv.org/abs/2408.11805",
      "summary": "Learning from demonstrations has shown to be an effective approach to robotic manipulation, especially with the recently collected large-scale robot data with teleoperation systems. Building an efficient teleoperation system across diverse robot platforms has become more crucial than ever. However, there is a notable lack of cost-effective and user-friendly teleoperation systems for different end-effectors, e.g., anthropomorphic robot hands and grippers, that can operate across multiple platforms. To address this issue, we develop ACE, a cross-platform visual-exoskeleton system for low-cost dexterous teleoperation. Our system utilizes a hand-facing camera to capture 3D hand poses and an exoskeleton mounted on a portable base, enabling accurate real-time capture of both finger and wrist poses. Compared to previous systems, which often require hardware customization according to different robots, our single system can generalize to humanoid hands, arm-hands, arm-gripper, and quadruped-gripper systems with high-precision teleoperation. This enables imitation learning for complex manipulation tasks on diverse platforms.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "https://github.com/ACETeleop/ACETeleop",
      "source_repo": "Awesome Humanoid Manipulation, Awesome Humanoid Robot Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped",
            "humanoid",
            "manipulation",
            "[T]teleoperation"
          ],
          "score": 12.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2022] MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control. [imitation]",
      "arxiv_id": "2208.07363",
      "arxiv_url": "https://arxiv.org/pdf/2208.07363.pdf",
      "summary": "Simulated humanoids are an appealing research domain due to their physical capabilities. Nonetheless, they are also challenging to control, as a policy must drive an unstable, discontinuous, and high-dimensional physical system. One widely studied approach is to utilize motion capture (MoCap) data to teach the humanoid agent low-level skills (e.g., standing, walking, and running) that can then be re-used to synthesize high-level behaviors. However, even with MoCap data, controlling simulated humanoids remains very hard, as MoCap data offers only kinematic information. Finding physical control inputs to realize the demonstrated motions requires computationally intensive methods like reinforcement learning. Thus, despite the publicly available MoCap data, its utility has been limited to institutions with large-scale compute. In this work, we dramatically lower the barrier for productive research on this topic by training and releasing high-quality agents that can track over three hours of MoCap data for a simulated humanoid in the dm_control physics-based environment. We release MoCapAct (Motion Capture with Actions), a dataset of these expert agents and their rollouts, which contain proprioceptive observations and actions. We demonstrate the utility of MoCapAct by using it to train a single hierarchical policy capable of tracking the entire MoCap dataset within dm_control and show the learned low-level component can be re-used to efficiently learn downstream high-level tasks. Finally, we use MoCapAct to train an autoregressive GPT model and show that it can control a simulated humanoid to perform natural motion completion given a motion prompt.\n  Videos of the results and links to the code and dataset are available at https://microsoft.github.io/MoCapAct.",
      "authors": [],
      "year": "2022",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid control"
          ],
          "score": 12.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[Science Robotics] Learning Agile and Dynamic Motor Skills for Legged Robots.",
      "arxiv_id": "1901.08652",
      "arxiv_url": "https://arxiv.org/pdf/1901.08652.pdf",
      "summary": "Legged robots pose one of the greatest challenges in robotics. Dynamic and agile maneuvers of animals cannot be imitated by existing methods that are crafted by humans. A compelling alternative is reinforcement learning, which requires minimal craftsmanship and promotes the natural evolution of a control policy. However, so far, reinforcement learning research for legged robots is mainly limited to simulation, and only few and comparably simple examples have been deployed on real systems. The primary reason is that training with real robots, particularly with dynamically balancing systems, is complicated and expensive. In the present work, we introduce a method for training a neural network policy in simulation and transferring it to a state-of-the-art legged system, thereby leveraging fast, automated, and cost-effective data generation schemes. The approach is applied to the ANYmal robot, a sophisticated medium-dog-sized quadrupedal system. Using policies trained in simulation, the quadrupedal machine achieves locomotion skills that go beyond what had been achieved with prior methods: ANYmal is capable of precisely and energy-efficiently following high-level body velocity commands, running faster than before, and recovering from falling even in complex configurations.",
      "authors": [],
      "year": "",
      "venue": "arxiv",
      "code_url": "https://github.com/junja94/anymal_science_robotics_supplementary",
      "source_repo": "Awesome Humanoid Learning, Awesome Humanoid Robot Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped",
            "[T]legged robot",
            "locomotion",
            "ANYmal"
          ],
          "score": 12.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] A Comprehensive Survey on World Models for Embodied AI",
      "arxiv_id": "2510.16732",
      "arxiv_url": "https://www.arxiv.org/pdf/2510.16732",
      "summary": "Embodied AI requires agents that perceive, act, and anticipate how actions reshape future world states. World models serve as internal simulators that capture environment dynamics, enabling forward and counterfactual rollouts to support perception, prediction, and decision making. This survey presents a unified framework for world models in embodied AI. Specifically, we formalize the problem setting and learning objectives, and propose a three-axis taxonomy encompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2) Temporal Modeling, Sequential Simulation and Inference vs. Global Difference Prediction; (3) Spatial Representation, Global Latent Vector, Token Feature Sequence, Spatial Latent Grid, and Decomposed Rendering Representation. We systematize data resources and metrics across robotics, autonomous driving, and general video settings, covering pixel prediction quality, state-level understanding, and task performance. Furthermore, we offer a quantitative comparison of state-of-the-art models and distill key open challenges, including the scarcity of unified datasets and the need for evaluation metrics that assess physical consistency over pixel fidelity, the trade-off between model performance and the computational efficiency required for real-time control, and the core modeling difficulty of achieving long-horizon temporal consistency while mitigating error accumulation. Finally, we maintain a curated bibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]embodied AI"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": [
        "autonomous driving"
      ]
    },
    {
      "title": "[2025] [Physical Intelligence] Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better",
      "arxiv_id": "2505.23705",
      "arxiv_url": "https://arxiv.org/pdf/2505.23705",
      "summary": "Vision-language-action (VLA) models provide a powerful approach to training control policies for physical systems, such as robots, by combining end-to-end learning with transfer of semantic knowledge from web-scale vision-language model (VLM) training. However, the constraints of real-time control are often at odds with the design of VLMs: the most powerful VLMs have tens or hundreds of billions of parameters, presenting an obstacle to real-time inference, and operate on discrete tokens rather than the continuous-valued outputs that are required for controlling robots. To address this challenge, recent VLA models have used specialized modules for efficient continuous control, such as action experts or continuous output heads, which typically require adding new untrained parameters to the pretrained VLM backbone. While these modules improve real-time and control capabilities, it remains an open question whether they preserve or degrade the semantic knowledge contained in the pretrained VLM, and what effect they have on the VLA training dynamics. In this paper, we study this question in the context of VLAs that include a continuous diffusion or flow matching action expert, showing that naively including such experts significantly harms both training speed and knowledge transfer. We provide an extensive analysis of various design choices, their impact on performance and knowledge transfer, and propose a technique for insulating the VLM backbone during VLA training that mitigates this issue. Videos are available at https://pi.website/research/knowledge_insulation.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "flow matching"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [PsiBot] DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping",
      "arxiv_id": "2502.20900",
      "arxiv_url": "https://arxiv.org/pdf/2502.20900",
      "summary": "Dexterous grasping remains a fundamental yet challenging problem in robotics. A general-purpose robot must be capable of grasping diverse objects in arbitrary scenarios. However, existing research typically relies on restrictive assumptions, such as single-object settings or limited environments, showing constrained generalization. We present DexGraspVLA, a hierarchical framework for robust generalization in language-guided general dexterous grasping and beyond. It utilizes a pre-trained Vision-Language model as the high-level planner and learns a diffusion-based low-level Action controller. The key insight to achieve generalization lies in iteratively transforming diverse language and visual inputs into domain-invariant representations via foundation models, where imitation learning can be effectively applied due to the alleviation of domain shift. Notably, our method achieves a 90+% dexterous grasping success rate under thousands of challenging unseen cluttered scenes. Empirical analysis confirms the consistency of internal model behavior across environmental variations, validating our design. DexGraspVLA also, for the first time, simultaneously demonstrates free-form long-horizon prompt execution, robustness to adversarial objects and human disturbance, and failure recovery. Extended application to nonprehensile grasping further proves its generality. Project website: https://dexgraspvla.github.io.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "foundation model"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Diffusion Transformer Policy: Scaling Diffusion Transformer for Generalist Visual-Language-Action Learning",
      "arxiv_id": "2410.15959",
      "arxiv_url": "https://arxiv.org/pdf/2410.15959",
      "summary": "Recent large vision-language-action models pretrained on diverse robot datasets have demonstrated the potential for generalizing to new environments with a few in-domain data. However, those approaches usually predict individual discretized or continuous action by a small action head, which limits the ability in handling diverse action spaces. In contrast, we model the continuous action sequence with a large multi-modal diffusion transformer, dubbed as Diffusion Transformer Policy, in which we directly denoise action chunks by a large transformer model rather than a small action head for action embedding. By leveraging the scaling capability of transformers, the proposed approach can effectively model continuous end-effector actions across large diverse robot datasets, and achieve better generalization performance. Extensive experiments demonstrate the effectiveness and generalization of Diffusion Transformer Policy on Maniskill2, Libero, Calvin and SimplerEnv, as well as the real-world Franka arm, achieving consistent better performance on Real-to-Sim benchmark SimplerEnv, real-world Franka Arm and Libero compared to OpenVLA and Octo. Specifically, without bells and whistles, the proposed approach achieves state-of-the-art performance with only a single third-view camera stream in the Calvin task ABC-&gt;D, improving the average number of tasks completed in a row of 5 to 3.6, and the pretraining stage significantly facilitates the success sequence length on the Calvin by over 1.2.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]transformer policy"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "Octo",
            "OpenVLA"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation",
      "arxiv_id": "2502.09268",
      "arxiv_url": "https://arxiv.org/pdf/2502.09268",
      "summary": "With the rapid development of embodied artificial intelligence, significant progress has been made in vision-language-action (VLA) models for general robot decision-making. However, the majority of existing VLAs fail to account for the inevitable external perturbations encountered during deployment. These perturbations introduce unforeseen state information to the VLA, resulting in inaccurate actions and consequently, a significant decline in generalization performance. The classic internal model control (IMC) principle demonstrates that a closed-loop system with an internal model that includes external input signals can accurately track the reference input and effectively offset the disturbance. We propose a novel closed-loop VLA method GEVRM that integrates the IMC principle to enhance the robustness of robot visual manipulation. The text-guided video generation model in GEVRM can generate highly expressive future visual planning goals. Simultaneously, we evaluate perturbations by simulating responses, which are called internal embeddings and optimized through prototype contrastive learning. This allows the model to implicitly infer and distinguish perturbations from the external environment. The proposed GEVRM achieves state-of-the-art performance on both standard and perturbed CALVIN benchmarks and shows significant improvements in realistic robot tasks.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "contrastive learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] SPECI: Skill Prompts based Hierarchical Continual Imitation Learning for Robot Manipulation",
      "arxiv_id": "2504.15561",
      "arxiv_url": "https://arxiv.org/pdf/2504.15561",
      "summary": "Real-world robot manipulation in dynamic unstructured environments requires lifelong adaptability to evolving objects, scenes and tasks. Traditional imitation learning relies on static training paradigms, which are ill-suited for lifelong adaptation. Although Continual Imitation Learnin (CIL) enables incremental task adaptation while preserving learned knowledge, current CIL methods primarily overlook the intrinsic skill characteristics of robot manipulation or depend on manually defined and rigid skills, leading to suboptimal cross-task knowledge transfer. To address these issues, we propose Skill Prompts-based HiErarchical Continual Imitation Learning (SPECI), a novel end-to-end hierarchical CIL policy architecture for robot manipulation. The SPECI framework consists of a multimodal perception and fusion module for heterogeneous sensory information encoding, a high-level skill inference module for dynamic skill extraction and selection, and a low-level action execution module for precise action generation. To enable efficient knowledge transfer on both skill and task levels, SPECI performs continual implicit skill acquisition and reuse via an expandable skill codebook and an attention-driven skill selection mechanism. Furthermore, we introduce mode approximation to augment the last two modules with task-specific and task-sharing parameters, thereby enhancing task-level knowledge transfer. Extensive experiments on diverse manipulation task suites demonstrate that SPECI consistently outperforms state-of-the-art CIL methods across all evaluated metrics, revealing exceptional bidirectional knowledge transfer and superior overall performance.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [NeurIPS 25] BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models",
      "arxiv_id": "2506.07961",
      "arxiv_url": "https://arxiv.org/pdf/2506.07961",
      "summary": "Recently, leveraging pre-trained vision-language models (VLMs) for building vision-language-action (VLA) models has emerged as a promising approach to effective robot manipulation learning. However, only few methods incorporate 3D signals into VLMs for action prediction, and they do not fully leverage the spatial structure inherent in 3D data, leading to low sample efficiency. In this paper, we introduce BridgeVLA, a novel 3D VLA model that (1) projects 3D inputs to multiple 2D images, ensuring input alignment with the VLM backbone, and (2) utilizes 2D heatmaps for action prediction, unifying the input and output spaces within a consistent 2D image space. In addition, we propose a scalable pre-training method that equips the VLM backbone with the capability to predict 2D heatmaps before downstream policy learning. Extensive experiments show the proposed method is able to learn 3D manipulation efficiently and effectively. BridgeVLA outperforms state-of-the-art baseline methods across three simulation benchmarks. In RLBench, it improves the average success rate from 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better performance in challenging generalization settings, boosting the average success rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing baseline methods in terms of average success rate. In real-robot experiments, BridgeVLA outperforms a state-of-the-art baseline method by 32% on average. It generalizes robustly in multiple out-of-distribution settings, including visual disturbances and unseen instructions. Remarkably, it is able to achieve a success rate of 96.8% on 10+ tasks with only 3 trajectories per task, highlighting its extraordinary sample efficiency. Project Website:https://bridgevla.github.io/",
      "authors": [],
      "year": "2025",
      "venue": "NeurIPS",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models",
      "arxiv_id": "2506.09930",
      "arxiv_url": "https://arxiv.org/pdf/2506.09930",
      "summary": "One promise that Vision-Language-Action (VLA) models hold over traditional imitation learning for robotics is to leverage the broad generalization capabilities of large Vision-Language Models (VLMs) to produce versatile, \"generalist\" robot policies. However, current evaluations of VLAs remain insufficient. Traditional imitation learning benchmarks are unsuitable due to the lack of language instructions. Emerging benchmarks for VLAs that incorporate language often come with limited evaluation tasks and do not intend to investigate how much VLM pretraining truly contributes to the generalization capabilities of the downstream robotic policy. Meanwhile, much research relies on real-world robot setups designed in isolation by different institutions, which creates a barrier for reproducibility and accessibility. To address this gap, we introduce a unified probing suite of 50 simulation-based tasks across 10 subcategories spanning language instruction, vision, and objects. We systematically evaluate several state-of-the-art VLA architectures on this suite to understand their generalization capability. Our results show that while VLM backbones endow VLAs with robust perceptual understanding and high level planning, which we refer to as good intentions, this does not reliably translate into precise motor execution: when faced with out-of-distribution observations, policies often exhibit coherent intentions, but falter in action execution. Moreover, finetuning on action data can erode the original VLM's generalist reasoning abilities. We release our task suite and evaluation code to serve as a standardized benchmark for future VLAs and to drive research on closing the perception-to-action gap. More information, including the source code, can be found at https://ai4ce.github.io/INT-ACT/",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Time- Diffusion Policy with Action Discrimination for Robotic Manipulation",
      "arxiv_id": "2506.09422",
      "arxiv_url": "https://arxiv.org/pdf/2506.09422",
      "summary": "In many complex scenarios, robotic manipulation relies on generative models to estimate the distribution of multiple successful actions. As the diffusion model has better training robustness than other generative models, it performs well in imitation learning through successful robot demonstrations. However, the diffusion-based policy methods typically require significant time to iteratively denoise robot actions, which hinders real-time responses in robotic manipulation. Moreover, existing diffusion policies model a time-varying action denoising process, whose temporal complexity increases the difficulty of model training and leads to suboptimal action accuracy. To generate robot actions efficiently and accurately, we present the Time-Unified Diffusion Policy (TUDP), which utilizes action recognition capabilities to build a time-unified denoising process. On the one hand, we build a time-unified velocity field in action space with additional action discrimination information. By unifying all timesteps of action denoising, our velocity field reduces the difficulty of policy learning and speeds up action generation. On the other hand, we propose an action-wise training method, which introduces an action discrimination branch to supply additional action discrimination information. Through action-wise training, the TUDP implicitly learns the ability to discern successful actions to better denoising accuracy. Our method achieves state-of-the-art performance on RLBench with the highest success rate of 82.6% on a multi-view setup and 83.8% on a single-view setup. In particular, when using fewer denoising iterations, TUDP achieves a more significant improvement in success rate. Additionally, TUDP can produce accurate actions for a wide range of real-world tasks.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning",
            "imitation learning",
            "[T]diffusion policy"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Latent Action Pretraining from Videos",
      "arxiv_id": "2410.11758",
      "arxiv_url": "https://arxiv.org/pdf/2410.11758",
      "summary": "We introduce Latent Action Pretraining for general Action models (LAPA), an unsupervised method for pretraining Vision-Language-Action (VLA) models without ground-truth robot action labels. Existing Vision-Language-Action models require action labels typically collected by human teleoperators during pretraining, which significantly limits possible data sources and scale. In this work, we propose a method to learn from internet-scale videos that do not have robot action labels. We first train an action quantization model leveraging VQ-VAE-based objective to learn discrete latent actions between image frames, then pretrain a latent VLA model to predict these latent actions from observations and task descriptions, and finally finetune the VLA on small-scale robot manipulation data to map from latent to robot actions. Experimental results demonstrate that our method significantly outperforms existing techniques that train robot manipulation policies from large-scale videos. Furthermore, it outperforms the state-of-the-art VLA model trained with robotic action labels on real-world manipulation tasks that require language conditioning, generalization to unseen objects, and semantic generalization to unseen instructions. Training only on human manipulation videos also shows positive transfer, opening up the potential for leveraging web-scale data for robotics foundation model.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "VQ-VAE"
          ],
          "score": 2.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA",
            "foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "1_robot_core",
        "4_motion_diffusion",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] RHINO: Learning Real-Time Humanoid-Human-Object Interaction from Human Demonstrations",
      "arxiv_id": "2502.13134",
      "arxiv_url": "https://arxiv.org/pdf/2502.13134",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid"
          ],
          "score": 6.0
        },
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "[T]human-object interaction"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "1_robot_core",
        "5_interaction_reaction"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] NVSPolicy: Adaptive Novel-View Synthesis for Generalizable Language-Conditioned Policy Learning",
      "arxiv_id": "2505.10359",
      "arxiv_url": "https://arxiv.org/pdf/2505.10359",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]policy learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]language conditioned"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning",
      "arxiv_id": "2504.00907",
      "arxiv_url": "https://arxiv.org/pdf/2504.00907",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning",
      "arxiv_id": "2504.12680",
      "arxiv_url": "https://arxiv.org/pdf/2504.12680",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Teacher Motion Priors: Enhancing Robot Locomotion over Challenging Terrain",
      "arxiv_id": "2504.10390",
      "arxiv_url": "https://arxiv.org/pdf/2504.10390",
      "summary": "Achieving robust locomotion on complex terrains remains a challenge due to high dimensional control and environmental uncertainties. This paper introduces a teacher prior framework based on the teacher student paradigm, integrating imitation and auxiliary task learning to improve learning efficiency and generalization. Unlike traditional paradigms that strongly rely on encoder-based state embeddings, our framework decouples the network design, simplifying the policy network and deployment. A high performance teacher policy is first trained using privileged information to acquire generalizable motion skills. The teacher's motion distribution is transferred to the student policy, which relies only on noisy proprioceptive data, via a generative adversarial mechanism to mitigate performance degradation caused by distributional shifts. Additionally, auxiliary task learning enhances the student policy's feature representation, speeding up convergence and improving adaptability to varying terrains. The framework is validated on a humanoid robot, showing a great improvement in locomotion stability on dynamic terrains and significant reductions in development costs. This work provides a practical solution for deploying robust locomotion strategies in humanoid robots.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid",
            "humanoid robot",
            "[T]locomotion"
          ],
          "score": 10.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "teacher-student",
            "privileged information"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 13.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Neural Brain: A Neuroscience-inspired Framework for Embodied Agents",
      "arxiv_id": "2505.07634",
      "arxiv_url": "https://arxiv.org/pdf/2505.07634",
      "summary": "The rapid evolution of artificial intelligence (AI) has shifted from static, data-driven models to dynamic systems capable of perceiving and interacting with real-world environments. Despite advancements in pattern recognition and symbolic reasoning, current AI systems, such as large language models, remain disembodied, unable to physically engage with the world. This limitation has driven the rise of embodied AI, where autonomous agents, such as humanoid robots, must navigate and manipulate unstructured environments with human-like adaptability. At the core of this challenge lies the concept of Neural Brain, a central intelligence system designed to drive embodied agents with human-like adaptability. A Neural Brain must seamlessly integrate multimodal sensing and perception with cognitive capabilities. Achieving this also requires an adaptive memory system and energy-efficient hardware-software co-design, enabling real-time action in dynamic environments. This paper introduces a unified framework for the Neural Brain of embodied agents, addressing two fundamental challenges: (1) defining the core components of Neural Brain and (2) bridging the gap between static AI models and the dynamic adaptability required for real-world deployment. To this end, we propose a biologically inspired architecture that integrates multimodal active sensing, perception-cognition-action function, neuroplasticity-based memory storage and updating, and neuromorphic hardware/software optimization. Furthermore, we also review the latest research on embodied agents across these four aspects and analyze the gap between current AI systems and human intelligence. By synthesizing insights from neuroscience, we outline a roadmap towards the development of generalizable, autonomous agents capable of human-level intelligence in real-world scenarios.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid",
            "humanoid robot"
          ],
          "score": 4.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "embodied AI",
            "large language model",
            "multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 13.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] TrackVLA: Embodied Visual Tracking in the Wild",
      "arxiv_id": "2505.23189",
      "arxiv_url": "https://arxiv.org/pdf/2505.23189",
      "summary": "Embodied visual tracking is a fundamental skill in Embodied AI, enabling an agent to follow a specific target in dynamic environments using only egocentric vision. This task is inherently challenging as it requires both accurate target recognition and effective trajectory planning under conditions of severe occlusion and high scene dynamics. Existing approaches typically address this challenge through a modular separation of recognition and planning. In this work, we propose TrackVLA, a Vision-Language-Action (VLA) model that learns the synergy between object recognition and trajectory planning. Leveraging a shared LLM backbone, we employ a language modeling head for recognition and an anchor-based diffusion model for trajectory planning. To train TrackVLA, we construct an Embodied Visual Tracking Benchmark (EVT-Bench) and collect diverse difficulty levels of recognition samples, resulting in a dataset of 1.7 million samples. Through extensive experiments in both synthetic and real-world environments, TrackVLA demonstrates SOTA performance and strong generalizability. It significantly outperforms existing methods on public benchmarks in a zero-shot manner while remaining robust to high dynamics and occlusion in real-world scenarios at 10 FPS inference speed. Our project page is: https://pku-epic.github.io/TrackVLA-web.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "egocentric",
            "egocentric vision"
          ],
          "score": 4.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "embodied AI",
            "vision-language-action",
            "VLA"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 13.0,
      "hit_pillars": [
        "6_video_extraction",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI, IEEE/ASME Transactions on Mechatronics 2025",
      "arxiv_id": "2407.06886",
      "arxiv_url": "https://arxiv.org/pdf/2407.06886",
      "summary": "Embodied Artificial Intelligence (Embodied AI) is crucial for achieving Artificial General Intelligence (AGI) and serves as a foundation for various applications (e.g., intelligent mechatronics systems, smart manufacturing) that bridge cyberspace and the physical world. Recently, the emergence of Multi-modal Large Models (MLMs) and World Models (WMs) have attracted significant attention due to their remarkable perception, interaction, and reasoning capabilities, making them a promising architecture for embodied agents. In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of representative works of embodied robots and simulators, to fully understand the research focuses and their limitations. Then, we analyze four main research targets: 1) embodied perception, 2) embodied interaction, 3) embodied agent, and 4) sim-to-real adaptation, covering state-of-the-art methods, essential paradigms, and comprehensive datasets. Additionally, we explore the complexities of MLMs in virtual and real embodied agents, highlighting their significance in facilitating interactions in digital and physical environments. Finally, we summarize the challenges and limitations of embodied AI and discuss potential future directions. We hope this survey will serve as a foundational reference for the research community. The associated project can be found at https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Embodied AI Paper List, Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "sim-to-real"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "world model"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]embodied AI"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 12.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.04] Learning Visuotactile Skills with Two Multifingered Hands [IL] [touch] [[project](https://toruowo.github.io/hato/)]",
      "arxiv_id": "2404.16823",
      "arxiv_url": "http://arxiv.org/abs/2404.16823",
      "summary": "Aiming to replicate human-like dexterity, perceptual experiences, and motion patterns, we explore learning from human demonstrations using a bimanual system with multifingered hands and visuotactile data. Two significant challenges exist: the lack of an affordable and accessible teleoperation system suitable for a dual-arm setup with multifingered hands, and the scarcity of multifingered hand hardware equipped with touch sensing. To tackle the first challenge, we develop HATO, a low-cost hands-arms teleoperation system that leverages off-the-shelf electronics, complemented with a software suite that enables efficient data collection; the comprehensive software suite also supports multimodal data processing, scalable policy learning, and smooth policy deployment. To tackle the latter challenge, we introduce a novel hardware adaptation by repurposing two prosthetic hands equipped with touch sensors for research. Using visuotactile data collected from our system, we learn skills to complete long-horizon, high-precision tasks which are difficult to achieve without multifingered dexterity and touch feedback. Furthermore, we empirically investigate the effects of dataset size, sensing modality, and visual input preprocessing on policy learning. Our results mark a promising step forward in bimanual multifingered manipulation from visuotactile data. Videos, code, and datasets can be found at https://toruowo.github.io/hato/ .",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "https://github.com/toruowo/hato",
      "source_repo": "Awesome Humanoid Manipulation, Awesome Humanoid Robot Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "bi-manual",
            "dual-arm",
            "teleoperation"
          ],
          "score": 8.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 12.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Fast Adaptation with Behavioral Foundation Models",
      "arxiv_id": "2504.07896",
      "arxiv_url": "https://arxiv.org/abs/2504.07896",
      "summary": "Unsupervised zero-shot reinforcement learning (RL) has emerged as a powerful paradigm for pretraining behavioral foundation models (BFMs), enabling agents to solve a wide range of downstream tasks specified via reward functions in a zero-shot fashion, i.e., without additional test-time learning or planning. This is achieved by learning self-supervised task embeddings alongside corresponding near-optimal behaviors and incorporating an inference procedure to directly retrieve the latent task embedding and associated policy for any given reward function. Despite promising results, zero-shot policies are often suboptimal due to errors induced by the unsupervised training process, the embedding, and the inference procedure. In this paper, we focus on devising fast adaptation strategies to improve the zero-shot performance of BFMs in a few steps of online interaction with the environment while avoiding any performance drop during the adaptation process. Notably, we demonstrate that existing BFMs learn a set of skills containing more performant policies than those identified by their inference procedure, making them well-suited for fast adaptation. Motivated by this observation, we propose both actor-critic and actor-only fast adaptation strategies that search in the low-dimensional task-embedding space of the pre-trained BFM to rapidly improve the performance of its zero-shot policies on any downstream task. Notably, our approach mitigates the initial \"unlearning\" phase commonly observed when fine-tuning pre-trained RL models. We evaluate our fast adaptation strategies on top of four state-of-the-art zero-shot RL methods in multiple navigation and locomotion domains. Our results show that they achieve 10-40% improvement over their zero-shot performance in a few tens of episodes, outperforming existing baselines.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "locomotion"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 12.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Temporal Representation Alignment: Successor Features Enable Emergent Compositionality in Robot Instruction Following Temporal Representation Alignment",
      "arxiv_id": "2502.05454",
      "arxiv_url": "https://arxiv.org/pdf/2502.05454",
      "summary": "Effective task representations should facilitate compositionality, such that after learning a variety of basic tasks, an agent can perform compound tasks consisting of multiple steps simply by composing the representations of the constituent steps together. While this is conceptually simple and appealing, it is not clear how to automatically learn representations that enable this sort of compositionality. We show that learning to associate the representations of current and future states with a temporal alignment loss can improve compositional generalization, even in the absence of any explicit subtask planning or reinforcement learning. We evaluate our approach across diverse robotic manipulation tasks as well as in simulation, showing substantial improvements for tasks specified with either language or goal images.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]instruction following"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 12.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action",
      "arxiv_id": "2511.22134",
      "arxiv_url": "https://arxiv.org/pdf/2511.22134",
      "summary": "To build a generalizable Vision-Language-Action (VLA) model with strong reasoning ability, a common strategy is to first train a specialist VLA on robot demonstrations to acquire reliable manipulation skills, and then incorporate mixed annotated robot data together with multimodal data to restore broader reasoning capabilities. However, we observe that the resulting reasoning VLA often suffers from degraded action performance compared to the specialist model before fine-tuning, a phenomenon we refer to as action degeneration. To address this issue, we propose DualVLA, which enhances action performance through carefully designed post-training while still preserving reasoning capability. We first introduce a dual-layer data pruning method that removes redundant embodied reasoning, preventing it from adversely influencing action learning. To further strengthen action generation, we design a dual-teacher adaptive distillation strategy that assigns different supervision signals to different data domains while maintaining reasoning ability. To fill the evaluation gap for generalist VLAs, we also propose VLA Score, which decouples VLA capability into reasoning, intention, action, and alignment dimensions for a more fine-grained assessment. Experiments show that DualVLA achieves an average success rate of 61.0 in SimplerEnv and an average score of 65.4 across eight competitive multimodal benchmarks, demonstrating a stronger balance between precise action execution and multimodal understanding. Project Website: https://costaliya.github.io/DualVLA/.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "distillation"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA",
            "multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 12.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] DreamControl: Human-Inspired Whole-Body Humanoid Control for Scene Interaction via Guided Diffusion [[project](https://genrobo.github.io/DreamControl/)]",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "",
      "code_url": "https://github.com/GenRobo/DreamControl/tree/main",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid control"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] HDMI: Learning Interactive Humanoid Whole-Body Control from Human Videos [[project](https://hdmi-humanoid.github.io/#/)]",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "",
      "code_url": "https://github.com/LeCAR-Lab/HDMI?tab=readme-ov-file",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]whole-body control"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] RUKA: Rethinking the Design of Humanoid Hands with Learning",
      "arxiv_id": "2504.13165",
      "arxiv_url": "https://arxiv.org/pdf/2504.13165",
      "summary": "Dexterous manipulation is a fundamental capability for robotic systems, yet progress has been limited by hardware trade-offs between precision, compactness, strength, and affordability. Existing control methods impose compromises on hand designs and applications. However, learning-based approaches present opportunities to rethink these trade-offs, particularly to address challenges with tendon-driven actuation and low-cost materials. This work presents RUKA, a tendon-driven humanoid hand that is compact, affordable, and capable. Made from 3D-printed parts and off-the-shelf components, RUKA has 5 fingers with 15 underactuated degrees of freedom enabling diverse human-like grasps. Its tendon-driven actuation allows powerful grasping in a compact, human-sized form factor. To address control challenges, we learn joint-to-actuator and fingertip-to-actuator models from motion-capture data collected by the MANUS glove, leveraging the hand's morphological accuracy. Extensive evaluations demonstrate RUKA's superior reachability, durability, and strength compared to other robotic hands. Teleoperation tasks further showcase RUKA's dexterous movements. The open-source design and assembly instructions of RUKA, code, and data are available at https://ruka-hand.github.io/.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "manipulation",
            "dexterous manipulation",
            "teleoperation"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] HumanoidPano: Hybrid Spherical Panoramic-LiDAR Cross-Modal Perception for Humanoid Robots",
      "arxiv_id": "2503.09010",
      "arxiv_url": "https://arxiv.org/abs/2503.09010",
      "summary": "The perceptual system design for humanoid robots poses unique challenges due to inherent structural constraints that cause severe self-occlusion and limited field-of-view (FOV). We present HumanoidPano, a novel hybrid cross-modal perception framework that synergistically integrates panoramic vision and LiDAR sensing to overcome these limitations. Unlike conventional robot perception systems that rely on monocular cameras or standard multi-sensor configurations, our method establishes geometrically-aware modality alignment through a spherical vision transformer, enabling seamless fusion of 360 visual context with LiDAR's precise depth measurements. First, Spherical Geometry-aware Constraints (SGC) leverage panoramic camera ray properties to guide distortion-regularized sampling offsets for geometric alignment. Second, Spatial Deformable Attention (SDA) aggregates hierarchical 3D features via spherical offsets, enabling efficient 360°-to-BEV fusion with geometrically complete object representations. Third, Panoramic Augmentation (AUG) combines cross-view transformations and semantic alignment to enhance BEV-panoramic feature consistency during data augmentation. Extensive evaluations demonstrate state-of-the-art performance on the 360BEV-Matterport benchmark. Real-world deployment on humanoid platforms validates the system's capability to generate accurate BEV segmentation maps through panoramic-LiDAR co-perception, directly enabling downstream navigation tasks in complex environments. Our work establishes a new paradigm for embodied perception in humanoid robotics.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning, Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[SIGGRAPH] MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting [[project](https://research.nvidia.com/labs/par/maskedmimic/)]",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "SIGGRAPH",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "[T]physics-based character",
            "[T]character control"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "8_physics_animation"
      ],
      "excluded": false,
      "exclusion_keywords": [
        "inpainting"
      ]
    },
    {
      "title": "[RSS] Design and Control of a Bipedal Robotic Character",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "RSS",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bipedal",
            "[T]biped"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Humanoid Robots at work: where are we ? [Survey]",
      "arxiv_id": "2404.04249",
      "arxiv_url": "https://arxiv.org/abs/2404.04249",
      "summary": "Launched by Elon Musk and its Optimus, we are witnessing a new race in which many companies have already engaged. The objective it to put at work a new generation of humanoid robots in demanding industrial environments within 2 or 3 years. Is this objective realistic ? The aim of this document and its main contributions is to provide some hints by covering the following topics: First an analysis of 12 companies based on eight criteria that will help us to distinguish companies based on their maturity and approach to the market; second as these humanoids are very complex systems we will provide an overview of the technological challenges to be addressed; third when humanoids are deployed at scale, Operation and Maintenance become critical and the we will explore what is new with these complex machines; Finally Pilots are the last step to test the feasibility of a new system before mass deployment. This is an important step to test the maturity of a product and the strategy of the humanoid supplier to address a market and two pragmatic approaches will be discussed.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[IEEE-RAL] Learning Complex Motor Skills for Legged Robot Fall Recovery.",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "RAL",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]legged robot",
            "[T]fall recovery"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[IEEE-RAL] Linear Policies are Sufficient to Realize Robust Bipedal Walking on Challenging Terrains.",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "RAL",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bipedal",
            "[T]biped"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[PMLR] Learning Locomotion Skills for Cassie: Iterative Design and Sim-to-Real.",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]locomotion",
            "[T]sim-to-real"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "bipedal-robot-learning-collection",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bipedal",
            "[T]biped"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "awesome-legged-locomotion-learning",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]legged locomotion",
            "[T]locomotion"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "awesome-humanoid-manipulation",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]manipulation"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Exploring Embodied Multimodal Large Models: Development, Datasets, and Future Directions",
      "arxiv_id": "2502.15336",
      "arxiv_url": "https://arxiv.org/pdf/2502.15336",
      "summary": "Embodied multimodal large models (EMLMs) have gained significant attention in recent years due to their potential to bridge the gap between perception, cognition, and action in complex, real-world environments. This comprehensive review explores the development of such models, including Large Language Models (LLMs), Large Vision Models (LVMs), and other models, while also examining other emerging architectures. We discuss the evolution of EMLMs, with a focus on embodied perception, navigation, interaction, and simulation. Furthermore, the review provides a detailed analysis of the datasets used for training and evaluating these models, highlighting the importance of diverse, high-quality data for effective learning. The paper also identifies key challenges faced by EMLMs, including issues of scalability, generalization, and real-time decision-making. Finally, we outline future directions, emphasizing the integration of multimodal sensing, reasoning, and action to advance the development of increasingly autonomous systems. By providing an in-depth analysis of state-of-the-art methods and identifying critical gaps, this paper aims to inspire future advancements in EMLMs and their applications across diverse domains.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "[T]multimodal"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [Physical Intelligence] FAST: Efficient Action Tokenization for Vision-Language-Action Models",
      "arxiv_id": "2501.09747",
      "arxiv_url": "https://arxiv.org/pdf/2501.09747",
      "summary": "Autoregressive sequence models, such as Transformer-based vision-language action (VLA) policies, can be tremendously effective for capturing complex and generalizable robotic behaviors. However, such models require us to choose a tokenization of our continuous action signals, which determines how the discrete symbols predicted by the model map to continuous robot actions. We find that current approaches for robot action tokenization, based on simple per-dimension, per-timestep binning schemes, typically perform poorly when learning dexterous skills from high-frequency robot data. To address this challenge, we propose a new compression-based tokenization scheme for robot actions, based on the discrete cosine transform. Our tokenization approach, Frequency-space Action Sequence Tokenization (FAST), enables us to train autoregressive VLAs for highly dexterous and high-frequency tasks where standard discretization methods fail completely. Based on FAST, we release FAST+, a universal robot action tokenizer, trained on 1M real robot action trajectories. It can be used as a black-box tokenizer for a wide range of robot action sequences, with diverse action spaces and control frequencies. Finally, we show that, when combined with the pi0 VLA, our method can scale to training on 10k hours of robot data and match the performance of diffusion VLAs, while reducing training time by up to 5x.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Universal Actions for Enhanced Embodied Foundation Models",
      "arxiv_id": "2501.10105",
      "arxiv_url": "https://arxiv.org/pdf/2501.10105",
      "summary": "Training on diverse, internet-scale data is a key factor in the success of recent large foundation models. Yet, using the same recipe for building embodied agents has faced noticeable difficulties. Despite the availability of many crowd-sourced embodied datasets, their action spaces often exhibit significant heterogeneity due to distinct physical embodiment and control interfaces for different robots, causing substantial challenges in developing embodied foundation models using cross-domain data. In this paper, we introduce UniAct, a new embodied foundation modeling framework operating in a Universal Action Space. Our learned universal actions capture the generic atomic behaviors across diverse robots by exploiting their shared structural features, and enable enhanced cross-domain data utilization and cross-embodiment generalizations by eliminating the notorious heterogeneity. The universal actions can be efficiently translated back to heterogeneous actionable commands by simply adding embodiment-specific details, from which fast adaptation to new robots becomes simple and straightforward. Our 0.5B instantiation of UniAct outperforms 14X larger SOTA embodied foundation models in extensive evaluations on various real-world and simulation robots, showcasing exceptional cross-embodiment control and adaptation capability, highlighting the crucial benefit of adopting universal actions. Project page: https://github.com/2toinf/UniAct",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "cross-embodiment"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "7_retargeting",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] An Atomic Skill Library Construction Method for Data-Efficient Embodied Manipulation",
      "arxiv_id": "2501.15068",
      "arxiv_url": "https://arxiv.org/pdf/2501.15068",
      "summary": "Embodied manipulation is a fundamental ability in the realm of embodied artificial intelligence. Although current embodied manipulation models show certain generalizations in specific settings, they struggle in new environments and tasks due to the complexity and diversity of real-world scenarios. The traditional end-to-end data collection and training manner leads to significant data demands. Decomposing end-to-end tasks into atomic skills helps reduce data requirements and improves the task success rate. However, existing methods are limited by predefined skill sets that cannot be dynamically updated. To address the issue, we introduce a three-wheeled data-driven method to build an atomic skill library. We divide tasks into subtasks using the Vision-Language-Planning (VLP). Then, atomic skill definitions are formed by abstracting the subtasks. Finally, an atomic skill library is constructed via data collection and Vision-Language-Action (VLA) fine-tuning. As the atomic skill library expands dynamically with the three-wheel update strategy, the range of tasks it can cover grows naturally. In this way, our method shifts focus from end-to-end tasks to atomic skills, significantly reducing data costs while maintaining high performance and enabling efficient adaptation to new tasks. Extensive experiments in real-world settings demonstrate the effectiveness and efficiency of our approach.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs",
      "arxiv_id": "2503.01378",
      "arxiv_url": "https://arxiv.org/pdf/2503.01378",
      "summary": "This paper introduces CognitiveDrone, a novel Vision-Language-Action (VLA) model tailored for complex Unmanned Aerial Vehicles (UAVs) tasks that demand advanced cognitive abilities. Trained on a dataset comprising over 8,000 simulated flight trajectories across three key categories-Human Recognition, Symbol Understanding, and Reasoning-the model generates real-time 4D action commands based on first-person visual inputs and textual instructions. To further enhance performance in intricate scenarios, we propose CognitiveDrone-R1, which integrates an additional Vision-Language Model (VLM) reasoning module to simplify task directives prior to high-frequency control. Experimental evaluations using our open-source benchmark, CognitiveDroneBench, reveal that while a racing-oriented model (RaceVLA) achieves an overall success rate of 31.3%, the base CognitiveDrone model reaches 59.6%, and CognitiveDrone-R1 attains a success rate of 77.2%. These results demonstrate improvements of up to 30% in critical cognitive tasks, underscoring the effectiveness of incorporating advanced reasoning capabilities into UAV control systems. Our contributions include the development of a state-of-the-art VLA model for UAV control and the introduction of the first dedicated benchmark for assessing cognitive tasks in drone operations. The complete repository is available at cognitivedrone.github.io",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "[T]VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": [
        "UAV",
        "drone",
        "aerial vehicle"
      ]
    },
    {
      "title": "[2025] OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction",
      "arxiv_id": "2503.03734",
      "arxiv_url": "https://arxiv.org/pdf/2503.03734",
      "summary": "Vision-Language-Action (VLA) models aim to predict robotic actions based on visual observations and language instructions. Existing approaches require fine-tuning pre-trained visionlanguage models (VLMs) as visual and language features are independently fed into downstream policies, degrading the pre-trained semantic alignments. We propose OTTER, a novel VLA architecture that leverages these existing alignments through explicit, text-aware visual feature extraction. Instead of processing all visual features, OTTER selectively extracts and passes only task-relevant visual features that are semantically aligned with the language instruction to the policy transformer. This allows OTTER to keep the pre-trained vision-language encoders frozen. Thereby, OTTER preserves and utilizes the rich semantic understanding learned from large-scale pre-training, enabling strong zero-shot generalization capabilities. In simulation and real-world experiments, OTTER significantly outperforms existing VLA models, demonstrating strong zeroshot generalization to novel objects and environments. Video, code, checkpoints, and dataset: https://ottervla.github.io/.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] TLA: Tactile-Language-Action Model for Contact-Rich Manipulation",
      "arxiv_id": "2503.08548",
      "arxiv_url": "https://arxiv.org/pdf/2503.08548",
      "summary": "Significant progress has been made in vision-language models. However, language-conditioned robotic manipulation for contact-rich tasks remains underexplored, particularly in terms of tactile sensing. To address this gap, we introduce the Tactile-Language-Action (TLA) model, which effectively processes sequential tactile feedback via cross-modal language grounding to enable robust policy generation in contact-intensive scenarios. In addition, we construct a comprehensive dataset that contains 24k pairs of tactile action instruction data, customized for fingertip peg-in-hole assembly, providing essential resources for TLA training and evaluation. Our results show that TLA significantly outperforms traditional imitation learning methods (e.g., diffusion policy) in terms of effective action generation and action accuracy, while demonstrating strong generalization capabilities by achieving over 85\\% success rate on previously unseen assembly clearances and peg shapes. We publicly release all data and code in the hope of advancing research in language-conditioned tactile manipulation skill learning. Project website: https://sites.google.com/view/tactile-language-action/",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning",
            "diffusion policy"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "language conditioned"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning",
      "arxiv_id": "2505.13888",
      "arxiv_url": "https://arxiv.org/pdf/2505.13888",
      "summary": "Leveraging pretrained Vision-Language Models (VLMs) to map language instruction and visual observations to raw low-level actions, Vision-Language-Action models (VLAs) hold great promise for achieving general-purpose robotic systems. Despite their advancements, existing VLAs tend to spuriously correlate task-irrelevant visual features with actions, limiting their generalization capacity beyond the training data. To tackle this challenge, we propose Intrinsic Spatial Reasoning (InSpire), a simple yet effective approach that mitigates the adverse effects of spurious correlations by boosting the spatial reasoning ability of VLAs. Specifically, InSpire redirects the VLA's attention to task-relevant factors by prepending the question \"In which direction is the [object] relative to the robot?\" to the language instruction and aligning the answer \"right/left/up/down/front/back/grasped\" and predicted actions with ground-truth. Notably, InSpire can be used as a plugin to enhance existing autoregressive VLAs, requiring no extra training data or interaction with other large models. Extensive experimental results in both simulation and real-world environments demonstrate the effectiveness and flexibility of our approach.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Hume: Introducing System-2 Thinking in Visual-Language-Action Model",
      "arxiv_id": "2505.21432",
      "arxiv_url": "https://arxiv.org/pdf/2505.21432",
      "summary": "Humans practice slow thinking before performing actual actions when handling complex tasks in the physical world. This thinking paradigm, recently, has achieved remarkable advancement in boosting Large Language Models (LLMs) to solve complex tasks in digital domains. However, the potential of slow thinking remains largely unexplored for robotic foundation models interacting with the physical world. In this work, we propose Hume: a dual-system Vision-Language-Action (VLA) model with value-guided System-2 thinking and cascaded action denoising, exploring human-like thinking capabilities of Vision-Language-Action models for dexterous robot control. System 2 of Hume implements value-Guided thinking by extending a Vision-Language-Action Model backbone with a novel value-query head to estimate the state-action value of predicted actions. The value-guided thinking is conducted by repeat sampling multiple action candidates and selecting one according to state-action value. System 1 of Hume is a lightweight reactive visuomotor policy that takes System 2 selected action and performs cascaded action denoising for dexterous robot control. At deployment time, System 2 performs value-guided thinking at a low frequency while System 1 asynchronously receives the System 2 selected action candidate and predicts fluid actions in real time. We show that Hume outperforms the existing state-of-the-art Vision-Language-Action models across multiple simulation benchmark and real-robot deployments.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA",
            "large language model",
            "foundation model"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] LoHoVLA: A  Vision-Language-Action Model for Long-Horizon Embodied Tasks",
      "arxiv_id": "2506.00411",
      "arxiv_url": "https://arxiv.org/pdf/2506.00411",
      "summary": "Real-world embodied agents face long-horizon tasks, characterized by high-level goals demanding multi-step solutions beyond single actions. Successfully navigating these requires both high-level task planning (i.e., decomposing goals into sub-tasks) and low-level motion control (i.e., generating precise robot actions). While existing vision language action (VLA) models and hierarchical architectures offer potential in embodied tasks, the former often falter in planning, and the latter can suffer from coordination issues, both hampering performance. We introduce a new unified VLA framework for long-horizon tasks, dubbed LoHoVLA, to overcome these limitations. LoHoVLA leverages a large pretrained vision language model (VLM) as the backbone to jointly generate language and action tokens for sub-task generation and robot action prediction, respectively. This shared representation promotes better generalization across tasks. Additionally, LoHoVLA embraces a hierarchical closed-loop control mechanism to mitigate errors originating from both high-level planning and low-level control. To train LoHoVLA, we introduce LoHoSet, a dataset built on the Ravens simulator, containing 20 long-horizon tasks, each with 1,000 expert demonstrations composed of visual observations, linguistic goals, sub-tasks, and robot actions. Experimental results show that LoHoVLA significantly surpasses both hierarchical and standard VLA approaches on long-horizon embodied tasks in the Ravens simulator. These findings underscore the promise of unified architectures for advancing generalizable embodied intelligence.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [ICCV 25] CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Games",
      "arxiv_id": "2503.09527",
      "arxiv_url": "https://arxiv.org/pdf/2503.09527",
      "summary": "Recent advances in Vision-Language-Action models (VLAs) have expanded the capabilities of embodied intelligence. However, significant challenges remain in real-time decision-making in complex 3D environments, which demand second-level responses, high-resolution perception, and tactical reasoning under dynamic conditions. To advance the field, we introduce CombatVLA, an efficient VLA model optimized for combat tasks in 3D action role-playing games(ARPGs). Specifically, our CombatVLA is a 3B model trained on video-action pairs collected by an action tracker, where the data is formatted as action-of-thought (AoT) sequences. Thereafter, CombatVLA seamlessly integrates into an action execution framework, allowing efficient inference through our truncated AoT strategy. Experimental results demonstrate that CombatVLA not only outperforms all existing models on the combat understanding benchmark but also achieves a 50-fold acceleration in game combat. Moreover, it has a higher task success rate than human players. We will open-source all resources, including the action tracker, dataset, benchmark, model weights, training code, and the implementation of the framework at https://combatvla.github.io/.",
      "authors": [],
      "year": "2025",
      "venue": "ICCV",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs",
      "arxiv_id": "2509.11480",
      "arxiv_url": "https://arxiv.org/pdf/2509.11480",
      "summary": "Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic control, yet their performance scaling across model architectures and hardware platforms, as well as their associated power budgets, remain poorly understood. This work presents an evaluation of five representative VLA models -- spanning state-of-the-art baselines and two newly proposed architectures -- targeting edge and datacenter GPU platforms. Using the LIBERO benchmark, we measure accuracy alongside system-level metrics, including latency, throughput, and peak memory usage, under varying edge power constraints and high-performance datacenter GPU configurations. Our results identify distinct scaling trends: (1) architectural choices, such as action tokenization and model backbone size, strongly influence throughput and memory footprint; (2) power-constrained edge devices exhibit non-linear performance degradation, with some configurations matching or exceeding older datacenter GPUs; and (3) high-throughput variants can be achieved without significant accuracy loss. These findings provide actionable insights when selecting and optimizing VLAs across a range of deployment constraints. Our work challenges current assumptions about the superiority of datacenter hardware for robotic inference.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Verifier-free Test-Time Sampling for Vision Language Action Models",
      "arxiv_id": "2510.05681",
      "arxiv_url": "https://arxiv.org/pdf/2510.05681",
      "summary": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance in robot control. However, they remain fundamentally limited in tasks that require high precision due to their single-inference paradigm. While test-time scaling approaches using external verifiers have shown promise, they require additional training and fail to generalize to unseen conditions. We propose Masking Distribution Guided Selection (MG-Select), a novel test-time scaling framework for VLAs that leverages the model's internal properties without requiring additional training or external modules. Our approach utilizes KL divergence from a reference action token distribution as a confidence metric for selecting the optimal action from multiple candidates. We introduce a reference distribution generated by the same VLA but with randomly masked states and language conditions as inputs, ensuring maximum uncertainty while remaining aligned with the target task distribution. Additionally, we propose a joint training strategy that enables the model to learn both conditional and unconditional distributions by applying dropout to state and language conditions, thereby further improving the quality of the reference distribution. Our experiments demonstrate that MG-Select achieves significant performance improvements, including a 28%/35% improvement in real-world in-distribution/out-of-distribution tasks, along with a 168% relative gain on RoboCasa pick-and-place tasks trained with 30 demonstrations.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [CoRL 25] RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models",
      "arxiv_id": "2506.17811",
      "arxiv_url": "https://arxiv.org/pdf/2506.17811",
      "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in visuomotor control, yet ensuring their robustness in unstructured real-world environments remains a persistent challenge. In this paper, we investigate test-time scaling through the lens of sampling and verification as means to enhance the robustness and generalization of VLAs. We first demonstrate that the relationship between action error and the number of generated samples follows an exponentiated power law across a range of VLAs, indicating the existence of inference-time scaling laws. Building on these insights, we introduce RoboMonkey, a test-time scaling framework for VLAs. At deployment, RoboMonkey samples a small set of actions from a VLA, applies Gaussian perturbation and majority voting to construct an action proposal distribution, and then uses a Vision Language Model (VLM)-based verifier to select the optimal action. We propose a synthetic data generation pipeline for training such VLM-based action verifiers, and demonstrate that scaling the synthetic dataset consistently improves verification and downstream accuracy. Through extensive simulated and hardware experiments, we show that pairing existing VLAs with RoboMonkey yields significant performance gains, achieving a 25% absolute improvement on out-of-distribution tasks and 9% on in-distribution tasks. Additionally, when adapting to new robot setups, we show that fine-tuning both VLAs and action verifiers yields a 7% performance increase compared to fine-tuning VLAs alone.",
      "authors": [],
      "year": "2025",
      "venue": "CoRL",
      "code_url": "https://github.com/robomonkey-vla/RoboMonkey",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "VLA"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy",
      "arxiv_id": "2510.13778",
      "arxiv_url": "https://arxiv.org/pdf/2510.13778",
      "summary": "We introduce InternVLA-M1, a unified framework for spatial grounding and robot control that advances instruction-following robots toward scalable, general-purpose intelligence. Its core idea is spatially guided vision-language-action training, where spatial grounding serves as the critical link between instructions and robot actions. InternVLA-M1 employs a two-stage pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning data to determine ``where to act'' by aligning instructions with visual, embodiment-agnostic positions, and (ii) spatially guided action post-training to decide ``how to act'' by generating embodiment-aware actions through plug-and-play spatial prompting. This spatially guided training recipe yields consistent gains: InternVLA-M1 outperforms its variant without spatial guidance by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO Franka, while demonstrating stronger spatial reasoning capability in box, point, and trace prediction. To further scale instruction following, we built a simulation engine to collect 244K generalizable pick-and-place episodes, enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with synthetic co-training, achieved +20.6% on unseen objects and novel configurations. Moreover, in long-horizon reasoning-intensive scenarios, it surpassed existing works by over 10%. These results highlight spatially guided training as a unifying principle for scalable and resilient generalist robots. Code and models are available at https://github.com/InternRobotics/InternVLA-M1.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/InternRobotics/InternVLA-M1",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action",
            "instruction following"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] BiTLA: A Bimanual Tactile-Language-Action Model for Contact-Rich Robotic Manipulation",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]bi-manual"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] RT-Affordance: Affordances are Versatile Intermediate Representations for Robot Manipulation",
      "arxiv_id": "2411.02704",
      "arxiv_url": "https://arxiv.org/pdf/2411.02704",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]affordance"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] LOVON: Legged Open-Vocabulary Object Navigator",
      "arxiv_id": "2507.06747",
      "arxiv_url": "https://arxiv.org/pdf/2507.06747",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]open-vocabulary",
            "[T]open vocabulary"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] VR-Robo: A Real-to-Sim-to-Real Framework for Visual Robot Navigation and Locomotion",
      "arxiv_id": "2502.01536",
      "arxiv_url": "https://arxiv.org/pdf/2502.01536",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]locomotion",
            "[T]sim-to-real"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] You Only Teach Once: Learn One-Shot Bimanual Robotic Manipulation from Video Demonstrations",
      "arxiv_id": "2501.14208",
      "arxiv_url": "https://arxiv.org/pdf/2501.14208",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]bi-manual"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] CordViP: Correspondence-based Visuomotor Policy for Dexterous Manipulation in Real-World",
      "arxiv_id": "2502.08449",
      "arxiv_url": "https://arxiv.org/pdf/2502.08449",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]dexterous manipulation"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] COMBO-Grasp: Learning Constraint-Based Manipulation for Bimanual Occluded Grasping",
      "arxiv_id": "2502.08054",
      "arxiv_url": "https://arxiv.org/pdf/2502.08054",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]bi-manual"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References",
      "arxiv_id": "2502.09614",
      "arxiv_url": "https://arxiv.org/pdf/2502.09614",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]dexterous manipulation"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Learning Dexterous In-Hand Manipulation with Multifingered Hands via Visuomotor Diffusion",
      "arxiv_id": "2503.02587",
      "arxiv_url": "https://arxiv.org/pdf/2503.02587",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]in-hand manipulation"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Rethinking Bimanual Robotic Manipulation: Learning with Decoupled Interaction Framework",
      "arxiv_id": "2503.09186",
      "arxiv_url": "https://arxiv.org/pdf/2503.09186",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]bi-manual"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] KineDex: Learning Tactile-Informed Visuomotor Policies via Kinesthetic Teaching for Dexterous Manipulation",
      "arxiv_id": "2505.01974",
      "arxiv_url": "https://arxiv.org/pdf/2505.01974",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]dexterous manipulation"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] GLOVER++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation",
      "arxiv_id": "2505.11865",
      "arxiv_url": "https://arxiv.org/pdf/2505.11865",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]affordance"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Adaptive Visuo-Tactile Fusion with Predictive Force Attention for Dexterous Manipulation",
      "arxiv_id": "2505.13982",
      "arxiv_url": "https://arxiv.org/pdf/2505.13982",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]dexterous manipulation"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [RSS 25] Dex1B: Learning with 1B Demonstrations for Dexterous Manipulation",
      "arxiv_id": "2506.17198",
      "arxiv_url": "https://arxiv.org/pdf/2506.17198",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "RSS",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]dexterous manipulation"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] ViTacFormer: Learning Cross-Modal Representation for Visuo-Tactile Dexterous Manipulation",
      "arxiv_id": "2506.15953",
      "arxiv_url": "https://arxiv.org/pdf/2506.15953",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]dexterous manipulation"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] 3D FlowMatch Actor: Unified 3D Policy for Singleand Dual-Arm Manipulation",
      "arxiv_id": "2508.11002",
      "arxiv_url": "https://arxiv.org/pdf/2508.11002",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/nickgkan/3d_flowmatch_actor",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]dual-arm"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] FBI: Learning Dexterous In-hand Manipulation with Dynamic Visuotactile Shortcut Policy",
      "arxiv_id": "2508.14441",
      "arxiv_url": "https://arxiv.org/pdf/2508.14441",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]in-hand manipulation"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] OVAMOS: A Framework for Open-Vocabulary Multi-Object Search in Unknown Environments",
      "arxiv_id": "2503.02106",
      "arxiv_url": "https://arxiv.org/pdf/2503.02106",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]open-vocabulary",
            "[T]open vocabulary"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented Manipulation",
      "arxiv_id": "2503.03556",
      "arxiv_url": "https://arxiv.org/pdf/2503.03556",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]affordance"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] A Real-to-Sim-to-Real Approach to Robotic Manipulation with VLM-Generated Iterative Keypoint Rewards",
      "arxiv_id": "2502.08643",
      "arxiv_url": "https://arxiv.org/pdf/2502.08643",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]sim-to-real"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.07] Open-TeleVision Teleoperation with Immersive Active Visual Feedback [IL] [[project](https://robot-tv.github.io/)]",
      "arxiv_id": "2407.01512",
      "arxiv_url": "https://arxiv.org/abs/2407.01512",
      "summary": "Teleoperation serves as a powerful method for collecting on-robot data essential for robot learning from demonstrations. The intuitiveness and ease of use of the teleoperation system are crucial for ensuring high-quality, diverse, and scalable data. To achieve this, we propose an immersive teleoperation system Open-TeleVision that allows operators to actively perceive the robot's surroundings in a stereoscopic manner. Additionally, the system mirrors the operator's arm and hand movements on the robot, creating an immersive experience as if the operator's mind is transmitted to a robot embodiment. We validate the effectiveness of our system by collecting data and training imitation learning policies on four long-horizon, precise tasks (Can Sorting, Can Insertion, Folding, and Unloading) for 2 different humanoid robots and deploy them in the real world. The system is open-sourced at: https://robot-tv.github.io/",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "https://github.com/OpenTeleVision/TeleVision",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid",
            "humanoid robot",
            "[T]teleoperation"
          ],
          "score": 10.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 11.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.11] AsymDex: Leveraging Asymmetry and Relative Motion in Learning Bimanual Dexterity [RL]",
      "arxiv_id": "2411.13020",
      "arxiv_url": "https://arxiv.org/abs/2411.13020",
      "summary": "We present Asymmetric Dexterity (AsymDex), a novel and simple reinforcement learning (RL) framework that can efficiently learn a large class of bimanual skills in multi-fingered hands without relying on demonstrations. Two crucial insights enable AsymDex to reduce the observation and action space dimensions and improve sample efficiency. First, true ambidexterity is rare in humans and most of us exhibit strong \"handedness\". Inspired by this observation, we assign complementary roles to each hand: the facilitating hand repositions and reorients one object, while the dominant hand performs complex manipulations to achieve the desired result (e.g., opening a bottle cap, or pouring liquids). Second, controlling the relative motion between the hands is crucial for coordination and synchronization of the two hands. As such, we design relative observation and action spaces and leverage a relative-pose tracking controller. Further, we propose a two-phase decomposition in which AsymDex can be readily integrated with recent advances in grasp learning to facilitate both the acquisition and manipulation of objects using two hands. Unlike existing RL-based methods for bimanual dexterity with multi-fingered hands, which are either sample inefficient or tailored to a specific task, AsymDex can efficiently learn a wide variety of bimanual skills that exhibit asymmetry. Detailed experiments on seven asymmetric bimanual dexterous manipulation tasks (four simulated and three real-world) reveal that AsymDex consistently outperforms strong baselines that challenge our design choices. The project website is at https://sites.google.com/view/asymdex-2025/.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "dexterous manipulation",
            "[T]bi-manual"
          ],
          "score": 10.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 11.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Let Humanoids Hike! Integrative Skill Development on Complex Trails",
      "arxiv_id": "2505.06218",
      "arxiv_url": "https://arxiv.org/pdf/2505.06218",
      "summary": "Hiking on complex trails demands balance, agility, and adaptive decision-making over unpredictable terrain. Current humanoid research remains fragmented and inadequate for hiking: locomotion focuses on motor skills without long-term goals or situational awareness, while semantic navigation overlooks real-world embodiment and local terrain variability. We propose training humanoids to hike on complex trails, driving integrative skill development across visual perception, decision making, and motor execution. We develop a learning framework, LEGO-H, that enables a vision-equipped humanoid robot to hike complex trails autonomously. We introduce two technical innovations: 1) A temporal vision transformer variant - tailored into Hierarchical Reinforcement Learning framework - anticipates future local goals to guide movement, seamlessly integrating locomotion with goal-directed navigation. 2) Latent representations of joint movement patterns, combined with hierarchical metric learning - enhance Privileged Learning scheme - enable smooth policy transfer from privileged training to onboard execution. These components allow LEGO-H to handle diverse physical and environmental challenges without relying on predefined motion patterns. Experiments across varied simulated trails and robot morphologies highlight LEGO-H's versatility and robustness, positioning hiking as a compelling testbed for embodied autonomy and LEGO-H as a baseline for future humanoid development.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "locomotion"
          ],
          "score": 10.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 11.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [ByteDance Seed] GR-3 Technical Report",
      "arxiv_id": "2507.15493",
      "arxiv_url": "https://arxiv.org/pdf/2507.15493",
      "summary": "We report our recent progress towards building generalist robot policies, the development of GR-3. GR-3 is a large-scale vision-language-action (VLA) model. It showcases exceptional capabilities in generalizing to novel objects, environments, and instructions involving abstract concepts. Furthermore, it can be efficiently fine-tuned with minimal human trajectory data, enabling rapid and cost-effective adaptation to new settings. GR-3 also excels in handling long-horizon and dexterous tasks, including those requiring bi-manual manipulation and mobile movement, showcasing robust and reliable performance. These capabilities are achieved through a multi-faceted training recipe that includes co-training with web-scale vision-language data, efficient fine-tuning from human trajectory data collected via VR devices, and effective imitation learning with robot trajectory data. In addition, we introduce ByteMini, a versatile bi-manual mobile robot designed with exceptional flexibility and reliability, capable of accomplishing a wide range of tasks when integrated with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the state-of-the-art baseline method, $π_0$, on a wide variety of challenging tasks. We hope GR-3 can serve as a step towards building generalist robots capable of assisting humans in daily life.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "bi-manual"
          ],
          "score": 4.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 11.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] GRAPE: Generalizing Robot Policy via Preference Alignment",
      "arxiv_id": "2411.19309",
      "arxiv_url": "https://arxiv.org/pdf/2411.19309",
      "summary": "Despite the recent advancements of vision-language-action (VLA) models on a variety of robotics tasks, they suffer from critical issues such as poor generalizability to unseen tasks, due to their reliance on behavior cloning exclusively from successful rollouts. Furthermore, they are typically fine-tuned to replicate demonstrations collected by experts under different settings, thus introducing distribution bias and limiting their adaptability to diverse manipulation objectives, such as efficiency, safety, and task completion. To bridge this gap, we introduce GRAPE: Generalizing Robot Policy via Preference Alignment. Specifically, GRAPE aligns VLAs on a trajectory level and implicitly models reward from both successful and failure trials to boost generalizability to diverse tasks. Moreover, GRAPE breaks down complex manipulation tasks to independent stages and automatically guides preference modeling through customized spatiotemporal constraints with keypoints proposed by a large vision-language model. Notably, these constraints are flexible and can be customized to align the model with varying objectives, such as safety, efficiency, or task success. We evaluate GRAPE across a diverse array of tasks in both real-world and simulated environments. Experimental results demonstrate that GRAPE enhances the performance of state-of-the-art VLA models, increasing success rates on in-domain and unseen manipulation tasks by 51.79% and 58.20%, respectively. Additionally, GRAPE can be aligned with various objectives, such as safety and efficiency, reducing collision rates by 37.44% and rollout step-length by 11.15%, respectively. All code, models, and data are available at https://grape-vla.github.io/",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "behavior cloning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "spatiotemporal"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 11.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "8_physics_animation",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.08] RP1M: A Large-Scale Motion Dataset for Piano Playing with Bi-Manual Dexterous Robot Hands [IL] [[project](https://rp1m.github.io/)]",
      "arxiv_id": "2408.11048",
      "arxiv_url": "https://arxiv.org/abs/2408.11048",
      "summary": "It has been a long-standing research goal to endow robot hands with human-level dexterity. Bi-manual robot piano playing constitutes a task that combines challenges from dynamic tasks, such as generating fast while precise motions, with slower but contact-rich manipulation problems. Although reinforcement learning based approaches have shown promising results in single-task performance, these methods struggle in a multi-song setting. Our work aims to close this gap and, thereby, enable imitation learning approaches for robot piano playing at scale. To this end, we introduce the Robot Piano 1 Million (RP1M) dataset, containing bi-manual robot piano playing motion data of more than one million trajectories. We formulate finger placements as an optimal transport problem, thus, enabling automatic annotation of vast amounts of unlabeled songs. Benchmarking existing imitation learning approaches shows that such approaches reach state-of-the-art robot piano playing performance by leveraging RP1M.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "https://github.com/google-research/robopianist",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "[T]bi-manual"
          ],
          "score": 8.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "imitation learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 11.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Towards Embodiment Scaling Laws Robot Locomotion",
      "arxiv_id": "2505.05753",
      "arxiv_url": "https://arxiv.org/pdf/2505.05753",
      "summary": "Cross-embodiment generalization underpins the vision of building generalist embodied agents for any robot, yet its enabling factors remain poorly understood. We investigate embodiment scaling laws, the hypothesis that increasing the number of training embodiments improves generalization to unseen ones, using robot locomotion as a test bed. We procedurally generate ~1,000 embodiments with topological, geometric, and joint-level kinematic variations, and train policies on random subsets. We observe positive scaling trends supporting the hypothesis, and find that embodiment scaling enables substantially broader generalization than data scaling on fixed embodiments. Our best policy, trained on the full dataset, transfers zero-shot to novel embodiments in simulation and the real world, including the Unitree Go2 and H1. These results represent a step toward general embodied intelligence, with relevance to adaptive control for configurable robots, morphology co-design, and beyond.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]locomotion",
            "Unitree"
          ],
          "score": 8.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "cross-embodiment"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 11.0,
      "hit_pillars": [
        "1_robot_core",
        "7_retargeting"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding",
      "arxiv_id": "2501.04693",
      "arxiv_url": "https://arxiv.org/pdf/2501.04693",
      "summary": "Interacting with the world is a multi-sensory experience: achieving effective general-purpose interaction requires making use of all available modalities -- including vision, touch, and audio -- to fill in gaps from partial observation. For example, when vision is occluded reaching into a bag, a robot should rely on its senses of touch and sound. However, state-of-the-art generalist robot policies are typically trained on large datasets to predict robot actions solely from visual and proprioceptive observations. In this work, we propose FuSe, a novel approach that enables finetuning visuomotor generalist policies on heterogeneous sensor modalities for which large datasets are not readily available by leveraging natural language as a common cross-modal grounding. We combine a multimodal contrastive loss with a sensory-grounded language generation loss to encode high-level semantics. In the context of robot manipulation, we show that FuSe enables performing challenging tasks that require reasoning jointly over modalities such as vision, touch, and sound in a zero-shot setting, such as multimodal prompting, compositional cross-modal prompting, and descriptions of objects it interacts with. We show that the same recipe is applicable to widely different generalist policies, including both diffusion-based generalist policies and large vision-language-action (VLA) models. Extensive experiments in the real world show that FuSeis able to increase success rates by over 20% compared to all considered baselines.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA",
            "multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 11.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Integrating LMM Planners and 3D Skill Policies for Generalizable Manipulation",
      "arxiv_id": "2501.18733",
      "arxiv_url": "https://arxiv.org/pdf/2501.18733",
      "summary": "The recent advancements in visual reasoning capabilities of large multimodal models (LMMs) and the semantic enrichment of 3D feature fields have expanded the horizons of robotic capabilities. These developments hold significant potential for bridging the gap between high-level reasoning from LMMs and low-level control policies utilizing 3D feature fields. In this work, we introduce LMM-3DP, a framework that can integrate LMM planners and 3D skill Policies. Our approach consists of three key perspectives: high-level planning, low-level control, and effective integration. For high-level planning, LMM-3DP supports dynamic scene understanding for environment disturbances, a critic agent with self-feedback, history policy memorization, and reattempts after failures. For low-level control, LMM-3DP utilizes a semantic-aware 3D feature field for accurate manipulation. In aligning high-level and low-level control for robot actions, language embeddings representing the high-level policy are jointly attended with the 3D feature field in the 3D transformer for seamless integration. We extensively evaluate our approach across multiple skills and long-horizon tasks in a real-world kitchen environment. Our results show a significant 1.45x success rate increase in low-level control and an approximate 1.5x improvement in high-level planning accuracy compared to LLM-based baselines. Demo videos and an overview of LMM-3DP are available at https://lmm-3dp-release.github.io.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 11.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] VidBot: Learning Generalizable 3D Actions from In-the-Wild 2D Human Videos for Zero-Shot Robotic Manipulation",
      "arxiv_id": "2503.07135",
      "arxiv_url": "https://arxiv.org/pdf/2503.07135",
      "summary": "Future robots are envisioned as versatile systems capable of performing a variety of household tasks. The big question remains, how can we bridge the embodiment gap while minimizing physical robot learning, which fundamentally does not scale well. We argue that learning from in-the-wild human videos offers a promising solution for robotic manipulation tasks, as vast amounts of relevant data already exist on the internet. In this work, we present VidBot, a framework enabling zero-shot robotic manipulation using learned 3D affordance from in-the-wild monocular RGB-only human videos. VidBot leverages a pipeline to extract explicit representations from them, namely 3D hand trajectories from videos, combining a depth foundation model with structure-from-motion techniques to reconstruct temporally consistent, metric-scale 3D affordance representations agnostic to embodiments. We introduce a coarse-to-fine affordance learning model that first identifies coarse actions from the pixel space and then generates fine-grained interaction trajectories with a diffusion model, conditioned on coarse actions and guided by test-time constraints for context-aware interaction planning, enabling substantial generalization to novel scenes and embodiments. Extensive experiments demonstrate the efficacy of VidBot, which significantly outperforms counterparts across 13 manipulation tasks in zero-shot settings and can be seamlessly deployed across robot systems in real-world environments. VidBot paves the way for leveraging everyday human videos to make robot learning more scalable.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "affordance"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 11.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] ACTLLM: Action Consistency Tuned Large Language Model",
      "arxiv_id": "2506.21250",
      "arxiv_url": "https://arxiv.org/pdf/2506.21250",
      "summary": "This paper introduces ACTLLM (Action Consistency Tuned Large Language Model), a novel approach for robot manipulation in dynamic environments. Traditional vision-based systems often struggle to learn visual representations that excel in both task execution and spatial reasoning, thereby limiting their adaptability in dynamic environments. ACTLLM addresses these challenges by harnessing language to craft structured scene descriptors, providing a uniform interface for both spatial understanding and task performance through flexible language instructions. Moreover, we introduce a novel action consistency constraint that aligns visual perception with corresponding actions, thereby enhancing the learning of actionable visual representations. Additionally, we have reformulated the Markov decision process for manipulation tasks into a multi-turn visual dialogue framework. This approach enables the modeling of long-term task execution with enhanced contextual relevance derived from the history of task execution. During our evaluation, ACTLLM excels in diverse scenarios, proving its effectiveness on challenging vision-based robot manipulation tasks.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 11.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] VideoVLA: Video Generators Can Be Generalizable Robot Manipulators",
      "arxiv_id": "2512.06963",
      "arxiv_url": "https://arxiv.org/pdf/2512.06963",
      "summary": "Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA",
            "instruction following"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 11.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] [RSS 24] Octo: An Open-Source Generalist Robot Policy",
      "arxiv_id": "2405.12213",
      "arxiv_url": "https://arxiv.org/pdf/2405.12213",
      "summary": "Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.",
      "authors": [],
      "year": "2024",
      "venue": "RSS",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]Octo"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 11.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Revisiting Reward Design and Evaluation for Robust Humanoid Standing and Walking [[project](https://b-vm.github.io/Robust-SaW/)]",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reward design"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[3DV] Grasping Field: Learning Implicit Representations for Human Grasps",
      "arxiv_id": "2008.04451",
      "arxiv_url": "https://arxiv.org/pdf/2008.04451.pdf",
      "summary": "Robotic grasping of house-hold objects has made remarkable progress in recent years. Yet, human grasps are still difficult to synthesize realistically. There are several key reasons: (1) the human hand has many degrees of freedom (more than robotic manipulators); (2) the synthesized hand should conform to the surface of the object; and (3) it should interact with the object in a semantically and physically plausible manner. To make progress in this direction, we draw inspiration from the recent progress on learning-based implicit representations for 3D object reconstruction. Specifically, we propose an expressive representation for human grasp modelling that is efficient and easy to integrate with deep neural networks. Our insight is that every point in a three-dimensional space can be characterized by the signed distances to the surface of the hand and the object, respectively. Consequently, the hand, the object, and the contact area can be represented by implicit surfaces in a common space, in which the proximity between the hand and the object can be modelled explicitly. We name this 3D to 2D mapping as Grasping Field, parameterize it with a deep neural network, and learn it from data. We demonstrate that the proposed grasping field is an effective and expressive representation for human grasp generation. Specifically, our generative model is able to synthesize high-quality human grasps, given only on a 3D object point cloud. The extensive experiments demonstrate that our generative model compares favorably with a strong baseline and approaches the level of natural human grasps. Our method improves the physical plausibility of the hand-object contact reconstruction and achieves comparable performance for 3D hand reconstruction compared to state-of-the-art methods.",
      "authors": [],
      "year": "2008",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]implicit representation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "physically plausible"
          ],
          "score": 2.5
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "hand reconstruction"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "3_perception_slam",
        "4_motion_diffusion",
        "6_video_extraction"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes",
      "arxiv_id": "2408.03539",
      "arxiv_url": "https://www.arxiv.org/abs/2408.03539",
      "summary": "Reinforcement learning (RL), particularly its combination with deep neural networks referred to as deep RL (DRL), has shown tremendous promise across a wide range of applications, suggesting its potential for enabling the development of sophisticated robotic behaviors. Robotics problems, however, pose fundamental difficulties for the application of RL, stemming from the complexity and cost of interacting with the physical world. This article provides a modern survey of DRL for robotics, with a particular focus on evaluating the real-world successes achieved with DRL in realizing several key robotic competencies. Our analysis aims to identify the key factors underlying those exciting successes, reveal underexplored areas, and provide an overall characterization of the status of DRL in robotics. We highlight several important avenues for future work, emphasizing the need for stable and sample-efficient real-world RL paradigms, holistic approaches for discovering and integrating various competencies to tackle complex long-horizon, open-world tasks, and principled development and evaluation procedures. This survey is designed to offer insights for both RL practitioners and roboticists toward harnessing RL's power to create generally capable real-world robotic systems.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "[T]deep reinforcement learning",
            "DRL"
          ],
          "score": 10.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] A Survey on Diffusion Policy for Robotic Manipulation: Taxonomy, Analysis, and Future Directions",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] GeoManip: Geometric Constraints as General Interfaces for Robot Manipulation",
      "arxiv_id": "2501.09783",
      "arxiv_url": "https://arxiv.org/pdf/2501.09783",
      "summary": "We present GeoManip, a framework to enable generalist robots to leverage essential conditions derived from object and part relationships, as geometric constraints, for robot manipulation. For example, cutting the carrot requires adhering to a geometric constraint: the blade of the knife should be perpendicular to the carrot's direction. By interpreting these constraints through symbolic language representations and translating them into low-level actions, GeoManip bridges the gap between natural language and robotic execution, enabling greater generalizability across diverse even unseen tasks, objects, and scenarios. Unlike vision-language-action models that require extensive training, operates training-free by utilizing large foundational models: a constraint generation module that predicts stage-specific geometric constraints and a geometry parser that identifies object parts involved in these constraints. A solver then optimizes trajectories to satisfy inferred constraints from task descriptions and the scene. Furthermore, GeoManip learns in-context and provides five appealing human-robot interaction features: on-the-fly policy adaptation, learning from human demonstrations, learning from failure cases, long-horizon action planning, and efficient data collection for imitation learning. Extensive evaluations on both simulations and real-world scenarios demonstrate GeoManip's state-of-the-art performance, with superior out-of-distribution generalization while avoiding costly model training.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] From Foresight to Forethought: VLM-In-the-Loop Policy Steering via Latent Alignment",
      "arxiv_id": "2502.01828",
      "arxiv_url": "https://arxiv.org/pdf/2502.01828",
      "summary": "While generative robot policies have demonstrated significant potential in learning complex, multimodal behaviors from demonstrations, they still exhibit diverse failures at deployment-time. Policy steering offers an elegant solution to reducing the chance of failure by using an external verifier to select from low-level actions proposed by an imperfect generative policy. Here, one might hope to use a Vision Language Model (VLM) as a verifier, leveraging its open-world reasoning capabilities. However, off-the-shelf VLMs struggle to understand the consequences of low-level robot actions as they are represented fundamentally differently than the text and images the VLM was trained on. In response, we propose FOREWARN, a novel framework to unlock the potential of VLMs as open-vocabulary verifiers for runtime policy steering. Our key idea is to decouple the VLM's burden of predicting action outcomes (foresight) from evaluation (forethought). For foresight, we leverage a latent world model to imagine future latent states given diverse low-level action plans. For forethought, we align the VLM with these predicted latent states to reason about the consequences of actions in its native representation--natural language--and effectively filter proposed plans. We validate our framework across diverse robotic manipulation tasks, demonstrating its ability to bridge representational gaps and provide robust, generalizable policy steering. Videos can be found on the project website: https://yilin-wu98.github.io/forewarn/.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "world model"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "open-vocabulary",
            "open vocabulary"
          ],
          "score": 4.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Towards Safe Robot Foundation Models",
      "arxiv_id": "2503.07404",
      "arxiv_url": "https://arxiv.org/pdf/2503.07404",
      "summary": "Robot foundation models hold the potential for deployment across diverse environments, from industrial applications to household tasks. While current research focuses primarily on the policies' generalization capabilities across a variety of tasks, it fails to address safety, a critical requirement for deployment on real-world systems. In this paper, we introduce a safety layer designed to constrain the action space of any generalist policy appropriately. Our approach uses ATACOM, a safe reinforcement learning algorithm that creates a safe action space and, therefore, ensures safe state transitions. By extending ATACOM to generalist policies, our method facilitates their deployment in safety-critical scenarios without requiring any specific safety fine-tuning. We demonstrate the effectiveness of this safety layer in an air hockey environment, where it prevents a puck-hitting agent from colliding with its surroundings, a failure observed in generalist policies.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse",
      "arxiv_id": "2503.16365",
      "arxiv_url": "https://arxiv.org/pdf/2503.16365",
      "summary": "Recently, action-based decision-making in open-world environments has gained significant attention. Visual Language Action (VLA) models, pretrained on large-scale web datasets, have shown promise in decision-making tasks. However, previous work has primarily focused on action post-training, often neglecting enhancements to the foundational model itself. In response, we introduce a novel approach, Act from Visual Language Post-Training, which refines Visual Language Models (VLMs) through visual and linguistic guidance in a self-supervised manner. This enhancement improves the models' capabilities in world knowledge, visual recognition, and spatial grounding in open-world environments. Following the above post-training paradigms, we obtain the first VLA models in Minecraft that can follow human instructions on over 1k different atomic tasks, including crafting, smelting, cooking, mining, and killing. Our experiments demonstrate that post-training on non-trajectory tasks leads to a significant 40% improvement over the best agent baseline on a diverse set of atomic tasks. Furthermore, we demonstrate that our approach surpasses traditional imitation learning-based policies in Minecraft, achieving state-of-the-art performance. We have open-sourced the code, models, and datasets to foster further research. The project page can be found in https://craftjarvis.github.io/JarvisVLA.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]VLA"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Training Strategies for Efficient Embodied Reasoning",
      "arxiv_id": "2505.08243",
      "arxiv_url": "https://arxiv.org/pdf/2505.08243",
      "summary": "Robot chain-of-thought reasoning (CoT) -- wherein a model predicts helpful intermediate representations before choosing actions -- provides an effective method for improving the generalization and performance of robot policies, especially vision-language-action models (VLAs). While such approaches have been shown to improve performance and generalization, they suffer from core limitations, like needing specialized robot reasoning data and slow inference speeds. To design new robot reasoning approaches that address these issues, a more complete characterization of why reasoning helps policy performance is critical. We hypothesize several mechanisms by which robot reasoning improves policies -- (1) better representation learning, (2) improved learning curricularization, and (3) increased expressivity -- then devise simple variants of robot CoT reasoning to isolate and test each one. We find that learning to generate reasonings does lead to better VLA representations, while attending to the reasonings aids in actually leveraging these features for improved action prediction. Our results provide us with a better understanding of why CoT reasoning helps VLAs, which we use to introduce two simple and lightweight alternative recipes for robot reasoning. Our proposed approaches achieve significant performance gains over non-reasoning policies, state-of-the-art results on the LIBERO-90 benchmark, and a 3x inference speedup compared to standard robot reasoning.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "representation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA",
            "chain-of-thought"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] DreamGen: Unlocking Genearlization in Robot Learning through Video World Models",
      "arxiv_id": "2505.12705",
      "arxiv_url": "https://arxiv.org/abs/2505.12705",
      "summary": "We introduce DreamGen, a simple yet highly effective 4-stage pipeline for training robot policies that generalize across behaviors and environments through neural trajectories - synthetic robot data generated from video world models. DreamGen leverages state-of-the-art image-to-video generative models, adapting them to the target robot embodiment to produce photorealistic synthetic videos of familiar or novel tasks in diverse environments. Since these models generate only videos, we recover pseudo-action sequences using either a latent action model or an inverse-dynamics model (IDM). Despite its simplicity, DreamGen unlocks strong behavior and environment generalization: a humanoid robot can perform 22 new behaviors in both seen and unseen environments, while requiring teleoperation data from only a single pick-and-place task in one environment. To evaluate the pipeline systematically, we introduce DreamGen Bench, a video generation benchmark that shows a strong correlation between benchmark performance and downstream policy success. Our work establishes a promising new axis for scaling robot learning well beyond manual data collection. Code available at https://github.com/NVIDIA/GR00T-Dreams.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid",
            "humanoid robot",
            "teleoperation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] WorldVLA: Towards Autoregressive Action World Model",
      "arxiv_id": "2506.21539",
      "arxiv_url": "https://arxiv.org/pdf/2506.21539",
      "summary": "We present WorldVLA, an autoregressive action world model that unifies action and image understanding and generation. Our WorldVLA intergrates Vision-Language-Action (VLA) model and world model in one single framework. The world model predicts future images by leveraging both action and image understanding, with the purpose of learning the underlying physics of the environment to improve action generation. Meanwhile, the action model generates the subsequent actions based on image observations, aiding in visual understanding and in turn helps visual generation of the world model. We demonstrate that WorldVLA outperforms standalone action and world models, highlighting the mutual enhancement between the world model and the action model. In addition, we find that the performance of the action model deteriorates when generating sequences of actions in an autoregressive manner. This phenomenon can be attributed to the model's limited generalization capability for action prediction, leading to the propagation of errors from earlier actions to subsequent ones. To address this issue, we propose an attention mask strategy that selectively masks prior actions during the generation of the current action, which shows significant performance improvement in the action chunk generation task.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation",
      "arxiv_id": "2508.13998",
      "arxiv_url": "https://arxiv.org/pdf/2508.13998",
      "summary": "Generalization in embodied AI is hindered by the \"seeing-to-doing gap,\" which stems from data scarcity and embodiment heterogeneity. To address this, we pioneer \"pointing\" as a unified, embodiment-agnostic intermediate representation, defining four core embodied pointing abilities that bridge high-level vision-language comprehension with low-level action primitives. We introduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed for embodied reasoning and pointing. We use a wide range of embodied and general visual reasoning datasets as sources to construct a large-scale dataset, Embodied-Points-200K, which supports key embodied pointing capabilities. We then train Embodied-R1 using a two-stage Reinforced Fine-tuning (RFT) curriculum with a specialized multi-task reward design. Embodied-R1 achieves state-of-the-art performance on 11 embodied spatial and pointing benchmarks. Critically, it demonstrates robust zero-shot generalization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5% across 8 real-world XArm tasks without any task-specific fine-tuning, representing a 62% improvement over strong baselines. Furthermore, the model exhibits high robustness against diverse visual disturbances. Our work shows that a pointing-centric representation, combined with an RFT training paradigm, offers an effective and generalizable pathway to closing the perception-action gap in robotics.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/pickxiguapi/Embodied-R1",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reward design"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "embodied AI"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [TRO] Hierarchical Diffusion Policy: Manipulation Trajectory Generation Via Contact Guidance",
      "arxiv_id": "2411.12982",
      "arxiv_url": "https://arxiv.org/pdf/2411.12982",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "TRO",
      "code_url": "https://github.com/dexin-wang/Hierarchical-Diffusion-Policy",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] FUNCTO: Function-Centric One-Shot Imitation Learning for Tool Manipulation",
      "arxiv_id": "2502.11744",
      "arxiv_url": "https://arxiv.org/pdf/2502.11744",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Pick-and-place Manipulation Across Grippers Without Retraining: A Learning-optimization Diffusion Policy Approach",
      "arxiv_id": "2502.15613",
      "arxiv_url": "https://arxiv.org/pdf/2502.15613",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] One-Shot Dual-Arm Imitation Learning",
      "arxiv_id": "2503.06831",
      "arxiv_url": "https://arxiv.org/pdf/2503.06831",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]dual-arm"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] RoboCopilot: Human-in-the-loop Interactive Imitation Learning for Robot Manipulation",
      "arxiv_id": "2503.07771",
      "arxiv_url": "https://arxiv.org/pdf/2503.07771",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Rethinking Latent Representations in Behavior Cloning: An Information Bottleneck Approach for Robot Manipulation",
      "arxiv_id": "2502.02853",
      "arxiv_url": "https://arxiv.org/pdf/2502.02853",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]behavior cloning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] LaDi-WM: A Latent Diffusion-based World Model for Predictive Manipulation",
      "arxiv_id": "2505.11528",
      "arxiv_url": "https://arxiv.org/pdf/2505.11528",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [AAAI 25] FlowPolicy: Enabling Fast and Robust 3D Flow-Based Policy via Consistency Flow Matching for Robot Manipulation",
      "arxiv_id": "2412.04987",
      "arxiv_url": "https://arxiv.org/abs/2412.04987",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]flow matching"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] GAF: Gaussian Action Field as a Dynamic World Model for Robotic Manipulation",
      "arxiv_id": "2506.14135",
      "arxiv_url": "https://arxiv.org/pdf/2506.14135",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [IROS 25] Robust Instant Policy: Leveraging Student’s t-Regression Model for Robust In-context Imitation Learning of Robot Manipulation",
      "arxiv_id": "2506.15157",
      "arxiv_url": "https://arxiv.org/pdf/2506.15157",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "IROS",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] World4Omni: A Zero-Shot Framework from Image Generation World Model to Robotic Manipulation",
      "arxiv_id": "2506.23919",
      "arxiv_url": "https://arxiv.org/pdf/2506.23919",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": [
        "image generation"
      ]
    },
    {
      "title": "[2025] Mean Flow Tames Policy Learning in 1-step for Robotic Manipulation",
      "arxiv_id": "2507.10543",
      "arxiv_url": "https://arxiv.org/pdf/2507.10543",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]policy learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] PinchBot: Long-Horizon Deformable Manipulation with Guided Diffusion Policy",
      "arxiv_id": "2507.17846",
      "arxiv_url": "https://arxiv.org/pdf/2507.17846",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] ManipDreamer3D : Synthesizing Plausible Robotic Manipulation Video with Occupancy-aware 3D Trajectory",
      "arxiv_id": "2509.05314",
      "arxiv_url": "https://arxiv.org/pdf/2509.05314",
      "summary": "Data scarcity continues to be a major challenge in the field of robotic manipulation. Although diffusion models provide a promising solution for generating robotic manipulation videos, existing methods largely depend on 2D trajectories, which inherently face issues with 3D spatial ambiguity. In this work, we present a novel framework named ManipDreamer3D for generating plausible 3D-aware robotic manipulation videos from the input image and the text instruction. Our method combines 3D trajectory planning with a reconstructed 3D occupancy map created from a third-person perspective, along with a novel trajectory-to-video diffusion model. Specifically, ManipDreamer3D first reconstructs the 3D occupancy representation from the input image and then computes an optimized 3D end-effector trajectory, minimizing path length while avoiding collisions. Next, we employ a latent editing technique to create video sequences from the initial image latent and the optimized 3D trajectory. This process conditions our specially trained trajectory-to-video diffusion model to produce robotic pick-and-place videos. Our method generates robotic videos with autonomously planned plausible 3D trajectories, significantly reducing human intervention requirements. Experimental results demonstrate superior visual quality compared to existing methods.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]dreamer"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025]  [CoRL 25] CLASS: Contrastive Learning via Action Sequence Supervision for Robot Manipulation",
      "arxiv_id": "2508.01600",
      "arxiv_url": "https://arxiv.org/pdf/2508.01600",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "CoRL",
      "code_url": "https://github.com/sean1295/CLASS/tree/main",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]contrastive learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Learning Robotic Manipulation Policies from Point Clouds with Conditional Flow Matching",
      "arxiv_id": "2409.07343",
      "arxiv_url": "https://arxiv.org/pdf/2409.07343",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]flow matching"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] ManiCM: Real-time 3D Diffusion Policy via Consistency Model for Robotic Manipulation",
      "arxiv_id": "2406.01586",
      "arxiv_url": "https://arxiv.org/pdf/2406.01586",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Language-Guided Object-Centric Diffusion Policy for Collision-Aware Robotic Manipulation",
      "arxiv_id": "2407.00451",
      "arxiv_url": "https://arxiv.org/pdf/2407.00451",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Scaling diffusion policy in transformer to 1 billion parameters for robotic manipulation",
      "arxiv_id": "2409.14411",
      "arxiv_url": "https://arxiv.org/pdf/2409.14411",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Data Scaling Laws in Imitation Learning for Robotic Manipulation",
      "arxiv_id": "2410.18647",
      "arxiv_url": "https://arxiv.org/pdf/2410.18647",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2023] Exploring Visual Pre-training for Robot Manipulation: Datasets, Models and Methods",
      "arxiv_id": "2308.03620",
      "arxiv_url": "https://arxiv.org/pdf/2308.03620",
      "summary": "",
      "authors": [],
      "year": "2023",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]visual pre-training"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] VLP: Vision-Language Preference Learning for Embodied Manipulation",
      "arxiv_id": "2502.11918",
      "arxiv_url": "https://arxiv.org/pdf/2502.11918",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]preference learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Object-Centric World Model for Language-Guided Manipulation",
      "arxiv_id": "2503.06170",
      "arxiv_url": "https://arxiv.org/pdf/2503.06170",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] LLM-based Interactive Imitation Learning for Robotic Manipulation",
      "arxiv_id": "2504.21769",
      "arxiv_url": "https://arxiv.org/pdf/2504.21769",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Impact of Static Friction on Sim2Real in Robotic Reinforcement Learning",
      "arxiv_id": "2503.01255",
      "arxiv_url": "https://arxiv.org/pdf/2503.01255",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]sim2real"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025.02] Dexterous Safe Control for Humanoids in Cluttered Environments via Projected Safe Set Algorithm [Control] [[project](https://toruowo.github.io/recipe/)]",
      "arxiv_id": "2502.02858",
      "arxiv_url": "https://arxiv.org/abs/2502.02858",
      "summary": "It is critical to ensure safety for humanoid robots in real-world applications without compromising performance. In this paper, we consider the problem of dexterous safety, featuring limb-level geometry constraints for avoiding both external and self-collisions in cluttered environments. Compared to safety with simplified bounding geometries in sprase environments, dexterous safety produces numerous constraints which often lead to infeasible constraint sets when solving for safe robot control. To address this issue, we propose Projected Safe Set Algorithm (p-SSA), an extension of classical safe control algorithms to multi-constraint cases. p-SSA relaxes conflicting constraints in a principled manner, minimizing safety violations to guarantee feasible robot control. We verify our approach in simulation and on a real Unitree G1 humanoid robot performing complex collision avoidance tasks. Results show that p-SSA enables the humanoid to operate robustly in challenging situations with minimal safety violations and directly generalizes to various tasks with zero parameter tuning.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "Unitree"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.07] Grasping Diverse Objects with Simulated Humanoids [RL] [[project](https://www.zhengyiluo.com/Omnigrasp-Site/)]",
      "arxiv_id": "2407.11385",
      "arxiv_url": "https://arxiv.org/abs/2407.11385",
      "summary": "We present a method for controlling a simulated humanoid to grasp an object and move it to follow an object's trajectory. Due to the challenges in controlling a humanoid with dexterous hands, prior methods often use a disembodied hand and only consider vertical lifts or short trajectories. This limited scope hampers their applicability for object manipulation required for animation and simulation. To close this gap, we learn a controller that can pick up a large number (&gt;1200) of objects and carry them to follow randomly generated trajectories. Our key insight is to leverage a humanoid motion representation that provides human-like motor skills and significantly speeds up training. Using only simplistic reward, state, and object representations, our method shows favorable scalability on diverse objects and trajectories. For training, we do not need a dataset of paired full-body motion and object trajectories. At test time, we only require the object mesh and desired trajectories for grasping and transporting. To demonstrate the capabilities of our method, we show state-of-the-art success rates in following object trajectories and generalizing to unseen objects. Code and models will be released.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "manipulation",
            "dexterous hand"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.04] HOI-M3: Capture Multiple Humans and Objects Interaction within Contextual Environment [mocap] [[project](https://juzezhang.github.io/HOIM3_ProjectPage/)]",
      "arxiv_id": "2404.00299",
      "arxiv_url": "https://arxiv.org/pdf/2404.00299.pdf",
      "summary": "Humans naturally interact with both others and the surrounding multiple objects, engaging in various social activities. However, recent advances in modeling human-object interactions mostly focus on perceiving isolated individuals and objects, due to fundamental data scarcity. In this paper, we introduce HOI-M3, a novel large-scale dataset for modeling the interactions of Multiple huMans and Multiple objects. Notably, it provides accurate 3D tracking for both humans and objects from dense RGB and object-mounted IMU inputs, covering 199 sequences and 181M frames of diverse humans and objects under rich activities. With the unique HOI-M3 dataset, we introduce two novel data-driven tasks with companion strong baselines: monocular capture and unstructured generation of multiple human-object interactions. Extensive experiments demonstrate that our dataset is challenging and worthy of further research about multiple human-object interactions and behavior analysis. Our HOI-M3 dataset, corresponding codes, and pre-trained models will be disseminated to the community for future research.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "https://github.com/Juzezhang/NeuralDome_Toolbox",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "human-object interaction",
            "[T]HOI"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "5_interaction_reaction"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Unleashing Humanoid Reaching Potential via Real-world-Ready Skill Space [[project](https://zzk273.github.io/R2S2/)]",
      "arxiv_id": "2505.10918",
      "arxiv_url": "https://www.arxiv.org/pdf/2505.10918",
      "summary": "Humans possess a large reachable space in the 3D world, enabling interaction with objects at varying heights and distances. However, realizing such large-space reaching on humanoids is a complex whole-body control problem and requires the robot to master diverse skills simultaneously-including base positioning and reorientation, height and body posture adjustments, and end-effector pose control. Learning from scratch often leads to optimization difficulty and poor sim2real transferability. To address this challenge, we propose Real-world-Ready Skill Space (R2S2). Our approach begins with a carefully designed skill library consisting of real-world-ready primitive skills. We ensure optimal performance and robust sim2real transfer through individual skill tuning and sim2real evaluation. These skills are then ensembled into a unified latent space, serving as a structured prior that helps task execution in an efficient and sim2real transferable manner. A high-level planner, trained to sample skills from this space, enables the robot to accomplish real-world goal-reaching tasks. We demonstrate zero-shot sim2real transfer and validate R2S2 in multiple challenging goal-reaching scenarios.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/GalaxyGeneralRobotics/OpenWBT",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "whole-body control",
            "sim2real"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Think-Then-React: Towards Unconstrained Human Action-to-Reaction Generation",
      "arxiv_id": "2503.16451",
      "arxiv_url": "https://arxiv.org/pdf/2503.16451",
      "summary": "Modeling human-like action-to-reaction generation has significant real-world applications, like human-robot interaction and games. Despite recent advancements in single-person motion generation, it is still challenging to well handle action-to-reaction generation, due to the difficulty of directly predicting reaction from action sequence without prompts, and the absence of a unified representation that effectively encodes multi-person motion. To address these challenges, we introduce Think-Then-React (TTR), a large language-model-based framework designed to generate human-like reactions. First, with our fine-grained multimodal training strategy, TTR is capable to unify two processes during inference: a thinking process that explicitly infers action intentions and reasons corresponding reaction description, which serve as semantic prompts, and a reacting process that predicts reactions based on input action and the inferred semantic prompts. Second, to effectively represent multi-person motion in language models, we propose a unified motion tokenizer by decoupling egocentric pose and absolute space features, which effectively represents action and reaction motion with same encoding. Extensive experiments demonstrate that TTR outperforms existing baselines, achieving significant improvements in evaluation metrics, such as reducing FID from 3.988 to 1.942.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "motion generation",
            "motion tokenizer"
          ],
          "score": 5.0
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "egocentric"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "4_motion_diffusion",
        "6_video_extraction",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Diffusing in Someone Else's Shoes: Robotic Perspective Taking with Diffusion [Manipulation]",
      "arxiv_id": "2404.07735",
      "arxiv_url": "https://arxiv.org/abs/2404.07735",
      "summary": "Humanoid robots can benefit from their similarity to the human shape by learning from humans. When humans teach other humans how to perform actions, they often demonstrate the actions, and the learning human imitates the demonstration to get an idea of how to perform the action. Being able to mentally transfer from a demonstration seen from a third-person perspective to how it should look from a first-person perspective is fundamental for this ability in humans. As this is a challenging task, it is often simplified for robots by creating demonstrations from the first-person perspective. Creating these demonstrations allows for an easier imitation but requires more effort. Therefore, we introduce a novel diffusion model that enables the robot to learn from the third-person demonstrations directly by learning to generate the first-person perspective from the third-person perspective. The model translates the size and rotations of objects and the environment between the two perspectives. This allows us to utilise the benefits of easy-to-produce third-person demonstrations and easy-to-imitate first-person demonstrations.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid",
            "humanoid robot",
            "[T]manipulation"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2022] Imitate and Repurpose: Learning Reusable Robot Movement Skills From Human and Animal Behaviors. [imitation]",
      "arxiv_id": "2203.17138",
      "arxiv_url": "https://arxiv.org/abs/2203.17138",
      "summary": "We investigate the use of prior knowledge of human and animal movement to learn reusable locomotion skills for real legged robots. Our approach builds upon previous work on imitating human or dog Motion Capture (MoCap) data to learn a movement skill module. Once learned, this skill module can be reused for complex downstream tasks. Importantly, due to the prior imposed by the MoCap data, our approach does not require extensive reward engineering to produce sensible and natural looking behavior at the time of reuse. This makes it easy to create well-regularized, task-oriented controllers that are suitable for deployment on real robots. We demonstrate how our skill module can be used for imitation, and train controllable walking and ball dribbling policies for both the ANYmal quadruped and OP3 humanoid. These policies are then deployed on hardware via zero-shot simulation-to-reality transfer. Accompanying videos are available at https://bit.ly/robot-npmp.",
      "authors": [],
      "year": "2022",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped",
            "legged robot",
            "humanoid",
            "locomotion",
            "ANYmal"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] FLaRe: Achieving Masterful and Adaptive Robot Policies with Large-Scale Reinforcement Learning Fine-Tuning",
      "arxiv_id": "2409.16578",
      "arxiv_url": "https://arxiv.org/abs/2409.16578",
      "summary": "In recent years, the Robotics field has initiated several efforts toward building generalist robot policies through large-scale multi-task Behavior Cloning. However, direct deployments of these policies have led to unsatisfactory performance, where the policy struggles with unseen states and tasks. How can we break through the performance plateau of these models and elevate their capabilities to new heights? In this paper, we propose FLaRe, a large-scale Reinforcement Learning fine-tuning framework that integrates robust pre-trained representations, large-scale training, and gradient stabilization techniques. Our method aligns pre-trained policies towards task completion, achieving state-of-the-art (SoTA) performance both on previously demonstrated and on entirely novel tasks and embodiments. Specifically, on a set of long-horizon mobile manipulation tasks, FLaRe achieves an average success rate of 79.5% in unseen environments, with absolute improvements of +23.6% in simulation and +30.7% on real robots over prior SoTA methods. By utilizing only sparse rewards, our approach can enable generalizing to new capabilities beyond the pretraining data with minimal human effort. Moreover, we demonstrate rapid adaptation to new embodiments and behaviors with less than a day of fine-tuning. Videos can be found on the project website at https://robot-flare.github.io/",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "mobile manipulation"
          ],
          "score": 4.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "behavior cloning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] cVLA: Towards Efficient Camera-Space VLAs",
      "arxiv_id": "2507.02190",
      "arxiv_url": "https://arxiv.org/pdf/2507.02190",
      "summary": "Vision-Language-Action (VLA) models offer a compelling framework for tackling complex robotic manipulation tasks, but they are often expensive to train. In this paper, we propose a novel VLA approach that leverages the competitive performance of Vision Language Models (VLMs) on 2D images to directly infer robot end-effector poses in image frame coordinates. Unlike prior VLA models that output low-level controls, our model predicts trajectory waypoints, making it both more efficient to train and robot embodiment agnostic. Despite its lightweight design, our next-token prediction architecture effectively learns meaningful and executable robot trajectories. We further explore the underutilized potential of incorporating depth images, inference-time techniques such as decoding strategies, and demonstration-conditioned action generation. Our model is trained on a simulated dataset and exhibits strong sim-to-real transfer capabilities. We evaluate our approach using a combination of simulated and real data, demonstrating its effectiveness on a real robotic system.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "sim-to-real"
          ],
          "score": 4.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] RILe: Reinforced Imitation Learning",
      "arxiv_id": "2406.08472",
      "arxiv_url": "https://arxiv.org/pdf/2406.08472",
      "summary": "Acquiring complex behaviors is essential for artificially intelligent agents, yet learning these behaviors in high-dimensional settings poses a significant challenge due to the vast search space. Traditional reinforcement learning (RL) requires extensive manual effort for reward function engineering. Inverse reinforcement learning (IRL) uncovers reward functions from expert demonstrations but relies on an iterative process that is often computationally expensive. Imitation learning (IL) provides a more efficient alternative by directly comparing an agent's actions to expert demonstrations; however, in high-dimensional environments, such direct comparisons often offer insufficient feedback for effective learning. We introduce RILe (Reinforced Imitation Learning), a framework that combines the strengths of imitation learning and inverse reinforcement learning to learn a dense reward function efficiently and achieve strong performance in high-dimensional tasks. RILe employs a novel trainer-student framework: the trainer learns an adaptive reward function, and the student uses this reward signal to imitate expert behaviors. By dynamically adjusting its guidance as the student evolves, the trainer provides nuanced feedback across different phases of learning. Our framework produces high-performing policies in high-dimensional tasks where direct imitation fails to replicate complex behaviors. We validate RILe in challenging robotic locomotion tasks, demonstrating that it significantly outperforms existing methods and achieves near-expert performance across multiple settings.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning, Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "locomotion"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "[T]imitation learning",
            "inverse reinforcement learning"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 9.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[SIG GRAPH] Catch & Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks",
      "arxiv_id": "1911.06636",
      "arxiv_url": "https://arxiv.org/abs/1911.06636",
      "summary": "We address the longstanding challenge of producing flexible, realistic humanoid character controllers that can perform diverse whole-body tasks involving object interactions. This challenge is central to a variety of fields, from graphics and animation to robotics and motor neuroscience. Our physics-based environment uses realistic actuation and first-person perception -- including touch sensors and egocentric vision -- with a view to producing active-sensing behaviors (e.g. gaze direction), transferability to real robots, and comparisons to the biology. We develop an integrated neural-network based approach consisting of a motor primitive module, human demonstrations, and an instructed reinforcement learning regime with curricula and task variations. We demonstrate the utility of our approach for several tasks, including goal-conditioned box carrying and ball catching, and we characterize its behavioral robustness. The resulting controllers can be deployed in real-time on a standard PC. See overview video, https://youtu.be/2rQAW-8gQQk .",
      "authors": [],
      "year": "",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "egocentric",
            "egocentric vision"
          ],
          "score": 4.0
        },
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "character control"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 9.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "6_video_extraction",
        "8_physics_animation"
      ],
      "excluded": false,
      "exclusion_keywords": [
        "biology"
      ]
    },
    {
      "title": "[2025] [ICCV 25] DexVLG: Dexterous Vision-Language-Grasp Model at Scale",
      "arxiv_id": "2507.02747",
      "arxiv_url": "https://arxiv.org/pdf/2507.02747",
      "summary": "As large models gain traction, vision-language-action (VLA) systems are enabling robots to tackle increasingly complex tasks. However, limited by the difficulty of data collection, progress has mainly focused on controlling simple gripper end-effectors. There is little research on functional grasping with large models for human-like dexterous hands. In this paper, we introduce DexVLG, a large Vision-Language-Grasp model for Dexterous grasp pose prediction aligned with language instructions using single-view RGBD input. To accomplish this, we generate a dataset of 170 million dexterous grasp poses mapped to semantic parts across 174,000 objects in simulation, paired with detailed part-level captions. This large-scale dataset, named DexGraspNet 3.0, is used to train a VLM and flow-matching-based pose head capable of producing instruction-aligned grasp poses for tabletop objects. To assess DexVLG's performance, we create benchmarks in physics-based simulations and conduct real-world experiments. Extensive testing demonstrates DexVLG's strong zero-shot generalization capabilities-achieving over 76% zero-shot execution success rate and state-of-the-art part-grasp accuracy in simulation-and successful part-aligned grasps on physical objects in real-world scenarios.",
      "authors": [],
      "year": "2025",
      "venue": "ICCV",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "dexterous hand"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "flow matching"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 9.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Any-point Trajectory Modeling for Policy Learning",
      "arxiv_id": "2401.00025",
      "arxiv_url": "https://arxiv.org/pdf/2401.00025",
      "summary": "Learning from demonstration is a powerful method for teaching robots new skills, and having more demonstration data often improves policy learning. However, the high cost of collecting demonstration data is a significant bottleneck. Videos, as a rich data source, contain knowledge of behaviors, physics, and semantics, but extracting control-specific information from them is challenging due to the lack of action labels. In this work, we introduce a novel framework, Any-point Trajectory Modeling (ATM), that utilizes video demonstrations by pre-training a trajectory model to predict future trajectories of arbitrary points within a video frame. Once trained, these trajectories provide detailed control guidance, enabling the learning of robust visuomotor policies with minimal action-labeled data. Across over 130 language-conditioned tasks we evaluated in both simulation and the real world, ATM outperforms strong video pre-training baselines by 80% on average. Furthermore, we show effective transfer learning of manipulation skills from human videos and videos from a different robot morphology. Visualizations and code are available at: \\url{https://xingyu-lin.github.io/atm}.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]policy learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "language conditioned"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 9.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "paper",
      "arxiv_id": "2402.18294",
      "arxiv_url": "https://arxiv.org/abs/2402.18294",
      "summary": "Recently, humanoid robots have made significant advances in their ability to perform challenging tasks due to the deployment of Reinforcement Learning (RL), however, the inherent complexity of humanoid robots, including the difficulty of designing complicated reward functions and training entire sophisticated systems, still poses a notable challenge. To conquer these challenges, after many iterations and in-depth investigations, we have meticulously developed a full-size humanoid robot, \"Adam\", whose innovative structural design greatly improves the efficiency and effectiveness of the imitation learning process. In addition, we have developed a novel imitation learning framework based on an adversarial motion prior, which applies not only to Adam but also to humanoid robots in general. Using the framework, Adam can exhibit unprecedented human-like characteristics in locomotion tasks. Our experimental results demonstrate that the proposed framework enables Adam to achieve human-comparable performance in complex locomotion tasks, marking the first time that human locomotion data has been used for imitation learning in a full-size humanoid robot.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid",
            "humanoid robot",
            "locomotion"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "imitation learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Stage-Wise Reward Shaping for Acrobatic Robots: A Constrained Multi-Objective Reinforcement Learning Approach",
      "arxiv_id": "2409.15755",
      "arxiv_url": "https://arxiv.org/pdf/2409.15755",
      "summary": "As the complexity of tasks addressed through reinforcement learning (RL) increases, the definition of reward functions also has become highly complicated. We introduce an RL method aimed at simplifying the reward-shaping process through intuitive strategies. Initially, instead of a single reward function composed of various terms, we define multiple reward and cost functions within a constrained multi-objective RL (CMORL) framework. For tasks involving sequential complex movements, we segment the task into distinct stages and define multiple rewards and costs for each stage. Finally, we introduce a practical CMORL algorithm that maximizes objectives based on these rewards while satisfying constraints defined by the costs. The proposed method has been successfully demonstrated across a variety of acrobatic tasks in both simulation and real-world environments. Additionally, it has been shown to successfully perform tasks compared to existing RL and constrained RL algorithms. Our code is available at https://github.com/rllab-snu/Stage-Wise-CMORL.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "[T]reward shaping"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[PMLR] Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning. [platform]",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "",
      "code_url": "https://github.com/leggedrobotics/legged_gym",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "[T]deep reinforcement learning"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Multimodal Perception for Goal-oriented Navigation: A Survey",
      "arxiv_id": "2504.15643",
      "arxiv_url": "https://arxiv.org/pdf/2504.15643",
      "summary": "Goal-oriented navigation presents a fundamental challenge for autonomous systems, requiring agents to navigate complex environments to reach designated targets. This survey offers a comprehensive analysis of multimodal navigation approaches through the unifying perspective of inference domains, exploring how agents perceive, reason about, and navigate environments using visual, linguistic, and acoustic information. Our key contributions include organizing navigation methods based on their primary environmental reasoning mechanisms across inference domains; systematically analyzing how shared computational foundations support seemingly disparate approaches across different navigation tasks; identifying recurring patterns and distinctive strengths across various navigation paradigms; and examining the integration challenges and opportunities of multimodal perception to enhance navigation capabilities. In addition, we review approximately 200 relevant articles to provide an in-depth understanding of the current landscape.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Diffusion Models for Robotic Manipulation: A Survey",
      "arxiv_id": "2504.08438",
      "arxiv_url": "https://arxiv.org/pdf/2504.08438",
      "summary": "Diffusion generative models have demonstrated remarkable success in visual domains such as image and video generation. They have also recently emerged as a promising approach in robotics, especially in robot manipulations. Diffusion models leverage a probabilistic framework, and they stand out with their ability to model multi-modal distributions and their robustness to high-dimensional input and output spaces. This survey provides a comprehensive review of state-of-the-art diffusion models in robotic manipulation, including grasp learning, trajectory planning, and data augmentation. Diffusion models for scene and image augmentation lie at the intersection of robotics and computer vision for vision-based tasks to enhance generalizability and data scarcity. This paper also presents the two main frameworks of diffusion models and their integration with imitation learning and reinforcement learning. In addition, it discusses the common architectures and benchmarks and points out the challenges and advantages of current state-of-the-art diffusion-based methods.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "imitation learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Embodied-AI with large models: research and challenges",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]embodied AI"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [Physical Intelligence] π0.6: a VLA that Learns from Experience",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]VLA"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [AgiBot] Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation",
      "arxiv_id": "2508.05635",
      "arxiv_url": "https://arxiv.org/pdf/2508.05635",
      "summary": "We introduce Genie Envisioner (GE), a unified world foundation platform for robotic manipulation that integrates policy learning, evaluation, and simulation within a single video-generative framework. At its core, GE-Base is a large-scale, instruction-conditioned video diffusion model that captures the spatial, temporal, and semantic dynamics of real-world robotic interactions in a structured latent space. Built upon this foundation, GE-Act maps latent representations to executable action trajectories through a lightweight, flow-matching decoder, enabling precise and generalizable policy inference across diverse embodiments with minimal supervision. To support scalable evaluation and training, GE-Sim serves as an action-conditioned neural simulator, producing high-fidelity rollouts for closed-loop policy development. The platform is further equipped with EWMBench, a standardized benchmark suite measuring visual fidelity, physical consistency, and instruction-action alignment. Together, these components establish Genie Envisioner as a scalable and practical foundation for instruction-driven, general-purpose embodied intelligence. All code, models, and benchmarks will be released publicly.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/AgibotTech/Genie-Envisioner",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning",
            "flow matching"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] iManip: Skill-Incremental Learning for Robotic Manipulation",
      "arxiv_id": "2503.07087",
      "arxiv_url": "https://arxiv.org/pdf/2503.07087",
      "summary": "The development of a generalist agent with adaptive multiple manipulation skills has been a long-standing goal in the robotics community. In this paper, we explore a crucial task, skill-incremental learning, in robotic manipulation, which is to endow the robots with the ability to learn new manipulation skills based on the previous learned knowledge without re-training. First, we build a skill-incremental environment based on the RLBench benchmark, and explore how traditional incremental methods perform in this setting. We find that they suffer from severe catastrophic forgetting due to the previous methods on classification overlooking the characteristics of temporality and action complexity in robotic manipulation tasks. Towards this end, we propose an incremental Manip}ulation framework, termed iManip, to mitigate the above issues. We firstly design a temporal replay strategy to maintain the integrity of old skills when learning new skill. Moreover, we propose the extendable PerceiverIO, consisting of an action prompt with extendable weight to adapt to new action primitives in new skill. Extensive experiments show that our framework performs well in Skill-Incremental Learning. Codes of the skill-incremental environment with our framework will be open-source.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "generalist agent"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] FP3: A 3D Foundation Policy for Robotic Manipulation",
      "arxiv_id": "2503.08950",
      "arxiv_url": "https://arxiv.org/pdf/2503.08950",
      "summary": "Following its success in natural language processing and computer vision, foundation models that are pre-trained on large-scale multi-task datasets have also shown great potential in robotics. However, most existing robot foundation models rely solely on 2D image observations, ignoring 3D geometric information, which is essential for robots to perceive and reason about the 3D world. In this paper, we introduce FP3, a first large-scale 3D foundation policy model for robotic manipulation. FP3 builds on a scalable diffusion transformer architecture and is pre-trained on 60k trajectories with point cloud observations. With the model design and diverse pre-training data, FP3 can be efficiently fine-tuned for downstream tasks while exhibiting strong generalization capabilities. Experiments on real robots demonstrate that with only 80 demonstrations, FP3 is able to learn a new task with over 90% success rates in novel environments with unseen objects, significantly surpassing existing robot foundation models.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] RoboFlamingo-Plus: Fusion of Depth and RGB Perception with Vision-Language Models for Enhanced Robotic Manipulation",
      "arxiv_id": "2503.19510",
      "arxiv_url": "https://arxiv.org/pdf/2503.19510",
      "summary": "As robotic technologies advancing towards more complex multimodal interactions and manipulation tasks, the integration of advanced Vision-Language Models (VLMs) has become a key driver in the field. Despite progress with current methods, challenges persist in fusing depth and RGB information within 3D environments and executing tasks guided by linguistic instructions. In response to these challenges, we have enhanced the existing RoboFlamingo framework by introducing RoboFlamingo-Plus, which incorporates depth data into VLMs to significantly improve robotic manipulation performance. Our research achieves a nuanced fusion of RGB and depth information by integrating a pre-trained Vision Transformer (ViT) with a resampling technique, closely aligning this combined data with linguistic cues for superior multimodal understanding. The novelty of RoboFlamingo-Plus lies in its adaptation of inputs for depth data processing, leveraging a pre-trained resampler for depth feature extraction, and employing cross-attention mechanisms for optimal feature integration. These improvements allow RoboFlamingo-Plus to not only deeply understand 3D environments but also easily perform complex, language-guided tasks in challenging settings. Experimental results show that RoboFlamingo-Plus boosts robotic manipulation by 10-20% over current methods, marking a significant advancement. Codes and model weights are public at RoboFlamingo-Plus.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [Physical Intelligence] [NeurIPS 25] Real-Time Execution of Action Chunking Flow Policies",
      "arxiv_id": "2506.07339",
      "arxiv_url": "https://arxiv.org/pdf/2506.07339",
      "summary": "Modern AI systems, especially those interacting with the physical world, increasingly require real-time performance. However, the high latency of state-of-the-art generalist models, including recent vision-language action models (VLAs), poses a significant challenge. While action chunking has enabled temporal consistency in high-frequency control tasks, it does not fully address the latency problem, leading to pauses or out-of-distribution jerky movements at chunk boundaries. This paper presents a novel inference-time algorithm that enables smooth asynchronous execution of action chunking policies. Our method, real-time chunking (RTC), is applicable to any diffusion- or flow-based VLA out of the box with no re-training. It generates the next action chunk while executing the current one, \"freezing\" actions guaranteed to execute and \"inpainting\" the rest. To test RTC, we introduce a new benchmark of 12 highly dynamic tasks in the Kinetix simulator, as well as evaluate 6 challenging real-world bimanual manipulation tasks. Results demonstrate that RTC is fast, performant, and uniquely robust to inference delay, significantly improving task throughput and enabling high success rates in precise tasks $\\unicode{x2013}$ such as lighting a match $\\unicode{x2013}$ even in the presence of significant latency. See https://pi.website/research/real_time_chunking for videos.",
      "authors": [],
      "year": "2025",
      "venue": "NeurIPS",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "bi-manual",
            "bimanual manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "VLA"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": [
        "inpainting"
      ]
    },
    {
      "title": "[2025] MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption",
      "arxiv_id": "2510.05580",
      "arxiv_url": "https://arxiv.org/pdf/2510.05580",
      "summary": "Vision-Language-Action (VLA) models show promise in embodied reasoning, yet remain far from true generalists-they often require task-specific fine-tuning, and generalize poorly to unseen tasks. We propose MetaVLA, a unified, backbone-agnostic post-training framework for efficient and scalable alignment. MetaVLA introduces Context-Aware Meta Co-Training, which consolidates diverse target tasks into a single fine-tuning stage while leveraging structurally diverse auxiliary tasks to improve in-domain generalization. Unlike naive multi-task SFT, MetaVLA integrates a lightweight meta-learning mechanism-derived from Attentive Neural Processes-to enable rapid adaptation from diverse contexts with minimal architectural change or inference overhead. On the LIBERO benchmark, MetaVLA with six auxiliary tasks outperforms OpenVLA by up to 8.0% on long-horizon tasks, reduces training steps from 240K to 75K, and cuts GPU time by ~76%. These results show that scalable, low-resource post-training is achievable-paving the way toward general-purpose embodied agents. Code will be available.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA",
            "OpenVLA"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] [Physical Intelligence] π0: A Vision-Language-Action Flow Model for General Robot Control",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] MiniVLA: A Better VLA with a Smaller Footprint",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]VLA"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models",
      "arxiv_id": "2412.14058",
      "arxiv_url": "https://arxiv.org/abs/2412.14058",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Multimodal Diffusion Transformer: Learning Versatile Behavior from Multimodal Goals",
      "arxiv_id": "2407.05996",
      "arxiv_url": "https://arxiv.org/pdf/2407.05996",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] An Embodied Generalist Agent in 3D World",
      "arxiv_id": "2311.12871",
      "arxiv_url": "https://arxiv.org/pdf/2311.12871",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]generalist agent"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Robotic Control via Embodied Chain-of-Thought Reasoning",
      "arxiv_id": "2407.08693",
      "arxiv_url": "https://arxiv.org/pdf/2407.08693",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]chain-of-thought"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Yell At Your Robot: Improving On-the-Fly from Language Corrections",
      "arxiv_id": "2403.12910",
      "arxiv_url": "https://arxiv.org/pdf/2403.12910",
      "summary": "Hierarchical policies that combine language and low-level control have been shown to perform impressively long-horizon robotic tasks, by leveraging either zero-shot high-level planners like pretrained language and vision-language models (LLMs/VLMs) or models trained on annotated robotic demonstrations. However, for complex and dexterous skills, attaining high success rates on long-horizon tasks still represents a major challenge -- the longer the task is, the more likely it is that some stage will fail. Can humans help the robot to continuously improve its long-horizon task performance through intuitive and natural feedback? In this paper, we make the following observation: high-level policies that index into sufficiently rich and expressive low-level language-conditioned skills can be readily supervised with human feedback in the form of language corrections. We show that even fine-grained corrections, such as small movements (\"move a bit to the left\"), can be effectively incorporated into high-level policies, and that such corrections can be readily obtained from humans observing the robot and making occasional suggestions. This framework enables robots not only to rapidly adapt to real-time language feedback, but also incorporate this feedback into an iterative training scheme that improves the high-level policy's ability to correct errors in both low-level execution and high-level decision-making purely from verbal feedback. Our evaluation on real hardware shows that this leads to significant performance improvement in long-horizon, dexterous manipulation tasks without the need for any additional teleoperation. Videos and code are available at https://yay-robot.github.io/.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "dexterous manipulation",
            "teleoperation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "language conditioned"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Run-time Observation Interventions Make Vision-Language-Action Models More Visually Robust",
      "arxiv_id": "2410.01971",
      "arxiv_url": "https://arxiv.org/pdf/2410.01971",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Actra: Optimized Transformer Architecture for Vision-Language-Action Models in Robot Learning",
      "arxiv_id": "2408.01147",
      "arxiv_url": "https://arxiv.org/pdf/2408.01147",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2023] RT-1: Robotics Transformer for Real-World Control at Scale",
      "arxiv_id": "2212.06817",
      "arxiv_url": "https://arxiv.org/pdf/2212.06817",
      "summary": "",
      "authors": [],
      "year": "2023",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]RT-1"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2023] Vision-Language Foundation Models as Effective Robot Imitators",
      "arxiv_id": "2311.01378",
      "arxiv_url": "https://arxiv.org/pdf/2311.01378",
      "summary": "",
      "authors": [],
      "year": "2023",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2023] Compositional Foundation Models for Hierarchical Planning",
      "arxiv_id": "2309.08587",
      "arxiv_url": "https://arxiv.org/pdf/2309.08587",
      "summary": "",
      "authors": [],
      "year": "2023",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2023] Prompt a Robot to Walk with Large Language Models",
      "arxiv_id": "2309.09969",
      "arxiv_url": "https://arxiv.org/pdf/2309.09969",
      "summary": "",
      "authors": [],
      "year": "2023",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] HA-VLN: A Benchmark for Human-Aware Navigation in Discrete-Continuous Environments with Dynamic Multi-Human Interactions, Real-World Validation, and an Open Leaderboard",
      "arxiv_id": "2503.14229",
      "arxiv_url": "https://arxiv.org/pdf/2503.14229",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]VLN"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Unseen from Seen: Rewriting Observation-Instruction Using Foundation Models for Augmenting Vision-Language Navigation",
      "arxiv_id": "2503.18065",
      "arxiv_url": "https://arxiv.org/pdf/2503.18065",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model",
      "arxiv_id": "2508.10416",
      "arxiv_url": "https://arxiv.org/pdf/2508.10416",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]vision-language-action"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [CoRL 25] GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation",
      "arxiv_id": "2509.10454",
      "arxiv_url": "https://arxiv.org/pdf/2509.10454",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "CoRL",
      "code_url": "https://github.com/bagh2178/GC-VLN",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]VLN"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Embodied Navigation Foundation Model",
      "arxiv_id": "2509.12129",
      "arxiv_url": "https://arxiv.org/pdf/2509.12129",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [CoRL 25] Search-TTA: A Multimodal Test-Time Adaptation Framework for Visual Search in the Wild",
      "arxiv_id": "2505.11350",
      "arxiv_url": "https://arxiv.org/pdf/2505.11350",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "CoRL",
      "code_url": "https://github.com/marmotlab/Search-TTA-VLN",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking",
      "arxiv_id": "2510.07134",
      "arxiv_url": "https://arxiv.org/pdf/2510.07134",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]VLA"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation",
      "arxiv_id": "2512.08186",
      "arxiv_url": "https://arxiv.org/pdf/2512.08186",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/InternRobotics/InternNav",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2023] Bridging Zero-shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill",
      "arxiv_id": "2309.10309",
      "arxiv_url": "https://arxiv.org/pdf/2309.10309",
      "summary": "",
      "authors": [],
      "year": "2023",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Steering Your Diffusion Policy with Latent Space Reinforcement Learning",
      "arxiv_id": "2506.15799",
      "arxiv_url": "https://arxiv.org/pdf/2506.15799",
      "summary": "Robotic control policies learned from human demonstrations have achieved impressive results in many real-world applications. However, in scenarios where initial performance is not satisfactory, as is often the case in novel open-world settings, such behavioral cloning (BC)-learned policies typically require collecting additional human demonstrations to further improve their behavior -- an expensive and time-consuming process. In contrast, reinforcement learning (RL) holds the promise of enabling autonomous online policy improvement, but often falls short of achieving this due to the large number of samples it typically requires. In this work we take steps towards enabling fast autonomous adaptation of BC-trained policies via efficient real-world RL. Focusing in particular on diffusion policies -- a state-of-the-art BC methodology -- we propose diffusion steering via reinforcement learning (DSRL): adapting the BC policy by running RL over its latent-noise space. We show that DSRL is highly sample efficient, requires only black-box access to the BC policy, and enables effective real-world autonomous policy improvement. Furthermore, DSRL avoids many of the challenges associated with finetuning diffusion policies, obviating the need to modify the weights of the base policy at all. We demonstrate DSRL on simulated benchmarks, real-world robotic tasks, and for adapting pretrained generalist policies, illustrating its sample efficiency and effective performance at real-world policy improvement.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "[T]diffusion policy"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Beyond Behavior Cloning: Robustness through Interactive Imitation and Contrastive Learning",
      "arxiv_id": "2502.07645",
      "arxiv_url": "https://arxiv.org/pdf/2502.07645",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]behavior cloning",
            "[T]contrastive learning"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Generalist World Model Pre-Training for Efficient Reinforcement Learning",
      "arxiv_id": "2502.19544",
      "arxiv_url": "https://arxiv.org/pdf/2502.19544",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "[T]world model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] SHADOW: Leveraging Segmentation Masks for Cross-Embodiment Policy Transfer",
      "arxiv_id": "2503.00774",
      "arxiv_url": "https://arxiv.org/pdf/2503.00774",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "[T]cross-embodiment"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "7_retargeting"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representations",
      "arxiv_id": "2505.08787",
      "arxiv_url": "https://arxiv.org/pdf/2505.08787",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "[T]cross-embodiment"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "7_retargeting"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] H2R: A Human-to-Robot Data Augmentation for Robot Pre-training from Videos",
      "arxiv_id": "2505.11920",
      "arxiv_url": "https://arxiv.org/pdf/2505.11920",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "[T]human-to-robot"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "7_retargeting"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [ICCV 25] Spatial-Temporal Aware Visuomotor Diffusion Policy Learning",
      "arxiv_id": "2507.06710",
      "arxiv_url": "https://arxiv.org/pdf/2507.06710",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "ICCV",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]policy learning",
            "[T]diffusion policy"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [CoRL 25] DiWA: Diffusion Policy Adaptation with World Models",
      "arxiv_id": "2508.03645",
      "arxiv_url": "https://arxiv.org/pdf/2508.03645",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "CoRL",
      "code_url": "https://github.com/acl21/diwa",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy",
            "[T]world model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] 3D Flow Diffusion Policy: Visuomotor Policy Learning via Generating Flow in 3D Space",
      "arxiv_id": "2509.18676",
      "arxiv_url": "https://arxiv.org/pdf/2509.18676",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]policy learning",
            "[T]diffusion policy"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] [RSS 25] 3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple 3D Representations",
      "arxiv_id": "2403.03954",
      "arxiv_url": "https://arxiv.org/pdf/2403.03954",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "RSS",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]policy learning",
            "[T]diffusion policy"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] [IROS 25] Mamba Policy: Towards Efficient 3D Diffusion Policy with Hybrid Selective State Models",
      "arxiv_id": "2409.07163",
      "arxiv_url": "https://arxiv.org/pdf/2409.07163",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "IROS",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy",
            "[T]Mamba"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] One-Step Diffusion Policy: Fast Visuomotor Policies via Diffusion Distillation",
      "arxiv_id": "2410.21257",
      "arxiv_url": "https://arxiv.org/pdf/2410.21257",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy",
            "[T]distillation"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Consistency policy: Accelerated visuomotor policies via consistency distillation",
      "arxiv_id": "2405.07503",
      "arxiv_url": "https://arxiv.org/pdf/2405.07503",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]consistency policy",
            "[T]distillation"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Code-as-Symbolic-Planner: Foundation Model-Based Robot Planning via Symbolic Code Generation",
      "arxiv_id": "2503.01700",
      "arxiv_url": "https://arxiv.org/pdf/2503.01700",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Large Language Models as Natural Selector for Embodied Soft Robot Design",
      "arxiv_id": "2503.02249",
      "arxiv_url": "https://arxiv.org/pdf/2503.02249",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] FlowPlan: Zero-Shot Task Planning with LLM Flow Engineering for Robotic Instruction Following",
      "arxiv_id": "2503.02698",
      "arxiv_url": "https://arxiv.org/pdf/2503.02698",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]instruction following"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Self-Corrective Task Planning by Inverse Prompting with Large Language Models",
      "arxiv_id": "2503.07317",
      "arxiv_url": "https://arxiv.org/pdf/2503.07317",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] SafePlan: Leveraging Formal Logic and Chain-of-Thought Reasoning for Enhanced Safety in LLM-based Robotic Task Planning",
      "arxiv_id": "2503.06892",
      "arxiv_url": "https://arxiv.org/pdf/2503.06892",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]chain-of-thought"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] STAR: A Foundation Model-driven Framework for Robust Task Planning and Failure Recovery in Robotic Systems",
      "arxiv_id": "2503.06060",
      "arxiv_url": "https://arxiv.org/pdf/2503.06060",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] MetaFold: Language-Guided Multi-Category Garment Folding Framework via Trajectory Generation and Foundation Model",
      "arxiv_id": "2503.08372",
      "arxiv_url": "https://arxiv.org/pdf/2503.08372",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] NVP-HRI: Zero Shot Natural Voice and Posture-based Human-Robot Interaction via Large Language Model",
      "arxiv_id": "2503.09335",
      "arxiv_url": "https://arxiv.org/pdf/2503.09335",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Safety Aware Task Planning via Large Language Models in Robotics",
      "arxiv_id": "2503.15707",
      "arxiv_url": "https://arxiv.org/pdf/2503.15707",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] IRef-VLA: A Benchmark for Interactive Referential Grounding with Imperfect Language in 3D Scenes",
      "arxiv_id": "2503.17406",
      "arxiv_url": "https://arxiv.org/pdf/2503.17406",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]VLA"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Trajectory Adaptation Using Large Language Models",
      "arxiv_id": "2504.12755",
      "arxiv_url": "https://arxiv.org/pdf/2504.12755",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] SAS-Prompt: Large Language Models as Numerical Optimizers for Robot Self-Improvement",
      "arxiv_id": "2504.20459",
      "arxiv_url": "https://arxiv.org/pdf/2504.20459",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Identifying Uncertainty in Self-Adaptive Robotics with Large Language Models",
      "arxiv_id": "2504.20684",
      "arxiv_url": "https://arxiv.org/pdf/2504.20684",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Building Cooperative Embodied Agents Modularly with Large Language Models",
      "arxiv_id": "2307.02485",
      "arxiv_url": "https://arxiv.org/pdf/2307.02485",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World",
      "arxiv_id": "2401.08577",
      "arxiv_url": "https://arxiv.org/pdf/2401.08577",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2023] LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models",
      "arxiv_id": "2212.04088",
      "arxiv_url": "https://arxiv.org/pdf/2212.04088",
      "summary": "",
      "authors": [],
      "year": "2023",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2023] LLM+P: Empowering Large Language Models with Optimal Planning Proficiency",
      "arxiv_id": "2304.11477",
      "arxiv_url": "https://arxiv.org/pdf/2304.11477",
      "summary": "",
      "authors": [],
      "year": "2023",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2023] Code as Policies: Language Model Programs for Embodied Control",
      "arxiv_id": "2209.07753",
      "arxiv_url": "https://arxiv.org/pdf/2209.07753",
      "summary": "",
      "authors": [],
      "year": "2023",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]Code as Policies"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[Paper",
      "arxiv_id": "2505.01458",
      "arxiv_url": "https://arxiv.org/pdf/2505.01458",
      "summary": "Navigation and manipulation are core capabilities in Embodied AI, yet training agents with these capabilities in the real world faces high costs and time complexity. Therefore, sim-to-real transfer has emerged as a key approach, yet the sim-to-real gap persists. This survey examines how physics simulators address this gap by analyzing their properties overlooked in previous surveys. We also analyze their features for navigation and manipulation tasks, along with hardware requirements. Additionally, we offer a resource with benchmark datasets, metrics, simulation platforms, and cutting-edge methods-such as world models and geometric equivariance-to help researchers select suitable tools while accounting for hardware constraints.",
      "authors": [],
      "year": "",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Embodied AI Paper List, Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "sim-to-real"
          ],
          "score": 4.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "world model"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "embodied AI"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 8.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations",
      "arxiv_id": "2412.14803",
      "arxiv_url": "https://arxiv.org/pdf/2412.14803",
      "summary": "Visual representations play a crucial role in developing generalist robotic policies. Previous vision encoders, typically pre-trained with single-image reconstruction or two-image contrastive learning, tend to capture static information, often neglecting the dynamic aspects vital for embodied tasks. Recently, video diffusion models (VDMs) demonstrate the ability to predict future frames and showcase a strong understanding of physical world. We hypothesize that VDMs inherently produce visual representations that encompass both current static information and predicted future dynamics, thereby providing valuable guidance for robot action learning. Based on this hypothesis, we propose the Video Prediction Policy (VPP), which learns implicit inverse dynamics model conditioned on predicted future representations inside VDMs. To predict more precise future, we fine-tune pre-trained video foundation model on robot datasets along with internet human manipulation data. In experiments, VPP achieves a 18.6\\% relative improvement on the Calvin ABC-D generalization benchmark compared to the previous state-of-the-art, and demonstrates a 31.6\\% increase in success rates for complex real-world dexterous manipulation tasks. Project page at https://video-prediction-policy.github.io",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "dexterous manipulation"
          ],
          "score": 4.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "contrastive learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 8.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] SE(3)-Equivariant Robot Learning and Control: A Tutorial Survey",
      "arxiv_id": "2503.09829",
      "arxiv_url": "https://arxiv.org/pdf/2503.09829",
      "summary": "Recent advances in deep learning and Transformers have driven major breakthroughs in robotics by employing techniques such as imitation learning, reinforcement learning, and LLM-based multimodal perception and decision-making. However, conventional deep learning and Transformer models often struggle to process data with inherent symmetries and invariances, typically relying on large datasets or extensive data augmentation. Equivariant neural networks overcome these limitations by explicitly integrating symmetry and invariance into their architectures, leading to improved efficiency and generalization. This tutorial survey reviews a wide range of equivariant deep learning and control methods for robotics, from classic to state-of-the-art, with a focus on SE(3)-equivariant models that leverage the natural 3D rotational and translational symmetries in visual robotic manipulation and control design. Using unified mathematical notation, we begin by reviewing key concepts from group theory, along with matrix Lie groups and Lie algebras. We then introduce foundational group-equivariant neural network design and show how the group-equivariance can be obtained through their structure. Next, we discuss the applications of SE(3)-equivariant neural networks in robotics in terms of imitation learning and reinforcement learning. The SE(3)-equivariant control design is also reviewed from the perspective of geometric control. Finally, we highlight the challenges and future directions of equivariant methods in developing more robust, sample-efficient, and multi-modal real-world robotic systems.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "imitation learning"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics",
      "arxiv_id": "2406.10721",
      "arxiv_url": "https://arxiv.org/pdf/2406.10721",
      "summary": "From rearranging objects on a table to putting groceries into shelves, robots must plan precise action points to perform tasks accurately and reliably. In spite of the recent adoption of vision language models (VLMs) to control robot behavior, VLMs struggle to precisely articulate robot actions using language. We introduce an automatic synthetic data generation pipeline that instruction-tunes VLMs to robotic domains and needs. Using the pipeline, we train RoboPoint, a VLM that predicts image keypoint affordances given language instructions. Compared to alternative approaches, our method requires no real-world data collection or human demonstration, making it much more scalable to diverse environments and viewpoints. In addition, RoboPoint is a general model that enables several downstream applications such as robot navigation, manipulation, and augmented reality (AR) assistance. Our experiments demonstrate that RoboPoint outperforms state-of-the-art VLMs (GPT-4o) and visual prompting techniques (PIVOT) by 21.8% in the accuracy of predicting spatial affordance and by 30.5% in the success rate of downstream tasks. Project website: https://robo-point.github.io.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]affordance"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] GraspMolmo: Generalizable Task-Oriented Grasping via Large-Scale Synthetic Data Generation",
      "arxiv_id": "2505.13441",
      "arxiv_url": "https://arxiv.org/pdf/2505.13441",
      "summary": "We present GrasMolmo, a generalizable open-vocabulary task-oriented grasping (TOG) model. GraspMolmo predicts semantically appropriate, stable grasps conditioned on a natural language instruction and a single RGB-D frame. For instance, given \"pour me some tea\", GraspMolmo selects a grasp on a teapot handle rather than its body. Unlike prior TOG methods, which are limited by small datasets, simplistic language, and uncluttered scenes, GraspMolmo learns from PRISM, a novel large-scale synthetic dataset of 379k samples featuring cluttered environments and diverse, realistic task descriptions. We fine-tune the Molmo visual-language model on this data, enabling GraspMolmo to generalize to novel open-vocabulary instructions and objects. In challenging real-world evaluations, GraspMolmo achieves state-of-the-art results, with a 70% prediction success on complex tasks, compared to the 35% achieved by the next best alternative. GraspMolmo also successfully demonstrates the ability to predict semantically correct bimanual grasps zero-shot. We release our synthetic dataset, code, model, and benchmarks to accelerate research in task-semantic robotic manipulation, which, along with videos, are available at https://abhaybd.github.io/GraspMolmo/.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "bi-manual"
          ],
          "score": 4.0
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "open-vocabulary",
            "open vocabulary"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Vision in Action: Learning Active Perception from Human Demonstrations",
      "arxiv_id": "2506.15666",
      "arxiv_url": "https://arxiv.org/pdf/2506.15666",
      "summary": "We present Vision in Action (ViA), an active perception system for bimanual robot manipulation. ViA learns task-relevant active perceptual strategies (e.g., searching, tracking, and focusing) directly from human demonstrations. On the hardware side, ViA employs a simple yet effective 6-DoF robotic neck to enable flexible, human-like head movements. To capture human active perception strategies, we design a VR-based teleoperation interface that creates a shared observation space between the robot and the human operator. To mitigate VR motion sickness caused by latency in the robot's physical movements, the interface uses an intermediate 3D scene representation, enabling real-time view rendering on the operator side while asynchronously updating the scene with the robot's latest observations. Together, these design elements enable the learning of robust visuomotor policies for three complex, multi-stage bimanual manipulation tasks involving visual occlusions, significantly outperforming baseline systems.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "bi-manual",
            "bimanual manipulation",
            "teleoperation"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "kvil)]",
      "arxiv_id": "2403.03270",
      "arxiv_url": "https://arxiv.org/abs/2403.03270",
      "summary": "Visual imitation learning has achieved impressive progress in learning unimanual manipulation tasks from a small set of visual observations, thanks to the latest advances in computer vision. However, learning bimanual coordination strategies and complex object relations from bimanual visual demonstrations, as well as generalizing them to categorical objects in novel cluttered scenes remain unsolved challenges. In this paper, we extend our previous work on keypoints-based visual imitation learning (\\mbox{K-VIL})~\\cite{gao_kvil_2023} to bimanual manipulation tasks. The proposed Bi-KVIL jointly extracts so-called \\emph{Hybrid Master-Slave Relationships} (HMSR) among objects and hands, bimanual coordination strategies, and sub-symbolic task representations. Our bimanual task representation is object-centric, embodiment-independent, and viewpoint-invariant, thus generalizing well to categorical objects in novel scenes. We evaluate our approach in various real-world applications, showcasing its ability to learn fine-grained bimanual manipulation tasks from a small number of human demonstration videos. Videos and source code are available at https://sites.google.com/view/bi-kvil.",
      "authors": [],
      "year": "",
      "venue": "arxiv",
      "code_url": "https://github.com/wyngjf/bi-kvil-pub",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "bi-manual",
            "bimanual manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Generative Artificial Intelligence in Robotic Manipulation: A Survey",
      "arxiv_id": "2503.03464",
      "arxiv_url": "https://arxiv.org/pdf/2503.03464",
      "summary": "This survey provides a comprehensive review on recent advancements of generative learning models in robotic manipulation, addressing key challenges in the field. Robotic manipulation faces critical bottlenecks, including significant challenges in insufficient data and inefficient data acquisition, long-horizon and complex task planning, and the multi-modality reasoning ability for robust policy learning performance across diverse environments. To tackle these challenges, this survey introduces several generative model paradigms, including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), diffusion models, probabilistic flow models, and autoregressive models, highlighting their strengths and limitations. The applications of these models are categorized into three hierarchical layers: the Foundation Layer, focusing on data generation and reward generation; the Intermediate Layer, covering language, code, visual, and state generation; and the Policy Layer, emphasizing grasp generation and trajectory generation. Each layer is explored in detail, along with notable works that have advanced the state of the art. Finally, the survey outlines future research directions and challenges, emphasizing the need for improved efficiency in data utilization, better handling of long-horizon tasks, and enhanced generalization across diverse robotic scenarios. All the related resources, including research papers, open-source data, and projects, are collected for the community in https://github.com/GAI4Manipulation/AwesomeGAIManipulation",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] FRMD: Fast Robot Motion Diffusion with Consistency-Distilled Movement Primitives for Smooth Action Generation",
      "arxiv_id": "2503.02048",
      "arxiv_url": "https://arxiv.org/pdf/2503.02048",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "[T]motion diffusion"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "4_motion_diffusion"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Pre-training Auto-regressive Robotic Models with 4D Representations",
      "arxiv_id": "2502.13142",
      "arxiv_url": "https://arxiv.org/pdf/2502.13142",
      "summary": "Foundation models pre-trained on massive unlabeled datasets have revolutionized natural language and computer vision, exhibiting remarkable generalization capabilities, thus highlighting the importance of pre-training. Yet, efforts in robotics have struggled to achieve similar success, limited by either the need for costly robotic annotations or the lack of representations that effectively model the physical world. In this paper, we introduce ARM4R, an Auto-regressive Robotic Model that leverages low-level 4D Representations learned from human video data to yield a better pre-trained robotic model. Specifically, we focus on utilizing 3D point tracking representations from videos derived by lifting 2D representations into 3D space via monocular depth estimation across time. These 4D representations maintain a shared geometric structure between the points and robot state representations up to a linear transformation, enabling efficient transfer learning from human video data to low-level robotic control. Our experiments show that ARM4R can transfer efficiently from human video data to robotics and consistently improves performance on tasks across various robot environments and configurations.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation",
            "monocular depth"
          ],
          "score": 4.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 7.0,
      "hit_pillars": [
        "3_perception_slam",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model",
      "arxiv_id": "2501.15830",
      "arxiv_url": "https://arxiv.org/pdf/2501.15830",
      "summary": "In this paper, we claim that spatial understanding is the keypoint in robot manipulation, and propose SpatialVLA to explore effective spatial representations for the robot foundation model. Specifically, we introduce Ego3D Position Encoding to inject 3D information into the input observations of the visual-language-action model, and propose Adaptive Action Grids to represent spatial robot movement actions with adaptive discretized action grids, facilitating learning generalizable and transferrable spatial action knowledge for cross-robot control. SpatialVLA is first pre-trained on top of a vision-language model with 1.1 Million real-world robot episodes, to learn a generalist manipulation policy across multiple robot environments and tasks. After pre-training, SpatialVLA is directly applied to perform numerous tasks in a zero-shot manner. The superior results in both simulation and real-world robots demonstrate its advantage of inferring complex robot motion trajectories and its strong in-domain multi-task generalization ability. We further show the proposed Adaptive Action Grids offer a new and effective way to fine-tune the pre-trained SpatialVLA model for new simulation and real-world setups, where the pre-learned action grids are re-discretized to capture robot-specific spatial action movements of new setups. The superior results from extensive evaluations demonstrate the exceptional in-distribution generalization and out-of-distribution adaptation capability, highlighting the crucial benefit of the proposed spatial-aware representations for generalist robot policy learning. All the details and codes will be open-sourced.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "awesome-humanoid-learning",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2023",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.12] GigaHands: A Massive Annotated Dataset of Bimanual Hand Activities [Dataset] [[project](https://ivl.cs.brown.edu/research/gigahands.html)]",
      "arxiv_id": "2412.04244",
      "arxiv_url": "https://www.arxiv.org/abs/2412.04244",
      "summary": "Understanding bimanual human hand activities is a critical problem in AI and robotics. We cannot build large models of bimanual activities because existing datasets lack the scale, coverage of diverse hand activities, and detailed annotations. We introduce GigaHands, a massive annotated dataset capturing 34 hours of bimanual hand activities from 56 subjects and 417 objects, totaling 14k motion clips derived from 183 million frames paired with 84k text annotations. Our markerless capture setup and data acquisition protocol enable fully automatic 3D hand and object estimation while minimizing the effort required for text annotation. The scale and diversity of GigaHands enable broad applications, including text-driven action synthesis, hand motion captioning, and dynamic radiance field reconstruction. Our website are avaliable at https://ivl.cs.brown.edu/research/gigahands.html .",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bi-manual"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.11] Bimanual Dexterity for Complex Tasks [IL] [[project](https://bidex-teleop.github.io/)]",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bi-manual"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.09] ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation [VLM] [[project](https://rekep-robot.github.io/)]",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "",
      "code_url": "https://github.com/huangwl18/ReKep",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Track Any Motions under Any Disturbances",
      "arxiv_id": "2509.13833",
      "arxiv_url": "https://arxiv.org/abs/2509.13833",
      "summary": "A foundational humanoid motion tracker is expected to be able to track diverse, highly dynamic, and contact-rich motions. More importantly, it needs to operate stably in real-world scenarios against various dynamics disturbances, including terrains, external forces, and physical property changes for general practical use. To achieve this goal, we propose Any2Track (Track Any motions under Any disturbances), a two-stage RL framework to track various motions under multiple disturbances in the real world. Any2Track reformulates dynamics adaptability as an additional capability on top of basic action execution and consists of two key components: AnyTracker and AnyAdapter. AnyTracker is a general motion tracker with a series of careful designs to track various motions within a single policy. AnyAdapter is a history-informed adaptation module that endows the tracker with online dynamics adaptability to overcome the sim2real gap and multiple real-world disturbances. We deploy Any2Track on Unitree G1 hardware and achieve a successful sim2real transfer in a zero-shot manner. Any2Track performs exceptionally well in tracking various motions under multiple real-world disturbances.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid",
            "sim2real",
            "Unitree"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] RoboOcc: Enhancing the Geometric and Semantic Scene Understanding for Robots",
      "arxiv_id": "2504.14604",
      "arxiv_url": "https://arxiv.org/pdf/2504.14604",
      "summary": "3D occupancy prediction enables the robots to obtain spatial fine-grained geometry and semantics of the surrounding scene, and has become an essential task for embodied perception. Existing methods based on 3D Gaussians instead of dense voxels do not effectively exploit the geometry and opacity properties of Gaussians, which limits the network's estimation of complex environments and also limits the description of the scene by 3D Gaussians. In this paper, we propose a 3D occupancy prediction method which enhances the geometric and semantic scene understanding for robots, dubbed RoboOcc. It utilizes the Opacity-guided Self-Encoder (OSE) to alleviate the semantic ambiguity of overlapping Gaussians and the Geometry-aware Cross-Encoder (GCE) to accomplish the fine-grained geometric modeling of the surrounding scene. We conduct extensive experiments on Occ-ScanNet and EmbodiedOcc-ScanNet datasets, and our RoboOcc achieves state-of the-art performance in both local and global camera settings. Further, in ablation studies of Gaussian parameters, the proposed RoboOcc outperforms the state-of-the-art methods by a large margin of (8.47, 6.27) in IoU and mIoU metric, respectively. The codes will be released soon.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]scene understanding"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] HumanPlus: Humanoid Shadowing and Imitation from Humans [[project](https://humanoid-ai.github.io/)]",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "",
      "code_url": "https://github.com/MarkFzp/humanplus",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2021] Pushing the Limits: Running at 3.2m/s on Cassie. [sim2real]",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2021",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]sim2real"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[IEEE RAS] Learning Whole-body Motor Skills for Humanoids.",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "Awesome_Quadrupedal_Robots",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]quadruped"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "Awesome-Implicit-NeRF-Robotics",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]NeRF"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "Legged-Robots",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]legged robot"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [Science Robotics] A Review of Learning-based Dynamics Models for Robotic Manipulation",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [AgiBot] AgiBot World Colosseo: Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Think Small, Act Big: Primitive-level Skill Prompt Learning for Lifelong Robot Manipulation",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] CubeRobot: Grounding Language in Rubik's Cube Manipulation via Vision-Language Model",
      "arxiv_id": "2503.19225",
      "arxiv_url": "https://arxiv.org/pdf/2503.19225",
      "summary": "We introduce CoinFT, a capacitive 6-axis force/torque (F/T) sensor that is compact, light, low-cost, and robust with an average mean-squared error of 0.11N for force and 0.84mNm for moment when the input ranges from 0~10N and 0~4N in normal and shear directions, respectively. CoinFT is a stack of two rigid PCBs with comb-shaped electrodes connected by an array of silicone rubber pillars. The microcontroller interrogates the electrodes in different subsets in order to enhance sensitivity for measuring 6-axis F/T. The combination of desirable features of CoinFT enables various contact-rich robot interactions at a scale, across different embodiment domains including drones, robot end-effectors, and wearable haptic devices. We demonstrate the utility of CoinFT on drones by performing an attitude-based force control to perform tasks that require careful contact force modulation. The design, fabrication, and firmware of CoinFT are open-sourced at https://hojung-choi.github.io/coinft.github.io/.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": [
        "drone"
      ]
    },
    {
      "title": "[2025] DexTOG: Learning Task-Oriented Dexterous Grasp with Language Condition",
      "arxiv_id": "2504.04573",
      "arxiv_url": "https://arxiv.org/pdf/2504.04573",
      "summary": "This study introduces a novel language-guided diffusion-based learning framework, DexTOG, aimed at advancing the field of task-oriented grasping (TOG) with dexterous hands. Unlike existing methods that mainly focus on 2-finger grippers, this research addresses the complexities of dexterous manipulation, where the system must identify non-unique optimal grasp poses under specific task constraints, cater to multiple valid grasps, and search in a high degree-of-freedom configuration space in grasp planning. The proposed DexTOG includes a diffusion-based grasp pose generation model, DexDiffu, and a data engine to support the DexDiffu. By leveraging DexTOG, we also proposed a new dataset, DexTOG-80K, which was developed using a shadow robot hand to perform various tasks on 80 objects from 5 categories, showcasing the dexterity and multi-tasking capabilities of the robotic hand. This research not only presents a significant leap in dexterous TOG but also provides a comprehensive dataset and simulation validation, setting a new benchmark in robotic manipulation research.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "dexterous hand",
            "dexterous manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [CVPR 25] RoboGround: Robotic Manipulation with Grounded Vision-Language Priors",
      "arxiv_id": "2504.21530",
      "arxiv_url": "https://arxiv.org/pdf/2504.21530",
      "summary": "Recent advancements in robotic manipulation have highlighted the potential of intermediate representations for improving policy generalization. In this work, we explore grounding masks as an effective intermediate representation, balancing two key advantages: (1) effective spatial guidance that specifies target objects and placement areas while also conveying information about object shape and size, and (2) broad generalization potential driven by large-scale vision-language models pretrained on diverse grounding datasets. We introduce RoboGround, a grounding-aware robotic manipulation system that leverages grounding masks as an intermediate representation to guide policy networks in object manipulation tasks. To further explore and enhance generalization, we propose an automated pipeline for generating large-scale, simulated data with a diverse set of objects and instructions. Extensive experiments show the value of our dataset and the effectiveness of grounding masks as intermediate guidance, significantly enhancing the generalization abilities of robot policies.",
      "authors": [],
      "year": "2025",
      "venue": "CVPR",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Task Reconstruction and Extrapolation for using Text Latent",
      "arxiv_id": "2505.03500",
      "arxiv_url": "https://arxiv.org/pdf/2505.03500",
      "summary": "Vision-language-action models (VLAs) often achieve high performance on demonstrated tasks but struggle significantly when required to extrapolate, combining skills learned from different tasks in novel ways. For instance, VLAs might successfully put the cream cheese in the bowl and put the bowl on top of the cabinet, yet still fail to put the cream cheese on top of the cabinet. In this work, we demonstrate that behaviors from distinct tasks can be effectively recombined by manipulating the VLA's internal representations at inference time. Concretely, we identify the text latent by averaging the text tokens' hidden states across all demonstrated trajectories for a specific base task. For executing an extrapolated task, we can temporally interpolate the text latent of the two base tasks and add it back to the text hidden states, so sub-behaviors from the two tasks will be activated sequentially. We evaluate this approach using the newly created libero-ood benchmark, featuring 20 tasks extrapolated from standard LIBERO suites. The results on libero-ood show that all SOTA VLAs achieve &lt; 15% success rate, while $\\pi0$ with text latent interpolation reaches an 83% success rate. Further qualitative analysis reveals a tendency for VLAs to exhibit spatial overfitting, mapping object names to demonstrated locations rather than achieving genuine object and goal understanding. Additionally, we find that decoding the text latent yields human-unreadable prompts that can nevertheless instruct the VLA to achieve a 70% success rate on standard LIBERO suites, enabling private instruction or backdoor attacks.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [Physical Intelligence] Training-Time Action Conditioning for Efficient Real-Time Chunking",
      "arxiv_id": "2512.05964",
      "arxiv_url": "https://arxiv.org/pdf/2512.05964",
      "summary": "Real-time chunking (RTC) enables vision-language-action models (VLAs) to generate smooth, reactive robot trajectories by asynchronously predicting action chunks and conditioning on previously committed actions via inference-time inpainting. However, this inpainting method introduces computational overhead that increases inference latency. In this work, we propose a simple alternative: simulating inference delay at training time and conditioning on action prefixes directly, eliminating any inference-time overhead. Our method requires no modifications to the model architecture or robot runtime, and can be implemented with only a few additional lines of code. In simulated experiments, we find that training-time RTC outperforms inference-time RTC at higher inference delays. In real-world experiments on box building and espresso making tasks with the $π_{0.6}$ VLA, we demonstrate that training-time RTC maintains both task performance and speed parity with inference-time RTC while being computationally cheaper. Our results suggest that training-time action conditioning is a practical drop-in replacement for inference-time inpainting in real-time robot control.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": [
        "inpainting"
      ]
    },
    {
      "title": "[2024] RoboUniView: Visual-Language Model with Unified View Representation for Robotic Manipulation",
      "arxiv_id": "2406.18977",
      "arxiv_url": "https://arxiv.org/pdf/2406.18977",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation",
      "arxiv_id": "2410.06158",
      "arxiv_url": "https://arxiv.org/pdf/2410.06158",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Moto: Latent Motion Token as the Bridging Language for Robot Manipulation",
      "arxiv_id": "2412.04445",
      "arxiv_url": "https://arxiv.org/pdf/2412.04445",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] General Flow as Foundation Affordance for Scalable Robot Learning",
      "arxiv_id": "2401.11439",
      "arxiv_url": "https://arxiv.org/pdf/2401.11439",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]affordance"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2023] Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation",
      "arxiv_id": "2312.13139",
      "arxiv_url": "https://arxiv.org/pdf/2312.13139",
      "summary": "",
      "authors": [],
      "year": "2023",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2023] Zero-Shot Robotic Manipulation with Pretrained Image-Editing Diffusion Models",
      "arxiv_id": "2310.10639",
      "arxiv_url": "https://arxiv.org/pdf/2310.10639",
      "summary": "",
      "authors": [],
      "year": "2023",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] MapNav: A Novel Memory Representation via Annotated Semantic Maps for VLM-based Vision-and-Language Navigation",
      "arxiv_id": "2502.13451",
      "arxiv_url": "https://arxiv.org/pdf/2502.13451",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]semantic map"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [CoRL] ManiFlow: A General Robot Manipulation Policy via Consistency Flow Training",
      "arxiv_id": "2509.01819",
      "arxiv_url": "https://arxiv.org/pdf/2509.01819",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "CoRL",
      "code_url": "https://github.com/geyan21/ManiFlow_Policy",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [ByteDance Seed] Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation",
      "arxiv_id": "2506.09990",
      "arxiv_url": "https://arxiv.org/pdf/2506.09990",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills",
      "arxiv_id": "2502.01143",
      "arxiv_url": "https://arxiv.org/pdf/2502.01143",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] S2-Diffusion: Generalizing from Instance-level to Category-level Skills in Robot Manipulation",
      "arxiv_id": "2502.09389",
      "arxiv_url": "https://arxiv.org/pdf/2502.09389",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Retrieval Dexterity: Efficient Object Retrieval in Clutters with Dexterous Hand",
      "arxiv_id": "2502.18423",
      "arxiv_url": "https://arxiv.org/pdf/2502.18423",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]dexterous hand"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] From planning to policy: distilling Skill-RRT for long-horizon prehensile and non-prehensile manipulation",
      "arxiv_id": "2502.18015",
      "arxiv_url": "https://arxiv.org/pdf/2502.18015",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] FetchBot: Object Fetching in Cluttered Shelves via Zero-Shot Sim2Real",
      "arxiv_id": "2502.17894",
      "arxiv_url": "https://arxiv.org/pdf/2502.17894",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]sim2real"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Point Policy: Unifying Observations and Actions with Key Points for Robot Manipulation",
      "arxiv_id": "2502.20391",
      "arxiv_url": "https://arxiv.org/pdf/2502.20391",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] AVR: Active Vision-Driven Robotic Precision Manipulation with Viewpoint and Focal Length Optimization",
      "arxiv_id": "2503.01439",
      "arxiv_url": "https://arxiv.org/pdf/2503.01439",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] ArticuBot: Learning Universal Articulated Object Manipulation Policy via Large Scale Simulation",
      "arxiv_id": "2503.03045",
      "arxiv_url": "https://arxiv.org/pdf/2503.03045",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Persistent Object Gaussian Splat (POGS) for Tracking Human and Robot Manipulation of Irregularly Shaped Objects",
      "arxiv_id": "2503.05189",
      "arxiv_url": "https://arxiv.org/pdf/2503.05189",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] GAT-Grasp: Gesture-Driven Affordance Transfer for Task-Aware Robotic Grasping",
      "arxiv_id": "2503.06227",
      "arxiv_url": "https://arxiv.org/pdf/2503.06227",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]affordance"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] ES-Parkour: Advanced Robot Parkour with Bio-inspired Event Camera and Spiking Neural Network",
      "arxiv_id": "2503.09985",
      "arxiv_url": "https://arxiv.org/pdf/2503.09985",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]parkour"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] MoE-Loco: Mixture of Experts for Multitask Locomotion",
      "arxiv_id": "2503.08564",
      "arxiv_url": "https://arxiv.org/pdf/2503.08564",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]locomotion"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] DyWA: Dynamics-adaptive World Action Model for Generalizable Non-prehensile Manipulation",
      "arxiv_id": "2503.16806",
      "arxiv_url": "https://arxiv.org/pdf/2503.16806",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Sim-and-Real Co-Training: A Simple Recipe for Vision-Based Robotic Manipulation",
      "arxiv_id": "2503.24361",
      "arxiv_url": "https://arxiv.org/pdf/2503.24361",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] HACTS: a Human-As-Copilot Teleoperation System for Robot Learning",
      "arxiv_id": "2503.24070",
      "arxiv_url": "https://arxiv.org/pdf/2503.24070",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]teleoperation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] ZeroMimic: Distilling Robotic Manipulation Skills from Web Videos",
      "arxiv_id": "2503.23877",
      "arxiv_url": "https://arxiv.org/pdf/2503.23877",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Two by Two: Learning Multi-Task Pairwise Objects Assembly for Generalizable Robot Manipulation",
      "arxiv_id": "2504.06961",
      "arxiv_url": "https://arxiv.org/pdf/2504.06961",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation",
      "arxiv_id": "2412.15109",
      "arxiv_url": "https://arxiv.org/pdf/2412.15109",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot Manipulation",
      "arxiv_id": "2505.10105",
      "arxiv_url": "https://arxiv.org/pdf/2505.10105",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Zero-Shot Visual Generalization in Robot Manipulation",
      "arxiv_id": "2505.11719",
      "arxiv_url": "https://arxiv.org/pdf/2505.11719",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Object-Centric Representations Improve Policy Generalization in Robot Manipulation",
      "arxiv_id": "2505.11563",
      "arxiv_url": "https://arxiv.org/pdf/2505.11563",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] EquAct: An SE(3)-Equivariant Multi-Task Transformer for Open-Loop Robotic Manipulation",
      "arxiv_id": "2505.21351",
      "arxiv_url": "https://arxiv.org/pdf/2505.21351",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Gondola: Grounded Vision Language Planning for Generalizable Robotic Manipulation",
      "arxiv_id": "2506.11261",
      "arxiv_url": "https://arxiv.org/pdf/2506.11261",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Touch begins where vision ends: Generalizable policies for contact-rich manipulation",
      "arxiv_id": "2506.13762",
      "arxiv_url": "https://arxiv.org/pdf/2506.13762",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Tactile Beyond Pixels: Multisensory Touch Representations for Robot Manipulation",
      "arxiv_id": "2506.14754",
      "arxiv_url": "https://arxiv.org/pdf/2506.14754",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Spatial Policy: Guiding Visuomotor Robotic Manipulation with Spatial-Aware Modeling and Reasoning",
      "arxiv_id": "2508.15874",
      "arxiv_url": "https://arxiv.org/pdf/2508.15874",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "https://github.com/PlantPotatoOnMoon/SpatialPolicy",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Motion Before Action: Diffusing Object Motion as Manipulation Condition",
      "arxiv_id": "2411.09658",
      "arxiv_url": "https://arxiv.org/pdf/2411.09658",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] SPOT: SE(3) Pose Trajectory Diffusion for Object-Centric Manipulation",
      "arxiv_id": "2411.00965",
      "arxiv_url": "https://arxiv.org/pdf/2411.00965",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] G3Flow: Generative 3D Semantic Flow for Pose-aware and Generalizable Object Manipulation",
      "arxiv_id": "2411.18369",
      "arxiv_url": "https://arxiv.org/pdf/2411.18369",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation",
      "arxiv_id": "2410.08001",
      "arxiv_url": "https://arxiv.org/pdf/2410.08001",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Imagination Policy: Using Generative Point Cloud Models for Learning Manipulation Policies",
      "arxiv_id": "2406.11740",
      "arxiv_url": "https://arxiv.org/pdf/2406.11740",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Act3D: 3D Feature Field Transformers for Multi-Task Robotic Manipulation",
      "arxiv_id": "2306.17817",
      "arxiv_url": "https://arxiv.org/pdf/2306.17817",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Lift3D Foundation Policy: Lifting 2D Large-Scale Pretrained Models for Robust 3D Robotic Manipulation",
      "arxiv_id": "2411.18623",
      "arxiv_url": "https://arxiv.org/pdf/2411.18623",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Learning Diffusion Policies from Demonstrations For Compliant Contact-rich Manipulation",
      "arxiv_id": "2410.19235",
      "arxiv_url": "https://arxiv.org/pdf/2410.19235",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] RoboDexVLM: Visual Language Model-Enabled Task Planning and Motion Control for Dexterous Robot Manipulation",
      "arxiv_id": "2503.01616",
      "arxiv_url": "https://arxiv.org/pdf/2503.01616",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation",
      "arxiv_id": "2502.16707",
      "arxiv_url": "https://arxiv.org/pdf/2502.16707v1",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Bridging VLM and KMP: Enabling Fine-grained robotic manipulation via Semantic Keypoints Representation",
      "arxiv_id": "2503.02748",
      "arxiv_url": "https://arxiv.org/pdf/2503.02748",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Perceiving, Reasoning, Adapting: A Dual-Layer Framework for VLM-Guided Precision Robotic Manipulation",
      "arxiv_id": "2503.05064",
      "arxiv_url": "https://arxiv.org/pdf/2503.05064",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] AffordDexGrasp: Open-set Language-guided Dexterous Grasp with Generalizable-Instructive Affordance",
      "arxiv_id": "2503.07360",
      "arxiv_url": "https://arxiv.org/pdf/2503.07360",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]affordance"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] IMPACT : Intelligent Motion Planning with Acceptable Contact Trajectories via Vision-Language Models",
      "arxiv_id": "2503.10110",
      "arxiv_url": "https://arxiv.org/pdf/2503.10110",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]motion planning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Mitigating Cross-Modal Distraction and Ensuring Geometric Feasibility via Affordance-Guided, Self-Consistent MLLMs for Food Preparation Task Planning",
      "arxiv_id": "2503.13055",
      "arxiv_url": "https://arxiv.org/pdf/2503.13055",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]affordance"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Diffusion Dynamics Models with Generative State Estimation for Cloth Manipulation",
      "arxiv_id": "2503.11999",
      "arxiv_url": "https://arxiv.org/pdf/2503.11999",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] DeCo: Task Decomposition and Skill Composition for Zero-Shot Generalization in Long-Horizon 3D Manipulation",
      "arxiv_id": "2505.00527",
      "arxiv_url": "https://arxiv.org/pdf/2505.00527",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] RoBridge: A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation",
      "arxiv_id": "2505.01709",
      "arxiv_url": "https://arxiv.org/pdf/2505.01709",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation",
      "arxiv_id": "2505.08548",
      "arxiv_url": "https://arxiv.org/pdf/2505.08548",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] RobotSmith: Generative Robotic Tool Design for Acquisition of Complex Manipulation Skills",
      "arxiv_id": "2506.14763",
      "arxiv_url": "https://arxiv.org/pdf/2506.14763",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] CASPER: Inferring Diverse Intents for Assistive Teleoperation with Vision Language Models",
      "arxiv_id": "2506.14727",
      "arxiv_url": "https://arxiv.org/pdf/2506.14727",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]teleoperation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] FrankenBot: Brain-Morphic Modular Orchestration for Robotic Manipulation with Vision-Language Models",
      "arxiv_id": "2506.21627",
      "arxiv_url": "https://arxiv.org/pdf/2506.21627",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] RoboPearls: Editable Video Simulation for Robot Manipulation",
      "arxiv_id": "2506.22756",
      "arxiv_url": "https://arxiv.org/pdf/2506.22756",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic Manipulation",
      "arxiv_id": "2406.11548",
      "arxiv_url": "https://arxiv.org/pdf/2406.11548",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints",
      "arxiv_id": "2501.03841",
      "arxiv_url": "https://arxiv.org/pdf/2501.03841",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Physically Grounded Vision-Language Models for Robotic Manipulation",
      "arxiv_id": "2309.02561",
      "arxiv_url": "https://arxiv.org/pdf/2309.02561",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]affordance"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Surface-Based Manipulation",
      "arxiv_id": "2502.19389",
      "arxiv_url": "https://arxiv.org/pdf/2502.19389",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] RE3SIM: Generating High-Fidelity Simulation Data via 3D-Photorealistic Real-to-Sim for Robotic Manipulation",
      "arxiv_id": "2502.08645",
      "arxiv_url": "https://arxiv.org/pdf/2502.08645",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Few-shot Sim2Real Based on High Fidelity Rendering with Force Feedback Teleop",
      "arxiv_id": "2503.01301",
      "arxiv_url": "https://arxiv.org/pdf/2503.01301",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]sim2real"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] An Real-Sim-Real (RSR) Loop Framework for Generalizable Robotic Policy Transfer with Differentiable Simulation",
      "arxiv_id": "2503.10118",
      "arxiv_url": "https://arxiv.org/pdf/2503.10118",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "[T]differentiable simulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "8_physics_animation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation",
      "arxiv_id": "2506.06677",
      "arxiv_url": "https://arxiv.org/pdf/2506.06677",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation",
      "arxiv_id": "2505.09698",
      "arxiv_url": "https://arxiv.org/pdf/2505.09698",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [CVPR 25] RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins",
      "arxiv_id": "2409.02920",
      "arxiv_url": "https://arxiv.org/pdf/2409.02920",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "CVPR",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]dual-arm"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] AutoEval: Autonomous Evaluation of Generalist Robot Manipulation Policies in the Real World",
      "arxiv_id": "2503.24278",
      "arxiv_url": "https://arxiv.org/pdf/2503.24278",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] DexGarmentLab: Dexterous Garment Manipulation Environment with Generalizable Policy",
      "arxiv_id": "2505.11032",
      "arxiv_url": "https://arxiv.org/pdf/2505.11032",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] MuBlE: MuJoCo and Blender simulation Environment and Benchmark for Task Planning in Robot Manipulation",
      "arxiv_id": "2503.02834",
      "arxiv_url": "https://arxiv.org/pdf/2503.02834",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "cap.github.io/)]",
      "arxiv_id": "2403.07788",
      "arxiv_url": "https://arxiv.org/abs/2403.07788",
      "summary": "Imitation learning from human hand motion data presents a promising avenue for imbuing robots with human-like dexterity in real-world manipulation tasks. Despite this potential, substantial challenges persist, particularly with the portability of existing hand motion capture (mocap) systems and the complexity of translating mocap data into effective robotic policies. To tackle these issues, we introduce DexCap, a portable hand motion capture system, alongside DexIL, a novel imitation algorithm for training dexterous robot skills directly from human hand mocap data. DexCap offers precise, occlusion-resistant tracking of wrist and finger motions based on SLAM and electromagnetic field together with 3D observations of the environment. Utilizing this rich dataset, DexIL employs inverse kinematics and point cloud-based imitation learning to seamlessly replicate human actions with robot hands. Beyond direct learning from human motion, DexCap also offers an optional human-in-the-loop correction mechanism during policy rollouts to refine and further improve task performance. Through extensive evaluation across six challenging dexterous manipulation tasks, our approach not only demonstrates superior performance but also showcases the system's capability to effectively learn from in-the-wild mocap data, paving the way for future data collection methods in the pursuit of human-level robot dexterity. More details can be found at https://dex-cap.github.io",
      "authors": [],
      "year": "",
      "venue": "arxiv",
      "code_url": "https://github.com/j96w/DexCap",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "dexterous manipulation"
          ],
          "score": 4.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 5.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] A Generative System for Robot-to-Human Handovers: from Intent Inference to Spatial Configuration Imagery",
      "arxiv_id": "2503.03579",
      "arxiv_url": "https://arxiv.org/pdf/2503.03579",
      "summary": "We propose a novel system for robot-to-human object handover that emulates human coworker interactions. Unlike most existing studies that focus primarily on grasping strategies and motion planning, our system focus on 1. inferring human handover intents, 2. imagining spatial handover configuration. The first one integrates multimodal perception-combining visual and verbal cues-to infer human intent. The second one using a diffusion-based model to generate the handover configuration, involving the spacial relationship among robot's gripper, the object, and the human hand, thereby mimicking the cognitive process of motor imagery. Experimental results demonstrate that our approach effectively interprets human cues and achieves fluent, human-like handovers, offering a promising solution for collaborative robotics. Code, videos, and data are available at: https://i3handover.github.io.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "motion planning"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 5.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "RoboMamba",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "",
      "code_url": "",
      "source_repo": "Embodied AI Paper List",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.10] Autonomous Character-Scene Interaction Synthesis from Text Instruction [mocap] [[project](https://lingomotions.com/)]",
      "arxiv_id": "2410.03187",
      "arxiv_url": "https://arxiv.org/abs/2410.03187",
      "summary": "Synthesizing human motions in 3D environments, particularly those with complex activities such as locomotion, hand-reaching, and human-object interaction, presents substantial demands for user-defined waypoints and stage transitions. These requirements pose challenges for current models, leading to a notable gap in automating the animation of characters from simple human inputs. This paper addresses this challenge by introducing a comprehensive framework for synthesizing multi-stage scene-aware interaction motions directly from a single text instruction and goal location. Our approach employs an auto-regressive diffusion model to synthesize the next motion segment, along with an autonomous scheduler predicting the transition for each action stage. To ensure that the synthesized motions are seamlessly integrated within the environment, we propose a scene representation that considers the local perception both at the start and the goal location. We further enhance the coherence of the generated motion by integrating frame embeddings with language input. Additionally, to support model training, we present a comprehensive motion-captured dataset comprising 16 hours of motion sequences in 120 indoor scenes covering 40 types of motions, each annotated with precise language descriptions. Experimental results demonstrate the efficacy of our method in generating high-quality, multi-stage motions closely aligned with environmental and textual conditions.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "locomotion"
          ],
          "score": 2.0
        },
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "human-object interaction"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "1_robot_core",
        "5_interaction_reaction"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "Reinforcement-Learning-in-Robotics",
      "arxiv_id": "",
      "arxiv_url": "",
      "summary": "",
      "authors": [],
      "year": "",
      "venue": "",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] ViSA-Flow: Accelerating Robot Skill Learning via Large-Scale Video Semantic Action Flow",
      "arxiv_id": "2505.01288",
      "arxiv_url": "https://arxiv.org/pdf/2505.01288",
      "summary": "One of the central challenges preventing robots from acquiring complex manipulation skills is the prohibitive cost of collecting large-scale robot demonstrations. In contrast, humans are able to learn efficiently by watching others interact with their environment. To bridge this gap, we introduce semantic action flow as a core intermediate representation capturing the essential spatio-temporal manipulator-object interactions, invariant to superficial visual differences. We present ViSA-Flow, a framework that learns this representation self-supervised from unlabeled large-scale video data. First, a generative model is pre-trained on semantic action flows automatically extracted from large-scale human-object interaction video data, learning a robust prior over manipulation structure. Second, this prior is efficiently adapted to a target robot by fine-tuning on a small set of robot demonstrations processed through the same semantic abstraction pipeline. We demonstrate through extensive experiments on the CALVIN benchmark and real-world tasks that ViSA-Flow achieves state-of-the-art performance, particularly in low-data regimes, outperforming prior methods by effectively transferring knowledge from human video observation to robotic execution. Videos are available at https://visaflow-web.github.io/ViSAFLOW.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "human-object interaction"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "1_robot_core",
        "5_interaction_reaction"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] RT-H: Action Hierarchies Using Language",
      "arxiv_id": "2403.01823",
      "arxiv_url": "https://arxiv.org/pdf/2403.01823v1",
      "summary": "Language provides a way to break down complex concepts into digestible pieces. Recent works in robot imitation learning use language-conditioned policies that predict actions given visual observations and the high-level task specified in language. These methods leverage the structure of natural language to share data between semantically similar tasks (e.g., \"pick coke can\" and \"pick an apple\") in multi-task datasets. However, as tasks become more semantically diverse (e.g., \"pick coke can\" and \"pour cup\"), sharing data between tasks becomes harder, so learning to map high-level tasks to actions requires much more demonstration data. To bridge tasks and actions, our insight is to teach the robot the language of actions, describing low-level motions with more fine-grained phrases like \"move arm forward\". Predicting these language motions as an intermediate step between tasks and actions forces the policy to learn the shared structure of low-level motions across seemingly disparate tasks. Furthermore, a policy that is conditioned on language motions can easily be corrected during execution through human-specified language motions. This enables a new paradigm for flexible policies that can learn from human intervention in language. Our method RT-H builds an action hierarchy using language motions: it first learns to predict language motions, and conditioned on this and the high-level task, it predicts actions, using visual context at all stages. We show that RT-H leverages this language-action hierarchy to learn policies that are more robust and flexible by effectively tapping into multi-task datasets. We show that these policies not only allow for responding to language interventions, but can also learn from such interventions and outperform methods that learn from teleoperated interventions. Our website and videos are found at https://rt-hierarchy.github.io.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "language conditioned"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Baku: An Efficient Transformer for Multi-Task Policy Learning",
      "arxiv_id": "2406.07539",
      "arxiv_url": "https://arxiv.org/pdf/2406.07539v1",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]policy learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] WMNav: Integrating Vision-Language Models into World Models for Object Goal Navigation",
      "arxiv_id": "2503.02247",
      "arxiv_url": "https://arxiv.org/pdf/2503.02247",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Vi-LAD: Vision-Language Attention Distillation for Socially-Aware Robot Navigation in Dynamic Environments",
      "arxiv_id": "2503.09820",
      "arxiv_url": "https://arxiv.org/pdf/2503.09820",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]distillation"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Flow Matching Policy Gradients",
      "arxiv_id": "2507.21053",
      "arxiv_url": "https://arxiv.org/pdf/2507.21053",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]flow matching"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] VITA: Vision-To-Action Flow Matching Policy",
      "arxiv_id": "2507.13231",
      "arxiv_url": "https://arxiv.org/pdf/2507.13231",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]flow matching"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025]  [CoRL 25 Oral] Streaming Flow Policy: Simplifying diffusion/flow-matching policies by treating action trajectories as flow trajectories",
      "arxiv_id": "2505.21851",
      "arxiv_url": "https://arxiv.org/pdf/2505.21851",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "CoRL",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]flow matching"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Modality-Composable Diffusion Policy via Inference-Time Distribution-level Composition",
      "arxiv_id": "2503.12466",
      "arxiv_url": "https://arxiv.org/pdf/2503.12466",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Adapt3R: Adaptive 3D Scene Representation for Domain Transfer in Imitation Learning",
      "arxiv_id": "2503.04877",
      "arxiv_url": "https://arxiv.org/pdf/2503.04877",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics",
      "arxiv_id": "2501.10100",
      "arxiv_url": "https://arxiv.org/pdf/2501.10100",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] VILP: Imitation Learning with Latent Video Planning",
      "arxiv_id": "2502.01784",
      "arxiv_url": "https://arxiv.org/pdf/2502.01784",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] MTDP: Modulated Transformer Diffusion Policy Model",
      "arxiv_id": "2502.09029",
      "arxiv_url": "https://arxiv.org/pdf/2502.09029",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Responsive Noise-Relaying Diffusion Policy: Responsive and Efficient Visuomotor Control",
      "arxiv_id": "2502.12724",
      "arxiv_url": "https://arxiv.org/pdf/2502.12724",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] IMLE Policy: Fast and Sample Efficient Visuomotor Policy Learning via Implicit Maximum Likelihood Estimation",
      "arxiv_id": "2502.12371",
      "arxiv_url": "https://arxiv.org/pdf/2502.12371",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]policy learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] X-IL: Exploring the Design Space of Imitation Learning Policies",
      "arxiv_id": "2502.12330",
      "arxiv_url": "https://arxiv.org/pdf/2502.12330",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Towards Fusing Point Cloud and Visual Representations for Imitation Learning",
      "arxiv_id": "2502.12320",
      "arxiv_url": "https://arxiv.org/pdf/2502.12320",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] FACTR: Force-Attending Curriculum Training for Contact-Rich Policy Learning",
      "arxiv_id": "2502.17432",
      "arxiv_url": "https://arxiv.org/pdf/2502.17432",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]policy learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] DemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning",
      "arxiv_id": "2502.16932",
      "arxiv_url": "https://arxiv.org/pdf/2502.16932",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]policy learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Action Tokenizer Matters in In-Context Imitation Learning",
      "arxiv_id": "2503.01206",
      "arxiv_url": "https://arxiv.org/pdf/2503.01206",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] OPG-Policy: Occluded Push-Grasp Policy Learning with Amodal Segmentation",
      "arxiv_id": "2503.04089",
      "arxiv_url": "https://arxiv.org/pdf/2503.04089",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]policy learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] RA-DP: Rapid Adaptive Diffusion Policy for Training-Free High-frequency Robotics Replanning",
      "arxiv_id": "2503.04051",
      "arxiv_url": "https://arxiv.org/pdf/2503.04051",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Robotic Compliant Object Prying Using Diffusion Policy Guided by Vision and Force Observations",
      "arxiv_id": "2503.03998",
      "arxiv_url": "https://arxiv.org/pdf/2503.03998",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] How to Train Your Robots? The Impact of Demonstration Modality on Imitation Learning",
      "arxiv_id": "2503.07017",
      "arxiv_url": "https://arxiv.org/pdf/2503.07017",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Enhanced View Planning for Robotic Harvesting: Tackling Occlusions with Imitation Learning",
      "arxiv_id": "2503.10334",
      "arxiv_url": "https://arxiv.org/pdf/2503.10334",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] World Modeling Makes a Better Planner: Dual Preference Optimization for Embodied Task Planning",
      "arxiv_id": "2503.10480",
      "arxiv_url": "https://arxiv.org/pdf/2503.10480",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Elastic Motion Policy: An Adaptive Dynamical System for Robust and Efficient One-Shot Imitation Learning",
      "arxiv_id": "2503.08029",
      "arxiv_url": "https://arxiv.org/pdf/2503.08029",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] AdaWorld: Learning Adaptable World Models with Latent Actions",
      "arxiv_id": "2503.18938",
      "arxiv_url": "https://arxiv.org/pdf/2503.18938",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets",
      "arxiv_id": "2504.02792",
      "arxiv_url": "https://arxiv.org/pdf/2504.02792",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Grasping Deformable Objects via Reinforcement Learning with Cross-Modal Attention to Visuo-Tactile Inputs",
      "arxiv_id": "2504.15595",
      "arxiv_url": "https://arxiv.org/pdf/2504.15595",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Few-Shot Vision-Language Action-Incremental Policy Learning",
      "arxiv_id": "2504.15517",
      "arxiv_url": "https://arxiv.org/pdf/2504.15517",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]policy learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Latent Diffusion Planning for Imitation Learning",
      "arxiv_id": "2504.16925",
      "arxiv_url": "https://arxiv.org/pdf/2504.16925",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] H3DP: Triply-Hierarchical Diffusion Policy for Visuomotor Learning",
      "arxiv_id": "2505.07819",
      "arxiv_url": "https://arxiv.org/pdf/2505.07819",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] DataMIL: Selecting Data for Robot Imitation Learning with Datamodels",
      "arxiv_id": "2505.09603",
      "arxiv_url": "https://arxiv.org/pdf/2505.09603",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] IN-RIL: Interleaved Reinforcement and Imitation Learning for Policy Fine-Tuning",
      "arxiv_id": "2505.10442",
      "arxiv_url": "https://arxiv.org/pdf/2505.10442",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] A Practical Guide for Incorporating Symmetry in Diffusion Policy",
      "arxiv_id": "2505.13431",
      "arxiv_url": "https://arxiv.org/pdf/2505.13431",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Evaluating Robot Policies in a World Model",
      "arxiv_id": "2506.00613",
      "arxiv_url": "https://arxiv.org/pdf/2506.00613",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] SAIL: Faster-than-Demonstration Execution of Imitation Learning Policies",
      "arxiv_id": "2506.11948",
      "arxiv_url": "https://arxiv.org/pdf/2506.11948",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] DemoDiffusion: One-Shot Human Imitation using pre-trained Diffusion Policy",
      "arxiv_id": "2506.20668",
      "arxiv_url": "https://arxiv.org/pdf/2506.20668",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Reinforcement Learning with Action Chunking",
      "arxiv_id": "2507.07969",
      "arxiv_url": "https://arxiv.org/pdf/2507.07969",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] [IROS 25] Real-time Iteration Scheme for Diffusion Policy",
      "arxiv_id": "2508.05396",
      "arxiv_url": "https://arxiv.org/pdf/2508.05396",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "IROS",
      "code_url": "https://github.com/RTI-DP/rti-dp/",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Sparse diffusion policy: A sparse, reusable, and flexible policy for robot learning",
      "arxiv_id": "2407.01531",
      "arxiv_url": "https://arxiv.org/pdf/2407.01531",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] [ICLR 25] Diffusion Policy Policy Optimization",
      "arxiv_id": "2409.00588",
      "arxiv_url": "https://arxiv.org/pdf/2409.00588",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "ICLR",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] EquiBot: SIM(3)-Equivariant Diffusion Policy for Generalizable and Data Efficient Learning",
      "arxiv_id": "2407.01479",
      "arxiv_url": "https://arxiv.org/pdf/2407.01479",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Equivariant Diffusion Policy",
      "arxiv_id": "2407.01812",
      "arxiv_url": "https://arxiv.org/pdf/2407.01812",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Diffusion Policy Attacker: Crafting Adversarial Attacks for Diffusion-based Policies",
      "arxiv_id": "2405.19424",
      "arxiv_url": "https://arxiv.org/pdf/2405.19424",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] GenDP: 3D Semantic Fields for Category-Level Generalizable Diffusion Policy",
      "arxiv_id": "2410.17488",
      "arxiv_url": "https://arxiv.org/pdf/2410.17488",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Prediction with Action: Visual Policy Learning via Joint Denoising Process",
      "arxiv_id": "2411.18179",
      "arxiv_url": "https://arxiv.org/pdf/2411.18179",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]policy learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Streaming Diffusion Policy: Fast Policy Synthesis with Variable Noise Diffusion Models",
      "arxiv_id": "2406.04806",
      "arxiv_url": "https://arxiv.org/pdf/2406.04806",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]diffusion policy"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive Prediction",
      "arxiv_id": "2412.06782",
      "arxiv_url": "https://arxiv.org/pdf/2412.06782",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]policy learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] In-Context Imitation Learning via Next-Token Prediction",
      "arxiv_id": "2408.15980",
      "arxiv_url": "https://arxiv.org/pdf/2408.15980",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] HybridGen: VLM-Guided Hybrid Planning for Scalable Data Generation of Imitation Learning",
      "arxiv_id": "2503.13171",
      "arxiv_url": "https://arxiv.org/pdf/2503.13171",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Leveraging Language Models for Out-of-Distribution Recovery in Reinforcement Learning",
      "arxiv_id": "2503.17125",
      "arxiv_url": "https://arxiv.org/pdf/2503.17125",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] EWMBENCH: Evaluating Scene, Motion, and Semantic Quality in Embodied World Models",
      "arxiv_id": "2505.09694",
      "arxiv_url": "https://arxiv.org/pdf/2505.09694",
      "summary": "",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Towards Diverse Behaviors: A Benchmark for Imitation Learning with Human Demonstrations",
      "arxiv_id": "2402.14606",
      "arxiv_url": "https://arxiv.org/pdf/2402.14606",
      "summary": "",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] Vision-language navigation: a survey and taxonomy",
      "arxiv_id": "2108.11544",
      "arxiv_url": "https://arxiv.org/pdf/2108.11544",
      "summary": "Vision-Language Navigation (VLN) tasks require an agent to follow human language instructions to navigate in previously unseen environments. This challenging field involving problems in natural language processing, computer vision, robotics, etc., has spawn many excellent works focusing on various VLN tasks. This paper provides a comprehensive survey and an insightful taxonomy of these tasks based on the different characteristics of language instructions in these tasks. Depending on whether the navigation instructions are given for once or multiple times, this paper divides the tasks into two categories, i.e., single-turn and multi-turn tasks. For single-turn tasks, we further subdivide them into goal-oriented and route-oriented based on whether the instructions designate a single goal location or specify a sequence of multiple locations. For multi-turn tasks, we subdivide them into passive and interactive tasks based on whether the agent is allowed to question the instruction or not. These tasks require different capabilities of the agent and entail various model designs. We identify progress made on the tasks and look into the limitations of existing VLN models and task settings. Finally, we discuss several open issues of VLN and point out some opportunities in the future, i.e., incorporating knowledge with VLN models and implementing them in the real physical world.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "VLN"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Pixel Motion as Universal Representation for Robot Control",
      "arxiv_id": "2505.07817",
      "arxiv_url": "https://arxiv.org/pdf/2505.07817",
      "summary": "We present LangToMo, a vision-language-action framework structured as a dual-system architecture that uses pixel motion forecasts as intermediate representations. Our high-level System 2, an image diffusion model, generates text-conditioned pixel motion sequences from a single frame to guide robot control. Pixel motion-a universal, interpretable, and motion-centric representation-can be extracted from videos in a weakly-supervised manner, enabling diffusion model training on any video-caption data. Treating generated pixel motion as learned universal representations, our low level System 1 module translates these into robot actions via motion-to-action mapping functions, which can be either hand-crafted or learned with minimal supervision. System 2 operates as a high-level policy applied at sparse temporal intervals, while System 1 acts as a low-level policy at dense temporal intervals. This hierarchical decoupling enables flexible, scalable, and generalizable robot control under both unsupervised and supervised settings, bridging the gap between language, motion, and action. Checkout https://kahnchana.github.io/LangToMo",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Running VLAs at Real-time Speed",
      "arxiv_id": "2510.26742",
      "arxiv_url": "https://arxiv.org/pdf/2510.26742",
      "summary": "In this paper, we show how to run pi0-level multi-view VLA at 30Hz frame rate and at most 480Hz trajectory frequency using a single consumer GPU. This enables dynamic and real-time tasks that were previously believed to be unattainable by large VLA models. To achieve it, we introduce a bag of strategies to eliminate the overheads in model inference. The real-world experiment shows that the pi0 policy with our strategy achieves a 100% success rate in grasping a falling pen task. Based on the results, we further propose a full streaming inference framework for real-time robot control of VLA. Code is available at https://github.com/Dexmal/realtime-vla.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "VLA"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] AMPLIFY: Actionless Motion Priors for Robot Learning from Videos",
      "arxiv_id": "2506.14198",
      "arxiv_url": "https://arxiv.org/pdf/2506.14198",
      "summary": "Action-labeled data for robotics is scarce and expensive, limiting the generalization of learned policies. In contrast, vast amounts of action-free video data are readily available, but translating these observations into effective policies remains a challenge. We introduce AMPLIFY, a novel framework that leverages large-scale video data by encoding visual dynamics into compact, discrete motion tokens derived from keypoint trajectories. Our modular approach separates visual motion prediction from action inference, decoupling the challenges of learning what motion defines a task from how robots can perform it. We train a forward dynamics model on abundant action-free videos and an inverse dynamics model on a limited set of action-labeled examples, allowing for independent scaling. Extensive evaluations demonstrate that the learned dynamics are both accurate, achieving up to 3.7x better MSE and over 2.5x better pixel prediction accuracy compared to prior approaches, and broadly useful. In downstream policy learning, our dynamics predictions enable a 1.2-2.2x improvement in low-data regimes, a 1.4x average improvement by learning from action-free human videos, and the first generalization to LIBERO tasks from zero in-distribution action data. Beyond robotic control, we find the dynamics learned by AMPLIFY to be a versatile latent world model, enhancing video prediction quality. Our results present a novel paradigm leveraging heterogeneous data sources to build efficient, generalizable world models. More information can be found at https://amplify-robotics.github.io/.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning",
            "world model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "2_algo_arch"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024.06] CORE4D: A 4D Human-Object-Human Interaction Dataset for Collaborative Object REarrangement [mocap] [[project](https://core4d.github.io/)]",
      "arxiv_id": "2406.19353",
      "arxiv_url": "https://arxiv.org/pdf/2406.19353",
      "summary": "Understanding how humans cooperatively rearrange household objects is critical for VR/AR and human-robot interaction. However, in-depth studies on modeling these behaviors are under-researched due to the lack of relevant datasets. We fill this gap by presenting CORE4D, a novel large-scale 4D human-object-human interaction dataset focusing on collaborative object rearrangement, which encompasses diverse compositions of various object geometries, collaboration modes, and 3D scenes. With 1K human-object-human motion sequences captured in the real world, we enrich CORE4D by contributing an iterative collaboration retargeting strategy to augment motions to a variety of novel objects. Leveraging this approach, CORE4D comprises a total of 11K collaboration sequences spanning 3K real and virtual object shapes. Benefiting from extensive motion patterns provided by CORE4D, we benchmark two tasks aiming at generating human-object interaction: human-object motion forecasting and interaction synthesis. Extensive experiments demonstrate the effectiveness of our collaboration retargeting strategy and indicate that CORE4D has posed new challenges to existing human-object interaction generation methodologies.",
      "authors": [],
      "year": "2024",
      "venue": "arxiv",
      "code_url": "https://github.com/leolyliu/CORE4D-Instructions",
      "source_repo": "Awesome Humanoid Manipulation",
      "matched_interests": [
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "human-object interaction"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 2.5,
      "hit_pillars": [
        "5_interaction_reaction"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] Diffusion-based Planning with Learned Viability Filters",
      "arxiv_id": "2502.19564",
      "arxiv_url": "https://arxiv.org/pdf/2502.19564",
      "summary": "Diffusion models can be used as a motion planner by sampling from a distribution of possible futures. However, the samples may not satisfy hard constraints that exist only implicitly in the training data, e.g., avoiding falls or not colliding with a wall. We propose learned viability filters that efficiently predict the future success of any given plan, i.e., diffusion sample, and thereby enforce an implicit future-success constraint. Multiple viability filters can also be composed together. We demonstrate the approach on detailed footstep planning for challenging 3D human locomotion tasks, showing the effectiveness of viability filters in performing online planning and control for box-climbing, step-over walls, and obstacle avoidance. We further show that using viability filters is significantly faster than guidance-based diffusion prediction.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Humanoid Learning",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "locomotion"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2024] [ICRA 24 Best Paper] Open X-Embodiment: Robotic Learning Datasets and RT-X Models",
      "arxiv_id": "2310.08864",
      "arxiv_url": "https://arxiv.org/pdf/2310.08864",
      "summary": "Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train generalist X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. More details can be found on the project website https://robotics-transformer-x.github.io.",
      "authors": [],
      "year": "2024",
      "venue": "ICRA",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    },
    {
      "title": "[2025] RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics",
      "arxiv_id": "2506.04308",
      "arxiv_url": "https://arxiv.org/pdf/2506.04308",
      "summary": "Spatial referring is a fundamental capability of embodied robots to interact with the 3D physical world. However, even with the powerful pretrained vision language models (VLMs), recent approaches are still not qualified to accurately understand the complex 3D scenes and dynamically reason about the instruction-indicated locations for interaction. To this end, we propose RoboRefer, a 3D-aware VLM that can first achieve precise spatial understanding by integrating a disentangled but dedicated depth encoder via supervised fine-tuning (SFT). Moreover, RoboRefer advances generalized multi-step spatial reasoning via reinforcement fine-tuning (RFT), with metric-sensitive process reward functions tailored for spatial referring tasks. To support SFT and RFT training, we introduce RefSpatial, a large-scale dataset of 20M QA pairs (2x prior), covering 31 spatial relations (vs. 15 prior) and supporting complex reasoning processes (up to 5 steps). In addition, we introduce RefSpatial-Bench, a challenging benchmark filling the gap in evaluating spatial referring with multi-step reasoning. Experiments show that SFT-trained RoboRefer achieves state-of-the-art spatial understanding, with an average success rate of 89.6%. RFT-trained RoboRefer further outperforms all other baselines by a large margin, even surpassing Gemini-2.5-Pro by 17.4% in average accuracy on RefSpatial-Bench. Notably, RoboRefer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (e,g., UR5, G1 humanoid) in cluttered real-world scenes. Please see the project page at https://zhoues.github.io/RoboRefer.",
      "authors": [],
      "year": "2025",
      "venue": "arxiv",
      "code_url": "",
      "source_repo": "Awesome Embodied VLA/VA/VLN",
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ],
      "excluded": false,
      "exclusion_keywords": []
    }
  ],
  "sources": [
    "Embodied AI Paper List",
    "Awesome Humanoid Manipulation",
    "Awesome Humanoid Learning",
    "Awesome Embodied VLA/VA/VLN",
    "Awesome Humanoid Robot Learning"
  ]
}