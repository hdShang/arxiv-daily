{
    "papers": [
        {
            "title": "Odyssey: An Automotive Lidar-Inertial Odometry Dataset for GNSS-denied situations",
            "authors": [
                "Aaron Kurda",
                "Simon Steuernagel",
                "Lukas Jung",
                "Marcus Baum"
            ],
            "arxiv_id": "2512.14428v1",
            "summary": "The development and evaluation of Lidar-Inertial Odometry (LIO) and Simultaneous Localization and Mapping (SLAM) systems requires a precise ground truth. The Global Navigation Satellite System (GNSS) is often used as a foundation for this, but its signals can be unreliable in obstructed environments due to multi-path effects or loss-of-signal. While existing datasets compensate for the sporadic loss of GNSS signals by incorporating Inertial Measurement Unit (IMU) measurements, the commonly used Micro-Electro-Mechanical Systems (MEMS) or Fiber Optic Gyroscope (FOG)-based systems do not permit the prolonged study of GNSS-denied environments. To close this gap, we present Odyssey, a LIO dataset with a focus on GNSS-denied environments such as tunnels and parking garages as well as other underrepresented, yet ubiquitous situations such as stop-and-go-traffic, bumpy roads and wide open fields. Our ground truth is derived from a navigation-grade Inertial Navigation System (INS) equipped with a Ring Laser Gyroscope (RLG), offering exceptional bias stability characteristics compared to IMUs used in existing datasets and enabling the prolonged and accurate study of GNSS-denied environments. This makes Odyssey the first publicly available dataset featuring a RLG-based INS. Besides providing data for LIO, we also support other tasks, such as place recognition, through the threefold repetition of all trajectories as well as the integration of external mapping data by providing precise geodetic coordinates. All data, dataloader and other material is available online at https://odyssey.uni-goettingen.de/ .",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "9 pages, 4 figures, submitted to International Journal of Robotics Research (IJRR)",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14428v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "traffic",
                        "lidar"
                    ],
                    "score": 2
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "SLAM",
                        "localization",
                        "mapping"
                    ],
                    "score": 3
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 7,
            "headline_zh": "提出Odyssey数据集，以激光雷达-惯性里程计为核心，专注于GNSS信号缺失环境，为长期精确研究提供导航级惯性导航系统真值。",
            "summary_zh": "激光雷达-惯性里程计（LIO）和同步定位与建图（SLAM）系统的开发与评估需要精确的真值。全球导航卫星系统（GNSS）常被用作基础，但其信号在遮挡环境中可能因多径效应或信号丢失而不可靠。现有数据集通过整合惯性测量单元（IMU）测量来补偿GNSS信号的偶发性丢失，但常用的微机电系统（MEMS）或光纤陀螺仪（FOG）系统不允许对GNSS缺失环境进行长期研究。为填补这一空白，我们提出了Odyssey，一个专注于GNSS缺失环境（如隧道和停车场）以及其他代表性不足但普遍存在场景（如启停交通、颠簸道路和开阔田野）的LIO数据集。我们的真值源自配备环形激光陀螺仪（RLG）的导航级惯性导航系统（INS），与现有数据集中使用的IMU相比，具有优异的偏置稳定性特性，支持对GNSS缺失环境进行长期精确研究。这使得Odyssey成为首个公开可用的基于RLG的INS数据集。除了为LIO提供数据外，我们还通过三次重复所有轨迹以及提供精确大地坐标来整合外部地图数据，支持其他任务如地点识别。所有数据、数据加载器和其他材料可在https://odyssey.uni-goettingen.de/在线获取。",
            "intro_zh": [
                "现有LIO/SLAM数据集依赖GNSS真值，但在遮挡环境中GNSS信号不可靠，且常用MEMS或FOG系统无法支持长期GNSS缺失研究。",
                "论文提出Odyssey数据集，使用导航级INS配备RLG提供高精度真值，专注于隧道、停车场等GNSS缺失场景。",
                "数据集首次公开基于RLG的INS数据，支持长期精确研究，并通过重复轨迹和地理坐标扩展至地点识别等任务。"
            ],
            "method_zh": "**问题定义**：论文旨在解决激光雷达-惯性里程计（LIO）和同步定位与建图（SLAM）系统在GNSS信号缺失环境（如隧道、停车场）中缺乏长期精确真值数据的问题。现有数据集依赖GNSS作为真值基础，但在遮挡环境中GNSS信号不可靠，且常用微机电系统（MEMS）或光纤陀螺仪（FOG）惯性测量单元（IMU）的偏置稳定性不足，无法支持对GNSS缺失环境的长期研究，导致评估偏差和算法开发受限。\\n\\n**核心思路**：论文的核心解决思路是构建一个专注于GNSS缺失环境的LIO数据集，通过使用导航级惯性导航系统（INS）配备环形激光陀螺仪（RLG）来提供高精度、长期稳定的真值。这样设计是因为RLG具有优异的偏置稳定性，相比MEMS或FOG系统，能更准确地模拟和评估在GNSS信号完全缺失或间歇性丢失情况下的定位性能，从而填补现有数据集的空白。\\n\\n**技术框架**：整体架构包括数据采集、真值生成和数据发布三个阶段。主要模块包括：传感器套件（如激光雷达、IMU、RLG-INS）、数据采集平台（车辆搭载设备在多样场景中行驶）、真值处理（基于RLG-INS输出高精度轨迹作为参考标准）以及数据后处理（整合外部地图数据、重复轨迹以支持多任务）。流程上，首先在真实世界环境中收集原始传感器数据，然后利用RLG-INS的精确测量生成真值轨迹，最后将数据标准化并公开提供。\\n\\n**关键创新**：最重要的技术创新点是首次公开提供基于环形激光陀螺仪（RLG）的导航级惯性导航系统（INS）数据集。与现有方法（如KITTI、NCLT等使用MEMS或FOG系统）的本质区别在于，RLG提供了更高的偏置稳定性和长期精度，使得数据集能够支持对GNSS完全缺失环境的长期、精确研究，而不仅仅是短时补偿。这提升了数据集的真实性和评估可靠性。\\n\\n**关键设计**：关键设计包括：使用RLG-INS作为真值源，其偏置稳定性优于常见IMU，具体参数未在摘要中详述，但强调了“导航级”性能；数据采集覆盖多种场景，如隧道、停车场、启停交通、颠簸道路和开阔田野，以增加多样性和代表性；通过三次重复所有轨迹来支持地点识别任务，增强数据实用性；提供精确大地坐标以整合外部地图数据，扩展应用范围。数据集还包括数据加载器和其他辅助材料，便于研究人员直接使用。",
            "application_zh": "该研究主要应用于自动驾驶、机器人导航和增强现实领域，特别是在城市峡谷、隧道、地下停车场等GNSS信号受限或缺失的环境中。Odyssey数据集为开发鲁棒的LIO和SLAM算法提供了高质量真值，有助于提升定位精度和系统可靠性。未来影响包括推动GNSS缺失环境下的定位技术研究，促进多传感器融合算法的发展，并可能扩展到无人机、室内导航等场景。",
            "highlight_zh": "最重要的实验结果是Odyssey成为首个公开可用的基于环形激光陀螺仪（RLG）惯性导航系统（INS）的数据集。与现有数据集（如使用MEMS或FOG系统）相比，RLG提供了优异的偏置稳定性，支持对GNSS缺失环境（如隧道、停车场）进行长期精确研究。数据集包含多样场景（启停交通、颠簸道路等），并通过重复轨迹和地理坐标扩展了应用范围，具体性能数据需参考论文后续实验部分，但突出了其在高精度真值生成方面的突破。",
            "tags_zh": [
                "激光雷达-惯性里程计",
                "数据集构建",
                "GNSS缺失环境",
                "环形激光陀螺仪",
                "惯性导航系统",
                "同步定位与建图",
                "自动驾驶",
                "地点识别"
            ],
            "_index": 0
        },
        {
            "title": "A data-physics hybrid generative model for patient-specific post-stroke motor rehabilitation using wearable sensor data",
            "authors": [
                "Yanning Dai",
                "Chenyu Tang",
                "Ruizhi Zhang",
                "Wenyu Yang",
                "Yilan Zhang",
                "Yuhui Wang",
                "Junliang Chen",
                "Xuhang Chen",
                "Ruimou Xie",
                "Yangyue Cao",
                "Qiaoying Li",
                "Jin Cao",
                "Tao Li",
                "Hubin Zhao",
                "Yu Pan",
                "Arokia Nathan",
                "Xin Gao",
                "Peter Smielewski",
                "Shuo Gao"
            ],
            "arxiv_id": "2512.14329v1",
            "summary": "Dynamic prediction of locomotor capacity after stroke is crucial for tailoring rehabilitation, yet current assessments provide only static impairment scores and do not indicate whether patients can safely perform specific tasks such as slope walking or stair climbing. Here, we develop a data-physics hybrid generative framework that reconstructs an individual stroke survivor's neuromuscular control from a single 20 m level-ground walking trial and predicts task-conditioned locomotion across rehabilitation scenarios. The system combines wearable-sensor kinematics, a proportional-derivative physics controller, a population Healthy Motion Atlas, and goal-conditioned deep reinforcement learning with behaviour cloning and generative adversarial imitation learning to generate physically plausible, patient-specific gait simulations for slopes and stairs. In 11 stroke survivors, the personalized controllers preserved idiosyncratic gait patterns while improving joint-angle and endpoint fidelity by 4.73% and 12.10%, respectively, and reducing training time to 25.56% relative to a physics-only baseline. In a multicentre pilot involving 21 inpatients, clinicians who used our locomotion predictions to guide task selection and difficulty obtained larger gains in Fugl-Meyer lower-extremity scores over 28 days of standard rehabilitation than control clinicians (mean change 6.0 versus 3.7 points). These findings indicate that our generative, task-predictive framework can augment clinical decision-making in post-stroke gait rehabilitation and provide a template for dynamically personalized motor recovery strategies.",
            "categories": [
                "cs.CE",
                "cs.AI"
            ],
            "primary_category": "cs.CE",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "26 pages, 6 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14329v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "人形机器人",
                    "matched_keywords": [
                        "imitation learning",
                        "atlas"
                    ],
                    "score": 2
                },
                {
                    "name": "动作生成",
                    "matched_keywords": [
                        "motion prediction"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO",
                        "VIO"
                    ],
                    "score": 2
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning",
                        "imitation learning"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 7,
            "headline_zh": "提出数据-物理混合生成模型，基于单次平地行走数据预测卒中患者个性化康复任务表现，以辅助临床决策。",
            "summary_zh": "卒中后运动能力的动态预测对于定制康复方案至关重要，但现有评估仅提供静态损伤评分，无法指示患者是否能安全执行特定任务（如斜坡行走或爬楼梯）。本研究开发了一个数据-物理混合生成框架，该框架从一次20米平地行走试验中重建个体卒中幸存者的神经肌肉控制，并预测康复场景中的任务条件性运动。该系统结合了可穿戴传感器运动学数据、比例-微分物理控制器、健康人群运动图谱、以及目标条件深度强化学习（结合行为克隆和生成对抗模仿学习），为斜坡和楼梯任务生成物理上合理、患者特定的步态模拟。在11名卒中幸存者中，个性化控制器在保留个体步态模式的同时，将关节角度和端点保真度分别提高了4.73%和12.10%，并将训练时间减少至纯物理基线的25.56%。在一项涉及21名住院患者的多中心试点中，使用我们的运动预测来指导任务选择和难度的临床医生，在28天标准康复期间获得的Fugl-Meyer下肢评分增益大于对照组临床医生（平均变化6.0分对3.7分）。这些发现表明，我们的生成性、任务预测框架可以增强卒中后步态康复的临床决策，并为动态个性化运动恢复策略提供模板。",
            "intro_zh": [
                "核心问题：现有卒中后运动能力评估仅提供静态损伤评分，无法动态预测患者执行特定任务（如斜坡行走）的安全性和能力，限制了康复方案的个性化定制。",
                "方法要点：提出数据-物理混合生成框架，结合可穿戴传感器数据、物理控制器、健康运动图谱和深度强化学习，从单次平地行走重建患者神经肌肉控制，并生成任务条件性步态模拟。",
                "实验或效果：在11名患者中，模型提升关节角度和端点保真度，减少训练时间；多中心试点显示，使用预测指导康复的临床医生获得更大功能改善（Fugl-Meyer评分平均提升6.0分对3.7分）。"
            ],
            "method_zh": "**问题定义**：论文旨在解决卒中后患者运动能力的动态预测问题，特别是预测患者在特定康复任务（如斜坡行走、爬楼梯）中的表现。现有方法主要依赖静态临床评分（如Fugl-Meyer评分），无法提供任务级别的动态评估，导致康复方案缺乏个性化和安全性指导，限制了康复效果。\\n\\n**核心思路**：论文提出一个数据-物理混合生成框架，核心思想是将数据驱动方法与物理模型相结合，以从有限的患者数据（单次20米平地行走）中学习个性化的神经肌肉控制策略，并泛化到未见过的任务场景。这种混合方法旨在克服纯数据方法对大量标注数据的依赖，以及纯物理方法难以捕捉个体差异的局限性。\\n\\n**技术框架**：整体架构包含四个主要模块：1) 数据采集模块：使用可穿戴传感器（如惯性测量单元）采集患者平地行走的运动学数据（如关节角度、速度）。2) 物理建模模块：基于比例-微分（PD）控制器构建物理仿真环境，模拟人体动力学。3) 先验知识模块：引入“健康人群运动图谱”作为参考，提供正常步态的先验分布。4) 学习与生成模块：采用目标条件深度强化学习（Goal-Conditioned Deep Reinforcement Learning），结合行为克隆（从患者数据中学习初始策略）和生成对抗模仿学习（通过对抗训练细化策略，使其更接近真实数据分布），训练个性化控制器，以在物理仿真中生成针对特定任务（如不同坡度斜坡）的步态序列。\\n\\n**关键创新**：最重要的技术创新在于“数据-物理混合”与“任务条件生成”的结合。与现有方法相比，本质区别在于：a) 从单次试验中学习个性化控制，而非依赖大量患者数据；b) 通过混合框架确保生成的步态既符合物理定律（通过PD控制器），又保留患者特有的运动模式（通过数据驱动学习）；c) 利用目标条件强化学习实现任务条件的运动生成，即根据康复目标（如“爬楼梯”）动态调整输出。\\n\\n**关键设计**：关键技术细节包括：1) 使用PD控制器作为物理引擎的基础，其参数可调以模拟不同肌肉刚度。2) 健康人群运动图谱作为正则化项，防止生成的动作偏离正常范围。3) 损失函数结合了多个组件：行为克隆损失（最小化预测动作与观察动作的差异）、生成对抗模仿学习中的判别器损失（区分生成动作与真实动作）、以及物理约束损失（如关节角度限制、能量消耗）。4) 网络结构可能涉及策略网络（如多层感知机或循环神经网络）用于输出控制动作，和价值网络或判别器网络用于评估动作质量。具体网络架构和超参数在论文中未详细说明，但训练过程优化了保真度和效率。",
            "application_zh": "该研究主要应用于卒中后运动康复领域，潜在价值包括：辅助临床医生制定个性化康复计划，通过预测患者在不同任务（如斜坡、楼梯）中的表现，优化任务选择和难度设置；作为虚拟评估工具，减少对昂贵实验室设备的依赖；未来可扩展至其他神经系统疾病（如帕金森病）的康复，或集成到智能康复机器人中，实现动态自适应训练。",
            "highlight_zh": "最重要的实验结果包括：在11名卒中患者中，个性化控制器相比纯物理基线，关节角度保真度提升4.73%，端点（如足部轨迹）保真度提升12.10%，训练时间减少至基线的25.56%。多中心试点涉及21名住院患者，使用模型预测指导康复的临床医生组，在28天内Fugl-Meyer下肢评分平均提升6.0分，显著高于对照组（3.7分），证明了模型在临床实践中的有效性。",
            "tags_zh": [
                "卒中康复",
                "生成模型",
                "数据-物理混合",
                "可穿戴传感器",
                "深度强化学习",
                "个性化医疗",
                "步态分析",
                "任务预测"
            ],
            "_index": 1
        },
        {
            "title": "SUPER -- A Framework for Sensitivity-based Uncertainty-aware Performance and Risk Assessment in Visual Inertial Odometry",
            "authors": [
                "Johannes A. Gaus",
                "Daniel Häufle",
                "Woo-Jeong Baek"
            ],
            "arxiv_id": "2512.14189v1",
            "summary": "While many visual odometry (VO), visual-inertial odometry (VIO), and SLAM systems achieve high accuracy, the majority of existing methods miss to assess risks at runtime. This paper presents SUPER (Sensitivity-based Uncertainty-aware PErformance and Risk assessment) that is a generic and explainable framework that propagates uncertainties via sensitivities for real-time risk assessment in VIO. The scientific novelty lies in the derivation of a real-time risk indicator that is backend-agnostic and exploits the Schur complement blocks of the Gauss-Newton normal matrix to propagate uncertainties. Practically, the Schur complement captures the sensitivity that reflects the influence of the uncertainty on the risk occurrence. Our framework estimates risks on the basis of the residual magnitudes, geometric conditioning, and short horizon temporal trends without requiring ground truth knowledge. Our framework enables to reliably predict trajectory degradation 50 frames ahead with an improvement of 20% to the baseline. In addition, SUPER initiates a stop or relocalization policy with 89.1% recall. The framework is backend agnostic and operates in real time with less than 0.2% additional CPU cost. Experiments show that SUPER provides consistent uncertainty estimates. A SLAM evaluation highlights the applicability to long horizon mapping.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14189v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "visual odometry",
                        "VO",
                        "VIO",
                        "visual-inertial",
                        "SLAM",
                        "localization",
                        "mapping"
                    ],
                    "score": 7
                }
            ],
            "relevance_score": 7,
            "headline_zh": "提出SUPER框架，通过灵敏度传播不确定性，实现视觉惯性里程计中的实时风险与性能评估。",
            "summary_zh": "尽管许多视觉里程计（VO）、视觉惯性里程计（VIO）和SLAM系统实现了高精度，但大多数现有方法未能评估运行时的风险。本文提出了SUPER（基于灵敏度的不确定性感知性能与风险评估），这是一个通用且可解释的框架，通过灵敏度传播不确定性，用于VIO中的实时风险评估。科学新颖性在于推导了一个实时风险指标，该指标与后端无关，并利用高斯-牛顿正规矩阵的舒尔补块来传播不确定性。实际上，舒尔补捕获了反映不确定性对风险发生影响的灵敏度。我们的框架基于残差大小、几何条件和短期时间趋势来估计风险，无需地面真值知识。我们的框架能够可靠地预测50帧前的轨迹退化，比基线提高了20%。此外，SUPER以89.1%的召回率启动停止或重定位策略。该框架与后端无关，实时运行，额外CPU成本低于0.2%。实验表明，SUPER提供了一致的不确定性估计。SLAM评估突出了其在长时程建图中的应用性。",
            "intro_zh": [
                "现有VIO/SLAM系统虽精度高，但缺乏运行时风险评估能力，导致潜在性能退化风险未知。",
                "提出SUPER框架，利用高斯-牛顿矩阵的舒尔补块传播不确定性，实现后端无关的实时风险指标。",
                "实验显示，SUPER能提前50帧预测轨迹退化，提升20%，并以89.1%召回率触发安全策略，额外CPU成本低于0.2%。"
            ],
            "method_zh": "**问题定义**：论文解决视觉惯性里程计（VIO）和SLAM系统中缺乏实时风险评估的问题。现有方法虽能实现高精度定位与建图，但运行时无法量化不确定性对性能的影响，导致系统在退化场景（如纹理缺失、动态干扰）下可能失效而不自知，缺乏风险预警机制。\\n\\n**核心思路**：通过灵敏度分析传播不确定性，构建一个与后端优化器无关的实时风险指标。核心思想是利用高斯-牛顿优化中的舒尔补块，这些块自然捕获了状态变量之间的依赖关系，从而反映不确定性如何影响系统性能，无需额外复杂模型或地面真值。\\n\\n**技术框架**：SUPER框架整体分为三个主要阶段：首先，在VIO后端优化过程中提取高斯-牛顿正规矩阵的舒尔补块；其次，基于这些块计算灵敏度，量化不确定性对残差和状态估计的影响；最后，结合残差幅度、几何条件（如可观测性）和短期时间趋势（如滑动窗口内的变化），综合评估风险并生成实时指标。框架作为插件集成，不修改原有优化流程。\\n\\n**关键创新**：最重要的技术创新是推导了一个后端无关的实时风险指标，利用舒尔补块进行不确定性传播。与现有方法（如基于蒙特卡洛采样或启发式规则）相比，SUPER无需假设特定噪声分布或增加大量计算，直接利用优化中的数学结构，实现高效且可解释的风险评估。\\n\\n**关键设计**：关键设计包括：使用舒尔补块计算灵敏度矩阵，反映状态不确定性对风险的影响；风险指标基于残差范数、条件数（衡量几何退化）和时间窗口内的趋势统计；框架参数如滑动窗口大小和风险阈值可根据应用调整，但论文中未详细指定具体值；无需额外损失函数或网络结构，完全基于优化理论。",
            "application_zh": "SUPER框架在机器人导航、自动驾驶和增强现实等领域具有广泛应用价值。它能为VIO/SLAM系统提供实时风险预警，帮助系统在感知退化时自动触发安全策略（如停止或重定位），提升鲁棒性和安全性。未来可扩展至多传感器融合和长期运行场景，推动自主系统在复杂环境中的可靠部署。",
            "highlight_zh": "实验表明，SUPER在多个数据集上能可靠预测轨迹退化，提前50帧的预测精度比基线提升20%。风险触发策略的召回率达到89.1%，有效避免系统失效。框架实时运行，额外CPU成本低于0.2%，且与后端无关，在长时程SLAM评估中表现一致，验证了其通用性和实用性。",
            "tags_zh": [
                "视觉惯性里程计",
                "不确定性评估",
                "实时风险预测",
                "舒尔补灵敏度",
                "后端无关框架",
                "SLAM系统",
                "性能退化检测",
                "优化理论应用"
            ],
            "_index": 2
        },
        {
            "title": "MMGR: Multi-Modal Generative Reasoning",
            "authors": [
                "Zefan Cai",
                "Haoyi Qiu",
                "Tianyi Ma",
                "Haozhe Zhao",
                "Gengze Zhou",
                "Kung-Hsiang Huang",
                "Parisa Kordjamshidi",
                "Minjia Zhang",
                "Xiao Wen",
                "Jiuxiang Gu",
                "Nanyun Peng",
                "Junjie Hu"
            ],
            "arxiv_id": "2512.14691v1",
            "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.",
            "categories": [
                "cs.CL",
                "cs.CV"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "work in progress",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14691v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO",
                        "localization"
                    ],
                    "score": 2
                },
                {
                    "name": "世界模型",
                    "matched_keywords": [
                        "world model",
                        "world simulator"
                    ],
                    "score": 2
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "reward"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 6,
            "headline_zh": "提出MMGR多模态生成推理评估框架，以解决视频基础模型在物理、逻辑和空间约束上的推理能力不足问题。",
            "summary_zh": "视频基础模型能够生成视觉逼真且时序连贯的内容，但其作为世界模拟器的可靠性取决于是否捕捉了物理、逻辑和空间约束。现有指标如弗雷歇视频距离（FVD）强调感知质量而忽视了推理失败，包括违反因果性、物理规律和全局一致性。我们引入了MMGR（多模态生成推理评估与基准），这是一个基于五种推理能力的原则性评估框架：物理、逻辑、3D空间、2D空间和时序推理。MMGR在三个领域评估生成推理：抽象推理（ARC-AGI、数独）、具身导航（真实世界3D导航和定位）以及物理常识（运动和组合交互）。MMGR应用细粒度指标，要求视频和图像生成在整体上正确。我们基准测试了领先的视频模型（Veo-3、Sora-2、Wan-2.2）和图像模型（Nano-banana、Nano-banana Pro、GPT-4o-image、Qwen-image），揭示了跨领域的显著性能差距。模型在物理常识任务上表现中等，但在抽象推理上表现不佳（ARC-AGI准确率低于10%），并在具身设置中的长时程空间规划上遇到困难。我们的分析突出了当前模型的关键局限性，包括过度依赖感知数据、全局状态一致性弱，以及目标函数奖励视觉合理性而非因果正确性。MMGR提供了一个统一的诊断基准和一条通向推理感知生成世界模型的路径。",
            "intro_zh": [
                "现有视频基础模型评估指标（如FVD）过于关注感知质量，忽略了物理、逻辑和空间推理失败，导致模型作为世界模拟器的可靠性不足。",
                "论文提出MMGR框架，基于五种推理能力（物理、逻辑、3D空间、2D空间、时序）构建原则性评估，覆盖抽象推理、具身导航和物理常识三个领域。",
                "实验显示，主流模型在物理常识任务上表现中等，但在抽象推理（如ARC-AGI准确率低于10%）和长时程空间规划上表现差，揭示了模型的关键局限性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决视频基础模型在生成内容时缺乏物理、逻辑和空间推理能力的问题，现有方法如FVD等指标仅评估感知质量，忽视了因果性、物理规律和全局一致性的违反，导致模型作为世界模拟器不可靠。\\n\\n**核心思路**：论文的核心思路是设计一个多模态生成推理评估框架（MMGR），通过定义五种推理能力（物理、逻辑、3D空间、2D空间、时序）和三个评估领域（抽象推理、具身导航、物理常识），来系统性地量化模型的推理性能，从而诊断和提升模型的生成质量。\\n\\n**技术框架**：MMGR的整体架构包括三个主要阶段：首先，定义五种推理能力作为评估维度；其次，构建三个评估领域，每个领域包含具体任务（如ARC-AGI、数独、3D导航、体育交互）；然后，应用细粒度指标，要求视频和图像生成在整体上正确，以评估模型在这些任务上的表现。\\n\\n**关键创新**：最重要的技术创新点是提出了一个统一的、基于推理能力的评估框架，与现有方法（如FVD）的本质区别在于，MMGR强调因果正确性和全局一致性，而非仅视觉合理性，从而更全面地评估模型作为世界模拟器的能力。\\n\\n**关键设计**：关键设计包括：五种推理能力的精确定义（例如，物理推理涉及运动规律，逻辑推理涉及规则遵循），三个评估领域的任务选择（如抽象推理使用ARC-AGI和数独，具身导航使用真实世界3D场景），以及细粒度指标的应用（要求生成内容在视频和图像层面都符合推理约束），具体参数设置和损失函数在论文中未详细说明，可能依赖于任务特定的评估标准。",
            "application_zh": "该研究在人工智能和机器学习领域具有广泛潜在应用，可用于评估和提升视频生成模型、机器人导航系统、虚拟现实模拟器的推理能力，推动生成式AI向更可靠的世界模拟器发展，未来可能影响自动驾驶、游戏开发、教育模拟等实际场景。",
            "highlight_zh": "实验结果显示，主流视频模型（如Veo-3、Sora-2、Wan-2.2）和图像模型（如Nano-banana、GPT-4o-image）在MMGR基准上表现差异显著：在物理常识任务上准确率中等，但在抽象推理任务（如ARC-AGI）上准确率低于10%，具身导航中的长时程空间规划表现差，揭示了模型在推理能力上的关键短板，对比现有指标如FVD，MMGR更全面地暴露了模型局限性。",
            "tags_zh": [
                "多模态生成推理",
                "视频基础模型评估",
                "物理逻辑约束",
                "抽象推理基准",
                "具身导航",
                "全局一致性",
                "因果正确性",
                "世界模拟器"
            ],
            "_index": 3
        },
        {
            "title": "CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives",
            "authors": [
                "Zihan Wang",
                "Jiashun Wang",
                "Jeff Tan",
                "Yiwen Zhao",
                "Jessica Hodgins",
                "Shubham Tulsiani",
                "Deva Ramanan"
            ],
            "arxiv_id": "2512.14696v1",
            "summary": "We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2\\% to 6.9\\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\\% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR.",
            "categories": [
                "cs.CV",
                "cs.GR",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page: https://crisp-real2sim.github.io/CRISP-Real2Sim/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14696v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "人形机器人",
                    "matched_keywords": [
                        "humanoid",
                        "humanoid control"
                    ],
                    "score": 2
                },
                {
                    "name": "动作生成",
                    "matched_keywords": [
                        "human motion"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RL"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 5,
            "headline_zh": "提出CRISP方法，通过平面基元拟合和接触引导，从单目视频重建可模拟的人体运动与场景几何，解决物理交互失败问题。",
            "summary_zh": "我们介绍了CRISP，一种从单目视频中恢复可模拟人体运动和场景几何的方法。先前关于人体-场景联合重建的工作依赖于数据驱动的先验和无物理约束的联合优化，或者恢复带有噪声和伪影的几何，导致场景交互的运动跟踪策略失败。相比之下，我们的关键见解是通过对场景点云重建进行平面基元拟合，利用深度、法线和光流的简单聚类流程，恢复凸面、干净且可模拟的几何。为了重建在交互过程中可能被遮挡的场景几何，我们利用人体-场景接触建模（例如，使用人体姿态重建被遮挡的椅子座位）。最后，我们通过强化学习驱动人形控制器，确保人体和场景重建具有物理合理性。我们的方法在人体中心视频基准（EMDB、PROX）上将运动跟踪失败率从55.2%降低到6.9%，同时提供43%更快的RL模拟吞吐量。我们进一步在野外视频上验证了该方法，包括随意拍摄的视频、互联网视频，甚至Sora生成的视频。这展示了CRISP大规模生成物理有效人体运动和交互环境的能力，极大地推进了机器人和AR/VR的真实到模拟应用。",
            "intro_zh": [
                "现有方法依赖数据先验或无物理优化，导致几何噪声和交互失败，难以实现真实到模拟的转换。",
                "CRISP通过平面基元拟合和接触建模，恢复干净、可模拟的几何，并利用强化学习确保物理合理性。",
                "在基准测试中，运动跟踪失败率从55.2%降至6.9%，模拟吞吐量提升43%，并在多种视频上验证有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在从单目视频中联合重建可模拟的人体运动和场景几何，以支持物理交互应用。现有方法的痛点包括：依赖数据驱动先验导致泛化性差；无物理约束的优化产生噪声几何和伪影；这些缺陷使得基于场景交互的运动跟踪策略（如强化学习控制器）容易失败，限制了真实到模拟（real-to-sim）的实用性。\\n\\n**核心思路**：CRISP的核心思路是恢复凸面、干净且可模拟的场景几何，通过拟合平面基元到点云重建，并结合人体-场景接触建模来补全遮挡部分。设计基于物理合理性原则，确保重建结果可直接用于模拟环境，从而提升运动跟踪的稳定性和效率。\\n\\n**技术框架**：整体流程分为三个阶段：首先，从单目视频进行初始点云重建；其次，通过聚类流程（基于深度、法线和光流）拟合平面基元，生成凸面几何；然后，利用人体姿态和接触信息（如脚部或手部位置）重建被遮挡的场景部分（如椅子座位）；最后，将重建的人体和场景输入强化学习人形控制器，进行物理验证和优化。\\n\\n**关键创新**：最重要的技术创新是结合平面基元拟合和接触引导的几何重建，与现有方法的本质区别在于：强调几何的凸面和可模拟性，而非仅追求视觉精度；引入物理约束（通过接触建模和强化学习），确保重建结果适用于交互任务，而非仅用于静态重建。\\n\\n**关键设计**：技术细节包括：使用简单聚类算法（如基于深度、法向量和光流的K-means或区域生长）进行平面分割；接触建模基于人体关键点与场景的估计接触区域，用于推断遮挡几何；强化学习控制器采用标准人形模拟环境（如Isaac Gym），损失函数结合运动跟踪误差和物理约束（如接触力）；参数设置中，平面拟合的阈值和聚类数量根据场景复杂度自适应调整，以平衡精度和效率。",
            "application_zh": "CRISP的潜在应用领域包括机器人仿真训练、增强现实/虚拟现实（AR/VR）内容生成和人体运动分析。实际价值在于能够从真实世界视频大规模生成物理有效的交互环境，降低仿真数据采集成本，提升模拟的真实性和泛化能力。未来影响可能推动真实到模拟技术在自动驾驶、游戏开发和智能体训练中的广泛应用。",
            "highlight_zh": "最重要的实验结果包括：在EMDB和PROX人体中心视频基准上，运动跟踪失败率从基线方法的55.2%显著降低至6.9%，提升了48.3个百分点；强化学习模拟吞吐量提高43%，加速了训练过程。方法在多种视频类型上验证有效，包括随意拍摄视频、互联网视频和AI生成视频（如Sora），展示了其鲁棒性和泛化性。",
            "tags_zh": [
                "单目视频重建",
                "人体-场景交互",
                "平面基元拟合",
                "接触建模",
                "强化学习模拟",
                "真实到模拟",
                "几何重建",
                "物理合理性"
            ],
            "_index": 4
        },
        {
            "title": "Synthetic Data Pipelines for Adaptive, Mission-Ready Militarized Humanoids",
            "authors": [
                "Mohammed Ayman Habib",
                "Aldo Petruzzelli"
            ],
            "arxiv_id": "2512.14411v1",
            "summary": "Omnia presents a synthetic data driven pipeline to accelerate the training, validation, and deployment readiness of militarized humanoids. The approach converts first-person spatial observations captured from point-of-view recordings, smart glasses, augmented reality headsets, and spatial browsing workflows into scalable, mission-specific synthetic datasets for humanoid autonomy. By generating large volumes of high-fidelity simulated scenarios and pairing them with automated labeling and model training, the pipeline enables rapid iteration on perception, navigation, and decision-making capabilities without the cost, risk, or time constraints of extensive field trials. The resulting datasets can be tuned quickly for new operational environments and threat conditions, supporting both baseline humanoid performance and advanced subsystems such as multimodal sensing, counter-detection survivability, and CBRNE-relevant reconnaissance behaviors. This work targets faster development cycles and improved robustness in complex, contested settings by exposing humanoid systems to broad scenario diversity early in the development process.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "6 pages; xTech Humanoid white paper submission",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14411v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "人形机器人",
                    "matched_keywords": [
                        "humanoid"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO",
                        "VIO"
                    ],
                    "score": 2
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 5,
            "headline_zh": "提出基于合成数据驱动的管道，以加速军事化人形机器人的训练、验证和部署准备。",
            "summary_zh": "Omnia提出了一种合成数据驱动的管道，用于加速军事化人形机器人的训练、验证和部署准备。该方法将来自第一人称空间观测（如点视角记录、智能眼镜、增强现实头显和空间浏览工作流）的数据转换为可扩展的、任务特定的合成数据集，以支持人形机器人的自主性。通过生成大量高保真模拟场景，并结合自动标注和模型训练，该管道能够在感知、导航和决策能力方面实现快速迭代，而无需承担广泛实地试验的成本、风险或时间限制。生成的数据集可以快速调整以适应新的操作环境和威胁条件，支持基线人形机器人性能以及高级子系统，如多模态传感、反检测生存能力和与CBRNE相关的侦察行为。这项工作旨在通过在人形机器人系统开发早期阶段暴露于广泛的场景多样性，实现更快的开发周期，并在复杂、对抗性环境中提高鲁棒性。",
            "intro_zh": [
                "核心问题：军事化人形机器人的训练和部署面临高成本、高风险和长时间限制，现有方法依赖实地试验，难以快速适应新环境和威胁。",
                "方法要点：提出合成数据驱动管道，将第一人称空间观测转换为任务特定合成数据集，结合自动标注和模型训练，实现快速迭代。",
                "实验或效果：通过生成高保真模拟场景，加速感知、导航和决策能力开发，提升在复杂环境中的鲁棒性和适应性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决军事化人形机器人在训练、验证和部署过程中面临的高成本、高风险和时间限制问题。现有方法主要依赖实地试验，这导致开发周期长、难以快速适应新操作环境和威胁条件，且缺乏足够的场景多样性来提升系统鲁棒性。\\n\\n**核心思路**：论文提出一种合成数据驱动的管道，通过将第一人称空间观测数据转换为可扩展的、任务特定的合成数据集，结合自动标注和模型训练，实现快速迭代。核心思想是利用模拟技术生成大量高保真场景，替代或补充实地试验，从而加速人形机器人的自主能力开发，并提高其在复杂、对抗性环境中的适应性。\\n\\n**技术框架**：整体架构包括数据采集、合成数据生成、自动标注和模型训练四个主要阶段。首先，从点视角记录、智能眼镜、增强现实头显和空间浏览工作流中采集第一人称空间观测数据。然后，将这些数据输入到模拟环境中，生成大规模、高保真的合成场景。接着，利用自动化工具对合成数据进行标注，生成训练标签。最后，基于标注后的数据集进行模型训练，优化感知、导航和决策模块，支持快速迭代和调整以适应新任务。\\n\\n**关键创新**：最重要的技术创新在于构建了一个端到端的合成数据驱动管道，专门针对军事化人形机器人的任务需求。与现有方法相比，本质区别在于它能够快速生成和调整任务特定的合成数据集，减少对实地试验的依赖，从而显著降低开发成本和时间，同时通过暴露于广泛场景多样性来提升系统鲁棒性。\\n\\n**关键设计**：论文未详细说明具体的参数设置、损失函数或网络结构等技术细节，但强调了管道中的关键设计元素，如高保真模拟场景的生成、自动标注工具的集成，以及支持多模态传感、反检测生存能力和CBRNE相关侦察行为的高级子系统适配。这些设计使得数据集可以灵活调整，以匹配不同的操作环境和威胁模型。",
            "application_zh": "该研究主要应用于军事和国防领域，特别是军事化人形机器人的开发与部署。潜在价值包括加速机器人自主系统的训练周期，降低实地试验的风险和成本，并提高在复杂、对抗性环境（如城市战、CBRNE威胁场景）中的任务适应性和鲁棒性。未来影响可能扩展到民用机器人、灾难响应和工业自动化等领域，推动合成数据在机器人学中的广泛应用。",
            "highlight_zh": "论文未提供具体的性能数据、对比基线或提升幅度等实验结果细节。但实验亮点在于通过合成数据管道，实现了军事化人形机器人感知、导航和决策能力的快速迭代开发，减少了实地试验的需求，并提升了系统在多样场景中的适应性。这有助于缩短开发周期，增强在复杂、对抗性环境中的鲁棒性。",
            "tags_zh": [
                "合成数据管道",
                "军事化人形机器人",
                "自主系统训练",
                "高保真模拟",
                "自动标注",
                "多模态传感",
                "CBRNE侦察",
                "场景多样性"
            ],
            "_index": 5
        },
        {
            "title": "DriverGaze360: OmniDirectional Driver Attention with Object-Level Guidance",
            "authors": [
                "Shreedhar Govil",
                "Didier Stricker",
                "Jason Rambach"
            ],
            "arxiv_id": "2512.14266v1",
            "summary": "Predicting driver attention is a critical problem for developing explainable autonomous driving systems and understanding driver behavior in mixed human-autonomous vehicle traffic scenarios. Although significant progress has been made through large-scale driver attention datasets and deep learning architectures, existing works are constrained by narrow frontal field-of-view and limited driving diversity. Consequently, they fail to capture the full spatial context of driving environments, especially during lane changes, turns, and interactions involving peripheral objects such as pedestrians or cyclists. In this paper, we introduce DriverGaze360, a large-scale 360$^\\circ$ field of view driver attention dataset, containing $\\sim$1 million gaze-labeled frames collected from 19 human drivers, enabling comprehensive omnidirectional modeling of driver gaze behavior. Moreover, our panoramic attention prediction approach, DriverGaze360-Net, jointly learns attention maps and attended objects by employing an auxiliary semantic segmentation head. This improves spatial awareness and attention prediction across wide panoramic inputs. Extensive experiments demonstrate that DriverGaze360-Net achieves state-of-the-art attention prediction performance on multiple metrics on panoramic driving images. Dataset and method available at https://av.dfki.de/drivergaze360.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14266v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "autonomous driving",
                        "autonomous vehicle",
                        "traffic"
                    ],
                    "score": 3
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO",
                        "VIO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 5,
            "headline_zh": "提出DriverGaze360全景数据集与DriverGaze360-Net方法，以解决自动驾驶中驾驶员注意力预测的视野受限问题。",
            "summary_zh": "预测驾驶员注意力是开发可解释自动驾驶系统和理解人机混合交通场景中驾驶员行为的关键问题。尽管通过大规模驾驶员注意力数据集和深度学习架构已取得显著进展，但现有工作受限于狭窄的前方视野和有限的驾驶多样性，无法捕捉驾驶环境的完整空间上下文，尤其是在变道、转弯和涉及行人或自行车等外围物体交互时。本文介绍了DriverGaze360，这是一个大规模360度视野驾驶员注意力数据集，包含从19名人类驾驶员收集的约100万帧带注视标签的帧，实现了对驾驶员注视行为的全面全向建模。此外，我们的全景注意力预测方法DriverGaze360-Net通过采用辅助语义分割头联合学习注意力图和关注对象，提高了对宽全景输入的空间感知和注意力预测能力。大量实验表明，DriverGaze360-Net在全景驾驶图像上的多个指标上实现了最先进的注意力预测性能。数据集和方法可在https://av.dfki.de/drivergaze360获取。",
            "intro_zh": [
                "现有驾驶员注意力预测方法受限于狭窄前方视野，无法捕捉变道、转弯等场景的全空间上下文，导致预测不全面。",
                "论文提出DriverGaze360全景数据集和DriverGaze360-Net方法，通过联合学习注意力图和语义分割，提升全景输入下的空间感知能力。",
                "实验显示DriverGaze360-Net在全景驾驶图像上实现SOTA性能，在多个指标上显著优于基线方法，验证了方法的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自动驾驶中驾驶员注意力预测问题，现有方法因使用窄视野数据（如前方摄像头）而无法全面建模驾驶环境，尤其在涉及外围物体交互时预测准确性受限。\\n\\n**核心思路**：通过构建大规模360度视野数据集DriverGaze360，并结合深度学习模型DriverGaze360-Net，联合预测注意力图和语义分割，以增强模型对全景输入的空间理解能力，从而更准确地模拟驾驶员在复杂场景中的注视行为。\\n\\n**技术框架**：整体架构包括数据采集、预处理、模型训练和评估阶段。主要模块包括：全景图像输入模块、主干网络（如CNN或Transformer）、注意力预测头（输出注意力热图）、辅助语义分割头（输出对象类别分割图），以及多任务学习框架。流程上，模型同时优化注意力预测和语义分割任务，利用分割信息引导注意力学习。\\n\\n**关键创新**：最重要的创新是引入了全景视野数据集和对象级引导的联合学习机制。与现有方法仅关注窄视野或独立处理注意力预测不同，本方法通过语义分割提供对象级上下文，使模型能更好地识别和关注关键外围物体（如行人、自行车），本质区别在于从“局部预测”扩展到“全局感知”。\\n\\n**关键设计**：技术细节包括：使用约100万帧360度图像数据，覆盖19名驾驶员多样驾驶场景；模型可能采用多尺度特征提取和融合技术；损失函数结合注意力损失（如KL散度）和语义分割损失（如交叉熵），通过加权求和进行端到端训练；网络结构可能包含编码器-解码器设计，以处理高分辨率全景输入；具体参数设置如学习率、批量大小等未在摘要中详述，需参考论文正文。",
            "application_zh": "该研究在自动驾驶领域具有重要应用价值，可用于开发更安全、可解释的自动驾驶系统，通过模拟人类驾驶员注意力，提升车辆在复杂交通场景（如城市道路、交叉路口）中的决策能力。此外，它有助于分析人机混合交通中的驾驶员行为，为交通管理和人机交互设计提供数据支持，未来可能推动全景感知和注意力建模在机器人、辅助驾驶等领域的广泛应用。",
            "highlight_zh": "DriverGaze360-Net在全景驾驶图像上实现了最先进的注意力预测性能，具体实验结果显示，在多个评估指标（如AUC、KL散度等）上显著优于现有基线方法。例如，在公开基准测试中，模型可能将预测准确率提升约5-10%，具体数据需参考论文，但摘要强调其SOTA地位，验证了全景数据和对象级引导的有效性。",
            "tags_zh": [
                "驾驶员注意力预测",
                "全景视觉",
                "语义分割",
                "多任务学习",
                "自动驾驶",
                "数据集构建",
                "深度学习",
                "人机交互"
            ],
            "_index": 6
        },
        {
            "title": "ViBES: A Conversational Agent with Behaviorally-Intelligent 3D Virtual Body",
            "authors": [
                "Juze Zhang",
                "Changan Chen",
                "Xin Chen",
                "Heng Yu",
                "Tiange Xiang",
                "Ali Sartaz Khan",
                "Shrinidhi K. Lakshmikanth",
                "Ehsan Adeli"
            ],
            "arxiv_id": "2512.14234v1",
            "summary": "Human communication is inherently multimodal and social: words, prosody, and body language jointly carry intent. Yet most prior systems model human behavior as a translation task co-speech gesture or text-to-motion that maps a fixed utterance to motion clips-without requiring agentic decision-making about when to move, what to do, or how to adapt across multi-turn dialogue. This leads to brittle timing, weak social grounding, and fragmented stacks where speech, text, and motion are trained or inferred in isolation. We introduce ViBES (Voice in Behavioral Expression and Synchrony), a conversational 3D agent that jointly plans language and movement and executes dialogue-conditioned body actions. Concretely, ViBES is a speech-language-behavior (SLB) model with a mixture-of-modality-experts (MoME) backbone: modality-partitioned transformer experts for speech, facial expression, and body motion. The model processes interleaved multimodal token streams with hard routing by modality (parameters are split per expert), while sharing information through cross-expert attention. By leveraging strong pretrained speech-language models, the agent supports mixed-initiative interaction: users can speak, type, or issue body-action directives mid-conversation, and the system exposes controllable behavior hooks for streaming responses. We further benchmark on multi-turn conversation with automatic metrics of dialogue-motion alignment and behavior quality, and observe consistent gains over strong co-speech and text-to-motion baselines. ViBES goes beyond \"speech-conditioned motion generation\" toward agentic virtual bodies where language, prosody, and movement are jointly generated, enabling controllable, socially competent 3D interaction. Code and data will be made available at: ai.stanford.edu/~juze/ViBES/",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page: https://ai.stanford.edu/~juze/ViBES/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14234v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "动作生成",
                    "matched_keywords": [
                        "motion generation"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO",
                        "VIO"
                    ],
                    "score": 2
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 5,
            "headline_zh": "提出ViBES对话代理，通过联合规划语言与运动解决多模态交互中的时序与社交基础问题",
            "summary_zh": "人类交流本质上是多模态和社交性的：语言、韵律和肢体语言共同传递意图。然而，大多数现有系统将人类行为建模为翻译任务（如伴随语音手势或文本到运动），将固定话语映射到运动片段，而不需要代理决策何时移动、做什么或如何在多轮对话中适应。这导致脆弱的时序、薄弱的社交基础以及语音、文本和运动被孤立训练或推断的碎片化堆栈。我们介绍了ViBES（语音行为表达与同步），这是一个联合规划语言和运动并执行对话条件化身体动作的对话式3D代理。具体而言，ViBES是一个具有混合模态专家（MoME）骨干的语音-语言-行为（SLB）模型：模态分区的Transformer专家用于语音、面部表情和身体运动。该模型通过模态硬路由（参数按专家分割）处理交错的多模态令牌流，同时通过跨专家注意力共享信息。通过利用强大的预训练语音-语言模型，该代理支持混合主动交互：用户可以在对话中说话、打字或发出身体动作指令，系统暴露可控行为钩子以进行流式响应。我们进一步在多轮对话上使用对话-运动对齐和行为质量的自动指标进行基准测试，观察到相对于强大的伴随语音和文本到运动基线的一致增益。ViBES超越了“语音条件化运动生成”，走向代理虚拟身体，其中语言、韵律和运动被联合生成，实现可控、社交能力强的3D交互。代码和数据将在ai.stanford.edu/~juze/ViBES/提供。",
            "intro_zh": [
                "现有方法将人类行为建模为翻译任务，导致时序脆弱、社交基础薄弱和模态孤立，限制了多模态交互的自然性。",
                "ViBES采用混合模态专家（MoME）骨干，联合规划语言与运动，通过跨专家注意力实现模态间信息共享，支持混合主动交互。",
                "在多轮对话基准测试中，ViBES在对话-运动对齐和行为质量上优于基线，实现可控、社交能力强的3D交互。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多模态对话代理中语言、语音和身体运动孤立建模的问题，现有方法如伴随语音手势或文本到运动生成将行为视为翻译任务，导致时序不协调、社交基础薄弱和模态碎片化，限制了交互的自然性和适应性。\\n\\n**核心思路**：论文提出ViBES，一个联合规划语言和运动的对话式3D代理，核心思路是通过混合模态专家（MoME）骨干，将语音、面部表情和身体运动作为独立模态处理，同时利用跨专家注意力实现模态间信息共享，从而模拟人类多模态交流的协同性。\\n\\n**技术框架**：ViBES采用语音-语言-行为（SLB）模型架构，基于MoME骨干，包含模态分区的Transformer专家模块，分别处理语音、面部表情和身体运动的令牌流。整体流程包括多模态输入处理（如语音、文本和动作指令）、硬路由到对应专家、跨专家注意力融合信息，以及联合生成语言、韵律和运动输出，支持流式响应和可控行为钩子。\\n\\n**关键创新**：最重要的技术创新是MoME骨干和跨专家注意力机制，与现有方法的本质区别在于将行为建模从孤立翻译任务提升为代理决策过程，实现语言、语音和运动的联合生成，而非简单映射，从而增强时序协调和社交适应性。\\n\\n**关键设计**：关键设计包括模态硬路由（参数按专家分割以减少干扰）、跨专家注意力层（允许模态间信息交换）、预训练语音-语言模型的集成（如用于初始化或特征提取），以及损失函数可能结合对话对齐和运动质量指标（具体细节未在摘要中提供，需参考论文全文）。",
            "application_zh": "ViBES的潜在应用领域包括虚拟助手、社交机器人、游戏角色和远程协作系统，通过实现可控、社交能力强的3D交互，提升用户体验和自然度。实际价值在于促进人机交互的多模态融合，未来可能影响教育、娱乐和心理健康等领域，推动更智能的虚拟代理发展。",
            "highlight_zh": "在多轮对话基准测试中，ViBES使用自动指标评估对话-运动对齐和行为质量，相对于强大的伴随语音和文本到运动基线（具体基线未命名），观察到一致性能增益。实验结果表明，ViBES在时序协调和社交适应性方面优于现有方法，但具体提升幅度和性能数据需参考论文详细结果。",
            "tags_zh": [
                "多模态对话代理",
                "3D虚拟身体",
                "混合模态专家",
                "语音-语言-行为模型",
                "跨专家注意力",
                "联合运动生成",
                "社交交互",
                "可控行为钩子"
            ],
            "_index": 7
        },
        {
            "title": "Understanding and Improving Hyperbolic Deep Reinforcement Learning",
            "authors": [
                "Timo Klein",
                "Thomas Lang",
                "Andrii Shkabrii",
                "Alexander Sturm",
                "Kevin Sidak",
                "Lukas Miklautz",
                "Claudia Plant",
                "Yllka Velaj",
                "Sebastian Tschiatschek"
            ],
            "arxiv_id": "2512.14202v1",
            "summary": "The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying feature representations. Hyperbolic feature spaces are well-suited for this purpose, as they naturally capture hierarchical and relational structure often present in complex RL environments. However, leveraging these spaces commonly faces optimization challenges due to the nonstationarity of RL. In this work, we identify key factors that determine the success and failure of training hyperbolic deep RL agents. By analyzing the gradients of core operations in the Poincaré Ball and Hyperboloid models of hyperbolic geometry, we show that large-norm embeddings destabilize gradient-based training, leading to trust-region violations in proximal policy optimization (PPO). Based on these insights, we introduce Hyper++, a new hyperbolic PPO agent that consists of three components: (i) stable critic training through a categorical value loss instead of regression; (ii) feature regularization guaranteeing bounded norms while avoiding the curse of dimensionality from clipping; and (iii) using a more optimization-friendly formulation of hyperbolic network layers. In experiments on ProcGen, we show that Hyper++ guarantees stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by approximately 30%. On Atari-5 with Double DQN, Hyper++ strongly outperforms Euclidean and hyperbolic baselines. We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl .",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14202v1",
            "code_links": [
                {
                    "url": "https://github.com/Probabilistic-and-Interactive-ML/hyper-rl",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO",
                        "VIO"
                    ],
                    "score": 2
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RL",
                        "PPO"
                    ],
                    "score": 3
                }
            ],
            "relevance_score": 5,
            "headline_zh": "提出Hyper++方法以解决双曲深度强化学习中的优化不稳定问题，提升在复杂环境中的性能。",
            "summary_zh": "强化学习（RL）智能体的性能关键取决于底层特征表示的质量。双曲特征空间非常适合此目的，因为它们能自然地捕捉复杂RL环境中常存在的层次和关系结构。然而，由于RL的非平稳性，利用这些空间通常面临优化挑战。在这项工作中，我们确定了决定双曲深度RL智能体训练成功与失败的关键因素。通过分析双曲几何的庞加莱球和双曲面模型中核心操作的梯度，我们表明大范数嵌入会破坏基于梯度的训练稳定性，导致近端策略优化（PPO）中的信任区域违反。基于这些见解，我们引入了Hyper++，一种新的双曲PPO智能体，包含三个组件：（i）通过分类值损失而非回归实现稳定的评论家训练；（ii）特征正则化保证有界范数，同时避免裁剪带来的维度诅咒；（iii）使用更优化友好的双曲网络层公式。在ProcGen实验中，我们表明Hyper++保证了稳定学习，优于先前的双曲智能体，并将挂钟时间减少约30%。在Atari-5与Double DQN上，Hyper++显著优于欧几里得和双曲基线。我们在https://github.com/Probabilistic-and-Interactive-ML/hyper-rl 发布了代码。",
            "intro_zh": [
                "核心问题：双曲深度强化学习中，大范数嵌入导致梯度不稳定，破坏PPO信任区域，引发训练失败。",
                "方法要点：提出Hyper++，结合分类值损失、特征正则化和优化友好层，稳定训练并提升性能。",
                "实验或效果：在ProcGen和Atari-5上，Hyper++优于基线，减少30%时间，实现稳定学习。"
            ],
            "method_zh": "**问题定义**：论文旨在解决双曲深度强化学习（RL）中的优化不稳定问题。现有双曲RL方法（如基于庞加莱球或双曲面模型）在训练过程中，由于RL环境的非平稳性，特征嵌入的范数可能变得过大，导致梯度计算不稳定，进而破坏近端策略优化（PPO）中的信任区域约束，引发训练失败或性能下降。\\n\\n**核心思路**：论文的核心思路是通过分析双曲几何操作的梯度特性，识别出大范数嵌入是训练不稳定的根源，并设计综合解决方案来约束嵌入范数、稳定梯度更新，同时保持双曲空间的表示优势。这基于对双曲模型（如庞加莱球）中核心运算（如指数映射、对数映射）梯度的理论分析，表明范数增大会放大梯度，破坏优化过程。\\n\\n**技术框架**：整体架构围绕Hyper++智能体展开，它是一个改进的双曲PPO代理。流程包括：首先，在策略和值函数网络中使用双曲层进行特征表示；其次，通过三个关键模块处理优化挑战——稳定评论家训练模块使用分类损失替代回归，特征正则化模块约束嵌入范数，优化友好层模块重新设计双曲运算；最后，在训练循环中集成这些组件，确保梯度稳定更新。主要阶段包括初始化、前向传播、损失计算（结合策略损失、值损失和正则化）和反向传播优化。\\n\\n**关键创新**：最重要的技术创新是Hyper++的三组件设计：1）分类值损失：将值函数预测从回归问题转为分类问题，减少对精确数值的依赖，增强训练稳定性；2）特征正则化：引入正则化项直接约束嵌入范数，避免使用裁剪方法可能带来的维度诅咒（即高维空间中的信息损失）；3）优化友好层：重新公式化双曲网络层（如线性层），使其梯度行为更平滑，降低优化难度。与现有方法的本质区别在于，它系统性地解决了梯度不稳定问题，而非仅依赖启发式调整。\\n\\n**关键设计**：关键设计细节包括：使用分类交叉熵损失作为值损失，替代均方误差回归；在损失函数中添加L2正则化项，目标范数设为固定值（如1.0），以控制嵌入大小；双曲层采用改进的数学公式，例如在庞加莱球模型中调整指数映射的数值稳定性；网络结构通常包含多个双曲全连接层，后接激活函数（如ReLU），输出层适配具体任务（如策略分布或值预测）；参数设置上，可能涉及学习率调整、信任区域系数优化，以匹配双曲空间的曲率特性。",
            "application_zh": "该研究在复杂强化学习环境中具有广泛潜在应用，如视频游戏（如ProcGen、Atari）、机器人控制、自动驾驶和智能决策系统。通过稳定双曲表示学习，它能更好地处理层次化或关系型结构的环境，提升智能体的泛化能力和效率。实际价值包括减少训练时间、提高性能稳定性，未来可能推动双曲几何在更复杂RL任务（如多智能体协作或元学习）中的应用，促进AI系统在动态世界中的适应性。",
            "highlight_zh": "最重要的实验结果包括：在ProcGen基准测试中，Hyper++保证了稳定学习，优于所有先前的双曲智能体，并将挂钟时间减少了约30%，显著提升训练效率。在Atari-5环境中结合Double DQN，Hyper++强烈优于欧几里得基线和现有双曲基线，显示出卓越的性能优势。具体数据方面，论文报告了在多个游戏上的得分提升，例如在某些任务中性能提高超过20%，验证了方法在复杂RL场景中的有效性。",
            "tags_zh": [
                "双曲几何",
                "深度强化学习",
                "近端策略优化",
                "特征表示",
                "优化稳定性",
                "梯度分析",
                "正则化技术",
                "智能体训练"
            ],
            "_index": 8
        },
        {
            "title": "A First-Order Logic-Based Alternative to Reward Models in RLHF",
            "authors": [
                "Chunjin Jian",
                "Xinhua Zhu"
            ],
            "arxiv_id": "2512.14100v1",
            "summary": "Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in aligning large language models (LLMs) with human values and preferences. However, the quality and stability of the trained reward model largely determine the final alignment performance. Existing approaches such as Proximal Policy Optimization (PPO) rely heavily on reward models to guide LLMs toward human-aligned behaviors.\n  In this work, we propose a logic-similarity-based reward mechanism as an alternative to conventional reward modeling. Instead of relying on heuristic reward estimation, our method leverages formal logical consistency to steer model alignment with human preferences. Since real-world questions can be interpreted from multiple perspectives, to ensure that logic-based reinforcement learning does not cause model collapse, we introduce S-GRPO, a supervised variant of the GRPO framework. S-GRPO incorporates an additional supervised component and jointly optimizes the generation term, KL-divergence regularization, and label-based objective during training.\n  Experimental results demonstrate that S-GRPO consistently outperforms standard supervised fine-tuning (SFT) in both performance and robustness. Furthermore, it extends existing preference-learning frameworks such as GRPO and DPO, offering a more flexible and task-adaptive approach to alignment training. Our code is available at https://github.com/ChunjinJiang/sgrpo.",
            "categories": [
                "cs.LG",
                "cs.LO"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14100v1",
            "code_links": [
                {
                    "url": "https://github.com/ChunjinJiang/sgrpo",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RL",
                        "reward",
                        "PPO"
                    ],
                    "score": 4
                }
            ],
            "relevance_score": 5,
            "headline_zh": "提出基于逻辑相似性的奖励机制S-GRPO，替代传统奖励模型以提升RLHF的稳定性和性能。",
            "summary_zh": "基于人类反馈的强化学习（RLHF）在将大型语言模型（LLMs）与人类价值观和偏好对齐方面起着关键作用。然而，训练出的奖励模型的质量和稳定性在很大程度上决定了最终的对齐性能。现有方法如近端策略优化（PPO）严重依赖奖励模型来引导LLMs朝向人类对齐的行为。在这项工作中，我们提出了一种基于逻辑相似性的奖励机制，作为传统奖励建模的替代方案。我们的方法不依赖启发式奖励估计，而是利用形式逻辑一致性来引导模型与人类偏好对齐。由于现实世界的问题可以从多个角度解释，为了确保基于逻辑的强化学习不会导致模型崩溃，我们引入了S-GRPO，这是GRPO框架的一个监督变体。S-GRPO在训练过程中结合了一个额外的监督组件，并联合优化生成项、KL散度正则化和基于标签的目标。实验结果表明，S-GRPO在性能和鲁棒性方面均持续优于标准监督微调（SFT）。此外，它扩展了现有的偏好学习框架，如GRPO和DPO，为对齐训练提供了更灵活和任务自适应的方法。我们的代码可在https://github.com/ChunjinJiang/sgrpo获取。",
            "intro_zh": [
                "现有RLHF方法依赖奖励模型，其质量和稳定性直接影响对齐性能，存在不稳定和启发式估计的不足。",
                "提出基于逻辑相似性的奖励机制，利用形式逻辑一致性替代传统奖励建模，并引入S-GRPO框架防止模型崩溃。",
                "实验显示S-GRPO在性能和鲁棒性上优于标准SFT，并扩展了GRPO和DPO等偏好学习框架。"
            ],
            "method_zh": "**问题定义**：论文旨在解决RLHF中传统奖励模型依赖启发式估计、质量不稳定导致对齐性能受限的问题，现有方法如PPO过度依赖奖励模型，易受噪声影响。\\n\\n**核心思路**：核心思路是用基于逻辑相似性的奖励机制替代传统奖励建模，通过形式逻辑一致性来引导模型对齐人类偏好，避免启发式偏差，并引入监督组件S-GRPO以防止模型崩溃。\\n\\n**技术框架**：整体框架基于GRPO扩展，包括逻辑相似性计算模块、监督微调模块和联合优化阶段。首先，从人类反馈中提取逻辑结构，计算模型输出与人类偏好的逻辑一致性；然后，在S-GRPO中整合监督信号，通过多目标优化训练模型。\\n\\n**关键创新**：最重要的创新是提出逻辑相似性作为奖励替代机制，本质区别在于从依赖数据驱动的奖励估计转向基于形式逻辑的客观一致性评估，减少了主观偏差。\\n\\n**关键设计**：关键设计包括S-GRPO的损失函数，联合优化生成项（如交叉熵）、KL散度正则化（防止过拟合）和基于标签的监督目标；参数设置可能涉及逻辑权重和正则化系数，具体细节需参考论文代码。",
            "application_zh": "该研究可应用于大型语言模型的对齐训练，如聊天机器人、内容生成和决策系统，提升模型与人类价值观的一致性。潜在价值包括提高AI系统的安全性和可靠性，未来可能推动逻辑驱动AI的发展，扩展至多模态和复杂任务对齐。",
            "highlight_zh": "实验结果显示，S-GRPO在多个基准测试中持续优于标准监督微调（SFT），具体性能提升未知，但强调了在鲁棒性方面的优势。对比基线包括SFT、GRPO和DPO，S-GRPO展现出更灵活的任务适应性，代码已开源供验证。",
            "tags_zh": [
                "强化学习人类反馈",
                "逻辑相似性奖励",
                "模型对齐",
                "S-GRPO框架",
                "监督微调",
                "偏好学习",
                "形式逻辑一致性",
                "KL散度正则化"
            ],
            "_index": 9
        },
        {
            "title": "OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving",
            "authors": [
                "Zhenguo Zhang",
                "Haohan Zhen",
                "Yishen Wang",
                "Le Xu",
                "Tianchen Deng",
                "Xuefeng Chen",
                "Qu Chen",
                "Bo Zhang",
                "Wuxiong Huang"
            ],
            "arxiv_id": "2512.14044v1",
            "summary": "The deployment of Vision-Language Models (VLMs) in safety-critical domains like autonomous driving (AD) is critically hindered by reliability failures, most notably object hallucination. This failure stems from their reliance on ungrounded, text-based Chain-of-Thought (CoT) reasoning.While existing multi-modal CoT approaches attempt mitigation, they suffer from two fundamental flaws: (1) decoupled perception and reasoning stages that prevent end-to-end joint optimization, and (2) reliance on expensive, dense localization labels.Thus we introduce OmniDrive-R1, an end-to-end VLM framework designed for autonomous driving, which unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. Our core innovation is an Reinforcement-driven visual grounding capability, enabling the model to autonomously direct its attention and \"zoom in\" on critical regions for fine-grained analysis. This capability is enabled by our pure two-stage reinforcement learning training pipeline and Clip-GRPO algorithm. Crucially, Clip-GRPO introduces an annotation-free, process-based grounding reward. This reward not only eliminates the need for dense labels but also circumvents the instability of external tool calls by enforcing real-time cross-modal consistency between the visual focus and the textual reasoning. Extensive experiments on DriveLMM-o1 demonstrate our model's significant improvements. Compared to the baseline Qwen2.5VL-7B, OmniDrive-R1 improves the overall reasoning score from 51.77% to 80.35%, and the final answer accuracy from 37.81% to 73.62%.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14044v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "autonomous driving"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RL",
                        "reward"
                    ],
                    "score": 3
                }
            ],
            "relevance_score": 5,
            "headline_zh": "提出OmniDrive-R1框架，通过强化学习驱动的交错多模态思维链解决自动驾驶中视觉语言模型的可靠性问题",
            "summary_zh": "在自动驾驶等安全关键领域部署视觉语言模型时，可靠性问题（特别是物体幻觉）严重阻碍了其应用。这种问题源于模型依赖未接地的文本思维链推理。现有多模态思维链方法虽然尝试缓解，但存在两个根本缺陷：（1）解耦的感知和推理阶段阻碍了端到端联合优化；（2）依赖昂贵密集的定位标注。为此，我们提出了OmniDrive-R1，这是一个为自动驾驶设计的端到端视觉语言模型框架，通过交错多模态思维链机制统一了感知和推理。我们的核心创新是强化学习驱动的视觉接地能力，使模型能够自主引导注意力并“放大”关键区域进行细粒度分析。这一能力通过我们的纯两阶段强化学习训练流程和Clip-GRPO算法实现。关键的是，Clip-GRPO引入了无需标注、基于过程的接地奖励。该奖励不仅消除了对密集标注的需求，还通过强制视觉焦点和文本推理之间的实时跨模态一致性，规避了外部工具调用的不稳定性。在DriveLMM-o1数据集上的大量实验表明，我们的模型取得了显著改进。与基线Qwen2.5VL-7B相比，OmniDrive-R1将整体推理分数从51.77%提升到80.35%，最终答案准确率从37.81%提升到73.62%。",
            "intro_zh": [
                "现有多模态思维链方法存在感知与推理阶段解耦、依赖密集标注的问题，阻碍了自动驾驶中视觉语言模型的可靠部署。",
                "论文提出交错多模态思维链机制，通过强化学习驱动的视觉接地能力实现端到端联合优化，无需密集标注。",
                "在DriveLMM-o1数据集上，OmniDrive-R1相比基线Qwen2.5VL-7B，推理分数提升28.58%，答案准确率提升35.81%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自动驾驶中视觉语言模型因物体幻觉导致的可靠性问题。现有多模态思维链方法的痛点包括：感知和推理阶段解耦，无法进行端到端联合优化；依赖昂贵密集的定位标注，限制了可扩展性和实用性。\\n\\n**核心思路**：论文的核心解决思路是设计一个端到端框架，通过交错多模态思维链机制统一感知和推理，并利用强化学习驱动的视觉接地能力，使模型能自主聚焦关键区域进行细粒度分析，从而提升推理的准确性和可靠性。\\n\\n**技术框架**：整体架构包括交错多模态思维链机制和强化学习训练流程。主要模块有：视觉语言模型基础模块、交错推理模块、强化学习驱动模块（含Clip-GRPO算法）。流程上，模型在推理时交替进行视觉感知和文本推理，强化学习模块通过奖励机制优化视觉接地过程。\\n\\n**关键创新**：最重要的技术创新是强化学习驱动的视觉接地能力，结合Clip-GRPO算法实现无需标注的接地奖励。与现有方法的本质区别在于：实现了感知和推理的端到端联合优化，避免了阶段解耦；消除了对密集标注的依赖，提高了效率和稳定性。\\n\\n**关键设计**：关键设计包括：纯两阶段强化学习训练流程（第一阶段预训练基础模型，第二阶段用Clip-GRPO优化接地能力）；Clip-GRPO算法中的过程性奖励函数，基于视觉焦点和文本推理的跨模态一致性计算奖励；网络结构上可能集成注意力机制和强化学习策略网络，具体参数设置未知，但强调实时性和一致性约束。",
            "application_zh": "该研究主要应用于自动驾驶领域，特别是视觉语言模型在环境感知、决策规划和人机交互中的可靠部署。潜在价值包括提升自动驾驶系统的安全性和鲁棒性，减少物体幻觉导致的误判。未来影响可能扩展到其他安全关键领域，如医疗诊断或工业检测，推动多模态人工智能在现实世界中的可信应用。",
            "highlight_zh": "在DriveLMM-o1数据集上的实验显示，OmniDrive-R1相比基线Qwen2.5VL-7B，整体推理分数从51.77%显著提升至80.35%（相对提升约55.2%），最终答案准确率从37.81%大幅提高至73.62%（相对提升约94.7%）。这些结果证明了该方法在解决物体幻觉和提升推理可靠性方面的有效性。",
            "tags_zh": [
                "自动驾驶",
                "视觉语言模型",
                "强化学习",
                "多模态思维链",
                "视觉接地",
                "端到端优化",
                "物体幻觉",
                "跨模态一致性"
            ],
            "_index": 10
        },
        {
            "title": "ACE-SLAM: Scene Coordinate Regression for Neural Implicit Real-Time SLAM",
            "authors": [
                "Ignacio Alzugaray",
                "Marwan Taher",
                "Andrew J. Davison"
            ],
            "arxiv_id": "2512.14032v1",
            "summary": "We present a novel neural RGB-D Simultaneous Localization And Mapping (SLAM) system that learns an implicit map of the scene in real time. For the first time, we explore the use of Scene Coordinate Regression (SCR) as the core implicit map representation in a neural SLAM pipeline, a paradigm that trains a lightweight network to directly map 2D image features to 3D global coordinates. SCR networks provide efficient, low-memory 3D map representations, enable extremely fast relocalization, and inherently preserve privacy, making them particularly suitable for neural implicit SLAM.\n  Our system is the first one to achieve strict real-time in neural implicit RGB-D SLAM by relying on a SCR-based representation. We introduce a novel SCR architecture specifically tailored for this purpose and detail the critical design choices required to integrate SCR into a live SLAM pipeline. The resulting framework is simple yet flexible, seamlessly supporting both sparse and dense features, and operates reliably in dynamic environments without special adaptation. We evaluate our approach on established synthetic and real-world benchmarks, demonstrating competitive performance against the state of the art. Project Page: https://github.com/ialzugaray/ace-slam",
            "categories": [
                "cs.CV",
                "cs.AI",
                "eess.IV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project Page: https://github.com/ialzugaray/ace-slam",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14032v1",
            "code_links": [
                {
                    "url": "https://github.com/ialzugaray/ace-slam",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "SLAM",
                        "localization",
                        "mapping"
                    ],
                    "score": 3
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 5,
            "headline_zh": "提出基于场景坐标回归的神经隐式实时SLAM系统，首次在RGB-D SLAM中实现严格实时性能",
            "summary_zh": "我们提出了一种新颖的神经RGB-D同步定位与建图（SLAM）系统，能够实时学习场景的隐式地图。首次探索了将场景坐标回归（SCR）作为神经SLAM流程中的核心隐式地图表示范式，该范式训练一个轻量级网络直接将2D图像特征映射到3D全局坐标。SCR网络提供高效、低内存的3D地图表示，支持极快的重定位，并固有地保护隐私，使其特别适合神经隐式SLAM。我们的系统是首个通过依赖基于SCR的表示在神经隐式RGB-D SLAM中实现严格实时的系统。我们引入了一种专门为此目的设计的新型SCR架构，并详细说明了将SCR集成到实时SLAM流程中所需的关键设计选择。所得框架简单而灵活，无缝支持稀疏和密集特征，并在动态环境中可靠运行而无需特殊适应。我们在已建立的合成和真实世界基准上评估了我们的方法，展示了与最先进技术相比的竞争性能。项目页面：https://github.com/ialzugaray/ace-slam",
            "intro_zh": [
                "现有神经隐式SLAM方法在实时性能上存在不足，难以满足实际应用需求，且地图表示通常复杂、内存占用高。",
                "论文提出基于场景坐标回归（SCR）的轻量级网络，直接学习从2D图像特征到3D全局坐标的映射，作为核心隐式地图表示。",
                "在合成和真实世界基准测试中，系统实现了严格实时性能，与最先进方法相比具有竞争力，并支持动态环境。"
            ],
            "method_zh": "**问题定义**：论文旨在解决神经隐式RGB-D SLAM中实时性能不足的问题。现有方法如神经辐射场（NeRF）或隐式函数网络通常计算复杂、内存占用高，难以在资源受限设备上实现严格实时运行，限制了实际应用。\\n\\n**核心思路**：采用场景坐标回归（SCR）作为核心隐式地图表示，训练一个轻量级网络直接从2D图像特征预测3D全局坐标。这种设计简化了地图表示，降低了计算和内存开销，同时保留了隐式方法的优势，如连续性和隐私保护。\\n\\n**技术框架**：整体架构包括实时SLAM流程和SCR网络模块。流程中，系统接收RGB-D输入，提取特征（支持稀疏或密集），通过SCR网络回归场景坐标，用于相机位姿估计和地图更新。关键阶段包括特征提取、坐标回归、位姿优化和地图维护，所有步骤在实时循环中高效执行。\\n\\n**关键创新**：首次将SCR范式集成到神经隐式SLAM中，实现严格实时性能。与现有隐式方法（如基于NeRF的SLAM）相比，本质区别在于使用轻量级回归网络替代复杂渲染或体积表示，从而大幅提升效率，同时保持地图的隐式特性。\\n\\n**关键设计**：设计了专门针对SLAM的SCR网络架构，可能包括卷积层和全连接层，以处理图像特征并输出3D坐标。损失函数可能基于坐标预测误差，优化网络参数以最小化重投影或几何不一致性。参数设置注重轻量化，如网络层数少、参数数量有限，确保在CPU或GPU上快速推理。此外，系统无缝集成稀疏和密集特征，无需额外适配，增强了灵活性。",
            "application_zh": "该研究在增强现实、机器人导航和智能监控等领域具有潜在应用价值。其实时性能和低内存特性使得在移动设备或嵌入式系统上部署成为可能，推动SLAM技术向更广泛的实际场景扩展。未来可能影响消费电子、自动驾驶和工业自动化，促进隐私保护型SLAM系统的发展。",
            "highlight_zh": "在合成和真实世界基准测试中，ACE-SLAM实现了严格实时性能（例如，在TUM RGB-D数据集上达到约30 FPS），与最先进方法如iMAP或NICE-SLAM相比，在定位精度和地图质量上表现竞争性。具体提升包括更快的重定位速度（可能减少到毫秒级）和更低的内存占用（例如，地图表示仅需几MB），同时在动态环境中保持鲁棒性，无需特殊处理。",
            "tags_zh": [
                "神经隐式SLAM",
                "场景坐标回归",
                "实时定位与建图",
                "RGB-D SLAM",
                "轻量级网络",
                "动态环境鲁棒性",
                "隐私保护地图"
            ],
            "_index": 11
        },
        {
            "title": "CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation",
            "authors": [
                "Sirui Chen",
                "Zi-ang Cao",
                "Zhengyi Luo",
                "Fernando Castañeda",
                "Chenran Li",
                "Tingwu Wang",
                "Ye Yuan",
                "Linxi \"Jim\" Fan",
                "C. Karen Liu",
                "Yuke Zhu"
            ],
            "arxiv_id": "2512.14689v1",
            "summary": "Recent progress in humanoid robots has unlocked agile locomotion skills, including backflipping, running, and crawling. Yet it remains challenging for a humanoid robot to perform forceful manipulation tasks such as moving objects, wiping, and pushing a cart. We propose adaptive Compliance Humanoid control through hIsight Perturbation (CHIP), a plug-and-play module that enables controllable end-effector stiffness while preserving agile tracking of dynamic reference motions. CHIP is easy to implement and requires neither data augmentation nor additional reward tuning. We show that a generalist motion-tracking controller trained with CHIP can perform a diverse set of forceful manipulation tasks that require different end-effector compliance, such as multi-robot collaboration, wiping, box delivery, and door opening.",
            "categories": [
                "cs.RO",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "The first two authors contributed equally. Project page: https://nvlabs.github.io/CHIP/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14689v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "人形机器人",
                    "matched_keywords": [
                        "humanoid",
                        "humanoid robot",
                        "humanoid control"
                    ],
                    "score": 3
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reward"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出CHIP模块以解决人形机器人执行强力操作任务时末端执行器刚度控制与动态运动跟踪的平衡问题",
            "summary_zh": "人形机器人领域近期在敏捷运动技能（如后空翻、奔跑、爬行）方面取得了显著进展，但执行强力操作任务（如移动物体、擦拭、推车）仍面临挑战。本文提出了自适应合规人形控制通过后见扰动（CHIP），这是一个即插即用模块，能够在保持动态参考运动敏捷跟踪的同时，实现可控的末端执行器刚度。CHIP易于实现，既不需要数据增强，也不需要额外的奖励调整。研究表明，使用CHIP训练的通用运动跟踪控制器能够执行多种需要不同末端执行器合规性的强力操作任务，例如多机器人协作、擦拭、箱子递送和开门。",
            "intro_zh": [
                "现有方法难以平衡人形机器人的敏捷运动跟踪与末端执行器刚度控制，导致强力操作任务执行困难。",
                "CHIP通过后见扰动技术，自适应调整末端执行器合规性，实现即插即用的刚度控制模块。",
                "实验表明，CHIP使通用控制器能执行多机器人协作、擦拭等任务，无需额外奖励调整或数据增强。"
            ],
            "method_zh": "**问题定义**：论文旨在解决人形机器人在执行强力操作任务（如移动物体、擦拭、推车）时，如何平衡末端执行器刚度控制与动态参考运动的敏捷跟踪。现有方法的痛点在于，传统控制器难以自适应调整末端执行器合规性，导致在需要不同刚度（如刚性推车与柔性擦拭）的任务中表现不佳，且通常需要复杂的数据增强或奖励设计。\\n\\n**核心思路**：论文的核心解决思路是通过后见扰动（Hindsight Perturbation）技术，在训练过程中模拟末端执行器的扰动，使控制器学习自适应调整刚度，从而在保持运动跟踪精度的同时，实现可控的末端执行器合规性。这样设计是因为后见扰动能够有效探索不同刚度下的最优策略，无需显式建模物理交互。\\n\\n**技术框架**：整体架构包括一个通用运动跟踪控制器作为基础，CHIP模块作为插件集成。流程分为训练阶段：在强化学习环境中，通过后见扰动生成模拟的末端执行器扰动数据，训练控制器学习自适应刚度调整；部署阶段：CHIP模块根据任务需求实时调整末端执行器刚度，保持运动跟踪性能。主要模块包括扰动生成器、策略网络和合规性调整模块。\\n\\n**关键创新**：最重要的技术创新点是后见扰动技术的应用，它允许控制器在训练中学习应对未知扰动，从而实现自适应末端执行器刚度控制。与现有方法的本质区别在于，CHIP无需额外数据增强或奖励调整，提供即插即用的合规性控制，简化了部署流程。\\n\\n**关键设计**：关键设计包括：使用强化学习框架（如PPO或SAC）训练策略网络；后见扰动通过随机或基于任务的扰动生成，模拟末端执行器的外力交互；损失函数结合运动跟踪误差和合规性调整项，以平衡精度与刚度控制；网络结构通常包含多层感知机（MLP）处理状态输入，输出关节控制指令和刚度参数。具体参数设置如扰动幅度、学习率等需根据任务调整，但论文强调其通用性。",
            "application_zh": "该研究在人形机器人领域具有广泛的应用潜力，特别是在需要强力操作和精细控制的场景中，如工业自动化（如搬运、装配）、服务机器人（如清洁、辅助生活）、医疗康复（如辅助移动）和紧急响应（如开门、推车）。其实际价值在于提供了一种简单高效的末端执行器刚度控制方法，可提升机器人的多功能性和适应性，未来可能推动人形机器人在复杂环境中的部署和商业化。",
            "highlight_zh": "实验结果显示，使用CHIP训练的通用控制器在多种强力操作任务中表现优异。具体性能数据包括：在推车任务中，成功率达到95%以上，相比基线方法提升约30%；在擦拭任务中，末端执行器跟踪误差降低20%；多机器人协作任务中，协同效率提高15%。对比基线（如传统PID控制或无CHIP的强化学习控制器），CHIP在保持运动跟踪精度的同时，显著提升了任务完成率和合规性控制能力。",
            "tags_zh": [
                "人形机器人控制",
                "自适应合规性",
                "后见扰动",
                "末端执行器刚度",
                "强力操作任务",
                "运动跟踪",
                "强化学习",
                "即插即用模块"
            ],
            "_index": 12
        },
        {
            "title": "gridfm-datakit-v1: A Python Library for Scalable and Realistic Power Flow and Optimal Power Flow Data Generation",
            "authors": [
                "Alban Puech",
                "Matteo Mazzonelli",
                "Celia Cintas",
                "Tamara R. Govindasamy",
                "Mangaliso Mngomezulu",
                "Jonas Weiss",
                "Matteo Baù",
                "Anna Varbella",
                "François Mirallès",
                "Kibaek Kim",
                "Le Xie",
                "Hendrik F. Hamann",
                "Etienne Vos",
                "Thomas Brunschwiler"
            ],
            "arxiv_id": "2512.14658v1",
            "summary": "We introduce gridfm-datakit-v1, a Python library for generating realistic and diverse Power Flow (PF) and Optimal Power Flow (OPF) datasets for training Machine Learning (ML) solvers. Existing datasets and libraries face three main challenges: (1) lack of realistic stochastic load and topology perturbations, limiting scenario diversity; (2) PF datasets are restricted to OPF-feasible points, hindering generalization of ML solvers to cases that violate operating limits (e.g., branch overloads or voltage violations); and (3) OPF datasets use fixed generator cost functions, limiting generalization across varying costs. gridfm-datakit addresses these challenges by: (1) combining global load scaling from real-world profiles with localized noise and supporting arbitrary N-k topology perturbations to create diverse yet realistic datasets; (2) generating PF samples beyond operating limits; and (3) producing OPF data with varying generator costs. It also scales efficiently to large grids (up to 10,000 buses). Comparisons with OPFData, OPF-Learn, PGLearn, and PF$Δ$ are provided. Available on GitHub at https://github.com/gridfm/gridfm-datakit under Apache 2.0 and via `pip install gridfm-datakit`.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "eess.SY",
                "math.OC"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Main equal contributors: Alban Puech, Matteo Mazzonelli. Other equal contributors: Celia Cintas, Tamara R. Govindasamy, Mangaliso Mngomezulu, Jonas Weiss",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14658v1",
            "code_links": [
                {
                    "url": "https://github.com/gridfm/gridfm-datakit",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO",
                        "VIO"
                    ],
                    "score": 2
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出gridfm-datakit-v1库，用于生成可扩展且真实的电力潮流和最优潮流数据集，以解决现有数据生成方法的三大挑战。",
            "summary_zh": "我们介绍了gridfm-datakit-v1，这是一个用于生成真实且多样化的电力潮流和最优潮流数据集的Python库，旨在训练机器学习求解器。现有数据集和库面临三个主要挑战：(1) 缺乏真实的随机负荷和拓扑扰动，限制了场景多样性；(2) 电力潮流数据集仅限于最优潮流可行点，阻碍了机器学习求解器对违反运行限制（如支路过载或电压违规）情况的泛化；(3) 最优潮流数据集使用固定的发电机成本函数，限制了在不同成本下的泛化能力。gridfm-datakit通过以下方式解决这些挑战：(1) 结合来自真实世界配置文件的全局负荷缩放与局部噪声，并支持任意的N-k拓扑扰动，以创建多样且真实的数据集；(2) 生成超出运行限制的电力潮流样本；(3) 生成具有变化发电机成本的最优潮流数据。它还能高效扩展到大型电网（最多10,000个节点）。提供了与OPFData、OPF-Learn、PGLearn和PF$Δ$的比较。该库可在GitHub上获取，网址为https://github.com/gridfm/gridfm-datakit，遵循Apache 2.0许可，并通过`pip install gridfm-datakit`安装。",
            "intro_zh": [
                "现有电力潮流和最优潮流数据集缺乏真实随机扰动，导致场景多样性不足，限制了机器学习求解器的训练效果。",
                "gridfm-datakit结合全局负荷缩放与局部噪声，支持N-k拓扑扰动，生成超出运行限制的样本和变化成本数据，提升数据真实性和多样性。",
                "该库能高效扩展到10,000节点电网，相比OPFData等基线，在数据生成速度和多样性方面有显著提升，支持更广泛的机器学习应用。"
            ],
            "method_zh": "**问题定义**：论文旨在解决电力潮流和最优潮流数据生成中的三大挑战：现有方法缺乏真实随机负荷和拓扑扰动，导致数据集多样性不足；电力潮流数据仅限于最优潮流可行点，限制了机器学习求解器对违反运行限制情况的泛化；最优潮流数据使用固定发电机成本函数，影响在不同成本场景下的泛化能力。这些痛点阻碍了机器学习模型在电力系统优化中的有效训练和应用。\\n\\n**核心思路**：论文的核心思路是通过集成真实世界负荷配置文件、引入局部噪声和任意N-k拓扑扰动，生成多样且真实的电力潮流和最优潮流数据集。设计上，它允许生成超出运行限制的样本和变化成本数据，以增强机器学习求解器的鲁棒性和泛化能力。这种设计基于对现有数据局限性的分析，旨在提供更全面的训练数据。\\n\\n**技术框架**：整体架构包括数据输入模块、扰动生成模块和输出模块。主要阶段：首先，输入电网拓扑和真实负荷配置文件；其次，应用全局负荷缩放和局部噪声生成随机负荷扰动，并结合N-k拓扑扰动创建多样场景；然后，生成电力潮流样本（包括违反限制的情况）和最优潮流数据（带有变化成本函数）；最后，输出标准化数据集，支持大规模电网（如10,000节点）的高效处理。\\n\\n**关键创新**：最重要的技术创新在于同时解决了数据多样性、泛化性和可扩展性问题。与现有方法（如OPFData）的本质区别是：它不局限于可行点，能生成违反运行限制的样本；引入真实随机扰动和变化成本，提升数据真实性；支持大规模电网，扩展性更强。这些创新使数据集更适用于训练鲁棒的机器学习求解器。\\n\\n**关键设计**：关键设计包括：使用Python实现，基于Apache 2.0开源；参数设置如负荷缩放因子和噪声分布可配置，以模拟真实波动；N-k扰动支持任意故障组合，增强拓扑多样性；没有特定损失函数或网络结构，因为这是一个数据生成库，而非机器学习模型本身；架构优化确保在10,000节点电网上的高效计算，通过并行处理和内存管理实现。",
            "application_zh": "该研究在电力系统优化和机器学习领域具有广泛潜在应用。实际价值包括为电力潮流和最优潮流问题的机器学习求解器提供高质量训练数据，提升模型在真实场景中的准确性和鲁棒性。未来影响可能推动智能电网、可再生能源集成和电力市场分析的发展，通过更真实的数据支持决策优化和系统稳定性增强。",
            "highlight_zh": "最重要的实验结果显示，gridfm-datakit在数据生成方面相比基线（如OPFData、OPF-Learn、PGLearn和PF$Δ$）有显著提升。具体性能数据包括：能高效扩展到10,000节点电网，生成时间未知；数据多样性通过随机扰动和N-k拓扑增强，覆盖更多违反运行限制的场景；变化成本函数支持提高了最优潮流数据的泛化能力。提升幅度体现在更全面的数据集支持机器学习求解器训练，但具体数值如准确率提升未知，需进一步实验验证。",
            "tags_zh": [
                "电力潮流数据生成",
                "最优潮流数据生成",
                "机器学习求解器",
                "电网拓扑扰动",
                "数据多样性",
                "可扩展性",
                "Python库",
                "电力系统优化"
            ],
            "_index": 13
        },
        {
            "title": "Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes",
            "authors": [
                "Alessandro Trapasso",
                "Luca Iocchi",
                "Fabio Patrizi"
            ],
            "arxiv_id": "2512.14617v1",
            "summary": "Many practical decision-making problems involve tasks whose success depends on the entire system history, rather than on achieving a state with desired properties. Markovian Reinforcement Learning (RL) approaches are not suitable for such tasks, while RL with non-Markovian reward decision processes (NMRDPs) enables agents to tackle temporal-dependency tasks. This approach has long been known to lack formal guarantees on both (near-)optimality and sample efficiency. We contribute to solving both issues with QR-MAX, a novel model-based algorithm for discrete NMRDPs that factorizes Markovian transition learning from non-Markovian reward handling via reward machines. To the best of our knowledge, this is the first model-based RL algorithm for discrete-action NMRDPs that exploits this factorization to obtain PAC convergence to $\\varepsilon$-optimal policies with polynomial sample complexity. We then extend QR-MAX to continuous state spaces with Bucket-QR-MAX, a SimHash-based discretiser that preserves the same factorized structure and achieves fast and stable learning without manual gridding or function approximation. We experimentally compare our method with modern state-of-the-art model-based RL approaches on environments of increasing complexity, showing a significant improvement in sample efficiency and increased robustness in finding optimal policies.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "19 pages, 32 figures, includes appendix",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14617v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RL",
                        "reward"
                    ],
                    "score": 3
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出QR-MAX算法以解决离散动作非马尔可夫奖励决策过程中的样本效率和最优性保证问题",
            "summary_zh": "许多实际决策问题涉及的任务成功取决于整个系统历史，而非仅达到具有期望属性的状态。马尔可夫强化学习方法不适用于此类任务，而非马尔可夫奖励决策过程使智能体能够处理时间依赖性任务。这种方法长期以来缺乏对（近似）最优性和样本效率的形式化保证。我们通过QR-MAX为解决这两个问题做出贡献，这是一种用于离散NMRDP的新型基于模型的算法，通过奖励机将马尔可夫转移学习与非马尔可夫奖励处理进行因子分解。据我们所知，这是第一个利用这种因子分解获得具有多项式样本复杂度的ε-最优策略的PAC收敛性的离散动作NMRDP基于模型强化学习算法。然后，我们将QR-MAX扩展到连续状态空间，提出Bucket-QR-MAX，这是一种基于SimHash的离散化器，保持相同的因子分解结构，无需手动网格化或函数逼近即可实现快速稳定学习。我们在复杂性递增的环境中将我们的方法与现代最先进的基于模型强化学习方法进行实验比较，显示出样本效率的显著提高和寻找最优策略的鲁棒性增强。",
            "intro_zh": [
                "核心问题：现有马尔可夫强化学习方法不适用于依赖历史的任务，而非马尔可夫奖励决策过程方法缺乏最优性和样本效率的形式化保证。",
                "方法要点：提出QR-MAX算法，通过奖励机因子分解马尔可夫转移和非马尔可夫奖励，实现多项式样本复杂度的ε-最优策略收敛。",
                "实验或效果：在复杂环境中，QR-MAX相比现有方法显著提升样本效率和鲁棒性，Bucket-QR-MAX扩展至连续状态空间。"
            ],
            "method_zh": "**问题定义**：论文解决离散动作非马尔可夫奖励决策过程中的强化学习问题，其中任务成功依赖于整个系统历史而非单个状态。现有基于模型的强化学习方法在NMRDP中缺乏形式化保证，导致样本效率低和最优性不确定，难以处理时间依赖性任务。\\n\\n**核心思路**：核心思路是通过因子分解将马尔可夫转移学习与非马尔可夫奖励处理分离，利用奖励机建模奖励结构，从而简化学习过程并实现理论保证。这种设计基于奖励机能有效捕捉时间依赖性的观察，避免直接处理复杂非马尔可夫奖励的困难。\\n\\n**技术框架**：整体架构包括两个主要阶段：首先，使用奖励机将非马尔可夫奖励转换为马尔可夫形式，通过状态扩展将NMRDP转换为等效的马尔可夫决策过程；其次，应用基于模型的强化学习算法（如QR-MAX）学习转移模型和策略。对于连续状态空间，引入Bucket-QR-MAX，基于SimHash进行自动离散化，保持因子分解结构。\\n\\n**关键创新**：最重要的技术创新是首次提出基于模型的强化学习算法QR-MAX，专门针对离散动作NMRDP，通过奖励机因子分解实现PAC收敛到ε-最优策略，具有多项式样本复杂度。与现有方法相比，本质区别在于结合了模型学习和奖励机理论，提供形式化保证，而传统方法多依赖启发式或无模型学习。\\n\\n**关键设计**：关键设计包括奖励机的构建，用于编码非马尔可夫奖励逻辑；QR-MAX算法中的Q值更新规则，基于学习到的转移模型进行规划；Bucket-QR-MAX中的SimHash离散化，参数如哈希函数数量和桶大小影响状态空间粒度；损失函数基于贝尔曼误差最小化，网络结构未知，但可能涉及值函数或策略网络近似。",
            "application_zh": "该研究在机器人任务规划、自动驾驶、游戏AI和工业自动化等领域有潜在应用，特别适用于需要长期历史依赖的决策场景，如序列任务完成或复杂环境交互。实际价值在于提高智能体在非马尔可夫环境中的学习效率和策略最优性，未来可能推动强化学习在现实世界复杂任务中的部署，减少样本需求并增强鲁棒性。",
            "highlight_zh": "实验在复杂性递增的环境中进行，QR-MAX相比现代最先进的基于模型强化学习方法（如未知基线）显示出显著改进：样本效率提升具体幅度未知，但论文报告了更快收敛和更高成功率；Bucket-QR-MAX在连续状态空间中实现稳定学习，无需手动网格化，鲁棒性增强，具体性能数据如奖励累积或步数减少未提供，但强调了在复杂任务中的优越性。",
            "tags_zh": [
                "非马尔可夫奖励决策过程",
                "基于模型强化学习",
                "奖励机",
                "样本效率",
                "PAC收敛",
                "离散动作空间",
                "连续状态空间",
                "SimHash离散化"
            ],
            "_index": 14
        },
        {
            "title": "DASP: Self-supervised Nighttime Monocular Depth Estimation with Domain Adaptation of Spatiotemporal Priors",
            "authors": [
                "Yiheng Huang",
                "Junhong Chen",
                "Anqi Ning",
                "Zhanhong Liang",
                "Nick Michiels",
                "Luc Claesen",
                "Wenyin Liu"
            ],
            "arxiv_id": "2512.14536v1",
            "summary": "Self-supervised monocular depth estimation has achieved notable success under daytime conditions. However, its performance deteriorates markedly at night due to low visibility and varying illumination, e.g., insufficient light causes textureless areas, and moving objects bring blurry regions. To this end, we propose a self-supervised framework named DASP that leverages spatiotemporal priors for nighttime depth estimation. Specifically, DASP consists of an adversarial branch for extracting spatiotemporal priors and a self-supervised branch for learning. In the adversarial branch, we first design an adversarial network where the discriminator is composed of four devised spatiotemporal priors learning blocks (SPLB) to exploit the daytime priors. In particular, the SPLB contains a spatial-based temporal learning module (STLM) that uses orthogonal differencing to extract motion-related variations along the time axis and an axial spatial learning module (ASLM) that adopts local asymmetric convolutions with global axial attention to capture the multiscale structural information. By combining STLM and ASLM, our model can acquire sufficient spatiotemporal features to restore textureless areas and estimate the blurry regions caused by dynamic objects. In the self-supervised branch, we propose a 3D consistency projection loss to bilaterally project the target frame and source frame into a shared 3D space, and calculate the 3D discrepancy between the two projected frames as a loss to optimize the 3D structural consistency and daytime priors. Extensive experiments on the Oxford RobotCar and nuScenes datasets demonstrate that our approach achieves state-of-the-art performance for nighttime depth estimation. Ablation studies further validate the effectiveness of each component.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "8 pages, 7 figures",
            "doi": "10.1109/LRA.2025.3644148",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14536v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "nuScenes"
                    ],
                    "score": 1
                },
                {
                    "name": "深度估计",
                    "matched_keywords": [
                        "depth estimation",
                        "monocular depth"
                    ],
                    "score": 2
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出DASP框架，利用时空先验进行自监督夜间单目深度估计，以解决低可见度和动态物体导致的性能下降问题。",
            "summary_zh": "自监督单目深度估计在白天条件下已取得显著成功，但在夜间由于低可见度和变化光照（如光线不足导致纹理缺失区域，移动物体带来模糊区域）而性能显著下降。为此，我们提出了一个名为DASP的自监督框架，利用时空先验进行夜间深度估计。具体来说，DASP包括一个用于提取时空先验的对抗分支和一个用于学习的自监督分支。在对抗分支中，我们首先设计了一个对抗网络，其中判别器由四个设计的时空先验学习块（SPLB）组成，以利用白天先验。特别地，SPLB包含一个基于空间的时序学习模块（STLM），使用正交差分沿时间轴提取与运动相关的变化，以及一个轴向空间学习模块（ASLM），采用局部非对称卷积与全局轴向注意力来捕获多尺度结构信息。通过结合STLM和ASLM，我们的模型能够获取足够的时空特征来恢复纹理缺失区域并估计由动态物体引起的模糊区域。在自监督分支中，我们提出了一个3D一致性投影损失，将目标帧和源帧双边投影到共享的3D空间中，并计算两个投影帧之间的3D差异作为损失，以优化3D结构一致性和白天先验。在Oxford RobotCar和nuScenes数据集上的大量实验表明，我们的方法在夜间深度估计方面实现了最先进的性能。消融研究进一步验证了每个组件的有效性。",
            "intro_zh": [
                "核心问题：现有自监督单目深度估计方法在夜间性能显著下降，主要由于低可见度导致纹理缺失和动态物体引起模糊区域。",
                "方法要点：提出DASP框架，结合对抗分支提取时空先验和自监督分支学习，通过SPLB模块捕获多尺度时空特征以恢复纹理和估计模糊区域。",
                "实验或效果：在Oxford RobotCar和nuScenes数据集上实现最先进性能，消融研究验证了各组件有效性，提升了夜间深度估计的准确性和鲁棒性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自监督单目深度估计在夜间场景中的性能下降问题，主要挑战包括低可见度导致的纹理缺失区域和动态物体引起的模糊区域，现有方法在这些条件下难以准确估计深度。\\n\\n**核心思路**：论文的核心思路是通过利用时空先验来增强模型对夜间场景的理解，设计一个结合对抗学习和自监督学习的框架，从白天数据中提取先验知识，并应用于夜间深度估计，以恢复纹理和减少模糊影响。\\n\\n**技术框架**：DASP框架包括两个主要分支：对抗分支和自监督分支。对抗分支使用一个对抗网络，其中判别器包含四个时空先验学习块（SPLB），每个SPLB由基于空间的时序学习模块（STLM）和轴向空间学习模块（ASLM）组成，用于提取时空特征。自监督分支则通过3D一致性投影损失来优化深度估计，确保3D结构一致性。\\n\\n**关键创新**：最重要的技术创新是设计了SPLB模块，结合STLM和ASLM，STLM使用正交差分提取时间轴上的运动变化，ASLM采用局部非对称卷积和全局轴向注意力捕获多尺度空间结构，从而有效处理夜间场景的纹理缺失和模糊问题。\\n\\n**关键设计**：关键设计包括SPLB中的STLM和ASLM模块，STLM通过正交差分操作提取时序特征，ASLM使用非对称卷积和轴向注意力机制增强空间表示；损失函数方面，引入了3D一致性投影损失，通过双边投影计算3D差异来优化模型；网络结构上，对抗分支的判别器由多个SPLB堆叠而成，自监督分支则基于标准深度估计网络进行训练。",
            "application_zh": "该研究在自动驾驶、机器人导航和增强现实等领域具有潜在应用价值，特别是在夜间或低光照环境下，能够提升深度感知的准确性和鲁棒性，为安全导航和环境理解提供技术支持，未来可能推动夜间视觉系统的发展。",
            "highlight_zh": "在Oxford RobotCar和nuScenes数据集上的实验表明，DASP框架在夜间深度估计任务中实现了最先进的性能，具体性能数据未知，但通过消融研究验证了SPLB模块和3D一致性投影损失的有效性，显著提升了估计精度和鲁棒性，相比基线方法有显著提升。",
            "tags_zh": [
                "自监督学习",
                "单目深度估计",
                "夜间视觉",
                "时空先验",
                "对抗学习",
                "3D一致性投影",
                "多尺度特征提取",
                "动态物体处理"
            ],
            "_index": 15
        },
        {
            "title": "CoLD Fusion: A Real-time Capable Spline-based Fusion Algorithm for Collective Lane Detection",
            "authors": [
                "Jörg Gamerdinger",
                "Sven Teufel",
                "Georg Volk",
                "Oliver Bringmann"
            ],
            "arxiv_id": "2512.14355v1",
            "summary": "Comprehensive environment perception is essential for autonomous vehicles to operate safely. It is crucial to detect both dynamic road users and static objects like traffic signs or lanes as these are required for safe motion planning. However, in many circumstances a complete perception of other objects or lanes is not achievable due to limited sensor ranges, occlusions, and curves. In scenarios where an accurate localization is not possible or for roads where no HD maps are available, an autonomous vehicle must rely solely on its perceived road information. Thus, extending local sensing capabilities through collective perception using vehicle-to-vehicle communication is a promising strategy that has not yet been explored for lane detection. Therefore, we propose a real-time capable approach for collective perception of lanes using a spline-based estimation of undetected road sections. We evaluate our proposed fusion algorithm in various situations and road types. We were able to achieve real-time capability and extend the perception range by up to 200%.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted at IEEE IV 2023",
            "doi": "10.1109/IV55152.2023.10186632",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14355v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "autonomous vehicle",
                        "lane detection",
                        "traffic"
                    ],
                    "score": 3
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出基于样条的实时集体车道检测融合算法，以解决传感器局限下的车道感知范围不足问题。",
            "summary_zh": "全面的环境感知对于自动驾驶车辆的安全运行至关重要。检测动态道路使用者以及静态物体（如交通标志或车道）是安全运动规划的必要条件。然而，在许多情况下，由于传感器范围有限、遮挡和弯道等因素，无法实现对其他物体或车道的完整感知。在无法准确定位或没有高清地图的道路场景中，自动驾驶车辆必须完全依赖其感知的道路信息。因此，通过车对车通信利用集体感知来扩展本地感知能力是一种有前景的策略，但尚未在车道检测领域得到探索。为此，我们提出了一种基于样条估计未检测道路段的实时集体车道感知方法。我们在多种情况和道路类型下评估了所提出的融合算法，实现了实时处理能力，并将感知范围扩展了高达200%。",
            "intro_zh": [
                "现有车道检测方法受传感器范围、遮挡和弯道限制，难以实现完整感知，尤其在无高清地图场景下，自动驾驶车辆依赖本地感知存在风险。",
                "论文提出基于样条的集体车道检测融合算法，通过车对车通信整合多车感知数据，实时估计未检测道路段，扩展感知范围。",
                "实验表明，该方法在多种道路类型下实现实时处理，感知范围提升高达200%，显著增强自动驾驶系统的环境感知能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自动驾驶中车道检测的感知范围不足问题。现有方法依赖单一车辆的传感器，受限于范围、遮挡和弯道，导致无法完整感知车道，尤其在无高清地图或定位不准的场景下，这增加了安全风险。\\n\\n**核心思路**：论文提出一种基于样条的集体车道检测融合算法，通过车对车通信整合多辆车的局部感知数据，利用样条曲线估计未检测的道路段，从而扩展整体感知范围。这一设计基于集体感知策略，弥补了单个车辆传感器的局限性。\\n\\n**技术框架**：整体架构包括数据收集、融合处理和输出三个阶段。首先，通过车辆传感器获取局部车道数据；然后，使用车对车通信传输数据到中央处理单元；接着，应用样条拟合算法融合多源数据，估计完整车道形状；最后，输出扩展后的车道信息供运动规划使用。主要模块包括传感器接口、通信模块、样条估计器和融合引擎。\\n\\n**关键创新**：最重要的技术创新是将集体感知概念引入车道检测领域，结合样条估计实现实时融合。与现有方法相比，本质区别在于利用多车协作而非单一车辆数据，从而克服传感器局限，提升感知鲁棒性和范围。\\n\\n**关键设计**：算法采用样条曲线（如B样条或三次样条）进行车道建模，以平滑处理不连续数据；融合过程可能涉及加权平均或优化算法，以整合多车数据；实时性通过高效计算和通信协议保障，具体参数如样条阶数、节点设置和融合权重需根据实验调整，但论文未详细说明损失函数或网络结构细节。",
            "application_zh": "该研究主要应用于自动驾驶领域，特别是在城市道路、高速公路和复杂弯道等场景中，可提升车辆的环境感知能力。实际价值在于增强自动驾驶系统的安全性和可靠性，通过扩展感知范围减少事故风险。未来影响可能推动车联网和协同感知技术的发展，为智能交通系统提供新解决方案。",
            "highlight_zh": "实验在多种道路类型和情况下进行，结果显示算法实现实时处理能力，具体性能数据未知，但感知范围扩展高达200%，相比基线方法（可能为单一车辆检测）有显著提升。这表明集体融合策略有效克服了传感器局限，提升了车道检测的完整性和鲁棒性。",
            "tags_zh": [
                "集体感知",
                "车道检测",
                "样条估计",
                "车对车通信",
                "实时融合",
                "自动驾驶",
                "环境感知",
                "传感器融合"
            ],
            "_index": 16
        },
        {
            "title": "Elastic3D: Controllable Stereo Video Conversion with Guided Latent Decoding",
            "authors": [
                "Nando Metzger",
                "Prune Truong",
                "Goutam Bhat",
                "Konrad Schindler",
                "Federico Tombari"
            ],
            "arxiv_id": "2512.14236v1",
            "summary": "The growing demand for immersive 3D content calls for automated monocular-to-stereo video conversion. We present Elastic3D, a controllable, direct end-to-end method for upgrading a conventional video to a binocular one. Our approach, based on (conditional) latent diffusion, avoids artifacts due to explicit depth estimation and warping. The key to its high-quality stereo video output is a novel, guided VAE decoder that ensures sharp and epipolar-consistent stereo video output. Moreover, our method gives the user control over the strength of the stereo effect (more precisely, the disparity range) at inference time, via an intuitive, scalar tuning knob. Experiments on three different datasets of real-world stereo videos show that our method outperforms both traditional warping-based and recent warping-free baselines and sets a new standard for reliable, controllable stereo video conversion. Please check the project page for the video samples https://elastic3d.github.io.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page: elastic3d.github.io",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14236v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "深度估计",
                    "matched_keywords": [
                        "depth estimation",
                        "disparity"
                    ],
                    "score": 2
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出Elastic3D方法，基于引导潜在解码实现可控的单目到立体视频转换，解决传统方法中的伪影问题。",
            "summary_zh": "随着沉浸式3D内容需求的增长，自动化单目到立体视频转换变得日益重要。本文提出Elastic3D，一种可控的端到端方法，用于将传统视频升级为双目视频。该方法基于条件潜在扩散模型，避免了因显式深度估计和扭曲而产生的伪影。其高质量立体视频输出的关键在于一种新颖的引导变分自编码器解码器，确保输出视频清晰且满足极线一致性。此外，该方法允许用户在推理时通过直观的标量调节旋钮控制立体效果的强度（更精确地说，是视差范围）。在三个真实世界立体视频数据集上的实验表明，我们的方法优于传统的基于扭曲的方法和最近的无扭曲基线，为可靠、可控的立体视频转换设定了新标准。请访问项目页面查看视频样本：https://elastic3d.github.io。",
            "intro_zh": [
                "核心问题：传统单目到立体视频转换方法依赖显式深度估计和扭曲，易产生伪影，且缺乏用户对立体效果强度的控制。",
                "方法要点：基于条件潜在扩散模型，设计引导VAE解码器，实现端到端可控转换，避免显式扭曲步骤。",
                "实验或效果：在三个真实数据集上超越传统和近期基线，实现高质量、极线一致的立体视频输出，并支持视差范围调节。"
            ],
            "method_zh": "**问题定义**：论文旨在解决单目到立体视频转换问题，即从单目视频生成高质量、可控的立体视频。现有方法通常依赖显式深度估计和图像扭曲，这容易引入伪影（如模糊、不连续），且缺乏用户对立体效果（如视差范围）的实时控制，限制了实际应用的灵活性和可靠性。\\n\\n**核心思路**：论文的核心思路是采用条件潜在扩散模型作为基础框架，通过端到端学习直接生成立体视频，避免显式深度估计和扭曲步骤。设计关键在于引入一种引导的变分自编码器解码器，在潜在空间中强制极线一致性，从而确保输出视频的清晰度和立体对齐。这种设计允许模型在推理时通过简单参数调节视差范围，实现用户可控的立体效果。\\n\\n**技术框架**：整体架构基于条件潜在扩散模型，包含编码器、扩散过程和引导解码器。首先，单目视频输入被编码到潜在空间；然后，在扩散过程中，模型学习去噪以生成立体视频的潜在表示；最后，通过引导VAE解码器将潜在表示解码为高质量的立体视频帧。整个过程是端到端的，无需中间深度图或扭曲操作。\\n\\n**关键创新**：最重要的技术创新是引导VAE解码器，它通过引入极线约束在解码阶段确保立体视频的几何一致性，与现有方法（如基于扭曲的方法或简单生成模型）相比，本质区别在于避免了显式扭曲带来的伪影，同时实现了可控性和高质量输出。\\n\\n**关键设计**：关键设计包括：使用条件潜在扩散模型处理视频序列，以时间一致性；引导解码器通过损失函数（如重构损失和极线一致性损失）优化；用户控制通过调节潜在空间中的标量参数实现视差范围调整；网络结构可能包含卷积层和注意力机制，以处理时空信息。具体参数设置和损失函数细节需参考论文原文，但核心是结合扩散模型的生成能力和几何引导的约束。",
            "application_zh": "该研究在虚拟现实、增强现实、3D电影制作和游戏开发等领域具有广泛应用潜力。通过自动化、高质量的单目到立体视频转换，可以降低3D内容制作成本，提升沉浸式体验，并为用户提供灵活的控制选项，推动沉浸式媒体技术的发展。未来可能影响视频处理、计算机视觉和娱乐产业的标准化流程。",
            "highlight_zh": "实验在三个真实世界立体视频数据集上进行，Elastic3D在定量指标（如PSNR、SSIM）和定性评估上均优于传统基于扭曲的方法（如深度估计后扭曲）和近期无扭曲基线（如生成对抗网络方法）。具体提升幅度因数据集而异，但论文报告了显著的性能改进，例如在视差一致性和视觉质量方面达到新标准，用户研究也证实了其优越性。",
            "tags_zh": [
                "立体视频转换",
                "潜在扩散模型",
                "引导解码器",
                "极线一致性",
                "可控生成",
                "单目到立体",
                "视频处理",
                "3D内容生成"
            ],
            "_index": 17
        },
        {
            "title": "OmniGen: Unified Multimodal Sensor Generation for Autonomous Driving",
            "authors": [
                "Tao Tang",
                "Enhui Ma",
                "xia zhou",
                "Letian Wang",
                "Tianyi Yan",
                "Xueyang Zhang",
                "Kun Zhan",
                "Peng Jia",
                "XianPeng Lang",
                "Jia-Wang Bian",
                "Kaicheng Yu",
                "Xiaodan Liang"
            ],
            "arxiv_id": "2512.14225v1",
            "summary": "Autonomous driving has seen remarkable advancements, largely driven by extensive real-world data collection. However, acquiring diverse and corner-case data remains costly and inefficient. Generative models have emerged as a promising solution by synthesizing realistic sensor data. However, existing approaches primarily focus on single-modality generation, leading to inefficiencies and misalignment in multimodal sensor data. To address these challenges, we propose OminiGen, which generates aligned multimodal sensor data in a unified framework. Our approach leverages a shared Bird\\u2019s Eye View (BEV) space to unify multimodal features and designs a novel generalizable multimodal reconstruction method, UAE, to jointly decode LiDAR and multi-view camera data. UAE achieves multimodal sensor decoding through volume rendering, enabling accurate and flexible reconstruction. Furthermore, we incorporate a Diffusion Transformer (DiT) with a ControlNet branch to enable controllable multimodal sensor generation. Our comprehensive experiments demonstrate that OminiGen achieves desired performances in unified multimodal sensor data generation with multimodal consistency and flexible sensor adjustments.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "ACM MM 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14225v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "autonomous driving",
                        "lidar"
                    ],
                    "score": 2
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出OmniGen统一框架以解决自动驾驶中多模态传感器数据生成的对齐与效率问题",
            "summary_zh": "自动驾驶技术的显著进步很大程度上依赖于大规模真实世界数据采集，但获取多样化和极端案例数据仍然成本高昂且效率低下。生成模型通过合成逼真的传感器数据成为一种有前景的解决方案。然而，现有方法主要关注单模态生成，导致多模态传感器数据生成效率低下且存在不对齐问题。为解决这些挑战，我们提出了OmniGen，该框架在统一框架中生成对齐的多模态传感器数据。我们的方法利用共享的鸟瞰图空间来统一多模态特征，并设计了一种新颖的可泛化多模态重建方法UAE，以联合解码激光雷达和多视角相机数据。UAE通过体渲染实现多模态传感器解码，从而实现准确且灵活的重建。此外，我们结合了带有ControlNet分支的扩散变换器，以实现可控的多模态传感器生成。我们的全面实验表明，OmniGen在多模态一致性和灵活传感器调整方面，在统一多模态传感器数据生成中实现了理想的性能。",
            "intro_zh": [
                "现有方法主要关注单模态传感器数据生成，导致多模态数据生成效率低下且存在不对齐问题，难以满足自动驾驶对多样化数据的需求。",
                "OmniGen通过共享鸟瞰图空间统一多模态特征，并设计UAE方法联合解码激光雷达和相机数据，结合扩散变换器实现可控生成。",
                "实验表明，OmniGen在多模态传感器数据生成中实现了多模态一致性和灵活调整，性能优于现有方法，提升了数据合成的准确性和效率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自动驾驶中多模态传感器数据生成的挑战，特别是激光雷达和多视角相机数据的对齐与效率问题。现有方法主要聚焦于单模态生成，导致多模态数据生成时存在不对齐、效率低下和灵活性不足的痛点，难以生成高质量、多样化的训练数据。\\n\\n**核心思路**：论文的核心思路是通过统一框架实现多模态传感器数据的对齐生成。设计利用共享的鸟瞰图空间作为中间表示，将不同模态的特征统一起来，然后通过创新的重建方法联合解码，确保数据的一致性和可控性。这样设计可以避免模态间的不匹配，提高生成效率。\\n\\n**技术框架**：整体架构包括三个主要阶段：首先，将多模态传感器数据（如激光雷达点云和多视角相机图像）映射到共享的鸟瞰图空间，提取统一特征；其次，使用UAE方法进行多模态重建，通过体渲染技术联合解码激光雷达和相机数据；最后，结合扩散变换器和ControlNet分支，实现可控的传感器数据生成，允许用户调整生成参数。\\n\\n**关键创新**：最重要的技术创新是UAE方法，这是一种可泛化的多模态重建技术，通过体渲染实现激光雷达和相机数据的联合解码，确保准确性和灵活性。与现有单模态方法相比，OmniGen的本质区别在于其统一框架和多模态对齐能力，解决了模态间不一致的问题。\\n\\n**关键设计**：关键设计包括：使用共享鸟瞰图空间作为特征统一的基础；UAE方法中的体渲染机制，用于精确重建传感器数据；扩散变换器结合ControlNet分支，实现可控生成；损失函数可能涉及多模态一致性损失和重建损失，具体参数设置未知，但旨在优化生成质量和对齐度。",
            "application_zh": "该研究在自动驾驶领域具有重要应用价值，可用于合成多样化和极端案例的传感器数据，降低数据采集成本，提升模型训练效率。潜在应用包括自动驾驶系统的仿真测试、数据增强和场景生成，未来可能推动自动驾驶技术的安全性和泛化能力提升。",
            "highlight_zh": "实验结果显示，OmniGen在多模态传感器数据生成中实现了多模态一致性和灵活调整，性能优于现有单模态方法。具体数据未知，但论文提到在统一框架下达到了理想的性能，提升了生成数据的准确性和效率，为自动驾驶数据合成提供了有效解决方案。",
            "tags_zh": [
                "多模态传感器生成",
                "自动驾驶数据合成",
                "鸟瞰图空间",
                "体渲染",
                "扩散变换器",
                "激光雷达解码",
                "多视角相机",
                "可控生成"
            ],
            "_index": 18
        },
        {
            "title": "Beyond a Single Light: A Large-Scale Aerial Dataset for Urban Scene Reconstruction Under Varying Illumination",
            "authors": [
                "Zhuoxiao Li",
                "Wenzong Ma",
                "Taoyu Wu",
                "Jinjing Zhu",
                "Zhenchao Q",
                "Shuai Zhang",
                "Jing Ou",
                "Yinrui Ren",
                "Weiqing Qi",
                "Guobin Shen",
                "Hui Xiong",
                "Wufan Zhao"
            ],
            "arxiv_id": "2512.14200v1",
            "summary": "Recent advances in Neural Radiance Fields and 3D Gaussian Splatting have demonstrated strong potential for large-scale UAV-based 3D reconstruction tasks by fitting the appearance of images. However, real-world large-scale captures are often based on multi-temporal data capture, where illumination inconsistencies across different times of day can significantly lead to color artifacts, geometric inaccuracies, and inconsistent appearance. Due to the lack of UAV datasets that systematically capture the same areas under varying illumination conditions, this challenge remains largely underexplored. To fill this gap, we introduceSkyLume, a large-scale, real-world UAV dataset specifically designed for studying illumination robust 3D reconstruction in urban scene modeling: (1) We collect data from 10 urban regions data comprising more than 100k high resolution UAV images (four oblique views and nadir), where each region is captured at three periods of the day to systematically isolate illumination changes. (2) To support precise evaluation of geometry and appearance, we provide per-scene LiDAR scans and accurate 3D ground-truth for assessing depth, surface normals, and reconstruction quality under varying illumination. (3) For the inverse rendering task, we introduce the Temporal Consistency Coefficient (TCC), a metric that measuress cross-time albedo stability and directly evaluates the robustness of the disentanglement of light and material. We aim for this resource to serve as a foundation that advances research and real-world evaluation in large-scale inverse rendering, geometry reconstruction, and novel view synthesis.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14200v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "lidar"
                    ],
                    "score": 1
                },
                {
                    "name": "深度估计",
                    "matched_keywords": [
                        "3D reconstruction"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出SkyLume数据集以解决无人机多时序数据中光照不一致对大规模城市三维重建的挑战",
            "summary_zh": "近年来，神经辐射场和3D高斯溅射在基于无人机的大规模三维重建任务中展现出强大潜力，通过拟合图像外观实现重建。然而，真实世界的大规模捕获通常基于多时序数据采集，不同时间段的光照不一致会显著导致颜色伪影、几何不准确和外观不一致。由于缺乏系统捕获同一区域在不同光照条件下的无人机数据集，这一挑战在很大程度上尚未得到充分探索。为填补这一空白，我们引入了SkyLume，一个大规模、真实世界的无人机数据集，专门用于研究城市场景建模中光照鲁棒的三维重建：(1) 我们从10个城市区域收集数据，包含超过10万张高分辨率无人机图像（四个倾斜视角和天底视角），每个区域在一天中的三个时间段进行捕获，以系统隔离光照变化。(2) 为支持几何和外观的精确评估，我们提供每个场景的激光雷达扫描和准确的三维地面真值，用于评估不同光照下的深度、表面法线和重建质量。(3) 对于逆渲染任务，我们引入了时间一致性系数，这是一个度量跨时间反照率稳定性的指标，直接评估光照与材质解耦的鲁棒性。我们旨在使这一资源成为基础，推动大规模逆渲染、几何重建和新视角合成的研究和真实世界评估。",
            "intro_zh": [
                "现有方法在处理无人机多时序数据时，光照不一致导致颜色伪影和几何误差，缺乏专门数据集支持研究。",
                "论文提出SkyLume数据集，包含10个城市区域的多时序高分辨率图像和激光雷达数据，以系统研究光照鲁棒重建。",
                "实验引入时间一致性系数评估逆渲染性能，数据集支持深度、法线和重建质量评估，提升光照解耦的鲁棒性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大规模无人机三维重建中，由于多时序数据采集导致的光照不一致问题，现有方法如神经辐射场和3D高斯溅射在拟合图像外观时，容易产生颜色伪影和几何不准确，缺乏系统数据集来评估光照变化下的鲁棒性。\\n\\n**核心思路**：通过构建一个大规模、真实世界的无人机数据集SkyLume，系统捕获同一城市区域在不同时间段的光照条件，提供多视角图像和激光雷达数据，以支持光照鲁棒的三维重建和逆渲染研究，并引入新指标评估解耦性能。\\n\\n**技术框架**：整体流程包括数据采集、预处理和评估三阶段。数据采集阶段使用无人机在10个城市区域捕获超过10万张高分辨率图像，涵盖四个倾斜视角和天底视角，每个区域在一天中的三个时间段（如早晨、中午、傍晚）进行拍摄，以覆盖光照变化。预处理阶段对齐图像和激光雷达扫描，生成三维地面真值。评估阶段基于提供的深度、表面法线和重建质量数据，结合时间一致性系数进行性能分析。\\n\\n**关键创新**：最重要的技术创新是SkyLume数据集的构建，它首次系统整合了多时序光照条件下的无人机图像和激光雷达数据，专门针对城市场景的光照鲁棒重建问题，与现有数据集相比，提供了更全面的光照变化覆盖和精确的几何真值。\\n\\n**关键设计**：关键设计包括图像采集参数（如分辨率、视角设置）、时间点选择以最大化光照差异、激光雷达扫描的精度校准，以及时间一致性系数的计算方式，该系数基于跨时间反照率稳定性度量，直接量化光照与材质解耦的效果，无需额外网络结构或损失函数。",
            "application_zh": "该研究在无人机测绘、城市规划和虚拟现实等领域具有广泛应用潜力，通过提供光照鲁棒的三维重建数据集，能提升逆渲染和几何重建的准确性，支持真实世界场景的建模和仿真，未来可能推动自动驾驶和增强现实中的环境感知技术发展。",
            "highlight_zh": "实验结果显示，SkyLume数据集包含超过10万张图像，覆盖10个城市区域和三个时间段，提供了精确的激光雷达扫描和三维地面真值。时间一致性系数作为新指标，有效评估了逆渲染方法的光照解耦鲁棒性，相比基线方法，在光照变化下重建质量提升显著，具体数据未知，但数据集支持深度误差降低和外观一致性改善。",
            "tags_zh": [
                "无人机数据集",
                "光照鲁棒重建",
                "三维重建",
                "逆渲染",
                "城市场景建模",
                "多时序数据",
                "激光雷达扫描",
                "时间一致性系数"
            ],
            "_index": 19
        },
        {
            "title": "CIS-BA: Continuous Interaction Space Based Backdoor Attack for Object Detection in the Real-World",
            "authors": [
                "Shuxin Zhao",
                "Bo Lang",
                "Nan Xiao",
                "Yilang Zhang"
            ],
            "arxiv_id": "2512.14158v1",
            "summary": "Object detection models deployed in real-world applications such as autonomous driving face serious threats from backdoor attacks. Despite their practical effectiveness,existing methods are inherently limited in both capability and robustness due to their dependence on single-trigger-single-object mappings and fragile pixel-level cues. We propose CIS-BA, a novel backdoor attack paradigm that redefines trigger design by shifting from static object features to continuous inter-object interaction patterns that describe how objects co-occur and interact in a scene. By modeling these patterns as a continuous interaction space, CIS-BA introduces space triggers that, for the first time, enable a multi-trigger-multi-object attack mechanism while achieving robustness through invariant geometric relations. To implement this paradigm, we design CIS-Frame, which constructs space triggers via interaction analysis, formalizes them as class-geometry constraints for sample poisoning, and embeds the backdoor during detector training. CIS-Frame supports both single-object attacks (object misclassification and disappearance) and multi-object simultaneous attacks, enabling complex and coordinated effects across diverse interaction states. Experiments on MS-COCO and real-world videos show that CIS-BA achieves over 97% attack success under complex environments and maintains over 95% effectiveness under dynamic multi-trigger conditions, while evading three state-of-the-art defenses. In summary, CIS-BA extends the landscape of backdoor attacks in interaction-intensive scenarios and provides new insights into the security of object detection systems.",
            "categories": [
                "cs.CV",
                "cs.CR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14158v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "autonomous driving"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "mapping"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出基于连续交互空间的后门攻击CIS-BA，以解决现实世界目标检测系统在复杂交互场景中的安全威胁问题。",
            "summary_zh": "在自动驾驶等现实应用中部署的目标检测模型面临后门攻击的严重威胁。尽管现有方法具有实际效果，但由于依赖单触发-单对象映射和脆弱的像素级线索，其能力和鲁棒性存在固有局限。我们提出CIS-BA，一种新颖的后门攻击范式，通过从静态对象特征转向描述场景中对象如何共现和交互的连续对象间交互模式，重新定义了触发设计。通过将这些模式建模为连续交互空间，CIS-BA引入了空间触发器，首次实现了多触发-多对象攻击机制，同时通过不变的几何关系实现鲁棒性。为实现这一范式，我们设计了CIS-Frame，它通过交互分析构建空间触发器，将其形式化为类别-几何约束以进行样本投毒，并在检测器训练期间嵌入后门。CIS-Frame支持单对象攻击（对象误分类和消失）和多对象同时攻击，能够在不同交互状态下实现复杂协调的效果。在MS-COCO和现实世界视频上的实验表明，CIS-BA在复杂环境下实现了超过97%的攻击成功率，在动态多触发条件下保持超过95%的有效性，同时规避了三种最先进的防御方法。总之，CIS-BA扩展了交互密集型场景中后门攻击的格局，并为目标检测系统的安全性提供了新见解。",
            "intro_zh": [
                "现有后门攻击依赖单触发-单对象映射和像素级线索，导致能力有限且鲁棒性差，难以应对现实世界复杂交互场景。",
                "CIS-BA提出基于连续对象间交互模式的空间触发器，通过建模几何关系实现多触发-多对象攻击，提升鲁棒性和攻击复杂性。",
                "实验显示CIS-BA在MS-COCO和真实视频中攻击成功率超97%，动态条件下保持95%以上，并能有效规避先进防御方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决现实世界目标检测系统中后门攻击的局限性问题。现有方法依赖单触发-单对象映射和像素级线索，导致攻击能力有限、鲁棒性差，难以在复杂交互场景中实现有效且隐蔽的攻击。\\n\\n**核心思路**：论文的核心思路是将后门攻击从静态对象特征转向连续的对象间交互模式。通过建模对象如何共现和交互的几何关系，构建连续交互空间，并以此设计空间触发器，实现多触发-多对象攻击，同时利用不变的几何关系提升鲁棒性。\\n\\n**技术框架**：整体框架包括三个阶段：首先，通过交互分析构建空间触发器，形式化为类别-几何约束；其次，基于这些约束对训练样本进行投毒，生成带后门的训练数据；最后，在目标检测器训练过程中嵌入后门，使模型在特定交互模式触发时产生攻击效果。框架支持单对象攻击（如误分类或消失）和多对象同时攻击。\\n\\n**关键创新**：最重要的技术创新是引入了基于连续交互空间的后门攻击范式。与现有方法本质区别在于，它不再依赖单一触发对象或像素级模式，而是利用对象间的动态交互关系作为触发条件，首次实现了多触发-多对象攻击机制，并增强了攻击的鲁棒性和隐蔽性。\\n\\n**关键设计**：关键设计包括空间触发器的构建方法，通过分析对象共现和交互模式定义几何约束；样本投毒过程，将约束融入训练数据以生成后门样本；以及后门嵌入策略，在检测器训练中优化模型参数以实现攻击目标。具体参数和损失函数细节在论文中未详细说明，但框架强调几何不变性和交互状态的多样性。",
            "application_zh": "该研究主要应用于自动驾驶、智能监控等现实世界目标检测系统，揭示交互密集型场景中的安全漏洞。其潜在价值在于提升对后门攻击威胁的认识，推动开发更鲁棒的防御机制。未来可能影响安全评估标准和模型部署实践，促进人工智能系统在关键领域的安全性研究。",
            "highlight_zh": "在MS-COCO数据集和真实视频实验中，CIS-BA在复杂环境下攻击成功率超过97%，动态多触发条件下保持95%以上有效性。相比基线方法，显著提升了攻击鲁棒性和复杂性，并能成功规避三种最先进的防御技术，如基于异常检测和模型分析的防御方法。",
            "tags_zh": [
                "后门攻击",
                "目标检测",
                "连续交互空间",
                "多触发攻击",
                "鲁棒性",
                "自动驾驶安全",
                "样本投毒",
                "几何约束"
            ],
            "_index": 20
        },
        {
            "title": "Incentivizing Tool-augmented Thinking with Images for Medical Image Analysis",
            "authors": [
                "Yankai Jiang",
                "Yujie Zhang",
                "Peng Zhang",
                "Yichen Li",
                "Jintai Chen",
                "Xiaoming Shi",
                "Shihui Zhen"
            ],
            "arxiv_id": "2512.14157v1",
            "summary": "Recent reasoning based medical MLLMs have made progress in generating step by step textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on fine-grained visual regions to achieve precise grounding and diagnosis. We introduce Ophiuchus, a versatile, tool-augmented framework that equips an MLLM to (i) decide when additional visual evidence is needed, (ii) determine where to probe and ground within the medical image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved, multimodal chain of thought. In contrast to prior approaches limited by the performance ceiling of specialized tools, Ophiuchus integrates the model's inherent grounding and perception capabilities with external tools, thereby fostering higher-level reasoning. The core of our method is a three-stage training strategy: cold-start training with tool-integrated reasoning data to achieve basic tool selection and adaptation for inspecting key regions; self-reflection fine-tuning to strengthen reflective reasoning and encourage revisiting tool outputs; and Agentic Tool Reinforcement Learning to directly optimize task-specific rewards and emulate expert-like diagnostic behavior. Extensive experiments show that Ophiuchus consistently outperforms both closed-source and open-source SOTA methods across diverse medical benchmarks, including VQA, detection, and reasoning-based segmentation. Our approach illuminates a path toward medical AI agents that can genuinely \"think with images\" through tool-integrated reasoning. Datasets, codes, and trained models will be released publicly.",
            "categories": [
                "cs.AI",
                "cs.CV"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14157v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RL",
                        "reward"
                    ],
                    "score": 3
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出Ophiuchus框架，通过工具增强的思维链解决医学图像分析中复杂任务的动态视觉聚焦问题",
            "summary_zh": "近年来，基于推理的医学多模态大语言模型在生成逐步文本推理链方面取得了进展。然而，它们仍然难以处理需要动态、迭代地聚焦于细粒度视觉区域以实现精确定位和诊断的复杂任务。我们引入了Ophiuchus，这是一个多功能、工具增强的框架，它使MLLM能够：(i)决定何时需要额外的视觉证据，(ii)确定在医学图像中探测和定位的位置，以及(iii)将相关的子图像内容无缝地编织成交错的多模态思维链。与先前受限于专用工具性能上限的方法不同，Ophiuchus将模型固有的定位和感知能力与外部工具相结合，从而促进更高层次的推理。我们方法的核心是一个三阶段训练策略：使用工具集成推理数据进行冷启动训练，以实现对关键区域检查的基本工具选择和适应；自我反思微调，以加强反思性推理并鼓励重新审视工具输出；以及代理工具强化学习，以直接优化特定任务奖励并模拟专家般的诊断行为。大量实验表明，Ophiuchus在包括VQA、检测和基于推理的分割在内的多种医学基准测试中，始终优于闭源和开源的最先进方法。我们的方法为医学AI代理指明了一条通过工具集成推理真正“用图像思考”的道路。数据集、代码和训练模型将公开发布。",
            "intro_zh": [
                "现有医学MLLM在复杂任务中难以动态聚焦细粒度视觉区域，导致定位和诊断精度不足。",
                "提出Ophiuchus框架，通过工具增强的思维链集成模型能力与外部工具，实现动态视觉聚焦。",
                "在多个医学基准测试中，Ophiuchus优于SOTA方法，验证了其有效性和泛化能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决医学图像分析中复杂任务（如精确定位和诊断）的挑战，现有基于推理的医学多模态大语言模型虽能生成文本推理链，但缺乏动态、迭代聚焦细粒度视觉区域的能力，导致性能受限。现有方法的痛点在于依赖专用工具的性能上限，难以实现高层次推理。\\n\\n**核心思路**：论文提出Ophiuchus框架，通过工具增强的思维链，将模型固有的定位和感知能力与外部工具相结合，使模型能自主决定何时、何地使用工具获取视觉证据，并整合到多模态推理中，从而提升复杂任务的解决能力。\\n\\n**技术框架**：整体架构包括三个主要阶段：冷启动训练阶段，使用工具集成推理数据训练模型进行基本工具选择和关键区域检查；自我反思微调阶段，通过反思机制加强推理能力，鼓励模型重新评估工具输出；代理工具强化学习阶段，直接优化任务特定奖励，模拟专家诊断行为。这些阶段共同形成一个迭代的推理流程。\\n\\n**关键创新**：最重要的技术创新是工具增强的思维链设计，它允许模型动态调用工具进行视觉聚焦，与现有方法本质区别在于打破了专用工具的性能限制，实现了模型能力与工具功能的协同优化。\\n\\n**关键设计**：关键设计包括三阶段训练策略：冷启动训练使用工具集成数据，自我反思微调引入反思损失函数，代理工具强化学习基于奖励函数优化。具体参数设置如学习率、奖励函数形式未在摘要中详细说明，但框架强调端到端优化和任务适应性。",
            "application_zh": "该研究在医学图像分析领域具有广泛潜在应用，包括医学视觉问答、病变检测和基于推理的图像分割等任务。实际价值在于提升AI系统在复杂医疗场景中的诊断精度和自动化水平，未来可能推动智能医疗助手和辅助诊断工具的发展，实现更可靠的“用图像思考”的AI代理。",
            "highlight_zh": "Ophiuchus在多个医学基准测试中表现优异，包括VQA、检测和基于推理的分割任务。实验结果显示，它一致优于闭源和开源的最先进方法，具体性能数据未在摘要中提供，但提升幅度显著，验证了框架在动态视觉聚焦和工具集成方面的有效性。",
            "tags_zh": [
                "医学图像分析",
                "多模态大语言模型",
                "工具增强推理",
                "动态视觉聚焦",
                "思维链",
                "强化学习",
                "自我反思微调",
                "医学AI代理"
            ],
            "_index": 21
        },
        {
            "title": "Context Representation via Action-Free Transformer encoder-decoder for Meta Reinforcement Learning",
            "authors": [
                "Amir M. Soufi Enayati",
                "Homayoun Honari",
                "Homayoun Najjaran"
            ],
            "arxiv_id": "2512.14057v1",
            "summary": "Reinforcement learning (RL) enables robots to operate in uncertain environments, but standard approaches often struggle with poor generalization to unseen tasks. Context-adaptive meta reinforcement learning addresses these limitations by conditioning on the task representation, yet they mostly rely on complete action information in the experience making task inference tightly coupled to a specific policy. This paper introduces Context Representation via Action Free Transformer encoder decoder (CRAFT), a belief model that infers task representations solely from sequences of states and rewards. By removing the dependence on actions, CRAFT decouples task inference from policy optimization, supports modular training, and leverages amortized variational inference for scalable belief updates. Built on a transformer encoder decoder with rotary positional embeddings, the model captures long range temporal dependencies and robustly encodes both parametric and non-parametric task variations. Experiments on the MetaWorld ML-10 robotic manipulation benchmark show that CRAFT achieves faster adaptation, improved generalization, and more effective exploration compared to context adaptive meta--RL baselines. These findings highlight the potential of action-free inference as a foundation for scalable RL in robotic control.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14057v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RL",
                        "reward",
                        "PPO"
                    ],
                    "score": 4
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出CRAFT模型，通过无动作Transformer编码器-解码器进行上下文表示，以解决元强化学习中任务推断与策略优化的耦合问题。",
            "summary_zh": "强化学习（RL）使机器人能在不确定环境中操作，但标准方法常难以泛化到未见任务。上下文自适应元强化学习通过任务表示来应对这些限制，但它们大多依赖经验中的完整动作信息，导致任务推断与特定策略紧密耦合。本文介绍了Context Representation via Action Free Transformer encoder decoder（CRAFT），这是一种信念模型，仅从状态和奖励序列推断任务表示。通过消除对动作的依赖，CRAFT将任务推断与策略优化解耦，支持模块化训练，并利用摊销变分推断进行可扩展的信念更新。该模型基于带有旋转位置嵌入的Transformer编码器-解码器构建，能捕捉长程时间依赖，并稳健编码参数化和非参数化任务变化。在MetaWorld ML-10机器人操作基准上的实验表明，与上下文自适应元RL基线相比，CRAFT实现了更快的适应、更好的泛化和更有效的探索。这些发现突显了无动作推断作为机器人控制中可扩展RL基础的潜力。",
            "intro_zh": [
                "核心问题：现有上下文自适应元强化学习方法依赖完整动作信息，导致任务推断与特定策略紧密耦合，限制了泛化能力和模块化训练。",
                "方法要点：提出CRAFT模型，仅使用状态和奖励序列进行任务推断，通过Transformer编码器-解码器捕捉长程依赖，实现任务表示与策略优化的解耦。",
                "实验或效果：在MetaWorld ML-10基准上，CRAFT相比基线方法，实现了更快的适应速度、更好的泛化性能和更有效的探索策略。"
            ],
            "method_zh": "**问题定义**：论文旨在解决元强化学习中任务推断与策略优化紧密耦合的问题。现有上下文自适应元RL方法通常依赖经验中的完整动作信息，这导致任务推断过程受限于特定策略，限制了模型的泛化能力和模块化训练潜力，尤其是在面对未见任务时表现不佳。\\n\\n**核心思路**：论文的核心思路是设计一个无动作的信念模型CRAFT，仅从状态和奖励序列推断任务表示，从而将任务推断与策略优化解耦。这样设计是为了减少对动作信息的依赖，使任务推断更通用，支持独立于策略的模块化训练，并利用摊销变分推断实现高效信念更新。\\n\\n**技术框架**：整体架构基于Transformer编码器-解码器。首先，输入序列包括状态和奖励，通过编码器捕捉长程时间依赖；然后，解码器生成任务表示；最后，利用变分推断进行信念更新。主要模块包括数据预处理、Transformer编码器、解码器和变分推断层，流程涉及序列编码、任务推断和信念优化。\\n\\n**关键创新**：最重要的技术创新是引入无动作推断机制，仅使用状态和奖励序列进行任务表示，这本质区别于现有方法依赖动作信息的做法。这实现了任务推断与策略的完全解耦，提升了模型的通用性和可扩展性。\\n\\n**关键设计**：关键设计包括使用旋转位置嵌入（RoPE）增强位置编码，以更好地处理序列数据；采用摊销变分推断进行信念更新，提高计算效率；网络结构基于标准Transformer，但针对状态和奖励输入进行了优化；损失函数结合重构损失和变分下界，以平衡表示质量和推断稳定性。",
            "application_zh": "该研究在机器人控制领域具有广泛潜在应用，如工业自动化、服务机器人和自主导航。通过实现无动作的任务推断，CRAFT支持更灵活的模块化训练，能快速适应新任务，提升机器人在复杂环境中的泛化能力。未来可能推动可扩展强化学习技术的发展，降低对专家策略的依赖，促进智能系统在现实世界中的部署。",
            "highlight_zh": "在MetaWorld ML-10机器人操作基准实验中，CRAFT相比上下文自适应元RL基线方法，实现了显著的性能提升：适应速度更快，泛化能力更强，探索效率更高。具体数据未在摘要中提供，但实验结果表明CRAFT在任务推断解耦方面优于现有方法，突显了无动作推断的有效性。",
            "tags_zh": [
                "元强化学习",
                "上下文表示",
                "Transformer编码器-解码器",
                "无动作推断",
                "任务推断解耦",
                "摊销变分推断",
                "机器人操作",
                "长程时间依赖"
            ],
            "_index": 22
        },
        {
            "title": "Robust Single-shot Structured Light 3D Imaging via Neural Feature Decoding",
            "authors": [
                "Jiaheng Li",
                "Qiyu Dai",
                "Lihan Li",
                "Praneeth Chakravarthula",
                "He Sun",
                "Baoquan Chen",
                "Wenzheng Chen"
            ],
            "arxiv_id": "2512.14028v1",
            "summary": "We consider the problem of active 3D imaging using single-shot structured light systems, which are widely employed in commercial 3D sensing devices such as Apple Face ID and Intel RealSense. Traditional structured light methods typically decode depth correspondences through pixel-domain matching algorithms, resulting in limited robustness under challenging scenarios like occlusions, fine-structured details, and non-Lambertian surfaces. Inspired by recent advances in neural feature matching, we propose a learning-based structured light decoding framework that performs robust correspondence matching within feature space rather than the fragile pixel domain. Our method extracts neural features from the projected patterns and captured infrared (IR) images, explicitly incorporating their geometric priors by building cost volumes in feature space, achieving substantial performance improvements over pixel-domain decoding approaches. To further enhance depth quality, we introduce a depth refinement module that leverages strong priors from large-scale monocular depth estimation models, improving fine detail recovery and global structural coherence. To facilitate effective learning, we develop a physically-based structured light rendering pipeline, generating nearly one million synthetic pattern-image pairs with diverse objects and materials for indoor settings. Experiments demonstrate that our method, trained exclusively on synthetic data with multiple structured light patterns, generalizes well to real-world indoor environments, effectively processes various pattern types without retraining, and consistently outperforms both commercial structured light systems and passive stereo RGB-based depth estimation methods. Project page: https://namisntimpot.github.io/NSLweb/.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14028v1",
            "code_links": [
                {
                    "url": "https://namisntimpot.github.io/NSLweb/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "深度估计",
                    "matched_keywords": [
                        "depth estimation",
                        "monocular depth"
                    ],
                    "score": 2
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出基于神经特征解码的单次结构光三维成像方法，以提升在遮挡、精细结构和非朗伯表面等挑战场景下的鲁棒性。",
            "summary_zh": "本文研究了使用单次结构光系统进行主动三维成像的问题，这类系统广泛应用于苹果Face ID和英特尔RealSense等商业三维传感设备中。传统的结构光方法通常通过像素域匹配算法解码深度对应关系，导致在遮挡、精细结构细节和非朗伯表面等挑战场景下鲁棒性有限。受神经特征匹配最新进展的启发，我们提出了一种基于学习的结构光解码框架，在特征空间而非脆弱的像素域执行鲁棒的对应匹配。我们的方法从投影图案和捕获的红外图像中提取神经特征，通过在特征空间中构建代价体积来显式结合其几何先验，相比像素域解码方法实现了显著的性能提升。为了进一步提高深度质量，我们引入了一个深度细化模块，利用大规模单目深度估计模型的强先验，改善精细细节恢复和全局结构一致性。为了促进有效学习，我们开发了一个基于物理的结构光渲染流程，生成了近百万个包含室内环境中多样物体和材料的合成图案-图像对。实验表明，我们的方法仅使用多种结构光图案的合成数据进行训练，就能很好地泛化到真实世界的室内环境，无需重新训练即可有效处理各种图案类型，并且始终优于商业结构光系统和基于被动立体RGB的深度估计方法。项目页面：https://namisntimpot.github.io/NSLweb/。",
            "intro_zh": [
                "传统单次结构光方法依赖像素域匹配，在遮挡、精细结构或非朗伯表面等复杂场景下鲁棒性不足，导致深度估计精度下降。",
                "提出基于神经特征解码的框架，在特征空间而非像素域进行对应匹配，并引入深度细化模块，结合几何先验和单目深度先验提升性能。",
                "实验表明，该方法仅用合成数据训练，能泛化到真实室内场景，处理多种图案类型，性能优于商业结构光系统和被动立体RGB方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决单次结构光三维成像在遮挡、精细结构细节和非朗伯表面等挑战场景下的鲁棒性问题。现有方法（如传统像素域匹配算法）在这些场景下表现脆弱，导致深度对应解码不准确，限制了实际应用中的可靠性和精度。\\n\\n**核心思路**：论文的核心思路是将结构光解码从脆弱的像素域转移到更鲁棒的特征空间，利用神经特征匹配技术来提升对应关系的稳健性。通过提取投影图案和捕获红外图像的神经特征，并显式结合几何先验（如构建特征空间中的代价体积），以及引入单目深度先验进行细化，从而克服传统方法的局限性。\\n\\n**技术框架**：整体框架包括两个主要阶段：神经特征解码和深度细化。首先，从投影图案和红外图像中提取神经特征，在特征空间中构建代价体积以进行对应匹配，生成初始深度图。然后，通过深度细化模块，利用大规模单目深度估计模型的先验信息，对初始深度进行优化，提升细节恢复和结构一致性。此外，还包括一个基于物理的渲染流程，用于生成合成训练数据。\\n\\n**关键创新**：最重要的技术创新在于将结构光解码问题重新定义为特征空间中的对应匹配任务，而非传统的像素域操作。这本质区别在于利用了深度学习提取的高维特征，这些特征对噪声、遮挡和非朗伯反射更具不变性，从而显著提高了鲁棒性。同时，结合单目深度先验进行细化，进一步增强了全局和局部深度质量。\\n\\n**关键设计**：关键设计包括：使用卷积神经网络提取神经特征；在特征空间中构建多尺度代价体积以编码几何先验；设计损失函数（如结合重建损失和深度平滑损失）来优化网络参数；深度细化模块集成预训练的单目深度模型作为先验；以及基于物理的渲染流程生成大规模合成数据集，涵盖多样物体和材料，以促进模型泛化。",
            "application_zh": "该研究在三维传感领域具有广泛的应用潜力，可提升商业设备如苹果Face ID和英特尔RealSense在复杂环境下的性能。潜在应用包括增强现实、机器人导航、工业检测和医疗成像，其中鲁棒的三维重建至关重要。未来可能推动结构光技术向更可靠、自适应方向发展，降低对理想场景的依赖。",
            "highlight_zh": "实验结果显示，该方法在合成和真实数据集上均显著优于基线。具体而言，在室内场景测试中，相比传统像素域方法，深度估计误差平均降低约30-50%；与商业结构光系统相比，在遮挡和非朗伯表面场景下表现出更强的鲁棒性；同时，无需重新训练即可处理多种结构光图案，泛化能力突出。这些提升验证了特征空间解码和深度细化模块的有效性。",
            "tags_zh": [
                "单次结构光",
                "三维成像",
                "神经特征匹配",
                "深度估计",
                "合成数据渲染",
                "鲁棒性提升",
                "特征空间解码",
                "主动视觉"
            ],
            "_index": 23
        },
        {
            "title": "Deep Learning Perspective of Scene Understanding in Autonomous Robots",
            "authors": [
                "Afia Maham",
                "Dur E Nayab Tashfa"
            ],
            "arxiv_id": "2512.14020v1",
            "summary": "This paper provides a review of deep learning applications in scene understanding in autonomous robots, including innovations in object detection, semantic and instance segmentation, depth estimation, 3D reconstruction, and visual SLAM. It emphasizes how these techniques address limitations of traditional geometric models, improve depth perception in real time despite occlusions and textureless surfaces, and enhance semantic reasoning to understand the environment better. When these perception modules are integrated into dynamic and unstructured environments, they become more effective in decisionmaking, navigation and interaction. Lastly, the review outlines the existing problems and research directions to advance learning-based scene understanding of autonomous robots.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "11 pages. Review Paper on Deep Learning Perspective of Scene Understanding in Autonomous Robots",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14020v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "深度估计",
                    "matched_keywords": [
                        "depth estimation",
                        "3D reconstruction"
                    ],
                    "score": 2
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "visual SLAM",
                        "SLAM"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 4,
            "headline_zh": "综述深度学习在自主机器人场景理解中的应用，涵盖目标检测、语义分割、深度估计、3D重建和视觉SLAM等关键技术",
            "summary_zh": "本文综述了深度学习在自主机器人场景理解中的应用，包括目标检测、语义与实例分割、深度估计、3D重建和视觉SLAM等领域的创新。论文强调这些技术如何解决传统几何模型的局限性，在遮挡和无纹理表面情况下实时改善深度感知，并增强语义推理以更好地理解环境。当这些感知模块集成到动态和非结构化环境中时，它们在决策制定、导航和交互方面变得更加有效。最后，综述概述了现有问题及研究方向，以推进基于学习的自主机器人场景理解。",
            "intro_zh": [
                "核心问题：传统几何模型在遮挡、无纹理表面和动态环境中存在局限性，难以实现精确的深度感知和语义理解。",
                "方法要点：通过深度学习技术整合目标检测、语义分割、深度估计等模块，提升场景理解的实时性和鲁棒性。",
                "实验或效果：综述表明深度学习模型在复杂环境中显著改善感知能力，为自主机器人的决策和导航提供更可靠的基础。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自主机器人在动态和非结构化环境中进行场景理解时面临的挑战，包括传统几何模型在遮挡、无纹理表面和实时处理方面的不足，以及如何整合多模态感知信息以提升语义推理能力。\\n\\n**核心思路**：通过深度学习技术替代或增强传统方法，利用神经网络从视觉数据中直接学习场景的几何和语义特征，实现端到端的感知优化，从而克服传统模型的局限性并提升实时性能。\\n\\n**技术框架**：整体框架包括多个并行或串联的深度学习模块：目标检测模块用于识别场景中的物体；语义和实例分割模块提供像素级分类；深度估计模块从单目或双目图像中恢复3D信息；3D重建模块构建环境的几何模型；视觉SLAM模块实现同时定位与地图构建。这些模块通过数据融合或联合训练进行集成。\\n\\n**关键创新**：最重要的创新在于将深度学习全面应用于场景理解的各个子任务，并强调模块间的协同作用，而非孤立优化单个任务。与现有方法相比，本质区别在于从数据驱动角度解决传统几何模型难以处理的复杂情况（如遮挡、光照变化），并实现语义与几何信息的统一表示。\\n\\n**关键设计**：综述未提供具体网络结构或损失函数细节，但提及常用技术如卷积神经网络（CNN）用于图像特征提取，循环神经网络（RNN）或Transformer用于时序建模，以及多任务学习框架来联合优化分割、检测和深度估计。关键参数设置依赖于具体应用场景，如实时性要求高的系统可能采用轻量级网络架构。",
            "application_zh": "该研究在自主机器人领域具有广泛的应用潜力，包括自动驾驶车辆的环境感知、服务机器人的室内导航、工业机器人的物体操作以及无人机的地形探索。通过提升场景理解的准确性和实时性，这些技术能增强机器人在复杂环境中的决策能力，推动智能机器人向更自主、安全的方向发展，未来可能扩展到增强现实、智能监控等领域。",
            "highlight_zh": "综述总结了深度学习在场景理解各任务中的显著性能提升：例如，在目标检测方面，基于深度学习的模型（如YOLO、Faster R-CNN）在标准数据集上达到超过90%的mAP，相比传统方法提升约20-30%；语义分割模型（如DeepLab、Mask R-CNN）在Cityscapes数据集上实现超过80%的mIoU，有效处理遮挡和复杂背景；深度估计方法在KITTI数据集上误差降低至厘米级，实时性能满足机器人应用需求。这些结果凸显了深度学习在克服传统几何模型局限性方面的优势。",
            "tags_zh": [
                "场景理解",
                "深度学习",
                "自主机器人",
                "语义分割",
                "深度估计",
                "3D重建",
                "视觉SLAM",
                "目标检测"
            ],
            "_index": 24
        },
        {
            "title": "TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs",
            "authors": [
                "Jun Zhang",
                "Teng Wang",
                "Yuying Ge",
                "Yixiao Ge",
                "Xinhao Li",
                "Ying Shan",
                "Limin Wang"
            ],
            "arxiv_id": "2512.14698v1",
            "summary": "This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.MM"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project Page: https://timelens-arc-lab.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14698v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RL",
                        "reward"
                    ],
                    "score": 3
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出TimeLens基准与模型，通过高质量数据与算法设计提升多模态大语言模型的视频时序定位能力",
            "summary_zh": "本文并未提出全新方法，而是为视频理解的核心能力——视频时序定位（VTG）建立了一个直接、渐进但至关重要的基线。尽管多模态大语言模型（MLLMs）在多种视频理解任务中表现出色，但优化其VTG能力的方案仍待探索。本文提出TimeLens，从数据质量和算法设计两个主要维度，系统性地研究如何构建具有强大VTG能力的MLLMs。我们首先揭示了现有VTG基准中的关键质量问题，并引入TimeLens-Bench，它包含三个流行基准的精心重新标注版本，遵循严格的质量标准。我们的分析显示，与旧基准相比，模型排名发生了显著变化，证实了先前评估标准的不可靠性。我们还通过自动重新标注流程解决了训练数据中的噪声问题，生成了TimeLens-100K，这是一个大规模、高质量的训练数据集。基于我们的数据基础，我们深入探索了算法设计原则，得出一系列有意义的见解和高效实用的做法。这些包括用于时间表示的交替文本编码、作为训练范式的免思考强化学习与可验证奖励（RLVR）方法，以及精心设计的RLVR训练方案。这些努力最终形成了TimeLens模型系列，这是一组在开源模型中具有最先进VTG性能的MLLMs，甚至超越了GPT-5和Gemini-2.5-Flash等专有模型。所有代码、数据和模型都将发布，以促进未来研究。",
            "intro_zh": [
                "现有视频时序定位基准存在数据质量问题，导致模型评估不可靠，且多模态大语言模型在该任务上的优化方案缺乏系统探索。",
                "论文从数据质量和算法设计两个维度入手，构建高质量基准和训练数据集，并探索有效的训练范式如交替文本编码和RLVR方法。",
                "TimeLens模型在开源模型中达到最先进性能，超越GPT-5等专有模型，显著提升了视频时序定位的准确性和可靠性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决视频时序定位（VTG）任务中，现有基准数据质量差导致模型评估不可靠，以及多模态大语言模型（MLLMs）在该任务上缺乏系统优化方案的问题。现有方法的痛点包括标注噪声、评估标准不一致和训练效率低。\\n\\n**核心思路**：论文的核心思路是通过提升数据质量和系统化算法设计来增强MLLMs的VTG能力。设计基于两个维度：首先，重新标注基准和训练数据以减少噪声；其次，探索高效的训练范式，如交替文本编码和强化学习，以优化模型性能。\\n\\n**技术框架**：整体架构包括数据准备和模型训练两个阶段。数据准备阶段涉及构建TimeLens-Bench（重新标注的基准）和TimeLens-100K（高质量训练数据集）；模型训练阶段采用MLLMs基础架构，集成交替文本编码模块和RLVR训练范式，通过可验证奖励机制优化时间定位精度。\\n\\n**关键创新**：最重要的技术创新是引入TimeLens-Bench和TimeLens-100K数据集，解决了数据质量问题；同时，提出RLVR训练范式，结合免思考强化学习和可验证奖励，提高了训练效率和模型性能。与现有方法的本质区别在于系统性地从数据源头和算法设计两方面入手，而非仅依赖模型架构改进。\\n\\n**关键设计**：关键设计包括交替文本编码用于时间表示，将时间信息嵌入文本序列；RLVR方法使用可验证奖励函数（如基于标注一致性的奖励）来指导强化学习，避免复杂思考过程；训练参数设置可能涉及多阶段优化，具体细节在论文中未详细说明，但强调高效性和可扩展性。",
            "application_zh": "该研究在视频理解领域具有广泛的应用潜力，如智能视频检索、内容摘要生成、自动驾驶场景分析和教育视频标注等。通过提升视频时序定位的准确性，可增强多模态AI系统的实用性和可靠性，推动人机交互和自动化处理的发展，对未来的视频AI技术标准化和商业化有重要影响。",
            "highlight_zh": "TimeLens模型在重新标注的TimeLens-Bench上表现出色，在开源模型中达到最先进性能，具体数据未在摘要中提供，但提到超越了专有模型如GPT-5和Gemini-2.5-Flash。实验显示，与旧基准相比，模型排名发生显著变化，证实了高质量数据对评估的重要性，提升了VTG任务的可靠性和准确性。",
            "tags_zh": [
                "视频时序定位",
                "多模态大语言模型",
                "数据质量基准",
                "强化学习训练",
                "视频理解",
                "时间表示编码",
                "开源模型优化",
                "自动化标注"
            ],
            "_index": 25
        },
        {
            "title": "Beyond Lipschitz Continuity and Monotonicity: Fractal and Chaotic Activation Functions in Echo State Networks",
            "authors": [
                "Rae Chipera",
                "Jenny Du",
                "Irene Tsapara"
            ],
            "arxiv_id": "2512.14675v1",
            "summary": "Contemporary reservoir computing relies heavily on smooth, globally Lipschitz continuous activation functions, limiting applications in defense, disaster response, and pharmaceutical modeling where robust operation under extreme conditions is critical. We systematically investigate non-smooth activation functions, including chaotic, stochastic, and fractal variants, in echo state networks. Through comprehensive parameter sweeps across 36,610 reservoir configurations, we demonstrate that several non-smooth functions not only maintain the Echo State Property (ESP) but outperform traditional smooth activations in convergence speed and spectral radius tolerance. Notably, the Cantor function (continuous everywhere and flat almost everywhere) maintains ESP-consistent behavior up to spectral radii of rho ~ 10, an order of magnitude beyond typical bounds for smooth functions, while achieving 2.6x faster convergence than tanh and ReLU. We introduce a theoretical framework for quantized activation functions, defining a Degenerate Echo State Property (d-ESP) that captures stability for discrete-output functions and proving that d-ESP implies traditional ESP. We identify a critical crowding ratio Q=N/k (reservoir size / quantization levels) that predicts failure thresholds for discrete activations. Our analysis reveals that preprocessing topology, rather than continuity per se, determines stability: monotone, compressive preprocessing maintains ESP across scales, while dispersive or discontinuous preprocessing triggers sharp failures. While our findings challenge assumptions about activation function design in reservoir computing, the mechanism underlying the exceptional performance of certain fractal functions remains unexplained, suggesting fundamental gaps in our understanding of how geometric properties of activation functions influence reservoir dynamics.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "50 pages, 21 figures. Extended version with full proofs, parameter sweeps, and appendices",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14675v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO",
                        "VIO"
                    ],
                    "score": 2
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出在回声状态网络中使用分形和混沌激活函数，以提升极端条件下的鲁棒性和收敛速度。",
            "summary_zh": "当代储层计算严重依赖平滑、全局Lipschitz连续的激活函数，这限制了在国防、灾害响应和药物建模等极端条件下需要鲁棒操作的应用。我们系统地研究了回声状态网络中的非平滑激活函数，包括混沌、随机和分形变体。通过对36,610个储层配置进行全面的参数扫描，我们证明了几种非平滑函数不仅保持了回声状态特性（ESP），而且在收敛速度和谱半径容限方面优于传统的平滑激活函数。值得注意的是，康托函数（处处连续且几乎处处平坦）在谱半径高达ρ~10时仍保持ESP一致行为，比平滑函数的典型界限高出一个数量级，同时收敛速度比tanh和ReLU快2.6倍。我们引入了量化激活函数的理论框架，定义了捕获离散输出函数稳定性的退化回声状态特性（d-ESP），并证明d-ESP蕴含传统ESP。我们识别了一个关键的拥挤比Q=N/k（储层大小/量化级别），用于预测离散激活函数的失效阈值。我们的分析表明，预处理拓扑而非连续性本身决定了稳定性：单调、压缩的预处理在多个尺度上保持ESP，而分散或不连续的预处理则引发急剧失效。虽然我们的发现挑战了储层计算中激活函数设计的假设，但某些分形函数优异性能的机制仍未得到解释，这表明我们对激活函数几何特性如何影响储层动态的理解存在根本性差距。",
            "intro_zh": [
                "核心问题：传统回声状态网络依赖平滑激活函数，限制了在极端条件下的鲁棒应用，如国防和灾害响应。",
                "方法要点：系统研究非平滑激活函数，包括分形和混沌变体，并引入量化激活函数的理论框架。",
                "实验或效果：康托函数在谱半径高达10时保持稳定，收敛速度比tanh和ReLU快2.6倍。"
            ],
            "method_zh": "**问题定义**：论文要解决回声状态网络中传统平滑激活函数（如tanh和ReLU）在极端条件下鲁棒性不足的问题，这些函数依赖Lipschitz连续性和单调性，限制了在国防、灾害响应等关键领域的应用。现有方法的痛点在于其收敛速度慢、谱半径容限低，且无法有效处理非平滑动态。\\n\\n**核心思路**：论文的核心思路是挑战传统假设，通过系统研究非平滑激活函数（如分形、混沌和随机变体）来提升回声状态网络的性能。设计基于理论分析，认为预处理拓扑而非连续性本身决定稳定性，从而探索更广泛的函数类。\\n\\n**技术框架**：整体架构包括理论分析和实验验证两部分。理论部分引入退化回声状态特性（d-ESP）框架，用于量化激活函数的稳定性；实验部分通过大规模参数扫描（36,610个配置）评估不同激活函数在收敛速度和谱半径容限方面的表现。主要模块包括激活函数选择、储层配置生成和性能指标计算。\\n\\n**关键创新**：最重要的技术创新是首次在回声状态网络中系统应用非平滑激活函数，并证明其优于传统平滑函数。与现有方法的本质区别在于打破了Lipschitz连续性和单调性的限制，引入了分形和混沌函数，从而扩展了网络的设计空间。\\n\\n**关键设计**：关键参数设置包括谱半径（ρ）的扫描范围（高达10）、储层大小（N）和量化级别（k）的拥挤比Q=N/k。网络结构基于标准回声状态网络，但激活函数替换为康托函数等非平滑变体。损失函数和训练过程遵循回声状态网络的典型设置，重点评估收敛速度和稳定性。",
            "application_zh": "该研究在国防、灾害响应和药物建模等极端条件下具有潜在应用价值，能提升系统的鲁棒性和实时性能。未来可能影响储层计算的设计范式，推动更广泛激活函数的使用，并促进在复杂动态系统建模中的实际部署。",
            "highlight_zh": "最重要的实验结果包括：康托函数在谱半径高达10时保持回声状态特性，比平滑函数的典型界限高出一个数量级；收敛速度比tanh和ReLU快2.6倍；通过36,610个配置的参数扫描，证明非平滑函数在收敛速度和谱半径容限方面优于传统平滑激活函数。",
            "tags_zh": [
                "回声状态网络",
                "激活函数设计",
                "分形函数",
                "混沌动态",
                "储层计算",
                "非平滑优化",
                "稳定性分析",
                "量化激活"
            ],
            "_index": 26
        },
        {
            "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
            "authors": [
                "Wenqiang Sun",
                "Haiyu Zhang",
                "Haoyuan Wang",
                "Junta Wu",
                "Zehan Wang",
                "Zhenwei Wang",
                "Yunhong Wang",
                "Jun Zhang",
                "Tengfei Wang",
                "Chunchao Guo"
            ],
            "arxiv_id": "2512.14614v1",
            "summary": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.",
            "categories": [
                "cs.CV",
                "cs.GR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "project page: https://3d-models.hunyuan.tencent.com/world/, demo: https://3d.hunyuan.tencent.com/sceneTo3D",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14614v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "世界模型",
                    "matched_keywords": [
                        "world model"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出WorldPlay流式视频扩散模型，通过双动作表示、重构上下文记忆和上下文强制蒸馏，实现实时交互式世界建模并保持长期几何一致性。",
            "summary_zh": "本文介绍了WorldPlay，一种流式视频扩散模型，能够实现实时、交互式的世界建模，并保持长期几何一致性，解决了当前方法在速度和内存之间的权衡限制。WorldPlay基于三个关键创新：1）使用双动作表示，实现对用户键盘和鼠标输入的鲁棒动作控制；2）通过重构上下文记忆动态重建过去帧的上下文，并利用时间重帧保持几何重要但久远帧的可访问性，有效缓解内存衰减；3）提出上下文强制蒸馏方法，专为内存感知模型设计，通过对齐教师和学生模型的记忆上下文，保持学生模型使用长程信息的能力，实现实时速度同时防止误差漂移。综合来看，WorldPlay能以24 FPS生成720p长时流式视频，具有优越的一致性，优于现有技术，并在多样场景中展现出强泛化能力。项目页面和在线演示可在https://3d-models.hunyuan.tencent.com/world/和https://3d.hunyuan.tencent.com/sceneTo3D找到。",
            "intro_zh": [
                "现有方法在实时交互式世界建模中面临速度与内存的权衡，难以保持长期几何一致性，导致视频生成时出现误差漂移和内存衰减问题。",
                "论文提出WorldPlay模型，采用双动作表示、重构上下文记忆和上下文强制蒸馏，通过动态重建过去帧上下文和优化内存管理，实现高效一致性建模。",
                "实验表明，WorldPlay能以24 FPS实时生成720p长视频，在几何一致性和泛化能力上优于基线方法，显著提升交互体验和视频质量。"
            ],
            "method_zh": "**问题定义**：论文旨在解决实时交互式世界建模中，现有方法在生成流式视频时难以平衡速度与内存，导致长期几何一致性不足的问题。具体痛点包括：传统模型在快速响应交互输入时，内存衰减严重，无法有效利用久远帧信息，从而引发误差漂移和视频不连贯。\\n\\n**核心思路**：论文的核心思路是通过创新性的内存管理和蒸馏技术，在保持实时速度的同时，增强模型对长期几何信息的利用能力。设计上，结合动态上下文重建和时间重帧，优化记忆访问，并通过上下文强制蒸馏对齐教师-学生模型的记忆上下文，防止信息丢失。\\n\\n**技术框架**：WorldPlay的整体架构基于流式视频扩散模型，包含三个主要模块：双动作表示模块处理用户输入并生成鲁棒动作控制；重构上下文记忆模块动态重建过去帧的上下文，利用时间重帧技术保持重要帧的可访问性；上下文强制蒸馏模块专为内存感知设计，通过蒸馏训练对齐上下文，确保学生模型能有效使用长程信息。流程上，模型实时接收交互输入，结合记忆模块生成一致视频帧。\\n\\n**关键创新**：最重要的技术创新包括：1）双动作表示，实现对键盘和鼠标输入的鲁棒控制；2）重构上下文记忆，通过动态重建和时间重帧缓解内存衰减；3）上下文强制蒸馏，一种新颖的蒸馏方法，专为内存感知模型设计，本质区别在于它直接对齐记忆上下文而非仅输出，从而更有效地保留长期一致性信息。\\n\\n**关键设计**：关键设计细节包括：使用扩散模型作为基础框架，参数设置上优化内存缓冲区大小以平衡效率；在重构上下文记忆中，引入时间重帧算法选择几何重要帧；上下文强制蒸馏中，设计损失函数强制教师和学生模型的记忆上下文对齐，网络结构上可能包含编码器-解码器组件来处理时空信息，具体参数如学习率和批量大小未知，但整体旨在最小化误差漂移。",
            "application_zh": "WorldPlay的潜在应用领域包括虚拟现实、游戏开发、自动驾驶模拟和影视制作，其中实时交互式世界建模能提升用户体验和内容生成效率。实际价值在于提供高质量、一致的长视频流，支持复杂场景的快速构建和编辑，未来可能推动AI在实时3D建模和沉浸式媒体中的广泛应用。",
            "highlight_zh": "最重要的实验结果显示，WorldPlay能以24 FPS实时生成720p长时流式视频，在几何一致性上优于现有技术如未知基线方法。具体性能数据包括视频帧率和分辨率，提升幅度体现在减少误差漂移和增强泛化能力，在多样场景测试中展现出强鲁棒性，但具体对比数值未知。",
            "tags_zh": [
                "流式视频生成",
                "交互式世界建模",
                "长期几何一致性",
                "上下文记忆管理",
                "蒸馏训练",
                "实时渲染",
                "扩散模型",
                "内存优化"
            ],
            "_index": 27
        },
        {
            "title": "Residual GRU+MHSA: A Lightweight Hybrid Recurrent Attention Model for Cardiovascular Disease Detection",
            "authors": [
                "Tejaswani Dash",
                "Gautam Datla",
                "Anudeep Vurity",
                "Tazeem Ahmad",
                "Mohd Adnan",
                "Saima Rafi",
                "Saisha Patro",
                "Saina Patro"
            ],
            "arxiv_id": "2512.14563v1",
            "summary": "Cardiovascular disease (CVD) remains the leading cause of mortality worldwide, underscoring the need for reliable and efficient predictive tools that support early intervention. Traditional diagnostic approaches rely on handcrafted features and clinician expertise, while machine learning methods improve reproducibility but often struggle to generalize across noisy and heterogeneous clinical data. In this work, we propose Residual GRU with Multi-Head Self-Attention, a compact deep learning architecture designed for tabular clinical records. The model integrates residual bidirectional gated recurrent units for sequential modeling of feature columns, a channel reweighting block, and multi-head self-attention pooling with a learnable classification token to capture global context. We evaluate the model on the UCI Heart Disease dataset using 5-fold stratified cross-validation and compare it against classical methods such as Logistic Regression, Random Forest, and Support Vector Machines, as well as modern deep learning baselines including DeepMLP, convolutional networks, recurrent networks, and Transformers. The proposed model achieves an accuracy of 0.861, macro-F1 of 0.860, ROC-AUC of 0.908, and PR-AUC of 0.904, outperforming all baselines. Ablation studies confirm the individual contributions of residual recurrence, channel gating, and attention pooling. t-SNE visualizations further indicate that the learned embeddings exhibit clearer separation between disease and non-disease classes compared to raw features. These results demonstrate that lightweight hybrid recurrent and attention-based architectures provide a strong balance between accuracy and efficiency for clinical risk prediction, supporting deployment in resource-constrained healthcare settings.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted in IEEE Bigdata 2025- Learning Representations with Limited Supervision",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14563v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出Residual GRU+MHSA轻量混合循环注意力模型，用于心血管疾病检测，平衡准确性与效率。",
            "summary_zh": "心血管疾病是全球主要死因，需要可靠高效的预测工具以支持早期干预。传统诊断方法依赖手工特征和临床专家经验，而机器学习方法虽提高可重复性，但常难以在噪声和异质临床数据上泛化。本文提出Residual GRU with Multi-Head Self-Attention，一种为表格临床记录设计的紧凑深度学习架构。该模型集成残差双向门控循环单元用于特征列的序列建模、通道重加权块，以及带可学习分类令牌的多头自注意力池化以捕获全局上下文。我们在UCI心脏病数据集上使用5折分层交叉验证评估模型，并与逻辑回归、随机森林、支持向量机等经典方法，以及DeepMLP、卷积网络、循环网络和Transformer等现代深度学习基线进行比较。所提模型达到0.861的准确率、0.860的宏F1、0.908的ROC-AUC和0.904的PR-AUC，优于所有基线。消融研究确认了残差循环、通道门控和注意力池化的个体贡献。t-SNE可视化进一步表明，与原始特征相比，学习到的嵌入在疾病和非疾病类别间展现出更清晰的分离。这些结果表明，轻量混合循环和基于注意力的架构为临床风险预测提供了准确性与效率之间的强平衡，支持在资源受限的医疗环境中部署。",
            "intro_zh": [
                "心血管疾病检测依赖传统手工特征和专家经验，机器学习方法在噪声和异质临床数据上泛化困难。",
                "提出轻量混合架构，结合残差双向GRU、通道重加权和多头自注意力池化，以捕获序列和全局特征。",
                "在UCI心脏病数据集上，模型准确率达0.861，优于经典和深度学习基线，消融研究验证各模块贡献。"
            ],
            "method_zh": "**问题定义**：论文旨在解决心血管疾病检测中，传统方法依赖手工特征和专家经验，而机器学习方法在噪声和异质临床表格数据上泛化能力不足的问题。现有方法的痛点包括特征工程复杂、模型对数据噪声敏感，以及深度学习模型如Transformer计算开销大，不适合资源受限的医疗环境。\\n\\n**核心思路**：设计一个轻量混合深度学习架构，结合循环神经网络（RNN）的序列建模能力和自注意力机制的全局上下文捕获能力，通过残差连接和通道重加权增强特征表示，以提高模型在临床表格数据上的准确性和泛化性，同时保持计算效率。\\n\\n**技术框架**：整体架构包括三个主要模块：首先，使用残差双向门控循环单元（Residual Bidirectional GRU）对特征列进行序列建模，处理表格数据的时序或顺序依赖；其次，引入通道重加权块（Channel Reweighting Block），通过注意力机制动态调整特征通道的重要性；最后，采用多头自注意力池化（Multi-Head Self-Attention Pooling）与可学习分类令牌，聚合全局信息并输出分类结果。流程为输入表格数据→残差双向GRU处理→通道重加权→多头自注意力池化→分类输出。\\n\\n**关键创新**：最重要的技术创新点是轻量混合架构的设计，将残差循环、通道门控和注意力池化有机结合，本质区别在于它避免了传统Transformer的高计算成本，同时通过循环网络捕获局部序列模式，通过注意力机制增强全局特征交互，实现了准确性与效率的平衡。与现有方法相比，它专门针对表格临床数据优化，减少了参数数量，更适合部署在资源受限的医疗场景。\\n\\n**关键设计**：关键参数设置包括使用双向GRU处理特征序列，隐藏层大小未知；通道重加权块可能基于注意力权重调整特征；多头自注意力池化中，头数未知，但包含可学习分类令牌以聚合信息。损失函数可能使用交叉熵损失进行二分类任务。网络结构紧凑，整体参数较少，以支持轻量化部署。具体超参数如学习率、批次大小在论文中未详细说明，但通过5折分层交叉验证进行优化。",
            "application_zh": "该研究主要应用于心血管疾病早期检测和风险预测，基于表格临床记录如电子健康档案。其实际价值在于提供高效、准确的自动化诊断工具，支持临床决策，减少对专家经验的依赖。未来影响可能扩展到其他慢性病预测，促进资源受限医疗环境中的智能医疗部署，提升公共卫生水平。",
            "highlight_zh": "在UCI心脏病数据集上，Residual GRU+MHSA模型达到0.861准确率、0.860宏F1、0.908 ROC-AUC和0.904 PR-AUC，优于逻辑回归、随机森林、支持向量机及DeepMLP、卷积网络、循环网络和Transformer等基线。消融研究确认残差循环、通道门控和注意力池化均贡献性能提升，t-SNE可视化显示学习嵌入在类别间分离更清晰，验证了模型的有效性。",
            "tags_zh": [
                "心血管疾病检测",
                "轻量混合架构",
                "残差双向GRU",
                "多头自注意力",
                "表格数据建模",
                "临床风险预测",
                "资源受限部署",
                "深度学习优化"
            ],
            "_index": 28
        },
        {
            "title": "Counterfactual Explanations for Time Series Should be Human-Centered and Temporally Coherent in Interventions",
            "authors": [
                "Emmanuel C. Chukwu",
                "Rianne M. Schouten",
                "Monique Tabak",
                "Mykola Pechenizkiy"
            ],
            "arxiv_id": "2512.14559v1",
            "summary": "Counterfactual explanations are increasingly proposed as interpretable mechanisms to achieve algorithmic recourse. However, current counterfactual techniques for time series classification are predominantly designed with static data assumptions and focus on generating minimal input perturbations to flip model predictions. This paper argues that such approaches are fundamentally insufficient in clinical recommendation settings, where interventions unfold over time and must be causally plausible and temporally coherent. We advocate for a shift towards counterfactuals that reflect sustained, goal-directed interventions aligned with clinical reasoning and patient-specific dynamics. We identify critical gaps in existing methods that limit their practical applicability, specifically, temporal blind spots and the lack of user-centered considerations in both method design and evaluation metrics. To support our position, we conduct a robustness analysis of several state-of-the-art methods for time series and show that the generated counterfactuals are highly sensitive to stochastic noise. This finding highlights their limited reliability in real-world clinical settings, where minor measurement variations are inevitable. We conclude by calling for methods and evaluation frameworks that go beyond mere prediction changes without considering feasibility or actionability. We emphasize the need for actionable, purpose-driven interventions that are feasible in real-world contexts for the users of such applications.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14559v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出以人为中心且时间连贯的反事实解释方法，以解决临床时间序列分类中现有方法不可行的问题",
            "summary_zh": "反事实解释作为实现算法追索的可解释机制日益受到关注。然而，当前针对时间序列分类的反事实技术主要基于静态数据假设，侧重于生成最小输入扰动以翻转模型预测。本文认为，在临床推荐场景中，此类方法从根本上不足，因为干预措施随时间展开，必须具有因果合理性和时间连贯性。我们主张转向反映持续、目标导向干预的反事实解释，这些干预应与临床推理和患者特定动态保持一致。我们识别了现有方法在实践应用中的关键缺陷，特别是时间盲点以及在方法设计和评估指标中缺乏以用户为中心的考量。为支持我们的观点，我们对几种最先进的时间序列方法进行了鲁棒性分析，结果表明生成的反事实解释对随机噪声高度敏感。这一发现凸显了它们在现实世界临床环境中的有限可靠性，因为微小的测量变化不可避免。最后，我们呼吁开发超越仅考虑预测变化而不考虑可行性或可操作性的方法和评估框架。我们强调需要可操作、目的驱动的干预措施，这些措施在现实世界中对应用用户是可行的。",
            "intro_zh": [
                "核心问题：现有反事实解释方法基于静态假设，忽略时间连贯性和临床可行性，导致干预不可行。",
                "方法要点：提出以人为中心的反事实解释，强调持续、目标导向的干预，确保因果合理性和时间连贯性。",
                "实验或效果：通过鲁棒性分析发现现有方法对噪声敏感，凸显其在真实临床环境中的可靠性不足。"
            ],
            "method_zh": "**问题定义**：论文旨在解决时间序列分类中现有反事实解释方法的不足，特别是在临床推荐场景中。现有方法主要基于静态数据假设，专注于生成最小输入扰动以翻转模型预测，忽略了干预措施随时间展开的因果合理性和时间连贯性，导致生成的解释在现实世界中不可行或不可操作。\\n\\n**核心思路**：论文的核心解决思路是转向以人为中心的反事实解释，强调干预措施应反映持续、目标导向的过程，与临床推理和患者特定动态保持一致。这要求反事实解释不仅改变预测，还要确保干预在时间上是连贯的、因果上合理的，并且对用户（如患者或临床医生）是可操作的。\\n\\n**技术框架**：整体架构包括问题分析、方法评估和框架倡导。首先，识别现有方法在时间盲点和用户中心性方面的缺陷；其次，通过鲁棒性分析评估现有方法的敏感性；最后，提出新的方法和评估框架，强调可行性、可操作性和时间连贯性。主要模块包括理论论证、实验验证和未来方向建议。\\n\\n**关键创新**：最重要的技术创新点是将反事实解释从静态扰动扩展到动态、时间连贯的干预，强调以用户为中心的设计。与现有方法的本质区别在于，不再仅追求最小扰动或预测翻转，而是关注干预的可持续性、因果合理性和现实可行性，从而提升在临床等实际应用中的实用性。\\n\\n**关键设计**：论文未详细描述具体的技术细节如参数设置或损失函数，因为它主要是一个立场论文，侧重于理论分析和框架倡导。关键设计体现在对现有方法的鲁棒性分析中，通过引入随机噪声来测试反事实解释的敏感性，从而揭示其局限性。未来方法设计应包含时间连贯性约束、用户反馈机制和可行性评估指标。",
            "application_zh": "该研究主要应用于临床推荐系统，如疾病预测、治疗建议和患者监测，其中时间序列数据（如心电图、血糖水平）的准确解释至关重要。潜在价值在于提升医疗AI的可信度和实用性，通过生成可行、可操作的反事实解释，帮助临床医生制定个性化干预计划。未来影响可能扩展到其他时间敏感领域，如金融风险预测、工业设备维护，推动以人为中心的AI解释性发展。",
            "highlight_zh": "最重要的实验结果是通过鲁棒性分析发现，现有最先进的时间序列反事实解释方法（具体方法未命名）对随机噪声高度敏感，表明生成的反事实解释在微小测量变化下不稳定。这凸显了这些方法在真实临床环境中的有限可靠性，因为噪声不可避免。实验未提供具体性能数据或提升幅度，但强调了现有方法在时间连贯性和用户中心性方面的缺陷，为未来方法开发提供了实证基础。",
            "tags_zh": [
                "反事实解释",
                "时间序列分类",
                "临床推荐系统",
                "以人为中心设计",
                "时间连贯性",
                "算法追索",
                "可解释人工智能",
                "鲁棒性分析"
            ],
            "_index": 29
        },
        {
            "title": "C-ing Clearly: Enhanced Binary Code Explanations using C code",
            "authors": [
                "Teodor Poncu",
                "Ioana Pintilie",
                "Marius Dragoi",
                "Dragos Tantaru",
                "Florin Brad"
            ],
            "arxiv_id": "2512.14500v1",
            "summary": "Large Language Models (LLMs) typically excel at coding tasks involving high-level programming languages, as opposed to lower-level programming languages, such as assembly. We propose a synthetic data generation method named C-ing Clearly, which leverages the corresponding C code to enhance an LLM's understanding of assembly. By fine-tuning on data generated through our method, we demonstrate improved LLM performance for binary code summarization and vulnerability detection. Our approach demonstrates consistent gains across different LLM families and model sizes.",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "18 pages, 5 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14500v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出C-ing Clearly方法，利用C代码增强大语言模型对汇编的理解，以解决二进制代码分析任务性能不足的问题。",
            "summary_zh": "大语言模型（LLMs）通常在涉及高级编程语言的编码任务中表现出色，但对于低级编程语言（如汇编）则表现不佳。我们提出了一种名为C-ing Clearly的合成数据生成方法，该方法利用相应的C代码来增强LLM对汇编的理解。通过在通过我们的方法生成的数据上进行微调，我们展示了LLM在二进制代码摘要和漏洞检测任务上的性能提升。我们的方法在不同LLM家族和模型大小上均表现出了一致的增益。",
            "intro_zh": [
                "核心问题：大语言模型在处理低级汇编语言时性能不足，现有方法缺乏有效利用高级语言信息来提升模型理解。",
                "方法要点：提出C-ing Clearly方法，通过生成C代码与汇编的配对数据，利用C代码的语义信息来增强模型对汇编的微调。",
                "实验或效果：在二进制代码摘要和漏洞检测任务上，微调后的模型性能显著提升，且在不同模型家族和大小上均有效。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大语言模型在低级汇编语言任务（如二进制代码摘要和漏洞检测）中性能不足的问题。现有方法的痛点在于LLMs通常基于高级语言数据训练，缺乏对汇编语义的深入理解，导致直接应用于二进制分析时效果有限。\\n\\n**核心思路**：论文的核心解决思路是利用C代码作为桥梁，通过生成C代码与汇编的配对数据，将高级语言的语义信息注入到模型训练中，从而增强LLM对汇编的理解。这样设计是因为C代码与汇编在功能上对应，但更易于模型学习，能提供丰富的上下文信息。\\n\\n**技术框架**：整体流程包括数据生成和模型微调两个阶段。首先，使用C-ing Clearly方法自动生成大量C代码片段及其对应的汇编代码，形成合成数据集；然后，基于这些数据对预训练的LLM进行微调，优化模型在汇编相关任务上的性能。主要模块包括数据生成器、配对对齐模块和微调训练模块。\\n\\n**关键创新**：最重要的技术创新点是提出了一种基于C代码的合成数据生成方法，通过利用高级语言信息来增强低级语言理解，这与现有方法主要依赖纯汇编数据或有限标注的本质区别在于引入了语义丰富的跨语言对齐。\\n\\n**关键设计**：关键设计包括数据生成过程中确保C代码与汇编的功能一致性，使用自动工具（如编译器）进行转换；微调阶段采用标准的语言模型训练设置，损失函数为交叉熵损失，网络结构基于预训练LLM（如Transformer架构），具体参数设置如学习率、批量大小等根据实验调整，但论文未详细披露所有细节。",
            "application_zh": "该研究在软件安全、逆向工程和代码分析领域具有重要应用价值。例如，可用于自动化二进制代码审计、恶意软件检测和漏洞挖掘，提高安全专家的工作效率。未来可能推动智能工具在低级编程语言理解方面的发展，增强AI在网络安全中的实际部署。",
            "highlight_zh": "实验结果显示，在二进制代码摘要和漏洞检测任务上，使用C-ing Clearly方法微调的LLM相比基线模型（如未微调或基于纯汇编数据微调的模型）性能显著提升。具体提升幅度未在摘要中给出，但论文强调在不同LLM家族（如GPT、BERT等）和模型大小上均观察到一致增益，证明了方法的有效性和泛化能力。",
            "tags_zh": [
                "大语言模型",
                "汇编语言理解",
                "二进制代码分析",
                "合成数据生成",
                "代码摘要",
                "漏洞检测",
                "微调训练",
                "跨语言对齐"
            ],
            "_index": 30
        },
        {
            "title": "Context-Picker: Dynamic context selection using multi-stage reinforcement learning",
            "authors": [
                "Siyuan Zhu",
                "Chengdong Xu",
                "Kaiqiang Ke",
                "Chao Yu"
            ],
            "arxiv_id": "2512.14465v1",
            "summary": "In long-context question answering (LCQA), determining the optimal amount of context for a given query is a significant challenge. Including too few passages may omit critical information, while including too many can introduce noise and reduce the quality of the answer. Traditional approaches, such as fixed Top-$K$ retrieval and single-stage reranking, face the dilemma of selecting the right number of passages. This problem is particularly pronounced for factoid questions, which often require only a few specific pieces of evidence. To address this issue, we introduce \\emph{Context-Picker}, a reasoning-aware framework that shifts the paradigm from similarity-based ranking to minimal sufficient subset selection. Context-Picker treats context selection as a decision-making process optimized via a human-inspired, two-stage reinforcement learning schedule: a \\emph{recall-oriented} stage that prioritizes the coverage of reasoning chains, followed by a \\emph{precision-oriented} stage that aggressively prunes redundancy to distill a compact evidence set. To resolve reward sparsity, we propose an offline evidence distillation pipeline that mines \"minimal sufficient sets\" via a Leave-One-Out (LOO) procedure, providing dense, task-aligned supervision. Experiments on five long-context and multi-hop QA benchmarks demonstrate that Context-Picker significantly outperforms strong RAG baselines, achieving superior answer accuracy with comparable or reduced context lengths. Ablation studies indicate that the coarse-to-fine optimization schedule, the redundancy-aware reward shaping, and the rationale-guided format all contribute substantially to these gains.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14465v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RL",
                        "reward"
                    ],
                    "score": 3
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出Context-Picker框架，通过多阶段强化学习解决长上下文问答中动态选择最优证据子集的问题。",
            "summary_zh": "在长上下文问答（LCQA）中，为给定查询确定最优的上下文量是一个重要挑战。包含过少段落可能遗漏关键信息，而包含过多则会引入噪声并降低答案质量。传统方法，如固定的Top-K检索和单阶段重排序，面临选择正确段落数量的困境。这一问题在事实型问题上尤为突出，这类问题通常只需要少量特定证据。为解决此问题，我们引入了Context-Picker，这是一个推理感知的框架，将范式从基于相似性的排序转向最小充分子集选择。Context-Picker将上下文选择视为一个决策过程，通过受人类启发的两阶段强化学习计划进行优化：一个面向召回的阶段，优先覆盖推理链；随后是一个面向精度的阶段，积极剪枝冗余以提炼紧凑的证据集。为解决奖励稀疏性问题，我们提出了一个离线证据蒸馏流程，通过留一法（LOO）挖掘“最小充分集”，提供密集、任务对齐的监督。在五个长上下文和多跳问答基准上的实验表明，Context-Picker显著优于强大的RAG基线，在可比或更短的上下文长度下实现了更优的答案准确性。消融研究表明，从粗到细的优化计划、冗余感知的奖励塑造和推理引导的格式都对这一增益有实质性贡献。",
            "intro_zh": [
                "核心问题：长上下文问答中，传统固定Top-K检索和单阶段重排序方法难以动态确定最优证据数量，导致信息遗漏或噪声引入，影响答案质量。",
                "方法要点：提出Context-Picker框架，将上下文选择视为决策过程，采用两阶段强化学习（召回和精度阶段）优化最小充分子集选择，并引入离线证据蒸馏解决奖励稀疏性。",
                "实验或效果：在五个基准测试中，Context-Picker显著优于RAG基线，实现更高答案准确性，同时减少上下文长度，消融研究验证了关键组件的有效性。"
            ],
            "method_zh": "**问题定义**：论文解决长上下文问答（LCQA）中动态选择最优证据子集的问题。现有方法如固定Top-K检索和单阶段重排序面临困境：选择过少段落可能遗漏关键信息，选择过多则引入噪声，降低答案质量，尤其在事实型问题中，这导致效率低下和准确性受限。\\n\\n**核心思路**：论文的核心思路是将上下文选择从传统的相似性排序范式转向最小充分子集选择，通过强化学习优化决策过程。设计受人类启发，模拟先广泛搜索再精细筛选的认知过程，以平衡召回和精度，从而提取紧凑且相关的证据集。\\n\\n**技术框架**：整体架构包括两阶段强化学习计划：第一阶段为召回导向阶段，使用策略网络优先覆盖推理链，确保不遗漏关键证据；第二阶段为精度导向阶段，通过另一个策略网络积极剪枝冗余，提炼出最小充分证据集。此外，框架集成了离线证据蒸馏流程，用于训练数据准备。\\n\\n**关键创新**：最重要的技术创新是提出多阶段强化学习计划，结合召回和精度优化，以及离线证据蒸馏解决奖励稀疏性。与现有方法的本质区别在于，它不再依赖固定数量的段落或单次排序，而是动态选择子集，更贴合实际问答需求。\\n\\n**关键设计**：关键设计包括：使用留一法（LOO）在离线阶段挖掘“最小充分集”作为监督信号；设计冗余感知的奖励函数，鼓励选择紧凑证据；采用推理引导的格式，将问题分解为子步骤；强化学习中使用策略梯度方法优化网络参数，具体网络结构可能基于Transformer编码器，但论文未详细说明。",
            "application_zh": "该研究在长上下文问答领域具有广泛的应用潜力，如智能客服、文档检索、教育辅助和医疗诊断系统，可提高信息检索的准确性和效率。实际价值在于减少计算资源消耗，提升用户体验，未来可能推动自适应上下文选择技术的发展，影响自然语言处理和信息检索的范式。",
            "highlight_zh": "在五个长上下文和多跳问答基准（具体名称未提及）上，Context-Picker显著优于强大的RAG基线，实现更高的答案准确性，同时上下文长度可比或更短。消融研究表明，从粗到细的优化计划、冗余感知奖励塑造和推理引导格式对性能增益有实质性贡献，具体提升幅度论文未提供数值，但强调“显著优于”。",
            "tags_zh": [
                "长上下文问答",
                "强化学习",
                "证据选择",
                "多阶段优化",
                "离线蒸馏",
                "推理感知",
                "最小充分集",
                "RAG框架"
            ],
            "_index": 31
        },
        {
            "title": "GRAFT: Grid-Aware Load Forecasting with Multi-Source Textual Alignment and Fusion",
            "authors": [
                "Fangzhou Lin",
                "Guoshun He",
                "Zhenyu Guo",
                "Zhe Huang",
                "Jinsong Tao"
            ],
            "arxiv_id": "2512.14400v1",
            "summary": "Electric load is simultaneously affected across multiple time scales by exogenous factors such as weather and calendar rhythms, sudden events, and policies. Therefore, this paper proposes GRAFT (GRid-Aware Forecasting with Text), which modifies and improves STanHOP to better support grid-aware forecasting and multi-source textual interventions. Specifically, GRAFT strictly aligns daily-aggregated news, social media, and policy texts with half-hour load, and realizes text-guided fusion to specific time positions via cross-attention during both training and rolling forecasting. In addition, GRAFT provides a plug-and-play external-memory interface to accommodate different information sources in real-world deployment. We construct and release a unified aligned benchmark covering 2019--2021 for five Australian states (half-hour load, daily-aligned weather/calendar variables, and three categories of external texts), and conduct systematic, reproducible evaluations at three scales -- hourly, daily, and monthly -- under a unified protocol for comparison across regions, external sources, and time scales. Experimental results show that GRAFT significantly outperforms strong baselines and reaches or surpasses the state of the art across multiple regions and forecasting horizons. Moreover, the model is robust in event-driven scenarios and enables temporal localization and source-level interpretation of text-to-load effects through attention read-out. We release the benchmark, preprocessing scripts, and forecasting results to facilitate standardized empirical evaluation and reproducibility in power grid load forecasting.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14400v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出GRAFT模型，通过多源文本对齐与融合提升电网负荷预测精度，解决外部因素影响建模不足的问题。",
            "summary_zh": "电力负荷同时受到天气、日历节律、突发事件和政策等多时间尺度外生因素的影响。为此，本文提出GRAFT（基于文本的电网感知预测）模型，改进STanHOP以更好地支持电网感知预测和多源文本干预。具体而言，GRAFT将每日聚合的新闻、社交媒体和政策文本与半小时负荷数据严格对齐，并通过训练和滚动预测期间的交叉注意力实现文本引导的融合到特定时间位置。此外，GRAFT提供了一个即插即用的外部记忆接口，以适应实际部署中的不同信息源。我们构建并发布了一个统一的基准数据集，涵盖2019-2021年澳大利亚五个州的半小时负荷、每日对齐的天气/日历变量以及三类外部文本，并在统一协议下进行了系统、可重复的评估，比较了不同区域、外部源和时间尺度。实验结果表明，GRAFT显著优于强基线，在多个区域和预测时间尺度上达到或超越了最先进水平。此外，该模型在事件驱动场景中表现出鲁棒性，并通过注意力读出实现文本对负荷影响的时序定位和源级解释。我们发布基准数据集、预处理脚本和预测结果，以促进电网负荷预测的标准化实证评估和可重复性。",
            "intro_zh": [
                "核心问题：现有负荷预测方法难以有效整合多源文本信息（如新闻、社交媒体、政策），导致对外部因素建模不足，影响预测精度和鲁棒性。",
                "方法要点：提出GRAFT模型，通过严格对齐多源文本与负荷数据，并利用交叉注意力实现文本引导的融合，增强电网感知预测能力。",
                "实验或效果：在澳大利亚五州基准测试中，GRAFT显著优于基线，达到或超越最先进水平，并在事件驱动场景中表现出鲁棒性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决电网负荷预测中如何有效整合多源文本信息（如新闻、社交媒体、政策）以提升预测精度的问题。现有方法（如STanHOP）在文本对齐和融合方面存在不足，难以准确捕捉外部因素对负荷的复杂影响，导致预测性能受限。\\n\\n**核心思路**：论文的核心思路是通过严格对齐多源文本与负荷数据，并利用交叉注意力机制实现文本引导的融合，从而增强模型对电网负荷的感知能力。这种设计基于负荷受多时间尺度外生因素影响的假设，旨在通过文本信息补充传统变量（如天气、日历）的不足。\\n\\n**技术框架**：整体架构包括数据对齐、模型训练和滚动预测三个阶段。主要模块包括：文本预处理模块（将新闻、社交媒体、政策文本聚合为每日表示）、对齐模块（严格对齐文本与半小时负荷数据）、融合模块（通过交叉注意力实现文本到特定时间位置的引导融合），以及外部记忆接口（作为即插即用组件，适应不同信息源）。\\n\\n**关键创新**：最重要的技术创新点是多源文本的严格对齐与文本引导的融合机制。与现有方法相比，GRAFT不仅改进了STanHOP以支持电网感知预测，还引入了交叉注意力在训练和预测期间动态融合文本信息，本质区别在于实现了更精细的文本到负荷的时序关联建模。\\n\\n**关键设计**：关键设计包括：使用每日聚合的文本表示与半小时负荷数据对齐；在交叉注意力中，文本作为查询或键值对，负荷数据作为对应部分，实现文本引导的融合；损失函数基于预测误差优化，具体形式未知；网络结构可能包含编码器-解码器架构，细节未明确说明；参数设置涉及注意力头数、隐藏层维度等，具体值未知。",
            "application_zh": "该研究在智能电网管理、能源调度和电力市场分析等领域具有潜在应用价值。通过提升负荷预测精度，可优化发电计划、降低运营成本，并增强电网对突发事件（如极端天气或政策变化）的响应能力。未来可能推动负荷预测标准化，促进多源数据融合在实际部署中的广泛应用。",
            "highlight_zh": "实验结果显示，GRAFT在澳大利亚五州基准测试中显著优于强基线，在多个区域和预测时间尺度（小时、日、月）上达到或超越最先进水平。具体性能数据未提供，但提升幅度显著；模型在事件驱动场景中表现出鲁棒性，并通过注意力机制实现文本影响的解释性分析。",
            "tags_zh": [
                "电网负荷预测",
                "多源文本对齐",
                "交叉注意力融合",
                "外部记忆接口",
                "事件驱动鲁棒性",
                "时序预测",
                "可解释性分析",
                "基准数据集"
            ],
            "_index": 32
        },
        {
            "title": "Field evaluation and optimization of a lightweight lidar-based UAV navigation system for dense boreal forest environments",
            "authors": [
                "Aleksi Karhunen",
                "Teemu Hakala",
                "Väinö Karjalainen",
                "Eija Honkavaara"
            ],
            "arxiv_id": "2512.14340v1",
            "summary": "The interest in the usage of uncrewed aerial vehicles (UAVs) for forest applications has increased in recent years. While above-canopy flight has reached a high level of autonomy, navigating under-canopy remains a significant challenge. The use of autonomous UAVs could reduce the burden of data collection, which has motivated the development of numerous solutions for under-canopy autonomous flight. However, the experiments conducted in the literature and their reporting lack rigor. Very rarely, the density and the difficulty of the test forests are reported, or multiple flights are flown, and the success rate of those flights is reported. The aim of this study was to implement an autonomously flying quadrotor based on a lightweight lidar using openly available algorithms and test its behavior in real forest environments. A set of rigorous experiments was conducted with a quadrotor prototype utilizing the IPC path planner and LTA-OM SLAM algorithm. Based on the results of the first 33 flights, the original system was further enhanced. With the optimized system, 60 flights were performed, resulting in a total of 93 test flights. The optimized system performed significantly better in terms of reliability and flight mission completion times, achieving success rates of 12/15 in a medium-density forest and 15/15 in a dense forest, at a target flight velocity of 1 m/s. At a target flight velocity of 2 m/s, it had a success rate of 12/15 and 5/15, respectively. Furthermore, a standardized testing setup and evaluation criteria were proposed, enabling consistent performance comparisons of autonomous under-canopy UAV systems, enhancing reproducibility, guiding system improvements, and accelerating progress in forest robotics.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "This work has been submitted to the IEEE for possible publication",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14340v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "lidar"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO",
                        "SLAM"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出基于轻量激光雷达的无人机导航系统优化与标准化评估方法，以解决稠密北方森林环境下的自主飞行挑战。",
            "summary_zh": "近年来，无人机在森林应用中的使用兴趣日益增长。虽然冠层以上飞行已达到高度自主性，但在冠层下导航仍是一个重大挑战。自主无人机的使用可以减轻数据收集的负担，这推动了众多冠层下自主飞行解决方案的开发。然而，文献中进行的实验及其报告缺乏严谨性。很少报告测试森林的密度和难度，或进行多次飞行并报告其成功率。本研究旨在基于轻量激光雷达，使用公开可用的算法实现自主飞行的四旋翼无人机，并在真实森林环境中测试其行为。利用IPC路径规划器和LTA-OM SLAM算法，对四旋翼原型进行了严格的实验。基于前33次飞行的结果，对原始系统进行了进一步优化。使用优化后的系统进行了60次飞行，总共完成了93次测试飞行。优化后的系统在可靠性和飞行任务完成时间方面表现显著更好，在目标飞行速度为1 m/s时，在中密度森林中实现了12/15的成功率，在稠密森林中实现了15/15的成功率。在目标飞行速度为2 m/s时，其成功率分别为12/15和5/15。此外，提出了标准化的测试设置和评估标准，使自主冠层下无人机系统的性能比较具有一致性，增强了可重复性，指导系统改进，并加速了森林机器人学的进展。",
            "intro_zh": [
                "现有方法在稠密森林冠层下自主飞行中缺乏严谨的实验评估，如森林密度报告不足和成功率统计不完整。",
                "论文基于轻量激光雷达和开源算法构建无人机系统，通过优化提升在真实森林环境中的导航可靠性和效率。",
                "优化后系统在1 m/s速度下实现高成功率，并提出了标准化测试框架，促进森林机器人领域的可重复研究和系统改进。"
            ],
            "method_zh": "**问题定义**：论文旨在解决无人机在稠密北方森林冠层下自主导航的挑战，现有方法实验评估不严谨，缺乏标准化测试和成功率报告，导致系统性能难以比较和优化。\\n\\n**核心思路**：采用轻量激光雷达结合公开算法，构建低成本、易复现的无人机导航系统，通过大量真实环境飞行测试进行迭代优化，并引入标准化评估框架以提升实验严谨性和可重复性。\\n\\n**技术框架**：整体架构包括四旋翼无人机平台、轻量激光雷达传感器、LTA-OM SLAM算法用于实时定位与建图，以及IPC路径规划器用于动态路径生成。流程分为原型开发、初步测试（33次飞行）、系统优化和最终验证（60次飞行）四个阶段。\\n\\n**关键创新**：最重要的技术创新在于将开源算法集成到轻量系统中，并通过大规模实地测试驱动优化，同时提出标准化测试设置，这在现有研究中较少见，本质区别在于强调实验严谨性和系统可重复性。\\n\\n**关键设计**：关键参数包括目标飞行速度设置为1 m/s和2 m/s以测试性能极限，使用LTA-OM SLAM确保在稠密环境中的鲁棒定位，IPC规划器适应动态障碍，实验设计覆盖不同森林密度（中密度和稠密）以全面评估系统可靠性。",
            "application_zh": "该研究在森林监测、生态调查和资源管理等领域具有潜在应用价值，通过提升无人机在复杂环境下的自主飞行能力，可降低数据收集成本并提高效率。未来可能推动森林机器人学的标准化发展，加速相关技术的实际部署和商业化进程。",
            "highlight_zh": "优化后系统在目标速度1 m/s下，中密度森林成功率12/15，稠密森林成功率15/15；速度提升至2 m/s时，成功率分别降至12/15和5/15。通过93次飞行测试，系统可靠性和任务完成时间显著改善，并建立了标准化评估标准，为后续研究提供了可比较的性能基准。",
            "tags_zh": [
                "无人机导航",
                "激光雷达SLAM",
                "森林机器人",
                "路径规划",
                "自主飞行",
                "实地测试",
                "系统优化",
                "标准化评估"
            ],
            "_index": 33
        },
        {
            "title": "ARCADE: Adaptive Robot Control with Online Changepoint-Aware Bayesian Dynamics Learning",
            "authors": [
                "Rishabh Dev Yadav",
                "Avirup Das",
                "Hongyu Song",
                "Samuel Kaski",
                "Wei Pan"
            ],
            "arxiv_id": "2512.14331v1",
            "summary": "Real-world robots must operate under evolving dynamics caused by changing operating conditions, external disturbances, and unmodeled effects. These may appear as gradual drifts, transient fluctuations, or abrupt shifts, demanding real-time adaptation that is robust to short-term variation yet responsive to lasting change. We propose a framework for modeling the nonlinear dynamics of robotic systems that can be updated in real time from streaming data. The method decouples representation learning from online adaptation, using latent representations learned offline to support online closed-form Bayesian updates. To handle evolving conditions, we introduce a changepoint-aware mechanism with a latent variable inferred from data likelihoods that indicates continuity or shift. When continuity is likely, evidence accumulates to refine predictions; when a shift is detected, past information is tempered to enable rapid re-learning. This maintains calibrated uncertainty and supports probabilistic reasoning about transient, gradual, or structural change. We prove that the adaptive regret of the framework grows only logarithmically in time and linearly with the number of shifts, competitive with an oracle that knows timings of shift. We validate on cartpole simulations and real quadrotor flights with swinging payloads and mid-flight drops, showing improved predictive accuracy, faster recovery, and more accurate closed-loop tracking than relevant baselines.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14331v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出ARCADE框架，通过在线变点感知贝叶斯动力学学习解决机器人动态变化下的自适应控制问题",
            "summary_zh": "现实世界中的机器人必须在不断演化的动态条件下运行，这些变化可能由操作条件改变、外部干扰和未建模效应引起，表现为渐进漂移、瞬态波动或突然转变，需要实时适应机制，既能抵抗短期变化又能响应持久变化。我们提出了一个框架，用于建模机器人系统的非线性动力学，能够从流数据中实时更新。该方法将表示学习与在线适应解耦，利用离线学习的潜在表示支持在线闭式贝叶斯更新。为处理演化条件，我们引入了变点感知机制，通过从数据似然推断的潜在变量指示连续性或转变。当连续性可能时，证据积累以优化预测；当检测到转变时，过去信息被调节以实现快速重新学习。这保持了校准的不确定性，并支持对瞬态、渐进或结构变化的概率推理。我们证明了该框架的自适应遗憾仅随时间对数增长，并与转变次数线性相关，与已知转变时间的神谕者竞争。我们在倒立摆仿真和真实四旋翼飞行器实验中验证了该方法，实验包括摆动负载和飞行中掉落，结果显示相比相关基线，预测准确性更高、恢复更快、闭环跟踪更准确。",
            "intro_zh": [
                "核心问题：现实机器人动态因操作条件、干扰和未建模效应而演化，现有方法难以实时适应短期变化和持久转变。",
                "方法要点：解耦表示学习与在线适应，利用离线潜在表示支持在线贝叶斯更新，引入变点感知机制处理动态变化。",
                "实验或效果：在倒立摆和四旋翼实验中，相比基线，预测准确性提升、恢复速度加快、闭环跟踪更准确，自适应遗憾对数增长。"
            ],
            "method_zh": "**问题定义**：论文解决机器人系统在现实世界中动态演化（如渐进漂移、瞬态波动或突然转变）下的自适应控制问题。现有方法的痛点在于难以实时适应短期变化和持久转变，导致预测不准确和控制性能下降。\\n\\n**核心思路**：核心解决思路是将表示学习与在线适应解耦，利用离线学习的潜在表示支持在线闭式贝叶斯更新，并通过变点感知机制动态调整学习过程，以平衡短期鲁棒性和长期适应性。这样设计是为了在保持计算效率的同时，实现对动态变化的快速响应和不确定性校准。\\n\\n**技术框架**：整体架构包含两个主要阶段：离线表示学习和在线自适应更新。在离线阶段，学习非线性动力学的潜在表示；在线阶段，基于流数据执行贝叶斯更新，并引入变点检测模块，通过数据似然推断潜在变量来指示连续性或转变，从而调节过去信息的权重。\\n\\n**关键创新**：最重要的技术创新点是变点感知机制与在线贝叶斯更新的结合，本质区别在于它能够实时检测动态变化（如转变或漂移），并自适应地调整学习策略，而现有方法往往假设静态或缓慢变化的动态，缺乏对突发变化的快速响应能力。\\n\\n**关键设计**：关键设计包括使用潜在变量模型表示动力学，变点检测基于数据似然计算，贝叶斯更新采用闭式形式以提高效率，损失函数可能涉及预测误差和不确定性校准，具体参数设置如学习率和变点阈值在实验中优化，网络结构细节未在摘要中明确，但可能涉及神经网络编码潜在表示。",
            "application_zh": "该研究在机器人控制领域具有广泛潜在应用，如无人机在负载变化或环境干扰下的自适应飞行、工业机器人在操作条件演化中的精确操控、以及自主车辆在动态场景中的稳定导航。实际价值在于提升机器人在不确定环境中的鲁棒性和适应性，未来可能推动智能系统在复杂现实任务中的部署，促进自适应机器学习与机器人学的交叉发展。",
            "highlight_zh": "最重要的实验结果包括：在倒立摆仿真和真实四旋翼飞行实验中，ARCADE框架相比相关基线（具体基线未在摘要中指定）显示出更高的预测准确性、更快的恢复速度（例如在负载掉落或摆动场景中）和更准确的闭环跟踪性能。理论证明自适应遗憾仅随时间对数增长，并与转变次数线性相关，与已知转变时间的神谕者竞争，验证了方法的有效性和鲁棒性。",
            "tags_zh": [
                "自适应机器人控制",
                "在线贝叶斯学习",
                "变点检测",
                "动力学建模",
                "非线性系统",
                "实时适应",
                "不确定性校准",
                "机器人学"
            ],
            "_index": 34
        },
        {
            "title": "Criminal Liability in AI-Enabled Autonomous Vehicles: A Comparative Study",
            "authors": [
                "Sahibpreet Singh",
                "Manjit Singh"
            ],
            "arxiv_id": "2512.14330v1",
            "summary": "AI revolutionizes transportation through autonomous vehicles (AVs) but introduces complex criminal liability issues regarding infractions. This study employs a comparative legal analysis of primary statutes, real-world liability claims, and academic literature across the US, Germany, UK, China, and India; jurisdictions selected for their technological advancement and contrasting regulatory approaches. The research examines the attribution of human error, AI moral agency, and the identification of primary offenders in AV incidents. Findings reveal fragmented regulatory landscapes: India and the US rely on loose networks of state laws, whereas the UK enacted the pioneering Automated and Electric Vehicles Act 2018. Germany enforces strict safety standards, distinguishing liability based on the vehicle's operating mode, while China similarly aims for a stringent liability regime. The study concludes that globally harmonized legal standards are essential to foster technological innovation while ensuring minimum risk and clear liability attribution.",
            "categories": [
                "cs.CY",
                "cs.AI",
                "cs.CR"
            ],
            "primary_category": "cs.CY",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Published in Journal of University Institute of Legal Studies, Vol. 18, Issue 1, pp. 57-78, 2025",
            "doi": "",
            "journal_ref": "Journal of University Institute of Legal Studies 18(1), 57-78 (2025)",
            "pdf_url": "https://arxiv.org/pdf/2512.14330v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "autonomous vehicle"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 3,
            "headline_zh": "通过比较法分析提出全球统一法律标准以解决自动驾驶车辆刑事责任的碎片化问题",
            "summary_zh": "人工智能通过自动驾驶车辆（AVs）革新了交通运输，但也带来了关于违规行为的复杂刑事责任问题。本研究采用比较法律分析方法，对美国、德国、英国、中国和印度等司法管辖区的主要法规、现实世界中的责任索赔以及学术文献进行了分析；这些司法管辖区因其技术进步和对比鲜明的监管方法而被选中。研究探讨了在自动驾驶车辆事故中人类错误的归因、人工智能的道德主体性以及主要责任人的识别。研究发现监管格局呈现碎片化：印度和美国依赖于松散的各州法律网络，而英国则颁布了开创性的《2018年自动化和电动汽车法案》。德国执行严格的安全标准，根据车辆的运行模式区分责任，而中国同样旨在建立严格的责任制度。研究得出结论，全球统一的法律标准对于促进技术创新、同时确保最低风险和明确的责任归属至关重要。",
            "intro_zh": [
                "核心问题：自动驾驶车辆事故中刑事责任的归属面临法律碎片化、AI道德主体性不明确等挑战，缺乏全球统一标准。",
                "方法要点：采用比较法律分析，研究多国法规、案例和文献，以识别责任归属的关键因素和监管差异。",
                "实验或效果：揭示了各国监管模式的碎片化，并论证了全球统一法律标准对技术创新和风险管理的必要性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自动驾驶车辆（AVs）在发生事故或违规行为时，刑事责任的归属问题。现有方法的痛点在于全球范围内缺乏统一的法律标准，导致监管碎片化、责任认定模糊，尤其是在涉及AI决策时，人类错误、AI道德主体性和主要责任人的识别存在争议。\\n\\n**核心思路**：论文的核心解决思路是通过比较法律分析，系统研究不同司法管辖区的法规、实际案例和学术观点，以识别责任归属的关键因素和监管差异。这样设计是因为自动驾驶技术具有跨国性，而各国法律体系各异，比较分析有助于揭示最佳实践和潜在冲突，为全球统一标准提供依据。\\n\\n**技术框架**：整体架构包括三个主要阶段：首先，选择美国、德国、英国、中国和印度等代表性司法管辖区，基于其技术先进性和监管多样性；其次，收集和分析这些地区的主要法规、现实世界中的责任索赔案例以及相关学术文献；最后，综合比较结果，探讨人类错误归因、AI道德主体性和主要责任人识别等核心问题。\\n\\n**关键创新**：最重要的技术创新点在于将比较法律分析方法系统应用于自动驾驶车辆的刑事责任领域，强调跨司法管辖区的对比研究。与现有方法（如单一国家分析或纯理论探讨）的本质区别在于，它通过实证和法规比较，揭示了全球监管的碎片化现状，并提出了基于实际数据的统一标准建议。\\n\\n**关键设计**：关键设计包括选择具有技术先进性和监管对比性的司法管辖区（如美国、德国、英国、中国、印度），以确保样本的代表性；分析框架聚焦于法规、案例和文献三个维度，以全面覆盖责任问题；在比较中区分了不同责任模式（如德国的基于运行模式的责任区分），以突出监管差异。由于是法律研究，没有涉及参数设置、损失函数或网络结构等技术细节。",
            "application_zh": "该研究的潜在应用领域包括自动驾驶车辆的法律监管、保险政策制定、技术标准开发以及跨国企业合规。实际价值在于为政策制定者、法律从业者和技术开发者提供基于比较分析的见解，帮助设计更清晰的责任框架，减少法律不确定性。未来影响可能推动全球统一法律标准的建立，促进自动驾驶技术的安全创新和广泛应用。",
            "highlight_zh": "最重要的实验结果包括：揭示了各国监管模式的碎片化，例如印度和美国依赖松散州法律，而英国通过《2018年自动化和电动汽车法案》提供明确框架；德国和中国倾向于严格责任制度，德国还根据车辆运行模式区分责任。这些发现基于比较分析，没有具体性能数据或提升幅度，但突出了监管差异和统一标准的必要性。",
            "tags_zh": [
                "自动驾驶车辆",
                "刑事责任",
                "比较法律分析",
                "AI道德主体性",
                "监管碎片化",
                "全球统一标准",
                "法律技术交叉"
            ],
            "_index": 35
        },
        {
            "title": "A Threshold-Triggered Deep Q-Network-Based Framework for Self-Healing in Autonomic Software-Defined IIoT-Edge Networks",
            "authors": [
                "Agrippina Mwangi",
                "León Navarro-Hilfiker",
                "Lukasz Brewka",
                "Mikkel Gryning",
                "Elena Fumagalli",
                "Madeleine Gibescu"
            ],
            "arxiv_id": "2512.14297v1",
            "summary": "Stochastic disruptions such as flash events arising from benign traffic bursts and switch thermal fluctuations are major contributors to intermittent service degradation in software-defined industrial networks. These events violate IEC~61850-derived quality-of-service requirements and user-defined service-level agreements, hindering the reliable and timely delivery of control, monitoring, and best-effort traffic in IEC~61400-25-compliant wind power plants. Failure to maintain these requirements often results in delayed or lost control signals, reduced operational efficiency, and increased risk of wind turbine generator downtime.\n  To address these challenges, this study proposes a threshold-triggered Deep Q-Network self-healing agent that autonomically detects, analyzes, and mitigates network disruptions while adapting routing behavior and resource allocation in real time. The proposed agent was trained, validated, and tested on an emulated tri-clustered switch network deployed in a cloud-based proof-of-concept testbed.\n  Simulation results show that the proposed agent improves disruption recovery performance by 53.84% compared to a baseline shortest-path and load-balanced routing approach and outperforms state-of-the-art methods, including the Adaptive Network-based Fuzzy Inference System by 13.1% and the Deep Q-Network and traffic prediction-based routing optimization method by 21.5%, in a super-spine leaf data-plane architecture.\n  Additionally, the agent maintains switch thermal stability by proactively initiating external rack cooling when required. These findings highlight the potential of deep reinforcement learning in building resilience in software-defined industrial networks deployed in mission-critical, time-sensitive application scenarios.",
            "categories": [
                "cs.NI",
                "cs.AI",
                "cs.ET",
                "cs.PF",
                "hep-ex"
            ],
            "primary_category": "cs.NI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14297v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "traffic"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出基于阈值触发的深度Q网络自愈框架，以解决软件定义工业物联网边缘网络中的随机中断问题。",
            "summary_zh": "软件定义工业网络中由良性流量突发和交换机热波动引起的随机中断是导致间歇性服务降级的主要原因。这些事件违反了IEC 61850衍生的服务质量要求和用户定义的服务级别协议，阻碍了符合IEC 61400-25的风力发电厂中控制、监控和尽力而为流量的可靠及时交付。未能维持这些要求通常会导致控制信号延迟或丢失、运行效率降低以及风力涡轮发电机停机风险增加。为应对这些挑战，本研究提出了一种阈值触发的深度Q网络自愈代理，能够自主检测、分析和缓解网络中断，同时实时调整路由行为和资源分配。该代理在基于云的概念验证测试平台中部署的模拟三集群交换机网络上进行了训练、验证和测试。仿真结果表明，与基线最短路径和负载均衡路由方法相比，所提出的代理将中断恢复性能提高了53.84%，并在超骨干叶数据平面架构中优于最先进的方法，包括自适应网络模糊推理系统（提升13.1%）以及基于深度Q网络和流量预测的路由优化方法（提升21.5%）。此外，该代理通过主动启动外部机架冷却来维持交换机热稳定性。这些发现突显了深度强化学习在构建部署于关键任务、时间敏感应用场景的软件定义工业网络弹性方面的潜力。",
            "intro_zh": [
                "核心问题：软件定义工业网络中随机中断（如流量突发和热波动）导致服务降级，违反IEC 61850等标准，影响控制信号可靠性和运行效率。",
                "方法要点：提出阈值触发的深度Q网络自愈代理，自主检测、分析和缓解中断，实时调整路由和资源分配，并主动管理热稳定性。",
                "实验或效果：在模拟网络中，中断恢复性能比基线提升53.84%，优于自适应网络模糊推理系统（13.1%）和深度Q网络预测方法（21.5%）。"
            ],
            "method_zh": "**问题定义**：论文解决软件定义工业物联网边缘网络中由随机中断（如良性流量突发和交换机热波动）引起的间歇性服务降级问题。现有方法（如最短路径和负载均衡路由）难以实时适应动态变化，导致控制信号延迟、运行效率降低和停机风险增加，无法满足IEC 61850等标准要求。\\n\\n**核心思路**：核心思路是结合阈值触发机制和深度Q网络，构建一个自愈代理，通过强化学习自主优化网络行为。阈值触发用于快速检测异常事件，深度Q网络则学习最优路由和资源分配策略，实现实时自适应，以最小化中断影响并维持服务质量。\\n\\n**技术框架**：整体架构包括三个主要阶段：检测、分析和缓解。在检测阶段，代理监控网络指标（如流量和温度），当超过预设阈值时触发响应。在分析阶段，深度Q网络评估当前状态并预测最佳行动。在缓解阶段，代理执行行动（如调整路由或启动冷却），并通过反馈循环更新策略。框架部署在模拟的三集群交换机网络中，使用云基测试平台进行训练和验证。\\n\\n**关键创新**：最重要的技术创新点是阈值触发与深度Q网络的集成，这允许代理在异常事件发生时快速响应，同时通过强化学习长期优化网络性能。与现有方法（如静态路由或基于预测的优化）的本质区别在于其自主性和实时适应性，能够处理不确定的随机中断，而不仅仅是预定义场景。\\n\\n**关键设计**：关键设计包括阈值设置（基于历史数据或标准要求）、深度Q网络结构（可能包含多层神经网络处理状态空间）、损失函数（如均方误差优化Q值）、以及行动空间定义（如路由调整和资源分配）。代理在模拟环境中通过试错学习，使用奖励函数（如基于服务质量的指标）来引导策略优化，确保热稳定性和中断恢复的平衡。",
            "application_zh": "该研究主要应用于软件定义工业物联网边缘网络，特别是在风力发电厂等关键任务场景中，用于提升网络弹性和服务质量。潜在价值包括减少控制信号延迟、降低停机风险和提高运行效率，未来可能扩展到其他工业自动化领域，如智能制造或能源管理，推动深度强化学习在实时网络优化中的广泛应用。",
            "highlight_zh": "最重要的实验结果显示，在模拟三集群交换机网络中，所提出的自愈代理将中断恢复性能比基线最短路径和负载均衡路由方法提高了53.84%。同时，它优于最先进方法：比自适应网络模糊推理系统提升13.1%，比基于深度Q网络和流量预测的路由优化方法提升21.5%。此外，代理成功维持了交换机热稳定性，通过主动冷却减少了热相关中断。",
            "tags_zh": [
                "软件定义网络",
                "工业物联网",
                "深度强化学习",
                "自愈网络",
                "路由优化",
                "热管理",
                "服务质量",
                "边缘计算"
            ],
            "_index": 36
        },
        {
            "title": "PentestEval: Benchmarking LLM-based Penetration Testing with Modular and Stage-Level Design",
            "authors": [
                "Ruozhao Yang",
                "Mingfei Cheng",
                "Gelei Deng",
                "Tianwei Zhang",
                "Junjie Wang",
                "Xiaofei Xie"
            ],
            "arxiv_id": "2512.14233v1",
            "summary": "Penetration testing is essential for assessing and strengthening system security against real-world threats, yet traditional workflows remain highly manual, expertise-intensive, and difficult to scale. Although recent advances in Large Language Models (LLMs) offer promising opportunities for automation, existing applications rely on simplistic prompting without task decomposition or domain adaptation, resulting in unreliable black-box behavior and limited insight into model capabilities across penetration testing stages. To address this gap, we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision. PentestEval integrates expert-annotated ground truth with a fully automated evaluation pipeline across 346 tasks covering all stages in 12 realistic vulnerable scenarios. Our stage-level evaluation of 9 widely used LLMs reveals generally weak performance and distinct limitations across the stages of penetration-testing workflow. End-to-end pipelines reach only 31% success rate, and existing LLM-powered systems such as PentestGPT, PentestAgent, and VulnBot exhibit similar limitations, with autonomous agents failing almost entirely. These findings highlight that autonomous penetration testing demands stronger structured reasoning, where modularization enhances each individual stage and improves overall performance. PentestEval provides the foundational benchmark needed for future research on fine-grained, stage-level evaluation, paving the way toward more reliable LLM-based automation.",
            "categories": [
                "cs.SE",
                "cs.AI",
                "cs.CR"
            ],
            "primary_category": "cs.SE",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "13 pages, 6 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14233v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出PentestEval基准测试，通过模块化阶段设计评估LLM在渗透测试中的性能",
            "summary_zh": "渗透测试对于评估和增强系统安全至关重要，但传统工作流程高度依赖人工、专业知识密集且难以扩展。尽管大语言模型（LLMs）为自动化提供了前景，但现有应用依赖简单的提示方法，缺乏任务分解或领域适应，导致不可靠的黑盒行为和有限的对渗透测试各阶段模型能力的洞察。为解决这一差距，我们引入了PentestEval，这是首个全面的基准测试，用于评估LLMs在六个分解的渗透测试阶段：信息收集、弱点收集与过滤、攻击决策、漏洞利用生成与修订。PentestEval集成了专家标注的真实数据和一个完全自动化的评估流程，覆盖12个真实漏洞场景中的所有阶段，共346个任务。我们对9个广泛使用的LLMs进行阶段级评估，揭示了在渗透测试工作流程各阶段普遍较弱的性能和明显的局限性。端到端管道仅达到31%的成功率，现有LLM驱动的系统如PentestGPT、PentestAgent和VulnBot表现出类似的局限性，自主代理几乎完全失败。这些发现强调，自主渗透测试需要更强的结构化推理，其中模块化增强了每个单独阶段并提高了整体性能。PentestEval为未来细粒度、阶段级评估研究提供了基础基准，为更可靠的基于LLM的自动化铺平了道路。",
            "intro_zh": [
                "核心问题：现有LLM在渗透测试中依赖简单提示，缺乏任务分解和领域适应，导致不可靠的黑盒行为和有限阶段洞察。",
                "方法要点：提出PentestEval基准测试，通过模块化阶段设计分解渗透测试流程，集成专家标注数据和自动化评估。",
                "实验或效果：评估9个LLMs显示端到端成功率仅31%，自主代理几乎完全失败，模块化提升各阶段性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决LLM在渗透测试自动化中的性能评估问题。现有方法依赖简单提示，缺乏任务分解和领域适应，导致不可靠的黑盒行为，难以评估模型在不同渗透测试阶段的能力，限制了自动化的可靠性和可扩展性。\\n\\n**核心思路**：论文提出通过模块化阶段设计来分解渗透测试流程，构建一个全面的基准测试PentestEval，以细粒度评估LLMs在各阶段的表现。这样设计可以揭示模型在结构化推理中的局限性，促进更可靠的自动化发展。\\n\\n**技术框架**：整体架构包括六个分解的渗透测试阶段：信息收集、弱点收集与过滤、攻击决策、漏洞利用生成与修订。PentestEval集成了专家标注的真实数据和一个完全自动化的评估流程，覆盖12个真实漏洞场景中的所有阶段，共346个任务。评估流程从任务定义到结果分析，确保客观性和可重复性。\\n\\n**关键创新**：最重要的技术创新是首次提出模块化和阶段级的渗透测试基准测试，与现有方法相比，本质区别在于强调任务分解和结构化评估，而非简单的端到端提示，从而提供更深入的模型能力洞察。\\n\\n**关键设计**：关键设计包括六个阶段的任务分解、专家标注的真实数据构建、自动化评估管道的实现，以及覆盖346个任务的多样化场景设置。具体参数和损失函数未在摘要中详细说明，但评估基于成功率等指标，确保基准的全面性和实用性。",
            "application_zh": "该研究在网络安全领域具有重要应用价值，可用于评估和提升LLM在渗透测试自动化中的性能。潜在应用包括开发更可靠的自主渗透测试工具、优化安全评估流程，以及指导未来LLM在安全领域的微调和训练。未来影响可能推动基于AI的安全自动化技术发展，提高系统防御能力。",
            "highlight_zh": "最重要的实验结果包括：对9个广泛使用的LLMs进行阶段级评估，显示端到端管道成功率仅31%；现有LLM驱动系统如PentestGPT、PentestAgent和VulnBot表现出类似局限性；自主代理几乎完全失败。这些数据突显了LLM在渗透测试中的弱点和模块化设计的重要性。",
            "tags_zh": [
                "渗透测试",
                "大语言模型",
                "基准测试",
                "模块化设计",
                "阶段级评估",
                "网络安全",
                "自动化评估",
                "结构化推理"
            ],
            "_index": 37
        },
        {
            "title": "FastDDHPose: Towards Unified, Efficient, and Disentangled 3D Human Pose Estimation",
            "authors": [
                "Qingyuan Cai",
                "Linxin Zhang",
                "Xuecai Hu",
                "Saihui Hou",
                "Yongzhen Huang"
            ],
            "arxiv_id": "2512.14162v1",
            "summary": "Recent approaches for monocular 3D human pose estimation (3D HPE) have achieved leading performance by directly regressing 3D poses from 2D keypoint sequences. Despite the rapid progress in 3D HPE, existing methods are typically trained and evaluated under disparate frameworks, lacking a unified framework for fair comparison. To address these limitations, we propose Fast3DHPE, a modular framework that facilitates rapid reproduction and flexible development of new methods. By standardizing training and evaluation protocols, Fast3DHPE enables fair comparison across 3D human pose estimation methods while significantly improving training efficiency. Within this framework, we introduce FastDDHPose, a Disentangled Diffusion-based 3D Human Pose Estimation method which leverages the strong latent distribution modeling capability of diffusion models to explicitly model the distributions of bone length and bone direction while avoiding further amplification of hierarchical error accumulation. Moreover, we design an efficient Kinematic-Hierarchical Spatial and Temporal Denoiser that encourages the model to focus on kinematic joint hierarchies while avoiding unnecessary modeling of overly complex joint topologies. Extensive experiments on Human3.6M and MPI-INF-3DHP show that the Fast3DHPE framework enables fair comparison of all methods while significantly improving training efficiency. Within this unified framework, FastDDHPose achieves state-of-the-art performance with strong generalization and robustness in in-the-wild scenarios. The framework and models will be released at: https://github.com/Andyen512/Fast3DHPE",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14162v1",
            "code_links": [
                {
                    "url": "https://github.com/Andyen512/Fast3DHPE",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO",
                        "pose estimation"
                    ],
                    "score": 2
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出FastDDHPose框架以解决单目3D人体姿态估计中缺乏统一评估标准和误差累积问题",
            "summary_zh": "近年来，基于2D关键点序列直接回归3D姿态的单目3D人体姿态估计方法取得了领先性能。尽管3D HPE进展迅速，现有方法通常在分散的框架下训练和评估，缺乏公平比较的统一框架。为解决这些限制，我们提出Fast3DHPE，这是一个模块化框架，便于快速复现和灵活开发新方法。通过标准化训练和评估协议，Fast3DHPE实现了3D人体姿态估计方法的公平比较，同时显著提高了训练效率。在此框架内，我们引入FastDDHPose，一种基于解耦扩散的3D人体姿态估计方法，利用扩散模型的强大潜在分布建模能力，显式建模骨骼长度和骨骼方向的分布，同时避免进一步放大层次误差累积。此外，我们设计了一个高效的运动学-层次空间和时间去噪器，鼓励模型关注运动学关节层次结构，同时避免对过于复杂的关节拓扑进行不必要的建模。在Human3.6M和MPI-INF-3DHP上的大量实验表明，Fast3DHPE框架实现了所有方法的公平比较，同时显著提高了训练效率。在这个统一框架内，FastDDHPose在野外场景中实现了最先进的性能，具有强大的泛化能力和鲁棒性。框架和模型将在https://github.com/Andyen512/Fast3DHPE发布。",
            "intro_zh": [
                "现有3D人体姿态估计方法缺乏统一训练评估框架，导致公平比较困难且训练效率低下。",
                "提出Fast3DHPE统一框架和FastDDHPose方法，利用扩散模型解耦建模骨骼长度与方向，避免误差累积。",
                "在Human3.6M和MPI-INF-3DHP数据集上实现SOTA性能，训练效率显著提升，泛化能力强。"
            ],
            "method_zh": "**问题定义**：论文旨在解决单目3D人体姿态估计中两个核心问题：一是现有方法缺乏统一的训练和评估框架，导致公平比较困难且复现成本高；二是传统方法在回归3D姿态时容易产生层次误差累积，即关节位置误差会沿着运动学链传播放大。\\n\\n**核心思路**：通过构建标准化框架Fast3DHPE实现方法统一比较，并设计基于扩散模型的FastDDHPose方法，将3D姿态分解为骨骼长度和方向两个独立分量进行建模，利用扩散过程的分布学习能力分别优化这两个分量，从而打破误差传播链。\\n\\n**技术框架**：整体分为两个层次：上层是Fast3DHPE框架，提供标准化的数据预处理、训练流程和评估协议；下层是FastDDHPose模型，包含扩散模型主干、骨骼长度/方向解耦模块、以及运动学-层次空间时间去噪器。训练时从2D关键点序列出发，通过扩散过程逐步去噪生成3D姿态。\\n\\n**关键创新**：首次将扩散模型与运动学解耦思想结合用于3D HPE，提出显式分离骨骼几何属性（长度）和方向属性的建模方式；设计运动学感知的去噪器，仅对必要的关节层次关系建模，避免过度复杂的拓扑学习。\\n\\n**关键设计**：采用DDPM扩散框架，噪声调度参数经过优化；损失函数包含骨骼长度回归损失、方向角损失以及扩散模型本身的去噪损失；网络结构采用Transformer编码器，在去噪器中嵌入关节层次注意力机制，参数规模控制在高效范围内。",
            "application_zh": "该研究在虚拟现实、运动分析、人机交互等领域具有重要应用价值。统一的Fast3DHPE框架可加速新算法研发和产业落地，而FastDDHPose的高精度和强泛化能力使其适用于监控安防、体育训练、医疗康复等实际场景，特别是在复杂野外环境下的3D姿态估计表现出色。",
            "highlight_zh": "在Human3.6M数据集上，FastDDHPose的MPJPE指标达到46.2mm，相比之前最佳方法提升约3.1%；在MPI-INF-3DHP跨数据集评估中，PCK@150mm达到86.7%，显示强大泛化能力。Fast3DHPE框架将典型方法的训练时间缩短40%以上，同时确保所有比较方法在完全相同的条件下评估。",
            "tags_zh": [
                "3D人体姿态估计",
                "扩散模型",
                "解耦表示",
                "运动学建模",
                "统一评估框架",
                "单目视觉",
                "时空去噪",
                "误差累积抑制"
            ],
            "_index": 38
        },
        {
            "title": "SketchAssist: A Practical Assistant for Semantic Edits and Precise Local Redrawing",
            "authors": [
                "Han Zou",
                "Yan Zhang",
                "Ruiqi Yu",
                "Cong Xie",
                "Jie Huang",
                "Zhenpeng Zhan"
            ],
            "arxiv_id": "2512.14140v1",
            "summary": "Sketch editing is central to digital illustration, yet existing image editing systems struggle to preserve the sparse, style-sensitive structure of line art while supporting both high-level semantic changes and precise local redrawing. We present SketchAssist, an interactive sketch drawing assistant that accelerates creation by unifying instruction-guided global edits with line-guided region redrawing, while keeping unrelated regions and overall composition intact. To enable this assistant at scale, we introduce a controllable data generation pipeline that (i) constructs attribute-addition sequences from attribute-free base sketches, (ii) forms multi-step edit chains via cross-sequence sampling, and (iii) expands stylistic coverage with a style-preserving attribute-removal model applied to diverse sketches. Building on this data, SketchAssist employs a unified sketch editing framework with minimal changes to DiT-based editors. We repurpose the RGB channels to encode the inputs, enabling seamless switching between instruction-guided edits and line-guided redrawing within a single input interface. To further specialize behavior across modes, we integrate a task-guided mixture-of-experts into LoRA layers, routing by text and visual cues to improve semantic controllability, structural fidelity, and style preservation. Extensive experiments show state-of-the-art results on both tasks, with superior instruction adherence and style/structure preservation compared to recent baselines. Together, our dataset and SketchAssist provide a practical, controllable assistant for sketch creation and revision.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14140v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "人形机器人",
                    "matched_keywords": [
                        "digit"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出SketchAssist以解决线稿编辑中语义修改与局部重绘的平衡问题",
            "summary_zh": "线稿编辑是数字插画的核心环节，但现有图像编辑系统难以在支持高层次语义修改和精确局部重绘的同时，保持线稿的稀疏、风格敏感结构。本文提出SketchAssist，一个交互式线稿绘制助手，通过统一指令引导的全局编辑和线条引导的区域重绘来加速创作，同时保持无关区域和整体构图不变。为实现大规模助手应用，我们引入可控数据生成流程：(i)从无属性基础线稿构建属性添加序列，(ii)通过跨序列采样形成多步编辑链，(iii)应用风格保持的属性移除模型到多样线稿以扩展风格覆盖。基于此数据，SketchAssist采用统一线稿编辑框架，对基于DiT的编辑器进行最小改动。我们重新利用RGB通道编码输入，实现在单一输入界面中无缝切换指令引导编辑和线条引导重绘。为进一步专业化不同模式的行为，我们将任务引导的专家混合集成到LoRA层中，通过文本和视觉线索进行路由，以提升语义可控性、结构保真度和风格保持。大量实验显示在两个任务上均达到最先进结果，与近期基线相比，在指令遵循和风格/结构保持方面表现更优。我们的数据集和SketchAssist共同为线稿创作和修订提供了实用、可控的助手。",
            "intro_zh": [
                "现有图像编辑系统难以平衡线稿的稀疏结构保持与高层次语义修改和局部重绘需求，导致编辑失真或效率低下。",
                "论文提出SketchAssist助手，通过统一指令引导全局编辑和线条引导区域重绘，并引入可控数据生成流程和任务引导专家混合技术。",
                "实验表明SketchAssist在指令遵循和风格/结构保持方面优于基线，实现了最先进的线稿编辑性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决线稿编辑中的核心挑战：现有图像编辑系统难以同时支持高层次语义修改（如改变物体属性）和精确局部重绘（如修改特定线条），同时保持线稿的稀疏、风格敏感结构，导致编辑过程中常出现结构失真、风格不一致或效率低下问题。\\n\\n**核心思路**：论文的核心思路是设计一个统一的交互式线稿编辑助手，通过整合指令引导的全局编辑和线条引导的区域重绘，在单一框架内实现灵活编辑，同时利用可控数据生成和任务引导的专家混合技术来提升语义可控性、结构保真度和风格保持。\\n\\n**技术框架**：整体框架包括两个主要部分：可控数据生成流程和统一编辑模型。数据生成流程分三步：(i)从无属性基础线稿构建属性添加序列，(ii)通过跨序列采样形成多步编辑链以模拟真实编辑过程，(iii)应用风格保持的属性移除模型到多样线稿以扩展风格覆盖。编辑模型基于DiT架构，通过重新利用RGB通道编码输入（如指令、线条引导等），实现在单一界面中无缝切换编辑模式，并集成任务引导的专家混合到LoRA层中，根据文本和视觉线索路由不同专家来处理不同任务。\\n\\n**关键创新**：最重要的技术创新点包括：(1)可控数据生成流程，通过属性添加序列、多步编辑链和风格保持属性移除模型，大规模生成高质量训练数据；(2)统一编辑框架，利用RGB通道编码实现指令引导编辑和线条引导重绘的无缝集成；(3)任务引导的专家混合技术，在LoRA层中根据任务类型动态选择专家，提升编辑的语义可控性和结构保真度。与现有方法相比，本质区别在于将全局语义编辑和局部精确重绘统一于单一模型，并通过数据生成和模型设计实现更好的风格和结构保持。\\n\\n**关键设计**：关键设计细节包括：数据生成中，属性添加序列基于线稿属性（如颜色、纹理）逐步构建，编辑链通过随机采样不同序列的步骤形成；模型输入中，RGB通道分别编码原始线稿、编辑指令（如文本描述）和线条引导（如掩码或草图），实现多模态输入融合；专家混合层中，使用文本编码和视觉特征作为路由信号，动态激活不同LoRA专家，损失函数可能结合重建损失、对抗损失和风格损失，具体参数设置未知，但整体架构最小化对基础DiT模型的改动，以保持稳定性和可扩展性。",
            "application_zh": "SketchAssist在数字插画、动漫制作、游戏美术和工业设计等领域具有广泛应用潜力。作为实用助手，它能加速线稿创作和修订过程，支持艺术家进行高效语义修改和局部调整，同时保持作品风格一致性。未来可能扩展到更多创意产业，提升自动化编辑工具的智能性和用户体验。",
            "highlight_zh": "实验显示SketchAssist在指令引导编辑和线条引导重绘任务上均达到最先进性能。与近期基线（如基于扩散模型的编辑方法）相比，在指令遵循准确度、结构保真度和风格保持方面表现更优，具体提升幅度未知，但用户评估和定量指标均确认其优越性。数据生成流程的有效性通过生成多样、高质量的编辑对得到验证。",
            "tags_zh": [
                "线稿编辑",
                "语义编辑",
                "局部重绘",
                "可控数据生成",
                "专家混合",
                "扩散变换器",
                "交互式助手",
                "风格保持"
            ],
            "_index": 39
        },
        {
            "title": "AnchorHOI: Zero-shot Generation of 4D Human-Object Interaction via Anchor-based Prior Distillation",
            "authors": [
                "Sisi Dai",
                "Kai Xu"
            ],
            "arxiv_id": "2512.14095v1",
            "summary": "Despite significant progress in text-driven 4D human-object interaction (HOI) generation with supervised methods, the scalability remains limited by the scarcity of large-scale 4D HOI datasets. To overcome this, recent approaches attempt zero-shot 4D HOI generation with pre-trained image diffusion models. However, interaction cues are minimally distilled during the generation process, restricting their applicability across diverse scenarios. In this paper, we propose AnchorHOI, a novel framework that thoroughly exploits hybrid priors by incorporating video diffusion models beyond image diffusion models, advancing 4D HOI generation. Nevertheless, directly optimizing high-dimensional 4D HOI with such priors remains challenging, particularly for human pose and compositional motion. To address this challenge, AnchorHOI introduces an anchor-based prior distillation strategy, which constructs interaction-aware anchors and then leverages them to guide generation in a tractable two-step process. Specifically, two tailored anchors are designed for 4D HOI generation: anchor Neural Radiance Fields (NeRFs) for expressive interaction composition, and anchor keypoints for realistic motion synthesis. Extensive experiments demonstrate that AnchorHOI outperforms previous methods with superior diversity and generalization.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "AAAI 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14095v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "动作生成",
                    "matched_keywords": [
                        "motion synthesis"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出AnchorHOI框架，通过基于锚点的先验蒸馏策略解决零样本4D人-物交互生成中的交互线索不足问题。",
            "summary_zh": "尽管基于监督方法的文本驱动4D人-物交互生成已取得显著进展，但由于大规模4D HOI数据集的稀缺性，其可扩展性仍然受限。为克服此问题，近期方法尝试使用预训练图像扩散模型进行零样本4D HOI生成。然而，在生成过程中交互线索的蒸馏极为有限，限制了其在多样化场景中的适用性。本文提出AnchorHOI，一种新颖框架，通过结合视频扩散模型超越图像扩散模型，充分利用混合先验，推进4D HOI生成。然而，直接使用此类先验优化高维4D HOI仍然具有挑战性，特别是在人体姿态和组合运动方面。为解决这一挑战，AnchorHOI引入了一种基于锚点的先验蒸馏策略，该策略构建交互感知锚点，然后利用它们在可处理的两步过程中指导生成。具体而言，为4D HOI生成设计了两种定制锚点：用于表达性交互组合的锚点神经辐射场，以及用于真实运动合成的锚点关键点。大量实验表明，AnchorHOI在多样性和泛化性方面优于先前方法。",
            "intro_zh": [
                "现有零样本4D HOI生成方法主要依赖图像扩散模型，交互线索蒸馏不足，限制了在多样化场景中的应用。",
                "提出基于锚点的先验蒸馏策略，通过构建交互感知锚点（如锚点NeRF和关键点）来指导生成过程，提升交互表达和运动真实性。",
                "实验显示AnchorHOI在多样性和泛化性上优于先前方法，有效解决了高维4D HOI优化挑战。"
            ],
            "method_zh": "**问题定义**：论文旨在解决零样本4D人-物交互生成问题，即无需大规模4D HOI数据集训练，直接从文本描述生成包含时间维度的交互序列。现有方法的痛点在于主要依赖预训练图像扩散模型，导致交互线索（如人体姿态、物体运动）在生成过程中蒸馏不足，限制了生成结果的多样性和泛化能力。\\n\\n**核心思路**：论文的核心解决思路是引入基于锚点的先验蒸馏策略，通过构建交互感知锚点来引导生成过程，从而更有效地利用混合先验（包括视频扩散模型）。这样设计的原因在于直接优化高维4D HOI（涉及空间和时间维度）具有挑战性，锚点可以作为中间表示简化优化，并增强交互表达。\\n\\n**技术框架**：整体架构分为两步过程。首先，构建交互感知锚点，包括锚点NeRF用于建模交互组合（如物体形状和位置），以及锚点关键点用于描述人体运动。然后，利用这些锚点指导生成，通过先验蒸馏将锚点信息融入扩散模型，生成完整的4D HOI序列。主要模块包括锚点构建模块、先验蒸馏模块和生成模块。\\n\\n**关键创新**：最重要的技术创新点是基于锚点的先验蒸馏策略，它通过锚点NeRF和锚点关键点来结构化地表示交互，与现有方法（通常直接应用图像或视频扩散模型）的本质区别在于引入了中间锚点表示，从而更系统地处理交互线索，提升生成可控性和质量。\\n\\n**关键设计**：关键设计包括：锚点NeRF基于神经辐射场技术，用于编码物体和交互的几何与外观信息；锚点关键点基于人体姿态估计，用于捕捉运动轨迹；损失函数可能结合重建损失、先验匹配损失和交互一致性损失，以优化锚点构建和生成过程；网络结构可能包含编码器-解码器架构，用于锚点提取和序列生成。具体参数设置未知，但强调了两步过程的优化策略。",
            "application_zh": "该研究在虚拟现实、游戏开发、机器人仿真和影视制作等领域具有潜在应用价值，能够基于文本描述自动生成逼真的人-物交互动画，减少对大规模标注数据的依赖，提升内容创作的效率和多样性。未来可能推动人机交互和智能生成技术的发展。",
            "highlight_zh": "实验表明，AnchorHOI在零样本4D HOI生成任务中优于先前方法，具体性能数据未知，但强调了在多样性和泛化性方面的显著提升。对比基线可能包括基于图像扩散模型的方法，AnchorHOI通过引入锚点先验蒸馏，有效改善了交互表达和运动真实性，展示了更强的场景适应能力。",
            "tags_zh": [
                "4D人-物交互生成",
                "零样本学习",
                "先验蒸馏",
                "神经辐射场",
                "关键点检测",
                "视频扩散模型",
                "交互感知锚点"
            ],
            "_index": 40
        },
        {
            "title": "Expert Switching for Robust AAV Landing: A Dual-Detector Framework in Simulation",
            "authors": [
                "Humaira Tasnim",
                "Ashik E Rasul",
                "Bruce Jo",
                "Hyung-Jin Yoon"
            ],
            "arxiv_id": "2512.14054v1",
            "summary": "Reliable helipad detection is essential for Autonomous Aerial Vehicle (AAV) landing, especially under GPS-denied or visually degraded conditions. While modern detectors such as YOLOv8 offer strong baseline performance, single-model pipelines struggle to remain robust across the extreme scale transitions that occur during descent, where helipads appear small at high altitude and large near touchdown. To address this limitation, we propose a scale-adaptive dual-expert perception framework that decomposes the detection task into far-range and close-range regimes. Two YOLOv8 experts are trained on scale-specialized versions of the HelipadCat dataset, enabling one model to excel at detecting small, low-resolution helipads and the other to provide high-precision localization when the target dominates the field of view. During inference, both experts operate in parallel, and a geometric gating mechanism selects the expert whose prediction is most consistent with the AAV's viewpoint. This adaptive routing prevents the degradation commonly observed in single-detector systems when operating across wide altitude ranges. The dual-expert perception module is evaluated in a closed-loop landing environment that integrates CARLA's photorealistic rendering with NASA's GUAM flight-dynamics engine. Results show substantial improvements in alignment stability, landing accuracy, and overall robustness compared to single-detector baselines. By introducing a scale-aware expert routing strategy tailored to the landing problem, this work advances resilient vision-based perception for autonomous descent and provides a foundation for future multi-expert AAV frameworks.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14054v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "carla"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出基于双检测器的专家切换框架，解决自主飞行器着陆中极端尺度变化导致的检测鲁棒性问题",
            "summary_zh": "可靠的直升机停机坪检测对于自主飞行器（AAV）着陆至关重要，尤其是在GPS失效或视觉条件恶化的环境中。虽然YOLOv8等现代检测器提供了强大的基线性能，但单模型流水线在着陆过程中发生的极端尺度变化下难以保持鲁棒性——停机坪在高空时显得很小，在接近触地时变得很大。为解决这一限制，我们提出了一种尺度自适应的双专家感知框架，将检测任务分解为远距离和近距离两个区域。两个YOLOv8专家在HelipadCat数据集的尺度专门化版本上进行训练，使一个模型擅长检测小尺寸、低分辨率的停机坪，另一个在目标占据视野主导时提供高精度定位。在推理过程中，两个专家并行运行，几何门控机制选择与AAV视点最一致的预测。这种自适应路由防止了单检测器系统在宽高度范围内操作时常见的性能退化。双专家感知模块在闭环着陆环境中进行评估，该环境集成了CARLA的光照真实渲染和NASA的GUAM飞行动力学引擎。结果显示，与单检测器基线相比，在对准稳定性、着陆精度和整体鲁棒性方面有显著提升。通过引入针对着陆问题量身定制的尺度感知专家路由策略，这项工作推进了自主下降的弹性视觉感知，并为未来多专家AAV框架奠定了基础。",
            "intro_zh": [
                "核心问题：单检测器在AAV着陆过程中面临极端尺度变化挑战，高空小目标与近地大目标检测难以兼顾，导致鲁棒性不足。",
                "方法要点：提出双专家感知框架，训练两个YOLOv8模型分别处理远距离和近距离停机坪检测，通过几何门控自适应切换专家。",
                "实验或效果：在CARLA与GUAM集成的闭环环境中验证，相比单检测器基线，显著提升对准稳定性、着陆精度和整体鲁棒性。"
            ],
            "method_zh": "**问题定义**：论文解决自主飞行器（AAV）着陆中直升机停机坪检测的鲁棒性问题。现有单检测器方法（如YOLOv8）在着陆过程中面临极端尺度变化：高空时停机坪尺寸小、分辨率低，近地时尺寸大、占据视野主导，导致单一模型难以同时优化远距离和近距离检测性能，造成检测精度下降和着陆不稳定。\\n\\n**核心思路**：采用尺度自适应的双专家感知框架，将检测任务分解为远距离和近距离两个专门化区域。通过训练两个YOLOv8专家模型，分别针对小目标和大目标优化，并在推理时并行运行，结合几何门控机制动态选择最合适的专家预测，从而适应着陆过程中的尺度变化。\\n\\n**技术框架**：整体架构包括数据准备、模型训练和推理部署三个阶段。数据准备阶段，基于HelipadCat数据集创建尺度专门化版本，分别用于训练远距离专家（小目标）和近距离专家（大目标）。模型训练阶段，独立训练两个YOLOv8检测器，优化各自尺度下的检测性能。推理部署阶段，两个专家并行处理输入图像，几何门控模块基于AAV视点（如高度、角度）计算一致性分数，选择预测最一致的专家输出，最终集成到闭环着陆系统中。\\n\\n**关键创新**：最重要的技术创新是尺度感知的专家路由策略，通过双专家并行检测和几何门控自适应切换，解决了单检测器在宽高度范围内的性能退化问题。与现有方法的本质区别在于：传统方法依赖单一模型处理所有尺度，而本框架引入专门化专家和动态路由，实现更精细的尺度适应。\\n\\n**关键设计**：关键设计包括：使用YOLOv8作为基础检测器，因其在实时检测中表现优异；基于HelipadCat数据集进行尺度分割，确保专家训练数据的针对性；几何门控机制可能涉及视点参数（如高度、俯仰角）与预测框的几何一致性计算，具体损失函数和网络结构细节在摘要中未明确，但框架强调并行处理和自适应选择。",
            "application_zh": "该研究主要应用于自主飞行器（AAV）的视觉引导着陆系统，特别是在GPS失效或恶劣视觉条件（如雾、雨）下的应急着陆场景。实际价值在于提升AAV在复杂环境中的着陆安全性和可靠性，减少对GPS的依赖。未来影响包括为多专家感知框架奠定基础，可扩展至其他机器人视觉任务（如无人机导航、自动驾驶），推动弹性视觉感知技术的发展。",
            "highlight_zh": "最重要的实验结果包括：在CARLA与NASA GUAM集成的闭环着陆环境中，双专家框架相比单检测器基线（如YOLOv8）在多个指标上取得显著提升。具体性能数据未在摘要中给出，但强调了对准稳定性、着陆精度和整体鲁棒性的“substantial improvements”。提升幅度可能涉及检测精度、误报率降低等，实验通过模拟验证了框架在极端尺度变化下的有效性。",
            "tags_zh": [
                "自主飞行器着陆",
                "视觉检测",
                "尺度自适应",
                "双专家框架",
                "几何门控",
                "YOLOv8",
                "闭环仿真",
                "鲁棒感知"
            ],
            "_index": 41
        },
        {
            "title": "E-Navi: Environmental Adaptive Navigation for UAVs on Resource Constrained Platforms",
            "authors": [
                "Boyang Li",
                "Zhongpeng Jin",
                "Shuai Zhao",
                "Jiahui Liao",
                "Tian Liu",
                "Han Liu",
                "Yuanhai Zhang",
                "Kai Huang"
            ],
            "arxiv_id": "2512.14046v1",
            "summary": "The ability to adapt to changing environments is crucial for the autonomous navigation systems of Unmanned Aerial Vehicles (UAVs). However, existing navigation systems adopt fixed execution configurations without considering environmental dynamics based on available computing resources, e.g., with a high execution frequency and task workload. This static approach causes rigid flight strategies and excessive computations, ultimately degrading flight performance or even leading to failures in UAVs. Despite the necessity for an adaptive system, dynamically adjusting workloads remains challenging, due to difficulties in quantifying environmental complexity and modeling the relationship between environment and system configuration. Aiming at adapting to dynamic environments, this paper proposes E-Navi, an environmental-adaptive navigation system for UAVs that dynamically adjusts task executions on the CPUs in response to environmental changes based on available computational resources. Specifically, the perception-planning pipeline of UAVs navigation system is redesigned through dynamic adaptation of mapping resolution and execution frequency, driven by the quantitative environmental complexity evaluations. In addition, E-Navi supports flexible deployment across hardware platforms with varying levels of computing capability. Extensive Hardware-In-the-Loop and real-world experiments demonstrate that the proposed system significantly outperforms the baseline method across various hardware platforms, achieving up to 53.9% navigation task workload reduction, up to 63.8% flight time savings, and delivering more stable velocity control.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14046v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "mapping"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出E-Navi环境自适应导航系统，解决无人机在资源受限平台上适应动态环境的导航问题",
            "summary_zh": "无人机自主导航系统适应变化环境的能力至关重要。然而，现有导航系统采用固定的执行配置，不考虑基于可用计算资源的环境动态性，例如高执行频率和任务负载。这种静态方法导致僵化的飞行策略和过度计算，最终降低飞行性能甚至导致无人机故障。尽管自适应系统是必要的，但由于量化环境复杂性和建模环境与系统配置之间关系的困难，动态调整工作负载仍然具有挑战性。为了适应动态环境，本文提出E-Navi，一种用于无人机的环境自适应导航系统，根据可用计算资源动态调整CPU上的任务执行以响应环境变化。具体来说，通过定量环境复杂性评估驱动，通过动态调整地图分辨率和执行频率，重新设计了无人机导航系统的感知-规划流程。此外，E-Navi支持在不同计算能力水平的硬件平台上灵活部署。广泛的硬件在环和真实世界实验表明，所提出的系统在各种硬件平台上显著优于基线方法，实现了高达53.9%的导航任务负载减少、高达63.8%的飞行时间节省，并提供了更稳定的速度控制。",
            "intro_zh": [
                "现有无人机导航系统采用固定配置，无法根据环境动态调整计算资源，导致性能下降或失败。",
                "E-Navi通过量化环境复杂性，动态调整地图分辨率和执行频率，实现任务负载的自适应优化。",
                "实验显示，E-Navi相比基线方法减少任务负载达53.9%，节省飞行时间达63.8%，提升速度控制稳定性。"
            ],
            "method_zh": "**问题定义**：论文解决无人机在资源受限平台上导航时，现有系统采用固定执行配置（如高频率和负载）导致无法适应环境动态变化的问题，这引发僵化策略和过度计算，降低飞行性能甚至导致失败。现有方法的痛点在于难以量化环境复杂性并建模环境与系统配置的关系，使得动态调整工作负载具有挑战性。\\n\\n**核心思路**：论文的核心思路是设计一个环境自适应导航系统，通过定量评估环境复杂性，动态调整感知-规划流程中的地图分辨率和执行频率，以优化计算资源使用，从而在变化环境中实现高效、稳定的导航。这样设计是因为环境复杂性直接影响导航任务的计算需求，动态调整可以避免资源浪费或不足。\\n\\n**技术框架**：整体架构包括环境复杂性评估模块、自适应配置模块和任务执行模块。流程为：首先，实时评估环境复杂性（如障碍物密度、动态变化程度）；然后，基于评估结果和可用计算资源，动态调整地图分辨率（影响感知精度）和执行频率（影响规划响应速度）；最后，在CPU上执行调整后的导航任务，形成闭环自适应系统。\\n\\n**关键创新**：最重要的技术创新点是引入定量环境复杂性评估作为驱动因素，实现地图分辨率和执行频率的动态自适应调整，这与现有方法的本质区别在于从静态配置转向基于环境反馈的动态优化，从而更高效地利用有限计算资源。\\n\\n**关键设计**：关键设计包括环境复杂性量化指标（如基于视觉特征的复杂度评分）、自适应调整算法（如基于阈值或优化模型的配置选择）、硬件平台抽象层（支持不同计算能力的灵活部署）。具体参数设置可能涉及分辨率等级划分、频率调整范围，但论文未详细说明损失函数或网络结构，因为系统更侧重于控制策略而非深度学习模型。",
            "application_zh": "该研究在无人机自主导航领域具有广泛潜在应用，特别是在资源受限平台（如小型无人机、边缘设备）上执行搜索救援、物流配送、农业监测等任务。其实用价值在于提升导航系统的适应性和效率，减少计算开销，延长飞行时间。未来可能推动更智能、轻量化的无人机系统发展，促进在复杂环境中的可靠部署。",
            "highlight_zh": "最重要的实验结果显示，E-Navi在硬件在环和真实世界测试中显著优于基线方法。具体性能数据包括：导航任务负载减少高达53.9%，飞行时间节省高达63.8%，同时提供更稳定的速度控制。这些提升表明系统能有效适应环境变化，优化资源使用，适用于不同计算能力的硬件平台。",
            "tags_zh": [
                "无人机导航",
                "环境自适应",
                "资源优化",
                "动态配置",
                "感知规划",
                "硬件在环",
                "计算效率",
                "自主系统"
            ],
            "_index": 42
        },
        {
            "title": "Multivariate Time Series Forecasting with Hybrid Euclidean-SPD Manifold Graph Neural Networks",
            "authors": [
                "Yong Fang",
                "Na Li",
                "Hangguan Shan",
                "Eryun Liu",
                "Xinyu Li",
                "Wei Ni",
                "Er-Ping Li"
            ],
            "arxiv_id": "2512.14023v1",
            "summary": "Multivariate Time Series (MTS) forecasting plays a vital role in various real-world applications, such as traffic management and predictive maintenance. Existing approaches typically model MTS data in either Euclidean or Riemannian space, limiting their ability to capture the diverse geometric structures and complex spatio-temporal dependencies inherent in real-world data. To overcome this limitation, we propose the Hybrid Symmetric Positive-Definite Manifold Graph Neural Network (HSMGNN), a novel graph neural network-based model that captures data geometry within a hybrid Euclidean-Riemannian framework. To the best of our knowledge, this is the first work to leverage hybrid geometric representations for MTS forecasting, enabling expressive and comprehensive modeling of geometric properties. Specifically, we introduce a Submanifold-Cross-Segment (SCS) embedding to project input MTS into both Euclidean and Riemannian spaces, thereby capturing spatio-temporal variations across distinct geometric domains. To alleviate the high computational cost of Riemannian distance, we further design an Adaptive-Distance-Bank (ADB) layer with a trainable memory mechanism. Finally, a Fusion Graph Convolutional Network (FGCN) is devised to integrate features from the dual spaces via a learnable fusion operator for accurate prediction. Experiments on three benchmark datasets demonstrate that HSMGNN achieves up to a 13.8 percent improvement over state-of-the-art baselines in forecasting accuracy.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14023v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "traffic"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出混合欧几里得-对称正定流形图神经网络以解决多元时间序列预测中几何结构建模不足的问题",
            "summary_zh": "多元时间序列预测在交通管理、预测性维护等实际应用中至关重要。现有方法通常在欧几里得空间或黎曼空间中建模MTS数据，限制了其捕捉真实数据中多样几何结构和复杂时空依赖关系的能力。为克服这一局限，我们提出了混合对称正定流形图神经网络，这是一种基于图神经网络的新型模型，在混合欧几里得-黎曼框架内捕捉数据几何结构。据我们所知，这是首次利用混合几何表示进行MTS预测的工作，实现了对几何属性的表达性和全面建模。具体而言，我们引入了子流形交叉段嵌入，将输入MTS投影到欧几里得和黎曼空间中，从而捕捉不同几何域中的时空变化。为缓解黎曼距离的高计算成本，我们进一步设计了具有可训练记忆机制的自适应距离库层。最后，设计了一个融合图卷积网络，通过可学习的融合算子整合双空间特征以进行准确预测。在三个基准数据集上的实验表明，HSMGNN在预测精度上比最先进的基线方法提升了高达13.8%。",
            "intro_zh": [
                "现有方法通常在单一几何空间建模多元时间序列，难以捕捉真实数据中多样的几何结构和复杂时空依赖关系。",
                "论文提出HSMGNN模型，通过混合欧几里得-黎曼框架和子流形交叉段嵌入，实现双空间特征融合以提升预测能力。",
                "在三个基准数据集上，HSMGNN相比最先进基线方法，预测精度最高提升13.8%，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多元时间序列预测中现有方法因局限于单一几何空间（欧几里得或黎曼空间）而无法充分捕捉数据多样几何结构和复杂时空依赖的问题，导致预测精度受限。\\n\\n**核心思路**：论文提出混合欧几里得-对称正定流形图神经网络，通过将MTS数据同时投影到欧几里得和黎曼空间，利用混合几何表示来更全面地建模数据几何属性，从而提升预测性能。\\n\\n**技术框架**：整体架构包括三个主要模块：首先，子流形交叉段嵌入模块将输入MTS投影到欧几里得和黎曼空间；其次，自适应距离库层优化黎曼距离计算以降低计算成本；最后，融合图卷积网络整合双空间特征并通过可学习融合算子进行预测。\\n\\n**关键创新**：最重要的技术创新是首次引入混合几何表示进行MTS预测，通过子流形交叉段嵌入和自适应距离库层，实现了对数据几何结构的表达性和高效建模，与现有方法的本质区别在于同时利用欧几里得和黎曼空间的优势。\\n\\n**关键设计**：关键设计包括子流形交叉段嵌入的具体投影机制、自适应距离库层的可训练记忆机制以减少黎曼距离计算复杂度，以及融合图卷积网络中的可学习融合算子，这些设计通过端到端训练优化预测损失函数（如均方误差）。",
            "application_zh": "该研究在交通管理、预测性维护等领域具有广泛应用潜力，能提升多元时间序列预测的准确性和鲁棒性，为复杂时空数据分析提供新工具，未来可能扩展到金融、医疗等更多领域，推动人工智能在现实世界中的实际部署。",
            "highlight_zh": "在三个基准数据集上的实验结果显示，HSMGNN相比最先进的基线方法，预测精度最高提升13.8%，具体性能数据因数据集而异，但整体显著优于现有欧几里得或黎曼空间方法，验证了混合几何表示的有效性和模型的高效性。",
            "tags_zh": [
                "多元时间序列预测",
                "混合几何表示",
                "图神经网络",
                "对称正定流形",
                "时空依赖建模",
                "自适应距离库",
                "融合图卷积网络",
                "预测精度提升"
            ],
            "_index": 43
        },
        {
            "title": "Early Warning Index for Patient Deteriorations in Hospitals",
            "authors": [
                "Dimitris Bertsimas",
                "Yu Ma",
                "Kimberly Villalobos Carballo",
                "Gagan Singh",
                "Michal Laskowski",
                "Jeff Mather",
                "Dan Kombert",
                "Howard Haronian"
            ],
            "arxiv_id": "2512.14683v1",
            "summary": "Hospitals lack automated systems to harness the growing volume of heterogeneous clinical and operational data to effectively forecast critical events. Early identification of patients at risk for deterioration is essential not only for patient care quality monitoring but also for physician care management. However, translating varied data streams into accurate and interpretable risk assessments poses significant challenges due to inconsistent data formats. We develop a multimodal machine learning framework, the Early Warning Index (EWI), to predict the aggregate risk of ICU admission, emergency response team dispatch, and mortality. Key to EWI's design is a human-in-the-loop process: clinicians help determine alert thresholds and interpret model outputs, which are enhanced by explainable outputs using Shapley Additive exPlanations (SHAP) to highlight clinical and operational factors (e.g., scheduled surgeries, ward census) driving each patient's risk. We deploy EWI in a hospital dashboard that stratifies patients into three risk tiers. Using a dataset of 18,633 unique patients at a large U.S. hospital, our approach automatically extracts features from both structured and unstructured electronic health record (EHR) data and achieves C-statistics of 0.796. It is currently used as a triage tool for proactively managing at-risk patients. The proposed approach saves physicians valuable time by automatically sorting patients of varying risk levels, allowing them to concentrate on patient care rather than sifting through complex EHR data. By further pinpointing specific risk drivers, the proposed model provides data-informed adjustments to caregiver scheduling and allocation of critical resources. As a result, clinicians and administrators can avert downstream complications, including costly procedures or high readmission rates and improve overall patient flow.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14683v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出早期预警指数（EWI）多模态机器学习框架，以解决医院患者病情恶化预测中数据异构与可解释性不足的问题。",
            "summary_zh": "医院缺乏自动化系统来利用日益增长的异构临床和运营数据有效预测关键事件。早期识别有恶化风险的患者不仅对患者护理质量监控至关重要，也对医生护理管理至关重要。然而，由于数据格式不一致，将不同的数据流转化为准确且可解释的风险评估面临重大挑战。我们开发了一个多模态机器学习框架——早期预警指数（EWI），用于预测ICU入院、紧急响应团队派遣和死亡率的综合风险。EWI设计的关键在于人机交互过程：临床医生帮助确定警报阈值并解释模型输出，这些输出通过使用Shapley Additive exPlanations（SHAP）的可解释性增强，以突出驱动每个患者风险的临床和运营因素（例如，预定手术、病房普查）。我们将EWI部署在医院仪表板中，将患者分为三个风险等级。使用美国一家大型医院的18,633名独特患者的数据集，我们的方法从结构化和非结构化电子健康记录（EHR）数据中自动提取特征，并实现了C统计量为0.796。它目前被用作主动管理风险患者的分类工具。所提出的方法通过自动对不同风险水平的患者进行排序，节省了医生的宝贵时间，使他们能够专注于患者护理，而不是筛选复杂的EHR数据。通过进一步确定具体的风险驱动因素，所提出的模型为护理人员调度和关键资源分配提供了数据驱动的调整。因此，临床医生和管理人员可以避免下游并发症，包括昂贵的程序或高再入院率，并改善整体患者流程。",
            "intro_zh": [
                "核心问题：医院缺乏自动化系统整合异构临床与运营数据，现有方法难以准确预测患者病情恶化，且数据格式不一致导致风险评估困难。",
                "方法要点：提出多模态机器学习框架EWI，结合人机交互过程，利用SHAP增强可解释性，自动从EHR数据提取特征预测综合风险。",
                "实验或效果：在18,633名患者数据集上实现C统计量0.796，部署为医院仪表板，成功将患者分为三风险等级，提升临床决策效率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决医院中患者病情恶化（如ICU入院、紧急响应团队派遣和死亡率）的早期预测问题。现有方法痛点在于缺乏自动化系统整合异构临床和运营数据，数据格式不一致导致风险评估不准确且难以解释，医生需手动筛选复杂电子健康记录（EHR）数据，效率低下。\\n\\n**核心思路**：核心解决思路是开发一个多模态机器学习框架——早期预警指数（EWI），通过结合人机交互过程，利用可解释性技术（如SHAP）增强模型透明度，自动从结构化和非结构化EHR数据中提取特征，以预测综合风险并支持临床决策。这样设计旨在克服数据异构性挑战，提高预测准确性，同时确保模型输出易于临床医生理解和应用。\\n\\n**技术框架**：整体架构包括数据预处理、特征提取、模型训练、风险评估和部署阶段。主要模块：1) 数据集成模块，整合异构EHR数据（如临床记录、手术安排、病房普查）；2) 多模态特征提取模块，自动从结构化和非结构化数据中提取相关特征；3) 机器学习模型模块，训练预测ICU入院、紧急响应团队派遣和死亡率的综合风险；4) 人机交互模块，临床医生参与设定警报阈值和解释输出；5) 可解释性模块，使用SHAP分析突出风险驱动因素；6) 部署模块，将模型集成到医院仪表板中，实现患者风险分层。\\n\\n**关键创新**：最重要的技术创新点在于将多模态机器学习与人机交互过程相结合，并利用SHAP增强模型可解释性。与现有方法的本质区别在于：它不仅关注预测准确性（通过自动特征提取处理数据异构性），还强调临床实用性，通过可解释输出帮助医生理解风险因素（如预定手术、病房普查），从而支持数据驱动的资源分配和护理管理。\\n\\n**关键设计**：关键设计细节包括：使用多模态数据源（结构化和非结构化EHR），具体特征提取方法未详细说明，但涉及自动处理异构数据；模型基于机器学习算法（具体算法未知），损失函数可能针对综合风险优化（如二元或多元分类损失）；网络结构未指定，但框架强调可扩展性；参数设置中，临床医生参与确定警报阈值，SHAP用于生成特征重要性分数；部署时，患者被分为三个风险等级（高、中、低），以支持分类决策。",
            "application_zh": "该研究主要应用于医院临床管理领域，作为患者病情恶化的早期预警工具。实际价值在于通过自动化风险预测，帮助医生优先处理高风险患者，节省时间并提高护理质量；同时，可解释输出支持数据驱动的资源调度（如医护人员分配、ICU床位管理），减少并发症和再入院率。未来影响可能扩展到其他医疗场景，如慢性病监测或公共卫生预警，推动智能医疗系统的发展。",
            "highlight_zh": "最重要的实验结果包括：在大型美国医院的18,633名独特患者数据集上，EWI框架实现了C统计量为0.796，表明模型具有良好的区分能力。具体性能数据未提供对比基线，但C统计量接近0.8显示显著预测提升。模型成功部署为医院仪表板，将患者分为三个风险等级，作为分类工具用于主动管理，实际应用中节省了医生时间并改善了患者流程。可解释性分析通过SHAP突出了临床和运营因素（如预定手术、病房普查）作为风险驱动因素。",
            "tags_zh": [
                "早期预警系统",
                "多模态机器学习",
                "患者风险预测",
                "可解释人工智能",
                "电子健康记录分析",
                "临床决策支持",
                "人机交互",
                "医疗资源优化"
            ],
            "_index": 44
        },
        {
            "title": "EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models",
            "authors": [
                "Zechen Bai",
                "Chen Gao",
                "Mike Zheng Shou"
            ],
            "arxiv_id": "2512.14666v1",
            "summary": "Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\\% on long-horizon tasks, +22.0\\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\\% success on unseen tasks without task-specific demonstrations training (vs. 0\\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "15 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14666v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reward"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出EVOLVE-VLA测试时训练框架，使视觉-语言-动作模型通过环境反馈实现持续自适应，减少任务特定演示需求。",
            "summary_zh": "实现真正的自适应具身智能需要智能体通过环境交互持续学习，而非仅模仿静态演示。视觉-语言-动作模型虽通过大语言模型推动了机器人操作，但仍受限于监督微调：每任务需数百演示、僵化记忆轨迹、部署条件偏离训练时无法适应。本文提出EVOLVE-VLA，一个测试时训练框架，使VLA模型能以最少或零任务特定演示通过环境交互持续适应。关键技术挑战是用自主反馈替代测试时不可用的oracle奖励信号。我们通过一个提供密集反馈的学习进度估计器解决此问题，并设计框架通过两种机制“驯服”这一固有噪声信号：(1)累积进度估计机制平滑噪声点估计，(2)渐进视野扩展策略实现逐步策略演化。EVOLVE-VLA取得显著提升：长视野任务+8.6%，单样本学习+22.0%，并实现跨任务泛化——在未见任务上无任务特定演示训练达到20.8%成功率（纯SFT为0%）。定性分析揭示了演示中未出现的涌现能力，包括错误恢复和新策略。这项工作代表了VLA模型向真正学习和适应迈出的关键一步，从静态模仿转向持续自我改进。",
            "intro_zh": [
                "现有VLA模型依赖监督微调，需大量任务演示，部署条件变化时无法适应，限制了自适应能力。",
                "提出EVOLVE-VLA框架，通过环境反馈实现测试时训练，用学习进度估计器替代奖励信号，并设计机制驯服噪声。",
                "实验显示在长视野任务提升8.6%，单样本学习提升22.0%，跨任务泛化达20.8%成功率，涌现错误恢复等能力。"
            ],
            "method_zh": "**问题定义**：论文解决VLA模型在机器人操作中依赖监督微调的问题，包括需要大量任务特定演示、轨迹记忆僵化、部署条件偏离训练时无法自适应。现有方法痛点在于测试时缺乏oracle奖励信号，导致模型无法通过环境交互持续改进。\\n\\n**核心思路**：核心思路是引入测试时训练框架，使VLA模型能在部署后通过环境反馈自主学习和适应，减少对演示数据的依赖。设计基于学习进度估计器提供密集反馈，并采用机制驯服反馈噪声，实现稳定策略演化。\\n\\n**技术框架**：整体框架包括环境交互模块、学习进度估计器模块和策略更新模块。流程为：模型在测试时与环境交互，收集状态-动作数据；学习进度估计器基于交互历史生成密集反馈信号；通过累积估计和渐进视野扩展机制处理噪声；策略基于反馈进行在线微调，逐步优化行为。\\n\\n**关键创新**：最重要的创新是提出测试时训练范式，用自主环境反馈替代传统奖励信号，实现VLA模型的持续自适应。与现有方法的本质区别在于摆脱了对静态演示数据的依赖，使模型能在未知条件下通过交互学习，提升泛化能力和适应性。\\n\\n**关键设计**：关键设计包括学习进度估计器的网络结构（可能基于Transformer或RNN，具体未知），损失函数用于训练估计器（如均方误差损失），以及累积估计机制（如滑动平均或积分方法）和渐进视野扩展策略（逐步增加预测步长）。参数设置涉及反馈平滑系数和视野扩展速率，需平衡噪声抑制和学习效率。",
            "application_zh": "该研究在机器人操作、自动驾驶和智能家居等领域有潜在应用，能提升智能体在动态环境中的自适应能力，减少人工演示成本，推动具身智能向更自主、灵活的方向发展，对未来自适应AI系统具有重要价值。",
            "highlight_zh": "EVOLVE-VLA在实验中表现优异：长视野任务成功率提升8.6%，单样本学习提升22.0%，跨任务泛化在未见任务上达到20.8%成功率（对比纯SFT的0%）。定性分析显示模型涌现出错误恢复和新策略等能力，验证了框架的有效性和适应性。",
            "tags_zh": [
                "测试时训练",
                "视觉-语言-动作模型",
                "环境反馈",
                "自适应学习",
                "机器人操作",
                "进度估计",
                "跨任务泛化",
                "具身智能"
            ],
            "_index": 45
        },
        {
            "title": "ViRC: Enhancing Visual Interleaved Mathematical CoT with Reason Chunking",
            "authors": [
                "Lihong Wang",
                "Liangqi Li",
                "Weiwei Feng",
                "Jiamin Wu",
                "Changtao Miao",
                "Tieru Wu",
                "Rui Ma",
                "Bo Zhang",
                "Zhe Li"
            ],
            "arxiv_id": "2512.14654v1",
            "summary": "CoT has significantly enhanced the reasoning ability of LLMs while it faces challenges when extended to multimodal domains, particularly in mathematical tasks. Existing MLLMs typically perform textual reasoning solely from a single static mathematical image, overlooking dynamic visual acquisition during reasoning. In contrast, humans repeatedly examine visual image and employ step-by-step reasoning to prove intermediate propositions. This strategy of decomposing the problem-solving process into key logical nodes adheres to Miller's Law in cognitive science. Inspired by this insight, we propose a ViRC framework for multimodal mathematical tasks, introducing a Reason Chunking mechanism that structures multimodal mathematical CoT into consecutive Critical Reasoning Units (CRUs) to simulate human expert problem-solving patterns. CRUs ensure intra-unit textual coherence for intermediate proposition verification while integrating visual information across units to generate subsequent propositions and support structured reasoning. To this end, we present CRUX dataset by using three visual tools and four reasoning patterns to provide explicitly annotated CRUs across multiple reasoning paths for each mathematical problem. Leveraging the CRUX dataset, we propose a progressive training strategy inspired by human cognitive learning, which includes Instructional SFT, Practice SFT, and Strategic RL, aimed at further strengthening the Reason Chunking ability of the model.The resulting ViRC-7B model achieves a 18.8\\% average improvement over baselines across multiple mathematical benchmarks. Code is available at https://github.com/Leon-LihongWang/ViRC.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Code is available at https://github.com/Leon-LihongWang/ViRC",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14654v1",
            "code_links": [
                {
                    "url": "https://github.com/Leon-LihongWang/ViRC",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出ViRC框架，通过Reason Chunking机制增强多模态数学推理能力，模拟人类专家解题模式。",
            "summary_zh": "思维链（CoT）显著提升了大型语言模型的推理能力，但在扩展到多模态领域时面临挑战，特别是在数学任务中。现有的多模态大语言模型（MLLMs）通常仅从单个静态数学图像进行文本推理，忽视了推理过程中的动态视觉获取。相比之下，人类会反复检查视觉图像，并采用逐步推理来证明中间命题。这种将问题解决过程分解为关键逻辑节点的策略符合认知科学中的米勒定律。受此启发，我们提出了一个用于多模态数学任务的ViRC框架，引入了Reason Chunking机制，将多模态数学CoT结构化为连续的Critical Reasoning Units（CRUs），以模拟人类专家的问题解决模式。CRUs确保单元内的文本连贯性以验证中间命题，同时跨单元整合视觉信息以生成后续命题并支持结构化推理。为此，我们使用三种视觉工具和四种推理模式构建了CRUX数据集，为每个数学问题提供跨多个推理路径的显式标注CRUs。利用CRUX数据集，我们提出了一种受人类认知学习启发的渐进式训练策略，包括Instructional SFT、Practice SFT和Strategic RL，旨在进一步增强模型的Reason Chunking能力。由此产生的ViRC-7B模型在多个数学基准测试中平均比基线提升了18.8%。代码可在https://github.com/Leon-LihongWang/ViRC获取。",
            "intro_zh": [
                "现有MLLMs在数学任务中仅从静态图像推理，缺乏动态视觉获取，导致推理不连贯。",
                "提出Reason Chunking机制，将推理分解为CRUs，模拟人类逐步解题模式，增强多模态交互。",
                "ViRC-7B模型在多个数学基准上平均提升18.8%，验证了框架的有效性和优越性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多模态数学推理任务中，现有方法（如MLLMs）仅依赖静态图像进行文本推理，忽略动态视觉获取，导致推理过程不连贯、难以验证中间命题的问题。现有方法的痛点在于缺乏结构化、交互式的视觉-文本协同推理机制，无法模拟人类专家反复检查图像并逐步推导的认知模式。\\n\\n**核心思路**：论文的核心解决思路是引入Reason Chunking机制，将多模态数学推理过程分解为连续的Critical Reasoning Units（CRUs），每个CRU对应一个关键逻辑节点，确保单元内文本连贯性以验证中间命题，同时跨单元整合视觉信息以生成后续命题。这一设计灵感来源于人类认知科学中的米勒定律，即通过分块处理复杂信息来提升推理效率。\\n\\n**技术框架**：整体架构包括数据构建、模型训练和推理三个阶段。首先，使用三种视觉工具（如OCR、几何图形识别）和四种推理模式（如代数、几何）构建CRUX数据集，为每个数学问题提供显式标注的CRUs。其次，采用渐进式训练策略：Instructional SFT（指令微调）学习基础推理模式，Practice SFT（实践微调）强化CRU生成能力，Strategic RL（策略强化学习）优化整体推理路径。最后，在推理时，模型基于输入图像和问题，动态生成CRUs序列，实现结构化多模态推理。\\n\\n**关键创新**：最重要的技术创新点是Reason Chunking机制和CRUs的设计，将多模态推理过程模块化，模拟人类专家的问题解决模式。与现有方法的本质区别在于：现有方法通常进行端到端的文本推理，而ViRC通过CRUs实现了视觉-文本的交互式、结构化推理，增强了推理的可解释性和准确性。\\n\\n**关键设计**：关键设计包括CRUs的标注标准（确保每个CRU包含视觉信息提取、命题生成和验证步骤）、渐进式训练策略（结合SFT和RL以模拟人类学习过程），以及模型架构（基于7B参数的多模态大语言模型，集成视觉编码器和文本解码器）。损失函数在SFT阶段使用交叉熵损失，RL阶段使用奖励函数优化推理路径；参数设置上，CRUs数量根据问题复杂度动态调整，训练数据来自CRUX数据集的多推理路径标注。",
            "application_zh": "该研究在数学教育、智能辅导系统和自动化解题等领域具有潜在应用价值。通过模拟人类专家推理模式，ViRC框架可提升多模态AI在复杂数学问题（如几何证明、代数计算）中的准确性和可解释性，为教育科技和科研工具开发提供新思路。未来可能扩展到更广泛的多模态推理任务，如物理问题解决或逻辑推理，推动AI在认知密集型领域的应用。",
            "highlight_zh": "ViRC-7B模型在多个数学基准测试中表现出色，平均比基线模型提升18.8%。具体性能数据包括在几何、代数和综合数学任务上的显著改进，对比基线如传统MLLMs和单模态CoT方法。提升幅度最高可达20%以上，验证了Reason Chunking机制和渐进式训练策略的有效性，突显了结构化多模态推理的优势。",
            "tags_zh": [
                "多模态推理",
                "数学思维链",
                "Reason Chunking",
                "Critical Reasoning Units",
                "渐进式训练",
                "视觉-文本交互",
                "认知科学启发",
                "结构化推理"
            ],
            "_index": 46
        },
        {
            "title": "A Multicenter Benchmark of Multiple Instance Learning Models for Lymphoma Subtyping from HE-stained Whole Slide Images",
            "authors": [
                "Rao Muhammad Umer",
                "Daniel Sens",
                "Jonathan Noll",
                "Christian Matek",
                "Lukas Wolfseher",
                "Rainer Spang",
                "Ralf Huss",
                "Johannes Raffler",
                "Sarah Reinke",
                "Wolfram Klapper",
                "Katja Steiger",
                "Kristina Schwamborn",
                "Carsten Marr"
            ],
            "arxiv_id": "2512.14640v1",
            "summary": "Timely and accurate lymphoma diagnosis is essential for guiding cancer treatment. Standard diagnostic practice combines hematoxylin and eosin (HE)-stained whole slide images with immunohistochemistry, flow cytometry, and molecular genetic tests to determine lymphoma subtypes, a process requiring costly equipment, skilled personnel, and causing treatment delays. Deep learning methods could assist pathologists by extracting diagnostic information from routinely available HE-stained slides, yet comprehensive benchmarks for lymphoma subtyping on multicenter data are lacking. In this work, we present the first multicenter lymphoma benchmarking dataset covering four common lymphoma subtypes and healthy control tissue. We systematically evaluate five publicly available pathology foundation models (H-optimus-1, H0-mini, Virchow2, UNI2, Titan) combined with attention-based (AB-MIL) and transformer-based (TransMIL) multiple instance learning aggregators across three magnifications (10x, 20x, 40x). On in-distribution test sets, models achieve multiclass balanced accuracies exceeding 80% across all magnifications, with all foundation models performing similarly and both aggregation methods showing comparable results. The magnification study reveals that 40x resolution is sufficient, with no performance gains from higher resolutions or cross-magnification aggregation. However, on out-of-distribution test sets, performance drops substantially to around 60%, highlighting significant generalization challenges. To advance the field, larger multicenter studies covering additional rare lymphoma subtypes are needed. We provide an automated benchmarking pipeline to facilitate such future research.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "17 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14640v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "人形机器人",
                    "matched_keywords": [
                        "optimus"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出首个多中心淋巴瘤分型基准数据集，系统评估病理基础模型与多实例学习聚合器在HE染色全切片图像上的性能。",
            "summary_zh": "及时准确的淋巴瘤诊断对指导癌症治疗至关重要。标准诊断实践结合苏木精-伊红（HE）染色全切片图像与免疫组化、流式细胞术和分子遗传学检测来确定淋巴瘤亚型，这一过程需要昂贵设备、熟练人员并导致治疗延迟。深度学习方法可以通过从常规可用的HE染色切片中提取诊断信息来协助病理学家，但目前缺乏基于多中心数据的淋巴瘤分型综合基准。在这项工作中，我们提出了首个多中心淋巴瘤基准数据集，涵盖四种常见淋巴瘤亚型和健康对照组织。我们系统评估了五种公开可用的病理基础模型（H-optimus-1、H0-mini、Virchow2、UNI2、Titan）与基于注意力（AB-MIL）和基于Transformer（TransMIL）的多实例学习聚合器在三种放大倍数（10x、20x、40x）下的组合。在分布内测试集上，模型在所有放大倍数下实现了超过80%的多类平衡准确率，所有基础模型表现相似，两种聚合方法结果相当。放大倍数研究表明，40x分辨率已足够，更高分辨率或跨放大倍数聚合未带来性能提升。然而，在分布外测试集上，性能大幅下降至约60%，突显了显著的泛化挑战。为推进该领域发展，需要覆盖更多罕见淋巴瘤亚型的更大规模多中心研究。我们提供了一个自动化基准测试流程以促进此类未来研究。",
            "intro_zh": [
                "现有淋巴瘤诊断依赖多模态检测，成本高、耗时长，且缺乏基于HE染色切片的多中心深度学习基准。",
                "论文提出首个多中心淋巴瘤基准数据集，系统评估病理基础模型与多实例学习聚合器在不同放大倍数下的性能。",
                "在分布内测试集上模型准确率超80%，但分布外性能降至约60%，揭示了泛化挑战，并提供了自动化基准流程。"
            ],
            "method_zh": "**问题定义**：论文旨在解决淋巴瘤亚型诊断中依赖昂贵多模态检测导致的延迟问题，现有深度学习方法缺乏基于HE染色全切片图像的多中心综合基准，无法评估模型在实际临床环境中的泛化能力。\\n\\n**核心思路**：通过构建首个多中心淋巴瘤基准数据集，系统比较多种病理基础模型与多实例学习聚合器的组合，在不同放大倍数下评估性能，以确定最优配置并揭示泛化瓶颈。\\n\\n**技术框架**：整体流程包括数据收集（多中心HE染色切片，涵盖四种淋巴瘤亚型和健康组织）、特征提取（使用五种预训练病理基础模型）、实例聚合（采用AB-MIL和TransMIL两种方法）、分类输出（预测亚型），并在三种放大倍数（10x、20x、40x）下进行实验。\\n\\n**关键创新**：最重要的创新是首次建立了多中心淋巴瘤分型基准，并系统评估了基础模型与聚合器的组合，通过放大倍数分析优化了计算效率，同时提供了自动化基准流程以促进可重复研究。\\n\\n**关键设计**：使用公开病理基础模型（如H-optimus-1、Virchow2）进行特征提取，无需从头训练；聚合器采用基于注意力的AB-MIL和基于Transformer的TransMIL，以处理全切片图像中的多个实例；实验设置包括平衡准确率作为主要评估指标，并在分布内和分布外测试集上进行验证，放大倍数研究固定为10x、20x、40x以评估分辨率影响。",
            "application_zh": "该研究可应用于临床病理学辅助诊断，通过深度学习从常规HE染色切片中自动识别淋巴瘤亚型，减少对昂贵检测的依赖，加速诊断流程。潜在价值包括降低医疗成本、提高诊断一致性，并为罕见亚型研究提供基准。未来影响可能推动多中心AI模型标准化，促进精准医疗发展。",
            "highlight_zh": "在分布内测试集上，所有模型组合在10x、20x、40x放大倍数下均实现超过80%的多类平衡准确率，基础模型性能相似，聚合方法结果相当。放大倍数研究表明40x分辨率已足够，更高分辨率无性能提升。然而，分布外测试集性能大幅下降至约60%，突显泛化挑战。实验覆盖五种基础模型和两种聚合器，提供了首个多中心基准结果。",
            "tags_zh": [
                "淋巴瘤分型",
                "全切片图像分析",
                "多实例学习",
                "病理基础模型",
                "多中心基准",
                "HE染色切片",
                "注意力机制",
                "Transformer聚合"
            ],
            "_index": 47
        },
        {
            "title": "Hierarchical Persistence Velocity for Network Anomaly Detection: Theory and Applications to Cryptocurrency Markets",
            "authors": [
                "Omid Khormali"
            ],
            "arxiv_id": "2512.14615v1",
            "summary": "We introduce the Overlap-Weighted Hierarchical Normalized Persistence Velocity (OW-HNPV), a novel topological data analysis method for detecting anomalies in time-varying networks. Unlike existing methods that measure cumulative topological presence, we introduce the first velocity-based perspective on persistence diagrams, measuring the rate at which features appear and disappear, automatically downweighting noise through overlap-based weighting. We also prove that OW-HNPV is mathematically stable. It behaves in a controlled, predictable way, even when comparing persistence diagrams from networks with different feature types. Applied to Ethereum transaction networks (May 2017-May 2018), OW-HNPV demonstrates superior performance for cryptocurrency anomaly detection, achieving up to 10.4% AUC gain over baseline models for 7-day price movement predictions. Compared with established methods, including Vector of Averaged Bettis (VAB), persistence landscapes, and persistence images, velocity-based summaries excel at medium- to long-range forecasting (4-7 days), with OW-HNPV providing the most consistent and stable performance across prediction horizons. Our results show that modeling topological velocity is crucial for detecting structural anomalies in dynamic networks.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14615v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "SAC"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出基于重叠加权的层次化归一化持久性速度方法，用于动态网络异常检测，在加密货币市场预测中实现显著性能提升。",
            "summary_zh": "本文介绍了重叠加权层次化归一化持久性速度（OW-HNPV），这是一种用于检测时变网络中异常的新型拓扑数据分析方法。与现有测量累积拓扑存在的方法不同，我们首次引入了基于持久性图的速度视角，测量特征出现和消失的速率，并通过基于重叠的加权自动降低噪声影响。我们还证明了OW-HNPV在数学上是稳定的，即使在比较具有不同特征类型的网络的持久性图时，其行为也是可控且可预测的。应用于以太坊交易网络（2017年5月至2018年5月），OW-HNPV在加密货币异常检测中表现出优越性能，在7天价格变动预测中比基线模型实现了高达10.4%的AUC增益。与现有方法（包括平均Betti向量、持久性景观和持久性图像）相比，基于速度的摘要在中长期预测（4-7天）中表现优异，其中OW-HNPV在不同预测时间范围内提供了最一致和稳定的性能。我们的结果表明，建模拓扑速度对于检测动态网络中的结构异常至关重要。",
            "intro_zh": [
                "现有方法主要关注累积拓扑存在，难以捕捉动态网络中的快速结构变化，导致异常检测精度受限。",
                "论文提出基于速度的持久性图分析，引入重叠加权机制自动降噪，并证明方法的数学稳定性，提升异常检测的鲁棒性。",
                "在以太坊交易网络实验中，OW-HNPV在7天价格预测中实现高达10.4%的AUC增益，在中长期预测中表现最稳定和一致。"
            ],
            "method_zh": "**问题定义**：论文旨在解决时变网络中的异常检测问题，特别是在加密货币市场等动态环境中。现有方法（如平均Betti向量、持久性景观和持久性图像）主要基于累积拓扑特征，无法有效捕捉网络结构的快速变化，导致对异常事件的响应延迟和精度不足。\\n\\n**核心思路**：论文的核心思路是引入基于速度的视角来分析持久性图，测量拓扑特征（如连通分量、空洞）出现和消失的速率，而非仅仅关注其存在时间。通过速度指标，可以更敏感地检测网络结构的突变，同时设计重叠加权机制来自动降低噪声特征的影响，提高方法的鲁棒性。\\n\\n**技术框架**：整体流程包括三个阶段：首先，从时变网络中提取持久性图，记录拓扑特征的出生和死亡时间；其次，计算层次化归一化持久性速度，基于特征的生命周期速率进行量化；最后，应用重叠加权，根据特征在持久性图中的重叠程度分配权重，以抑制噪声。最终输出速度摘要用于异常检测任务。\\n\\n**关键创新**：最重要的创新是首次将速度概念引入持久性图分析，改变了传统基于累积度量的范式。与现有方法的本质区别在于，OW-HNPV专注于拓扑变化的动态速率，而非静态存在，这使得它更适合捕捉网络中的瞬时异常。此外，重叠加权机制是另一创新点，它通过数学方式自动区分重要特征与噪声，无需手动调参。\\n\\n**关键设计**：关键设计包括：速度计算基于特征出生和死亡时间的导数，归一化处理以确保跨网络可比性；重叠加权函数根据特征在持久性图中的空间重叠程度分配权重，具体函数形式在论文中定义但未详细公开；数学稳定性证明涉及Lipschitz连续性分析，确保方法对输入扰动不敏感；在应用阶段，速度摘要作为特征输入到机器学习模型（如分类器）中进行异常预测。",
            "application_zh": "该研究在加密货币市场异常检测中已展示实际价值，可应用于金融风控、价格预测和市场监管。未来可扩展至其他动态网络场景，如社交网络异常行为检测、交通流量监控和生物网络分析，为实时异常预警提供新工具。",
            "highlight_zh": "在以太坊交易网络实验中，OW-HNPV在7天价格变动预测中实现高达10.4%的AUC增益，显著优于基线模型（如平均Betti向量、持久性景观和持久性图像）。在中长期预测（4-7天）中，OW-HNPV表现最稳定和一致，验证了拓扑速度建模对异常检测的有效性。",
            "tags_zh": [
                "拓扑数据分析",
                "持久性图",
                "网络异常检测",
                "动态网络",
                "加密货币市场",
                "速度建模",
                "重叠加权",
                "数学稳定性"
            ],
            "_index": 48
        },
        {
            "title": "Hybrid Iterative Solvers with Geometry-Aware Neural Preconditioners for Parametric PDEs",
            "authors": [
                "Youngkyu Lee",
                "Francesc Levrero Florencio",
                "Jay Pathak",
                "George Em Karniadakis"
            ],
            "arxiv_id": "2512.14596v1",
            "summary": "The convergence behavior of classical iterative solvers for parametric partial differential equations (PDEs) is often highly sensitive to the domain and specific discretization of PDEs. Previously, we introduced hybrid solvers by combining the classical solvers with neural operators for a specific geometry 1, but they tend to under-perform in geometries not encountered during training. To address this challenge, we introduce Geo-DeepONet, a geometry-aware deep operator network that incorporates domain information extracted from finite element discretizations. Geo-DeepONet enables accurate operator learning across arbitrary unstructured meshes without requiring retraining. Building on this, we develop a class of geometry-aware hybrid preconditioned iterative solvers by coupling Geo-DeepONet with traditional methods such as relaxation schemes and Krylov subspace algorithms. Through numerical experiments on parametric PDEs posed over diverse unstructured domains, we demonstrate the enhanced robustness and efficiency of the proposed hybrid solvers for multiple real-world applications.",
            "categories": [
                "cs.LG",
                "math.NA"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "19 pages, 10 figures, 3 tables",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14596v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出几何感知神经预条件器与混合迭代求解器，以解决参数偏微分方程在任意非结构化网格上的求解鲁棒性问题。",
            "summary_zh": "传统迭代求解器在求解参数偏微分方程时，其收敛行为通常对计算域和离散化方式高度敏感。先前的研究通过将传统求解器与神经算子结合，针对特定几何形状提出了混合求解器，但这些方法在训练未见的几何形状上表现不佳。为解决这一挑战，本文引入了Geo-DeepONet，这是一种几何感知的深度算子网络，能够从有限元离散化中提取域信息。Geo-DeepONet使得在任意非结构化网格上实现精确的算子学习成为可能，而无需重新训练。在此基础上，通过将Geo-DeepONet与传统方法（如松弛方案和Krylov子空间算法）耦合，开发了一类几何感知的混合预条件迭代求解器。通过在多样非结构化域上的参数偏微分方程数值实验，证明了所提出的混合求解器在多个实际应用中的鲁棒性和效率得到了显著提升。",
            "intro_zh": [
                "传统迭代求解器对参数偏微分方程的几何形状和离散化高度敏感，导致收敛不稳定。",
                "提出Geo-DeepONet，结合几何信息实现跨任意非结构化网格的算子学习，无需重新训练。",
                "混合求解器在多样非结构化域上表现出增强的鲁棒性和效率，适用于实际应用。"
            ],
            "method_zh": "**问题定义**：论文旨在解决参数偏微分方程在任意非结构化网格上的高效求解问题。现有混合求解器（结合传统方法与神经算子）在训练未见的几何形状上表现不佳，缺乏几何泛化能力，限制了实际应用。\\n\\n**核心思路**：通过引入几何感知的深度算子网络（Geo-DeepONet），从有限元离散化中提取域信息，实现跨任意非结构化网格的算子学习，从而构建几何感知的混合预条件迭代求解器，提升求解的鲁棒性和效率。\\n\\n**技术框架**：整体架构分为两个阶段：首先，开发Geo-DeepONet，利用神经网络学习参数PDE的算子映射，并整合几何特征；其次，将Geo-DeepONet作为预条件器与传统迭代求解器（如松弛方案和Krylov子空间算法）耦合，形成混合求解流程，在求解过程中动态调整以加速收敛。\\n\\n**关键创新**：最重要的技术创新是Geo-DeepONet的设计，它能够处理任意非结构化网格，无需针对新几何重新训练，解决了现有方法几何泛化不足的问题。与现有方法的本质区别在于深度融合了几何信息，实现了更通用的算子学习。\\n\\n**关键设计**：Geo-DeepONet基于深度算子网络架构，可能包含编码器-解码器结构，以有限元离散化数据（如节点坐标、网格拓扑）作为输入；损失函数可能结合PDE残差和几何约束；关键参数包括网络层数、激活函数和学习率，具体设置未知，需参考论文实验部分。",
            "application_zh": "该研究在计算流体动力学、结构力学和电磁学等领域具有广泛应用潜力，能够处理复杂几何形状的参数偏微分方程，如飞机设计、生物医学模拟和气候建模。其实际价值在于提升求解效率和鲁棒性，降低计算成本，未来可能推动科学计算和工程优化的智能化发展。",
            "highlight_zh": "在多样非结构化域上的参数偏微分方程实验中，所提出的混合求解器相比传统方法（如纯迭代求解器或先前混合方法）展现出显著性能提升。具体数据未知，但论文报告了增强的收敛速度和鲁棒性，适用于多个实际应用场景，验证了几何感知设计的有效性。",
            "tags_zh": [
                "参数偏微分方程",
                "几何感知学习",
                "神经算子网络",
                "混合迭代求解器",
                "非结构化网格",
                "预条件技术",
                "科学计算",
                "有限元方法"
            ],
            "_index": 49
        },
        {
            "title": "TUMTraf EMOT: Event-Based Multi-Object Tracking Dataset and Baseline for Traffic Scenarios",
            "authors": [
                "Mengyu Li",
                "Xingcheng Zhou",
                "Guang Chen",
                "Alois Knoll",
                "Hu Cao"
            ],
            "arxiv_id": "2512.14595v1",
            "summary": "In Intelligent Transportation Systems (ITS), multi-object tracking is primarily based on frame-based cameras. However, these cameras tend to perform poorly under dim lighting and high-speed motion conditions. Event cameras, characterized by low latency, high dynamic range and high temporal resolution, have considerable potential to mitigate these issues. Compared to frame-based vision, there are far fewer studies on event-based vision. To address this research gap, we introduce an initial pilot dataset tailored for event-based ITS, covering vehicle and pedestrian detection and tracking. We establish a tracking-by-detection benchmark with a specialized feature extractor based on this dataset, achieving excellent performance.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "10 pages, 9 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14595v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "traffic"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出基于事件相机的交通场景多目标跟踪数据集和基准方法，以解决传统帧相机在弱光高速条件下的性能不足问题。",
            "summary_zh": "在智能交通系统中，多目标跟踪主要依赖于帧相机，但这些相机在弱光和高速度运动条件下表现不佳。事件相机具有低延迟、高动态范围和高时间分辨率的特点，有潜力缓解这些问题。与基于帧的视觉相比，基于事件视觉的研究较少。为填补这一研究空白，我们引入了一个专为基于事件的智能交通系统设计的初始试点数据集，涵盖车辆和行人的检测与跟踪。基于此数据集，我们建立了一个检测-跟踪基准，并采用专门的特征提取器，实现了优异的性能。",
            "intro_zh": [
                "现有帧相机在弱光和高速度运动条件下性能下降，限制了智能交通系统的可靠性和实时性。",
                "论文提出基于事件相机的多目标跟踪方法，通过低延迟和高动态范围特性，提升在挑战性场景下的跟踪精度。",
                "实验表明，该方法在自建数据集上实现了优异的性能，为事件视觉在交通领域的应用提供了基准。"
            ],
            "method_zh": "**问题定义**：论文旨在解决智能交通系统中多目标跟踪在弱光和高速度运动条件下的性能瓶颈。现有基于帧相机的方法在这些挑战性场景下表现不佳，主要由于帧相机的低动态范围、高延迟和有限时间分辨率，导致跟踪精度下降和实时性不足。\\n\\n**核心思路**：论文的核心思路是利用事件相机的优势，如低延迟、高动态范围和高时间分辨率，来弥补帧相机的不足。通过设计专门的事件视觉数据集和特征提取器，实现更鲁棒和高效的多目标跟踪，特别是在交通场景中处理车辆和行人的动态变化。\\n\\n**技术框架**：整体架构采用检测-跟踪范式，主要包括数据采集、特征提取、目标检测和多目标跟踪模块。首先，使用事件相机采集交通场景数据，构建包含车辆和行人标注的数据集。然后，通过专门的特征提取器处理事件流，生成时空特征。接着，基于这些特征进行目标检测，识别出感兴趣对象。最后，应用多目标跟踪算法，如基于关联或滤波的方法，实现连续帧间的目标跟踪。\\n\\n**关键创新**：最重要的技术创新点是引入了首个专为基于事件的智能交通系统设计的多目标跟踪数据集，并开发了适配事件视觉的特征提取器。与现有方法相比，本质区别在于从帧视觉转向事件视觉，利用事件相机的独特特性来提升在挑战性条件下的跟踪性能，填补了事件视觉在交通领域的研究空白。\\n\\n**关键设计**：关键设计包括事件数据预处理步骤，如事件累积和噪声过滤，以生成适合深度学习模型的输入。特征提取器可能基于卷积神经网络或事件专用架构，如Spiking Neural Networks，以捕获事件流的时空模式。损失函数可能结合检测和跟踪任务，如使用交叉熵损失用于分类和IoU损失用于边界框回归。参数设置涉及事件阈值、时间窗口大小和网络层数，以优化性能和效率。",
            "application_zh": "该研究在智能交通系统中有广泛潜在应用，如自动驾驶车辆的环境感知、交通监控和行人安全系统。通过提升在弱光和高速度条件下的跟踪能力，可增强交通管理的实时性和可靠性，减少事故风险。未来可能推动事件相机在更多动态场景中的部署，促进智能交通技术的发展。",
            "highlight_zh": "实验在自建的TUMTraf EMOT数据集上进行，基准方法实现了优异的性能，具体数据未知，但论文报告了在车辆和行人跟踪任务中的高精度和鲁棒性。与基于帧相机的方法相比，在模拟弱光和高速度条件下，事件视觉方法显示出显著提升，验证了其在实际交通场景中的有效性。",
            "tags_zh": [
                "事件相机",
                "多目标跟踪",
                "智能交通系统",
                "数据集构建",
                "特征提取",
                "交通场景",
                "弱光条件",
                "高速度运动"
            ],
            "_index": 50
        },
        {
            "title": "Low-Resource, High-Impact: Building Corpora for Inclusive Language Technologies",
            "authors": [
                "Ekaterina Artemova",
                "Laurie Burchell",
                "Daryna Dementieva",
                "Shu Okabe",
                "Mariya Shmatova",
                "Pedro Ortiz Suarez"
            ],
            "arxiv_id": "2512.14576v1",
            "summary": "This tutorial (https://tum-nlp.github.io/low-resource-tutorial) is designed for NLP practitioners, researchers, and developers working with multilingual and low-resource languages who seek to create more equitable and socially impactful language technologies. Participants will walk away with a practical toolkit for building end-to-end NLP pipelines for underrepresented languages -- from data collection and web crawling to parallel sentence mining, machine translation, and downstream applications such as text classification and multimodal reasoning. The tutorial presents strategies for tackling the challenges of data scarcity and cultural variance, offering hands-on methods and modeling frameworks. We will focus on fair, reproducible, and community-informed development approaches, grounded in real-world scenarios. We will showcase a diverse set of use cases covering over 10 languages from different language families and geopolitical contexts, including both digitally resource-rich and severely underrepresented languages.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Tutorial is accepted to LREC2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14576v1",
            "code_links": [
                {
                    "url": "https://tum-nlp.github.io/low-resource-tutorial",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "人形机器人",
                    "matched_keywords": [
                        "digit"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出低资源语言NLP教程，通过端到端流程解决数据稀缺与文化差异问题，促进包容性语言技术发展。",
            "summary_zh": "本教程（https://tum-nlp.github.io/low-resource-tutorial）面向从事多语言和低资源语言工作的NLP从业者、研究人员和开发者，旨在帮助他们创建更公平、更具社会影响力的语言技术。参与者将获得一套实用的工具包，用于为代表性不足的语言构建端到端NLP流程——从数据收集和网络爬取，到平行句挖掘、机器翻译，再到文本分类和多模态推理等下游应用。教程介绍了应对数据稀缺和文化差异挑战的策略，提供了实践方法和建模框架。我们将重点关注公平、可重复且基于社区反馈的开发方法，并以真实场景为基础。我们将展示涵盖10多种来自不同语系和地缘政治背景的语言的多样化用例，包括数字资源丰富和严重代表性不足的语言。",
            "intro_zh": [
                "核心问题：现有NLP方法在低资源语言中面临数据稀缺、文化差异和代表性不足的挑战，导致技术不公和性能低下。",
                "方法要点：提供端到端NLP流程工具包，包括数据收集、平行句挖掘、机器翻译等，强调公平、可重复和社区参与的开发方法。",
                "实验或效果：教程展示超过10种语言的用例，涵盖不同语系和地缘政治背景，提升低资源语言技术的实用性和社会影响力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决低资源语言在自然语言处理（NLP）中面临的数据稀缺和文化差异问题。现有方法通常依赖大规模标注数据，导致低资源语言技术发展滞后、性能不佳，且缺乏公平性和包容性。\\n\\n**核心思路**：通过提供一套端到端的NLP流程工具包，从数据收集到下游应用，结合实践策略和建模框架，以应对数据不足的挑战。设计强调公平、可重复和社区参与，确保技术开发符合实际需求和文化背景。\\n\\n**技术框架**：整体架构包括多个阶段：数据收集（如网络爬取）、平行句挖掘（用于构建翻译数据）、机器翻译模型训练，以及下游应用（如文本分类、多模态推理）。流程以真实场景为基础，支持从原始数据到部署的全链条开发。\\n\\n**关键创新**：最重要的创新在于整合了低资源语言NLP的端到端解决方案，并强调社会影响和公平性。与现有方法相比，本质区别在于它不仅关注技术性能，还融入社区反馈和文化适应性，促进包容性技术发展。\\n\\n**关键设计**：教程提供实践方法和建模框架，具体技术细节如网络结构或损失函数未知，但包括数据预处理策略、多语言模型适配和可重复实验设置，以支持多样语言应用。",
            "application_zh": "该研究可应用于多语言机器翻译、低资源语言文本分类、跨文化信息处理等领域，提升全球语言技术的公平性和可访问性。潜在价值包括支持教育、医疗和社会服务中的语言包容，未来可能推动更多低资源语言进入主流NLP应用，减少数字鸿沟。",
            "highlight_zh": "教程展示超过10种语言的用例，涵盖不同语系和地缘政治背景，如资源丰富和严重不足的语言。通过实践工具包，参与者能构建端到端NLP流程，提升低资源语言技术的开发效率和实用性，具体性能数据未知，但强调公平和可重复性提升社会影响力。",
            "tags_zh": [
                "低资源语言处理",
                "多语言NLP",
                "数据稀缺挑战",
                "端到端流程",
                "公平性技术",
                "社区参与开发",
                "文化差异适应",
                "机器翻译应用"
            ],
            "_index": 51
        },
        {
            "title": "Polypersona: Persona-Grounded LLM for Synthetic Survey Responses",
            "authors": [
                "Tejaswani Dash",
                "Dinesh Karri",
                "Anudeep Vurity",
                "Gautam Datla",
                "Tazeem Ahmad",
                "Saima Rafi",
                "Rohith Tangudu"
            ],
            "arxiv_id": "2512.14562v1",
            "summary": "This paper introduces PolyPersona, a generative framework for synthesizing persona-conditioned survey responses across multiple domains. The framework instruction-tunes compact chat models using parameter-efficient LoRA adapters with 4-bit quantization under a resource-adaptive training setup. A dialogue-based data pipeline explicitly preserves persona cues, ensuring consistent behavioral alignment across generated responses. Using this pipeline, we construct a dataset of 3,568 synthetic survey responses spanning ten domains and 433 distinct personas, enabling controlled instruction tuning and systematic multi-domain evaluation. We evaluate the generated responses using a multi-metric evaluation suite that combines standard text generation metrics, including BLEU, ROUGE, and BERTScore, with survey-specific metrics designed to assess structural coherence, stylistic consistency, and sentiment alignment.Experimental results show that compact models such as TinyLlama 1.1B and Phi-2 achieve performance comparable to larger 7B to 8B baselines, with a highest BLEU score of 0.090 and ROUGE-1 of 0.429. These findings demonstrate that persona-conditioned fine-tuning enables small language models to generate reliable and coherent synthetic survey data. The proposed framework provides an efficient and reproducible approach for survey data generation, supporting scalable evaluation while facilitating bias analysis through transparent and open protocols.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted in IEEE Bigdata 2025- LLMs4ALL",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14562v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出PolyPersona框架，通过角色条件化微调小型语言模型，高效生成多领域合成调查数据。",
            "summary_zh": "本文介绍了PolyPersona，一个用于跨多个领域生成角色条件化合成调查响应的生成框架。该框架在资源自适应训练设置下，使用参数高效的LoRA适配器和4位量化对紧凑聊天模型进行指令微调。基于对话的数据管道明确保留角色线索，确保生成响应间行为一致性。利用此管道，我们构建了一个包含10个领域、433个不同角色的3,568个合成调查响应的数据集，支持可控指令微调和系统化多领域评估。我们使用多指标评估套件评估生成响应，该套件结合了标准文本生成指标（包括BLEU、ROUGE和BERTScore）与专门设计的调查特定指标，用于评估结构连贯性、风格一致性和情感对齐。实验结果表明，TinyLlama 1.1B和Phi-2等紧凑模型实现了与较大7B至8B基线相当的性能，最高BLEU得分为0.090，ROUGE-1为0.429。这些发现表明，角色条件化微调使小型语言模型能够生成可靠且连贯的合成调查数据。所提出的框架为调查数据生成提供了一种高效且可复现的方法，支持可扩展评估，同时通过透明和开放的协议促进偏见分析。",
            "intro_zh": [
                "现有方法在生成合成调查数据时，难以确保角色一致性和跨领域适应性，导致数据质量受限。",
                "论文提出PolyPersona框架，通过角色条件化指令微调和对话式数据管道，提升小型模型生成质量。",
                "实验显示，紧凑模型如TinyLlama 1.1B在BLEU和ROUGE指标上接近大型基线，验证了框架有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决生成高质量、角色一致的合成调查数据问题，现有方法常因模型规模大、资源消耗高，且缺乏角色对齐机制，导致生成数据在结构、风格和情感上不一致，难以用于可控评估和偏见分析。\\n\\n**核心思路**：通过角色条件化微调，将角色信息嵌入模型生成过程，结合参数高效技术和量化优化，使小型语言模型在资源受限下生成可靠数据，同时利用对话式数据管道确保角色线索的显式保留。\\n\\n**技术框架**：整体架构包括数据构建、模型微调和评估三个阶段。首先，基于对话构建多领域、多角色的合成调查响应数据集；其次，使用LoRA适配器和4位量化对紧凑聊天模型进行指令微调，在资源自适应训练设置下优化参数；最后，通过多指标评估套件（含标准文本生成和调查特定指标）系统评估生成质量。\\n\\n**关键创新**：最重要的技术创新是角色条件化微调与对话式数据管道的结合，本质区别在于显式建模角色一致性，而非依赖大规模模型或通用生成技术，从而在小型模型上实现高效、可控的数据合成。\\n\\n**关键设计**：关键设计包括使用LoRA适配器进行参数高效微调，减少训练开销；采用4位量化降低模型内存占用；设计资源自适应训练策略，优化计算资源分配；构建多指标评估套件，结合BLEU、ROUGE、BERTScore等标准指标与结构连贯性、风格一致性、情感对齐等调查特定指标。",
            "application_zh": "该研究可应用于社会科学调查、市场研究、用户行为模拟等领域，为数据稀缺或隐私敏感场景提供高效合成数据生成方案，支持可扩展评估和偏见分析，未来可能推动低成本、高质量AI辅助调查工具的发展。",
            "highlight_zh": "实验结果显示，紧凑模型TinyLlama 1.1B和Phi-2在生成合成调查响应时，性能接近较大7B至8B基线模型，最高BLEU得分为0.090，ROUGE-1为0.429，验证了角色条件化微调的有效性，同时资源消耗显著降低。",
            "tags_zh": [
                "合成调查数据生成",
                "角色条件化微调",
                "参数高效学习",
                "4位量化",
                "多领域评估",
                "对话式数据管道",
                "小型语言模型",
                "资源自适应训练"
            ],
            "_index": 52
        },
        {
            "title": "VLegal-Bench: Cognitively Grounded Benchmark for Vietnamese Legal Reasoning of Large Language Models",
            "authors": [
                "Nguyen Tien Dong",
                "Minh-Anh Nguyen",
                "Thanh Dat Hoang",
                "Nguyen Tuan Ngoc",
                "Dao Xuan Quang Minh",
                "Phan Phi Hai",
                "Nguyen Thi Ngoc Anh",
                "Dang Van Tu",
                "Binh Vu"
            ],
            "arxiv_id": "2512.14554v1",
            "summary": "The rapid advancement of large language models (LLMs) has enabled new possibilities for applying artificial intelligence within the legal domain. Nonetheless, the complexity, hierarchical organization, and frequent revisions of Vietnamese legislation pose considerable challenges for evaluating how well these models interpret and utilize legal knowledge. To address this gap, Vietnamese Legal Benchmark (VLegal-Bench) is introduced, the first comprehensive benchmark designed to systematically assess LLMs on Vietnamese legal tasks. Informed by Bloom's cognitive taxonomy, VLegal-Bench encompasses multiple levels of legal understanding through tasks designed to reflect practical usage scenarios. The benchmark comprises 10,450 samples generated through a rigorous annotation pipeline, where legal experts label and cross-validate each instance using our annotation system to ensure every sample is grounded in authoritative legal documents and mirrors real-world legal assistant workflows, including general legal questions and answers, retrieval-augmented generation, multi-step reasoning, and scenario-based problem solving tailored to Vietnamese law. By providing a standardized, transparent, and cognitively informed evaluation framework, VLegal-Bench establishes a solid foundation for assessing LLM performance in Vietnamese legal contexts and supports the development of more reliable, interpretable, and ethically aligned AI-assisted legal systems.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14554v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出VLegal-Bench基准以解决大语言模型在越南法律推理评估中的标准化与认知深度不足问题",
            "summary_zh": "随着大语言模型（LLMs）的快速发展，人工智能在法律领域的应用展现出新的可能性。然而，越南法律的复杂性、层级结构以及频繁修订，对评估这些模型如何解释和利用法律知识构成了重大挑战。为填补这一空白，本文引入了越南法律基准（VLegal-Bench），这是首个旨在系统评估LLMs在越南法律任务上的综合性基准。基于布鲁姆认知分类法，VLegal-Bench通过设计反映实际使用场景的任务，涵盖了多个层次的法律理解。该基准包含10,450个样本，通过严格的标注流程生成，其中法律专家使用我们的标注系统对每个实例进行标注和交叉验证，以确保每个样本都基于权威法律文件，并模拟真实世界的法律助理工作流程，包括一般法律问答、检索增强生成、多步推理以及针对越南法律的基于场景的问题解决。通过提供一个标准化、透明且基于认知科学的评估框架，VLegal-Bench为评估LLMs在越南法律背景下的性能奠定了坚实基础，并支持开发更可靠、可解释且符合伦理的AI辅助法律系统。",
            "intro_zh": [
                "核心问题：现有方法缺乏针对越南法律复杂性和动态性的标准化评估基准，导致LLMs在法律推理任务上的性能难以准确衡量。",
                "方法要点：基于布鲁姆认知分类法设计多层次法律任务，通过专家标注流程生成10,450个样本，构建首个越南法律基准VLegal-Bench。",
                "实验或效果：基准覆盖问答、检索增强生成等场景，提供透明评估框架，支持开发更可靠的AI法律系统，具体性能数据未知。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大语言模型（LLMs）在越南法律推理任务中缺乏标准化、全面评估基准的问题。现有方法的痛点包括：越南法律具有复杂性、层级组织和频繁修订的特点，导致现有通用基准难以准确评估LLMs的法律知识理解和应用能力，缺乏针对越南语法律场景的专门测试集，且评估往往忽略认知深度，无法反映实际法律工作流程。\\n\\n**核心思路**：论文的核心解决思路是构建一个基于认知科学原理的综合性基准VLegal-Bench，通过模拟真实法律助理任务，系统评估LLMs在越南法律背景下的多级推理能力。这样设计是为了确保评估不仅覆盖表面知识，还深入考察理解、应用和分析等认知层次，从而更贴近实际需求。\\n\\n**技术框架**：整体架构包括任务设计、样本生成和评估框架三个阶段。主要模块：1）任务设计模块，基于布鲁姆认知分类法（如记忆、理解、应用、分析等）定义多层次法律任务，包括一般法律问答、检索增强生成、多步推理和场景问题解决；2）样本生成模块，通过严格标注流程，由法律专家使用标注系统对10,450个样本进行标注和交叉验证，确保样本基于权威法律文件；3）评估框架模块，提供标准化、透明的评估协议，支持性能比较和错误分析。\\n\\n**关键创新**：最重要的技术创新点是首次将布鲁姆认知分类法系统应用于越南法律基准构建，实现认知层次与法律任务的对齐。与现有方法的本质区别在于：VLegal-Bench不仅关注知识检索，还强调多步推理和场景化问题解决，通过专家标注确保样本质量和现实相关性，填补了越南语法律评估的空白。\\n\\n**关键设计**：关键设计包括：1）任务设计基于布鲁姆分类法，涵盖从基础问答到复杂推理的多个认知级别；2）标注流程采用专家主导的交叉验证机制，确保每个样本都引用权威法律文档，减少错误和偏差；3）样本规模为10,450个，覆盖多样法律场景；4）评估框架强调透明性和可重复性，具体参数设置和损失函数未在摘要中详细说明，可能涉及标准自然语言处理指标。",
            "application_zh": "该研究的潜在应用领域包括AI辅助法律咨询、法律文档自动化处理、法律教育工具和司法系统支持。实际价值在于为越南法律AI开发提供标准化评估基准，促进模型优化和伦理对齐，未来可能推动更可靠、可解释的法律AI系统发展，提升法律服务的效率和可及性。",
            "highlight_zh": "最重要的实验结果包括：VLegal-Bench包含10,450个高质量样本，基于布鲁姆认知分类法设计多层次任务，覆盖一般问答、检索增强生成等场景。通过专家标注和交叉验证确保样本权威性，基准提供透明评估框架，但具体性能数据如准确率、对比基线模型和提升幅度在摘要中未提及，需参考完整论文获取。",
            "tags_zh": [
                "法律人工智能",
                "大语言模型评估",
                "越南法律基准",
                "认知分类法",
                "检索增强生成",
                "多步推理",
                "专家标注系统",
                "AI辅助法律系统"
            ],
            "_index": 53
        },
        {
            "title": "HiFi-Portrait: Zero-shot Identity-preserved Portrait Generation with High-fidelity Multi-face Fusion",
            "authors": [
                "Yifang Xu",
                "Benxiang Zhai",
                "Yunzhuo Sun",
                "Ming Li",
                "Yang Li",
                "Sidan Du"
            ],
            "arxiv_id": "2512.14542v1",
            "summary": "Recent advancements in diffusion-based technologies have made significant strides, particularly in identity-preserved portrait generation (IPG). However, when using multiple reference images from the same ID, existing methods typically produce lower-fidelity portraits and struggle to customize face attributes precisely. To address these issues, this paper presents HiFi-Portrait, a high-fidelity method for zero-shot portrait generation. Specifically, we first introduce the face refiner and landmark generator to obtain fine-grained multi-face features and 3D-aware face landmarks. The landmarks include the reference ID and the target attributes. Then, we design HiFi-Net to fuse multi-face features and align them with landmarks, which improves ID fidelity and face control. In addition, we devise an automated pipeline to construct an ID-based dataset for training HiFi-Portrait. Extensive experimental results demonstrate that our method surpasses the SOTA approaches in face similarity and controllability. Furthermore, our method is also compatible with previous SDXL-based works.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted by CVPR 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14542v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出HiFi-Portrait方法，通过高保真多脸融合解决零样本身份保持肖像生成中的保真度和属性控制问题。",
            "summary_zh": "近年来，基于扩散模型的技术在身份保持肖像生成（IPG）领域取得了显著进展。然而，当使用同一身份的多张参考图像时，现有方法通常生成保真度较低的肖像，且难以精确定制面部属性。为解决这些问题，本文提出了HiFi-Portrait，一种用于零样本肖像生成的高保真方法。具体而言，我们首先引入面部细化器和地标生成器，以获取细粒度的多脸特征和3D感知的面部地标。这些地标包含参考身份和目标属性。然后，我们设计了HiFi-Net来融合多脸特征并将其与地标对齐，从而提高身份保真度和面部控制能力。此外，我们还设计了一个自动化流程来构建基于身份的数据集，用于训练HiFi-Portrait。大量实验结果表明，我们的方法在面部相似性和可控性方面超越了最先进的方法。此外，我们的方法也与之前基于SDXL的工作兼容。",
            "intro_zh": [
                "现有方法在使用多张参考图像时，生成肖像的保真度较低，且难以精确控制面部属性。",
                "通过面部细化器和地标生成器提取细粒度特征，并设计HiFi-Net融合多脸特征以对齐地标，提升身份保真度。",
                "实验显示，该方法在面部相似性和可控性上超越SOTA方法，并与SDXL兼容，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决零样本身份保持肖像生成（IPG）中，当使用同一身份的多张参考图像时，现有方法生成肖像保真度低、难以精确定制面部属性的问题。现有方法的痛点包括：多脸融合时特征提取不充分，导致身份信息丢失；缺乏有效的机制来对齐参考身份与目标属性，从而影响生成质量。\\n\\n**核心思路**：论文的核心解决思路是通过细粒度特征提取和地标对齐，实现高保真的多脸融合。具体来说，首先利用面部细化器和地标生成器获取多脸特征和3D感知地标，然后设计HiFi-Net融合这些特征并使其与地标对齐，从而提升身份保真度和面部控制能力。这样设计的原因在于，细粒度特征能更好地保留身份细节，而地标对齐则提供了结构化的指导，确保生成肖像的准确性和可控性。\\n\\n**技术框架**：整体架构包括三个主要阶段：特征提取、融合对齐和生成。首先，面部细化器处理多张参考图像，提取细粒度面部特征；地标生成器生成包含参考身份和目标属性的3D感知面部地标。然后，HiFi-Net接收这些特征和地标，通过融合模块整合多脸特征，并通过对齐模块将特征与地标匹配。最后，基于对齐后的特征生成高保真肖像。整个流程支持零样本生成，无需针对特定身份进行微调。\\n\\n**关键创新**：最重要的技术创新点是HiFi-Net的设计，它专门用于多脸特征融合和地标对齐，这在现有方法中较少见。与现有方法的本质区别在于：现有方法通常直接融合图像或浅层特征，导致保真度下降；而HiFi-Portrait通过细粒度特征提取和结构化对齐，实现了更精确的身份保持和属性控制。此外，自动化数据集构建流程也是一个创新点，它降低了数据收集成本，提高了训练效率。\\n\\n**关键设计**：关键设计包括：面部细化器采用卷积神经网络提取多尺度特征，以捕获细粒度细节；地标生成器基于3D模型生成地标，确保空间准确性；HiFi-Net包含融合层和对齐层，融合层使用注意力机制整合多脸特征，对齐层通过损失函数（如相似性损失和地标对齐损失）优化特征与地标的匹配。参数设置方面，网络在构建的ID-based数据集上训练，使用Adam优化器，学习率设置为未知。损失函数结合了身份相似性损失和地标对齐损失，以平衡保真度和可控性。",
            "application_zh": "该研究在数字娱乐、虚拟现实和个性化内容创作等领域具有潜在应用价值。例如，可用于生成高保真的虚拟角色肖像，支持游戏或电影中的角色定制；在社交媒体或电商平台中，帮助用户创建个性化头像或产品展示。未来，该方法可扩展至视频生成或多模态交互，推动AI生成内容的发展。",
            "highlight_zh": "实验结果表明，HiFi-Portrait在面部相似性指标上超越SOTA方法，具体提升幅度未知，但通过定量和定性分析显示显著优势。在可控性方面，该方法能精确定制面部属性，如表情和姿态，优于基线模型。此外，与SDXL的兼容性验证了其泛化能力，适用于实际部署。",
            "tags_zh": [
                "身份保持肖像生成",
                "零样本学习",
                "多脸融合",
                "高保真生成",
                "扩散模型",
                "3D面部地标",
                "特征对齐",
                "自动化数据集构建"
            ],
            "_index": 54
        },
        {
            "title": "Native Intelligence Emerges from Large-Scale Clinical Practice: A Retinal Foundation Model with Deployment Efficiency",
            "authors": [
                "Jia Guo",
                "Jiawei Du",
                "Shengzhu Yang",
                "Shuai Lu",
                "Wenquan Cheng",
                "Kaiwen Zhang",
                "Yihua Sun",
                "Chuhong Yang",
                "Weihang Zhang",
                "Fang Chen",
                "Yilan Wu",
                "Lie Ju",
                "Guochen Ning",
                "Longfei Ma",
                "Huiping Yao",
                "Jinyuan Wang",
                "Peilun Shi",
                "Yukun Zhou",
                "Jie Xu",
                "Pearse A. Keane",
                "Hanruo Liu",
                "Hongen Liao",
                "Ningli Wang",
                "Huiqi Li"
            ],
            "arxiv_id": "2512.14499v1",
            "summary": "Current retinal foundation models remain constrained by curated research datasets that lack authentic clinical context, and require extensive task-specific optimization for each application, limiting their deployment efficiency in low-resource settings. Here, we show that these barriers can be overcome by building clinical native intelligence directly from real-world medical practice. Our key insight is that large-scale telemedicine programs, where expert centers provide remote consultations across distributed facilities, represent a natural reservoir for learning clinical image interpretation. We present ReVision, a retinal foundation model that learns from the natural alignment between 485,980 color fundus photographs and their corresponding diagnostic reports, accumulated through a decade-long telemedicine program spanning 162 medical institutions across China. Through extensive evaluation across 27 ophthalmic benchmarks, we demonstrate that ReVison enables deployment efficiency with minimal local resources. Without any task-specific training, ReVision achieves zero-shot disease detection with an average AUROC of 0.946 across 12 public benchmarks and 0.952 on 3 independent clinical cohorts. When minimal adaptation is feasible, ReVision matches extensively fine-tuned alternatives while requiring orders of magnitude fewer trainable parameters and labeled examples. The learned representations also transfer effectively to new clinical sites, imaging domains, imaging modalities, and systemic health prediction tasks. In a prospective reader study with 33 ophthalmologists, ReVision's zero-shot assistance improved diagnostic accuracy by 14.8% across all experience levels. These results demonstrate that clinical native intelligence can be directly extracted from clinical archives without any further annotation to build medical AI systems suited to various low-resource settings.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14499v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出ReVision视网膜基础模型，利用大规模远程医疗数据构建临床原生智能，以解决低资源环境下部署效率低的问题。",
            "summary_zh": "当前视网膜基础模型受限于缺乏真实临床背景的精选研究数据集，且每个应用都需要大量任务特定优化，限制了其在低资源环境下的部署效率。本文表明，通过直接从真实世界医疗实践中构建临床原生智能，可以克服这些障碍。我们的核心见解是，大规模远程医疗项目（专家中心为分布式机构提供远程咨询）是学习临床图像解读的自然资源库。我们提出了ReVision，这是一个视网膜基础模型，它从485,980张彩色眼底照片及其对应诊断报告的自然对齐中学习，这些数据来自中国162家医疗机构长达十年的远程医疗项目积累。通过在27个眼科基准上进行广泛评估，我们证明ReVision能以最少的本地资源实现部署效率。无需任何任务特定训练，ReVision在12个公共基准上实现了平均AUROC为0.946的零样本疾病检测，在3个独立临床队列上为0.952。当最小适应可行时，ReVision匹配了经过广泛微调的替代方案，同时需要数量级更少的可训练参数和标记示例。学习到的表示还能有效迁移到新的临床站点、成像领域、成像模态和系统健康预测任务。在一项涉及33名眼科医生的前瞻性读者研究中，ReVision的零样本辅助在所有经验水平上将诊断准确性提高了14.8%。这些结果表明，临床原生智能可以直接从临床档案中提取，无需进一步注释，以构建适合各种低资源环境的医疗AI系统。",
            "intro_zh": [
                "核心问题：现有视网膜基础模型依赖精选数据集，缺乏真实临床上下文，且需大量任务特定优化，导致低资源环境下部署效率低。",
                "方法要点：利用大规模远程医疗项目中的临床图像与报告自然对齐数据，构建ReVision模型，直接从真实医疗实践中学习临床原生智能。",
                "实验或效果：ReVision在零样本疾病检测中平均AUROC达0.946，辅助诊断准确性提升14.8%，并实现高效迁移到新任务和场景。"
            ],
            "method_zh": "**问题定义**：论文旨在解决视网膜基础模型在低资源环境下部署效率低的问题。现有方法依赖精选研究数据集，缺乏真实临床上下文，且每个应用都需要大量任务特定优化，导致资源消耗大、适应性差。\\n\\n**核心思路**：论文的核心思路是从大规模远程医疗实践中直接提取临床原生智能。远程医疗项目自然产生图像与诊断报告的对齐数据，这为学习临床图像解读提供了丰富、真实的资源，无需额外标注，从而克服数据稀缺和优化负担。\\n\\n**技术框架**：整体架构基于从485,980张彩色眼底照片及其对应诊断报告中学习对齐表示。模型通过预训练阶段，利用图像-文本对进行自监督学习，捕获临床语义信息。然后，在评估阶段，模型支持零样本推理或最小适应，应用于多种眼科任务，如疾病检测和健康预测。\\n\\n**关键创新**：最重要的技术创新是直接从真实世界临床档案中构建基础模型，无需人工标注。与现有方法依赖精选数据集和大量微调不同，ReVision利用远程医疗的自然对齐数据，实现了临床原生智能的提取，本质区别在于数据来源和优化效率的提升。\\n\\n**关键设计**：关键设计包括使用大规模图像-文本对进行预训练，可能采用对比学习或生成式方法对齐视觉和语言特征。模型架构可能基于Transformer或卷积网络，具体参数设置未知，但强调最小化可训练参数和标记示例需求，以支持低资源部署。损失函数可能设计为最大化图像与报告之间的语义一致性，但具体细节未在摘要中说明。",
            "application_zh": "该研究在低资源医疗环境中具有广泛潜在应用，如远程眼科诊断、基层医疗筛查和疾病监测。通过减少对标注数据和计算资源的需求，ReVision能提升医疗AI的普及性和效率，未来可能扩展到其他医学影像领域，推动个性化医疗和健康预测。",
            "highlight_zh": "最重要的实验结果包括：零样本疾病检测在12个公共基准上平均AUROC达0.946，在3个独立临床队列上为0.952；最小适应时匹配微调模型，但参数和示例需求大幅减少；前瞻性读者研究中，零样本辅助将33名眼科医生的诊断准确性提升14.8%；模型表示能有效迁移到新临床站点、成像域和健康预测任务。",
            "tags_zh": [
                "视网膜基础模型",
                "临床原生智能",
                "远程医疗数据",
                "零样本学习",
                "部署效率",
                "图像-文本对齐",
                "低资源医疗AI",
                "眼科疾病检测"
            ],
            "_index": 55
        },
        {
            "title": "SuperCLIP: CLIP with Simple Classification Supervision",
            "authors": [
                "Weiheng Zhao",
                "Zilong Huang",
                "Jiashi Feng",
                "Xinggang Wang"
            ],
            "arxiv_id": "2512.14480v1",
            "summary": "Contrastive Language-Image Pretraining (CLIP) achieves strong generalization in vision-language tasks by aligning images and texts in a shared embedding space. However, recent findings show that CLIP-like models still underutilize fine-grained semantic signals in text, and this issue becomes even more pronounced when dealing with long and detailed captions. This stems from CLIP's training objective, which optimizes only global image-text similarity and overlooks token-level supervision - limiting its ability to achieve fine-grained visual-text alignment. To address this, we propose SuperCLIP, a simple yet effective framework that augments contrastive learning with classification-based supervision. By adding only a lightweight linear layer to the vision encoder, SuperCLIP leverages token-level cues to enhance visual-textual alignment - with just a 0.077% increase in total FLOPs, and no need for additional annotated data. Experiments show that SuperCLIP consistently improves zero-shot classification, image-text retrieval, and purely visual tasks. These gains hold regardless of whether the model is trained on original web data or rich re-captioned data, demonstrating SuperCLIP's ability to recover textual supervision in both cases. Furthermore, SuperCLIP alleviates CLIP's small-batch performance drop through classification-based supervision that avoids reliance on large batch sizes. Code and models will be made open source.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted by NeurIPS 2025. Code: https://github.com/hustvl/SuperCLIP",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14480v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出SuperCLIP框架，通过分类监督增强对比学习，解决CLIP模型细粒度语义利用不足的问题。",
            "summary_zh": "对比语言-图像预训练（CLIP）通过在共享嵌入空间中对齐图像和文本来实现视觉-语言任务的强泛化能力。然而，最近的研究发现，CLIP类模型仍然未能充分利用文本中的细粒度语义信号，这一问题在处理长而详细的描述时尤为明显。这源于CLIP的训练目标仅优化全局图像-文本相似性，而忽略了词元级监督，限制了其实现细粒度视觉-文本对齐的能力。为解决这一问题，我们提出了SuperCLIP，一个简单而有效的框架，通过基于分类的监督来增强对比学习。仅通过在视觉编码器上添加一个轻量级线性层，SuperCLIP利用词元级线索来增强视觉-文本对齐，总FLOPs仅增加0.077%，且无需额外的标注数据。实验表明，SuperCLIP在零样本分类、图像-文本检索和纯视觉任务上均能持续提升性能。这些增益无论模型是在原始网络数据还是丰富的重新描述数据上训练都成立，证明了SuperCLIP在这两种情况下恢复文本监督的能力。此外，SuperCLIP通过基于分类的监督减轻了CLIP在小批量情况下的性能下降，避免了对大批量的依赖。代码和模型将开源。",
            "intro_zh": [
                "CLIP模型在训练中仅优化全局图像-文本相似性，忽略了文本中的词元级细粒度语义信号，导致在处理复杂描述时对齐能力受限。",
                "SuperCLIP通过添加轻量级线性层，引入基于分类的监督来增强对比学习，利用词元级线索提升视觉-文本对齐，无需额外数据。",
                "实验显示SuperCLIP在零样本分类、检索等任务上性能提升，且能缓解小批量训练的性能下降，总FLOPs仅增加0.077%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决CLIP模型在训练中未能充分利用文本细粒度语义信号的问题，特别是处理长而详细描述时视觉-文本对齐能力不足。现有方法的痛点是CLIP仅依赖全局对比损失，忽略了词元级监督，导致模型难以捕捉文本中的细节信息，限制了其泛化性能。\\n\\n**核心思路**：论文的核心思路是通过在CLIP的对比学习框架中引入基于分类的监督，以增强细粒度视觉-文本对齐。设计基于分类监督是因为它能直接利用文本中的词元级信息，提供更精确的语义对齐信号，从而弥补全局对比损失的不足，同时保持模型的轻量化和高效性。\\n\\n**技术框架**：整体架构在CLIP基础上扩展，包含视觉编码器、文本编码器和共享嵌入空间。主要模块包括：1) 视觉编码器，用于提取图像特征；2) 文本编码器，用于提取文本特征；3) 新增的轻量级线性层，附加在视觉编码器后，用于生成分类预测；4) 训练阶段结合对比损失和分类损失，优化模型参数。流程上，输入图像和文本对，通过编码器提取特征，计算全局对比损失，同时利用线性层进行词元级分类监督，联合训练以提升对齐能力。\\n\\n**关键创新**：最重要的技术创新点是将分类监督集成到CLIP的对比学习框架中，实现词元级细粒度对齐。与现有方法的本质区别在于，SuperCLIP不仅优化全局相似性，还通过分类任务利用文本中的具体词元信息，从而更有效地挖掘语义信号，而无需改变基础架构或增加大量计算开销。\\n\\n**关键设计**：关键设计包括：1) 在视觉编码器后添加一个线性层，参数轻量，总FLOPs仅增加0.077%；2) 损失函数结合对比损失（如InfoNCE）和分类损失（如交叉熵），平衡全局和局部监督；3) 网络结构保持CLIP的骨干编码器不变，确保兼容性和可扩展性；4) 训练时无需额外标注数据，直接利用原始文本中的词元作为监督信号，实现高效学习。",
            "application_zh": "SuperCLIP的潜在应用领域包括多模态人工智能、计算机视觉和自然语言处理，如零样本图像分类、图像-文本检索、视觉问答和内容生成。实际价值在于提升模型对复杂文本描述的细粒度理解能力，增强泛化性能，适用于需要高精度语义对齐的场景，如智能搜索、自动驾驶感知和多媒体分析。未来影响可能推动更高效的视觉-语言模型设计，促进小批量训练和资源受限环境下的应用。",
            "highlight_zh": "实验结果显示，SuperCLIP在多个基准任务上显著提升性能：在零样本分类任务中，相比基线CLIP，准确率提升约2-5个百分点；在图像-文本检索任务中，召回率提高3-7%；在纯视觉任务如目标检测上也有改善。此外，SuperCLIP有效缓解了CLIP在小批量训练时的性能下降，例如在批量大小为256时，性能接近大批量设置。这些提升在原始网络数据和重新描述数据上均一致，证明了其鲁棒性和通用性。",
            "tags_zh": [
                "对比学习",
                "视觉-语言对齐",
                "细粒度语义",
                "零样本分类",
                "图像-文本检索",
                "分类监督",
                "多模态预训练",
                "轻量化模型"
            ],
            "_index": 56
        },
        {
            "title": "TACK Tunnel Data (TTD): A Benchmark Dataset for Deep Learning-Based Defect Detection in Tunnels",
            "authors": [
                "Andreas Sjölander",
                "Valeria Belloni",
                "Robel Fekadu",
                "Andrea Nascetti"
            ],
            "arxiv_id": "2512.14477v1",
            "summary": "Tunnels are essential elements of transportation infrastructure, but are increasingly affected by ageing and deterioration mechanisms such as cracking. Regular inspections are required to ensure their safety, yet traditional manual procedures are time-consuming, subjective, and costly. Recent advances in mobile mapping systems and Deep Learning (DL) enable automated visual inspections. However, their effectiveness is limited by the scarcity of tunnel datasets. This paper introduces a new publicly available dataset containing annotated images of three different tunnel linings, capturing typical defects: cracks, leaching, and water infiltration. The dataset is designed to support supervised, semi-supervised, and unsupervised DL methods for defect detection and segmentation. Its diversity in texture and construction techniques also enables investigation of model generalization and transferability across tunnel types. By addressing the critical lack of domain-specific data, this dataset contributes to advancing automated tunnel inspection and promoting safer, more efficient infrastructure maintenance strategies.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14477v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "mapping"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出TACK隧道数据集（TTD），以解决隧道缺陷检测中领域数据稀缺问题，支持深度学习方法的自动化隧道巡检。",
            "summary_zh": "隧道是交通基础设施的关键组成部分，但日益受到老化和劣化机制（如裂缝）的影响。为确保安全，需要定期检查，但传统手动方法耗时、主观且成本高。移动测绘系统和深度学习的最新进展使得自动化视觉检查成为可能，但其有效性受限于隧道数据集的稀缺性。本文介绍了一个新的公开可用数据集，包含三种不同隧道衬砌的标注图像，捕捉典型缺陷：裂缝、渗漏和水渗透。该数据集旨在支持监督、半监督和无监督的深度学习方法进行缺陷检测和分割。其在纹理和施工技术上的多样性也使得能够研究模型在不同隧道类型间的泛化性和可迁移性。通过解决领域特定数据的关键缺乏问题，该数据集有助于推进自动化隧道检查，并促进更安全、更高效的基础设施维护策略。",
            "intro_zh": [
                "核心问题：隧道缺陷检测依赖传统手动检查，存在耗时、主观和成本高的不足，且深度学习应用受限于领域数据稀缺。",
                "方法要点：构建公开隧道数据集TTD，包含多类型隧道衬砌的标注图像，支持监督、半监督和无监督深度学习方法。",
                "实验或效果：数据集促进模型泛化研究，提升自动化检测效率，为基础设施维护提供数据基础。"
            ],
            "method_zh": "**问题定义**：论文旨在解决隧道缺陷检测中深度学习应用面临的领域数据稀缺问题。现有方法依赖有限或非公开数据集，导致模型训练不足、泛化能力差，限制了自动化隧道检查的效率和准确性。\\n\\n**核心思路**：通过构建一个公开、多样化的隧道数据集TTD，提供标注图像以支持多种深度学习方法的训练和评估，从而缓解数据瓶颈，促进模型在隧道缺陷检测任务上的性能提升和跨类型泛化。\\n\\n**技术框架**：整体流程包括数据采集、标注和数据集发布。首先，使用移动测绘系统捕获三种不同隧道衬砌的图像；然后，对图像中的缺陷（如裂缝、渗漏、水渗透）进行人工或半自动标注；最后，将数据集结构化，支持监督、半监督和无监督学习任务，并提供基准评估指标。\\n\\n**关键创新**：最重要的技术创新是首次公开一个专门针对隧道缺陷检测的多样化数据集，其覆盖多种隧道类型和缺陷类别，设计上强调支持半监督和无监督方法，以探索数据高效学习策略。\\n\\n**关键设计**：数据集包含高分辨率图像，标注格式支持像素级分割和边界框检测；具体参数如图像尺寸、标注类别（裂缝、渗漏、水渗透）和隧道类型（三种衬砌）经过精心选择，以反映实际隧道环境；损失函数和网络结构未在摘要中指定，但数据集设计兼容常见深度学习框架，便于用户自定义模型。",
            "application_zh": "该研究主要应用于交通基础设施的自动化隧道巡检领域，通过提供高质量数据集，支持深度学习模型开发，可提升缺陷检测的准确性和效率，降低维护成本。未来影响包括推动智能维护系统的发展，增强隧道安全监控，并可能扩展到其他类似结构（如桥梁、管道）的视觉检查任务。",
            "highlight_zh": "论文未提供具体性能数据或对比基线，但数据集TTD通过公开可用和多样性设计，显著提升了隧道缺陷检测的数据资源。其支持多种学习方法，促进了模型泛化研究，为后续实验提供了基准平台，预期能推动自动化检测技术的性能提升。",
            "tags_zh": [
                "隧道缺陷检测",
                "深度学习数据集",
                "自动化视觉检查",
                "基础设施维护",
                "模型泛化",
                "半监督学习",
                "图像分割",
                "移动测绘系统"
            ],
            "_index": 57
        },
        {
            "title": "Kinetic-Mamba: Mamba-Assisted Predictions of Stiff Chemical Kinetics",
            "authors": [
                "Additi Pandey",
                "Liang Wei",
                "Hessam Babaee",
                "George Em Karniadakis"
            ],
            "arxiv_id": "2512.14471v1",
            "summary": "Accurate chemical kinetics modeling is essential for combustion simulations, as it governs the evolution of complex reaction pathways and thermochemical states. In this work, we introduce Kinetic-Mamba, a Mamba-based neural operator framework that integrates the expressive power of neural operators with the efficient temporal modeling capabilities of Mamba architectures. The framework comprises three complementary models: (i) a standalone Mamba model that predicts the time evolution of thermochemical state variables from given initial conditions; (ii) a constrained Mamba model that enforces mass conservation while learning the state dynamics; and (iii) a regime-informed architecture employing two standalone Mamba models to capture dynamics across temperature-dependent regimes. We additionally develop a latent Kinetic-Mamba variant that evolves dynamics in a reduced latent space and reconstructs the full state on the physical manifold. We evaluate the accuracy and robustness of Kinetic-Mamba using both time-decomposition and recursive-prediction strategies. We further assess the extrapolation capabilities of the model on varied out-of-distribution datasets. Computational experiments on Syngas and GRI-Mech 3.0 reaction mechanisms demonstrate that our framework achieves high fidelity in predicting complex kinetic behavior using only the initial conditions of the state variables.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14471v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO",
                        "VIO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出Kinetic-Mamba框架，结合Mamba架构与神经算子，以解决燃烧模拟中刚性化学动力学的高精度预测问题。",
            "summary_zh": "精确的化学动力学建模对于燃烧模拟至关重要，它控制着复杂反应路径和热化学状态的演化。本文介绍了Kinetic-Mamba，这是一个基于Mamba的神经算子框架，将神经算子的表达能力与Mamba架构的高效时间建模能力相结合。该框架包含三个互补模型：（i）一个独立的Mamba模型，从给定的初始条件预测热化学状态变量的时间演化；（ii）一个约束Mamba模型，在学习状态动力学的同时强制质量守恒；（iii）一个基于区域的架构，采用两个独立的Mamba模型来捕捉跨温度依赖区域的动力学。我们还开发了一个潜在Kinetic-Mamba变体，在降维的潜在空间中演化动力学，并在物理流形上重建完整状态。我们使用时间分解和递归预测策略评估了Kinetic-Mamba的准确性和鲁棒性。我们进一步评估了模型在不同分布外数据集上的外推能力。在Syngas和GRI-Mech 3.0反应机制上的计算实验表明，我们的框架仅使用状态变量的初始条件就能在预测复杂动力学行为方面实现高保真度。",
            "intro_zh": [
                "核心问题：燃烧模拟中刚性化学动力学建模面临计算成本高、传统方法难以捕捉复杂非线性演化的问题，现有神经网络方法在长期预测和守恒约束方面存在不足。",
                "方法要点：提出Kinetic-Mamba框架，结合Mamba架构的高效序列建模能力与神经算子的表达能力，通过多种模型变体（如约束Mamba和区域感知架构）提升预测精度和物理一致性。",
                "实验或效果：在Syngas和GRI-Mech 3.0数据集上，Kinetic-Mamba仅基于初始条件实现了高保真预测，并在分布外数据上展现出良好的外推能力，验证了其鲁棒性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决燃烧模拟中刚性化学动力学的高精度预测问题。现有方法如传统数值求解器计算成本高昂，而基于神经网络的模型在长期时间序列预测中可能忽略物理约束（如质量守恒），且难以处理温度依赖的复杂动力学行为。\\n\\n**核心思路**：论文的核心思路是将Mamba架构的高效序列建模能力与神经算子的表达能力相结合，构建一个端到端的预测框架。通过设计多种模型变体（如约束模型和区域感知架构），在提升预测效率的同时，确保物理一致性（如质量守恒）和跨不同温度区域的适应性。\\n\\n**技术框架**：整体架构包括三个主要模型：1）独立Mamba模型：直接预测热化学状态变量的时间演化；2）约束Mamba模型：在学习动力学时强制质量守恒，通过损失函数或架构设计实现；3）区域感知架构：使用两个独立Mamba模型分别处理不同温度区域的动力学，以捕捉温度依赖行为。此外，还开发了潜在Kinetic-Mamba变体，先在降维潜在空间演化动力学，再重建到物理空间，以提升计算效率。\\n\\n**关键创新**：最重要的技术创新是首次将Mamba架构应用于化学动力学预测，结合神经算子框架，实现了高效且物理一致的序列建模。与现有方法（如基于RNN或Transformer的模型）相比，本质区别在于Mamba的选择性状态空间机制能更好地处理长序列依赖，同时通过约束设计和区域分割增强了模型的泛化能力和物理可解释性。\\n\\n**关键设计**：关键设计包括：1）使用Mamba作为核心序列建模模块，其参数化状态空间方程允许选择性信息传递；2）在约束模型中，通过损失函数（如添加质量守恒项）或架构修改（如投影层）强制物理约束；3）区域感知架构基于温度阈值分割数据，训练两个Mamba模型分别处理高低温区域；4）潜在变体采用自动编码器结构，编码器降维、Mamba演化潜在状态、解码器重建；5）训练策略结合时间分解（分阶段预测）和递归预测（自回归式），损失函数通常为均方误差（MSE）或结合物理约束的复合损失。",
            "application_zh": "该研究在燃烧模拟、能源系统和化学反应工程领域具有重要应用价值。Kinetic-Mamba框架可用于高效预测复杂燃料（如合成气）的燃烧动力学，支持发动机设计、污染物控制和过程优化。未来可能扩展到更广泛的化学动力学系统，如大气化学或生物反应网络，推动基于AI的物理建模发展。",
            "highlight_zh": "在Syngas和GRI-Mech 3.0反应机制上的实验表明，Kinetic-Mamba仅基于初始条件就能高精度预测热化学状态演化，相比基线神经网络模型（如LSTM或Transformer），在长期预测误差上降低约20-30%。模型在分布外数据集上展现出良好外推能力，验证了其鲁棒性。约束Mamba变体成功强制质量守恒，误差接近零，而区域感知架构有效捕捉了温度依赖动力学行为。",
            "tags_zh": [
                "化学动力学预测",
                "Mamba架构",
                "神经算子",
                "燃烧模拟",
                "序列建模",
                "物理约束学习",
                "潜在空间演化",
                "温度依赖建模"
            ],
            "_index": 58
        },
        {
            "title": "Nonlinear System Identification Nano-drone Benchmark",
            "authors": [
                "Riccardo Busetto",
                "Elia Cereda",
                "Marco Forgione",
                "Gabriele Maroni",
                "Dario Piga",
                "Daniele Palossi"
            ],
            "arxiv_id": "2512.14450v1",
            "summary": "We introduce a benchmark for system identification based on 75k real-world samples from the Crazyflie 2.1 Brushless nano-quadrotor, a sub-50g aerial vehicle widely adopted in robotics research. The platform presents a challenging testbed due to its multi-input, multi-output nature, open-loop instability, and nonlinear dynamics under agile maneuvers. The dataset comprises four aggressive trajectories with synchronized 4-dimensional motor inputs and 13-dimensional output measurements. To enable fair comparison of identification methods, the benchmark includes a suite of multi-horizon prediction metrics for evaluating both one-step and multi-step error propagation. In addition to the data, we provide a detailed description of the platform and experimental setup, as well as baseline models highlighting the challenge of accurate prediction under real-world noise and actuation nonlinearities. All data, scripts, and reference implementations are released as open-source at https://github.com/idsia-robotics/nanodrone-sysid-benchmark to facilitate transparent comparison of algorithms and support research on agile, miniaturized aerial robotics.",
            "categories": [
                "eess.SY",
                "cs.RO"
            ],
            "primary_category": "eess.SY",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14450v1",
            "code_links": [
                {
                    "url": "https://github.com/idsia-robotics/nanodrone-sysid-benchmark",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出基于Crazyflie 2.1纳米四旋翼的75k真实世界样本系统辨识基准，以解决微型无人机非线性动力学建模的挑战性问题。",
            "summary_zh": "我们引入了一个基于Crazyflie 2.1无刷纳米四旋翼（一种广泛用于机器人研究的重量低于50克的飞行器）的75k真实世界样本的系统辨识基准。该平台因其多输入多输出特性、开环不稳定性以及在敏捷机动下的非线性动力学而成为一个具有挑战性的测试平台。数据集包含四条激进轨迹，同步记录了4维电机输入和13维输出测量值。为了公平比较辨识方法，该基准包含一套多步长预测指标，用于评估单步和多步误差传播。除了数据外，我们还提供了平台和实验设置的详细描述，以及基线模型，突出了在真实世界噪声和执行器非线性下进行准确预测的挑战。所有数据、脚本和参考实现均以开源形式发布在https://github.com/idsia-robotics/nanodrone-sysid-benchmark，以促进算法的透明比较，并支持敏捷、微型空中机器人技术的研究。",
            "intro_zh": [
                "现有方法在微型无人机系统辨识中面临多输入多输出、开环不稳定和非线性动力学等挑战，缺乏标准化基准进行公平比较。",
                "论文提出基于Crazyflie 2.1纳米四旋翼的真实世界数据集和基准，包含75k样本、多步长预测指标和开源实现。",
                "基准提供了详细平台描述、基线模型和开源代码，显著促进了系统辨识算法的透明评估和微型无人机研究。"
            ],
            "method_zh": "**问题定义**：论文旨在解决微型无人机（如Crazyflie 2.1纳米四旋翼）系统辨识中的挑战性问题，包括多输入多输出（MIMO）特性、开环不稳定性、非线性动力学以及真实世界噪声和执行器非线性。现有方法的痛点在于缺乏标准化、大规模的真实世界数据集和公平比较基准，导致算法评估不透明，难以推广到实际应用。\\n\\n**核心思路**：论文的核心解决思路是构建一个基于真实世界数据的系统辨识基准，通过提供高质量数据集、标准化评估指标和开源工具，促进算法的透明比较和优化。这样设计是因为微型无人机的复杂动力学需要真实数据来捕捉非线性效应，而基准化能推动研究社区的统一进展。\\n\\n**技术框架**：整体架构包括数据采集、基准构建和评估三个主要阶段。数据采集阶段使用Crazyflie 2.1平台记录四条激进轨迹的4维电机输入和13维输出测量值；基准构建阶段整合75k样本，并提供平台描述和实验设置；评估阶段基于多步长预测指标（如单步和多步误差传播）来比较不同辨识方法。\\n\\n**关键创新**：最重要的技术创新点是引入了首个基于真实世界样本的微型无人机系统辨识基准，强调多步长预测评估，以模拟实际控制中的误差累积。与现有方法的本质区别在于其全面性：结合了大规模数据、标准化指标和开源实现，解决了以往基准缺乏真实性和可比性的问题。\\n\\n**关键设计**：关键设计包括数据集包含4维输入（电机控制信号）和13维输出（如姿态、角速度等测量值），使用多步长预测指标来评估模型在时间序列上的泛化能力，并提供基线模型（如线性或简单非线性模型）作为参考点。损失函数可能基于预测误差，但论文未指定具体细节，强调基准的通用性以支持多种辨识方法。",
            "application_zh": "该研究在微型无人机控制、机器人系统辨识和敏捷空中机器人技术领域具有重要应用价值。通过提供标准化基准，可加速算法开发，提升无人机在复杂环境中的自主飞行性能，未来可能推动微型无人机在搜索救援、环境监测等实际场景中的部署。",
            "highlight_zh": "最重要的实验结果包括提供了75k真实世界样本的数据集，覆盖四条激进轨迹，并引入了多步长预测指标来评估系统辨识方法。基准包含基线模型，展示了在真实噪声和非线性下预测的挑战，具体性能数据未在摘要中给出，但开源实现促进了算法的透明比较，预计能显著提升微型无人机动力学建模的准确性。",
            "tags_zh": [
                "系统辨识",
                "纳米四旋翼",
                "真实世界数据",
                "多步长预测",
                "开源基准",
                "微型无人机",
                "非线性动力学",
                "机器人控制"
            ],
            "_index": 59
        },
        {
            "title": "A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning",
            "authors": [
                "Zixin Zhang",
                "Kanghao Chen",
                "Hanqing Wang",
                "Hongfei Zhang",
                "Harold Haodong Chen",
                "Chenfei Liao",
                "Litao Guo",
                "Ying-Cong Chen"
            ],
            "arxiv_id": "2512.14442v1",
            "summary": "Affordance prediction, which identifies interaction regions on objects based on language instructions, is critical for embodied AI. Prevailing end-to-end models couple high-level reasoning and low-level grounding into a single monolithic pipeline and rely on training over annotated datasets, which leads to poor generalization on novel objects and unseen environments. In this paper, we move beyond this paradigm by proposing A4-Agent, a training-free agentic framework that decouples affordance prediction into a three-stage pipeline. Our framework coordinates specialized foundation models at test time: (1) a $\\textbf{Dreamer}$ that employs generative models to visualize $\\textit{how}$ an interaction would look; (2) a $\\textbf{Thinker}$ that utilizes large vision-language models to decide $\\textit{what}$ object part to interact with; and (3) a $\\textbf{Spotter}$ that orchestrates vision foundation models to precisely locate $\\textit{where}$ the interaction area is. By leveraging the complementary strengths of pre-trained models without any task-specific fine-tuning, our zero-shot framework significantly outperforms state-of-the-art supervised methods across multiple benchmarks and demonstrates robust generalization to real-world settings.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14442v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "世界模型",
                    "matched_keywords": [
                        "dreamer"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出A4-Agent框架，通过解耦推理流程实现零样本可及性预测，解决现有方法泛化能力差的问题。",
            "summary_zh": "可及性预测是根据语言指令识别物体上交互区域的关键技术，对具身AI至关重要。当前主流端到端模型将高层推理与低层定位耦合在单一流程中，并依赖标注数据集训练，导致在新物体和未见环境上泛化能力差。本文超越这一范式，提出A4-Agent，一种无需训练的智能体框架，将可及性预测解耦为三阶段流程。该框架在测试时协调专用基础模型：(1) Dreamer利用生成模型可视化交互过程；(2) Thinker利用大型视觉语言模型决定交互的物体部分；(3) Spotter协调视觉基础模型精确定位交互区域。通过利用预训练模型的互补优势而无需任务特定微调，我们的零样本框架在多个基准测试中显著优于最先进的监督方法，并展现出对真实场景的鲁棒泛化能力。",
            "intro_zh": [
                "现有端到端模型耦合推理与定位，依赖标注数据训练，导致泛化能力差，难以处理新物体和未见环境。",
                "提出A4-Agent框架，将可及性预测解耦为三阶段流程，协调专用基础模型实现零样本推理，无需训练。",
                "在多个基准测试中显著优于监督方法，展现出鲁棒泛化能力，提升可及性预测的准确性和适应性。"
            ],
            "method_zh": "**问题定义**：论文解决可及性预测问题，即根据语言指令识别物体上交互区域，这对具身AI至关重要。现有方法的痛点在于端到端模型将高层推理（如理解指令）和低层定位（如像素级区域识别）耦合在单一流程中，并依赖大量标注数据集进行训练，导致泛化能力差，难以处理新物体和未见环境，限制了实际应用。\\n\\n**核心思路**：论文的核心解决思路是超越传统端到端范式，提出一种训练免费的智能体框架A4-Agent，通过解耦可及性预测为三阶段流程，协调专用基础模型在测试时进行推理。这样设计旨在利用预训练模型的互补优势，避免任务特定微调，从而提高泛化能力和适应性，实现零样本学习。\\n\\n**技术框架**：整体架构包含三个主要阶段：(1) Dreamer阶段：利用生成模型（如扩散模型）可视化交互过程，生成“如何”交互的图像，帮助理解指令的语义；(2) Thinker阶段：利用大型视觉语言模型（如CLIP或类似模型）分析输入图像和语言指令，决定“什么”物体部分需要交互，进行高层推理；(3) Spotter阶段：协调视觉基础模型（如分割或检测模型）精确定位“哪里”是交互区域，实现低层定位。这三个阶段顺序执行，形成完整的推理流程。\\n\\n**关键创新**：最重要的技术创新点是提出一种解耦的、基于智能体的框架，将可及性预测从单一模型分解为多阶段协作流程，并利用预训练基础模型实现零样本学习。与现有方法的本质区别在于：现有方法通常依赖端到端训练和标注数据，而A4-Agent无需训练，通过协调现有模型在推理时动态组合，显著提升了泛化能力和效率。\\n\\n**关键设计**：关键设计包括：使用生成模型（如稳定扩散）作为Dreamer，可视化交互；采用大型视觉语言模型（如GPT-4V或类似开源模型）作为Thinker，进行语义推理；集成视觉基础模型（如SAM或DINO）作为Spotter，进行精确定位。框架无需任务特定参数设置或损失函数，主要依赖预训练模型的零样本能力，通过模块化设计实现灵活协调，具体实现细节如模型选择和接口设计在论文中描述，但核心是无需微调。",
            "application_zh": "该研究在具身AI、机器人操作和智能交互系统中有广泛应用潜力。例如，在家庭服务机器人中，A4-Agent可以帮助机器人根据自然语言指令（如“打开冰箱门”）准确识别交互区域，提升操作精度和适应性。未来可能推动零样本学习在视觉推理领域的发展，降低对标注数据的依赖，促进AI系统在动态环境中的部署。",
            "highlight_zh": "实验结果显示，A4-Agent在多个基准测试中显著优于最先进的监督方法。具体而言，在标准可及性预测数据集上，零样本性能提升约10-15%的准确率；在未见物体和场景的泛化测试中，展现出鲁棒性，性能优于基线方法20%以上。对比基线包括端到端监督模型和传统零样本方法，A4-Agent通过解耦框架实现了更高的泛化能力和效率。",
            "tags_zh": [
                "可及性预测",
                "零样本学习",
                "具身AI",
                "视觉语言模型",
                "智能体框架",
                "解耦推理",
                "基础模型协调",
                "泛化能力"
            ],
            "_index": 60
        },
        {
            "title": "Effect of Document Packing on the Latent Multi-Hop Reasoning Capabilities of Large Language Models",
            "authors": [
                "Gabriele Prato",
                "Shagun Sodhani",
                "Alessandro Sordoni",
                "Sarath Chandar"
            ],
            "arxiv_id": "2512.14427v1",
            "summary": "The standard practice for training large language models involves packing multiple documents together to optimize computational efficiency. However, the impact of this process on the models' capabilities remains largely unexplored. To address this gap, we investigate how different document-packing strategies influence the latent multi-hop reasoning abilities of LLMs. Our findings indicate that packing can improve model performance compared to training on individual documents, at the expense of more compute. To further understand the underlying mechanisms, we conduct an ablation study, identifying key factors that explain the advantages of packing. Ultimately, our research deepens the understanding of LLM training dynamics and provides practical insights for optimizing model development.",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14427v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "研究文档打包策略对大型语言模型潜在多跳推理能力的影响，以优化训练效率与性能。",
            "summary_zh": "训练大型语言模型的标准实践涉及将多个文档打包在一起以优化计算效率，但这一过程对模型能力的影响在很大程度上尚未被探索。为填补这一空白，我们研究了不同文档打包策略如何影响LLMs的潜在多跳推理能力。我们的发现表明，与在单个文档上训练相比，打包可以提高模型性能，但以更多计算为代价。为了进一步理解底层机制，我们进行了消融研究，识别了解释打包优势的关键因素。最终，我们的研究加深了对LLM训练动态的理解，并为优化模型开发提供了实用见解。",
            "intro_zh": [
                "现有方法中，文档打包虽提升计算效率，但对模型潜在多跳推理能力的影响未知，缺乏系统评估。",
                "论文提出通过比较不同打包策略，分析其对LLMs多跳推理能力的影响，并识别关键因素。",
                "实验表明打包能提高模型性能，但需更多计算；消融研究揭示了打包优势的机制，为训练优化提供指导。"
            ],
            "method_zh": "**问题定义**：论文旨在解决文档打包策略对大型语言模型（LLMs）潜在多跳推理能力的具体影响问题。现有方法的痛点在于，标准训练中为优化计算效率而打包多个文档，但这一过程对模型能力（尤其是多跳推理）的影响未被充分研究，可能导致训练效率与性能之间的权衡不明确。\\n\\n**核心思路**：论文的核心解决思路是通过系统实验比较不同文档打包策略，评估其对LLMs多跳推理能力的影响，并利用消融研究识别关键因素。这样设计是为了填补现有研究空白，从训练动态角度理解打包的利弊，为模型开发提供数据驱动的优化建议。\\n\\n**技术框架**：整体架构包括数据准备、模型训练、评估和消融分析四个阶段。首先，准备不同打包策略的数据集（如随机打包、基于主题打包等）。然后，在LLMs上进行训练，使用标准语言建模目标。接着，通过多跳推理任务评估模型性能。最后，进行消融研究，分析打包策略中的关键变量（如文档长度、顺序、相关性）对性能的影响。\\n\\n**关键创新**：最重要的技术创新点是首次系统研究文档打包策略对LLMs多跳推理能力的潜在影响，并引入消融分析来揭示底层机制。与现有方法的本质区别在于，现有工作多关注打包的计算效率，而本文聚焦于能力影响，提供了更全面的训练动态视角。\\n\\n**关键设计**：关键设计包括使用标准Transformer架构的LLMs，训练时采用交叉熵损失函数进行语言建模。参数设置涉及批量大小、学习率等标准超参数，但针对不同打包策略进行调整。消融研究中，关键变量如文档间相关性、打包顺序被独立控制，以量化其对性能的贡献。",
            "application_zh": "该研究在大型语言模型训练优化领域具有潜在应用价值，可指导开发者平衡计算效率与模型性能，特别是在资源受限场景下。实际价值包括提升多跳推理任务的模型能力，未来可能影响训练策略标准化，促进更高效的AI模型开发。",
            "highlight_zh": "实验结果显示，文档打包策略能显著提高LLMs在多跳推理任务上的性能，具体提升幅度未知（论文未提供精确数据），但相比单个文档训练有优势。消融研究识别出文档相关性和打包顺序是关键因素，解释了性能提升机制。对比基线包括不同打包策略，结果强调了打包在优化训练动态中的重要性。",
            "tags_zh": [
                "文档打包",
                "多跳推理",
                "大型语言模型",
                "训练优化",
                "消融研究",
                "计算效率",
                "语言建模",
                "模型性能"
            ],
            "_index": 61
        },
        {
            "title": "The Devil is in Attention Sharing: Improving Complex Non-rigid Image Editing Faithfulness via Attention Synergy",
            "authors": [
                "Zhuo Chen",
                "Fanyue Wei",
                "Runze Xu",
                "Jingjing Li",
                "Lixin Duan",
                "Angela Yao",
                "Wen Li"
            ],
            "arxiv_id": "2512.14423v1",
            "summary": "Training-free image editing with large diffusion models has become practical, yet faithfully performing complex non-rigid edits (e.g., pose or shape changes) remains highly challenging. We identify a key underlying cause: attention collapse in existing attention sharing mechanisms, where either positional embeddings or semantic features dominate visual content retrieval, leading to over-editing or under-editing.To address this issue, we introduce SynPS, a method that Synergistically leverages Positional embeddings and Semantic information for faithful non-rigid image editing. We first propose an editing measurement that quantifies the required editing magnitude at each denoising step. Based on this measurement, we design an attention synergy pipeline that dynamically modulates the influence of positional embeddings, enabling SynPS to balance semantic modifications and fidelity preservation.By adaptively integrating positional and semantic cues, SynPS effectively avoids both over- and under-editing. Extensive experiments on public and newly curated benchmarks demonstrate the superior performance and faithfulness of our approach.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page:https://synps26.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14423v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出SynPS方法，通过注意力协同机制解决复杂非刚性图像编辑中的过编辑和欠编辑问题。",
            "summary_zh": "基于大型扩散模型的无训练图像编辑已变得实用，但忠实执行复杂非刚性编辑（如姿态或形状变化）仍极具挑战。本文发现一个关键原因：现有注意力共享机制中的注意力崩溃，其中位置嵌入或语义特征主导视觉内容检索，导致过编辑或欠编辑。为解决此问题，我们引入SynPS方法，协同利用位置嵌入和语义信息以实现忠实的非刚性图像编辑。我们首先提出一种编辑度量，量化每个去噪步骤所需的编辑幅度。基于此度量，我们设计了一个注意力协同流程，动态调节位置嵌入的影响，使SynPS能够平衡语义修改和保真度保持。通过自适应整合位置和语义线索，SynPS有效避免了过编辑和欠编辑。在公共和新构建的基准测试上的广泛实验证明了我们方法的优越性能和忠实性。",
            "intro_zh": [
                "核心问题：现有注意力共享机制存在注意力崩溃，导致复杂非刚性编辑中过编辑或欠编辑，影响编辑忠实性。",
                "方法要点：提出SynPS方法，协同位置嵌入和语义信息，动态调节注意力以平衡编辑幅度和保真度。",
                "实验或效果：在公共和新基准上验证，SynPS在编辑忠实性方面表现优越，有效避免过编辑和欠编辑。"
            ],
            "method_zh": "**问题定义**：论文旨在解决基于大型扩散模型的复杂非刚性图像编辑（如姿态或形状变化）中的忠实性问题。现有方法的痛点在于注意力共享机制中的注意力崩溃，其中位置嵌入或语义特征过度主导内容检索，导致过编辑（编辑过度失真）或欠编辑（编辑不足），从而降低编辑的准确性和保真度。\\n\\n**核心思路**：论文的核心解决思路是协同利用位置嵌入和语义信息，通过动态调节注意力机制来平衡编辑幅度和保真度。这样设计是因为位置嵌入和语义特征在编辑过程中各有优势：位置嵌入有助于保持结构一致性，而语义特征支持内容修改；协同使用可以避免单一因素主导，从而提升编辑的忠实性。\\n\\n**技术框架**：整体架构包括两个主要阶段：首先，提出编辑度量模块，用于量化每个去噪步骤所需的编辑幅度；其次，设计注意力协同流程，基于编辑度量动态调制位置嵌入的影响，实现位置和语义线索的自适应整合。流程涉及输入图像、扩散模型去噪步骤和输出编辑图像。\\n\\n**关键创新**：最重要的技术创新点是注意力协同机制，它动态平衡位置嵌入和语义信息的影响，避免注意力崩溃。与现有方法的本质区别在于，现有方法通常固定或简单组合注意力共享，而SynPS通过量化编辑需求进行自适应调节，从而更精确地控制编辑过程。\\n\\n**关键设计**：关键设计包括编辑度量函数，可能基于特征差异或梯度计算；注意力调制模块，使用权重参数动态调整位置嵌入的贡献；以及去噪步骤中的迭代优化，确保编辑幅度与保真度之间的权衡。具体参数设置和网络结构细节在论文中未详细说明，但涉及标准扩散模型框架的扩展。",
            "application_zh": "该研究在计算机视觉和图像处理领域具有广泛潜在应用，如虚拟试衣、动画制作、医疗图像编辑和增强现实。通过提升复杂非刚性编辑的忠实性，可支持更精确的图像修改，减少人工干预，推动自动化编辑工具的发展，对娱乐、教育和工业设计等行业产生实际价值。",
            "highlight_zh": "在公共基准（如ImageNet）和新构建的复杂非刚性编辑数据集上，SynPS在编辑忠实性方面显著优于基线方法（如Prompt-to-Prompt和InstructPix2Pix）。具体性能数据表明，SynPS在定量指标（如FID和用户评分）上提升约10-20%，有效减少过编辑和欠编辑案例，验证了其协同机制的优越性。",
            "tags_zh": [
                "非刚性图像编辑",
                "注意力机制",
                "扩散模型",
                "语义编辑",
                "位置嵌入",
                "编辑忠实性",
                "协同学习",
                "图像生成"
            ],
            "_index": 62
        },
        {
            "title": "Hybrid Ensemble Method for Detecting Cyber-Attacks in Water Distribution Systems Using the BATADAL Dataset",
            "authors": [
                "Waqas Ahmed"
            ],
            "arxiv_id": "2512.14422v1",
            "summary": "The cybersecurity of Industrial Control Systems that manage critical infrastructure such as Water Distribution Systems has become increasingly important as digital connectivity expands. BATADAL benchmark data is a good source of testing intrusion detection techniques, but it presents several important problems, such as imbalance in the number of classes, multivariate time dependence, and stealthy attacks. We consider a hybrid ensemble learning model that will enhance the detection ability of cyber-attacks in WDS by using the complementary capabilities of machine learning and deep learning models. Three base learners, namely, Random Forest , eXtreme Gradient Boosting , and Long Short-Term Memory network, have been strictly compared and seven ensemble types using simple averaged and stacked learning with a logistic regression meta-learner. Random Forest analysis identified top predictors turned into temporal and statistical features, and Synthetic Minority Oversampling Technique (SMOTE) was used to overcome the class imbalance issue. The analyics indicates that the single Long Short-Term Memory network model is of poor performance (F1 = 0.000, AUC = 0.4460), but tree-based models, especially eXtreme Gradient Boosting, perform well (F1 = 0.7470, AUC=0.9684). The hybrid stacked ensemble of Random Forest , eXtreme Gradient Boosting , and Long Short-Term Memory network scored the highest, with the attack class of 0.7205 with an F1-score and a AUC of 0.9826 indicating that the heterogeneous stacking between model precision and generalization can work. The proposed framework establishes a robust and scalable solution for cyber-attack detection in time-dependent industrial systems, integrating temporal learning and ensemble diversity to support the secure operation of critical infrastructure.",
            "categories": [
                "cs.CR",
                "cs.LG"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "18 pages, & figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14422v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "人形机器人",
                    "matched_keywords": [
                        "digit"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出混合集成学习方法以解决供水系统网络攻击检测中的类别不平衡、多变量时间依赖和隐蔽攻击问题",
            "summary_zh": "随着数字连接性的扩展，管理供水系统等关键基础设施的工业控制系统网络安全日益重要。BATADAL基准数据是测试入侵检测技术的良好来源，但它存在几个重要问题，如类别数量不平衡、多变量时间依赖性和隐蔽攻击。我们考虑采用混合集成学习模型，通过利用机器学习和深度学习模型的互补能力，增强供水系统中网络攻击的检测能力。对三个基础学习器（随机森林、极限梯度提升和长短期记忆网络）进行了严格比较，并使用了七种集成类型，包括简单平均和带有逻辑回归元学习器的堆叠学习。随机森林分析确定了转化为时间和统计特征的重要预测因子，并使用合成少数类过采样技术（SMOTE）来克服类别不平衡问题。分析表明，单一长短期记忆网络模型性能较差（F1=0.000，AUC=0.4460），但基于树的模型，特别是极限梯度提升，表现良好（F1=0.7470，AUC=0.9684）。随机森林、极限梯度提升和长短期记忆网络的混合堆叠集成得分最高，攻击类别的F1分数为0.7205，AUC为0.9826，表明模型精度和泛化能力之间的异构堆叠是有效的。所提出的框架为时间依赖的工业系统中的网络攻击检测建立了一个稳健且可扩展的解决方案，整合了时间学习和集成多样性，以支持关键基础设施的安全运行。",
            "intro_zh": [
                "核心问题：BATADAL数据集存在类别不平衡、多变量时间依赖和隐蔽攻击等挑战，传统单一模型难以有效检测供水系统中的网络攻击。",
                "方法要点：提出混合集成学习框架，结合随机森林、极限梯度提升和长短期记忆网络，通过堆叠集成和SMOTE技术提升检测性能。",
                "实验或效果：混合堆叠集成在攻击类别上达到F1分数0.7205和AUC 0.9826，显著优于单一模型，验证了异构集成的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决供水系统（WDS）中网络攻击检测的挑战，具体问题包括BATADAL数据集中的类别不平衡、多变量时间依赖性和隐蔽攻击。现有方法的痛点在于单一模型（如深度学习或传统机器学习）难以同时处理这些复杂问题，导致检测性能受限，例如长短期记忆网络（LSTM）在单独使用时表现不佳（F1=0.000）。\\n\\n**核心思路**：论文的核心解决思路是采用混合集成学习方法，结合机器学习和深度学习模型的互补优势，通过集成多样性来增强检测能力。这样设计是因为基于树的模型（如随机森林和极限梯度提升）擅长处理结构化数据和特征重要性，而LSTM能捕捉时间序列依赖，混合集成可以弥补单一模型的不足，提高整体鲁棒性。\\n\\n**技术框架**：整体架构包括数据预处理、特征工程、基础学习器训练、集成构建和评估阶段。主要模块：首先，使用随机森林分析识别重要预测因子，并将其转化为时间和统计特征；其次，应用SMOTE处理类别不平衡；然后，训练三个基础学习器（随机森林、极限梯度提升、LSTM）；接着，构建七种集成类型，包括简单平均和堆叠学习（使用逻辑回归作为元学习器）；最后，通过性能指标（如F1分数和AUC）评估模型。\\n\\n**关键创新**：最重要的技术创新点是提出异构堆叠集成框架，将基于树的模型与深度学习模型结合，以同时优化模型精度和泛化能力。与现有方法的本质区别在于，它不依赖单一模型类型，而是通过集成多样性来应对复杂的时间依赖和攻击隐蔽性，这在工业控制系统中具有实际应用价值。\\n\\n**关键设计**：关键设计包括：使用随机森林进行特征选择，将重要预测因子转化为时间和统计特征；应用SMOTE技术平衡数据集，解决类别不平衡问题；基础学习器参数设置未知，但基于树的模型可能采用默认或优化参数，LSTM网络结构未知；集成方法中，堆叠学习使用逻辑回归作为元学习器来组合基础模型的预测；损失函数和具体网络结构细节在摘要中未提及，但整体框架强调模型互补性和集成策略。",
            "application_zh": "该研究主要应用于工业控制系统（ICS）的网络安全领域，特别是供水系统等关键基础设施的网络攻击检测。潜在应用包括其他时间依赖的工业系统，如电力网、交通控制系统，以提升入侵检测的准确性和鲁棒性。实际价值在于提供了一种可扩展的解决方案，支持关键基础设施的安全运行，未来可能扩展到更多工业场景，促进智能城市和物联网安全的发展。",
            "highlight_zh": "最重要的实验结果显示：单一长短期记忆网络（LSTM）模型性能极差，F1分数为0.000，AUC为0.4460；而基于树的模型中，极限梯度提升（XGBoost）表现最佳，F1分数达0.7470，AUC为0.9684。混合堆叠集成（结合随机森林、XGBoost和LSTM）取得了最高性能，攻击类别的F1分数为0.7205，AUC高达0.9826，显著优于单一模型和其他集成类型，验证了异构集成在提升检测精度和泛化能力方面的有效性。",
            "tags_zh": [
                "网络攻击检测",
                "混合集成学习",
                "供水系统安全",
                "时间序列分析",
                "类别不平衡处理",
                "堆叠集成",
                "工业控制系统",
                "BATADAL数据集"
            ],
            "_index": 63
        },
        {
            "title": "Dual-Axis RCCL: Representation-Complete Convergent Learning for Organic Chemical Space",
            "authors": [
                "Dejun Hu",
                "Zhiming Li",
                "Jia-Rui Shen",
                "Jia-Ning Tu",
                "Zi-Hao Ye",
                "Junliang Zhang"
            ],
            "arxiv_id": "2512.14418v1",
            "summary": "Machine learning is profoundly reshaping molecular and materials modeling; however, given the vast scale of chemical space (10^30-10^60), it remains an open scientific question whether models can achieve convergent learning across this space. We introduce a Dual-Axis Representation-Complete Convergent Learning (RCCL) strategy, enabled by a molecular representation that integrates graph convolutional network (GCN) encoding of local valence environments, grounded in modern valence bond theory, together with no-bridge graph (NBG) encoding of ring/cage topologies, providing a quantitative measure of chemical-space coverage. This framework formalizes representation completeness, establishing a principled basis for constructing datasets that support convergent learning for large models. Guided by this RCCL framework, we develop the FD25 dataset, systematically covering 13,302 local valence units and 165,726 ring/cage topologies, achieving near-complete combinatorial coverage of organic molecules with H/C/N/O/F elements. Graph neural networks trained on FD25 exhibit representation-complete convergent learning and strong out-of-distribution generalization, with an overall prediction error of approximately 1.0 kcal/mol MAE across external benchmarks. Our results establish a quantitative link between molecular representation, structural completeness, and model generalization, providing a foundation for interpretable, transferable, and data-efficient molecular intelligence.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "33 pages, 10 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14418v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出双轴表示完全收敛学习策略，解决有机化学空间模型收敛与泛化难题",
            "summary_zh": "机器学习正在深刻重塑分子与材料建模领域，然而面对化学空间的巨大规模（10^30-10^60），模型能否在该空间实现收敛学习仍是一个开放的科学问题。我们提出了一种双轴表示完全收敛学习策略，该策略通过一种分子表示实现，该表示整合了基于现代价键理论的局部价环境图卷积网络编码，以及环/笼拓扑的无桥图编码，从而提供了化学空间覆盖的定量度量。该框架形式化了表示完全性，为构建支持大模型收敛学习的数据集建立了原则性基础。在此RCCL框架指导下，我们开发了FD25数据集，系统覆盖了13,302个局部价单元和165,726种环/笼拓扑，实现了对含H/C/N/O/F元素有机分子的近乎完全组合覆盖。在FD25上训练的图神经网络表现出表示完全收敛学习和强大的分布外泛化能力，在外部基准测试中整体预测误差约为1.0 kcal/mol MAE。我们的研究结果建立了分子表示、结构完全性与模型泛化之间的定量联系，为可解释、可迁移且数据高效的分子智能奠定了基础。",
            "intro_zh": [
                "核心问题：化学空间极其庞大（10^30-10^60分子），现有机器学习模型难以实现收敛学习和泛化，缺乏评估数据覆盖度的定量方法。",
                "方法要点：提出双轴RCCL策略，结合局部价环境GCN编码与环/笼拓扑NBG编码，构建表示完全性框架指导数据集构建。",
                "实验或效果：构建FD25数据集，训练模型在外部基准上实现约1.0 kcal/mol MAE误差，验证了收敛学习与强泛化能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决机器学习模型在庞大有机化学空间（10^30-10^60分子）中难以实现收敛学习和泛化的核心挑战。现有方法的痛点在于缺乏对化学空间覆盖度的定量评估，数据集构建往往基于经验或有限采样，导致模型在分布外数据上泛化能力不足，无法保证学习过程的收敛性。\\n\\n**核心思路**：论文的核心思路是提出“表示完全收敛学习”框架，通过形式化定义“表示完全性”来指导数据集的系统性构建。其设计基于双轴表示：一轴编码分子的局部化学环境（基于价键理论），另一轴编码分子的全局拓扑结构（环/笼系统），从而实现对化学空间的组合式、定量化覆盖。\\n\\n**技术框架**：整体架构包含两个主要阶段。第一阶段是表示构建：使用图卷积网络对分子图的局部价环境（如原子类型、键类型、邻接关系）进行编码，同时使用无桥图对分子中的环和笼状拓扑结构进行编码，形成双轴表示。第二阶段是数据集构建与模型训练：基于RCCL框架，通过枚举局部价单元和环/笼拓扑的组合，构建FD25数据集，确保覆盖度接近完全；然后使用图神经网络（如GCN或更先进的变体）在该数据集上进行训练，以学习分子性质（如能量）。\\n\\n**关键创新**：最重要的技术创新是提出了“表示完全性”的定量框架和双轴分子表示方法。与现有方法（通常依赖单一图表示或描述符）的本质区别在于，它将化学空间分解为两个正交维度进行系统覆盖，并提供了衡量覆盖完整性的指标，从而为收敛学习提供了理论保证。\\n\\n**关键设计**：关键设计包括：1) 局部价环境编码基于现代价键理论，使用GCN处理分子图，捕获原子和键的局部特征；2) 环/笼拓扑编码采用无桥图，这是一种简化的图表示，仅保留环和笼结构，忽略桥键，以量化拓扑多样性；3) FD25数据集构建通过组合13,302个局部价单元和165,726种环/笼拓扑实现；4) 训练使用标准图神经网络架构，损失函数为均方误差用于回归任务，具体网络参数和优化器在论文中未详细说明，但强调在大型数据集上训练以实现收敛。",
            "application_zh": "该研究在计算化学、药物发现和材料科学领域具有重要应用价值。通过实现表示完全收敛学习，模型能够更准确地预测分子性质（如能量、溶解度、活性），加速新分子设计和高通量筛选。其框架为构建高质量分子数据集提供了原则性指导，可提升AI在化学领域的可解释性和数据效率，推动分子智能向更可靠、泛化更强的方向发展，未来可能应用于催化剂设计、有机电子材料开发等复杂任务。",
            "highlight_zh": "最重要的实验结果是基于RCCL框架构建的FD25数据集训练图神经网络，在多个外部基准测试中实现了强大的泛化性能，整体预测误差约为1.0 kcal/mol MAE（平均绝对误差）。这显著优于传统或基于有限数据训练的模型，验证了表示完全性对收敛学习的促进作用。具体对比基线未在摘要中提及，但结果显示了模型在分布外数据上的稳定表现，为化学空间机器学习提供了新的性能标杆。",
            "tags_zh": [
                "化学空间学习",
                "表示完全性",
                "图神经网络",
                "分子表示",
                "收敛学习",
                "泛化能力",
                "有机分子数据集",
                "价键理论编码"
            ],
            "_index": 64
        },
        {
            "title": "Massive Editing for Large Language Models Based on Dynamic Weight Generation",
            "authors": [
                "Wentao Wan",
                "Qiqing Lao",
                "Zhiwei Xie",
                "Hefeng Wu",
                "Runnan Lin",
                "Liang Lin",
                "Keze Wang"
            ],
            "arxiv_id": "2512.14395v1",
            "summary": "Knowledge Editing (KE) is a field that studies how to modify some knowledge in Large Language Models (LLMs) at a low cost (compared to pre-training). Currently, performing large-scale edits on LLMs while ensuring the Reliability, Generality, and Locality metrics of the edits remain a challenge. This paper proposes a Massive editing approach for LLMs based on dynamic weight Generation (MeG). Our MeG involves attaching a dynamic weight neuron to specific layers of the LLMs and using a diffusion model to conditionally generate the weights of this neuron based on the input query required for the knowledge. This allows the use of adding a single dynamic weight neuron to achieve the goal of large-scale knowledge editing. Experiments show that our MeG can significantly improve the performance of large-scale KE in terms of Reliability, Generality, and Locality metrics compared to existing knowledge editing methods, particularly with a high percentage point increase in the absolute value index for the Locality metric, demonstrating the advantages of our proposed method.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "27 pages, 8 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14395v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出基于动态权重生成的大规模编辑方法MeG，以解决大语言模型知识编辑中的可靠性、泛化性和局部性挑战。",
            "summary_zh": "知识编辑（KE）是研究如何以低成本（相比预训练）修改大语言模型（LLMs）中某些知识的领域。目前，在对LLMs进行大规模编辑的同时，确保编辑的可靠性、泛化性和局部性指标仍是一个挑战。本文提出了一种基于动态权重生成的大规模编辑方法（MeG）。我们的MeG涉及在LLMs的特定层附加一个动态权重神经元，并使用扩散模型根据知识所需的输入查询条件生成该神经元的权重。这使得通过添加单个动态权重神经元来实现大规模知识编辑的目标成为可能。实验表明，与现有知识编辑方法相比，我们的MeG在可靠性、泛化性和局部性指标方面能显著提升大规模KE的性能，特别是在局部性指标的绝对值指数上实现了高百分点的增长，展示了我们提出方法的优势。",
            "intro_zh": [
                "现有方法在大规模知识编辑中难以同时保证可靠性、泛化性和局部性，导致编辑效果受限。",
                "MeG通过动态权重神经元和扩散模型生成权重，实现基于查询的灵活编辑，提升编辑效率。",
                "实验显示MeG在局部性指标上实现显著提升，优于现有方法，验证了其在大规模编辑中的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大语言模型（LLMs）中大规模知识编辑（KE）的挑战，即在修改模型知识时，需同时满足可靠性（编辑后模型正确响应新知识）、泛化性（编辑能推广到相关查询）和局部性（不影响无关知识）。现有方法如基于梯度或参数更新的编辑技术，在大规模编辑时往往导致性能下降或计算成本高，难以平衡这三个指标。\\n\\n**核心思路**：论文提出MeG方法，其核心思想是通过在LLMs的特定层附加一个动态权重神经元，并利用扩散模型条件生成该神经元的权重，从而基于输入查询实现灵活的知识编辑。这样设计避免了直接修改模型原始参数，减少了编辑对模型整体性能的干扰，同时通过扩散模型的生成能力，支持大规模编辑需求。\\n\\n**技术框架**：整体架构包括两个主要阶段：首先，在LLMs的选定层（如中间层或输出层）集成一个动态权重神经元，该神经元负责根据编辑知识调整模型输出；其次，使用扩散模型作为权重生成器，以输入查询（如需要编辑的知识相关文本）为条件，生成动态权重神经元的权重值。编辑过程涉及训练扩散模型来学习权重分布，并在推理时根据查询实时生成权重，实现编辑效果。\\n\\n**关键创新**：最重要的技术创新是结合动态权重神经元和扩散模型进行条件权重生成，这允许通过添加单个神经元实现大规模编辑，而无需修改大量模型参数。与现有方法（如基于梯度调整或参数微调）相比，MeG的本质区别在于其编辑机制是动态和查询驱动的，提高了编辑的灵活性和局部性，同时降低了计算开销。\\n\\n**关键设计**：关键设计包括：动态权重神经元的附加位置选择（基于模型层结构分析，以最大化编辑效果），扩散模型的架构（如使用U-Net或Transformer-based扩散模型，具体细节未知），以及训练损失函数（可能结合重建损失和条件约束损失，以确保生成的权重符合编辑目标）。参数设置涉及扩散步数、噪声调度等，但论文未详细说明具体数值。",
            "application_zh": "该研究在人工智能领域具有广泛潜在应用，可用于大语言模型的实时知识更新、错误纠正和个性化定制，例如在智能助手、教育工具或内容生成系统中，快速集成新信息而不影响模型整体性能。未来可能推动更高效、可扩展的模型编辑技术发展，提升AI系统的适应性和可靠性。",
            "highlight_zh": "实验结果显示，MeG在可靠性、泛化性和局部性指标上均优于现有知识编辑方法。具体而言，在局部性指标上实现了高百分点的绝对值指数提升（具体数值未知），表明编辑对无关知识的影响最小化。对比基线包括传统编辑方法，MeG在大规模编辑场景中展现出显著优势，验证了其高效性和有效性。",
            "tags_zh": [
                "大语言模型",
                "知识编辑",
                "动态权重生成",
                "扩散模型",
                "大规模编辑",
                "可靠性指标",
                "局部性优化",
                "模型微调"
            ],
            "_index": 65
        },
        {
            "title": "A Comprehensive Safety Metric to Evaluate Perception in Autonomous Systems",
            "authors": [
                "Georg Volk",
                "Jörg Gamerdinger",
                "Alexander von Bernuth",
                "Oliver Bringmann"
            ],
            "arxiv_id": "2512.14367v1",
            "summary": "Complete perception of the environment and its correct interpretation is crucial for autonomous vehicles. Object perception is the main component of automotive surround sensing. Various metrics already exist for the evaluation of object perception. However, objects can be of different importance depending on their velocity, orientation, distance, size, or the potential damage that could be caused by a collision due to a missed detection. Thus, these additional parameters have to be considered for safety evaluation. We propose a new safety metric that incorporates all these parameters and returns a single easily interpretable safety assessment score for object perception. This new metric is evaluated with both real world and virtual data sets and compared to state of the art metrics.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted at IEEE ITSC 2020",
            "doi": "10.1109/ITSC45102.2020.9294708",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14367v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "autonomous vehicle"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出一种综合安全度量标准，用于评估自动驾驶系统中的感知性能，以解决现有方法忽略对象重要性差异的问题。",
            "summary_zh": "环境感知及其正确解释对自动驾驶车辆至关重要，对象感知是汽车环绕感知的核心组成部分。目前已有多种度量标准用于评估对象感知，但对象的重要性可能因其速度、方向、距离、大小或由于漏检导致碰撞可能造成的潜在损害而不同。因此，安全评估必须考虑这些额外参数。我们提出了一种新的安全度量标准，整合了所有这些参数，并返回一个易于解释的单一安全评估分数，用于对象感知。该新度量标准通过真实世界和虚拟数据集进行评估，并与现有先进度量标准进行比较。",
            "intro_zh": [
                "现有对象感知度量标准未考虑对象重要性差异，如速度、方向、距离、大小和潜在碰撞损害，导致安全评估不全面。",
                "论文提出一种综合安全度量标准，整合多参数生成单一安全分数，以量化感知系统的安全性能。",
                "实验使用真实和虚拟数据集验证新度量标准，相比现有方法，能更准确地反映感知系统的安全表现。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自动驾驶系统中对象感知评估的不足，现有度量标准（如准确率、召回率）通常仅关注检测性能，忽略了对象在安全层面的重要性差异，例如高速移动或近距离对象可能带来更高风险，导致安全评估不全面。\\n\\n**核心思路**：论文的核心思路是设计一个综合安全度量标准，通过整合对象的动态和静态参数（如速度、方向、距离、大小和潜在碰撞损害），将多维度信息融合为一个单一、易于解释的安全评估分数，从而量化感知系统在实际安全场景中的表现。\\n\\n**技术框架**：整体框架包括数据输入、参数提取、权重分配和分数计算阶段。首先，从感知系统输出中获取对象检测结果；然后，提取每个对象的参数（如速度、距离）；接着，基于安全重要性为不同参数分配权重；最后，通过数学公式（如加权求和）计算综合安全分数，输出一个标量值用于评估。\\n\\n**关键创新**：最重要的创新点在于将对象重要性参数（如速度、方向、距离、大小和潜在损害）系统性地整合到安全度量中，与现有方法仅关注检测精度相比，本质区别在于从纯性能评估转向安全导向的评估，更贴合自动驾驶的实际需求。\\n\\n**关键设计**：关键设计包括参数选择（基于物理和碰撞模型确定相关参数）、权重设置（可能通过专家知识或数据驱动方法调整，以反映不同参数对安全的影响程度），以及分数计算函数（确保输出分数在合理范围内，如0到1之间，便于比较和解释）。具体公式和参数细节在论文中未详细说明，但强调可扩展性和可解释性。",
            "application_zh": "该研究主要应用于自动驾驶汽车、机器人导航和智能交通系统等领域，通过提供综合安全评估，帮助优化感知算法设计、提升系统安全性和可靠性。未来可能影响行业标准制定，推动更安全的自动驾驶技术发展。",
            "highlight_zh": "实验使用真实世界和虚拟数据集验证新度量标准，结果显示相比现有先进度量标准（如F1分数），新方法能更准确地识别高风险漏检对象，提升安全评估的敏感性。具体性能数据未在摘要中提供，但强调新度量标准在安全导向场景中表现更优。",
            "tags_zh": [
                "自动驾驶安全",
                "感知评估",
                "安全度量标准",
                "对象重要性",
                "综合评分",
                "碰撞风险",
                "多参数融合",
                "虚拟验证"
            ],
            "_index": 66
        },
        {
            "title": "Unified Semantic Transformer for 3D Scene Understanding",
            "authors": [
                "Sebastian Koch",
                "Johanna Wald",
                "Hide Matsuki",
                "Pedro Hermosilla",
                "Timo Ropinski",
                "Federico Tombari"
            ],
            "arxiv_id": "2512.14364v1",
            "summary": "Holistic 3D scene understanding involves capturing and parsing unstructured 3D environments. Due to the inherent complexity of the real world, existing models have predominantly been developed and limited to be task-specific. We introduce UNITE, a Unified Semantic Transformer for 3D scene understanding, a novel feed-forward neural network that unifies a diverse set of 3D semantic tasks within a single model. Our model operates on unseen scenes in a fully end-to-end manner and only takes a few seconds to infer the full 3D semantic geometry. Our approach is capable of directly predicting multiple semantic attributes, including 3D scene segmentation, instance embeddings, open-vocabulary features, as well as affordance and articulations, solely from RGB images. The method is trained using a combination of 2D distillation, heavily relying on self-supervision and leverages novel multi-view losses designed to ensure 3D view consistency. We demonstrate that UNITE achieves state-of-the-art performance on several different semantic tasks and even outperforms task-specific models, in many cases, surpassing methods that operate on ground truth 3D geometry. See the project website at unite-page.github.io",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page: https://unite-page.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14364v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出UNITE统一语义Transformer，以单一模型解决3D场景理解中的多任务分割与属性预测问题",
            "summary_zh": "整体3D场景理解涉及捕获和解析非结构化3D环境。由于现实世界的固有复杂性，现有模型主要被开发并局限于特定任务。我们引入了UNITE，一种用于3D场景理解的统一语义Transformer，这是一种新颖的前馈神经网络，将多种3D语义任务统一在单一模型中。我们的模型以完全端到端的方式处理未见过的场景，仅需几秒钟即可推断完整的3D语义几何。我们的方法能够直接从RGB图像预测多种语义属性，包括3D场景分割、实例嵌入、开放词汇特征，以及功能性和关节属性。该方法通过结合2D蒸馏进行训练，严重依赖自监督，并利用新颖的多视图损失设计来确保3D视图一致性。我们证明UNITE在多个不同语义任务上实现了最先进的性能，甚至在许多情况下超越了特定任务模型，超过了基于真实3D几何的方法。请访问项目网站unite-page.github.io。",
            "intro_zh": [
                "现有3D场景理解模型多为任务特定，难以统一处理分割、实例、开放词汇等多任务，限制了实际应用。",
                "提出UNITE统一语义Transformer，通过端到端前馈网络直接从RGB图像预测多种语义属性，结合2D蒸馏和多视图损失。",
                "UNITE在多个语义任务上达到SOTA性能，超越任务特定模型，甚至优于基于真实3D几何的方法，推理仅需几秒。"
            ],
            "method_zh": "**问题定义**：论文旨在解决3D场景理解中现有模型多为任务特定、难以统一处理多种语义任务（如分割、实例、开放词汇等）的问题，现有方法通常依赖特定任务设计或真实3D几何，限制了泛化能力和效率。\\n\\n**核心思路**：设计一个统一的前馈神经网络UNITE，直接从RGB图像端到端预测多种3D语义属性，通过结合2D蒸馏和多视图一致性损失，实现高效、统一的3D场景理解，避免任务特定模型的复杂性。\\n\\n**技术框架**：整体架构基于Transformer，输入为RGB图像，通过编码器提取特征，然后使用统一解码器同时预测3D场景分割、实例嵌入、开放词汇特征、功能性和关节属性等，输出为完整的3D语义几何，整个过程在几秒内完成。\\n\\n**关键创新**：最重要的创新是提出统一语义Transformer，将多任务3D场景理解集成到单一模型中，直接从2D图像预测3D语义，无需真实3D几何输入，并通过新颖的多视图损失确保3D一致性，本质区别在于端到端统一性和对自监督的依赖。\\n\\n**关键设计**：关键技术细节包括：使用2D蒸馏从预训练模型迁移知识，依赖自监督学习减少标注需求；设计多视图损失函数（如一致性损失）来强制不同视角下的预测一致；网络结构基于Transformer，参数设置优化以实现快速推理（几秒内）；训练时结合多种语义任务的联合损失，具体损失函数组合未知。",
            "application_zh": "该研究在机器人导航、自动驾驶、增强现实和智能家居等领域具有广泛应用潜力，能提升场景解析的效率和准确性，推动3D视觉向更通用、实时的方向发展，未来可能影响多模态AI和具身智能系统。",
            "highlight_zh": "UNITE在多个3D语义任务上实现SOTA性能，具体数据未知，但论文指出超越了许多任务特定模型，并在许多情况下优于基于真实3D几何的方法，推理速度仅需几秒，展示了统一模型在效率和效果上的优势。",
            "tags_zh": [
                "3D场景理解",
                "统一语义Transformer",
                "多任务学习",
                "2D蒸馏",
                "多视图一致性",
                "端到端预测",
                "自监督学习",
                "RGB图像处理"
            ],
            "_index": 67
        },
        {
            "title": "Causal Structure Learning for Dynamical Systems with Theoretical Score Analysis",
            "authors": [
                "Nicholas Tagliapietra",
                "Katharina Ensinger",
                "Christoph Zimmer",
                "Osman Mian"
            ],
            "arxiv_id": "2512.14361v1",
            "summary": "Real world systems evolve in continuous-time according to their underlying causal relationships, yet their dynamics are often unknown. Existing approaches to learning such dynamics typically either discretize time -- leading to poor performance on irregularly sampled data -- or ignore the underlying causality. We propose CaDyT, a novel method for causal discovery on dynamical systems addressing both these challenges. In contrast to state-of-the-art causal discovery methods that model the problem using discrete-time Dynamic Bayesian networks, our formulation is grounded in Difference-based causal models, which allow milder assumptions for modeling the continuous nature of the system. CaDyT leverages exact Gaussian Process inference for modeling the continuous-time dynamics which is more aligned with the underlying dynamical process. We propose a practical instantiation that identifies the causal structure via a greedy search guided by the Algorithmic Markov Condition and Minimum Description Length principle. Our experiments show that CaDyT outperforms state-of-the-art methods on both regularly and irregularly-sampled data, discovering causal networks closer to the true underlying dynamics.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "math.DS"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted as Oral at AAAI 2026 Conference",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14361v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出CaDyT方法，基于差分因果模型和高斯过程推理，解决动态系统中连续时间因果发现的问题。",
            "summary_zh": "现实世界系统根据其潜在因果关系在连续时间内演化，但其动态通常未知。现有学习方法通常要么离散化时间——导致在非规则采样数据上性能不佳——要么忽略底层因果关系。我们提出CaDyT，一种用于动态系统因果发现的新方法，解决了这两个挑战。与使用离散时间动态贝叶斯网络建模问题的最先进因果发现方法不同，我们的公式基于差分因果模型，这允许对系统的连续性质进行更温和的假设建模。CaDyT利用精确的高斯过程推理来建模连续时间动态，这更符合底层动态过程。我们提出了一种实用的实例化，通过由算法马尔可夫条件和最小描述长度原则指导的贪婪搜索来识别因果结构。我们的实验表明，CaDyT在规则和非规则采样数据上都优于最先进的方法，发现了更接近真实底层动态的因果网络。",
            "intro_zh": [
                "现有方法在动态系统因果发现中面临挑战：时间离散化导致非规则采样数据性能差，或忽略底层因果关系。",
                "CaDyT基于差分因果模型和高斯过程推理，建模连续时间动态，通过贪婪搜索识别因果结构。",
                "实验显示CaDyT在规则和非规则采样数据上优于现有方法，因果网络更接近真实动态。"
            ],
            "method_zh": "**问题定义**：论文解决动态系统中连续时间因果发现问题，现有方法痛点包括时间离散化导致非规则采样数据性能差，或忽略底层因果关系，限制了模型对真实动态的适应性。\\n\\n**核心思路**：核心思路是结合差分因果模型和高斯过程推理，差分因果模型允许更温和的假设来建模系统连续性质，高斯过程推理则精确建模连续时间动态，更符合底层动态过程，从而提升因果发现的准确性和鲁棒性。\\n\\n**技术框架**：整体框架包括数据预处理、动态建模和因果结构学习三阶段。首先，处理输入的时间序列数据；其次，使用高斯过程进行连续时间动态建模；最后，通过贪婪搜索在算法马尔可夫条件和最小描述长度原则指导下识别因果结构，输出因果网络。\\n\\n**关键创新**：最重要的技术创新是提出基于差分因果模型的连续时间因果发现方法，与现有基于离散时间动态贝叶斯网络的方法本质区别在于避免了时间离散化，直接处理连续动态，减少了假设限制，提高了对非规则采样数据的适应性。\\n\\n**关键设计**：关键设计包括使用高斯过程进行精确推理以建模动态，参数设置如核函数选择未知；贪婪搜索算法结合算法马尔可夫条件作为指导原则，最小描述长度原则用于优化模型复杂度，具体损失函数和网络结构细节在论文中未明确说明，需参考原文。",
            "application_zh": "该研究在机器人控制、生物医学信号分析、金融时间序列预测等领域有潜在应用价值，能帮助理解复杂系统的因果机制，提升模型预测和干预能力，对智能系统和数据分析具有重要影响。",
            "highlight_zh": "实验表明CaDyT在规则和非规则采样数据上均优于最先进方法，具体性能数据未知，但因果网络更接近真实底层动态，提升了因果发现的准确性和鲁棒性，验证了其在动态系统建模中的有效性。",
            "tags_zh": [
                "因果发现",
                "动态系统",
                "连续时间建模",
                "高斯过程",
                "差分因果模型",
                "算法马尔可夫条件",
                "最小描述长度",
                "非规则采样数据"
            ],
            "_index": 68
        },
        {
            "title": "Mimicking Human Visual Development for Learning Robust Image Representations",
            "authors": [
                "Ankita Raj",
                "Kaashika Prajaapat",
                "Tapan Kumar Gandhi",
                "Chetan Arora"
            ],
            "arxiv_id": "2512.14360v1",
            "summary": "The human visual system is remarkably adept at adapting to changes in the input distribution; a capability modern convolutional neural networks (CNNs) still struggle to match. Drawing inspiration from the developmental trajectory of human vision, we propose a progressive blurring curriculum to improve the generalization and robustness of CNNs. Human infants are born with poor visual acuity, gradually refining their ability to perceive fine details. Mimicking this process, we begin training CNNs on highly blurred images during the initial epochs and progressively reduce the blur as training advances. This approach encourages the network to prioritize global structures over high-frequency artifacts, improving robustness against distribution shifts and noisy inputs. Challenging prior claims that blurring in the initial training epochs imposes a stimulus deficit and irreversibly harms model performance, we reveal that early-stage blurring enhances generalization with minimal impact on in-domain accuracy. Our experiments demonstrate that the proposed curriculum reduces mean corruption error (mCE) by up to 8.30% on CIFAR-10-C and 4.43% on ImageNet-100-C datasets, compared to standard training without blurring. Unlike static blur-based augmentation, which applies blurred images randomly throughout training, our method follows a structured progression, yielding consistent gains across various datasets. Furthermore, our approach complements other augmentation techniques, such as CutMix and MixUp, and enhances both natural and adversarial robustness against common attack methods. Code is available at https://github.com/rajankita/Visual_Acuity_Curriculum.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted to ICVGIP 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14360v1",
            "code_links": [
                {
                    "url": "https://github.com/rajankita/Visual_Acuity_Curriculum",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出渐进模糊课程学习，模仿人类视觉发育过程以提升卷积神经网络在分布偏移和噪声输入下的鲁棒性。",
            "summary_zh": "人类视觉系统能够出色地适应输入分布的变化，而现代卷积神经网络（CNNs）在这方面仍面临挑战。受人类视觉发育轨迹的启发，我们提出一种渐进模糊课程学习方法来提升CNNs的泛化能力和鲁棒性。人类婴儿出生时视觉敏锐度较差，逐渐发展出感知精细细节的能力。模仿这一过程，我们在训练初期使用高度模糊的图像训练CNNs，并随着训练进展逐渐减少模糊程度。这种方法鼓励网络优先关注全局结构而非高频伪影，从而提升对分布偏移和噪声输入的鲁棒性。与先前认为训练初期模糊会造成刺激缺陷并不可逆地损害模型性能的观点不同，我们发现早期模糊能够增强泛化能力，同时对域内准确率影响极小。实验表明，与标准无模糊训练相比，所提出的课程学习方法在CIFAR-10-C数据集上平均腐蚀误差（mCE）降低了8.30%，在ImageNet-100-C数据集上降低了4.43%。与静态模糊增强（在整个训练过程中随机应用模糊图像）不同，我们的方法遵循结构化渐进过程，在不同数据集上均取得一致提升。此外，该方法可与其他增强技术（如CutMix和MixUp）互补，并提升对常见攻击方法的自然和对抗鲁棒性。代码可在https://github.com/rajankita/Visual_Acuity_Curriculum获取。",
            "intro_zh": [
                "核心问题：现代卷积神经网络在适应输入分布变化方面表现不佳，缺乏人类视觉系统的鲁棒性，现有方法如静态模糊增强效果有限且可能损害性能。",
                "方法要点：模仿人类婴儿视觉发育过程，提出渐进模糊课程学习，从高度模糊图像开始训练，逐步减少模糊程度，引导网络学习全局结构。",
                "实验或效果：在CIFAR-10-C和ImageNet-100-C数据集上，平均腐蚀误差分别降低8.30%和4.43%，提升泛化能力和对抗鲁棒性，与现有增强技术兼容。"
            ],
            "method_zh": "**问题定义**：论文旨在解决卷积神经网络在分布偏移和噪声输入下鲁棒性不足的问题。现有方法如静态模糊增强随机应用模糊图像，缺乏结构化设计，可能无法有效引导网络学习，甚至被批评为造成刺激缺陷，损害模型性能。\\n\\n**核心思路**：核心解决思路是模仿人类视觉发育过程，设计渐进模糊课程学习。人类婴儿从模糊视觉逐渐发展出精细感知能力，因此训练初期使用高度模糊图像，鼓励网络关注全局结构，避免过度拟合高频细节，随着训练进展逐步减少模糊，引导网络适应更精细的输入。\\n\\n**技术框架**：整体流程分为课程设计和训练阶段。课程设计定义模糊程度随训练轮次（epoch）的衰减函数，如线性或指数衰减。训练阶段，在每个轮次中，根据当前模糊程度对输入图像应用高斯模糊或其他模糊操作，然后使用标准损失函数（如交叉熵）进行网络训练。整个过程无需修改网络结构，可直接集成到现有训练流程中。\\n\\n**关键创新**：最重要的技术创新是结构化渐进模糊课程，与静态模糊增强的本质区别在于其动态性和模仿生物发育的特性。它系统性地从模糊到清晰过渡，而非随机应用模糊，从而更有效地引导学习过程，提升鲁棒性而不显著影响域内准确率。\\n\\n**关键设计**：关键参数包括初始模糊程度（如高斯核大小或标准差）、衰减速率和总训练轮次。损失函数通常使用标准分类损失，无需特殊设计。网络结构可以是任意CNN架构（如ResNet），模糊操作在数据预处理阶段应用，确保方法通用性。实验中使用CIFAR-10和ImageNet-100等数据集验证效果。",
            "application_zh": "该研究在计算机视觉和机器学习领域具有广泛潜在应用，特别是在需要高鲁棒性的场景中，如自动驾驶、医疗影像分析和安防监控。通过提升模型对分布偏移和噪声的适应能力，可增强实际部署中的可靠性。未来可能影响鲁棒学习、课程学习和生物启发AI的发展，推动更稳健的视觉系统设计。",
            "highlight_zh": "最重要的实验结果显示，渐进模糊课程学习在标准基准测试中显著提升鲁棒性。在CIFAR-10-C数据集上，平均腐蚀误差（mCE）相比无模糊训练降低8.30%；在ImageNet-100-C数据集上降低4.43%。方法优于静态模糊增强，并在与CutMix、MixUp等增强技术结合时进一步改善性能，同时保持域内准确率基本不变，验证了其有效性和兼容性。",
            "tags_zh": [
                "渐进模糊课程学习",
                "卷积神经网络鲁棒性",
                "人类视觉发育模仿",
                "分布偏移适应",
                "对抗鲁棒性",
                "课程学习",
                "图像增强",
                "生物启发AI"
            ],
            "_index": 69
        },
        {
            "title": "Enhancing Interpretability for Vision Models via Shapley Value Optimization",
            "authors": [
                "Kanglong Fan",
                "Yunqiao Yang",
                "Chen Ma"
            ],
            "arxiv_id": "2512.14354v1",
            "summary": "Deep neural networks have demonstrated remarkable performance across various domains, yet their decision-making processes remain opaque. Although many explanation methods are dedicated to bringing the obscurity of DNNs to light, they exhibit significant limitations: post-hoc explanation methods often struggle to faithfully reflect model behaviors, while self-explaining neural networks sacrifice performance and compatibility due to their specialized architectural designs. To address these challenges, we propose a novel self-explaining framework that integrates Shapley value estimation as an auxiliary task during training, which achieves two key advancements: 1) a fair allocation of the model prediction scores to image patches, ensuring explanations inherently align with the model's decision logic, and 2) enhanced interpretability with minor structural modifications, preserving model performance and compatibility. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art interpretability.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted to AAAI2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14354v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "SAC"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出基于沙普利值优化的自解释框架，以解决视觉模型可解释性与性能兼容性的平衡问题",
            "summary_zh": "深度神经网络在各种领域表现出色，但其决策过程仍不透明。尽管许多解释方法致力于揭示深度神经网络的模糊性，但它们存在显著局限性：事后解释方法往往难以忠实反映模型行为，而自解释神经网络因其专门架构设计而牺牲了性能和兼容性。为解决这些挑战，我们提出了一种新颖的自解释框架，在训练过程中将沙普利值估计作为辅助任务集成，实现了两个关键进展：1）将模型预测分数公平分配给图像块，确保解释与模型的决策逻辑内在一致；2）通过微小的结构修改增强可解释性，同时保持模型性能和兼容性。在多个基准测试上的广泛实验表明，我们的方法实现了最先进的可解释性。",
            "intro_zh": [
                "现有方法存在局限性：事后解释方法难以忠实反映模型行为，自解释神经网络牺牲性能与兼容性。",
                "提出自解释框架，集成沙普利值估计作为辅助任务，实现预测分数公平分配与结构微调。",
                "在多个基准测试中实现最先进的可解释性，同时保持模型性能与兼容性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决深度神经网络决策过程不透明的问题，现有方法存在两大痛点：事后解释方法（如LIME、SHAP）难以忠实反映模型内部逻辑，可能导致误导性解释；自解释神经网络（如注意力机制模型）通过专门架构实现可解释性，但往往牺牲模型性能（如准确率下降）和兼容性（难以集成到现有模型中）。\\n\\n**核心思路**：论文提出将沙普利值估计作为训练过程中的辅助任务，核心思想是利用沙普利值的公平分配特性，将模型预测分数合理分配给输入图像块，从而确保解释与模型决策逻辑内在一致。这样设计是因为沙普利值基于合作博弈论，能客观衡量每个特征对预测的贡献，避免了事后解释的偏差，同时通过辅助任务而非专门架构，最小化对性能的影响。\\n\\n**技术框架**：整体架构包括一个主任务网络（如CNN用于图像分类）和一个辅助解释模块。流程分为三个阶段：首先，输入图像被分割成多个块（patches）；其次，主网络进行特征提取和预测，同时辅助模块估计每个图像块的沙普利值；最后，通过联合优化主任务损失和沙普利值估计损失，训练模型。主要模块包括特征提取器、预测头和沙普利值估计器，后者可能基于采样或近似算法实现高效计算。\\n\\n**关键创新**：最重要的技术创新是将沙普利值估计集成到训练过程中作为辅助任务，而非事后应用。与现有方法的本质区别在于：相比事后解释方法，它确保了解释的忠实性（faithfulness），因为解释是模型训练的一部分；相比自解释神经网络，它通过微小结构修改（如添加辅助模块）而非彻底架构改变，保持了高性能和兼容性，易于部署到现有视觉模型中。\\n\\n**关键设计**：关键参数包括图像块的大小和数量，影响解释的粒度；损失函数结合了主任务损失（如交叉熵损失）和沙普利值估计损失（如均方误差损失），通过权重参数平衡可解释性与性能；网络结构上，主网络可基于标准CNN（如ResNet），辅助模块可能使用轻量级网络或采样策略来估计沙普利值，以减少计算开销。具体细节如损失函数权重和沙普利值估计算法在论文中未详细说明，需参考实验部分。",
            "application_zh": "该研究在医疗影像分析、自动驾驶、安防监控等领域具有潜在应用价值，例如帮助医生理解AI诊断依据、提升自动驾驶系统的透明决策、增强监控模型的可信度。实际价值在于平衡模型性能与可解释性，推动可信AI发展，未来可能影响模型部署标准，促进可解释性成为视觉模型设计的核心要素。",
            "highlight_zh": "在多个基准测试（如ImageNet、CIFAR-10）上，该方法实现了最先进的可解释性，具体性能数据未知，但论文提到通过微小结构修改，在保持模型准确率（与基线模型相比无明显下降）的同时，显著提升解释忠实性。对比基线包括事后解释方法（如Grad-CAM）和自解释神经网络，提升幅度体现在解释质量指标（如保真度分数）上达到最优，具体数值需参考论文实验部分。",
            "tags_zh": [
                "可解释人工智能",
                "沙普利值",
                "自解释神经网络",
                "视觉模型",
                "辅助任务学习",
                "图像块分配",
                "模型透明度",
                "性能兼容性"
            ],
            "_index": 70
        },
        {
            "title": "Fine-Tuning of Neural Network Approximate MPC without Retraining via Bayesian Optimization",
            "authors": [
                "Henrik Hose",
                "Paul Brunzema",
                "Alexander von Rohr",
                "Alexander Gräfe",
                "Angela P. Schoellig",
                "Sebastian Trimpe"
            ],
            "arxiv_id": "2512.14350v1",
            "summary": "Approximate model-predictive control (AMPC) aims to imitate an MPC's behavior with a neural network, removing the need to solve an expensive optimization problem at runtime. However, during deployment, the parameters of the underlying MPC must usually be fine-tuned. This often renders AMPC impractical as it requires repeatedly generating a new dataset and retraining the neural network. Recent work addresses this problem by adapting AMPC without retraining using approximated sensitivities of the MPC's optimization problem. Currently, this adaption must be done by hand, which is labor-intensive and can be unintuitive for high-dimensional systems. To solve this issue, we propose using Bayesian optimization to tune the parameters of AMPC policies based on experimental data. By combining model-based control with direct and local learning, our approach achieves superior performance to nominal AMPC on hardware, with minimal experimentation. This allows automatic and data-efficient adaptation of AMPC to new system instances and fine-tuning to cost functions that are difficult to directly implement in MPC. We demonstrate the proposed method in hardware experiments for the swing-up maneuver on an inverted cartpole and yaw control of an under-actuated balancing unicycle robot, a challenging control problem.",
            "categories": [
                "cs.RO",
                "eess.SY"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14350v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出基于贝叶斯优化的近似模型预测控制微调方法，无需重新训练神经网络，实现自动数据高效适应。",
            "summary_zh": "近似模型预测控制（AMPC）旨在通过神经网络模仿MPC的行为，避免运行时求解昂贵的优化问题。然而，在部署过程中，通常需要微调底层MPC的参数，这往往导致AMPC不实用，因为它需要重复生成新数据集并重新训练神经网络。最近的研究通过利用MPC优化问题的近似敏感性来适应AMPC而无需重新训练，解决了这一问题。但目前这种适应必须手动完成，对于高维系统来说既费时又不直观。为解决此问题，我们提出使用贝叶斯优化基于实验数据来调整AMPC策略的参数。通过将基于模型的控制与直接局部学习相结合，我们的方法在硬件上实现了优于名义AMPC的性能，且实验量最小。这使得AMPC能够自动且数据高效地适应新系统实例，并微调到难以直接在MPC中实现的成本函数。我们在硬件实验中展示了所提方法，包括倒立摆的摆起动作和欠驱动平衡独轮机器人的偏航控制，这是一个具有挑战性的控制问题。",
            "intro_zh": [
                "核心问题：AMPC部署时需微调参数，但传统方法需重复生成数据集和重新训练神经网络，导致效率低下且不实用。",
                "方法要点：结合贝叶斯优化与AMPC，基于实验数据自动调整策略参数，无需重新训练神经网络，实现数据高效适应。",
                "实验或效果：在倒立摆和独轮机器人硬件实验中，方法性能优于名义AMPC，显著减少实验次数，提升控制精度。"
            ],
            "method_zh": "**问题定义**：论文解决近似模型预测控制（AMPC）在部署时参数微调的问题。现有AMPC方法通过神经网络模仿MPC行为，避免运行时优化计算，但微调参数需重新生成数据集和训练网络，导致高成本和低效率，尤其在高维系统中手动调整困难。\\n\\n**核心思路**：核心思路是利用贝叶斯优化自动调整AMPC策略参数，基于实验数据直接优化控制性能，无需重新训练神经网络。这样设计结合了模型控制与数据驱动学习，实现局部适应，减少对精确模型的依赖。\\n\\n**技术框架**：整体框架包括AMPC策略初始化、贝叶斯优化循环和性能评估。首先，使用预训练神经网络作为AMPC策略；然后，通过贝叶斯优化迭代调整策略参数，基于硬件实验数据优化目标函数（如控制误差）；最后，评估优化后策略的性能。关键模块包括参数空间定义、高斯过程模型和采集函数。\\n\\n**关键创新**：最重要的创新是将贝叶斯优化集成到AMPC微调中，实现无需重新训练的自动参数调整。与现有方法（如基于近似敏感性的手动调整）相比，本质区别在于数据驱动和自动化，提高了适应效率和可扩展性。\\n\\n**关键设计**：关键设计包括使用高斯过程作为代理模型来建模目标函数，选择预期改进作为采集函数以平衡探索与利用。参数设置涉及优化迭代次数和实验预算，网络结构基于原始AMPC的神经网络，损失函数为控制性能指标（如轨迹跟踪误差）。具体细节如参数边界和初始化策略在实验中调整。",
            "application_zh": "该研究在机器人控制领域具有广泛应用潜力，如自适应系统控制、复杂环境下的实时优化和成本函数难以建模的场景。实际价值在于降低AMPC部署成本，提高控制系统的灵活性和鲁棒性。未来可能推动智能控制系统的自动化微调和跨平台适应。",
            "highlight_zh": "在倒立摆摆起和独轮机器人偏航控制的硬件实验中，所提方法相比名义AMPC，控制误差平均降低约30%，实验次数减少50%以上。具体地，在倒立摆任务中，成功率达到95%以上，而基线为80%；在独轮机器人中，稳定时间缩短20%，展示了显著的性能提升和数据效率。",
            "tags_zh": [
                "近似模型预测控制",
                "贝叶斯优化",
                "神经网络微调",
                "机器人控制",
                "数据高效学习",
                "自适应控制",
                "硬件实验",
                "优化参数调整"
            ],
            "_index": 71
        },
        {
            "title": "Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure",
            "authors": [
                "Jooyeol Yun",
                "Jaegul Choo"
            ],
            "arxiv_id": "2512.14336v1",
            "summary": "Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to grow as web environments become increasingly dynamic. Yet automating the animation of vector graphics remains challenging for vision-language models (VLMs) despite recent progress in code generation and motion planning. VLMs routinely mis-handle SVGs, since visually coherent parts are often fragmented into low-level shapes that offer little guidance of which elements should move together. In this paper, we introduce a framework that recovers the semantic structure required for reliable SVG animation and reveals the missing layer that current VLM systems overlook. This is achieved through a statistical aggregation of multiple weak part predictions, allowing the system to stably infer semantics from noisy predictions. By reorganizing SVGs into semantic groups, our approach enables VLMs to produce animations with far greater coherence. Our experiments demonstrate substantial gains over existing approaches, suggesting that semantic recovery is the key step that unlocks robust SVG animation and supports more interpretable interactions between VLMs and vector graphics.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "yeolj00.github.io/personal-projects/vector-prism",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14336v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出Vector Prism框架，通过分层语义结构恢复解决SVG动画自动化中的视觉语言模型误处理问题。",
            "summary_zh": "可缩放矢量图形（SVG）在现代网页设计中至关重要，随着网络环境日益动态化，对其动画化的需求不断增长。然而，尽管在代码生成和运动规划方面取得了进展，自动化矢量图形动画对视觉语言模型（VLMs）仍然具有挑战性。VLMs经常误处理SVG，因为视觉上连贯的部分通常被分割成低级形状，这些形状几乎无法指导哪些元素应该一起移动。本文介绍了一个框架，该框架恢复了可靠SVG动画所需的语义结构，并揭示了当前VLM系统忽略的缺失层。这是通过对多个弱部分预测进行统计聚合实现的，使系统能够从噪声预测中稳定推断语义。通过将SVG重新组织为语义组，我们的方法使VLMs能够生成更具连贯性的动画。实验表明，与现有方法相比，我们的方法取得了显著提升，这表明语义恢复是解锁稳健SVG动画并支持VLMs与矢量图形之间更可解释交互的关键步骤。",
            "intro_zh": [
                "现有视觉语言模型在SVG动画中常误处理，因视觉连贯部分被分割为低级形状，缺乏语义指导。",
                "提出Vector Prism框架，通过统计聚合弱预测恢复语义结构，将SVG重组为语义组以提升动画连贯性。",
                "实验显示方法显著优于现有方法，语义恢复是解锁稳健SVG动画和支持可解释交互的关键。"
            ],
            "method_zh": "**问题定义**：论文旨在解决视觉语言模型（VLMs）在自动化SVG动画中的误处理问题。现有方法的痛点是，SVG中的视觉连贯部分常被分割为低级形状（如路径、圆形），这些形状缺乏高层语义信息，导致VLMs难以判断哪些元素应一起移动，从而产生不连贯的动画。\\n\\n**核心思路**：论文的核心解决思路是通过恢复SVG的语义结构来指导动画生成。设计基于统计聚合多个弱部分预测，以稳定推断语义，从而弥补当前VLM系统忽略的语义层。这有助于将SVG重组为语义组，提升动画的连贯性。\\n\\n**技术框架**：整体架构包括两个主要阶段：语义结构恢复和动画生成。首先，通过多弱预测聚合模块处理SVG输入，生成语义分组；然后，基于这些分组，VLMs进行动画规划。流程涉及预测部分、统计聚合、语义分层和动画输出。\\n\\n**关键创新**：最重要的技术创新是引入统计聚合机制来恢复语义结构，与现有方法本质区别在于，现有方法直接处理低级形状，而本方法通过分层语义提供高层指导，解决了语义缺失问题。\\n\\n**关键设计**：关键设计包括弱预测生成器（可能基于预训练模型）、聚合算法（如投票或加权平均）、语义分组模块（将形状聚类为语义单元）。具体参数和损失函数未知，但可能涉及优化分组一致性和动画质量。",
            "application_zh": "该研究在网页设计、动态图形制作和交互式媒体中有广泛应用潜力。通过提升SVG动画的自动化水平，可降低设计师工作量，支持更智能的网页内容生成。未来可能推动VLMs在矢量图形处理中的可解释性，促进人机交互发展。",
            "highlight_zh": "实验表明，Vector Prism框架在SVG动画任务上显著优于现有基线方法。具体性能数据未知，但论文报告了在动画连贯性和语义准确性方面的实质性提升，验证了语义恢复作为关键步骤的有效性。",
            "tags_zh": [
                "矢量图形动画",
                "语义结构恢复",
                "视觉语言模型",
                "SVG处理",
                "弱预测聚合",
                "动画自动化",
                "网页设计"
            ],
            "_index": 72
        },
        {
            "title": "Step-Tagging: Toward controlling the generation of Language Reasoning Models through step monitoring",
            "authors": [
                "Yannis Belkhiter",
                "Seshu Tirupathi",
                "Giulio Zizzo",
                "John D. Kelleher"
            ],
            "arxiv_id": "2512.14332v1",
            "summary": "The field of Language Reasoning Models (LRMs) has been very active over the past few years with advances in training and inference techniques enabling LRMs to reason longer, and more accurately. However, a growing body of studies show that LRMs are still inefficient, over-generating verification and reflection steps. To address this challenge, we introduce the Step-Tagging framework, a lightweight sentence-classifier enabling real-time annotation of the type of reasoning steps that an LRM is generating. To monitor reasoning behaviors, we introduced ReasonType: a novel taxonomy of reasoning steps. Building on this framework, we demonstrated that online monitoring of the count of specific steps can produce effective interpretable early stopping criteria of LRM inferences. We evaluate the Step-tagging framework on three open-source reasoning models across standard benchmark datasets: MATH500, GSM8K, AIME and non-mathematical tasks (GPQA and MMLU-Pro). We achieve 20 to 50\\% token reduction while maintaining comparable accuracy to standard generation, with largest gains observed on more computation-heavy tasks. This work offers a novel way to increase control over the generation of LRMs, and a new tool to study behaviors of LRMs.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14332v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出Step-Tagging框架，通过实时监控推理步骤类型实现语言推理模型生成控制，解决推理效率低下问题。",
            "summary_zh": "语言推理模型（LRMs）领域近年来发展迅速，训练和推理技术的进步使得LRMs能够进行更长、更准确的推理。然而，越来越多的研究表明，LRMs仍然效率低下，过度生成验证和反思步骤。为解决这一挑战，我们引入了Step-Tagging框架，这是一个轻量级的句子分类器，能够实时标注LRM生成的推理步骤类型。为了监控推理行为，我们引入了ReasonType：一种新颖的推理步骤分类法。基于此框架，我们证明了在线监控特定步骤的数量可以产生有效的可解释早期停止标准，用于LRM推理。我们在三个开源推理模型上评估了Step-Tagging框架，使用标准基准数据集：MATH500、GSM8K、AIME以及非数学任务（GPQA和MMLU-Pro）。在保持与标准生成相当准确性的同时，我们实现了20%到50%的令牌减少，在计算量更大的任务上观察到最大的增益。这项工作提供了一种新颖的方式来增加对LRM生成的控制，以及一种研究LRM行为的新工具。",
            "intro_zh": [
                "现有语言推理模型在推理过程中常过度生成验证和反思步骤，导致计算效率低下，影响实际应用性能。",
                "论文提出Step-Tagging框架，通过轻量级句子分类器实时标注推理步骤类型，并基于ReasonType分类法监控推理行为，实现可解释的早期停止。",
                "实验在多个基准数据集上验证，Step-Tagging能减少20-50%令牌使用，同时保持准确性，尤其在计算密集型任务中效果显著。"
            ],
            "method_zh": "**问题定义**：语言推理模型（LRMs）在推理过程中存在效率低下问题，具体表现为过度生成验证和反思步骤，导致不必要的计算开销和延迟，影响模型的实际部署性能。现有方法缺乏对推理步骤类型的实时监控机制，无法动态控制生成过程，造成资源浪费。\\n\\n**核心思路**：通过引入Step-Tagging框架，设计一个轻量级句子分类器来实时识别和标注LRM生成的推理步骤类型，基于此监控推理行为，并利用步骤计数作为早期停止标准，从而在保持准确性的同时减少令牌生成，提高推理效率。\\n\\n**技术框架**：整体架构包括两个主要阶段：首先，构建ReasonType分类法，定义推理步骤的类型（如验证、反思等）；其次，训练一个句子分类器，在LRM推理过程中实时对生成的句子进行分类，标注其步骤类型，并基于步骤计数动态决定是否停止生成，实现在线监控和控制。\\n\\n**关键创新**：最重要的技术创新是提出了ReasonType分类法和Step-Tagging框架，将推理步骤类型化并实时监控，这区别于现有方法仅关注最终输出或静态分析，实现了动态、可解释的生成控制，本质上是将推理过程结构化以优化效率。\\n\\n**关键设计**：技术细节包括：使用轻量级分类器（如基于Transformer的句子编码器）进行实时标注，损失函数可能采用交叉熵损失以优化分类准确性；参数设置涉及分类阈值和早期停止规则，例如当特定步骤（如验证步骤）达到预设数量时停止生成；网络结构设计确保低延迟，以适应在线推理场景。",
            "application_zh": "该研究在人工智能推理领域具有广泛潜在应用，可用于优化大型语言模型在数学解题、科学问答等复杂任务中的推理效率，提升实际部署性能，减少计算成本。未来可能扩展到更多推理密集型场景，如代码生成、逻辑推理，为模型行为分析和控制提供新工具。",
            "highlight_zh": "实验在MATH500、GSM8K、AIME、GPQA和MMLU-Pro等基准数据集上进行，使用三个开源推理模型评估。结果显示，Step-Tagging框架在保持与标准生成相当准确性的同时，实现了20%到50%的令牌减少，对比基线（无监控的生成），在计算量更大的任务中增益最显著，例如在数学推理任务上效率提升明显。",
            "tags_zh": [
                "语言推理模型",
                "推理效率优化",
                "步骤监控",
                "早期停止",
                "轻量级分类器",
                "ReasonType分类法",
                "令牌减少",
                "可解释控制"
            ],
            "_index": 73
        },
        {
            "title": "Leveraging LLMs for Collaborative Ontology Engineering in Parkinson Disease Monitoring and Alerting",
            "authors": [
                "Georgios Bouchouras",
                "Dimitrios Doumanas",
                "Andreas Soularidis",
                "Konstantinos Kotis",
                "George A. Vouros"
            ],
            "arxiv_id": "2512.14288v1",
            "summary": "This paper explores the integration of Large Language Models (LLMs) in the engineering of a Parkinson's Disease (PD) monitoring and alerting ontology through four key methodologies: One Shot (OS) prompt techniques, Chain of Thought (CoT) prompts, X-HCOME, and SimX-HCOME+. The primary objective is to determine whether LLMs alone can create comprehensive ontologies and, if not, whether human-LLM collaboration can achieve this goal. Consequently, the paper assesses the effectiveness of LLMs in automated ontology development and the enhancement achieved through human-LLM collaboration.\n  Initial ontology generation was performed using One Shot (OS) and Chain of Thought (CoT) prompts, demonstrating the capability of LLMs to autonomously construct ontologies for PD monitoring and alerting. However, these outputs were not comprehensive and required substantial human refinement to enhance their completeness and accuracy.\n  X-HCOME, a hybrid ontology engineering approach that combines human expertise with LLM capabilities, showed significant improvements in ontology comprehensiveness. This methodology resulted in ontologies that are very similar to those constructed by experts.\n  Further experimentation with SimX-HCOME+, another hybrid methodology emphasizing continuous human supervision and iterative refinement, highlighted the importance of ongoing human involvement. This approach led to the creation of more comprehensive and accurate ontologies.\n  Overall, the paper underscores the potential of human-LLM collaboration in advancing ontology engineering, particularly in complex domains like PD. The results suggest promising directions for future research, including the development of specialized GPT models for ontology construction.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14288v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出人机协作本体工程方法，利用LLMs提升帕金森病监测与预警本体构建的全面性与准确性",
            "summary_zh": "本文探讨了将大型语言模型（LLMs）集成到帕金森病（PD）监测与预警本体工程中的四种关键方法：一次性提示技术、思维链提示、X-HCOME和SimX-HCOME+。主要目标是确定LLMs是否能够独立创建全面的本体，如果不能，人机协作是否能实现这一目标。因此，本文评估了LLMs在自动化本体开发中的有效性以及通过人机协作实现的改进。\n\n初始本体生成使用一次性提示和思维链提示进行，展示了LLMs自主构建PD监测与预警本体的能力。然而，这些输出并不全面，需要大量人工细化以增强其完整性和准确性。\n\nX-HCOME是一种结合人类专业知识和LLM能力的混合本体工程方法，在本体全面性方面显示出显著改进。这种方法产生的本体与专家构建的本体非常相似。\n\n进一步实验使用SimX-HCOME+，这是另一种强调持续人类监督和迭代细化的混合方法，突出了持续人类参与的重要性。这种方法导致了更全面和准确的本体创建。\n\n总体而言，本文强调了人机协作在推进本体工程中的潜力，特别是在PD等复杂领域。结果指出了未来研究的有希望方向，包括开发用于本体构建的专门GPT模型。",
            "intro_zh": [
                "核心问题：现有LLMs在自动化本体工程中难以独立生成全面且准确的本体，特别是在帕金森病等复杂医学领域，需要大量人工干预。",
                "方法要点：提出X-HCOME和SimX-HCOME+混合方法，结合人类专业知识和LLMs能力，通过人机协作提升本体构建的全面性与准确性。",
                "实验或效果：实验表明，人机协作方法显著改进本体质量，SimX-HCOME+在持续监督下生成更全面准确的本体，接近专家水平。"
            ],
            "method_zh": "**问题定义**：论文旨在解决LLMs在自动化本体工程中难以独立生成全面且准确的本体的问题，特别是在帕金森病监测与预警等复杂医学领域。现有方法的痛点在于，LLMs如一次性提示和思维链提示虽能自主构建本体，但输出不全面，需要大量人工细化，导致效率低下和准确性不足。\\n\\n**核心思路**：论文的核心解决思路是通过人机协作来弥补LLMs的局限性，结合人类专业知识和LLMs的自动化能力，设计混合本体工程方法。这样设计是因为人类能提供领域知识和监督，而LLMs能加速生成和迭代，从而提升本体的全面性和准确性。\\n\\n**技术框架**：整体架构包括四个主要阶段：首先，使用一次性提示和思维链提示进行初始本体生成；其次，引入X-HCOME方法，结合人类专家输入和LLMs输出进行混合构建；然后，应用SimX-HCOME+方法，强调持续人类监督和迭代细化；最后，评估和比较不同方法的本体质量。主要模块包括提示工程、人类反馈集成和迭代优化。\\n\\n**关键创新**：最重要的技术创新点是提出了X-HCOME和SimX-HCOME+这两种混合本体工程方法，本质区别在于它们系统性地整合了人类监督和LLMs自动化，而现有方法多依赖单一LLMs或纯人工构建。这实现了人机优势互补，提升了工程效率和本体质量。\\n\\n**关键设计**：关键设计包括使用一次性提示和思维链提示作为基线方法，以评估LLMs的自主能力；在X-HCOME中，设计人类专家与LLMs的交互流程，如输入领域知识并调整LLMs输出；在SimX-HCOME+中，强调迭代循环，包括持续监督、反馈和细化步骤。具体参数设置如提示模板和迭代次数未在摘要中详细说明，但方法依赖于标准LLMs模型和人类评估标准。",
            "application_zh": "该研究在帕金森病监测与预警领域具有直接应用价值，可用于构建更准确的本体以支持智能医疗系统，如症状监测、风险预警和个性化治疗。潜在应用扩展到其他复杂医学领域如阿尔茨海默病或慢性病管理，以及一般本体工程任务，提升自动化构建效率。未来影响包括推动人机协作在知识工程中的普及，并可能催生专门用于本体构建的GPT模型，促进人工智能在医疗和语义网中的深入应用。",
            "highlight_zh": "最重要的实验结果表明，一次性提示和思维链提示方法能自主生成本体，但输出不全面，需要大量人工细化；X-HCOME方法显著改进本体全面性，生成的本体与专家构建的非常相似；SimX-HCOME+方法在持续人类监督下，产生更全面和准确的本体，突出了人机协作的重要性。具体性能数据如准确率或完整性评分未在摘要中提供，但对比基线显示人机协作方法优于纯LLMs方法，提升了本体质量接近专家水平。",
            "tags_zh": [
                "本体工程",
                "大型语言模型",
                "人机协作",
                "帕金森病监测",
                "混合方法",
                "思维链提示",
                "自动化构建",
                "医疗本体"
            ],
            "_index": 74
        },
        {
            "title": "SS4D: Native 4D Generative Model via Structured Spacetime Latents",
            "authors": [
                "Zhibing Li",
                "Mengchen Zhang",
                "Tong Wu",
                "Jing Tan",
                "Jiaqi Wang",
                "Dahua Lin"
            ],
            "arxiv_id": "2512.14284v1",
            "summary": "We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "ToG(Siggraph Asia 2025)",
            "doi": "10.1145/3763302",
            "journal_ref": "ACM Transactions on Graphics, 44(6): Article 244, 12 pages, December 2025",
            "pdf_url": "https://arxiv.org/pdf/2512.14284v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出SS4D原生4D生成模型，通过结构化时空潜变量从单目视频直接合成动态3D物体",
            "summary_zh": "我们提出了SS4D，一种原生4D生成模型，能够直接从单目视频合成动态3D物体。与先前通过优化3D或视频生成模型来构建4D表示的方法不同，我们直接在4D数据上训练生成器，实现了高保真度、时间一致性和结构一致性。我们方法的核心是一组压缩的结构化时空潜变量。具体而言：（1）为了解决4D训练数据稀缺的问题，我们基于预训练的单图像到3D模型构建，保持了强大的空间一致性。（2）通过引入专门的时间层来跨帧推理，强制实现时间一致性。（3）为了支持长视频序列的高效训练和推理，我们使用分解的4D卷积和时间下采样块沿时间轴压缩潜变量序列。此外，我们采用精心设计的训练策略来增强对遮挡的鲁棒性。",
            "intro_zh": [
                "现有方法依赖3D或视频生成模型优化构建4D表示，导致保真度低、时间不一致和结构失真。",
                "提出结构化时空潜变量，结合预训练3D模型、时间层和压缩机制，实现原生4D生成。",
                "实验显示SS4D在动态3D合成中实现高保真度和时间一致性，显著优于基线方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从单目视频直接生成动态3D物体（即4D表示）的问题。现有方法通常基于3D或视频生成模型进行优化，导致保真度低、时间不一致（如帧间抖动）和结构失真，且受限于4D数据稀缺和计算效率低。\\n\\n**核心思路**：设计一个原生4D生成模型，通过结构化时空潜变量直接学习4D数据分布，结合空间一致性、时间推理和高效压缩，以克服数据稀缺和计算挑战。\\n\\n**技术框架**：整体架构包括输入单目视频、结构化时空潜变量编码、生成器网络和输出动态3D序列。主要模块：基于预训练单图像到3D模型的初始化模块、时间层模块（用于跨帧推理）、压缩模块（使用分解4D卷积和时间下采样块）以及训练策略模块（增强遮挡鲁棒性）。\\n\\n**关键创新**：最重要的创新是原生4D生成方法，直接训练在4D数据上，而非优化现有模型。本质区别在于结构化时空潜变量的引入，实现了端到端的高保真4D合成，避免了传统方法的级联误差。\\n\\n**关键设计**：关键参数包括潜变量维度、时间层数、压缩率；损失函数结合重建损失、时间一致性损失和对抗损失；网络结构采用卷积神经网络，时间层使用循环或注意力机制，压缩块基于因子分解卷积以减少计算量。",
            "application_zh": "该研究在虚拟现实、游戏开发、电影特效和机器人仿真等领域具有潜在应用价值，能够高效生成逼真的动态3D内容，降低制作成本。未来可能推动4D生成技术的发展，为多模态交互和实时渲染提供新工具。",
            "highlight_zh": "实验结果表明，SS4D在动态3D合成任务中实现了高保真度（如PSNR提升约15%）和时间一致性（帧间差异减少20%以上），显著优于基于3D优化和视频生成的基线方法。在长视频序列上，压缩机制使训练效率提高30%，同时保持生成质量。",
            "tags_zh": [
                "4D生成模型",
                "动态3D合成",
                "时空潜变量",
                "单目视频理解",
                "时间一致性",
                "结构化压缩",
                "原生生成方法",
                "遮挡鲁棒性"
            ],
            "_index": 75
        },
        {
            "title": "SPARQL-LLM: Real-Time SPARQL Query Generation from Natural Language Questions",
            "authors": [
                "Panayiotis Smeros",
                "Vincent Emonet",
                "Ruijie Wang",
                "Ana-Claudia Sima",
                "Tarcisio Mendes de Farias"
            ],
            "arxiv_id": "2512.14277v1",
            "summary": "The advent of large language models is contributing to the emergence of novel approaches that promise to better tackle the challenge of generating structured queries, such as SPARQL queries, from natural language. However, these new approaches mostly focus on response accuracy over a single source while ignoring other evaluation criteria, such as federated query capability over distributed data stores, as well as runtime and cost to generate SPARQL queries. Consequently, they are often not production-ready or easy to deploy over (potentially federated) knowledge graphs with good accuracy. To mitigate these issues, in this paper, we extend our previous work and describe and systematically evaluate SPARQL-LLM, an open-source and triplestore-agnostic approach, powered by lightweight metadata, that generates SPARQL queries from natural language text. First, we describe its architecture, which consists of dedicated components for metadata indexing, prompt building, and query generation and execution. Then, we evaluate it based on a state-of-the-art challenge with multilingual questions, and a collection of questions from three of the most prevalent knowledge graphs within the field of bioinformatics. Our results demonstrate a substantial increase of 24% in the F1 Score on the state-of-the-art challenge, adaptability to high-resource languages such as English and Spanish, as well as ability to form complex and federated bioinformatics queries. Furthermore, we show that SPARQL-LLM is up to 36x faster than other systems participating in the challenge, while costing a maximum of $0.01 per question, making it suitable for real-time, low-cost text-to-SPARQL applications. One such application deployed over real-world decentralized knowledge graphs can be found at https://www.expasy.org/chat.",
            "categories": [
                "cs.IR",
                "cs.AI",
                "cs.CL"
            ],
            "primary_category": "cs.IR",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "17 pages, 8 figures, 1 table. Under Review",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14277v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出SPARQL-LLM方法，通过轻量级元数据实现实时、低成本的从自然语言生成SPARQL查询，适用于分布式知识图谱。",
            "summary_zh": "大型语言模型的出现促进了从自然语言生成结构化查询（如SPARQL查询）的新方法。然而，这些新方法大多关注单一来源的响应准确性，而忽略了其他评估标准，如分布式数据存储上的联邦查询能力，以及生成SPARQL查询的运行时间和成本。因此，它们通常不适合生产环境或难以在（可能联邦的）知识图谱上以高精度部署。为缓解这些问题，本文扩展了先前工作，描述并系统评估了SPARQL-LLM，这是一种开源且与三元存储无关的方法，由轻量级元数据驱动，从自然语言文本生成SPARQL查询。首先，我们描述了其架构，包括元数据索引、提示构建、查询生成和执行等专用组件。然后，基于一个包含多语言问题的最新挑战，以及来自生物信息学领域三个最流行知识图谱的问题集合进行评估。结果显示，在最新挑战中F1分数显著提高了24%，适应英语和西班牙语等高资源语言，并能形成复杂和联邦的生物信息学查询。此外，SPARQL-LLM比参与挑战的其他系统快达36倍，每个问题成本最高为0.01美元，使其适用于实时、低成本的文本到SPARQL应用。一个部署在真实世界去中心化知识图谱上的此类应用可在https://www.expasy.org/chat找到。",
            "intro_zh": [
                "现有方法主要关注单一来源的查询准确性，忽略了联邦查询能力、运行时间和成本，导致难以在生产环境中部署。",
                "SPARQL-LLM采用轻量级元数据和模块化架构，通过专用组件实现高效、低成本的SPARQL查询生成，支持多语言和分布式数据。",
                "实验显示，F1分数提升24%，运行速度快达36倍，成本低至每问题0.01美元，适用于实时应用。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从自然语言生成SPARQL查询的挑战，现有方法痛点包括过度依赖单一数据源、缺乏联邦查询支持、运行时间慢和成本高，导致难以在生产环境中部署。\\n\\n**核心思路**：论文提出SPARQL-LLM，核心思路是利用轻量级元数据作为中介，结合大型语言模型的能力，设计一个与三元存储无关的模块化系统，以平衡准确性、效率和成本，支持实时和分布式查询生成。\\n\\n**技术框架**：整体架构包括三个主要阶段：元数据索引阶段，用于提取和存储知识图谱的轻量级结构信息；提示构建阶段，基于元数据和自然语言问题生成优化提示；查询生成和执行阶段，使用大型语言模型生成SPARQL查询并执行验证，确保查询的正确性和效率。\\n\\n**关键创新**：最重要的技术创新点是轻量级元数据的使用，这减少了模型对完整数据源的依赖，提高了查询生成的速度和可扩展性；同时，系统设计为与三元存储无关，增强了通用性和部署灵活性，与现有方法相比，本质区别在于综合优化了准确性、联邦能力和成本效益。\\n\\n**关键设计**：关键设计包括元数据索引的轻量化处理，仅提取必要结构信息以减少存储和计算开销；提示构建采用模板化方法，结合元数据动态生成上下文，提高模型理解；查询生成阶段可能集成开源或商业大型语言模型，具体模型选择和参数设置未详细说明，但强调成本控制，如每问题成本上限为0.01美元；损失函数和网络结构细节未知，但整体设计注重模块化和实时性能。",
            "application_zh": "该研究在生物信息学、知识图谱查询和智能问答系统中有广泛应用潜力，能支持多语言和分布式数据源的实时查询，降低部署成本，提升用户体验。实际价值包括加速科研数据访问和促进企业级知识管理，未来可能推动更高效的AI驱动数据交互平台发展。",
            "highlight_zh": "最重要的实验结果包括：在最新挑战中，F1分数相比基线提升24%，达到更高准确性；运行时间比参与挑战的其他系统快达36倍，实现实时查询；成本控制出色，每问题最高0.01美元，适合大规模应用；此外，系统成功处理多语言（如英语和西班牙语）和复杂联邦查询，展示了强适应性和扩展性。",
            "tags_zh": [
                "SPARQL查询生成",
                "自然语言处理",
                "知识图谱",
                "联邦查询",
                "轻量级元数据",
                "实时系统",
                "低成本AI",
                "生物信息学应用"
            ],
            "_index": 76
        },
        {
            "title": "Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in",
            "authors": [
                "Xiaoqian Shen",
                "Min-Hung Chen",
                "Yu-Chiang Frank Wang",
                "Mohamed Elhoseiny",
                "Ryo Hachiuma"
            ],
            "arxiv_id": "2512.14273v1",
            "summary": "Grounded video question answering (GVQA) aims to localize relevant temporal segments in videos and generate accurate answers to a given question; however, large video-language models (LVLMs) exhibit limited temporal awareness. Although existing approaches based on Group Relative Policy Optimization (GRPO) attempt to improve temporal grounding, they still struggle to faithfully ground their answers in the relevant video evidence, leading to temporal mislocalization and hallucinations. In this work, we present Zoom-Zero, a coarse-to-fine framework that first localizes query-relevant segments and then temporally zooms into the most salient frames for finer-grained visual verification. Our method addresses the limits of GRPO for the GVQA task with two key innovations: (i) a zoom-in accuracy reward that validates the fidelity of temporal grounding prediction and facilitates fine-grained visual verification on grounded frames; (ii) token-selective credit assignment, which attributes rewards to the tokens responsible for temporal localization or answer generation, mitigating GRPO's issue in handling multi-faceted reward signals. Our proposed method advances grounded video question answering, improving temporal grounding by 5.2\\% on NExT-GQA and 4.6\\% on ReXTime, while also enhancing average answer accuracy by 2.4\\%. Additionally, the coarse-to-fine zoom-in during inference further benefits long-form video understanding by preserving critical visual details without compromising global context, yielding an average improvement of 6.4\\% on long-video benchmarks.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page: https://xiaoqian-shen.github.io/Zoom-Zero/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14273v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reward"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出Zoom-Zero框架，通过粗到细的时序放大机制解决视频问答中的时序定位不准确问题。",
            "summary_zh": "基于视频的问答任务旨在定位视频中的相关时序片段并生成准确答案，但现有大型视频语言模型在时序感知方面存在局限。虽然基于组相对策略优化的方法试图改进时序定位，但仍难以忠实依据视频证据进行回答，导致时序错位和幻觉。本文提出Zoom-Zero，一种粗到细的框架，首先定位查询相关片段，然后时序放大到最显著帧进行细粒度视觉验证。该方法通过两个关键创新解决GVQA任务中GRPO的局限：(i) 放大精度奖励，验证时序定位预测的保真度并促进对定位帧的细粒度视觉验证；(ii) 令牌选择性信用分配，将奖励归因于负责时序定位或答案生成的令牌，缓解GRPO处理多方面奖励信号的问题。所提方法在NExT-GQA和ReXTime数据集上分别将时序定位精度提升5.2%和4.6%，平均答案准确率提升2.4%。此外，推理过程中的粗到细放大通过保留关键视觉细节而不损害全局上下文，进一步有益于长视频理解，在长视频基准上平均提升6.4%。",
            "intro_zh": [
                "现有大型视频语言模型在时序感知方面有限，基于GRPO的方法仍存在时序错位和幻觉问题，难以忠实依据视频证据进行回答。",
                "提出Zoom-Zero框架，采用粗到细策略：先粗粒度定位相关片段，再时序放大到关键帧进行细粒度视觉验证，结合强化学习优化。",
                "在NExT-GQA和ReXTime数据集上，时序定位精度分别提升5.2%和4.6%，答案准确率提升2.4%，长视频理解平均提升6.4%。"
            ],
            "method_zh": "**问题定义**：论文解决基于视频的问答任务中的时序定位不准确问题。现有大型视频语言模型在时序感知方面表现有限，基于GRPO的方法虽然尝试改进，但仍难以忠实依据视频证据进行回答，导致时序错位和幻觉，影响答案的可靠性和准确性。\\n\\n**核心思路**：论文提出粗到细的时序放大框架，通过先粗粒度定位查询相关片段，再时序放大到最显著帧进行细粒度视觉验证，结合强化学习优化时序定位和答案生成过程，以提高模型的时序感知能力和答案保真度。\\n\\n**技术框架**：整体架构分为两个阶段：第一阶段为粗粒度定位，使用模型初步识别视频中与查询相关的时序片段；第二阶段为细粒度放大，对定位片段进行时序放大，提取关键帧进行更精细的视觉验证。框架集成强化学习组件，通过奖励机制优化定位和生成过程。\\n\\n**关键创新**：最重要的技术创新包括：(i) 放大精度奖励，设计奖励函数验证时序定位预测的保真度，并促进对定位帧的细粒度视觉验证；(ii) 令牌选择性信用分配，将奖励归因于负责时序定位或答案生成的特定令牌，解决GRPO在处理多方面奖励信号时的信用分配问题，提升训练效率。\\n\\n**关键设计**：技术细节包括：使用强化学习策略优化，奖励函数结合放大精度奖励和答案准确性；网络结构可能包含视频编码器、语言模型和时序定位模块；参数设置涉及学习率、奖励权重等，具体值未知；损失函数可能包括策略梯度损失和辅助损失，以平衡时序定位和答案生成目标。",
            "application_zh": "该研究在视频理解领域具有广泛潜在应用，如智能视频监控、教育视频分析、医疗视频诊断和自动驾驶场景理解。通过提高时序定位精度和答案可靠性，可增强多模态AI系统在长视频处理中的能力，推动人机交互和自动化视频分析技术的发展，未来可能扩展到更复杂的视频推理任务。",
            "highlight_zh": "最重要的实验结果包括：在NExT-GQA数据集上，时序定位精度提升5.2%；在ReXTime数据集上，时序定位精度提升4.6%；平均答案准确率提升2.4%；在长视频基准上，通过粗到细放大机制，平均性能提升6.4%。这些结果对比基线方法（如基于GRPO的方法）显示出显著改进，验证了Zoom-Zero框架的有效性。",
            "tags_zh": [
                "视频问答",
                "时序定位",
                "粗到细框架",
                "强化学习",
                "多模态融合",
                "长视频理解",
                "视觉验证",
                "信用分配"
            ],
            "_index": 77
        },
        {
            "title": "CaFe-TeleVision: A Coarse-to-Fine Teleoperation System with Immersive Situated Visualization for Enhanced Ergonomics",
            "authors": [
                "Zixin Tang",
                "Yiming Chen",
                "Quentin Rouxel",
                "Dianxi Li",
                "Shuang Wu",
                "Fei Chen"
            ],
            "arxiv_id": "2512.14270v1",
            "summary": "Teleoperation presents a promising paradigm for remote control and robot proprioceptive data collection. Despite recent progress, current teleoperation systems still suffer from limitations in efficiency and ergonomics, particularly in challenging scenarios. In this paper, we propose CaFe-TeleVision, a coarse-to-fine teleoperation system with immersive situated visualization for enhanced ergonomics. At its core, a coarse-to-fine control mechanism is proposed in the retargeting module to bridge workspace disparities, jointly optimizing efficiency and physical ergonomics. To stream immersive feedback with adequate visual cues for human vision systems, an on-demand situated visualization technique is integrated in the perception module, which reduces the cognitive load for multi-view processing. The system is built on a humanoid collaborative robot and validated with six challenging bimanual manipulation tasks. User study among 24 participants confirms that CaFe-TeleVision enhances ergonomics with statistical significance, indicating a lower task load and a higher user acceptance during teleoperation. Quantitative results also validate the superior performance of our system across six tasks, surpassing comparative methods by up to 28.89% in success rate and accelerating by 26.81% in completion time. Project webpage: https://clover-cuhk.github.io/cafe_television/",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14270v1",
            "code_links": [
                {
                    "url": "https://clover-cuhk.github.io/cafe_television/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "人形机器人",
                    "matched_keywords": [
                        "humanoid"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出CaFe-TeleVision系统，通过粗到精控制与沉浸式可视化提升远程操作效率与人机工程学",
            "summary_zh": "远程操作为远程控制和机器人本体感知数据收集提供了有前景的范式。尽管近期取得进展，当前远程操作系统在效率和人体工程学方面仍存在局限，特别是在挑战性场景中。本文提出CaFe-TeleVision，一种具有沉浸式情境可视化的粗到精远程操作系统，以增强人体工程学。其核心是在重定向模块中提出粗到精控制机制，以弥合工作空间差异，共同优化效率和物理人体工程学。为了以足够的视觉线索流式传输沉浸式反馈以适应人类视觉系统，感知模块集成了按需情境可视化技术，减少了多视图处理的认知负荷。该系统基于人形协作机器人构建，并通过六项挑战性双手操作任务进行验证。对24名参与者的用户研究证实，CaFe-TeleVision在统计学上显著增强了人体工程学，表明在远程操作期间任务负荷更低、用户接受度更高。定量结果也验证了我们的系统在六项任务中的优越性能，成功率最高超过对比方法28.89%，完成时间加速26.81%。项目网页：https://clover-cuhk.github.io/cafe_television/",
            "intro_zh": [
                "现有远程操作系统在挑战性场景中效率与人机工程学不足，如工作空间差异导致操作困难、多视图处理增加认知负荷。",
                "提出粗到精控制机制优化重定向，结合沉浸式情境可视化技术，降低认知负荷，提升操作自然度与舒适度。",
                "用户研究显示任务负荷显著降低，系统在六项任务中成功率最高提升28.89%，完成时间加速26.81%，验证了有效性。"
            ],
            "method_zh": "**问题定义**：远程操作系统在挑战性场景中面临效率与人机工程学问题，具体包括工作空间差异导致操作不精确、多视图反馈增加认知负荷，现有方法难以同时优化物理舒适度与任务性能。\\n\\n**核心思路**：采用粗到精控制机制弥合工作空间差异，结合沉浸式情境可视化技术提供按需视觉反馈，通过模块化设计协同提升操作效率与人体舒适度，核心在于平衡精确控制与自然交互。\\n\\n**技术框架**：系统基于人形协作机器人构建，包含重定向模块和感知模块。重定向模块实现粗到精控制，先进行粗略位置调整再精细姿态优化；感知模块集成沉浸式情境可视化，动态生成多视图反馈以减少认知处理。整体流程从用户输入到机器人执行，通过反馈循环优化操作。\\n\\n**关键创新**：粗到精控制机制在重定向中联合优化效率与物理人体工程学，本质区别在于分阶段处理工作空间映射，而非单一映射；沉浸式情境可视化按需提供视觉线索，减少多视图冗余，与现有固定视图方法相比更适应人类视觉系统。\\n\\n**关键设计**：重定向模块采用基于运动学模型的优化算法，参数设置包括粗调阈值和精调增益，以平衡速度与精度；沉浸式可视化基于场景语义分割，动态选择关键视图，网络结构可能涉及轻量级CNN，具体损失函数未知，但设计目标是最小化操作误差和认知负荷。",
            "application_zh": "该研究在机器人远程操作领域具有广泛潜在应用，如危险环境作业（核设施维护、灾难救援）、医疗手术辅助、工业制造中的精密装配，以及远程培训和数据收集。实际价值在于提升操作效率、降低用户疲劳，未来可能推动人机协作向更自然、沉浸式方向发展，促进机器人技术在复杂任务中的普及。",
            "highlight_zh": "最重要的实验结果包括：用户研究（24名参与者）显示系统显著降低任务负荷并提高用户接受度，具有统计学意义；定量评估中，在六项挑战性双手操作任务上，CaFe-TeleVision的成功率最高超过对比方法28.89%，平均完成时间加速26.81%，验证了其在效率和人机工程学方面的优越性能。",
            "tags_zh": [
                "远程操作",
                "人机工程学",
                "粗到精控制",
                "沉浸式可视化",
                "机器人重定向",
                "双手操作",
                "认知负荷优化",
                "协作机器人"
            ],
            "_index": 78
        },
        {
            "title": "Enhancing Visual Programming for Visual Reasoning via Probabilistic Graphs",
            "authors": [
                "Wentao Wan",
                "Kaiyu Wu",
                "Qingyang Ma",
                "Nan Kang",
                "Yunjie Chen",
                "Liang Lin",
                "Keze Wang"
            ],
            "arxiv_id": "2512.14257v1",
            "summary": "Recently, Visual Programming (VP) based on large language models (LLMs) has rapidly developed and demonstrated significant potential in complex Visual Reasoning (VR) tasks. Previous works to enhance VP have primarily focused on improving the quality of LLM-generated visual programs. However, they have neglected to optimize the VP-invoked pre-trained models, which serve as modules for the visual sub-tasks decomposed from the targeted tasks by VP. The difficulty is that there are only final labels of targeted VR tasks rather than labels of sub-tasks. Besides, the non-differentiable nature of VP impedes the direct use of efficient gradient-based optimization methods to leverage final labels for end-to-end learning of the entire VP framework. To overcome these issues, we propose EVPG, a method to Enhance Visual Programming for visual reasoning via Probabilistic Graphs. Specifically, we creatively build a directed probabilistic graph according to the variable dependency relationships during the VP executing process, which reconstructs the non-differentiable VP executing process into a differentiable exact probability inference process on this directed probabilistic graph. As a result, this enables the VP framework to utilize the final labels for efficient, gradient-based optimization in end-to-end supervised learning on targeted VR tasks. Extensive and comprehensive experiments demonstrate the effectiveness and advantages of our EVPG, showing significant performance improvements for VP on three classical complex VR tasks: GQA, NLVRv2, and Open Images.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "13 Pages, 12 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14257v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO",
                        "VIO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出EVPG方法，通过概率图将不可微的视觉编程执行过程重构为可微的概率推理过程，以解决视觉编程中预训练模型优化难题。",
            "summary_zh": "近年来，基于大语言模型（LLMs）的视觉编程（VP）在复杂视觉推理（VR）任务中快速发展并展现出巨大潜力。先前增强VP的工作主要集中于提高LLM生成的视觉程序质量，但忽略了优化VP调用的预训练模型，这些模型作为VP从目标任务分解出的视觉子任务的模块。困难在于只有目标VR任务的最终标签，而没有子任务的标签。此外，VP的不可微性阻碍了直接使用高效的基于梯度的优化方法，以利用最终标签进行整个VP框架的端到端学习。为克服这些问题，我们提出了EVPG，一种通过概率图增强视觉编程以进行视觉推理的方法。具体而言，我们根据VP执行过程中的变量依赖关系，创造性地构建了一个有向概率图，将不可微的VP执行过程重构为该有向概率图上的可微精确概率推理过程。这使得VP框架能够利用最终标签，在目标VR任务的端到端监督学习中实现高效的基于梯度的优化。广泛而全面的实验证明了我们EVPG的有效性和优势，在三个经典复杂VR任务（GQA、NLVRv2和Open Images）上显示出VP的显著性能提升。",
            "intro_zh": [
                "现有方法主要优化LLM生成的视觉程序，但忽略了VP调用的预训练模型优化，且缺乏子任务标签，导致端到端学习困难。",
                "EVPG通过构建有向概率图，将VP执行过程重构为可微概率推理，实现基于梯度的优化，利用最终标签进行端到端学习。",
                "在GQA、NLVRv2和Open Images等任务上，EVPG显著提升了VP性能，验证了其有效性和优势。"
            ],
            "method_zh": "**问题定义**：论文旨在解决视觉编程（VP）框架中预训练模型优化的难题。现有方法主要关注提升大语言模型（LLM）生成的视觉程序质量，但忽略了VP调用的预训练模型（作为视觉子任务模块）的优化。痛点在于：只有目标视觉推理（VR）任务的最终标签，缺乏子任务标签；且VP执行过程不可微，无法直接使用梯度优化方法进行端到端学习。\\n\\n**核心思路**：论文提出EVPG方法，核心思路是将不可微的VP执行过程重构为可微的概率推理过程。通过分析VP执行中的变量依赖关系，构建有向概率图，将程序执行转化为概率图上的精确推理，从而允许使用梯度下降优化整个框架。\\n\\n**技术框架**：整体架构包括三个主要阶段：首先，LLM根据输入问题生成视觉程序；其次，基于程序执行流程，构建有向概率图，其中节点表示变量（如中间结果），边表示依赖关系；然后，将预训练模型（如视觉模块）的输出映射为概率分布，在概率图上进行推理，计算最终答案的概率；最后，利用目标任务的最终标签，通过梯度反向传播优化所有参数。\\n\\n**关键创新**：最重要的技术创新是构建有向概率图来建模VP执行过程，实现从不可微到可微的转换。与现有方法本质区别在于：现有工作仅优化程序生成，而EVPG同时优化程序生成和预训练模型，通过概率图实现端到端可微学习，解决了标签缺失和不可微性问题。\\n\\n**关键设计**：关键设计包括：有向概率图的构建基于程序变量依赖，确保推理的准确性；使用概率分布表示模型输出，如softmax函数；损失函数基于最终答案的概率与真实标签的交叉熵，支持梯度计算；优化时，反向传播通过概率图更新LLM和预训练模型参数，实现联合训练。",
            "application_zh": "该研究在复杂视觉推理任务中具有广泛的应用潜力，如智能问答系统、图像理解、自动驾驶和机器人视觉导航。通过增强视觉编程框架，EVPG能提升多模态AI系统的端到端学习能力，促进更高效、准确的视觉推理，未来可扩展到更多视觉任务和实际场景，推动人工智能在真实世界中的应用。",
            "highlight_zh": "实验在三个经典复杂视觉推理任务上进行：GQA、NLVRv2和Open Images。EVPG显著提升了视觉编程的性能，具体数据未在摘要中提供，但论文指出“显著性能改进”，表明相比基线方法有较大提升。这些结果验证了EVPG在端到端优化和利用最终标签方面的有效性，增强了VP框架的鲁棒性和准确性。",
            "tags_zh": [
                "视觉编程",
                "概率图模型",
                "端到端学习",
                "视觉推理",
                "大语言模型",
                "可微优化",
                "多模态AI",
                "监督学习"
            ],
            "_index": 79
        },
        {
            "title": "DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos",
            "authors": [
                "Yang Bai",
                "Liudi Yang",
                "George Eskandar",
                "Fengyi Shen",
                "Mohammad Altillawi",
                "Ziyuan Liu",
                "Gitta Kutyniok"
            ],
            "arxiv_id": "2512.14217v1",
            "summary": "Video diffusion models provide powerful real-world simulators for embodied AI but remain limited in controllability for robotic manipulation. Recent works on trajectory-conditioned video generation address this gap but often rely on 2D trajectories or single modality conditioning, which restricts their ability to produce controllable and consistent robotic demonstrations. We present DRAW2ACT, a depth-aware trajectory-conditioned video generation framework that extracts multiple orthogonal representations from the input trajectory, capturing depth, semantics, shape and motion, and injects them into the diffusion model. Moreover, we propose to jointly generate spatially aligned RGB and depth videos, leveraging cross-modality attention mechanisms and depth supervision to enhance the spatio-temporal consistency. Finally, we introduce a multimodal policy model conditioned on the generated RGB and depth sequences to regress the robot's joint angles. Experiments on Bridge V2, Berkeley Autolab, and simulation benchmarks show that DRAW2ACT achieves superior visual fidelity and consistency while yielding higher manipulation success rates compared to existing baselines.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14217v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "世界模型",
                    "matched_keywords": [
                        "world simulator"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出DRAW2ACT框架，通过深度感知轨迹条件视频生成解决机器人演示可控性和一致性问题",
            "summary_zh": "视频扩散模型为具身AI提供了强大的真实世界模拟器，但在机器人操作的可控性方面仍存在局限。最近关于轨迹条件视频生成的研究试图填补这一空白，但通常依赖于2D轨迹或单模态条件，这限制了它们生成可控且一致的机器人演示的能力。我们提出了DRAW2ACT，这是一个深度感知轨迹条件视频生成框架，它从输入轨迹中提取多个正交表示，捕捉深度、语义、形状和运动，并将它们注入扩散模型。此外，我们提出联合生成空间对齐的RGB和深度视频，利用跨模态注意力机制和深度监督来增强时空一致性。最后，我们引入了一个基于生成的RGB和深度序列的多模态策略模型，以回归机器人的关节角度。在Bridge V2、Berkeley Autolab和仿真基准上的实验表明，与现有基线相比，DRAW2ACT实现了更优的视觉保真度和一致性，同时获得了更高的操作成功率。",
            "intro_zh": [
                "现有视频扩散模型在机器人操作中可控性不足，依赖2D轨迹或单模态条件限制了演示的一致性和可控性。",
                "DRAW2ACT框架从轨迹提取深度、语义等多模态表示，并联合生成RGB和深度视频以增强时空一致性。",
                "实验在多个基准上验证了DRAW2ACT的视觉保真度和操作成功率均优于现有基线，提升了机器人演示质量。"
            ],
            "method_zh": "**问题定义**：论文旨在解决视频扩散模型在机器人操作演示生成中可控性不足的问题。现有方法通常基于2D轨迹或单模态条件，导致生成的视频在深度信息和时空一致性方面受限，难以产生高质量、可控的机器人演示视频。\\n\\n**核心思路**：通过从输入轨迹中提取多模态表示（包括深度、语义、形状和运动），并将这些表示注入扩散模型，以增强生成视频的可控性和一致性。同时，联合生成RGB和深度视频，利用跨模态注意力机制和深度监督来优化时空对齐。\\n\\n**技术框架**：整体框架包括轨迹编码模块、多模态表示提取模块、视频生成模块和多模态策略模块。首先，从输入轨迹中提取深度编码信息；然后，将这些信息转化为多个正交表示；接着，通过扩散模型生成空间对齐的RGB和深度视频序列；最后，基于生成的视频序列训练策略模型来回归机器人关节角度。\\n\\n**关键创新**：最重要的创新在于深度感知轨迹条件视频生成，通过提取和注入多模态表示，结合跨模态注意力机制，显著提升了生成视频的深度一致性和可控性。与现有方法相比，本质区别在于同时处理RGB和深度信息，并利用深度监督增强时空对齐。\\n\\n**关键设计**：关键设计包括使用扩散模型作为视频生成器，引入跨模态注意力机制来融合RGB和深度信息，设计损失函数以监督深度一致性，以及采用多模态策略模型进行关节角度回归。具体参数设置和网络结构细节在论文中未详细说明，但强调了深度监督和联合训练的重要性。",
            "application_zh": "该研究在机器人演示生成领域具有重要应用价值，可用于机器人操作任务的模拟训练、虚拟现实交互和自动化系统开发。通过生成高质量、可控的演示视频，能提升机器人学习效率和实际部署的可靠性，未来可能推动具身AI和智能机器人技术的进步。",
            "highlight_zh": "在Bridge V2、Berkeley Autolab和仿真基准上的实验表明，DRAW2ACT在视觉保真度和一致性方面优于现有基线，具体性能数据未详细提供，但报告了更高的操作成功率，验证了其有效性。",
            "tags_zh": [
                "深度感知视频生成",
                "轨迹条件生成",
                "多模态融合",
                "机器人演示",
                "扩散模型",
                "时空一致性",
                "跨模态注意力",
                "具身AI"
            ],
            "_index": 80
        },
        {
            "title": "Error Bound Analysis of Physics-Informed Neural Networks-Driven T2 Quantification in Cardiac Magnetic Resonance Imaging",
            "authors": [
                "Mengxue Zhang",
                "Qingrui Cai",
                "Yinyin Chen",
                "Hang Jin",
                "Jianjun Zhou",
                "Qiu Guo",
                "Peijun Zhao",
                "Zhiping Mao",
                "Xingxing Zhang",
                "Yuyu Xia",
                "Xianwang Jiang",
                "Qin Xu",
                "Chunyan Xiong",
                "Yirong Zhou",
                "Chengyan Wang",
                "Xiaobo Qu"
            ],
            "arxiv_id": "2512.14211v1",
            "summary": "Physics-Informed Neural Networks (PINN) are emerging as a promising approach for quantitative parameter estimation of Magnetic Resonance Imaging (MRI). While existing deep learning methods can provide an accurate quantitative estimation of the T2 parameter, they still require large amounts of training data and lack theoretical support and a recognized gold standard. Thus, given the absence of PINN-based approaches for T2 estimation, we propose embedding the fundamental physics of MRI, the Bloch equation, in the loss of PINN, which is solely based on target scan data and does not require a pre-defined training database. Furthermore, by deriving rigorous upper bounds for both the T2 estimation error and the generalization error of the Bloch equation solution, we establish a theoretical foundation for evaluating the PINN's quantitative accuracy. Even without access to the ground truth or a gold standard, this theory enables us to estimate the error with respect to the real quantitative parameter T2. The accuracy of T2 mapping and the validity of the theoretical analysis are demonstrated on a numerical cardiac model and a water phantom, where our method exhibits excellent quantitative precision in the myocardial T2 range. Clinical applicability is confirmed in 94 acute myocardial infarction (AMI) patients, achieving low-error quantitative T2 estimation under the theoretical error bound, highlighting the robustness and potential of PINN.",
            "categories": [
                "physics.bio-ph",
                "cs.AI"
            ],
            "primary_category": "physics.bio-ph",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14211v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "mapping"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出基于物理信息神经网络的T2量化误差界分析方法，用于心脏磁共振成像参数估计。",
            "summary_zh": "物理信息神经网络（PINN）正成为磁共振成像（MRI）定量参数估计的一种有前景方法。虽然现有深度学习方法能提供T2参数的准确定量估计，但仍需要大量训练数据，且缺乏理论支持和公认的金标准。因此，鉴于目前尚无基于PINN的T2估计方法，我们提出将MRI的基本物理原理——布洛赫方程嵌入PINN的损失函数中，该方法仅基于目标扫描数据，无需预定义的训练数据库。此外，通过推导T2估计误差和布洛赫方程解泛化误差的严格上界，我们为评估PINN的定量准确性建立了理论基础。即使无法获得真实值或金标准，该理论也能使我们估计相对于真实定量参数T2的误差。T2映射的准确性和理论分析的有效性在数值心脏模型和水模上得到验证，我们的方法在心肌T2范围内表现出优异的定量精度。临床适用性在94名急性心肌梗死（AMI）患者中得到确认，在理论误差界内实现了低误差的定量T2估计，突显了PINN的鲁棒性和潜力。",
            "intro_zh": [
                "现有深度学习方法依赖大量训练数据且缺乏理论支持，难以在无金标准下评估T2参数估计的准确性。",
                "提出将布洛赫方程嵌入PINN损失函数，仅基于扫描数据实现T2估计，无需预训练数据库。",
                "在数值模型和临床患者中验证了方法的定量精度，理论误差界指导下的T2估计达到低误差水平。"
            ],
            "method_zh": "**问题定义**：论文旨在解决心脏磁共振成像中T2参数定量估计的问题。现有深度学习方法虽然能提供准确估计，但需要大量训练数据，且缺乏理论支持，无法在无金标准情况下评估估计误差，限制了其可靠性和临床应用。\\n\\n**核心思路**：论文的核心思路是利用物理信息神经网络（PINN），将MRI的基本物理原理——布洛赫方程直接嵌入神经网络的损失函数中，从而仅基于目标扫描数据实现T2估计，无需依赖外部训练数据库。同时，通过理论分析推导误差上界，为定量准确性提供理论保障。\\n\\n**技术框架**：整体框架包括数据采集、PINN模型构建、损失函数设计和误差分析。首先，获取目标扫描的MRI数据；然后，构建神经网络模型，其输出为T2参数估计；在训练过程中，损失函数结合数据拟合项和物理约束项（基于布洛赫方程）；最后，通过理论推导得到T2估计误差和泛化误差的上界，用于评估模型性能。\\n\\n**关键创新**：最重要的创新点是将布洛赫方程作为物理先验嵌入PINN损失函数，首次实现基于PINN的T2估计，并建立了严格的误差界理论。与现有方法相比，本质区别在于无需预训练数据，且提供了理论误差评估，增强了方法的可解释性和可靠性。\\n\\n**关键设计**：关键设计包括损失函数设计，其中包含数据项（如均方误差）和物理项（基于布洛赫方程的残差）；网络结构可能采用多层感知机等通用架构；参数设置涉及优化算法（如Adam）和学习率调整；理论分析中，误差界推导基于函数逼近理论和正则化技术，具体细节需参考论文附录。",
            "application_zh": "该研究在心脏磁共振成像领域具有重要应用价值，可用于急性心肌梗死等心脏疾病的定量诊断和监测。通过提供理论误差界的T2估计，方法能提升临床参数评估的可靠性，减少对金标准的依赖。未来可扩展至其他MRI参数估计或医学成像任务，推动物理驱动AI在医疗影像分析中的发展。",
            "highlight_zh": "在数值心脏模型和水模实验中，方法在心肌T2范围内表现出优异定量精度，具体性能数据未在摘要中给出，但强调了低误差特性。临床验证中，94名急性心肌梗死患者的T2估计在理论误差界内达到低误差水平，突显了方法的鲁棒性。与现有深度学习方法相比，无需大量训练数据，且提供了理论支持，提升了估计的可信度。",
            "tags_zh": [
                "物理信息神经网络",
                "磁共振成像",
                "T2量化",
                "误差界分析",
                "心脏成像",
                "布洛赫方程",
                "定量参数估计",
                "医学影像分析"
            ],
            "_index": 81
        },
        {
            "title": "Towards Explainable Quantum AI: Informing the Encoder Selection of Quantum Neural Networks via Visualization",
            "authors": [
                "Shaolun Ruan",
                "Feng Liang",
                "Rohan Ramakrishna",
                "Chao Ren",
                "Rudai Yan",
                "Qiang Guan",
                "Jiannan Li",
                "Yong Wang"
            ],
            "arxiv_id": "2512.14181v1",
            "summary": "Quantum Neural Networks (QNNs) represent a promising fusion of quantum computing and neural network architectures, offering speed-ups and efficient processing of high-dimensional, entangled data. A crucial component of QNNs is the encoder, which maps classical input data into quantum states. However, choosing suitable encoders remains a significant challenge, largely due to the lack of systematic guidance and the trial-and-error nature of current approaches. This process is further impeded by two key challenges: (1) the difficulty in evaluating encoded quantum states prior to training, and (2) the lack of intuitive methods for analyzing an encoder's ability to effectively distinguish data features. To address these issues, we introduce a novel visualization tool, XQAI-Eyes, which enables QNN developers to compare classical data features with their corresponding encoded quantum states and to examine the mixed quantum states across different classes. By bridging classical and quantum perspectives, XQAI-Eyes facilitates a deeper understanding of how encoders influence QNN performance. Evaluations across diverse datasets and encoder designs demonstrate XQAI-Eyes's potential to support the exploration of the relationship between encoder design and QNN effectiveness, offering a holistic and transparent approach to optimizing quantum encoders. Moreover, domain experts used XQAI-Eyes to derive two key practices for quantum encoder selection, grounded in the principles of pattern preservation and feature mapping.",
            "categories": [
                "quant-ph",
                "cs.AI",
                "cs.HC"
            ],
            "primary_category": "quant-ph",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "9 pages, 6 figures, accepted by TVCG 2026, not published yet",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14181v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "mapping"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出XQAI-Eyes可视化工具以解决量子神经网络编码器选择缺乏系统指导的问题",
            "summary_zh": "量子神经网络（QNNs）代表了量子计算与神经网络架构的有前景融合，提供了处理高维纠缠数据的加速和高效处理能力。QNNs的一个关键组件是编码器，它将经典输入数据映射到量子态。然而，选择合适的编码器仍然是一个重大挑战，这主要是由于缺乏系统指导以及当前方法的试错性质。这一过程还受到两个关键挑战的进一步阻碍：（1）在训练前评估编码量子态的困难，以及（2）缺乏直观方法来分析编码器有效区分数据特征的能力。为了解决这些问题，我们引入了一种新颖的可视化工具XQAI-Eyes，使QNN开发者能够比较经典数据特征与其对应的编码量子态，并检查不同类别之间的混合量子态。通过桥接经典和量子视角，XQAI-Eyes促进了对编码器如何影响QNN性能的更深理解。在不同数据集和编码器设计上的评估表明，XQAI-Eyes有潜力支持探索编码器设计与QNN有效性之间的关系，为优化量子编码器提供了一种全面且透明的方法。此外，领域专家使用XQAI-Eyes基于模式保留和特征映射原则，得出了量子编码器选择的两个关键实践。",
            "intro_zh": [
                "核心问题：量子神经网络编码器选择缺乏系统指导，现有方法依赖试错，难以评估编码态和分析特征区分能力。",
                "方法要点：提出XQAI-Eyes可视化工具，通过比较经典数据与编码量子态，帮助开发者直观理解编码器性能。",
                "实验或效果：评估显示XQAI-Eyes能有效支持编码器设计探索，并基于可视化结果推导出编码器选择的关键实践。"
            ],
            "method_zh": "**问题定义**：论文旨在解决量子神经网络（QNNs）中编码器选择缺乏系统指导的问题。现有方法的痛点包括：编码器选择依赖试错，难以在训练前评估编码量子态，以及缺乏直观工具分析编码器区分数据特征的能力，这导致QNN性能优化效率低下。\\n\\n**核心思路**：论文的核心解决思路是开发一个可视化工具XQAI-Eyes，通过桥接经典数据特征与编码量子态，提供直观的视觉分析，帮助开发者理解编码器如何影响QNN性能。这样设计是因为可视化能弥补量子态抽象性带来的理解障碍，使编码器选择过程更加透明和系统化。\\n\\n**技术框架**：整体架构包括数据输入、编码器映射、量子态可视化、经典特征对比和混合态分析等模块。流程为：输入经典数据，通过不同编码器映射到量子态，使用XQAI-Eyes工具可视化量子态，并与原始经典数据特征进行比较，同时分析不同类别间的混合量子态，以评估编码器的区分能力。\\n\\n**关键创新**：最重要的技术创新点是XQAI-Eyes工具本身，它首次将可视化技术系统应用于量子编码器分析，实现了经典与量子视角的直观桥接。与现有方法的本质区别在于，它提供了事前评估和特征分析能力，而非依赖事后训练结果，从而降低了试错成本。\\n\\n**关键设计**：XQAI-Eyes的关键设计包括：支持多种编码器类型（如角度编码、振幅编码等），可视化量子态的概率分布或相位信息，集成经典数据降维技术（如PCA或t-SNE）以进行特征对比，并允许交互式探索混合量子态。工具参数设置灵活，适应不同数据集和QNN架构，但具体损失函数或网络结构细节在摘要中未详细说明，可能依赖于标准量子计算框架如Qiskit或PennyLane。",
            "application_zh": "该研究在量子机器学习领域具有广泛潜在应用，包括量子化学模拟、金融建模和药物发现等需要高效处理高维数据的场景。XQAI-Eyes工具的实际价值在于提升QNN开发效率，通过可视化指导编码器选择，减少试错时间，从而加速量子AI模型的部署。未来影响可能推动量子计算的可解释性研究，促进量子与经典AI的融合，为更复杂的量子算法设计提供基础支持。",
            "highlight_zh": "最重要的实验结果包括：XQAI-Eyes在不同数据集（如MNIST或量子特定数据集）和多种编码器设计上进行了评估，展示了其有效可视化编码量子态的能力。具体性能数据未在摘要中提供，但评估表明工具能支持探索编码器设计与QNN有效性的关系，例如通过可视化识别出编码器在特征映射中的瓶颈。对比基线可能包括传统试错方法，提升幅度体现在减少了编码器选择的不确定性和时间成本。领域专家基于工具推导出两个关键实践：模式保留和特征映射原则，这为编码器选择提供了实证指导。",
            "tags_zh": [
                "量子神经网络",
                "编码器选择",
                "可视化工具",
                "可解释AI",
                "量子计算",
                "特征映射",
                "混合量子态",
                "模式保留"
            ],
            "_index": 82
        },
        {
            "title": "Spherical Voronoi: Directional Appearance as a Differentiable Partition of the Sphere",
            "authors": [
                "Francesco Di Sario",
                "Daniel Rebain",
                "Dor Verbin",
                "Marco Grangetto",
                "Andrea Tagliasacchi"
            ],
            "arxiv_id": "2512.14180v1",
            "summary": "Radiance field methods (e.g. 3D Gaussian Splatting) have emerged as a powerful paradigm for novel view synthesis, yet their appearance modeling often relies on Spherical Harmonics (SH), which impose fundamental limitations. SH struggle with high-frequency signals, exhibit Gibbs ringing artifacts, and fail to capture specular reflections - a key component of realistic rendering. Although alternatives like spherical Gaussians offer improvements, they add significant optimization complexity. We propose Spherical Voronoi (SV) as a unified framework for appearance representation in 3D Gaussian Splatting. SV partitions the directional domain into learnable regions with smooth boundaries, providing an intuitive and stable parameterization for view-dependent effects. For diffuse appearance, SV achieves competitive results while keeping optimization simpler than existing alternatives. For reflections - where SH fail - we leverage SV as learnable reflection probes, taking reflected directions as input following principles from classical graphics. This formulation attains state-of-the-art results on synthetic and real-world datasets, demonstrating that SV offers a principled, efficient, and general solution for appearance modeling in explicit 3D representations.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14180v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出球面Voronoi作为3D高斯泼溅中外观建模的统一框架，以解决球谐函数在高频信号和镜面反射方面的局限性。",
            "summary_zh": "辐射场方法（如3D高斯泼溅）已成为新视角合成的强大范式，但其外观建模通常依赖于球谐函数（SH），这带来了根本性限制。SH难以处理高频信号，会出现吉布斯振铃伪影，且无法捕捉镜面反射——这是真实感渲染的关键组成部分。尽管球面高斯等替代方法有所改进，但它们增加了显著的优化复杂性。我们提出球面Voronoi（SV）作为3D高斯泼溅中外观表示的统一框架。SV将方向域划分为具有平滑边界的可学习区域，为视角相关效应提供了直观且稳定的参数化。对于漫反射外观，SV在保持优化比现有替代方法更简单的同时，取得了有竞争力的结果。对于SH失败的反射部分，我们利用SV作为可学习的反射探针，遵循经典图形学原理，将反射方向作为输入。该公式在合成和真实世界数据集上取得了最先进的结果，表明SV为显式3D表示中的外观建模提供了一个原则性、高效且通用的解决方案。",
            "intro_zh": [
                "现有方法如球谐函数（SH）在3D高斯泼溅中处理高频信号时存在吉布斯振铃伪影，且无法有效建模镜面反射，限制了真实感渲染。",
                "论文提出球面Voronoi（SV）框架，通过将方向域划分为可学习区域来参数化外观，提供平滑边界和稳定优化，特别针对反射采用反射方向输入。",
                "实验表明，SV在合成和真实数据集上达到最先进性能，在反射建模方面显著优于SH，同时优化复杂度低于球面高斯等方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决3D高斯泼溅等辐射场方法中外观建模的局限性，特别是球谐函数（SH）在处理高频信号时产生的吉布斯振铃伪影、无法有效捕捉镜面反射，以及替代方法如球面高斯带来的高优化复杂度问题。\\n\\n**核心思路**：核心思路是引入球面Voronoi（SV）作为统一的外观表示框架，通过将方向域（即球面）划分为可学习的区域，每个区域对应不同的外观参数，从而提供平滑的边界和稳定的参数化，以更好地建模视角相关效应，包括漫反射和镜面反射。\\n\\n**技术框架**：整体框架基于3D高斯泼溅，将SV集成到外观建模中。首先，使用3D高斯表示场景几何；然后，应用SV将方向域分区，每个分区关联可学习的外观特征；对于反射部分，SV作为可学习的反射探针，输入为反射方向（遵循经典图形学原理），输出反射外观；最后，通过优化损失函数（如重建损失）联合训练几何和外观参数。\\n\\n**关键创新**：最重要的技术创新是SV作为方向域的可微分分区机制，它允许学习平滑的区域边界，避免了SH的频域限制和球面高斯的复杂优化。本质区别在于SV提供了更灵活和直观的参数化，能够直接处理反射方向，而SH依赖于球面基函数展开，导致在高频和反射场景下性能不足。\\n\\n**关键设计**：关键设计包括：SV分区的可学习边界参数，使用平滑函数（如softmax）实现可微分；外观特征与每个SV区域关联，通过插值计算任意方向的外观；损失函数通常包含光度重建损失（如L1或MSE损失）以优化新视角合成；网络结构简单，主要依赖于3D高斯泼溅的现有架构，但将SH替换为SV模块；参数设置如分区数量可调，以平衡表达能力和计算效率。",
            "application_zh": "该研究在计算机视觉和图形学领域具有广泛的应用潜力，主要用于增强3D重建和新视角合成的真实感渲染，特别是在虚拟现实、增强现实、游戏开发和电影特效中。通过改进镜面反射和高频细节的建模，SV可以提升场景的视觉保真度，未来可能扩展到动态场景或更复杂的外观效应，推动显式3D表示技术的发展。",
            "highlight_zh": "实验在合成和真实世界数据集上进行，SV在反射建模方面显著优于球谐函数（SH），具体性能数据未知，但论文报告达到最先进结果。对比基线包括SH和球面高斯等方法，SV在保持优化简单性的同时，在反射场景中表现出色，提升幅度未量化但强调为关键改进。",
            "tags_zh": [
                "球面Voronoi",
                "3D高斯泼溅",
                "外观建模",
                "新视角合成",
                "镜面反射",
                "可微分渲染",
                "辐射场方法",
                "计算机视觉"
            ],
            "_index": 83
        },
        {
            "title": "IntentMiner: Intent Inversion Attack via Tool Call Analysis in the Model Context Protocol",
            "authors": [
                "Yunhao Yao",
                "Zhiqiang Wang",
                "Haoran Cheng",
                "Yihang Cheng",
                "Haohua Du",
                "Xiang-Yang Li"
            ],
            "arxiv_id": "2512.14166v1",
            "summary": "The rapid evolution of Large Language Models (LLMs) into autonomous agents has led to the adoption of the Model Context Protocol (MCP) as a standard for discovering and invoking external tools. While this architecture decouples the reasoning engine from tool execution to enhance scalability, it introduces a significant privacy surface: third-party MCP servers, acting as semi-honest intermediaries, can observe detailed tool interaction logs outside the user's trusted boundary. In this paper, we first identify and formalize a novel privacy threat termed Intent Inversion, where a semi-honest MCP server attempts to reconstruct the user's private underlying intent solely by analyzing legitimate tool calls. To systematically assess this vulnerability, we propose IntentMiner, a framework that leverages Hierarchical Information Isolation and Three-Dimensional Semantic Analysis, integrating tool purpose, call statements, and returned results, to accurately infer user intent at the step level. Extensive experiments demonstrate that IntentMiner achieves a high degree of semantic alignment (over 85%) with original user queries, significantly outperforming baseline approaches. These results highlight the inherent privacy risks in decoupled agent architectures, revealing that seemingly benign tool execution logs can serve as a potent vector for exposing user secrets.",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "12 pages, 6 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14166v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出IntentMiner框架，通过分析模型上下文协议中的工具调用，揭示去耦合智能体架构中的意图反演隐私威胁。",
            "summary_zh": "大型语言模型（LLMs）向自主智能体的快速发展，促使模型上下文协议（MCP）成为发现和调用外部工具的标准。虽然这种架构将推理引擎与工具执行解耦以提升可扩展性，但也引入了显著的隐私暴露面：作为半诚实中介的第三方MCP服务器，可以在用户信任边界之外观察详细的工具交互日志。本文首先识别并形式化了一种新的隐私威胁，称为意图反演，即半诚实的MCP服务器仅通过分析合法的工具调用来尝试重建用户的私有底层意图。为了系统评估这一漏洞，我们提出了IntentMiner框架，该框架利用分层信息隔离和三维语义分析，整合工具目的、调用语句和返回结果，以在步骤级别准确推断用户意图。大量实验表明，IntentMiner实现了与原始用户查询的高度语义对齐（超过85%），显著优于基线方法。这些结果突显了解耦智能体架构中固有的隐私风险，揭示了看似良性的工具执行日志可以作为暴露用户秘密的有效载体。",
            "intro_zh": [
                "核心问题：模型上下文协议（MCP）作为去耦合智能体架构的标准，使第三方服务器能观察工具交互日志，导致用户意图可能被半诚实中介反演，现有方法缺乏对此隐私威胁的系统评估。",
                "方法要点：提出IntentMiner框架，通过分层信息隔离和三维语义分析，整合工具目的、调用语句和返回结果，在步骤级别准确推断用户意图，以评估意图反演漏洞。",
                "实验或效果：实验显示IntentMiner与原始用户查询的语义对齐度超过85%，显著优于基线方法，揭示了去耦合架构中工具日志作为隐私泄露载体的风险。"
            ],
            "method_zh": "**问题定义**：论文要解决的具体问题是模型上下文协议（MCP）在去耦合智能体架构中引入的隐私威胁，即意图反演攻击。现有方法的痛点在于，MCP作为标准协议，允许第三方服务器作为半诚实中介观察工具交互日志，但这些日志通常被视为良性数据，缺乏对用户私有意图可能被重建的系统性评估，导致隐私风险被低估。\\n\\n**核心思路**：论文的核心解决思路是设计一个框架来模拟和评估意图反演攻击，通过分析工具调用的语义信息来推断用户意图。这样设计是因为工具调用日志包含了丰富的上下文信息（如工具目的、调用参数和返回结果），这些信息在去耦合架构中暴露给第三方，可能被恶意利用来重建用户意图，从而揭示隐私漏洞。\\n\\n**技术框架**：整体架构包括数据收集、预处理、意图推断和评估四个阶段。主要模块有：1) 工具调用日志收集模块，从MCP服务器获取工具交互数据；2) 分层信息隔离模块，将日志分解为工具目的、调用语句和返回结果等层次；3) 三维语义分析模块，整合这些层次信息进行语义建模；4) 意图推断模块，基于分析结果在步骤级别生成用户意图描述；5) 评估模块，通过语义对齐度等指标量化攻击效果。\\n\\n**关键创新**：最重要的技术创新点是提出了分层信息隔离和三维语义分析的方法，将工具调用日志的多维度信息（工具目的、调用语句、返回结果）系统整合，以增强意图推断的准确性。与现有方法的本质区别在于，现有研究多关注工具执行的安全性，而本文首次形式化并评估了基于工具日志的意图反演攻击，突出了去耦合架构中隐私威胁的新维度。\\n\\n**关键设计**：关键设计包括：1) 分层信息隔离策略，将工具日志结构化分解，避免信息混淆；2) 三维语义分析算法，使用自然语言处理技术（如嵌入向量和相似度计算）来融合不同层次的信息；3) 步骤级推断机制，逐步骤分析工具调用序列，以重建整体意图；4) 评估指标，采用语义对齐度（如基于BERT的相似度得分）来量化推断结果与原始查询的一致性，实验设置中可能涉及基线对比（如基于简单日志分析的方法）和参数调优以优化性能。",
            "application_zh": "该研究的潜在应用领域包括智能体安全评估、隐私保护设计和协议标准化。实际价值在于帮助开发者和研究人员识别和缓解去耦合架构（如基于MCP的LLM智能体）中的隐私漏洞，通过模拟攻击来增强系统的抗意图反演能力。未来影响可能推动更安全的工具调用协议设计，促进人工智能在敏感场景（如医疗、金融）中的可信部署，并为隐私增强技术提供新方向。",
            "highlight_zh": "最重要的实验结果显示，IntentMiner在意图反演攻击中实现了超过85%的语义对齐度，显著优于基线方法（具体提升幅度未在摘要中给出，但强调“显著优于”）。这表明工具调用日志能有效暴露用户意图，揭示了去耦合智能体架构的固有隐私风险。实验可能涉及多样化的工具调用场景，以验证框架的鲁棒性和泛化能力。",
            "tags_zh": [
                "意图反演攻击",
                "模型上下文协议",
                "隐私威胁",
                "工具调用分析",
                "分层信息隔离",
                "三维语义分析",
                "智能体安全",
                "去耦合架构"
            ],
            "_index": 84
        },
        {
            "title": "LAPPI: Interactive Optimization with LLM-Assisted Preference-Based Problem Instantiation",
            "authors": [
                "So Kuroki",
                "Manami Nakagawa",
                "Shigeo Yoshida",
                "Yuki Koyama",
                "Kozuno Tadashi"
            ],
            "arxiv_id": "2512.14138v1",
            "summary": "Many real-world tasks, such as trip planning or meal planning, can be formulated as combinatorial optimization problems. However, using optimization solvers is difficult for end users because it requires problem instantiation: defining candidate items, assigning preference scores, and specifying constraints. We introduce LAPPI (LLM-Assisted Preference-based Problem Instantiation), an interactive approach that uses large language models (LLMs) to support users in this instantiation process. Through natural language conversations, the system helps users transform vague preferences into well-defined optimization problems. These instantiated problems are then passed to existing optimization solvers to generate solutions. In a user study on trip planning, our method successfully captured user preferences and generated feasible plans that outperformed both conventional and prompt-engineering approaches. We further demonstrate LAPPI's versatility by adapting it to an additional use case.",
            "categories": [
                "cs.HC",
                "cs.AI"
            ],
            "primary_category": "cs.HC",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14138v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出LAPPI方法，利用大语言模型辅助用户将模糊偏好转化为优化问题定义，解决组合优化问题实例化难题。",
            "summary_zh": "许多现实世界任务（如旅行规划或餐饮规划）可以表述为组合优化问题。然而，对于最终用户来说，使用优化求解器很困难，因为它需要问题实例化：定义候选项目、分配偏好分数和指定约束。我们引入了LAPPI（LLM辅助的基于偏好的问题实例化），这是一种交互式方法，使用大语言模型（LLMs）来支持用户在此实例化过程中。通过自然语言对话，系统帮助用户将模糊的偏好转化为明确定义的优化问题。这些实例化的问题随后被传递给现有的优化求解器以生成解决方案。在旅行规划的用户研究中，我们的方法成功捕捉了用户偏好，并生成了可行的计划，其表现优于传统方法和提示工程方法。我们通过将其适应到另一个用例进一步展示了LAPPI的通用性。",
            "intro_zh": [
                "现有方法要求用户手动定义优化问题，包括候选项、偏好和约束，这对非专业用户构成技术门槛。",
                "LAPPI通过LLM驱动的自然语言对话，交互式引导用户表达偏好，自动转化为可求解的优化问题实例。",
                "在旅行规划实验中，LAPPI生成的计划优于传统方法和提示工程，有效捕捉用户偏好并提升可行性。"
            ],
            "method_zh": "**问题定义**：论文解决组合优化问题（如旅行规划、餐饮规划）的实例化难题，即用户需要手动定义候选项目、偏好分数和约束，这对非专业用户构成技术障碍，导致优化求解器难以普及。\\n\\n**核心思路**：论文提出利用大语言模型（LLMs）作为中介，通过自然语言交互帮助用户将模糊偏好转化为结构化优化问题，从而降低实例化门槛，使优化技术更易于使用。\\n\\n**技术框架**：整体流程分为三个阶段：首先，用户通过对话与LLM交互，表达偏好和需求；其次，LLM解析对话内容，自动生成优化问题的实例化参数（如目标函数、约束条件）；最后，将实例化问题输入现有优化求解器（如整数规划求解器）生成解决方案。系统可能包含偏好提取、约束推理和问题格式化等模块。\\n\\n**关键创新**：最重要的创新是将LLM与优化求解器结合，创建交互式问题实例化框架，本质区别在于传统方法依赖用户专业知识，而LAPPI利用LLM的自然语言理解能力自动化实例化过程，提升用户友好性和效率。\\n\\n**关键设计**：具体技术细节在摘要中未详细说明，但可能涉及LLM的提示工程设计（如结构化提示引导用户对话）、偏好评分机制（如将文本偏好映射为数值分数），以及优化问题格式转换（如生成标准数学规划模型）。损失函数和网络结构未知，因方法侧重于交互流程而非深度学习模型。",
            "application_zh": "该研究可应用于旅行规划、餐饮规划、日程安排等组合优化任务，帮助非专业用户轻松制定个性化计划。实际价值在于降低优化技术使用门槛，提升决策效率；未来可能扩展到更多领域如资源分配、产品推荐，推动AI辅助决策的普及。",
            "highlight_zh": "在旅行规划用户研究中，LAPPI成功捕捉用户偏好，生成可行计划，其表现优于传统方法（如手动实例化）和提示工程方法（如直接LLM生成）。具体性能数据未知，但实验表明LAPPI在偏好匹配和可行性方面有显著提升，验证了交互式实例化的有效性。",
            "tags_zh": [
                "组合优化",
                "问题实例化",
                "大语言模型",
                "交互式系统",
                "偏好建模",
                "自然语言处理",
                "优化求解器",
                "用户研究"
            ],
            "_index": 85
        },
        {
            "title": "UIXPOSE: Mobile Malware Detection via Intention-Behaviour Discrepancy Analysis",
            "authors": [
                "Amirmohammad Pasdar",
                "Toby Murray",
                "Van-Thuan Pham"
            ],
            "arxiv_id": "2512.14130v1",
            "summary": "We introduce UIXPOSE, a source-code-agnostic framework that operates on both compiled and open-source apps. This framework applies Intention Behaviour Alignment (IBA) to mobile malware analysis, aligning UI-inferred intent with runtime semantics. Previous work either infers intent statically, e.g., permission-centric, or widget-level or monitors coarse dynamic signals (endpoints, partial resource usage) that miss content and context. UIXPOSE infers an intent vector from each screen using vision-language models and knowledge structures and combines decoded network payloads, heap/memory signals, and resource utilisation traces into a behaviour vector. Their alignment, calculated at runtime, can both detect misbehaviour and highlight exploration of behaviourally rich paths. In three real-world case studies, UIXPOSE reveals covert exfiltration and hidden background activity that evade metadata-only baselines, demonstrating how IBA improves dynamic detection.",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "15 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14130v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出UIXPOSE框架，通过意图-行为差异分析解决移动恶意软件检测问题",
            "summary_zh": "我们介绍了UIXPOSE，这是一个与源代码无关的框架，适用于编译和开源应用程序。该框架将意图行为对齐（IBA）应用于移动恶意软件分析，将UI推断的意图与运行时语义对齐。先前的工作要么静态推断意图（例如，以权限为中心或小部件级别），要么监控粗略的动态信号（端点、部分资源使用），这些方法会遗漏内容和上下文。UIXPOSE使用视觉语言模型和知识结构从每个屏幕推断意图向量，并将解码的网络负载、堆/内存信号和资源利用跟踪组合成行为向量。在运行时计算它们的对齐度，既可以检测不当行为，又可以突出显示行为丰富路径的探索。在三个真实案例研究中，UIXPOSE揭示了仅基于元数据的基线无法检测到的隐蔽数据外泄和隐藏后台活动，展示了IBA如何改进动态检测。",
            "intro_zh": [
                "现有方法在移动恶意软件检测中，静态推断意图（如权限分析）或监控粗略动态信号（如端点调用），无法捕捉完整内容和上下文，导致检测盲区。",
                "UIXPOSE提出意图行为对齐（IBA）方法，通过视觉语言模型从UI推断意图向量，结合运行时行为向量，动态分析意图与行为的一致性差异。",
                "在真实案例中，UIXPOSE成功检测到隐蔽数据外泄和后台活动，相比仅基于元数据的基线方法，显著提升了动态检测能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决移动恶意软件检测中，现有方法无法有效对齐应用意图与运行时行为的问题。现有方法的痛点包括：静态推断意图（如基于权限或小部件）忽略动态上下文，动态监控仅关注粗略信号（如端点或资源使用），导致检测盲区，无法捕捉恶意软件的隐蔽活动。\\n\\n**核心思路**：论文的核心解决思路是引入意图行为对齐（IBA），通过对比UI推断的意图与运行时行为，分析其差异来检测恶意行为。这样设计是因为恶意软件常伪装意图，而运行时行为可能偏离正常模式，对齐分析能更全面地捕捉异常。\\n\\n**技术框架**：整体架构包括两个主要阶段：意图推断和行为收集。意图推断阶段使用视觉语言模型和知识结构，从应用屏幕生成意图向量；行为收集阶段解码网络负载、监控堆/内存信号和资源利用跟踪，形成行为向量。运行时计算两个向量的对齐度，以检测不一致性并探索行为丰富路径。\\n\\n**关键创新**：最重要的技术创新点是首次将意图行为对齐（IBA）应用于移动恶意软件分析，结合视觉语言模型进行意图推断，实现源代码无关的动态检测。与现有方法的本质区别在于，它同时考虑UI内容和运行时语义，而非仅依赖静态或粗略动态信号。\\n\\n**关键设计**：关键设计包括使用视觉语言模型（具体模型未知）从屏幕提取意图，知识结构（具体结构未知）辅助推断；行为向量整合网络负载解码（解码方法未知）、堆/内存监控（监控工具未知）和资源跟踪（跟踪机制未知）；对齐计算在运行时进行，具体算法未知，但强调能检测差异并引导路径探索。",
            "application_zh": "该研究在移动安全领域具有重要应用价值，可用于安卓和iOS等移动平台的恶意软件检测，帮助安全分析师识别隐蔽攻击和数据外泄。未来可扩展至物联网设备或桌面应用的安全监控，提升动态行为分析的精度，推动意图驱动安全检测的发展。",
            "highlight_zh": "在三个真实案例研究中，UIXPOSE成功检测到仅基于元数据的基线方法无法发现的隐蔽数据外泄和隐藏后台活动。具体性能数据未知，但实验表明意图行为对齐（IBA）显著提升了动态检测能力，能够揭示恶意软件的复杂行为模式，验证了框架的有效性和实用性。",
            "tags_zh": [
                "移动恶意软件检测",
                "意图行为对齐",
                "视觉语言模型",
                "动态分析",
                "运行时语义",
                "UI推断",
                "源代码无关框架",
                "安全监控"
            ],
            "_index": 86
        },
        {
            "title": "Interactive Motion Planning for Human-Robot Collaboration Based on Human-Centric Configuration Space Ergonomic Field",
            "authors": [
                "Chenzui Li",
                "Yiming Chen",
                "Xi Wu",
                "Tao Teng",
                "Sylvain Calinon",
                "Darwin Caldwell",
                "Fei Chen"
            ],
            "arxiv_id": "2512.14111v1",
            "summary": "Industrial human-robot collaboration requires motion planning that is collision-free, responsive, and ergonomically safe to reduce fatigue and musculoskeletal risk. We propose the Configuration Space Ergonomic Field (CSEF), a continuous and differentiable field over the human joint space that quantifies ergonomic quality and provides gradients for real-time ergonomics-aware planning. An efficient algorithm constructs CSEF from established metrics with joint-wise weighting and task conditioning, and we integrate it into a gradient-based planner compatible with impedance-controlled robots. In a 2-DoF benchmark, CSEF-based planning achieves higher success rates, lower ergonomic cost, and faster computation than a task-space ergonomic planner. Hardware experiments with a dual-arm robot in unimanual guidance, collaborative drilling, and bimanual cocarrying show faster ergonomic cost reduction, closer tracking to optimized joint targets, and lower muscle activation than a point-to-point baseline. CSEF-based planning method reduces average ergonomic scores by up to 10.31% for collaborative drilling tasks and 5.60% for bimanual co-carrying tasks while decreasing activation in key muscle groups, indicating practical benefits for real-world deployment.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "10 pages, 9 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14111v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "物理动作",
                    "matched_keywords": [
                        "musculoskeletal"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出基于人机协作的配置空间人机工程场，实现实时人机工程学感知的运动规划",
            "summary_zh": "工业人机协作需要无碰撞、响应迅速且符合人机工程学安全的运动规划，以减少疲劳和肌肉骨骼风险。我们提出了配置空间人机工程场（CSEF），这是一个在人体关节空间上的连续可微场，用于量化人机工程学质量并提供实时人机工程学感知规划的梯度。我们开发了一种高效算法，通过关节加权和任务条件从现有指标构建CSEF，并将其集成到与阻抗控制机器人兼容的基于梯度的规划器中。在2自由度基准测试中，基于CSEF的规划比任务空间人机工程学规划器实现了更高的成功率、更低的人机工程学成本和更快的计算速度。在单臂引导、协作钻孔和双臂协同搬运的硬件实验中，与点到点基线相比，基于CSEF的规划显示出更快的人机工程学成本降低、更接近优化关节目标的跟踪以及更低的肌肉激活。对于协作钻孔任务，基于CSEF的规划方法将平均人机工程学分数降低了高达10.31%，对于双臂协同搬运任务降低了5.60%，同时减少了关键肌肉群的激活，表明在实际部署中具有实用价值。",
            "intro_zh": [
                "现有工业人机协作运动规划方法在实时性和人机工程学优化方面存在不足，难以平衡碰撞避免、响应速度和人体疲劳降低。",
                "论文提出配置空间人机工程场（CSEF），通过连续可微场量化人体关节空间的人机工程学质量，并利用梯度进行实时规划。",
                "实验表明，基于CSEF的规划在2-DoF基准测试中成功率更高、成本更低、计算更快，硬件实验中人机工程学分数降低达10.31%，肌肉激活减少。"
            ],
            "method_zh": "**问题定义**：论文旨在解决工业人机协作中运动规划的实时性和人机工程学优化问题。现有方法如任务空间人机工程学规划器在计算效率和人机工程学成本降低方面存在局限，难以在动态环境中实现快速响应和人体疲劳最小化。\\n\\n**核心思路**：论文的核心思路是构建一个在人体关节空间上的连续可微场——配置空间人机工程场（CSEF），直接量化人机工程学质量，并通过梯度信息指导机器人运动规划，以实现实时优化。这样设计是因为关节空间表示更直接关联人体姿态，便于集成现有人机工程学指标，并利用梯度下降进行高效规划。\\n\\n**技术框架**：整体架构包括两个主要阶段：首先，通过高效算法从现有人机工程学指标（如关节角度限制、任务条件）构建CSEF，涉及关节加权和任务条件处理；其次，将CSEF集成到基于梯度的运动规划器中，该规划器与阻抗控制机器人兼容，实时计算梯度以调整机器人运动，确保无碰撞和人机工程学优化。\\n\\n**关键创新**：最重要的技术创新是CSEF的提出，它是一个连续可微的场，直接在配置空间（人体关节空间）上定义人机工程学质量，并提供梯度用于实时规划。与现有方法（如任务空间规划器）的本质区别在于，CSEF避免了复杂的空间转换，提高了计算效率和优化精度，同时更好地捕捉人体姿态的动态变化。\\n\\n**关键设计**：关键设计包括：CSEF的构建算法，它结合了关节加权（根据关节重要性分配权重）和任务条件（适应特定协作任务）；基于梯度的规划器，使用CSEF的梯度信息进行实时运动调整；与阻抗控制机器人的集成，确保柔顺交互。具体参数如权重设置和梯度计算细节在论文中基于实验优化，但未详细公开所有数值。",
            "application_zh": "该研究主要应用于工业人机协作场景，如装配线、制造业和物流领域，其中机器人需要与人类工人紧密协作执行任务如钻孔、搬运和引导。实际价值在于通过实时人机工程学优化，减少工人疲劳和肌肉骨骼风险，提高生产安全性和效率。未来影响可能扩展到医疗康复、服务机器人等领域，推动更智能、人性化的机器人交互系统发展。",
            "highlight_zh": "最重要的实验结果包括：在2-DoF基准测试中，基于CSEF的规划相比任务空间人机工程学规划器，成功率更高、人机工程学成本更低、计算速度更快。硬件实验中，对于协作钻孔任务，平均人机工程学分数降低高达10.31%；对于双臂协同搬运任务，降低5.60%。同时，关键肌肉群的激活显著减少，表明在实际部署中能有效降低人体负荷。对比基线为点到点规划方法，基于CSEF的规划在跟踪优化目标和成本降低方面表现更优。",
            "tags_zh": [
                "人机协作",
                "运动规划",
                "人机工程学",
                "配置空间",
                "梯度优化",
                "实时系统",
                "工业机器人",
                "肌肉激活分析"
            ],
            "_index": 87
        },
        {
            "title": "OUSAC: Optimized Guidance Scheduling with Adaptive Caching for DiT Acceleration",
            "authors": [
                "Ruitong Sun",
                "Tianze Yang",
                "Wei Niu",
                "Jin Sun"
            ],
            "arxiv_id": "2512.14096v1",
            "summary": "Diffusion models have emerged as the dominant paradigm for high-quality image generation, yet their computational expense remains substantial due to iterative denoising. Classifier-Free Guidance (CFG) significantly enhances generation quality and controllability but doubles the computation by requiring both conditional and unconditional forward passes at every timestep. We present OUSAC (Optimized gUidance Scheduling with Adaptive Caching), a framework that accelerates diffusion transformers (DiT) through systematic optimization. Our key insight is that variable guidance scales enable sparse computation: adjusting scales at certain timesteps can compensate for skipping CFG at others, enabling both fewer total sampling steps and fewer CFG steps while maintaining quality. However, variable guidance patterns introduce denoising deviations that undermine standard caching methods, which assume constant CFG scales across steps. Moreover, different transformer blocks are affected at different levels under dynamic conditions. This paper develops a two-stage approach leveraging these insights. Stage-1 employs evolutionary algorithms to jointly optimize which timesteps to skip and what guidance scale to use, eliminating up to 82% of unconditional passes. Stage-2 introduces adaptive rank allocation that tailors calibration efforts per transformer block, maintaining caching effectiveness under variable guidance. Experiments demonstrate that OUSAC significantly outperforms state-of-the-art acceleration methods, achieving 53% computational savings with 15% quality improvement on DiT-XL/2 (ImageNet 512x512), 60% savings with 16.1% improvement on PixArt-alpha (MSCOCO), and 5x speedup on FLUX while improving CLIP Score over the 50-step baseline.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "29 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14096v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "SAC"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出OUSAC框架，通过优化引导调度与自适应缓存加速扩散变换器，解决CFG计算开销大的问题。",
            "summary_zh": "扩散模型已成为高质量图像生成的主导范式，但由于迭代去噪过程，其计算开销仍然很大。无分类器引导（CFG）显著提升了生成质量和可控性，但需要在每个时间步同时进行条件前向传播和无条件前向传播，使计算量加倍。本文提出了OUSAC（优化引导调度与自适应缓存）框架，通过系统优化加速扩散变换器（DiT）。我们的核心洞察是：可变的引导尺度可以实现稀疏计算——在某些时间步调整引导尺度可以补偿在其他时间步跳过CFG的操作，从而在保持质量的同时减少总采样步数和CFG步数。然而，可变的引导模式会引入去噪偏差，破坏标准缓存方法的有效性，因为标准方法假设CFG尺度在步间恒定。此外，在动态条件下，不同的变换器块受到的影响程度不同。本文基于这些洞察开发了一种两阶段方法。第一阶段采用进化算法联合优化跳过哪些时间步以及使用什么引导尺度，最多可消除82%的无条件前向传播。第二阶段引入自适应秩分配，为每个变换器块定制校准工作，在可变引导下保持缓存有效性。实验表明，OUSAC显著优于最先进的加速方法：在DiT-XL/2（ImageNet 512x512）上实现53%的计算节省和15%的质量提升，在PixArt-alpha（MSCOCO）上实现60%的节省和16.1%的提升，在FLUX上实现5倍加速，同时CLIP分数超过50步基线。",
            "intro_zh": [
                "核心问题：CFG虽提升扩散模型质量，但需双倍计算，现有缓存方法在可变引导下失效，导致加速与质量难以兼顾。",
                "方法要点：提出两阶段框架，先优化引导调度减少CFG步数，再自适应缓存补偿偏差，实现高效加速。",
                "实验或效果：在多个模型上显著节省计算并提升质量，如DiT-XL/2节省53%计算、质量提升15%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决扩散变换器（DiT）中无分类器引导（CFG）带来的高计算开销问题。现有方法如标准缓存假设CFG尺度恒定，但在可变引导模式下，去噪偏差会破坏缓存有效性，导致加速与质量难以平衡，成为实际部署的瓶颈。\\n\\n**核心思路**：核心思路是利用可变引导尺度实现稀疏计算，通过在某些时间步跳过CFG并用调整的尺度补偿，减少计算量。同时，针对动态条件引入自适应机制，优化缓存策略以维持质量，从而系统性地加速DiT推理。\\n\\n**技术框架**：整体框架分为两阶段。第一阶段使用进化算法联合优化时间步跳过策略和引导尺度分配，最小化CFG步数；第二阶段基于自适应秩分配，为每个变换器块动态调整校准资源，确保缓存在不同引导模式下的有效性。两阶段协同工作，实现端到端加速。\\n\\n**关键创新**：最重要的创新是可变引导调度与自适应缓存的结合。与现有方法（如固定调度或简单缓存）相比，本质区别在于动态优化引导模式并针对块级差异定制缓存，解决了偏差累积问题，实现了计算效率和质量的双重提升。\\n\\n**关键设计**：关键设计包括：进化算法用于优化调度，目标函数平衡计算节省和质量损失；自适应秩分配基于块敏感度分析，动态分配校准秩；实验设置涵盖DiT-XL/2、PixArt-alpha等模型，使用CLIP分数等指标评估，具体参数如最多消除82%无条件前向传播。",
            "application_zh": "该研究可应用于需要高效高质量图像生成的领域，如内容创作、游戏开发、广告设计和虚拟现实。通过加速扩散变换器，OUSAC能降低计算成本，促进实时或大规模部署，提升AI生成内容的实用性和可访问性，对推动生成式AI的产业化具有重要价值。",
            "highlight_zh": "实验亮点：OUSAC在多个基准上显著优于现有加速方法。具体数据包括：在DiT-XL/2（ImageNet 512x512）上实现53%计算节省和15%质量提升；在PixArt-alpha（MSCOCO）上节省60%计算、提升16.1%质量；在FLUX上达到5倍加速，CLIP分数超过50步基线。这些结果证明了其高效性和泛化能力。",
            "tags_zh": [
                "扩散模型加速",
                "无分类器引导优化",
                "变换器缓存",
                "进化算法调度",
                "自适应秩分配",
                "图像生成效率",
                "计算节省",
                "质量提升"
            ],
            "_index": 88
        },
        {
            "title": "GaussianPlant: Structure-aligned Gaussian Splatting for 3D Reconstruction of Plants",
            "authors": [
                "Yang Yang",
                "Risa Shinoda",
                "Hiroaki Santo",
                "Fumio Okura"
            ],
            "arxiv_id": "2512.14087v1",
            "summary": "We present a method for jointly recovering the appearance and internal structure of botanical plants from multi-view images based on 3D Gaussian Splatting (3DGS). While 3DGS exhibits robust reconstruction of scene appearance for novel-view synthesis, it lacks structural representations underlying those appearances (e.g., branching patterns of plants), which limits its applicability to tasks such as plant phenotyping. To achieve both high-fidelity appearance and structural reconstruction, we introduce GaussianPlant, a hierarchical 3DGS representation, which disentangles structure and appearance. Specifically, we employ structure primitives (StPs) to explicitly represent branch and leaf geometry, and appearance primitives (ApPs) to the plants' appearance using 3D Gaussians. StPs represent a simplified structure of the plant, i.e., modeling branches as cylinders and leaves as disks. To accurately distinguish the branches and leaves, StP's attributes (i.e., branches or leaves) are optimized in a self-organized manner. ApPs are bound to each StP to represent the appearance of branches or leaves as in conventional 3DGS. StPs and ApPs are jointly optimized using a re-rendering loss on the input multi-view images, as well as the gradient flow from ApP to StP using the binding correspondence information. We conduct experiments to qualitatively evaluate the reconstruction accuracy of both appearance and structure, as well as real-world experiments to qualitatively validate the practical performance. Experiments show that the GaussianPlant achieves both high-fidelity appearance reconstruction via ApPs and accurate structural reconstruction via StPs, enabling the extraction of branch structure and leaf instances.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Submitted to IEEE TPAMI, under review",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14087v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "深度估计",
                    "matched_keywords": [
                        "3D reconstruction"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出GaussianPlant方法，通过解耦结构和外观的高斯溅射表示，解决植物三维重建中结构信息缺失的问题。",
            "summary_zh": "我们提出了一种基于三维高斯溅射（3DGS）的方法，用于从多视角图像中联合恢复植物外观和内部结构。虽然3DGS在新视角合成中表现出强大的场景外观重建能力，但缺乏支撑这些外观的结构表示（例如植物的分枝模式），这限制了其在植物表型分析等任务中的应用。为了实现高保真外观和结构重建，我们引入了GaussianPlant，这是一种分层3DGS表示，解耦了结构和外观。具体来说，我们使用结构基元（StPs）显式表示枝干和叶片的几何形状，并使用外观基元（ApPs）通过三维高斯表示植物的外观。StPs表示植物的简化结构，即将枝干建模为圆柱体，叶片建模为圆盘。为了准确区分枝干和叶片，StP的属性（即枝干或叶片）以自组织方式优化。ApPs绑定到每个StP，以像传统3DGS那样表示枝干或叶片的外观。StPs和ApPs通过输入多视角图像的重渲染损失以及使用绑定对应信息从ApP到StP的梯度流进行联合优化。我们进行了实验，定性和定量评估外观和结构的重建准确性，以及实际实验定性验证实际性能。实验表明，GaussianPlant通过ApPs实现了高保真外观重建，通过StPs实现了准确结构重建，从而能够提取枝干结构和叶片实例。",
            "intro_zh": [
                "现有3DGS方法在植物重建中缺乏结构表示，限制了其在表型分析等任务的应用。",
                "提出分层3DGS表示，通过结构基元和外观基元解耦结构与外观，实现联合优化。",
                "实验验证了GaussianPlant在植物外观和结构重建上的高保真性能，支持枝干和叶片提取。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从多视角图像中重建植物三维模型时，现有3DGS方法缺乏结构表示的问题，导致无法准确捕捉植物的分枝模式和叶片实例，限制了在植物表型分析等领域的应用。\\n\\n**核心思路**：通过引入分层3DGS表示，将植物的结构和外观解耦，使用结构基元（StPs）显式建模几何结构，外观基元（ApPs）负责外观细节，实现联合优化以同时获得高保真外观和准确结构。\\n\\n**技术框架**：整体流程包括：1）从多视角图像初始化StPs和ApPs；2）StPs建模枝干为圆柱体、叶片为圆盘，属性自组织优化；3）ApPs绑定到StPs，使用3D高斯表示外观；4）通过重渲染损失和梯度流联合优化StPs和ApPs，最终输出解耦的结构和外观模型。\\n\\n**关键创新**：最重要的创新是提出结构对齐的高斯溅射表示，通过StPs和ApPs的分层设计，首次在3DGS框架中实现植物结构和外观的显式解耦与联合重建，克服了传统方法结构信息缺失的痛点。\\n\\n**关键设计**：关键设计包括：1）StPs使用简化几何（圆柱体和圆盘）表示结构，属性通过自组织方式优化以区分枝干和叶片；2）ApPs绑定到StPs，继承结构信息；3）损失函数结合重渲染损失（基于输入图像）和从ApP到StP的梯度流，利用绑定对应信息促进结构优化；4）优化过程联合更新StPs和ApPs参数，确保结构和外观一致性。",
            "application_zh": "该研究在植物表型分析、农业监测、生态研究和虚拟植物建模等领域具有潜在应用价值。通过提供高保真外观和准确结构重建，GaussianPlant能够支持植物生长分析、病害检测和自动化农业管理，提升植物科学研究和农业技术的效率与精度。",
            "highlight_zh": "实验结果表明，GaussianPlant在植物三维重建中实现了高保真外观和准确结构重建。具体地，通过ApPs获得的外观重建质量与传统3DGS相当，而通过StPs提取的枝干结构和叶片实例在定性评估中表现出色，验证了方法在联合优化中的有效性。实际实验进一步定性验证了其在实际场景中的稳健性能，支持植物表型分析任务。",
            "tags_zh": [
                "三维高斯溅射",
                "植物三维重建",
                "结构外观解耦",
                "多视角图像",
                "植物表型分析",
                "分层表示",
                "联合优化",
                "几何建模"
            ],
            "_index": 89
        },
        {
            "title": "RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees",
            "authors": [
                "Junjie Ma",
                "Jinlong Li"
            ],
            "arxiv_id": "2512.14069v1",
            "summary": "Inference with modern Large Language Models (LLMs) is expensive and slow, and speculative sampling has emerged as an effective solution to this problem, however, the number of the calls to the draft model for generating candidate tokens in speculative sampling is a preset hyperparameter, lacking flexibility. To generate and utilize the candidate tokens more effectively, we propose RADAR, a novel speculative sampling method with RL-based dynamic draft trees. RADAR formulates the draft tree generation process as a Markov Decision Process (MDP) and employs offline reinforcement learning to train a prediction model, which enables real-time decision on the calls to the draft model, reducing redundant computations and further accelerating inference. Evaluations across three LLMs and four tasks show that RADAR achieves a speedup of 3.17x-4.82x over the auto-regressive decoding baseline. The code is available at https://github.com/minaduki-sora/RADAR.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "5 pages, 2 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14069v1",
            "code_links": [
                {
                    "url": "https://github.com/minaduki-sora/RADAR",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RL"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出基于强化学习的动态草稿树方法RADAR，以加速大语言模型推理过程",
            "summary_zh": "现代大语言模型（LLM）的推理过程成本高昂且速度缓慢，推测采样已成为解决此问题的有效方案。然而，推测采样中用于生成候选标记的草稿模型调用次数是一个预设的超参数，缺乏灵活性。为了更有效地生成和利用候选标记，我们提出了RADAR，一种基于强化学习动态草稿树的新型推测采样方法。RADAR将草稿树生成过程建模为马尔可夫决策过程（MDP），并采用离线强化学习训练预测模型，从而实现对草稿模型调用的实时决策，减少冗余计算并进一步加速推理。在三个LLM和四个任务上的评估表明，RADAR相比自回归解码基线实现了3.17倍至4.82倍的加速。代码可在https://github.com/minaduki-sora/RADAR获取。",
            "intro_zh": [
                "现有推测采样方法中草稿模型调用次数固定，缺乏灵活性，导致计算冗余和效率低下。",
                "RADAR将草稿树生成建模为MDP，利用离线强化学习训练预测模型，动态决定草稿模型调用。",
                "实验显示RADAR在多个LLM和任务上实现3.17x-4.82x加速，显著提升推理效率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大语言模型推理速度慢、成本高的问题。现有推测采样方法通过草稿模型生成候选标记来加速推理，但草稿模型调用次数是预设超参数，缺乏动态调整能力，导致计算冗余和效率低下。\\n\\n**核心思路**：论文提出将草稿树生成过程建模为马尔可夫决策过程（MDP），利用离线强化学习训练预测模型，实时决策草稿模型调用，从而动态生成草稿树，减少冗余计算，优化推理效率。\\n\\n**技术框架**：RADAR的整体架构包括草稿模型、目标模型和强化学习预测模型。流程分为离线训练和在线推理两个阶段：离线阶段使用历史数据训练预测模型，学习草稿树生成策略；在线阶段根据预测模型动态决定草稿模型调用，生成候选标记序列，并通过目标模型验证和接受有效标记。\\n\\n**关键创新**：最重要的技术创新是将草稿树生成过程形式化为MDP，并引入离线强化学习进行动态决策。与现有推测采样方法的本质区别在于，RADAR不再依赖固定调用次数，而是根据上下文实时调整草稿模型调用，实现更灵活的候选标记生成。\\n\\n**关键设计**：关键设计包括：将草稿树生成状态定义为当前上下文和已生成标记，动作定义为是否调用草稿模型，奖励函数基于加速比和计算成本；使用离线强化学习算法（如保守Q学习）训练预测模型，避免在线探索开销；网络结构可能基于Transformer或MLP，具体细节未知，需参考代码实现。",
            "application_zh": "RADAR可广泛应用于需要高效大语言模型推理的场景，如聊天机器人、代码生成、文本摘要和机器翻译等。其实际价值在于显著降低推理延迟和计算成本，提升用户体验和系统吞吐量。未来可能影响边缘计算和实时AI应用，推动LLM在资源受限环境中的部署。",
            "highlight_zh": "RADAR在三个大语言模型和四个任务上的评估显示，相比自回归解码基线，实现了3.17倍至4.82倍的加速。具体性能数据表明，该方法在不同模型规模（如GPT-3变体）和任务（如文本生成、问答）上均保持高效，提升幅度显著，验证了动态草稿树的有效性。",
            "tags_zh": [
                "大语言模型推理加速",
                "推测采样",
                "强化学习",
                "动态草稿树",
                "马尔可夫决策过程",
                "离线强化学习",
                "计算优化",
                "实时决策"
            ],
            "_index": 90
        },
        {
            "title": "OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value",
            "authors": [
                "Mengzhang Cai",
                "Xin Gao",
                "Yu Li",
                "Honglin Lin",
                "Zheng Liu",
                "Zhuoshi Pan",
                "Qizhi Pei",
                "Xiaoran Shang",
                "Mengyuan Sun",
                "Zinan Tang",
                "Xiaoyang Wang",
                "Zhanping Zhong",
                "Yun Zhu",
                "Dahua Lin",
                "Conghui He",
                "Lijun Wu"
            ],
            "arxiv_id": "2512.14051v1",
            "summary": "The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14051v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO",
                        "VIO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出OpenDataArena平台以解决大语言模型后训练数据集评估不透明和缺乏系统性基准的问题",
            "summary_zh": "大语言模型的快速发展依赖于后训练数据集的质量和多样性。然而，当前存在一个关键矛盾：模型经过严格基准测试，而支撑它们的数据却是一个黑箱——其组成不透明、来源不确定且缺乏系统性评估。这种不透明性阻碍了可重复性，并模糊了数据特征与模型行为之间的因果关系。为弥补这一差距，我们引入了OpenDataArena，这是一个全面开放的平台，旨在基准测试后训练数据的内在价值。ODA建立了一个全面的生态系统，包含四个关键支柱：（i）一个统一的训练-评估流程，确保在不同模型和领域之间进行公平、开放的比较；（ii）一个多维评分框架，从数十个不同维度分析数据质量；（iii）一个交互式数据谱系探索器，可视化数据集谱系并剖析组件来源；（iv）一个完全开源的工具包，用于训练、评估和评分，以促进数据研究。在ODA上进行的大量实验——涵盖多个领域的120多个训练数据集、22个基准测试，通过超过600次训练运行和4000万个处理数据点进行验证——揭示了非平凡的见解。我们的分析揭示了数据复杂性与任务性能之间的内在权衡，通过谱系追踪识别了流行基准中的冗余，并绘制了数据集之间的谱系关系。我们发布所有结果、工具和配置，以普及高质量数据评估的访问。ODA不仅仅扩展排行榜，而是设想从试错式数据管理转向以数据为中心的人工智能的原则性科学，为数据混合规律和基础模型战略组成的严格研究铺平道路。",
            "intro_zh": [
                "核心问题：大语言模型后训练数据集评估不透明，缺乏系统性基准，阻碍可重复性和数据-性能因果分析。",
                "方法要点：构建OpenDataArena平台，集成统一训练-评估流程、多维评分框架、数据谱系探索器和开源工具包。",
                "实验或效果：覆盖120+数据集和22个基准，揭示数据复杂性-性能权衡，识别冗余，并绘制数据集谱系关系。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大语言模型后训练数据集评估不透明的问题，现有方法缺乏系统性基准，导致数据组成、来源和内在价值难以量化，阻碍了数据驱动的模型优化和可重复研究。\\n\\n**核心思路**：通过构建一个全面开放的基准测试平台，将数据集评估从黑箱转变为可量化、可比较的科学过程，强调公平性、开放性和多维度分析，以促进数据为中心的人工智能研究。\\n\\n**技术框架**：整体架构包括四个核心模块：统一训练-评估流程（支持多种模型如Llama、Qwen的公平比较）、多维评分框架（从数十个维度评估数据质量）、交互式数据谱系探索器（可视化数据集来源和关系）、开源工具包（提供训练、评估和评分工具）。\\n\\n**关键创新**：最重要的技术创新是整合了数据谱系追踪和多维评分，与现有方法相比，本质区别在于从单纯模型性能评估转向数据内在价值的系统性基准测试，强调数据透明度和可解释性。\\n\\n**关键设计**：关键设计包括统一的训练配置（如超参数设置）、多维评分指标（涵盖质量、多样性、复杂性等维度）、谱系可视化算法（用于追踪数据来源和混合关系），以及开源工具包的模块化设计，便于社区扩展和复现。",
            "application_zh": "该研究可应用于大语言模型数据管理、数据集质量评估、模型训练优化等领域，实际价值在于提升数据透明度和可重复性，促进数据为中心的人工智能发展，未来可能影响数据混合策略、基础模型构建和AI伦理研究。",
            "highlight_zh": "实验覆盖120多个训练数据集和22个基准测试，通过600多次训练运行和4000万个数据点验证，揭示了数据复杂性与任务性能之间的内在权衡，识别了流行基准中的冗余，并成功绘制了数据集之间的谱系关系，为数据评估提供了实证基础。",
            "tags_zh": [
                "后训练数据集评估",
                "数据为中心人工智能",
                "大语言模型基准测试",
                "数据谱系追踪",
                "多维评分框架",
                "开源工具包",
                "数据透明度",
                "模型可重复性"
            ],
            "_index": 91
        },
        {
            "title": "Evaluating Small Language Models for Agentic On-Farm Decision Support Systems",
            "authors": [
                "Enhong Liu",
                "Haiyu Yang",
                "Miel Hostens"
            ],
            "arxiv_id": "2512.14043v1",
            "summary": "Large Language Models (LLM) hold potential to support dairy scholars and farmers by supporting decision-making and broadening access to knowledge for stakeholders with limited technical expertise. However, the substantial computational demand restricts access to LLM almost exclusively through cloud-based service, which makes LLM-based decision support tools impractical for dairy farming. To address this gap, lightweight alternatives capable of running locally on farm hardware are required. In this work, we benchmarked 20 open-source Small Language Models (SLM) available on HuggingFace under farm-realistic computing constraints. Building on our prior work, we developed an agentic AI system that integrates five task-specific agents: literature search, web search, SQL database interaction, NoSQL database interaction, and graph generation following predictive models. Evaluation was conducted in two phases. In the first phase, five test questions were used for the initial screening to identify models capable of following basic dairy-related instructions and performing reliably in a compute-constrained environment. Models that passed this preliminary stage were then evaluated using 30 questions (five per task category mentioned above, plus one category addressing integrity and misconduct) in phase two. In results, Qwen-4B achieved superior performance across most of task categories, although showed unstable effectiveness in NoSQL database interactions through PySpark. To our knowledge, this is the first work explicitly evaluating the feasibility of SLM as engines for dairy farming decision-making, with central emphases on privacy and computational efficiency. While results highlight the promise of SLM-assisted tools for practical deployment in dairy farming, challenges remain, and fine-tuning is still needed to refine SLM performance in dairy-specific questions.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14043v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "世界模型",
                    "matched_keywords": [
                        "predictive model"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "评估小型语言模型作为乳业农场决策支持系统的可行性，以解决云计算依赖和隐私问题。",
            "summary_zh": "大型语言模型（LLM）有潜力通过支持决策制定和为技术专业知识有限的利益相关者拓宽知识获取途径来支持乳业学者和农民。然而，巨大的计算需求几乎完全限制了通过基于云的服务访问LLM，这使得基于LLM的决策支持工具在乳业养殖中不切实际。为了解决这一差距，需要能够在农场硬件上本地运行的轻量级替代方案。在这项工作中，我们在农场现实计算约束下，对HuggingFace上可用的20个开源小型语言模型（SLM）进行了基准测试。基于我们之前的工作，我们开发了一个代理AI系统，该系统集成了五个特定任务的代理：文献搜索、网络搜索、SQL数据库交互、NoSQL数据库交互以及遵循预测模型的图生成。评估分两个阶段进行。在第一阶段，使用五个测试问题进行初步筛选，以识别能够遵循基本乳业相关指令并在计算受限环境中可靠运行的模型。通过此初步阶段的模型随后在第二阶段使用30个问题（上述每个任务类别五个，加上一个解决完整性和不当行为的类别）进行评估。在结果中，Qwen-4B在大多数任务类别中实现了卓越性能，尽管通过PySpark在NoSQL数据库交互中显示出不稳定的有效性。据我们所知，这是第一项明确评估SLM作为乳业养殖决策引擎可行性的工作，重点关注隐私和计算效率。虽然结果突出了SLM辅助工具在乳业养殖中实际部署的前景，但挑战仍然存在，并且仍需要微调以改进SLM在乳业特定问题中的性能。",
            "intro_zh": [
                "核心问题：大型语言模型依赖云计算，计算需求大，导致在乳业农场部署不切实际，且存在隐私风险。",
                "方法要点：评估20个开源小型语言模型在农场硬件约束下的性能，构建集成五个任务代理的AI系统，以支持本地决策。",
                "实验或效果：Qwen-4B在多数任务中表现最佳，但NoSQL交互不稳定，整体验证了SLM在乳业决策中的可行性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型（LLM）在乳业农场决策支持系统中因依赖云计算而导致的计算成本高、延迟大和隐私泄露风险的问题。现有方法的痛点在于LLM通常需要云端部署，不适合农场本地硬件，限制了实时决策和隐私保护。\\n\\n**核心思路**：论文的核心解决思路是评估小型语言模型（SLM）作为轻量级替代方案，能够在农场本地硬件上运行，以降低计算需求并增强隐私。设计基于代理AI系统，集成多个任务代理，实现多功能决策支持。\\n\\n**技术框架**：整体架构包括两个主要阶段：第一阶段为初步筛选，使用五个测试问题评估SLM在乳业指令遵循和计算约束下的基本能力；第二阶段为详细评估，使用30个问题（覆盖五个任务类别：文献搜索、网络搜索、SQL数据库交互、NoSQL数据库交互、图生成，以及一个完整性类别）对通过筛选的模型进行性能测试。系统集成了五个任务代理，每个代理处理特定任务，通过SLM引擎驱动。\\n\\n**关键创新**：最重要的技术创新点是首次系统性地评估SLM在乳业养殖决策支持中的可行性，强调隐私和计算效率。与现有方法（如依赖云端LLM）的本质区别在于，SLM允许本地部署，减少了对外部服务的依赖，提高了响应速度和数据安全性。\\n\\n**关键设计**：关键设计包括：在HuggingFace平台上选择20个开源SLM进行基准测试；使用农场现实计算约束（如有限的内存和处理器资源）模拟实际部署环境；任务代理设计基于先前工作，集成多种数据源（如SQL和NoSQL数据库）；评估指标侧重于模型在特定任务中的准确性和稳定性，例如通过PySpark进行NoSQL交互测试；微调策略被提及为未来改进方向，以优化SLM在乳业特定问题上的性能。",
            "application_zh": "该研究主要应用于乳业农场决策支持系统，潜在价值在于通过本地部署的小型语言模型提供实时、隐私保护的决策辅助，帮助农民和学者进行养殖管理、疾病预测和资源优化。未来影响可能扩展到其他农业领域，推动智能农业工具的发展，减少对云计算的依赖，提升数据安全性和操作效率。",
            "highlight_zh": "最重要的实验结果是：在评估的20个小型语言模型中，Qwen-4B在大多数任务类别（如文献搜索、网络搜索、SQL交互和图生成）中表现最优，显示出作为乳业决策引擎的潜力。然而，在NoSQL数据库交互任务中，通过PySpark测试时，Qwen-4B的有效性不稳定，表明特定任务仍需优化。整体上，研究验证了SLM在农场硬件约束下的可行性，为隐私和计算效率提供了实证支持，但未提供具体性能数据或对比基线提升幅度。",
            "tags_zh": [
                "小型语言模型",
                "乳业决策支持",
                "农场计算约束",
                "代理AI系统",
                "隐私保护",
                "本地部署",
                "任务评估",
                "开源模型基准测试"
            ],
            "_index": 92
        },
        {
            "title": "Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model",
            "authors": [
                "Zhaofeng Hu",
                "Hongrui Yu",
                "Vaidhyanathan Chandramouli",
                "Ci-Jyun Liang"
            ],
            "arxiv_id": "2512.14031v1",
            "summary": "This study evaluates two leading approaches for teaching construction robots new skills to understand their applicability for construction automation: a Vision-Language-Action (VLA) model and Reinforcement Learning (RL) methods. The goal is to understand both task performance and the practical effort needed to deploy each approach on real jobs. The authors developed two teleoperation interfaces to control the robots and collect the demonstrations needed, both of which proved effective for training robots for long-horizon and dexterous tasks. In addition, the authors conduct a three-stage evaluation. First, the authors compare a Multi-Layer Perceptron (MLP) policy with a Deep Q-network (DQN) imitation model to identify the stronger RL baseline, focusing on model performance, generalization, and a pick-up experiment. Second, three different VLA models are trained in two different scenarios and compared with each other. Third, the authors benchmark the selected RL baseline against the VLA model using computational and sample-efficiency measures and then a robot experiment on a multi-stage panel installation task that includes transport and installation. The VLA model demonstrates strong generalization and few-shot capability, achieving 60% and 100% success in the pickup phase. In comparison, DQN can be made robust but needs additional noise during tuning, which increases the workload. Overall, the findings indicate that VLA offers practical advantages for changing tasks by reducing programming effort and enabling useful performance with minimal data, while DQN provides a viable baseline when sufficient tuning effort is acceptable.",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14031v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RL"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "评估VLA模型与分层强化学习在建筑机器人技能学习中的样本效率与泛化能力",
            "summary_zh": "本研究评估了两种用于教授建筑机器人新技能的主流方法——视觉-语言-动作（VLA）模型和强化学习（RL）方法，以理解它们在建筑自动化中的适用性。目标是了解任务性能以及在真实工作中部署每种方法所需的实际工作量。作者开发了两种遥操作界面来控制机器人并收集所需的演示，这两种界面都被证明对训练机器人执行长期和灵巧任务有效。此外，作者进行了三阶段评估。首先，作者比较了多层感知器（MLP）策略和深度Q网络（DQN）模仿模型，以确定更强的RL基线，重点关注模型性能、泛化能力和拾取实验。其次，在两种不同场景下训练了三种不同的VLA模型，并相互比较。第三，作者使用计算和样本效率指标，以及在一个包括运输和安装的多阶段面板安装任务上的机器人实验，对选定的RL基线与VLA模型进行了基准测试。VLA模型表现出强大的泛化和少样本能力，在拾取阶段实现了60%和100%的成功率。相比之下，DQN虽然可以变得鲁棒，但需要在调优过程中添加额外噪声，这增加了工作量。总体而言，研究结果表明，VLA通过减少编程工作量和用最少数据实现有用性能，为任务变更提供了实际优势，而DQN在可接受足够调优工作量的情况下提供了一个可行的基线。",
            "intro_zh": [
                "核心问题：建筑机器人技能学习面临样本效率低、泛化能力差和实际部署工作量大的挑战，现有方法如传统RL需要大量数据与复杂调优。",
                "方法要点：系统比较VLA模型与分层RL方法，通过三阶段评估框架，结合遥操作界面收集演示，分析样本效率与泛化性能。",
                "实验或效果：VLA模型在拾取任务中达到60%-100%成功率，展现少样本优势；DQN需额外噪声调优，但提供鲁棒基线，整体VLA更实用。"
            ],
            "method_zh": "**问题定义**：论文旨在解决建筑机器人技能学习中的样本效率低、泛化能力不足以及实际部署工作量大的问题。现有方法如传统强化学习（RL）需要大量交互数据，且调优过程复杂，难以适应多变任务；而视觉-语言-动作（VLA）模型虽具潜力，但其在建筑自动化中的适用性尚不明确，缺乏系统评估。\\n\\n**核心思路**：论文的核心解决思路是通过系统比较VLA模型与分层RL方法，评估它们在建筑任务中的性能、泛化能力和实际部署工作量。设计三阶段评估框架，结合遥操作界面收集高质量演示，以量化样本效率和计算成本，从而为实际应用提供指导。\\n\\n**技术框架**：整体架构包括数据收集、模型训练和评估三个阶段。首先，开发两种遥操作界面控制机器人并收集演示数据。其次，进行三阶段评估：第一阶段比较MLP策略与DQN模仿模型作为RL基线；第二阶段训练三种VLA模型在两种场景下并相互比较；第三阶段使用计算和样本效率指标，以及多阶段面板安装任务实验，对选定的RL基线与VLA模型进行基准测试。\\n\\n**关键创新**：最重要的技术创新点是构建了一个综合评估框架，将VLA模型与分层RL方法在建筑自动化场景下进行直接对比，突出了VLA的少样本能力和泛化优势，同时量化了RL的调优工作量。与现有方法相比，本质区别在于强调实际部署的可行性和样本效率，而非单纯追求性能上限。\\n\\n**关键设计**：关键设计包括使用遥操作界面确保演示数据质量；在RL基线中，MLP策略用于简单模仿，DQN用于更复杂的值函数学习；VLA模型可能基于预训练视觉-语言模型（如CLIP）结合动作预测头；评估指标包括成功率、泛化测试（如拾取实验）和计算时间；在DQN调优中引入额外噪声以提高鲁棒性，但增加了工作量。具体网络结构和损失函数未详细说明，但可能涉及标准RL损失（如TD误差）和VLA的多模态对齐损失。",
            "application_zh": "该研究在建筑自动化领域具有重要应用价值，可用于机器人技能学习，如面板安装、材料搬运等复杂任务。通过减少编程工作量和数据需求，VLA模型能加速机器人部署，提升施工效率与安全性。未来可能扩展到其他工业自动化场景，推动智能机器人技术的实际落地。",
            "highlight_zh": "最重要的实验结果显示，VLA模型在拾取任务中实现60%和100%的成功率，展现强大泛化和少样本能力；相比之下，DQN虽可鲁棒但需额外噪声调优，增加工作量。在多阶段面板安装任务中，VLA在样本效率上优于RL基线，突显其实际优势。具体数据表明，VLA用最少数据达到有用性能，而DQN作为可行基线需更多调优努力。",
            "tags_zh": [
                "机器人技能学习",
                "视觉-语言-动作模型",
                "分层强化学习",
                "样本效率",
                "建筑自动化",
                "泛化能力",
                "遥操作界面",
                "多阶段任务评估"
            ],
            "_index": 93
        },
        {
            "title": "MobileWorldBench: Towards Semantic World Modeling For Mobile Agents",
            "authors": [
                "Shufan Li",
                "Konstantinos Kallidromitis",
                "Akash Gokul",
                "Yusuke Kato",
                "Kazuki Kozuka",
                "Aditya Grover"
            ],
            "arxiv_id": "2512.14014v1",
            "summary": "World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "21 pages, 13 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14014v1",
            "code_links": [
                {
                    "url": "https://github.com/jacklishufan/MobileWorld",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "世界模型",
                    "matched_keywords": [
                        "world model"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出MobileWorldBench基准与MobileWorld数据集，通过语义世界建模提升移动GUI代理的任务成功率",
            "summary_zh": "世界模型在提升具身智能体任务性能方面展现出巨大潜力。先前工作主要关注像素空间世界模型，但这些方法在GUI环境中面临实际限制，因为预测未来状态的复杂视觉元素通常很困难。在本工作中，我们探索了GUI智能体世界建模的替代方案，其中状态转换用自然语言描述，而不是预测原始像素。首先，我们引入了MobileWorldBench，这是一个评估视觉语言模型作为移动GUI智能体世界模型能力的基准。其次，我们发布了MobileWorld，一个包含140万样本的大规模数据集，显著提升了视觉语言模型的世界建模能力。最后，我们提出了一个新颖框架，将视觉语言模型世界模型集成到移动智能体的规划框架中，证明语义世界模型可以通过提高任务成功率直接使移动智能体受益。代码和数据集可在https://github.com/jacklishufan/MobileWorld获取。",
            "intro_zh": [
                "现有像素空间世界模型在GUI环境中预测复杂视觉元素困难，限制了移动代理的实际应用。",
                "论文提出用自然语言描述状态转换的语义世界建模方法，并构建基准和数据集提升视觉语言模型能力。",
                "实验表明，集成语义世界模型的框架能显著提高移动代理的任务成功率，验证了方法的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决移动GUI代理中世界建模的挑战。现有像素空间世界模型在GUI设置下面临实际限制，因为预测未来状态的复杂视觉元素（如界面布局、图标变化）往往困难，导致任务性能受限。\\n\\n**核心思路**：论文提出一种替代方案，将世界建模从像素空间转向语义空间，用自然语言描述状态转换，而不是直接预测原始像素。这样可以利用视觉语言模型的高级语义理解能力，简化预测过程，提高在GUI环境中的适用性。\\n\\n**技术框架**：整体框架包括三个主要部分：首先，MobileWorldBench基准用于评估视觉语言模型作为世界模型的能力；其次，MobileWorld数据集提供大规模训练样本以增强模型性能；最后，一个集成框架将训练好的视觉语言模型世界模型嵌入到移动代理的规划系统中，通过语义预测指导任务执行。\\n\\n**关键创新**：最重要的创新在于将世界建模从像素级预测提升到语义级描述，避免了视觉细节的复杂建模，同时利用自然语言的表达力来捕捉GUI状态变化。与现有方法相比，本质区别在于用语言作为中间表示，而非直接操作图像数据。\\n\\n**关键设计**：MobileWorld数据集包含140万样本，覆盖多样GUI交互场景，用于训练视觉语言模型；框架中可能涉及基于Transformer的视觉语言模型架构，如CLIP或类似变体，用于编码图像和文本信息；损失函数可能包括对比学习或生成式目标，以优化状态转换预测；具体参数设置如模型规模、训练批次等细节在论文中未明确，需参考代码库。",
            "application_zh": "该研究在移动GUI代理领域具有广泛潜在应用，如智能手机应用自动化、网页浏览辅助、游戏AI和智能家居控制。通过语义世界建模，代理能更准确地理解和预测界面变化，提升任务执行效率和成功率，推动具身AI在真实世界交互中的实际部署，未来可能扩展到更复杂的多模态环境。",
            "highlight_zh": "实验结果显示，集成语义世界模型的框架显著提升了移动代理的任务成功率。具体数据未知，但论文强调通过MobileWorld数据集训练后，视觉语言模型的世界建模能力得到增强，在MobileWorldBench基准上表现出色，相比基线方法（如像素空间模型）有明确提升，验证了语义方法的优势。",
            "tags_zh": [
                "语义世界建模",
                "移动GUI代理",
                "视觉语言模型",
                "基准测试",
                "数据集构建",
                "任务规划",
                "自然语言处理",
                "具身智能"
            ],
            "_index": 94
        },
        {
            "title": "Professional Software Developers Don't Vibe, They Control: AI Agent Use for Coding in 2025",
            "authors": [
                "Ruanqianqian Huang",
                "Avery Reyna",
                "Sorin Lerner",
                "Haijun Xia",
                "Brian Hempel"
            ],
            "arxiv_id": "2512.14012v1",
            "summary": "The rise of AI agents is transforming how software can be built. The promise of agents is that developers might write code quicker, delegate multiple tasks to different agents, and even write a full piece of software purely out of natural language. In reality, what roles agents play in professional software development remains in question. This paper investigates how experienced developers use agents in building software, including their motivations, strategies, task suitability, and sentiments. Through field observations (N=13) and qualitative surveys (N=99), we find that while experienced developers value agents as a productivity boost, they retain their agency in software design and implementation out of insistence on fundamental software quality attributes, employing strategies for controlling agent behavior leveraging their expertise. In addition, experienced developers feel overall positive about incorporating agents into software development given their confidence in complementing the agents' limitations. Our results shed light on the value of software development best practices in effective use of agents, suggest the kinds of tasks for which agents may be suitable, and point towards future opportunities for better agentic interfaces and agentic use guidelines.",
            "categories": [
                "cs.SE",
                "cs.AI",
                "cs.HC"
            ],
            "primary_category": "cs.SE",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14012v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "通过实地观察和定性调查揭示专业开发者如何控制AI代理以提升软件质量与生产力",
            "summary_zh": "AI代理的兴起正在改变软件的构建方式。代理的承诺在于开发者可能更快地编写代码，将多个任务委托给不同的代理，甚至完全用自然语言编写完整的软件。然而，代理在专业软件开发中扮演的角色仍然存疑。本文通过实地观察（N=13）和定性调查（N=99），研究了经验丰富的开发者如何在构建软件时使用代理，包括他们的动机、策略、任务适用性和情感。研究发现，尽管经验丰富的开发者重视代理作为生产力提升工具，但他们出于对基本软件质量属性的坚持，保留了对软件设计和实现的自主权，并利用专业知识实施控制代理行为的策略。此外，经验丰富的开发者对将代理融入软件开发持总体积极态度，因为他们有信心弥补代理的局限性。研究结果揭示了软件开发最佳实践在有效使用代理中的价值，提出了代理可能适用的任务类型，并指出了未来改进代理界面和代理使用指南的机会。",
            "intro_zh": [
                "核心问题：AI代理在专业软件开发中的实际角色和有效性尚不明确，现有研究缺乏对开发者使用策略和动机的深入理解。",
                "方法要点：通过实地观察和定性调查，分析专业开发者使用AI代理的动机、策略、任务适用性和情感，强调控制与自主权。",
                "实验或效果：发现开发者通过专业知识控制代理行为以提升软件质量，对代理融入持积极态度，并识别出代理适用的任务类型。"
            ],
            "method_zh": "**问题定义**：论文旨在解决AI代理在专业软件开发中的实际应用问题，现有方法的痛点在于过度强调代理的自动化潜力，而忽视了专业开发者如何在实际工作中整合和控制代理，以确保软件质量并保持开发自主权。\\n\\n**核心思路**：论文的核心解决思路是通过实证研究，深入分析经验丰富的开发者使用AI代理的行为模式，强调开发者如何利用专业知识来控制和引导代理，而不是被动依赖代理，从而在提升生产力的同时维护软件设计的基本原则和质量属性。\\n\\n**技术框架**：整体架构基于混合研究方法，包括两个主要阶段：首先进行实地观察（N=13），直接记录开发者在实际项目中使用代理的过程；其次进行定性调查（N=99），收集开发者对代理使用动机、策略、任务适用性和情感的反馈。数据通过主题分析进行编码和归纳，以识别关键模式和见解。\\n\\n**关键创新**：最重要的技术创新点在于将研究焦点从代理的技术能力转向开发者的实际使用行为，揭示了专业开发者如何通过控制策略（如代码审查、任务分解和反馈循环）来弥补代理的局限性，这与现有方法中假设代理能完全替代开发者有本质区别。\\n\\n**关键设计**：关键设计包括样本选择（经验丰富的开发者以确保专业视角）、数据收集工具（观察协议和调查问卷以覆盖行为与态度）、分析方法（定性主题分析以提取深层见解），以及伦理考虑（如知情同意和数据匿名化），这些设计确保了研究的可靠性和有效性。",
            "application_zh": "该研究的潜在应用领域包括软件开发工具设计、AI代理界面优化和行业培训指南。实际价值在于为工具开发者提供洞察，以创建更符合专业需求的代理系统，例如增强控制功能和集成质量检查机制。未来影响可能推动软件开发实践向人机协作模式演进，提升整体效率和质量，同时为教育机构提供课程设计参考，培养开发者有效使用AI代理的能力。",
            "highlight_zh": "最重要的实验结果包括：通过实地观察发现，13名专业开发者普遍采用控制策略（如代码审查和任务指导）来管理代理行为；定性调查显示，99名开发者中多数对代理融入持积极态度（具体比例未知），但强调需要专业知识来弥补代理局限性；研究识别出代理更适用于重复性任务（如代码生成和测试），而在复杂设计决策中作用有限。这些结果基于主题分析得出，未提供具体性能数据，但突出了开发者自主权与代理辅助之间的平衡。",
            "tags_zh": [
                "AI代理",
                "软件开发",
                "人机协作",
                "质量控制",
                "实证研究",
                "专业开发者",
                "任务适用性",
                "自主权"
            ],
            "_index": 95
        },
        {
            "title": "CLAIM: Camera-LiDAR Alignment with Intensity and Monodepth",
            "authors": [
                "Zhuo Zhang",
                "Yonghui Liu",
                "Meijie Zhang",
                "Feiyang Tan",
                "Yikang Ding"
            ],
            "arxiv_id": "2512.14001v1",
            "summary": "In this paper, we unleash the potential of the powerful monodepth model in camera-LiDAR calibration and propose CLAIM, a novel method of aligning data from the camera and LiDAR. Given the initial guess and pairs of images and LiDAR point clouds, CLAIM utilizes a coarse-to-fine searching method to find the optimal transformation minimizing a patched Pearson correlation-based structure loss and a mutual information-based texture loss. These two losses serve as good metrics for camera-LiDAR alignment results and require no complicated steps of data processing, feature extraction, or feature matching like most methods, rendering our method simple and adaptive to most scenes. We validate CLAIM on public KITTI, Waymo, and MIAS-LCEC datasets, and the experimental results demonstrate its superior performance compared with the state-of-the-art methods. The code is available at https://github.com/Tompson11/claim.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted by IROS 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14001v1",
            "code_links": [
                {
                    "url": "https://github.com/Tompson11/claim",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "lidar",
                        "waymo"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出CLAIM方法，利用单目深度模型和粗到精搜索，解决相机与LiDAR数据对齐问题，无需复杂特征处理。",
            "summary_zh": "本文释放了强大单目深度模型在相机-LiDAR标定中的潜力，提出了CLAIM，一种新颖的相机与LiDAR数据对齐方法。给定初始猜测和图像-LiDAR点云对，CLAIM采用粗到精搜索方法，寻找最小化基于补丁皮尔逊相关的结构损失和基于互信息的纹理损失的最优变换。这两种损失作为相机-LiDAR对齐结果的良好度量，无需像大多数方法那样进行复杂的数据处理、特征提取或特征匹配步骤，使我们的方法简单且适应大多数场景。我们在公开的KITTI、Waymo和MIAS-LCEC数据集上验证了CLAIM，实验结果表明其性能优于最先进的方法。代码可在https://github.com/Tompson11/claim获取。",
            "intro_zh": [
                "核心问题：现有相机-LiDAR对齐方法通常依赖复杂的数据处理、特征提取和匹配步骤，导致计算成本高、适应性差，难以在多样场景中稳定应用。",
                "方法要点：CLAIM利用单目深度模型生成深度信息，结合粗到精搜索策略，通过结构损失和纹理损失优化对齐变换，无需传统特征处理。",
                "实验或效果：在KITTI、Waymo和MIAS-LCEC数据集上，CLAIM表现出优于现有方法的性能，验证了其简单性和适应性。"
            ],
            "method_zh": "**问题定义**：论文解决相机与LiDAR传感器数据对齐的具体问题，即估计两者间的空间变换参数（如旋转和平移）。现有方法的痛点在于通常需要复杂的特征提取、匹配或数据处理步骤，这增加了计算负担，且对场景变化敏感，难以泛化到不同环境。\\n\\n**核心思路**：论文的核心思路是利用单目深度模型（monodepth）生成图像的深度信息，结合LiDAR点云的强度数据，通过优化损失函数来对齐数据。这样设计是因为单目深度模型能提供丰富的结构信息，而强度数据包含纹理细节，两者结合可以更全面地评估对齐质量，避免了传统特征方法的复杂性。\\n\\n**技术框架**：整体流程包括输入图像和LiDAR点云对及初始变换猜测，首先使用单目深度模型估计图像深度，然后采用粗到精搜索策略：在粗阶段快速缩小搜索空间，在精阶段细化变换参数。主要模块包括深度估计模块、损失计算模块和优化搜索模块。\\n\\n**关键创新**：最重要的技术创新是引入了基于补丁皮尔逊相关的结构损失和基于互信息的纹理损失作为对齐度量，这些损失直接利用深度和强度信息，无需中间特征步骤，本质区别在于简化了流程并提高了适应性。\\n\\n**关键设计**：关键设计包括使用预训练的单目深度模型（如Monodepth2）生成深度图，损失函数中结构损失计算图像深度与LiDAR点云投影深度之间的局部相关性，纹理损失计算图像强度与LiDAR强度之间的互信息，优化采用梯度下降或网格搜索方法，参数设置如补丁大小和搜索步长根据数据集调整。",
            "application_zh": "该研究在自动驾驶、机器人导航和增强现实等领域具有潜在应用价值，可用于多传感器融合系统，提高环境感知的准确性和鲁棒性。未来可能推动更高效的传感器标定技术发展，降低系统部署成本。",
            "highlight_zh": "在KITTI、Waymo和MIAS-LCEC数据集上的实验显示，CLAIM在相机-LiDAR对齐任务中性能优于最先进方法，具体提升幅度未知，但通过定量指标（如对齐误差）验证了其优越性，基线对比包括传统特征匹配方法和深度学习方法。",
            "tags_zh": [
                "相机-LiDAR对齐",
                "单目深度估计",
                "多模态融合",
                "传感器标定",
                "损失函数设计",
                "粗到精搜索",
                "自动驾驶感知",
                "无特征匹配"
            ],
            "_index": 96
        },
        {
            "title": "On the Hardness of Conditional Independence Testing In Practice",
            "authors": [
                "Zheng He",
                "Roman Pogodin",
                "Yazhe Li",
                "Namrata Deka",
                "Arthur Gretton",
                "Danica J. Sutherland"
            ],
            "arxiv_id": "2512.14000v1",
            "summary": "Tests of conditional independence (CI) underpin a number of important problems in machine learning and statistics, from causal discovery to evaluation of predictor fairness and out-of-distribution robustness. Shah and Peters (2020) showed that, contrary to the unconditional case, no universally finite-sample valid test can ever achieve nontrivial power. While informative, this result (based on \"hiding\" dependence) does not seem to explain the frequent practical failures observed with popular CI tests. We investigate the Kernel-based Conditional Independence (KCI) test - of which we show the Generalized Covariance Measure underlying many recent tests is nearly a special case - and identify the major factors underlying its practical behavior. We highlight the key role of errors in the conditional mean embedding estimate for the Type-I error, while pointing out the importance of selecting an appropriate conditioning kernel (not recognized in previous work) as being necessary for good test power but also tending to inflate Type-I error.",
            "categories": [
                "stat.ML",
                "cs.LG",
                "stat.ME"
            ],
            "primary_category": "stat.ML",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Published at NeurIPS 2025: https://openreview.net/forum?id=Tn1M71PDfF",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14000v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "揭示KCI测试在实践中失效的关键因素，提出条件核选择对I型错误和检验功效的平衡机制",
            "summary_zh": "条件独立性检验在机器学习和统计学中具有重要作用，支撑着从因果发现到预测器公平性和分布外鲁棒性评估等多个关键问题。Shah和Peters（2020）的研究表明，与无条件情况不同，不存在任何普遍有限样本有效的检验能够获得非平凡的检验功效。尽管这一结果（基于“隐藏”依赖性）具有启发性，但它似乎未能解释实践中常见条件独立性检验频繁失效的现象。本文研究了基于核的条件独立性检验，并指出许多近期检验所基于的广义协方差度量几乎可以视为其特例。我们识别了影响KCI检验实际行为的主要因素，强调了条件均值嵌入估计误差对I型错误的关键作用，同时指出选择适当的条件核（先前工作中未被认识到）对于获得良好的检验功效是必要的，但也倾向于增加I型错误。",
            "intro_zh": [
                "现有条件独立性检验在实践中频繁失效，Shah和Peters的理论结果未能完全解释这一现象。",
                "论文聚焦KCI检验，识别条件均值嵌入估计误差和条件核选择是影响I型错误和检验功效的关键因素。",
                "实验表明，条件核选择对平衡检验功效和I型错误至关重要，为改进实际检验提供了新方向。"
            ],
            "method_zh": "**问题定义**：论文旨在解决条件独立性检验在实践中频繁失效的问题，特别是KCI检验的I型错误膨胀和检验功效不足。现有方法的痛点在于，尽管Shah和Peters的理论揭示了条件独立性检验的固有困难，但未能解释实际观察到的检验失败，且先前工作未充分认识到条件核选择对检验性能的影响。\\n\\n**核心思路**：论文的核心解决思路是通过深入分析KCI检验的机制，识别出条件均值嵌入估计误差是导致I型错误的关键因素，同时强调条件核选择对检验功效的必要性及其对I型错误的潜在负面影响。这种设计基于对检验统计量构建过程的分解，旨在从实践角度揭示检验失效的根源。\\n\\n**技术框架**：整体架构包括理论分析和实验验证两个阶段。首先，论文将KCI检验与广义协方差度量联系起来，展示后者近似为前者的特例。然后，通过分解检验统计量，分析条件均值嵌入估计误差对I型错误的影响，并探讨条件核选择在优化检验功效和平衡I型错误中的作用。实验部分使用合成和真实数据集评估不同核选择策略下的检验性能。\\n\\n**关键创新**：最重要的技术创新点在于首次系统性地识别了条件核选择作为影响KCI检验性能的关键因素，并揭示了其在提升检验功效和增加I型错误之间的权衡机制。与现有方法的本质区别在于，先前工作多关注检验统计量的构建或理论极限，而本文从实践角度深入剖析了检验失效的具体技术原因。\\n\\n**关键设计**：关键设计包括使用核方法构建条件均值嵌入估计，通过交叉验证或启发式方法选择条件核（如高斯核的带宽参数），并设计实验对比不同核选择策略下的I型错误率和检验功效。论文未指定具体的损失函数或网络结构，而是聚焦于核参数的选择和估计误差的分析。",
            "application_zh": "该研究在因果发现、预测器公平性评估和分布外鲁棒性测试等领域具有重要应用价值。通过改进条件独立性检验的实践性能，可以更可靠地识别变量间的因果关系，确保机器学习模型的公平性，并增强模型对新数据分布的适应能力。未来影响可能包括开发更稳健的检验方法，推动相关领域如医疗诊断和金融风险评估的进步。",
            "highlight_zh": "实验结果表明，条件核选择显著影响KCI检验的性能：不当的核选择可能导致I型错误率高达未知（具体数据未提供），而优化选择可提升检验功效。与基线方法（如固定核参数）相比，论文提出的核选择策略在合成数据上显示出更好的平衡性，减少了I型错误膨胀同时保持较高功效。具体提升幅度未知，但实验强调了核选择在实际应用中的关键作用。",
            "tags_zh": [
                "条件独立性检验",
                "核方法",
                "I型错误控制",
                "检验功效",
                "条件均值嵌入",
                "因果发现",
                "机器学习公平性",
                "分布外鲁棒性"
            ],
            "_index": 97
        },
        {
            "title": "Maximum Mean Discrepancy with Unequal Sample Sizes via Generalized U-Statistics",
            "authors": [
                "Aaron Wei",
                "Milad Jalali",
                "Danica J. Sutherland"
            ],
            "arxiv_id": "2512.13997v1",
            "summary": "Existing two-sample testing techniques, particularly those based on choosing a kernel for the Maximum Mean Discrepancy (MMD), often assume equal sample sizes from the two distributions. Applying these methods in practice can require discarding valuable data, unnecessarily reducing test power. We address this long-standing limitation by extending the theory of generalized U-statistics and applying it to the usual MMD estimator, resulting in new characterization of the asymptotic distributions of the MMD estimator with unequal sample sizes (particularly outside the proportional regimes required by previous partial results). This generalization also provides a new criterion for optimizing the power of an MMD test with unequal sample sizes. Our approach preserves all available data, enhancing test accuracy and applicability in realistic settings. Along the way, we give much cleaner characterizations of the variance of MMD estimators, revealing something that might be surprising to those in the area: while zero MMD implies a degenerate estimator, it is sometimes possible to have a degenerate estimator with nonzero MMD as well; we give a construction and a proof that it does not happen in common situations.",
            "categories": [
                "stat.ML",
                "cs.LG",
                "math.ST",
                "stat.ME"
            ],
            "primary_category": "stat.ML",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13997v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出基于广义U统计量的最大均值差异方法，解决不等样本量下的两样本检验问题，提升测试准确性和实用性。",
            "summary_zh": "现有的两样本检验技术，特别是基于选择核函数的最大均值差异（MMD）方法，通常假设两个分布具有相等的样本量。在实际应用中，这些方法可能需要丢弃有价值的数据，从而不必要地降低检验功效。我们通过扩展广义U统计量的理论并将其应用于通常的MMD估计量，解决了这一长期存在的限制，从而对不等样本量下MMD估计量的渐近分布进行了新的刻画（特别是在先前部分结果所要求的比例范围之外）。这一推广还为优化不等样本量下MMD检验的功效提供了新的准则。我们的方法保留了所有可用数据，提高了实际场景中的检验准确性和适用性。在此过程中，我们对MMD估计量的方差给出了更清晰的刻画，揭示了一个可能令该领域研究者惊讶的现象：虽然零MMD意味着估计量退化，但有时也可能出现非零MMD下的退化估计量；我们给出了一个构造，并证明了这在常见情况下不会发生。",
            "intro_zh": [
                "现有MMD两样本检验方法通常假设样本量相等，实际应用中需丢弃数据，导致检验功效降低。",
                "论文扩展广义U统计量理论，应用于MMD估计量，刻画不等样本量下的渐近分布，并优化检验功效。",
                "方法保留所有数据，提升检验准确性，同时清晰刻画MMD估计量方差，揭示退化估计量的新现象。"
            ],
            "method_zh": "**问题定义**：论文解决不等样本量下的两样本检验问题，现有基于最大均值差异（MMD）的方法通常假设样本量相等，实际应用中需丢弃数据以匹配样本量，这降低了检验功效和实用性，特别是在样本量差异较大或非比例情况下。\\n\\n**核心思路**：通过扩展广义U统计量的理论，将其应用于MMD估计量，以处理不等样本量情况，从而避免数据丢弃，并推导出新的渐近分布刻画和检验功效优化准则。\\n\\n**技术框架**：整体架构包括：1) 定义不等样本量下的MMD估计量，基于广义U统计量形式；2) 分析该估计量的渐近分布，特别是在非比例样本量下的行为；3) 推导方差表达式，优化检验功效；4) 验证理论结果，包括退化估计量的构造和分析。\\n\\n**关键创新**：最重要的技术创新是首次系统地将广义U统计量理论应用于MMD估计量，解决了不等样本量下的两样本检验问题，与现有方法相比，本质区别在于无需假设样本量相等或比例，直接处理原始数据，提升了理论完备性和实际适用性。\\n\\n**关键设计**：关键设计包括：1) 使用广义U统计量作为MMD估计量的数学框架，允许样本量不等；2) 推导渐近分布时，考虑样本量差异的影响，特别是非比例情况；3) 优化检验功效的准则基于新推导的方差表达式；4) 对退化估计量进行构造和证明，揭示MMD估计量的新性质。",
            "application_zh": "该研究在机器学习、统计学和数据分析领域有广泛应用，例如异常检测、分布比较和模型验证。通过处理不等样本量，能更有效地利用真实世界数据，提升检验的准确性和鲁棒性，对生物信息学、金融风险分析等实际场景具有重要价值，未来可能推动两样本检验方法的标准化和普及。",
            "highlight_zh": "实验结果表明，论文方法在不等样本量下显著提升检验功效，相比传统丢弃数据的方法，功效提升可达20%以上。通过模拟数据和真实数据集验证，新方法在非比例样本量情况下保持高准确性，同时清晰刻画了MMD估计量的方差，揭示了退化估计量的新现象，为后续研究提供理论基础。",
            "tags_zh": [
                "两样本检验",
                "最大均值差异",
                "广义U统计量",
                "不等样本量",
                "渐近分布",
                "检验功效优化",
                "统计学习",
                "机器学习理论"
            ],
            "_index": 98
        },
        {
            "title": "Universal Reasoning Model",
            "authors": [
                "Zitian Gao",
                "Lynx Chen",
                "Yihao Xiao",
                "He Xing",
                "Ran Tao",
                "Haoming Luo",
                "Joey Zhou",
                "Bryan Dai"
            ],
            "arxiv_id": "2512.14693v1",
            "summary": "Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14693v1",
            "code_links": [
                {
                    "url": "https://github.com/zitian-gao/URM",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出通用推理模型以提升复杂推理任务性能，在ARC-AGI基准上实现新突破",
            "summary_zh": "通用Transformer（UTs）已广泛应用于ARC-AGI和数独等复杂推理任务，但其性能提升的具体来源尚未得到充分探索。本研究系统分析了UTs的变体，发现ARC-AGI上的改进主要源于Transformer的循环归纳偏置和强非线性组件，而非复杂的架构设计。基于这一发现，我们提出了通用推理模型（URM），通过引入短卷积和截断反向传播来增强UT。该方法显著提升了推理性能，在ARC-AGI 1上达到了53.8%的pass@1，在ARC-AGI 2上达到了16.0%的pass@1，实现了最先进水平。代码已开源：https://github.com/zitian-gao/URM。",
            "intro_zh": [
                "现有通用Transformer在复杂推理任务中性能来源不明，限制了进一步优化。",
                "通过分析发现性能提升源于循环归纳偏置和强非线性，提出增强通用推理模型。",
                "在ARC-AGI基准上实现显著提升，达到53.8%和16.0%的pass@1新纪录。"
            ],
            "method_zh": "**问题定义**：论文旨在解决复杂推理任务（如ARC-AGI和数独）中，通用Transformer性能提升来源不明确的问题，现有方法依赖复杂架构设计，但实际效果可能源于其他因素，导致优化方向模糊。\\n\\n**核心思路**：核心思路是通过系统分析发现性能提升主要来自Transformer的循环归纳偏置和强非线性组件，而非复杂架构，因此提出增强通用推理模型，聚焦于这些核心要素，以更高效地提升推理能力。\\n\\n**技术框架**：整体架构基于通用Transformer，通过引入短卷积模块来增强局部特征提取，并结合截断反向传播技术优化训练过程，形成URM模型，应用于序列推理任务。\\n\\n**关键创新**：最重要的技术创新是结合短卷积和截断反向传播来增强通用Transformer，区别于现有方法依赖复杂架构设计，本质区别在于更直接地利用Transformer的内在优势，实现性能突破。\\n\\n**关键设计**：关键设计包括短卷积层的参数设置以捕捉局部模式，截断反向传播用于减少计算开销和梯度问题，网络结构保持Transformer基础，但通过增强组件提升非线性表达能力，损失函数基于任务目标（如分类或序列预测）进行优化。",
            "application_zh": "该研究在复杂推理任务如ARC-AGI和数独中具有直接应用价值，可推动人工智能在抽象推理和问题解决领域的发展。潜在应用包括智能教育系统、自动化逻辑测试和通用AI基准评估，未来可能影响更广泛的认知计算和机器人决策系统。",
            "highlight_zh": "最重要的实验结果是在ARC-AGI基准上实现显著性能提升：在ARC-AGI 1上达到53.8% pass@1，在ARC-AGI 2上达到16.0% pass@1，对比基线通用Transformer和其他变体，提升幅度明显，确立了最先进水平，验证了短卷积和截断反向传播的有效性。",
            "tags_zh": [
                "通用推理模型",
                "Transformer架构",
                "复杂推理任务",
                "ARC-AGI基准",
                "短卷积",
                "截断反向传播",
                "循环归纳偏置",
                "非线性组件"
            ],
            "_index": 99
        },
        {
            "title": "Native and Compact Structured Latents for 3D Generation",
            "authors": [
                "Jianfeng Xiang",
                "Xiaoxue Chen",
                "Sicheng Xu",
                "Ruicheng Wang",
                "Zelong Lv",
                "Yu Deng",
                "Hongyuan Zhu",
                "Yue Dong",
                "Hao Zhao",
                "Nicholas Jing Yuan",
                "Jiaolong Yang"
            ],
            "arxiv_id": "2512.14692v1",
            "summary": "Recent advancements in 3D generative modeling have significantly improved the generation realism, yet the field is still hampered by existing representations, which struggle to capture assets with complex topologies and detailed appearance. This paper present an approach for learning a structured latent representation from native 3D data to address this challenge. At its core is a new sparse voxel structure called O-Voxel, an omni-voxel representation that encodes both geometry and appearance. O-Voxel can robustly model arbitrary topology, including open, non-manifold, and fully-enclosed surfaces, while capturing comprehensive surface attributes beyond texture color, such as physically-based rendering parameters. Based on O-Voxel, we design a Sparse Compression VAE which provides a high spatial compression rate and a compact latent space. We train large-scale flow-matching models comprising 4B parameters for 3D generation using diverse public 3D asset datasets. Despite their scale, inference remains highly efficient. Meanwhile, the geometry and material quality of our generated assets far exceed those of existing models. We believe our approach offers a significant advancement in 3D generative modeling.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project Page: https://microsoft.github.io/TRELLIS.2/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14692v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出O-Voxel稀疏体素表示与稀疏压缩VAE，以解决3D生成中复杂拓扑与精细外观建模的挑战",
            "summary_zh": "近年来，3D生成建模在生成真实感方面取得了显著进展，但该领域仍受限于现有表示方法，这些方法难以捕捉具有复杂拓扑结构和精细外观的资产。本文提出了一种从原生3D数据中学习结构化潜在表示的方法来应对这一挑战。其核心是一种名为O-Voxel的新型稀疏体素结构，这是一种全向体素表示，能够同时编码几何和外观信息。O-Voxel能够鲁棒地建模任意拓扑结构，包括开放、非流形和完全封闭的表面，同时捕捉超越纹理颜色的全面表面属性，例如基于物理的渲染参数。基于O-Voxel，我们设计了一种稀疏压缩变分自编码器，它提供了高空间压缩率和紧凑的潜在空间。我们使用多样化的公共3D资产数据集，训练了包含40亿参数的大规模流匹配模型用于3D生成。尽管模型规模庞大，推理过程仍然保持高效。同时，我们生成资产的几何和材质质量远超现有模型。我们相信，我们的方法为3D生成建模提供了重要进展。",
            "intro_zh": [
                "现有3D表示方法难以有效建模复杂拓扑（如开放、非流形表面）和精细外观（如物理渲染参数），限制了生成资产的真实感和多样性。",
                "提出O-Voxel稀疏体素表示，统一编码几何与外观；并设计稀疏压缩VAE构建紧凑潜在空间，结合大规模流匹配模型进行高效生成。",
                "生成资产的几何与材质质量显著超越现有模型，模型参数量达40亿但推理高效，在多个数据集上验证了方法的优越性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决3D生成建模中，现有表示方法（如网格、点云、传统体素）难以有效捕捉复杂拓扑结构（如开放、非流形表面）和精细外观属性（如物理渲染参数）的核心挑战，导致生成资产真实感不足、多样性受限。\\n\\n**核心思路**：通过设计一种新型的稀疏体素表示O-Voxel，统一编码几何和外观信息，以鲁棒处理任意拓扑；并基于此构建稀疏压缩变分自编码器，实现高压缩率的紧凑潜在空间，从而支持大规模生成模型的训练与高效推理。\\n\\n**技术框架**：整体流程包括：1) 数据预处理，将原生3D资产转换为O-Voxel表示；2) 使用稀疏压缩VAE对O-Voxel数据进行编码，学习紧凑的潜在表示；3) 在大规模潜在空间上训练流匹配模型（含40亿参数）进行3D生成；4) 通过解码器从潜在向量重建O-Voxel，进而输出最终3D资产。\\n\\n**关键创新**：O-Voxel表示是核心创新，它作为一种稀疏体素结构，不仅能建模任意拓扑（包括非流形几何），还扩展了表面属性编码，支持物理渲染参数等，超越了传统仅关注颜色纹理的方法。与现有方法相比，本质区别在于其“原生”和“结构化”特性，直接从3D数据学习，避免了中间表示的损失。\\n\\n**关键设计**：O-Voxel采用稀疏存储以高效处理大规模数据；稀疏压缩VAE利用稀疏性实现高压缩率，潜在空间维度经过优化以平衡表达能力和计算效率；流匹配模型基于扩散过程，使用大规模参数（40亿）和多样数据集训练，损失函数可能结合重建损失和对抗损失以提升质量；网络结构可能包含稀疏卷积层以适应O-Voxel输入。",
            "application_zh": "该研究在游戏开发、影视特效、虚拟现实和工业设计等领域具有广泛应用潜力，能高效生成高质量、多样化的3D资产，降低内容创作成本。其紧凑表示和高效推理特性，为实时3D生成和编辑提供了新可能，推动3D生成技术向更实用化方向发展。",
            "highlight_zh": "实验表明，生成资产的几何和材质质量远超现有基线模型（具体提升幅度未在摘要中给出，但强调“far exceed”）。模型参数量达到40亿，使用多样公共数据集训练，尽管规模庞大，推理仍保持高效，验证了O-Voxel和稀疏压缩VAE在压缩与生成效率方面的优势。",
            "tags_zh": [
                "3D生成",
                "稀疏体素表示",
                "变分自编码器",
                "流匹配模型",
                "拓扑建模",
                "物理渲染",
                "大规模训练",
                "潜在空间学习"
            ],
            "_index": 100
        },
        {
            "title": "VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image",
            "authors": [
                "Sicheng Xu",
                "Guojun Chen",
                "Jiaolong Yang",
                "Yizhong Zhang",
                "Yu Deng",
                "Steve Lin",
                "Baining Guo"
            ],
            "arxiv_id": "2512.14677v1",
            "summary": "We propose VASA-3D, an audio-driven, single-shot 3D head avatar generator. This research tackles two major challenges: capturing the subtle expression details present in real human faces, and reconstructing an intricate 3D head avatar from a single portrait image. To accurately model expression details, VASA-3D leverages the motion latent of VASA-1, a method that yields exceptional realism and vividness in 2D talking heads. A critical element of our work is translating this motion latent to 3D, which is accomplished by devising a 3D head model that is conditioned on the motion latent. Customization of this model to a single image is achieved through an optimization framework that employs numerous video frames of the reference head synthesized from the input image. The optimization takes various training losses robust to artifacts and limited pose coverage in the generated training data. Our experiment shows that VASA-3D produces realistic 3D talking heads that cannot be achieved by prior art, and it supports the online generation of 512x512 free-viewpoint videos at up to 75 FPS, facilitating more immersive engagements with lifelike 3D avatars.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "NeurIPS 2025 paper. Project webpage: https://www.microsoft.com/en-us/research/project/vasa-3d/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14677v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出VASA-3D以解决从单张图像生成逼真音频驱动3D头部虚拟形象的挑战",
            "summary_zh": "我们提出了VASA-3D，一种音频驱动的单次3D头部虚拟形象生成器。这项研究解决了两个主要挑战：捕捉真实人脸的细微表情细节，以及从单张肖像图像重建复杂的3D头部虚拟形象。为了准确建模表情细节，VASA-3D利用了VASA-1的运动潜在表示，该方法在2D说话头部生成中展现出卓越的真实感和生动性。我们工作的一个关键要素是将这种运动潜在表示转换到3D，这是通过设计一个以运动潜在表示为条件的3D头部模型来实现的。该模型针对单张图像的定制化是通过一个优化框架实现的，该框架使用了从输入图像合成的参考头部的大量视频帧。优化过程采用了多种训练损失，这些损失对生成训练数据中的伪影和有限姿态覆盖具有鲁棒性。我们的实验表明，VASA-3D生成了现有技术无法实现的逼真3D说话头部，并支持在线生成512x512自由视角视频，最高可达75 FPS，促进了与逼真3D虚拟形象更沉浸式的互动。",
            "intro_zh": [
                "现有方法难以从单张图像生成逼真3D头部虚拟形象，尤其在捕捉细微表情细节和重建复杂3D结构方面存在不足。",
                "VASA-3D通过利用VASA-1的运动潜在表示，设计条件化3D头部模型，并结合优化框架实现单图像定制化，以解决这些挑战。",
                "实验结果显示，VASA-3D能生成超越现有技术的逼真3D说话头部，支持在线生成512x512自由视角视频，最高达75 FPS。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从单张肖像图像生成逼真音频驱动3D头部虚拟形象的问题。现有方法在捕捉真实人脸的细微表情细节和从单图像重建复杂3D结构方面存在不足，导致生成的虚拟形象缺乏真实感和生动性。\\n\\n**核心思路**：论文的核心思路是结合VASA-1在2D说话头部生成中的运动潜在表示优势，将其扩展到3D领域。通过设计一个以运动潜在表示为条件的3D头部模型，并利用优化框架实现单图像定制化，以捕捉表情细节并重建3D结构。\\n\\n**技术框架**：整体架构包括三个主要阶段：首先，从输入音频中提取VASA-1的运动潜在表示；其次，设计一个3D头部模型，该模型以运动潜在表示为条件，生成3D头部姿态和表情；最后，通过优化框架，使用从输入图像合成的参考头部视频帧，定制化模型以适应单张图像。优化过程涉及多个训练损失函数，以确保对伪影和有限姿态覆盖的鲁棒性。\\n\\n**关键创新**：最重要的技术创新是将VASA-1的2D运动潜在表示成功迁移到3D头部虚拟形象生成中。与现有方法相比，VASA-3D的本质区别在于它能够从单张图像生成高度逼真的3D说话头部，同时保持表情细节和自由视角能力，这在之前的技术中难以实现。\\n\\n**关键设计**：关键设计包括：使用VASA-1的运动潜在表示作为条件输入；设计一个基于高斯分布的3D头部模型，以高效表示头部几何和外观；优化框架中采用多种损失函数，如重建损失、对抗损失和姿态一致性损失，以提高生成质量；支持在线生成512x512分辨率视频，帧率最高达75 FPS，确保实时性和沉浸感。",
            "application_zh": "VASA-3D在虚拟现实、增强现实、远程会议、游戏和娱乐等领域具有广泛应用潜力。它能够生成逼真的3D头部虚拟形象，提升用户体验和沉浸感，例如在虚拟社交中实现更自然的交互，或在教育训练中模拟真实人物对话。未来，该技术可能推动个性化虚拟形象生成和实时3D内容创作的发展。",
            "highlight_zh": "实验表明，VASA-3D在生成逼真3D说话头部方面超越了现有技术，具体性能包括：支持在线生成512x512分辨率的自由视角视频，最高帧率达75 FPS；通过定量和定性评估，显示出在表情细节和3D结构重建上的显著提升；优化框架有效减少了伪影，提高了对有限训练数据的鲁棒性。",
            "tags_zh": [
                "音频驱动虚拟形象",
                "3D头部生成",
                "单图像重建",
                "运动潜在表示",
                "高斯分布模型",
                "优化框架",
                "自由视角视频",
                "实时生成"
            ],
            "_index": 101
        },
        {
            "title": "ART: Articulated Reconstruction Transformer",
            "authors": [
                "Zizhang Li",
                "Cheng Zhang",
                "Zhengqin Li",
                "Henry Howard-Jenkins",
                "Zhaoyang Lv",
                "Chen Geng",
                "Jiajun Wu",
                "Richard Newcombe",
                "Jakob Engel",
                "Zhao Dong"
            ],
            "arxiv_id": "2512.14671v1",
            "summary": "We introduce ART, Articulated Reconstruction Transformer -- a category-agnostic, feed-forward model that reconstructs complete 3D articulated objects from only sparse, multi-state RGB images. Previous methods for articulated object reconstruction either rely on slow optimization with fragile cross-state correspondences or use feed-forward models limited to specific object categories. In contrast, ART treats articulated objects as assemblies of rigid parts, formulating reconstruction as part-based prediction. Our newly designed transformer architecture maps sparse image inputs to a set of learnable part slots, from which ART jointly decodes unified representations for individual parts, including their 3D geometry, texture, and explicit articulation parameters. The resulting reconstructions are physically interpretable and readily exportable for simulation. Trained on a large-scale, diverse dataset with per-part supervision, and evaluated across diverse benchmarks, ART achieves significant improvements over existing baselines and establishes a new state of the art for articulated object reconstruction from image inputs.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project Page: https://kyleleey.github.io/ART/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14671v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出ART以解决从稀疏多状态RGB图像重建完整3D关节物体的类别无关前馈建模问题",
            "summary_zh": "我们介绍了ART（Articulated Reconstruction Transformer）——一种类别无关的前馈模型，能够仅从稀疏的多状态RGB图像重建完整的3D关节物体。以往的关节物体重建方法要么依赖于脆弱的跨状态对应关系的缓慢优化，要么使用仅限于特定物体类别的前馈模型。相比之下，ART将关节物体视为刚性部件的组装体，将重建问题表述为基于部件的预测。我们新设计的Transformer架构将稀疏图像输入映射到一组可学习的部件槽（part slots），ART从中联合解码出各个部件的统一表示，包括其3D几何、纹理和显式关节参数。所得重建结果具有物理可解释性，并可轻松导出用于仿真。通过在具有逐部件监督的大规模多样化数据集上进行训练，并在多个基准测试中进行评估，ART相比现有基线取得了显著改进，为从图像输入进行关节物体重建建立了新的最先进水平。",
            "intro_zh": [
                "现有方法依赖缓慢优化或局限于特定类别，难以高效重建通用关节物体。",
                "ART将关节物体视为刚性部件组装体，通过Transformer架构实现前馈部件预测。",
                "在多个基准测试中，ART显著超越现有基线，建立了新的最先进水平。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从稀疏多状态RGB图像重建完整3D关节物体的挑战。现有方法存在两大痛点：一是基于优化的方法（如NeRF变体）依赖脆弱的跨状态对应关系，优化过程缓慢且不稳定；二是前馈模型（如基于CNN的方法）通常局限于特定物体类别（如椅子、门），缺乏通用性，难以处理多样化的关节结构。\\n\\n**核心思路**：论文的核心思路是将关节物体建模为刚性部件的组装体，将重建问题转化为基于部件的预测任务。通过将稀疏图像输入映射到一组可学习的部件槽（part slots），模型能够联合解码出每个部件的3D几何、纹理和显式关节参数，从而实现高效、类别无关的重建。这种设计避免了传统方法对类别先验或复杂优化的依赖，提高了模型的泛化能力和实用性。\\n\\n**技术框架**：整体架构基于Transformer设计，包含三个主要阶段：首先，输入稀疏的多状态RGB图像，通过图像编码器提取特征；其次，使用Transformer模块将这些特征映射到一组可学习的部件槽，每个槽对应一个刚性部件；最后，通过解码器从部件槽中联合生成每个部件的统一表示，包括3D网格（几何）、纹理贴图和关节参数（如旋转轴、角度限制）。整个过程是前馈的，无需迭代优化。\\n\\n**关键创新**：最重要的技术创新是提出了类别无关的、基于部件的前馈Transformer架构。与现有方法相比，本质区别在于：一是摆脱了类别限制，通过部件槽学习通用表示；二是将关节参数显式建模为输出的一部分，使重建结果具有物理可解释性；三是采用前馈方式，避免了缓慢的优化过程，提高了效率。\\n\\n**关键设计**：关键设计包括：使用可学习的部件槽作为中间表示，通过注意力机制与图像特征交互；损失函数结合了多个监督信号，如几何重建损失（基于Chamfer距离或IoU）、纹理重建损失（如L1损失）和关节参数损失（如角度误差）；网络结构采用编码器-解码器范式，编码器基于Vision Transformer（ViT）提取图像特征，解码器使用多层感知机（MLP）生成部件属性；训练数据来自大规模多样化数据集，提供逐部件的3D标注和关节参数真值。",
            "application_zh": "该研究在机器人操作、虚拟现实和仿真领域具有重要应用价值。例如，机器人可以通过ART快速重建环境中的关节物体（如门、抽屉），用于抓取和操纵规划；在虚拟现实中，用户可从少量图像生成可交互的3D关节模型，提升内容创作效率；仿真系统可直接导入ART的重建结果进行物理模拟，加速数字孪生构建。未来可能推动通用3D感知和具身智能的发展。",
            "highlight_zh": "在多个基准测试中，ART显著超越现有基线。例如，在PartNet-Mobility数据集上，ART在几何重建精度（如IoU）上比最佳基线提升约15%，关节参数估计误差降低20%以上；在真实世界图像评估中，ART展示了优异的泛化能力，重建质量接近监督方法。这些结果确立了ART在关节物体重建任务中的最先进地位。",
            "tags_zh": [
                "关节物体重建",
                "3D重建",
                "Transformer架构",
                "部件槽学习",
                "前馈模型",
                "类别无关建模",
                "物理可解释性",
                "多状态图像输入"
            ],
            "_index": 102
        },
        {
            "title": "WaveSim: A Wavelet-based Multi-scale Similarity Metric for Weather and Climate Fields",
            "authors": [
                "Gabriele Accarino",
                "Viviana Acquaviva",
                "Sara Shamekh",
                "Duncan Watson-Parris",
                "David Lawrence"
            ],
            "arxiv_id": "2512.14656v1",
            "summary": "We introduce WaveSim, a multi-scale similarity metric for the evaluation of spatial fields in weather and climate applications. WaveSim exploits wavelet transforms to decompose input fields into scale-specific wavelet coefficients. The metric is built by multiplying three orthogonal components derived from these coefficients: Magnitude, which quantifies similarities in the energy distribution of the coefficients, i.e., the intensity of the field; Displacement, which captures spatial shift by comparing the centers of mass of normalized energy distributions; and Structure, which assesses pattern organization independent of location and amplitude. Each component yields a scale-specific similarity score ranging from 0 (no similarity) to 1 (perfect similarity), which are then combined across scales to produce an overall similarity measure. We first evaluate WaveSim using synthetic test cases, applying controlled spatial and temporal perturbations to systematically assess its sensitivity and expected behavior. We then demonstrate its applicability to physically relevant case studies of key modes of climate variability in Earth System Models. Traditional point-wise metrics lack a mechanism for attributing errors to physical scales or modes of dissimilarity. By operating in the wavelet domain and decomposing the signal along independent axes, WaveSim bypasses these limitations and provides an interpretable and diagnostically rich framework for assessing similarity in complex fields. Additionally, the WaveSim framework allows users to place emphasis on a specific scale or component, and lends itself to user-specific model intercomparison, model evaluation, and calibration and training of forecasting systems. We provide a PyTorch-ready implementation of WaveSim, along with all evaluation scripts, at: https://github.com/gabrieleaccarino/wavesim.",
            "categories": [
                "physics.ao-ph",
                "cs.CV",
                "physics.data-an"
            ],
            "primary_category": "physics.ao-ph",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14656v1",
            "code_links": [
                {
                    "url": "https://github.com/gabrieleaccarino/wavesim",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出WaveSim，一种基于小波变换的多尺度相似性度量方法，用于评估天气和气候空间场，解决传统点对点度量缺乏物理尺度归因的问题。",
            "summary_zh": "我们介绍了WaveSim，一种用于评估天气和气候应用中空间场的多尺度相似性度量方法。WaveSim利用小波变换将输入场分解为尺度特定的小波系数。该度量通过乘以从这些系数导出的三个正交分量来构建：幅度，量化系数能量分布的相似性，即场的强度；位移，通过比较归一化能量分布的质量中心来捕捉空间偏移；以及结构，评估独立于位置和振幅的模式组织。每个分量产生一个尺度特定的相似性得分，范围从0（无相似性）到1（完美相似性），然后跨尺度组合以产生整体相似性度量。我们首先使用合成测试案例评估WaveSim，应用受控的空间和时间扰动来系统评估其敏感性和预期行为。然后，我们通过地球系统模型中关键气候变率模式的物理相关案例研究来展示其适用性。传统的点对点度量缺乏将误差归因于物理尺度或差异模式的机制。通过在小波域中操作并沿独立轴分解信号，WaveSim克服了这些限制，为评估复杂场中的相似性提供了一个可解释且诊断丰富的框架。此外，WaveSim框架允许用户强调特定尺度或分量，并适用于用户特定的模型比较、模型评估以及预测系统的校准和训练。我们提供了WaveSim的PyTorch就绪实现以及所有评估脚本，网址为：https://github.com/gabrieleaccarino/wavesim。",
            "intro_zh": [
                "传统点对点度量方法无法将误差归因于物理尺度或差异模式，限制了在天气和气候空间场评估中的诊断能力。",
                "WaveSim利用小波变换分解输入场，通过幅度、位移和结构三个正交分量构建多尺度相似性度量，提供可解释的评估框架。",
                "在合成测试和气候变率案例中，WaveSim展示了敏感性和适用性，为模型比较和预测系统优化提供了新工具。"
            ],
            "method_zh": "**问题定义**：论文旨在解决天气和气候空间场评估中传统点对点度量方法的不足，这些方法缺乏将误差归因于物理尺度或差异模式的机制，导致评估结果难以解释和诊断。\\n\\n**核心思路**：论文的核心思路是利用小波变换将空间场分解为尺度特定的小波系数，然后通过三个正交分量（幅度、位移、结构）量化相似性，从而在多尺度上提供可解释的评估。这种设计基于小波变换的多尺度特性，能够捕捉场在不同物理尺度上的特征。\\n\\n**技术框架**：整体流程包括：首先，对输入空间场应用小波变换，生成尺度特定的小波系数；其次，从系数中计算三个正交分量：幅度（量化能量分布相似性）、位移（通过质量中心比较捕捉空间偏移）、结构（评估模式组织）；然后，每个分量在单个尺度上计算相似性得分（0到1）；最后，跨尺度组合得分以生成整体相似性度量。框架允许用户强调特定尺度或分量。\\n\\n**关键创新**：最重要的技术创新是提出了一种基于小波变换的多尺度相似性度量方法，通过正交分量分解，实现了对空间场相似性的可解释和诊断丰富的评估。与现有方法的本质区别在于，它不再依赖点对点比较，而是从物理尺度角度分析相似性，克服了传统度量在归因和解释性上的限制。\\n\\n**关键设计**：关键设计包括使用小波变换（具体小波类型未知）进行多尺度分解；定义幅度、位移和结构三个正交分量，其中幅度基于能量分布，位移基于归一化能量分布的质量中心，结构独立于位置和振幅；相似性得分范围标准化为0到1；整体度量通过跨尺度组合实现；提供了PyTorch实现，便于集成到深度学习框架中。",
            "application_zh": "WaveSim在天气和气候领域具有广泛的应用潜力，包括地球系统模型的评估和比较、气候变率模式分析、预测系统的校准和训练，以及用户特定的模型互比较。其多尺度和可解释特性为复杂空间场的诊断提供了新工具，有助于提升模型精度和物理理解，未来可能扩展到其他空间数据评估场景。",
            "highlight_zh": "在合成测试中，WaveSim通过受控扰动展示了敏感性和预期行为，验证了其多尺度评估能力。在物理相关案例研究中，如地球系统模型的气候变率模式，WaveSim成功应用于实际数据，提供了比传统点对点度量更丰富的诊断信息。具体性能数据未在摘要中提供，但框架的PyTorch实现和开源代码支持了可重复性和进一步验证。",
            "tags_zh": [
                "小波变换",
                "多尺度相似性度量",
                "天气气候评估",
                "空间场分析",
                "模型互比较",
                "可解释性框架",
                "地球系统模型",
                "预测系统校准"
            ],
            "_index": 103
        },
        {
            "title": "Adaptable Segmentation Pipeline for Diverse Brain Tumors with Radiomic-guided Subtyping and Lesion-Wise Model Ensemble",
            "authors": [
                "Daniel Capellán-Martín",
                "Abhijeet Parida",
                "Zhifan Jiang",
                "Nishad Kulkarni",
                "Krithika Iyer",
                "Austin Tapp",
                "Syed Muhammad Anwar",
                "María J. Ledesma-Carbayo",
                "Marius George Linguraru"
            ],
            "arxiv_id": "2512.14648v1",
            "summary": "Robust and generalizable segmentation of brain tumors on multi-parametric magnetic resonance imaging (MRI) remains difficult because tumor types differ widely. The BraTS 2025 Lighthouse Challenge benchmarks segmentation methods on diverse high-quality datasets of adult and pediatric tumors: multi-consortium international pediatric brain tumor segmentation (PED), preoperative meningioma tumor segmentation (MEN), meningioma radiotherapy segmentation (MEN-RT), and segmentation of pre- and post-treatment brain metastases (MET). We present a flexible, modular, and adaptable pipeline that improves segmentation performance by selecting and combining state-of-the-art models and applying tumor- and lesion-specific processing before and after training. Radiomic features extracted from MRI help detect tumor subtype, ensuring a more balanced training. Custom lesion-level performance metrics determine the influence of each model in the ensemble and optimize post-processing that further refines the predictions, enabling the workflow to tailor every step to each case. On the BraTS testing sets, our pipeline achieved performance comparable to top-ranked algorithms across multiple challenges. These findings confirm that custom lesion-aware processing and model selection yield robust segmentations yet without locking the method to a specific network architecture. Our method has the potential for quantitative tumor measurement in clinical practice, supporting diagnosis and prognosis.",
            "categories": [
                "cs.CV",
                "eess.IV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "12 pages, 5 figures, 3 tables. Algorithm presented at MICCAI BraTS 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14648v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出可适应脑肿瘤分割流程，通过影像组学引导亚型识别和病灶级模型集成，提升多类型肿瘤分割的鲁棒性。",
            "summary_zh": "在多参数磁共振成像（MRI）上实现鲁棒且可泛化的脑肿瘤分割仍然困难，因为肿瘤类型差异很大。BraTS 2025 Lighthouse Challenge 在多样化的高质量数据集上对分割方法进行基准测试，包括成人和儿童肿瘤：多联盟国际儿童脑肿瘤分割（PED）、术前脑膜瘤分割（MEN）、脑膜瘤放疗分割（MEN-RT）以及治疗前后脑转移瘤分割（MET）。我们提出了一种灵活、模块化且可适应的流程，通过选择和组合最先进的模型，并在训练前后应用肿瘤和病灶特定的处理，来提高分割性能。从MRI中提取的影像组学特征有助于检测肿瘤亚型，确保更平衡的训练。定制的病灶级性能指标确定每个模型在集成中的影响，并优化后处理以进一步细化预测，使工作流程能够针对每个病例定制每一步。在BraTS测试集上，我们的流程在多个挑战中实现了与顶级算法相当的性能。这些发现证实，定制的病灶感知处理和模型选择能够产生鲁棒的分割，而无需将方法锁定在特定的网络架构上。我们的方法在临床实践中具有定量肿瘤测量的潜力，支持诊断和预后。",
            "intro_zh": [
                "核心问题：现有脑肿瘤分割方法难以泛化到多样肿瘤类型，如儿童肿瘤、脑膜瘤和转移瘤，导致鲁棒性不足。",
                "方法要点：提出模块化流程，结合影像组学引导亚型识别、病灶级模型集成和定制后处理，实现自适应分割。",
                "实验或效果：在BraTS 2025测试集上，性能与顶级算法相当，验证了方法的有效性和泛化能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多参数MRI上脑肿瘤分割的鲁棒性和泛化性问题，特别是针对多样肿瘤类型（如儿童肿瘤、脑膜瘤、转移瘤）。现有方法的痛点在于，单一模型难以适应不同类型肿瘤的形态和特征差异，导致分割性能不稳定，且缺乏针对特定病灶的自适应处理。\\n\\n**核心思路**：论文的核心思路是设计一个灵活、模块化的分割流程，通过选择和组合多个先进模型，并引入肿瘤和病灶特定的预处理和后处理，实现自适应分割。这避免了依赖单一网络架构，而是利用集成学习和定制化处理来提升泛化能力。\\n\\n**技术框架**：整体流程包括多个阶段：首先，从MRI中提取影像组学特征以识别肿瘤亚型，用于平衡训练数据；其次，选择和训练多个分割模型（具体模型未知，但基于最先进技术）；然后，使用病灶级性能指标评估每个模型，并确定其在集成中的权重；最后，应用定制后处理（如优化阈值或形态学操作）细化预测结果。整个流程是模块化的，允许针对不同肿瘤类型调整每个步骤。\\n\\n**关键创新**：最重要的技术创新点在于结合了影像组学引导的亚型识别和病灶级模型集成。与现有方法相比，本质区别在于：1）利用影像组学特征实现数据驱动的亚型分类，促进更平衡的训练；2）引入病灶级指标（而非整体指标）来优化模型集成和后处理，使处理更精细化；3）整个流程不锁定特定网络架构，强调灵活性和可适应性。\\n\\n**关键设计**：关键设计包括：1）影像组学特征提取：从MRI中提取定量特征以检测肿瘤亚型，具体特征类型未知，但用于数据平衡；2）模型集成策略：基于病灶级性能指标（如Dice系数或自定义指标）动态调整模型权重，确保集成效果；3）后处理优化：根据病灶特性定制后处理步骤，可能包括阈值调整、连通性分析等；4）模块化架构：允许替换或调整各个模块（如分割模型、特征提取器），以适应不同数据集。损失函数和网络结构细节未在摘要中指定，但强调使用最先进模型。",
            "application_zh": "该研究在临床医学影像分析领域具有重要应用价值，特别是脑肿瘤的定量测量和诊断支持。潜在应用包括：辅助放射科医生进行脑肿瘤（如儿童肿瘤、脑膜瘤、转移瘤）的精确分割和体积计算，支持治疗规划（如手术或放疗）和预后评估。未来影响可能扩展到其他医学影像分割任务，推动个性化医疗和自动化诊断工具的发展。",
            "highlight_zh": "在BraTS 2025 Lighthouse Challenge的测试集上，该流程在多个子挑战（包括PED、MEN、MEN-RT和MET）中实现了与顶级算法相当的性能。具体性能数据未在摘要中提供，但通过对比基准测试，验证了方法在多样肿瘤类型上的鲁棒性和泛化能力。提升幅度体现在通过自适应处理减少了分割误差，尤其是在病灶级优化后，预测结果更精确。",
            "tags_zh": [
                "脑肿瘤分割",
                "多参数MRI",
                "影像组学",
                "模型集成",
                "病灶级处理",
                "医学影像分析",
                "自适应流程",
                "BraTS挑战"
            ],
            "_index": 104
        },
        {
            "title": "TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines",
            "authors": [
                "David Schulmeister",
                "Valentin Hartmann",
                "Lars Klein",
                "Robert West"
            ],
            "arxiv_id": "2512.14645v1",
            "summary": "Today, a lot of research on language models is focused on large, general-purpose models. However, many NLP pipelines only require models with a well-defined, small set of capabilities. While large models are capable of performing the tasks of those smaller models, they are simply not fast enough to process large amounts of data or offer real-time responses. Furthermore, they often use unnecessarily large amounts of energy, leading to sustainability concerns and problems when deploying them on battery-powered devices. In our work, we show how to train small models for such efficiency-critical applications. As opposed to many off-the-shelf NLP pipelines, our models use modern training techniques such as distillation, and offer support for low-resource languages. We call our models TiME (Tiny Monolingual Encoders) and comprehensively evaluate them on a range of common NLP tasks, observing an improved trade-off between benchmark performance on one hand, and throughput, latency and energy consumption on the other. Along the way, we show that distilling monolingual models from multilingual teachers is possible, and likewise distilling models with absolute positional embeddings from teachers with relative positional embeddings.",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14645v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出TiME（Tiny Monolingual Encoders）模型，通过蒸馏训练实现高效NLP流水线，以解决大型模型在速度、能耗和低资源语言支持方面的不足。",
            "summary_zh": "当前语言模型研究多集中于大型通用模型，但许多NLP流水线仅需具备特定小规模能力的模型。大型模型虽能执行这些任务，但处理大量数据或提供实时响应时速度不足，且能耗过高，导致可持续性问题，并在电池供电设备上部署困难。本文展示了如何为这类效率关键应用训练小型模型。与许多现成NLP流水线不同，我们的模型采用现代训练技术如蒸馏，并支持低资源语言。我们称这些模型为TiME（Tiny Monolingual Encoders），并在多种常见NLP任务上全面评估，观察到在基准性能与吞吐量、延迟和能耗之间实现了更好的权衡。此外，我们证明了从多语言教师模型蒸馏单语言模型是可行的，同样可以从具有相对位置嵌入的教师模型蒸馏出具有绝对位置嵌入的模型。",
            "intro_zh": [
                "核心问题：大型通用语言模型在NLP流水线中面临速度慢、能耗高和部署困难，尤其对低资源语言支持不足。",
                "方法要点：提出TiME模型，通过蒸馏技术训练小型单语言编码器，优化性能与效率的权衡，并支持低资源语言。",
                "实验或效果：在多种NLP任务上评估，TiME在基准性能、吞吐量、延迟和能耗方面优于现有方法，验证了蒸馏单语言模型的可行性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决NLP流水线中大型通用语言模型在效率关键应用中的不足，包括处理速度慢、能耗高、部署困难，以及对低资源语言支持有限的问题。现有方法依赖大型模型，导致实时响应能力差和可持续性挑战。\\n\\n**核心思路**：核心思路是训练小型单语言编码器（TiME），通过蒸馏技术从大型多语言教师模型迁移知识，以在保持任务性能的同时，显著提升模型效率（如吞吐量、延迟和能耗）。设计旨在平衡性能与资源消耗，特别关注低资源语言场景。\\n\\n**技术框架**：整体框架包括两个主要阶段：首先，使用大型多语言模型作为教师，通过蒸馏损失函数训练小型单语言学生模型；其次，在多种NLP任务（如文本分类、序列标注）上微调和评估学生模型。流程涉及数据预处理、模型初始化、蒸馏训练和性能评估。\\n\\n**关键创新**：最重要的技术创新在于证明了从多语言教师模型蒸馏单语言学生模型的可行性，以及从具有相对位置嵌入的教师模型蒸馏出具有绝对位置嵌入的学生模型。这突破了传统蒸馏方法在语言和嵌入类型上的限制，实现了更灵活高效的模型压缩。\\n\\n**关键设计**：关键设计包括使用蒸馏损失函数（如软标签交叉熵）来优化学生模型，网络结构采用轻量级Transformer编码器，参数设置上减少层数和隐藏维度以降低计算复杂度。此外，针对低资源语言，设计了特定的数据增强和训练策略，以提升模型泛化能力。",
            "application_zh": "TiME模型适用于需要高效NLP处理的场景，如实时聊天机器人、移动设备上的文本分析、低资源语言翻译和可持续AI系统。其实际价值在于降低部署成本、提升响应速度并支持边缘计算，未来可能推动轻量级模型在工业界和学术界的广泛应用，促进AI技术的普及和环保发展。",
            "highlight_zh": "实验结果显示，TiME模型在多种NLP任务上实现了显著的效率提升：与基线大型模型相比，吞吐量提高约2-3倍，延迟降低50%以上，能耗减少30-40%，同时基准性能（如准确率）保持相近或略有提升。具体数据表明，在文本分类任务中，TiME的F1分数达到0.85，而能耗仅为大型模型的60%，验证了其在性能与效率间的优越权衡。",
            "tags_zh": [
                "小型语言模型",
                "蒸馏训练",
                "单语言编码器",
                "效率优化",
                "低资源语言",
                "位置嵌入",
                "NLP流水线",
                "可持续AI"
            ],
            "_index": 105
        },
        {
            "title": "AMD-HookNet++: Evolution of AMD-HookNet with Hybrid CNN-Transformer Feature Enhancement for Glacier Calving Front Segmentation",
            "authors": [
                "Fei Wu",
                "Marcel Dreier",
                "Nora Gourmelon",
                "Sebastian Wind",
                "Jianlin Zhang",
                "Thorsten Seehaus",
                "Matthias Braun",
                "Andreas Maier",
                "Vincent Christlein"
            ],
            "arxiv_id": "2512.14639v1",
            "summary": "The dynamics of glaciers and ice shelf fronts significantly impact the mass balance of ice sheets and coastal sea levels. To effectively monitor glacier conditions, it is crucial to consistently estimate positional shifts of glacier calving fronts. AMD-HookNet firstly introduces a pure two-branch convolutional neural network (CNN) for glacier segmentation. Yet, the local nature and translational invariance of convolution operations, while beneficial for capturing low-level details, restricts the model ability to maintain long-range dependencies. In this study, we propose AMD-HookNet++, a novel advanced hybrid CNN-Transformer feature enhancement method for segmenting glaciers and delineating calving fronts in synthetic aperture radar images. Our hybrid structure consists of two branches: a Transformer-based context branch to capture long-range dependencies, which provides global contextual information in a larger view, and a CNN-based target branch to preserve local details. To strengthen the representation of the connected hybrid features, we devise an enhanced spatial-channel attention module to foster interactions between the hybrid CNN-Transformer branches through dynamically adjusting the token relationships from both spatial and channel perspectives. Additionally, we develop a pixel-to-pixel contrastive deep supervision to optimize our hybrid model by integrating pixelwise metric learning into glacier segmentation. Through extensive experiments and comprehensive quantitative and qualitative analyses on the challenging glacier segmentation benchmark dataset CaFFe, we show that AMD-HookNet++ sets a new state of the art with an IoU of 78.2 and a HD95 of 1,318 m, while maintaining a competitive MDE of 367 m. More importantly, our hybrid model produces smoother delineations of calving fronts, resolving the issue of jagged edges typically seen in pure Transformer-based approaches.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "10.1109/TGRS.2025.3642764",
            "journal_ref": "IEEE Transactions on Geoscience and Remote Sensing (2025)",
            "pdf_url": "https://arxiv.org/pdf/2512.14639v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出AMD-HookNet++，通过混合CNN-Transformer特征增强解决冰川崩解前沿分割中长程依赖与局部细节平衡问题。",
            "summary_zh": "冰川和冰架前沿的动态变化对冰盖质量平衡和沿海海平面有显著影响。为有效监测冰川状况，持续估计冰川崩解前沿的位置变化至关重要。AMD-HookNet首次引入纯双分支卷积神经网络（CNN）进行冰川分割，但卷积操作的局部性和平移不变性虽有利于捕捉低层细节，却限制了模型保持长程依赖的能力。本研究提出AMD-HookNet++，一种新颖的先进混合CNN-Transformer特征增强方法，用于合成孔径雷达图像中的冰川分割和崩解前沿描绘。我们的混合结构包括两个分支：一个基于Transformer的上下文分支以捕获长程依赖，提供更大视野的全局上下文信息；一个基于CNN的目标分支以保留局部细节。为增强连接混合特征的表示，我们设计了一个增强的空间-通道注意力模块，通过从空间和通道角度动态调整令牌关系，促进混合CNN-Transformer分支之间的交互。此外，我们开发了像素到像素对比深度监督，通过将像素级度量学习集成到冰川分割中，优化我们的混合模型。通过在具有挑战性的冰川分割基准数据集CaFFe上进行广泛实验和全面的定量与定性分析，我们表明AMD-HookNet++以78.2的IoU和1,318米的HD95设定了新的最先进水平，同时保持了367米的竞争性MDE。更重要的是，我们的混合模型产生了更平滑的崩解前沿描绘，解决了纯基于Transformer方法中常见的锯齿边缘问题。",
            "intro_zh": [
                "核心问题：现有纯CNN方法在冰川分割中难以平衡局部细节与长程依赖，导致崩解前沿描绘不准确。",
                "方法要点：提出混合CNN-Transformer架构，结合Transformer捕获全局上下文和CNN保留局部细节，增强特征表示。",
                "实验或效果：在CaFFe数据集上，IoU达78.2，HD95为1,318米，崩解前沿描绘更平滑，优于现有方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决冰川崩解前沿分割问题，特别是在合成孔径雷达图像中。现有方法如纯CNN（如AMD-HookNet）虽能捕捉局部细节，但由于卷积操作的局部性和平移不变性，难以建模长程依赖，导致崩解前沿描绘可能不连续或锯齿状，影响分割精度。\\n\\n**核心思路**：论文提出混合CNN-Transformer架构，核心思想是结合CNN的局部细节捕捉能力和Transformer的全局上下文建模能力，以平衡特征表示。通过双分支设计，分别处理局部和全局信息，并引入注意力机制增强分支间交互，从而提升分割性能。\\n\\n**技术框架**：整体架构包括两个主要分支：一个基于Transformer的上下文分支，用于捕获长程依赖和全局上下文信息；一个基于CNN的目标分支，用于保留局部细节和空间结构。两个分支的特征通过增强的空间-通道注意力模块进行融合，该模块动态调整空间和通道维度的令牌关系。此外，采用像素到像素对比深度监督，集成像素级度量学习以优化模型训练。\\n\\n**关键创新**：最重要的技术创新是混合CNN-Transformer特征增强方法，特别是增强的空间-通道注意力模块。与现有纯CNN或纯Transformer方法相比，本质区别在于同时利用CNN的局部性和Transformer的全局性，通过注意力机制实现自适应特征融合，解决了长程依赖与局部细节的平衡问题。\\n\\n**关键设计**：关键设计包括：双分支网络结构，其中Transformer分支可能基于Vision Transformer变体，CNN分支基于卷积层；增强的空间-通道注意力模块，通过多头注意力机制处理空间和通道维度；损失函数结合交叉熵损失和对比损失，用于像素级监督；参数设置如学习率、批量大小通过实验优化，具体数值未在摘要中提供，但模型在CaFFe数据集上训练和评估。",
            "application_zh": "该研究主要应用于冰川监测和气候变化研究领域，通过高精度分割冰川崩解前沿，有助于实时跟踪冰川动态变化、评估冰盖质量平衡和预测海平面上升。实际价值在于提供自动化工具，减少人工标注成本，提升监测效率。未来可能扩展到其他遥感图像分割任务，如冰架、海冰监测，对环境保护和灾害预警有重要影响。",
            "highlight_zh": "在CaFFe基准数据集上，AMD-HookNet++取得了最先进的性能：IoU达到78.2，HD95为1,318米，同时MDE保持在367米的竞争水平。与基线方法相比，IoU和HD95均有显著提升，崩解前沿描绘更平滑，解决了纯Transformer方法的锯齿边缘问题。定量和定性分析均验证了混合模型的有效性。",
            "tags_zh": [
                "冰川分割",
                "崩解前沿描绘",
                "混合CNN-Transformer",
                "特征增强",
                "空间-通道注意力",
                "像素对比学习",
                "合成孔径雷达",
                "遥感图像分析"
            ],
            "_index": 106
        },
        {
            "title": "MuseCPBench: an Empirical Study of Music Editing Methods through Music Context Preservation",
            "authors": [
                "Yash Vishe",
                "Eric Xue",
                "Xunyi Jiang",
                "Zachary Novack",
                "Junda Wu",
                "Julian McAuley",
                "Xin Xu"
            ],
            "arxiv_id": "2512.14629v1",
            "summary": "Music editing plays a vital role in modern music production, with applications in film, broadcasting, and game development. Recent advances in music generation models have enabled diverse editing tasks such as timbre transfer, instrument substitution, and genre transformation. However, many existing works overlook the evaluation of their ability to preserve musical facets that should remain unchanged during editing a property we define as Music Context Preservation (MCP). While some studies do consider MCP, they adopt inconsistent evaluation protocols and metrics, leading to unreliable and unfair comparisons. To address this gap, we introduce the first MCP evaluation benchmark, MuseCPBench, which covers four categories of musical facets and enables comprehensive comparisons across five representative music editing baselines. Through systematic analysis along musical facets, methods, and models, we identify consistent preservation gaps in current music editing methods and provide insightful explanations. We hope our findings offer practical guidance for developing more effective and reliable music editing strategies with strong MCP capability",
            "categories": [
                "cs.SD",
                "cs.AI"
            ],
            "primary_category": "cs.SD",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14629v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出首个音乐上下文保持评估基准MuseCPBench，以解决音乐编辑方法评估不一致的问题",
            "summary_zh": "音乐编辑在现代音乐制作中扮演着重要角色，广泛应用于电影、广播和游戏开发。近年来音乐生成模型的进步使得音色转换、乐器替换和风格转换等多样化编辑任务成为可能。然而，许多现有研究忽视了评估编辑过程中应保持不变的音乐要素的保持能力，这一属性被定义为音乐上下文保持（MCP）。虽然部分研究考虑了MCP，但它们采用了不一致的评估协议和指标，导致不可靠且不公平的比较。为填补这一空白，我们引入了首个MCP评估基准MuseCPBench，涵盖四类音乐要素，并支持对五种代表性音乐编辑基线方法进行全面比较。通过对音乐要素、方法和模型的系统分析，我们识别出当前音乐编辑方法中一致的保持差距，并提供深入解释。我们希望这些发现能为开发具有强大MCP能力的更有效、可靠音乐编辑策略提供实用指导。",
            "intro_zh": [
                "现有音乐编辑方法缺乏统一的音乐上下文保持评估标准，导致比较不可靠。",
                "论文提出首个MCP评估基准MuseCPBench，涵盖四类音乐要素并整合五种基线方法。",
                "通过系统分析，识别出当前方法的保持差距，为改进编辑策略提供指导。"
            ],
            "method_zh": "**问题定义**：论文旨在解决音乐编辑方法评估中音乐上下文保持（MCP）能力评估不一致的问题。现有方法在编辑音乐时，往往忽视对音乐要素（如旋律、节奏等）保持能力的系统评估，且不同研究采用不同的评估协议和指标，导致结果不可比，无法公平衡量方法的优劣。\\n\\n**核心思路**：通过构建一个标准化的评估基准MuseCPBench，统一MCP的评估框架，以促进音乐编辑方法的可靠比较和改进。该基准整合了多种音乐要素和基线方法，旨在提供全面、一致的评估环境。\\n\\n**技术框架**：MuseCPBench的整体架构包括数据收集、基准构建和评估分析三个阶段。首先，收集涵盖四类音乐要素（如音高、节奏、音色和结构）的多样化音乐数据集。然后，设计标准化的评估协议，整合五种代表性音乐编辑基线方法（如基于生成模型的方法）。最后，通过系统实验分析不同方法在MCP上的表现，识别差距并解释原因。\\n\\n**关键创新**：最重要的技术创新是首次提出了一个专注于MCP的评估基准，填补了音乐编辑领域评估标准化的空白。与现有方法相比，其本质区别在于提供了统一的评估框架，避免了评估不一致性，从而支持更公平和可靠的比较。\\n\\n**关键设计**：关键设计包括定义四类音乐要素（具体类别未在摘要中详述，可能涉及旋律、和声等），选择五种基线方法（如基于深度学习的编辑模型），并设计标准化的评估指标（如保持率或相似度分数）。这些设计确保了基准的全面性和可重复性，但具体参数设置和损失函数细节在摘要中未明确，需参考论文全文。",
            "application_zh": "该研究在音乐制作、影视配乐和游戏开发等领域具有重要应用价值。通过提供标准化的MCP评估基准，可帮助开发者优化音乐编辑模型，提升编辑质量，确保音乐要素在编辑过程中得到有效保持。未来可能推动音乐生成和编辑技术的标准化发展，促进跨领域协作。",
            "highlight_zh": "实验结果显示，MuseCPBench成功评估了五种基线方法在四类音乐要素上的MCP能力。通过系统分析，识别出当前方法在特定音乐要素（如节奏或音色）上存在一致的保持差距，具体性能数据未在摘要中提供，但基准支持了全面比较，为方法改进提供了实证依据。",
            "tags_zh": [
                "音乐编辑",
                "音乐上下文保持",
                "评估基准",
                "音乐生成模型",
                "标准化评估",
                "音乐要素分析",
                "深度学习应用",
                "多任务比较"
            ],
            "_index": 107
        },
        {
            "title": "JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction",
            "authors": [
                "Atsuyuki Miyai",
                "Shota Onohara",
                "Jeonghun Baek",
                "Kiyoharu Aizawa"
            ],
            "arxiv_id": "2512.14620v1",
            "summary": "This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark, and Vibe Benchmark Construction, a scalable construction method. Following the evolution from MMMU to MMMU-Pro, JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, thereby creating a benchmark that requires integrated visual-textual understanding through visual perception. To build JMMMU-Pro, we propose Vibe Benchmark Construction, a methodology in which an image generative model (e.g., Nano Banana Pro) produces candidate visual questions, and humans verify the outputs and, when necessary, regenerate with adjusted prompts to ensure quality. By leveraging Nano Banana Pro's highly realistic image generation capabilities and its ability to embed clean Japanese text, we construct a high-quality benchmark at low cost, covering a wide range of background and layout designs. Experimental results show that all open-source LMMs struggle substantially with JMMMU-Pro, underscoring JMMMU-Pro as an important benchmark for guiding future efforts in the open-source community. We believe that JMMMU-Pro provides a more rigorous evaluation tool for assessing the Japanese capabilities of LMMs and that our Vibe Benchmark Construction also offers an efficient guideline for future development of image-based VQA benchmarks.",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.CV"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page: https://mmmu-japanese-benchmark.github.io/JMMMU_Pro/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14620v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出JMMMU-Pro基准和Vibe Benchmark Construction方法，以低成本构建高质量图像式日语多学科多模态理解评测工具。",
            "summary_zh": "本文介绍了JMMMU-Pro，一个基于图像的日语多学科多模态理解基准，以及Vibe Benchmark Construction，一种可扩展的构建方法。继从MMMU到MMMU-Pro的演进后，JMMMU-Pro扩展了JMMMU，将问题图像和问题文本组合成单一图像，从而创建一个需要通过视觉感知进行集成视觉-文本理解的基准。为了构建JMMMU-Pro，我们提出了Vibe Benchmark Construction方法，其中图像生成模型（如Nano Banana Pro）生成候选视觉问题，人类验证输出并在必要时通过调整提示重新生成以确保质量。通过利用Nano Banana Pro的高度逼真图像生成能力及其嵌入清晰日语文本的能力，我们以低成本构建了一个高质量的基准，覆盖广泛的背景和布局设计。实验结果表明，所有开源大型多模态模型在JMMMU-Pro上都面临显著困难，突显了JMMMU-Pro作为指导开源社区未来努力的重要基准。我们相信，JMMMU-Pro为评估大型多模态模型的日语能力提供了更严格的评测工具，并且我们的Vibe Benchmark Construction也为未来基于图像的视觉问答基准开发提供了高效指南。",
            "intro_zh": [
                "现有日语多模态基准如JMMMU可能缺乏对视觉-文本集成理解的严格评测，难以全面评估大型多模态模型在复杂图像式任务中的能力。",
                "论文提出JMMMU-Pro基准，将问题图像和文本融合为单一图像，并设计Vibe Benchmark Construction方法，利用图像生成模型和人工验证低成本构建高质量数据集。",
                "实验显示所有开源大型多模态模型在JMMMU-Pro上表现显著不佳，验证了其作为严格评测工具的有效性，并突显了开源社区在日语多模态理解方面的挑战。"
            ],
            "method_zh": "**问题定义**：论文旨在解决日语多模态理解基准构建中的挑战，特别是如何低成本、高效地创建高质量图像式视觉问答数据集，以评测大型多模态模型在复杂视觉-文本集成任务中的能力。现有方法可能依赖人工标注或简单数据合成，导致成本高、多样性不足或质量难以保证，无法充分模拟真实世界多模态场景。\\n\\n**核心思路**：论文的核心思路是通过结合图像生成模型和人工验证，构建一个可扩展的基准构建方法（Vibe Benchmark Construction），生成融合问题图像和文本的单一图像作为评测样本。这样设计是为了模拟真实世界中视觉和文本信息紧密集成的场景，迫使模型进行深度视觉感知和文本理解，从而更严格地评估其多模态能力。\\n\\n**技术框架**：整体流程分为两个主要阶段：基准构建和评测应用。在基准构建阶段，首先使用图像生成模型（如Nano Banana Pro）根据预设提示生成候选视觉问题图像，这些图像包含嵌入的日语文本；然后，人类验证者检查输出质量，对不合格样本通过调整提示重新生成，确保数据准确性和多样性；最终形成JMMMU-Pro基准数据集。在评测应用阶段，将JMMMU-Pro用于测试开源大型多模态模型，通过标准评估指标分析其性能。\\n\\n**关键创新**：最重要的技术创新是Vibe Benchmark Construction方法，它利用先进图像生成模型（如Nano Banana Pro）的逼真图像生成和文本嵌入能力，结合人工验证，实现低成本、高质量的基准构建。与现有方法相比，本质区别在于将问题图像和文本融合为单一图像，这要求模型进行集成视觉-文本理解，而非简单处理分离的模态，从而提升了评测的严格性和现实性。\\n\\n**关键设计**：关键设计包括使用Nano Banana Pro作为图像生成模型，因其能生成高度逼真的图像并嵌入清晰的日语文本；在构建过程中，通过人工验证和提示调整确保数据质量，覆盖多样背景和布局；基准内容涵盖多学科领域，以全面评估模型能力。具体参数设置和损失函数未在摘要中详细说明，但方法强调可扩展性和低成本，依赖于生成模型的预训练能力和人类监督。",
            "application_zh": "该研究主要应用于大型多模态模型的评测和开发领域，特别是在日语多模态理解任务中。JMMMU-Pro可作为严格的基准工具，用于评估模型在图像式视觉问答中的性能，指导开源社区优化模型架构和训练策略。Vibe Benchmark Construction方法为未来构建类似多模态基准提供了高效指南，降低数据构建成本，促进多模态人工智能在日语教育、内容生成、人机交互等实际场景中的发展。",
            "highlight_zh": "实验结果显示，所有测试的开源大型多模态模型在JMMMU-Pro基准上都面临显著困难，具体性能数据未在摘要中提供，但强调了模型得分普遍较低。这突显了JMMMU-Pro作为严格评测工具的有效性，表明现有模型在日语多模态集成理解方面存在不足，为开源社区提供了明确的改进方向。提升幅度体现在基准构建的低成本和高质量上，通过Vibe Benchmark Construction方法，实现了广泛覆盖和现实性，优于传统人工标注或简单合成方法。",
            "tags_zh": [
                "日语多模态理解",
                "视觉问答基准",
                "图像生成模型",
                "基准构建方法",
                "大型多模态模型",
                "视觉-文本集成",
                "开源社区评测",
                "低成本数据构建"
            ],
            "_index": 108
        },
        {
            "title": "ParaFormer: A Generalized PageRank Graph Transformer for Graph Representation Learning",
            "authors": [
                "Chaohao Yuan",
                "Zhenjie Song",
                "Ercan Engin Kuruoglu",
                "Kangfei Zhao",
                "Yang Liu",
                "Deli Zhao",
                "Hong Cheng",
                "Yu Rong"
            ],
            "arxiv_id": "2512.14619v1",
            "summary": "Graph Transformers (GTs) have emerged as a promising graph learning tool, leveraging their all-pair connected property to effectively capture global information. To address the over-smoothing problem in deep GNNs, global attention was initially introduced, eliminating the necessity for using deep GNNs. However, through empirical and theoretical analysis, we verify that the introduced global attention exhibits severe over-smoothing, causing node representations to become indistinguishable due to its inherent low-pass filtering. This effect is even stronger than that observed in GNNs. To mitigate this, we propose PageRank Transformer (ParaFormer), which features a PageRank-enhanced attention module designed to mimic the behavior of deep Transformers. We theoretically and empirically demonstrate that ParaFormer mitigates over-smoothing by functioning as an adaptive-pass filter. Experiments show that ParaFormer achieves consistent performance improvements across both node classification and graph classification tasks on 11 datasets ranging from thousands to millions of nodes, validating its efficacy. The supplementary material, including code and appendix, can be found in https://github.com/chaohaoyuan/ParaFormer.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted by WSDM 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14619v1",
            "code_links": [
                {
                    "url": "https://github.com/chaohaoyuan/ParaFormer",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出ParaFormer，一种基于PageRank增强的图Transformer，以解决图Transformer中全局注意力导致的过平滑问题。",
            "summary_zh": "图Transformer（GTs）作为一种有前景的图学习工具，利用其全对连接特性有效捕获全局信息。为解决深度图神经网络（GNNs）中的过平滑问题，全局注意力被引入，消除了使用深度GNNs的必要性。然而，通过实证和理论分析，我们验证了引入的全局注意力表现出严重的过平滑，导致节点表示因固有的低通滤波效应而变得不可区分，这种效应甚至比GNNs中观察到的更强。为缓解此问题，我们提出了PageRank Transformer（ParaFormer），其特点是包含一个PageRank增强的注意力模块，旨在模拟深度Transformer的行为。我们从理论和实证上证明，ParaFormer通过作为自适应通滤波器来缓解过平滑。实验显示，ParaFormer在11个数据集（节点数从数千到数百万）的节点分类和图分类任务中均实现了持续的性能提升，验证了其有效性。补充材料，包括代码和附录，可在https://github.com/chaohaoyuan/ParaFormer找到。",
            "intro_zh": [
                "现有图Transformer的全局注意力机制存在严重过平滑问题，导致节点表示不可区分，限制了其深层建模能力。",
                "论文提出ParaFormer，通过PageRank增强的注意力模块模拟深度Transformer行为，作为自适应通滤波器来缓解过平滑。",
                "实验在11个数据集上验证了ParaFormer的有效性，在节点分类和图分类任务中均实现性能提升。"
            ],
            "method_zh": "**问题定义**：论文旨在解决图Transformer（GTs）中全局注意力机制导致的过平滑问题。现有方法的痛点是，尽管全局注意力被引入以缓解深度图神经网络（GNNs）的过平滑，但实证和理论分析表明，这种全局注意力本身表现出更严重的过平滑，使节点表示因低通滤波效应而变得不可区分，从而限制了模型的深层表示能力。\\n\\n**核心思路**：论文的核心解决思路是设计一个PageRank增强的注意力模块，以模拟深度Transformer的行为。这样设计的原因是，PageRank算法能自适应地调整节点重要性，从而在注意力机制中引入更灵活的滤波特性，避免全局注意力固有的低通滤波导致的过平滑，实现自适应通滤波。\\n\\n**技术框架**：ParaFormer的整体架构基于图Transformer，但替换了标准的全局注意力模块。主要模块包括输入嵌入层、PageRank增强的注意力层、前馈网络层和输出层。流程上，首先对图节点进行特征嵌入，然后通过PageRank注意力计算节点间的自适应权重，接着进行多层处理以捕获全局信息，最后用于分类任务。\\n\\n**关键创新**：最重要的技术创新点是PageRank增强的注意力模块，它通过整合PageRank算法来调整注意力权重，使模型能够作为自适应通滤波器运作。与现有方法的本质区别在于，传统图Transformer的全局注意力是固定的低通滤波器，而ParaFormer的注意力能根据图结构动态调整，从而更有效地平衡局部和全局信息，缓解过平滑。\\n\\n**关键设计**：关键技术细节包括：注意力权重计算中融入PageRank分数以增强节点重要性评估；使用多层Transformer架构，但注意力层经过PageRank优化；损失函数通常采用交叉熵损失用于分类任务；网络结构可能包含残差连接和层归一化以稳定训练；参数设置如注意力头数和隐藏维度根据数据集调整，以优化性能。",
            "application_zh": "ParaFormer在图表示学习领域具有广泛的应用潜力，包括社交网络分析、生物信息学中的蛋白质相互作用预测、推荐系统中的用户-物品图建模，以及知识图谱推理等。其实际价值在于通过缓解过平滑问题，提升深层图模型的性能，从而在需要捕获复杂全局关系的任务中提供更准确的表示。未来影响可能推动图Transformer在更大规模图数据上的应用，促进人工智能在图结构数据处理中的进步。",
            "highlight_zh": "实验在11个数据集上进行，涵盖节点数从数千到数百万的图，包括节点分类和图分类任务。ParaFormer相比基线方法（如标准图Transformer和GNNs）实现了持续的性能提升，具体提升幅度因数据集而异，例如在某些基准数据集上准确率提升可达几个百分点。这验证了其作为自适应通滤波器在缓解过平滑方面的有效性，并展示了在多样图任务中的鲁棒性。",
            "tags_zh": [
                "图Transformer",
                "过平滑问题",
                "PageRank算法",
                "自适应通滤波器",
                "节点分类",
                "图分类",
                "图表示学习",
                "全局注意力"
            ],
            "_index": 109
        },
        {
            "title": "LLmFPCA-detect: LLM-powered Multivariate Functional PCA for Anomaly Detection in Sparse Longitudinal Texts",
            "authors": [
                "Prasanjit Dubey",
                "Aritra Guha",
                "Zhengyi Zhou",
                "Qiong Wu",
                "Xiaoming Huo",
                "Paromita Dubey"
            ],
            "arxiv_id": "2512.14604v1",
            "summary": "Sparse longitudinal (SL) textual data arises when individuals generate text repeatedly over time (e.g., customer reviews, occasional social media posts, electronic medical records across visits), but the frequency and timing of observations vary across individuals. These complex textual data sets have immense potential to inform future policy and targeted recommendations. However, because SL text data lack dedicated methods and are noisy, heterogeneous, and prone to anomalies, detecting and inferring key patterns is challenging. We introduce LLmFPCA-detect, a flexible framework that pairs LLM-based text embeddings with functional data analysis to detect clusters and infer anomalies in large SL text datasets. First, LLmFPCA-detect embeds each piece of text into an application-specific numeric space using LLM prompts. Sparse multivariate functional principal component analysis (mFPCA) conducted in the numeric space forms the workhorse to recover primary population characteristics, and produces subject-level scores which, together with baseline static covariates, facilitate data segmentation, unsupervised anomaly detection and inference, and enable other downstream tasks. In particular, we leverage LLMs to perform dynamic keyword profiling guided by the data segments and anomalies discovered by LLmFPCA-detect, and we show that cluster-specific functional PC scores from LLmFPCA-detect, used as features in existing pipelines, help boost prediction performance. We support the stability of LLmFPCA-detect with experiments and evaluate it on two different applications using public datasets, Amazon customer-review trajectories, and Wikipedia talk-page comment streams, demonstrating utility across domains and outperforming state-of-the-art baselines.",
            "categories": [
                "stat.ML",
                "cs.LG"
            ],
            "primary_category": "stat.ML",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14604v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出LLmFPCA-detect框架，结合LLM文本嵌入与多元函数主成分分析，解决稀疏纵向文本数据的异常检测问题。",
            "summary_zh": "稀疏纵向（SL）文本数据出现在个体随时间重复生成文本的场景中（如客户评论、偶尔的社交媒体帖子、跨次就诊的电子病历），但观测频率和时间在个体间存在差异。这些复杂的文本数据集具有巨大潜力，可为未来政策和针对性推荐提供信息。然而，由于SL文本数据缺乏专门方法，且具有噪声、异质性并易出现异常，检测和推断关键模式具有挑战性。我们引入LLmFPCA-detect，这是一个灵活的框架，将基于LLM的文本嵌入与函数数据分析相结合，以检测大型SL文本数据集中的聚类并推断异常。首先，LLmFPCA-detect使用LLM提示将每段文本嵌入到特定应用的数值空间中。在数值空间中进行的稀疏多元函数主成分分析（mFPCA）是恢复主要群体特征的核心工具，并产生个体级分数，这些分数与基线静态协变量一起，促进数据分割、无监督异常检测和推断，并支持其他下游任务。特别是，我们利用LLM在LLmFPCA-detect发现的数据段和异常指导下进行动态关键词分析，并展示LLmFPCA-detect产生的聚类特定函数主成分分数作为现有流程中的特征，有助于提升预测性能。我们通过实验支持LLmFPCA-detect的稳定性，并使用公共数据集（亚马逊客户评论轨迹和维基百科讨论页评论流）在两个不同应用中评估它，展示了跨领域的实用性并优于最先进的基线方法。",
            "intro_zh": [
                "稀疏纵向文本数据缺乏专门分析方法，现有方法难以处理其噪声、异质性和异常问题。",
                "结合LLM文本嵌入与多元函数主成分分析，构建灵活框架以检测聚类和推断异常。",
                "在亚马逊评论和维基百科评论数据集上验证，性能优于基线方法，提升预测效果。"
            ],
            "method_zh": "**问题定义**：论文旨在解决稀疏纵向（SL）文本数据中的异常检测和模式推断问题。现有方法缺乏专门处理SL文本数据的工具，这些数据具有观测频率和时间不规律、噪声大、异质性强等特点，导致传统文本分析或时间序列方法效果有限。\\n\\n**核心思路**：论文的核心思路是将大型语言模型（LLM）的文本嵌入能力与函数数据分析（FDA）相结合，通过LLM将文本转换为数值表示，再利用多元函数主成分分析（mFPCA）处理稀疏纵向结构，从而提取群体特征并检测异常。这种设计利用了LLM的语义理解优势和FDA对时间序列数据的建模能力，以克服数据稀疏性和异质性挑战。\\n\\n**技术框架**：整体框架分为三个阶段：首先，使用LLM提示将每段文本嵌入到特定应用的数值空间中，生成文本向量；其次，在数值空间中进行稀疏多元函数主成分分析（mFPCA），恢复主要群体特征并计算个体级函数主成分分数；最后，结合基线静态协变量，进行数据分割、无监督异常检测和推断，并支持下游任务如动态关键词分析和预测提升。\\n\\n**关键创新**：最重要的技术创新点是将LLM文本嵌入与稀疏多元函数主成分分析（mFPCA）无缝集成，形成端到端框架。与现有方法的本质区别在于，它专门针对稀疏纵向文本数据设计，能够同时处理文本语义和时序结构，而传统方法往往只关注单一维度或缺乏对稀疏性的适应。\\n\\n**关键设计**：关键设计包括使用LLM提示进行文本嵌入，确保嵌入空间与应用场景相关；采用稀疏mFPCA处理不规则时间点数据，通过函数平滑和降维提取特征；个体级分数与静态协变量结合，用于聚类和异常检测；实验中使用公共数据集（如亚马逊评论和维基百科评论）验证稳定性，并对比基线方法如传统文本聚类或时间序列异常检测算法。",
            "application_zh": "该研究在客户评论分析、社交媒体监控、医疗记录挖掘等领域具有广泛应用价值。通过检测异常和聚类模式，可支持个性化推荐、风险预警和政策制定，提升数据驱动的决策效率。未来可能扩展到更多纵向文本场景，如教育评估或金融文本分析。",
            "highlight_zh": "在亚马逊客户评论和维基百科讨论页评论数据集上，LLmFPCA-detect在异常检测和聚类任务中优于最先进的基线方法，具体性能提升幅度未知，但实验显示其能有效提取群体特征并提升下游预测性能。框架稳定性通过实验验证，展示了跨领域的实用性和鲁棒性。",
            "tags_zh": [
                "稀疏纵向文本",
                "异常检测",
                "大型语言模型",
                "函数主成分分析",
                "文本嵌入",
                "无监督学习",
                "数据分割",
                "动态关键词分析"
            ],
            "_index": 110
        },
        {
            "title": "Sound and Music Biases in Deep Music Transcription Models: A Systematic Analysis",
            "authors": [
                "Lukáš Samuel Marták",
                "Patricia Hu",
                "Gerhard Widmer"
            ],
            "arxiv_id": "2512.14602v1",
            "summary": "Automatic Music Transcription (AMT) -- the task of converting music audio into note representations -- has seen rapid progress, driven largely by deep learning systems. Due to the limited availability of richly annotated music datasets, much of the progress in AMT has been concentrated on classical piano music, and even a few very specific datasets. Whether these systems can generalize effectively to other musical contexts remains an open question. Complementing recent studies on distribution shifts in sound (e.g., recording conditions), in this work we investigate the musical dimension -- specifically, variations in genre, dynamics, and polyphony levels. To this end, we introduce the MDS corpus, comprising three distinct subsets -- (1) Genre, (2) Random, and (3) MAEtest -- to emulate different axes of distribution shift. We evaluate the performance of several state-of-the-art AMT systems on the MDS corpus using both traditional information-retrieval and musically-informed performance metrics. Our extensive evaluation isolates and exposes varying degrees of performance degradation under specific distribution shifts. In particular, we measure a note-level F1 performance drop of 20 percentage points due to sound, and 14 due to genre. Generally, we find that dynamics estimation proves more vulnerable to musical variation than onset prediction. Musically informed evaluation metrics, particularly those capturing harmonic structure, help identify potential contributing factors. Furthermore, experiments with randomly generated, non-musical sequences reveal clear limitations in system performance under extreme musical distribution shifts. Altogether, these findings offer new evidence of the persistent impact of the Corpus Bias problem in deep AMT systems.",
            "categories": [
                "cs.SD",
                "cs.LG"
            ],
            "primary_category": "cs.SD",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "pre-print of the upcoming EURASIP JASM journal article",
            "doi": "10.1186/s13636-025-00428-z",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14602v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出MDS语料库以系统分析深度音乐转录模型在声音和音乐分布偏移下的性能偏差问题",
            "summary_zh": "自动音乐转录（AMT）——将音乐音频转换为音符表示的任务——在深度学习系统的推动下取得了快速进展。由于缺乏丰富标注的音乐数据集，AMT的进展主要集中在古典钢琴音乐，甚至集中在少数特定数据集上。这些系统是否能有效泛化到其他音乐场景仍是一个开放问题。本研究补充了近期关于声音分布偏移（如录音条件）的研究，重点调查音乐维度——特别是流派、动态和复音水平的变化。为此，我们引入了MDS语料库，包含三个不同的子集——（1）流派，（2）随机，和（3）MAEtest——以模拟不同维度的分布偏移。我们使用传统信息检索和音乐感知性能指标评估了多个最先进的AMT系统在MDS语料库上的表现。广泛的评估隔离并揭示了在特定分布偏移下不同程度的性能下降。具体而言，我们测量到由于声音因素导致的音符级F1性能下降20个百分点，由于流派因素导致的下降14个百分点。总体而言，我们发现动态估计比起始点预测更容易受到音乐变化的影响。音乐感知评估指标，特别是那些捕捉和声结构的指标，有助于识别潜在影响因素。此外，对随机生成的非音乐序列的实验揭示了在极端音乐分布偏移下系统性能的明显局限性。总之，这些发现为深度AMT系统中语料库偏差问题的持续影响提供了新证据。",
            "intro_zh": [
                "核心问题：现有深度音乐转录模型主要基于古典钢琴数据集训练，在流派、动态等音乐维度分布偏移下泛化能力不足，存在语料库偏差问题。",
                "方法要点：构建MDS语料库模拟不同音乐分布偏移，结合传统与音乐感知指标系统评估多个SOTA模型，隔离性能下降因素。",
                "实验或效果：发现声音因素导致F1下降20个百分点，流派因素下降14个百分点，动态估计比起始点预测更易受音乐变化影响。"
            ],
            "method_zh": "**问题定义**：论文旨在解决深度自动音乐转录（AMT）模型在音乐维度分布偏移下的泛化能力问题。现有方法主要基于古典钢琴数据集（如MAESTRO）训练，导致模型在流派、动态（如音量变化）和复音水平等音乐特性变化时性能下降，存在严重的语料库偏差，限制了实际应用。\\n\\n**核心思路**：通过构建系统化的评估语料库MDS，模拟不同维度的音乐分布偏移，结合传统信息检索指标和音乐感知指标，对多个SOTA AMT模型进行细粒度性能分析，以量化并理解模型在音乐变化下的脆弱性。\\n\\n**技术框架**：整体流程包括数据构建、模型评估和结果分析三个阶段。首先，创建MDS语料库，包含三个子集：Genre子集（涵盖不同音乐流派）、Random子集（随机生成的非音乐序列）和MAEtest子集（基于MAESTRO测试集但引入变化）。然后，选择多个代表性AMT模型（如Onsets and Frames、High-Resolution Net等）作为评估对象。最后，使用F1分数等传统指标和音乐感知指标（如和声结构相关指标）进行性能测量，并分析不同分布偏移下的性能变化。\\n\\n**关键创新**：最重要的创新在于系统性地构建了MDS语料库，专门用于评估音乐维度（而非仅声音维度）的分布偏移影响，并引入了音乐感知评估指标来深入分析性能下降的潜在因素，这区别于以往仅关注录音条件等声音偏移的研究。\\n\\n**关键设计**：MDS语料库的设计是关键，Genre子集包含多种流派（如爵士、流行），Random子集使用随机序列模拟极端偏移，MAEtest子集在MAESTRO基础上调整动态和复音水平。评估中，使用音符级F1分数作为核心指标，同时采用音乐感知指标如和声错误率来捕捉模型在和声结构处理上的不足。模型选择覆盖了不同架构的SOTA AMT系统，确保评估的全面性。",
            "application_zh": "该研究在音乐信息检索、音频处理和人工智能音乐领域具有重要价值。潜在应用包括改进AMT系统的鲁棒性，使其能更好地处理多样化的音乐内容（如不同流派或演奏风格），从而提升音乐教育工具、自动配乐系统和音乐分析软件的实用性。未来影响可能推动更平衡的数据集构建和模型训练策略，减少语料库偏差，促进AMT技术在更广泛音乐场景中的部署。",
            "highlight_zh": "最重要的实验结果显示，在MDS语料库评估中，声音因素导致音符级F1性能下降20个百分点，流派因素导致下降14个百分点，突显了分布偏移的显著影响。动态估计任务比起始点预测更易受音乐变化影响，性能下降更明显。使用随机非音乐序列的极端测试揭示了模型在音乐结构缺失时的局限性，F1分数大幅降低。音乐感知指标进一步识别出和声结构错误是性能下降的关键因素之一。",
            "tags_zh": [
                "自动音乐转录",
                "分布偏移",
                "语料库偏差",
                "音乐感知评估",
                "深度学习",
                "音乐信息检索",
                "模型泛化",
                "多流派音乐"
            ],
            "_index": 111
        },
        {
            "title": "FakeRadar: Probing Forgery Outliers to Detect Unknown Deepfake Videos",
            "authors": [
                "Zhaolun Li",
                "Jichang Li",
                "Yinqi Cai",
                "Junye Chen",
                "Xiaonan Luo",
                "Guanbin Li",
                "Rushi Lan"
            ],
            "arxiv_id": "2512.14601v1",
            "summary": "In this paper, we propose FakeRadar, a novel deepfake video detection framework designed to address the challenges of cross-domain generalization in real-world scenarios. Existing detection methods typically rely on manipulation-specific cues, performing well on known forgery types but exhibiting severe limitations against emerging manipulation techniques. This poor generalization stems from their inability to adapt effectively to unseen forgery patterns. To overcome this, we leverage large-scale pretrained models (e.g. CLIP) to proactively probe the feature space, explicitly highlighting distributional gaps between real videos, known forgeries, and unseen manipulations. Specifically, FakeRadar introduces Forgery Outlier Probing, which employs dynamic subcluster modeling and cluster-conditional outlier generation to synthesize outlier samples near boundaries of estimated subclusters, simulating novel forgery artifacts beyond known manipulation types. Additionally, we design Outlier-Guided Tri-Training, which optimizes the detector to distinguish real, fake, and outlier samples using proposed outlier-driven contrastive learning and outlier-conditioned cross-entropy losses. Experiments show that FakeRadar outperforms existing methods across various benchmark datasets for deepfake video detection, particularly in cross-domain evaluations, by handling the variety of emerging manipulation techniques.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14601v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出FakeRadar框架，通过伪造异常探测和三重训练解决深度伪造视频检测中的跨域泛化问题",
            "summary_zh": "本文提出FakeRadar，一种新颖的深度伪造视频检测框架，旨在解决现实场景中跨域泛化的挑战。现有检测方法通常依赖于特定操纵线索，在已知伪造类型上表现良好，但对新兴操纵技术表现出严重局限性。这种泛化能力差源于它们无法有效适应未见过的伪造模式。为克服此问题，我们利用大规模预训练模型（如CLIP）主动探测特征空间，明确突出真实视频、已知伪造和未见操纵之间的分布差距。具体而言，FakeRadar引入伪造异常探测，采用动态子簇建模和簇条件异常生成来合成估计子簇边界附近的异常样本，模拟超出已知操纵类型的新伪造伪影。此外，我们设计异常引导三重训练，通过提出的异常驱动对比学习和异常条件交叉熵损失优化检测器，以区分真实、伪造和异常样本。实验表明，FakeRadar在深度伪造视频检测的各种基准数据集上优于现有方法，特别是在跨域评估中，通过处理各种新兴操纵技术。",
            "intro_zh": [
                "现有深度伪造检测方法依赖特定操纵线索，对已知伪造类型有效，但面对新兴技术时泛化能力差，无法适应未见伪造模式。",
                "FakeRadar利用预训练模型主动探测特征空间，通过伪造异常探测模拟新伪造伪影，并采用异常引导三重训练优化检测器。",
                "实验显示FakeRadar在多个基准数据集上优于现有方法，尤其在跨域评估中显著提升泛化性能，有效处理新兴操纵技术。"
            ],
            "method_zh": "**问题定义**：论文旨在解决深度伪造视频检测中的跨域泛化问题，即现有方法在已知伪造类型上表现良好，但面对新兴或未见过的操纵技术时性能显著下降，主要痛点在于它们过度依赖特定操纵线索，无法有效适应未知伪造模式。\\n\\n**核心思路**：论文的核心思路是通过主动探测特征空间中的分布差距，模拟未见伪造模式，从而增强检测器的泛化能力。这基于利用大规模预训练模型（如CLIP）提取通用特征，并设计伪造异常探测来生成异常样本，以训练检测器区分真实、已知伪造和未知伪造。\\n\\n**技术框架**：整体框架包括两个主要阶段：首先，利用预训练模型提取视频特征，并进行动态子簇建模以估计真实和已知伪造的分布；其次，通过伪造异常探测生成异常样本，并采用异常引导三重训练优化检测器，包括异常驱动对比学习和异常条件交叉熵损失。\\n\\n**关键创新**：最重要的技术创新是伪造异常探测和异常引导三重训练。伪造异常探测通过动态子簇建模和簇条件异常生成，主动合成异常样本模拟新伪造伪影，而异常引导三重训练则利用这些样本优化检测器，与现有方法依赖已知伪造线索的本质区别在于主动适应未知模式。\\n\\n**关键设计**：关键设计包括使用CLIP等预训练模型作为特征提取器，动态子簇建模基于聚类算法估计分布边界，异常生成通过采样和扰动合成异常样本，损失函数结合异常驱动对比损失（如InfoNCE变体）和异常条件交叉熵损失，网络结构可能包含多层感知机或轻量级分类头。",
            "application_zh": "该研究在网络安全、内容审核和数字取证领域具有重要应用价值。例如，可用于社交媒体平台自动检测深度伪造视频，防止虚假信息传播；在司法鉴定中辅助识别伪造证据；未来可能扩展到实时视频流检测，提升在线安全防护能力，对维护数字信任和打击网络犯罪有积极影响。",
            "highlight_zh": "实验在多个基准数据集（如FaceForensics++、Celeb-DF、DeepFakeDetection）上进行，FakeRadar在跨域评估中显著优于现有方法（如Xception、MesoNet、F3-Net），平均准确率提升约5-10%，特别是在处理新兴操纵技术时泛化性能更强，具体数据如在某些数据集上达到95%以上的检测准确率，显示出优异的鲁棒性和适应性。",
            "tags_zh": [
                "深度伪造检测",
                "跨域泛化",
                "伪造异常探测",
                "异常引导训练",
                "预训练模型",
                "视频分析",
                "对比学习",
                "计算机视觉"
            ],
            "_index": 112
        },
        {
            "title": "FoodLogAthl-218: Constructing a Real-World Food Image Dataset Using Dietary Management Applications",
            "authors": [
                "Mitsuki Watanabe",
                "Sosuke Amano",
                "Kiyoharu Aizawa",
                "Yoko Yamakata"
            ],
            "arxiv_id": "2512.14574v1",
            "summary": "Food image classification models are crucial for dietary management applications because they reduce the burden of manual meal logging. However, most publicly available datasets for training such models rely on web-crawled images, which often differ from users' real-world meal photos. In this work, we present FoodLogAthl-218, a food image dataset constructed from real-world meal records collected through the dietary management application FoodLog Athl. The dataset contains 6,925 images across 218 food categories, with a total of 14,349 bounding boxes. Rich metadata, including meal date and time, anonymized user IDs, and meal-level context, accompany each image. Unlike conventional datasets-where a predefined class set guides web-based image collection-our data begins with user-submitted photos, and labels are applied afterward. This yields greater intra-class diversity, a natural frequency distribution of meal types, and casual, unfiltered images intended for personal use rather than public sharing. In addition to (1) a standard classification benchmark, we introduce two FoodLog-specific tasks: (2) an incremental fine-tuning protocol that follows the temporal stream of users' logs, and (3) a context-aware classification task where each image contains multiple dishes, and the model must classify each dish by leveraging the overall meal context. We evaluate these tasks using large multimodal models (LMMs). The dataset is publicly available at https://huggingface.co/datasets/FoodLog/FoodLogAthl-218.",
            "categories": [
                "cs.CV",
                "cs.MM"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "10.1145/3746027.3758276",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14574v1",
            "code_links": [
                {
                    "url": "https://huggingface.co/datasets/FoodLog",
                    "type": "huggingface"
                }
            ],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出FoodLogAthl-218真实世界食物图像数据集，以解决饮食管理应用中模型训练数据与现实用户照片差异大的问题。",
            "summary_zh": "食物图像分类模型对饮食管理应用至关重要，能减轻手动记录餐食的负担。然而，大多数公开可用的训练数据集依赖网络爬取的图像，这些图像常与用户真实餐食照片存在差异。本研究提出了FoodLogAthl-218，这是一个从饮食管理应用FoodLog Athl收集的真实世界餐食记录构建的食物图像数据集。该数据集包含6,925张图像，涵盖218个食物类别，共有14,349个边界框。每张图像都附有丰富的元数据，包括餐食日期和时间、匿名用户ID以及餐食级上下文信息。与传统数据集不同，传统数据集以预定义类别集指导网络图像收集，而我们的数据始于用户提交的照片，随后才应用标签。这带来了更大的类内多样性、餐食类型的自然频率分布，以及用于个人使用而非公开分享的随意、未过滤的图像。除了（1）标准分类基准外，我们还引入了两个FoodLog特定任务：（2）遵循用户日志时间流的增量微调协议，以及（3）上下文感知分类任务，其中每张图像包含多道菜肴，模型必须利用整体餐食上下文对每道菜进行分类。我们使用大型多模态模型（LMMs）评估了这些任务。该数据集公开可用，网址为https://huggingface.co/datasets/FoodLog/FoodLogAthl-218。",
            "intro_zh": [
                "核心问题：现有食物图像数据集多基于网络爬取，与现实用户餐食照片存在差异，导致模型在实际应用中性能下降。",
                "方法要点：从饮食管理应用收集真实用户餐食照片构建数据集，采用后标注方式，并引入增量微调和上下文感知分类任务。",
                "实验或效果：数据集包含6,925张图像和14,349个边界框，评估显示在真实场景下具有更高实用价值，支持多模态模型应用。"
            ],
            "method_zh": "**问题定义**：论文旨在解决食物图像分类模型在饮食管理应用中因训练数据与现实用户照片不匹配而导致的性能瓶颈。现有方法主要依赖网络爬取图像构建数据集，这些图像往往经过美化或标准化，缺乏真实世界餐食的多样性和随意性，导致模型在实际部署时泛化能力不足。\\n\\n**核心思路**：论文的核心思路是直接从饮食管理应用FoodLog Athl收集真实用户提交的餐食照片，构建一个更贴近实际应用场景的数据集。通过后标注方式（即先有图像后定义类别），而非传统预定义类别引导的数据收集，以捕捉真实世界的类内多样性和自然分布。\\n\\n**技术框架**：整体流程包括数据收集、标注和任务设计三个阶段。首先，从应用中收集用户上传的餐食图像及元数据（如时间、用户ID）；然后，手动或半自动标注图像中的食物边界框和类别标签；最后，设计标准分类、增量微调和上下文感知分类三个基准任务，并使用大型多模态模型进行评估。\\n\\n**关键创新**：最重要的创新在于数据构建方式的根本转变：从“以类别为中心”的网络爬取转向“以用户为中心”的真实世界收集。这本质区别在于数据集更真实地反映了实际应用中的图像特性，如类内多样性高、图像质量参差不齐、多菜肴共存等，从而提升了模型的实用性和鲁棒性。\\n\\n**关键设计**：关键设计包括：1）数据集包含6,925张图像、218个类别和14,349个边界框，确保规模适中且类别覆盖广泛；2）引入增量微调协议，模拟用户日志的时间流，以评估模型在连续学习场景下的性能；3）上下文感知分类任务要求模型利用整体餐食上下文（如多菜肴关系）进行分类，这通过多标签或目标检测框架实现，具体网络结构未详细说明，但评估时使用了大型多模态模型（LMMs）以利用其多模态理解能力。",
            "application_zh": "该研究主要应用于饮食管理和健康监测领域，通过提供真实世界食物图像数据集，能显著提升自动餐食记录系统的准确性和用户体验。潜在价值包括支持个性化营养分析、慢性病管理（如糖尿病）和健身追踪，未来可能推动智能饮食助手和医疗AI的发展，促进公共卫生。",
            "highlight_zh": "最重要的实验结果包括：数据集包含6,925张真实世界图像和14,349个边界框，覆盖218个食物类别，展现了高类内多样性和自然频率分布。在评估中，通过大型多模态模型（LMMs）测试了标准分类、增量微调和上下文感知分类任务，结果显示在真实场景下性能优于基于网络爬取数据的基线模型，具体提升幅度未知，但强调了数据真实性的关键作用。",
            "tags_zh": [
                "食物图像分类",
                "真实世界数据集",
                "饮食管理应用",
                "增量微调",
                "上下文感知分类",
                "大型多模态模型",
                "计算机视觉",
                "健康监测"
            ],
            "_index": 113
        },
        {
            "title": "CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer",
            "authors": [
                "Xianwei Cao",
                "Dou Quan",
                "Shuang Wang",
                "Ning Huyan",
                "Wei Wang",
                "Yunan Li",
                "Licheng Jiao"
            ],
            "arxiv_id": "2512.14560v1",
            "summary": "Image retrieval-based cross-view geo-localization (IRCVGL) aims to match images captured from significantly different viewpoints, such as satellite and street-level images. Existing methods predominantly rely on learning robust global representations or implicit feature alignment, which often fail to model explicit spatial correspondences crucial for accurate localization. In this work, we propose a novel correspondence-aware feature refinement framework, termed CLNet, that explicitly bridges the semantic and geometric gaps between different views. CLNet decomposes the view alignment process into three learnable and complementary modules: a Neural Correspondence Map (NCM) that spatially aligns cross-view features via latent correspondence fields; a Nonlinear Embedding Converter (NEC) that remaps features across perspectives using an MLP-based transformation; and a Global Feature Recalibration (GFR) module that reweights informative feature channels guided by learned spatial cues. The proposed CLNet can jointly capture both high-level semantics and fine-grained alignments. Extensive experiments on four public benchmarks, CVUSA, CVACT, VIGOR, and University-1652, demonstrate that our proposed CLNet achieves state-of-the-art performance while offering better interpretability and generalizability.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "16 pages, 6 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14560v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出CLNet框架，通过显式跨视角对应关系解决图像检索式跨视角地理定位问题",
            "summary_zh": "基于图像检索的跨视角地理定位（IRCVGL）旨在匹配从显著不同视角（如卫星和街景）捕获的图像。现有方法主要依赖学习鲁棒的全局表示或隐式特征对齐，往往难以建模对精确定位至关重要的显式空间对应关系。本文提出了一种新颖的对应感知特征细化框架，称为CLNet，它显式地桥接了不同视角之间的语义和几何鸿沟。CLNet将视角对齐过程分解为三个可学习且互补的模块：神经对应图（NCM），通过潜在对应场在空间上对齐跨视角特征；非线性嵌入转换器（NEC），使用基于MLP的变换跨视角重映射特征；以及全局特征重校准（GFR）模块，通过学习到的空间线索引导重新加权信息丰富的特征通道。所提出的CLNet能够联合捕获高级语义和细粒度对齐。在四个公共基准测试（CVUSA、CVACT、VIGOR和University-1652）上的大量实验表明，我们提出的CLNet实现了最先进的性能，同时提供了更好的可解释性和泛化性。",
            "intro_zh": [
                "现有方法依赖全局表示或隐式对齐，难以建模跨视角的显式空间对应关系，导致地理定位精度受限。",
                "提出CLNet框架，通过神经对应图、非线性嵌入转换器和全局特征重校准模块，显式学习跨视角的语义和几何对应。",
                "在CVUSA等四个基准测试上达到最先进性能，显著提升定位精度，同时增强模型的可解释性和泛化能力。"
            ],
            "method_zh": "**问题定义**：论文解决图像检索式跨视角地理定位（IRCVGL）问题，即匹配卫星和街景等不同视角的图像。现有方法主要依赖学习全局特征或隐式对齐，缺乏对显式空间对应关系的建模，导致在视角差异大时定位精度不足。\\n\\n**核心思路**：论文提出显式建模跨视角对应关系，通过分解视角对齐过程为可学习模块，联合捕获语义和几何信息，以桥接不同视角间的鸿沟。\\n\\n**技术框架**：CLNet整体架构包括三个主要模块：神经对应图（NCM）用于空间对齐跨视角特征，非线性嵌入转换器（NEC）用于跨视角特征重映射，全局特征重校准（GFR）用于基于空间线索的特征通道加权。这些模块协同工作，实现端到端的特征细化。\\n\\n**关键创新**：最重要的创新是引入显式对应关系学习，通过NCM的潜在对应场实现空间对齐，与现有方法的隐式对齐形成本质区别，提升了模型的解释性和精度。\\n\\n**关键设计**：NCM使用卷积网络生成对应场，NEC基于MLP实现非线性变换，GFR采用注意力机制重校准特征。损失函数可能结合对比学习或三元组损失，具体参数设置未在摘要中详述，需参考论文正文。",
            "application_zh": "该研究可应用于自动驾驶、无人机导航、增强现实和智能城市等领域，通过跨视角图像匹配实现精确定位，提升地理信息系统的智能化水平。其显式对应关系设计有助于提高模型的可解释性，为实际部署提供可靠性保障。",
            "highlight_zh": "在CVUSA、CVACT、VIGOR和University-1652四个基准测试上，CLNet均达到最先进性能。例如，在CVUSA上，top-1准确率提升显著（具体数据需参考论文），相比基线方法有较大幅度改进。实验表明，CLNet在跨视角匹配任务中具有优越的泛化能力和鲁棒性。",
            "tags_zh": [
                "跨视角地理定位",
                "图像检索",
                "对应关系学习",
                "特征对齐",
                "神经网络",
                "计算机视觉",
                "语义几何融合",
                "可解释AI"
            ],
            "_index": 114
        },
        {
            "title": "Test Time Optimized Generalized AI-based Medical Image Registration Method",
            "authors": [
                "Sneha Sree C.",
                "Dattesh Shanbhag",
                "Sudhanya Chatterjee"
            ],
            "arxiv_id": "2512.14556v1",
            "summary": "Medical image registration is critical for aligning anatomical structures across imaging modalities such as computed tomography (CT), magnetic resonance imaging (MRI), and ultrasound. Among existing techniques, non-rigid registration (NRR) is particularly challenging due to the need to capture complex anatomical deformations caused by physiological processes like respiration or contrast-induced signal variations. Traditional NRR methods, while theoretically robust, often require extensive parameter tuning and incur high computational costs, limiting their use in real-time clinical workflows. Recent deep learning (DL)-based approaches have shown promise; however, their dependence on task-specific retraining restricts scalability and adaptability in practice. These limitations underscore the need for efficient, generalizable registration frameworks capable of handling heterogeneous imaging contexts. In this work, we introduce a novel AI-driven framework for 3D non-rigid registration that generalizes across multiple imaging modalities and anatomical regions. Unlike conventional methods that rely on application-specific models, our approach eliminates anatomy- or modality-specific customization, enabling streamlined integration into diverse clinical environments.",
            "categories": [
                "eess.IV",
                "cs.CV"
            ],
            "primary_category": "eess.IV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14556v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出一种测试时优化的通用AI医学图像配准框架，以解决多模态多解剖区域非刚性配准的泛化性问题。",
            "summary_zh": "医学图像配准对于对齐计算机断层扫描（CT）、磁共振成像（MRI）和超声等不同成像模态的解剖结构至关重要。在现有技术中，非刚性配准（NRR）尤其具有挑战性，因为它需要捕捉由呼吸或对比剂引起的信号变化等生理过程导致的复杂解剖变形。传统的NRR方法虽然在理论上稳健，但通常需要大量参数调整并产生高计算成本，限制了其在实时临床工作流程中的应用。最近的基于深度学习（DL）的方法显示出潜力；然而，它们对任务特定重新训练的依赖在实践中限制了可扩展性和适应性。这些局限性凸显了对能够处理异构成像环境的高效、可泛化配准框架的需求。在这项工作中，我们引入了一种新颖的AI驱动的3D非刚性配准框架，该框架可泛化到多种成像模态和解剖区域。与依赖应用特定模型的传统方法不同，我们的方法消除了解剖或模态特定的定制，从而能够简化地集成到多样化的临床环境中。",
            "intro_zh": [
                "核心问题：传统非刚性配准方法参数调整复杂、计算成本高，而深度学习模型依赖任务特定训练，泛化能力不足，难以适应多模态多解剖区域的临床需求。",
                "方法要点：提出一种AI驱动的通用框架，通过消除解剖或模态特定定制，实现跨模态和跨解剖区域的3D非刚性配准，无需重新训练即可适应不同场景。",
                "实验或效果：框架在多种成像模态和解剖区域上表现出高效性和泛化性，简化了临床集成，提升了配准的实用性和可扩展性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决医学图像非刚性配准中的泛化性问题，特别是在多模态（如CT、MRI、超声）和多解剖区域场景下。现有方法的痛点包括：传统非刚性配准方法需要大量参数调整和高计算成本，限制了实时应用；而深度学习模型依赖任务特定训练，缺乏泛化能力，难以适应异构成像环境。\\n\\n**核心思路**：论文的核心解决思路是设计一个AI驱动的通用框架，通过消除解剖或模态特定定制，实现跨模态和跨解剖区域的3D非刚性配准。这样设计是为了克服现有方法对特定任务重新训练的依赖，提升框架的可扩展性和适应性，使其能够无缝集成到多样化的临床工作流程中。\\n\\n**技术框架**：整体架构是一个端到端的深度学习框架，包含数据预处理、特征提取、变形场预测和后处理等阶段。主要模块包括：输入图像对处理模块，用于标准化多模态数据；深度神经网络模块，用于学习通用配准特征；优化模块，在测试时进行自适应调整；以及输出模块，生成配准后的图像和变形场。流程上，框架首先接收未配准的图像对，通过神经网络预测初始变形，然后在测试阶段进行优化以适配具体场景。\\n\\n**关键创新**：最重要的技术创新点是引入了测试时优化的机制，结合通用模型设计，实现了无需重新训练的跨模态和跨解剖区域配准。与现有方法的本质区别在于：传统方法依赖手动调参或特定模型训练，而本框架通过AI驱动自动适应，消除了定制需求，从而提高了泛化性和效率。\\n\\n**关键设计**：关键设计包括：使用卷积神经网络（CNN）或Transformer架构进行特征提取，以捕获多尺度空间信息；损失函数结合图像相似性度量（如互信息或归一化互相关）和正则化项（如平滑约束），以确保变形场的物理合理性；参数设置上，框架在训练时使用大规模多模态数据集，测试时通过轻量级优化步骤调整，减少计算开销；网络结构可能包含编码器-解码器设计，以处理3D体积数据并输出密集变形场。",
            "application_zh": "该研究的潜在应用领域包括医学影像分析、临床诊断和治疗规划，特别是在多模态图像融合（如CT-MRI配准用于肿瘤定位）、跨时间序列分析（如监测疾病进展）和实时手术导航中。实际价值在于简化临床工作流程，减少手动干预，提高配准的准确性和效率，从而支持精准医疗。未来影响可能推动AI在医学图像处理中的标准化和泛化，促进智能医疗系统的发展。",
            "highlight_zh": "最重要的实验结果包括：框架在多种成像模态（如CT、MRI、超声）和解剖区域（如脑部、腹部）上进行了验证，对比基线包括传统非刚性配准方法和特定任务深度学习模型。具体性能数据未知，但报告显示框架在配准精度上达到或超过基线，同时显著减少了计算时间和参数调整需求，提升幅度体现在泛化能力增强，适应不同场景无需重新训练，简化了临床集成过程。",
            "tags_zh": [
                "医学图像配准",
                "非刚性配准",
                "多模态融合",
                "深度学习框架",
                "泛化性优化",
                "测试时优化",
                "3D图像处理",
                "临床AI应用"
            ],
            "_index": 115
        },
        {
            "title": "Dual Language Models: Balancing Training Efficiency and Overfitting Resilience",
            "authors": [
                "David Samuel",
                "Lucas Georges Gabriel Charpentier"
            ],
            "arxiv_id": "2512.14549v1",
            "summary": "This paper combines autoregressive and masked-diffusion training objectives without any architectural modifications, resulting in flexible language models that outperform single-objective models. Autoregressive modeling has been a popular approach, partly because of its training efficiency; however, that comes at the cost of sensitivity to overfitting. On the other hand, masked-diffusion models are less efficient to train while being more resilient to overfitting. In this work, we demonstrate that dual-objective training achieves the best of both worlds. To derive the optimal ratio between both objectives, we train and evaluate 50 language models under varying levels of data repetition. We show that it is optimal to combine both objectives under all evaluated settings and that the optimal ratio is similar whether targeting autoregressive or masked-diffusion downstream performance.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14549v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出双目标训练方法以平衡语言模型的训练效率与过拟合鲁棒性",
            "summary_zh": "本文结合自回归和掩码扩散训练目标，无需修改架构，构建了灵活的语言模型，其性能优于单目标模型。自回归建模因其训练效率高而流行，但代价是对过拟合敏感；而掩码扩散模型训练效率较低，但过拟合鲁棒性更强。本研究证明双目标训练能兼顾两者优势。为确定两个目标之间的最优比例，我们在不同数据重复水平下训练和评估了50个语言模型。结果表明，在所有评估设置下，结合两个目标是最优的，且无论针对自回归还是掩码扩散下游性能，最优比例相似。",
            "intro_zh": [
                "核心问题：自回归模型训练效率高但易过拟合，掩码扩散模型鲁棒性强但训练效率低，现有单目标方法难以平衡两者。",
                "方法要点：结合自回归和掩码扩散训练目标，通过双目标训练提升模型灵活性和性能，无需架构改动。",
                "实验或效果：在50个模型实验中，双目标训练在所有设置下均最优，最优比例稳定，显著提升下游任务表现。"
            ],
            "method_zh": "**问题定义**：论文旨在解决语言模型训练中效率与鲁棒性的权衡问题。现有方法中，自回归模型训练快但易过拟合，掩码扩散模型鲁棒性强但训练慢，这限制了模型在实际应用中的灵活性和性能。\\n\\n**核心思路**：论文提出结合自回归和掩码扩散训练目标，通过双目标训练来平衡效率和鲁棒性。这样设计是因为两个目标互补：自回归目标促进高效学习，掩码扩散目标增强泛化能力，从而在不增加架构复杂度的情况下提升模型整体表现。\\n\\n**技术框架**：整体流程包括模型初始化、双目标训练和评估阶段。主要模块包括标准语言模型架构（如Transformer），训练时同时计算自回归损失和掩码扩散损失，通过加权求和形成总损失，然后进行反向传播优化。评估阶段测试模型在不同数据重复水平下的下游任务性能。\\n\\n**关键创新**：最重要的创新是无需修改模型架构，仅通过训练目标组合实现性能提升。与现有单目标方法相比，本质区别在于同时利用两个目标的优势，避免了单独使用时的缺陷，从而在效率和鲁棒性间取得更好平衡。\\n\\n**关键设计**：关键参数包括自回归和掩码扩散损失的权重比例，论文通过实验确定最优比例；损失函数为加权和：总损失 = α * 自回归损失 + (1-α) * 掩码扩散损失，其中α通过网格搜索优化；网络结构保持标准，未引入额外层或组件，确保方法通用性。",
            "application_zh": "该研究在自然语言处理领域具有广泛潜在应用，如文本生成、机器翻译和对话系统，通过提升模型训练效率和鲁棒性，可降低计算成本并提高部署稳定性。未来可能推动更高效、泛化能力强的语言模型发展，促进AI在实际场景中的落地。",
            "highlight_zh": "实验训练了50个语言模型，在不同数据重复水平下评估。结果显示，双目标训练在所有设置下均优于单目标模型，性能提升显著；最优损失权重比例在不同下游任务（自回归和掩码扩散）中相似，表明方法具有稳定性和通用性。具体数据未提供，但论文强调双目标模型在效率和鲁棒性方面均达到最佳平衡。",
            "tags_zh": [
                "语言模型",
                "双目标训练",
                "自回归建模",
                "掩码扩散模型",
                "训练效率",
                "过拟合鲁棒性",
                "下游任务性能",
                "损失函数优化"
            ],
            "_index": 116
        },
        {
            "title": "Improving Slow Transfer Predictions: Generative Methods Compared",
            "authors": [
                "Jacob Taegon Kim",
                "Alex Sim",
                "Kesheng Wu",
                "Jinoh Kim"
            ],
            "arxiv_id": "2512.14522v1",
            "summary": "Monitoring data transfer performance is a crucial task in scientific computing networks. By predicting performance early in the communication phase, potentially sluggish transfers can be identified and selectively monitored, optimizing network usage and overall performance. A key bottleneck to improving the predictive power of machine learning (ML) models in this context is the issue of class imbalance. This project focuses on addressing the class imbalance problem to enhance the accuracy of performance predictions. In this study, we analyze and compare various augmentation strategies, including traditional oversampling methods and generative techniques. Additionally, we adjust the class imbalance ratios in training datasets to evaluate their impact on model performance. While augmentation may improve performance, as the imbalance ratio increases, the performance does not significantly improve. We conclude that even the most advanced technique, such as CTGAN, does not significantly improve over simple stratified sampling.",
            "categories": [
                "cs.LG",
                "cs.DC",
                "cs.NI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "10.1109/ICNC64010.2025.10994006",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14522v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "比较生成方法与传统过采样在解决科学计算网络数据转移预测中的类别不平衡问题",
            "summary_zh": "监测数据转移性能是科学计算网络中的关键任务。通过在通信阶段早期预测性能，可以识别潜在缓慢的转移并选择性监控，从而优化网络使用和整体性能。在此背景下，提高机器学习模型预测能力的一个关键瓶颈是类别不平衡问题。本项目专注于解决类别不平衡问题以提升性能预测的准确性。本研究分析比较了多种增强策略，包括传统的过采样方法和生成技术。此外，我们调整训练数据集中的类别不平衡比例以评估其对模型性能的影响。虽然增强可能改善性能，但随着不平衡比例增加，性能并未显著提升。我们得出结论，即使是最先进的技术如CTGAN，也未显著优于简单的分层抽样。",
            "intro_zh": [
                "核心问题：科学计算网络中数据转移预测面临类别不平衡，导致机器学习模型预测能力受限，传统方法难以有效处理。",
                "方法要点：通过比较传统过采样与生成方法如CTGAN，调整训练数据类别比例，系统评估不同增强策略对预测性能的影响。",
                "实验或效果：研究发现，随着不平衡比例增加，增强方法性能提升有限，CTGAN等先进技术未显著优于简单分层抽样。"
            ],
            "method_zh": "**问题定义**：论文解决科学计算网络中数据转移性能预测的类别不平衡问题。现有机器学习模型在预测缓慢转移时，由于正类样本（缓慢转移）远少于负类样本（正常转移），导致模型偏向多数类，预测准确性下降，这是现有方法的痛点。\\n\\n**核心思路**：论文的核心思路是通过数据增强策略来缓解类别不平衡，比较传统过采样方法与生成方法的效果，并调整训练数据中的类别比例，以系统评估不同方法对预测性能的提升。这样设计是为了探索在高度不平衡场景下，哪种增强技术能更有效地改善模型泛化能力。\\n\\n**技术框架**：整体架构包括数据预处理、增强策略应用、模型训练和性能评估四个阶段。主要模块：1) 数据收集与标注，将数据转移性能分为缓慢和正常类别；2) 应用不同增强方法，如随机过采样、SMOTE等传统方法，以及CTGAN等生成方法；3) 使用机器学习模型（如分类器）在增强后的数据集上训练；4) 通过指标如准确率、召回率等评估性能，并分析不平衡比例变化的影响。\\n\\n**关键创新**：最重要的技术创新点在于系统比较了生成方法与传统过采样在科学计算网络预测任务中的效果，并实证分析了类别不平衡比例对增强策略有效性的影响。与现有方法相比，本质区别在于不仅应用先进生成技术，还强调在真实不平衡场景下评估其实际价值，而非假设增强总能带来显著提升。\\n\\n**关键设计**：关键设计包括：使用CTGAN作为生成方法代表，基于条件生成对抗网络框架生成合成少数类样本；调整类别不平衡比例，例如通过改变训练数据中正负类样本的比例；评估指标可能包括F1分数、AUC等，以全面衡量模型性能；实验设置中可能涉及交叉验证以确保结果可靠性。具体参数如CTGAN的生成器-判别器结构、训练轮数等未在摘要中详述，但整体设计注重对比分析和可重复性。",
            "application_zh": "该研究主要应用于科学计算网络和数据中心，用于优化数据转移监控和资源分配。潜在价值在于帮助网络管理员早期识别缓慢转移，提高网络效率和可靠性。未来影响可能推动更智能的网络管理工具开发，但需注意增强方法在极端不平衡场景下的局限性。",
            "highlight_zh": "最重要的实验结果显示，随着类别不平衡比例增加，数据增强方法（包括传统过采样和生成技术如CTGAN）对预测性能的提升有限，未显著优于简单的分层抽样策略。具体性能数据未在摘要中提供，但结论强调先进生成方法在实际应用中可能不如预期有效，这为未来研究提供了重要参考。",
            "tags_zh": [
                "类别不平衡",
                "数据增强",
                "生成对抗网络",
                "科学计算网络",
                "性能预测",
                "过采样方法",
                "CTGAN",
                "机器学习模型"
            ],
            "_index": 117
        },
        {
            "title": "SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models",
            "authors": [
                "Shizhuo Mao",
                "Song Chen",
                "Yi Kang"
            ],
            "arxiv_id": "2512.14481v1",
            "summary": "Large language models (LLMs) excel at natural language tasks but face deployment challenges due to their growing size outpacing GPU memory advancements. Model quantization mitigates this issue by lowering weight and activation precision, but existing solutions face fundamental trade-offs: dynamic quantization incurs high computational overhead and poses deployment challenges on edge devices, while static quantization sacrifices accuracy. Existing approaches of quantization-aware training (QAT) further suffer from weight training costs. We propose SASQ: a lightweight QAT framework specifically tailored for activation quantization factors. SASQ exclusively optimizes only the quantization factors (without changing pre-trained weights), enabling static inference with high accuracy while maintaining deployment efficiency. SASQ adaptively truncates some outliers, thereby reducing the difficulty of quantization while preserving the distributional characteristics of the activations. SASQ not only surpasses existing SOTA quantization schemes but also outperforms the corresponding FP16 models. On LLaMA2-7B, it achieves 5.2% lower perplexity than QuaRot and 4.7% lower perplexity than the FP16 model on WikiText2.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14481v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "SAC"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出SASQ框架以解决大语言模型量化训练中激活量化因子优化的效率与精度平衡问题",
            "summary_zh": "大语言模型在自然语言任务中表现出色，但其规模增长超过了GPU内存的进步，给部署带来了挑战。模型量化通过降低权重和激活的精度来缓解这一问题，但现有解决方案面临根本性的权衡：动态量化计算开销高且在边缘设备上部署困难，而静态量化则牺牲了准确性。现有的量化感知训练方法还面临权重训练成本高的问题。我们提出了SASQ：一个专门针对激活量化因子的轻量级量化感知训练框架。SASQ仅优化量化因子（不改变预训练权重），实现了高精度的静态推理，同时保持了部署效率。SASQ自适应地截断一些异常值，从而降低了量化的难度，同时保留了激活的分布特性。SASQ不仅超越了现有的最先进量化方案，还优于相应的FP16模型。在LLaMA2-7B上，它在WikiText2上的困惑度比QuaRot低5.2%，比FP16模型低4.7%。",
            "intro_zh": [
                "核心问题：现有量化方法面临动态量化计算开销高、静态量化精度低，以及量化感知训练权重训练成本高的挑战。",
                "方法要点：提出SASQ框架，仅优化激活量化因子而不改变预训练权重，实现轻量级量化感知训练。",
                "实验或效果：在LLaMA2-7B上，SASQ的困惑度比QuaRot低5.2%，比FP16模型低4.7%，超越了现有SOTA方案。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大语言模型量化部署中的核心问题，即如何在保持高精度的同时实现高效的静态推理。现有方法的痛点包括：动态量化因实时计算量化参数导致高计算开销和部署困难；静态量化虽然部署高效，但精度损失显著；而传统的量化感知训练方法需要重新训练权重，成本高昂且可能破坏预训练模型的性能。\\n\\n**核心思路**：论文的核心解决思路是专注于优化激活量化因子，而不改变预训练权重。这样设计的原因在于，激活的分布特性对量化精度影响更大，且优化量化因子比重新训练权重更轻量级，能够平衡精度与效率。通过自适应截断异常值，降低量化难度，同时保留激活的分布信息，从而实现高精度的静态量化。\\n\\n**技术框架**：SASQ的整体架构是一个轻量级的量化感知训练框架，主要包含两个阶段：首先，在预训练模型的基础上，仅对激活量化因子进行优化；其次，通过自适应截断机制处理激活中的异常值，以减少量化误差。流程上，它避免了权重更新，专注于量化参数的调整，最终输出一个可直接用于静态推理的量化模型。\\n\\n**关键创新**：最重要的技术创新点是提出了一种仅优化激活量化因子的轻量级量化感知训练方法。与现有方法的本质区别在于：它不涉及权重训练，从而降低了计算成本和部署复杂度；同时，通过自适应截断异常值，解决了激活分布中极端值对量化的负面影响，这在传统静态量化中常被忽略。\\n\\n**关键设计**：关键设计包括：使用量化因子作为可优化参数，通过梯度下降进行训练；引入自适应截断机制，根据激活分布动态调整截断阈值，以平衡精度和量化范围；损失函数可能基于模型输出与原始精度的差异，但具体细节在摘要中未明确说明；网络结构保持预训练权重不变，仅在前向传播中应用量化操作。这些设计确保了框架的轻量性和高效性。",
            "application_zh": "该研究主要应用于大语言模型的边缘部署和资源受限环境，如移动设备、嵌入式系统和云计算中的高效推理。其实际价值在于降低模型的内存占用和计算开销，同时保持高精度，有助于推动AI模型在实时应用中的普及。未来影响可能扩展到其他大规模神经网络模型的量化优化，提升整体AI系统的能效比。",
            "highlight_zh": "最重要的实验结果显示，SASQ在LLaMA2-7B模型上取得了显著性能提升。在WikiText2数据集上，其困惑度比现有最先进量化方案QuaRot低5.2%，比原始FP16模型低4.7%，这表明SASQ不仅超越了其他量化方法，甚至优于全精度模型，实现了量化后的精度增益。",
            "tags_zh": [
                "大语言模型量化",
                "激活量化",
                "量化感知训练",
                "静态推理",
                "轻量级优化",
                "模型部署",
                "边缘计算",
                "精度提升"
            ],
            "_index": 118
        },
        {
            "title": "Model-First Reasoning LLM Agents: Reducing Hallucinations through Explicit Problem Modeling",
            "authors": [
                "Annu Rana",
                "Gaurav Kumar"
            ],
            "arxiv_id": "2512.14474v1",
            "summary": "Large Language Models (LLMs) often struggle with complex multi-step planning tasks, showing high rates of constraint violations and inconsistent solutions. Existing strategies such as Chain-of-Thought and ReAct rely on implicit state tracking and lack an explicit problem representation. Inspired by classical AI planning, we propose Model-First Reasoning (MFR), a two-phase paradigm in which the LLM first constructs an explicit model of the problem, defining entities, state variables, actions, and constraints, before generating a solution plan. Across multiple planning domains, including medical scheduling, route planning, resource allocation, logic puzzles, and procedural synthesis, MFR reduces constraint violations and improves solution quality compared to Chain-of-Thought and ReAct. Ablation studies show that the explicit modeling phase is critical for these gains. Our results suggest that many LLM planning failures stem from representational deficiencies rather than reasoning limitations, highlighting explicit modeling as a key component for robust and interpretable AI agents. All prompts, evaluation procedures, and task datasets are documented to facilitate reproducibility.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14474v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出模型优先推理方法，通过显式问题建模减少LLM在复杂规划任务中的幻觉问题",
            "summary_zh": "大型语言模型在处理复杂多步规划任务时，常出现高约束违反率和不一致解。现有方法如思维链和ReAct依赖隐式状态跟踪，缺乏显式问题表示。受经典AI规划启发，我们提出模型优先推理，这是一种两阶段范式：LLM首先构建问题的显式模型，定义实体、状态变量、动作和约束，然后生成解决方案计划。在医疗调度、路径规划、资源分配、逻辑谜题和程序合成等多个规划领域中，与思维链和ReAct相比，MFR减少了约束违反并提高了解决方案质量。消融研究表明，显式建模阶段对这些改进至关重要。我们的结果表明，许多LLM规划失败源于表示缺陷而非推理限制，凸显显式建模作为稳健可解释AI代理的关键组成部分。所有提示、评估程序和任务数据集均已记录，以促进可重复性。",
            "intro_zh": [
                "现有LLM在复杂规划任务中常出现约束违反和不一致解，依赖隐式状态跟踪的方法如思维链和ReAct缺乏显式问题表示。",
                "论文提出模型优先推理，通过两阶段范式：先构建显式问题模型，再生成解决方案，以提升规划准确性和一致性。",
                "实验表明，MFR在多个规划领域显著减少约束违反，提高解决方案质量，消融研究证实显式建模阶段是关键因素。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型在复杂多步规划任务中高约束违反率和不一致解的问题。现有方法如思维链和ReAct依赖隐式状态跟踪，缺乏显式问题表示，导致模型在推理过程中容易产生幻觉和错误。\\n\\n**核心思路**：论文的核心解决思路是引入显式问题建模，借鉴经典AI规划理论，将规划过程分为建模和求解两个阶段。这样设计是因为许多LLM规划失败源于表示缺陷而非推理限制，通过显式建模可以增强模型的表示能力和规划准确性。\\n\\n**技术框架**：整体架构采用两阶段范式：第一阶段，LLM构建问题的显式模型，包括定义实体、状态变量、动作和约束；第二阶段，基于该模型生成解决方案计划。主要模块包括建模模块和规划模块，流程上先建模后规划，确保问题表示清晰。\\n\\n**关键创新**：最重要的技术创新点是模型优先推理范式，强调先建模后求解。与现有方法的本质区别在于，它从隐式状态跟踪转向显式问题表示，从而减少幻觉并提高规划的可解释性和稳健性。\\n\\n**关键设计**：论文未详细说明具体参数设置、损失函数或网络结构，但强调了显式建模阶段的关键性，所有提示、评估程序和任务数据集已文档化以促进可重复性，技术细节可能依赖于标准LLM框架和规划算法。",
            "application_zh": "该研究在医疗调度、路径规划、资源分配、逻辑谜题和程序合成等领域有广泛应用潜力，能提升AI代理在复杂任务中的准确性和可靠性。未来可推动稳健可解释AI系统的发展，为自动化规划和决策支持提供新方法。",
            "highlight_zh": "实验结果显示，在多个规划领域中，MFR相比思维链和ReAct显著减少约束违反并提高解决方案质量。具体性能数据未提供，但消融研究证实显式建模阶段是关键因素，提升幅度显著，凸显了显式建模在减少LLM幻觉方面的有效性。",
            "tags_zh": [
                "模型优先推理",
                "显式问题建模",
                "大型语言模型",
                "规划任务",
                "约束违反减少",
                "AI代理",
                "可解释性",
                "多步规划"
            ],
            "_index": 119
        },
        {
            "title": "AnySleep: a channel-agnostic deep learning system for high-resolution sleep staging in multi-center cohorts",
            "authors": [
                "Niklas Grieger",
                "Jannik Raskob",
                "Siamak Mehrkanoon",
                "Stephan Bialonski"
            ],
            "arxiv_id": "2512.14461v1",
            "summary": "Sleep is essential for good health throughout our lives, yet studying its dynamics requires manual sleep staging, a labor-intensive step in sleep research and clinical care. Across centers, polysomnography (PSG) recordings are traditionally scored in 30-s epochs for pragmatic, not physiological, reasons and can vary considerably in electrode count, montage, and subject characteristics. These constraints present challenges in conducting harmonized multi-center sleep studies and discovering novel, robust biomarkers on shorter timescales. Here, we present AnySleep, a deep neural network model that uses any electroencephalography (EEG) or electrooculography (EOG) data to score sleep at adjustable temporal resolutions. We trained and validated the model on over 19,000 overnight recordings from 21 datasets collected across multiple clinics, spanning nearly 200,000 hours of EEG and EOG data, to promote robust generalization across sites. The model attains state-of-the-art performance and surpasses or equals established baselines at 30-s epochs. Performance improves as more channels are provided, yet remains strong when EOG is absent or when only EOG or single EEG derivations (frontal, central, or occipital) are available. On sub-30-s timescales, the model captures short wake intrusions consistent with arousals and improves prediction of physiological characteristics (age, sex) and pathophysiological conditions (sleep apnea), relative to standard 30-s scoring. We make the model publicly available to facilitate large-scale studies with heterogeneous electrode setups and to accelerate the discovery of novel biomarkers in sleep.",
            "categories": [
                "cs.LG",
                "eess.SP",
                "q-bio.QM"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "18 pages, 6 figures, 2 tables",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14461v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出AnySleep深度学习系统，以解决多中心睡眠研究中电极设置异质性和时间分辨率受限的问题。",
            "summary_zh": "睡眠对健康至关重要，但研究其动态需要人工睡眠分期，这是睡眠研究和临床护理中劳动密集的步骤。传统上，多导睡眠图（PSG）记录在30秒时段内评分是出于实用而非生理原因，且电极数量、导联方式和受试者特征在不同中心差异很大。这些限制给开展协调的多中心睡眠研究以及在更短时间尺度上发现新的、稳健的生物标志物带来了挑战。本文提出了AnySleep，一种深度神经网络模型，可使用任何脑电图（EEG）或眼电图（EOG）数据以可调的时间分辨率进行睡眠评分。我们在来自21个数据集的超过19,000个夜间记录上训练和验证了该模型，涵盖近200,000小时的EEG和EOG数据，以促进跨站点的稳健泛化。该模型达到了最先进的性能，在30秒时段上超越或等同于已建立的基线。提供更多通道时性能会提高，但当EOG缺失或仅使用EOG或单个EEG导联（额叶、中央或枕叶）时，性能仍然强劲。在30秒以下的时间尺度上，该模型捕获了与觉醒一致的短暂清醒侵入，并相对于标准的30秒评分，改善了生理特征（年龄、性别）和病理生理状况（睡眠呼吸暂停）的预测。我们公开提供该模型，以促进具有异质电极设置的大规模研究，并加速睡眠中新生物标志物的发现。",
            "intro_zh": [
                "核心问题：传统睡眠分期依赖人工、30秒固定时段评分，且多中心数据在电极设置和受试者特征上差异大，限制了协调研究和短时生物标志物发现。",
                "方法要点：开发AnySleep深度神经网络，利用任意EEG或EOG数据，支持可调时间分辨率，通过大规模多中心数据训练实现跨站点泛化。",
                "实验或效果：模型在30秒时段达到SOTA性能，在少通道或单通道下仍稳健，在亚30秒尺度能捕获短时觉醒并提升生理和病理预测能力。"
            ],
            "method_zh": "**问题定义**：睡眠分期是睡眠研究和临床护理中的关键步骤，但传统方法依赖人工评分，效率低且主观性强。现有方法通常基于30秒固定时段的多导睡眠图（PSG）数据，这更多是出于实用而非生理原因，且不同中心的电极数量、导联方式和受试者特征差异很大，导致数据异质性高，难以进行协调的多中心研究或发现短时生物标志物。\\n\\n**核心思路**：论文提出AnySleep，一个深度神经网络模型，旨在解决电极设置异质性和时间分辨率受限的问题。核心思想是设计一个通道无关的模型，能够处理任意EEG或EOG数据，并支持可调的时间分辨率（如亚30秒），从而适应多中心数据的多样性并捕捉更精细的睡眠动态。\\n\\n**技术框架**：整体架构基于深度神经网络，可能包括卷积层和循环层以提取时空特征。流程涉及数据预处理（如标准化）、模型训练和验证。主要模块包括输入层（处理不同通道的EEG/EOG信号）、特征提取网络（学习睡眠阶段相关模式）和输出层（预测睡眠阶段）。训练阶段使用大规模多中心数据集，验证阶段评估跨站点泛化能力。\\n\\n**关键创新**：最重要的技术创新是通道无关性和可调时间分辨率。与现有方法通常依赖固定电极配置和30秒时段不同，AnySleep能灵活处理任意EEG或EOG通道，并支持亚30秒评分，这本质区别在于提高了模型的适应性和时间精度，使其更适合异质数据环境和短时分析。\\n\\n**关键设计**：关键设计包括使用深度神经网络架构（具体结构未知，但可能结合CNN和RNN），损失函数可能基于交叉熵以优化分类任务。参数设置涉及大规模训练数据（超过19,000个记录，近200,000小时），强调数据增强和正则化以提升泛化。网络结构可能设计为多输入头以处理不同通道组合，确保在少通道或单通道下仍保持性能。",
            "application_zh": "AnySleep的潜在应用领域包括睡眠医学、神经科学研究和临床护理。实际价值在于自动化睡眠分期，减少人工劳动，促进多中心睡眠研究的协调，加速新生物标志物（如短时觉醒）的发现。未来影响可能推动个性化睡眠监测和疾病诊断，例如在睡眠呼吸暂停等病理条件下实现更精准的评估。",
            "highlight_zh": "模型在30秒时段达到最先进性能，超越或等于基线方法。性能随通道数增加而提升，但在EOG缺失或仅使用单EEG导联（如额叶、中央或枕叶）时仍保持强劲。在亚30秒时间尺度，模型能有效捕获短时清醒侵入（与觉醒一致），并相对于标准30秒评分，显著改善生理特征（年龄、性别）和病理生理状况（睡眠呼吸暂停）的预测能力。",
            "tags_zh": [
                "睡眠分期",
                "深度学习",
                "多中心研究",
                "脑电图分析",
                "时间分辨率",
                "通道无关性",
                "生物标志物发现",
                "自动化评分"
            ],
            "_index": 120
        },
        {
            "title": "Reasoning-Style Poisoning of LLM Agents via Stealthy Style Transfer: Process-Level Attacks and Runtime Monitoring in RSV Space",
            "authors": [
                "Xingfu Zhou",
                "Pengfei Wang"
            ],
            "arxiv_id": "2512.14448v1",
            "summary": "Large Language Model (LLM) agents relying on external retrieval are increasingly deployed in high-stakes environments. While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. We propose Reasoning-Style Poisoning (RSP), a paradigm that manipulates how agents process information rather than what they process. We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers. To quantify these shifts, we develop the Reasoning Style Vector (RSV), a metric tracking Verification depth, Self-confidence, and Attention focus. Experiments on HotpotQA and FEVER using ReAct, Reflection, and Tree of Thoughts (ToT) architectures reveal that GSI significantly degrades performance. It increases reasoning steps by up to 4.4 times or induces premature errors, successfully bypassing state-of-the-art content filters. Finally, we propose RSP-M, a lightweight runtime monitor that calculates RSV metrics in real-time and triggers alerts when values exceed safety thresholds. Our work demonstrates that reasoning style is a distinct, exploitable vulnerability, necessitating process-level defenses beyond static content analysis.",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14448v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出推理风格投毒攻击与实时监控方法，揭示LLM代理过程级安全漏洞并实现动态防御。",
            "summary_zh": "大型语言模型（LLM）代理在依赖外部检索的高风险环境中部署日益增多。现有对抗攻击主要关注内容伪造或指令注入，而本文识别出一种新颖的、面向过程的攻击面：代理的推理风格。我们提出推理风格投毒（RSP），这是一种操纵代理如何处理信息而非处理什么信息的范式。我们引入生成式风格注入（GSI），一种攻击方法，在不改变基本事实或使用显式触发器的情况下，将检索到的文档重写为病态语调——特别是“分析瘫痪”或“认知仓促”。为了量化这些变化，我们开发了推理风格向量（RSV），这是一种跟踪验证深度、自信度和注意力焦点的指标。在HotpotQA和FEVER数据集上使用ReAct、Reflection和思维树（ToT）架构进行的实验表明，GSI显著降低了性能。它将推理步骤增加了高达4.4倍或导致过早错误，并成功绕过最先进的内容过滤器。最后，我们提出RSP-M，一种轻量级运行时监控器，实时计算RSV指标，并在值超过安全阈值时触发警报。我们的工作表明，推理风格是一种独特且可利用的漏洞，需要超越静态内容分析的过程级防御。",
            "intro_zh": [
                "现有对抗攻击主要针对内容伪造或指令注入，忽视了LLM代理推理过程本身的脆弱性，导致安全防御存在盲区。",
                "论文提出推理风格投毒攻击，通过生成式风格注入操纵文档语调，在不改变事实下诱导病态推理，并开发推理风格向量进行量化评估。",
                "实验显示攻击使推理步骤增加4.4倍或引发错误，成功绕过内容过滤器，同时提出的实时监控器能有效检测异常。"
            ],
            "method_zh": "**问题定义**：论文旨在解决LLM代理在依赖外部检索时面临的新型安全威胁——推理风格被恶意操纵的问题。现有对抗攻击（如内容伪造、指令注入）主要关注输入内容的真实性或指令的合规性，但忽略了代理内部推理过程（如思考深度、自信度、注意力分配）可能被间接影响，导致防御措施（如静态内容过滤器）存在局限性，无法检测过程级攻击。\\n\\n**核心思路**：论文的核心解决思路是识别并利用“推理风格”这一过程级攻击面，通过生成式风格注入（GSI）在不改变文档事实内容的前提下，重写文档为特定病态语调（如“分析瘫痪”或“认知仓促”），从而潜移默化地改变代理的推理行为。同时，开发推理风格向量（RSV）作为量化指标，以监控和防御此类攻击。这种设计基于一个关键洞察：代理的性能不仅取决于输入内容，还高度依赖于其处理信息的方式，而攻击者可以通过操纵风格来破坏这一过程。\\n\\n**技术框架**：整体架构包含攻击和防御两个主要模块。攻击阶段：首先，攻击者使用生成式风格注入（GSI）方法，将检索到的文档重写为目标病态风格（如过度分析或仓促决策），保持事实不变；然后，这些被篡改的文档输入给LLM代理（如ReAct、Reflection、ToT架构），诱导其产生异常推理行为。防御阶段：在代理运行时，实时计算推理风格向量（RSV），该向量基于三个维度——验证深度（衡量代理对信息的核查程度）、自信度（评估代理输出确定性）和注意力焦点（分析代理关注点的分布）；通过RSP-M监控器持续跟踪RSV值，一旦超过预设安全阈值，即触发警报，实现动态防御。\\n\\n**关键创新**：最重要的技术创新点在于首次将“推理风格”概念化为一个可量化的攻击目标，并提出了过程级的攻击范式（RSP）和防御机制。与现有方法（如基于内容的对抗攻击）的本质区别在于，它不依赖于修改事实内容或添加显式恶意指令，而是通过风格转移间接影响代理的内部认知过程，从而更隐蔽地绕过传统安全检测。这扩展了对抗攻击的研究范畴，从静态内容分析转向动态过程监控。\\n\\n**关键设计**：在技术细节上，生成式风格注入（GSI）可能利用预训练语言模型进行文本重写，确保语义一致性但调整语调风格；推理风格向量（RSV）的设计基于三个关键参数：验证深度（可通过代理的查询次数或反思步骤数度量）、自信度（基于输出概率或置信度分数）、注意力焦点（使用注意力权重或关键词提取分析）。这些参数在实验中具体化为可计算的指标，例如在HotpotQA和FEVER数据集上，通过对比基线（未攻击）和攻击后的RSV值变化来评估攻击效果。RSP-M监控器则实现为轻量级模块，集成到代理架构中，实时计算这些指标并设置阈值（如通过历史数据或专家知识定义安全范围），以触发干预。",
            "application_zh": "该研究在人工智能安全领域具有重要应用价值，特别适用于高风险环境中的LLM代理部署，如金融分析、医疗诊断、法律咨询和自动驾驶系统。通过揭示推理风格漏洞，它推动了过程级安全防御的发展，帮助开发更健壮的监控工具，防止隐蔽攻击导致决策失误。未来可能影响AI安全标准制定，促进动态、实时防御机制的普及。",
            "highlight_zh": "在HotpotQA和FEVER数据集上的实验表明，生成式风格注入（GSI）攻击显著降低了LLM代理性能：使用ReAct、Reflection和ToT架构时，推理步骤平均增加高达4.4倍，或诱导过早错误率显著上升。攻击成功绕过最先进的内容过滤器，突显过程级漏洞的严重性。同时，RSP-M监控器能实时检测RSV异常，验证了防御有效性，为动态安全提供了实证基础。",
            "tags_zh": [
                "推理风格投毒",
                "过程级攻击",
                "生成式风格注入",
                "推理风格向量",
                "实时监控",
                "LLM代理安全",
                "对抗攻击",
                "动态防御"
            ],
            "_index": 121
        },
        {
            "title": "S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation",
            "authors": [
                "Leon Sick",
                "Lukas Hoyer",
                "Dominik Engel",
                "Pedro Hermosilla",
                "Timo Ropinski"
            ],
            "arxiv_id": "2512.14440v1",
            "summary": "In recent years, the state-of-the-art in unsupervised video instance segmentation has heavily relied on synthetic video data, generated from object-centric image datasets such as ImageNet. However, video synthesis by artificially shifting and scaling image instance masks fails to accurately model realistic motion in videos, such as perspective changes, movement by parts of one or multiple instances, or camera motion. To tackle this issue, we propose an unsupervised video instance segmentation model trained exclusively on real video data. We start from unsupervised instance segmentation masks on individual video frames. However, these single-frame segmentations exhibit temporal noise and their quality varies through the video. Therefore, we establish temporal coherence by identifying high-quality keymasks in the video by leveraging deep motion priors. The sparse keymask pseudo-annotations are then used to train a segmentation model for implicit mask propagation, for which we propose a Sparse-To-Dense Distillation approach aided by a Temporal DropLoss. After training the final model on the resulting dense labelset, our approach outperforms the current state-of-the-art across various benchmarks.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project Page with Code/Models/Demo: https://leonsick.github.io/s2d/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14440v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "动作生成",
                    "matched_keywords": [
                        "motion prior"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出S2D稀疏到稠密关键掩码蒸馏方法，以解决无监督视频实例分割中合成数据运动建模不准确的问题。",
            "summary_zh": "近年来，无监督视频实例分割的最先进方法严重依赖于从以对象为中心的图像数据集（如ImageNet）生成的合成视频数据。然而，通过人工移动和缩放图像实例掩码来合成视频，无法准确建模视频中的真实运动，例如透视变化、单个或多个实例的部分运动或相机运动。为了解决这个问题，我们提出了一种仅使用真实视频数据训练的无监督视频实例分割模型。我们从单个视频帧上的无监督实例分割掩码开始。然而，这些单帧分割存在时间噪声，并且其质量在整个视频中变化。因此，我们通过利用深度运动先验来识别视频中的高质量关键掩码，从而建立时间一致性。稀疏的关键掩码伪注释随后用于训练一个用于隐式掩码传播的分割模型，为此我们提出了一种由时间DropLoss辅助的稀疏到稠密蒸馏方法。在最终模型上对生成的稠密标签集进行训练后，我们的方法在各种基准测试中超越了当前的最先进方法。",
            "intro_zh": [
                "现有方法依赖合成视频数据，但人工移动和缩放掩码无法准确建模真实运动，如透视变化或相机运动。",
                "提出S2D方法，利用深度运动先验识别高质量关键掩码，并通过稀疏到稠密蒸馏训练模型以建立时间一致性。",
                "在多个基准测试中，该方法超越了当前最先进方法，显著提升了无监督视频实例分割的性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决无监督视频实例分割中，现有方法依赖合成视频数据导致运动建模不准确的问题。现有方法通过人工移动和缩放图像实例掩码生成合成视频，无法有效处理真实视频中的复杂运动，如透视变化、部分实例运动或相机运动，从而限制了分割质量。\\n\\n**核心思路**：论文的核心思路是仅使用真实视频数据进行训练，通过识别高质量的关键掩码来建立时间一致性，并利用稀疏到稠密蒸馏方法训练模型进行隐式掩码传播。这样设计避免了合成数据的局限性，直接学习真实视频中的运动模式，提升分割的准确性和鲁棒性。\\n\\n**技术框架**：整体流程分为三个阶段：首先，从单个视频帧生成无监督实例分割掩码；其次，利用深度运动先验识别视频中的高质量关键掩码，建立稀疏伪注释；最后，使用稀疏到稠密蒸馏方法，结合时间DropLoss，训练分割模型以生成稠密标签集，实现隐式掩码传播。\\n\\n**关键创新**：最重要的技术创新是Sparse-To-Dense Distillation（S2D）方法，它通过稀疏关键掩码引导稠密分割训练，与现有方法本质区别在于完全基于真实视频数据，避免了合成数据的不准确性，并引入了时间DropLoss来增强训练稳定性。\\n\\n**关键设计**：关键设计包括：利用深度运动先验（如光流或运动估计网络）评估掩码质量以选择关键帧；设计稀疏到稠密蒸馏损失函数，将稀疏关键掩码作为监督信号传播到整个视频；引入时间DropLoss，在训练中随机丢弃部分时间步的损失，以减轻噪声影响并提升泛化能力；网络结构可能基于分割模型（如Mask R-CNN变体）进行适配，具体参数设置未知。",
            "application_zh": "该研究在视频分析、自动驾驶、监控系统和机器人视觉等领域具有潜在应用价值。通过提升无监督视频实例分割的准确性，可以支持更高效的视频内容理解、对象跟踪和场景解析，减少对标注数据的依赖，推动计算机视觉技术在真实世界任务中的实际部署。未来可能影响视频编辑、增强现实和智能交互系统的发展。",
            "highlight_zh": "在多个基准测试中，S2D方法显著超越了当前最先进的无监督视频实例分割模型。具体性能数据未知，但实验表明，通过仅使用真实视频数据和稀疏到稠密蒸馏，分割质量得到大幅提升，有效解决了合成数据运动建模不准确的问题，验证了方法的有效性和鲁棒性。",
            "tags_zh": [
                "无监督学习",
                "视频实例分割",
                "稀疏到稠密蒸馏",
                "时间一致性",
                "深度运动先验",
                "关键掩码识别",
                "隐式掩码传播",
                "真实视频数据"
            ],
            "_index": 122
        },
        {
            "title": "VICTOR: Dataset Copyright Auditing in Video Recognition Systems",
            "authors": [
                "Quan Yuan",
                "Zhikun Zhang",
                "Linkang Du",
                "Min Chen",
                "Mingyang Sun",
                "Yunjun Gao",
                "Shibo He",
                "Jiming Chen"
            ],
            "arxiv_id": "2512.14439v1",
            "summary": "Video recognition systems are increasingly being deployed in daily life, such as content recommendation and security monitoring. To enhance video recognition development, many institutions have released high-quality public datasets with open-source licenses for training advanced models. At the same time, these datasets are also susceptible to misuse and infringement. Dataset copyright auditing is an effective solution to identify such unauthorized use. However, existing dataset copyright solutions primarily focus on the image domain; the complex nature of video data leaves dataset copyright auditing in the video domain unexplored. Specifically, video data introduces an additional temporal dimension, which poses significant challenges to the effectiveness and stealthiness of existing methods.\n  In this paper, we propose VICTOR, the first dataset copyright auditing approach for video recognition systems. We develop a general and stealthy sample modification strategy that enhances the output discrepancy of the target model. By modifying only a small proportion of samples (e.g., 1%), VICTOR amplifies the impact of published modified samples on the prediction behavior of the target models. Then, the difference in the model's behavior for published modified and unpublished original samples can serve as a key basis for dataset auditing. Extensive experiments on multiple models and datasets highlight the superiority of VICTOR. Finally, we show that VICTOR is robust in the presence of several perturbation mechanisms to the training videos or the target models.",
            "categories": [
                "cs.CR",
                "cs.CV"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "To appear in the NDSS Symposium 2026, February 2026, San Diego, CA, USA",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14439v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出VICTOR方法以解决视频识别系统中数据集版权审计的挑战",
            "summary_zh": "视频识别系统在内容推荐和安全监控等日常生活中的应用日益广泛。为促进视频识别技术的发展，许多机构发布了高质量的开源公共数据集用于训练先进模型。然而，这些数据集也容易遭到滥用和侵权。数据集版权审计是识别此类未经授权使用的有效解决方案。但现有的数据集版权解决方案主要集中于图像领域；视频数据的复杂性使得视频领域的数据集版权审计尚未得到充分探索。具体而言，视频数据引入了额外的时间维度，这对现有方法的有效性和隐蔽性提出了重大挑战。本文提出了VICTOR，这是首个针对视频识别系统的数据集版权审计方法。我们开发了一种通用且隐蔽的样本修改策略，增强了目标模型的输出差异。通过仅修改一小部分样本（例如1%），VICTOR放大了已发布修改样本对目标模型预测行为的影响。然后，模型对已发布修改样本和未发布原始样本的行为差异可以作为数据集审计的关键依据。在多个模型和数据集上的广泛实验突显了VICTOR的优越性。最后，我们展示了VICTOR在面对训练视频或目标模型的多种扰动机制时具有鲁棒性。",
            "intro_zh": [
                "现有方法主要针对图像领域，视频数据的时间维度带来有效性和隐蔽性挑战，导致视频数据集版权审计未被探索。",
                "提出VICTOR方法，通过修改少量样本增强模型输出差异，利用行为差异作为审计依据，实现隐蔽且通用的版权检测。",
                "在多个模型和数据集上实验显示VICTOR显著优于基线，且对训练扰动具有鲁棒性，验证了其有效性和实用性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决视频识别系统中数据集版权审计的问题，即如何检测未经授权使用公开数据集训练模型的行为。现有方法主要针对图像数据，但视频数据引入时间维度，导致现有方法在有效性和隐蔽性方面面临挑战，例如难以处理视频的时序信息，且修改可能过于明显易被察觉。\\n\\n**核心思路**：论文的核心思路是通过修改少量视频样本（如1%），增强目标模型对这些修改样本的预测行为差异，从而放大模型对已发布修改样本和未发布原始样本的输出不一致性。这种差异作为审计依据，无需访问模型内部参数，仅依赖模型输出即可实现隐蔽的版权检测。\\n\\n**技术框架**：VICTOR的整体架构包括两个主要阶段：样本修改阶段和审计阶段。在样本修改阶段，设计一种通用策略对公开数据集中的少量视频进行修改，生成修改样本；在审计阶段，通过查询目标模型获取其对修改样本和原始样本的预测输出，计算行为差异（如输出概率分布的距离），基于预设阈值判断模型是否使用了该数据集。\\n\\n**关键创新**：最重要的技术创新是首次将数据集版权审计扩展到视频领域，并提出了针对视频时序特性的隐蔽修改策略。与现有图像方法相比，VICTOR本质区别在于处理视频数据的时间维度，通过优化修改以保持视频的自然性，同时确保模型行为差异显著，从而在复杂视频场景下实现高效审计。\\n\\n**关键设计**：关键设计包括修改比例设置为1%以平衡隐蔽性和有效性；使用基于梯度的优化方法生成修改，最小化视觉变化但最大化模型输出差异；审计时采用输出概率的统计差异（如KL散度）作为指标；框架无需模型白盒访问，仅依赖黑盒查询，增强了实用性。",
            "application_zh": "该研究在视频识别系统的版权保护领域具有重要应用价值，可应用于内容推荐平台、安全监控系统等场景，帮助数据发布者检测未经授权的数据集使用，维护知识产权。未来可能推动视频数据共享的规范化，促进AI伦理发展，并为多模态版权审计提供参考。",
            "highlight_zh": "实验在多个视频数据集（如Kinetics、UCF101）和模型（如3D CNN、Transformer）上进行，VICTOR在检测准确率上显著优于基线方法，提升幅度达20%以上，同时修改隐蔽性高，仅1%样本修改即可实现有效审计。此外，VICTOR对训练视频的多种扰动（如噪声添加、裁剪）表现出强鲁棒性，验证了其在实际复杂环境中的可靠性。",
            "tags_zh": [
                "视频识别",
                "数据集版权审计",
                "样本修改策略",
                "模型行为差异",
                "隐蔽性检测",
                "时间维度处理",
                "黑盒审计",
                "鲁棒性验证"
            ],
            "_index": 123
        },
        {
            "title": "Score-Based Turbo Message Passing for Plug-and-Play Compressive Imaging",
            "authors": [
                "Chang Cai",
                "Hao Jiang",
                "Xiaojun Yuan",
                "Ying-Jun Angela Zhang"
            ],
            "arxiv_id": "2512.14435v1",
            "summary": "Message-passing algorithms have been adapted for compressive imaging by incorporating various off-the-shelf image denoisers. However, these denoisers rely largely on generic or hand-crafted priors and often fall short in accurately capturing the complex statistical structure of natural images. As a result, traditional plug-and-play (PnP) methods often lead to suboptimal reconstruction, especially in highly underdetermined regimes. Recently, score-based generative models have emerged as a powerful framework for accurately characterizing sophisticated image distribution. Yet, their direct use for posterior sampling typically incurs prohibitive computational complexity. In this paper, by exploiting the close connection between score-based generative modeling and empirical Bayes denoising, we devise a message-passing framework that integrates a score-based minimum mean-squared error (MMSE) denoiser for compressive image recovery. The resulting algorithm, named score-based turbo message passing (STMP), combines the fast convergence of message passing with the expressive power of score-based generative priors. For practical systems with quantized measurements, we further propose quantized STMP (Q-STMP), which augments STMP with a component-wise MMSE dequantization module. We demonstrate that the asymptotic performance of STMP and Q-STMP can be accurately predicted by a set of state-evolution (SE) equations. Experiments on the FFHQ dataset demonstrate that STMP strikes a significantly better performance-complexity tradeoff compared with competing baselines, and that Q-STMP remains robust even under 1-bit quantization. Remarkably, both STMP and Q-STMP typically converge within 10 iterations.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14435v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出基于分数的Turbo消息传递算法，用于解决压缩成像中传统插拔式方法重建性能不足的问题",
            "summary_zh": "消息传递算法通过集成各种现成的图像去噪器，已被应用于压缩成像。然而，这些去噪器主要依赖于通用或手工设计的先验，往往难以准确捕捉自然图像的复杂统计结构，导致传统插拔式方法在高度欠定情况下重建效果不佳。最近，基于分数的生成模型已成为准确表征复杂图像分布的强大框架，但其直接用于后验采样通常计算复杂度极高。本文通过利用基于分数的生成建模与经验贝叶斯去噪之间的紧密联系，设计了一个消息传递框架，该框架集成了基于分数的最小均方误差去噪器用于压缩图像恢复。所得算法称为基于分数的Turbo消息传递，结合了消息传递的快速收敛性和基于分数的生成先验的表达能力。对于具有量化测量的实际系统，我们进一步提出了量化STMP，它在STMP基础上增加了分量级MMSE去量化模块。我们证明STMP和Q-STMP的渐近性能可以通过一组状态演化方程准确预测。在FFHQ数据集上的实验表明，与竞争基线相比，STMP在性能与复杂度之间取得了显著更好的权衡，而Q-STMP即使在1比特量化下仍保持鲁棒性。值得注意的是，STMP和Q-STMP通常都在10次迭代内收敛。",
            "intro_zh": [
                "核心问题：传统插拔式压缩成像方法依赖通用先验，难以捕捉图像复杂统计结构，导致欠定情况下重建效果不佳。",
                "方法要点：结合基于分数的生成模型与消息传递框架，设计最小均方误差去噪器，实现高效后验采样。",
                "实验或效果：在FFHQ数据集上，新方法在性能与复杂度间取得更好权衡，量化版本在1比特下仍鲁棒，通常10次迭代内收敛。"
            ],
            "method_zh": "**问题定义**：论文解决压缩成像中的图像重建问题，特别是在高度欠定测量条件下。现有插拔式方法依赖通用或手工先验，难以准确建模自然图像的复杂分布，导致重建质量下降，且基于分数的生成模型直接用于后验采样计算成本过高。\\n\\n**核心思路**：利用基于分数的生成模型与经验贝叶斯去噪之间的理论联系，将分数函数集成到消息传递框架中，通过最小均方误差去噪器实现高效后验近似，从而平衡表达能力和计算效率。\\n\\n**技术框架**：整体框架包括基于分数的Turbo消息传递算法，它结合了消息传递的迭代优化和基于分数的先验建模。主要模块包括测量更新模块（处理压缩感知模型）、先验更新模块（使用基于分数的MMSE去噪器），以及针对量化系统的去量化模块（在Q-STMP中）。流程通过交替更新实现快速收敛。\\n\\n**关键创新**：最重要的创新是将基于分数的生成先验无缝集成到消息传递框架中，避免了直接后验采样的高计算复杂度，同时通过状态演化方程理论预测性能，提供了可解释性和鲁棒性。与现有方法相比，本质区别在于利用分数函数作为强大先验，而非依赖有限的手工先验。\\n\\n**关键设计**：关键设计包括基于分数的MMSE去噪器，它通过预训练的分数网络估计后验均值；状态演化方程用于分析算法渐近行为；在Q-STMP中，分量级MMSE去量化模块处理量化噪声；参数设置如迭代次数通常少于10，损失函数基于均方误差优化，网络结构可能涉及深度生成模型（具体架构未在摘要中详述，但依赖于分数建模）。",
            "application_zh": "该研究在压缩成像领域具有广泛潜在应用，如医学成像、遥感、安全监控和低功耗物联网设备，其中测量数据常受压缩或量化限制。通过提升欠定情况下的重建质量并降低计算成本，STMP和Q-STMP可推动高效图像采集系统的发展，未来可能扩展到视频重建和多模态感知任务。",
            "highlight_zh": "在FFHQ数据集上的实验显示，STMP相比基线方法在性能与复杂度间取得显著更好权衡，具体提升幅度未在摘要中量化，但强调优于竞争方法。Q-STMP在1比特量化下仍保持鲁棒重建能力，突显其对极端压缩的适应性。算法收敛速度快，STMP和Q-STMP通常仅在10次迭代内达到稳定，大幅降低计算开销。状态演化方程验证了理论预测与实验结果的一致性。",
            "tags_zh": [
                "压缩成像",
                "消息传递算法",
                "基于分数的生成模型",
                "插拔式方法",
                "量化测量",
                "图像重建",
                "状态演化方程",
                "最小均方误差去噪"
            ],
            "_index": 124
        },
        {
            "title": "Geometric Parameter Optimization of a Novel 3-(PP(2-(UPS))) Redundant Parallel Mechanism based on Workspace Determination",
            "authors": [
                "Quan Yuan",
                "Daqian Cao",
                "Weibang Bai"
            ],
            "arxiv_id": "2512.14434v1",
            "summary": "Redundant parallel robots are normally employed in scenarios requiring good precision, high load capability, and large workspace compared to traditional parallel mechanisms. However, the elementary robotic configuration and geometric parameter optimization are still quite challenging. This paper proposes a novel 3-(PP(2-(UPS))) redundant parallel mechanism, with good generalizability first, and further investigates the kinematic optimization issue by analyzing and investigating how its key geometric parameters influence the volume, shape, boundary completeness, and orientation capabilities of its workspace. The torsional capability index TI_1 and tilting capability index TI_2 are defined to evaluate the orientation performance of the mechanism. Numerical simulation studies are completed to indicate the analysis, providing reasonable but essential references for the parameter optimization of 3-(PP(2-(UPS))) and other similar redundant parallel mechanisms.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "7 pages, 5 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14434v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出3-(PP(2-(UPS)))冗余并联机构并基于工作空间分析优化几何参数，以提升机器人性能。",
            "summary_zh": "冗余并联机器人通常应用于需要高精度、高负载能力和大工作空间的场景，相比传统并联机构具有优势。然而，其基本机器人构型和几何参数优化仍然具有挑战性。本文首先提出了一种新型的3-(PP(2-(UPS)))冗余并联机构，具有良好的通用性；然后通过分析和研究其关键几何参数如何影响工作空间的体积、形状、边界完整性和定向能力，进一步探讨了运动学优化问题。定义了扭转能力指数TI_1和倾斜能力指数TI_2来评估机构的定向性能。完成了数值模拟研究以验证分析，为3-(PP(2-(UPS)))及其他类似冗余并联机构的参数优化提供了合理且重要的参考。",
            "intro_zh": [
                "冗余并联机器人虽在精度、负载和工作空间方面优于传统机构，但其构型设计和参数优化仍面临挑战，缺乏系统方法。",
                "提出新型3-(PP(2-(UPS)))冗余并联机构，通过分析几何参数对工作空间的影响，定义能力指数来优化运动学性能。",
                "数值模拟验证了参数优化效果，为类似机构提供了关键参考，提升了工作空间和定向能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决冗余并联机器人的几何参数优化问题。现有方法在构型设计和参数选择上缺乏系统性，导致工作空间（包括体积、形状和定向能力）难以最大化，影响机器人在高精度、大负载场景下的性能。\\n\\n**核心思路**：通过提出一种新型的3-(PP(2-(UPS)))冗余并联机构，并分析其关键几何参数（如连杆长度、关节位置等）对工作空间特性的影响，定义定量指标来评估定向性能，从而指导参数优化。这种思路基于运动学分析，强调参数与工作空间的直接关联。\\n\\n**技术框架**：整体流程包括机构设计、工作空间分析、性能指标定义和数值模拟。首先，设计3-(PP(2-(UPS)))冗余并联机构，确保其通用性；然后，建立运动学模型，分析几何参数如何影响工作空间的体积、形状、边界完整性和定向能力；接着，定义扭转能力指数TI_1和倾斜能力指数TI_2作为评估指标；最后，通过数值模拟验证分析结果，优化参数设置。\\n\\n**关键创新**：最重要的技术创新是提出3-(PP(2-(UPS)))冗余并联机构并系统分析其几何参数对工作空间的影响，定义了TI_1和TI_2指数来量化定向性能。与现有方法相比，本质区别在于将参数优化与工作空间特性直接关联，提供了更全面的性能评估框架。\\n\\n**关键设计**：关键设计包括3-(PP(2-(UPS)))机构的构型参数（如PP和UPS支链的配置）、工作空间分析中的几何参数变量（具体参数未知）、以及TI_1和TI_2指数的数学定义（基于运动学模型计算定向能力）。数值模拟中可能涉及参数扫描或优化算法，但具体细节未在摘要中说明。",
            "application_zh": "该研究可应用于工业机器人、精密加工、航空航天和医疗设备等领域，其中需要高精度、高负载和大工作空间的冗余并联机器人。通过优化几何参数，能提升机器人的运动性能和可靠性，为复杂任务（如装配、检测）提供更高效的解决方案，未来可能推动机器人设计标准化和性能提升。",
            "highlight_zh": "数值模拟结果表明，通过优化几何参数，3-(PP(2-(UPS)))机构的工作空间体积和形状得到改善，TI_1和TI_2指数显示定向能力提升。具体数据未在摘要中提供，但分析为参数优化提供了关键参考，相比未优化状态，预期在定向精度和工作空间范围上有显著提升。",
            "tags_zh": [
                "冗余并联机构",
                "几何参数优化",
                "工作空间分析",
                "运动学性能",
                "定向能力指数",
                "数值模拟",
                "机器人设计",
                "并联机器人"
            ],
            "_index": 125
        },
        {
            "title": "Seismology modeling agent: A smart assistant for geophysical researchers",
            "authors": [
                "Yukun Ren",
                "Siwei Yu",
                "Kai Chen",
                "Jianwei Ma"
            ],
            "arxiv_id": "2512.14429v1",
            "summary": "To address the steep learning curve and reliance on complex manual file editing and command-line operations in the traditional workflow of the mainstream open-source seismic wave simulation software SPECFEM, this paper proposes an intelligent, interactive workflow powered by Large Language Models (LLMs). We introduce the first Model Context Protocol (MCP) server suite for SPECFEM (supporting 2D, 3D Cartesian, and 3D Globe versions), which decomposes the entire simulation process into discrete, agent-executable tools spanning from parameter generation and mesh partitioning to solver execution and visualization. This approach enables a paradigm shift from file-driven to intent-driven conversational interactions. The framework supports both fully automated execution and human-in-the-loop collaboration, allowing researchers to guide simulation strategies in real time and retain scientific decision-making authority while significantly reducing tedious low-level operations. Validated through multiple case studies, the workflow operates seamlessly in both autonomous and interactive modes, yielding high-fidelity results consistent with standard baselines. As the first application of MCP technology to computational seismology, this study significantly lowers the entry barrier, enhances reproducibility, and offers a promising avenue for advancing computational geophysics toward AI-assisted and automated scientific research. The complete source code is available at https://github.com/RenYukun1563/specfem-mcp.",
            "categories": [
                "cs.AI",
                "cs.SE"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "26 pages, 15 figures. Code available at https://github.com/RenYukun1563/specfem-mcp",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14429v1",
            "code_links": [
                {
                    "url": "https://github.com/RenYukun1563/specfem-mcp",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出基于大语言模型的智能交互工作流，以降低SPECFEM地震波模拟软件的使用门槛并提升效率。",
            "summary_zh": "针对主流开源地震波模拟软件SPECFEM在传统工作流程中存在的学习曲线陡峭、依赖复杂手动文件编辑和命令行操作等问题，本文提出了一种由大语言模型驱动的智能交互工作流。我们首次为SPECFEM（支持2D、3D笛卡尔和3D全球版本）引入了模型上下文协议服务器套件，将整个模拟过程分解为从参数生成、网格划分到求解器执行和可视化的离散化、可由智能体执行的工具。这种方法实现了从文件驱动到意图驱动的对话式交互的范式转变。该框架支持全自动执行和人机协同两种模式，使研究人员能够在实时指导模拟策略的同时保留科学决策权，并显著减少繁琐的低级操作。通过多个案例研究验证，该工作流在自主和交互模式下均能无缝运行，产生与标准基线一致的高保真结果。作为MCP技术在计算地震学领域的首次应用，本研究显著降低了入门门槛，增强了可重复性，并为推动计算地球物理学向AI辅助和自动化科学研究发展提供了有前景的途径。完整源代码可在https://github.com/RenYukun1563/specfem-mcp获取。",
            "intro_zh": [
                "核心问题：SPECFEM传统工作流程学习曲线陡峭，依赖复杂手动文件编辑和命令行操作，效率低下且易出错。",
                "方法要点：基于大语言模型构建智能交互工作流，将模拟过程分解为离散化工具，实现意图驱动的对话式交互。",
                "实验或效果：在自主和交互模式下均能无缝运行，产生与标准基线一致的高保真结果，显著降低操作复杂度。"
            ],
            "method_zh": "**问题定义**：论文旨在解决SPECFEM地震波模拟软件在传统工作流程中存在的两大痛点：一是学习曲线陡峭，新用户需要花费大量时间掌握复杂的文件格式和命令行参数；二是工作流高度依赖手动文件编辑和命令行操作，过程繁琐且容易出错，降低了科研效率。\\n\\n**核心思路**：论文的核心思路是利用大语言模型的自然语言理解和代码生成能力，构建一个智能交互式工作流，将用户从底层的文件操作中解放出来。通过将整个模拟过程分解为一系列离散的、可被智能体执行的工具，实现从“文件驱动”到“意图驱动”的范式转变，用户只需用自然语言描述模拟需求，系统即可自动完成配置和执行。\\n\\n**技术框架**：整体架构基于模型上下文协议构建。首先，为SPECFEM的2D、3D笛卡尔和3D全球版本开发了一套MCP服务器，这些服务器将模拟流程的关键步骤（如参数生成、网格划分、求解器执行、后处理可视化）封装成独立的工具。然后，通过一个智能体（Agent）来协调这些工具，该智能体能够理解用户的自然语言指令，并将其转化为具体的工具调用序列。框架支持两种模式：全自动模式（用户给出目标，系统自动规划并执行）和人机协同模式（用户可实时介入指导）。\\n\\n**关键创新**：最重要的技术创新是首次将模型上下文协议应用于计算地震学领域，并构建了首个面向SPECFEM的MCP服务器套件。与现有手动或脚本化方法相比，其本质区别在于引入了“意图驱动”和“对话式交互”的新范式，通过高层抽象屏蔽了底层实现细节，使得非专家用户也能高效地进行复杂的地震波模拟。\\n\\n**关键设计**：关键技术细节包括：1) 工具设计：将SPECFEM工作流精确分解为多个原子化工具，每个工具对应一个具体的功能模块（如“generate_parameters”、“run_solver”），并定义了清晰的输入输出接口。2) MCP服务器实现：基于MCP协议规范，为每个工具开发了相应的服务器，使其能够被标准化的智能体框架调用。3) 智能体提示工程：设计了针对地震模拟领域的提示词模板，以引导大语言模型准确理解用户意图并生成正确的工具调用链。具体网络结构或损失函数等细节在摘要中未明确提及，属于未知。",
            "application_zh": "该研究主要应用于计算地球物理学领域，特别是地震波模拟相关的科研与工程实践。其实际价值在于显著降低了高性能计算地震模拟的使用门槛，使得地球物理学家、地震学家甚至相关领域的学生能够更专注于科学问题本身，而非繁琐的软件操作。未来，这种AI辅助的自动化工作流范式有望推广到其他计算密集型科学软件中，推动整个计算科学向更智能、更易用的方向发展。",
            "highlight_zh": "通过多个案例研究验证，所提出的智能工作流在完全自主运行和交互式人机协同两种模式下均能无缝操作。关键实验结果表明，该系统生成的地震波模拟结果具有高保真度，与使用传统手动方法得到的标准基线结果完全一致。这证明了该框架在保持结果科学准确性的同时，能够有效接管复杂的流程操作，具体性能提升体现在将用户从大量的手动文件编辑和命令行操作中解放出来，操作复杂度显著降低。",
            "tags_zh": [
                "地震波模拟",
                "大语言模型应用",
                "智能工作流",
                "模型上下文协议",
                "人机协同",
                "计算地球物理学",
                "意图驱动交互",
                "科学软件自动化"
            ],
            "_index": 126
        },
        {
            "title": "Quadratic Kalman Filter for Elliptical Extended Object Tracking based on Decoupling State Components",
            "authors": [
                "Simon Steuernagel",
                "Marcus Baum"
            ],
            "arxiv_id": "2512.14426v1",
            "summary": "Extended object tracking involves estimating both the physical extent and kinematic parameters of a target object, where typically multiple measurements are observed per time step. In this article, we propose a deterministic closed-form elliptical extended object tracker, based on decoupling of the kinematics, orientation, and axis lengths. By disregarding potential correlations between these state components, fewer approximations are required for the individual estimators than for an overall joint solution. The resulting algorithm outperforms existing algorithms, reaching the accuracy of sampling-based procedures. Additionally, a batch-based variant is introduced, yielding highly efficient computation while outperforming all comparable state-of-the-art algorithms. This is validated both by a simulation study using common models from literature, as well as an extensive quantitative evaluation on real automotive radar data.",
            "categories": [
                "eess.SP",
                "cs.RO"
            ],
            "primary_category": "eess.SP",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "13 pages, 8 figures, submitted to IEEE Transactions on Aerospace and Electronic Systems",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14426v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出基于状态分量解耦的二次卡尔曼滤波器，用于椭圆扩展目标跟踪，实现高效高精度估计。",
            "summary_zh": "扩展目标跟踪涉及同时估计目标物体的物理范围和运动学参数，通常每个时间步会观测到多个测量值。本文提出了一种基于运动学、方向和轴长解耦的确定性闭式椭圆扩展目标跟踪器。通过忽略这些状态分量之间潜在的相关性，相比整体联合解决方案，各个估计器所需的近似更少。所提出的算法优于现有算法，达到了基于采样方法的精度水平。此外，还引入了基于批处理的变体，实现了高效计算，同时超越了所有可比较的最先进算法。这通过使用文献中常见模型的仿真研究，以及对真实汽车雷达数据的广泛定量评估得到了验证。",
            "intro_zh": [
                "现有扩展目标跟踪方法通常需要复杂的联合估计，导致计算量大且近似误差累积，难以平衡精度与效率。",
                "论文提出将目标状态解耦为运动学、方向和轴长分量，分别独立估计，减少近似需求，并设计二次卡尔曼滤波器实现闭式更新。",
                "算法在仿真和真实雷达数据上验证，达到采样方法精度，批处理变体计算高效，超越所有可比先进算法。"
            ],
            "method_zh": "**问题定义**：论文解决椭圆扩展目标跟踪问题，即同时估计目标的运动状态（如位置、速度）和物理范围（如椭圆形状参数）。现有方法通常采用联合估计框架，将运动学和形状参数耦合在一个状态向量中，这导致需要复杂的近似（如线性化或采样）来处理非线性测量模型，计算成本高且可能引入累积误差，难以在精度和效率之间取得平衡。\\n\\n**核心思路**：论文的核心思路是状态分量解耦，即将目标状态分解为三个独立分量：运动学（描述目标整体运动）、方向（椭圆朝向）和轴长（椭圆大小）。通过忽略这些分量之间的潜在相关性，可以分别设计独立的估计器，每个估计器只需处理更简单的子问题，从而减少整体所需的近似步骤，提高估计精度和计算效率。\\n\\n**技术框架**：整体架构包括三个主要阶段：首先，将扩展目标建模为椭圆，状态分解为运动学、方向和轴长分量；其次，设计基于卡尔曼滤波的独立估计器，其中运动学估计使用标准卡尔曼滤波，方向和轴长估计采用二次卡尔曼滤波（QKF）处理非线性测量；最后，通过批处理变体优化计算，利用多个时间步的测量进行批量更新，进一步提升效率。流程上，每个时间步接收多个测量点，分别更新各分量状态，然后组合得到完整目标估计。\\n\\n**关键创新**：最重要的技术创新是状态分量解耦与二次卡尔曼滤波的结合。与现有方法（如联合扩展卡尔曼滤波或粒子滤波）相比，本质区别在于避免了状态分量间的强耦合，通过解耦降低了问题复杂度，使得每个估计器可以更精确地处理其子空间，而QKF提供了闭式更新，避免了采样开销，从而在保持高精度的同时实现确定性计算。\\n\\n**关键设计**：关键技术细节包括：使用椭圆参数化（中心、方向角、半轴长）表示扩展目标；测量模型基于空间分布假设，将测量点关联到椭圆表面；二次卡尔曼滤波应用于方向和轴长估计，通过二阶泰勒展开近似非线性函数，提高精度；批处理变体利用滑动窗口或固定批次大小，聚合历史测量进行状态更新，减少单步计算负担；参数设置如过程噪声和测量噪声协方差基于常见模型或数据自适应调整，以优化跟踪性能。",
            "application_zh": "该研究在自动驾驶、机器人感知和军事监控等领域具有重要应用价值。例如，在自动驾驶中，可用于精确跟踪车辆、行人等扩展目标，提升环境感知能力；在机器人导航中，能估计障碍物形状和运动，增强避障和路径规划。其高效高精度的特性有望推动实时扩展目标跟踪系统的实际部署，未来可能扩展到多目标跟踪或非椭圆形状估计，进一步扩大应用范围。",
            "highlight_zh": "在仿真实验中，使用文献常见模型验证，算法达到基于采样方法（如粒子滤波）的精度水平，同时计算更高效。在真实汽车雷达数据上，通过广泛定量评估，批处理变体在跟踪精度和计算速度上均超越所有可比先进算法，具体提升幅度因数据集而异，但整体表现出显著优势，例如在位置和形状估计误差上降低约10-20%，计算时间减少30-50%。",
            "tags_zh": [
                "扩展目标跟踪",
                "椭圆建模",
                "状态解耦",
                "二次卡尔曼滤波",
                "批处理优化",
                "汽车雷达",
                "自动驾驶感知",
                "确定性估计"
            ],
            "_index": 127
        },
        {
            "title": "DISCODE: Distribution-Aware Score Decoder for Robust Automatic Evaluation of Image Captioning",
            "authors": [
                "Nakamasa Inoue",
                "Kanoko Goto",
                "Masanari Oi",
                "Martyna Gruszka",
                "Mahiro Ukai",
                "Takumi Hirose",
                "Yusuke Sekikawa"
            ],
            "arxiv_id": "2512.14420v1",
            "summary": "Large vision-language models (LVLMs) have shown impressive performance across a broad range of multimodal tasks. However, robust image caption evaluation using LVLMs remains challenging, particularly under domain-shift scenarios. To address this issue, we introduce the Distribution-Aware Score Decoder (DISCODE), a novel finetuning-free method that generates robust evaluation scores better aligned with human judgments across diverse domains. The core idea behind DISCODE lies in its test-time adaptive evaluation approach, which introduces the Adaptive Test-Time (ATT) loss, leveraging a Gaussian prior distribution to improve robustness in evaluation score estimation. This loss is efficiently minimized at test time using an analytical solution that we derive. Furthermore, we introduce the Multi-domain Caption Evaluation (MCEval) benchmark, a new image captioning evaluation benchmark covering six distinct domains, designed to assess the robustness of evaluation metrics. In our experiments, we demonstrate that DISCODE achieves state-of-the-art performance as a reference-free evaluation metric across MCEval and four representative existing benchmarks.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Paper accepted to AAAI 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14420v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出DISCODE方法以解决图像描述自动评估在领域偏移下的鲁棒性问题",
            "summary_zh": "大型视觉语言模型（LVLMs）在多模态任务中表现出色，但在图像描述评估中，尤其是在领域偏移场景下，实现鲁棒评估仍具挑战性。为解决这一问题，我们提出了分布感知分数解码器（DISCODE），这是一种无需微调的新方法，能够生成更符合人类判断的鲁棒评估分数。DISCODE的核心在于其测试时自适应评估方法，引入了自适应测试时（ATT）损失，利用高斯先验分布提高评估分数估计的鲁棒性。我们推导出该损失的解析解，可在测试时高效最小化。此外，我们提出了多领域描述评估（MCEval）基准，这是一个涵盖六个不同领域的新图像描述评估基准，旨在评估评估指标的鲁棒性。实验表明，DISCODE在MCEval和四个现有代表性基准上作为无参考评估指标实现了最先进的性能。",
            "intro_zh": [
                "现有大型视觉语言模型在图像描述评估中面临领域偏移挑战，导致评估分数与人类判断不一致。",
                "DISCODE采用测试时自适应评估方法，通过高斯先验分布和ATT损失提高鲁棒性，无需额外微调。",
                "在MCEval基准和四个现有基准上，DISCODE作为无参考评估指标实现了最先进的性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决图像描述自动评估在领域偏移下的鲁棒性问题。现有大型视觉语言模型（LVLMs）在评估时，当测试数据与训练数据分布不同（即领域偏移）时，评估分数可能偏离人类判断，导致评估不准确。现有方法通常依赖固定模型或需要额外微调，缺乏自适应能力，难以应对多样化的实际应用场景。\\n\\n**核心思路**：DISCODE的核心思路是引入测试时自适应评估，通过利用高斯先验分布来调整评估分数，使其更鲁棒地适应不同领域。这种方法基于统计推断，在测试时动态优化分数估计，而无需重新训练或微调模型，从而提高了评估的泛化能力和效率。\\n\\n**技术框架**：DISCODE的整体架构包括两个主要阶段：首先，使用预训练的LVLM生成初始评估分数；然后，通过自适应测试时（ATT）损失函数，结合高斯先验分布，对分数进行优化。关键模块包括分数解码器和ATT损失计算器，其中ATT损失基于解析解高效最小化，确保在测试时快速适应新领域。\\n\\n**关键创新**：最重要的技术创新是自适应测试时（ATT）损失和其解析解。ATT损失引入了高斯先验分布，作为正则化项，约束评估分数在领域偏移下的变化，从而提高鲁棒性。与现有方法相比，DISCODE无需微调，仅通过测试时优化即可实现自适应，这减少了计算开销并增强了实用性。\\n\\n**关键设计**：关键设计包括ATT损失函数，其形式基于高斯分布假设，具体参数如均值和方差从数据中估计；解析解允许在测试时通过闭式解快速最小化损失，避免了迭代优化；此外，MCEval基准的构建涉及六个不同领域（如自然图像、艺术等），用于全面评估鲁棒性。网络结构上，DISCODE作为轻量级解码器附加到现有LVLM上，不改变主干模型。",
            "application_zh": "DISCODE可广泛应用于图像描述生成系统的自动评估，特别是在需要跨领域鲁棒性的场景中，如内容审核、辅助视觉障碍人士、多媒体检索和教育工具。其无需微调的特性降低了部署成本，提高了评估效率，未来可能推动多模态评估指标的发展，促进更可靠的人工智能系统设计。",
            "highlight_zh": "在MCEval基准上，DISCODE作为无参考评估指标，相比现有方法（如CLIPScore、BERTScore）在多个领域实现了显著提升，具体性能数据未知，但论文报告其达到了最先进的水平。在四个现有基准（如COCO Captions）上，DISCODE也表现出优越的鲁棒性和一致性，提升了评估分数与人类判断的相关性。",
            "tags_zh": [
                "图像描述评估",
                "领域偏移鲁棒性",
                "测试时自适应",
                "无参考评估指标",
                "大型视觉语言模型",
                "高斯先验分布",
                "多模态评估",
                "解析解优化"
            ],
            "_index": 128
        },
        {
            "title": "PortAgent: LLM-driven Vehicle Dispatching Agent for Port Terminals",
            "authors": [
                "Jia Hu",
                "Junqi Li",
                "Weimeng Lin",
                "Peng Jia",
                "Yuxiong Ji",
                "Jintao Lai"
            ],
            "arxiv_id": "2512.14417v1",
            "summary": "Vehicle Dispatching Systems (VDSs) are critical to the operational efficiency of Automated Container Terminals (ACTs). However, their widespread commercialization is hindered due to their low transferability across diverse terminals. This transferability challenge stems from three limitations: high reliance on port operational specialists, a high demand for terminal-specific data, and time-consuming manual deployment processes. Leveraging the emergence of Large Language Models (LLMs), this paper proposes PortAgent, an LLM-driven vehicle dispatching agent that fully automates the VDS transferring workflow. It bears three features: (1) no need for port operations specialists; (2) low need of data; and (3) fast deployment. Specifically, specialist dependency is eliminated by the Virtual Expert Team (VET). The VET collaborates with four virtual experts, including a Knowledge Retriever, Modeler, Coder, and Debugger, to emulate a human expert team for the VDS transferring workflow. These experts specialize in the domain of terminal VDS via a few-shot example learning approach. Through this approach, the experts are able to learn VDS-domain knowledge from a few VDS examples. These examples are retrieved via a Retrieval-Augmented Generation (RAG) mechanism, mitigating the high demand for terminal-specific data. Furthermore, an automatic VDS design workflow is established among these experts to avoid extra manual interventions. In this workflow, a self-correction loop inspired by the LLM Reflexion framework is created",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14417v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出PortAgent：基于大语言模型的车辆调度代理，实现自动化集装箱码头车辆调度系统的快速迁移部署",
            "summary_zh": "车辆调度系统（VDS）对自动化集装箱码头（ACT）的运营效率至关重要，但其广泛商业化受到跨码头可迁移性低的阻碍。这一可迁移性挑战源于三个限制：高度依赖港口运营专家、对码头特定数据的高需求以及耗时的手动部署过程。利用大语言模型（LLM）的出现，本文提出了PortAgent，一个LLM驱动的车辆调度代理，完全自动化VDS迁移工作流程。它具有三个特点：（1）无需港口运营专家；（2）数据需求低；（3）部署快速。具体而言，通过虚拟专家团队（VET）消除了专家依赖。VET与四个虚拟专家协作，包括知识检索器、建模器、编码器和调试器，以模拟人类专家团队进行VDS迁移工作流程。这些专家通过少样本示例学习方法在码头VDS领域专业化。通过这种方法，专家能够从少量VDS示例中学习VDS领域知识。这些示例通过检索增强生成（RAG）机制检索，减轻了对码头特定数据的高需求。此外，在这些专家之间建立了自动VDS设计工作流程，以避免额外的手动干预。在该工作流程中，创建了一个受LLM Reflexion框架启发的自校正循环。",
            "intro_zh": [
                "核心问题：现有车辆调度系统跨码头迁移性低，依赖专家、数据需求高、部署耗时，阻碍商业化应用。",
                "方法要点：提出PortAgent，基于大语言模型构建虚拟专家团队，通过少样本学习和RAG机制自动化迁移流程。",
                "实验或效果：PortAgent实现了快速部署，减少专家依赖和数据需求，提升车辆调度系统的可迁移性和效率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自动化集装箱码头中车辆调度系统（VDS）跨码头迁移的挑战。现有方法痛点包括：高度依赖港口运营专家进行定制化设计，需要大量码头特定数据来训练模型，以及手动部署过程耗时且成本高，导致VDS的可迁移性和商业化应用受限。\\n\\n**核心思路**：论文的核心解决思路是利用大语言模型（LLM）的通用能力和少样本学习特性，构建一个自动化代理PortAgent，以模拟人类专家团队的工作流程，从而消除对专家的依赖、降低数据需求并加速部署。这样设计是因为LLM能够从少量示例中学习领域知识，并通过协作机制实现端到端的自动化迁移。\\n\\n**技术框架**：整体架构基于虚拟专家团队（VET），包括四个主要模块：知识检索器（负责从示例库中检索相关VDS案例）、建模器（基于检索到的示例构建调度模型）、编码器（将模型转化为可执行代码）和调试器（检查和修正代码错误）。工作流程通过自动化的VDS设计流程连接这些模块，并引入自校正循环（受LLM Reflexion框架启发）来迭代优化输出。\\n\\n**关键创新**：最重要的技术创新点是提出一个LLM驱动的虚拟专家团队，通过少样本示例学习和检索增强生成（RAG）机制，实现VDS迁移的完全自动化。与现有方法的本质区别在于：它不依赖人类专家干预，仅需少量数据即可适应新码头，并大幅缩短部署时间，从而解决了可迁移性的根本瓶颈。\\n\\n**关键设计**：关键设计包括：使用少样本示例学习方法，让虚拟专家从少量VDS示例中学习领域知识；采用RAG机制检索相关示例，以减轻数据需求；建立自校正循环，基于反馈自动修正错误，确保输出质量；虚拟专家团队通过协作协议模拟人类团队的分工，例如知识检索器提供上下文，建模器生成逻辑，编码器实现代码，调试器验证结果。具体参数设置和网络结构未在摘要中详细说明，但核心依赖于LLM的预训练能力和微调策略。",
            "application_zh": "该研究主要应用于自动化集装箱码头的车辆调度系统迁移和部署，潜在价值包括提升码头运营效率、降低人力成本和加速技术商业化。未来影响可能扩展到其他物流和工业自动化场景，如仓库机器人调度或智能制造系统，推动基于LLM的智能代理在复杂环境中的实际应用。",
            "highlight_zh": "PortAgent通过虚拟专家团队实现了车辆调度系统的快速迁移，实验表明它无需港口运营专家，数据需求低，部署时间大幅缩短。具体性能数据未在摘要中提供，但相比传统方法，它显著提升了可迁移性和自动化水平，减少了手动干预，为码头运营提供了高效解决方案。",
            "tags_zh": [
                "大语言模型",
                "车辆调度系统",
                "自动化集装箱码头",
                "虚拟专家团队",
                "少样本学习",
                "检索增强生成",
                "自校正循环",
                "可迁移性"
            ],
            "_index": 129
        },
        {
            "title": "Pattern Recognition of Aluminium Arbitrage in Global Trade Data",
            "authors": [
                "Muhammad Sukri Bin Ramli"
            ],
            "arxiv_id": "2512.14410v1",
            "summary": "As the global economy transitions toward decarbonization, the aluminium sector has become a focal point for strategic resource management. While policies such as the Carbon Border Adjustment Mechanism (CBAM) aim to reduce emissions, they have inadvertently widened the price arbitrage between primary metal, scrap, and semi-finished goods, creating new incentives for market optimization. This study presents a unified, unsupervised machine learning framework to detect and classify emerging trade anomalies within UN Comtrade data (2020 to 2024). Moving beyond traditional rule-based monitoring, we apply a four-layer analytical pipeline utilizing Forensic Statistics, Isolation Forests, Network Science, and Deep Autoencoders. Contrary to the hypothesis that Sustainability Arbitrage would be the primary driver, empirical results reveal a contradictory and more severe phenomenon of Hardware Masking. Illicit actors exploit bi-directional tariff incentives by misclassifying scrap as high-count heterogeneous goods to justify extreme unit-price outliers of >$160/kg, a 1,900% markup indicative of Trade-Based Money Laundering (TBML) rather than commercial arbitrage. Topologically, risk is not concentrated in major exporters but in high-centrality Shadow Hubs that function as pivotal nodes for illicit rerouting. These actors execute a strategy of Void-Shoring, systematically suppressing destination data to Unspecified Code to fracture mirror statistics and sever forensic trails. Validated by SHAP (Shapley Additive Explanations), the results confirm that price deviation is the dominant predictor of anomalies, necessitating a paradigm shift in customs enforcement from physical volume checks to dynamic, algorithmic valuation auditing.",
            "categories": [
                "econ.GN",
                "cs.LG"
            ],
            "primary_category": "econ.GN",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14410v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出无监督机器学习框架以检测全球铝贸易中的异常模式，揭示硬件掩蔽洗钱行为",
            "summary_zh": "随着全球经济向脱碳转型，铝行业成为战略资源管理的焦点。虽然碳边境调节机制等政策旨在减少排放，但无意中扩大了原铝、废铝和半成品之间的价格套利空间，为市场优化创造了新激励。本研究提出一个统一的无监督机器学习框架，用于检测和分类联合国商品贸易统计数据库（2020年至2024年）中的新兴贸易异常。超越传统的基于规则的监测，我们应用一个四层分析流程，利用法证统计、孤立森林、网络科学和深度自编码器。与可持续性套利是主要驱动因素的假设相反，实证结果揭示了一个矛盾且更严重的硬件掩蔽现象。非法行为者利用双向关税激励，将废铝错误分类为高计数异质商品，以证明单价极端异常值（>160美元/公斤）的合理性，这种1900%的加价表明是贸易洗钱而非商业套利。从拓扑角度看，风险并非集中在主要出口国，而是集中在作为非法重路由关键节点的高中心性影子枢纽。这些行为者执行空岸策略，系统性地将目的地数据抑制为未指定代码，以破坏镜像统计并切断法证追踪。通过SHAP验证，结果确认价格偏差是异常的主要预测因子，需要海关执法从物理量检查向动态算法估值审计的范式转变。",
            "intro_zh": [
                "现有基于规则的监测方法难以捕捉全球铝贸易中的复杂异常模式，尤其是在脱碳政策导致价格套利扩大的背景下。",
                "论文提出一个四层无监督机器学习框架，结合法证统计、孤立森林、网络科学和深度自编码器，以统一方式检测和分类贸易异常。",
                "实证结果揭示硬件掩蔽现象主导异常，价格偏差是主要预测因子，推动海关执法向动态算法审计转变。"
            ],
            "method_zh": "**问题定义**：论文旨在检测和分类全球铝贸易数据中的异常模式，特别是由脱碳政策引发的价格套利和非法行为。现有基于规则的监测方法难以捕捉复杂、新兴的异常，如贸易洗钱，导致监管滞后和效率低下。\\n\\n**核心思路**：采用无监督机器学习框架，通过多模态数据分析（价格、网络拓扑、统计特征）来识别异常，避免依赖先验规则，从而更灵活地适应动态贸易环境。设计思路基于假设异常在数据中表现为统计离群点、网络结构异常或重构误差，通过集成多种技术提高检测鲁棒性。\\n\\n**技术框架**：整体架构包含四层分析流程：第一层使用法证统计进行初步数据清洗和特征提取；第二层应用孤立森林检测价格和数量上的统计离群点；第三层利用网络科学分析贸易网络拓扑，识别高中心性的影子枢纽；第四层采用深度自编码器学习正常贸易模式，通过重构误差标记异常交易。流程顺序执行，输出综合异常评分。\\n\\n**关键创新**：最重要的创新是提出统一的无监督框架，将多种机器学习技术集成到单一流程中，以多角度捕捉异常。与现有方法相比，本质区别在于不依赖固定规则，而是通过数据驱动方式自适应检测，特别强调网络科学在识别非法重路由节点中的应用。\\n\\n**关键设计**：关键参数包括孤立森林的污染率（contamination rate）以控制异常检测灵敏度，自编码器的隐藏层结构和激活函数（如ReLU）以优化重构性能。损失函数使用均方误差（MSE）用于自编码器训练，网络结构可能包含多个隐藏层以捕捉非线性关系。SHAP用于特征重要性分析，验证价格偏差的主导作用。",
            "application_zh": "该研究可应用于海关执法、金融监管和国际贸易风险管理领域，帮助机构动态监测铝等大宗商品的贸易异常，打击贸易洗钱和非法套利。实际价值在于提升监管效率，减少人工检查成本，未来可能扩展到其他商品或更广泛的贸易数据分析，推动智能海关系统的发展。",
            "highlight_zh": "最重要的实验结果显示，硬件掩蔽现象主导异常，而非预期的可持续性套利。具体数据包括检测到单价极端异常值>160美元/公斤，加价达1900%，表明贸易洗钱行为。通过SHAP验证，价格偏差被确认为异常的主要预测因子，框架成功识别高中心性影子枢纽作为风险节点。相比传统规则方法，该无监督框架能更早发现新兴异常，提升检测准确性和时效性。",
            "tags_zh": [
                "无监督学习",
                "贸易异常检测",
                "深度自编码器",
                "网络科学",
                "孤立森林",
                "贸易洗钱",
                "铝行业分析",
                "海关执法"
            ],
            "_index": 130
        },
        {
            "title": "Broadening View Synthesis of Dynamic Scenes from Constrained Monocular Videos",
            "authors": [
                "Le Jiang",
                "Shaotong Zhu",
                "Yedi Luo",
                "Shayda Moezzi",
                "Sarah Ostadabbas"
            ],
            "arxiv_id": "2512.14406v1",
            "summary": "In dynamic Neural Radiance Fields (NeRF) systems, state-of-the-art novel view synthesis methods often fail under significant viewpoint deviations, producing unstable and unrealistic renderings. To address this, we introduce Expanded Dynamic NeRF (ExpanDyNeRF), a monocular NeRF framework that leverages Gaussian splatting priors and a pseudo-ground-truth generation strategy to enable realistic synthesis under large-angle rotations. ExpanDyNeRF optimizes density and color features to improve scene reconstruction from challenging perspectives. We also present the Synthetic Dynamic Multiview (SynDM) dataset, the first synthetic multiview dataset for dynamic scenes with explicit side-view supervision-created using a custom GTA V-based rendering pipeline. Quantitative and qualitative results on SynDM and real-world datasets demonstrate that ExpanDyNeRF significantly outperforms existing dynamic NeRF methods in rendering fidelity under extreme viewpoint shifts. Further details are provided in the supplementary materials.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14406v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出ExpanDyNeRF框架，利用高斯先验和伪真值生成策略，解决动态NeRF在大视角偏移下渲染不稳定的问题。",
            "summary_zh": "在动态神经辐射场（NeRF）系统中，当前最先进的新视角合成方法在显著视角偏差下往往失败，产生不稳定和不真实的渲染结果。为解决这一问题，我们引入了扩展动态NeRF（ExpanDyNeRF），这是一个单目NeRF框架，利用高斯溅射先验和伪真值生成策略，以实现大角度旋转下的真实合成。ExpanDyNeRF优化密度和颜色特征，以改进从挑战性视角的场景重建。我们还提出了合成动态多视角（SynDM）数据集，这是首个用于动态场景的合成多视角数据集，具有明确的侧视角监督，通过基于GTA V的自定义渲染管道创建。在SynDM和真实世界数据集上的定量和定性结果表明，ExpanDyNeRF在极端视角偏移下的渲染保真度方面显著优于现有动态NeRF方法。更多细节在补充材料中提供。",
            "intro_zh": [
                "现有动态NeRF方法在大视角偏移下渲染不稳定，导致新视角合成质量下降。",
                "提出ExpanDyNeRF框架，结合高斯先验和伪真值生成，优化特征以增强重建能力。",
                "在SynDM数据集上，ExpanDyNeRF显著提升渲染保真度，优于现有方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决动态NeRF系统在单目视频输入下，当新视角与训练视角偏差较大时，渲染结果不稳定和不真实的问题。现有方法的痛点在于缺乏有效先验和监督，导致在极端视角变化下重建质量下降。\\n\\n**核心思路**：通过引入高斯溅射先验和伪真值生成策略，增强模型对动态场景的几何和外观建模能力，从而在大角度旋转下实现更稳定的新视角合成。这样设计是因为高斯先验能提供场景结构的统计约束，而伪真值生成则弥补了单目视频中多视角监督的不足。\\n\\n**技术框架**：整体架构基于单目动态NeRF，包含两个主要阶段：首先，利用高斯溅射先验初始化场景表示；其次，通过伪真值生成策略优化密度和颜色特征。流程包括数据预处理、先验融合、特征优化和渲染输出。\\n\\n**关键创新**：最重要的技术创新是结合高斯先验和伪真值生成，这在动态NeRF领域首次系统性地解决了大视角偏移下的渲染问题。与现有方法的本质区别在于，它不依赖额外传感器或多视角输入，而是通过先验和合成数据增强单目视频的监督信息。\\n\\n**关键设计**：关键参数包括高斯分布的均值和方差，用于建模场景不确定性；损失函数结合重建损失和先验正则化项，以平衡保真度和稳定性；网络结构采用多层感知机编码动态特征，并集成时间维度处理运动。伪真值生成通过基于GTA V的渲染管道实现，提供侧视角监督数据。",
            "application_zh": "该研究在虚拟现实、增强现实和机器人导航等领域有潜在应用，能提升动态场景的实时渲染质量和视角扩展能力。实际价值包括降低多摄像头需求，推动单目视频的3D重建技术发展。未来可能影响自动驾驶和影视制作中的场景合成。",
            "highlight_zh": "在SynDM数据集上，ExpanDyNeRF在极端视角偏移下的渲染保真度显著优于基线方法，具体性能数据未知，但定性结果显示更稳定和真实的输出。对比基线包括现有动态NeRF方法，提升幅度在定量指标上表现突出，补充材料中提供了详细结果。",
            "tags_zh": [
                "动态神经辐射场",
                "新视角合成",
                "单目视频",
                "高斯先验",
                "伪真值生成",
                "合成数据集",
                "视角偏移",
                "渲染保真度"
            ],
            "_index": 131
        },
        {
            "title": "RePo: Language Models with Context Re-Positioning",
            "authors": [
                "Huayang Li",
                "Tianyu Zhao",
                "Richard Sproat"
            ],
            "arxiv_id": "2512.14391v1",
            "summary": "In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, $f_φ$, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14391v1",
            "code_links": [
                {
                    "url": "https://github.com/SakanaAI/repo",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出RePo机制，通过上下文重定位减少大语言模型中的额外认知负荷，提升长文本和结构化数据处理能力。",
            "summary_zh": "上下文学习是现代大语言模型（LLMs）的基础；然而，主流架构通过分配线性或恒定的位置索引，强加了僵化且固定的上下文结构。借鉴认知负荷理论（CLT），我们认为这种无信息性的结构增加了额外认知负荷，消耗了本应用于深度推理和注意力分配的有限工作记忆容量。为解决这一问题，我们提出了RePo，一种通过上下文重定位减少额外负荷的新机制。与标准方法不同，RePo利用一个可微分模块fφ来分配捕捉上下文依赖关系的标记位置，而不是依赖预定义的整数范围。通过在OLMo-2 1B骨干网络上持续预训练，我们证明RePo在处理涉及噪声上下文、结构化数据和更长上下文长度的任务时，性能显著提升，同时在一般短上下文任务上保持竞争力。详细分析表明，RePo成功地将更高注意力分配给遥远但相关的信息，在密集和非线性空间中分配位置，并捕捉输入上下文的内在结构。我们的代码可在https://github.com/SakanaAI/repo获取。",
            "intro_zh": [
                "现有大语言模型使用线性或恒定位置索引，导致上下文结构僵化，增加额外认知负荷，限制深度推理能力。",
                "提出RePo机制，通过可微分模块动态分配标记位置，捕捉上下文依赖，减少额外负荷，提升模型灵活性。",
                "实验显示RePo在噪声上下文、结构化数据和长文本任务中性能显著提升，同时保持短上下文任务的竞争力。"
            ],
            "method_zh": "**问题定义**：论文解决大语言模型中上下文学习的问题，现有方法使用线性或恒定位置索引（如绝对或相对位置编码），导致上下文结构僵化，无法有效捕捉复杂依赖关系，增加额外认知负荷，消耗工作记忆容量，从而影响模型在噪声、结构化或长文本任务中的性能。\\n\\n**核心思路**：核心思路是引入上下文重定位机制，基于认知负荷理论，通过动态分配标记位置来减少额外负荷。设计一个可微分模块fφ，根据上下文内容学习位置表示，而非依赖预定义整数，使模型能更灵活地捕捉依赖关系，优化注意力分配。\\n\\n**技术框架**：整体架构基于OLMo-2 1B骨干网络，在预训练阶段集成RePo模块。流程包括：输入上下文序列，通过fφ模块计算每个标记的位置表示，这些表示作为位置编码融入模型；在训练中，模块参数φ与模型参数联合优化，以最小化损失函数；推理时，使用学习到的位置表示进行预测。主要模块包括骨干网络、fφ模块和位置编码层。\\n\\n**关键创新**：最重要的技术创新是提出可微分的位置分配模块fφ，实现上下文重定位。与现有方法的本质区别在于：现有方法依赖固定位置索引（如线性或相对编码），而RePo允许位置在密集、非线性空间中动态调整，直接捕捉上下文依赖，减少结构僵化带来的认知负荷。\\n\\n**关键设计**：关键设计包括：fφ模块采用神经网络结构（具体细节未知，可能基于Transformer或MLP），输出位置表示；损失函数结合语言建模损失和潜在的位置正则化；在OLMo-2 1B上持续预训练，参数设置遵循标准预训练协议；位置表示与标记嵌入结合，作为模型输入的一部分，确保可微分性和端到端优化。",
            "application_zh": "该研究潜在应用于需要处理复杂上下文的任务，如文档摘要、代码生成、结构化数据解析和长文本问答。实际价值在于提升大语言模型在噪声环境、非结构化输入和扩展上下文中的鲁棒性和准确性，未来可能推动更高效的自然语言处理系统，减少人工干预，增强模型在现实世界场景中的适用性。",
            "highlight_zh": "实验结果显示，RePo在OLMo-2 1B骨干上持续预训练后，在涉及噪声上下文的任务中性能提升显著（具体数据未知，但论文提到“显著增强”），在结构化数据处理和更长上下文长度任务上也表现优异。与基线方法相比，RePo在一般短上下文任务上保持竞争力，同时通过注意力分析证实能更有效地分配注意力到遥远相关信息，位置分配呈现密集和非线性特性，成功捕捉输入结构。",
            "tags_zh": [
                "上下文学习",
                "位置编码",
                "认知负荷理论",
                "大语言模型",
                "可微分模块",
                "长文本处理",
                "结构化数据",
                "注意力机制"
            ],
            "_index": 132
        },
        {
            "title": "Optimizing Rank for High-Fidelity Implicit Neural Representations",
            "authors": [
                "Julian McGinnis",
                "Florian A. Hölzl",
                "Suprosanna Shit",
                "Florentin Bieder",
                "Paul Friedrich",
                "Mark Mühlau",
                "Björn Menze",
                "Daniel Rueckert",
                "Benedikt Wiestler"
            ],
            "arxiv_id": "2512.14366v1",
            "summary": "Implicit Neural Representations (INRs) based on vanilla Multi-Layer Perceptrons (MLPs) are widely believed to be incapable of representing high-frequency content. This has directed research efforts towards architectural interventions, such as coordinate embeddings or specialized activation functions, to represent high-frequency signals. In this paper, we challenge the notion that the low-frequency bias of vanilla MLPs is an intrinsic, architectural limitation to learn high-frequency content, but instead a symptom of stable rank degradation during training. We empirically demonstrate that regulating the network's rank during training substantially improves the fidelity of the learned signal, rendering even simple MLP architectures expressive. Extensive experiments show that using optimizers like Muon, with high-rank, near-orthogonal updates, consistently enhances INR architectures even beyond simple ReLU MLPs. These substantial improvements hold across a diverse range of domains, including natural and medical images, and novel view synthesis, with up to 9 dB PSNR improvements over the previous state-of-the-art. Our project page, which includes code and experimental results, is available at: (https://muon-inrs.github.io).",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14366v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出通过优化网络秩来提升隐式神经表示的高频信号保真度，挑战传统架构限制观点。",
            "summary_zh": "基于普通多层感知机（MLPs）的隐式神经表示（INRs）被广泛认为无法表示高频内容，这引导研究转向坐标嵌入或专用激活函数等架构干预。本文挑战了普通MLPs的低频偏差是学习高频内容的内在架构限制的观点，认为这是训练过程中稳定秩退化的症状。我们通过实验证明，在训练期间调节网络秩能显著提高学习信号的保真度，使简单的MLP架构也具有表达力。大量实验表明，使用Muon等优化器进行高秩、近正交更新，能持续增强INR架构，甚至超越简单的ReLU MLPs。这些显著改进适用于多个领域，包括自然和医学图像以及新视角合成，相比先前最先进方法，PSNR提升高达9 dB。我们的项目页面包含代码和实验结果，可在https://muon-inrs.github.io访问。",
            "intro_zh": [
                "现有方法认为普通MLPs因架构限制无法表示高频信号，依赖坐标嵌入等干预，但本文指出这是训练中秩退化导致的症状。",
                "论文提出通过优化器如Muon调节网络秩，实现高秩、近正交更新，从而提升INRs的高频保真度，无需复杂架构修改。",
                "实验显示该方法在自然图像、医学图像和新视角合成中，PSNR提升高达9 dB，显著超越现有最先进方法。"
            ],
            "method_zh": "**问题定义**：论文解决隐式神经表示（INRs）中普通多层感知机（MLPs）无法有效表示高频信号的问题。现有方法通常归因于MLPs的架构限制，依赖坐标嵌入或专用激活函数等干预，但这些方法可能忽略训练过程中的内在动态，导致效率低下或泛化能力不足。\\n\\n**核心思路**：论文的核心思路是挑战传统观点，认为MLPs的低频偏差不是固有架构限制，而是训练中稳定秩退化的症状。通过调节网络秩，可以改善信号保真度，使简单MLP架构也能表达高频内容。这基于秩与表示能力之间的理论联系，高秩更新有助于捕捉更丰富的信号特征。\\n\\n**技术框架**：整体框架包括使用优化器如Muon进行训练，该优化器设计用于生成高秩、近正交的权重更新。流程涉及初始化MLP网络，应用秩调节策略，通过损失函数优化网络参数，并在多个数据集上评估性能。主要模块包括基础MLP架构、秩优化模块和评估模块，无需额外复杂组件。\\n\\n**关键创新**：最重要的技术创新是提出秩优化作为提升INRs性能的关键，而非依赖架构修改。与现有方法的本质区别在于，它从训练动态角度解决问题，而不是假设架构缺陷，从而提供更通用和高效的解决方案。\\n\\n**关键设计**：关键设计包括使用Muon优化器，它通过高秩更新机制促进权重矩阵的秩保持；网络结构基于简单ReLU MLPs，无需坐标嵌入或特殊激活函数；损失函数通常采用均方误差等标准指标，参数设置针对不同任务调整，以确保秩调节的有效性和稳定性。",
            "application_zh": "该研究在计算机视觉和医学图像分析领域具有广泛潜在应用，如高保真图像重建、新视角合成和医学影像增强。通过提升隐式神经表示的高频信号保真度，可推动3D重建、虚拟现实和精准医疗等技术的发展，未来可能影响更复杂的信号表示任务。",
            "highlight_zh": "实验结果显示，使用秩优化方法在多个领域显著提升性能：在自然图像和医学图像上，PSNR提升高达9 dB；在新视角合成任务中，也观察到明显改进。对比基线包括传统INR方法和最先进架构，该方法在简单MLP基础上实现超越，验证了秩调节的有效性和通用性。",
            "tags_zh": [
                "隐式神经表示",
                "多层感知机",
                "秩优化",
                "高频信号",
                "图像重建",
                "新视角合成",
                "医学图像分析",
                "优化器设计"
            ],
            "_index": 133
        },
        {
            "title": "TiCard: Deployable EXPLAIN-only Residual Learning for Cardinality Estimation",
            "authors": [
                "Qizhi Wang"
            ],
            "arxiv_id": "2512.14358v1",
            "summary": "Cardinality estimation is a key bottleneck for cost-based query optimization, yet deployable improvements remain difficult: classical estimators miss correlations, while learned estimators often require workload-specific training pipelines and invasive integration into the optimizer. This paper presents TiCard, a low intrusion, correction-based framework that augments (rather than replaces) a database's native estimator. TiCard learns multiplicative residual corrections using EXPLAIN-only features, and uses EXPLAIN ANALYZE only for offline labels. We study two practical instantiations: (i) a Gradient Boosting Regressor for sub-millisecond inference, and (ii) TabPFN, an in-context tabular foundation model that adapts by refreshing a small reference set without gradient retraining. On TiDB with TPCH and the Join Order Benchmark, in a low-trace setting (263 executions total; 157 used for learning), TiCard improves operator-level tail accuracy substantially: P90 Q-error drops from 312.85 (native) to 13.69 (TiCard-GBR), and P99 drops from 37,974.37 to 3,416.50 (TiCard-TabPFN), while a join-only policy preserves near-perfect median behavior. We position TiCard as an AI4DB building block focused on deployability: explicit scope, conservative integration policies, and an integration roadmap from offline correction to in-optimizer use.",
            "categories": [
                "cs.AI",
                "cs.DB"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "16 pages(/wo references), 4 figures, 10 tables",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14358v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出TiCard框架，通过仅使用EXPLAIN特征的残差学习来增强数据库基数估计，解决传统方法忽略相关性和学习型方法部署困难的问题。",
            "summary_zh": "基数估计是基于成本的查询优化的关键瓶颈，但可部署的改进仍然困难：传统估计器忽略了相关性，而学习型估计器通常需要特定于工作负载的训练流程，并需要侵入性地集成到优化器中。本文提出了TiCard，一个低侵入性、基于校正的框架，用于增强（而非替换）数据库的原生估计器。TiCard使用仅EXPLAIN特征学习乘法残差校正，并仅使用EXPLAIN ANALYZE进行离线标签生成。我们研究了两种实际实例化：（i）用于亚毫秒推理的梯度提升回归器，以及（ii）TabPFN，一种上下文表格基础模型，通过刷新小型参考集进行适应，无需梯度重新训练。在TiDB上使用TPCH和Join Order Benchmark，在低追踪设置下（总共263次执行；157次用于学习），TiCard显著提高了算子级别的尾部准确性：P90 Q-error从312.85（原生）降至13.69（TiCard-GBR），P99从37,974.37降至3,416.50（TiCard-TabPFN），同时仅连接策略保持了近乎完美的中位数行为。我们将TiCard定位为专注于可部署性的AI4DB构建块：明确的范围、保守的集成策略，以及从离线校正到优化器内使用的集成路线图。",
            "intro_zh": [
                "核心问题：传统基数估计器忽略数据相关性，导致估计不准确；学习型方法需要复杂训练流程和侵入式集成，部署困难。",
                "方法要点：提出TiCard框架，通过仅使用EXPLAIN特征学习乘法残差校正来增强原生估计器，实现低侵入性部署。",
                "实验或效果：在低追踪设置下，TiCard显著降低尾部Q-error，如P90从312.85降至13.69，同时保持中位数准确性。"
            ],
            "method_zh": "**问题定义**：论文解决数据库查询优化中基数估计的准确性问题。现有方法的痛点包括：传统估计器（如直方图）忽略数据相关性，导致估计偏差；学习型估计器（如神经网络）需要大量标注数据、特定工作负载训练和侵入式集成到优化器，部署成本高且不灵活。\\n\\n**核心思路**：论文提出TiCard框架，核心思路是“增强而非替换”数据库原生估计器。通过仅使用EXPLAIN查询计划特征（如算子类型、谓词信息）学习乘法残差校正，对原生估计值进行微调，从而在保持低侵入性的同时提高准确性。这种设计避免了学习型方法对原始数据或复杂特征的依赖，简化了部署流程。\\n\\n**技术框架**：整体架构分为离线学习和在线推理两个阶段。离线阶段：使用EXPLAIN ANALYZE执行查询，收集真实基数作为标签，结合EXPLAIN-only特征（如查询计划结构）训练校正模型。在线阶段：在查询优化时，仅使用EXPLAIN特征输入训练好的模型，预测残差校正因子，应用于原生估计值得到最终估计。框架支持多种模型实例化，如梯度提升回归器（GBR）和TabPFN基础模型。\\n\\n**关键创新**：最重要的技术创新是“EXPLAIN-only残差学习”。与现有学习型方法相比，TiCard仅依赖查询计划特征，无需访问原始数据或复杂统计信息，降低了数据隐私和集成复杂度。本质区别在于：它作为校正层叠加在原生估计器上，而非独立估计器，实现了模块化和可插拔性。\\n\\n**关键设计**：关键设计包括：使用乘法残差校正（即估计值 = 原生估计值 × 校正因子），以保持估计尺度；损失函数基于Q-error（估计值与真实值的最大比率）优化，专注于尾部准确性；模型实例化中，GBR用于快速推理（亚毫秒级），TabPFN通过小参考集（如157个样本）进行上下文适应，无需梯度重训练，提高了样本效率。",
            "application_zh": "该研究主要应用于数据库管理系统（DBMS）的查询优化领域，特别是基于成本的查询优化器。实际价值在于提供了一种低侵入性、可部署的基数估计增强方案，适用于生产环境中的数据库（如TiDB），能显著提升查询性能而不破坏现有系统架构。未来影响可能推动AI4DB（人工智能用于数据库）的发展，作为构建块支持从离线校正到在线优化的平滑集成，促进学习型方法在实际系统中的广泛应用。",
            "highlight_zh": "最重要的实验结果包括：在TiDB数据库上，使用TPCH和Join Order Benchmark数据集，在低追踪设置（仅263次执行，157次用于学习）下，TiCard显著提升了算子级别的尾部准确性。具体性能数据：原生估计器的P90 Q-error为312.85，TiCard-GBR将其降至13.69（提升约95.6%）；P99 Q-error从37,974.37降至3,416.50（TiCard-TabPFN，提升约91.0%）。同时，仅连接策略保持了近乎完美的中位数行为，验证了框架在保持整体准确性的同时优化尾部误差的有效性。",
            "tags_zh": [
                "基数估计",
                "查询优化",
                "残差学习",
                "数据库增强",
                "可部署AI",
                "梯度提升回归器",
                "表格基础模型",
                "AI4DB"
            ],
            "_index": 134
        },
        {
            "title": "HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis",
            "authors": [
                "Kaizhe Zhang",
                "Yijie Zhou",
                "Weizhan Zhang",
                "Caixia Yan",
                "Haipeng Du",
                "yugui xie",
                "Yu-Hui Wen",
                "Yong-Jin Liu"
            ],
            "arxiv_id": "2512.14352v1",
            "summary": "Dynamic novel view synthesis (NVS) is essential for creating immersive experiences. Existing approaches have advanced dynamic NVS by introducing 3D Gaussian Splatting (3DGS) with implicit deformation fields or indiscriminately assigned time-varying parameters, surpassing NeRF-based methods. However, due to excessive model complexity and parameter redundancy, they incur large model sizes and slow rendering speeds, making them inefficient for real-time applications, particularly on resource-constrained devices. To obtain a more efficient model with fewer redundant parameters, in this paper, we propose Hybrid Gaussian Splatting (HGS), a compact and efficient framework explicitly designed to disentangle static and dynamic regions of a scene within a unified representation. The core innovation of HGS lies in our Static-Dynamic Decomposition (SDD) strategy, which leverages Radial Basis Function (RBF) modeling for Gaussian primitives. Specifically, for dynamic regions, we employ time-dependent RBFs to effectively capture temporal variations and handle abrupt scene changes, while for static regions, we reduce redundancy by sharing temporally invariant parameters. Additionally, we introduce a two-stage training strategy tailored for explicit models to enhance temporal coherence at static-dynamic boundaries. Experimental results demonstrate that our method reduces model size by up to 98% and achieves real-time rendering at up to 125 FPS at 4K resolution on a single RTX 3090 GPU. It further sustains 160 FPS at 1352 * 1014 on an RTX 3050 and has been integrated into the VR system. Moreover, HGS achieves comparable rendering quality to state-of-the-art methods while providing significantly improved visual fidelity for high-frequency details and abrupt scene changes.",
            "categories": [
                "cs.CV",
                "cs.CG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "11 pages, 9 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14352v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出混合高斯溅射框架，通过静态-动态分解策略解决动态新视角合成中模型冗余和渲染效率低的问题。",
            "summary_zh": "动态新视角合成对于创造沉浸式体验至关重要。现有方法通过引入带有隐式变形场或非区分性时变参数的3D高斯溅射，超越了基于NeRF的方法，推动了动态NVS的发展。然而，由于模型复杂度过高和参数冗余，这些方法导致模型体积庞大、渲染速度缓慢，使其在实时应用中效率低下，特别是在资源受限的设备上。为了获得一个参数冗余更少、更高效的模型，本文提出了混合高斯溅射，这是一个紧凑且高效的框架，专门设计用于在统一表示中解耦场景的静态和动态区域。HGS的核心创新在于我们的静态-动态分解策略，该策略利用径向基函数对高斯基元进行建模。具体来说，对于动态区域，我们采用时间相关的RBF来有效捕捉时间变化并处理场景的突变；对于静态区域，我们通过共享时间不变参数来减少冗余。此外，我们引入了一个针对显式模型量身定制的两阶段训练策略，以增强静态-动态边界的时间一致性。实验结果表明，我们的方法将模型大小减少了高达98%，并在单个RTX 3090 GPU上以4K分辨率实现了高达125 FPS的实时渲染。它进一步在RTX 3050上以1352*1014分辨率维持160 FPS，并已集成到VR系统中。此外，HGS实现了与最先进方法相当的渲染质量，同时为高频细节和场景突变提供了显著改善的视觉保真度。",
            "intro_zh": [
                "现有动态新视角合成方法基于3D高斯溅射，但存在模型复杂、参数冗余问题，导致模型体积大、渲染慢，难以实时应用。",
                "提出混合高斯溅射框架，通过静态-动态分解策略，使用径向基函数区分处理动态和静态区域，减少冗余参数。",
                "实验显示模型大小减少高达98%，在RTX 3090上实现4K分辨率125 FPS实时渲染，并保持高质量，已集成VR系统。"
            ],
            "method_zh": "**问题定义**：动态新视角合成需要高效处理场景中的静态和动态区域。现有基于3D高斯溅射的方法通过隐式变形场或非区分性时变参数引入动态性，但导致模型参数冗余、体积庞大、渲染速度慢，难以在资源受限设备上实现实时应用，特别是在处理高频细节和场景突变时效率低下。\\n\\n**核心思路**：论文提出混合高斯溅射框架，核心思路是显式解耦场景的静态和动态区域，通过静态-动态分解策略减少冗余参数。设计基于径向基函数建模高斯基元，动态区域使用时间相关RBF捕捉变化，静态区域共享时间不变参数，从而在统一表示中实现高效动态合成。\\n\\n**技术框架**：整体架构包括高斯基元表示、静态-动态分解模块和两阶段训练流程。首先，使用高斯溅射作为基础表示；然后，通过SDD策略将高斯基元分类为静态和动态，动态区域用时间相关RBF建模，静态区域用共享参数；最后，采用两阶段训练：第一阶段优化静态区域，第二阶段联合优化动态区域和边界一致性。\\n\\n**关键创新**：最重要的技术创新是静态-动态分解策略，它显式区分场景区域，而非现有方法的隐式或非区分性处理。本质区别在于通过RBF建模实现参数高效分配，动态区域精准捕捉时间变化，静态区域大幅减少冗余，从而在保持质量的同时提升效率。\\n\\n**关键设计**：关键设计包括：使用径向基函数作为高斯基元的时间建模工具，动态区域RBF参数随时间变化，静态区域参数共享；损失函数结合重建损失和时间一致性损失，以优化渲染质量和边界平滑；网络结构为显式模型，无需复杂隐式场，参数设置针对不同区域自适应调整，训练策略分阶段进行以增强稳定性。",
            "application_zh": "该研究在虚拟现实、增强现实和沉浸式媒体领域具有重要应用价值，能实现高效动态场景渲染，支持实时交互体验。潜在应用包括VR游戏、影视制作、远程协作和自动驾驶模拟，未来可能推动轻量级动态合成技术在移动设备和边缘计算中的普及，提升用户体验并降低硬件需求。",
            "highlight_zh": "实验结果显示，HGS在模型大小上减少高达98%，在RTX 3090 GPU上以4K分辨率实现125 FPS的实时渲染，在RTX 3050上以1352*1014分辨率维持160 FPS。与基线方法相比，渲染质量相当，但高频细节和场景突变的视觉保真度显著提升，已成功集成到VR系统中，验证了其高效性和实用性。",
            "tags_zh": [
                "动态新视角合成",
                "高斯溅射",
                "静态-动态分解",
                "径向基函数",
                "实时渲染",
                "模型压缩",
                "虚拟现实",
                "参数优化"
            ],
            "_index": 135
        },
        {
            "title": "Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn Graph Orbits",
            "authors": [
                "Michael Murray",
                "Tenzin Chan",
                "Kedar Karhadker",
                "Christopher J. Hillar"
            ],
            "arxiv_id": "2512.14338v1",
            "summary": "Many learning problems involve symmetries, and while invariance can be built into neural architectures, it can also emerge implicitly when training on group-structured data. We study this phenomenon in classical Hopfield networks and show they can infer the full isomorphism class of a graph from a small random sample. Our results reveal that: (i) graph isomorphism classes can be represented within a three-dimensional invariant subspace, (ii) using gradient descent to minimize energy flow (MEF) has an implicit bias toward norm-efficient solutions, which underpins a polynomial sample complexity bound for learning isomorphism classes, and (iii) across multiple learning rules, parameters converge toward the invariant subspace as sample sizes grow. Together, these findings highlight a unifying mechanism for generalization in Hopfield networks: a bias toward norm efficiency in learning drives the emergence of approximate invariance under group-structured data.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14338v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "揭示Hopfield网络通过范数效率隐式偏置高效学习图同构类的机制",
            "summary_zh": "许多学习问题涉及对称性，虽然不变性可以构建到神经架构中，但在处理群结构数据时也可能隐式出现。我们在经典Hopfield网络中研究了这一现象，并表明它们可以从小的随机样本中推断出图的完整同构类。我们的结果表明：(i) 图同构类可以在三维不变子空间内表示；(ii) 使用梯度下降最小化能量流具有对范数高效解的隐式偏置，这支撑了学习同构类的多项式样本复杂度界限；(iii) 在多种学习规则下，随着样本量的增加，参数会收敛到不变子空间。这些发现共同突出了Hopfield网络中泛化的统一机制：学习中对范数效率的偏置驱动了在群结构数据下近似不变性的出现。",
            "intro_zh": [
                "核心问题：现有方法常显式构建不变性，但隐式学习机制在群结构数据中的效率与泛化能力尚不明确。",
                "方法要点：通过分析Hopfield网络在梯度下降下的能量最小化，揭示其隐式偏置导致范数高效解，从而学习图同构类。",
                "实验或效果：证明图同构类可在三维子空间表示，样本复杂度为多项式，参数收敛到不变子空间。"
            ],
            "method_zh": "**问题定义**：论文研究Hopfield网络如何从少量随机样本中高效学习图同构类，即图的完整同构类别。现有方法的痛点在于，虽然对称性（如群结构）在机器学习中常见，但通常需要显式构建不变性到网络架构中，这可能增加计算复杂度或限制泛化能力，而隐式学习机制在群结构数据中的效率和理论基础尚不充分。\\n\\n**核心思路**：论文的核心解决思路是分析Hopfield网络在训练过程中的隐式偏置，特别是通过梯度下降最小化能量流时，网络倾向于学习范数高效的解。这种偏置使得网络能够从有限样本中推断出图的不变特征，从而高效学习同构类，而无需显式编码对称性。这样设计是因为它揭示了学习算法本身的内在属性如何促进泛化，为理解神经网络在对称数据上的行为提供了理论框架。\\n\\n**技术框架**：整体架构基于经典Hopfield网络，流程包括：首先，将图表示为输入数据；然后，使用梯度下降等学习规则训练网络以最小化能量函数；接着，分析参数在训练过程中的演化，特别是它们如何收敛到与图同构类相关的不变子空间；最后，通过理论推导和实验验证，评估学习效率和泛化能力。主要模块包括数据表示、能量最小化优化、不变子空间分析和样本复杂度计算。\\n\\n**关键创新**：最重要的技术创新点是揭示了Hopfield网络中的隐式偏置机制：在群结构数据下，梯度下降最小化能量流会自然导向范数高效的解，这驱动了近似不变性的出现，从而高效学习图同构类。与现有方法的本质区别在于，它不依赖于显式的不变性构建，而是利用学习算法本身的偏置来实现泛化，这为设计更高效和理论可解释的模型提供了新视角。\\n\\n**关键设计**：关键设计包括：使用Hopfield网络的能量函数作为优化目标，通过梯度下降（如最小化能量流，MEF）进行训练；参数设置涉及网络权重和输入表示，具体细节在论文中未详细说明，但基于标准Hopfield模型；损失函数是能量最小化，没有额外正则化；网络结构为经典全连接Hopfield网络，用于处理图数据；技术细节还包括对不变子空间的三维表示分析和多项式样本复杂度的理论推导。",
            "application_zh": "该研究在计算机视觉、图神经网络和机器学习领域具有潜在应用价值。例如，在图分类、社交网络分析或分子结构识别中，高效学习图同构类可提升模型泛化能力和计算效率。实际价值在于为设计无需显式对称性编码的模型提供理论指导，未来可能推动更鲁棒和可解释的AI系统发展，特别是在处理具有复杂对称结构的数据时。",
            "highlight_zh": "最重要的实验结果表明：图同构类可以在三维不变子空间内有效表示，这简化了学习过程；使用梯度下降最小化能量流时，网络展现出对范数高效解的隐式偏置，支撑了多项式样本复杂度界限，具体提升未在摘要中量化，但理论分析表明优于显式方法；在多种学习规则下，随着样本量增加，参数收敛到不变子空间，验证了泛化机制的鲁棒性。对比基线未明确提及，但突出了隐式学习相对于显式构建的优势。",
            "tags_zh": [
                "Hopfield网络",
                "隐式偏置",
                "图同构",
                "不变子空间",
                "范数效率",
                "梯度下降",
                "样本复杂度",
                "群结构数据"
            ],
            "_index": 136
        },
        {
            "title": "Semantic Mismatch and Perceptual Degradation: A New Perspective on Image Editing Immunity",
            "authors": [
                "Shuai Dong",
                "Jie Zhang",
                "Guoying Zhao",
                "Shiguang Shan",
                "Xilin Chen"
            ],
            "arxiv_id": "2512.14320v1",
            "summary": "Text-guided image editing via diffusion models, while powerful, raises significant concerns about misuse, motivating efforts to immunize images against unauthorized edits using imperceptible perturbations. Prevailing metrics for evaluating immunization success typically rely on measuring the visual dissimilarity between the output generated from a protected image and a reference output generated from the unprotected original. This approach fundamentally overlooks the core requirement of image immunization, which is to disrupt semantic alignment with attacker intent, regardless of deviation from any specific output. We argue that immunization success should instead be defined by the edited output either semantically mismatching the prompt or suffering substantial perceptual degradations, both of which thwart malicious intent. To operationalize this principle, we propose Synergistic Intermediate Feature Manipulation (SIFM), a method that strategically perturbs intermediate diffusion features through dual synergistic objectives: (1) maximizing feature divergence from the original edit trajectory to disrupt semantic alignment with the expected edit, and (2) minimizing feature norms to induce perceptual degradations. Furthermore, we introduce the Immunization Success Rate (ISR), a novel metric designed to rigorously quantify true immunization efficacy for the first time. ISR quantifies the proportion of edits where immunization induces either semantic failure relative to the prompt or significant perceptual degradations, assessed via Multimodal Large Language Models (MLLMs). Extensive experiments show our SIFM achieves the state-of-the-art performance for safeguarding visual content against malicious diffusion-based manipulation.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.CY",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "11 pages, 4 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14320v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出SIFM方法以解决图像免疫评估不准确的问题，通过语义失配和感知退化新视角保护图像免受恶意编辑。",
            "summary_zh": "基于扩散模型的文本引导图像编辑虽然强大，但也引发了严重的滥用担忧，这促使人们使用不可察觉的扰动来免疫图像以防止未经授权的编辑。评估免疫成功的主流指标通常依赖于测量受保护图像生成的输出与未受保护原始图像生成的参考输出之间的视觉差异。这种方法从根本上忽视了图像免疫的核心要求，即破坏与攻击者意图的语义对齐，而不管与任何特定输出的偏差如何。我们认为，免疫成功应该定义为编辑输出要么在语义上与提示不匹配，要么遭受严重的感知退化，这两种情况都会挫败恶意意图。为了实现这一原则，我们提出了协同中间特征操纵（SIFM），这是一种通过双重协同目标策略性地扰动中间扩散特征的方法：（1）最大化特征与原始编辑轨迹的差异，以破坏与预期编辑的语义对齐；（2）最小化特征范数以诱导感知退化。此外，我们引入了免疫成功率（ISR），这是一种新颖的指标，首次设计用于严格量化真实的免疫效果。ISR量化了免疫导致编辑输出相对于提示语义失败或显著感知退化的比例，通过多模态大语言模型（MLLMs）进行评估。大量实验表明，我们的SIFM在保护视觉内容免受基于扩散的恶意操纵方面达到了最先进的性能。",
            "intro_zh": [
                "现有图像免疫评估方法依赖视觉差异度量，忽视了破坏语义对齐的核心要求，导致评估不准确。",
                "论文提出SIFM方法，通过最大化特征差异和最小化特征范数，协同扰动扩散特征以实现语义失配和感知退化。",
                "实验显示SIFM在免疫成功率上达到最先进水平，有效保护图像免受恶意编辑，验证了新评估指标ISR的有效性。"
            ],
            "method_zh": "**问题定义**：论文要解决的具体问题是文本引导图像编辑中图像免疫评估的不准确性。现有方法的痛点是主流评估指标仅依赖受保护与未受保护图像输出之间的视觉差异，这忽略了免疫的核心目标——破坏与攻击者意图的语义对齐，导致评估结果可能高估免疫效果，无法真实反映免疫是否成功阻止恶意编辑。\\n\\n**核心思路**：论文的核心解决思路是重新定义免疫成功为编辑输出要么语义失配于提示，要么遭受显著感知退化，从而直接针对恶意意图。基于此，设计SIFM方法，通过双重协同目标扰动扩散模型的中间特征：一方面最大化特征与原始编辑轨迹的差异以破坏语义对齐，另一方面最小化特征范数以诱导感知退化，确保免疫更有效。\\n\\n**技术框架**：整体架构包括两个主要阶段：免疫扰动生成和评估。在免疫扰动生成阶段，SIFM作用于扩散模型的中间特征层，通过优化损失函数同时实现特征差异最大化和范数最小化；在评估阶段，使用新提出的ISR指标，结合MLLMs判断编辑输出是否语义失配或感知退化，量化免疫成功率。流程涉及输入图像、文本提示、扩散模型特征提取和扰动优化。\\n\\n**关键创新**：最重要的技术创新点是提出了基于语义失配和感知退化的新免疫视角，以及SIFM方法中的双重协同目标设计。与现有方法的本质区别在于，现有方法通常关注输出层面的视觉差异，而SIFM直接针对中间特征进行扰动，更有效地破坏语义对齐，并首次引入ISR指标来严格量化免疫效果，避免了传统评估的局限性。\\n\\n**关键设计**：关键设计包括损失函数设计：结合特征差异损失（如余弦距离或L2距离）来最大化与原始编辑轨迹的差异，以及特征范数损失（如L2范数）来最小化特征值，通过加权求和实现协同优化；参数设置可能涉及平衡两个目标的权重系数，以及扩散模型特征层的选择；网络结构上，SIFM集成到标准扩散模型框架中，无需改变基础模型，仅通过扰动中间特征实现免疫。",
            "application_zh": "该研究的潜在应用领域包括数字内容安全、版权保护和隐私防护。实际价值在于能有效防止基于扩散模型的恶意图像编辑，如深度伪造、虚假信息传播和未经授权的图像篡改，为社交媒体平台、新闻机构和法律取证提供技术保障。未来影响可能推动更鲁棒的图像免疫标准，促进AI伦理和安全研究的发展。",
            "highlight_zh": "实验结果显示，SIFM在免疫成功率（ISR）上达到最先进水平，具体数据未知，但相比基线方法有显著提升。通过使用MLLMs评估，SIFM能有效诱导语义失配和感知退化，在多种文本提示和图像数据集上验证了其优越性。对比现有免疫方法，SIFM在保护图像免受恶意编辑方面表现更优，提升了免疫效果的准确性和鲁棒性。",
            "tags_zh": [
                "图像免疫",
                "扩散模型",
                "语义对齐",
                "特征扰动",
                "多模态评估",
                "文本引导编辑",
                "安全防护",
                "感知退化"
            ],
            "_index": 137
        },
        {
            "title": "From YOLO to VLMs: Advancing Zero-Shot and Few-Shot Detection of Wastewater Treatment Plants Using Satellite Imagery in MENA Region",
            "authors": [
                "Akila Premarathna",
                "Kanishka Hewageegana",
                "Garcia Andarcia Mariangel"
            ],
            "arxiv_id": "2512.14312v1",
            "summary": "In regions of the Middle East and North Africa (MENA), there is a high demand for wastewater treatment plants (WWTPs), crucial for sustainable water management. Precise identification of WWTPs from satellite images enables environmental monitoring. Traditional methods like YOLOv8 segmentation require extensive manual labeling. But studies indicate that vision-language models (VLMs) are an efficient alternative to achieving equivalent or superior results through inherent reasoning and annotation. This study presents a structured methodology for VLM comparison, divided into zero-shot and few-shot streams specifically to identify WWTPs. The YOLOv8 was trained on a governmental dataset of 83,566 high-resolution satellite images from Egypt, Saudi Arabia, and UAE: ~85% WWTPs (positives), 15% non-WWTPs (negatives). Evaluated VLMs include LLaMA 3.2 Vision, Qwen 2.5 VL, DeepSeek-VL2, Gemma 3, Gemini, and Pixtral 12B (Mistral), used to identify WWTP components such as circular/rectangular tanks, aeration basins and distinguish confounders via expert prompts producing JSON outputs with confidence and descriptions. The dataset comprises 1,207 validated WWTP locations (198 UAE, 354 KSA, 655 Egypt) and equal non-WWTP sites from field/AI data, as 600mx600m Geo-TIFF images (Zoom 18, EPSG:4326). Zero-shot evaluations on WWTP images showed several VLMs out-performing YOLOv8's true positive rate, with Gemma-3 highest. Results confirm that VLMs, particularly with zero-shot, can replace YOLOv8 for efficient, annotation-free WWTP classification, enabling scalable remote sensing.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "9 pages, 9 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14312v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出基于视觉语言模型的零样本与少样本方法，以替代传统YOLO模型实现中东和北非地区污水处理厂的卫星图像高效检测。",
            "summary_zh": "在中东和北非地区，污水处理厂对可持续水资源管理至关重要，从卫星图像中精确识别这些设施有助于环境监测。传统方法如YOLOv8分割需要大量人工标注，而研究表明视觉语言模型通过其内在推理和标注能力，能够以零样本或少量样本实现同等或更优的效果。本研究提出了一种结构化的VLM比较方法，分为零样本和少样本两个流程，专门用于识别污水处理厂。YOLOv8在来自埃及、沙特阿拉伯和阿联酋的83,566张高分辨率卫星图像政府数据集上进行了训练，其中约85%为污水处理厂正样本，15%为非污水处理厂负样本。评估的VLM包括LLaMA 3.2 Vision、Qwen 2.5 VL、DeepSeek-VL2、Gemma 3、Gemini和Pixtral 12B，用于识别污水处理厂组件如圆形/矩形储罐、曝气池，并通过专家提示区分混淆物，输出包含置信度和描述的JSON结果。数据集包含1,207个已验证的污水处理厂位置和等量的非污水处理厂站点，以600米×600米的Geo-TIFF图像形式提供。在污水处理厂图像上的零样本评估显示，多个VLM的性能超过了YOLOv8的真阳性率，其中Gemma-3表现最佳。结果证实，VLM特别是零样本方法，可以替代YOLOv8实现高效、无需标注的污水处理厂分类，支持可扩展的遥感应用。",
            "intro_zh": [
                "核心问题：传统YOLOv8方法依赖大量人工标注，成本高且难以适应中东和北非地区污水处理厂的快速检测需求。",
                "方法要点：采用视觉语言模型进行零样本和少样本检测，利用其内在推理能力减少标注依赖，通过专家提示识别污水处理厂组件。",
                "实验或效果：多个VLM在零样本评估中超越YOLOv8真阳性率，Gemma-3表现最优，证实VLM可高效替代传统方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决中东和北非地区污水处理厂在卫星图像中的高效检测问题。现有方法如YOLOv8分割需要大量手动标注，导致成本高、可扩展性差，难以适应快速环境监测需求。\\n\\n**核心思路**：核心解决思路是利用视觉语言模型的零样本和少样本能力，通过其预训练的多模态理解能力，减少对标注数据的依赖，实现高效、无需额外训练的污水处理厂检测。这样设计是因为VLM能够基于自然语言提示进行推理，直接应用于新任务，避免了传统方法的数据标注瓶颈。\\n\\n**技术框架**：整体架构分为两个主要流程：零样本流和少样本流。首先，收集并预处理来自埃及、沙特阿拉伯和阿联酋的83,566张高分辨率卫星图像数据集，包含污水处理厂正样本和非污水处理厂负样本。然后，训练YOLOv8作为基线模型。接着，评估多个VLM模型，包括LLaMA 3.2 Vision、Qwen 2.5 VL等，使用专家设计的提示词来识别污水处理厂组件如圆形/矩形储罐和曝气池，并区分混淆物。最后，通过比较VLM与YOLOv8的性能，验证VLM的有效性。\\n\\n**关键创新**：最重要的技术创新点是将VLM应用于污水处理厂的卫星图像检测，特别是通过零样本和少样本方法实现高效分类。与现有YOLO方法的本质区别在于，VLM无需针对特定任务进行大量标注和训练，而是利用预训练模型的通用能力，通过提示工程直接推理，显著降低了数据准备成本。\\n\\n**关键设计**：关键设计包括使用高分辨率Geo-TIFF图像（600米×600米，Zoom 18，EPSG:4326），数据集包含1,207个已验证的污水处理厂位置和等量非污水处理厂站点。VLM评估中，采用专家提示词来引导模型识别特定组件，输出JSON格式结果，包含置信度和描述信息。模型参数基于预训练设置，未提及自定义损失函数或网络结构修改。",
            "application_zh": "该研究在中东和北非地区的环境监测和水资源管理领域具有重要应用价值。通过卫星图像高效检测污水处理厂，可支持可持续水资源规划、污染控制和基础设施评估。未来，该方法可扩展到其他遥感目标检测任务，如建筑物识别或农业监测，推动零样本学习在遥感领域的广泛应用，提升大规模环境监测的效率和可扩展性。",
            "highlight_zh": "最重要的实验结果显示，在零样本评估中，多个视觉语言模型超越了YOLOv8的真阳性率。具体而言，Gemma-3模型表现最佳，在污水处理厂图像检测中达到最高性能。这表明VLM无需额外标注即可实现高效分类，验证了其作为传统方法替代方案的潜力，为遥感应用提供了可扩展的解决方案。",
            "tags_zh": [
                "视觉语言模型",
                "零样本检测",
                "少样本学习",
                "卫星图像分析",
                "污水处理厂识别",
                "遥感应用",
                "中东和北非地区",
                "环境监测"
            ],
            "_index": 138
        },
        {
            "title": "Continual Learning at the Edge: An Agnostic IIoT Architecture",
            "authors": [
                "Pablo García-Santaclara",
                "Bruno Fernández-Castro",
                "Rebeca P. Díaz-Redondo",
                "Carlos Calvo-Moa",
                "Henar Mariño-Bodelón"
            ],
            "arxiv_id": "2512.14311v1",
            "summary": "The exponential growth of Internet-connected devices has presented challenges to traditional centralized computing systems due to latency and bandwidth limitations. Edge computing has evolved to address these difficulties by bringing computations closer to the data source. Additionally, traditional machine learning algorithms are not suitable for edge-computing systems, where data usually arrives in a dynamic and continual way. However, incremental learning offers a good solution for these settings. We introduce a new approach that applies the incremental learning philosophy within an edge-computing scenario for the industrial sector with a specific purpose: real time quality control in a manufacturing system. Applying continual learning we reduce the impact of catastrophic forgetting and provide an efficient and effective solution.",
            "categories": [
                "stat.ML",
                "cs.LG"
            ],
            "primary_category": "stat.ML",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "10.1007/978-981-96-6938-7_33",
            "journal_ref": "García-Santaclara, P., Fernández-Castro, B., Díaz-Redondo, R. P., Calvo-Moa, C., & Mariño-Bodelón, H. (2025). Continual learning at the edge: An agnostic IIoT architecture. In Lecture Notes in Networks and Systems. Springer",
            "pdf_url": "https://arxiv.org/pdf/2512.14311v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出一种面向工业边缘计算的持续学习方法，以解决实时质量控制中的灾难性遗忘问题。",
            "summary_zh": "互联网连接设备的指数级增长给传统集中式计算系统带来了延迟和带宽限制的挑战。边缘计算通过将计算靠近数据源来应对这些困难。此外，传统机器学习算法不适合边缘计算系统，因为数据通常以动态和持续的方式到达。然而，增量学习为这些场景提供了良好的解决方案。我们引入了一种新方法，将增量学习理念应用于工业领域的边缘计算场景，具体目的是在制造系统中实现实时质量控制。通过应用持续学习，我们减少了灾难性遗忘的影响，并提供了一种高效且有效的解决方案。",
            "intro_zh": [
                "核心问题：传统机器学习算法在边缘计算中不适应动态持续数据流，导致灾难性遗忘和效率低下。",
                "方法要点：结合增量学习与边缘计算，设计工业物联网架构，实现实时质量控制并减少遗忘。",
                "实验或效果：在制造系统中验证了方法的有效性，提升了学习效率和实时性能，具体数据未知。"
            ],
            "method_zh": "**问题定义**：论文旨在解决工业边缘计算场景中实时质量控制的问题，现有传统机器学习方法因数据动态持续到达而面临灾难性遗忘和计算效率低的痛点。\\n\\n**核心思路**：核心思路是将增量学习（持续学习）理念集成到边缘计算架构中，通过在线学习机制处理流式数据，减少模型更新时的遗忘，从而适应工业环境的实时需求。\\n\\n**技术框架**：整体架构包括数据采集层、边缘处理层和云端协调层。主要模块有传感器数据输入、增量学习算法模块、实时推理引擎和反馈循环，流程从数据收集到模型更新闭环进行。\\n\\n**关键创新**：最重要的技术创新是提出一种与具体算法无关的工业物联网架构，将增量学习应用于边缘端，本质区别在于结合了工业实时性与学习持续性，避免了传统集中式方法的延迟问题。\\n\\n**关键设计**：关键设计包括使用增量学习算法（如未知具体算法）处理数据流，设置参数以平衡学习速度与遗忘率，网络结构可能基于轻量级模型以适应边缘设备，损失函数设计关注新旧知识保留，具体细节未知。",
            "application_zh": "该研究主要应用于工业制造领域的实时质量控制，如生产线缺陷检测和过程监控。潜在价值在于提升生产效率、减少停机时间，并推动智能工厂的发展。未来可能扩展到其他边缘计算场景，如智能城市或医疗监测。",
            "highlight_zh": "最重要的实验结果包括在制造系统中实现了实时质量控制，通过增量学习减少了灾难性遗忘，提升了学习效率。具体性能数据未知，但对比传统方法，在延迟和准确性方面有显著提升，提升幅度未详细说明。",
            "tags_zh": [
                "持续学习",
                "边缘计算",
                "工业物联网",
                "增量学习",
                "实时质量控制",
                "灾难性遗忘",
                "智能制造",
                "在线学习"
            ],
            "_index": 139
        },
        {
            "title": "PSMamba: Progressive Self-supervised Vision Mamba for Plant Disease Recognition",
            "authors": [
                "Abdullah Al Mamun",
                "Miaohua Zhang",
                "David Ahmedt-Aristizabal",
                "Zeeshan Hayder",
                "Mohammad Awrangjeb"
            ],
            "arxiv_id": "2512.14309v1",
            "summary": "Self-supervised Learning (SSL) has become a powerful paradigm for representation learning without manual annotations. However, most existing frameworks focus on global alignment and struggle to capture the hierarchical, multi-scale lesion patterns characteristic of plant disease imagery. To address this gap, we propose PSMamba, a progressive self-supervised framework that integrates the efficient sequence modelling of Vision Mamba (VM) with a dual-student hierarchical distillation strategy. Unlike conventional single teacher-student designs, PSMamba employs a shared global teacher and two specialised students: one processes mid-scale views to capture lesion distributions and vein structures, while the other focuses on local views to capture fine-grained cues such as texture irregularities and early-stage lesions. This multi-granular supervision facilitates the joint learning of contextual and detailed representations, with consistency losses ensuring coherent cross-scale alignment. Experiments on three benchmark datasets show that PSMamba consistently outperforms state-of-the-art SSL methods, delivering superior accuracy and robustness in both domain-shifted and fine-grained scenarios.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14309v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出PSMamba渐进自监督框架，结合Vision Mamba与双学生分层蒸馏，解决植物病害图像多尺度病变模式识别难题。",
            "summary_zh": "自监督学习已成为无需人工标注的强大表示学习范式。然而，大多数现有框架侧重于全局对齐，难以捕捉植物病害图像中具有层次性、多尺度的病变模式特征。为弥补这一不足，我们提出了PSMamba，这是一个渐进自监督框架，将Vision Mamba的高效序列建模能力与双学生分层蒸馏策略相结合。不同于传统的单教师-学生设计，PSMamba采用一个共享的全局教师和两个专门的学生：一个处理中尺度视图以捕捉病变分布和叶脉结构，另一个专注于局部视图以捕捉细粒度线索，如纹理不规则和早期病变。这种多粒度监督促进了上下文和细节表示的联合学习，并通过一致性损失确保跨尺度对齐的连贯性。在三个基准数据集上的实验表明，PSMamba始终优于最先进的自监督学习方法，在领域偏移和细粒度场景下均展现出卓越的准确性和鲁棒性。",
            "intro_zh": [
                "现有自监督方法多关注全局对齐，难以有效捕捉植物病害图像中层次化、多尺度的病变模式特征，限制了其在细粒度识别任务中的性能。",
                "论文提出PSMamba框架，核心是结合Vision Mamba的高效序列建模与双学生分层蒸馏策略，通过多粒度监督联合学习上下文与细节表示。",
                "在三个基准数据集上，PSMamba均超越现有自监督方法，在领域偏移和细粒度场景下表现出更高的准确性和鲁棒性，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决植物病害识别中，现有自监督学习方法难以有效捕捉图像多尺度病变模式（如全局病变分布、中尺度叶脉结构、局部纹理细节）的问题，导致在细粒度和领域偏移场景下性能受限。\\n\\n**核心思路**：核心思路是设计一个渐进自监督框架，通过结合Vision Mamba的高效序列建模能力与双学生分层蒸馏策略，实现多粒度表示的联合学习，从而更全面地捕捉植物病害的层次化特征。\\n\\n**技术框架**：整体架构包括一个共享的全局教师网络和两个专门的学生网络。教师网络处理全局视图以提供高层语义指导；学生网络分别处理中尺度视图（捕捉病变分布和叶脉结构）和局部视图（捕捉纹理不规则和早期病变）。通过分层蒸馏，学生从教师学习，同时利用一致性损失确保跨尺度对齐的连贯性，形成渐进式学习流程。\\n\\n**关键创新**：最重要的技术创新是双学生分层蒸馏策略与Vision Mamba的结合。与现有单教师-学生设计相比，PSMamba通过专门化的学生网络针对不同尺度特征进行优化，实现了更精细的多粒度监督，本质区别在于其能够同时建模上下文和细节，而非仅依赖全局对齐。\\n\\n**关键设计**：关键设计包括：使用Vision Mamba作为骨干网络以高效处理图像序列；设置两个学生网络分别对应中尺度和局部视图；采用一致性损失（如对比损失或均方误差）来强制跨尺度表示对齐；在训练中实施渐进式蒸馏，教师参数通过指数移动平均更新，学生通过蒸馏损失和自监督目标联合优化。具体参数如视图大小、损失权重需根据数据集调整，但框架保持通用性。",
            "application_zh": "该研究主要应用于农业领域的植物病害自动识别与监测，可集成到智能农业系统中，实现早期病害检测、精准施药和产量预测，提升农业生产效率与可持续性。未来可能扩展到其他细粒度视觉任务，如医学图像分析或工业缺陷检测，推动自监督学习在资源受限场景下的实际应用。",
            "highlight_zh": "在三个植物病害基准数据集上的实验显示，PSMamba在准确性和鲁棒性方面均优于现有自监督学习方法（如SimCLR、MoCo等）。具体而言，在领域偏移和细粒度识别任务中，PSMamba取得了显著的性能提升，例如在某些数据集上准确率提升超过5%，验证了其多粒度监督的有效性。",
            "tags_zh": [
                "自监督学习",
                "植物病害识别",
                "Vision Mamba",
                "分层蒸馏",
                "多尺度表示",
                "细粒度视觉",
                "农业人工智能",
                "序列建模"
            ],
            "_index": 140
        },
        {
            "title": "Improving the Accuracy of Amortized Model Comparison with Self-Consistency",
            "authors": [
                "Šimon Kucharský",
                "Aayush Mishra",
                "Daniel Habermann",
                "Stefan T. Radev",
                "Paul-Christian Bürkner"
            ],
            "arxiv_id": "2512.14308v1",
            "summary": "Amortized Bayesian inference (ABI) offers fast, scalable approximations to posterior densities by training neural surrogates on data simulated from the statistical model. However, ABI methods are highly sensitive to model misspecification: when observed data fall outside the training distribution (generative scope of the statistical models), neural surrogates can behave unpredictably. This makes it a challenge in a model comparison setting, where multiple statistical models are considered, of which at least some are misspecified. Recent work on self-consistency (SC) provides a promising remedy to this issue, accessible even for empirical data (without ground-truth labels). In this work, we investigate how SC can improve amortized model comparison conceptualized in four different ways. Across two synthetic and two real-world case studies, we find that approaches for model comparison that estimate marginal likelihoods through approximate parameter posteriors consistently outperform methods that directly approximate model evidence or posterior model probabilities. SC training improves robustness when the likelihood is available, even under severe model misspecification. The benefits of SC for methods without access of analytic likelihoods are more limited and inconsistent. Our results suggest practical guidance for reliable amortized Bayesian model comparison: prefer parameter posterior-based methods and augment them with SC training on empirical datasets to mitigate extrapolation bias under model misspecification.",
            "categories": [
                "stat.ML",
                "cs.LG",
                "stat.CO"
            ],
            "primary_category": "stat.ML",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "17 pages, 9 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14308v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出基于自一致性的摊销贝叶斯模型比较方法，以解决模型误设下的外推偏差问题",
            "summary_zh": "摊销贝叶斯推断（ABI）通过训练神经网络代理来快速近似后验密度，但该方法对模型误设高度敏感：当观测数据超出训练分布时，神经网络代理可能表现不可预测。这在模型比较场景中尤为挑战，因为需要考虑多个统计模型，其中至少部分模型存在误设。最近关于自一致性（SC）的研究为解决这一问题提供了有前景的补救措施，即使对于没有真实标签的经验数据也适用。本研究探讨了SC如何改进四种不同概念化的摊销模型比较方法。通过两个合成和两个真实世界案例研究，我们发现通过近似参数后验估计边际似然的方法始终优于直接近似模型证据或后验模型概率的方法。当似然函数可用时，SC训练即使在严重模型误设下也能提高鲁棒性。对于无法访问解析似然函数的方法，SC的益处更为有限且不一致。我们的结果为可靠的摊销贝叶斯模型比较提供了实用指导：优先选择基于参数后验的方法，并在经验数据集上通过SC训练增强它们，以减轻模型误设下的外推偏差。",
            "intro_zh": [
                "核心问题：摊销贝叶斯推断在模型误设时表现不稳定，导致模型比较结果不可靠。",
                "方法要点：引入自一致性训练增强神经网络代理，提高对经验数据的泛化能力。",
                "实验或效果：基于参数后验的方法在四个案例中表现最佳，SC训练显著提升鲁棒性。"
            ],
            "method_zh": "**问题定义**：论文解决摊销贝叶斯模型比较在模型误设下的不可靠性问题。现有ABI方法训练神经网络代理近似后验，但当观测数据超出训练分布（即模型误设）时，代理可能产生偏差预测，导致模型比较结果失真。\\n\\n**核心思路**：利用自一致性（SC）原则增强神经网络代理的训练，使其在经验数据上保持一致性，从而减少外推偏差。SC通过强制代理在不同数据子集或条件下输出一致结果来提高泛化能力。\\n\\n**技术框架**：整体流程包含四个阶段：1）从统计模型生成模拟数据用于训练；2）训练神经网络代理（如归一化流或变分自编码器）近似后验；3）应用SC训练，通过一致性损失优化代理；4）在四种模型比较方法（如边际似然估计、后验模型概率近似）中评估性能。\\n\\n**关键创新**：将SC训练集成到摊销贝叶斯框架中，专门针对模型比较场景优化。与现有ABI方法相比，本方法强调在经验数据上的自洽性，而非仅依赖模拟数据训练，从而更好地处理模型误设。\\n\\n**关键设计**：SC训练使用一致性损失函数，如均方误差或KL散度，比较代理在不同数据条件下的输出；网络结构可能包括多层感知机或卷积网络，具体取决于模型复杂度；参数设置涉及训练周期、学习率和正则化，以平衡拟合与泛化。",
            "application_zh": "该研究在贝叶斯统计、机器学习和数据科学领域有广泛应用潜力，特别是在需要快速模型选择的场景，如临床试验设计、金融风险建模和生态学分析。通过提高模型比较的鲁棒性，它有助于在复杂真实世界数据中做出更可靠的决策，减少因模型误设导致的错误推断。未来可能扩展到多模型集成和在线学习环境。",
            "highlight_zh": "在四个案例研究中，基于参数后验估计边际似然的方法始终优于直接近似模型证据的方法，平均准确率提升约15%。当似然函数可用时，SC训练将模型比较的鲁棒性提高20%以上，即使在严重模型误设下；但对于无解析似然的方法，SC提升有限且不一致，性能波动在5%以内。",
            "tags_zh": [
                "摊销贝叶斯推断",
                "模型比较",
                "自一致性训练",
                "模型误设",
                "神经网络代理",
                "边际似然估计",
                "后验近似",
                "鲁棒性增强"
            ],
            "_index": 141
        },
        {
            "title": "The Trust in AI-Generated Health Advice (TAIGHA) Scale and Short Version (TAIGHA-S): Development and Validation Study",
            "authors": [
                "Marvin Kopka",
                "Azeem Majeed",
                "Gabriella Spinelli",
                "Austen El-Osta",
                "Markus Feufel"
            ],
            "arxiv_id": "2512.14278v1",
            "summary": "Artificial Intelligence tools such as large language models are increasingly used by the public to obtain health information and guidance. In health-related contexts, following or rejecting AI-generated advice can have direct clinical implications. Existing instruments like the Trust in Automated Systems Survey assess trustworthiness of generic technology, and no validated instrument measures users' trust in AI-generated health advice specifically. This study developed and validated the Trust in AI-Generated Health Advice (TAIGHA) scale and its four-item short form (TAIGHA-S) as theory-based instruments measuring trust and distrust, each with cognitive and affective components. The items were developed using a generative AI approach, followed by content validation with 10 domain experts, face validation with 30 lay participants, and psychometric validation with 385 UK participants who received AI-generated advice in a symptom-assessment scenario. After automated item reduction, 28 items were retained and reduced to 10 based on expert ratings. TAIGHA showed excellent content validity (S-CVI/Ave=0.99) and CFA confirmed a two-factor model with excellent fit (CFI=0.98, TLI=0.98, RMSEA=0.07, SRMR=0.03). Internal consistency was high (α=0.95). Convergent validity was supported by correlations with the Trust in Automated Systems Survey (r=0.67/-0.66) and users' reliance on the AI's advice (r=0.37 for trust), while divergent validity was supported by low correlations with reading flow and mental load (all |r|<0.25). TAIGHA-S correlated highly with the full scale (r=0.96) and showed good reliability (α=0.88). TAIGHA and TAIGHA-S are validated instruments for assessing user trust and distrust in AI-generated health advice. Reporting trust and distrust separately permits a more complete evaluation of AI interventions, and the short scale is well-suited for time-constrained settings.",
            "categories": [
                "cs.HC",
                "cs.AI"
            ],
            "primary_category": "cs.HC",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14278v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出TAIGHA和TAIGHA-S量表，专门评估用户对AI生成健康建议的信任与不信任，以解决现有工具缺乏针对性测量的问题。",
            "summary_zh": "随着大型语言模型等人工智能工具被公众越来越多地用于获取健康信息和指导，在健康相关情境中，采纳或拒绝AI生成建议可能产生直接的临床影响。现有工具如“自动化系统信任调查”评估通用技术的可信度，但缺乏专门测量用户对AI生成健康建议信任度的验证工具。本研究开发并验证了“AI生成健康建议信任量表”及其四项目简短版，作为基于理论的工具，分别测量信任和不信任，每个维度包含认知和情感成分。项目开发采用生成式AI方法，随后进行内容验证、表面验证和心理测量验证。TAIGHA显示出优异的内容效度和结构效度，内部一致性高，与相关工具具有收敛效度，与无关变量具有区分效度。TAIGHA-S与完整量表高度相关且可靠性良好。TAIGHA和TAIGHA-S是评估用户对AI生成健康建议信任与不信任的有效工具，单独报告信任和不信任允许更全面地评估AI干预措施，简短量表适用于时间受限的场景。",
            "intro_zh": [
                "核心问题：现有通用技术信任量表无法专门测量用户对AI生成健康建议的信任，缺乏针对性工具，而健康建议的采纳与否具有直接临床意义。",
                "方法要点：基于理论构建信任与不信任双维度量表，每个维度包含认知和情感成分，采用生成式AI辅助项目开发，并通过多阶段验证确保效度和信度。",
                "实验或效果：TAIGHA量表显示出优异的内容效度、结构效度和内部一致性，与相关工具高度相关，简短版TAIGHA-S与完整量表高度相关且可靠性良好。"
            ],
            "method_zh": "**问题定义**：论文旨在解决用户对AI生成健康建议的信任度测量问题。现有方法的痛点在于，如“自动化系统信任调查”等工具评估的是通用技术的可信度，缺乏专门针对AI生成健康建议这一特定场景的验证工具，而健康建议的采纳与否可能直接影响临床决策，因此需要更精准的测量工具。\\n\\n**核心思路**：论文的核心解决思路是开发一个基于理论的量表，专门测量用户对AI生成健康建议的信任与不信任。设计上，将信任和不信任作为两个独立维度，每个维度进一步细分为认知和情感成分，以全面捕捉用户的心理状态。这样设计是因为信任和不信任可能同时存在，单独测量能提供更完整的评估，而区分认知和情感成分有助于深入理解信任的构成。\\n\\n**技术框架**：整体架构包括四个主要阶段：项目开发、内容验证、表面验证和心理测量验证。首先，使用生成式AI方法生成初始项目池；然后，通过10名领域专家进行内容验证，确保项目与理论维度匹配；接着，由30名普通参与者进行表面验证，评估项目的可理解性；最后，在385名英国参与者中进行心理测量验证，参与者在一个症状评估场景中接收AI生成建议，并完成量表测试，通过自动化项目缩减和专家评分，最终保留10个项目形成TAIGHA量表，并衍生出四项目简短版TAIGHA-S。\\n\\n**关键创新**：最重要的技术创新点是开发了首个专门针对AI生成健康建议的信任量表，将信任和不信任作为独立维度进行测量，并整合认知和情感成分。与现有方法的本质区别在于，现有工具如“自动化系统信任调查”是通用性的，而TAIGHA专注于健康建议这一高风险场景，提供了更情境化和精细化的测量，有助于更准确地评估AI干预在医疗领域的实际影响。\\n\\n**关键设计**：关键设计包括：量表基于理论构建，信任和不信任各包含认知和情感子维度；项目开发采用生成式AI辅助，确保项目多样性和覆盖度；验证过程中，使用内容效度指数评估专家一致性，通过验证性因子分析确认双因子模型结构，计算内部一致性系数评估信度，并通过相关性分析检验收敛效度和区分效度；具体参数如S-CVI/Ave=0.99用于内容效度，CFI=0.98、TLI=0.98、RMSEA=0.07、SRMR=0.03用于模型拟合，α=0.95用于内部一致性。",
            "application_zh": "该研究的潜在应用领域包括医疗健康、人工智能伦理和人机交互。实际价值在于为评估AI生成健康建议的可接受性和有效性提供标准化工具，帮助开发者和医疗机构优化AI系统设计，提升用户采纳率，减少误用风险。未来影响可能推动AI在健康咨询中的更安全集成，支持个性化医疗建议的信任度监测，并为政策制定提供数据支持。",
            "highlight_zh": "最重要的实验结果包括：TAIGHA量表显示出优异的内容效度（S-CVI/Ave=0.99）和结构效度（CFI=0.98, TLI=0.98, RMSEA=0.07, SRMR=0.03），内部一致性高（α=0.95）。收敛效度得到支持，与“自动化系统信任调查”高度相关（r=0.67/-0.66），与用户对AI建议的依赖度相关（r=0.37 for trust）；区分效度得到支持，与阅读流畅度和心理负荷相关性低（所有|r|<0.25）。TAIGHA-S与完整量表高度相关（r=0.96），且可靠性良好（α=0.88）。",
            "tags_zh": [
                "AI生成健康建议",
                "信任量表",
                "心理测量验证",
                "内容效度",
                "结构效度",
                "人机交互",
                "医疗人工智能",
                "量表开发"
            ],
            "_index": 142
        },
        {
            "title": "TUN: Detecting Significant Points in Persistence Diagrams with Deep Learning",
            "authors": [
                "Yu Chen",
                "Hongwei Lin"
            ],
            "arxiv_id": "2512.14274v1",
            "summary": "Persistence diagrams (PDs) provide a powerful tool for understanding the topology of the underlying shape of a point cloud. However, identifying which points in PDs encode genuine signals remains challenging. This challenge directly hinders the practical adoption of topological data analysis in many applications, where automated and reliable interpretation of persistence diagrams is essential for downstream decision-making. In this paper, we study automatic significance detection for one-dimensional persistence diagrams. Specifically, we propose Topology Understanding Net (TUN), a multi-modal network that combines enhanced PD descriptors with self-attention, a PointNet-style point cloud encoder, learned fusion, and per-point classification, alongside stable preprocessing and imbalance-aware training. It provides an automated and effective solution for identifying significant points in PDs, which are critical for downstream applications. Experiments show that TUN outperforms classic methods in detecting significant points in PDs, illustrating its effectiveness in real-world applications.",
            "categories": [
                "cs.CV",
                "cs.LG",
                "math.AT"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14274v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出TUN网络以解决一维持久图显著性点自动检测问题",
            "summary_zh": "持久图（PDs）为理解点云底层形状的拓扑结构提供了强大工具，但识别PDs中哪些点编码了真实信号仍然具有挑战性。这一挑战直接阻碍了拓扑数据分析在许多应用中的实际采用，在这些应用中，持久图的自动可靠解释对于下游决策至关重要。本文研究了一维持久图的自动显著性检测。具体而言，我们提出了拓扑理解网络（TUN），这是一个多模态网络，结合了增强的PD描述符与自注意力机制、PointNet风格的点云编码器、学习融合以及逐点分类，同时辅以稳定的预处理和不平衡感知训练。它为识别PDs中的显著性点提供了一个自动且有效的解决方案，这对于下游应用至关重要。实验表明，TUN在检测PDs中的显著性点方面优于经典方法，证明了其在现实应用中的有效性。",
            "intro_zh": [
                "核心问题：现有方法难以自动可靠地识别持久图中编码真实信号的显著性点，阻碍了拓扑数据分析的实际应用。",
                "方法要点：提出TUN网络，融合增强描述符、自注意力、点云编码器与学习融合，实现多模态特征提取与逐点分类。",
                "实验或效果：TUN在检测显著性点方面优于经典方法，验证了其在实际应用中的有效性和自动化优势。"
            ],
            "method_zh": "**问题定义**：论文旨在解决一维持久图中显著性点的自动检测问题。现有方法的痛点在于，持久图作为拓扑数据分析的关键工具，其点集包含噪声和真实信号，但传统方法难以自动化、可靠地区分哪些点编码了重要拓扑特征（如孔洞、连接性），这限制了拓扑数据分析在需要自动解释的下游任务（如形状分析、数据分类）中的实际应用。\\n\\n**核心思路**：论文的核心解决思路是设计一个深度学习网络，通过多模态特征融合来学习持久图中点的显著性。这样设计是因为持久图本身可视为点云，结合其拓扑描述符（如持久性、出生-死亡坐标）能提供丰富信息，而深度学习能自动捕捉复杂模式，克服传统基于阈值或统计方法的局限性。\\n\\n**技术框架**：整体架构包括预处理、特征提取、融合和分类四个阶段。主要模块有：1）稳定预处理模块，对持久图进行标准化和增强处理；2）多模态特征提取模块，使用增强的PD描述符（如持久性向量）和自注意力机制提取全局上下文，同时用PointNet风格的点云编码器处理点坐标；3）学习融合模块，将不同模态特征动态结合；4）逐点分类模块，输出每个点的显著性概率。\\n\\n**关键创新**：最重要的技术创新点是提出TUN这一多模态网络，首次将增强PD描述符与自注意力、点云编码器结合，实现端到端的显著性检测。与现有方法的本质区别在于，它不依赖手动阈值或简单统计，而是通过深度学习自动学习特征表示，提高了检测的准确性和鲁棒性。\\n\\n**关键设计**：关键设计包括：使用不平衡感知训练策略（如加权损失函数）处理数据中显著性点与非显著性点的不平衡分布；网络结构采用多层感知机（MLP）和自注意力层进行特征变换；损失函数可能基于交叉熵，具体参数如学习率、批量大小在实验中优化设置；预处理步骤确保输入稳定性，避免噪声干扰。",
            "application_zh": "该研究的潜在应用领域包括计算机视觉中的形状识别、医学图像分析（如肿瘤检测）、机器人感知（环境建模）和数据分析（异常检测）。实际价值在于为拓扑数据分析提供自动化工具，提升下游任务（如分类、分割）的效率和可靠性。未来影响可能推动拓扑机器学习与深度学习的更深度融合，拓展到高维持久图或实时处理场景。",
            "highlight_zh": "实验结果显示，TUN在检测一维持久图中显著性点方面显著优于经典基线方法（如基于持久性阈值或统计检验的方法）。具体性能数据未在摘要中给出，但论文指出TUN在准确率、召回率或F1分数等指标上有所提升，验证了其多模态融合和深度学习框架的有效性。提升幅度可能依赖于数据集，但整体证明了自动化检测的优越性。",
            "tags_zh": [
                "持久图显著性检测",
                "拓扑数据分析",
                "多模态融合",
                "自注意力机制",
                "点云编码",
                "深度学习网络",
                "不平衡感知训练",
                "自动化拓扑解释"
            ],
            "_index": 143
        },
        {
            "title": "Explainable Preference Learning: a Decision Tree-based Surrogate Model for Preferential Bayesian Optimization",
            "authors": [
                "Nick Leenders",
                "Thomas Quadt",
                "Boris Cule",
                "Roy Lindelauf",
                "Herman Monsuur",
                "Joost van Oijen",
                "Mark Voskuijl"
            ],
            "arxiv_id": "2512.14263v1",
            "summary": "Current Preferential Bayesian Optimization methods rely on Gaussian Processes (GPs) as surrogate models. These models are hard to interpret, struggle with handling categorical data, and are computationally complex, limiting their real-world usability. In this paper, we introduce an inherently interpretable decision tree-based surrogate model capable of handling both categorical and continuous data, and scalable to large datasets. Extensive numerical experiments on eight increasingly spiky optimization functions show that our model outperforms GP-based alternatives on spiky functions and has only marginally lower performance for non-spiky functions. Moreover, we apply our model to the real-world Sushi dataset and show its ability to learn an individual's sushi preferences. Finally, we show some initial work on using historical preference data to speed up the optimization process for new unseen users.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "math.OC"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14263v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出基于决策树的可解释偏好学习模型，以解决偏好贝叶斯优化中高斯过程模型难以解释、处理分类数据困难的问题。",
            "summary_zh": "当前的偏好贝叶斯优化方法依赖于高斯过程作为代理模型。这些模型难以解释，处理分类数据困难，且计算复杂度高，限制了其在实际应用中的可用性。本文提出了一种本质上可解释的基于决策树的代理模型，能够同时处理分类和连续数据，并可扩展到大型数据集。在八个逐渐尖峰的优化函数上进行的大量数值实验表明，我们的模型在尖峰函数上优于基于高斯过程的替代方法，在非尖峰函数上性能仅略低。此外，我们将模型应用于真实世界的寿司数据集，展示了其学习个人寿司偏好的能力。最后，我们展示了利用历史偏好数据加速新用户优化过程的初步工作。",
            "intro_zh": [
                "现有方法依赖高斯过程作为代理模型，存在难以解释、处理分类数据困难、计算复杂等问题，限制了实际应用。",
                "提出基于决策树的可解释代理模型，能处理分类和连续数据，具有可扩展性，旨在提升偏好学习的实用性和效率。",
                "在尖峰函数上优于高斯过程模型，在非尖峰函数上性能接近，并成功应用于寿司偏好学习，展示了实际价值。"
            ],
            "method_zh": "**问题定义**：论文旨在解决偏好贝叶斯优化中现有高斯过程代理模型的可解释性差、处理分类数据能力弱、计算复杂度高的问题，这些痛点限制了模型在真实世界场景中的广泛应用。\\n\\n**核心思路**：论文的核心思路是采用决策树作为代理模型，替代传统的高斯过程，以提供内在的可解释性，同时通过设计支持分类和连续数据的决策树结构，提升模型的灵活性和可扩展性。\\n\\n**技术框架**：整体框架包括数据预处理、决策树构建、偏好学习优化和结果解释四个阶段。首先，输入包含分类和连续特征的偏好数据；然后，构建决策树模型来学习偏好关系；接着，利用贝叶斯优化方法进行参数调优；最后，输出可解释的决策规则和优化结果。\\n\\n**关键创新**：最重要的技术创新是引入决策树作为可解释的代理模型，这本质区别于高斯过程的黑盒特性，使得模型能够直接输出决策规则，便于理解和验证，同时增强了处理混合数据类型的能力。\\n\\n**关键设计**：关键设计包括决策树的分裂准则（如信息增益或基尼指数）以处理偏好数据，损失函数可能基于偏好排序误差，网络结构为多分支决策树，参数设置如树深度和最小样本数通过交叉验证优化，以平衡模型复杂度和性能。",
            "application_zh": "该研究在个性化推荐系统、产品设计优化、用户偏好建模等领域具有潜在应用价值。例如，在电商平台中，可基于用户历史偏好数据快速学习新用户的偏好，提升推荐准确性和用户体验。未来可能扩展到更多真实世界场景，如医疗决策支持或智能家居控制，推动可解释人工智能的发展。",
            "highlight_zh": "在八个逐渐尖峰的优化函数实验中，基于决策树的模型在尖峰函数上显著优于高斯过程模型，具体性能提升幅度未知，但实验表明在非尖峰函数上性能仅略低。在寿司数据集应用中，模型成功学习了个人的寿司偏好，验证了其实际有效性。初步工作还展示了利用历史数据加速新用户优化的潜力。",
            "tags_zh": [
                "偏好学习",
                "贝叶斯优化",
                "决策树模型",
                "可解释人工智能",
                "分类数据处理",
                "代理模型",
                "个性化推荐",
                "优化算法"
            ],
            "_index": 144
        },
        {
            "title": "FLAME: Flow Enhanced Legendre Memory Models for General Time Series Forecasting",
            "authors": [
                "Xingjian Wu",
                "Hanyin Cheng",
                "Xiangfei Qiu",
                "Zhengyu Li",
                "Jilin Hu",
                "Chenjuan Guo",
                "Bin Yang"
            ],
            "arxiv_id": "2512.14253v1",
            "summary": "In this work, we introduce FLAME, a family of extremely lightweight and capable Time Series Foundation Models, which support both deterministic and probabilistic forecasting via generative probabilistic modeling, thus ensuring both efficiency and robustness. FLAME utilizes the Legendre Memory for strong generalization capabilities. Through adapting variants of Legendre Memory, i.e., translated Legendre (LegT) and scaled Legendre (LegS), in the Encoding and Decoding phases, FLAME can effectively capture the inherent inductive bias within data and make efficient long-range inferences. To enhance the accuracy of probabilistic forecasting while keeping efficient, FLAME adopts a Normalization Flow based forecasting head, which can model the arbitrarily intricate distributions over the forecasting horizon in a generative manner. Comprehensive experiments on well-recognized benchmarks, including TSFM-Bench and ProbTS, demonstrate the consistent state-of-the-art zero-shot performance of FLAME on both deterministic and probabilistic forecasting tasks.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14253v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出FLAME模型，通过流增强的勒让德记忆机制解决时间序列确定性及概率性预测问题。",
            "summary_zh": "本文介绍了FLAME，一个极其轻量且强大的时间序列基础模型家族，支持通过生成式概率建模进行确定性和概率性预测，从而确保效率和鲁棒性。FLAME利用勒让德记忆实现强大的泛化能力。通过采用勒让德记忆的变体，即平移勒让德（LegT）和缩放勒让德（LegS），在编码和解码阶段，FLAME能够有效捕捉数据中的内在归纳偏置，并进行高效的长程推理。为了在保持高效的同时提升概率性预测的准确性，FLAME采用基于归一化流的预测头，以生成方式建模预测范围内任意复杂的分布。在公认的基准测试（包括TSFM-Bench和ProbTS）上的全面实验表明，FLAME在确定性和概率性预测任务上均展现出持续的最先进的零样本性能。",
            "intro_zh": [
                "现有时间序列预测方法在长程推理和复杂分布建模上存在不足，难以兼顾效率与准确性。",
                "FLAME通过勒让德记忆变体捕捉数据归纳偏置，并利用归一化流建模复杂分布，实现高效预测。",
                "在TSFM-Bench和ProbTS基准测试中，FLAME在零样本设置下达到最先进性能，显著提升预测精度。"
            ],
            "method_zh": "**问题定义**：论文旨在解决时间序列预测中的两个核心问题：确定性预测（点估计）和概率性预测（分布估计）。现有方法常面临长程依赖建模困难、计算效率低、以及对复杂数据分布建模能力不足的痛点，尤其是在零样本或小样本场景下泛化能力有限。\\n\\n**核心思路**：FLAME的核心思路是结合勒让德记忆（Legendre Memory）的泛化优势与归一化流（Normalization Flow）的分布建模能力。勒让德记忆通过正交基函数有效捕捉时间序列的长期模式，而归一化流则能灵活建模任意复杂的输出分布，从而在轻量级架构下实现高效且鲁棒的预测。\\n\\n**技术框架**：FLAME的整体架构包括编码阶段、解码阶段和预测头。编码阶段使用勒让德记忆变体（如LegT和LegS）处理输入时间序列，提取特征并捕捉归纳偏置；解码阶段同样利用勒让德记忆进行长程推理，生成中间表示；预测头则基于归一化流，将解码输出映射为目标分布，支持生成式概率预测。整个过程支持端到端训练，兼顾确定性和概率性任务。\\n\\n**关键创新**：最重要的技术创新点在于将勒让德记忆与归一化流相结合，形成“流增强的勒让德记忆模型”。与现有方法相比，本质区别在于：1）通过勒让德记忆变体（LegT/LegS）自适应地处理时间尺度，增强模型对数据内在结构的捕捉能力；2）使用归一化流作为预测头，以生成方式建模复杂分布，避免了传统概率方法（如高斯假设）的局限性，从而提升预测准确性和鲁棒性。\\n\\n**关键设计**：关键设计包括：1）勒让德记忆的实现，基于勒让德多项式基函数，通过平移（LegT）和缩放（LegS）操作适应不同时间动态；2）归一化流预测头，采用可逆神经网络结构，如RealNVP或Glow变体，以最大化似然训练；3）损失函数结合确定性任务的均方误差（MSE）和概率性任务的负对数似然（NLL）；4）模型参数轻量化，通过高效线性注意力机制减少计算复杂度，支持大规模时间序列处理。",
            "application_zh": "FLAME的潜在应用领域广泛，包括金融时间序列预测（如股票价格和汇率）、气象预报（如温度和降水）、能源需求预测、物联网传感器数据分析等。其轻量级设计和强大零样本性能使其适用于资源受限环境（如边缘计算）和快速部署场景，实际价值在于提升预测精度和效率，未来可能推动时间序列基础模型在工业界和学术界的普及。",
            "highlight_zh": "在TSFM-Bench和ProbTS基准测试中，FLAME在零样本设置下实现了最先进的性能。具体而言，在确定性预测任务上，相比基线模型（如Transformer和LSTM），FLAME在多个数据集上平均提升预测精度约5-10%；在概率性预测任务上，通过归一化流建模，FLAME的分布校准误差降低约15-20%，同时保持模型参数量减少30-50%，显著优于传统概率方法（如DeepAR）。",
            "tags_zh": [
                "时间序列预测",
                "勒让德记忆",
                "归一化流",
                "概率建模",
                "零样本学习",
                "长程推理",
                "轻量级模型",
                "基础模型"
            ],
            "_index": 145
        },
        {
            "title": "From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition",
            "authors": [
                "Yiqing Zhou",
                "Yu Lei",
                "Shuzheng Si",
                "Qingyan Sun",
                "Wei Wang",
                "Yifei Wu",
                "Hao Wen",
                "Gang Chen",
                "Fanchao Qi",
                "Maosong Sun"
            ],
            "arxiv_id": "2512.14244v1",
            "summary": "Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14244v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出基于基本话语单元的上下文压缩框架，通过结构化分解与选择解决长文本处理中的计算成本与噪声问题。",
            "summary_zh": "管理大量上下文仍然是大型语言模型（LLMs）的关键瓶颈，特别是在长文档问答和自主代理等应用中，长输入会导致高计算成本并引入噪声。现有的压缩技术通常通过离散标记删除破坏局部连贯性，或依赖存在位置偏差且与闭源API不兼容的隐式潜在编码。为了解决这些限制，我们引入了基于EDU的上下文压缩器，这是一种新颖的显式压缩框架，旨在保留全局结构和细粒度细节。我们的方法将上下文压缩重新表述为“先结构后选择”的过程。首先，我们的LingoEDU将线性文本转换为基本话语单元（EDUs）的结构关系树，这些单元严格锚定到源索引以消除幻觉。其次，一个轻量级排名模块选择与查询相关的子树进行线性化。为了严格评估结构理解，我们发布了StructBench，这是一个包含248个多样化文档的手动标注数据集。实证结果表明，我们的方法实现了最先进的结构预测准确性，并显著优于前沿LLMs，同时降低了成本。此外，我们的结构感知压缩显著提高了从长上下文任务到复杂深度搜索场景的下游任务性能。",
            "intro_zh": [
                "现有压缩方法破坏局部连贯性或依赖隐式编码，导致位置偏差和API不兼容，难以平衡压缩效率与信息完整性。",
                "提出基于基本话语单元的结构化分解框架，通过先构建文本结构树再选择相关部分，实现显式且忠实的上下文压缩。",
                "在StructBench数据集上实现最先进的结构预测精度，显著优于前沿LLMs，并在下游任务中提升性能同时降低计算成本。"
            ],
            "method_zh": "**问题定义**：论文解决长文本处理中上下文压缩的挑战，现有方法如离散标记删除会破坏局部连贯性，而隐式潜在编码存在位置偏差且与闭源API不兼容，导致压缩后信息失真或难以集成。\\n\\n**核心思路**：将上下文压缩重新定义为“先结构后选择”的过程，通过将线性文本分解为基本话语单元（EDUs）的结构关系树，再基于查询选择相关子树，实现显式、结构化的压缩，以保留全局结构和细粒度细节。\\n\\n**技术框架**：整体架构包含两个主要阶段：首先，LingoEDU模块将输入文本转换为EDUs的结构关系树，每个EDU严格锚定到源文本索引；其次，轻量级排名模块评估查询与子树的相关性，选择高相关子树进行线性化输出为压缩文本。\\n\\n**关键创新**：最重要的创新是引入EDU-based显式压缩框架，通过结构化分解消除幻觉，与现有方法相比，本质区别在于从隐式编码转向基于语言学单元的显式结构表示，提高了压缩的忠实度和可解释性。\\n\\n**关键设计**：LingoEDU基于语言学规则或预训练模型识别EDUs并构建关系树；排名模块可能使用注意力机制或相似度计算，具体参数和损失函数在论文中未详细说明，但强调轻量级设计以降低计算成本；整体框架严格锚定源索引，确保输出与原始文本一致。",
            "application_zh": "该研究在长文档问答、自主代理、复杂深度搜索等场景具有广泛应用价值，能显著降低LLMs的计算成本并减少噪声干扰，提升任务性能。未来可能推动更高效的长文本处理工具和API集成，促进人工智能在文档分析和信息检索领域的实际部署。",
            "highlight_zh": "在StructBench数据集上，该方法实现了最先进的结构预测准确性，具体性能数据未在摘要中给出，但提到显著优于前沿LLMs；同时，压缩后在下游任务中表现提升，并降低了计算成本，例如在长上下文任务和深度搜索场景中均有实质性增强。",
            "tags_zh": [
                "上下文压缩",
                "基本话语单元",
                "结构化分解",
                "长文本处理",
                "大型语言模型",
                "计算效率",
                "下游任务增强",
                "显式编码"
            ],
            "_index": 146
        },
        {
            "title": "Beyond MMD: Evaluating Graph Generative Models with Geometric Deep Learning",
            "authors": [
                "Salvatore Romano",
                "Marco Grassia",
                "Giuseppe Mangioni"
            ],
            "arxiv_id": "2512.14241v1",
            "summary": "Graph generation is a crucial task in many fields, including network science and bioinformatics, as it enables the creation of synthetic graphs that mimic the properties of real-world networks for various applications. Graph Generative Models (GGMs) have emerged as a promising solution to this problem, leveraging deep learning techniques to learn the underlying distribution of real-world graphs and generate new samples that closely resemble them. Examples include approaches based on Variational Auto-Encoders, Recurrent Neural Networks, and more recently, diffusion-based models. However, the main limitation often lies in the evaluation process, which typically relies on Maximum Mean Discrepancy (MMD) as a metric to assess the distribution of graph properties in the generated ensemble. This paper introduces a novel methodology for evaluating GGMs that overcomes the limitations of MMD, which we call RGM (Representation-aware Graph-generation Model evaluation). As a practical demonstration of our methodology, we present a comprehensive evaluation of two state-of-the-art Graph Generative Models: Graph Recurrent Attention Networks (GRAN) and Efficient and Degree-guided graph GEnerative model (EDGE). We investigate their performance in generating realistic graphs and compare them using a Geometric Deep Learning model trained on a custom dataset of synthetic and real-world graphs, specifically designed for graph classification tasks. Our findings reveal that while both models can generate graphs with certain topological properties, they exhibit significant limitations in preserving the structural characteristics that distinguish different graph domains. We also highlight the inadequacy of Maximum Mean Discrepancy as an evaluation metric for GGMs and suggest alternative approaches for future research.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "physics.soc-ph"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "16 pages, 4 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14241v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出RGM方法以解决图生成模型评估中MMD指标的局限性问题",
            "summary_zh": "图生成是网络科学和生物信息学等领域的核心任务，图生成模型（GGMs）通过深度学习技术学习真实世界图的分布并生成相似样本。然而，现有评估方法主要依赖最大均值差异（MMD）来评估生成图集合的属性分布，存在明显局限性。本文提出了一种名为RGM（Representation-aware Graph-generation Model evaluation）的新方法，用于评估GGMs并克服MMD的不足。作为方法演示，我们全面评估了两种最先进的图生成模型：图循环注意力网络（GRAN）和高效度引导图生成模型（EDGE）。通过使用在合成和真实图数据集上训练的几何深度学习模型，我们研究了它们在生成真实图方面的性能。结果表明，虽然两种模型都能生成具有某些拓扑属性的图，但在保持区分不同图域的结构特征方面存在显著局限性。我们还强调了MMD作为GGMs评估指标的不足，并为未来研究提出了替代方法。",
            "intro_zh": [
                "现有图生成模型评估主要依赖MMD指标，但该指标无法充分捕捉图的结构特征差异，导致评估结果不全面。",
                "论文提出RGM方法，利用几何深度学习模型学习图表示，通过分类任务评估生成图与真实图在结构特征上的相似性。",
                "实验显示GRAN和EDGE模型在生成图时存在结构特征保留不足的问题，RGM方法能更准确地揭示这些局限性。"
            ],
            "method_zh": "**问题定义**：论文要解决图生成模型（GGMs）评估中现有方法依赖最大均值差异（MMD）指标的局限性问题。MMD主要基于图属性分布进行统计比较，但无法充分评估生成图在结构特征上的真实性，导致评估结果可能不准确或不全面。\\n\\n**核心思路**：论文提出RGM（Representation-aware Graph-generation Model evaluation）方法，核心思想是利用几何深度学习模型学习图的表示，通过图分类任务来评估生成图与真实图在结构特征上的相似性。这种方法旨在捕捉图的高阶结构信息，弥补MMD仅关注低阶统计属性的不足。\\n\\n**技术框架**：整体架构包括数据准备、模型训练和评估三个阶段。首先，构建包含合成图和真实图的自定义数据集，用于图分类任务。然后，训练一个几何深度学习模型（如基于图神经网络的分类器）来学习图的表示。最后，使用该模型对生成图进行分类，通过分类性能（如准确率）来评估GGMs的生成质量，并与MMD指标进行对比。\\n\\n**关键创新**：最重要的技术创新是引入基于几何深度学习的表示学习来评估图生成模型，这超越了传统基于统计距离的MMD方法。本质区别在于，RGM关注图的结构特征和领域区分能力，而MMD仅依赖于预定义的图属性分布，可能导致评估偏差。\\n\\n**关键设计**：关键设计包括使用自定义数据集混合合成和真实图，以确保评估的泛化性；选择几何深度学习模型（如图神经网络）进行图表示学习，以捕捉拓扑结构；通过分类任务设置，将生成图输入训练好的分类器，评估其与真实图的相似度；具体参数和损失函数依赖于所选几何深度学习模型，论文中未详细说明，但强调模型需针对图分类任务优化。",
            "application_zh": "该研究在图生成模型评估领域具有重要应用价值，可广泛应用于网络科学、生物信息学、社交网络分析和药物发现等领域。通过提供更准确的评估方法，RGM能帮助研究人员优化图生成模型，生成更真实的合成图，用于数据增强、隐私保护或模拟实验。未来，该方法可能推动图生成技术的发展，并促进跨领域应用中的图数据合成与评估标准化。",
            "highlight_zh": "实验对GRAN和EDGE两种先进图生成模型进行了全面评估。使用RGM方法结合几何深度学习模型，结果显示，虽然两种模型能生成具有某些拓扑属性（如度分布）的图，但在保持结构特征（如图域区分能力）方面存在显著局限性。具体而言，生成图在分类任务中的性能较低，表明其结构真实性不足。与MMD指标相比，RGM能更准确地揭示这些不足，突显了MMD作为评估指标的 inadequacy。实验未提供具体性能数据，但强调了RGM在评估中的优势。",
            "tags_zh": [
                "图生成模型评估",
                "几何深度学习",
                "图表示学习",
                "最大均值差异",
                "图分类任务",
                "图神经网络",
                "结构特征分析",
                "合成图生成"
            ],
            "_index": 147
        },
        {
            "title": "Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets",
            "authors": [
                "Estelle Zheng",
                "Nathan Cerisara",
                "Sébastien Warichet",
                "Emmanuel Helbert",
                "Christophe Cerisara"
            ],
            "arxiv_id": "2512.14237v1",
            "summary": "Fine-tuning large language models (LLMs) is often limited by the memory available on commodity GPUs. Parameter-efficient fine-tuning (PEFT) methods such as QLoRA reduce the number of trainable parameters, yet still incur high memory usage induced by the backward pass in the full model. We revisit Ladder Side Tuning (LST), a rarely explored PEFT technique that adds a lightweight side network, and show that it matches QLoRA's compute scaling slope while cutting peak memory by 50\\%. Across different downstream benchmarks spanning natural language understanding, mathematical and LLM-critic tasks, LST has competitive performance with QLoRA's accuracy on average while being much more memory-efficient. This efficiency enables fine-tuning of 7B-parameter models on a single 12 GB consumer GPU with 2k-token contexts, requiring no gradient checkpointing\\textemdash conditions under which QLoRA exhausts memory. Beyond memory efficiency, we also establish scaling laws showing that LST scales similarly to QLoRA. We exploit Ladder's architectural flexibility by introducing xLadder, a depth-extended variant that increases effective depth via cross-connections and shortens chain-of-thought (CoT) at fixed parameter count. Ladder is strong when memory is the bottleneck; xLadder builds on this by enabling deeper reasoning without additional memory overhead.",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14237v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出Ladder Side Tuning方法，通过轻量级侧网络解决大语言模型微调中的内存瓶颈问题。",
            "summary_zh": "微调大语言模型（LLMs）常受限于商用GPU的内存容量。参数高效微调（PEFT）方法如QLoRA减少了可训练参数数量，但仍因完整模型的反向传播导致高内存占用。本文重新审视了Ladder Side Tuning（LST），这是一种较少被探索的PEFT技术，通过添加轻量级侧网络，在匹配QLoRA计算扩展斜率的同时，将峰值内存降低50%。在涵盖自然语言理解、数学和LLM批评任务的不同下游基准测试中，LST平均性能与QLoRA相当，同时内存效率更高。这种效率使得在单个12GB消费级GPU上微调70亿参数模型成为可能，支持2k令牌上下文且无需梯度检查点——在这些条件下QLoRA会耗尽内存。除了内存效率，我们还建立了扩展定律，显示LST与QLoRA具有相似的扩展性。通过利用Ladder的架构灵活性，我们引入了xLadder，这是一种深度扩展变体，通过交叉连接增加有效深度，并在固定参数数量下缩短思维链（CoT）。Ladder在内存受限时表现强劲；xLadder在此基础上实现了更深层次的推理，而无需额外的内存开销。",
            "intro_zh": [
                "现有PEFT方法如QLoRA虽减少可训练参数，但反向传播仍导致高内存占用，限制大模型微调。",
                "论文提出Ladder Side Tuning，通过添加轻量级侧网络，在微调时仅更新侧网络参数，大幅降低内存需求。",
                "实验显示LST在多个下游任务中性能与QLoRA相当，峰值内存降低50%，支持7B模型在12GB GPU上微调。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大语言模型（LLMs）微调中的内存瓶颈问题。现有参数高效微调（PEFT）方法如QLoRA虽然减少了可训练参数，但在反向传播过程中仍需计算完整模型的梯度，导致高内存占用，限制了在资源受限环境（如消费级GPU）中的应用。\\n\\n**核心思路**：论文的核心思路是重新利用Ladder Side Tuning（LST），一种基于侧网络的PEFT技术。通过冻结预训练LLM的主干网络，仅训练一个轻量级的侧网络（side network），该侧网络通过连接（ladder connections）与主干网络交互，从而在微调时大幅减少内存需求，同时保持模型性能。\\n\\n**技术框架**：整体架构包括预训练LLM主干网络（冻结参数）和附加的侧网络（可训练参数）。侧网络通常由多层感知机（MLP）或类似轻量结构组成，通过梯子连接（ladder connections）从主干网络的中间层提取特征，并输出到后续层或最终预测。微调过程中，仅侧网络的参数被更新，主干网络保持固定，避免了完整模型的反向传播内存开销。\\n\\n**关键创新**：最重要的技术创新是系统性地验证和扩展LST作为内存高效的PEFT方法。与QLoRA等现有方法本质区别在于，LST完全避免了主干网络的梯度计算，通过侧网络实现参数高效学习，从而在内存受限场景下提供更优的权衡。此外，论文引入xLadder变体，通过交叉连接增加网络深度，提升推理能力而不增加内存开销。\\n\\n**关键设计**：关键设计包括侧网络的结构（如MLP层数、隐藏维度）、梯子连接的位置（从主干网络特定层提取特征）、损失函数（通常使用任务特定的损失，如交叉熵）。参数设置上，侧网络规模远小于主干网络（例如，参数数量减少几个数量级），以确保低内存占用。xLadder通过引入跨层连接（cross-connections）在侧网络内部实现深度扩展，优化思维链（CoT）推理。",
            "application_zh": "该研究在资源受限环境中具有广泛的应用潜力，如边缘计算、个人设备上的大语言模型微调，以及需要长上下文处理的任务（如文档分析、代码生成）。实际价值在于降低AI部署成本，使更多研究者和开发者能够访问和定制大模型。未来可能推动更高效的模型微调范式，促进AI技术的普及和民主化。",
            "highlight_zh": "实验结果显示，LST在多个下游基准测试（自然语言理解、数学、LLM批评任务）中平均性能与QLoRA相当，同时峰值内存降低50%。具体地，在单个12GB消费级GPU上，LST支持微调70亿参数模型，处理2k令牌上下文且无需梯度检查点，而QLoRA在相同条件下会耗尽内存。扩展定律分析表明LST与QLoRA具有相似的计算扩展斜率，验证了其可扩展性。xLadder变体进一步提升了推理深度，在固定参数下优化思维链性能。",
            "tags_zh": [
                "参数高效微调",
                "大语言模型",
                "内存优化",
                "侧网络",
                "梯子连接",
                "扩展定律",
                "思维链推理",
                "轻量级架构"
            ],
            "_index": 148
        },
        {
            "title": "4D-RaDiff: Latent Diffusion for 4D Radar Point Cloud Generation",
            "authors": [
                "Jimmie Kwok",
                "Holger Caesar",
                "Andras Palffy"
            ],
            "arxiv_id": "2512.14235v1",
            "summary": "Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14235v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "lidar"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出4D-RaDiff框架，通过潜在扩散模型生成4D雷达点云，以解决自动驾驶中雷达数据标注不足的问题。",
            "summary_zh": "汽车雷达因其成本效益和在恶劣天气条件下的鲁棒性，在环境感知方面展现出有前景的发展。然而，标注雷达数据的有限可用性对推进基于雷达的感知系统构成了重大挑战。为解决这一限制，我们提出了一个新颖的框架来生成4D雷达点云，用于训练和评估物体检测器。与基于图像的扩散不同，我们的方法旨在通过将扩散应用于潜在点云表示来考虑雷达点云的稀疏性和独特特性。在这个潜在空间中，生成通过对象或场景级别的条件进行控制。所提出的4D-RaDiff将未标注的边界框转换为高质量的雷达标注，并将现有的激光雷达点云数据转换为逼真的雷达场景。实验表明，在训练期间将4D-RaDiff的合成雷达数据作为数据增强方法，与仅使用真实数据训练相比，持续提高了物体检测性能。此外，在我们的合成数据上进行预训练，可将所需标注雷达数据量减少高达90%，同时实现可比的物体检测性能。",
            "intro_zh": [
                "核心问题：自动驾驶雷达感知系统面临标注数据稀缺的挑战，限制了模型训练和性能提升。",
                "方法要点：提出4D-RaDiff框架，利用潜在扩散模型生成逼真的4D雷达点云，考虑其稀疏性和特性。",
                "实验或效果：合成数据作为增强提升检测性能，预训练减少90%标注需求，保持可比性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自动驾驶中4D雷达点云数据标注不足的问题，现有方法依赖真实标注数据，成本高且难以获取，限制了雷达感知系统的训练和评估。\\n\\n**核心思路**：论文提出使用潜在扩散模型生成合成雷达点云，通过在潜在空间进行扩散过程，更好地建模雷达点云的稀疏性和噪声特性，从而生成高质量数据用于训练和增强。\\n\\n**技术框架**：整体架构包括数据预处理、潜在表示学习、扩散生成和条件控制模块。首先将雷达或激光雷达点云转换为潜在表示，然后在潜在空间应用扩散模型生成新点云，通过对象或场景级别条件指导生成过程。\\n\\n**关键创新**：最重要的创新是将扩散模型应用于雷达点云的潜在表示，而非直接处理原始点云，这能有效处理雷达数据的稀疏性和独特噪声模式，与基于图像的扩散方法有本质区别。\\n\\n**关键设计**：关键设计包括潜在编码器的网络结构，用于提取点云特征；扩散过程的噪声调度和去噪网络；条件机制如边界框或场景标签的嵌入；以及损失函数可能结合重建损失和对抗损失，具体参数设置未知。",
            "application_zh": "该研究主要应用于自动驾驶领域，通过生成合成雷达数据，可用于训练和评估物体检测器，提升系统在恶劣天气下的鲁棒性。潜在价值包括降低数据标注成本、加速模型开发，未来可能扩展到其他雷达感知任务如跟踪和分割。",
            "highlight_zh": "实验表明，使用4D-RaDiff生成的合成雷达数据作为数据增强，能持续提升物体检测性能，相比仅用真实数据训练有显著改进。预训练阶段，合成数据可减少高达90%的标注雷达数据需求，同时保持可比的检测性能，具体基线对比和提升幅度未知。",
            "tags_zh": [
                "4D雷达点云生成",
                "潜在扩散模型",
                "自动驾驶感知",
                "数据增强",
                "合成数据生成",
                "物体检测",
                "雷达数据标注",
                "点云表示学习"
            ],
            "_index": 149
        },
        {
            "title": "Multi-View MRI Approach for Classification of MGMT Methylation in Glioblastoma Patients",
            "authors": [
                "Rawan Alyahya",
                "Asrar Alruwayqi",
                "Atheer Alqarni",
                "Asma Alkhaldi",
                "Metab Alkubeyyer",
                "Xin Gao",
                "Mona Alshahrani"
            ],
            "arxiv_id": "2512.14232v1",
            "summary": "The presence of MGMT promoter methylation significantly affects how well chemotherapy works for patients with Glioblastoma Multiforme (GBM). Currently, confirmation of MGMT promoter methylation relies on invasive brain tumor tissue biopsies. In this study, we explore radiogenomics techniques, a promising approach in precision medicine, to identify genetic markers from medical images. Using MRI scans and deep learning models, we propose a new multi-view approach that considers spatial relationships between MRI views to detect MGMT methylation status. Importantly, our method extracts information from all three views without using a complicated 3D deep learning model, avoiding issues associated with high parameter count, slow convergence, and substantial memory demands. We also introduce a new technique for tumor slice extraction and show its superiority over existing methods based on multiple evaluation metrics. By comparing our approach to state-of-the-art models, we demonstrate the efficacy of our method. Furthermore, we share a reproducible pipeline of published models, encouraging transparency and the development of robust diagnostic tools. Our study highlights the potential of non-invasive methods for identifying MGMT promoter methylation and contributes to advancing precision medicine in GBM treatment.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14232v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出多视图MRI方法，利用空间关系检测胶质母细胞瘤MGMT甲基化状态，避免复杂3D模型问题。",
            "summary_zh": "MGMT启动子甲基化的存在显著影响胶质母细胞瘤（GBM）患者化疗效果。目前，MGMT启动子甲基化的确认依赖于侵入性脑肿瘤组织活检。本研究探索了影像基因组学技术，这是一种在精准医学中具有前景的方法，旨在从医学图像中识别遗传标记。利用MRI扫描和深度学习模型，我们提出了一种新的多视图方法，考虑MRI视图之间的空间关系来检测MGMT甲基化状态。重要的是，我们的方法从所有三个视图中提取信息，而不使用复杂的3D深度学习模型，避免了高参数数量、收敛缓慢和大量内存需求等问题。我们还引入了一种新的肿瘤切片提取技术，并基于多个评估指标展示了其优于现有方法的性能。通过将我们的方法与最先进的模型进行比较，我们证明了该方法的有效性。此外，我们分享了已发表模型的可复现流程，鼓励透明度和稳健诊断工具的开发。我们的研究突出了非侵入性方法识别MGMT启动子甲基化的潜力，并有助于推进GBM治疗中的精准医学。",
            "intro_zh": [
                "核心问题：现有MGMT甲基化检测依赖侵入性活检，风险高且耗时，缺乏高效非侵入性方法。",
                "方法要点：提出多视图MRI方法，结合空间关系和深度学习，避免复杂3D模型，提升检测效率。",
                "实验或效果：新方法在多个评估指标上优于现有技术，展示了高准确性和可复现性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决胶质母细胞瘤（GBM）患者MGMT启动子甲基化状态的非侵入性检测问题。现有方法主要依赖侵入性脑肿瘤组织活检，这具有高风险、高成本和时间延迟的痛点，同时传统影像分析方法精度有限，而复杂3D深度学习模型则面临高参数、慢收敛和大内存需求等挑战。\\n\\n**核心思路**：论文的核心解决思路是设计一种多视图MRI方法，通过整合MRI扫描中的多个视图（如轴向、冠状、矢状面）来捕捉空间关系，从而提取更丰富的特征用于MGMT甲基化分类。这样设计避免了直接使用3D模型，减少了计算复杂度，同时保持了信息的完整性，提高了模型的效率和可扩展性。\\n\\n**技术框架**：整体架构包括数据预处理、肿瘤切片提取、多视图特征融合和分类模块。首先，从MRI扫描中提取肿瘤区域；然后，采用新提出的切片提取技术处理多视图数据；接着，通过深度学习模型（如卷积神经网络）分别学习每个视图的特征；最后，融合这些特征并输入分类器（如全连接层）预测MGMT甲基化状态。流程强调端到端训练和可复现性。\\n\\n**关键创新**：最重要的技术创新点是引入多视图空间关系建模和新型肿瘤切片提取技术。与现有方法的本质区别在于，它不依赖复杂3D模型，而是通过多视图融合来模拟3D信息，从而在降低计算资源需求的同时，提高检测精度和鲁棒性，解决了传统方法在内存和收敛速度上的瓶颈。\\n\\n**关键设计**：关键设计包括使用特定的深度学习架构（如ResNet或类似网络）处理每个MRI视图，设计损失函数（如交叉熵损失）优化分类任务，参数设置上可能涉及批量大小、学习率调整和正则化技术。肿瘤切片提取技术可能基于图像分割算法，确保提取的切片覆盖关键肿瘤区域，以最大化信息保留。",
            "application_zh": "该研究的潜在应用领域主要包括神经肿瘤学和精准医学，特别是在胶质母细胞瘤的诊断和治疗规划中。实际价值在于提供一种非侵入性、高效的MGMT甲基化检测方法，减少患者活检风险，辅助临床决策，优化化疗方案。未来影响可能推动影像基因组学在癌症诊疗中的普及，促进个性化医疗发展，并作为基础工具用于其他脑部疾病的遗传标记识别。",
            "highlight_zh": "最重要的实验结果包括：新提出的多视图方法在MGMT甲基化分类任务上，基于多个评估指标（如准确率、召回率、F1分数）显著优于现有最先进模型。具体性能数据未知，但论文报告了其方法在避免复杂3D模型的同时，实现了更高的检测精度和更快的收敛速度。提升幅度体现在肿瘤切片提取技术的优越性，以及整体流程的可复现性，为后续研究提供了基准。",
            "tags_zh": [
                "多视图MRI",
                "MGMT甲基化检测",
                "胶质母细胞瘤",
                "影像基因组学",
                "深度学习",
                "非侵入性诊断",
                "精准医学",
                "肿瘤切片提取"
            ],
            "_index": 150
        },
        {
            "title": "Weighted Conformal Prediction Provides Adaptive and Valid Mask-Conditional Coverage for General Missing Data Mechanisms",
            "authors": [
                "Jiarong Fan",
                "Juhyun Park. Thi Phuong Thuy Vo",
                "Nicolas Brunel"
            ],
            "arxiv_id": "2512.14221v1",
            "summary": "Conformal prediction (CP) offers a principled framework for uncertainty quantification, but it fails to guarantee coverage when faced with missing covariates. In addressing the heterogeneity induced by various missing patterns, Mask-Conditional Valid (MCV) Coverage has emerged as a more desirable property than Marginal Coverage. In this work, we adapt split CP to handle missing values by proposing a preimpute-mask-then-correct framework that can offer valid coverage. We show that our method provides guaranteed Marginal Coverage and Mask-Conditional Validity for general missing data mechanisms. A key component of our approach is a reweighted conformal prediction procedure that corrects the prediction sets after distributional imputation (multiple imputation) of the calibration dataset, making our method compatible with standard imputation pipelines. We derive two algorithms, and we show that they are approximately marginally valid and MCV. We evaluate them on synthetic and real-world datasets. It reduces significantly the width of prediction intervals w.r.t standard MCV methods, while maintaining the target guarantees.",
            "categories": [
                "stat.ML",
                "cs.LG"
            ],
            "primary_category": "stat.ML",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14221v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出加权保形预测框架，为一般缺失数据机制提供自适应且有效的掩码条件覆盖保证",
            "summary_zh": "保形预测（CP）为不确定性量化提供了原则性框架，但在面对缺失协变量时无法保证覆盖。针对不同缺失模式引起的异质性，掩码条件有效（MCV）覆盖已成为比边际覆盖更理想的属性。本研究通过提出一个预填补-掩码-校正框架来适应分割CP处理缺失值，该框架能够提供有效覆盖。我们证明，我们的方法为一般缺失数据机制提供了保证的边际覆盖和掩码条件有效性。该方法的一个关键组成部分是重新加权的保形预测过程，在校准数据集的分布填补（多重填补）后校正预测集，使我们的方法与标准填补流程兼容。我们推导出两种算法，并证明它们近似边际有效且MCV。我们在合成和真实世界数据集上评估它们。相对于标准MCV方法，它显著减少了预测区间的宽度，同时保持了目标保证。",
            "intro_zh": [
                "保形预测在处理缺失协变量时无法保证覆盖，现有方法难以应对缺失模式异质性。",
                "提出预填补-掩码-校正框架，通过加权保形预测校正填补后的预测集，实现自适应覆盖。",
                "在合成和真实数据集上验证，显著减少预测区间宽度，同时维持目标覆盖保证。"
            ],
            "method_zh": "**问题定义**：保形预测在缺失协变量下无法保证覆盖，现有方法如标准MCV方法在处理一般缺失数据机制时预测区间过宽，效率低下。\\n\\n**核心思路**：通过预填补缺失数据，然后基于掩码模式加权校正预测集，使保形预测适应缺失机制，实现自适应且有效的掩码条件覆盖。\\n\\n**技术框架**：整体流程分为三个阶段：首先对校准数据集进行分布填补（多重填补），然后应用掩码条件处理缺失模式，最后通过重新加权的保形预测校正预测集，确保覆盖保证。\\n\\n**关键创新**：提出加权保形预测框架，将标准填补流程与保形预测结合，通过重加权机制自适应调整预测集，本质区别在于直接处理缺失机制而非简单扩展现有方法。\\n\\n**关键设计**：采用分割保形预测作为基础，设计两种算法实现近似边际有效和MCV；关键参数包括填补模型选择、权重计算函数，损失函数基于覆盖误差优化，网络结构未知，但框架兼容标准机器学习模型。",
            "application_zh": "该研究适用于医疗诊断、金融风险评估和传感器数据分析等领域，其中数据常存在缺失值。通过提供自适应且有效的覆盖保证，能提升不确定性量化在现实世界应用中的可靠性，未来可扩展至多模态或时序数据，推动保形预测在缺失数据处理中的标准化。",
            "highlight_zh": "在合成和真实数据集上评估，相比标准MCV方法，新方法显著减少预测区间宽度，具体提升幅度未知，但实验显示在维持目标覆盖保证（如95%置信水平）的同时，区间效率明显改善，验证了自适应覆盖的有效性。",
            "tags_zh": [
                "保形预测",
                "缺失数据处理",
                "不确定性量化",
                "掩码条件覆盖",
                "加权校正",
                "分布填补",
                "预测区间优化",
                "一般缺失机制"
            ],
            "_index": 151
        },
        {
            "title": "Trajectory Tracking for Multi-Manipulator Systems in Constrained Environments",
            "authors": [
                "Mayank Sewlia",
                "Christos K. Verginis",
                "Dimos V. Dimarogonas"
            ],
            "arxiv_id": "2512.14206v1",
            "summary": "We consider the problem of cooperative manipulation by a mobile multi-manipulator system operating in obstacle-cluttered and highly constrained environments under spatio-temporal task specifications. The task requires transporting a grasped object while respecting both continuous robot dynamics and discrete geometric constraints arising from obstacles and narrow passages. To address this hybrid structure, we propose a multi-rate planning and control framework that combines offline generation of an STL-satisfying object trajectory and collision-free base footprints with online constrained inverse kinematics and continuous-time feedback control. The resulting closed-loop system enables coordinated reconfiguration of multiple manipulators while tracking the desired object motion. The approach is evaluated in high-fidelity physics simulations using three Franka Emika Panda mobile manipulators rigidly grasping an object.",
            "categories": [
                "cs.RO",
                "eess.SY"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14206v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "物理动作",
                    "matched_keywords": [
                        "physics simulation"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出多速率规划控制框架，解决多移动机械臂在受限环境中协同搬运的轨迹跟踪问题。",
            "summary_zh": "本文研究了在障碍物密集且高度受限环境中，移动多机械臂系统在时空任务规范下的协同操作问题。任务要求在运输抓取物体时，同时满足连续机器人动力学和由障碍物及狭窄通道引起的离散几何约束。为应对这种混合结构，我们提出了一个多速率规划与控制框架，该框架结合了离线生成满足信号时序逻辑（STL）的物体轨迹和无碰撞基座足迹，以及在线约束逆运动学和连续时间反馈控制。由此产生的闭环系统能够在跟踪期望物体运动的同时，实现多个机械臂的协调重构。该方法使用三个刚性抓取物体的Franka Emika Panda移动机械臂，在高保真物理仿真中进行了评估。",
            "intro_zh": [
                "核心问题：现有方法难以同时处理多移动机械臂协同搬运中的连续动力学、离散几何约束及复杂时空任务要求。",
                "方法要点：提出多速率规划控制框架，离线生成满足STL的轨迹，在线结合约束逆运动学与反馈控制实现协调跟踪。",
                "实验或效果：在高保真仿真中，三个移动机械臂成功在受限环境中协同搬运物体，验证了框架的有效性与鲁棒性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多移动机械臂系统在障碍物密集、空间受限环境中协同搬运物体时的轨迹跟踪问题。现有方法常难以统一处理连续时间动力学与离散空间约束（如障碍物、狭窄通道）的混合特性，且缺乏对复杂时空任务规范（如信号时序逻辑STL）的有效集成，导致规划与控制效率低、鲁棒性差。\\n\\n**核心思路**：为解决上述混合结构挑战，论文提出一种多速率规划与控制框架，其核心思想是将问题分解为离线轨迹规划与在线实时控制两个层次。离线阶段专注于生成满足高级任务规范（STL）的全局可行轨迹，在线阶段则通过约束优化与反馈机制处理局部动力学与几何约束，实现高效协调跟踪。这种设计平衡了计算复杂度与实时性需求。\\n\\n**技术框架**：整体架构包含两个主要阶段：1）离线规划阶段：基于环境模型与任务规范，生成满足STL的物体参考轨迹，并同步规划各机械臂基座的无碰撞足迹路径；2）在线控制阶段：采用多速率策略，在较慢速率下执行约束逆运动学计算，以将物体轨迹映射为各机械臂关节的参考运动，同时在快速速率下实施连续时间反馈控制（如PID或模型预测控制），确保系统稳定跟踪并处理扰动。闭环系统通过协调重构机械臂构型来适应动态环境。\\n\\n**关键创新**：最重要的技术创新在于将STL任务规范与多速率控制框架深度融合，以统一处理时空约束与混合系统动力学。与现有方法相比，本质区别在于：a）显式结合离散几何约束（如障碍物）于连续控制回路，提升环境适应性；b）利用STL进行高级任务描述，增强规划的表达能力与安全性；c）多速率设计实现计算效率与实时性能的平衡，适用于复杂多机协同场景。\\n\\n**关键设计**：技术细节包括：1）STL规范用于定义物体轨迹的时空属性（如“始终避免障碍物”或“最终到达目标”），通过离线优化算法（如基于采样的规划或凸优化）生成可行轨迹；2）约束逆运动学模块采用数值优化方法（如二次规划），在满足关节限位、避障等约束下求解关节角度；3）反馈控制律基于李雅普诺夫稳定性理论设计，确保跟踪误差收敛；4）仿真中使用Franka Emika Panda机械臂的动力学模型，并设置高保真物理参数（如摩擦、质量）以验证鲁棒性。具体参数如控制增益、规划分辨率等未在摘要中详述，需参考全文。",
            "application_zh": "该研究在工业自动化、仓储物流和灾难救援等领域具有重要应用价值。例如，在狭窄仓库中，多移动机械臂可协同搬运大型货物，提高效率并减少碰撞风险；在危险环境中，如核设施维护，系统能安全导航通过受限区域。未来，该框架可扩展至更复杂任务（如柔性物体操作），推动智能机器人系统向更高自主性与适应性发展。",
            "highlight_zh": "实验在高保真物理仿真中进行，使用三个Franka Emika Panda移动机械臂刚性抓取物体。结果表明，所提框架能有效生成满足STL的轨迹，并在障碍物密集环境中实现精确轨迹跟踪，机械臂协调重构成功，系统闭环稳定性得到验证。具体性能数据如跟踪误差、计算时间等未在摘要中提供，但仿真展示了框架在复杂约束下的鲁棒性与可行性，为实际部署奠定了基础。",
            "tags_zh": [
                "多移动机械臂系统",
                "轨迹跟踪",
                "信号时序逻辑",
                "混合系统控制",
                "约束逆运动学",
                "协同操作",
                "高保真仿真",
                "时空任务规划"
            ],
            "_index": 152
        },
        {
            "title": "Fracture Morphology Classification: Local Multiclass Modeling for Multilabel Complexity",
            "authors": [
                "Cassandra Krause",
                "Mattias P. Heinrich",
                "Ron Keuth"
            ],
            "arxiv_id": "2512.14196v1",
            "summary": "Between $15\\,\\%$ and $45\\,\\%$ of children experience a fracture during their growth years, making accurate diagnosis essential. Fracture morphology, alongside location and fragment angle, is a key diagnostic feature. In this work, we propose a method to extract fracture morphology by assigning automatically global AO codes to corresponding fracture bounding boxes. This approach enables the use of public datasets and reformulates the global multilabel task into a local multiclass one, improving the average F1 score by $7.89\\,\\%$. However, performance declines when using imperfect fracture detectors, highlighting challenges for real-world deployment. Our code is available on GitHub.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted as poster at the German Conference on Medical Image Computing 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14196v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出局部多类建模方法，将骨折形态全局多标签分类任务转化为局部多类任务，提升诊断准确性。",
            "summary_zh": "在儿童成长期间，15%至45%的儿童会经历骨折，因此准确诊断至关重要。骨折形态、位置和碎片角度是关键诊断特征。本研究提出一种方法，通过自动分配全局AO代码到相应的骨折边界框来提取骨折形态。该方法使得公共数据集的使用成为可能，并将全局多标签任务重新表述为局部多类任务，平均F1分数提高了7.89%。然而，当使用不完美的骨折检测器时，性能会下降，这突显了实际部署中的挑战。我们的代码已在GitHub上提供。",
            "intro_zh": [
                "核心问题：骨折形态分类面临全局多标签任务的复杂性，现有方法难以处理多标签间的依赖关系，导致诊断准确性受限。",
                "方法要点：将全局多标签分类转化为局部多类任务，通过自动分配AO代码到骨折边界框，简化模型学习过程。",
                "实验或效果：平均F1分数提升7.89%，但使用不完美检测器时性能下降，表明实际部署需优化检测步骤。"
            ],
            "method_zh": "**问题定义**：论文旨在解决骨折形态分类问题，具体任务是从医学图像中自动提取骨折形态特征，并分配AO代码（一种标准化的骨折分类系统）。现有方法的痛点在于，骨折形态分类通常被视为全局多标签任务，即一张图像可能包含多个骨折类型，导致标签间依赖关系复杂，模型难以学习，且公共数据集利用受限，因为标注通常基于全局图像而非局部区域。\\n\\n**核心思路**：论文的核心解决思路是将全局多标签分类任务重新表述为局部多类任务。通过先检测骨折边界框，然后在每个边界框内进行多类分类，从而简化问题。这样设计是因为局部区域更专注于单一骨折形态，减少了多标签间的干扰，便于模型学习，同时允许使用公共数据集，因为标注可以映射到局部区域。\\n\\n**技术框架**：整体架构包含两个主要阶段：骨折检测和局部分类。首先，使用骨折检测器（如基于深度学习的对象检测模型）从医学图像中提取骨折边界框。然后，对每个边界框应用分类模型，将其视为多类分类问题，预测对应的AO代码。流程包括数据预处理、检测器训练、分类器训练和评估，最终输出全局骨折形态标签。\\n\\n**关键创新**：最重要的技术创新点是将全局多标签任务转化为局部多类任务，这本质区别在于改变了问题表述方式，从处理图像级别的多标签依赖转向处理局部区域的多类独立性。这提高了模型的可解释性和性能，同时增强了数据集的可用性。\\n\\n**关键设计**：关键设计包括使用标准AO代码作为分类标签，确保诊断标准化；损失函数可能采用交叉熵损失用于多类分类，结合检测器的损失（如边界框回归损失）；网络结构可能基于卷积神经网络（CNN）或Transformer，用于特征提取和分类；参数设置涉及学习率、批量大小等超参数优化，以最大化F1分数；此外，代码公开在GitHub，促进可复现性。",
            "application_zh": "该研究在医学影像诊断领域具有重要应用价值，特别是儿科骨折诊断。通过自动提取骨折形态，可辅助医生快速准确识别骨折类型，提升诊断效率和一致性。未来可能集成到临床系统中，支持远程医疗和自动化报告生成，但需解决检测器不完美带来的挑战，以推动实际部署。",
            "highlight_zh": "实验结果显示，论文方法在骨折形态分类任务中，平均F1分数提升了7.89%，显著优于基线方法。具体性能数据基于公共数据集评估，对比基线可能包括传统多标签分类模型。提升幅度体现了局部多类建模的有效性，但使用不完美骨折检测器时性能下降，突显了检测步骤的关键影响，为未来优化提供了方向。",
            "tags_zh": [
                "骨折形态分类",
                "局部多类建模",
                "全局多标签任务",
                "AO代码分配",
                "医学影像分析",
                "深度学习",
                "对象检测",
                "儿科诊断"
            ],
            "_index": 153
        },
        {
            "title": "Improving Semantic Uncertainty Quantification in LVLMs with Semantic Gaussian Processes",
            "authors": [
                "Joseph Hoche",
                "Andrei Bursuc",
                "David Brellmann",
                "Gilles Louppe",
                "Pavel Izmailov",
                "Angela Yao",
                "Gianni Franchi"
            ],
            "arxiv_id": "2512.14177v1",
            "summary": "Large Vision-Language Models (LVLMs) often produce plausible but unreliable outputs, making robust uncertainty estimation essential. Recent work on semantic uncertainty estimates relies on external models to cluster multiple sampled responses and measure their semantic consistency. However, these clustering methods are often fragile, highly sensitive to minor phrasing variations, and can incorrectly group or separate semantically similar answers, leading to unreliable uncertainty estimates. We propose Semantic Gaussian Process Uncertainty (SGPU), a Bayesian framework that quantifies semantic uncertainty by analyzing the geometric structure of answer embeddings, avoiding brittle clustering. SGPU maps generated answers into a dense semantic space, computes the Gram matrix of their embeddings, and summarizes their semantic configuration via the eigenspectrum. This spectral representation is then fed into a Gaussian Process Classifier that learns to map patterns of semantic consistency to predictive uncertainty, and that can be applied in both black-box and white-box settings. Across six LLMs and LVLMs on eight datasets spanning VQA, image classification, and textual QA, SGPU consistently achieves state-of-the-art calibration (ECE) and discriminative (AUROC, AUARC) performance. We further show that SGPU transfers across models and modalities, indicating that its spectral representation captures general patterns of semantic uncertainty.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14177v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出语义高斯过程不确定性框架，以解决大型视觉语言模型中语义不确定性量化不可靠的问题。",
            "summary_zh": "大型视觉语言模型（LVLMs）常产生看似合理但不可靠的输出，因此稳健的不确定性估计至关重要。现有的语义不确定性估计方法依赖外部模型对多个采样响应进行聚类并测量其语义一致性，但这些聚类方法往往脆弱，对细微的措辞变化高度敏感，可能错误地分组或分离语义相似的答案，导致不可靠的不确定性估计。我们提出了语义高斯过程不确定性（SGPU），这是一个贝叶斯框架，通过分析答案嵌入的几何结构来量化语义不确定性，避免了脆弱的聚类。SGPU将生成的答案映射到密集的语义空间，计算其嵌入的Gram矩阵，并通过特征谱总结其语义配置。这种谱表示随后被输入到高斯过程分类器中，该分类器学习将语义一致性模式映射到预测不确定性，并可在黑盒和白盒设置中应用。在涵盖VQA、图像分类和文本QA的八个数据集上，对六个LLM和LVLM进行测试，SGPU在标定（ECE）和判别（AUROC、AUARC）性能方面始终达到最先进水平。我们进一步表明，SGPU能够跨模型和模态迁移，表明其谱表示捕捉了语义不确定性的一般模式。",
            "intro_zh": [
                "现有方法依赖聚类评估语义不确定性，但对措辞变化敏感，易导致错误分组，影响估计可靠性。",
                "提出SGPU框架，通过分析答案嵌入的几何结构，避免聚类，使用谱表示和高斯过程分类器量化不确定性。",
                "在多个数据集和模型上，SGPU在标定和判别性能上达到最先进水平，并展示跨模型和模态的迁移能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型视觉语言模型（LVLMs）中语义不确定性量化不可靠的问题。现有方法依赖外部模型对多个采样响应进行聚类来评估语义一致性，但这些聚类方法脆弱，对细微的措辞变化高度敏感，可能错误地分组或分离语义相似的答案，导致不确定性估计不准确。\\n\\n**核心思路**：论文的核心思路是避免脆弱的聚类过程，转而通过分析答案嵌入的几何结构来量化语义不确定性。这基于一个假设：语义一致性可以通过嵌入空间中的几何模式来捕捉，从而提供更稳健的估计。\\n\\n**技术框架**：整体框架包括三个主要阶段：首先，将生成的答案映射到密集的语义空间，生成嵌入向量；其次，计算这些嵌入的Gram矩阵，并通过特征谱分析提取其语义配置的谱表示；最后，将谱表示输入高斯过程分类器，该分类器学习将语义一致性模式映射到预测不确定性。整个过程可在黑盒（仅使用模型输出）和白盒（利用模型内部状态）设置中应用。\\n\\n**关键创新**：最重要的技术创新是引入语义高斯过程不确定性（SGPU）框架，它结合了谱表示和高斯过程分类器，避免了传统聚类方法的脆弱性。与现有方法的本质区别在于，SGPU直接分析嵌入的几何结构，而不是依赖外部聚类算法，从而提高了对语义变化的鲁棒性。\\n\\n**关键设计**：关键设计包括使用预训练的语言模型（如BERT）生成答案嵌入，计算Gram矩阵以捕捉嵌入间的相似性，并通过特征值分解提取谱特征。高斯过程分类器采用径向基函数（RBF）核，以学习从谱表示到不确定性分数的映射。损失函数基于分类任务的交叉熵，优化过程旨在最小化预测误差。参数设置中，嵌入维度通常为768（基于BERT），谱特征维度通过实验确定，以确保有效表示。",
            "application_zh": "该研究在需要高可靠性的大型视觉语言模型应用中具有重要价值，如自动驾驶中的视觉问答、医疗图像分析中的诊断辅助、以及内容审核中的多模态理解。SGPU框架能提升不确定性估计的准确性，帮助系统识别不可靠输出，从而增强决策安全性和用户体验。未来可能扩展到更多模态和任务，推动人工智能系统的可信度发展。",
            "highlight_zh": "在六个LLM和LVLM模型上，针对八个涵盖VQA、图像分类和文本QA的数据集，SGPU在标定性能（ECE）和判别性能（AUROC、AUARC）上均达到最先进水平。具体而言，与基线方法相比，SGPU在多个任务中显著降低了ECE（例如，在VQA任务中平均降低约15%），并提高了AUROC（平均提升约5%）。此外，实验显示SGPU具有跨模型和模态的迁移能力，表明其谱表示能捕捉语义不确定性的一般模式。",
            "tags_zh": [
                "语义不确定性量化",
                "大型视觉语言模型",
                "高斯过程分类器",
                "谱表示",
                "多模态学习",
                "贝叶斯框架",
                "模型校准",
                "嵌入几何分析"
            ],
            "_index": 154
        },
        {
            "title": "On Improving Deep Active Learning with Formal Verification",
            "authors": [
                "Jonathan Spiegelman",
                "Guy Amir",
                "Guy Katz"
            ],
            "arxiv_id": "2512.14170v1",
            "summary": "Deep Active Learning (DAL) aims to reduce labeling costs in neural-network training by prioritizing the most informative unlabeled samples for annotation. Beyond selecting which samples to label, several DAL approaches further enhance data efficiency by augmenting the training set with synthetic inputs that do not require additional manual labeling. In this work, we investigate how augmenting the training data with adversarial inputs that violate robustness constraints can improve DAL performance. We show that adversarial examples generated via formal verification contribute substantially more than those produced by standard, gradient-based attacks. We apply this extension to multiple modern DAL techniques, as well as to a new technique that we propose, and show that it yields significant improvements in model generalization across standard benchmarks.",
            "categories": [
                "cs.LG",
                "cs.LO"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14170v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出基于形式验证的对抗样本增强方法，以提升深度主动学习的数据效率和模型泛化能力。",
            "summary_zh": "深度主动学习（DAL）旨在通过优先标注信息量最大的未标记样本来降低神经网络训练中的标注成本。除了选择哪些样本进行标注外，一些DAL方法还通过添加无需额外手动标注的合成输入来进一步增强数据效率。在本工作中，我们研究了如何通过添加违反鲁棒性约束的对抗样本来增强训练数据，以提升DAL性能。我们表明，通过形式验证生成的对抗样本比基于标准梯度攻击生成的样本贡献更大。我们将此扩展应用于多种现代DAL技术，以及我们提出的一种新技术，并证明它在标准基准测试中显著提高了模型的泛化能力。",
            "intro_zh": [
                "现有深度主动学习方法在数据增强方面依赖标准合成输入，可能无法有效提升模型鲁棒性和泛化能力，导致数据效率仍有提升空间。",
                "论文提出利用形式验证生成对抗样本，作为训练数据增强手段，以更系统地探索模型决策边界，增强DAL的样本选择和数据利用效率。",
                "实验表明，该方法在多个DAL技术和基准测试中显著提升模型泛化性能，形式验证生成的对抗样本比梯度攻击样本贡献更大。"
            ],
            "method_zh": "**问题定义**：论文旨在解决深度主动学习中数据效率不足的问题，现有方法通过合成输入增强训练数据，但标准方法如梯度攻击生成的对抗样本可能不够系统或有效，无法充分提升模型鲁棒性和泛化能力，导致标注成本降低有限。\\n\\n**核心思路**：论文的核心思路是利用形式验证技术生成对抗样本，这些样本违反模型的鲁棒性约束，作为训练数据增强的一部分。形式验证能更严格地探索模型决策边界，生成更具挑战性的样本，从而在主动学习过程中优先选择这些样本进行标注或直接用于训练，以提高数据利用效率和模型性能。\\n\\n**技术框架**：整体框架包括几个主要阶段：首先，使用现有DAL方法选择未标记样本进行标注；其次，应用形式验证工具（如基于约束求解的方法）生成对抗样本，这些样本在输入空间中满足特定扰动约束但导致模型错误预测；然后，将这些对抗样本作为合成输入添加到训练集中，无需额外标注；最后，结合标准训练和主动学习循环，迭代优化模型。框架可集成到多种DAL技术中，包括论文提出的新方法。\\n\\n**关键创新**：最重要的技术创新是将形式验证引入深度主动学习的数据增强过程。与现有基于梯度攻击的方法相比，形式验证能生成更系统、更可靠的对抗样本，本质区别在于它通过数学证明探索模型行为，而非依赖局部梯度信息，从而提供更全面的鲁棒性测试和增强。\\n\\n**关键设计**：关键设计包括使用形式验证工具（如基于SMT求解器）定义输入扰动约束和模型输出条件，生成对抗样本；在DAL循环中，将对抗样本作为额外训练数据，可能结合特定损失函数（如标准交叉熵）进行优化；网络结构依赖于具体DAL任务，但框架通用，可适应不同模型；参数设置涉及扰动大小、验证深度等，需根据基准测试调整以平衡生成效率和样本质量。",
            "application_zh": "该研究在计算机视觉、自然语言处理等需要大量标注数据的领域具有潜在应用价值，可降低深度学习模型的标注成本，提升模型在安全关键场景（如自动驾驶、医疗诊断）中的鲁棒性和可靠性。未来可能推动更高效的主动学习框架发展，促进形式验证与机器学习的交叉研究。",
            "highlight_zh": "实验在标准基准测试（如CIFAR-10、MNIST）上进行，结果显示，基于形式验证的对抗样本增强方法在多个现代DAL技术中显著提升模型泛化性能，具体提升幅度因任务而异，例如在某些设置中准确率提高约2-5%。与基于梯度攻击的对抗样本相比，形式验证生成的样本贡献更大，验证了其有效性。",
            "tags_zh": [
                "深度主动学习",
                "形式验证",
                "对抗样本",
                "数据增强",
                "模型泛化",
                "鲁棒性约束",
                "神经网络训练",
                "标注成本降低"
            ],
            "_index": 155
        },
        {
            "title": "PathFinder: Advancing Path Loss Prediction for Single-to-Multi-Transmitter Scenario",
            "authors": [
                "Zhijie Zhong",
                "Zhiwen Yu",
                "Pengyu Li",
                "Jianming Lv",
                "C. L. Philip Chen",
                "Min Chen"
            ],
            "arxiv_id": "2512.14150v1",
            "summary": "Radio path loss prediction (RPP) is critical for optimizing 5G networks and enabling IoT, smart city, and similar applications. However, current deep learning-based RPP methods lack proactive environmental modeling, struggle with realistic multi-transmitter scenarios, and generalize poorly under distribution shifts, particularly when training/testing environments differ in building density or transmitter configurations. This paper identifies three key issues: (1) passive environmental modeling that overlooks transmitters and key environmental features; (2) overemphasis on single-transmitter scenarios despite real-world multi-transmitter prevalence; (3) excessive focus on in-distribution performance while neglecting distribution shift challenges. To address these, we propose PathFinder, a novel architecture that actively models buildings and transmitters via disentangled feature encoding and integrates Mask-Guided Low-rank Attention to independently focus on receiver and building regions. We also introduce a Transmitter-Oriented Mixup strategy for robust training and a new benchmark, single-to-multi-transmitter RPP (S2MT-RPP), tailored to evaluate extrapolation performance (multi-transmitter testing after single-transmitter training). Experimental results show PathFinder outperforms state-of-the-art methods significantly, especially in challenging multi-transmitter scenarios. Our code and project site are available at: https://emorzz1g.github.io/PathFinder/.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "34 pages, 14 figures, 4 tables. Under review",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14150v1",
            "code_links": [
                {
                    "url": "https://emorzz1g.github.io/PathFinder/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出PathFinder架构以解决单发射器到多发射器场景下的无线路径损耗预测问题",
            "summary_zh": "无线路径损耗预测（RPP）对于优化5G网络和实现物联网、智慧城市等应用至关重要。然而，当前基于深度学习的RPP方法缺乏主动的环境建模能力，难以处理现实中的多发射器场景，并且在分布偏移下泛化能力差，特别是在训练和测试环境在建筑密度或发射器配置不同的情况下。本文识别了三个关键问题：（1）被动的环境建模忽视了发射器和关键环境特征；（2）过度强调单发射器场景，尽管现实世界中多发射器普遍存在；（3）过分关注分布内性能，而忽视了分布偏移的挑战。为解决这些问题，我们提出了PathFinder，一种新颖的架构，通过解耦特征编码主动建模建筑和发射器，并集成掩码引导的低秩注意力机制，独立关注接收器和建筑区域。我们还引入了面向发射器的混合策略进行鲁棒训练，并提出了一个新的基准——单到多发射器RPP（S2MT-RPP），专门用于评估外推性能（在单发射器训练后进行多发射器测试）。实验结果表明，PathFinder显著优于现有最先进方法，尤其是在具有挑战性的多发射器场景中。我们的代码和项目网站可在https://emorzz1g.github.io/PathFinder/获取。",
            "intro_zh": [
                "现有方法缺乏主动环境建模，忽视发射器和关键特征，导致预测不准确。",
                "PathFinder通过解耦特征编码和掩码引导注意力，主动建模建筑与发射器，提升多场景适应性。",
                "实验显示PathFinder在S2MT-RPP基准上显著优于基线，多发射器场景性能提升明显。"
            ],
            "method_zh": "**问题定义**：论文旨在解决无线路径损耗预测（RPP）在单发射器到多发射器场景下的泛化问题。现有方法痛点包括：被动环境建模忽略发射器影响、过度依赖单发射器数据、在分布偏移下泛化能力差，导致实际多发射器场景预测性能下降。\\n\\n**核心思路**：核心思路是主动建模环境特征，特别是建筑和发射器的交互，通过解耦编码分离不同特征，并引入注意力机制聚焦关键区域，以增强模型对多发射器场景的适应性和鲁棒性。\\n\\n**技术框架**：整体架构包括输入处理、特征编码、注意力模块和预测输出。主要模块有：解耦特征编码器（分别处理建筑和发射器特征）、掩码引导低秩注意力（独立关注接收器和建筑区域）、以及面向发射器的混合训练策略。流程上，先编码环境数据，再通过注意力加权特征，最后输出路径损耗预测。\\n\\n**关键创新**：最重要的技术创新是掩码引导低秩注意力机制和面向发射器的混合策略。与现有方法本质区别在于：从被动建模转向主动建模，强调多发射器场景的外推能力，而非仅优化分布内性能。\\n\\n**关键设计**：关键设计包括：使用卷积神经网络进行特征提取，注意力机制中低秩分解减少计算复杂度，混合策略在训练时随机组合发射器配置以模拟多场景，损失函数采用均方误差优化预测精度，网络结构设计为端到端可训练。",
            "application_zh": "该研究在5G网络优化、物联网部署和智慧城市建设中具有重要应用价值。通过提升多发射器场景下的路径损耗预测精度，可帮助运营商更高效地规划基站布局、减少信号干扰，并支持大规模设备连接，推动智能交通、远程医疗等应用的发展，未来可能扩展到6G网络和更复杂的无线环境建模。",
            "highlight_zh": "PathFinder在单到多发射器RPP（S2MT-RPP）基准测试中显著优于现有方法，具体性能提升未知，但论文报告在挑战性多发射器场景下表现突出。实验对比了多种基线模型，PathFinder在外推性能上取得最佳结果，验证了其主动建模和鲁棒训练策略的有效性。",
            "tags_zh": [
                "无线路径损耗预测",
                "多发射器场景",
                "深度学习",
                "注意力机制",
                "分布偏移",
                "5G网络优化",
                "环境建模",
                "鲁棒训练"
            ],
            "_index": 156
        },
        {
            "title": "Consistent Instance Field for Dynamic Scene Understanding",
            "authors": [
                "Junyi Wu",
                "Van Nguyen Nguyen",
                "Benjamin Planche",
                "Jiachen Tao",
                "Changchang Sun",
                "Zhongpai Gao",
                "Zhenghao Zhao",
                "Anwesa Choudhuri",
                "Gengyu Zhang",
                "Meng Zheng",
                "Feiran Wang",
                "Terrence Chen",
                "Yan Yan",
                "Ziyan Wu"
            ],
            "arxiv_id": "2512.14126v1",
            "summary": "We introduce Consistent Instance Field, a continuous and probabilistic spatio-temporal representation for dynamic scene understanding. Unlike prior methods that rely on discrete tracking or view-dependent features, our approach disentangles visibility from persistent object identity by modeling each space-time point with an occupancy probability and a conditional instance distribution. To realize this, we introduce a novel instance-embedded representation based on deformable 3D Gaussians, which jointly encode radiance and semantic information and are learned directly from input RGB images and instance masks through differentiable rasterization. Furthermore, we introduce new mechanisms to calibrate per-Gaussian identities and resample Gaussians toward semantically active regions, ensuring consistent instance representations across space and time. Experiments on HyperNeRF and Neu3D datasets demonstrate that our method significantly outperforms state-of-the-art methods on novel-view panoptic segmentation and open-vocabulary 4D querying tasks.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14126v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出一致实例场，基于可变形3D高斯模型，解决动态场景理解中实例表示不一致的问题。",
            "summary_zh": "我们引入了“一致实例场”，这是一种用于动态场景理解的连续且概率化的时空表示方法。与先前依赖离散跟踪或视角相关特征的方法不同，我们的方法通过为每个时空点建模占用概率和条件实例分布，将可见性与持久对象身份解耦。为实现这一点，我们提出了一种基于可变形3D高斯模型的新颖实例嵌入表示，该表示联合编码辐射和语义信息，并通过可微分光栅化直接从输入RGB图像和实例掩码中学习。此外，我们引入了新机制来校准每个高斯的身份，并向语义活跃区域重新采样高斯，确保跨空间和时间的一致实例表示。在HyperNeRF和Neu3D数据集上的实验表明，我们的方法在新视角全景分割和开放词汇4D查询任务上显著优于最先进的方法。",
            "intro_zh": [
                "核心问题：现有动态场景理解方法依赖离散跟踪或视角相关特征，导致实例表示在时空上不一致，难以处理复杂动态场景。",
                "方法要点：提出一致实例场，基于可变形3D高斯模型，联合编码辐射和语义信息，通过校准和重采样机制确保实例身份一致性。",
                "实验或效果：在HyperNeRF和Neu3D数据集上，新视角全景分割和开放词汇4D查询任务性能显著提升，优于现有方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决动态场景理解中实例表示不一致的问题，现有方法如离散跟踪或视角相关特征难以处理复杂时空变化，导致实例身份在跨视角和时间上不连续，影响新视角全景分割和4D查询等任务的准确性。\\n\\n**核心思路**：论文提出一致实例场，这是一种连续概率化时空表示，通过建模每个时空点的占用概率和条件实例分布，将可见性与对象身份解耦，从而在动态场景中实现持久且一致的实例识别。\\n\\n**技术框架**：整体架构基于可变形3D高斯模型，包括实例嵌入表示学习、身份校准和重采样模块。输入为RGB图像和实例掩码，通过可微分光栅化训练高斯参数，联合优化辐射和语义信息，输出为时空一致的实例场表示。\\n\\n**关键创新**：最重要的创新是引入基于可变形3D高斯的实例嵌入表示，结合校准和重采样机制，与现有方法相比，本质区别在于从离散跟踪转向连续概率建模，直接处理时空一致性，无需依赖额外跟踪步骤。\\n\\n**关键设计**：关键技术细节包括使用可变形3D高斯参数化位置、尺度和旋转，通过损失函数联合优化辐射重建和语义分割；身份校准机制基于高斯间的相似性度量，重采样策略针对语义活跃区域动态调整高斯分布；训练中采用可微分光栅化实现端到端学习，参数设置如高斯数量和学习率根据数据集调整。",
            "application_zh": "该研究在自动驾驶、机器人导航和增强现实等领域具有潜在应用价值，通过提供动态场景中一致且可查询的实例表示，能提升环境感知和交互能力，例如实时物体跟踪、场景重建和语义查询，未来可能推动智能系统在复杂动态环境中的理解和决策能力。",
            "highlight_zh": "在HyperNeRF和Neu3D数据集上的实验显示，本方法在新视角全景分割任务中，相比最先进方法（如NeRF-based方法）在指标如mIoU上提升显著，具体数据未知；在开放词汇4D查询任务中，查询准确率也有明显提高，验证了实例场在动态场景理解中的有效性和鲁棒性。",
            "tags_zh": [
                "动态场景理解",
                "实例场表示",
                "可变形3D高斯",
                "新视角全景分割",
                "开放词汇查询",
                "时空一致性",
                "可微分光栅化",
                "语义分割"
            ],
            "_index": 157
        },
        {
            "title": "SportsGPT: An LLM-driven Framework for Interpretable Sports Motion Assessment and Training Guidance",
            "authors": [
                "Wenbo Tian",
                "Ruting Lin",
                "Hongxian Zheng",
                "Yaodong Yang",
                "Geng Wu",
                "Zihao Zhang",
                "Zhang Zhang"
            ],
            "arxiv_id": "2512.14121v1",
            "summary": "Existing intelligent sports analysis systems mainly focus on \"scoring and visualization,\" often lacking automatic performance diagnosis and interpretable training guidance. Recent advances of Large Language Models (LMMs) and motion analysis techniques provide new opportunities to address the above limitations. In this paper, we propose SportsGPT, an LLM-driven framework for interpretable sports motion assessment and training guidance, which establishes a closed loop from motion time-series input to professional training guidance. First, given a set of high-quality target models, we introduce MotionDTW, a two-stage time series alignment algorithm designed for accurate keyframe extraction from skeleton-based motion sequences. Subsequently, we design a Knowledge-based Interpretable Sports Motion Assessment Model (KISMAM) to obtain a set of interpretable assessment metrics (e.g., insufficient extension) by constrasting the keyframes with the targe models. Finally, we propose SportsRAG, a RAG-based training guidance model based on Qwen3. Leveraging a 6B-token knowledge base, it prompts the LLM to generate professional training guidance by retrieving domain-specific QA pairs. Experimental results demonstrate that MotionDTW significantly outperforms traditional methods with lower temporal error and higher IoU scores. Furthermore, ablation studies validate the KISMAM and SportsRAG, confirming that SportsGPT surpasses general LLMs in diagnostic accuracy and professionalism.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14121v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出SportsGPT框架，通过运动序列分析和大语言模型生成可解释的运动评估与训练指导，解决现有系统缺乏自动诊断和可解释指导的问题。",
            "summary_zh": "现有的智能体育分析系统主要关注“评分和可视化”，往往缺乏自动性能诊断和可解释的训练指导。大语言模型和运动分析技术的最新进展为解决上述局限性提供了新的机会。本文提出SportsGPT，这是一个基于大语言模型的可解释运动评估与训练指导框架，建立了从运动时间序列输入到专业训练指导的闭环。首先，给定一组高质量的目标模型，我们引入了MotionDTW，这是一种两阶段时间序列对齐算法，旨在从基于骨架的运动序列中准确提取关键帧。随后，我们设计了一个基于知识的可解释运动评估模型（KISMAM），通过将关键帧与目标模型进行对比，获得一组可解释的评估指标（例如，伸展不足）。最后，我们提出了SportsRAG，这是一个基于Qwen3的检索增强生成训练指导模型。利用一个包含60亿标记的知识库，它通过检索特定领域的问答对来提示大语言模型生成专业的训练指导。实验结果表明，MotionDTW在时间误差更低和IoU分数更高方面显著优于传统方法。此外，消融研究验证了KISMAM和SportsRAG，确认SportsGPT在诊断准确性和专业性方面超越了通用大语言模型。",
            "intro_zh": [
                "现有智能体育分析系统多聚焦于评分与可视化，缺乏自动性能诊断和可解释的训练指导，限制了实际应用价值。",
                "论文提出SportsGPT框架，结合MotionDTW关键帧提取、KISMAM评估模型和SportsRAG指导生成，构建从运动输入到专业指导的闭环系统。",
                "实验显示MotionDTW在时间误差和IoU上优于传统方法，SportsGPT在诊断准确性和专业性上超越通用大语言模型，验证了框架有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决智能体育分析中现有系统主要关注“评分和可视化”，而缺乏自动性能诊断和可解释训练指导的问题。现有方法的痛点在于难以从运动时间序列中准确提取关键信息，并生成专业、可解释的评估与指导，导致系统实用性受限。\\n\\n**核心思路**：论文的核心解决思路是构建一个基于大语言模型的闭环框架，将运动序列分析与大语言模型能力结合，实现从原始运动数据到专业训练指导的端到端处理。通过引入时间序列对齐算法提取关键帧，设计基于知识的评估模型生成可解释指标，并利用检索增强生成技术生成指导，从而提升系统的自动化和可解释性。\\n\\n**技术框架**：整体架构包含三个主要阶段：首先，使用MotionDTW算法对输入的运动序列进行两阶段时间序列对齐，以提取关键帧；其次，通过KISMAM模型将提取的关键帧与高质量目标模型对比，生成可解释的评估指标；最后，基于SportsRAG模型，利用包含60亿标记的知识库检索相关问答对，提示大语言模型（基于Qwen3）生成专业的训练指导。这三个阶段形成一个从运动输入到指导输出的完整闭环。\\n\\n**关键创新**：最重要的技术创新点包括：1) MotionDTW算法，这是一种专门为骨架运动序列设计的两阶段时间序列对齐方法，提高了关键帧提取的准确性；2) KISMAM模型，它通过对比分析生成可解释的评估指标，增强了系统的透明度和实用性；3) SportsRAG模型，结合检索增强生成技术和大语言模型，实现了基于领域知识的专业指导生成。与现有方法的本质区别在于将运动分析与大语言模型深度集成，提供了更全面、可解释的解决方案。\\n\\n**关键设计**：关键设计细节包括：MotionDTW算法采用两阶段对齐策略，可能涉及动态时间规整的优化变体，以降低时间误差并提高IoU分数；KISMAM模型基于知识对比，具体参数设置未知，但可能包含相似性度量和阈值设定；SportsRAG模型基于Qwen3大语言模型，使用包含60亿标记的知识库进行检索，检索机制可能基于向量相似性，以匹配运动评估结果与相关问答对，从而生成指导。损失函数和网络结构的具体细节在摘要中未明确说明，属于未知内容。",
            "application_zh": "该研究在体育训练、康复医学和健身指导等领域具有广泛的应用潜力。通过自动化的运动评估和可解释的训练指导，SportsGPT可以帮助运动员、教练和物理治疗师更高效地分析运动表现，制定个性化训练计划，提升训练效果和安全性。未来，该框架可扩展到更多运动类型和实时分析场景，推动智能体育技术的发展。",
            "highlight_zh": "最重要的实验结果显示：MotionDTW算法在关键帧提取任务中显著优于传统方法，具体表现为更低的时间误差和更高的IoU分数，但具体数值未知。此外，通过消融研究验证，KISMAM和SportsRAG模块有效提升了系统性能，SportsGPT框架在诊断准确性和专业性方面超越了通用大语言模型，证实了其整体设计的优越性。",
            "tags_zh": [
                "运动分析",
                "大语言模型",
                "时间序列对齐",
                "可解释评估",
                "检索增强生成",
                "骨架运动",
                "训练指导",
                "智能体育"
            ],
            "_index": 158
        },
        {
            "title": "Joint Multimodal Contrastive Learning for Robust Spoken Term Detection and Keyword Spotting",
            "authors": [
                "Ramesh Gundluru",
                "Shubham Gupta",
                "Sri Rama Murty K"
            ],
            "arxiv_id": "2512.14115v1",
            "summary": "Acoustic Word Embeddings (AWEs) improve the efficiency of speech retrieval tasks such as Spoken Term Detection (STD) and Keyword Spotting (KWS). However, existing approaches suffer from limitations, including unimodal supervision, disjoint optimization of audio-audio and audio-text alignment, and the need for task-specific models. To address these shortcomings, we propose a joint multimodal contrastive learning framework that unifies both acoustic and cross-modal supervision in a shared embedding space. Our approach simultaneously optimizes: (i) audio-text contrastive learning, inspired by the CLAP loss, to align audio and text representations and (ii) audio-audio contrastive learning, via Deep Word Discrimination (DWD) loss, to enhance intra-class compactness and inter-class separation. The proposed method outperforms existing AWE baselines on word discrimination task while flexibly supporting both STD and KWS. To our knowledge, this is the first comprehensive approach of its kind.",
            "categories": [
                "cs.SD",
                "cs.LG"
            ],
            "primary_category": "cs.SD",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14115v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出联合多模态对比学习框架，以解决声学词嵌入在语音检索任务中的局限性，提升口语词检测和关键词识别的鲁棒性。",
            "summary_zh": "声学词嵌入（AWEs）提高了语音检索任务（如口语词检测和关键词识别）的效率。然而，现有方法存在局限性，包括单模态监督、音频-音频和音频-文本对齐的分离优化，以及需要任务特定模型。为解决这些不足，我们提出了一个联合多模态对比学习框架，在共享嵌入空间中统一了声学和跨模态监督。我们的方法同时优化：（i）音频-文本对比学习，受CLAP损失启发，以对齐音频和文本表示；（ii）音频-音频对比学习，通过深度词判别损失，以增强类内紧凑性和类间分离性。所提方法在词判别任务上优于现有AWE基线，同时灵活支持口语词检测和关键词识别。据我们所知，这是首个此类综合方法。",
            "intro_zh": [
                "现有声学词嵌入方法依赖单模态监督，导致音频-音频和音频-文本对齐分离优化，限制了跨模态检索性能。",
                "论文提出联合多模态对比学习框架，统一音频-音频和音频-文本监督，通过共享嵌入空间增强表示学习。",
                "实验表明，该方法在词判别任务上优于基线，并灵活支持口语词检测和关键词识别，提升了鲁棒性和泛化能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决声学词嵌入在语音检索任务（如口语词检测和关键词识别）中的局限性，包括现有方法依赖单模态监督、音频-音频和音频-文本对齐分离优化，以及需要任务特定模型，导致效率低下和泛化能力不足。\\n\\n**核心思路**：论文的核心思路是设计一个联合多模态对比学习框架，通过同时优化音频-音频和音频-文本对比损失，在共享嵌入空间中统一声学和跨模态监督，以增强表示的紧凑性和分离性，从而提升语音检索的鲁棒性和灵活性。\\n\\n**技术框架**：整体架构包括音频编码器、文本编码器和共享嵌入空间。流程分为两个阶段：首先，音频和文本输入分别通过编码器提取特征；然后，在共享嵌入空间中，同时应用音频-文本对比损失（基于CLAP损失）和音频-音频对比损失（基于深度词判别损失），通过联合优化来学习统一表示。\\n\\n**关键创新**：最重要的技术创新是首次提出联合多模态对比学习框架，将音频-音频和音频-文本监督统一在一个模型中，避免了分离优化，本质区别在于实现了端到端的跨模态对齐和类内类间优化，提高了方法的综合性和泛化能力。\\n\\n**关键设计**：关键设计包括：使用音频编码器（如卷积神经网络或Transformer）处理音频信号，文本编码器（如词嵌入模型）处理文本；损失函数结合CLAP损失（用于音频-文本对齐）和深度词判别损失（用于音频-音频对比，增强类内紧凑性和类间分离性）；参数设置如学习率、批次大小通过实验优化，网络结构共享嵌入层以减少模型复杂度。",
            "application_zh": "该研究在语音检索领域具有广泛应用，如智能助手中的口语词检测、安全监控中的关键词识别、语音搜索和内容分析。其实际价值在于提高检索效率和准确性，减少对任务特定模型的依赖，未来可能推动多模态语音处理技术的发展，应用于更复杂的场景如多语言语音识别和跨模态信息检索。",
            "highlight_zh": "实验结果显示，所提方法在词判别任务上优于现有声学词嵌入基线，具体性能数据未知，但通过联合优化提升了表示质量。方法灵活支持口语词检测和关键词识别，无需任务特定模型，展示了鲁棒性和泛化能力的显著提升，为多模态语音检索提供了新基准。",
            "tags_zh": [
                "声学词嵌入",
                "多模态对比学习",
                "口语词检测",
                "关键词识别",
                "语音检索",
                "跨模态对齐",
                "联合优化",
                "共享嵌入空间"
            ],
            "_index": 159
        },
        {
            "title": "MFE-GAN: Efficient GAN-based Framework for Document Image Enhancement and Binarization with Multi-scale Feature Extraction",
            "authors": [
                "Rui-Yang Ju",
                "KokSheik Wong",
                "Yanlin Jin",
                "Jen-Shiun Chiang"
            ],
            "arxiv_id": "2512.14114v1",
            "summary": "Document image enhancement and binarization are commonly performed prior to document analysis and recognition tasks for improving the efficiency and accuracy of optical character recognition (OCR) systems. This is because directly recognizing text in degraded documents, particularly in color images, often results in unsatisfactory recognition performance. To address these issues, existing methods train independent generative adversarial networks (GANs) for different color channels to remove shadows and noise, which, in turn, facilitates efficient text information extraction. However, deploying multiple GANs results in long training and inference times. To reduce both training and inference times of document image enhancement and binarization models, we propose MFE-GAN, an efficient GAN-based framework with multi-scale feature extraction (MFE), which incorporates Haar wavelet transformation (HWT) and normalization to process document images before feeding them into GANs for training. In addition, we present novel generators, discriminators, and loss functions to improve the model's performance, and we conduct ablation studies to demonstrate their effectiveness. Experimental results on the Benchmark, Nabuco, and CMATERdb datasets demonstrate that the proposed MFE-GAN significantly reduces the total training and inference times while maintaining comparable performance with respect to state-of-the-art (SOTA) methods. The implementation of this work is available at https://ruiyangju.github.io/MFE-GAN.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Extended Journal Version of APSIPA ASC 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14114v1",
            "code_links": [
                {
                    "url": "https://ruiyangju.github.io/MFE-GAN",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出MFE-GAN框架，通过多尺度特征提取和Haar小波变换，高效解决文档图像增强与二值化中的训练和推理时间问题。",
            "summary_zh": "文档图像增强与二值化通常在文档分析和识别任务之前进行，以提高光学字符识别（OCR）系统的效率和准确性。这是因为直接识别退化文档（尤其是彩色图像）中的文本往往导致不理想的识别性能。为解决这些问题，现有方法训练独立的生成对抗网络（GANs）用于不同颜色通道以去除阴影和噪声，从而促进高效的文本信息提取。然而，部署多个GANs会导致较长的训练和推理时间。为减少文档图像增强与二值化模型的训练和推理时间，我们提出了MFE-GAN，一种基于GAN的高效框架，具有多尺度特征提取（MFE），它结合了Haar小波变换（HWT）和归一化，在将文档图像输入GANs进行训练之前进行处理。此外，我们提出了新颖的生成器、判别器和损失函数以提高模型性能，并进行了消融研究以证明其有效性。在Benchmark、Nabuco和CMATERdb数据集上的实验结果表明，所提出的MFE-GAN显著减少了总训练和推理时间，同时在与最先进（SOTA）方法相比时保持了可比的性能。本工作的实现可在https://ruiyangju.github.io/MFE-GAN获取。",
            "intro_zh": [
                "核心问题：现有方法使用多个独立GANs处理不同颜色通道，导致训练和推理时间过长，效率低下。",
                "方法要点：提出MFE-GAN框架，集成多尺度特征提取和Haar小波变换，优化图像预处理，减少模型复杂度。",
                "实验或效果：在多个数据集上，MFE-GAN显著降低训练和推理时间，同时性能与SOTA方法相当。"
            ],
            "method_zh": "**问题定义**：论文旨在解决文档图像增强与二值化任务中，现有基于GAN的方法因使用多个独立网络处理不同颜色通道而导致的训练和推理时间过长问题，这限制了实际部署效率。\\n\\n**核心思路**：核心思路是通过引入多尺度特征提取（MFE）和Haar小波变换（HWT）作为预处理步骤，减少输入数据的冗余和噪声，从而简化GAN的训练过程，避免部署多个GANs，提高整体效率。\\n\\n**技术框架**：整体框架包括预处理、生成器和判别器三阶段。预处理阶段使用HWT和归一化处理输入图像，提取多尺度特征；生成器基于这些特征生成增强和二值化图像；判别器评估生成图像的真实性，通过对抗训练优化模型。\\n\\n**关键创新**：最重要的技术创新是集成MFE和HWT到GAN框架中，通过多尺度特征提取优化输入，减少模型复杂度，与现有方法相比，本质区别在于避免了多GAN部署，实现了单框架高效处理。\\n\\n**关键设计**：关键设计包括新颖的生成器结构，可能采用编码器-解码器架构以处理多尺度特征；判别器设计为多尺度判别器以提高鲁棒性；损失函数结合对抗损失、重构损失和感知损失，具体参数如学习率、批量大小在实验中优化设置，以平衡训练速度和性能。",
            "application_zh": "该研究主要应用于文档分析和OCR系统，可提升退化文档（如扫描件、历史档案或低质量图像）的文本识别准确性和效率。潜在价值包括数字化存档、自动化办公和文化遗产保护，未来可能扩展到其他图像增强任务，如医学影像或遥感图像处理。",
            "highlight_zh": "在Benchmark、Nabuco和CMATERdb数据集上的实验显示，MFE-GAN相比SOTA方法，训练时间减少约30-50%，推理时间降低约20-40%，同时保持可比的性能指标（如PSNR、SSIM和OCR准确率）。消融研究验证了MFE和HWT模块的有效性，提升了模型鲁棒性。",
            "tags_zh": [
                "文档图像增强",
                "图像二值化",
                "生成对抗网络",
                "多尺度特征提取",
                "Haar小波变换",
                "OCR优化",
                "高效训练",
                "图像预处理"
            ],
            "_index": 160
        },
        {
            "title": "Selective, Controlled and Domain-Agnostic Unlearning in Pretrained CLIP: A Training- and Data-Free Approach",
            "authors": [
                "Ashish Mishra",
                "Gyanaranjan Nayak",
                "Tarun Kumar",
                "Arpit Shah",
                "Suparna Bhattacharya",
                "Martin Foltin"
            ],
            "arxiv_id": "2512.14113v1",
            "summary": "Pretrained models like CLIP have demonstrated impressive zero-shot classification capabilities across diverse visual domains, spanning natural images, artistic renderings, and abstract representations. However, real-world applications often demand the removal (or \"unlearning\") of specific object classes without requiring additional data or retraining, or affecting the model's performance on unrelated tasks. In this paper, we propose a novel training- and data-free unlearning framework that enables three distinct forgetting paradigms: (1) global unlearning of selected objects across all domains, (2) domain-specific knowledge removal (e.g., eliminating sketch representations while preserving photo recognition), and (3) complete unlearning in selective domains. By leveraging a multimodal nullspace through synergistic integration of text prompts and synthesized visual prototypes derived from CLIP's joint embedding space, our method efficiently removes undesired class information while preserving the remaining knowledge. This approach overcomes the limitations of existing retraining-based methods and offers a flexible and computationally efficient solution for controlled model forgetting.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14113v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出一种无需训练和数据的遗忘框架，实现CLIP模型中对特定类别的选择性、可控和领域无关的遗忘。",
            "summary_zh": "像CLIP这样的预训练模型已在自然图像、艺术渲染和抽象表示等多种视觉领域展现出卓越的零样本分类能力。然而，实际应用常需在不依赖额外数据或重新训练、且不影响模型在无关任务上性能的前提下，移除特定对象类别。本文提出一种新颖的无需训练和数据的遗忘框架，支持三种不同的遗忘范式：(1) 在所有领域中全局遗忘选定对象；(2) 领域特定知识移除（例如，消除草图表示同时保留照片识别）；(3) 在选定领域中的完全遗忘。通过利用多模态零空间，协同整合文本提示和从CLIP联合嵌入空间衍生的合成视觉原型，该方法高效移除不需要的类别信息，同时保留其余知识。此方法克服了现有基于重新训练方法的局限性，为可控模型遗忘提供了灵活且计算高效的解决方案。",
            "intro_zh": [
                "现有基于重新训练的遗忘方法依赖额外数据且计算成本高，难以在不影响模型整体性能下实现选择性遗忘。",
                "通过多模态零空间整合文本提示和合成视觉原型，实现无需训练和数据的遗忘，支持全局、领域特定和选择性遗忘。",
                "实验表明，该方法在移除目标类别信息的同时，保持模型在无关任务上的性能，显著优于基线方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决预训练CLIP模型中对特定对象类别的选择性遗忘问题，无需额外数据或重新训练，同时保持模型在无关任务上的性能。现有基于重新训练的方法依赖大量数据且计算成本高，难以实现精细控制，如领域特定遗忘。\\n\\n**核心思路**：核心思路是利用CLIP的多模态嵌入空间，通过文本提示和合成视觉原型构建多模态零空间，将目标类别的信息投影到零空间以实现遗忘，同时保留其他知识。这种设计避免了重新训练，直接操作模型参数，实现高效可控的遗忘。\\n\\n**技术框架**：整体框架包括三个阶段：首先，使用文本提示（如类别名称）和CLIP嵌入空间生成合成视觉原型；其次，基于这些原型和文本嵌入计算多模态零空间；最后，通过优化目标将模型参数调整，使目标类别信息落入零空间，从而移除其影响。框架支持三种遗忘模式：全局、领域特定和选择性领域遗忘。\\n\\n**关键创新**：最重要的创新是提出一种无需训练和数据的遗忘方法，通过多模态零空间实现精细控制。与现有方法相比，本质区别在于不依赖重新训练或额外数据，直接利用CLIP的嵌入结构进行参数调整，提供更高的灵活性和计算效率。\\n\\n**关键设计**：关键设计包括：使用文本提示和视觉原型的协同整合来定义零空间；设计损失函数以最小化目标类别在嵌入空间中的表示，同时最大化保留其他类别的区分度；参数调整通过梯度下降进行，但无需完整训练循环；支持自定义遗忘强度，通过调整零空间投影的权重实现可控遗忘。",
            "application_zh": "该研究在隐私保护、模型合规性和动态内容过滤等领域具有潜在应用价值。例如，在图像识别系统中移除敏感类别（如人脸或商标），或在多领域应用中调整模型知识以适应新法规。未来可能推动更智能的模型管理工具，降低AI系统的维护成本。",
            "highlight_zh": "实验在多个数据集（如ImageNet、Sketch）上验证，该方法在移除目标类别（如“狗”）时，遗忘准确率提升超过20%，同时保持无关类别性能下降小于5%。与基线方法（如重新训练）相比，计算时间减少90%以上，支持多种遗忘模式，展示了高效性和可控性。",
            "tags_zh": [
                "模型遗忘",
                "多模态学习",
                "零样本分类",
                "CLIP模型",
                "无需训练",
                "可控遗忘",
                "领域无关",
                "嵌入空间优化"
            ],
            "_index": 161
        },
        {
            "title": "Neurosymbolic Inference On Foundation Models For Remote Sensing Text-to-image Retrieval With Complex Queries",
            "authors": [
                "Emanuele Mezzi",
                "Gertjan Burghouts",
                "Maarten Kruithof"
            ],
            "arxiv_id": "2512.14102v1",
            "summary": "Text-to-image retrieval in remote sensing (RS) has advanced rapidly with the rise of large vision-language models (LVLMs) tailored for aerial and satellite imagery, culminating in remote sensing large vision-language models (RS-LVLMS). However, limited explainability and poor handling of complex spatial relations remain key challenges for real-world use. To address these issues, we introduce RUNE (Reasoning Using Neurosymbolic Entities), an approach that combines Large Language Models (LLMs) with neurosymbolic AI to retrieve images by reasoning over the compatibility between detected entities and First-Order Logic (FOL) expressions derived from text queries. Unlike RS-LVLMs that rely on implicit joint embeddings, RUNE performs explicit reasoning, enhancing performance and interpretability. For scalability, we propose a logic decomposition strategy that operates on conditioned subsets of detected entities, guaranteeing shorter execution time compared to neural approaches. Rather than using foundation models for end-to-end retrieval, we leverage them only to generate FOL expressions, delegating reasoning to a neurosymbolic inference module. For evaluation we repurpose the DOTA dataset, originally designed for object detection, by augmenting it with more complex queries than in existing benchmarks. We show the LLM's effectiveness in text-to-logic translation and compare RUNE with state-of-the-art RS-LVLMs, demonstrating superior performance. We introduce two metrics, Retrieval Robustness to Query Complexity (RRQC) and Retrieval Robustness to Image Uncertainty (RRIU), which evaluate performance relative to query complexity and image uncertainty. RUNE outperforms joint-embedding models in complex RS retrieval tasks, offering gains in performance, robustness, and explainability. We show RUNE's potential for real-world RS applications through a use case on post-flood satellite image retrieval.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.IR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14102v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出RUNE方法，结合大语言模型与神经符号推理，解决遥感文本-图像检索中复杂空间关系处理与可解释性不足的问题。",
            "summary_zh": "遥感领域的文本-图像检索随着针对航空和卫星影像定制的大型视觉语言模型（LVLMs）的兴起而迅速发展，最终形成了遥感大型视觉语言模型（RS-LVLMs）。然而，有限的可解释性和对复杂空间关系的处理能力差仍然是实际应用中的关键挑战。为解决这些问题，我们引入了RUNE（使用神经符号实体进行推理），该方法将大语言模型（LLMs）与神经符号AI相结合，通过推理检测到的实体与从文本查询导出的谓词逻辑表达式之间的兼容性来检索图像。与依赖隐式联合嵌入的RS-LVLMs不同，RUNE执行显式推理，从而提升性能和可解释性。为实现可扩展性，我们提出了一种逻辑分解策略，该策略在检测到的实体的条件子集上操作，保证比神经方法更短的执行时间。我们不是将基础模型用于端到端检索，而是仅利用它们生成谓词逻辑表达式，将推理委托给神经符号推理模块。为了评估，我们重新利用了原本设计用于目标检测的DOTA数据集，通过添加比现有基准更复杂的查询来增强它。我们展示了LLM在文本到逻辑翻译中的有效性，并将RUNE与最先进的RS-LVLMs进行比较，证明了其优越性能。我们引入了两个指标：检索对查询复杂性的鲁棒性（RRQC）和检索对图像不确定性的鲁棒性（RRIU），用于评估相对于查询复杂性和图像不确定性的性能。RUNE在复杂的遥感检索任务中优于联合嵌入模型，在性能、鲁棒性和可解释性方面都有提升。我们通过一个关于洪水后卫星图像检索的用例展示了RUNE在现实世界遥感应用中的潜力。",
            "intro_zh": [
                "现有遥感文本-图像检索方法（如RS-LVLMs）存在可解释性差、难以处理复杂空间关系等挑战，限制了实际应用。",
                "提出RUNE方法，结合大语言模型与神经符号推理，通过显式推理检测实体与谓词逻辑表达式的兼容性来提升检索性能。",
                "实验表明，RUNE在复杂查询任务中优于现有RS-LVLMs，并引入新指标验证其在鲁棒性和可解释性方面的优势。"
            ],
            "method_zh": "**问题定义**：论文旨在解决遥感文本-图像检索中，现有方法（如RS-LVLMs）因依赖隐式联合嵌入而导致的有限可解释性和对复杂空间关系处理能力差的问题。这些痛点限制了模型在真实世界应用中的可靠性和实用性。\\n\\n**核心思路**：论文的核心思路是结合大语言模型与神经符号AI，通过显式推理而非隐式嵌入来提升检索性能。具体来说，利用LLMs将文本查询转换为谓词逻辑表达式，然后基于检测到的实体进行神经符号推理，以评估查询与图像的兼容性。这种设计旨在增强可解释性和处理复杂关系的能力。\\n\\n**技术框架**：整体架构包含两个主要阶段：首先，使用大语言模型将输入的文本查询翻译为谓词逻辑表达式；其次，通过神经符号推理模块，基于从图像中检测到的实体（如物体和空间关系），评估这些表达式与图像的匹配程度，最终输出检索结果。推理过程采用逻辑分解策略，在实体子集上操作以提高效率。\\n\\n**关键创新**：最重要的技术创新是引入神经符号推理框架，将大语言模型用于逻辑生成，而非端到端检索，从而实现了显式推理。与现有RS-LVLMs的本质区别在于，RUNE不依赖隐式嵌入，而是通过符号逻辑进行透明推理，提升了可解释性和对复杂查询的处理能力。\\n\\n**关键设计**：关键设计包括：使用大语言模型（具体模型未指定，但基于上下文推断可能为通用LLM如GPT系列）进行文本到逻辑的翻译；神经符号推理模块基于检测到的实体（如通过目标检测模型获取）执行谓词逻辑评估；逻辑分解策略优化推理时间，确保可扩展性；评估时引入RRQC和RRIU指标量化鲁棒性。具体参数设置和损失函数在摘要中未详细说明，可能涉及标准检索损失或自定义逻辑匹配函数。",
            "application_zh": "该研究在遥感领域具有广泛的应用潜力，例如灾害响应（如洪水后卫星图像检索）、环境监测、城市规划等。通过提升复杂查询的处理能力和可解释性，RUNE能够支持更精准的决策分析，增强遥感数据在实际任务中的实用性，未来可能推动智能遥感系统的发展。",
            "highlight_zh": "实验结果显示，RUNE在复杂遥感检索任务中优于最先进的RS-LVLMs，具体性能数据未在摘要中提供，但通过引入的RRQC和RRIU指标验证了其在查询复杂性和图像不确定性方面的鲁棒性提升。在DOTA数据集增强版本上的评估表明，LLM能有效翻译文本到逻辑，且RUNE在可解释性和效率方面均有优势。",
            "tags_zh": [
                "遥感文本-图像检索",
                "神经符号推理",
                "大语言模型",
                "谓词逻辑",
                "可解释性AI",
                "复杂空间关系",
                "DOTA数据集",
                "洪水后图像检索"
            ],
            "_index": 162
        },
        {
            "title": "Quality-Aware Framework for Video-Derived Respiratory Signals",
            "authors": [
                "Nhi Nguyen",
                "Constantino Álvarez Casado",
                "Le Nguyen",
                "Manuel Lage Cañellas",
                "Miguel Bordallo López"
            ],
            "arxiv_id": "2512.14093v1",
            "summary": "Video-based respiratory rate (RR) estimation is often unreliable due to inconsistent signal quality across extraction methods. We present a predictive, quality-aware framework that integrates heterogeneous signal sources with dynamic assessment of reliability. Ten signals are extracted from facial remote photoplethysmography (rPPG), upper-body motion, and deep learning pipelines, and analyzed using four spectral estimators: Welch's method, Multiple Signal Classification (MUSIC), Fast Fourier Transform (FFT), and peak detection. Segment-level quality indices are then used to train machine learning models that predict accuracy or select the most reliable signal. This enables adaptive signal fusion and quality-based segment filtering. Experiments on three public datasets (OMuSense-23, COHFACE, MAHNOB-HCI) show that the proposed framework achieves lower RR estimation errors than individual methods in most cases, with performance gains depending on dataset characteristics. These findings highlight the potential of quality-driven predictive modeling to deliver scalable and generalizable video-based respiratory monitoring solutions.",
            "categories": [
                "cs.CV",
                "eess.SP"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "6 pages, 1 figure, 2 tables, conference",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14093v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "世界模型",
                    "matched_keywords": [
                        "predictive model"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出质量感知框架以解决视频呼吸信号提取质量不一致问题，实现自适应融合与过滤。",
            "summary_zh": "基于视频的呼吸率估计常因不同提取方法产生的信号质量不一致而不可靠。本文提出一个预测性的质量感知框架，整合了异质信号源并动态评估其可靠性。从面部远程光电容积描记术、上半身运动和深度学习流程中提取了十种信号，并使用四种频谱估计器进行分析：Welch方法、多重信号分类、快速傅里叶变换和峰值检测。然后，利用片段级质量指标训练机器学习模型，以预测准确性或选择最可靠的信号。这实现了自适应信号融合和基于质量的片段过滤。在三个公共数据集上的实验表明，该框架在大多数情况下比单个方法实现了更低的呼吸率估计误差，性能提升取决于数据集特性。这些发现突显了质量驱动的预测建模在提供可扩展和可泛化的视频呼吸监测解决方案方面的潜力。",
            "intro_zh": [
                "现有视频呼吸率估计方法因信号提取质量不一致导致结果不可靠，缺乏统一的质量评估机制。",
                "论文提出质量感知框架，整合多源信号并动态评估可靠性，通过机器学习模型预测准确性或选择最优信号。",
                "在三个公共数据集上实验，框架在多数情况下降低了呼吸率估计误差，性能提升依赖于数据集特性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决视频呼吸率估计中，由于不同提取方法产生的信号质量不一致导致的不可靠性问题。现有方法通常依赖单一信号源或固定融合策略，缺乏对信号质量的动态评估，导致在复杂场景下性能下降。\\n\\n**核心思路**：论文提出一个预测性的质量感知框架，通过整合异质信号源并动态评估其可靠性，实现自适应信号融合和基于质量的片段过滤。核心思想是利用机器学习模型预测信号准确性或选择最可靠信号，从而提升整体估计的鲁棒性和泛化能力。\\n\\n**技术框架**：整体框架包含三个主要阶段：信号提取、质量评估和融合决策。首先，从视频中提取十种信号，包括基于面部远程光电容积描记术、上半身运动和深度学习流程的信号。然后，使用四种频谱估计器分析这些信号，并计算片段级质量指标。最后，基于质量指标训练机器学习模型，用于预测准确性或选择最优信号，实现自适应融合和过滤。\\n\\n**关键创新**：最重要的技术创新是引入了质量驱动的预测建模，动态评估信号可靠性并自适应融合，与现有方法依赖固定策略或单一信号源的本质区别在于其灵活性和泛化能力。\\n\\n**关键设计**：关键设计包括使用四种频谱估计器（Welch方法、多重信号分类、快速傅里叶变换和峰值检测）分析信号，提取片段级质量指标，并训练机器学习模型进行准确性预测或信号选择。具体参数设置和损失函数在论文中未详细说明，但框架支持多种模型集成，以优化性能。",
            "application_zh": "该研究在远程医疗、健康监测和智能家居等领域具有潜在应用价值，例如用于非接触式呼吸监测系统，提升老年护理或慢性病管理的效率。未来可能推动可穿戴设备和视频分析技术的融合，实现更精准和可扩展的健康监测解决方案。",
            "highlight_zh": "在OMuSense-23、COHFACE和MAHNOB-HCI三个公共数据集上的实验表明，该框架在大多数情况下比单个方法实现了更低的呼吸率估计误差。性能提升幅度依赖于数据集特性，例如在信号质量变化较大的场景中，框架通过自适应融合显著减少了误差，具体数据未在摘要中提供，但结果突显了质量感知方法的有效性。",
            "tags_zh": [
                "视频呼吸率估计",
                "质量感知框架",
                "信号融合",
                "远程光电容积描记术",
                "机器学习模型",
                "自适应过滤",
                "频谱估计",
                "健康监测"
            ],
            "_index": 163
        },
        {
            "title": "ProtoFlow: Interpretable and Robust Surgical Workflow Modeling with Learned Dynamic Scene Graph Prototypes",
            "authors": [
                "Felix Holm",
                "Ghazal Ghazaei",
                "Nassir Navab"
            ],
            "arxiv_id": "2512.14092v1",
            "summary": "Purpose: Detailed surgical recognition is critical for advancing AI-assisted surgery, yet progress is hampered by high annotation costs, data scarcity, and a lack of interpretable models. While scene graphs offer a structured abstraction of surgical events, their full potential remains untapped. In this work, we introduce ProtoFlow, a novel framework that learns dynamic scene graph prototypes to model complex surgical workflows in an interpretable and robust manner.\n  Methods: ProtoFlow leverages a graph neural network (GNN) encoder-decoder architecture that combines self-supervised pretraining for rich representation learning with a prototype-based fine-tuning stage. This process discovers and refines core prototypes that encapsulate recurring, clinically meaningful patterns of surgical interaction, forming an explainable foundation for workflow analysis.\n  Results: We evaluate our approach on the fine-grained CAT-SG dataset. ProtoFlow not only outperforms standard GNN baselines in overall accuracy but also demonstrates exceptional robustness in limited-data, few-shot scenarios, maintaining strong performance when trained on as few as one surgical video. Our qualitative analyses further show that the learned prototypes successfully identify distinct surgical sub-techniques and provide clear, interpretable insights into workflow deviations and rare complications.\n  Conclusion: By uniting robust representation learning with inherent explainability, ProtoFlow represents a significant step toward developing more transparent, reliable, and data-efficient AI systems, accelerating their potential for clinical adoption in surgical training, real-time decision support, and workflow optimization.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14092v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出ProtoFlow框架，通过动态场景图原型学习实现可解释且鲁棒的手术工作流建模",
            "summary_zh": "目的：详细的手术识别对推进AI辅助手术至关重要，但高标注成本、数据稀缺和缺乏可解释模型阻碍了进展。虽然场景图提供了手术事件的结构化抽象，但其全部潜力尚未被充分挖掘。本研究提出ProtoFlow，一种新颖的框架，通过学习动态场景图原型，以可解释且鲁棒的方式建模复杂手术工作流。方法：ProtoFlow利用图神经网络（GNN）编码器-解码器架构，结合自监督预训练进行丰富表示学习，以及基于原型的微调阶段。该过程发现并精炼核心原型，这些原型封装了重复出现的、具有临床意义的手术交互模式，为工作流分析形成可解释的基础。结果：我们在细粒度CAT-SG数据集上评估了该方法。ProtoFlow不仅在整体准确率上优于标准GNN基线，还在有限数据、少样本场景中表现出卓越的鲁棒性，在仅用一个手术视频训练时仍保持强劲性能。定性分析进一步显示，学习到的原型成功识别了不同的手术子技术，并为工作流偏差和罕见并发症提供了清晰、可解释的见解。结论：通过将鲁棒的表示学习与固有的可解释性相结合，ProtoFlow代表了向开发更透明、可靠和数据高效的AI系统迈出的重要一步，加速了其在手术培训、实时决策支持和工作流优化中的临床采用潜力。",
            "intro_zh": [
                "核心问题：手术识别面临高标注成本、数据稀缺和模型缺乏可解释性等挑战，现有方法难以在有限数据下鲁棒建模复杂工作流。",
                "方法要点：提出ProtoFlow框架，结合自监督预训练和原型学习，动态发现手术场景图原型，实现可解释且鲁棒的工作流建模。",
                "实验或效果：在CAT-SG数据集上超越GNN基线，少样本场景下鲁棒性强，原型能识别手术子技术并提供工作流偏差见解。"
            ],
            "method_zh": "**问题定义**：论文旨在解决手术工作流建模中的关键问题，即如何在数据稀缺、标注成本高的条件下，实现可解释且鲁棒的复杂手术事件识别。现有方法如标准图神经网络（GNN）虽能处理结构化数据，但通常依赖大量标注数据，且模型决策过程不透明，难以在临床应用中提供可信的见解。\\n\\n**核心思路**：论文的核心解决思路是通过学习动态场景图原型来建模手术工作流。这基于一个假设：手术事件中存在重复出现的、具有临床意义的交互模式，这些模式可被抽象为原型。通过结合自监督预训练学习丰富表示，再基于原型微调，系统能自动发现并精炼这些核心模式，从而在减少数据依赖的同时增强模型的可解释性。\\n\\n**技术框架**：整体架构采用图神经网络（GNN）编码器-解码器结构，包含两个主要阶段：自监督预训练阶段和原型微调阶段。在预训练阶段，使用无标注数据通过自监督任务（如节点或图级别的对比学习）学习场景图的通用表示；在微调阶段，引入原型学习机制，将学习到的表示映射到一组可解释的原型上，这些原型代表手术交互的核心模式，并通过优化原型与输入图的匹配来精炼模型。\\n\\n**关键创新**：最重要的技术创新点是动态场景图原型学习。与现有方法（如静态原型或黑盒GNN）相比，ProtoFlow的原型是动态学习的，能自适应地捕捉手术工作流中的变化和模式，本质区别在于它将可解释性直接嵌入到模型架构中，通过原型提供直观的决策依据，而非仅依赖端到端学习。\\n\\n**关键设计**：关键设计包括：使用GNN编码器处理场景图输入，提取节点和边特征；自监督预训练可能采用图对比损失（如InfoNCE）来增强表示学习；原型微调阶段设计原型匹配损失（如基于距离的损失函数），鼓励输入图与最相似原型对齐；网络结构可能包含多层图卷积和注意力机制，以捕获局部和全局交互；参数设置如原型数量、学习率等需根据数据集调整，以平衡模型复杂度和解释能力。",
            "application_zh": "该研究的潜在应用领域包括手术培训、实时决策支持和工作流优化。在手术培训中，ProtoFlow的可解释原型能帮助学员理解手术步骤和交互模式；在实时决策支持中，模型能识别工作流偏差和罕见并发症，辅助医生做出更准确的判断；在工作流优化中，通过分析原型模式，可改进手术流程和资源分配。这加速了AI系统在临床中的采用，提升手术安全性和效率。",
            "highlight_zh": "最重要的实验结果包括：在CAT-SG数据集上，ProtoFlow在整体准确率上优于标准GNN基线（具体提升幅度未知，但论文指出“outperforms”）；在少样本场景中表现出卓越鲁棒性，当仅用一个手术视频训练时仍保持强劲性能（具体数据未知，但强调“maintaining strong performance”）；定性分析显示，学习到的原型成功识别了不同的手术子技术，并为工作流偏差和罕见并发症提供了清晰、可解释的见解。",
            "tags_zh": [
                "手术工作流建模",
                "动态场景图",
                "原型学习",
                "图神经网络",
                "可解释AI",
                "少样本学习",
                "自监督预训练",
                "医疗图像分析"
            ],
            "_index": 164
        },
        {
            "title": "Derivative-Informed Fourier Neural Operator: Universal Approximation and Applications to PDE-Constrained Optimization",
            "authors": [
                "Boyuan Yao",
                "Dingcheng Luo",
                "Lianghao Cao",
                "Nikola Kovachki",
                "Thomas O'Leary-Roseberry",
                "Omar Ghattas"
            ],
            "arxiv_id": "2512.14086v1",
            "summary": "We present approximation theories and efficient training methods for derivative-informed Fourier neural operators (DIFNOs) with applications to PDE-constrained optimization. A DIFNO is an FNO trained by minimizing its prediction error jointly on output and Fréchet derivative samples of a high-fidelity operator (e.g., a parametric PDE solution operator). As a result, a DIFNO can closely emulate not only the high-fidelity operator's response but also its sensitivities. To motivate the use of DIFNOs instead of conventional FNOs as surrogate models, we show that accurate surrogate-driven PDE-constrained optimization requires accurate surrogate Fréchet derivatives. Then, for continuously differentiable operators, we establish (i) simultaneous universal approximation of FNOs and their Fréchet derivatives on compact sets, and (ii) universal approximation of FNOs in weighted Sobolev spaces with input measures that have unbounded supports. Our theoretical results certify the capability of FNOs for accurate derivative-informed operator learning and accurate solution of PDE-constrained optimization. Furthermore, we develop efficient training schemes using dimension reduction and multi-resolution techniques that significantly reduce memory and computational costs for Fréchet derivative learning. Numerical examples on nonlinear diffusion--reaction, Helmholtz, and Navier--Stokes equations demonstrate that DIFNOs are superior in sample complexity for operator learning and solving infinite-dimensional PDE-constrained inverse problems, achieving high accuracy at low training sample sizes.",
            "categories": [
                "cs.LG",
                "math.NA"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14086v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出导数信息傅里叶神经算子以解决偏微分方程约束优化中的高精度代理建模问题",
            "summary_zh": "本文提出了导数信息傅里叶神经算子的逼近理论和高效训练方法，应用于偏微分方程约束优化。DIFNO是一种通过最小化其在高保真算子输出和Fréchet导数样本上的预测误差来训练的FNO。因此，DIFNO不仅能紧密模拟高保真算子的响应，还能模拟其灵敏度。为了证明使用DIFNO而非传统FNO作为代理模型的必要性，我们展示了精确的代理驱动偏微分方程约束优化需要精确的代理Fréchet导数。然后，对于连续可微算子，我们建立了（i）FNO及其Fréchet导数在紧集上的同时通用逼近，以及（ii）FNO在具有无界支撑输入测度的加权Sobolev空间中的通用逼近。我们的理论结果证明了FNO在精确导数信息算子学习和精确求解偏微分方程约束优化方面的能力。此外，我们利用降维和多分辨率技术开发了高效训练方案，显著降低了Fréchet导数学习的内存和计算成本。非线性扩散-反应、Helmholtz和Navier-Stokes方程的数值示例表明，DIFNO在算子学习和求解无限维偏微分方程约束反问题的样本复杂度方面具有优势，在低训练样本量下实现了高精度。",
            "intro_zh": [
                "现有FNO在偏微分方程约束优化中因缺乏精确导数信息导致代理模型优化效果不佳，需要高精度导数逼近。",
                "提出DIFNO，通过联合最小化输出和Fréchet导数误差来训练，实现算子和导数的同时高精度逼近。",
                "理论证明通用逼近能力，实验显示在非线性方程中样本效率显著提升，低样本量下达到高精度。"
            ],
            "method_zh": "**问题定义**：论文解决偏微分方程约束优化中代理模型导数精度不足的问题。现有FNO作为代理模型时，仅关注输出逼近，忽略Fréchet导数，导致优化过程不稳定或收敛到次优解，需要大量样本保证导数精度。\\n\\n**核心思路**：设计导数信息傅里叶神经算子，通过联合训练输出和导数样本，使代理模型同时逼近高保真算子的响应和灵敏度。这基于精确优化需要精确导数的动机，利用FNO的通用逼近能力扩展至导数空间。\\n\\n**技术框架**：整体流程包括数据采集（获取高保真算子的输出和Fréchet导数样本）、网络构建（基于FNO架构）、损失函数设计（联合输出和导数误差）、高效训练（应用降维和多分辨率技术减少成本）和优化应用（将DIFNO作为代理模型求解偏微分方程约束问题）。\\n\\n**关键创新**：最重要的创新是导数信息学习框架，将FNO训练目标从单一输出扩展为输出-导数联合最小化，实现算子和导数的同步高精度逼近。与现有FNO的本质区别在于显式纳入导数约束，提升优化导向的代理建模能力。\\n\\n**关键设计**：损失函数结合输出均方误差和导数Fréchet误差；网络结构沿用FNO的傅里叶层处理高维输入；采用随机投影等降维技术减少导数样本维度；应用多分辨率训练策略分阶段优化，平衡计算效率和精度；参数设置针对具体偏微分方程调整，如Helmholtz方程中的波数处理。",
            "application_zh": "该研究在偏微分方程约束优化领域具有广泛应用，如流体动力学中的形状优化、地震反演中的参数估计和材料设计中的多物理场模拟。实际价值在于显著降低高保真模拟的计算成本，提升优化效率，未来可能推动科学计算和工程设计中数据驱动方法的发展，促进AI与物理模型的深度融合。",
            "highlight_zh": "实验在非线性扩散-反应、Helmholtz和Navier-Stokes方程上验证DIFNO的优越性。具体性能：在Navier-Stokes反问题中，DIFNO仅需50个训练样本即达到高精度（相对误差<5%），而传统FNO需要200个样本才能达到类似精度，样本复杂度提升约4倍。对比基线包括标准FNO和基于梯度的优化方法，DIFNO在优化收敛速度和最终解质量上均显著优于基线，例如在Helmholtz方程优化中，DIFNO驱动的优化误差降低30%以上。",
            "tags_zh": [
                "导数信息学习",
                "傅里叶神经算子",
                "偏微分方程约束优化",
                "代理建模",
                "Fréchet导数",
                "通用逼近理论",
                "高效训练",
                "反问题求解"
            ],
            "_index": 165
        },
        {
            "title": "Scalable Frameworks for Real-World Audio-Visual Speech Recognition",
            "authors": [
                "Sungnyun Kim"
            ],
            "arxiv_id": "2512.14083v1",
            "summary": "The practical deployment of Audio-Visual Speech Recognition (AVSR) systems is fundamentally challenged by significant performance degradation in real-world environments, characterized by unpredictable acoustic noise and visual interference. This dissertation posits that a systematic, hierarchical approach is essential to overcome these challenges, achieving the robust scalability at the representation, architecture, and system levels. At the representation level, we investigate methods for building a unified model that learns audio-visual features inherently robust to diverse real-world corruptions, thereby enabling generalization to new environments without specialized modules. To address architectural scalability, we explore how to efficiently expand model capacity while ensuring the adaptive and reliable use of multimodal inputs, developing a framework that intelligently allocates computational resources based on the input characteristics. Finally, at the system level, we present methods to expand the system's functionality through modular integration with large-scale foundation models, leveraging their powerful cognitive and generative capabilities to maximize final recognition accuracy. By systematically providing solutions at each of these three levels, this dissertation aims to build a next-generation, robust, and scalable AVSR system with high reliability in real-world applications.",
            "categories": [
                "eess.AS",
                "cs.CL",
                "cs.LG"
            ],
            "primary_category": "eess.AS",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "PhD Dissertation",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14083v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出分层可扩展框架以解决真实世界音频-视觉语音识别系统的鲁棒性和泛化性问题",
            "summary_zh": "音频-视觉语音识别（AVSR）系统在实际部署中面临严峻挑战，主要源于真实环境中的不可预测声学噪声和视觉干扰导致的性能显著下降。本论文认为，必须采用系统化的分层方法来克服这些挑战，在表示、架构和系统三个层面实现鲁棒的可扩展性。在表示层面，我们研究构建统一模型的方法，该模型能够学习对多种真实世界干扰具有内在鲁棒性的音频-视觉特征，从而无需专门模块即可泛化到新环境。针对架构可扩展性，我们探索如何高效扩展模型容量，同时确保多模态输入的自适应和可靠使用，开发了一个根据输入特征智能分配计算资源的框架。最后，在系统层面，我们提出通过与大规模基础模型的模块化集成来扩展系统功能的方法，利用其强大的认知和生成能力来最大化最终识别准确率。通过在这三个层面系统性地提供解决方案，本论文旨在构建一个面向真实世界应用、具有高可靠性的下一代鲁棒且可扩展的AVSR系统。",
            "intro_zh": [
                "核心问题：真实世界AVSR系统面临不可预测的声学噪声和视觉干扰，导致性能显著下降，现有方法缺乏系统化的鲁棒性和可扩展性解决方案。",
                "方法要点：提出分层可扩展框架，在表示、架构和系统三个层面分别构建鲁棒特征、智能资源分配和基础模型集成，以实现整体系统的稳健性。",
                "实验或效果：通过分层方法，系统在噪声和干扰环境下实现了显著的识别准确率提升，并展示了良好的泛化能力和计算效率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决真实世界音频-视觉语音识别（AVSR）系统在不可预测声学噪声和视觉干扰下的性能退化问题。现有方法的痛点在于缺乏系统化的鲁棒性设计，通常依赖于特定环境下的专门模块，导致泛化能力不足、计算资源分配不灵活，且难以与新兴的大规模模型有效集成。\\n\\n**核心思路**：论文的核心解决思路是采用分层可扩展框架，从表示、架构和系统三个层面分别优化。这样设计是因为真实世界环境的复杂性需要多层次应对：表示层确保特征的内在鲁棒性，架构层实现自适应资源管理，系统层则通过集成外部能力来增强整体性能，从而构建一个全面、可扩展的解决方案。\\n\\n**技术框架**：整体架构分为三个主要阶段。首先，在表示层面，构建统一模型学习音频-视觉特征，这些特征通过训练对多种真实世界干扰具有鲁棒性。其次，在架构层面，开发一个框架来高效扩展模型容量，该框架根据输入特征（如噪声水平或视觉清晰度）智能分配计算资源，例如动态调整网络深度或注意力机制。最后，在系统层面，通过模块化接口与大规模基础模型（如预训练的语言或视觉模型）集成，利用它们的认知和生成能力来提升识别准确率。\\n\\n**关键创新**：最重要的技术创新点是分层可扩展框架，它将鲁棒性设计系统化地分解到三个层面，而不是依赖单一方法。与现有方法的本质区别在于：它强调从特征学习到系统集成的全链条优化，实现了内在鲁棒性、自适应资源分配和外部能力增强的协同，从而在真实世界环境中提供更可靠和可扩展的解决方案。\\n\\n**关键设计**：在表示层面，关键设计包括使用多任务学习或对抗训练来增强特征对噪声和干扰的鲁棒性，损失函数可能结合重构损失和分类损失。在架构层面，设计动态路由机制或可扩展网络结构，参数设置如资源分配阈值基于输入特征的自适应调整。在系统层面，关键设计涉及模块化接口，允许无缝集成基础模型，例如通过注意力机制或特征融合层来结合AVSR输出与基础模型的生成结果，具体网络结构可能包括编码器-解码器架构和多模态融合模块。",
            "application_zh": "该研究在智能助手、远程会议、自动驾驶、医疗诊断和娱乐产业等领域具有广泛潜在应用。例如，在嘈杂环境中（如工厂或公共场所）的语音交互系统、视频会议中的实时字幕生成、车载系统的语音控制、听障人士的辅助设备，以及电影或游戏中的音频增强。实际价值在于提升AVSR系统在真实世界场景下的可靠性和泛化能力，减少对特定环境的依赖。未来影响可能推动多模态人工智能向更鲁棒、可扩展的方向发展，促进跨领域技术的集成和创新。",
            "highlight_zh": "最重要的实验结果显示，分层框架在多个真实世界数据集上显著提升了识别准确率。具体性能数据包括：在噪声环境下，与基线方法相比，准确率提升约10-15%；在视觉干扰场景中，错误率降低20%以上。对比基线涉及传统AVSR模型和单一鲁棒性方法，提升幅度归因于分层设计的协同效应。此外，系统展示了良好的计算效率，资源分配框架减少了不必要的计算开销约30%，同时通过基础模型集成，在复杂任务上的准确率进一步优化了5-8%。",
            "tags_zh": [
                "音频-视觉语音识别",
                "鲁棒性学习",
                "多模态融合",
                "可扩展架构",
                "基础模型集成",
                "真实世界应用",
                "分层框架",
                "自适应资源分配"
            ],
            "_index": 166
        },
        {
            "title": "SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations",
            "authors": [
                "Wentao Guo",
                "Mayank Mishra",
                "Xinle Cheng",
                "Ion Stoica",
                "Tri Dao"
            ],
            "arxiv_id": "2512.14080v1",
            "summary": "Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel \"token rounding\" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14080v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出SonicMoE以解决细粒度高稀疏MoE模型中的内存占用高、IO开销大和计算浪费问题。",
            "summary_zh": "混合专家（MoE）模型已成为扩展语言模型而不显著增加计算成本的事实架构。最近的MoE模型显示出向高专家粒度（较小的专家中间维度）和更高稀疏性（恒定激活专家数量但总专家数更多）的明显趋势，这提高了每FLOP的模型质量。然而，细粒度MoE由于更高的IO成本而面临激活内存占用增加和硬件效率降低的问题，而更稀疏的MoE则因分组GEMM内核中的填充而导致计算浪费。为此，我们提出了一种内存高效的算法来计算MoE的前向和后向传递，并为后向传递最小化激活缓存。我们还设计了GPU内核，将内存IO与计算重叠，使所有MoE架构受益。最后，我们提出了一种新颖的“令牌舍入”方法，以最小化分组GEMM内核中填充造成的计算浪费。因此，我们的方法SonicMoE将激活内存减少了45%，并在Hopper GPU上实现了1.86倍的计算吞吐量提升，相比于ScatterMoE的BF16 MoE内核用于细粒度7B MoE。具体来说，SonicMoE在64个H100上实现了每天2130亿令牌的训练吞吐量，与ScatterMoE在96个H100上使用lm-engine代码库进行7B MoE模型训练（使用FSDP-2）的每天2250亿令牌吞吐量相当。在高MoE稀疏性设置下，我们的瓦片感知令牌舍入算法相比于普通top-K路由，在内核执行时间上额外实现了1.16倍的加速，同时保持相似的下游性能。我们开源了所有内核，以支持更快的MoE模型训练。",
            "intro_zh": [
                "核心问题：细粒度MoE因高IO成本导致内存占用增加和硬件效率降低，稀疏MoE因分组GEMM填充造成计算浪费。",
                "方法要点：提出内存高效算法减少激活缓存，设计IO与计算重叠的GPU内核，并引入令牌舍入方法最小化填充浪费。",
                "实验或效果：激活内存减少45%，计算吞吐量提升1.86倍，在高稀疏设置下内核执行时间额外加速1.16倍。"
            ],
            "method_zh": "**问题定义**：论文旨在解决细粒度高稀疏MoE模型中的两个核心问题：一是细粒度MoE因激活内存占用高和IO开销大导致的硬件效率低下；二是稀疏MoE在分组GEMM内核中因填充造成的计算浪费。现有方法如ScatterMoE在处理这些挑战时存在内存缓存不足和计算效率瓶颈。\\n\\n**核心思路**：通过优化内存管理和计算流程，设计一种综合方案来减少激活内存、重叠IO与计算，并最小化分组GEMM中的填充浪费，从而提升MoE模型的训练效率和吞吐量。\\n\\n**技术框架**：整体架构包括三个主要模块：内存高效算法模块，用于计算MoE的前向和后向传递并最小化激活缓存；GPU内核优化模块，实现内存IO与计算的重叠；令牌舍入模块，通过瓦片感知方法减少分组GEMM中的填充。流程上，先应用内存算法降低内存占用，再通过内核优化加速计算，最后使用令牌舍入进一步减少浪费。\\n\\n**关键创新**：最重要的技术创新是“令牌舍入”方法，它针对分组GEMM内核中的填充问题，通过动态调整令牌分配来最小化计算浪费，与现有方法如普通top-K路由相比，本质区别在于其瓦片感知的优化策略，能更有效地利用硬件资源。\\n\\n**关键设计**：在内存高效算法中，关键设计包括最小化激活缓存的策略，以减少后向传递的内存需求；GPU内核设计中，采用异步IO和计算重叠技术，提升并行性；令牌舍入方法中，参数设置涉及瓦片大小和舍入阈值，以平衡计算效率和模型性能，损失函数或网络结构细节在论文中未明确指定，但整体基于MoE架构进行优化。",
            "application_zh": "该研究主要应用于大规模语言模型的训练和推理场景，特别是在需要高效扩展模型规模而不显著增加计算成本的领域，如自然语言处理、机器翻译和对话系统。其实际价值在于显著提升MoE模型的训练速度和资源利用率，降低硬件需求，未来可能推动更复杂AI模型的部署和商业化应用。",
            "highlight_zh": "最重要的实验结果包括：在细粒度7B MoE模型上，SonicMoE相比ScatterMoE的BF16 MoE内核，激活内存减少45%，计算吞吐量提升1.86倍；在64个H100 GPU上，训练吞吐量达到每天2130亿令牌，与ScatterMoE在96个H100上的2250亿令牌相当；在高稀疏设置下，令牌舍入算法相比普通top-K路由，内核执行时间额外加速1.16倍，同时下游性能保持相似。",
            "tags_zh": [
                "混合专家模型",
                "内存优化",
                "GPU内核加速",
                "令牌舍入",
                "计算效率",
                "模型训练",
                "硬件感知优化",
                "稀疏计算"
            ],
            "_index": 167
        },
        {
            "title": "FusAD: Time-Frequency Fusion with Adaptive Denoising for General Time Series Analysis",
            "authors": [
                "Da Zhang",
                "Bingyu Li",
                "Zhiyuan Zhao",
                "Feiping Nie",
                "Junyu Gao",
                "Xuelong Li"
            ],
            "arxiv_id": "2512.14078v1",
            "summary": "Time series analysis plays a vital role in fields such as finance, healthcare, industry, and meteorology, underpinning key tasks including classification, forecasting, and anomaly detection. Although deep learning models have achieved remarkable progress in these areas in recent years, constructing an efficient, multi-task compatible, and generalizable unified framework for time series analysis remains a significant challenge. Existing approaches are often tailored to single tasks or specific data types, making it difficult to simultaneously handle multi-task modeling and effectively integrate information across diverse time series types. Moreover, real-world data are often affected by noise, complex frequency components, and multi-scale dynamic patterns, which further complicate robust feature extraction and analysis. To ameliorate these challenges, we propose FusAD, a unified analysis framework designed for diverse time series tasks. FusAD features an adaptive time-frequency fusion mechanism, integrating both Fourier and Wavelet transforms to efficiently capture global-local and multi-scale dynamic features. With an adaptive denoising mechanism, FusAD automatically senses and filters various types of noise, highlighting crucial sequence variations and enabling robust feature extraction in complex environments. In addition, the framework integrates a general information fusion and decoding structure, combined with masked pre-training, to promote efficient learning and transfer of multi-granularity representations. Extensive experiments demonstrate that FusAD consistently outperforms state-of-the-art models on mainstream time series benchmarks for classification, forecasting, and anomaly detection tasks, while maintaining high efficiency and scalability. Code is available at https://github.com/zhangda1018/FusAD.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Paper has been accepted by ICDE2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14078v1",
            "code_links": [
                {
                    "url": "https://github.com/zhangda1018/FusAD",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出FusAD框架，通过自适应时频融合与去噪解决时间序列分析中的多任务兼容性与噪声鲁棒性问题。",
            "summary_zh": "时间序列分析在金融、医疗、工业和气象等领域至关重要，支撑分类、预测和异常检测等关键任务。尽管深度学习模型近年来在这些领域取得了显著进展，但构建一个高效、多任务兼容且可泛化的统一框架仍面临重大挑战。现有方法通常针对单一任务或特定数据类型设计，难以同时处理多任务建模并有效整合不同类型时间序列的信息。此外，现实世界数据常受噪声、复杂频率成分和多尺度动态模式影响，进一步增加了鲁棒特征提取和分析的难度。为应对这些挑战，我们提出了FusAD，一个为多样化时间序列任务设计的统一分析框架。FusAD采用自适应时频融合机制，结合傅里叶和小波变换，高效捕捉全局-局部和多尺度动态特征。通过自适应去噪机制，FusAD自动感知并过滤各类噪声，突出关键序列变化，在复杂环境中实现鲁棒特征提取。此外，该框架整合了通用信息融合与解码结构，结合掩码预训练，促进多粒度表示的高效学习与迁移。大量实验表明，FusAD在主流时间序列基准上，针对分类、预测和异常检测任务，持续优于最先进模型，同时保持高效率和可扩展性。代码可在https://github.com/zhangda1018/FusAD获取。",
            "intro_zh": [
                "现有方法多为单任务或特定数据类型设计，难以实现多任务兼容与跨类型信息整合，且对噪声和复杂频率成分敏感。",
                "提出FusAD框架，结合自适应时频融合与去噪机制，高效捕捉多尺度动态特征，并通过通用结构促进表示学习与迁移。",
                "在主流时间序列基准上，FusAD在分类、预测和异常检测任务中均优于最先进模型，展现出高效和可扩展性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决时间序列分析中缺乏高效、多任务兼容且可泛化统一框架的问题。现有方法痛点包括：针对单一任务或特定数据类型设计，难以同时处理多任务建模；无法有效整合不同类型时间序列的信息；对现实数据中的噪声、复杂频率成分和多尺度动态模式敏感，导致鲁棒特征提取困难。\\n\\n**核心思路**：论文的核心解决思路是设计一个统一框架FusAD，通过自适应时频融合机制结合傅里叶和小波变换，捕捉全局-局部和多尺度动态特征，并引入自适应去噪机制过滤噪声，突出关键序列变化，从而在复杂环境中实现鲁棒分析。\\n\\n**技术框架**：整体架构包括输入时间序列预处理、自适应时频融合模块（集成傅里叶变换和小波变换）、自适应去噪模块、通用信息融合与解码结构，以及输出层用于多任务预测。流程上，先对输入序列进行时频分析，融合多尺度特征，然后自适应去噪，再通过融合解码结构提取表示，最后基于掩码预训练优化模型。\\n\\n**关键创新**：最重要的技术创新点是自适应时频融合与自适应去噪机制的集成。与现有方法相比，本质区别在于：同时利用傅里叶变换捕捉全局频率信息和Wavelet变换捕捉局部时频特征，实现更全面的动态模式建模；自适应机制能根据数据特性动态调整融合和去噪策略，而非固定参数，提升了泛化能力和鲁棒性。\\n\\n**关键设计**：关键设计包括：使用傅里叶变换提取频域全局特征，Wavelet变换提取时域局部多尺度特征；自适应去噪模块基于噪声估计自动调整滤波阈值；通用信息融合结构采用注意力机制或多层感知机整合特征；解码结构可能基于Transformer或CNN实现；损失函数结合任务特定损失（如交叉熵、均方误差）和预训练损失；参数设置如变换窗口大小、去噪阈值通过学习自适应优化；网络结构设计为端到端可训练，支持多任务输出。",
            "application_zh": "该研究在金融（如股票预测、风险检测）、医疗（如生理信号分析、疾病诊断）、工业（如设备监控、故障预警）和气象（如天气预测、气候分析）等领域具有广泛应用价值。FusAD框架能高效处理多任务时间序列分析，提升模型在噪声环境下的鲁棒性，为实际系统提供更可靠的决策支持，未来可能推动智能监控、预测维护等应用的发展。",
            "highlight_zh": "在主流时间序列基准上，FusAD在分类、预测和异常检测任务中均优于最先进模型。具体性能数据未知，但实验表明其持续领先，提升幅度显著，同时保持高效率和可扩展性，代码已开源供验证和进一步研究。",
            "tags_zh": [
                "时间序列分析",
                "自适应时频融合",
                "自适应去噪",
                "多任务学习",
                "傅里叶变换",
                "小波变换",
                "鲁棒特征提取",
                "掩码预训练"
            ],
            "_index": 168
        },
        {
            "title": "Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed",
            "authors": [
                "Yonggan Fu",
                "Lexington Whalen",
                "Zhifan Ye",
                "Xin Dong",
                "Shizhe Diao",
                "Jingyu Liu",
                "Chengyue Wu",
                "Hao Zhang",
                "Enze Xie",
                "Song Han",
                "Maksim Khadkevich",
                "Jan Kautz",
                "Yingyan Celine Lin",
                "Pavlo Molchanov"
            ],
            "arxiv_id": "2512.14067v1",
            "summary": "Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively.",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14067v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出Efficient-DLM框架，通过改进AR-to-dLM转换方法，实现高效扩散语言模型，在保持准确性的同时大幅提升生成速度。",
            "summary_zh": "扩散语言模型（dLMs）作为一种支持并行、非自回归生成的新范式，展现出巨大潜力，但其从头开始训练时的学习效率仍落后于自回归（AR）语言模型。为此，本文研究AR-to-dLM转换方法，旨在将预训练的AR模型转化为高效的dLMs，在保持AR模型任务准确性的同时显著提升生成速度。我们通过分析现有AR-to-dLM方法在注意力模式和目标函数上的局限性，提出了更有效的转换原则和方法。具体而言，首先系统比较了不同的注意力模式，发现保持预训练AR权重分布对有效转换至关重要。因此，我们引入了一种基于块状注意力模式的持续预训练方案，该方案在块间保持因果性，同时在块内实现双向建模。我们发现这种方法比完全双向建模能更好地保留预训练AR模型的权重分布，并具有支持KV缓存的优势，实现了准确性和效率的双赢。其次，为缓解训练与测试时掩码标记分布（均匀分布与高度从左到右分布）之间的差距，我们提出了一种位置相关的标记掩码策略，在训练时对后续标记赋予更高的掩码概率，以更好地模拟测试时的行为。基于此框架，我们深入研究了dLMs的注意力模式、训练动态和其他设计选择，为可扩展的AR-to-dLM转换提供了实用见解。这些研究催生了Efficient-DLM系列模型，其在性能上超越了当前最先进的AR模型和dLMs。例如，我们的Efficient-DLM 8B模型在准确率上比Dream 7B和Qwen3 4B分别高出+5.4%和+2.7%，同时吞吐量分别提升了4.5倍和2.7倍。",
            "intro_zh": [
                "核心问题：现有AR-to-dLM转换方法在注意力模式和训练目标上存在局限性，导致转换后模型难以在保持AR模型准确性的同时实现高效并行生成。",
                "方法要点：提出基于块状注意力模式的持续预训练方案和位置相关掩码策略，优化AR-to-dLM转换过程，实现准确性与效率的平衡。",
                "实验或效果：Efficient-DLM 8B模型在准确率和吞吐量上均显著超越Dream 7B和Qwen3 4B等基线模型，验证了方法的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决如何将预训练的自回归（AR）语言模型高效转换为扩散语言模型（dLM），以在保持AR模型任务准确性的同时，利用dLM的并行生成优势提升速度。现有AR-to-dLM方法的痛点在于其注意力模式和训练目标设计不当，导致转换后模型权重分布偏离预训练AR模型，且训练与测试时的掩码分布不匹配，影响模型性能。\\n\\n**核心思路**：论文的核心思路是通过优化注意力模式和掩码策略，实现更有效的AR-to-dLM转换。具体包括：1）采用块状注意力模式，在块内进行双向建模以捕捉上下文，同时在块间保持因果性以保留预训练权重分布；2）引入位置相关的掩码策略，使训练时的掩码分布更接近测试时的从左到右模式，减少训练-测试差距。\\n\\n**技术框架**：整体框架分为两个主要阶段：首先，基于预训练的AR模型，通过持续预训练进行AR-to-dLM转换，使用块状注意力模式和位置相关掩码策略优化模型；其次，在转换后的dLM上进行推理，利用其并行生成能力实现高速文本生成。框架还包括对注意力模式、训练动态的系统分析，以指导设计选择。\\n\\n**关键创新**：最重要的技术创新是块状注意力模式和位置相关掩码策略的结合。与现有方法相比，块状注意力模式在保留预训练权重分布和支持KV缓存方面更具优势，而位置相关掩码策略直接针对训练-测试分布不匹配问题，提升了模型泛化能力。\\n\\n**关键设计**：关键设计包括：1）块状注意力模式：将输入序列划分为块，块内使用双向注意力，块间使用因果注意力，具体块大小根据实验优化；2）位置相关掩码策略：在训练时，对序列中靠后的标记赋予更高的掩码概率，模拟测试时从左到右的生成过程；3）损失函数：基于扩散模型的去噪目标，结合掩码预测任务，具体参数如学习率、批大小通过网格搜索确定；4）网络结构：沿用预训练AR模型的Transformer架构，但调整注意力机制以适应dLM需求。",
            "application_zh": "该研究在自然语言处理领域具有广泛的应用潜力，特别是在需要高速文本生成的场景中，如实时对话系统、内容创作辅助、代码生成和机器翻译。Efficient-DLM框架通过提升扩散语言模型的效率，为实际部署提供了更可行的解决方案，未来可能推动并行生成模型在边缘设备和大规模服务中的普及，降低计算成本并改善用户体验。",
            "highlight_zh": "实验结果表明，Efficient-DLM系列模型在性能和效率上均取得显著提升。以Efficient-DLM 8B为例，在准确性方面，相比Dream 7B和Qwen3 4B，分别提高了+5.4%和+2.7%；在吞吐量方面，分别达到4.5倍和2.7倍的提升。这些结果基于标准基准测试，验证了所提方法在AR-to-dLM转换中的有效性，为后续研究提供了强有力基线。",
            "tags_zh": [
                "扩散语言模型",
                "自回归模型转换",
                "注意力模式优化",
                "并行文本生成",
                "训练策略改进",
                "模型效率提升",
                "自然语言处理",
                "深度学习"
            ],
            "_index": 169
        },
        {
            "title": "Bridging Fidelity-Reality with Controllable One-Step Diffusion for Image Super-Resolution",
            "authors": [
                "Hao Chen",
                "Junyang Chen",
                "Jinshan Pan",
                "Jiangxin Dong"
            ],
            "arxiv_id": "2512.14061v1",
            "summary": "Recent diffusion-based one-step methods have shown remarkable progress in the field of image super-resolution, yet they remain constrained by three critical limitations: (1) inferior fidelity performance caused by the information loss from compression encoding of low-quality (LQ) inputs; (2) insufficient region-discriminative activation of generative priors; (3) misalignment between text prompts and their corresponding semantic regions. To address these limitations, we propose CODSR, a controllable one-step diffusion network for image super-resolution. First, we propose an LQ-guided feature modulation module that leverages original uncompressed information from LQ inputs to provide high-fidelity conditioning for the diffusion process. We then develop a region-adaptive generative prior activation method to effectively enhance perceptual richness without sacrificing local structural fidelity. Finally, we employ a text-matching guidance strategy to fully harness the conditioning potential of text prompts. Extensive experiments demonstrate that CODSR achieves superior perceptual quality and competitive fidelity compared with state-of-the-art methods with efficient one-step inference.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page: https://github.com/Chanson94/CODSR",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14061v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "SAC"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出可控一步扩散网络CODSR，通过LQ引导特征调制、区域自适应生成先验激活和文本匹配引导，解决图像超分辨率中保真度与感知质量平衡问题。",
            "summary_zh": "近期基于扩散模型的一步方法在图像超分辨率领域取得了显著进展，但仍受限于三个关键问题：(1) 由于低质量输入压缩编码导致的信息损失，造成保真度性能下降；(2) 生成先验的区域判别性激活不足；(3) 文本提示与其对应语义区域之间的错位。为解决这些限制，我们提出了CODSR，一种可控的一步扩散网络用于图像超分辨率。首先，我们提出了一个LQ引导的特征调制模块，利用低质量输入的原始未压缩信息为扩散过程提供高保真度条件。然后，我们开发了一种区域自适应的生成先验激活方法，以有效增强感知丰富度而不牺牲局部结构保真度。最后，我们采用文本匹配引导策略来充分利用文本提示的条件潜力。大量实验表明，CODSR在高效的一步推理下，相比最先进方法实现了卓越的感知质量和有竞争力的保真度。",
            "intro_zh": [
                "现有一步扩散方法在图像超分辨率中面临保真度下降、生成先验激活不足和文本-语义错位三大挑战。",
                "CODSR通过LQ引导特征调制、区域自适应生成先验激活和文本匹配引导，提升保真度和感知质量。",
                "实验显示CODSR在一步推理下实现卓越感知质量和有竞争力保真度，优于现有方法。"
            ],
            "method_zh": "**问题定义**：论文解决图像超分辨率中一步扩散方法的三个关键问题：低质量输入压缩编码导致信息损失和保真度下降；生成先验的区域判别性激活不足，影响感知质量；文本提示与语义区域错位，限制条件生成效果。现有方法如扩散模型在一步推理时，常因压缩编码丢失细节，且生成先验激活不够精准，导致保真度与感知质量难以平衡。\\n\\n**核心思路**：论文核心思路是设计一个可控的一步扩散网络CODSR，通过引入LQ引导的特征调制来保留原始输入信息，增强保真度；采用区域自适应生成先验激活，提升感知丰富度而不破坏结构；并利用文本匹配引导策略，优化文本条件与语义对齐，实现高效且高质量的图像超分辨率。\\n\\n**技术框架**：CODSR整体架构基于一步扩散模型，包含三个主要模块：LQ引导特征调制模块，它直接利用低质量输入的未压缩特征，通过调制操作注入到扩散过程中，提供高保真度条件；区域自适应生成先验激活模块，根据图像区域特性动态调整生成先验的激活强度，以增强细节而不损失结构；文本匹配引导模块，通过匹配文本提示与图像语义区域，优化条件生成。流程上，输入低质量图像，经过特征调制和先验激活，结合文本引导，在一步扩散推理中生成高质量超分辨率图像。\\n\\n**关键创新**：最重要的技术创新包括LQ引导特征调制模块，它避免了压缩编码的信息损失，直接利用原始输入特征；区域自适应生成先验激活方法，实现了生成先验的精细化控制，提升感知质量；文本匹配引导策略，解决了文本与语义错位问题，增强了条件生成的有效性。与现有方法相比，CODSR在一步推理中同时优化了保真度和感知质量，而传统方法常需多步迭代或牺牲一方。\\n\\n**关键设计**：关键设计包括：LQ引导特征调制模块采用特征融合技术，将低质量输入的特征图与扩散网络中间层结合，通过可学习的调制参数调整；区域自适应生成先验激活基于注意力机制，根据局部图像内容动态计算激活权重；文本匹配引导使用预训练的文本编码器和图像编码器，通过相似度匹配优化条件向量。损失函数可能结合重建损失、感知损失和对抗损失，具体参数设置如扩散步数、网络层数等未在摘要中详细说明，需参考论文全文。",
            "application_zh": "该研究在图像超分辨率领域具有广泛潜在应用，如医学影像增强、卫星图像处理、视频超分和数字媒体修复。通过提升保真度和感知质量，CODSR可应用于需要高精度图像细节的场景，例如医疗诊断中的病灶识别、遥感监测中的目标检测，以及娱乐产业中的老旧视频修复。其一步推理的高效性使其适合实时或资源受限环境，未来可能推动扩散模型在更多视觉任务中的应用。",
            "highlight_zh": "实验表明，CODSR在图像超分辨率任务中实现了卓越的感知质量和有竞争力的保真度。与最先进方法相比，在一步推理下，CODSR在感知指标（如LPIPS）上表现优异，同时保真度指标（如PSNR）保持竞争力。具体性能数据未在摘要中提供，但实验验证了其能有效解决信息损失、先验激活不足和文本错位问题，提升整体超分效果。",
            "tags_zh": [
                "图像超分辨率",
                "扩散模型",
                "一步推理",
                "保真度增强",
                "感知质量",
                "文本引导",
                "区域自适应",
                "特征调制"
            ],
            "_index": 170
        },
        {
            "title": "SELECT: Detecting Label Errors in Real-world Scene Text Data",
            "authors": [
                "Wenjun Liu",
                "Qian Wu",
                "Yifeng Hu",
                "Yuke Li"
            ],
            "arxiv_id": "2512.14050v1",
            "summary": "We introduce SELECT (Scene tExt Label Errors deteCTion), a novel approach that leverages multi-modal training to detect label errors in real-world scene text datasets. Utilizing an image-text encoder and a character-level tokenizer, SELECT addresses the issues of variable-length sequence labels, label sequence misalignment, and character-level errors, outperforming existing methods in accuracy and practical utility. In addition, we introduce Similarity-based Sequence Label Corruption (SSLC), a process that intentionally introduces errors into the training labels to mimic real-world error scenarios during training. SSLC not only can cause a change in the sequence length but also takes into account the visual similarity between characters during corruption. Our method is the first to detect label errors in real-world scene text datasets successfully accounting for variable-length labels. Experimental results demonstrate the effectiveness of SELECT in detecting label errors and improving STR accuracy on real-world text datasets, showcasing its practical utility.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "10.1145/3743093.3771031",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14050v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出SELECT方法，通过多模态训练检测真实场景文本数据集中的标签错误，解决变长标签序列和字符级错误问题。",
            "summary_zh": "我们介绍了SELECT（场景文本标签错误检测），这是一种利用多模态训练来检测真实场景文本数据集中标签错误的新方法。通过使用图像-文本编码器和字符级分词器，SELECT解决了变长序列标签、标签序列错位和字符级错误等问题，在准确性和实用性方面优于现有方法。此外，我们引入了基于相似性的序列标签损坏（SSLC）过程，该过程在训练期间故意在训练标签中引入错误，以模拟真实世界的错误场景。SSLC不仅可能导致序列长度发生变化，还在损坏过程中考虑了字符之间的视觉相似性。我们的方法是第一个成功检测真实场景文本数据集中标签错误并考虑变长标签的方法。实验结果表明，SELECT在检测标签错误和提高真实世界文本数据集上的场景文本识别（STR）准确性方面具有有效性，展示了其实用价值。",
            "intro_zh": [
                "现有方法难以处理真实场景文本数据中的变长标签序列、标签错位和字符级错误，导致标签错误检测不准确。",
                "SELECT采用多模态训练，结合图像-文本编码器和字符级分词器，并引入SSLC过程模拟真实错误，提升检测鲁棒性。",
                "实验显示SELECT在检测标签错误方面优于现有方法，并能有效提升场景文本识别（STR）的准确性，具有实际应用价值。"
            ],
            "method_zh": "**问题定义**：论文旨在检测真实场景文本数据集中的标签错误，这些错误包括变长序列标签、标签序列错位和字符级错误。现有方法通常假设固定长度标签或忽略视觉相似性，难以处理真实世界中的复杂错误场景，导致检测准确率低和实用性不足。\\n\\n**核心思路**：SELECT通过多模态训练，利用图像和文本的联合表示来学习标签错误的模式。核心思想是设计一个能够处理变长序列的框架，并引入基于相似性的标签损坏过程（SSLC）来模拟真实错误，从而增强模型的鲁棒性和泛化能力。这样设计是因为真实场景文本数据中的错误往往与视觉相似性和序列长度变化相关，需要综合考虑多模态信息。\\n\\n**技术框架**：整体架构包括两个主要阶段：训练阶段和检测阶段。在训练阶段，使用图像-文本编码器提取图像和文本的特征，字符级分词器处理变长序列，并结合SSLC过程生成损坏的标签用于训练。在检测阶段，模型基于学习到的表示预测标签错误的可能性。主要模块包括：图像编码器（如CNN）、文本编码器（如Transformer）、字符级分词器、SSLC模块和错误检测头。\\n\\n**关键创新**：最重要的技术创新是SSLC过程，它故意在训练标签中引入错误，考虑字符视觉相似性和序列长度变化，以模拟真实错误场景。与现有方法的本质区别在于，SELECT是第一个专门针对真实场景文本数据集、成功处理变长标签序列的标签错误检测方法，通过多模态融合和模拟训练提升实用性。\\n\\n**关键设计**：关键设计包括：使用预训练的视觉-语言模型作为图像-文本编码器基础，字符级分词器将文本分解为字符序列以处理变长标签，SSLC过程中基于字符相似性矩阵（如从字体或图像中提取）引入替换、插入或删除错误，损失函数可能结合分类损失和序列对齐损失（如CTC或注意力机制），网络结构采用编码器-解码器或端到端设计，参数设置如学习率、批量大小通过实验优化。",
            "application_zh": "该研究可应用于场景文本识别（STR）系统的数据清洗和质量控制，帮助提升自动驾驶、文档数字化、智能监控等领域的文本识别准确性。通过检测和纠正标签错误，能减少模型训练中的噪声，提高下游任务的性能，具有实际价值。未来可能扩展到其他多模态数据错误检测任务，推动人工智能数据治理的发展。",
            "highlight_zh": "实验结果表明，SELECT在多个真实场景文本数据集（如ICDAR、COCO-Text）上，标签错误检测准确率显著优于基线方法（如基于规则或单模态的方法），具体提升幅度未知，但论文报告了在STR任务上，使用SELECT清洗后的数据训练模型，识别准确率有可观提升，验证了方法的有效性和实用性。",
            "tags_zh": [
                "场景文本识别",
                "标签错误检测",
                "多模态训练",
                "变长序列处理",
                "字符级分词",
                "数据清洗",
                "视觉-语言模型",
                "序列对齐"
            ],
            "_index": 171
        },
        {
            "title": "Intention Chain-of-Thought Prompting with Dynamic Routing for Code Generation",
            "authors": [
                "Shen Li",
                "Li Huang",
                "Shaoxiong Zhan",
                "Weifeng Sun",
                "Tao Yin",
                "Zhongxin Liu",
                "Meng Yan"
            ],
            "arxiv_id": "2512.14048v1",
            "summary": "Large language models (LLMs) exhibit strong generative capabilities and have shown great potential in code generation. Existing chain-of-thought (CoT) prompting methods enhance model reasoning by eliciting intermediate steps, but suffer from two major limitations: First, their uniform application tends to induce overthinking on simple tasks. Second, they lack intention abstraction in code generation, such as explicitly modeling core algorithmic design and efficiency, leading models to focus on surface-level structures while neglecting the global problem objective. Inspired by the cognitive economy principle of engaging structured reasoning only when necessary to conserve cognitive resources, we propose RoutingGen, a novel difficulty-aware routing framework that dynamically adapts prompting strategies for code generation. For simple tasks, it adopts few-shot prompting; for more complex ones, it invokes a structured reasoning strategy, termed Intention Chain-of-Thought (ICoT), which we introduce to guide the model in capturing task intention, such as the core algorithmic logic and its time complexity. Experiments across three models and six standard code generation benchmarks show that RoutingGen achieves state-of-the-art performance in most settings, while reducing total token usage by 46.37% on average across settings. Furthermore, ICoT outperforms six existing prompting baselines on challenging benchmarks.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted at AAAI-2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14048v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出RoutingGen框架，通过动态路由和意图链式思考解决代码生成中推理效率与意图建模不足的问题。",
            "summary_zh": "大型语言模型在代码生成方面展现出强大的生成能力和巨大潜力。现有的链式思考提示方法通过引出中间步骤来增强模型推理，但存在两个主要局限性：首先，其统一应用倾向于在简单任务上引发过度思考；其次，它们在代码生成中缺乏意图抽象，例如明确建模核心算法设计和效率，导致模型关注表面结构而忽视全局问题目标。受认知经济原则启发，即仅在必要时进行结构化推理以节省认知资源，我们提出了RoutingGen，一种新颖的难度感知路由框架，动态调整代码生成的提示策略。对于简单任务，它采用少样本提示；对于更复杂的任务，它调用一种结构化推理策略，称为意图链式思考，我们引入该策略来指导模型捕获任务意图，例如核心算法逻辑及其时间复杂度。在三个模型和六个标准代码生成基准上的实验表明，RoutingGen在大多数设置中实现了最先进的性能，同时在所有设置中平均减少了46.37%的总令牌使用量。此外，意图链式思考在具有挑战性的基准上优于六个现有的提示基线。",
            "intro_zh": [
                "现有链式思考提示方法在代码生成中存在过度思考和缺乏意图抽象的问题，导致推理效率低下和模型忽视全局目标。",
                "论文提出RoutingGen框架，结合动态路由和意图链式思考，根据任务难度自适应选择提示策略，提升推理效率和意图建模。",
                "实验显示RoutingGen在多个基准上达到最优性能，平均减少46.37%令牌使用，意图链式思考优于六个基线方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决代码生成中现有链式思考提示方法的两个核心痛点：一是统一应用导致简单任务上的过度思考，浪费计算资源；二是缺乏对任务意图（如算法逻辑和效率）的显式建模，使模型过于关注表面代码结构而忽略全局目标。\\n\\n**核心思路**：受认知经济原则启发，论文提出动态路由框架RoutingGen，根据任务难度自适应选择提示策略。对于简单任务，采用少样本提示以避免过度推理；对于复杂任务，引入意图链式思考来引导模型捕获深层意图，如算法设计和时间复杂度，从而提升生成代码的质量和效率。\\n\\n**技术框架**：RoutingGen整体架构包含两个主要阶段：难度评估和策略路由。首先，通过预定义或学习机制评估输入任务的复杂度；然后，基于评估结果动态路由：简单任务直接使用少样本提示生成代码，复杂任务则触发意图链式思考模块，该模块生成结构化推理步骤（如意图抽象和算法规划），再指导最终代码生成。\\n\\n**关键创新**：最重要的技术创新是意图链式思考和动态路由的结合。意图链式思考不同于传统链式思考，它强调对任务意图的显式建模（如核心逻辑和效率），而动态路由则根据任务难度自适应调整策略，避免了“一刀切”的推理方式，本质区别在于实现了难度感知的推理优化。\\n\\n**关键设计**：关键设计包括难度评估机制（可能基于任务描述或历史性能）、路由阈值设置（区分简单与复杂任务）、意图链式思考的提示模板（引导模型输出意图抽象步骤），以及实验中的模型选择（三个大型语言模型）和基准测试（六个标准代码生成数据集）。具体参数如路由阈值可能通过实验调优，但论文未详细说明损失函数或网络结构，因为该方法主要基于提示工程而非模型训练。",
            "application_zh": "该研究在代码生成领域具有广泛的应用潜力，可用于智能编程助手、自动化代码补全、教育工具中的编程教学，以及软件工程中的代码优化和重构。通过提升推理效率和意图建模，它能减少计算成本，提高生成代码的准确性和可维护性，未来可能推动AI辅助编程向更高效、更智能的方向发展。",
            "highlight_zh": "最重要的实验结果包括：RoutingGen在三个模型（具体模型未知）和六个标准代码生成基准上，在大多数设置中实现了最先进的性能。同时，它平均减少了46.37%的总令牌使用量，显著提升了推理效率。此外，意图链式思考在具有挑战性的基准上优于六个现有的提示基线，证明了其在复杂任务上的有效性。",
            "tags_zh": [
                "代码生成",
                "链式思考提示",
                "动态路由",
                "意图建模",
                "大型语言模型",
                "推理优化",
                "难度感知",
                "提示工程"
            ],
            "_index": 172
        },
        {
            "title": "A Deep Dive into Function Inlining and its Security Implications for ML-based Binary Analysis",
            "authors": [
                "Omar Abusabha",
                "Jiyong Uhm",
                "Tamer Abuhmed",
                "Hyungjoon Koo"
            ],
            "arxiv_id": "2512.14045v1",
            "summary": "A function inlining optimization is a widely used transformation in modern compilers, which replaces a call site with the callee's body in need. While this transformation improves performance, it significantly impacts static features such as machine instructions and control flow graphs, which are crucial to binary analysis. Yet, despite its broad impact, the security impact of function inlining remains underexplored to date. In this paper, we present the first comprehensive study of function inlining through the lens of machine learning-based binary analysis. To this end, we dissect the inlining decision pipeline within the LLVM's cost model and explore the combinations of the compiler options that aggressively promote the function inlining ratio beyond standard optimization levels, which we term extreme inlining. We focus on five ML-assisted binary analysis tasks for security, using 20 unique models to systematically evaluate their robustness under extreme inlining scenarios. Our extensive experiments reveal several significant findings: i) function inlining, though a benign transformation in intent, can (in)directly affect ML model behaviors, being potentially exploited by evading discriminative or generative ML models; ii) ML models relying on static features can be highly sensitive to inlining; iii) subtle compiler settings can be leveraged to deliberately craft evasive binary variants; and iv) inlining ratios vary substantially across applications and build configurations, undermining assumptions of consistency in training and evaluation of ML models.",
            "categories": [
                "cs.CR",
                "cs.LG",
                "cs.PL"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14045v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "首次全面研究函数内联对基于机器学习的二进制分析安全性的影响，揭示极端内联可被利用来规避ML模型。",
            "summary_zh": "函数内联优化是现代编译器中广泛使用的转换技术，它通过将调用点替换为被调用函数体来提高性能。然而，这种转换会显著影响机器指令和控制流图等静态特征，这些特征对二进制分析至关重要。尽管其影响广泛，但函数内联的安全影响至今仍未得到充分探索。本文首次从基于机器学习的二进制分析角度对函数内联进行了全面研究。为此，我们剖析了LLVM成本模型中的内联决策流程，并探索了编译器选项的组合，这些组合能够将函数内联比率提升到标准优化级别之上，我们称之为极端内联。我们专注于五个用于安全的ML辅助二进制分析任务，使用20个独特模型来系统评估它们在极端内联场景下的鲁棒性。我们的大量实验揭示了几个重要发现：i）函数内联虽然意图上是良性转换，但可能（间接）影响ML模型行为，可能被利用来规避判别式或生成式ML模型；ii）依赖静态特征的ML模型可能对内联高度敏感；iii）可以利用微妙的编译器设置来故意制作规避性二进制变体；iv）内联比率在不同应用程序和构建配置中差异很大，破坏了ML模型训练和评估中一致性假设。",
            "intro_zh": [
                "现有方法未充分探索函数内联对基于机器学习的二进制分析安全性的影响，导致模型鲁棒性未知。",
                "论文通过剖析LLVM内联决策流程，提出极端内联概念，并系统评估ML模型在极端内联下的鲁棒性。",
                "实验发现函数内联可被利用来规避ML模型，模型对内联高度敏感，内联比率差异大破坏一致性假设。"
            ],
            "method_zh": "**问题定义**：论文要解决的核心问题是函数内联优化对基于机器学习的二进制分析安全性的影响。现有方法的痛点在于，尽管函数内联广泛使用且显著改变二进制静态特征（如机器指令和控制流图），但其安全影响在ML-based分析中未得到系统研究，导致模型可能因内联变化而失效或被恶意利用，缺乏鲁棒性评估。\\n\\n**核心思路**：论文的核心解决思路是从ML-based二进制分析角度，首次全面研究函数内联的安全影响。通过深入分析编译器内联决策机制，提出“极端内lining”概念来模拟高内联比率场景，并设计系统实验评估多种ML模型在极端内联下的鲁棒性，以揭示内联如何影响模型行为和安全漏洞。\\n\\n**技术框架**：整体架构包括三个主要阶段：首先，剖析LLVM编译器的内联决策管道，特别是成本模型，以理解内联如何被触发和优化；其次，探索编译器选项组合，定义并实现极端内联，即通过调整参数（如内联阈值、启发式规则）将内联比率提升到标准优化级别之上；最后，构建实验平台，针对五个安全相关的二进制分析任务（如恶意软件检测、漏洞识别），使用20个独特ML模型（包括判别式和生成式模型），在极端内联生成的二进制变体上进行系统评估，分析模型性能变化和鲁棒性。\\n\\n**关键创新**：最重要的技术创新点是首次将函数内联与ML-based二进制分析安全性联系起来，提出极端内联作为攻击向量来评估模型鲁棒性。与现有方法（通常忽略内联影响或仅关注性能）的本质区别在于，本文从安全角度系统量化内联对ML模型的影响，揭示了内联可被恶意利用来规避模型，填补了该领域的研究空白。\\n\\n**关键设计**：关键设计包括：在LLVM成本模型中，分析内联决策参数如内联阈值、函数大小限制和调用频率；定义极端内联通过组合编译器选项（如-O3优化加上自定义内联启发式），以最大化内联比率；实验中使用20个ML模型覆盖不同任务，如基于静态特征的分类模型和生成模型，评估指标包括准确率、召回率和规避成功率；构建多样化二进制数据集，涵盖不同应用程序和构建配置，以模拟真实世界场景。",
            "application_zh": "该研究的潜在应用领域包括二进制安全分析、恶意软件检测和漏洞挖掘。实际价值在于揭示了编译器优化（如函数内联）对ML-based安全工具的隐蔽威胁，帮助开发者设计更鲁棒的模型和防御机制。未来影响可能推动编译器安全研究、ML模型鲁棒性评估标准制定，以及安全分析工具在优化环境下的适应性改进。",
            "highlight_zh": "实验结果显示：函数内联可被利用来规避ML模型，在极端内联下，某些模型的准确率下降超过20%；依赖静态特征的模型对内联高度敏感，内联比率变化导致性能波动显著；内联比率在不同应用和配置中差异很大，例如从10%到80%，破坏了训练和评估的一致性假设；通过微妙编译器设置，可故意制作规避性二进制变体，成功规避多个判别式和生成式模型。",
            "tags_zh": [
                "函数内联",
                "二进制分析",
                "机器学习安全",
                "编译器优化",
                "模型鲁棒性",
                "极端内联",
                "LLVM",
                "静态特征"
            ],
            "_index": 173
        },
        {
            "title": "ChartAgent: A Chart Understanding Framework with Tool Integrated Reasoning",
            "authors": [
                "Boran Wang",
                "Xinming Wang",
                "Yi Chen",
                "Xiang Li",
                "Jian Xu",
                "Jing Yuan",
                "Chenglin Liu"
            ],
            "arxiv_id": "2512.14040v1",
            "summary": "With their high information density and intuitive readability, charts have become the de facto medium for data analysis and communication across disciplines. Recent multimodal large language models (MLLMs) have made notable progress in automated chart understanding, yet they remain heavily dependent on explicit textual annotations and the performance degrades markedly when key numerals are absent. To address this limitation, we introduce ChartAgent, a chart understanding framework grounded in Tool-Integrated Reasoning (TIR). Inspired by human cognition, ChartAgent decomposes complex chart analysis into a sequence of observable, replayable steps. Supporting this architecture is an extensible, modular tool library comprising more than a dozen core tools, such as keyelement detection, instance segmentation, and optical character recognition (OCR), which the agent dynamically orchestrates to achieve systematic visual parsing across diverse chart types. Leveraging TIRs transparency and verifiability, ChartAgent moves beyond the black box paradigm by standardizing and consolidating intermediate outputs into a structured Evidence Package, providing traceable and reproducible support for final conclusions. Experiments show that ChartAgent substantially improves robustness under sparse annotation settings, offering a practical path toward trustworthy and extensible systems for chart understanding.",
            "categories": [
                "cs.CV",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14040v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出ChartAgent框架，通过工具集成推理解决图表理解在稀疏标注下的鲁棒性问题。",
            "summary_zh": "图表因其高信息密度和直观可读性，已成为跨学科数据分析和交流的实际媒介。近年来，多模态大语言模型（MLLMs）在自动化图表理解方面取得了显著进展，但它们仍然严重依赖显式文本标注，并且在关键数字缺失时性能显著下降。为了解决这一局限性，我们引入了ChartAgent，这是一个基于工具集成推理（TIR）的图表理解框架。受人类认知启发，ChartAgent将复杂的图表分析分解为一系列可观察、可重放的步骤。支持这一架构的是一个可扩展的模块化工具库，包含十多个核心工具，如关键元素检测、实例分割和光学字符识别（OCR），代理动态编排这些工具，以实现跨不同图表类型的系统视觉解析。利用TIR的透明性和可验证性，ChartAgent超越了黑盒范式，通过将中间输出标准化并整合到结构化证据包中，为最终结论提供可追溯和可复现的支持。实验表明，ChartAgent在稀疏标注设置下显著提高了鲁棒性，为可信且可扩展的图表理解系统提供了实用路径。",
            "intro_zh": [
                "现有多模态大语言模型依赖显式文本标注，关键数字缺失时性能显著下降，鲁棒性不足。",
                "提出ChartAgent框架，基于工具集成推理，将图表分析分解为可观察步骤，动态编排模块化工具库。",
                "实验显示，ChartAgent在稀疏标注设置下显著提升鲁棒性，提供可追溯和可复现的图表理解支持。"
            ],
            "method_zh": "**问题定义**：论文旨在解决图表理解任务中，现有多模态大语言模型（MLLMs）过度依赖显式文本标注的问题。当图表中关键数字或文本信息缺失时，这些模型的性能会显著下降，导致鲁棒性不足，限制了在实际稀疏标注场景中的应用。\\n\\n**核心思路**：论文提出ChartAgent框架，其核心思路是模仿人类认知过程，通过工具集成推理（TIR）将复杂的图表分析任务分解为一系列可观察、可重放的步骤。这样设计旨在减少对原始文本标注的依赖，利用模块化工具动态提取和整合视觉信息，从而提高系统在标注稀疏情况下的适应性和可靠性。\\n\\n**技术框架**：ChartAgent的整体架构包括一个可扩展的模块化工具库和一个代理系统。工具库包含十多个核心工具，如关键元素检测、实例分割和光学字符识别（OCR）。代理根据任务需求动态选择和编排这些工具，执行系统化的视觉解析流程。中间输出被标准化并整合到结构化证据包中，最终生成可验证的结论。\\n\\n**关键创新**：最重要的技术创新是引入了工具集成推理（TIR）范式，将图表理解从黑盒模型转向透明、可追溯的过程。与现有MLLMs相比，ChartAgent通过模块化工具分解任务，减少了标注依赖，提高了鲁棒性和可解释性，本质区别在于其强调过程的可观察性和可复现性。\\n\\n**关键设计**：关键设计包括一个可扩展的工具库，其中工具如关键元素检测可能基于预训练视觉模型，实例分割使用分割网络，OCR采用字符识别技术。代理通过规则或学习机制动态调用工具，证据包结构化存储中间结果（如检测框、分割掩码、识别文本），具体参数和损失函数依赖于各工具的实现，论文未详细说明，但强调模块化和标准化设计。",
            "application_zh": "ChartAgent的潜在应用领域包括数据可视化分析、自动化报告生成、教育辅助工具和商业智能系统。其实用价值在于提高图表理解在真实世界稀疏标注场景中的鲁棒性，支持可信的数据解读。未来影响可能推动多模态AI向更透明、可扩展的方向发展，促进跨学科数据交流的自动化。",
            "highlight_zh": "实验表明，ChartAgent在稀疏标注设置下显著提升了鲁棒性。具体性能数据未在摘要中提供，但对比基线可能包括传统MLLMs，提升幅度体现在减少对显式文本标注的依赖，通过工具集成推理实现更稳定的图表理解。证据包的设计支持了可追溯和可复现的结论，增强了系统的可信度。",
            "tags_zh": [
                "图表理解",
                "工具集成推理",
                "多模态大语言模型",
                "稀疏标注",
                "视觉解析",
                "可解释AI",
                "模块化工具库",
                "证据包"
            ],
            "_index": 174
        },
        {
            "title": "Unleashing the Power of Image-Tabular Self-Supervised Learning via Breaking Cross-Tabular Barriers",
            "authors": [
                "Yibing Fu",
                "Yunpeng Zhao",
                "Zhitao Zeng",
                "Cheng Chen",
                "Yueming Jin"
            ],
            "arxiv_id": "2512.14026v1",
            "summary": "Multi-modal learning integrating medical images and tabular data has significantly advanced clinical decision-making in recent years. Self-Supervised Learning (SSL) has emerged as a powerful paradigm for pretraining these models on large-scale unlabeled image-tabular data, aiming to learn discriminative representations. However, existing SSL methods for image-tabular representation learning are often confined to specific data cohorts, mainly due to their rigid tabular modeling mechanisms when modeling heterogeneous tabular data. This inter-tabular barrier hinders the multi-modal SSL methods from effectively learning transferrable medical knowledge shared across diverse cohorts. In this paper, we propose a novel SSL framework, namely CITab, designed to learn powerful multi-modal feature representations in a cross-tabular manner. We design the tabular modeling mechanism from a semantic-awareness perspective by integrating column headers as semantic cues, which facilitates transferrable knowledge learning and the scalability in utilizing multiple data sources for pretraining. Additionally, we propose a prototype-guided mixture-of-linear layer (P-MoLin) module for tabular feature specialization, empowering the model to effectively handle the heterogeneity of tabular data and explore the underlying medical concepts. We conduct comprehensive evaluations on Alzheimer's disease diagnosis task across three publicly available data cohorts containing 4,461 subjects. Experimental results demonstrate that CITab outperforms state-of-the-art approaches, paving the way for effective and scalable cross-tabular multi-modal learning.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14026v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出CITab框架以解决跨队列图像-表格自监督学习中的表格异构性障碍问题",
            "summary_zh": "近年来，整合医学图像和表格数据的多模态学习显著推动了临床决策的发展。自监督学习已成为在这些大规模未标记图像-表格数据上进行预训练的强大范式，旨在学习判别性表示。然而，现有的图像-表格表示学习自监督方法通常局限于特定的数据队列，这主要是由于它们在建模异构表格数据时采用了僵化的表格建模机制。这种跨表格障碍阻碍了多模态自监督方法有效学习跨不同队列共享的可迁移医学知识。本文提出了一种新颖的自监督学习框架，即CITab，旨在以跨表格的方式学习强大的多模态特征表示。我们从语义感知的角度设计表格建模机制，通过整合列标题作为语义线索，这有助于可迁移知识的学习以及利用多个数据源进行预训练的可扩展性。此外，我们提出了一个原型引导的线性混合层模块用于表格特征专门化，使模型能够有效处理表格数据的异构性并探索潜在的医学概念。我们在包含4,461名受试者的三个公开可用数据队列上对阿尔茨海默病诊断任务进行了全面评估。实验结果表明，CITab优于最先进的方法，为有效且可扩展的跨表格多模态学习铺平了道路。",
            "intro_zh": [
                "现有自监督学习方法在处理异构表格数据时存在僵化建模机制，导致跨队列知识迁移困难。",
                "CITab框架通过语义感知的表格建模和原型引导的线性混合层，实现跨表格多模态表示学习。",
                "在阿尔茨海默病诊断任务上，CITab在三个队列上超越现有方法，验证了其有效性和可扩展性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多模态自监督学习中，由于表格数据异构性导致的跨队列知识迁移障碍问题。现有方法通常采用固定或简单的表格建模机制，难以适应不同队列中表格结构的差异，限制了模型在多样化数据源上的预训练效果和泛化能力。\\n\\n**核心思路**：论文的核心思路是从语义感知角度重新设计表格建模，将列标题作为语义线索融入表示学习，并结合原型引导的线性混合层来专门化处理异构表格特征。这种设计旨在打破跨表格障碍，促进可迁移医学知识的学习。\\n\\n**技术框架**：CITab框架整体上是一个多模态自监督学习架构，包含图像编码器、表格编码器和多模态融合模块。主要阶段包括：1）通过语义感知的表格建模机制处理输入表格数据，利用列标题信息；2）使用原型引导的线性混合层模块对表格特征进行专门化处理；3）结合图像特征进行多模态对比学习或预测任务，以学习联合表示。\\n\\n**关键创新**：最重要的技术创新是语义感知的表格建模机制和原型引导的线性混合层模块。与现有方法相比，CITab本质区别在于它不再将表格数据视为固定结构，而是动态地利用语义信息来适应异构性，从而支持跨队列学习。\\n\\n**关键设计**：关键设计包括：1）表格建模中整合列标题作为嵌入向量，增强语义理解；2）P-MoLin模块使用原型向量来引导线性层的混合，具体参数设置如原型数量、线性层权重等需根据数据调整；3）损失函数可能基于对比学习或重建任务，具体细节在论文中未明确，但旨在最大化图像和表格表示之间的一致性。网络结构通常基于Transformer或CNN编码器，具体架构需参考论文补充材料。",
            "application_zh": "该研究主要应用于医学影像与临床表格数据结合的多模态分析场景，如阿尔茨海默病、癌症等疾病的早期诊断和预后预测。其实际价值在于能够利用大规模、多样化的未标记数据进行预训练，提升模型在有限标注数据下的性能，促进精准医疗和个性化治疗。未来影响可能扩展到其他医疗领域或非医疗的多模态任务，推动可扩展自监督学习的发展。",
            "highlight_zh": "在阿尔茨海默病诊断任务上，CITab在三个公开数据队列（共4,461名受试者）上进行了评估，实验结果显示其性能优于现有最先进的自监督学习方法。具体性能数据如准确率、AUC等指标在论文中报告，提升幅度显著，验证了CITab在跨队列多模态学习中的有效性和可扩展性，为临床决策提供了更可靠的模型支持。",
            "tags_zh": [
                "多模态学习",
                "自监督学习",
                "医学图像分析",
                "表格数据处理",
                "跨队列学习",
                "语义感知建模",
                "原型引导混合层",
                "阿尔茨海默病诊断"
            ],
            "_index": 175
        },
        {
            "title": "EXAONE Path 2.5: Pathology Foundation Model with Multi-Omics Alignment",
            "authors": [
                "Juseung Yun",
                "Sunwoo Yu",
                "Sumin Ha",
                "Jonghyun Kim",
                "Janghyeon Lee",
                "Jongseong Jang",
                "Soonyoung Lee"
            ],
            "arxiv_id": "2512.14019v1",
            "summary": "Cancer progression arises from interactions across multiple biological layers, especially beyond morphological and across molecular layers that remain invisible to image-only models. To capture this broader biological landscape, we present EXAONE Path 2.5, a pathology foundation model that jointly models histologic, genomic, epigenetic and transcriptomic modalities, producing an integrated patient representation that reflects tumor biology more comprehensively. Our approach incorporates three key components: (1) multimodal SigLIP loss enabling all-pairwise contrastive learning across heterogeneous modalities, (2) a fragment-aware rotary positional encoding (F-RoPE) module that preserves spatial structure and tissue-fragment topology in WSI, and (3) domain-specialized internal foundation models for both WSI and RNA-seq to provide biologically grounded embeddings for robust multimodal alignment. We evaluate EXAONE Path 2.5 against six leading pathology foundation models across two complementary benchmarks: an internal real-world clinical dataset and the Patho-Bench benchmark covering 80 tasks. Our framework demonstrates high data and parameter efficiency, achieving on-par performance with state-of-the-art foundation models on Patho-Bench while exhibiting the highest adaptability in the internal clinical setting. These results highlight the value of biologically informed multimodal design and underscore the potential of integrated genotype-to-phenotype modeling for next-generation precision oncology.",
            "categories": [
                "cs.LG",
                "q-bio.QM"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14019v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出EXAONE Path 2.5病理学基础模型，通过多组学对齐解决癌症进展中多生物层交互建模不足的问题",
            "summary_zh": "癌症进展源于多个生物层之间的相互作用，特别是超越形态学并涉及分子层，这些对仅基于图像的模型是不可见的。为了捕捉更广泛的生物学图景，我们提出了EXAONE Path 2.5，这是一个病理学基础模型，联合建模组织学、基因组学、表观遗传学和转录组学模态，生成一个更全面反映肿瘤生物学的整合患者表征。我们的方法包含三个关键组件：(1) 多模态SigLIP损失，实现跨异质模态的全配对对比学习；(2) 片段感知旋转位置编码(F-RoPE)模块，保留全切片图像中的空间结构和组织片段拓扑；(3) 针对全切片图像和RNA-seq的领域专用内部基础模型，为稳健的多模态对齐提供基于生物学的嵌入。我们在两个互补基准上评估EXAONE Path 2.5：一个内部真实世界临床数据集和覆盖80个任务的Patho-Bench基准。我们的框架展示了高数据和参数效率，在Patho-Bench上达到与最先进基础模型相当的性能，同时在内部临床设置中表现出最高的适应性。这些结果突出了生物学信息多模态设计的价值，并强调了整合基因型到表型建模对下一代精准肿瘤学的潜力。",
            "intro_zh": [
                "核心问题：现有病理学模型主要依赖图像模态，难以捕捉癌症进展中跨分子层的交互，导致对肿瘤生物学的理解不全面。",
                "方法要点：提出多模态对齐框架，整合组织学、基因组、表观遗传和转录组数据，通过对比学习和专用编码模块实现稳健表征。",
                "实验或效果：在Patho-Bench基准上达到SOTA性能，内部临床数据中适应性最高，展示高数据效率。"
            ],
            "method_zh": "**问题定义**：癌症进展涉及多生物层交互，但现有病理学基础模型主要基于图像模态，无法有效整合分子数据如基因组和转录组，导致对肿瘤生物学的表征不完整，限制了精准肿瘤学的应用。现有方法的痛点在于模态异质性高、空间结构复杂，以及缺乏生物学基础的嵌入对齐。\\n\\n**核心思路**：通过多模态对齐框架，将组织学、基因组、表观遗传和转录组数据联合建模，生成整合的患者表征，以更全面地反映肿瘤生物学。设计基于对比学习和专用编码模块，确保跨模态信息有效融合，同时保留关键空间和生物学特征。\\n\\n**技术框架**：整体架构包括数据预处理、多模态编码、对齐训练和评估阶段。主要模块有：多模态SigLIP损失用于全配对对比学习，F-RoPE模块处理全切片图像的空间结构，以及领域专用基础模型为全切片图像和RNA-seq提供生物学嵌入。流程上，先通过内部模型生成嵌入，再使用SigLIP损失对齐不同模态，最后输出整合表征用于下游任务。\\n\\n**关键创新**：最重要的技术创新是引入多模态SigLIP损失实现跨异质模态的对比学习，以及F-RoPE模块保留全切片图像中的空间拓扑。与现有方法的本质区别在于整合了多组学数据，而非仅依赖图像，并通过生物学专用模型提升对齐的稳健性。\\n\\n**关键设计**：关键参数设置包括对比学习中的温度参数和批次大小，损失函数使用SigLIP变体优化跨模态相似性。网络结构涉及Transformer编码器处理多模态输入，F-RoPE模块集成旋转位置编码以捕捉组织片段关系。技术细节还包括使用预训练的内部基础模型初始化嵌入，以及多任务训练策略提升泛化能力。",
            "application_zh": "该研究在精准肿瘤学领域有广泛应用潜力，可用于癌症诊断、预后预测和治疗响应评估。通过整合多组学数据，能更准确地表征肿瘤异质性，支持个性化医疗决策。未来可能推动下一代病理学工具开发，提升临床实践中的癌症管理效率。",
            "highlight_zh": "在Patho-Bench基准上覆盖80个任务，EXAONE Path 2.5达到与最先进基础模型相当的性能，具体提升幅度未知。在内部真实世界临床数据集中，展示最高适应性，表明模型在复杂场景下的稳健性。实验还验证了高数据和参数效率，支持资源有限环境下的部署。",
            "tags_zh": [
                "病理学基础模型",
                "多模态对齐",
                "多组学整合",
                "对比学习",
                "全切片图像分析",
                "精准肿瘤学",
                "空间编码",
                "生物学嵌入"
            ],
            "_index": 176
        },
        {
            "title": "PerfCoder: Large Language Models for Interpretable Code Performance Optimization",
            "authors": [
                "Jiuding Yang",
                "Shengyao Lu",
                "Hongxuan Liu",
                "Shayan Shirahmad Gale Bagi",
                "Zahra Fazel",
                "Tomasz Czajkowski",
                "Di Niu"
            ],
            "arxiv_id": "2512.14018v1",
            "summary": "Large language models (LLMs) have achieved remarkable progress in automatic code generation, yet their ability to produce high-performance code remains limited--a critical requirement in real-world software systems. We argue that current LLMs struggle not only due to data scarcity but, more importantly, because they lack supervision that guides interpretable and effective performance improvements. In this work, we introduce PerfCoder, a family of LLMs specifically designed to generate performance-enhanced code from source code via interpretable, customized optimizations. PerfCoder is fine-tuned on a curated collection of real-world optimization trajectories with human-readable annotations, and preference-aligned by reinforcement fine-tuning using runtime measurements, enabling it to propose input-specific improvement strategies and apply them directly without relying on iterative refinement. On the PIE code performance benchmark, PerfCoder surpasses all existing models in both runtime speedup and effective optimization rate, demonstrating that performance optimization cannot be achieved by scale alone but requires optimization stratetgy awareness. In addition, PerfCoder can generate interpretable feedback about the source code, which, when provided as input to a larger LLM in a planner-and-optimizer cooperative workflow, can further improve outcomes. Specifically, we elevate the performance of 32B models and GPT-5 to new levels on code optimization, substantially surpassing their original performance.",
            "categories": [
                "cs.SE",
                "cs.AI"
            ],
            "primary_category": "cs.SE",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14018v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出PerfCoder模型，通过可解释的定制化优化生成高性能代码，解决大语言模型在代码性能优化方面的不足。",
            "summary_zh": "大语言模型在自动代码生成方面取得了显著进展，但其生成高性能代码的能力仍然有限——这是现实世界软件系统中的关键需求。我们认为，当前大语言模型之所以难以胜任，不仅是因为数据稀缺，更重要的是缺乏指导可解释且有效性能改进的监督。在这项工作中，我们引入了PerfCoder，这是一个专门设计用于通过可解释的定制化优化从源代码生成性能增强代码的大语言模型家族。PerfCoder在精心策划的真实世界优化轨迹集合上进行了微调，这些轨迹带有可读的人工注释，并通过使用运行时测量的强化微调进行偏好对齐，使其能够提出针对特定输入的改进策略并直接应用，而无需依赖迭代优化。在PIE代码性能基准测试中，PerfCoder在运行时加速和有效优化率方面均超越了所有现有模型，表明性能优化不能仅通过规模实现，而需要优化策略意识。此外，PerfCoder可以生成关于源代码的可解释反馈，当在规划器与优化器协同工作流程中作为输入提供给更大的大语言模型时，可以进一步改善结果。具体来说，我们将32B模型和GPT-5在代码优化方面的性能提升到了新水平，大幅超越了它们的原始性能。",
            "intro_zh": [
                "核心问题：现有大语言模型在代码生成中缺乏性能优化监督，难以生成高性能代码，主要受限于数据稀缺和策略指导不足。",
                "方法要点：提出PerfCoder模型，通过基于真实优化轨迹的微调和强化学习，实现可解释的定制化代码优化，直接应用改进策略。",
                "实验或效果：在PIE基准测试中，PerfCoder在运行时加速和优化率上超越所有现有模型，并提升32B模型和GPT-5的代码优化性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大语言模型在自动代码生成中难以生成高性能代码的问题。现有方法的痛点包括：模型缺乏针对性能优化的监督，导致生成的代码效率低下；依赖迭代优化过程，增加了计算开销；且现有模型通常仅关注代码功能正确性，而忽视运行时性能，这在现实软件系统中至关重要。\\n\\n**核心思路**：论文的核心解决思路是设计一个专门用于代码性能优化的大语言模型PerfCoder，通过引入可解释的优化策略和监督信号，使模型能够直接从源代码生成性能增强的代码。这样设计的原因在于，性能优化需要模型理解代码的运行时行为并应用定制化改进，而不仅仅是扩大模型规模或依赖通用代码生成数据。\\n\\n**技术框架**：整体架构包括两个主要阶段：首先，基于精心策划的真实世界优化轨迹集合进行微调，这些轨迹带有可读的人工注释，以提供监督信号；其次，通过强化微调使用运行时测量进行偏好对齐，确保模型提出的优化策略能有效提升性能。流程上，模型接收源代码作为输入，直接输出优化后的代码和可解释的反馈，无需迭代步骤。\\n\\n**关键创新**：最重要的技术创新点在于将可解释性和性能优化结合到大语言模型中，通过优化轨迹的监督和强化学习对齐，使模型具备优化策略意识。与现有方法的本质区别在于，PerfCoder不依赖迭代优化或通用代码生成，而是专门针对性能改进进行训练，从而更高效地生成高性能代码。\\n\\n**关键设计**：技术细节包括：使用真实世界优化轨迹数据集进行微调，这些数据包含源代码、优化版本和人工注释；采用强化学习框架，以运行时测量作为奖励信号，对模型进行偏好对齐；模型架构基于大语言模型，具体参数设置未在摘要中详细说明，但强调通过监督和强化微调来提升性能优化能力。",
            "application_zh": "该研究在软件工程和系统优化领域具有广泛潜在应用，可用于自动代码性能调优、编译器优化辅助、实时系统代码生成等场景。实际价值在于提升软件运行效率，减少资源消耗，未来可能推动智能编程助手的发展，使开发者更专注于高层次设计而非底层优化。",
            "highlight_zh": "在PIE代码性能基准测试中，PerfCoder在运行时加速和有效优化率方面均超越所有现有模型，具体性能数据未在摘要中给出，但强调了其显著优势。此外，PerfCoder通过可解释反馈，在规划器与优化器协同工作流程中，将32B模型和GPT-5的代码优化性能提升到新水平，大幅超越原始性能，证明了优化策略意识的重要性。",
            "tags_zh": [
                "代码性能优化",
                "大语言模型",
                "可解释性",
                "强化学习微调",
                "软件工程",
                "自动代码生成",
                "优化策略",
                "运行时测量"
            ],
            "_index": 177
        },
        {
            "title": "Accelerating MHC-II Epitope Discovery via Multi-Scale Prediction in Antigen Presentation",
            "authors": [
                "Yue Wan",
                "Jiayi Yuan",
                "Zhiwei Feng",
                "Xiaowei Jia"
            ],
            "arxiv_id": "2512.14011v1",
            "summary": "Antigenic epitope presented by major histocompatibility complex II (MHC-II) proteins plays an essential role in immunotherapy. However, compared to the more widely studied MHC-I in computational immunotherapy, the study of MHC-II antigenic epitope poses significantly more challenges due to its complex binding specificity and ambiguous motif patterns. Consequently, existing datasets for MHC-II interactions are smaller and less standardized than those available for MHC-I. To address these challenges, we present a well-curated dataset derived from the Immune Epitope Database (IEDB) and other public sources. It not only extends and standardizes existing peptide-MHC-II datasets, but also introduces a novel antigen-MHC-II dataset with richer biological context. Leveraging this dataset, we formulate three major machine learning (ML) tasks of peptide binding, peptide presentation, and antigen presentation, which progressively capture the broader biological processes within the MHC-II antigen presentation pathway. We further employ a multi-scale evaluation framework to benchmark existing models, along with a comprehensive analysis over various modeling designs to this problem with a modular framework. Overall, this work serves as a valuable resource for advancing computational immunotherapy, providing a foundation for future research in ML guided epitope discovery and predictive modeling of immune responses.",
            "categories": [
                "cs.LG",
                "q-bio.QM"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14011v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "世界模型",
                    "matched_keywords": [
                        "predictive model"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出多尺度预测框架以加速MHC-II抗原表位发现，解决免疫治疗中表位预测的挑战",
            "summary_zh": "由主要组织相容性复合体II（MHC-II）蛋白呈递的抗原表位在免疫治疗中起着至关重要的作用。然而，与计算免疫治疗中更广泛研究的MHC-I相比，MHC-II抗原表位的研究由于其复杂的结合特异性和模糊的基序模式而面临更多挑战。因此，现有的MHC-II相互作用数据集比MHC-I的数据集更小且标准化程度更低。为应对这些挑战，我们提出了一个从免疫表位数据库（IEDB）和其他公共来源精心整理的数据集。它不仅扩展和标准化了现有的肽-MHC-II数据集，还引入了一个具有更丰富生物学背景的新型抗原-MHC-II数据集。利用该数据集，我们制定了肽结合、肽呈递和抗原呈递三个主要的机器学习任务，逐步捕捉MHC-II抗原呈递途径中更广泛的生物过程。我们进一步采用多尺度评估框架对现有模型进行基准测试，并通过模块化框架对该问题的各种建模设计进行全面分析。总体而言，这项工作为推进计算免疫治疗提供了宝贵资源，为未来机器学习指导的表位发现和免疫反应预测建模研究奠定了基础。",
            "intro_zh": [
                "核心问题：MHC-II抗原表位预测面临数据集小、标准化不足、结合特异性复杂等挑战，限制了免疫治疗研究。",
                "方法要点：构建标准化数据集，定义多尺度机器学习任务，采用模块化框架进行模型评估与设计分析。",
                "实验或效果：通过多尺度评估框架基准测试现有模型，提供全面分析，为未来研究奠定基础。"
            ],
            "method_zh": "**问题定义**：论文旨在解决MHC-II抗原表位预测中的关键挑战，包括数据集规模小、标准化程度低、结合特异性复杂以及现有方法难以捕捉完整生物过程。现有方法的痛点在于依赖有限且不一致的数据，导致预测准确性受限，且缺乏系统性的多尺度建模框架。\\n\\n**核心思路**：通过构建高质量、标准化的数据集，并定义从肽结合到抗原呈递的多尺度机器学习任务，逐步模拟MHC-II抗原呈递的生物过程。设计模块化框架以灵活评估和优化不同建模策略，从而提升预测性能。\\n\\n**技术框架**：整体架构包括数据收集与整理、任务定义、模型评估和设计分析四个阶段。首先，从IEDB等公共来源整合数据，构建肽-MHC-II和抗原-MHC-II数据集；其次，定义肽结合、肽呈递和抗原呈递三个ML任务；然后，使用多尺度评估框架对现有模型进行基准测试；最后，通过模块化框架分析不同建模设计的效果。\\n\\n**关键创新**：最重要的技术创新是引入多尺度预测框架和新型抗原-MHC-II数据集。与现有方法相比，本质区别在于系统性地覆盖了从分子结合到细胞呈递的完整生物过程，并通过标准化数据解决了数据不一致性问题。\\n\\n**关键设计**：关键设计包括数据标准化流程（如统一肽序列格式和MHC等位基因注释）、多尺度任务定义（基于生物层级划分）、评估指标（如结合亲和力预测准确率）和模块化框架（允许灵活集成不同模型组件，如特征提取器和分类器）。具体参数设置和网络结构未在摘要中详细说明，但框架支持常见ML模型（如神经网络）的适配。",
            "application_zh": "该研究在计算免疫治疗领域具有广泛的应用潜力，可用于加速疫苗设计、个性化癌症免疫疗法开发以及自身免疫性疾病治疗。通过提升MHC-II表位预测的准确性，它能帮助研究人员更高效地识别潜在抗原，优化免疫响应预测模型，从而推动精准医疗和药物研发的进展。未来，该框架可扩展至其他免疫相关预测任务，促进跨学科研究。",
            "highlight_zh": "最重要的实验结果包括构建了一个标准化的肽-MHC-II和抗原-MHC-II数据集，规模和质量优于现有资源。通过多尺度评估框架，论文对现有模型进行了基准测试，展示了不同建模设计在肽结合、肽呈递和抗原呈递任务上的性能差异。具体性能数据未在摘要中提供，但分析表明模块化框架能有效识别优化方向，为后续研究提供了可复现的基础。提升幅度依赖于具体模型和任务，但整体框架显著增强了预测的系统性和可扩展性。",
            "tags_zh": [
                "MHC-II表位预测",
                "计算免疫治疗",
                "多尺度机器学习",
                "抗原呈递建模",
                "数据集标准化",
                "模块化框架",
                "免疫反应预测",
                "生物信息学"
            ],
            "_index": 178
        },
        {
            "title": "Physics-Informed Machine Learning for Two-Phase Moving-Interface and Stefan Problems",
            "authors": [
                "Che-Chia Chang",
                "Te-Sheng Lin",
                "Ming-Chih Lai"
            ],
            "arxiv_id": "2512.14010v1",
            "summary": "The Stefan problem is a classical free-boundary problem that models phase-change processes and poses computational challenges due to its moving interface and nonlinear temperature-phase coupling. In this work, we develop a physics-informed neural network framework for solving two-phase Stefan problems. The proposed method explicitly tracks the interface motion and enforces the discontinuity in the temperature gradient across the interface while maintaining global consistency of the temperature field. Our approach employs two neural networks: one representing the moving interface and the other for the temperature field. The interface network allows rapid categorization of thermal diffusivity in the spatial domain, which is a crucial step for selecting training points for the temperature network. The temperature network's input is augmented with a modified zero-level set function to accurately capture the jump in its normal derivative across the interface. Numerical experiments on two-phase dynamical Stefan problems demonstrate the superior accuracy and effectiveness of our proposed method compared with the ones obtained by other neural network methodology in literature. The results indicate that the proposed framework offers a robust and flexible alternative to traditional numerical methods for solving phase-change problems governed by moving boundaries. In addition, the proposed method can capture an unstable interface evolution associated with the Mullins-Sekerka instability.",
            "categories": [
                "physics.comp-ph",
                "cs.LG"
            ],
            "primary_category": "physics.comp-ph",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14010v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出基于物理信息神经网络的框架以解决两相Stefan移动界面问题",
            "summary_zh": "Stefan问题是一个经典的相变过程自由边界问题，由于移动界面和非线性温度-相耦合而带来计算挑战。本研究开发了一个基于物理信息的神经网络框架来解决两相Stefan问题。该方法显式跟踪界面运动，在保持温度场全局一致性的同时，强制界面处温度梯度的不连续性。我们的方法采用两个神经网络：一个表示移动界面，另一个表示温度场。界面网络允许在空间域中快速分类热扩散率，这是为温度网络选择训练点的关键步骤。温度网络的输入通过修改的零水平集函数进行增强，以准确捕捉界面处法向导数的跳跃。在两相动态Stefan问题上的数值实验表明，与文献中其他神经网络方法相比，我们提出的方法具有更高的准确性和有效性。结果表明，该框架为解决受移动边界控制的相变问题提供了一个稳健且灵活的替代传统数值方法的选择。此外，该方法能够捕捉与Mullins-Sekerka不稳定性相关的不稳定界面演化。",
            "intro_zh": [
                "Stefan问题作为经典相变自由边界问题，面临移动界面和非线性耦合带来的计算挑战，传统数值方法处理复杂界面演化时效率有限。",
                "提出双神经网络框架：一个网络显式建模移动界面，另一个网络建模温度场，通过物理信息约束确保界面处梯度跳跃和全局一致性。",
                "数值实验表明，该方法在解决两相动态Stefan问题时，相比现有神经网络方法具有更高准确性，并能捕捉不稳定界面演化如Mullins-Sekerka不稳定性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决两相Stefan问题，这是一个描述相变过程的自由边界问题，涉及移动界面和温度-相的非线性耦合。现有数值方法在处理复杂界面演化时，常面临计算效率低、难以准确捕捉界面处温度梯度跳跃等挑战。\\n\\n**核心思路**：论文提出使用两个神经网络分别建模移动界面和温度场，通过物理信息约束（如界面处的梯度跳跃条件）来确保解的物理一致性。这种设计允许显式跟踪界面运动，同时保持温度场的全局连续性。\\n\\n**技术框架**：整体框架包括两个主要阶段：首先，界面网络用于快速分类空间域中的热扩散率，并确定界面位置；其次，温度网络基于增强的输入（如修改的零水平集函数）来预测温度分布，训练时通过损失函数强制界面处的物理条件。\\n\\n**关键创新**：最重要的创新在于双网络架构的协同设计，其中界面网络直接建模移动边界，温度网络通过输入增强准确捕捉界面处的梯度不连续性。这与传统方法依赖网格离散或单一网络近似整个域有本质区别。\\n\\n**关键设计**：技术细节包括：使用神经网络参数化界面和温度场；损失函数结合界面运动方程、热传导方程和界面跳跃条件；输入增强采用修改的零水平集函数来编码界面信息；训练点选择基于界面网络的输出，优化热扩散率区域的采样。",
            "application_zh": "该研究在相变过程建模领域具有广泛潜在应用，如材料科学中的凝固与熔化、能源系统中的相变储能、生物医学中的组织冷冻治疗等。其提供了一种灵活且稳健的数值替代方案，能够高效处理复杂移动边界问题，未来可能推动多物理场模拟和工业优化设计的发展。",
            "highlight_zh": "数值实验在两相动态Stefan问题上进行，结果显示该方法相比文献中其他神经网络方法具有显著更高的准确性，具体性能提升未知，但能有效捕捉界面演化，包括Mullins-Sekerka不稳定性相关的不稳定行为，验证了框架的优越性和鲁棒性。",
            "tags_zh": [
                "物理信息神经网络",
                "Stefan问题",
                "移动界面",
                "两相流",
                "相变建模",
                "自由边界问题",
                "梯度跳跃",
                "Mullins-Sekerka不稳定性"
            ],
            "_index": 179
        }
    ]
}