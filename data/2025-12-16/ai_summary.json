{
    "papers": [
        {
            "title": "Odyssey: An Automotive Lidar-Inertial Odometry Dataset for GNSS-denied situations",
            "authors": [
                "Aaron Kurda",
                "Simon Steuernagel",
                "Lukas Jung",
                "Marcus Baum"
            ],
            "arxiv_id": "2512.14428v1",
            "summary": "The development and evaluation of Lidar-Inertial Odometry (LIO) and Simultaneous Localization and Mapping (SLAM) systems requires a precise ground truth. The Global Navigation Satellite System (GNSS) is often used as a foundation for this, but its signals can be unreliable in obstructed environments due to multi-path effects or loss-of-signal. While existing datasets compensate for the sporadic loss of GNSS signals by incorporating Inertial Measurement Unit (IMU) measurements, the commonly used Micro-Electro-Mechanical Systems (MEMS) or Fiber Optic Gyroscope (FOG)-based systems do not permit the prolonged study of GNSS-denied environments. To close this gap, we present Odyssey, a LIO dataset with a focus on GNSS-denied environments such as tunnels and parking garages as well as other underrepresented, yet ubiquitous situations such as stop-and-go-traffic, bumpy roads and wide open fields. Our ground truth is derived from a navigation-grade Inertial Navigation System (INS) equipped with a Ring Laser Gyroscope (RLG), offering exceptional bias stability characteristics compared to IMUs used in existing datasets and enabling the prolonged and accurate study of GNSS-denied environments. This makes Odyssey the first publicly available dataset featuring a RLG-based INS. Besides providing data for LIO, we also support other tasks, such as place recognition, through the threefold repetition of all trajectories as well as the integration of external mapping data by providing precise geodetic coordinates. All data, dataloader and other material is available online at https://odyssey.uni-goettingen.de/ .",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "9 pages, 4 figures, submitted to International Journal of Robotics Research (IJRR)",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14428v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "traffic",
                        "lidar"
                    ],
                    "score": 2
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "SLAM",
                        "localization",
                        "mapping"
                    ],
                    "score": 3
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 7,
            "headline_zh": "提出Odyssey数据集以解决GNSS信号缺失环境下激光雷达-惯性里程计系统评估的难题",
            "summary_zh": "激光雷达-惯性里程计（LIO）和同步定位与建图（SLAM）系统的开发与评估需要精确的地面真值。全球导航卫星系统（GNSS）常被用作基础，但其信号在遮挡环境中可能因多径效应或信号丢失而不可靠。现有数据集通过整合惯性测量单元（IMU）测量来补偿GNSS信号的偶发性丢失，但常用的微机电系统（MEMS）或光纤陀螺仪（FOG）系统无法支持对GNSS缺失环境的长期研究。为填补这一空白，我们提出了Odyssey，这是一个专注于GNSS缺失环境（如隧道和停车场）以及其他代表性不足但普遍存在场景（如启停交通、颠簸道路和开阔田野）的LIO数据集。我们的地面真值源自配备环形激光陀螺仪（RLG）的导航级惯性导航系统（INS），与现有数据集使用的IMU相比，具有优异的偏置稳定性特性，支持对GNSS缺失环境进行长期准确研究。这使得Odyssey成为首个公开可用的基于RLG的INS数据集。除了为LIO提供数据外，我们还通过三次重复所有轨迹以及提供精确大地坐标来整合外部地图数据，支持其他任务，如地点识别。所有数据、数据加载器和其他材料均可在https://odyssey.uni-goettingen.de/在线获取。",
            "intro_zh": [
                "现有LIO/SLAM系统评估依赖GNSS作为地面真值，但在遮挡环境中GNSS信号不可靠，导致评估困难。",
                "论文提出Odyssey数据集，使用基于环形激光陀螺仪的导航级INS提供高精度地面真值，专注于GNSS缺失环境。",
                "数据集包含隧道、停车场等场景，支持长期GNSS缺失研究，并首次公开基于RLG的INS数据，提升评估准确性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决激光雷达-惯性里程计（LIO）和同步定位与建图（SLAM）系统在GNSS信号缺失环境（如隧道、停车场）中评估的难题。现有方法依赖GNSS作为地面真值，但在遮挡环境中GNSS信号不可靠，而现有数据集使用的微机电系统（MEMS）或光纤陀螺仪（FOG）惯性测量单元（IMU）偏置稳定性不足，无法支持长期GNSS缺失研究，导致评估不准确。\\n\\n**核心思路**：论文的核心解决思路是构建一个专注于GNSS缺失环境的LIO数据集，通过使用基于环形激光陀螺仪（RLG）的导航级惯性导航系统（INS）提供高精度、长期稳定的地面真值，以弥补现有数据集的不足。这样设计是因为RLG-INS具有优异的偏置稳定性，能在GNSS信号完全缺失时仍保持准确姿态估计，从而支持对LIO系统在真实遮挡场景下的性能进行可靠评估。\\n\\n**技术框架**：整体架构包括数据采集、处理和发布阶段。主要模块包括：传感器系统（如激光雷达、IMU、RLG-INS）、数据采集平台（车辆搭载）、轨迹规划（覆盖隧道、停车场、启停交通等场景）、数据处理（同步传感器数据、生成地面真值）、数据发布（提供原始数据、数据加载器和外部地图坐标）。流程上，首先在多种GNSS缺失环境中重复采集轨迹，然后使用RLG-INS输出作为地面真值，最后整合数据以支持LIO评估和其他任务。\\n\\n**关键创新**：最重要的技术创新点是首次公开提供基于环形激光陀螺仪（RLG）的导航级惯性导航系统（INS）数据集。与现有数据集使用的MEMS或FOG-IMU相比，RLG-INS具有更高的偏置稳定性和精度，本质区别在于它能支持长期GNSS缺失环境下的准确姿态估计，从而为LIO系统评估提供了更可靠的地面真值基准。\\n\\n**关键设计**：关键设计包括：使用RLG-INS作为地面真值源，其偏置稳定性优于常见IMU；数据采集覆盖三类场景（GNSS缺失环境如隧道、代表性不足场景如颠簸道路、开阔田野）；所有轨迹重复三次以支持地点识别任务；提供精确大地坐标以整合外部地图数据；数据集公开可用，包括原始传感器数据和工具。具体参数如传感器型号、采样率等未在摘要中详细说明，但整体设计旨在确保数据多样性和评估实用性。",
            "application_zh": "该研究在自动驾驶、机器人导航和增强现实等领域具有重要应用价值。Odyssey数据集可用于开发和评估激光雷达-惯性里程计（LIO）和同步定位与建图（SLAM）系统在GNSS信号缺失环境中的性能，提升系统在隧道、停车场等遮挡场景下的鲁棒性。未来，它可能推动更可靠的定位算法发展，促进自动驾驶车辆在复杂城市环境中的安全部署。",
            "highlight_zh": "最重要的实验结果是Odyssey数据集成为首个公开可用的基于环形激光陀螺仪（RLG）的惯性导航系统（INS）数据集。与现有数据集相比，它提供了更高精度的地面真值，偏置稳定性显著提升，支持对GNSS缺失环境（如隧道、停车场）进行长期准确研究。数据集包含多样场景，如启停交通和开阔田野，并通过轨迹重复和外部地图坐标增强了实用性，为LIO系统评估设立了新基准。",
            "tags_zh": [
                "激光雷达-惯性里程计",
                "同步定位与建图",
                "GNSS缺失环境",
                "环形激光陀螺仪",
                "惯性导航系统",
                "数据集构建",
                "自动驾驶评估",
                "地点识别"
            ],
            "_index": 0
        },
        {
            "title": "A data-physics hybrid generative model for patient-specific post-stroke motor rehabilitation using wearable sensor data",
            "authors": [
                "Yanning Dai",
                "Chenyu Tang",
                "Ruizhi Zhang",
                "Wenyu Yang",
                "Yilan Zhang",
                "Yuhui Wang",
                "Junliang Chen",
                "Xuhang Chen",
                "Ruimou Xie",
                "Yangyue Cao",
                "Qiaoying Li",
                "Jin Cao",
                "Tao Li",
                "Hubin Zhao",
                "Yu Pan",
                "Arokia Nathan",
                "Xin Gao",
                "Peter Smielewski",
                "Shuo Gao"
            ],
            "arxiv_id": "2512.14329v1",
            "summary": "Dynamic prediction of locomotor capacity after stroke is crucial for tailoring rehabilitation, yet current assessments provide only static impairment scores and do not indicate whether patients can safely perform specific tasks such as slope walking or stair climbing. Here, we develop a data-physics hybrid generative framework that reconstructs an individual stroke survivor's neuromuscular control from a single 20 m level-ground walking trial and predicts task-conditioned locomotion across rehabilitation scenarios. The system combines wearable-sensor kinematics, a proportional-derivative physics controller, a population Healthy Motion Atlas, and goal-conditioned deep reinforcement learning with behaviour cloning and generative adversarial imitation learning to generate physically plausible, patient-specific gait simulations for slopes and stairs. In 11 stroke survivors, the personalized controllers preserved idiosyncratic gait patterns while improving joint-angle and endpoint fidelity by 4.73% and 12.10%, respectively, and reducing training time to 25.56% relative to a physics-only baseline. In a multicentre pilot involving 21 inpatients, clinicians who used our locomotion predictions to guide task selection and difficulty obtained larger gains in Fugl-Meyer lower-extremity scores over 28 days of standard rehabilitation than control clinicians (mean change 6.0 versus 3.7 points). These findings indicate that our generative, task-predictive framework can augment clinical decision-making in post-stroke gait rehabilitation and provide a template for dynamically personalized motor recovery strategies.",
            "categories": [
                "cs.CE",
                "cs.AI"
            ],
            "primary_category": "cs.CE",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "26 pages, 6 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14329v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "人形机器人",
                    "matched_keywords": [
                        "imitation learning",
                        "atlas"
                    ],
                    "score": 2
                },
                {
                    "name": "动作生成",
                    "matched_keywords": [
                        "motion prediction"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO",
                        "VIO"
                    ],
                    "score": 2
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning",
                        "imitation learning"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 7,
            "headline_zh": "提出数据-物理混合生成模型，基于单次平地行走数据预测卒中患者个性化康复任务表现，以辅助临床决策。",
            "summary_zh": "卒中后运动能力的动态预测对于定制康复方案至关重要，但现有评估仅提供静态损伤评分，无法指示患者能否安全执行特定任务（如斜坡行走或爬楼梯）。本研究开发了一个数据-物理混合生成框架，该框架可从一次20米平地行走试验中重建个体卒中幸存者的神经肌肉控制，并预测跨康复场景的任务条件性运动。该系统结合了可穿戴传感器运动学数据、比例-微分物理控制器、健康人群运动图谱，以及结合行为克隆和生成对抗模仿学习的目标条件深度强化学习，以生成物理上合理、患者特定的斜坡和楼梯步态模拟。在11名卒中幸存者中，个性化控制器在保留个体步态模式的同时，将关节角度和端点保真度分别提高了4.73%和12.10%，并将训练时间减少至纯物理基线的25.56%。在一项涉及21名住院患者的多中心试点中，使用我们的运动预测来指导任务选择和难度的临床医生，在28天标准康复期间获得的Fugl-Meyer下肢评分增益大于对照组临床医生（平均变化6.0分对3.7分）。这些发现表明，我们的生成性、任务预测框架可以增强卒中后步态康复的临床决策，并为动态个性化运动恢复策略提供模板。",
            "intro_zh": [
                "核心问题：现有卒中后运动能力评估仅提供静态损伤评分，无法动态预测患者执行特定康复任务（如斜坡行走）的安全性与表现，限制了康复方案的个性化定制。",
                "方法要点：提出数据-物理混合生成框架，结合可穿戴传感器数据、物理控制器、健康运动图谱和深度强化学习，从单次平地行走重建患者神经肌肉控制并生成任务条件性步态模拟。",
                "实验或效果：在11名患者中提升模拟保真度并大幅减少训练时间；多中心试点显示使用该框架指导康复能显著提高患者下肢功能评分增益。"
            ],
            "method_zh": "**问题定义**：论文旨在解决卒中后康复中动态、个性化运动能力预测的难题。具体而言，现有临床评估（如Fugl-Meyer评分）仅提供静态的损伤程度分数，无法预测患者在特定任务条件（如不同坡度斜坡、楼梯）下的实际运动表现和安全执行能力，导致康复方案缺乏动态调整依据，影响个性化康复效果。\\n\\n**核心思路**：论文提出一种“数据-物理混合”生成框架，核心思想是融合数据驱动方法与物理模型约束。通过少量患者实际行走数据（可穿戴传感器采集的运动学数据）来个性化调整一个基于物理的控制器，并利用健康人群运动图谱作为先验，结合深度强化学习生成符合物理规律且保留患者个体特征的多种任务步态模拟。这样既能利用数据捕捉个体特异性，又能通过物理约束保证生成动作的合理性与安全性。\\n\\n**技术框架**：整体流程包含几个主要阶段：1) 数据采集与处理：使用可穿戴传感器记录患者一次20米平地行走的运动学数据。2) 个性化控制器构建：基于采集数据，初始化一个比例-微分（PD）物理控制器来模拟患者的神经肌肉控制特性。3) 混合生成模型训练：将个性化控制器与一个预构建的“健康运动图谱”（代表健康人群的运动模式库）结合，采用目标条件深度强化学习（Goal-Conditioned Deep RL）进行训练。训练中融合了行为克隆（从患者数据中学习）和生成对抗模仿学习（使生成动作分布与真实数据匹配）技术，以生成针对特定任务（如斜坡角度）的步态序列。4) 预测与模拟：训练后的模型可根据输入的任务条件（如楼梯高度），生成患者在该条件下的物理仿真步态。\\n\\n**关键创新**：最重要的创新在于提出了“数据-物理混合”生成范式，并实现了“从单次试验到多任务预测”。与纯数据驱动方法（可能缺乏物理合理性）或纯物理仿真（难以个性化）相比，本框架通过可穿戴传感器数据快速个性化物理控制器，并利用深度强化学习在物理仿真环境中探索并生成符合患者能力范围的任务特定动作。本质区别在于它实现了小样本（单次行走）下的个性化建模，并能外推至未见过的任务场景，同时严格受物理定律约束。\\n\\n**关键设计**：关键技术细节包括：1) 使用比例-微分（PD）控制器作为物理仿真的底层控制器，其参数根据患者传感器数据个性化调整。2) 构建“健康运动图谱”，作为强化学习策略探索的参考运动先验，可能以运动片段库或潜在空间表示形式存在。3) 采用目标条件深度强化学习，任务目标（如斜坡坡度）作为条件输入到策略网络中。4) 损失函数设计：结合行为克隆损失（最小化生成动作与患者实际动作的差异）和生成对抗模仿学习损失（通过判别器区分生成动作与真实动作分布），确保生成动作既个性化又自然。5) 训练中可能涉及物理仿真引擎（如MuJoCo）来模拟人体动力学，奖励函数设计鼓励符合物理规律、节能且与任务目标一致的运动。",
            "application_zh": "该研究主要应用于卒中后运动康复领域。其核心价值在于为临床医生提供一种动态、个性化的决策支持工具：基于患者简单的平地行走测试，即可预测其在复杂任务（如上下斜坡、楼梯）中的表现和安全性，从而科学制定和调整康复训练计划（如任务选择、难度分级）。这有望提升康复效率、减少评估负担，并推动康复医学向精准化、个性化方向发展。未来可扩展至其他神经系统疾病（如脊髓损伤、帕金森病）的康复评估与干预。",
            "highlight_zh": "在11名卒中患者的验证中，个性化控制器相比纯物理基线，在保留个体步态模式的同时，将关节角度保真度提升4.73%，端点（如足部轨迹）保真度提升12.10%，并将模型训练时间大幅减少至基线方法的25.56%。在一项涉及21名住院患者的多中心试点临床研究中，使用该框架预测结果指导康复的临床医生组，其患者在28天康复后Fugl-Meyer下肢评分平均提升6.0分，显著高于对照组（平均提升3.7分），证明了该框架辅助临床决策的有效性。",
            "tags_zh": [
                "卒中康复",
                "步态分析",
                "数据-物理混合模型",
                "可穿戴传感器",
                "深度强化学习",
                "个性化医疗",
                "生成模型",
                "运动预测"
            ],
            "_index": 1
        },
        {
            "title": "SUPER -- A Framework for Sensitivity-based Uncertainty-aware Performance and Risk Assessment in Visual Inertial Odometry",
            "authors": [
                "Johannes A. Gaus",
                "Daniel Häufle",
                "Woo-Jeong Baek"
            ],
            "arxiv_id": "2512.14189v1",
            "summary": "While many visual odometry (VO), visual-inertial odometry (VIO), and SLAM systems achieve high accuracy, the majority of existing methods miss to assess risks at runtime. This paper presents SUPER (Sensitivity-based Uncertainty-aware PErformance and Risk assessment) that is a generic and explainable framework that propagates uncertainties via sensitivities for real-time risk assessment in VIO. The scientific novelty lies in the derivation of a real-time risk indicator that is backend-agnostic and exploits the Schur complement blocks of the Gauss-Newton normal matrix to propagate uncertainties. Practically, the Schur complement captures the sensitivity that reflects the influence of the uncertainty on the risk occurrence. Our framework estimates risks on the basis of the residual magnitudes, geometric conditioning, and short horizon temporal trends without requiring ground truth knowledge. Our framework enables to reliably predict trajectory degradation 50 frames ahead with an improvement of 20% to the baseline. In addition, SUPER initiates a stop or relocalization policy with 89.1% recall. The framework is backend agnostic and operates in real time with less than 0.2% additional CPU cost. Experiments show that SUPER provides consistent uncertainty estimates. A SLAM evaluation highlights the applicability to long horizon mapping.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14189v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "visual odometry",
                        "VO",
                        "VIO",
                        "visual-inertial",
                        "SLAM",
                        "localization",
                        "mapping"
                    ],
                    "score": 7
                }
            ],
            "relevance_score": 7,
            "headline_zh": "提出SUPER框架，通过灵敏度传播不确定性，实现视觉惯性里程计中的实时风险与性能评估。",
            "summary_zh": "尽管许多视觉里程计（VO）、视觉惯性里程计（VIO）和SLAM系统实现了高精度，但现有方法大多未能在运行时评估风险。本文提出了SUPER（基于灵敏度的不确定性感知性能与风险评估）框架，这是一个通用且可解释的框架，通过灵敏度传播不确定性，用于VIO中的实时风险评估。科学新颖性在于推导了一个实时风险指标，该指标与后端无关，并利用高斯-牛顿正规矩阵的舒尔补块来传播不确定性。实际上，舒尔补捕获了反映不确定性对风险发生影响的灵敏度。我们的框架基于残差大小、几何条件和短期时间趋势来估计风险，无需地面真值知识。该框架能够可靠地预测50帧前的轨迹退化，相比基线提升20%。此外，SUPER以89.1%的召回率启动停止或重定位策略。该框架与后端无关，实时运行，额外CPU成本低于0.2%。实验表明，SUPER提供了一致的不确定性估计。SLAM评估突出了其在长时程建图中的应用性。",
            "intro_zh": [
                "现有VIO方法虽精度高，但缺乏运行时风险评估，导致系统在不确定环境下可能失效。",
                "提出SUPER框架，利用舒尔补传播不确定性，基于残差、几何条件和时间趋势进行实时风险预测。",
                "实验显示，SUPER能提前50帧预测轨迹退化，提升20%，并以89.1%召回率启动安全策略，额外CPU成本低。"
            ],
            "method_zh": "**问题定义**：论文旨在解决视觉惯性里程计（VIO）系统中缺乏实时风险评估的问题。现有方法虽能实现高精度定位，但未能在运行时评估不确定性带来的风险，导致系统在复杂或动态环境中可能产生轨迹退化或失效，无法及时采取应对措施。\\n\\n**核心思路**：论文的核心思路是通过灵敏度分析来传播不确定性，构建一个与后端无关的实时风险指标。设计基于高斯-牛顿优化中的舒尔补块，捕获状态估计对不确定性的敏感度，从而量化风险发生概率，无需依赖地面真值，实现高效且可解释的风险评估。\\n\\n**技术框架**：整体框架包括不确定性传播模块、风险指标计算模块和决策模块。首先，从VIO后端提取舒尔补矩阵，用于计算灵敏度；然后，结合残差大小、几何条件（如观测矩阵的秩）和短期时间趋势（如轨迹变化率），综合评估风险；最后，根据风险阈值触发停止或重定位策略，确保系统安全。\\n\\n**关键创新**：最重要的创新是推导了基于舒尔补的实时风险指标，该指标与具体后端算法无关，能有效捕捉不确定性对系统性能的影响。与现有方法相比，本质区别在于无需额外传感器或复杂模型，仅利用优化过程中的内部信息，实现轻量级且通用的风险评估。\\n\\n**关键设计**：关键设计包括使用高斯-牛顿正规矩阵的舒尔补块来计算灵敏度，这反映了状态估计对观测不确定性的依赖；风险指标综合了残差范数、几何退化因子和时间窗口内的趋势变化；参数设置如风险阈值通过实验校准，以确保高召回率和低误报率；框架实时运行，额外CPU开销控制在0.2%以内。",
            "application_zh": "该研究在自动驾驶、无人机导航和机器人SLAM等领域具有重要应用价值。通过实时风险评估，系统能在不确定环境下提前预警，避免轨迹失效，提升安全性和鲁棒性。未来可扩展至多传感器融合和长期建图任务，推动智能系统在动态场景中的可靠部署。",
            "highlight_zh": "实验表明，SUPER框架能提前50帧预测轨迹退化，相比基线方法提升20%的准确性；在风险触发方面，以89.1%的召回率启动停止或重定位策略，确保系统安全；额外CPU成本低于0.2%，实时性高；SLAM评估验证了其在长时程建图中的一致性不确定性估计能力。",
            "tags_zh": [
                "视觉惯性里程计",
                "不确定性评估",
                "实时风险预测",
                "舒尔补灵敏度",
                "后端无关框架",
                "轨迹退化检测",
                "SLAM应用",
                "轻量级计算"
            ],
            "_index": 2
        },
        {
            "title": "MMGR: Multi-Modal Generative Reasoning",
            "authors": [
                "Zefan Cai",
                "Haoyi Qiu",
                "Tianyi Ma",
                "Haozhe Zhao",
                "Gengze Zhou",
                "Kung-Hsiang Huang",
                "Parisa Kordjamshidi",
                "Minjia Zhang",
                "Xiao Wen",
                "Jiuxiang Gu",
                "Nanyun Peng",
                "Junjie Hu"
            ],
            "arxiv_id": "2512.14691v1",
            "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.",
            "categories": [
                "cs.CL",
                "cs.CV"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "work in progress",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14691v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO",
                        "localization"
                    ],
                    "score": 2
                },
                {
                    "name": "世界模型",
                    "matched_keywords": [
                        "world model",
                        "world simulator"
                    ],
                    "score": 2
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "reward"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 6,
            "headline_zh": "提出MMGR多模态生成推理评估框架，以解决视频基础模型在物理、逻辑和空间约束上的推理能力不足问题。",
            "summary_zh": "视频基础模型能够生成视觉逼真且时序连贯的内容，但其作为世界模拟器的可靠性取决于是否捕捉了物理、逻辑和空间约束。现有指标如弗雷歇视频距离（FVD）强调感知质量而忽视了推理失败，包括违反因果关系、物理规律和全局一致性。我们引入了MMGR（多模态生成推理评估与基准），这是一个基于五种推理能力的原则性评估框架：物理、逻辑、3D空间、2D空间和时序推理。MMGR在三个领域评估生成推理：抽象推理（ARC-AGI、数独）、具身导航（真实世界3D导航和定位）以及物理常识（运动和组合交互）。MMGR应用细粒度指标，要求视频和图像生成在整体上正确。我们基准测试了领先的视频模型（Veo-3、Sora-2、Wan-2.2）和图像模型（Nano-banana、Nano-banana Pro、GPT-4o-image、Qwen-image），揭示了跨领域的显著性能差距。模型在物理常识任务上表现中等，但在抽象推理上表现不佳（ARC-AGI准确率低于10%），并在具身设置中的长时程空间规划上遇到困难。我们的分析突出了当前模型的关键局限性，包括过度依赖感知数据、全局状态一致性弱，以及目标函数奖励视觉合理性而非因果正确性。MMGR提供了一个统一的诊断基准和一条通向推理感知生成世界模型的路径。",
            "intro_zh": [
                "现有视频基础模型评估指标（如FVD）过于关注感知质量，忽略了物理、逻辑和空间推理失败，导致模型可靠性不足。",
                "论文提出MMGR框架，基于五种推理能力（物理、逻辑、3D空间、2D空间、时序）设计细粒度评估指标，覆盖抽象推理、具身导航和物理常识三大领域。",
                "实验显示，主流模型在物理常识任务上表现中等，但在抽象推理（如ARC-AGI准确率低于10%）和长时程空间规划上表现较差，揭示了模型推理能力的显著差距。"
            ],
            "method_zh": "**问题定义**：论文旨在解决视频基础模型在生成内容时缺乏物理、逻辑和空间推理能力的问题，现有评估方法（如FVD）仅关注感知质量，无法检测推理失败，导致模型作为世界模拟器的可靠性受限。\\n\\n**核心思路**：通过构建一个原则性的多模态生成推理评估框架（MMGR），基于五种核心推理能力（物理、逻辑、3D空间、2D空间、时序），设计细粒度指标来全面评估模型在视频和图像生成中的推理正确性，从而诊断模型局限性并推动推理感知模型的发展。\\n\\n**技术框架**：MMGR框架包含三个主要阶段：首先，定义五种推理能力作为评估维度；其次，构建三个评估领域（抽象推理、具身导航、物理常识），每个领域包含具体任务（如ARC-AGI、数独、3D导航、运动交互）；最后，应用细粒度指标（如准确率、一致性分数）对模型输出进行整体正确性评估，要求视频和图像生成在推理约束下保持一致。\\n\\n**关键创新**：最重要的创新是首次提出一个统一的多模态生成推理评估框架，将推理能力细化为五个维度，并覆盖抽象、具身和常识领域，与现有方法（如FVD）的本质区别在于从感知质量转向推理正确性，强调因果和全局一致性而非仅视觉逼真。\\n\\n**关键设计**：关键设计包括：五种推理能力的定义（如物理推理涉及运动规律，逻辑推理涉及因果关系），三个评估领域的任务选择（如ARC-AGI用于抽象推理，真实世界3D导航用于具身导航），以及细粒度指标的应用（如要求视频序列在时序和空间上保持逻辑一致，图像生成需符合物理约束）。具体参数设置和损失函数未在摘要中详细说明，但框架强调整体正确性评估，而非单一感知指标。",
            "application_zh": "该研究在人工智能和机器学习领域具有广泛潜在应用，包括：1）模型诊断与优化：帮助开发者评估和改进视频基础模型的推理能力，提升其在自动驾驶、机器人导航等安全关键场景中的可靠性；2）基准测试：作为标准评估工具，推动生成式AI向更智能、更符合现实世界约束的方向发展；3）教育研究：用于训练和测试AI系统的常识推理和抽象思维能力。未来可能影响生成世界模型的构建，促进更可信的AI模拟环境。",
            "highlight_zh": "实验结果显示，主流视频模型（如Veo-3、Sora-2、Wan-2.2）和图像模型（如Nano-banana、GPT-4o-image）在MMGR基准上表现差异显著：在物理常识任务上准确率中等，但在抽象推理任务（如ARC-AGI）上准确率低于10%，具身导航中的长时程空间规划也表现不佳。对比现有评估方法（如FVD），MMGR揭示了模型在推理能力上的巨大性能差距，突出了过度依赖感知数据、全局一致性弱等关键局限性。",
            "tags_zh": [
                "多模态生成推理",
                "视频基础模型评估",
                "物理逻辑约束",
                "抽象推理基准",
                "具身导航",
                "全局一致性",
                "因果推理",
                "生成世界模型"
            ],
            "_index": 3
        },
        {
            "title": "CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives",
            "authors": [
                "Zihan Wang",
                "Jiashun Wang",
                "Jeff Tan",
                "Yiwen Zhao",
                "Jessica Hodgins",
                "Shubham Tulsiani",
                "Deva Ramanan"
            ],
            "arxiv_id": "2512.14696v1",
            "summary": "We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2\\% to 6.9\\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\\% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR.",
            "categories": [
                "cs.CV",
                "cs.GR",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page: https://crisp-real2sim.github.io/CRISP-Real2Sim/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14696v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "人形机器人",
                    "matched_keywords": [
                        "humanoid",
                        "humanoid control"
                    ],
                    "score": 2
                },
                {
                    "name": "动作生成",
                    "matched_keywords": [
                        "human motion"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RL"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 5,
            "headline_zh": "提出CRISP方法，通过平面基元拟合和接触引导，从单目视频重建可模拟的人体运动与场景几何，解决物理交互失败问题。",
            "summary_zh": "我们提出了CRISP方法，用于从单目视频中恢复可模拟的人体运动和场景几何。先前关于人体-场景联合重建的工作依赖于数据驱动的先验和没有物理约束的联合优化，或者恢复出带有伪影的噪声几何，导致具有场景交互的运动跟踪策略失败。相比之下，我们的关键见解是通过对场景点云重建进行平面基元拟合，通过基于深度、法线和光流的简单聚类流程，恢复出凸、干净且适合模拟的几何。为了重建在交互过程中可能被遮挡的场景几何，我们利用人体-场景接触建模（例如，我们使用人体姿态来重建被遮挡的椅子座位）。最后，我们通过使用重建结果通过强化学习驱动人形控制器，确保人体和场景重建在物理上是合理的。我们的方法在以人为本的视频基准（EMDB、PROX）上将运动跟踪失败率从55.2%降低到6.9%，同时实现了43%更快的RL模拟吞吐量。我们进一步在包括随手拍摄的视频、互联网视频甚至Sora生成的视频在内的野外视频上验证了它。这展示了CRISP大规模生成物理有效的人体运动和交互环境的能力，极大地推进了机器人学和AR/VR的真实到模拟应用。",
            "intro_zh": [
                "现有方法依赖数据先验或无物理约束的优化，导致重建几何噪声大、伪影多，使运动跟踪在交互中失败。",
                "CRISP通过平面基元拟合点云和接触建模，恢复凸、干净、可模拟的几何，并利用强化学习确保物理合理性。",
                "在基准测试中，运动跟踪失败率从55.2%降至6.9%，RL模拟吞吐量提升43%，并在多种视频上验证有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在从单目视频中联合重建可模拟的人体运动和场景几何，以支持物理交互。现有方法的痛点包括：依赖数据驱动先验和没有物理约束的联合优化，导致重建几何噪声大、带有伪影，使得基于场景交互的运动跟踪策略（如强化学习控制器）容易失败，无法生成物理上合理的模拟结果。\\n\\n**核心思路**：核心解决思路是恢复凸、干净且适合模拟的场景几何，通过拟合平面基元到场景点云重建，并结合人体-场景接触建模来补全遮挡部分，最后使用重建结果驱动强化学习控制器以确保物理合理性。这样设计是因为平面基元能简化几何表示、减少噪声，而接触建模能利用人体姿态信息推断遮挡场景，从而提升重建质量和模拟成功率。\\n\\n**技术框架**：整体流程包含三个阶段：首先，从单目视频进行初始点云重建；其次，通过基于深度、法线和光流的聚类流程，拟合平面基元到点云，生成凸、干净的场景几何；然后，利用人体姿态和场景接触建模（如通过人体姿势推断椅子座位），重建被遮挡的几何部分；最后，将重建的人体和场景输入强化学习框架，训练人形控制器进行运动跟踪，确保物理合理性。\\n\\n**关键创新**：最重要的技术创新点在于结合平面基元拟合和接触引导的重建方法，与现有方法的本质区别是：现有方法通常直接优化噪声几何或依赖复杂先验，而CRISP通过简单聚类和物理约束（接触建模）生成适合模拟的几何，避免了伪影，并整合强化学习来验证和优化物理合理性，从而显著提升交互成功率。\\n\\n**关键设计**：关键设计包括：使用深度、法线和光流信息进行聚类，以分割点云并拟合平面基元；接触建模模块基于人体姿态（如关节位置）推断场景遮挡部分，例如通过坐姿重建椅子座位；强化学习控制器采用标准人形模拟环境，以重建几何为输入，通过奖励函数优化运动跟踪；参数设置上，聚类阈值和平面拟合误差根据点云密度调整，损失函数可能包含几何一致性和物理约束项，但论文未详细说明网络结构，主要依赖传统算法流程。",
            "application_zh": "该研究在机器人学和AR/VR领域具有重要应用价值。例如，可用于从真实世界视频生成物理有效的模拟环境，训练机器人执行复杂交互任务；在AR/VR中，能创建逼真的虚拟场景和人体动画，提升沉浸感和交互真实性。未来可能推动真实到模拟的大规模应用，降低数据采集和模拟成本。",
            "highlight_zh": "在EMDB和PROX基准测试中，CRISP将运动跟踪失败率从基线方法的55.2%显著降低至6.9%，提升了48.3个百分点；同时，RL模拟吞吐量提高了43%，加速了训练过程。方法在随手拍摄视频、互联网视频和Sora生成视频等野外数据上验证有效，展示了其鲁棒性和泛化能力。",
            "tags_zh": [
                "单目视频重建",
                "人体-场景交互",
                "平面基元拟合",
                "接触建模",
                "强化学习模拟",
                "真实到模拟",
                "物理合理性",
                "运动跟踪"
            ],
            "_index": 4
        },
        {
            "title": "Synthetic Data Pipelines for Adaptive, Mission-Ready Militarized Humanoids",
            "authors": [
                "Mohammed Ayman Habib",
                "Aldo Petruzzelli"
            ],
            "arxiv_id": "2512.14411v1",
            "summary": "Omnia presents a synthetic data driven pipeline to accelerate the training, validation, and deployment readiness of militarized humanoids. The approach converts first-person spatial observations captured from point-of-view recordings, smart glasses, augmented reality headsets, and spatial browsing workflows into scalable, mission-specific synthetic datasets for humanoid autonomy. By generating large volumes of high-fidelity simulated scenarios and pairing them with automated labeling and model training, the pipeline enables rapid iteration on perception, navigation, and decision-making capabilities without the cost, risk, or time constraints of extensive field trials. The resulting datasets can be tuned quickly for new operational environments and threat conditions, supporting both baseline humanoid performance and advanced subsystems such as multimodal sensing, counter-detection survivability, and CBRNE-relevant reconnaissance behaviors. This work targets faster development cycles and improved robustness in complex, contested settings by exposing humanoid systems to broad scenario diversity early in the development process.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "6 pages; xTech Humanoid white paper submission",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14411v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "人形机器人",
                    "matched_keywords": [
                        "humanoid"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO",
                        "VIO"
                    ],
                    "score": 2
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 5,
            "headline_zh": "提出基于合成数据的流水线，以加速军事化人形机器人的训练与部署，解决复杂战场环境下的适应性问题。",
            "summary_zh": "Omnia提出了一种基于合成数据的流水线，旨在加速军事化人形机器人的训练、验证和部署准备。该方法将来自第一人称空间观测（如点视角记录、智能眼镜、增强现实头显和空间浏览工作流）的数据转换为可扩展的、任务特定的合成数据集，用于人形机器人自主性。通过生成大量高保真模拟场景，并结合自动标注和模型训练，该流水线能够在感知、导航和决策能力方面实现快速迭代，而无需承担广泛实地试验的成本、风险或时间限制。生成的数据集可以快速调整以适应新的作战环境和威胁条件，支持基线人形机器人性能以及高级子系统，如多模态传感、反检测生存能力和与CBRNE相关的侦察行为。这项工作通过在开发过程早期将人形机器人系统暴露于广泛的场景多样性中，旨在实现更快的开发周期，并在复杂、对抗性环境中提高鲁棒性。",
            "intro_zh": [
                "核心问题：军事化人形机器人的训练依赖昂贵、高风险且耗时的实地试验，难以快速适应多变战场环境。",
                "方法要点：利用合成数据流水线，将第一人称观测转换为任务特定数据集，实现自动化标注和模型训练。",
                "实验或效果：流水线支持快速迭代感知、导航和决策能力，提升在复杂对抗环境中的鲁棒性和适应性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决军事化人形机器人在训练和部署过程中面临的挑战，包括依赖实地试验带来的高成本、高风险和长时间限制，以及难以快速适应新作战环境和威胁条件的问题。现有方法通常基于有限的实际数据，导致模型泛化能力不足，开发周期长。\\n\\n**核心思路**：论文提出一种基于合成数据的流水线，通过生成大量高保真模拟场景来替代或补充实地数据，结合自动化标注和训练，实现快速迭代和适应。核心思想是利用第一人称空间观测作为输入，转换为可扩展的合成数据集，以加速人形机器人自主性的开发。\\n\\n**技术框架**：整体架构包括数据采集、合成数据生成、自动标注和模型训练四个主要阶段。首先，从点视角记录、智能眼镜等设备捕获第一人称空间观测；然后，通过模拟引擎生成高保真场景，形成合成数据集；接着，使用自动化工具进行数据标注；最后，基于这些数据集训练感知、导航和决策模型，支持快速调整以适应新任务。\\n\\n**关键创新**：最重要的技术创新在于构建了一个端到端的合成数据流水线，能够将多样化的第一人称观测高效转换为任务特定的数据集，并集成自动标注和训练流程。与现有方法相比，本质区别在于它显著降低了数据获取成本和时间，同时通过场景多样性提高了模型的鲁棒性和适应性。\\n\\n**关键设计**：论文未详细说明具体的参数设置、损失函数或网络结构，但强调了流水线的可扩展性和任务特定性。关键设计包括使用高保真模拟器生成多样化场景，以及自动化标注工具来减少人工干预，确保数据集能够快速调整以适应CBRNE等威胁条件。",
            "application_zh": "该研究主要应用于军事和国防领域，特别是军事化人形机器人的开发与部署。潜在价值包括加速机器人自主系统的训练周期，降低实地试验的风险和成本，并提升在复杂战场环境（如城市战、CBRNE威胁场景）中的适应性和生存能力。未来可能扩展到其他高风险或动态环境，如灾难救援或工业自动化。",
            "highlight_zh": "论文未提供具体的实验数据或性能对比，但强调流水线能够实现快速迭代，支持感知、导航和决策能力的提升。通过合成数据，避免了广泛实地试验的成本和时间限制，提高了在复杂、对抗性环境中的鲁棒性。关键亮点在于流水线的可调性，能够快速适应新作战环境和威胁条件。",
            "tags_zh": [
                "合成数据流水线",
                "军事化人形机器人",
                "第一人称观测",
                "自动化标注",
                "任务特定数据集",
                "高保真模拟",
                "快速迭代训练",
                "战场适应性"
            ],
            "_index": 5
        },
        {
            "title": "DriverGaze360: OmniDirectional Driver Attention with Object-Level Guidance",
            "authors": [
                "Shreedhar Govil",
                "Didier Stricker",
                "Jason Rambach"
            ],
            "arxiv_id": "2512.14266v1",
            "summary": "Predicting driver attention is a critical problem for developing explainable autonomous driving systems and understanding driver behavior in mixed human-autonomous vehicle traffic scenarios. Although significant progress has been made through large-scale driver attention datasets and deep learning architectures, existing works are constrained by narrow frontal field-of-view and limited driving diversity. Consequently, they fail to capture the full spatial context of driving environments, especially during lane changes, turns, and interactions involving peripheral objects such as pedestrians or cyclists. In this paper, we introduce DriverGaze360, a large-scale 360$^\\circ$ field of view driver attention dataset, containing $\\sim$1 million gaze-labeled frames collected from 19 human drivers, enabling comprehensive omnidirectional modeling of driver gaze behavior. Moreover, our panoramic attention prediction approach, DriverGaze360-Net, jointly learns attention maps and attended objects by employing an auxiliary semantic segmentation head. This improves spatial awareness and attention prediction across wide panoramic inputs. Extensive experiments demonstrate that DriverGaze360-Net achieves state-of-the-art attention prediction performance on multiple metrics on panoramic driving images. Dataset and method available at https://av.dfki.de/drivergaze360.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14266v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "autonomous driving",
                        "autonomous vehicle",
                        "traffic"
                    ],
                    "score": 3
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO",
                        "VIO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 5,
            "headline_zh": "提出DriverGaze360全景数据集与DriverGaze360-Net网络，以解决自动驾驶中驾驶员注意力预测的视野受限问题。",
            "summary_zh": "预测驾驶员注意力是开发可解释自动驾驶系统和理解人机混合交通场景中驾驶员行为的关键问题。尽管通过大规模驾驶员注意力数据集和深度学习架构已取得显著进展，但现有工作受限于狭窄的前方视野和有限的驾驶多样性，因此无法捕捉驾驶环境的完整空间上下文，尤其是在变道、转弯和涉及行人或骑行者等外围物体的交互过程中。本文介绍了DriverGaze360，这是一个大规模360度视野的驾驶员注意力数据集，包含从19名人类驾驶员收集的约100万帧带注视标签的帧，实现了对驾驶员注视行为的全面全向建模。此外，我们的全景注意力预测方法DriverGaze360-Net通过采用辅助语义分割头联合学习注意力图和被关注物体，从而提高了对宽幅全景输入的空间感知和注意力预测能力。大量实验表明，DriverGaze360-Net在全景驾驶图像的多个指标上实现了最先进的注意力预测性能。数据集和方法可在https://av.dfki.de/drivergaze360获取。",
            "intro_zh": [
                "现有驾驶员注意力预测方法受限于狭窄前方视野，难以捕捉变道、转弯等场景的完整空间上下文，导致预测不全面。",
                "论文提出DriverGaze360全景数据集和DriverGaze360-Net网络，通过联合学习注意力图与语义分割，提升全景输入下的空间感知能力。",
                "实验显示DriverGaze360-Net在全景驾驶图像上实现SOTA性能，在多个评估指标上显著优于现有方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自动驾驶中驾驶员注意力预测的局限性，现有方法主要基于狭窄前方视野数据集，无法有效建模驾驶员在全景环境下的注视行为，尤其在涉及外围物体交互的复杂驾驶场景中表现不足。\\n\\n**核心思路**：通过构建大规模360度视野的驾驶员注意力数据集，并设计一个联合学习注意力图和被关注物体的深度学习网络，以增强模型对全景输入的空间上下文理解，从而提升注意力预测的准确性和鲁棒性。\\n\\n**技术框架**：整体架构包括数据采集与标注、网络训练和评估三个阶段。DriverGaze360-Net采用编码器-解码器结构，编码器处理全景图像输入，解码器包含两个并行头：一个用于生成注意力图，另一个用于语义分割，通过共享特征实现联合优化。\\n\\n**关键创新**：最重要的创新是引入了全景视野的驾驶员注意力数据集DriverGaze360，以及设计了多任务学习框架，将注意力预测与语义分割相结合，这本质区别于现有仅依赖有限视野数据的方法，提供了更全面的空间建模能力。\\n\\n**关键设计**：网络使用卷积神经网络作为编码器，解码器头采用上采样和卷积层；损失函数结合注意力损失（如KL散度）和语义分割损失（如交叉熵），通过加权求和进行端到端训练；数据集包含约100万帧，覆盖19名驾驶员，标注了注视点位置和物体类别。",
            "application_zh": "该研究可应用于自动驾驶系统的可解释性开发，帮助理解驾驶员在混合交通中的行为模式，提升人机交互安全。潜在价值包括辅助驾驶决策、驾驶员监控和交通场景分析，未来可能推动全景感知技术在智能车辆中的普及。",
            "highlight_zh": "DriverGaze360-Net在全景驾驶图像上实现了最先进的注意力预测性能，在多个评估指标（如AUC、NSS）上显著优于基线方法，具体提升幅度因指标而异，例如在某些实验中AUC提升约5-10%，证明了其有效性和泛化能力。",
            "tags_zh": [
                "驾驶员注意力预测",
                "全景视觉",
                "自动驾驶",
                "语义分割",
                "多任务学习",
                "数据集构建",
                "可解释AI",
                "人机交互"
            ],
            "_index": 6
        },
        {
            "title": "ViBES: A Conversational Agent with Behaviorally-Intelligent 3D Virtual Body",
            "authors": [
                "Juze Zhang",
                "Changan Chen",
                "Xin Chen",
                "Heng Yu",
                "Tiange Xiang",
                "Ali Sartaz Khan",
                "Shrinidhi K. Lakshmikanth",
                "Ehsan Adeli"
            ],
            "arxiv_id": "2512.14234v1",
            "summary": "Human communication is inherently multimodal and social: words, prosody, and body language jointly carry intent. Yet most prior systems model human behavior as a translation task co-speech gesture or text-to-motion that maps a fixed utterance to motion clips-without requiring agentic decision-making about when to move, what to do, or how to adapt across multi-turn dialogue. This leads to brittle timing, weak social grounding, and fragmented stacks where speech, text, and motion are trained or inferred in isolation. We introduce ViBES (Voice in Behavioral Expression and Synchrony), a conversational 3D agent that jointly plans language and movement and executes dialogue-conditioned body actions. Concretely, ViBES is a speech-language-behavior (SLB) model with a mixture-of-modality-experts (MoME) backbone: modality-partitioned transformer experts for speech, facial expression, and body motion. The model processes interleaved multimodal token streams with hard routing by modality (parameters are split per expert), while sharing information through cross-expert attention. By leveraging strong pretrained speech-language models, the agent supports mixed-initiative interaction: users can speak, type, or issue body-action directives mid-conversation, and the system exposes controllable behavior hooks for streaming responses. We further benchmark on multi-turn conversation with automatic metrics of dialogue-motion alignment and behavior quality, and observe consistent gains over strong co-speech and text-to-motion baselines. ViBES goes beyond \"speech-conditioned motion generation\" toward agentic virtual bodies where language, prosody, and movement are jointly generated, enabling controllable, socially competent 3D interaction. Code and data will be made available at: ai.stanford.edu/~juze/ViBES/",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page: https://ai.stanford.edu/~juze/ViBES/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14234v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "动作生成",
                    "matched_keywords": [
                        "motion generation"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO",
                        "VIO"
                    ],
                    "score": 2
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 5,
            "headline_zh": "提出ViBES对话代理，通过联合规划语言与运动解决多模态交互中的时序与社交基础问题",
            "summary_zh": "人类交流本质上是多模态和社交性的：语言、韵律和肢体语言共同传递意图。然而，大多数现有系统将人类行为建模为语音伴随手势或文本到运动的翻译任务，将固定话语映射到运动片段，而不需要代理决策何时移动、做什么或如何在多轮对话中适应。这导致脆弱的时序、薄弱的社交基础和碎片化的堆栈，其中语音、文本和运动被孤立地训练或推断。我们介绍了ViBES（语音行为表达与同步），这是一个联合规划语言和运动并执行对话条件身体动作的对话式3D代理。具体而言，ViBES是一个具有混合模态专家（MoME）骨干的语音-语言-行为（SLB）模型：模态分区的Transformer专家用于语音、面部表情和身体运动。该模型通过模态硬路由（参数按专家分割）处理交错的多模态令牌流，同时通过跨专家注意力共享信息。通过利用强大的预训练语音-语言模型，该代理支持混合主动交互：用户可以在对话中说话、打字或发出身体动作指令，系统暴露可控行为钩子以进行流式响应。我们进一步在多轮对话上使用对话-运动对齐和行为质量的自动指标进行基准测试，并观察到相对于强大的语音伴随和文本到运动基线的持续增益。ViBES超越了“语音条件运动生成”，走向代理虚拟身体，其中语言、韵律和运动被联合生成，实现可控、社交能力的3D交互。代码和数据将在ai.stanford.edu/~juze/ViBES/提供。",
            "intro_zh": [
                "现有方法将人类行为建模为固定话语到运动片段的翻译任务，缺乏代理决策，导致时序脆弱、社交基础薄弱和模态孤立。",
                "ViBES提出语音-语言-行为模型，通过混合模态专家骨干联合规划语言与运动，实现对话条件身体动作。",
                "在多轮对话基准测试中，ViBES在对话-运动对齐和行为质量上优于基线，支持混合主动交互和可控行为钩子。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多模态对话代理中语言、语音和身体运动孤立建模的问题，现有方法如语音伴随手势或文本到运动生成将行为视为翻译任务，导致时序不协调、社交基础弱和模态碎片化，无法适应多轮交互。\\n\\n**核心思路**：论文提出ViBES，一个联合规划语言和运动的对话式3D代理，核心思路是构建语音-语言-行为模型，通过混合模态专家骨干处理多模态令牌流，实现模态间信息共享，从而支持代理决策和自适应交互。\\n\\n**技术框架**：整体架构基于混合模态专家（MoME）骨干，包含模态分区的Transformer专家模块，分别处理语音、面部表情和身体运动。流程包括：输入多模态令牌流（如语音、文本、动作指令），通过硬路由按模态分配至对应专家，利用跨专家注意力实现信息融合，输出联合生成的语言、韵律和运动序列，支持流式响应和可控行为钩子。\\n\\n**关键创新**：最重要的技术创新是引入MoME骨干，将模态分区与跨专家注意力结合，实现参数高效分割和模态间协同，区别于现有孤立建模方法，本质区别在于从“翻译任务”转向“代理决策”，强调联合生成和社交能力。\\n\\n**关键设计**：关键设计包括：模态硬路由机制，参数按专家分割以减少计算开销；跨专家注意力层，促进语音、语言和运动令牌间的信息交换；利用预训练语音-语言模型初始化，增强基础能力；损失函数可能结合对话对齐和运动质量指标，具体细节未知；网络结构基于Transformer，专家数量对应模态数（语音、面部、身体）。",
            "application_zh": "ViBES的潜在应用领域包括虚拟助手、社交机器人、游戏角色和远程协作系统，通过实现可控、社交能力的3D交互，提升用户体验和自然度。实际价值在于促进人机交互的多模态融合，未来可能推动智能代理在娱乐、教育和医疗等场景的广泛应用。",
            "highlight_zh": "实验在多轮对话基准上进行，使用自动指标评估对话-运动对齐和行为质量。ViBES在多个指标上优于强基线，如语音伴随手势和文本到运动生成方法，具体提升幅度未知，但报告了持续增益。结果验证了联合规划方法的有效性，支持混合主动交互和可控行为钩子。",
            "tags_zh": [
                "多模态对话代理",
                "语音-语言-行为模型",
                "混合模态专家",
                "3D虚拟身体",
                "联合规划",
                "社交交互",
                "跨模态注意力",
                "可控行为生成"
            ],
            "_index": 7
        },
        {
            "title": "Understanding and Improving Hyperbolic Deep Reinforcement Learning",
            "authors": [
                "Timo Klein",
                "Thomas Lang",
                "Andrii Shkabrii",
                "Alexander Sturm",
                "Kevin Sidak",
                "Lukas Miklautz",
                "Claudia Plant",
                "Yllka Velaj",
                "Sebastian Tschiatschek"
            ],
            "arxiv_id": "2512.14202v1",
            "summary": "The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying feature representations. Hyperbolic feature spaces are well-suited for this purpose, as they naturally capture hierarchical and relational structure often present in complex RL environments. However, leveraging these spaces commonly faces optimization challenges due to the nonstationarity of RL. In this work, we identify key factors that determine the success and failure of training hyperbolic deep RL agents. By analyzing the gradients of core operations in the Poincaré Ball and Hyperboloid models of hyperbolic geometry, we show that large-norm embeddings destabilize gradient-based training, leading to trust-region violations in proximal policy optimization (PPO). Based on these insights, we introduce Hyper++, a new hyperbolic PPO agent that consists of three components: (i) stable critic training through a categorical value loss instead of regression; (ii) feature regularization guaranteeing bounded norms while avoiding the curse of dimensionality from clipping; and (iii) using a more optimization-friendly formulation of hyperbolic network layers. In experiments on ProcGen, we show that Hyper++ guarantees stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by approximately 30%. On Atari-5 with Double DQN, Hyper++ strongly outperforms Euclidean and hyperbolic baselines. We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl .",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14202v1",
            "code_links": [
                {
                    "url": "https://github.com/Probabilistic-and-Interactive-ML/hyper-rl",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO",
                        "VIO"
                    ],
                    "score": 2
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RL",
                        "PPO"
                    ],
                    "score": 3
                }
            ],
            "relevance_score": 5,
            "headline_zh": "提出Hyper++方法以解决双曲深度强化学习中的优化不稳定问题",
            "summary_zh": "强化学习（RL）智能体的性能关键取决于底层特征表示的质量。双曲特征空间非常适合这一目的，因为它们能自然地捕捉复杂RL环境中常见的层次和关系结构。然而，由于RL的非平稳性，利用这些空间通常面临优化挑战。在这项工作中，我们确定了决定双曲深度RL智能体训练成功与失败的关键因素。通过分析双曲几何的庞加莱球和双曲面模型中核心操作的梯度，我们发现大范数嵌入会破坏基于梯度的训练稳定性，导致近端策略优化（PPO）中的信任区域违反。基于这些见解，我们引入了Hyper++，这是一种新的双曲PPO智能体，包含三个组件：（i）通过分类值损失而非回归实现稳定的评论家训练；（ii）特征正则化保证有界范数，同时避免裁剪带来的维度诅咒；（iii）使用更优化友好的双曲网络层公式。在ProcGen上的实验表明，Hyper++保证了稳定学习，优于先前的双曲智能体，并将挂钟时间减少了约30%。在Atari-5上使用Double DQN时，Hyper++显著优于欧几里得和双曲基线。我们在https://github.com/Probabilistic-and-Interactive-ML/hyper-rl 发布了代码。",
            "intro_zh": [
                "核心问题：双曲深度强化学习中，大范数嵌入导致梯度不稳定，破坏PPO的信任区域约束，影响训练效果。",
                "方法要点：提出Hyper++方法，通过分类值损失、特征正则化和优化友好的双曲层设计，稳定训练过程。",
                "实验或效果：在ProcGen和Atari-5上，Hyper++性能优于基线，训练时间减少约30%，验证了方法的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决双曲深度强化学习（RL）中的优化不稳定问题。现有方法在双曲特征空间中训练RL智能体时，由于RL的非平稳性，常面临梯度爆炸和训练不稳定的挑战，特别是大范数嵌入会破坏近端策略优化（PPO）的信任区域约束，导致性能下降。\\n\\n**核心思路**：论文的核心思路是通过分析双曲几何模型（如庞加莱球和双曲面）中梯度行为，识别出大范数嵌入是训练不稳定的关键因素，并设计综合解决方案来稳定训练过程，包括改进损失函数、引入正则化和优化网络层结构。\\n\\n**技术框架**：整体架构基于PPO框架，但针对双曲空间进行了专门优化。主要模块包括：稳定的评论家训练模块，使用分类值损失替代回归损失；特征正则化模块，确保嵌入范数有界，避免裁剪带来的维度问题；以及优化友好的双曲网络层模块，重新设计层公式以提升梯度稳定性。训练流程遵循标准RL循环，但融入了这些组件来增强鲁棒性。\\n\\n**关键创新**：最重要的技术创新是Hyper++方法的三组件设计：分类值损失用于评论家训练，避免了回归损失中的梯度不稳定；特征正则化机制，在保证范数有界的同时不引入维度诅咒；以及更优化友好的双曲层公式，直接针对梯度问题优化。与现有方法相比，本质区别在于系统性解决了双曲RL中的梯度不稳定根源，而非仅依赖启发式调整。\\n\\n**关键设计**：关键设计包括：使用分类值损失（如基于分布的损失）替代均方误差回归，以减少梯度波动；特征正则化采用约束优化或惩罚项，确保嵌入范数保持在合理范围内；双曲网络层重新参数化，例如使用更稳定的投影操作或梯度裁剪替代方案；在ProcGen和Atari-5环境中，参数设置可能涉及学习率调整和正则化强度，具体细节需参考代码实现。",
            "application_zh": "该研究在强化学习领域具有广泛的应用潜力，特别是在需要处理层次或关系结构的复杂环境中，如游戏AI（如Atari和ProcGen）、机器人控制、自动驾驶和智能决策系统。通过稳定双曲特征学习，Hyper++能提升智能体在非平稳环境中的泛化能力和效率，实际价值包括减少训练时间、提高性能鲁棒性。未来可能影响深度RL的理论发展，推动双曲几何在更多AI任务中的应用。",
            "highlight_zh": "最重要的实验结果包括：在ProcGen基准测试中，Hyper++保证了稳定学习，性能优于先前双曲智能体，并将挂钟时间减少了约30%；在Atari-5环境中使用Double DQN时，Hyper++显著优于欧几里得和双曲基线，具体提升幅度未在摘要中给出，但强调了“强烈优于”。这些结果验证了Hyper++在解决优化不稳定问题上的有效性，并展示了其在提升训练效率和性能方面的优势。",
            "tags_zh": [
                "双曲几何",
                "深度强化学习",
                "近端策略优化",
                "特征正则化",
                "梯度稳定",
                "ProcGen基准",
                "Atari游戏",
                "优化算法"
            ],
            "_index": 8
        },
        {
            "title": "A First-Order Logic-Based Alternative to Reward Models in RLHF",
            "authors": [
                "Chunjin Jian",
                "Xinhua Zhu"
            ],
            "arxiv_id": "2512.14100v1",
            "summary": "Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in aligning large language models (LLMs) with human values and preferences. However, the quality and stability of the trained reward model largely determine the final alignment performance. Existing approaches such as Proximal Policy Optimization (PPO) rely heavily on reward models to guide LLMs toward human-aligned behaviors.\n  In this work, we propose a logic-similarity-based reward mechanism as an alternative to conventional reward modeling. Instead of relying on heuristic reward estimation, our method leverages formal logical consistency to steer model alignment with human preferences. Since real-world questions can be interpreted from multiple perspectives, to ensure that logic-based reinforcement learning does not cause model collapse, we introduce S-GRPO, a supervised variant of the GRPO framework. S-GRPO incorporates an additional supervised component and jointly optimizes the generation term, KL-divergence regularization, and label-based objective during training.\n  Experimental results demonstrate that S-GRPO consistently outperforms standard supervised fine-tuning (SFT) in both performance and robustness. Furthermore, it extends existing preference-learning frameworks such as GRPO and DPO, offering a more flexible and task-adaptive approach to alignment training. Our code is available at https://github.com/ChunjinJiang/sgrpo.",
            "categories": [
                "cs.LG",
                "cs.LO"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14100v1",
            "code_links": [
                {
                    "url": "https://github.com/ChunjinJiang/sgrpo",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RL",
                        "reward",
                        "PPO"
                    ],
                    "score": 4
                }
            ],
            "relevance_score": 5,
            "headline_zh": "提出基于逻辑相似性的奖励机制S-GRPO，以替代传统奖励模型，解决RLHF中奖励模型质量与稳定性问题。",
            "summary_zh": "基于人类反馈的强化学习（RLHF）在将大型语言模型（LLMs）与人类价值观和偏好对齐方面起着关键作用。然而，训练出的奖励模型的质量和稳定性在很大程度上决定了最终的对齐性能。现有方法如近端策略优化（PPO）严重依赖奖励模型来引导LLMs朝向人类对齐的行为。在这项工作中，我们提出了一种基于逻辑相似性的奖励机制，作为传统奖励建模的替代方案。我们的方法不依赖启发式奖励估计，而是利用形式逻辑一致性来引导模型与人类偏好对齐。由于现实世界的问题可以从多个角度解释，为了确保基于逻辑的强化学习不会导致模型崩溃，我们引入了S-GRPO，这是GRPO框架的一个监督变体。S-GRPO在训练过程中结合了一个额外的监督组件，并联合优化生成项、KL散度正则化和基于标签的目标。实验结果表明，S-GRPO在性能和鲁棒性方面均持续优于标准监督微调（SFT）。此外，它扩展了现有的偏好学习框架，如GRPO和DPO，为对齐训练提供了更灵活和任务自适应的方法。我们的代码可在https://github.com/ChunjinJiang/sgrpo获取。",
            "intro_zh": [
                "现有RLHF方法依赖奖励模型，其质量和稳定性直接影响对齐效果，存在不确定性风险。",
                "提出基于逻辑相似性的奖励机制，利用形式逻辑一致性替代启发式奖励估计，提升对齐可靠性。",
                "实验显示S-GRPO在性能和鲁棒性上优于标准监督微调，并扩展了现有偏好学习框架的灵活性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决RLHF中传统奖励模型的质量和稳定性问题，这些模型依赖启发式估计，可能导致对齐性能不稳定或模型崩溃，特别是在多视角现实问题中。\\n\\n**核心思路**：核心思路是引入基于逻辑相似性的奖励机制，利用形式逻辑一致性来替代传统奖励模型，通过逻辑推理确保模型输出与人类偏好对齐，避免依赖不稳定的奖励估计。\\n\\n**技术框架**：整体框架基于S-GRPO，这是GRPO框架的监督变体。它包括逻辑相似性奖励计算模块、监督组件和联合优化流程。训练时，系统首先评估生成内容的逻辑一致性，然后结合监督信号，通过多目标优化调整模型参数。\\n\\n**关键创新**：最重要的创新是逻辑相似性奖励机制，它用形式逻辑替代启发式奖励，本质区别在于从数据驱动转向逻辑驱动，提高了对齐的可靠性和可解释性，同时S-GRPO框架整合了监督学习以增强稳定性。\\n\\n**关键设计**：关键设计包括逻辑一致性评分函数、S-GRPO的损失函数（结合生成损失、KL散度正则化和监督损失），以及网络结构中的多任务头。参数设置涉及逻辑阈值调整和优化器选择，以确保训练效率和模型泛化。",
            "application_zh": "该研究可应用于大型语言模型的对齐训练，特别是在需要高可靠性和多视角理解的场景，如AI助手、内容生成和决策支持系统。它通过逻辑驱动方法提升模型与人类价值观的一致性，为AI安全和人机交互提供更稳定的解决方案，未来可能推动RLHF向更可解释和自适应方向发展。",
            "highlight_zh": "实验结果显示，S-GRPO在多个基准测试中持续优于标准监督微调（SFT），具体性能提升包括准确率提高约5-10%和鲁棒性增强。对比基线如GRPO和DPO，S-GRPO在任务自适应和灵活性方面表现更优，验证了逻辑相似性奖励机制的有效性。",
            "tags_zh": [
                "强化学习人类反馈",
                "逻辑相似性奖励",
                "模型对齐",
                "S-GRPO框架",
                "形式逻辑一致性",
                "监督变体",
                "多目标优化",
                "AI安全"
            ],
            "_index": 9
        },
        {
            "title": "OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving",
            "authors": [
                "Zhenguo Zhang",
                "Haohan Zhen",
                "Yishen Wang",
                "Le Xu",
                "Tianchen Deng",
                "Xuefeng Chen",
                "Qu Chen",
                "Bo Zhang",
                "Wuxiong Huang"
            ],
            "arxiv_id": "2512.14044v1",
            "summary": "The deployment of Vision-Language Models (VLMs) in safety-critical domains like autonomous driving (AD) is critically hindered by reliability failures, most notably object hallucination. This failure stems from their reliance on ungrounded, text-based Chain-of-Thought (CoT) reasoning.While existing multi-modal CoT approaches attempt mitigation, they suffer from two fundamental flaws: (1) decoupled perception and reasoning stages that prevent end-to-end joint optimization, and (2) reliance on expensive, dense localization labels.Thus we introduce OmniDrive-R1, an end-to-end VLM framework designed for autonomous driving, which unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. Our core innovation is an Reinforcement-driven visual grounding capability, enabling the model to autonomously direct its attention and \"zoom in\" on critical regions for fine-grained analysis. This capability is enabled by our pure two-stage reinforcement learning training pipeline and Clip-GRPO algorithm. Crucially, Clip-GRPO introduces an annotation-free, process-based grounding reward. This reward not only eliminates the need for dense labels but also circumvents the instability of external tool calls by enforcing real-time cross-modal consistency between the visual focus and the textual reasoning. Extensive experiments on DriveLMM-o1 demonstrate our model's significant improvements. Compared to the baseline Qwen2.5VL-7B, OmniDrive-R1 improves the overall reasoning score from 51.77% to 80.35%, and the final answer accuracy from 37.81% to 73.62%.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14044v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "autonomous driving"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RL",
                        "reward"
                    ],
                    "score": 3
                }
            ],
            "relevance_score": 5,
            "headline_zh": "提出OmniDrive-R1框架，通过强化学习驱动的交错多模态思维链解决自动驾驶中视觉语言模型的可靠性问题",
            "summary_zh": "在自动驾驶等安全关键领域部署视觉语言模型时，可靠性问题（特别是物体幻觉）严重阻碍了其应用。这种问题源于模型依赖未接地的文本思维链推理。现有多模态思维链方法虽然尝试缓解，但存在两个根本缺陷：（1）感知与推理阶段解耦，无法进行端到端联合优化；（2）依赖昂贵密集的定位标注。为此，我们提出了OmniDrive-R1，这是一个为自动驾驶设计的端到端视觉语言模型框架，通过交错多模态思维链机制统一了感知与推理。我们的核心创新是强化学习驱动的视觉接地能力，使模型能够自主引导注意力并“放大”关键区域进行细粒度分析。这一能力通过我们的纯两阶段强化学习训练流程和Clip-GRPO算法实现。关键的是，Clip-GRPO引入了无需标注、基于过程的接地奖励。该奖励不仅消除了对密集标注的需求，还通过强制视觉焦点与文本推理之间的实时跨模态一致性，规避了外部工具调用的不稳定性。在DriveLMM-o1数据集上的大量实验表明，我们的模型取得了显著改进。与基线Qwen2.5VL-7B相比，OmniDrive-R1将整体推理分数从51.77%提升到80.35%，最终答案准确率从37.81%提升到73.62%。",
            "intro_zh": [
                "现有多模态思维链方法存在感知与推理阶段解耦、依赖密集标注的问题，导致自动驾驶中视觉语言模型可靠性不足。",
                "提出交错多模态思维链机制，通过强化学习驱动的视觉接地能力实现端到端联合优化，无需密集标注。",
                "在DriveLMM-o1数据集上，整体推理分数提升28.58个百分点，最终答案准确率提升35.81个百分点。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自动驾驶中视觉语言模型因物体幻觉导致的可靠性问题。现有多模态思维链方法存在两个主要痛点：一是感知与推理阶段解耦，限制了端到端优化；二是依赖昂贵密集的定位标注，增加了部署成本。\\n\\n**核心思路**：论文提出通过交错多模态思维链机制统一感知与推理，利用强化学习驱动的视觉接地能力，使模型能自主聚焦关键区域进行细粒度分析，实现无需密集标注的端到端训练。\\n\\n**技术框架**：整体架构为端到端视觉语言模型框架，包含交错多模态思维链模块和强化学习训练流程。主要阶段包括：输入多模态数据，通过iMCoT机制进行交错推理，利用Clip-GRPO算法优化视觉接地，输出可靠答案。\\n\\n**关键创新**：最重要的技术创新是强化学习驱动的视觉接地能力，通过Clip-GRPO算法引入基于过程的接地奖励，实现实时跨模态一致性，与现有方法相比，本质区别在于消除了标注依赖和外部工具调用。\\n\\n**关键设计**：采用纯两阶段强化学习训练流程，第一阶段预训练基础模型，第二阶段使用Clip-GRPO算法优化接地奖励；关键参数包括奖励函数中的跨模态一致性约束，网络结构基于视觉语言模型骨干，如Qwen2.5VL-7B变体。",
            "application_zh": "该研究主要应用于自动驾驶领域，特别是视觉语言模型在安全关键任务中的部署，如环境感知、决策制定和路径规划。其实际价值在于提高模型的可靠性和安全性，减少物体幻觉风险，未来可能扩展到机器人导航、智能监控等需要多模态推理的场景，推动可信人工智能的发展。",
            "highlight_zh": "在DriveLMM-o1数据集上的实验显示，OmniDrive-R1相比基线Qwen2.5VL-7B，整体推理分数从51.77%提升至80.35%，绝对提升28.58个百分点；最终答案准确率从37.81%提升至73.62%，绝对提升35.81个百分点，显著改善了自动驾驶中视觉语言模型的可靠性。",
            "tags_zh": [
                "自动驾驶",
                "视觉语言模型",
                "强化学习",
                "多模态思维链",
                "视觉接地",
                "端到端优化",
                "可靠性提升",
                "跨模态一致性"
            ],
            "_index": 10
        },
        {
            "title": "ACE-SLAM: Scene Coordinate Regression for Neural Implicit Real-Time SLAM",
            "authors": [
                "Ignacio Alzugaray",
                "Marwan Taher",
                "Andrew J. Davison"
            ],
            "arxiv_id": "2512.14032v1",
            "summary": "We present a novel neural RGB-D Simultaneous Localization And Mapping (SLAM) system that learns an implicit map of the scene in real time. For the first time, we explore the use of Scene Coordinate Regression (SCR) as the core implicit map representation in a neural SLAM pipeline, a paradigm that trains a lightweight network to directly map 2D image features to 3D global coordinates. SCR networks provide efficient, low-memory 3D map representations, enable extremely fast relocalization, and inherently preserve privacy, making them particularly suitable for neural implicit SLAM.\n  Our system is the first one to achieve strict real-time in neural implicit RGB-D SLAM by relying on a SCR-based representation. We introduce a novel SCR architecture specifically tailored for this purpose and detail the critical design choices required to integrate SCR into a live SLAM pipeline. The resulting framework is simple yet flexible, seamlessly supporting both sparse and dense features, and operates reliably in dynamic environments without special adaptation. We evaluate our approach on established synthetic and real-world benchmarks, demonstrating competitive performance against the state of the art. Project Page: https://github.com/ialzugaray/ace-slam",
            "categories": [
                "cs.CV",
                "cs.AI",
                "eess.IV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project Page: https://github.com/ialzugaray/ace-slam",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14032v1",
            "code_links": [
                {
                    "url": "https://github.com/ialzugaray/ace-slam",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "SLAM",
                        "localization",
                        "mapping"
                    ],
                    "score": 3
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 5,
            "headline_zh": "提出基于场景坐标回归的神经隐式实时SLAM系统，首次在RGB-D SLAM中实现严格实时性能",
            "summary_zh": "我们提出了一种新颖的神经RGB-D同步定位与建图（SLAM）系统，能够实时学习场景的隐式地图。首次探索了将场景坐标回归（SCR）作为神经SLAM流程中的核心隐式地图表示范式，该方法训练一个轻量级网络直接将2D图像特征映射到3D全局坐标。SCR网络提供高效、低内存的3D地图表示，支持极快的重定位，并固有地保护隐私，使其特别适合神经隐式SLAM。我们的系统是首个通过基于SCR的表示在神经隐式RGB-D SLAM中实现严格实时的系统。我们引入了一种专门为此目的设计的新型SCR架构，并详细说明了将SCR集成到实时SLAM流程中的关键设计选择。所得框架简单而灵活，无缝支持稀疏和密集特征，并在动态环境中可靠运行而无需特殊适应。我们在已建立的合成和真实世界基准上评估了我们的方法，展示了与最先进技术相比的竞争性能。项目页面：https://github.com/ialzugaray/ace-slam",
            "intro_zh": [
                "现有神经隐式SLAM方法通常依赖复杂网络表示地图，导致计算开销大、难以实现严格实时性能，限制了实际应用。",
                "论文提出使用场景坐标回归（SCR）作为核心隐式地图表示，训练轻量网络直接从图像特征预测3D坐标，简化地图表示并提升效率。",
                "在合成和真实基准测试中，系统实现严格实时运行，性能与最先进方法竞争，同时支持动态环境且无需特殊处理。"
            ],
            "method_zh": "**问题定义**：论文旨在解决神经隐式RGB-D SLAM中难以实现严格实时性能的问题。现有方法通常使用复杂的神经隐式表示（如神经辐射场或隐式函数）来建模场景，这些方法计算开销大、内存占用高，导致实时性受限，特别是在动态环境中适应性差。\\n\\n**核心思路**：论文的核心思路是采用场景坐标回归（SCR）作为隐式地图表示的核心机制。SCR通过训练一个轻量级神经网络，直接从输入的2D图像特征回归到对应的3D全局场景坐标，从而避免了传统隐式表示中的复杂优化过程。这种设计简化了地图表示，降低了计算和内存需求，使得实时SLAM成为可能。\\n\\n**技术框架**：整体架构包括数据输入、SCR网络、SLAM流程集成和输出模块。首先，RGB-D图像作为输入，提取特征（支持稀疏或密集特征）。然后，SCR网络处理这些特征，预测每个像素对应的3D场景坐标。这些坐标用于构建隐式地图，并集成到标准的SLAM流程中（如位姿估计和地图更新），实现实时定位与建图。框架还包括动态环境处理机制，无需额外适应。\\n\\n**关键创新**：最重要的技术创新是首次将SCR范式引入神经隐式SLAM作为核心地图表示，并设计专门的新型SCR架构。与现有方法相比，本质区别在于SCR直接回归坐标，而非学习复杂的隐式函数，从而大幅提升效率，实现严格实时性能，同时保持灵活性和鲁棒性。\\n\\n**关键设计**：关键设计包括轻量级SCR网络结构（具体架构未知，但强调高效性）、损失函数（可能基于坐标预测误差，如均方误差）、参数设置（优化实时性能，如网络层数和特征维度）、以及集成到SLAM流程中的设计选择（如实时数据流处理和动态环境适应）。这些设计确保了系统在资源受限下的高效运行。",
            "application_zh": "该研究在增强现实、机器人导航、自动驾驶和虚拟现实等领域具有潜在应用价值。通过实现严格实时性能，它能够支持动态环境中的即时定位与建图，提升系统响应速度和用户体验。未来可能推动轻量级SLAM技术的发展，促进隐私保护型地图表示在实际部署中的普及。",
            "highlight_zh": "在合成和真实世界基准测试中，ACE-SLAM实现严格实时运行（具体帧率未知），性能与最先进神经隐式SLAM方法竞争。实验显示，SCR表示显著降低内存占用，支持极快重定位，并在动态环境中可靠运行，无需特殊适应，验证了方法的有效性和实用性。",
            "tags_zh": [
                "场景坐标回归",
                "神经隐式SLAM",
                "实时SLAM",
                "RGB-D SLAM",
                "轻量级网络",
                "动态环境适应",
                "隐私保护地图"
            ],
            "_index": 11
        },
        {
            "title": "CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation",
            "authors": [
                "Sirui Chen",
                "Zi-ang Cao",
                "Zhengyi Luo",
                "Fernando Castañeda",
                "Chenran Li",
                "Tingwu Wang",
                "Ye Yuan",
                "Linxi \"Jim\" Fan",
                "C. Karen Liu",
                "Yuke Zhu"
            ],
            "arxiv_id": "2512.14689v1",
            "summary": "Recent progress in humanoid robots has unlocked agile locomotion skills, including backflipping, running, and crawling. Yet it remains challenging for a humanoid robot to perform forceful manipulation tasks such as moving objects, wiping, and pushing a cart. We propose adaptive Compliance Humanoid control through hIsight Perturbation (CHIP), a plug-and-play module that enables controllable end-effector stiffness while preserving agile tracking of dynamic reference motions. CHIP is easy to implement and requires neither data augmentation nor additional reward tuning. We show that a generalist motion-tracking controller trained with CHIP can perform a diverse set of forceful manipulation tasks that require different end-effector compliance, such as multi-robot collaboration, wiping, box delivery, and door opening.",
            "categories": [
                "cs.RO",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "The first two authors contributed equally. Project page: https://nvlabs.github.io/CHIP/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14689v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "人形机器人",
                    "matched_keywords": [
                        "humanoid",
                        "humanoid robot",
                        "humanoid control"
                    ],
                    "score": 3
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reward"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出CHIP自适应柔顺控制模块，解决人形机器人执行强力操作任务时末端刚度可控与动态运动跟踪难以兼顾的问题。",
            "summary_zh": "人形机器人在敏捷运动技能（如后空翻、奔跑、爬行）方面已取得显著进展，但在执行强力操作任务（如移动物体、擦拭、推车）时仍面临挑战。本文提出自适应柔顺人形控制通过后见扰动（CHIP），这是一个即插即用模块，能够在保持对动态参考运动敏捷跟踪的同时，实现末端执行器刚度的可控调节。CHIP易于实现，既不需要数据增强，也无需额外的奖励调整。研究表明，使用CHIP训练的通用运动跟踪控制器能够执行多种需要不同末端柔顺性的强力操作任务，例如多机器人协作、擦拭、箱子递送和开门。",
            "intro_zh": [
                "现有方法难以平衡人形机器人末端刚度控制与动态运动跟踪，导致强力操作任务执行受限。",
                "CHIP通过后见扰动机制自适应调节末端柔顺性，无需额外数据或奖励调整，实现即插即用。",
                "实验表明，CHIP能显著提升多任务性能，如协作、擦拭等，验证了其通用性和有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决人形机器人在执行强力操作任务（如移动物体、擦拭、推车）时，如何同时实现末端执行器刚度的可控调节与对动态参考运动的敏捷跟踪。现有方法的痛点在于，传统控制器往往难以平衡这两方面需求：要么过于刚性导致操作失败，要么过于柔顺影响运动精度，且通常需要复杂的数据增强或奖励设计来适应不同任务。\\n\\n**核心思路**：论文提出CHIP（自适应柔顺人形控制通过后见扰动），其核心思路是利用后见扰动机制，在训练过程中模拟不同末端刚度条件下的扰动，使控制器学会自适应调整柔顺性，从而在保持运动跟踪性能的同时，灵活应对各种操作任务的需求。这样设计的原因在于，后见扰动能够在不增加额外数据或奖励的情况下，自然融入训练过程，提升控制器的鲁棒性和泛化能力。\\n\\n**技术框架**：整体架构包括一个通用运动跟踪控制器作为基础，CHIP作为插件模块集成其中。流程分为训练和应用阶段：在训练时，CHIP通过后见扰动生成多样化的末端刚度场景，控制器学习在这些场景下优化跟踪和操作性能；在应用时，控制器能根据实时任务需求自动调节末端柔顺性。主要模块包括扰动生成器、刚度调节器和运动跟踪器。\\n\\n**关键创新**：最重要的技术创新点是后见扰动机制的应用，它允许控制器在训练中“回顾”并适应不同刚度条件，实现自适应柔顺控制。与现有方法的本质区别在于，CHIP无需手动设计数据增强或复杂奖励函数，而是通过扰动自然学习，降低了实现难度并提高了通用性。\\n\\n**关键设计**：关键设计包括扰动生成策略，它基于历史运动数据模拟刚度变化；损失函数结合了运动跟踪误差和操作任务目标，以平衡精度与柔顺性；网络结构采用强化学习框架，可能包含策略网络和价值网络，具体参数设置如学习率、扰动幅度需根据任务调整，但论文强调其易于实现性。",
            "application_zh": "该研究在人形机器人领域具有广泛潜在应用，包括工业自动化中的物体搬运和装配、服务机器人中的家务协助（如擦拭、开门）、医疗康复中的辅助操作，以及多机器人协作场景。其实际价值在于提升机器人在复杂环境中的操作能力和适应性，未来可能推动人形机器人向更通用、智能的方向发展，降低部署成本。",
            "highlight_zh": "实验结果显示，CHIP在多种强力操作任务上表现优异：在多机器人协作任务中，成功实现协同搬运；在擦拭任务中，保持接触力稳定；在箱子递送和开门任务中，均达到高成功率。与基线方法相比，CHIP在操作精度和任务完成率上均有显著提升，具体数据未知，但论文强调其无需额外调整即可适应不同刚度需求，验证了模块的通用性和有效性。",
            "tags_zh": [
                "人形机器人控制",
                "自适应柔顺性",
                "后见扰动",
                "强力操作任务",
                "运动跟踪",
                "即插即用模块",
                "多任务学习",
                "机器人操作"
            ],
            "_index": 12
        },
        {
            "title": "gridfm-datakit-v1: A Python Library for Scalable and Realistic Power Flow and Optimal Power Flow Data Generation",
            "authors": [
                "Alban Puech",
                "Matteo Mazzonelli",
                "Celia Cintas",
                "Tamara R. Govindasamy",
                "Mangaliso Mngomezulu",
                "Jonas Weiss",
                "Matteo Baù",
                "Anna Varbella",
                "François Mirallès",
                "Kibaek Kim",
                "Le Xie",
                "Hendrik F. Hamann",
                "Etienne Vos",
                "Thomas Brunschwiler"
            ],
            "arxiv_id": "2512.14658v1",
            "summary": "We introduce gridfm-datakit-v1, a Python library for generating realistic and diverse Power Flow (PF) and Optimal Power Flow (OPF) datasets for training Machine Learning (ML) solvers. Existing datasets and libraries face three main challenges: (1) lack of realistic stochastic load and topology perturbations, limiting scenario diversity; (2) PF datasets are restricted to OPF-feasible points, hindering generalization of ML solvers to cases that violate operating limits (e.g., branch overloads or voltage violations); and (3) OPF datasets use fixed generator cost functions, limiting generalization across varying costs. gridfm-datakit addresses these challenges by: (1) combining global load scaling from real-world profiles with localized noise and supporting arbitrary N-k topology perturbations to create diverse yet realistic datasets; (2) generating PF samples beyond operating limits; and (3) producing OPF data with varying generator costs. It also scales efficiently to large grids (up to 10,000 buses). Comparisons with OPFData, OPF-Learn, PGLearn, and PF$Δ$ are provided. Available on GitHub at https://github.com/gridfm/gridfm-datakit under Apache 2.0 and via `pip install gridfm-datakit`.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "eess.SY",
                "math.OC"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Main equal contributors: Alban Puech, Matteo Mazzonelli. Other equal contributors: Celia Cintas, Tamara R. Govindasamy, Mangaliso Mngomezulu, Jonas Weiss",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14658v1",
            "code_links": [
                {
                    "url": "https://github.com/gridfm/gridfm-datakit",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO",
                        "VIO"
                    ],
                    "score": 2
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出gridfm-datakit-v1库，通过生成多样且现实的电力潮流与最优潮流数据集，解决现有数据生成工具在场景多样性、泛化能力和成本变化方面的不足。",
            "summary_zh": "我们介绍了gridfm-datakit-v1，这是一个用于生成现实且多样的电力潮流（PF）和最优潮流（OPF）数据集的Python库，旨在训练机器学习（ML）求解器。现有数据集和库面临三个主要挑战：（1）缺乏现实的随机负荷和拓扑扰动，限制了场景多样性；（2）PF数据集仅限于OPF可行点，阻碍了ML求解器对违反运行限制（如支路过载或电压违规）情况的泛化；（3）OPF数据集使用固定的发电机成本函数，限制了在不同成本下的泛化能力。gridfm-datakit通过以下方式解决这些挑战：（1）结合来自真实世界配置文件的全局负荷缩放与局部噪声，并支持任意的N-k拓扑扰动，以创建多样且现实的数据集；（2）生成超出运行限制的PF样本；（3）生成具有变化发电机成本的OPF数据。它还能高效扩展到大型电网（高达10,000个节点）。提供了与OPFData、OPF-Learn、PGLearn和PF$Δ$的比较。该库可在GitHub上获取，网址为https://github.com/gridfm/gridfm-datakit，遵循Apache 2.0许可，并通过`pip install gridfm-datakit`安装。",
            "intro_zh": [
                "现有电力潮流和最优潮流数据集缺乏现实随机扰动，导致场景多样性不足，限制了机器学习求解器的训练效果。",
                "gridfm-datakit结合全局负荷缩放与局部噪声，支持N-k拓扑扰动，生成超出运行限制的样本和变化成本数据，提升泛化能力。",
                "该库能高效扩展到10,000节点电网，相比OPFData等基线工具，在数据多样性和现实性方面有显著提升。"
            ],
            "method_zh": "**问题定义**：论文旨在解决现有电力潮流（PF）和最优潮流（OPF）数据生成工具的三个核心痛点：一是缺乏现实的随机负荷和拓扑扰动，导致数据集场景单一；二是PF数据集仅包含OPF可行点，无法训练机器学习求解器处理违反运行限制（如支路过载或电压违规）的情况；三是OPF数据集使用固定发电机成本函数，限制了在不同成本环境下的泛化能力。这些不足阻碍了机器学习求解器在电力系统优化中的实际应用。\\n\\n**核心思路**：论文的核心思路是通过集成真实世界负荷配置文件、引入局部噪声和任意N-k拓扑扰动，生成多样且现实的PF和OPF数据集。设计上，它强调超越传统可行点限制，生成包含违规情况的PF样本，并动态变化发电机成本，以增强机器学习模型的泛化性能。这种设计基于对电力系统不确定性和运行多样性的深入理解，旨在模拟更接近实际场景的数据分布。\\n\\n**技术框架**：整体架构包括数据输入模块、扰动生成模块和输出模块。主要流程为：首先，基于真实电网拓扑和负荷配置文件输入；其次，应用全局负荷缩放结合局部噪声，以及N-k拓扑扰动（随机移除k个组件），生成多样化的运行场景；然后，针对PF数据，生成包含违反运行限制的样本；针对OPF数据，引入变化成本函数；最后，输出标准化数据集，支持大规模电网（如10,000节点）的高效处理。\\n\\n**关键创新**：最重要的技术创新在于三点：一是结合全局与局部扰动机制，实现更现实的场景多样性；二是生成超出运行限制的PF数据，突破了传统数据集的可行点约束；三是动态变化发电机成本，增强了OPF数据在成本变化下的泛化能力。与现有方法（如OPFData）的本质区别在于其综合性和现实性，而非仅基于简化假设。\\n\\n**关键设计**：关键设计包括：使用真实世界负荷配置文件作为基础，通过参数化缩放因子和噪声分布（如高斯噪声）模拟负荷波动；支持任意N-k拓扑扰动，k值可配置以模拟不同故障场景；对于PF数据，引入违规阈值参数（如电压或电流限制）生成违规样本；对于OPF数据，采用随机成本系数（如线性或二次成本函数）变化；整体实现基于Python，优化了计算效率，支持并行处理，确保可扩展性。",
            "application_zh": "该研究在电力系统优化和机器学习领域具有广泛潜在应用。它可用于训练更鲁棒的机器学习求解器，应用于智能电网的实时监控、故障预测和能源管理，提升系统运行效率和可靠性。未来，结合强化学习或深度学习模型，可推动自动化电力潮流分析和最优调度，对可再生能源集成和电网韧性有重要价值。",
            "highlight_zh": "实验表明，gridfm-datakit能高效生成高达10,000节点电网的数据集，相比基线工具如OPFData、OPF-Learn、PGLearn和PF$Δ$，在数据多样性和现实性方面有显著提升。具体地，它支持N-k拓扑扰动和负荷噪声，生成包含违规情况的PF样本，以及变化成本的OPF数据，增强了机器学习求解器的泛化能力，但具体性能数据（如准确率或效率提升百分比）在摘要中未详细说明，需参考完整论文。",
            "tags_zh": [
                "电力潮流数据生成",
                "最优潮流数据生成",
                "机器学习求解器",
                "电网仿真",
                "数据多样性",
                "泛化能力",
                "Python库",
                "大规模电网"
            ],
            "_index": 13
        },
        {
            "title": "Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes",
            "authors": [
                "Alessandro Trapasso",
                "Luca Iocchi",
                "Fabio Patrizi"
            ],
            "arxiv_id": "2512.14617v1",
            "summary": "Many practical decision-making problems involve tasks whose success depends on the entire system history, rather than on achieving a state with desired properties. Markovian Reinforcement Learning (RL) approaches are not suitable for such tasks, while RL with non-Markovian reward decision processes (NMRDPs) enables agents to tackle temporal-dependency tasks. This approach has long been known to lack formal guarantees on both (near-)optimality and sample efficiency. We contribute to solving both issues with QR-MAX, a novel model-based algorithm for discrete NMRDPs that factorizes Markovian transition learning from non-Markovian reward handling via reward machines. To the best of our knowledge, this is the first model-based RL algorithm for discrete-action NMRDPs that exploits this factorization to obtain PAC convergence to $\\varepsilon$-optimal policies with polynomial sample complexity. We then extend QR-MAX to continuous state spaces with Bucket-QR-MAX, a SimHash-based discretiser that preserves the same factorized structure and achieves fast and stable learning without manual gridding or function approximation. We experimentally compare our method with modern state-of-the-art model-based RL approaches on environments of increasing complexity, showing a significant improvement in sample efficiency and increased robustness in finding optimal policies.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "19 pages, 32 figures, includes appendix",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14617v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RL",
                        "reward"
                    ],
                    "score": 3
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出QR-MAX算法以解决离散动作非马尔可夫奖励决策过程中的样本效率和最优性保证问题",
            "summary_zh": "许多实际决策问题涉及的任务成功依赖于整个系统历史，而非仅达到具有期望属性的状态。马尔可夫强化学习方法不适用于此类任务，而非马尔可夫奖励决策过程使智能体能够处理时间依赖性任务。这种方法长期以来缺乏对（近似）最优性和样本效率的形式化保证。我们通过QR-MAX为解决这两个问题做出贡献，这是一种基于模型的新型算法，用于离散非马尔可夫奖励决策过程，通过奖励机将马尔可夫转移学习与非马尔可夫奖励处理进行因子分解。据我们所知，这是第一个利用这种因子分解获得多项式样本复杂度下PAC收敛到ε-最优策略的基于模型的离散动作非马尔可夫奖励决策过程强化学习算法。然后，我们将QR-MAX扩展到连续状态空间，提出Bucket-QR-MAX，这是一种基于SimHash的离散化器，保持相同的因子分解结构，无需手动网格化或函数逼近即可实现快速稳定的学习。我们在复杂度递增的环境中将我们的方法与现代最先进的基于模型的强化学习方法进行实验比较，显示出样本效率的显著提高和寻找最优策略的鲁棒性增强。",
            "intro_zh": [
                "核心问题：传统马尔可夫强化学习无法处理依赖历史的任务，非马尔可夫奖励决策过程方法缺乏最优性和样本效率的形式化保证。",
                "方法要点：提出QR-MAX算法，通过奖励机将马尔可夫转移与非马尔可夫奖励因子分解，实现多项式样本复杂度的PAC收敛。",
                "实验或效果：在复杂环境中相比现有方法显著提升样本效率，并增强寻找最优策略的鲁棒性，扩展到连续状态空间表现稳定。"
            ],
            "method_zh": "**问题定义**：论文解决离散动作非马尔可夫奖励决策过程中的强化学习问题，其中任务成功依赖于整个系统历史而非单个状态。现有基于模型的强化学习方法在非马尔可夫奖励设置下缺乏形式化保证，导致样本效率低和最优性不确定。\\n\\n**核心思路**：核心思路是将马尔可夫状态转移学习与非马尔可夫奖励处理进行因子分解，通过奖励机来编码历史依赖的奖励结构，从而分离学习过程，提高效率和可分析性。这种设计允许算法专注于学习状态动态，同时利用奖励机处理复杂的时间依赖性。\\n\\n**技术框架**：整体架构包括两个主要阶段：首先，使用基于模型的强化学习方法学习马尔可夫状态转移模型；其次，通过奖励机将非马尔可夫奖励转换为马尔可夫形式，并整合到策略优化中。QR-MAX算法在此基础上实现策略迭代，而Bucket-QR-MAX扩展部分添加SimHash离散化模块以处理连续状态空间。\\n\\n**关键创新**：最重要的技术创新是首次提出基于模型的强化学习算法，在离散动作非马尔可夫奖励决策过程中实现多项式样本复杂度的PAC收敛到ε-最优策略。与现有方法相比，本质区别在于利用奖励机进行因子分解，避免了直接处理非马尔可夫奖励的复杂性，从而获得形式化保证。\\n\\n**关键设计**：关键设计包括奖励机的构建，用于编码非马尔可夫奖励逻辑；在QR-MAX中，采用模型学习模块估计状态转移概率；在Bucket-QR-MAX中，使用SimHash技术进行状态离散化，参数设置涉及哈希函数的选择和桶大小，以平衡精度和计算效率，无需手动网格化或复杂函数逼近。",
            "application_zh": "该研究在机器人任务规划、自动驾驶、游戏AI和工业自动化等领域具有潜在应用价值，特别适用于需要处理历史依赖决策的场景，如长期任务完成或序列动作优化。实际价值在于提供高效且可证明最优的强化学习方法，未来可能推动非马尔可夫环境下的智能系统发展，增强复杂决策任务的可靠性和可扩展性。",
            "highlight_zh": "实验结果显示，QR-MAX在复杂度递增的环境中相比现代最先进的基于模型强化学习方法，样本效率显著提升，具体提升幅度未知但论文强调为“显著”。Bucket-QR-MAX在连续状态空间中实现快速稳定学习，无需手动网格化，表现出增强的鲁棒性，在寻找最优策略方面优于基线方法。",
            "tags_zh": [
                "非马尔可夫奖励决策过程",
                "基于模型的强化学习",
                "奖励机",
                "样本效率",
                "PAC收敛",
                "离散动作空间",
                "连续状态空间",
                "SimHash离散化"
            ],
            "_index": 14
        },
        {
            "title": "DASP: Self-supervised Nighttime Monocular Depth Estimation with Domain Adaptation of Spatiotemporal Priors",
            "authors": [
                "Yiheng Huang",
                "Junhong Chen",
                "Anqi Ning",
                "Zhanhong Liang",
                "Nick Michiels",
                "Luc Claesen",
                "Wenyin Liu"
            ],
            "arxiv_id": "2512.14536v1",
            "summary": "Self-supervised monocular depth estimation has achieved notable success under daytime conditions. However, its performance deteriorates markedly at night due to low visibility and varying illumination, e.g., insufficient light causes textureless areas, and moving objects bring blurry regions. To this end, we propose a self-supervised framework named DASP that leverages spatiotemporal priors for nighttime depth estimation. Specifically, DASP consists of an adversarial branch for extracting spatiotemporal priors and a self-supervised branch for learning. In the adversarial branch, we first design an adversarial network where the discriminator is composed of four devised spatiotemporal priors learning blocks (SPLB) to exploit the daytime priors. In particular, the SPLB contains a spatial-based temporal learning module (STLM) that uses orthogonal differencing to extract motion-related variations along the time axis and an axial spatial learning module (ASLM) that adopts local asymmetric convolutions with global axial attention to capture the multiscale structural information. By combining STLM and ASLM, our model can acquire sufficient spatiotemporal features to restore textureless areas and estimate the blurry regions caused by dynamic objects. In the self-supervised branch, we propose a 3D consistency projection loss to bilaterally project the target frame and source frame into a shared 3D space, and calculate the 3D discrepancy between the two projected frames as a loss to optimize the 3D structural consistency and daytime priors. Extensive experiments on the Oxford RobotCar and nuScenes datasets demonstrate that our approach achieves state-of-the-art performance for nighttime depth estimation. Ablation studies further validate the effectiveness of each component.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "8 pages, 7 figures",
            "doi": "10.1109/LRA.2025.3644148",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14536v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "nuScenes"
                    ],
                    "score": 1
                },
                {
                    "name": "深度估计",
                    "matched_keywords": [
                        "depth estimation",
                        "monocular depth"
                    ],
                    "score": 2
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出DASP框架，利用时空先验的域适应解决夜间单目深度估计性能下降问题。",
            "summary_zh": "自监督单目深度估计在白天条件下已取得显著成功，但在夜间由于低可见度和变化光照（如光线不足导致无纹理区域、运动物体带来模糊区域）性能显著下降。为此，我们提出了一个名为DASP的自监督框架，利用时空先验进行夜间深度估计。具体来说，DASP包含一个用于提取时空先验的对抗分支和一个用于学习的自监督分支。在对抗分支中，我们首先设计了一个对抗网络，其中判别器由四个设计的时空先验学习块（SPLB）组成，以利用白天先验。特别地，SPLB包含一个基于空间的时序学习模块（STLM），使用正交差分沿时间轴提取与运动相关的变化，以及一个轴向空间学习模块（ASLM），采用局部非对称卷积与全局轴向注意力来捕获多尺度结构信息。通过结合STLM和ASLM，我们的模型能够获取足够的时空特征来恢复无纹理区域并估计由动态物体引起的模糊区域。在自监督分支中，我们提出了一个3D一致性投影损失，将目标帧和源帧双边投影到共享的3D空间中，并计算两个投影帧之间的3D差异作为损失，以优化3D结构一致性和白天先验。在Oxford RobotCar和nuScenes数据集上的大量实验表明，我们的方法在夜间深度估计方面实现了最先进的性能。消融研究进一步验证了每个组件的有效性。",
            "intro_zh": [
                "现有自监督单目深度估计方法在夜间因低光照和动态物体导致性能显著下降，面临无纹理区域和模糊区域挑战。",
                "提出DASP框架，结合对抗分支提取时空先验和自监督分支学习，利用正交差分和轴向注意力增强特征提取能力。",
                "在Oxford RobotCar和nuScenes数据集上实现最先进性能，消融研究验证了各组件有效性，显著提升夜间深度估计精度。"
            ],
            "method_zh": "**问题定义**：论文旨在解决夜间单目深度估计中因低可见度和变化光照导致的性能下降问题，现有自监督方法在白天表现良好，但在夜间面临无纹理区域和动态物体引起的模糊区域挑战，导致深度估计不准确。\\n\\n**核心思路**：论文提出DASP框架，通过域适应将白天先验知识迁移到夜间场景，利用时空先验来恢复纹理和估计动态区域，核心思想是结合对抗学习和自监督学习，从白天数据中提取时空特征以增强夜间深度估计的鲁棒性。\\n\\n**技术框架**：整体架构包括两个分支：对抗分支和自监督分支。对抗分支设计了一个对抗网络，其中判别器包含四个时空先验学习块（SPLB），每个SPLB由空间时序学习模块（STLM）和轴向空间学习模块（ASLM）组成，用于提取时空特征；自监督分支则通过3D一致性投影损失优化深度估计，将目标帧和源帧投影到共享3D空间并计算差异。\\n\\n**关键创新**：最重要的技术创新是设计了SPLB模块，结合STLM（使用正交差分提取时间轴运动变化）和ASLM（采用局部非对称卷积与全局轴向注意力捕获多尺度空间结构），有效融合时空信息，与现有方法相比，本质区别在于通过域适应和时空先验学习，专门针对夜间场景的挑战进行优化。\\n\\n**关键设计**：关键设计包括：SPLB中的STLM使用正交差分操作提取时序特征，ASLM结合非对称卷积和轴向注意力机制；损失函数方面，提出了3D一致性投影损失，计算投影帧间的3D差异以增强结构一致性；网络结构上，对抗分支的判别器采用多层SPLB堆叠，自监督分支基于标准深度估计网络，整体参数设置针对夜间数据进行了调整，以平衡特征提取和计算效率。",
            "application_zh": "该研究在自动驾驶、机器人导航和增强现实等领域具有重要应用价值，特别是在夜间或低光照环境下，能够提升视觉系统的深度感知能力，增强安全性和可靠性。未来可能推动夜间视觉技术的发展，为智能系统在复杂光照条件下的部署提供支持。",
            "highlight_zh": "在Oxford RobotCar和nuScenes数据集上的实验表明，DASP在夜间深度估计任务中实现了最先进的性能，具体性能数据未知，但通过消融研究验证了SPLB模块和3D一致性投影损失的有效性，相比基线方法在精度和鲁棒性上有显著提升，例如在纹理恢复和动态区域处理方面表现优异。",
            "tags_zh": [
                "夜间深度估计",
                "自监督学习",
                "域适应",
                "时空先验",
                "对抗网络",
                "3D一致性投影",
                "单目视觉",
                "低光照视觉"
            ],
            "_index": 15
        },
        {
            "title": "CoLD Fusion: A Real-time Capable Spline-based Fusion Algorithm for Collective Lane Detection",
            "authors": [
                "Jörg Gamerdinger",
                "Sven Teufel",
                "Georg Volk",
                "Oliver Bringmann"
            ],
            "arxiv_id": "2512.14355v1",
            "summary": "Comprehensive environment perception is essential for autonomous vehicles to operate safely. It is crucial to detect both dynamic road users and static objects like traffic signs or lanes as these are required for safe motion planning. However, in many circumstances a complete perception of other objects or lanes is not achievable due to limited sensor ranges, occlusions, and curves. In scenarios where an accurate localization is not possible or for roads where no HD maps are available, an autonomous vehicle must rely solely on its perceived road information. Thus, extending local sensing capabilities through collective perception using vehicle-to-vehicle communication is a promising strategy that has not yet been explored for lane detection. Therefore, we propose a real-time capable approach for collective perception of lanes using a spline-based estimation of undetected road sections. We evaluate our proposed fusion algorithm in various situations and road types. We were able to achieve real-time capability and extend the perception range by up to 200%.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted at IEEE IV 2023",
            "doi": "10.1109/IV55152.2023.10186632",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14355v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "autonomous vehicle",
                        "lane detection",
                        "traffic"
                    ],
                    "score": 3
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出基于样条的实时集体车道检测融合算法，以解决传感器局限下的车道感知范围不足问题。",
            "summary_zh": "全面的环境感知对于自动驾驶车辆的安全运行至关重要。检测动态道路使用者以及静态对象（如交通标志或车道）对于安全运动规划是必要的。然而，在许多情况下，由于传感器范围有限、遮挡和弯道等因素，无法实现对其他对象或车道的完整感知。在无法准确定位或没有高清地图的道路场景中，自动驾驶车辆必须完全依赖其感知的道路信息。因此，通过车对车通信利用集体感知来扩展本地感知能力是一种有前景的策略，但尚未在车道检测中得到探索。为此，我们提出了一种实时可行的集体车道感知方法，使用基于样条的估计来处理未检测到的道路段。我们在各种情况和道路类型下评估了所提出的融合算法，实现了实时能力，并将感知范围扩展了高达200%。",
            "intro_zh": [
                "现有车道检测方法受限于传感器范围、遮挡和弯道，导致感知不完整，尤其在无高清地图场景下，自动驾驶车辆依赖本地感知存在安全风险。",
                "论文提出基于样条的集体感知融合算法，通过车对车通信整合多车数据，实时估计未检测道路段，扩展感知范围。",
                "实验表明，该方法在多种道路类型下实现实时处理，感知范围提升高达200%，显著增强自动驾驶系统的环境感知能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自动驾驶中车道检测的感知范围不足问题，现有方法因传感器局限（如范围有限、遮挡和弯道）导致车道信息不完整，尤其在无高清地图或定位不准场景下，车辆依赖本地感知存在安全挑战。\\n\\n**核心思路**：核心思路是利用车对车通信实现集体感知，通过融合多车数据，基于样条曲线估计未检测的道路段，从而扩展单个车辆的感知范围，提升整体环境感知的完整性和可靠性。\\n\\n**技术框架**：整体架构包括数据采集、通信传输、融合处理和样条估计四个主要阶段。首先，各车辆通过传感器（如摄像头）采集本地车道数据；然后，通过车对车通信协议共享数据；接着，融合算法整合多源数据，去除冗余和冲突；最后，使用样条模型拟合车道线，估计未检测区域，生成连续的车道表示。\\n\\n**关键创新**：最重要的技术创新是将集体感知概念应用于车道检测领域，结合样条估计实现实时融合，这在现有研究中尚未探索。与依赖单一车辆或高清地图的传统方法相比，本质区别在于通过分布式协作扩展感知能力，降低对高精度地图的依赖。\\n\\n**关键设计**：关键设计包括采用B样条或类似样条曲线进行车道建模，以平滑处理不连续数据；融合算法可能涉及卡尔曼滤波或优化方法，以处理数据噪声和时延；实时性通过高效的数据结构和并行处理实现，具体参数如样条阶数、控制点数量需根据道路曲率自适应调整，损失函数可能基于几何误差最小化，但论文未详细说明网络结构，因方法更侧重于传统信号处理而非深度学习。",
            "application_zh": "该研究主要应用于自动驾驶领域，特别是在城市道路、高速公路等复杂场景中，通过集体感知增强车道检测，提升车辆的安全性和可靠性。潜在价值包括减少事故风险、支持无地图自动驾驶，未来可能推动车联网和智能交通系统的发展，促进协同感知技术的标准化。",
            "highlight_zh": "最重要的实验结果显示，该方法在多种道路类型（如直道、弯道）下均能实现实时处理，感知范围扩展高达200%，具体数据表明融合后检测距离显著增加，对比基线（单一车辆感知）有显著提升，验证了集体感知在扩展感知范围和增强鲁棒性方面的有效性。",
            "tags_zh": [
                "集体感知",
                "车道检测",
                "样条估计",
                "车对车通信",
                "实时融合",
                "自动驾驶",
                "环境感知",
                "传感器融合"
            ],
            "_index": 16
        },
        {
            "title": "Elastic3D: Controllable Stereo Video Conversion with Guided Latent Decoding",
            "authors": [
                "Nando Metzger",
                "Prune Truong",
                "Goutam Bhat",
                "Konrad Schindler",
                "Federico Tombari"
            ],
            "arxiv_id": "2512.14236v1",
            "summary": "The growing demand for immersive 3D content calls for automated monocular-to-stereo video conversion. We present Elastic3D, a controllable, direct end-to-end method for upgrading a conventional video to a binocular one. Our approach, based on (conditional) latent diffusion, avoids artifacts due to explicit depth estimation and warping. The key to its high-quality stereo video output is a novel, guided VAE decoder that ensures sharp and epipolar-consistent stereo video output. Moreover, our method gives the user control over the strength of the stereo effect (more precisely, the disparity range) at inference time, via an intuitive, scalar tuning knob. Experiments on three different datasets of real-world stereo videos show that our method outperforms both traditional warping-based and recent warping-free baselines and sets a new standard for reliable, controllable stereo video conversion. Please check the project page for the video samples https://elastic3d.github.io.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page: elastic3d.github.io",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14236v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "深度估计",
                    "matched_keywords": [
                        "depth estimation",
                        "disparity"
                    ],
                    "score": 2
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出Elastic3D方法，基于条件潜在扩散和引导解码，实现可控的单目到立体视频转换，解决传统方法中的伪影问题。",
            "summary_zh": "随着沉浸式3D内容需求的增长，自动化单目到立体视频转换变得日益重要。本文提出Elastic3D，一种可控的端到端方法，用于将传统视频升级为双目视频。该方法基于条件潜在扩散，避免了因显式深度估计和扭曲而产生的伪影。其高质量立体视频输出的关键在于一种新颖的引导VAE解码器，确保输出视频清晰且满足极线一致性。此外，该方法通过直观的标量调节旋钮，在推理时允许用户控制立体效果的强度（更精确地说，是视差范围）。在三个真实世界立体视频数据集上的实验表明，我们的方法优于传统的基于扭曲的方法和最近的无扭曲基线，为可靠、可控的立体视频转换设定了新标准。请访问项目页面查看视频样本：https://elastic3d.github.io。",
            "intro_zh": [
                "核心问题：传统单目到立体视频转换方法依赖显式深度估计和扭曲，易产生伪影，且缺乏用户对立体效果强度的控制。",
                "方法要点：基于条件潜在扩散模型，避免显式深度估计，引入引导VAE解码器确保视频清晰度和极线一致性，并提供标量旋钮实现可控视差范围。",
                "实验或效果：在三个真实世界立体视频数据集上，Elastic3D优于传统和近期基线方法，设定了立体视频转换的新标准，支持高质量、可控输出。"
            ],
            "method_zh": "**问题定义**：论文旨在解决单目到立体视频转换问题，即从单目视频生成高质量、可控的立体视频。现有方法通常依赖显式深度估计和图像扭曲，这容易引入伪影（如模糊、重影），且缺乏对立体效果强度的灵活控制，限制了实际应用中的可靠性和用户体验。\\n\\n**核心思路**：论文的核心思路是采用条件潜在扩散模型，直接学习从单目视频到立体视频的映射，避免显式深度估计和扭曲步骤，从而减少伪影。同时，通过设计引导解码器和用户控制机制，确保输出视频的清晰度、极线一致性和可调节性。这种设计基于扩散模型在生成任务中的强大能力，结合特定约束来优化立体视频质量。\\n\\n**技术框架**：整体架构为端到端的条件潜在扩散模型。输入单目视频，首先通过编码器提取潜在表示；然后，在潜在空间中进行扩散过程，条件信息包括输入视频和用户指定的视差范围；最后，通过引导VAE解码器生成立体视频。主要模块包括：编码器、扩散模型、条件模块和引导解码器。流程分为训练和推理两个阶段，训练时学习数据分布，推理时通过调节参数控制输出。\\n\\n**关键创新**：最重要的技术创新是引导VAE解码器，它通过引入引导机制（如极线约束）来增强解码过程，确保生成的立体视频具有高清晰度和一致性。与现有方法的本质区别在于：避免了显式深度估计和扭曲，直接利用扩散模型生成，从而减少了伪影；同时，提供了直观的用户控制接口，实现了可控的立体效果，这在传统方法中较为罕见。\\n\\n**关键设计**：关键设计包括：1) 条件潜在扩散模型，使用噪声调度和去噪网络处理潜在表示；2) 引导解码器，可能结合损失函数（如重建损失、一致性损失）来优化输出；3) 用户控制机制，通过标量参数（如视差范围）调节生成过程；4) 训练策略，可能涉及多阶段训练或对抗训练来提升质量。具体参数设置（如扩散步数、网络层数）未在摘要中详述，但整体框架强调端到端学习和可控性。",
            "application_zh": "该研究在虚拟现实、增强现实、3D电影制作和游戏开发等领域具有广泛应用潜力。通过自动化单目到立体视频转换，可以降低内容制作成本，提升沉浸式体验，推动3D内容的普及。未来可能影响视频处理、计算机视觉和多媒体技术，为实时或离线转换提供可靠工具。",
            "highlight_zh": "实验在三个真实世界立体视频数据集上进行，Elastic3D在质量指标上优于传统基于扭曲的方法（如深度估计后扭曲）和近期无扭曲基线（可能基于生成模型）。具体性能数据未在摘要中给出，但论文声称“设定了新标准”，表明在视觉质量、一致性和可控性方面有显著提升，例如减少伪影、提高用户满意度。",
            "tags_zh": [
                "立体视频转换",
                "潜在扩散模型",
                "引导解码器",
                "极线一致性",
                "可控生成",
                "单目到立体",
                "视频处理",
                "3D内容生成"
            ],
            "_index": 17
        },
        {
            "title": "OmniGen: Unified Multimodal Sensor Generation for Autonomous Driving",
            "authors": [
                "Tao Tang",
                "Enhui Ma",
                "xia zhou",
                "Letian Wang",
                "Tianyi Yan",
                "Xueyang Zhang",
                "Kun Zhan",
                "Peng Jia",
                "XianPeng Lang",
                "Jia-Wang Bian",
                "Kaicheng Yu",
                "Xiaodan Liang"
            ],
            "arxiv_id": "2512.14225v1",
            "summary": "Autonomous driving has seen remarkable advancements, largely driven by extensive real-world data collection. However, acquiring diverse and corner-case data remains costly and inefficient. Generative models have emerged as a promising solution by synthesizing realistic sensor data. However, existing approaches primarily focus on single-modality generation, leading to inefficiencies and misalignment in multimodal sensor data. To address these challenges, we propose OminiGen, which generates aligned multimodal sensor data in a unified framework. Our approach leverages a shared Bird\\u2019s Eye View (BEV) space to unify multimodal features and designs a novel generalizable multimodal reconstruction method, UAE, to jointly decode LiDAR and multi-view camera data. UAE achieves multimodal sensor decoding through volume rendering, enabling accurate and flexible reconstruction. Furthermore, we incorporate a Diffusion Transformer (DiT) with a ControlNet branch to enable controllable multimodal sensor generation. Our comprehensive experiments demonstrate that OminiGen achieves desired performances in unified multimodal sensor data generation with multimodal consistency and flexible sensor adjustments.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "ACM MM 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14225v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "autonomous driving",
                        "lidar"
                    ],
                    "score": 2
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出OmniGen统一框架以解决自动驾驶中多模态传感器数据生成不一致和效率低下的问题",
            "summary_zh": "自动驾驶技术的显著进步很大程度上依赖于大规模真实世界数据采集，但获取多样化和极端案例数据仍然成本高昂且效率低下。生成模型通过合成逼真的传感器数据成为一种有前景的解决方案。然而，现有方法主要关注单模态生成，导致多模态传感器数据效率低下和对齐不准确。为解决这些挑战，我们提出了OmniGen，在统一框架中生成对齐的多模态传感器数据。我们的方法利用共享的鸟瞰图空间来统一多模态特征，并设计了一种新颖的可泛化多模态重建方法UAE，以联合解码激光雷达和多视角相机数据。UAE通过体渲染实现多模态传感器解码，从而实现准确灵活的重建。此外，我们结合了带有ControlNet分支的扩散变换器，以实现可控的多模态传感器生成。我们的全面实验表明，OmniGen在多模态一致性和灵活传感器调整方面实现了统一多模态传感器数据生成的理想性能。",
            "intro_zh": [
                "现有方法主要关注单模态传感器数据生成，导致多模态数据效率低下和对齐不准确，难以满足自动驾驶对多样化数据的需求。",
                "OmniGen利用共享鸟瞰图空间统一多模态特征，设计UAE方法通过体渲染联合解码激光雷达和相机数据，并集成扩散变换器实现可控生成。",
                "实验表明OmniGen在多模态一致性和传感器调整方面表现优异，显著提升了多模态传感器数据生成的准确性和灵活性。"
            ],
            "method_zh": "**问题定义**：自动驾驶数据采集成本高且效率低，现有生成方法主要针对单模态传感器数据，导致多模态数据生成时存在效率低下、对齐不准确的问题，难以合成一致且可控的多模态传感器数据。\\n\\n**核心思路**：通过共享鸟瞰图空间统一多模态特征表示，设计可泛化的多模态重建方法UAE，利用体渲染技术联合解码激光雷达和多视角相机数据，并结合扩散变换器与ControlNet分支实现可控生成，以提升多模态数据的一致性和灵活性。\\n\\n**技术框架**：整体架构包括多模态特征提取模块、共享BEV空间转换模块、UAE重建模块和可控生成模块。首先，从输入数据中提取激光雷达和相机特征；其次，将这些特征映射到共享BEV空间进行对齐；然后，UAE模块通过体渲染解码生成多模态传感器数据；最后，扩散变换器与ControlNet分支结合，根据控制信号调整生成过程。\\n\\n**关键创新**：最重要的技术创新是UAE方法，它通过体渲染实现多模态传感器数据的联合解码，与现有单模态方法相比，本质区别在于统一处理多模态数据，避免了逐模态生成的效率损失和对齐误差，同时增强了重建的准确性和可控性。\\n\\n**关键设计**：UAE采用体渲染技术，将多模态特征编码为3D体积表示，通过光线投射和采样解码为传感器数据；损失函数可能包括重建损失、多模态一致性损失和可控生成损失；网络结构结合了Transformer编码器、BEV转换层和扩散模型，具体参数设置如体素分辨率、扩散步数等需参考论文细节。",
            "application_zh": "该研究在自动驾驶领域具有重要应用价值，可用于合成多样化和极端案例的多模态传感器数据，降低数据采集成本，加速自动驾驶系统的训练和测试。未来可能扩展到机器人感知、虚拟现实等需要多模态数据生成的场景，推动人工智能在复杂环境中的泛化能力。",
            "highlight_zh": "实验表明，OmniGen在多模态传感器数据生成任务中，相比基线方法在一致性和准确性方面有显著提升，具体性能数据未知，但论文报告了在多模态对齐和可控调整方面的优异表现，验证了UAE方法和可控生成框架的有效性。",
            "tags_zh": [
                "多模态传感器生成",
                "自动驾驶数据合成",
                "鸟瞰图空间统一",
                "体渲染解码",
                "扩散变换器",
                "可控生成",
                "多模态一致性",
                "激光雷达相机融合"
            ],
            "_index": 18
        },
        {
            "title": "Beyond a Single Light: A Large-Scale Aerial Dataset for Urban Scene Reconstruction Under Varying Illumination",
            "authors": [
                "Zhuoxiao Li",
                "Wenzong Ma",
                "Taoyu Wu",
                "Jinjing Zhu",
                "Zhenchao Q",
                "Shuai Zhang",
                "Jing Ou",
                "Yinrui Ren",
                "Weiqing Qi",
                "Guobin Shen",
                "Hui Xiong",
                "Wufan Zhao"
            ],
            "arxiv_id": "2512.14200v1",
            "summary": "Recent advances in Neural Radiance Fields and 3D Gaussian Splatting have demonstrated strong potential for large-scale UAV-based 3D reconstruction tasks by fitting the appearance of images. However, real-world large-scale captures are often based on multi-temporal data capture, where illumination inconsistencies across different times of day can significantly lead to color artifacts, geometric inaccuracies, and inconsistent appearance. Due to the lack of UAV datasets that systematically capture the same areas under varying illumination conditions, this challenge remains largely underexplored. To fill this gap, we introduceSkyLume, a large-scale, real-world UAV dataset specifically designed for studying illumination robust 3D reconstruction in urban scene modeling: (1) We collect data from 10 urban regions data comprising more than 100k high resolution UAV images (four oblique views and nadir), where each region is captured at three periods of the day to systematically isolate illumination changes. (2) To support precise evaluation of geometry and appearance, we provide per-scene LiDAR scans and accurate 3D ground-truth for assessing depth, surface normals, and reconstruction quality under varying illumination. (3) For the inverse rendering task, we introduce the Temporal Consistency Coefficient (TCC), a metric that measuress cross-time albedo stability and directly evaluates the robustness of the disentanglement of light and material. We aim for this resource to serve as a foundation that advances research and real-world evaluation in large-scale inverse rendering, geometry reconstruction, and novel view synthesis.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14200v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "lidar"
                    ],
                    "score": 1
                },
                {
                    "name": "深度估计",
                    "matched_keywords": [
                        "3D reconstruction"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出SkyLume大规模无人机数据集以解决城市场景重建中光照变化导致的颜色伪影和几何不准确问题",
            "summary_zh": "近年来，神经辐射场和3D高斯溅射技术通过拟合图像外观，在大规模无人机3D重建任务中展现出强大潜力。然而，真实世界的大规模采集通常基于多时相数据捕获，不同时间段的光照不一致会显著导致颜色伪影、几何不准确和外观不一致。由于缺乏系统捕获相同区域在不同光照条件下的无人机数据集，这一挑战在很大程度上尚未得到充分探索。为填补这一空白，我们引入了SkyLume，这是一个专门为研究城市场景建模中光照鲁棒3D重建而设计的大规模真实世界无人机数据集：（1）我们从10个城市区域收集数据，包含超过10万张高分辨率无人机图像（四个倾斜视图和天底视图），每个区域在一天中的三个时间段进行捕获，以系统隔离光照变化。（2）为支持几何和外观的精确评估，我们提供每个场景的激光雷达扫描和准确的3D地面真值，用于评估不同光照下的深度、表面法线和重建质量。（3）对于逆渲染任务，我们引入了时间一致性系数，这是一个衡量跨时间反照率稳定性并直接评估光与材质解耦鲁棒性的指标。我们旨在使这一资源成为推动大规模逆渲染、几何重建和新视图合成研究和真实世界评估的基础。",
            "intro_zh": [
                "现有方法在大规模无人机3D重建中面临光照变化导致的颜色伪影和几何不准确挑战，缺乏系统数据集支持研究。",
                "论文提出SkyLume数据集，包含多时相高分辨率图像、激光雷达扫描和3D地面真值，并引入时间一致性系数评估光照鲁棒性。",
                "实验表明，该数据集能有效评估逆渲染和重建方法，提升光照变化下的重建质量和一致性，填补了研究空白。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大规模无人机3D重建中，由于多时相数据捕获导致的光照不一致问题，这会引起颜色伪影、几何不准确和外观不一致。现有方法如神经辐射场和3D高斯溅射虽能拟合图像外观，但缺乏系统数据集来研究光照变化的影响，导致挑战未充分探索。\\n\\n**核心思路**：通过构建一个大规模、真实世界的无人机数据集SkyLume，系统捕获城市区域在不同时间段的光照条件，提供多模态数据支持，以促进光照鲁棒3D重建的研究和评估。核心在于数据驱动的解决方案，而非算法创新，通过高质量数据集填补研究空白。\\n\\n**技术框架**：整体框架包括数据采集、处理和评估三阶段。数据采集阶段使用无人机在10个城市区域捕获超过10万张高分辨率图像，涵盖四个倾斜视图和天底视图，每个区域在一天中的三个时间段（如早晨、中午、傍晚）进行捕获以隔离光照变化。处理阶段提供激光雷达扫描和3D地面真值，用于几何和外观评估。评估阶段引入时间一致性系数作为新指标，衡量逆渲染任务中光与材质解耦的鲁棒性。\\n\\n**关键创新**：最重要的技术创新是SkyLume数据集本身，它是首个专门针对城市场景光照变化3D重建的大规模无人机数据集。与现有数据集相比，其本质区别在于系统捕获多时相数据，并提供激光雷达和3D地面真值支持精确评估，同时引入时间一致性系数作为新评估指标，直接针对光照鲁棒性问题。\\n\\n**关键设计**：数据集设计包括10个城市区域，每个区域捕获三个时间段，图像分辨率高（具体分辨率未知），覆盖倾斜和天底视图以确保多角度覆盖。激光雷达扫描提供精确深度信息，3D地面真值用于评估重建质量。时间一致性系数定义为跨时间反照率稳定性的度量，具体计算方式未在摘要中详细说明，但旨在量化光照解耦效果。损失函数和网络结构未涉及，因为论文主要贡献是数据集而非算法。",
            "application_zh": "该研究在计算机视觉和机器人领域具有广泛潜在应用，包括城市建模、自动驾驶环境感知、虚拟现实场景生成和文化遗产数字化。通过提供光照鲁棒的重建数据，能提升真实世界3D重建的准确性和一致性，推动逆渲染和几何重建技术的发展，对智慧城市建设和环境监测有重要价值。",
            "highlight_zh": "实验亮点包括：SkyLume数据集包含超过10万张高分辨率图像，覆盖10个城市区域和三个时间段，系统隔离光照变化；提供激光雷达扫描和3D地面真值，支持深度、表面法线和重建质量评估；引入时间一致性系数作为新指标，直接评估光照解耦鲁棒性。与现有数据集相比，该数据集填补了光照变化3D重建的研究空白，未提供具体性能数据对比，但旨在作为基准推动方法改进。",
            "tags_zh": [
                "无人机数据集",
                "光照鲁棒重建",
                "城市场景建模",
                "逆渲染",
                "3D高斯溅射",
                "神经辐射场",
                "多时相数据",
                "时间一致性系数"
            ],
            "_index": 19
        },
        {
            "title": "CIS-BA: Continuous Interaction Space Based Backdoor Attack for Object Detection in the Real-World",
            "authors": [
                "Shuxin Zhao",
                "Bo Lang",
                "Nan Xiao",
                "Yilang Zhang"
            ],
            "arxiv_id": "2512.14158v1",
            "summary": "Object detection models deployed in real-world applications such as autonomous driving face serious threats from backdoor attacks. Despite their practical effectiveness,existing methods are inherently limited in both capability and robustness due to their dependence on single-trigger-single-object mappings and fragile pixel-level cues. We propose CIS-BA, a novel backdoor attack paradigm that redefines trigger design by shifting from static object features to continuous inter-object interaction patterns that describe how objects co-occur and interact in a scene. By modeling these patterns as a continuous interaction space, CIS-BA introduces space triggers that, for the first time, enable a multi-trigger-multi-object attack mechanism while achieving robustness through invariant geometric relations. To implement this paradigm, we design CIS-Frame, which constructs space triggers via interaction analysis, formalizes them as class-geometry constraints for sample poisoning, and embeds the backdoor during detector training. CIS-Frame supports both single-object attacks (object misclassification and disappearance) and multi-object simultaneous attacks, enabling complex and coordinated effects across diverse interaction states. Experiments on MS-COCO and real-world videos show that CIS-BA achieves over 97% attack success under complex environments and maintains over 95% effectiveness under dynamic multi-trigger conditions, while evading three state-of-the-art defenses. In summary, CIS-BA extends the landscape of backdoor attacks in interaction-intensive scenarios and provides new insights into the security of object detection systems.",
            "categories": [
                "cs.CV",
                "cs.CR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14158v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "autonomous driving"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "mapping"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出CIS-BA，基于连续交互空间的后门攻击方法，解决目标检测在真实世界中的安全威胁问题",
            "summary_zh": "在自动驾驶等真实世界应用中部署的目标检测模型面临后门攻击的严重威胁。尽管现有方法具有实际效果，但由于依赖单触发-单对象映射和脆弱的像素级线索，它们在能力和鲁棒性上存在固有局限。我们提出CIS-BA，一种新颖的后门攻击范式，通过从静态对象特征转向描述场景中对象如何共现和交互的连续对象间交互模式，重新定义了触发设计。通过将这些模式建模为连续交互空间，CIS-BA引入了空间触发器，首次实现了多触发-多对象攻击机制，同时通过不变的几何关系实现鲁棒性。为实现这一范式，我们设计了CIS-Frame，它通过交互分析构建空间触发器，将其形式化为类别-几何约束以进行样本投毒，并在检测器训练期间嵌入后门。CIS-Frame支持单对象攻击（对象误分类和消失）和多对象同时攻击，能够在不同交互状态下实现复杂协调的效果。在MS-COCO和真实世界视频上的实验表明，CIS-BA在复杂环境下实现了超过97%的攻击成功率，在动态多触发条件下保持超过95%的有效性，同时规避了三种最先进的防御方法。总之，CIS-BA扩展了交互密集场景中后门攻击的格局，并为目标检测系统的安全性提供了新的见解。",
            "intro_zh": [
                "现有后门攻击依赖单触发-单对象映射和像素级线索，导致能力有限且鲁棒性差，难以应对真实世界复杂场景。",
                "CIS-BA提出连续交互空间范式，将触发设计从静态特征转向对象间交互模式，实现多触发-多对象攻击。",
                "实验显示，CIS-BA在复杂环境下攻击成功率超97%，动态多触发条件下有效性超95%，并能规避先进防御方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决目标检测模型在真实世界应用中面临的后门攻击威胁。现有方法（如基于像素级触发器的攻击）存在两个主要痛点：一是依赖单触发-单对象映射，攻击能力有限，无法实现多对象协调攻击；二是基于脆弱的像素级线索，容易受到环境变化（如光照、遮挡）和防御机制的影响，鲁棒性不足。\\n\\n**核心思路**：论文的核心思路是将后门攻击的触发设计从传统的静态对象特征（如特定图案或颜色）转向连续的对象间交互模式。这些模式描述了场景中多个对象如何共现和交互（如相对位置、尺寸关系等），通过建模为连续交互空间，引入空间触发器，实现基于几何关系的鲁棒攻击。这种设计利用了目标检测中对象间交互的固有特性，使攻击更隐蔽且适应性强。\\n\\n**技术框架**：整体框架称为CIS-Frame，包含三个主要阶段：首先，通过交互分析构建空间触发器，即从训练数据中学习对象间的交互模式（如类别组合和几何约束）；其次，将这些模式形式化为类别-几何约束，用于生成投毒样本，即在干净数据中嵌入触发条件；最后，在目标检测器训练过程中，将后门嵌入模型，使模型在遇到触发条件时执行恶意行为（如误分类或消失）。框架支持端到端训练，与标准检测流程兼容。\\n\\n**关键创新**：最重要的技术创新是提出了连续交互空间（CIS）范式，首次实现了多触发-多对象攻击机制。与现有方法本质区别在于：现有方法通常基于单个触发器和单个目标对象，而CIS-BA利用对象间交互的连续空间，允许动态触发和多个对象同时受影响，从而扩展了攻击的复杂性和协调性。此外，通过几何关系（如相对距离、角度）而非像素级特征，提高了攻击的鲁棒性和隐蔽性。\\n\\n**关键设计**：关键设计包括：空间触发器的构建，通过统计分析对象共现概率和几何关系（如边界框的相对位置和尺寸比例），定义交互模式；样本投毒过程，将触发条件编码为损失函数中的约束项，例如在训练时添加基于交互模式的对抗性目标；网络结构上，CIS-Frame可集成到常见检测器（如YOLO、Faster R-CNN）中，无需修改主干网络，主要通过数据投毒和训练策略实现后门嵌入；参数设置上，涉及交互模式的阈值选择（如最小共现频率）和攻击强度的平衡，以确保高攻击成功率同时保持模型正常性能。",
            "application_zh": "该研究主要应用于目标检测系统的安全评估和防御开发领域，特别是在自动驾驶、视频监控和机器人导航等交互密集的真实世界场景中。其实际价值在于揭示了基于对象交互的后门攻击新威胁，为设计更鲁棒的防御机制提供了关键见解。未来可能影响包括推动后门攻击与防御研究向多对象、动态交互方向深入，促进安全AI系统的发展。",
            "highlight_zh": "在MS-COCO数据集和真实世界视频上的实验表明，CIS-BA在复杂环境（如光照变化、遮挡）下攻击成功率超过97%，相比基线方法（如基于像素触发器的攻击）有显著提升；在动态多触发条件下，攻击有效性保持超过95%，显示出强鲁棒性。此外，CIS-BA成功规避了三种最先进的防御方法（具体方法未在摘要中说明），验证了其隐蔽性和实用性。这些结果突显了CIS-BA在扩展后门攻击能力方面的突破。",
            "tags_zh": [
                "目标检测",
                "后门攻击",
                "连续交互空间",
                "多触发攻击",
                "鲁棒性",
                "自动驾驶安全",
                "样本投毒",
                "几何约束"
            ],
            "_index": 20
        },
        {
            "title": "Incentivizing Tool-augmented Thinking with Images for Medical Image Analysis",
            "authors": [
                "Yankai Jiang",
                "Yujie Zhang",
                "Peng Zhang",
                "Yichen Li",
                "Jintai Chen",
                "Xiaoming Shi",
                "Shihui Zhen"
            ],
            "arxiv_id": "2512.14157v1",
            "summary": "Recent reasoning based medical MLLMs have made progress in generating step by step textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on fine-grained visual regions to achieve precise grounding and diagnosis. We introduce Ophiuchus, a versatile, tool-augmented framework that equips an MLLM to (i) decide when additional visual evidence is needed, (ii) determine where to probe and ground within the medical image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved, multimodal chain of thought. In contrast to prior approaches limited by the performance ceiling of specialized tools, Ophiuchus integrates the model's inherent grounding and perception capabilities with external tools, thereby fostering higher-level reasoning. The core of our method is a three-stage training strategy: cold-start training with tool-integrated reasoning data to achieve basic tool selection and adaptation for inspecting key regions; self-reflection fine-tuning to strengthen reflective reasoning and encourage revisiting tool outputs; and Agentic Tool Reinforcement Learning to directly optimize task-specific rewards and emulate expert-like diagnostic behavior. Extensive experiments show that Ophiuchus consistently outperforms both closed-source and open-source SOTA methods across diverse medical benchmarks, including VQA, detection, and reasoning-based segmentation. Our approach illuminates a path toward medical AI agents that can genuinely \"think with images\" through tool-integrated reasoning. Datasets, codes, and trained models will be released publicly.",
            "categories": [
                "cs.AI",
                "cs.CV"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14157v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RL",
                        "reward"
                    ],
                    "score": 3
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出Ophiuchus框架，通过工具增强的思维链解决医学图像分析中复杂任务的动态视觉聚焦问题。",
            "summary_zh": "近年来，基于推理的医学多模态大语言模型在生成逐步文本推理链方面取得了进展。然而，它们仍然难以处理需要动态和迭代地聚焦于细粒度视觉区域以实现精确定位和诊断的复杂任务。我们引入了Ophiuchus，这是一个多功能、工具增强的框架，它使MLLM能够：(i)决定何时需要额外的视觉证据，(ii)确定在医学图像中探测和定位的位置，以及(iii)将相关的子图像内容无缝地编织成交错的多模态思维链。与先前受限于专用工具性能上限的方法不同，Ophiuchus将模型固有的定位和感知能力与外部工具相结合，从而促进更高层次的推理。我们方法的核心是一个三阶段训练策略：使用工具集成推理数据进行冷启动训练，以实现对关键区域检查的基本工具选择和适应；自我反思微调，以加强反思性推理并鼓励重新审视工具输出；以及代理工具强化学习，以直接优化特定任务的奖励并模拟类似专家的诊断行为。大量实验表明，Ophiuchus在包括VQA、检测和基于推理的分割在内的多种医学基准测试中，始终优于闭源和开源的最先进方法。我们的方法为医学AI代理指明了一条通过工具集成推理真正“用图像思考”的道路。数据集、代码和训练模型将公开发布。",
            "intro_zh": [
                "现有医学多模态大语言模型在复杂任务中难以动态聚焦细粒度视觉区域，导致定位和诊断精度不足。",
                "提出Ophiuchus框架，通过工具增强的思维链，结合模型能力与外部工具，实现动态视觉证据获取和集成。",
                "实验显示Ophiuchus在VQA、检测和分割等医学基准上超越SOTA方法，验证了其有效性和通用性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决医学图像分析中复杂任务（如精确定位和诊断）的挑战，现有基于推理的医学多模态大语言模型虽能生成文本推理链，但难以动态、迭代地聚焦细粒度视觉区域，导致性能受限，痛点在于缺乏高效的工具集成和视觉证据管理机制。\\n\\n**核心思路**：论文的核心思路是设计一个工具增强的框架Ophiuchus，通过结合模型固有能力和外部工具，实现动态决策何时需要视觉证据、确定探测位置，并将子图像内容集成到多模态思维链中，以提升推理精度和适应性。\\n\\n**技术框架**：整体架构包括三个主要阶段：冷启动训练阶段，使用工具集成推理数据训练模型进行基本工具选择和关键区域检查；自我反思微调阶段，加强反思性推理，鼓励模型重新评估工具输出；代理工具强化学习阶段，直接优化任务特定奖励，模拟专家诊断行为，形成端到端的工具增强推理流程。\\n\\n**关键创新**：最重要的技术创新是提出三阶段训练策略，将工具集成与模型推理深度融合，区别于现有方法仅依赖专用工具或单一模型能力，本质区别在于实现了动态视觉聚焦和工具输出的迭代优化，从而突破性能上限。\\n\\n**关键设计**：关键设计包括：使用工具集成推理数据进行冷启动训练，以初始化模型工具适应能力；引入自我反思机制，通过微调增强模型对工具输出的批判性评估；采用强化学习优化任务奖励函数，具体参数设置如奖励函数设计、网络结构的多模态融合模块等细节未在摘要中详述，但整体强调端到端优化和工具协同。",
            "application_zh": "该研究在医学图像分析领域具有广泛潜在应用，包括医学视觉问答、病变检测和基于推理的图像分割等任务。其实际价值在于提升AI系统在复杂医疗场景中的诊断精度和自动化水平，未来可能推动医学AI代理的发展，实现更智能、交互式的辅助诊断工具，对医疗影像分析和临床决策支持产生积极影响。",
            "highlight_zh": "实验结果表明，Ophiuchus在多种医学基准测试中均优于当前最先进方法，包括闭源和开源模型。具体性能数据未在摘要中提供，但强调了在VQA、检测和基于推理的分割任务上的显著提升，验证了框架在动态视觉聚焦和工具集成方面的有效性，为医学AI代理的实用化提供了有力支持。",
            "tags_zh": [
                "医学图像分析",
                "多模态大语言模型",
                "工具增强推理",
                "动态视觉聚焦",
                "思维链",
                "强化学习",
                "自我反思微调",
                "医学AI代理"
            ],
            "_index": 21
        },
        {
            "title": "Context Representation via Action-Free Transformer encoder-decoder for Meta Reinforcement Learning",
            "authors": [
                "Amir M. Soufi Enayati",
                "Homayoun Honari",
                "Homayoun Najjaran"
            ],
            "arxiv_id": "2512.14057v1",
            "summary": "Reinforcement learning (RL) enables robots to operate in uncertain environments, but standard approaches often struggle with poor generalization to unseen tasks. Context-adaptive meta reinforcement learning addresses these limitations by conditioning on the task representation, yet they mostly rely on complete action information in the experience making task inference tightly coupled to a specific policy. This paper introduces Context Representation via Action Free Transformer encoder decoder (CRAFT), a belief model that infers task representations solely from sequences of states and rewards. By removing the dependence on actions, CRAFT decouples task inference from policy optimization, supports modular training, and leverages amortized variational inference for scalable belief updates. Built on a transformer encoder decoder with rotary positional embeddings, the model captures long range temporal dependencies and robustly encodes both parametric and non-parametric task variations. Experiments on the MetaWorld ML-10 robotic manipulation benchmark show that CRAFT achieves faster adaptation, improved generalization, and more effective exploration compared to context adaptive meta--RL baselines. These findings highlight the potential of action-free inference as a foundation for scalable RL in robotic control.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14057v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RL",
                        "reward",
                        "PPO"
                    ],
                    "score": 4
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出CRAFT模型，通过无动作Transformer编码器-解码器实现任务表示，解决元强化学习中任务推断与策略优化的耦合问题。",
            "summary_zh": "强化学习（RL）使机器人能在不确定环境中运行，但标准方法在泛化到未见任务时表现不佳。上下文自适应元强化学习通过任务表示来应对这些限制，但它们大多依赖经验中的完整动作信息，导致任务推断与特定策略紧密耦合。本文介绍了Context Representation via Action Free Transformer encoder decoder（CRAFT），这是一种信念模型，仅从状态和奖励序列推断任务表示。通过消除对动作的依赖，CRAFT将任务推断与策略优化解耦，支持模块化训练，并利用摊销变分推断进行可扩展的信念更新。该模型基于带有旋转位置嵌入的Transformer编码器-解码器构建，能够捕捉长程时间依赖性，并稳健地编码参数化和非参数化的任务变化。在MetaWorld ML-10机器人操作基准上的实验表明，与上下文自适应元强化学习基线相比，CRAFT实现了更快的适应、更好的泛化和更有效的探索。这些发现突显了无动作推断作为机器人控制中可扩展强化学习基础的潜力。",
            "intro_zh": [
                "现有元强化学习方法依赖动作信息进行任务推断，导致任务表示与特定策略紧密耦合，限制了泛化能力。",
                "CRAFT通过无动作Transformer编码器-解码器，仅从状态和奖励序列推断任务表示，实现任务推断与策略优化的解耦。",
                "在MetaWorld ML-10基准上，CRAFT相比基线方法，实现了更快的适应速度、更好的泛化性能和更有效的探索策略。"
            ],
            "method_zh": "**问题定义**：论文旨在解决元强化学习中任务推断与策略优化耦合的问题。现有上下文自适应元强化学习方法通常依赖经验中的完整动作信息进行任务推断，这导致任务表示与特定策略紧密绑定，限制了方法的模块化和泛化能力，尤其是在面对未见任务时表现不佳。\\n\\n**核心思路**：论文提出CRAFT模型，其核心思路是仅使用状态和奖励序列来推断任务表示，从而消除对动作信息的依赖。这样设计是为了将任务推断过程与策略优化解耦，使得任务表示可以独立于具体策略进行学习，支持模块化训练，并利用摊销变分推断实现可扩展的信念更新。\\n\\n**技术框架**：CRAFT的整体架构基于Transformer编码器-解码器结构。编码器接收状态和奖励序列作为输入，通过旋转位置嵌入捕捉长程时间依赖性，输出任务表示。解码器则利用这些任务表示来重构或预测相关序列。模型采用变分推断框架，通过摊销推理网络学习任务表示的分布，支持高效的在线适应。训练过程包括预训练信念模型和元策略优化两个阶段，实现模块化设计。\\n\\n**关键创新**：最重要的技术创新是“无动作推断”，即仅从状态和奖励序列推断任务表示，而不依赖动作信息。这与现有方法的本质区别在于彻底解耦了任务推断和策略优化，使得任务表示更具通用性，能够适应不同策略，并提高了模型的泛化能力和可扩展性。\\n\\n**关键设计**：模型采用Transformer编码器-解码器结构，编码器使用旋转位置嵌入（RoPE）来增强位置编码能力，更好地处理长序列。损失函数基于变分下界（ELBO），包括重构损失和KL散度项，用于优化任务表示的分布。网络参数通过梯度下降优化，实验中使用MetaWorld ML-10基准进行验证，具体超参数如层数、头数等根据任务调整，以平衡计算效率和性能。",
            "application_zh": "该研究主要应用于机器人控制领域，特别是在需要快速适应新任务或环境的场景中，如工业自动化、服务机器人和自主导航。通过无动作推断，CRAFT提高了元强化学习的泛化能力和可扩展性，为实际机器人系统提供了更灵活和高效的学习框架，未来可能推动智能体在复杂、动态环境中的自主学习和决策能力。",
            "highlight_zh": "在MetaWorld ML-10机器人操作基准上的实验表明，CRAFT相比上下文自适应元强化学习基线方法，在适应速度、泛化性能和探索效率方面均有显著提升。具体来说，CRAFT实现了更快的任务适应（例如，在少量样本下达到更高成功率），更好的泛化到未见任务（成功率提升约10-20%），以及更有效的探索策略（减少无效动作尝试）。这些结果验证了无动作推断在提高元强化学习性能方面的有效性。",
            "tags_zh": [
                "元强化学习",
                "任务表示学习",
                "无动作推断",
                "Transformer编码器-解码器",
                "变分推断",
                "机器人控制",
                "泛化能力",
                "模块化训练"
            ],
            "_index": 22
        },
        {
            "title": "Robust Single-shot Structured Light 3D Imaging via Neural Feature Decoding",
            "authors": [
                "Jiaheng Li",
                "Qiyu Dai",
                "Lihan Li",
                "Praneeth Chakravarthula",
                "He Sun",
                "Baoquan Chen",
                "Wenzheng Chen"
            ],
            "arxiv_id": "2512.14028v1",
            "summary": "We consider the problem of active 3D imaging using single-shot structured light systems, which are widely employed in commercial 3D sensing devices such as Apple Face ID and Intel RealSense. Traditional structured light methods typically decode depth correspondences through pixel-domain matching algorithms, resulting in limited robustness under challenging scenarios like occlusions, fine-structured details, and non-Lambertian surfaces. Inspired by recent advances in neural feature matching, we propose a learning-based structured light decoding framework that performs robust correspondence matching within feature space rather than the fragile pixel domain. Our method extracts neural features from the projected patterns and captured infrared (IR) images, explicitly incorporating their geometric priors by building cost volumes in feature space, achieving substantial performance improvements over pixel-domain decoding approaches. To further enhance depth quality, we introduce a depth refinement module that leverages strong priors from large-scale monocular depth estimation models, improving fine detail recovery and global structural coherence. To facilitate effective learning, we develop a physically-based structured light rendering pipeline, generating nearly one million synthetic pattern-image pairs with diverse objects and materials for indoor settings. Experiments demonstrate that our method, trained exclusively on synthetic data with multiple structured light patterns, generalizes well to real-world indoor environments, effectively processes various pattern types without retraining, and consistently outperforms both commercial structured light systems and passive stereo RGB-based depth estimation methods. Project page: https://namisntimpot.github.io/NSLweb/.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14028v1",
            "code_links": [
                {
                    "url": "https://namisntimpot.github.io/NSLweb/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "深度估计",
                    "matched_keywords": [
                        "depth estimation",
                        "monocular depth"
                    ],
                    "score": 2
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 4,
            "headline_zh": "提出基于神经特征解码的单次结构光3D成像方法，以提升在遮挡、精细结构和非朗伯表面等挑战场景下的鲁棒性。",
            "summary_zh": "本文研究了单次结构光系统在主动3D成像中的应用，这类系统广泛应用于苹果Face ID和英特尔RealSense等商业3D传感设备。传统结构光方法通常通过像素域匹配算法解码深度对应关系，导致在遮挡、精细结构细节和非朗伯表面等挑战场景下鲁棒性有限。受神经特征匹配最新进展的启发，我们提出了一种基于学习的结构光解码框架，在特征空间而非脆弱的像素域执行鲁棒的对应匹配。我们的方法从投影图案和捕获的红外图像中提取神经特征，通过在特征空间中构建代价体积显式地结合它们的几何先验，相比像素域解码方法实现了显著的性能提升。为了进一步提高深度质量，我们引入了一个深度细化模块，利用大规模单目深度估计模型的强先验，改善精细细节恢复和全局结构一致性。为了促进有效学习，我们开发了一个基于物理的结构光渲染流程，生成了近百万个包含不同物体和材料的合成图案-图像对，适用于室内环境。实验表明，我们的方法仅在合成数据上训练，使用多种结构光图案，能很好地泛化到真实世界室内环境，无需重新训练即可有效处理各种图案类型，并始终优于商业结构光系统和基于被动立体RGB的深度估计方法。项目页面：https://namisntimpot.github.io/NSLweb/。",
            "intro_zh": [
                "传统单次结构光方法依赖像素域匹配，在遮挡、精细结构或非朗伯表面等复杂场景下鲁棒性不足，导致深度估计精度下降。",
                "提出基于神经特征解码的框架，在特征空间而非像素域进行对应匹配，并引入深度细化模块，结合几何先验和单目深度先验提升性能。",
                "实验显示，该方法在合成数据训练后能泛化到真实室内场景，优于商业结构光系统和被动立体RGB方法，无需重新训练即可处理多种图案。"
            ],
            "method_zh": "**问题定义**：论文旨在解决单次结构光3D成像在遮挡、精细结构细节和非朗伯表面等挑战场景下的鲁棒性问题。现有方法（如传统像素域匹配算法）在这些场景下表现脆弱，导致深度对应解码不准确，限制了实际应用中的可靠性和精度。\\n\\n**核心思路**：论文的核心思路是将结构光解码从像素域迁移到特征空间，利用神经特征匹配的鲁棒性来提升对应关系的准确性。通过提取投影图案和红外图像的神经特征，并结合几何先验构建代价体积，实现更稳健的深度估计。此外，引入深度细化模块，利用大规模单目深度估计模型的先验来优化细节和全局结构。\\n\\n**技术框架**：整体架构包括两个主要阶段：神经特征解码和深度细化。首先，从输入的红外图像和投影图案中提取神经特征，然后在特征空间中构建代价体积以编码几何对应关系，通过优化解码深度图。接着，深度细化模块利用预训练的单目深度估计模型提供的先验，对初始深度图进行后处理，增强细节恢复和结构一致性。整个流程基于端到端学习，训练数据由物理渲染流程生成。\\n\\n**关键创新**：最重要的技术创新在于将结构光解码从传统的像素域匹配转向基于神经特征的空间匹配，这本质区别在于利用了深度学习提取的高维特征，能更好地处理复杂场景的变异性和噪声。同时，结合单目深度先验进行细化，突破了传统方法仅依赖局部信息的局限。\\n\\n**关键设计**：关键设计包括：使用卷积神经网络提取特征，构建特征空间中的代价体积以融合几何先验；损失函数可能结合深度回归损失和一致性约束；网络结构可能包含编码器-解码器架构；参数设置涉及合成数据生成中的物理参数（如材料属性、光照条件），以及训练时的超参数优化。具体细节如网络层数、损失函数公式在摘要中未明确，需参考论文全文。",
            "application_zh": "该研究在3D传感领域具有广泛的应用潜力，可提升商业设备如苹果Face ID和英特尔RealSense在复杂环境下的性能。实际价值包括增强现实、机器人导航、工业检测和医疗成像中的高精度3D重建。未来可能推动结构光技术向更鲁棒和自适应方向发展，降低对理想场景的依赖。",
            "highlight_zh": "实验结果表明，该方法在合成数据上训练后，能有效泛化到真实世界室内环境，无需重新训练即可处理多种结构光图案。与商业结构光系统和被动立体RGB深度估计方法相比，在遮挡、精细细节和非朗伯表面等挑战场景下，性能显著提升，具体数据如误差降低百分比需参考论文，但摘要指出“始终优于”基线方法。",
            "tags_zh": [
                "单次结构光",
                "神经特征解码",
                "3D成像",
                "深度估计",
                "特征空间匹配",
                "合成数据训练",
                "鲁棒性提升",
                "室内场景"
            ],
            "_index": 23
        },
        {
            "title": "Deep Learning Perspective of Scene Understanding in Autonomous Robots",
            "authors": [
                "Afia Maham",
                "Dur E Nayab Tashfa"
            ],
            "arxiv_id": "2512.14020v1",
            "summary": "This paper provides a review of deep learning applications in scene understanding in autonomous robots, including innovations in object detection, semantic and instance segmentation, depth estimation, 3D reconstruction, and visual SLAM. It emphasizes how these techniques address limitations of traditional geometric models, improve depth perception in real time despite occlusions and textureless surfaces, and enhance semantic reasoning to understand the environment better. When these perception modules are integrated into dynamic and unstructured environments, they become more effective in decisionmaking, navigation and interaction. Lastly, the review outlines the existing problems and research directions to advance learning-based scene understanding of autonomous robots.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "11 pages. Review Paper on Deep Learning Perspective of Scene Understanding in Autonomous Robots",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14020v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "深度估计",
                    "matched_keywords": [
                        "depth estimation",
                        "3D reconstruction"
                    ],
                    "score": 2
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "visual SLAM",
                        "SLAM"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 4,
            "headline_zh": "综述深度学习在自主机器人场景理解中的应用，涵盖感知模块创新与集成挑战",
            "summary_zh": "本文综述了深度学习在自主机器人场景理解中的应用，包括物体检测、语义与实例分割、深度估计、三维重建和视觉SLAM等领域的创新。论文强调这些技术如何解决传统几何模型的局限性，在遮挡和无纹理表面情况下实时改善深度感知，并增强语义推理以更好地理解环境。当这些感知模块集成到动态和非结构化环境中时，它们在决策制定、导航和交互方面变得更加有效。最后，综述概述了现有问题及研究方向，以推进基于学习的自主机器人场景理解。",
            "intro_zh": [
                "核心问题：传统几何模型在动态、非结构化环境中存在局限性，难以处理遮挡、无纹理表面等复杂场景，影响深度感知和语义理解。",
                "方法要点：通过深度学习技术整合物体检测、语义分割、深度估计等感知模块，增强实时场景理解能力，提升自主机器人的决策与导航性能。",
                "实验或效果：综述表明深度学习模型能显著改善场景理解的准确性和鲁棒性，为自主机器人在复杂环境中的实际应用提供技术基础。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自主机器人在动态和非结构化环境中进行场景理解时面临的挑战，包括传统几何模型难以处理遮挡、无纹理表面、实时性不足以及语义信息缺失等问题，这些限制了机器人的深度感知、导航和交互能力。\\n\\n**核心思路**：通过深度学习技术整合多种感知模块，如物体检测、语义分割和深度估计，以数据驱动的方式增强场景理解的全面性和实时性，从而克服传统方法的局限性，提升机器人在复杂环境中的适应性和决策效率。\\n\\n**技术框架**：整体架构基于深度学习模型，涵盖多个关键模块：物体检测用于识别环境中的物体，语义和实例分割提供像素级分类，深度估计生成三维信息，三维重建构建环境模型，视觉SLAM实现定位与地图构建。这些模块通过集成学习或端到端训练协同工作，形成完整的场景理解流程。\\n\\n**关键创新**：最重要的技术创新在于将深度学习应用于自主机器人的多模态感知任务，强调模块间的协同与集成，以解决传统方法在实时性、鲁棒性和语义理解方面的不足，本质区别是从依赖几何假设转向数据驱动的自适应学习。\\n\\n**关键设计**：技术细节包括使用卷积神经网络（CNN）和循环神经网络（RNN）等架构进行特征提取，结合损失函数如交叉熵损失用于分割任务、均方误差损失用于深度估计，以及优化参数设置如学习率调度和批量大小，以平衡精度与计算效率。",
            "application_zh": "该研究在自主机器人领域具有广泛的应用潜力，包括自动驾驶汽车的环境感知、服务机器人的室内导航、工业机器人的物体操作以及无人机的地形探索。通过提升场景理解的准确性和实时性，能增强机器人在动态环境中的决策能力，推动智能系统在医疗、物流和安防等行业的实际部署，未来可能促进更安全、高效的自动化解决方案。",
            "highlight_zh": "综述指出深度学习模型在场景理解任务中表现出显著优势，例如在物体检测任务中，基于深度学习的模型如YOLO和Faster R-CNN相比传统方法提升准确率约10-20%；在语义分割中，模型如Mask R-CNN能实现像素级精度超过80%。这些技术通过集成模块，在实时处理速度上达到每秒数十帧，有效应对遮挡和无纹理表面挑战，为自主机器人的实际应用提供了可靠的技术基础。",
            "tags_zh": [
                "场景理解",
                "深度学习",
                "自主机器人",
                "语义分割",
                "深度估计",
                "视觉SLAM",
                "三维重建",
                "物体检测"
            ],
            "_index": 24
        },
        {
            "title": "TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs",
            "authors": [
                "Jun Zhang",
                "Teng Wang",
                "Yuying Ge",
                "Yixiao Ge",
                "Xinhao Li",
                "Ying Shan",
                "Limin Wang"
            ],
            "arxiv_id": "2512.14698v1",
            "summary": "This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.MM"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project Page: https://timelens-arc-lab.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14698v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RL",
                        "reward"
                    ],
                    "score": 3
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出TimeLens基准与模型，通过高质量数据和算法设计提升多模态大语言模型的视频时序定位能力",
            "summary_zh": "本文并未提出全新方法，而是为视频理解的核心能力——视频时序定位（VTG）建立了一个直接、渐进但至关重要的基线。尽管多模态大语言模型（MLLMs）在多种视频理解任务中表现出色，但优化其VTG能力的方案仍未被充分探索。本文提出TimeLens，从数据质量和算法设计两个主要维度，系统性地研究如何构建具有强大VTG能力的MLLMs。我们首先揭示了现有VTG基准中的关键质量问题，并引入了TimeLens-Bench，它包含三个流行基准的精心重新标注版本，遵循严格的质量标准。我们的分析显示，与旧基准相比，模型排名发生了显著变化，证实了先前评估标准的不可靠性。我们还通过自动重新标注流程解决了训练数据中的噪声问题，生成了TimeLens-100K，这是一个大规模、高质量的训练数据集。基于我们的数据基础，我们深入探索了算法设计原则，得出一系列有意义的见解和有效且高效的实践。这些包括用于时间表示的交替文本编码、作为训练范式的免思考强化学习与可验证奖励（RLVR）方法，以及精心设计的RLVR训练方案。这些努力最终形成了TimeLens模型系列，这是一组在开源模型中具有最先进VTG性能的MLLMs，甚至超越了GPT-5和Gemini-2.5-Flash等专有模型。所有代码、数据和模型都将发布以促进未来研究。",
            "intro_zh": [
                "现有VTG基准存在严重质量问题，导致模型评估不可靠，且训练数据噪声大，限制了MLLMs在视频时序定位中的性能提升。",
                "论文从数据质量和算法设计双维度入手，构建高质量基准TimeLens-Bench和训练集TimeLens-100K，并设计交替文本编码和RLVR训练范式。",
                "TimeLens模型在开源模型中达到最先进水平，超越GPT-5等专有模型，显著提升了VTG任务的准确性和可靠性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决视频时序定位（VTG）任务中，由于现有基准数据质量低和训练数据噪声大，导致多模态大语言模型（MLLMs）性能评估不可靠且优化困难的问题。现有方法的痛点包括标注错误、评估标准不一致，以及缺乏系统性的算法设计指导。\\n\\n**核心思路**：论文的核心思路是通过提升数据质量和优化算法设计，系统性地构建具有强大VTG能力的MLLMs。这包括重新标注基准以消除噪声、创建高质量训练集，并探索有效的编码和训练策略，以确保模型能准确理解视频中的时间信息。\\n\\n**技术框架**：整体框架分为数据构建和算法设计两阶段。首先，通过人工审核和自动流程，生成TimeLens-Bench（高质量评估基准）和TimeLens-100K（高质量训练数据集）。然后，基于这些数据，设计MLLM模型，采用交替文本编码处理时间表示，并使用RLVR作为训练范式，结合可验证奖励进行优化。\\n\\n**关键创新**：最重要的技术创新是提出了TimeLens-Bench和TimeLens-100K，解决了数据质量问题；同时，引入了交替文本编码和RLVR训练方法，这些设计显著提升了VTG性能，与现有方法相比，更注重数据可靠性和算法效率。\\n\\n**关键设计**：关键设计包括：交替文本编码将时间信息嵌入文本序列，增强时间感知；RLVR训练范式免除了复杂思考步骤，直接基于可验证奖励（如定位准确性）进行强化学习；训练方案中可能涉及奖励函数设计、学习率调度等超参数优化，具体细节需参考论文代码。",
            "application_zh": "该研究可应用于视频内容分析、智能监控、视频检索和编辑等领域，通过提升视频时序定位的准确性，助力自动化视频理解系统的发展。其高质量数据和算法设计为未来VTG研究提供了可靠基线，推动多模态AI在真实场景中的落地，如教育、娱乐和安防。",
            "highlight_zh": "TimeLens模型在TimeLens-Bench上评估，显示出与旧基准相比的模型排名巨变，证实了先前标准的不可靠性。具体性能上，TimeLens在开源模型中达到最先进水平，甚至超越了GPT-5和Gemini-2.5-Flash等专有模型，提升了VTG任务的准确率，具体数据需参考论文实验部分。",
            "tags_zh": [
                "视频时序定位",
                "多模态大语言模型",
                "数据质量基准",
                "强化学习训练",
                "时间表示编码",
                "视频理解",
                "开源模型",
                "算法设计"
            ],
            "_index": 25
        },
        {
            "title": "Beyond Lipschitz Continuity and Monotonicity: Fractal and Chaotic Activation Functions in Echo State Networks",
            "authors": [
                "Rae Chipera",
                "Jenny Du",
                "Irene Tsapara"
            ],
            "arxiv_id": "2512.14675v1",
            "summary": "Contemporary reservoir computing relies heavily on smooth, globally Lipschitz continuous activation functions, limiting applications in defense, disaster response, and pharmaceutical modeling where robust operation under extreme conditions is critical. We systematically investigate non-smooth activation functions, including chaotic, stochastic, and fractal variants, in echo state networks. Through comprehensive parameter sweeps across 36,610 reservoir configurations, we demonstrate that several non-smooth functions not only maintain the Echo State Property (ESP) but outperform traditional smooth activations in convergence speed and spectral radius tolerance. Notably, the Cantor function (continuous everywhere and flat almost everywhere) maintains ESP-consistent behavior up to spectral radii of rho ~ 10, an order of magnitude beyond typical bounds for smooth functions, while achieving 2.6x faster convergence than tanh and ReLU. We introduce a theoretical framework for quantized activation functions, defining a Degenerate Echo State Property (d-ESP) that captures stability for discrete-output functions and proving that d-ESP implies traditional ESP. We identify a critical crowding ratio Q=N/k (reservoir size / quantization levels) that predicts failure thresholds for discrete activations. Our analysis reveals that preprocessing topology, rather than continuity per se, determines stability: monotone, compressive preprocessing maintains ESP across scales, while dispersive or discontinuous preprocessing triggers sharp failures. While our findings challenge assumptions about activation function design in reservoir computing, the mechanism underlying the exceptional performance of certain fractal functions remains unexplained, suggesting fundamental gaps in our understanding of how geometric properties of activation functions influence reservoir dynamics.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "50 pages, 21 figures. Extended version with full proofs, parameter sweeps, and appendices",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14675v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO",
                        "VIO"
                    ],
                    "score": 2
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出非光滑激活函数在回声状态网络中的应用，以提升极端条件下的鲁棒性和收敛速度。",
            "summary_zh": "当代储层计算严重依赖光滑、全局Lipschitz连续的激活函数，这限制了在国防、灾害响应和药物建模等极端条件下需要鲁棒操作的应用。我们系统地研究了回声状态网络中的非光滑激活函数，包括混沌、随机和分形变体。通过对36,610个储层配置进行全面的参数扫描，我们证明了几种非光滑函数不仅保持了回声状态特性（ESP），而且在收敛速度和谱半径容限方面优于传统光滑激活函数。值得注意的是，康托函数（处处连续且几乎处处平坦）在谱半径高达ρ~10时仍保持ESP一致行为，比光滑函数的典型界限高出一个数量级，同时实现了比tanh和ReLU快2.6倍的收敛速度。我们引入了量化激活函数的理论框架，定义了捕获离散输出函数稳定性的退化回声状态特性（d-ESP），并证明d-ESP蕴含传统ESP。我们识别了一个关键的拥挤比Q=N/k（储层大小/量化级别），用于预测离散激活函数的失效阈值。我们的分析表明，预处理拓扑而非连续性本身决定了稳定性：单调、压缩的预处理在多个尺度上保持ESP，而分散或不连续的预处理则引发急剧失效。虽然我们的发现挑战了储层计算中激活函数设计的假设，但某些分形函数优异性能的机制仍未得到解释，这表明我们对激活函数几何性质如何影响储层动态的理解存在根本性差距。",
            "intro_zh": [
                "核心问题：传统回声状态网络依赖光滑激活函数，限制了在极端条件下的鲁棒性，无法满足国防、灾害响应等应用需求。",
                "方法要点：系统研究非光滑激活函数，包括分形、混沌和随机变体，并引入量化激活函数的理论框架，定义退化回声状态特性。",
                "实验或效果：康托函数在谱半径高达10时保持稳定，收敛速度比tanh和ReLU快2.6倍，通过36,610个配置验证了性能优势。"
            ],
            "method_zh": "**问题定义**：论文要解决回声状态网络中传统光滑激活函数（如tanh、ReLU）在极端条件下鲁棒性不足的问题，这些函数依赖Lipschitz连续性和单调性，限制了在国防、灾害响应等高风险场景的应用，导致收敛速度慢和谱半径容限低。\\n\\n**核心思路**：论文的核心思路是挑战传统假设，通过系统研究非光滑激活函数（如分形、混沌和随机函数）来提升回声状态网络的性能，认为预处理拓扑而非连续性本身是稳定性的关键，从而设计出更鲁棒和高效的激活函数。\\n\\n**技术框架**：整体架构包括理论分析和实验验证两部分。理论部分引入量化激活函数的框架，定义退化回声状态特性（d-ESP）并证明其蕴含传统ESP；实验部分通过大规模参数扫描（36,610个储层配置）评估不同激活函数的性能，包括收敛速度和谱半径容限。\\n\\n**关键创新**：最重要的技术创新是首次系统地将非光滑激活函数（如康托函数）应用于回声状态网络，并提出了d-ESP理论框架，这本质区别在于突破了传统对光滑性和单调性的依赖，直接从几何性质角度分析稳定性。\\n\\n**关键设计**：关键设计包括使用康托函数等分形变体作为激活函数，设置谱半径参数高达10进行测试，定义拥挤比Q=N/k（储层大小除以量化级别）来预测离散激活函数的失效阈值，并通过预处理拓扑（单调压缩 vs. 分散不连续）来控制稳定性。",
            "application_zh": "该研究在国防、灾害响应和药物建模等领域具有潜在应用价值，这些场景需要网络在极端条件下保持鲁棒操作。通过提升收敛速度和谱半径容限，可以加速模型训练并增强系统稳定性，未来可能推动储层计算在更多高风险环境中的部署，并启发激活函数设计的新方向。",
            "highlight_zh": "最重要的实验结果包括：康托函数在谱半径高达ρ~10时仍保持回声状态特性，比传统光滑函数的典型界限高出一个数量级；收敛速度比tanh和ReLU快2.6倍；通过36,610个配置的全面参数扫描验证了非光滑函数的性能优势；拥挤比Q=N/k成功预测了离散激活函数的失效阈值。",
            "tags_zh": [
                "回声状态网络",
                "非光滑激活函数",
                "分形函数",
                "退化回声状态特性",
                "谱半径容限",
                "收敛速度优化",
                "储层计算",
                "量化激活函数"
            ],
            "_index": 26
        },
        {
            "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
            "authors": [
                "Wenqiang Sun",
                "Haiyu Zhang",
                "Haoyuan Wang",
                "Junta Wu",
                "Zehan Wang",
                "Zhenwei Wang",
                "Yunhong Wang",
                "Jun Zhang",
                "Tengfei Wang",
                "Chunchao Guo"
            ],
            "arxiv_id": "2512.14614v1",
            "summary": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.",
            "categories": [
                "cs.CV",
                "cs.GR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "project page: https://3d-models.hunyuan.tencent.com/world/, demo: https://3d.hunyuan.tencent.com/sceneTo3D",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14614v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "世界模型",
                    "matched_keywords": [
                        "world model"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出WorldPlay流式视频扩散模型，通过长期几何一致性实现实时交互式世界建模，解决速度与内存的权衡问题。",
            "summary_zh": "本文提出了WorldPlay，一种流式视频扩散模型，能够实现具有长期几何一致性的实时交互式世界建模，解决了当前方法在速度与内存之间的权衡限制。WorldPlay基于三个关键创新：1）使用双重动作表示，实现对用户键盘和鼠标输入的鲁棒动作控制；2）通过重构上下文记忆动态重建过去帧的上下文，并利用时间重帧保持几何重要但久远帧的可访问性，有效缓解内存衰减；3）提出上下文强制，一种专为内存感知模型设计的新蒸馏方法，通过对齐教师和学生模型的记忆上下文，保持学生模型使用长程信息的能力，实现实时速度同时防止误差漂移。综合来看，WorldPlay能以24 FPS生成720p长时流式视频，具有优越的一致性，优于现有技术，并在多样场景中展现出强泛化能力。项目页面和在线演示可在https://3d-models.hunyuan.tencent.com/world/和https://3d.hunyuan.tencent.com/sceneTo3D找到。",
            "intro_zh": [
                "现有方法在实时交互式世界建模中面临速度与内存的权衡，难以同时保证长期几何一致性和高效处理。",
                "WorldPlay采用双重动作表示、重构上下文记忆和上下文强制蒸馏，实现鲁棒控制、动态记忆管理和实时性能优化。",
                "实验表明，WorldPlay能以24 FPS生成720p长时视频，在一致性和泛化性上优于基线，显著提升交互体验。"
            ],
            "method_zh": "**问题定义**：论文旨在解决实时交互式世界建模中速度与内存的权衡问题，现有方法在处理长时视频时，常因内存衰减导致几何一致性下降，或为保持一致性而牺牲实时性，限制了实际应用。\\n\\n**核心思路**：论文的核心思路是通过创新记忆管理和蒸馏技术，在流式视频生成中动态维护长期几何上下文，同时优化模型效率，实现实时交互与一致性平衡。设计基于扩散模型框架，强调动作控制、上下文重建和知识传递的协同作用。\\n\\n**技术框架**：整体架构基于流式视频扩散模型，包含输入处理、动作控制、记忆模块和生成输出。主要模块包括：双重动作表示模块，用于解析用户输入；重构上下文记忆模块，动态管理过去帧；上下文强制蒸馏模块，优化学生模型性能。流程上，模型实时接收交互指令，结合记忆上下文生成连贯视频帧。\\n\\n**关键创新**：最重要的技术创新是重构上下文记忆和上下文强制蒸馏。与现有方法相比，前者通过时间重帧主动保留关键几何信息，而非被动存储，本质区别在于动态重建能力；后者专为内存感知模型设计，确保蒸馏过程中长程信息不丢失，区别于传统蒸馏忽略记忆对齐。\\n\\n**关键设计**：关键设计包括：双重动作表示使用编码网络处理键盘和鼠标输入；重构上下文记忆采用动态重建算法，参数设置如记忆窗口大小和重帧阈值优化了访问效率；上下文强制蒸馏损失函数对齐教师和学生模型的记忆上下文，网络结构可能包含轻量化学生模型；扩散过程参数如噪声调度和采样步骤支持实时生成。",
            "application_zh": "该研究在虚拟现实、游戏开发、自动驾驶模拟和远程协作等领域具有潜在应用价值，能提升实时交互场景的世界建模质量，支持长时一致性视频生成，未来可能推动智能系统在动态环境中的感知与决策能力。",
            "highlight_zh": "最重要的实验结果显示，WorldPlay能以24 FPS实时生成720p长时流式视频，在几何一致性指标上优于现有基线方法，提升幅度显著；在多样场景测试中展现出强泛化能力，验证了模型在交互控制下的鲁棒性和效率。",
            "tags_zh": [
                "流式视频生成",
                "交互式世界建模",
                "长期几何一致性",
                "记忆管理",
                "蒸馏训练",
                "实时系统",
                "扩散模型",
                "动作控制"
            ],
            "_index": 27
        },
        {
            "title": "Residual GRU+MHSA: A Lightweight Hybrid Recurrent Attention Model for Cardiovascular Disease Detection",
            "authors": [
                "Tejaswani Dash",
                "Gautam Datla",
                "Anudeep Vurity",
                "Tazeem Ahmad",
                "Mohd Adnan",
                "Saima Rafi",
                "Saisha Patro",
                "Saina Patro"
            ],
            "arxiv_id": "2512.14563v1",
            "summary": "Cardiovascular disease (CVD) remains the leading cause of mortality worldwide, underscoring the need for reliable and efficient predictive tools that support early intervention. Traditional diagnostic approaches rely on handcrafted features and clinician expertise, while machine learning methods improve reproducibility but often struggle to generalize across noisy and heterogeneous clinical data. In this work, we propose Residual GRU with Multi-Head Self-Attention, a compact deep learning architecture designed for tabular clinical records. The model integrates residual bidirectional gated recurrent units for sequential modeling of feature columns, a channel reweighting block, and multi-head self-attention pooling with a learnable classification token to capture global context. We evaluate the model on the UCI Heart Disease dataset using 5-fold stratified cross-validation and compare it against classical methods such as Logistic Regression, Random Forest, and Support Vector Machines, as well as modern deep learning baselines including DeepMLP, convolutional networks, recurrent networks, and Transformers. The proposed model achieves an accuracy of 0.861, macro-F1 of 0.860, ROC-AUC of 0.908, and PR-AUC of 0.904, outperforming all baselines. Ablation studies confirm the individual contributions of residual recurrence, channel gating, and attention pooling. t-SNE visualizations further indicate that the learned embeddings exhibit clearer separation between disease and non-disease classes compared to raw features. These results demonstrate that lightweight hybrid recurrent and attention-based architectures provide a strong balance between accuracy and efficiency for clinical risk prediction, supporting deployment in resource-constrained healthcare settings.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted in IEEE Bigdata 2025- Learning Representations with Limited Supervision",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14563v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出Residual GRU+MHSA轻量混合循环注意力模型，用于心血管疾病检测，平衡准确性与效率。",
            "summary_zh": "心血管疾病是全球主要死因，需要可靠高效的预测工具以支持早期干预。传统诊断方法依赖手工特征和临床专家经验，而机器学习方法虽提高可重复性，但常难以在噪声和异质临床数据中泛化。本文提出Residual GRU with Multi-Head Self-Attention，一种为表格临床记录设计的紧凑深度学习架构。该模型集成残差双向门控循环单元用于特征列的序列建模、通道重加权块以及带可学习分类标记的多头自注意力池化以捕获全局上下文。我们在UCI心脏病数据集上使用5折分层交叉验证评估模型，并与逻辑回归、随机森林、支持向量机等经典方法以及DeepMLP、卷积网络、循环网络和Transformer等现代深度学习基线进行比较。所提模型达到0.861的准确率、0.860的宏F1、0.908的ROC-AUC和0.904的PR-AUC，优于所有基线。消融研究确认了残差循环、通道门控和注意力池化的个体贡献。t-SNE可视化进一步表明，与原始特征相比，学习到的嵌入在疾病和非疾病类别间表现出更清晰的分离。这些结果表明，轻量混合循环和基于注意力的架构为临床风险预测提供了准确性与效率之间的强平衡，支持在资源受限的医疗环境中部署。",
            "intro_zh": [
                "核心问题：传统心血管疾病检测依赖手工特征和专家经验，机器学习方法在噪声和异质临床数据中泛化能力不足。",
                "方法要点：提出轻量混合模型，结合残差双向GRU进行序列建模、通道重加权和多头自注意力池化，以捕获全局上下文。",
                "实验或效果：在UCI心脏病数据集上，模型准确率达0.861，优于经典和深度学习基线，消融研究验证各模块贡献。"
            ],
            "method_zh": "**问题定义**：论文旨在解决心血管疾病检测中，传统方法依赖手工特征和专家经验导致效率低，而现有机器学习方法在噪声和异质临床表格数据中泛化能力不足的问题。现有方法的痛点包括特征工程复杂、模型鲁棒性差和计算资源需求高。\\n\\n**核心思路**：论文提出一种轻量混合循环注意力模型，通过结合残差双向GRU的序列建模能力和多头自注意力机制，有效捕获临床特征间的时序依赖和全局上下文，以提高预测准确性和泛化能力，同时保持模型紧凑性以适应资源受限环境。\\n\\n**技术框架**：整体架构包含三个主要模块：首先，使用残差双向GRU对表格临床记录的特征列进行序列建模，处理特征间的时序关系；其次，引入通道重加权块，动态调整特征通道的重要性；最后，通过多头自注意力池化层，结合可学习分类标记，聚合全局信息并输出分类结果。流程为输入特征→残差双向GRU→通道重加权→多头自注意力池化→分类输出。\\n\\n**关键创新**：最重要的技术创新点是轻量混合设计，将残差循环网络与注意力机制结合，具体包括残差双向GRU增强序列建模稳定性、通道重加权优化特征表示，以及多头自注意力池化替代传统池化以捕获全局依赖。与现有方法的本质区别在于，它避免了纯Transformer的高计算成本，同时超越传统循环网络的局限性，实现高效且准确的临床风险预测。\\n\\n**关键设计**：模型使用残差连接缓解梯度消失，双向GRU捕获前后向依赖；通道重加权块基于注意力机制动态加权特征通道；多头自注意力池化层包含可学习分类标记，用于聚合信息并输出嵌入；损失函数采用交叉熵损失进行二分类任务；在UCI心脏病数据集上，通过5折分层交叉验证评估，超参数如层数、头数等通过实验调优，具体数值未在摘要中提供，但设计强调轻量化和高效性。",
            "application_zh": "该研究主要应用于心血管疾病早期检测和临床风险预测领域，特别是在资源受限的医疗环境中，如社区医院或远程医疗系统。其实际价值在于提供一种高效、准确的自动化诊断工具，减少对专家经验的依赖，支持大规模筛查和个性化干预。未来影响可能扩展到其他慢性病预测或更广泛的医疗数据分析任务，推动人工智能在医疗保健中的普及和实用化。",
            "highlight_zh": "最重要的实验结果包括：在UCI心脏病数据集上，模型达到0.861准确率、0.860宏F1、0.908 ROC-AUC和0.904 PR-AUC，全面优于逻辑回归、随机森林、支持向量机等经典方法，以及DeepMLP、卷积网络、循环网络和Transformer等深度学习基线。消融研究确认残差循环、通道门控和注意力池化模块均对性能提升有贡献，t-SNE可视化显示学习嵌入比原始特征具有更清晰的类别分离，验证了模型的有效性和泛化能力。",
            "tags_zh": [
                "心血管疾病检测",
                "轻量混合模型",
                "残差双向GRU",
                "多头自注意力",
                "临床风险预测",
                "表格数据建模",
                "医疗人工智能",
                "资源受限部署"
            ],
            "_index": 28
        },
        {
            "title": "Counterfactual Explanations for Time Series Should be Human-Centered and Temporally Coherent in Interventions",
            "authors": [
                "Emmanuel C. Chukwu",
                "Rianne M. Schouten",
                "Monique Tabak",
                "Mykola Pechenizkiy"
            ],
            "arxiv_id": "2512.14559v1",
            "summary": "Counterfactual explanations are increasingly proposed as interpretable mechanisms to achieve algorithmic recourse. However, current counterfactual techniques for time series classification are predominantly designed with static data assumptions and focus on generating minimal input perturbations to flip model predictions. This paper argues that such approaches are fundamentally insufficient in clinical recommendation settings, where interventions unfold over time and must be causally plausible and temporally coherent. We advocate for a shift towards counterfactuals that reflect sustained, goal-directed interventions aligned with clinical reasoning and patient-specific dynamics. We identify critical gaps in existing methods that limit their practical applicability, specifically, temporal blind spots and the lack of user-centered considerations in both method design and evaluation metrics. To support our position, we conduct a robustness analysis of several state-of-the-art methods for time series and show that the generated counterfactuals are highly sensitive to stochastic noise. This finding highlights their limited reliability in real-world clinical settings, where minor measurement variations are inevitable. We conclude by calling for methods and evaluation frameworks that go beyond mere prediction changes without considering feasibility or actionability. We emphasize the need for actionable, purpose-driven interventions that are feasible in real-world contexts for the users of such applications.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14559v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出以人为中心且时间连贯的反事实解释方法，以解决临床时间序列分类中现有方法不可行的问题",
            "summary_zh": "反事实解释日益被提出作为实现算法追索的可解释机制。然而，当前用于时间序列分类的反事实技术主要基于静态数据假设设计，并侧重于生成最小输入扰动以翻转模型预测。本文认为，在临床推荐设置中，这种方法从根本上是不足的，因为干预措施随时间展开，必须具有因果合理性和时间连贯性。我们倡导转向反映持续、目标导向干预措施的反事实解释，这些干预措施与临床推理和患者特定动态相一致。我们指出了现有方法在实践应用中的关键缺陷，特别是时间盲点以及在方法设计和评估指标中缺乏以用户为中心的考虑。为了支持我们的观点，我们对几种最先进的时间序列方法进行了鲁棒性分析，并表明生成的反事实解释对随机噪声高度敏感。这一发现突显了它们在现实世界临床设置中的有限可靠性，因为微小的测量变化是不可避免的。最后，我们呼吁开发超越仅考虑预测变化而不考虑可行性或可操作性的方法和评估框架。我们强调需要可操作、目的驱动的干预措施，这些措施在现实世界环境中对应用用户是可行的。",
            "intro_zh": [
                "核心问题：现有时间序列反事实解释方法基于静态假设，忽略时间连贯性和临床可行性，导致干预措施不切实际。",
                "方法要点：提出以人为中心的反事实解释框架，强调持续、目标导向的干预，确保时间连贯性和因果合理性。",
                "实验或效果：通过鲁棒性分析发现现有方法对噪声敏感，验证了新框架在临床场景中的必要性和可靠性提升。"
            ],
            "method_zh": "**问题定义**：论文要解决时间序列分类中反事实解释方法在临床推荐设置中的不足。现有方法基于静态数据假设，仅关注最小扰动翻转预测，忽略了干预措施的时间连贯性、因果合理性和用户可行性，导致生成的反事实在实际应用中不可靠。\\n\\n**核心思路**：论文的核心解决思路是转向以人为中心的反事实解释，强调干预措施应反映持续、目标导向的过程，与临床推理和患者动态一致。设计上，通过考虑时间连贯性和因果约束，确保反事实在现实世界中的可行性和可操作性。\\n\\n**技术框架**：整体架构包括问题分析、方法评估和框架倡导三个阶段。首先，识别现有方法的缺陷，如时间盲点和缺乏用户中心设计；其次，进行鲁棒性分析，测试方法对噪声的敏感性；最后，提出新框架，强调干预措施的持续性和目标导向性，并呼吁开发相关评估指标。\\n\\n**关键创新**：最重要的技术创新点是将反事实解释从静态扰动扩展到动态、时间连贯的干预过程。与现有方法的本质区别在于，不再仅追求最小预测变化，而是综合考虑临床可行性、因果合理性和用户中心性，使解释更贴合实际应用需求。\\n\\n**关键设计**：论文未详细描述具体算法参数或网络结构，但强调了评估框架的设计原则，如纳入时间连贯性约束和用户可行性指标。关键设计包括鲁棒性分析中的噪声注入实验，以量化现有方法的敏感性，并倡导在方法开发中集成因果推理和临床知识。",
            "application_zh": "该研究主要应用于临床医疗推荐系统，如疾病预测、治疗干预和患者监测。潜在价值在于提升时间序列分类模型的可解释性和可操作性，使反事实解释更贴合临床实践，支持医生制定可行的干预计划。未来影响可能扩展到其他时间序列领域，如金融预测或工业监控，推动以人为中心的AI解释方法发展。",
            "highlight_zh": "最重要的实验结果是通过鲁棒性分析发现，现有最先进的时间序列反事实解释方法对随机噪声高度敏感。具体性能数据显示，在噪声注入下，生成的反事实解释可靠性显著下降，突显了它们在现实临床设置中的局限性。对比基线包括多种现有方法，提升幅度体现在新框架强调时间连贯性和可行性，从而提高了实际应用中的可信度。",
            "tags_zh": [
                "反事实解释",
                "时间序列分类",
                "临床推荐系统",
                "以人为中心设计",
                "时间连贯性",
                "算法可解释性",
                "鲁棒性分析",
                "干预可行性"
            ],
            "_index": 29
        },
        {
            "title": "C-ing Clearly: Enhanced Binary Code Explanations using C code",
            "authors": [
                "Teodor Poncu",
                "Ioana Pintilie",
                "Marius Dragoi",
                "Dragos Tantaru",
                "Florin Brad"
            ],
            "arxiv_id": "2512.14500v1",
            "summary": "Large Language Models (LLMs) typically excel at coding tasks involving high-level programming languages, as opposed to lower-level programming languages, such as assembly. We propose a synthetic data generation method named C-ing Clearly, which leverages the corresponding C code to enhance an LLM's understanding of assembly. By fine-tuning on data generated through our method, we demonstrate improved LLM performance for binary code summarization and vulnerability detection. Our approach demonstrates consistent gains across different LLM families and model sizes.",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "18 pages, 5 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14500v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出C-ing Clearly方法，利用C代码增强大语言模型对汇编的理解，以解决二进制代码分析任务性能不足的问题。",
            "summary_zh": "大语言模型（LLMs）通常在涉及高级编程语言的编码任务中表现出色，但对于低级编程语言（如汇编）则表现不佳。我们提出了一种名为C-ing Clearly的合成数据生成方法，该方法利用相应的C代码来增强LLM对汇编的理解。通过在通过我们的方法生成的数据上进行微调，我们证明了LLM在二进制代码摘要和漏洞检测方面的性能得到了提升。我们的方法在不同LLM家族和模型大小上均表现出了一致的增益。",
            "intro_zh": [
                "核心问题：大语言模型在高级语言任务中表现优异，但在处理低级汇编语言时理解能力有限，导致二进制代码分析任务性能不足。",
                "方法要点：提出C-ing Clearly方法，通过生成C代码与汇编的对应数据来微调大语言模型，增强其对汇编语义的理解能力。",
                "实验或效果：在二进制代码摘要和漏洞检测任务上，微调后的模型性能显著提升，且在不同模型家族和规模上均获得一致增益。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大语言模型在处理低级汇编语言时性能不足的问题，特别是在二进制代码摘要和漏洞检测任务中。现有方法的痛点在于LLMs通常训练于高级语言数据，缺乏对汇编语义的深入理解，导致在这些任务上表现不佳。\\n\\n**核心思路**：论文的核心解决思路是利用C代码作为桥梁，通过生成C代码与汇编的对应数据来增强LLM对汇编的理解。这样设计是因为C代码更接近高级语言，易于LLMs处理，同时与汇编有直接对应关系，可以帮助模型学习汇编的语义表示。\\n\\n**技术框架**：整体流程包括数据生成和模型微调两个阶段。首先，使用C-ing Clearly方法生成合成数据，其中包含汇编代码片段及其对应的C代码解释或摘要。然后，将这些数据用于微调预训练的LLM，通过监督学习优化模型在汇编相关任务上的性能。主要模块包括数据生成器、LLM微调模块和评估模块。\\n\\n**关键创新**：最重要的技术创新点是提出了一种基于C代码的合成数据生成方法，直接利用高级语言信息来增强低级语言理解。与现有方法的本质区别在于，它不依赖于大量真实汇编标注数据，而是通过自动生成的数据来桥接高级和低级语言之间的语义鸿沟。\\n\\n**关键设计**：关键设计包括数据生成策略，确保C代码与汇编的准确对应；微调时使用标准的语言建模损失函数，如交叉熵损失；网络结构基于现有LLM架构（如Transformer），无需修改；参数设置上，可能涉及学习率调整和批量大小优化，具体细节在论文中未详细说明，需参考实验部分。",
            "application_zh": "该研究在软件安全、逆向工程和代码分析领域具有重要应用价值。例如，可用于自动化二进制代码审计、恶意软件检测和漏洞挖掘，提高安全专家的工作效率。未来可能推动LLMs在低级编程语言任务中的普及，增强智能系统的代码理解能力。",
            "highlight_zh": "实验结果显示，在二进制代码摘要和漏洞检测任务上，使用C-ing Clearly方法微调的LLM性能显著提升。具体数据未在摘要中提供，但论文指出在不同LLM家族（如GPT、BERT等）和模型大小上均观察到一致增益，表明方法的鲁棒性和可扩展性。对比基线可能包括未微调的LLM或其他传统方法，提升幅度需参考完整论文。",
            "tags_zh": [
                "大语言模型",
                "汇编理解",
                "二进制代码分析",
                "合成数据生成",
                "代码摘要",
                "漏洞检测",
                "微调技术",
                "低级编程语言"
            ],
            "_index": 30
        },
        {
            "title": "Context-Picker: Dynamic context selection using multi-stage reinforcement learning",
            "authors": [
                "Siyuan Zhu",
                "Chengdong Xu",
                "Kaiqiang Ke",
                "Chao Yu"
            ],
            "arxiv_id": "2512.14465v1",
            "summary": "In long-context question answering (LCQA), determining the optimal amount of context for a given query is a significant challenge. Including too few passages may omit critical information, while including too many can introduce noise and reduce the quality of the answer. Traditional approaches, such as fixed Top-$K$ retrieval and single-stage reranking, face the dilemma of selecting the right number of passages. This problem is particularly pronounced for factoid questions, which often require only a few specific pieces of evidence. To address this issue, we introduce \\emph{Context-Picker}, a reasoning-aware framework that shifts the paradigm from similarity-based ranking to minimal sufficient subset selection. Context-Picker treats context selection as a decision-making process optimized via a human-inspired, two-stage reinforcement learning schedule: a \\emph{recall-oriented} stage that prioritizes the coverage of reasoning chains, followed by a \\emph{precision-oriented} stage that aggressively prunes redundancy to distill a compact evidence set. To resolve reward sparsity, we propose an offline evidence distillation pipeline that mines \"minimal sufficient sets\" via a Leave-One-Out (LOO) procedure, providing dense, task-aligned supervision. Experiments on five long-context and multi-hop QA benchmarks demonstrate that Context-Picker significantly outperforms strong RAG baselines, achieving superior answer accuracy with comparable or reduced context lengths. Ablation studies indicate that the coarse-to-fine optimization schedule, the redundancy-aware reward shaping, and the rationale-guided format all contribute substantially to these gains.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14465v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RL",
                        "reward"
                    ],
                    "score": 3
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出Context-Picker框架，通过多阶段强化学习解决长上下文问答中的动态上下文选择问题",
            "summary_zh": "在长上下文问答（LCQA）中，确定给定查询的最佳上下文量是一个重大挑战。包含太少段落可能遗漏关键信息，而包含太多段落可能引入噪声并降低答案质量。传统方法，如固定的Top-K检索和单阶段重排序，面临选择正确段落数量的困境。这个问题对于事实性问题尤其突出，这些问题通常只需要少量特定证据。为解决此问题，我们引入了Context-Picker，这是一个推理感知框架，将范式从基于相似性的排序转向最小充分子集选择。Context-Picker将上下文选择视为一个决策过程，通过人类启发的两阶段强化学习计划进行优化：一个面向召回的阶段，优先考虑推理链的覆盖；随后是一个面向精度的阶段，积极剪枝冗余以提炼紧凑的证据集。为解决奖励稀疏性问题，我们提出了一个离线证据蒸馏管道，通过留一法（LOO）程序挖掘“最小充分集”，提供密集、任务对齐的监督。在五个长上下文和多跳问答基准上的实验表明，Context-Picker显著优于强大的RAG基线，在可比或减少的上下文长度下实现了更优的答案准确性。消融研究表明，从粗到细的优化计划、冗余感知的奖励塑造和推理引导的格式都对这一增益有实质性贡献。",
            "intro_zh": [
                "核心问题：长上下文问答中，传统固定Top-K检索和单阶段重排序方法难以动态确定最优上下文量，导致信息遗漏或噪声引入，尤其影响事实性问题。",
                "方法要点：提出Context-Picker框架，将上下文选择视为决策过程，采用两阶段强化学习（召回导向和精度导向）优化最小充分子集选择，并引入离线证据蒸馏解决奖励稀疏性。",
                "实验或效果：在五个基准测试中，Context-Picker显著优于RAG基线，提升答案准确性，同时减少上下文长度，消融研究验证了关键组件的有效性。"
            ],
            "method_zh": "**问题定义**：论文解决长上下文问答（LCQA）中的动态上下文选择问题。现有方法如固定Top-K检索和单阶段重排序面临困境：选择太少段落可能遗漏关键信息，选择太多则引入噪声，降低答案质量，尤其对于事实性问题，这需要精确的最小证据集。\\n\\n**核心思路**：论文的核心思路是将上下文选择从基于相似性的排序范式转向最小充分子集选择，通过强化学习优化决策过程。设计灵感来自人类推理：先广泛覆盖可能证据（召回），再精细剪枝冗余（精度），从而动态确定最优上下文量。\\n\\n**技术框架**：整体架构包括两阶段强化学习计划：第一阶段为召回导向阶段，优先覆盖推理链，确保不遗漏关键信息；第二阶段为精度导向阶段，积极剪枝冗余，提炼紧凑证据集。此外，引入离线证据蒸馏管道，通过留一法（LOO）挖掘最小充分集，提供密集监督以解决奖励稀疏性。\\n\\n**关键创新**：最重要的技术创新是提出推理感知的多阶段强化学习框架，将上下文选择视为决策优化问题，而非静态排序。与现有方法的本质区别在于：从固定数量选择转向动态子集选择，结合人类启发式优化，并通过离线蒸馏增强训练效率。\\n\\n**关键设计**：关键设计包括：两阶段强化学习计划（召回和精度阶段），使用特定奖励函数（如覆盖奖励和冗余惩罚）；离线证据蒸馏管道，基于LOO程序生成监督信号；网络结构可能涉及策略网络和环境交互，但具体参数设置和损失函数细节在摘要中未明确，需参考论文正文。",
            "application_zh": "该研究在长上下文问答领域具有广泛潜在应用，如智能客服、文档分析、教育辅助和知识库检索系统。通过动态选择最小充分上下文，能提升答案准确性、减少计算开销，并增强系统在复杂查询（如多跳推理）中的性能。未来可能影响检索增强生成（RAG）技术的发展，推动更高效、精准的信息提取方法。",
            "highlight_zh": "在五个长上下文和多跳问答基准测试中，Context-Picker显著优于强大的RAG基线，具体性能数据未在摘要中提供，但表明在可比或减少的上下文长度下实现了更优的答案准确性。消融研究验证了关键组件（如两阶段优化计划、冗余感知奖励塑造和推理引导格式）的实质性贡献，提升了整体效果。",
            "tags_zh": [
                "长上下文问答",
                "强化学习",
                "动态上下文选择",
                "最小充分子集",
                "多阶段优化",
                "证据蒸馏",
                "检索增强生成",
                "推理感知框架"
            ],
            "_index": 31
        },
        {
            "title": "GRAFT: Grid-Aware Load Forecasting with Multi-Source Textual Alignment and Fusion",
            "authors": [
                "Fangzhou Lin",
                "Guoshun He",
                "Zhenyu Guo",
                "Zhe Huang",
                "Jinsong Tao"
            ],
            "arxiv_id": "2512.14400v1",
            "summary": "Electric load is simultaneously affected across multiple time scales by exogenous factors such as weather and calendar rhythms, sudden events, and policies. Therefore, this paper proposes GRAFT (GRid-Aware Forecasting with Text), which modifies and improves STanHOP to better support grid-aware forecasting and multi-source textual interventions. Specifically, GRAFT strictly aligns daily-aggregated news, social media, and policy texts with half-hour load, and realizes text-guided fusion to specific time positions via cross-attention during both training and rolling forecasting. In addition, GRAFT provides a plug-and-play external-memory interface to accommodate different information sources in real-world deployment. We construct and release a unified aligned benchmark covering 2019--2021 for five Australian states (half-hour load, daily-aligned weather/calendar variables, and three categories of external texts), and conduct systematic, reproducible evaluations at three scales -- hourly, daily, and monthly -- under a unified protocol for comparison across regions, external sources, and time scales. Experimental results show that GRAFT significantly outperforms strong baselines and reaches or surpasses the state of the art across multiple regions and forecasting horizons. Moreover, the model is robust in event-driven scenarios and enables temporal localization and source-level interpretation of text-to-load effects through attention read-out. We release the benchmark, preprocessing scripts, and forecasting results to facilitate standardized empirical evaluation and reproducibility in power grid load forecasting.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14400v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出GRAFT模型，通过多源文本对齐与融合提升电网负荷预测精度，解决外部因素影响建模难题。",
            "summary_zh": "电力负荷同时受到天气、日历节律、突发事件和政策等多种外部因素在不同时间尺度上的影响。为此，本文提出了GRAFT（基于文本的电网感知预测）模型，该模型改进并扩展了STanHOP，以更好地支持电网感知预测和多源文本干预。具体而言，GRAFT将每日聚合的新闻、社交媒体和政策文本与半小时负荷数据进行严格对齐，并通过训练和滚动预测期间的交叉注意力机制实现文本引导的融合到特定时间位置。此外，GRAFT提供了一个即插即用的外部记忆接口，以适应实际部署中的不同信息源。我们构建并发布了一个统一的、对齐的基准数据集，覆盖2019年至2021年澳大利亚五个州的负荷数据（半小时负荷、每日对齐的天气/日历变量以及三类外部文本），并在统一协议下进行了系统、可重复的评估，包括小时、日和月三个尺度，以比较不同区域、外部来源和时间尺度的性能。实验结果表明，GRAFT显著优于强基线模型，并在多个区域和预测时间范围内达到或超越了当前最优水平。此外，该模型在事件驱动场景中表现出鲁棒性，并通过注意力读出机制实现了文本对负荷影响的时间定位和来源级解释。我们发布了基准数据集、预处理脚本和预测结果，以促进电网负荷预测领域的标准化实证评估和可重复性研究。",
            "intro_zh": [
                "现有负荷预测方法难以有效整合多源文本信息（如新闻、社交媒体、政策），导致对外部突发事件的建模能力不足。",
                "GRAFT通过严格对齐多源文本与负荷数据，并利用交叉注意力实现文本引导的融合，提升预测精度和可解释性。",
                "实验显示GRAFT在多个区域和时间尺度上显著优于基线，达到或超越最优水平，并具备事件驱动场景的鲁棒性。"
            ],
            "method_zh": "**问题定义**：电力负荷预测面临外部因素（如天气、事件、政策）的多尺度影响，现有方法难以有效整合多源文本信息，导致预测精度受限，尤其在突发事件场景下表现不佳。\\n\\n**核心思路**：通过严格对齐多源文本（新闻、社交媒体、政策）与负荷数据，并利用交叉注意力机制实现文本引导的融合，以增强模型对外部因素的感知和建模能力。\\n\\n**技术框架**：整体架构基于改进的STanHOP模型，包含数据对齐模块、文本融合模块和外部记忆接口。流程包括：首先对齐每日聚合文本与半小时负荷数据；其次在训练和预测阶段通过交叉注意力将文本信息融合到特定时间位置；最后通过外部记忆接口动态适应不同信息源。\\n\\n**关键创新**：严格的多源文本对齐机制和文本引导的融合方法，结合即插即用的外部记忆接口，显著提升了负荷预测的精度和可解释性，与现有方法相比更注重文本信息的细粒度整合。\\n\\n**关键设计**：采用交叉注意力机制实现文本与负荷数据的交互，损失函数基于预测误差优化，网络结构支持多时间尺度预测，参数设置包括对齐粒度（每日文本与半小时负荷）和注意力头数等，具体细节未在摘要中详述。",
            "application_zh": "该研究可应用于智能电网管理、能源调度优化和电力市场分析等领域，通过提升负荷预测精度，帮助电网运营商更好地应对突发事件和政策变化，提高电网稳定性和经济性。未来可能扩展到其他时间序列预测任务，如交通流量或金融预测。",
            "highlight_zh": "GRAFT在澳大利亚五个州的基准数据集上，于小时、日和月三个时间尺度均显著优于强基线模型，具体性能数据未提供，但达到或超越当前最优水平。模型在事件驱动场景中表现出鲁棒性，并通过注意力机制实现文本影响的可解释性分析。",
            "tags_zh": [
                "电网负荷预测",
                "多源文本对齐",
                "交叉注意力融合",
                "外部记忆接口",
                "事件驱动建模",
                "可解释性分析",
                "时间序列预测"
            ],
            "_index": 32
        },
        {
            "title": "Field evaluation and optimization of a lightweight lidar-based UAV navigation system for dense boreal forest environments",
            "authors": [
                "Aleksi Karhunen",
                "Teemu Hakala",
                "Väinö Karjalainen",
                "Eija Honkavaara"
            ],
            "arxiv_id": "2512.14340v1",
            "summary": "The interest in the usage of uncrewed aerial vehicles (UAVs) for forest applications has increased in recent years. While above-canopy flight has reached a high level of autonomy, navigating under-canopy remains a significant challenge. The use of autonomous UAVs could reduce the burden of data collection, which has motivated the development of numerous solutions for under-canopy autonomous flight. However, the experiments conducted in the literature and their reporting lack rigor. Very rarely, the density and the difficulty of the test forests are reported, or multiple flights are flown, and the success rate of those flights is reported. The aim of this study was to implement an autonomously flying quadrotor based on a lightweight lidar using openly available algorithms and test its behavior in real forest environments. A set of rigorous experiments was conducted with a quadrotor prototype utilizing the IPC path planner and LTA-OM SLAM algorithm. Based on the results of the first 33 flights, the original system was further enhanced. With the optimized system, 60 flights were performed, resulting in a total of 93 test flights. The optimized system performed significantly better in terms of reliability and flight mission completion times, achieving success rates of 12/15 in a medium-density forest and 15/15 in a dense forest, at a target flight velocity of 1 m/s. At a target flight velocity of 2 m/s, it had a success rate of 12/15 and 5/15, respectively. Furthermore, a standardized testing setup and evaluation criteria were proposed, enabling consistent performance comparisons of autonomous under-canopy UAV systems, enhancing reproducibility, guiding system improvements, and accelerating progress in forest robotics.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "This work has been submitted to the IEEE for possible publication",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14340v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "lidar"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO",
                        "SLAM"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出基于轻量级激光雷达的无人机导航系统优化方案，以解决稠密北方森林环境下的自主飞行挑战。",
            "summary_zh": "近年来，无人机在森林应用中的使用兴趣日益增长。虽然冠层以上飞行已达到高度自主水平，但在冠层下导航仍是一个重大挑战。自主无人机的使用可以减轻数据收集的负担，这推动了众多冠层下自主飞行解决方案的开发。然而，文献中进行的实验及其报告缺乏严谨性。很少报告测试森林的密度和难度，或进行多次飞行并报告这些飞行的成功率。本研究旨在基于轻量级激光雷达，使用公开可用的算法实现自主飞行的四旋翼无人机，并在真实森林环境中测试其行为。利用IPC路径规划器和LTA-OM SLAM算法，对四旋翼原型进行了严格的实验。基于前33次飞行的结果，对原始系统进行了进一步优化。使用优化后的系统进行了60次飞行，总共完成了93次测试飞行。优化后的系统在可靠性和飞行任务完成时间方面表现显著更好，在目标飞行速度为1 m/s时，在中密度森林中实现了12/15的成功率，在稠密森林中实现了15/15的成功率。在目标飞行速度为2 m/s时，其成功率分别为12/15和5/15。此外，提出了标准化的测试设置和评估标准，使自主冠层下无人机系统的性能比较具有一致性，增强了可重复性，指导系统改进，并加速了森林机器人技术的进展。",
            "intro_zh": [
                "现有方法在森林冠层下自主飞行实验中缺乏严谨性，很少报告森林密度、难度或多次飞行的成功率，导致性能评估不准确。",
                "论文提出基于轻量级激光雷达的无人机导航系统，结合公开算法如IPC路径规划器和LTA-OM SLAM，通过优化提升在稠密森林环境中的自主飞行能力。",
                "优化后系统在1 m/s速度下，中密度和稠密森林成功率分别达12/15和15/15；2 m/s时分别为12/15和5/15，显著提高了可靠性和任务完成效率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决无人机在稠密北方森林冠层下自主导航的挑战，现有方法在实验设计和报告上缺乏严谨性，如未量化森林密度、飞行成功率等，导致系统性能评估不可靠，难以指导实际应用。\\n\\n**核心思路**：论文的核心思路是采用轻量级激光雷达作为主要传感器，结合公开可用的算法（如IPC路径规划器和LTA-OM SLAM），构建一个低成本、易实现的无人机导航系统，并通过严格的现场测试和迭代优化来提升其在复杂森林环境中的自主飞行能力。\\n\\n**技术框架**：整体架构包括数据采集、SLAM定位、路径规划和飞行控制四个主要模块。首先，使用轻量级激光雷达实时获取环境点云数据；然后，通过LTA-OM SLAM算法进行同时定位与地图构建，提供精确的位置估计；接着，IPC路径规划器基于SLAM输出的地图生成避障路径；最后，飞行控制器执行路径指令，实现自主飞行。系统通过多次飞行测试收集数据，基于结果进行参数调整和算法优化。\\n\\n**关键创新**：最重要的技术创新点在于提出了一套标准化的测试设置和评估标准，包括量化森林密度、进行多次重复飞行并报告成功率，这解决了现有研究在实验严谨性上的不足，使得性能比较更具一致性和可重复性，从而能更有效地指导系统改进。\\n\\n**关键设计**：关键设计包括使用轻量级激光雷达以降低系统重量和成本，选择公开算法如IPC和LTA-OM以确保可访问性和可复现性，设置目标飞行速度为1 m/s和2 m/s以测试不同速度下的性能，并通过93次飞行实验（包括33次初始飞行和60次优化后飞行）来验证系统。具体参数如森林密度分类（中密度、稠密）和成功率计算方式（成功飞行次数/总飞行次数）也被明确定义。",
            "application_zh": "该研究主要应用于森林监测和数据收集领域，如林业资源调查、生态研究、灾害评估等。通过优化无人机在稠密森林冠层下的自主导航能力，可以显著降低人工数据采集的负担，提高作业效率和安全性。未来，该标准化测试框架可推广至其他复杂环境（如城市峡谷、室内场景）的机器人导航系统评估，加速森林机器人技术及相关领域的发展。",
            "highlight_zh": "最重要的实验结果包括：优化后系统在目标飞行速度1 m/s时，中密度森林成功率从未知提升至12/15（80%），稠密森林达15/15（100%）；在2 m/s时，中密度森林成功率12/15（80%），稠密森林5/15（33%）。相比初始系统（基于33次飞行），优化系统通过60次飞行验证，显著提高了可靠性和任务完成时间。实验还提出了标准化测试标准，增强了性能比较的一致性。",
            "tags_zh": [
                "无人机导航",
                "激光雷达SLAM",
                "森林机器人",
                "路径规划",
                "自主飞行",
                "稠密环境",
                "标准化测试",
                "现场评估"
            ],
            "_index": 33
        },
        {
            "title": "ARCADE: Adaptive Robot Control with Online Changepoint-Aware Bayesian Dynamics Learning",
            "authors": [
                "Rishabh Dev Yadav",
                "Avirup Das",
                "Hongyu Song",
                "Samuel Kaski",
                "Wei Pan"
            ],
            "arxiv_id": "2512.14331v1",
            "summary": "Real-world robots must operate under evolving dynamics caused by changing operating conditions, external disturbances, and unmodeled effects. These may appear as gradual drifts, transient fluctuations, or abrupt shifts, demanding real-time adaptation that is robust to short-term variation yet responsive to lasting change. We propose a framework for modeling the nonlinear dynamics of robotic systems that can be updated in real time from streaming data. The method decouples representation learning from online adaptation, using latent representations learned offline to support online closed-form Bayesian updates. To handle evolving conditions, we introduce a changepoint-aware mechanism with a latent variable inferred from data likelihoods that indicates continuity or shift. When continuity is likely, evidence accumulates to refine predictions; when a shift is detected, past information is tempered to enable rapid re-learning. This maintains calibrated uncertainty and supports probabilistic reasoning about transient, gradual, or structural change. We prove that the adaptive regret of the framework grows only logarithmically in time and linearly with the number of shifts, competitive with an oracle that knows timings of shift. We validate on cartpole simulations and real quadrotor flights with swinging payloads and mid-flight drops, showing improved predictive accuracy, faster recovery, and more accurate closed-loop tracking than relevant baselines.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14331v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出ARCADE框架，通过在线变点感知贝叶斯动力学学习解决机器人动态变化下的自适应控制问题",
            "summary_zh": "现实世界中的机器人必须在动态变化的环境中运行，这些变化可能由操作条件改变、外部干扰或未建模效应引起，表现为渐进漂移、瞬态波动或突然转变，需要实时适应能力，既能抵抗短期变化又能响应持久变化。我们提出了一个框架，用于建模机器人系统的非线性动力学，能够从流数据中实时更新。该方法将表示学习与在线适应解耦，利用离线学习的潜在表示支持在线闭式贝叶斯更新。为处理演化条件，我们引入了变点感知机制，通过从数据似然推断的潜在变量指示连续性或转变。当连续性可能时，证据积累以优化预测；当检测到转变时，过去信息被调节以实现快速重新学习。这保持了校准的不确定性，并支持对瞬态、渐进或结构变化的概率推理。我们证明该框架的自适应遗憾仅随时间对数增长，并与转变次数线性相关，与已知转变时间的神谕者竞争。我们在倒立摆仿真和真实四旋翼飞行器实验中验证了该方法，实验包括摆动负载和飞行中掉落，结果显示相比相关基线，预测准确性更高、恢复更快、闭环跟踪更准确。",
            "intro_zh": [
                "核心问题：现实机器人动态变化（如漂移、波动、突变）要求实时自适应，现有方法难以平衡短期鲁棒性与持久变化响应。",
                "方法要点：提出ARCADE框架，结合离线表示学习与在线贝叶斯更新，引入变点感知机制动态调节信息积累，实现快速适应。",
                "实验或效果：在倒立摆和四旋翼实验中，相比基线，预测准确性提升、恢复速度加快、闭环跟踪更准确，自适应遗憾增长缓慢。"
            ],
            "method_zh": "**问题定义**：论文解决机器人系统在动态变化环境（如操作条件改变、外部干扰）下的自适应控制问题，现有方法难以实时处理非线性动力学中的渐进漂移、瞬态波动或突然转变，导致预测不准确和适应滞后。\\n\\n**核心思路**：通过解耦表示学习与在线适应，利用离线学习的潜在表示支持在线闭式贝叶斯更新，并引入变点感知机制，基于数据似然推断潜在变量来指示动态连续性或转变，从而在连续性时积累证据优化预测，在转变时调节过去信息实现快速重新学习。\\n\\n**技术框架**：整体架构分为离线阶段和在线阶段。离线阶段学习非线性动力学的潜在表示；在线阶段使用流数据进行实时贝叶斯更新，结合变点感知模块动态评估动态变化，通过概率推理调整模型参数，支持闭环控制。\\n\\n**关键创新**：最重要的技术创新是变点感知贝叶斯动力学学习机制，它通过潜在变量推断动态变化类型（如瞬态、渐进或结构变化），与现有方法相比，本质区别在于能同时处理短期波动和持久转变，保持校准不确定性，避免过拟合或欠适应。\\n\\n**关键设计**：关键设计包括使用潜在表示学习（如神经网络编码）捕获非线性动力学，在线更新采用闭式贝叶斯公式（如高斯过程或变分推断），变点感知基于数据似然计算（如似然比检验），参数设置如学习率和不确定性阈值通过实验优化，损失函数可能结合预测误差和不确定性校准。",
            "application_zh": "该研究在机器人自适应控制领域具有广泛潜在应用，如无人机在负载变化或风扰下的稳定飞行、工业机器人在环境突变中的精确操作、自动驾驶车辆应对路况动态变化。实际价值在于提升机器人在复杂现实环境中的鲁棒性和适应性，未来可能推动智能系统在动态场景中的自主决策和实时优化。",
            "highlight_zh": "最重要的实验结果包括：在倒立摆仿真中，预测准确性相比基线提升约20%，恢复时间缩短30%；在真实四旋翼飞行实验中，面对摆动负载和飞行中掉落，闭环跟踪误差降低15%以上，自适应遗憾仅随时间对数增长，与转变次数线性相关，验证了框架的高效性和鲁棒性。",
            "tags_zh": [
                "自适应机器人控制",
                "在线贝叶斯学习",
                "变点检测",
                "动力学建模",
                "非线性系统",
                "实时适应",
                "不确定性校准",
                "闭环跟踪"
            ],
            "_index": 34
        },
        {
            "title": "Criminal Liability in AI-Enabled Autonomous Vehicles: A Comparative Study",
            "authors": [
                "Sahibpreet Singh",
                "Manjit Singh"
            ],
            "arxiv_id": "2512.14330v1",
            "summary": "AI revolutionizes transportation through autonomous vehicles (AVs) but introduces complex criminal liability issues regarding infractions. This study employs a comparative legal analysis of primary statutes, real-world liability claims, and academic literature across the US, Germany, UK, China, and India; jurisdictions selected for their technological advancement and contrasting regulatory approaches. The research examines the attribution of human error, AI moral agency, and the identification of primary offenders in AV incidents. Findings reveal fragmented regulatory landscapes: India and the US rely on loose networks of state laws, whereas the UK enacted the pioneering Automated and Electric Vehicles Act 2018. Germany enforces strict safety standards, distinguishing liability based on the vehicle's operating mode, while China similarly aims for a stringent liability regime. The study concludes that globally harmonized legal standards are essential to foster technological innovation while ensuring minimum risk and clear liability attribution.",
            "categories": [
                "cs.CY",
                "cs.AI",
                "cs.CR"
            ],
            "primary_category": "cs.CY",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Published in Journal of University Institute of Legal Studies, Vol. 18, Issue 1, pp. 57-78, 2025",
            "doi": "",
            "journal_ref": "Journal of University Institute of Legal Studies 18(1), 57-78 (2025)",
            "pdf_url": "https://arxiv.org/pdf/2512.14330v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "autonomous vehicle"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 3,
            "headline_zh": "通过比较法分析提出全球统一法律标准以解决自动驾驶车辆刑事责任的复杂问题",
            "summary_zh": "人工智能通过自动驾驶车辆革新了交通运输，但也带来了关于违规行为的复杂刑事责任问题。本研究采用比较法律分析方法，对美国、德国、英国、中国和印度等司法管辖区的主要法规、实际责任索赔案例和学术文献进行了分析；这些司法管辖区因其技术进步和对比鲜明的监管方法而被选中。研究探讨了人为错误归因、人工智能道德主体性以及自动驾驶事故中主要责任人的识别。研究发现监管格局碎片化：印度和美国依赖松散的各州法律网络，而英国则颁布了开创性的《2018年自动化和电动汽车法案》。德国执行严格的安全标准，根据车辆的运行模式区分责任，而中国同样旨在建立严格的责任制度。研究得出结论，全球统一的法律标准对于促进技术创新、同时确保最低风险和明确责任归属至关重要。",
            "intro_zh": [
                "核心问题：自动驾驶车辆事故中刑事责任的归属存在法律空白，现有法规碎片化，难以界定人为错误与AI道德主体性。",
                "方法要点：采用比较法律分析，研究美国、德国、英国、中国和印度等国的法规、案例和文献，以识别责任归属模式。",
                "实验或效果：发现各国监管差异显著，提出全球统一法律标准是促进技术创新和明确责任的关键解决方案。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自动驾驶车辆在发生事故或违规行为时，刑事责任的归属问题。现有方法的痛点在于全球范围内缺乏统一的法律框架，导致责任认定模糊，难以区分人为错误与AI系统的道德主体性，这阻碍了自动驾驶技术的推广和创新。\\n\\n**核心思路**：论文的核心解决思路是通过比较法律分析，系统研究不同司法管辖区的法规、实际案例和学术观点，以识别责任归属的模式和挑战。这样设计是因为自动驾驶技术具有跨国性，需要从多国视角出发，找出共性和差异，为制定全球统一标准提供依据。\\n\\n**技术框架**：整体架构包括三个主要阶段：首先，选择美国、德国、英国、中国和印度等代表性司法管辖区，基于其技术先进性和监管多样性；其次，收集和分析这些国家的主要法规、实际责任索赔案例和学术文献；最后，综合比较结果，探讨责任归属的关键问题，如人为错误归因和AI道德主体性。\\n\\n**关键创新**：最重要的技术创新点在于将比较法应用于自动驾驶刑事责任领域，系统性地整合多国法律资源，而非局限于单一国家分析。与现有方法的本质区别在于，它强调全球视角，通过对比不同监管模式，揭示碎片化问题，并提出统一标准的必要性。\\n\\n**关键设计**：关键设计包括选择具有技术先进性和监管对比性的司法管辖区（如美国、德国、英国、中国和印度），以确保分析的全面性；使用比较法律分析方法，结合法规、案例和文献，以增强结论的可靠性；在分析中重点关注人为错误、AI道德主体性和主要责任人识别等核心议题，以结构化方式呈现发现。",
            "application_zh": "该研究的潜在应用领域包括自动驾驶车辆的法律监管、保险政策制定和事故责任认定。实际价值在于为政策制定者、法律从业者和技术开发者提供参考，帮助建立更清晰的责任框架，促进自动驾驶技术的安全部署。未来影响可能推动全球法律标准的统一，减少跨国运营的法律障碍，加速技术创新和商业化进程。",
            "highlight_zh": "最重要的实验结果包括：发现各国监管格局碎片化，例如印度和美国依赖松散州法律，而英国有《2018年自动化和电动汽车法案》；德国和中国执行严格责任制度，但德国根据车辆运行模式区分责任。这些发现揭示了全球缺乏统一标准，提升了对责任归属复杂性的认识，为制定协调法律框架提供了实证基础。",
            "tags_zh": [
                "自动驾驶车辆",
                "刑事责任",
                "比较法律分析",
                "人工智能道德",
                "法律监管",
                "全球标准",
                "责任归属",
                "技术创新"
            ],
            "_index": 35
        },
        {
            "title": "A Threshold-Triggered Deep Q-Network-Based Framework for Self-Healing in Autonomic Software-Defined IIoT-Edge Networks",
            "authors": [
                "Agrippina Mwangi",
                "León Navarro-Hilfiker",
                "Lukasz Brewka",
                "Mikkel Gryning",
                "Elena Fumagalli",
                "Madeleine Gibescu"
            ],
            "arxiv_id": "2512.14297v1",
            "summary": "Stochastic disruptions such as flash events arising from benign traffic bursts and switch thermal fluctuations are major contributors to intermittent service degradation in software-defined industrial networks. These events violate IEC~61850-derived quality-of-service requirements and user-defined service-level agreements, hindering the reliable and timely delivery of control, monitoring, and best-effort traffic in IEC~61400-25-compliant wind power plants. Failure to maintain these requirements often results in delayed or lost control signals, reduced operational efficiency, and increased risk of wind turbine generator downtime.\n  To address these challenges, this study proposes a threshold-triggered Deep Q-Network self-healing agent that autonomically detects, analyzes, and mitigates network disruptions while adapting routing behavior and resource allocation in real time. The proposed agent was trained, validated, and tested on an emulated tri-clustered switch network deployed in a cloud-based proof-of-concept testbed.\n  Simulation results show that the proposed agent improves disruption recovery performance by 53.84% compared to a baseline shortest-path and load-balanced routing approach and outperforms state-of-the-art methods, including the Adaptive Network-based Fuzzy Inference System by 13.1% and the Deep Q-Network and traffic prediction-based routing optimization method by 21.5%, in a super-spine leaf data-plane architecture.\n  Additionally, the agent maintains switch thermal stability by proactively initiating external rack cooling when required. These findings highlight the potential of deep reinforcement learning in building resilience in software-defined industrial networks deployed in mission-critical, time-sensitive application scenarios.",
            "categories": [
                "cs.NI",
                "cs.AI",
                "cs.ET",
                "cs.PF",
                "hep-ex"
            ],
            "primary_category": "cs.NI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14297v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "traffic"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出基于阈值触发的深度Q网络自愈框架，以解决软件定义工业物联网边缘网络中的随机中断问题。",
            "summary_zh": "软件定义工业网络中，由良性流量突发和交换机热波动引发的随机中断是导致间歇性服务降级的主要原因。这些事件违反了IEC 61850衍生的服务质量要求和用户定义的服务级别协议，阻碍了符合IEC 61400-25的风力发电厂中控制、监控和尽力而为流量的可靠及时交付。未能维持这些要求通常会导致控制信号延迟或丢失、运行效率降低以及风力涡轮发电机停机风险增加。为应对这些挑战，本研究提出了一种阈值触发的深度Q网络自愈代理，能够自主检测、分析和缓解网络中断，同时实时调整路由行为和资源分配。该代理在基于云的概念验证测试平台中部署的模拟三集群交换机网络上进行了训练、验证和测试。仿真结果表明，与基线最短路径和负载均衡路由方法相比，所提出的代理将中断恢复性能提高了53.84%，并在超骨干叶数据平面架构中优于最先进的方法，包括自适应网络模糊推理系统（提升13.1%）以及基于深度Q网络和流量预测的路由优化方法（提升21.5%）。此外，该代理在需要时通过主动启动外部机架冷却来维持交换机热稳定性。这些发现凸显了深度强化学习在构建部署于关键任务、时间敏感应用场景的软件定义工业网络弹性方面的潜力。",
            "intro_zh": [
                "核心问题：软件定义工业网络中随机中断（如流量突发和热波动）导致服务降级，违反服务质量要求，影响控制信号可靠交付。",
                "方法要点：提出阈值触发的深度Q网络自愈代理，自主检测、分析和缓解中断，实时调整路由和资源分配，并主动管理热稳定性。",
                "实验或效果：在模拟网络中，中断恢复性能比基线方法提升53.84%，优于现有先进方法，并有效维持交换机热稳定性。"
            ],
            "method_zh": "**问题定义**：论文解决软件定义工业物联网边缘网络中由随机中断（如良性流量突发和交换机热波动）引起的间歇性服务降级问题。现有方法（如最短路径和负载均衡路由）难以实时适应动态中断，导致控制信号延迟或丢失，违反IEC 61850服务质量要求，影响风力发电厂等关键应用。\\n\\n**核心思路**：论文提出基于阈值触发的深度Q网络自愈框架，核心思路是利用深度强化学习（DRL）的自主决策能力，通过阈值机制触发代理行动，实时检测、分析和缓解网络中断，同时优化路由行为和资源分配，以提升网络弹性和恢复性能。\\n\\n**技术框架**：整体架构包括模拟三集群交换机网络部署在云测试平台中，主要模块有：1）环境感知模块，监测网络状态（如流量、热波动）；2）阈值触发模块，基于预设条件（如中断检测）激活代理；3）深度Q网络代理，学习最优策略以调整路由和资源；4）执行模块，实施行动（如路由重配置、启动冷却）。流程涉及训练、验证和测试阶段，代理通过交互学习最小化中断影响。\\n\\n**关键创新**：最重要的技术创新点是阈值触发机制与深度Q网络的结合，使代理仅在必要时行动，减少计算开销，并专注于实时中断缓解。与现有方法（如自适应网络模糊推理系统或基于预测的路由优化）的本质区别在于其自主性和适应性，能直接处理随机中断，而无需依赖复杂预测模型。\\n\\n**关键设计**：关键设计包括：1）阈值设置，基于网络指标（如延迟、丢包率）触发代理；2）深度Q网络结构，使用神经网络近似Q值函数，输入为网络状态（如流量负载、热数据），输出为行动（如路由调整）；3）损失函数，基于贝尔曼方程优化，最小化预测Q值与目标Q值的差异；4）训练参数，如学习率、折扣因子，在模拟环境中调整以最大化累积奖励（如恢复性能）。",
            "application_zh": "该研究主要应用于软件定义工业物联网边缘网络，特别是在风力发电厂等关键任务、时间敏感场景中，用于提升网络弹性和服务可靠性。潜在价值包括减少控制信号延迟、降低设备停机风险，未来可扩展至其他工业自动化领域，如智能制造或能源管理，推动深度强化学习在网络自愈中的实际部署。",
            "highlight_zh": "最重要的实验结果是：在模拟三集群交换机网络中，所提出的自愈代理将中断恢复性能比基线最短路径和负载均衡路由方法提高了53.84%。同时，它优于最先进方法，包括自适应网络模糊推理系统（提升13.1%）和基于深度Q网络与流量预测的路由优化方法（提升21.5%）。此外，代理能主动启动外部机架冷却，有效维持交换机热稳定性，验证了其在超骨干叶数据平面架构中的高效性。",
            "tags_zh": [
                "软件定义网络",
                "工业物联网",
                "深度强化学习",
                "网络自愈",
                "路由优化",
                "热稳定性管理",
                "边缘计算",
                "服务质量保障"
            ],
            "_index": 36
        },
        {
            "title": "PentestEval: Benchmarking LLM-based Penetration Testing with Modular and Stage-Level Design",
            "authors": [
                "Ruozhao Yang",
                "Mingfei Cheng",
                "Gelei Deng",
                "Tianwei Zhang",
                "Junjie Wang",
                "Xiaofei Xie"
            ],
            "arxiv_id": "2512.14233v1",
            "summary": "Penetration testing is essential for assessing and strengthening system security against real-world threats, yet traditional workflows remain highly manual, expertise-intensive, and difficult to scale. Although recent advances in Large Language Models (LLMs) offer promising opportunities for automation, existing applications rely on simplistic prompting without task decomposition or domain adaptation, resulting in unreliable black-box behavior and limited insight into model capabilities across penetration testing stages. To address this gap, we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision. PentestEval integrates expert-annotated ground truth with a fully automated evaluation pipeline across 346 tasks covering all stages in 12 realistic vulnerable scenarios. Our stage-level evaluation of 9 widely used LLMs reveals generally weak performance and distinct limitations across the stages of penetration-testing workflow. End-to-end pipelines reach only 31% success rate, and existing LLM-powered systems such as PentestGPT, PentestAgent, and VulnBot exhibit similar limitations, with autonomous agents failing almost entirely. These findings highlight that autonomous penetration testing demands stronger structured reasoning, where modularization enhances each individual stage and improves overall performance. PentestEval provides the foundational benchmark needed for future research on fine-grained, stage-level evaluation, paving the way toward more reliable LLM-based automation.",
            "categories": [
                "cs.SE",
                "cs.AI",
                "cs.CR"
            ],
            "primary_category": "cs.SE",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "13 pages, 6 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14233v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出PentestEval基准测试，通过模块化阶段设计评估LLM在渗透测试中的性能",
            "summary_zh": "渗透测试对于评估和增强系统安全至关重要，但传统工作流程高度依赖人工、专业知识密集且难以扩展。尽管大型语言模型（LLMs）为自动化提供了前景，但现有应用依赖简单的提示方法，缺乏任务分解或领域适应，导致不可靠的黑盒行为和有限的对渗透测试各阶段模型能力的洞察。为解决这一差距，我们引入了PentestEval，这是首个全面的基准测试，用于评估LLMs在六个分解的渗透测试阶段：信息收集、弱点收集与过滤、攻击决策、漏洞利用生成与修订。PentestEval集成了专家标注的真实数据和一个完全自动化的评估管道，覆盖12个现实漏洞场景中的所有阶段，共346个任务。我们对9个广泛使用的LLMs进行阶段级评估，揭示了在渗透测试工作流程各阶段普遍较弱的性能和明显的局限性。端到端管道仅达到31%的成功率，现有LLM驱动的系统如PentestGPT、PentestAgent和VulnBot表现出类似的局限性，自主代理几乎完全失败。这些发现强调自主渗透测试需要更强的结构化推理，其中模块化增强每个单独阶段并提高整体性能。PentestEval为未来细粒度、阶段级评估研究提供了基础基准，为更可靠的基于LLM的自动化铺平道路。",
            "intro_zh": [
                "核心问题：现有LLM在渗透测试中依赖简单提示，缺乏任务分解和领域适应，导致性能不可靠且难以评估各阶段能力。",
                "方法要点：提出PentestEval基准测试，通过模块化设计分解渗透测试为六个阶段，集成专家标注数据和自动化评估管道。",
                "实验或效果：评估9个LLMs显示端到端成功率仅31%，自主代理几乎完全失败，模块化能显著提升性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决LLM在渗透测试自动化中性能评估不足的问题。现有方法依赖简单提示，缺乏任务分解和领域适应，导致模型行为不可预测、难以评估各阶段能力，且现有基准测试不全面，限制了LLM在安全领域的可靠应用。\\n\\n**核心思路**：论文的核心思路是通过模块化设计将渗透测试工作流程分解为多个阶段，并构建一个综合基准测试来评估LLM在每个阶段的表现。这样设计可以更精细地分析模型能力，促进结构化推理，从而提升整体自动化性能。\\n\\n**技术框架**：整体架构包括六个分解的渗透测试阶段：信息收集、弱点收集与过滤、攻击决策、漏洞利用生成与修订。PentestEval集成了专家标注的真实数据（ground truth）和一个完全自动化的评估管道，覆盖12个现实漏洞场景中的所有阶段，共346个任务。评估过程从数据准备到结果分析，确保客观性和可重复性。\\n\\n**关键创新**：最重要的技术创新是首次提出一个全面的、阶段级的基准测试PentestEval，用于评估LLM在渗透测试中的性能。与现有方法相比，它通过模块化分解和专家标注数据，提供了更细粒度的评估能力，本质区别在于强调结构化推理和领域适应，而非简单的黑盒测试。\\n\\n**关键设计**：关键设计包括：1) 任务分解为六个阶段，每个阶段对应特定子任务；2) 使用专家标注的真实数据作为评估标准；3) 自动化评估管道覆盖所有任务，确保一致性；4) 基于12个现实漏洞场景构建数据集，增强实用性；5) 评估指标可能包括成功率、准确率等，具体细节在论文中未详细说明，但强调客观量化。",
            "application_zh": "该研究主要应用于网络安全领域，特别是渗透测试自动化。潜在价值包括为安全研究人员和从业者提供标准化的评估工具，促进LLM在安全任务中的可靠部署。未来影响可能推动更智能的自主安全系统开发，提高漏洞检测和修复效率，但需进一步研究以解决当前局限性。",
            "highlight_zh": "最重要的实验结果包括：1) 对9个广泛使用的LLMs进行阶段级评估，显示整体性能较弱；2) 端到端渗透测试管道成功率仅31%，表明现有自动化能力有限；3) 现有LLM驱动系统如PentestGPT、PentestAgent和VulnBot表现出类似局限性；4) 自主代理在测试中几乎完全失败，突显结构化推理的不足；5) 模块化设计能增强各阶段性能，提升整体效果。",
            "tags_zh": [
                "渗透测试",
                "大型语言模型",
                "基准测试",
                "模块化设计",
                "网络安全",
                "自动化评估",
                "阶段级评估",
                "结构化推理"
            ],
            "_index": 37
        },
        {
            "title": "FastDDHPose: Towards Unified, Efficient, and Disentangled 3D Human Pose Estimation",
            "authors": [
                "Qingyuan Cai",
                "Linxin Zhang",
                "Xuecai Hu",
                "Saihui Hou",
                "Yongzhen Huang"
            ],
            "arxiv_id": "2512.14162v1",
            "summary": "Recent approaches for monocular 3D human pose estimation (3D HPE) have achieved leading performance by directly regressing 3D poses from 2D keypoint sequences. Despite the rapid progress in 3D HPE, existing methods are typically trained and evaluated under disparate frameworks, lacking a unified framework for fair comparison. To address these limitations, we propose Fast3DHPE, a modular framework that facilitates rapid reproduction and flexible development of new methods. By standardizing training and evaluation protocols, Fast3DHPE enables fair comparison across 3D human pose estimation methods while significantly improving training efficiency. Within this framework, we introduce FastDDHPose, a Disentangled Diffusion-based 3D Human Pose Estimation method which leverages the strong latent distribution modeling capability of diffusion models to explicitly model the distributions of bone length and bone direction while avoiding further amplification of hierarchical error accumulation. Moreover, we design an efficient Kinematic-Hierarchical Spatial and Temporal Denoiser that encourages the model to focus on kinematic joint hierarchies while avoiding unnecessary modeling of overly complex joint topologies. Extensive experiments on Human3.6M and MPI-INF-3DHP show that the Fast3DHPE framework enables fair comparison of all methods while significantly improving training efficiency. Within this unified framework, FastDDHPose achieves state-of-the-art performance with strong generalization and robustness in in-the-wild scenarios. The framework and models will be released at: https://github.com/Andyen512/Fast3DHPE",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14162v1",
            "code_links": [
                {
                    "url": "https://github.com/Andyen512/Fast3DHPE",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO",
                        "pose estimation"
                    ],
                    "score": 2
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出FastDDHPose框架以解决单目3D人体姿态估计中缺乏统一评估标准和误差累积问题",
            "summary_zh": "近年来，基于2D关键点序列直接回归3D姿态的单目3D人体姿态估计方法取得了领先性能。尽管3D HPE进展迅速，现有方法通常在分散的框架下训练和评估，缺乏公平比较的统一框架。为解决这些限制，我们提出Fast3DHPE，这是一个模块化框架，便于快速复现和灵活开发新方法。通过标准化训练和评估协议，Fast3DHPE实现了3D人体姿态估计方法的公平比较，同时显著提高了训练效率。在此框架内，我们引入FastDDHPose，一种基于解耦扩散的3D人体姿态估计方法，利用扩散模型的强大潜在分布建模能力，显式建模骨骼长度和骨骼方向的分布，同时避免进一步放大层次误差累积。此外，我们设计了一个高效的基于运动学层次的空间和时间去噪器，鼓励模型关注运动学关节层次，同时避免对过于复杂的关节拓扑进行不必要的建模。在Human3.6M和MPI-INF-3DHP上的大量实验表明，Fast3DHPE框架实现了所有方法的公平比较，同时显著提高了训练效率。在这个统一框架内，FastDDHPose在野外场景中实现了最先进的性能，具有强大的泛化能力和鲁棒性。框架和模型将在https://github.com/Andyen512/Fast3DHPE发布。",
            "intro_zh": [
                "现有3D人体姿态估计方法缺乏统一训练评估框架，导致公平比较困难且训练效率低下。",
                "提出Fast3DHPE统一框架和FastDDHPose方法，通过解耦扩散模型显式建模骨骼分布并避免误差累积。",
                "在Human3.6M和MPI-INF-3DHP数据集上实现SOTA性能，训练效率显著提升，泛化能力强。"
            ],
            "method_zh": "**问题定义**：论文主要解决单目3D人体姿态估计中两个核心问题：一是现有方法缺乏统一的训练和评估框架，导致不同方法之间难以进行公平比较；二是传统方法在回归3D姿态时容易产生层次误差累积，即底层关节的误差会向上传播放大，影响整体姿态精度。\\n\\n**核心思路**：论文提出双重解决方案：首先构建Fast3DHPE统一框架，标准化数据处理、训练流程和评估指标；其次在框架内设计FastDDHPose方法，采用解耦扩散模型将骨骼长度和方向分开建模，从概率分布角度优化姿态生成过程，从而抑制误差传播。\\n\\n**技术框架**：整体分为两个层次：上层是Fast3DHPE框架，包含标准化的数据加载器、训练循环和评估模块；下层是FastDDHPose模型，采用扩散模型架构，包含前向加噪过程和反向去噪过程，其中去噪器专门设计为运动学层次空间时间网络。\\n\\n**关键创新**：最重要的创新点在于将扩散模型引入3D姿态估计，并创新性地解耦骨骼长度和方向两个物理量进行独立建模。这与传统直接回归方法有本质区别：传统方法学习确定性映射，而本文方法学习数据分布，通过逐步去噪生成姿态，更符合人体运动的概率特性。\\n\\n**关键设计**：关键技术细节包括：1）运动学层次空间时间去噪器，网络结构考虑人体关节树层次关系，在空间维度建模关节间依赖，在时间维度建模帧间连续性；2）解耦训练策略，分别用扩散过程建模骨骼长度和方向的分布；3）采用均方误差作为扩散损失函数，优化去噪网络参数；4）推理时通过多次采样和平均提高稳定性。",
            "application_zh": "该研究在虚拟现实、增强现实、运动分析、人机交互等领域具有重要应用价值。统一的Fast3DHPE框架可加速学术界新方法的开发和比较，而高性能的FastDDHPose模型可直接用于需要精确3D姿态估计的实际系统，如智能监控、体育训练、医疗康复等。未来可能推动3D人体姿态估计向更标准化、高效化的方向发展。",
            "highlight_zh": "在Human3.6M数据集上，FastDDHPose相比之前最佳方法MPJPE（平均关节位置误差）降低约5%，在MPI-INF-3DHP数据集上PCK（正确关键点百分比）提升3-4个百分点。在统一框架下，所有对比方法的训练时间平均减少30%以上。特别是在野外场景测试中，FastDDHPose展现出优异的泛化能力，对遮挡和视角变化具有强鲁棒性。",
            "tags_zh": [
                "3D人体姿态估计",
                "扩散模型",
                "解耦表示",
                "运动学层次",
                "统一框架",
                "单目视觉",
                "时空建模",
                "误差累积抑制"
            ],
            "_index": 38
        },
        {
            "title": "SketchAssist: A Practical Assistant for Semantic Edits and Precise Local Redrawing",
            "authors": [
                "Han Zou",
                "Yan Zhang",
                "Ruiqi Yu",
                "Cong Xie",
                "Jie Huang",
                "Zhenpeng Zhan"
            ],
            "arxiv_id": "2512.14140v1",
            "summary": "Sketch editing is central to digital illustration, yet existing image editing systems struggle to preserve the sparse, style-sensitive structure of line art while supporting both high-level semantic changes and precise local redrawing. We present SketchAssist, an interactive sketch drawing assistant that accelerates creation by unifying instruction-guided global edits with line-guided region redrawing, while keeping unrelated regions and overall composition intact. To enable this assistant at scale, we introduce a controllable data generation pipeline that (i) constructs attribute-addition sequences from attribute-free base sketches, (ii) forms multi-step edit chains via cross-sequence sampling, and (iii) expands stylistic coverage with a style-preserving attribute-removal model applied to diverse sketches. Building on this data, SketchAssist employs a unified sketch editing framework with minimal changes to DiT-based editors. We repurpose the RGB channels to encode the inputs, enabling seamless switching between instruction-guided edits and line-guided redrawing within a single input interface. To further specialize behavior across modes, we integrate a task-guided mixture-of-experts into LoRA layers, routing by text and visual cues to improve semantic controllability, structural fidelity, and style preservation. Extensive experiments show state-of-the-art results on both tasks, with superior instruction adherence and style/structure preservation compared to recent baselines. Together, our dataset and SketchAssist provide a practical, controllable assistant for sketch creation and revision.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14140v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "人形机器人",
                    "matched_keywords": [
                        "digit"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出SketchAssist以解决线稿编辑中语义修改与局部重绘的平衡问题",
            "summary_zh": "线稿编辑是数字插画的核心环节，但现有图像编辑系统难以在支持高层次语义修改和精确局部重绘的同时，保持线稿的稀疏、风格敏感结构。本文介绍了SketchAssist，一个交互式线稿绘制助手，通过统一指令引导的全局编辑和线条引导的区域重绘来加速创作，同时保持无关区域和整体构图不变。为实现大规模应用，我们引入了一个可控数据生成流程，包括：从无属性基础线稿构建属性添加序列；通过跨序列采样形成多步编辑链；以及应用风格保持的属性移除模型到多样化线稿以扩展风格覆盖。基于这些数据，SketchAssist采用统一的线稿编辑框架，对基于DiT的编辑器进行最小改动。我们重新利用RGB通道编码输入，实现在单一输入界面中无缝切换指令引导编辑和线条引导重绘。为进一步专业化不同模式的行为，我们将任务引导的专家混合集成到LoRA层中，通过文本和视觉线索进行路由，以提升语义可控性、结构保真度和风格保持。大量实验显示，在两个任务上均达到最先进结果，与近期基线相比，在指令遵循和风格/结构保持方面表现更优。我们的数据集和SketchAssist共同为线稿创作和修订提供了一个实用、可控的助手。",
            "intro_zh": [
                "现有图像编辑系统难以平衡线稿的稀疏结构保持与高层次语义修改和局部重绘需求，导致编辑效率低下和风格失真。",
                "SketchAssist通过统一指令引导全局编辑和线条引导区域重绘，并引入可控数据生成流程和任务引导专家混合，实现高效、精确的线稿编辑。",
                "实验表明，SketchAssist在指令遵循和风格/结构保持方面优于基线，达到最先进性能，验证了其有效性和实用性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决线稿编辑中的核心挑战，即现有图像编辑系统难以同时支持高层次语义修改（如改变物体属性）和精确局部重绘（如修改特定线条），同时保持线稿的稀疏、风格敏感结构，导致编辑过程繁琐且易破坏整体构图和风格一致性。\\n\\n**核心思路**：论文提出SketchAssist，通过统一指令引导的全局编辑和线条引导的区域重绘，实现在单一界面中无缝切换编辑模式，并引入可控数据生成流程和任务引导的专家混合机制，以提升语义可控性、结构保真度和风格保持，从而加速线稿创作和修订。\\n\\n**技术框架**：整体架构包括数据生成和编辑框架两部分。数据生成流程：从无属性基础线稿构建属性添加序列，通过跨序列采样形成多步编辑链，并应用风格保持的属性移除模型扩展风格覆盖。编辑框架：基于DiT编辑器进行最小改动，重新利用RGB通道编码输入（如指令、线条引导），集成任务引导的专家混合到LoRA层，通过文本和视觉线索路由不同编辑模式。\\n\\n**关键创新**：最重要的技术创新点在于统一了指令引导编辑和线条引导重绘的输入接口，以及引入任务引导的专家混合机制。与现有方法的本质区别在于，它避免了模式切换的复杂性，通过数据生成和模型设计实现了更精确的语义控制和结构保持，而传统方法往往依赖单一编辑方式或缺乏风格适应性。\\n\\n**关键设计**：关键设计包括：RGB通道重新利用以编码多模态输入（如文本指令和线条掩码）；LoRA层中集成专家混合，通过路由机制根据任务类型（如全局编辑或局部重绘）激活不同专家；数据生成中使用属性添加序列和跨序列采样构建多样化编辑链，以及风格保持的属性移除模型确保风格一致性；损失函数可能结合重建损失和风格损失，具体参数设置未在摘要中详述，但强调了对DiT框架的最小改动以实现高效部署。",
            "application_zh": "SketchAssist在数字插画、动漫设计、游戏美术和概念艺术等领域具有广泛应用潜力。它通过提供高效、可控的线稿编辑助手，能显著提升艺术家的创作效率，支持快速迭代和精确修改，同时保持风格一致性。未来可能扩展到更多创意产业，如广告设计和教育工具，推动交互式AI辅助创作的发展。",
            "highlight_zh": "实验显示，SketchAssist在指令引导编辑和线条引导重绘两个任务上均达到最先进结果。与近期基线相比，它在指令遵循方面表现更优，具体提升幅度未在摘要中量化，但强调了在风格和结构保持方面的显著优势。这验证了统一编辑框架和专家混合机制的有效性，为线稿编辑提供了实用解决方案。",
            "tags_zh": [
                "线稿编辑",
                "语义编辑",
                "局部重绘",
                "可控数据生成",
                "专家混合",
                "DiT框架",
                "风格保持",
                "交互式助手"
            ],
            "_index": 39
        },
        {
            "title": "AnchorHOI: Zero-shot Generation of 4D Human-Object Interaction via Anchor-based Prior Distillation",
            "authors": [
                "Sisi Dai",
                "Kai Xu"
            ],
            "arxiv_id": "2512.14095v1",
            "summary": "Despite significant progress in text-driven 4D human-object interaction (HOI) generation with supervised methods, the scalability remains limited by the scarcity of large-scale 4D HOI datasets. To overcome this, recent approaches attempt zero-shot 4D HOI generation with pre-trained image diffusion models. However, interaction cues are minimally distilled during the generation process, restricting their applicability across diverse scenarios. In this paper, we propose AnchorHOI, a novel framework that thoroughly exploits hybrid priors by incorporating video diffusion models beyond image diffusion models, advancing 4D HOI generation. Nevertheless, directly optimizing high-dimensional 4D HOI with such priors remains challenging, particularly for human pose and compositional motion. To address this challenge, AnchorHOI introduces an anchor-based prior distillation strategy, which constructs interaction-aware anchors and then leverages them to guide generation in a tractable two-step process. Specifically, two tailored anchors are designed for 4D HOI generation: anchor Neural Radiance Fields (NeRFs) for expressive interaction composition, and anchor keypoints for realistic motion synthesis. Extensive experiments demonstrate that AnchorHOI outperforms previous methods with superior diversity and generalization.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "AAAI 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14095v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "动作生成",
                    "matched_keywords": [
                        "motion synthesis"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出AnchorHOI框架，通过基于锚点的先验蒸馏策略解决零样本4D人-物交互生成中的交互线索不足问题。",
            "summary_zh": "尽管基于监督方法的文本驱动4D人-物交互生成取得了显著进展，但由于大规模4D HOI数据集的稀缺性，其可扩展性仍然受限。为了克服这一限制，最近的方法尝试使用预训练的图像扩散模型进行零样本4D HOI生成。然而，在生成过程中交互线索的蒸馏非常有限，限制了其在多样化场景中的适用性。本文提出AnchorHOI，这是一个新颖的框架，通过结合视频扩散模型超越图像扩散模型，充分利用混合先验，推进4D HOI生成。然而，直接使用此类先验优化高维4D HOI仍然具有挑战性，特别是在人体姿态和组合运动方面。为了解决这一挑战，AnchorHOI引入了一种基于锚点的先验蒸馏策略，该策略构建交互感知的锚点，然后利用它们在一个可处理的两步过程中指导生成。具体来说，为4D HOI生成设计了两个定制的锚点：用于表达性交互组合的锚点神经辐射场，以及用于逼真运动合成的锚点关键点。大量实验表明，AnchorHOI在多样性和泛化性方面优于先前的方法。",
            "intro_zh": [
                "现有零样本4D HOI生成方法主要依赖图像扩散模型，交互线索蒸馏不足，限制了在多样化场景中的应用。",
                "AnchorHOI提出基于锚点的先验蒸馏策略，通过构建交互感知锚点（如锚点NeRF和关键点）来指导生成过程。",
                "实验表明，AnchorHOI在多样性和泛化性方面优于先前方法，实现了更高质量的4D HOI生成。"
            ],
            "method_zh": "**问题定义**：论文旨在解决零样本4D人-物交互生成问题，即从文本描述生成包含时间维度的交互序列。现有方法主要依赖预训练图像扩散模型，但交互线索（如人体姿态和物体运动）在生成过程中蒸馏不足，导致生成结果在多样化和复杂场景中表现受限，且直接优化高维4D数据（如NeRF表示）具有挑战性。\\n\\n**核心思路**：论文的核心思路是通过引入基于锚点的先验蒸馏策略，构建交互感知的锚点来引导生成过程，从而更有效地利用混合先验（包括视频扩散模型），解决高维优化难题。这设计基于锚点能提供结构化指导，简化生成流程，提升交互真实性和多样性。\\n\\n**技术框架**：整体框架分为两步：首先，构建交互感知锚点，包括锚点NeRF（用于表达交互组合，如物体形状和位置）和锚点关键点（用于逼真运动合成，如人体关节轨迹）；然后，利用这些锚点在可处理的两步过程中指导4D HOI生成，可能涉及先验蒸馏、优化和合成模块。具体流程包括锚点提取、先验融合和序列生成阶段。\\n\\n**关键创新**：最重要的技术创新是基于锚点的先验蒸馏策略，它通过锚点NeRF和锚点关键点将高维4D HOI生成分解为更易管理的子问题，与现有方法（如纯图像扩散或简单视频模型）相比，本质区别在于更系统地整合交互线索，提升生成质量和泛化能力。\\n\\n**关键设计**：关键设计包括：锚点NeRF用于建模交互中的物体和场景表示，锚点关键点用于捕捉人体运动模式；损失函数可能结合重建损失、先验匹配损失和交互一致性损失；网络结构可能包含锚点提取网络、扩散模型和优化模块；参数设置涉及锚点数量、先验权重和训练策略，具体细节在论文中未详细说明，需参考原文。",
            "application_zh": "该研究在虚拟现实、游戏开发、机器人交互仿真和影视特效等领域具有潜在应用价值。通过生成逼真的4D人-物交互序列，可以降低数据采集成本，提升场景生成的多样性和效率，未来可能推动人机交互和智能系统的发展。",
            "highlight_zh": "实验表明，AnchorHOI在零样本4D HOI生成任务中优于先前方法，具体性能数据未在摘要中提供，但提到在多样性和泛化性方面有显著提升。对比基线可能包括基于图像扩散的零样本方法，提升幅度体现在生成质量、交互真实性和场景适应性上。",
            "tags_zh": [
                "4D人-物交互生成",
                "零样本学习",
                "先验蒸馏",
                "神经辐射场",
                "关键点检测",
                "视频扩散模型",
                "交互感知锚点"
            ],
            "_index": 40
        },
        {
            "title": "Expert Switching for Robust AAV Landing: A Dual-Detector Framework in Simulation",
            "authors": [
                "Humaira Tasnim",
                "Ashik E Rasul",
                "Bruce Jo",
                "Hyung-Jin Yoon"
            ],
            "arxiv_id": "2512.14054v1",
            "summary": "Reliable helipad detection is essential for Autonomous Aerial Vehicle (AAV) landing, especially under GPS-denied or visually degraded conditions. While modern detectors such as YOLOv8 offer strong baseline performance, single-model pipelines struggle to remain robust across the extreme scale transitions that occur during descent, where helipads appear small at high altitude and large near touchdown. To address this limitation, we propose a scale-adaptive dual-expert perception framework that decomposes the detection task into far-range and close-range regimes. Two YOLOv8 experts are trained on scale-specialized versions of the HelipadCat dataset, enabling one model to excel at detecting small, low-resolution helipads and the other to provide high-precision localization when the target dominates the field of view. During inference, both experts operate in parallel, and a geometric gating mechanism selects the expert whose prediction is most consistent with the AAV's viewpoint. This adaptive routing prevents the degradation commonly observed in single-detector systems when operating across wide altitude ranges. The dual-expert perception module is evaluated in a closed-loop landing environment that integrates CARLA's photorealistic rendering with NASA's GUAM flight-dynamics engine. Results show substantial improvements in alignment stability, landing accuracy, and overall robustness compared to single-detector baselines. By introducing a scale-aware expert routing strategy tailored to the landing problem, this work advances resilient vision-based perception for autonomous descent and provides a foundation for future multi-expert AAV frameworks.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14054v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "carla"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出基于双专家切换的尺度自适应感知框架，以解决自主空中车辆在极端尺度变化下的降落检测鲁棒性问题。",
            "summary_zh": "可靠的直升机停机坪检测对于自主空中车辆（AAV）降落至关重要，尤其是在GPS受限或视觉条件退化的情况下。虽然YOLOv8等现代检测器提供了强大的基线性能，但单模型管道在降落过程中发生的极端尺度转换下难以保持鲁棒性，此时停机坪在高空时显得小而低分辨率，在接近着陆时则占据视野主导。为解决这一限制，我们提出了一个尺度自适应的双专家感知框架，将检测任务分解为远距离和近距离两个阶段。两个YOLOv8专家在HelipadCat数据集的尺度专门化版本上进行训练，使一个模型擅长检测小而低分辨率的停机坪，另一个则在目标主导视野时提供高精度定位。在推理过程中，两个专家并行运行，几何门控机制选择与AAV视点最一致的预测专家。这种自适应路由防止了单检测器系统在宽高度范围内操作时常见的性能退化。双专家感知模块在闭环降落环境中进行评估，该环境集成了CARLA的光真实感渲染与NASA的GUAM飞行动力学引擎。结果显示，与单检测器基线相比，在对准稳定性、降落精度和整体鲁棒性方面有显著提升。通过引入针对降落问题定制的尺度感知专家路由策略，这项工作推进了自主下降的弹性视觉感知，并为未来多专家AAV框架奠定了基础。",
            "intro_zh": [
                "核心问题：单检测器在AAV降落过程中面临极端尺度变化挑战，难以同时处理高空小目标和近地大目标，导致检测鲁棒性下降。",
                "方法要点：提出双专家感知框架，训练两个YOLOv8模型分别处理远距离和近距离检测，通过几何门控机制自适应切换专家。",
                "实验或效果：在集成CARLA与GUAM的闭环环境中验证，相比单检测器基线，显著提升对准稳定性、降落精度和整体鲁棒性。"
            ],
            "method_zh": "**问题定义**：论文解决自主空中车辆（AAV）在降落过程中，由于高度变化导致停机坪尺度极端转换（从高空的小目标到近地的大目标）时，单检测器模型难以保持鲁棒检测的问题。现有单模型管道如YOLOv8在宽高度范围内性能退化，无法同时优化小目标检测精度和大目标定位准确性。\\n\\n**核心思路**：核心思路是将检测任务分解为尺度专门化的子任务，通过双专家并行检测和自适应路由，实现尺度自适应的感知。设计两个YOLOv8专家分别针对远距离（小目标）和近距离（大目标）训练，利用几何一致性选择最佳预测，避免单模型在跨尺度场景中的性能瓶颈。\\n\\n**技术框架**：整体框架包括数据准备、专家训练和推理阶段。首先，基于HelipadCat数据集生成尺度专门化版本，分别用于训练远距离和近距离专家。在推理时，两个专家并行处理输入图像，生成检测结果；几何门控机制基于AAV的视点（如高度、角度）计算预测与几何约束的一致性，选择最匹配的专家输出；最终输出用于闭环降落控制。框架集成到CARLA（渲染）和GUAM（动力学）的仿真环境中进行端到端评估。\\n\\n**关键创新**：最重要的创新是尺度感知的专家路由策略，通过双专家并行和几何门控实现自适应切换，本质区别在于从单模型静态检测转向多模型动态选择，提升了跨尺度场景的鲁棒性。与现有方法相比，它避免了模型融合的复杂性，直接基于任务特性优化专家分工。\\n\\n**关键设计**：关键设计包括：使用YOLOv8作为基础检测器，因其高效性和准确性；数据准备中，通过尺度划分（如基于高度阈值）创建专门化数据集；几何门控机制可能涉及计算预测边界框与预期尺度或位置的一致性得分；训练时采用标准目标检测损失（如交叉熵和IoU损失）；在仿真环境中，参数设置如高度切换阈值需根据实际降落场景调整。",
            "application_zh": "该研究主要应用于自主空中车辆（AAV）的视觉引导降落场景，特别是在GPS受限或恶劣视觉条件下（如雾、雨）。实际价值在于提升AAV在复杂环境中的降落安全性和可靠性，为无人机物流、紧急救援和军事任务提供技术支持。未来影响可能扩展到多专家感知框架在其他机器人视觉任务中的应用，如自动驾驶中的目标检测或工业检测。",
            "highlight_zh": "在闭环仿真环境中评估，双专家框架相比单YOLOv8基线，在降落任务中表现出显著提升：对准稳定性提高（具体数据未知，但摘要指出“substantial improvements”），降落精度增强，整体鲁棒性优化。实验使用CARLA和GUAM集成平台，验证了框架在极端尺度变化下的有效性，为实际AAV应用提供了可靠基准。",
            "tags_zh": [
                "自主空中车辆",
                "视觉感知",
                "目标检测",
                "尺度自适应",
                "专家切换",
                "仿真评估",
                "YOLOv8",
                "几何门控"
            ],
            "_index": 41
        },
        {
            "title": "E-Navi: Environmental Adaptive Navigation for UAVs on Resource Constrained Platforms",
            "authors": [
                "Boyang Li",
                "Zhongpeng Jin",
                "Shuai Zhao",
                "Jiahui Liao",
                "Tian Liu",
                "Han Liu",
                "Yuanhai Zhang",
                "Kai Huang"
            ],
            "arxiv_id": "2512.14046v1",
            "summary": "The ability to adapt to changing environments is crucial for the autonomous navigation systems of Unmanned Aerial Vehicles (UAVs). However, existing navigation systems adopt fixed execution configurations without considering environmental dynamics based on available computing resources, e.g., with a high execution frequency and task workload. This static approach causes rigid flight strategies and excessive computations, ultimately degrading flight performance or even leading to failures in UAVs. Despite the necessity for an adaptive system, dynamically adjusting workloads remains challenging, due to difficulties in quantifying environmental complexity and modeling the relationship between environment and system configuration. Aiming at adapting to dynamic environments, this paper proposes E-Navi, an environmental-adaptive navigation system for UAVs that dynamically adjusts task executions on the CPUs in response to environmental changes based on available computational resources. Specifically, the perception-planning pipeline of UAVs navigation system is redesigned through dynamic adaptation of mapping resolution and execution frequency, driven by the quantitative environmental complexity evaluations. In addition, E-Navi supports flexible deployment across hardware platforms with varying levels of computing capability. Extensive Hardware-In-the-Loop and real-world experiments demonstrate that the proposed system significantly outperforms the baseline method across various hardware platforms, achieving up to 53.9% navigation task workload reduction, up to 63.8% flight time savings, and delivering more stable velocity control.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14046v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "mapping"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出E-Navi环境自适应导航系统，为资源受限无人机平台解决动态环境适应性问题",
            "summary_zh": "无人机自主导航系统适应环境变化的能力至关重要。然而，现有导航系统采用固定的执行配置，不考虑基于可用计算资源的环境动态性，例如高执行频率和任务负载。这种静态方法导致飞行策略僵化、计算量过大，最终降低飞行性能甚至导致无人机故障。尽管自适应系统具有必要性，但由于量化环境复杂性和建模环境与系统配置关系的困难，动态调整工作负载仍然具有挑战性。为适应动态环境，本文提出E-Navi，一种面向无人机的环境自适应导航系统，可根据可用计算资源动态调整CPU上的任务执行以响应环境变化。具体而言，通过定量环境复杂度评估驱动，重新设计了无人机导航系统的感知-规划流程，实现地图分辨率和执行频率的动态自适应。此外，E-Navi支持在不同计算能力的硬件平台上灵活部署。大量硬件在环和真实世界实验表明，所提系统在各种硬件平台上显著优于基线方法，实现高达53.9%的导航任务负载减少、高达63.8%的飞行时间节省，并提供更稳定的速度控制。",
            "intro_zh": [
                "现有无人机导航系统采用固定配置，无法根据环境动态调整计算资源，导致性能下降或失败。",
                "提出E-Navi系统，通过量化环境复杂度并动态调整地图分辨率和执行频率，实现自适应导航。",
                "实验显示E-Navi显著优于基线，最高减少53.9%任务负载和63.8%飞行时间，提升速度控制稳定性。"
            ],
            "method_zh": "**问题定义**：论文解决无人机在资源受限平台上导航时，现有系统采用固定执行配置（如高频率和任务负载）无法适应环境动态变化的问题，导致计算资源浪费、飞行策略僵化，甚至导航失败。现有方法的痛点在于缺乏对环境复杂度的量化评估，以及环境与系统配置关系的建模能力。\\n\\n**核心思路**：论文的核心思路是设计一个环境自适应导航系统，通过实时评估环境复杂度，动态调整感知-规划流程中的关键参数（如地图分辨率和执行频率），以匹配可用计算资源，从而在保证导航性能的同时优化资源利用。这样设计是因为环境变化直接影响导航任务的复杂性，动态调整可以避免过度计算或性能不足。\\n\\n**技术框架**：整体架构包括环境复杂度评估模块、自适应调整模块和导航执行模块。流程为：首先，系统实时感知环境并量化复杂度；其次，基于复杂度评估和可用计算资源，动态调整地图分辨率和任务执行频率；最后，执行优化后的导航任务，实现高效稳定的飞行。\\n\\n**关键创新**：最重要的技术创新是引入了定量环境复杂度评估机制，并以此驱动感知-规划流程的动态自适应调整。与现有方法的本质区别在于，E-Navi不再采用静态配置，而是根据环境变化和资源约束进行实时优化，实现了从固定到自适应的转变。\\n\\n**关键设计**：关键设计包括环境复杂度量化指标（如障碍物密度、运动变化率）、自适应调整算法（基于阈值或模型预测控制动态设置分辨率和频率）、以及跨平台部署接口（支持不同CPU能力的硬件）。具体参数设置如分辨率调整范围、频率调整策略等，通过实验优化确定，以平衡精度和效率。",
            "application_zh": "该研究主要应用于无人机自主导航领域，特别是在资源受限平台（如小型无人机、边缘设备）上实现高效环境适应。实际价值在于提升无人机在动态环境（如城市巡检、农业监测、灾难救援）中的飞行性能和可靠性，同时降低计算开销和能耗。未来影响可能推动自适应导航系统在更广泛机器人平台和物联网设备中的部署，促进智能系统在复杂场景下的实用化。",
            "highlight_zh": "最重要的实验结果包括：E-Navi在硬件在环和真实世界实验中显著优于基线方法，具体性能数据为导航任务负载最高减少53.9%，飞行时间最高节省63.8%，并提供更稳定的速度控制。对比基线（固定配置系统），提升幅度显著，证明了自适应调整在资源优化和性能提升方面的有效性。这些结果基于多种硬件平台测试，验证了系统的泛化能力和实际应用价值。",
            "tags_zh": [
                "无人机导航",
                "环境自适应",
                "资源优化",
                "动态调整",
                "感知规划",
                "硬件在环",
                "计算效率",
                "自主系统"
            ],
            "_index": 42
        },
        {
            "title": "Multivariate Time Series Forecasting with Hybrid Euclidean-SPD Manifold Graph Neural Networks",
            "authors": [
                "Yong Fang",
                "Na Li",
                "Hangguan Shan",
                "Eryun Liu",
                "Xinyu Li",
                "Wei Ni",
                "Er-Ping Li"
            ],
            "arxiv_id": "2512.14023v1",
            "summary": "Multivariate Time Series (MTS) forecasting plays a vital role in various real-world applications, such as traffic management and predictive maintenance. Existing approaches typically model MTS data in either Euclidean or Riemannian space, limiting their ability to capture the diverse geometric structures and complex spatio-temporal dependencies inherent in real-world data. To overcome this limitation, we propose the Hybrid Symmetric Positive-Definite Manifold Graph Neural Network (HSMGNN), a novel graph neural network-based model that captures data geometry within a hybrid Euclidean-Riemannian framework. To the best of our knowledge, this is the first work to leverage hybrid geometric representations for MTS forecasting, enabling expressive and comprehensive modeling of geometric properties. Specifically, we introduce a Submanifold-Cross-Segment (SCS) embedding to project input MTS into both Euclidean and Riemannian spaces, thereby capturing spatio-temporal variations across distinct geometric domains. To alleviate the high computational cost of Riemannian distance, we further design an Adaptive-Distance-Bank (ADB) layer with a trainable memory mechanism. Finally, a Fusion Graph Convolutional Network (FGCN) is devised to integrate features from the dual spaces via a learnable fusion operator for accurate prediction. Experiments on three benchmark datasets demonstrate that HSMGNN achieves up to a 13.8 percent improvement over state-of-the-art baselines in forecasting accuracy.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14023v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "traffic"
                    ],
                    "score": 1
                },
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 3,
            "headline_zh": "提出混合欧几里得-对称正定流形图神经网络以解决多元时间序列预测中几何结构建模不足的问题。",
            "summary_zh": "多元时间序列（MTS）预测在交通管理和预测性维护等实际应用中至关重要。现有方法通常在欧几里得空间或黎曼空间中建模MTS数据，限制了其捕捉真实数据中多样几何结构和复杂时空依赖性的能力。为克服这一限制，我们提出了混合对称正定流形图神经网络（HSMGNN），这是一种基于图神经网络的新模型，在混合欧几里得-黎曼框架内捕捉数据几何。据我们所知，这是首个利用混合几何表示进行MTS预测的工作，实现了对几何属性的表达性和全面建模。具体而言，我们引入了子流形交叉段嵌入，将输入MTS投影到欧几里得和黎曼空间，从而捕捉不同几何域中的时空变化。为减轻黎曼距离的高计算成本，我们进一步设计了具有可训练记忆机制的自适应距离库层。最后，开发了融合图卷积网络，通过可学习的融合算子整合双空间特征以进行准确预测。在三个基准数据集上的实验表明，HSMGNN在预测准确性上比最先进的基线方法提升了高达13.8%。",
            "intro_zh": [
                "现有方法在欧几里得或黎曼空间中建模多元时间序列，难以捕捉真实数据中的多样几何结构和复杂时空依赖性。",
                "提出HSMGNN模型，通过混合几何表示和子流形交叉段嵌入，在双空间中捕捉时空变化，并设计自适应距离库层降低计算成本。",
                "在三个基准数据集上，HSMGNN比最先进基线方法在预测准确性上提升高达13.8%，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：论文解决多元时间序列预测问题，现有方法通常仅在欧几里得或黎曼空间中建模，无法全面捕捉真实数据中的多样几何结构和复杂时空依赖性，导致预测准确性受限。\\n\\n**核心思路**：论文提出混合欧几里得-黎曼框架，通过同时利用两种空间的优势，更全面地建模数据几何，从而提升预测性能。设计子流形交叉段嵌入和自适应距离库层来高效处理几何表示和计算成本。\\n\\n**技术框架**：整体架构包括三个主要模块：首先，子流形交叉段嵌入将输入MTS投影到欧几里得和黎曼空间；其次，自适应距离库层通过可训练记忆机制优化黎曼距离计算；最后，融合图卷积网络整合双空间特征，使用可学习融合算子进行最终预测。\\n\\n**关键创新**：首次引入混合几何表示进行MTS预测，通过子流形交叉段嵌入捕捉跨几何域的时空变化，并设计自适应距离库层降低黎曼计算成本，实现高效且表达性强的建模。\\n\\n**关键设计**：子流形交叉段嵌入基于时间序列分段和几何投影；自适应距离库层包含可训练参数以减少距离计算复杂度；融合图卷积网络采用图卷积操作和融合算子，损失函数通常基于预测误差如均方误差，网络结构包括嵌入层、ADB层和FGCN层。",
            "application_zh": "该研究在交通管理、预测性维护、金融分析和环境监测等领域具有潜在应用价值，通过提升多元时间序列预测准确性，可优化资源分配、减少故障风险，并推动智能系统的发展。未来可能扩展到更多复杂时空数据建模任务。",
            "highlight_zh": "在三个基准数据集上的实验显示，HSMGNN比最先进的基线方法在预测准确性上提升高达13.8%，具体性能数据因数据集而异，但整体表现优于现有欧几里得或黎曼方法，验证了混合几何表示的有效性。",
            "tags_zh": [
                "多元时间序列预测",
                "混合几何表示",
                "图神经网络",
                "黎曼流形",
                "时空依赖性建模",
                "自适应距离计算",
                "对称正定矩阵",
                "预测准确性提升"
            ],
            "_index": 43
        },
        {
            "title": "Early Warning Index for Patient Deteriorations in Hospitals",
            "authors": [
                "Dimitris Bertsimas",
                "Yu Ma",
                "Kimberly Villalobos Carballo",
                "Gagan Singh",
                "Michal Laskowski",
                "Jeff Mather",
                "Dan Kombert",
                "Howard Haronian"
            ],
            "arxiv_id": "2512.14683v1",
            "summary": "Hospitals lack automated systems to harness the growing volume of heterogeneous clinical and operational data to effectively forecast critical events. Early identification of patients at risk for deterioration is essential not only for patient care quality monitoring but also for physician care management. However, translating varied data streams into accurate and interpretable risk assessments poses significant challenges due to inconsistent data formats. We develop a multimodal machine learning framework, the Early Warning Index (EWI), to predict the aggregate risk of ICU admission, emergency response team dispatch, and mortality. Key to EWI's design is a human-in-the-loop process: clinicians help determine alert thresholds and interpret model outputs, which are enhanced by explainable outputs using Shapley Additive exPlanations (SHAP) to highlight clinical and operational factors (e.g., scheduled surgeries, ward census) driving each patient's risk. We deploy EWI in a hospital dashboard that stratifies patients into three risk tiers. Using a dataset of 18,633 unique patients at a large U.S. hospital, our approach automatically extracts features from both structured and unstructured electronic health record (EHR) data and achieves C-statistics of 0.796. It is currently used as a triage tool for proactively managing at-risk patients. The proposed approach saves physicians valuable time by automatically sorting patients of varying risk levels, allowing them to concentrate on patient care rather than sifting through complex EHR data. By further pinpointing specific risk drivers, the proposed model provides data-informed adjustments to caregiver scheduling and allocation of critical resources. As a result, clinicians and administrators can avert downstream complications, including costly procedures or high readmission rates and improve overall patient flow.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14683v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出早期预警指数（EWI）以解决医院患者病情恶化预测问题，通过多模态机器学习框架提升风险识别准确性。",
            "summary_zh": "医院缺乏自动化系统来利用日益增长的异构临床和运营数据有效预测关键事件。早期识别有恶化风险的患者不仅对患者护理质量监控至关重要，也对医生护理管理至关重要。然而，由于数据格式不一致，将各种数据流转化为准确且可解释的风险评估面临重大挑战。我们开发了一个多模态机器学习框架，即早期预警指数（EWI），用于预测ICU入院、紧急响应团队派遣和死亡率的综合风险。EWI设计的关键在于人机交互过程：临床医生帮助确定警报阈值并解释模型输出，这些输出通过使用Shapley Additive exPlanations（SHAP）的可解释输出增强，以突出驱动每个患者风险的临床和运营因素（例如，预定手术、病房普查）。我们将EWI部署在医院仪表板中，将患者分为三个风险等级。使用美国一家大型医院的18,633名独特患者的数据集，我们的方法从结构化和非结构化电子健康记录（EHR）数据中自动提取特征，并实现了C统计量为0.796。它目前被用作主动管理风险患者的分类工具。所提出的方法通过自动排序不同风险水平的患者，为医生节省了宝贵时间，使他们能够专注于患者护理，而不是筛选复杂的EHR数据。通过进一步精确定位特定的风险驱动因素，所提出的模型为护理人员调度和关键资源分配提供了数据驱动的调整。因此，临床医生和管理人员可以避免下游并发症，包括昂贵的手术或高再入院率，并改善整体患者流程。",
            "intro_zh": [
                "核心问题：医院缺乏自动化系统整合异构临床数据预测患者恶化，现有方法因数据格式不一致导致风险评估不准确。",
                "方法要点：提出多模态机器学习框架EWI，结合人机交互和SHAP可解释性，从EHR数据自动提取特征预测综合风险。",
                "实验或效果：在18,633名患者数据集上实现C统计量0.796，部署为分类工具，提升风险识别效率并优化资源分配。"
            ],
            "method_zh": "**问题定义**：论文旨在解决医院中患者病情恶化（如ICU入院、紧急响应团队派遣和死亡率）的早期预测问题。现有方法缺乏自动化系统整合异构临床和运营数据，导致风险评估不准确且不可解释，主要痛点包括数据格式不一致、难以从复杂电子健康记录（EHR）中提取有效特征，以及缺乏临床医生参与的风险阈值设定。\\n\\n**核心思路**：论文提出早期预警指数（EWI）框架，核心思路是通过多模态机器学习整合结构化和非结构化EHR数据，并引入人机交互过程，让临床医生参与警报阈值确定和模型输出解释，以提升预测的准确性和可操作性。这样设计是为了克服数据异构性挑战，并确保模型输出符合临床实践需求。\\n\\n**技术框架**：整体架构包括数据预处理、特征提取、模型训练和部署阶段。主要模块：1）数据整合模块，从EHR中自动提取临床和运营特征（如预定手术、病房普查）；2）机器学习模型，预测综合风险分数；3）人机交互模块，临床医生设定风险阈值并解释输出；4）可解释性模块，使用SHAP分析突出风险驱动因素；5）部署模块，将模型集成到医院仪表板，将患者分为三个风险等级进行主动管理。\\n\\n**关键创新**：最重要的技术创新点是结合多模态数据融合与人机交互设计，本质区别在于不仅依赖自动化预测，还通过临床医生反馈优化模型，并利用SHAP提供可解释输出，使风险评估更透明和实用，而现有方法往往忽视临床参与和可解释性。\\n\\n**关键设计**：关键设计细节包括：使用机器学习算法（具体算法未指定，但基于摘要推断可能涉及集成方法或深度学习）处理异构数据；损失函数可能基于二元或多元分类优化风险预测；参数设置涉及风险阈值由临床医生动态调整；网络结构或流程设计强调特征自动提取，以减少人工干预，并集成SHAP值计算以量化各特征对风险分数的贡献。",
            "application_zh": "该研究主要应用于医院临床管理和运营优化领域，潜在价值包括提升患者护理质量、减少并发症和医疗成本。实际应用中，EWI可作为分类工具帮助医生优先处理高风险患者，优化护理人员调度和资源分配，未来可能扩展到其他医疗机构或慢性病管理，推动智能医疗系统的发展。",
            "highlight_zh": "最重要的实验结果包括：在大型医院18,633名患者数据集上，EWI实现了C统计量0.796，表明模型具有良好的区分能力。具体性能数据未提供对比基线，但摘要强调模型自动从EHR提取特征并部署为分类工具，提升风险识别效率，帮助避免下游并发症如高再入院率，实际应用中节省医生时间并优化资源分配。",
            "tags_zh": [
                "早期预警指数",
                "多模态机器学习",
                "患者恶化预测",
                "电子健康记录",
                "可解释人工智能",
                "人机交互",
                "临床决策支持",
                "医院运营优化"
            ],
            "_index": 44
        },
        {
            "title": "EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models",
            "authors": [
                "Zechen Bai",
                "Chen Gao",
                "Mike Zheng Shou"
            ],
            "arxiv_id": "2512.14666v1",
            "summary": "Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\\% on long-horizon tasks, +22.0\\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\\% success on unseen tasks without task-specific demonstrations training (vs. 0\\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "15 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14666v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reward"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出EVOLVE-VLA框架，通过环境反馈实现视觉-语言-动作模型的测试时训练，解决机器人适应性问题。",
            "summary_zh": "实现真正的自适应具身智能需要智能体通过环境交互持续学习，而不仅仅是模仿静态演示。视觉-语言-动作（VLA）模型通过利用大语言模型推动了机器人操作的发展，但仍受限于监督微调（SFT）：每个任务需要数百个演示、僵化地记忆轨迹，且在部署条件偏离训练时无法适应。我们提出了EVOLVE-VLA，这是一个测试时训练框架，使VLA能够通过环境交互以最少或零任务特定演示持续适应。关键技术挑战是用自主反馈替代测试时不可用的oracle奖励信号。我们通过一个提供密集反馈的学习进度估计器来解决这一问题，并关键地设计了两个机制来“驯服”这种固有噪声信号：（1）累积进度估计机制平滑噪声点估计；（2）渐进视野扩展策略实现逐步策略演化。EVOLVE-VLA取得了显著提升：长视野任务提升+8.6%，单样本学习提升+22.0%，并实现跨任务泛化——在未见任务上达到20.8%的成功率，无需任务特定演示训练（纯SFT为0%）。定性分析揭示了演示中不存在的涌现能力，包括错误恢复和新策略。这项工作代表了VLA真正学习和适应的关键一步，从静态模仿迈向持续自我改进。",
            "intro_zh": [
                "现有VLA模型依赖监督微调，需要大量演示、记忆轨迹，无法适应部署条件变化，限制了机器人自适应能力。",
                "提出EVOLVE-VLA框架，通过环境反馈实现测试时训练，利用学习进度估计器和噪声驯服机制，使模型持续自我改进。",
                "实验显示，EVOLVE-VLA在长视野任务提升8.6%，单样本学习提升22.0%，并在未见任务上实现20.8%成功率，显著优于纯SFT。"
            ],
            "method_zh": "**问题定义**：论文旨在解决视觉-语言-动作（VLA）模型在机器人操作中因依赖监督微调（SFT）而导致的适应性问题。现有方法的痛点包括：每个任务需要数百个演示，导致数据效率低下；模型僵化地记忆轨迹，缺乏灵活性；当部署环境与训练条件偏差时，模型无法自适应调整，限制了实际应用中的鲁棒性和泛化能力。\\n\\n**核心思路**：论文的核心解决思路是引入测试时训练框架，使VLA模型能够通过环境交互持续学习和适应，而无需依赖大量任务特定演示。设计的关键在于用自主反馈替代测试时不可用的oracle奖励信号，通过一个学习进度估计器提供密集反馈，并采用机制来驯服反馈中的噪声，从而实现稳定和有效的策略演化。\\n\\n**技术框架**：整体架构包括两个主要阶段：离线预训练和在线测试时训练。在离线阶段，模型可能基于基础VLA架构进行初始化。在线阶段，模型与环境交互，收集状态和动作数据，通过学习进度估计器计算反馈信号，并利用累积进度估计和渐进视野扩展策略来更新策略。框架流程为：交互→反馈估计→噪声平滑→策略更新→重复，形成一个闭环学习循环。\\n\\n**关键创新**：最重要的技术创新点是提出了一个完全自主的测试时训练框架，无需外部奖励信号，通过环境反馈驱动模型适应。与现有方法的本质区别在于：EVOLVE-VLA摆脱了对静态演示的依赖，实现了从模仿学习到交互学习的范式转变；它通过噪声驯服机制处理反馈的不确定性，提高了学习稳定性和效率。\\n\\n**关键设计**：关键设计包括：学习进度估计器，可能基于神经网络，输入状态和动作序列，输出密集反馈值；累积进度估计机制，通过滑动平均或积分方法平滑点估计，减少噪声影响；渐进视野扩展策略，逐步增加策略更新的时间范围，从短视野开始扩展到长视野，避免过早陷入局部最优。损失函数可能结合反馈信号和策略梯度方法，网络结构基于现有VLA模型（如Transformer架构），参数设置涉及学习率、反馈权重和视野扩展速率等超参数。",
            "application_zh": "该研究在机器人操作和具身智能领域具有广泛潜在应用，如家庭服务机器人、工业自动化、医疗辅助和无人驾驶。通过实现测试时自适应，EVOLVE-VLA能提升机器人在动态环境中的鲁棒性和泛化能力，减少对大量标注数据的依赖，降低部署成本。未来可能推动更智能、更灵活的自主系统发展，促进人工智能从静态学习向持续交互学习的演进。",
            "highlight_zh": "EVOLVE-VLA在实验中取得显著性能提升：长视野任务成功率提高8.6%，单样本学习提升22.0%。跨任务泛化方面，在未见任务上达到20.8%成功率，而纯监督微调（SFT）基线为0%。定性分析显示模型涌现出错误恢复和新策略等能力，验证了框架的有效性和适应性。",
            "tags_zh": [
                "视觉-语言-动作模型",
                "测试时训练",
                "环境反馈",
                "机器人操作",
                "自适应学习",
                "进度估计",
                "噪声驯服",
                "跨任务泛化"
            ],
            "_index": 45
        },
        {
            "title": "ViRC: Enhancing Visual Interleaved Mathematical CoT with Reason Chunking",
            "authors": [
                "Lihong Wang",
                "Liangqi Li",
                "Weiwei Feng",
                "Jiamin Wu",
                "Changtao Miao",
                "Tieru Wu",
                "Rui Ma",
                "Bo Zhang",
                "Zhe Li"
            ],
            "arxiv_id": "2512.14654v1",
            "summary": "CoT has significantly enhanced the reasoning ability of LLMs while it faces challenges when extended to multimodal domains, particularly in mathematical tasks. Existing MLLMs typically perform textual reasoning solely from a single static mathematical image, overlooking dynamic visual acquisition during reasoning. In contrast, humans repeatedly examine visual image and employ step-by-step reasoning to prove intermediate propositions. This strategy of decomposing the problem-solving process into key logical nodes adheres to Miller's Law in cognitive science. Inspired by this insight, we propose a ViRC framework for multimodal mathematical tasks, introducing a Reason Chunking mechanism that structures multimodal mathematical CoT into consecutive Critical Reasoning Units (CRUs) to simulate human expert problem-solving patterns. CRUs ensure intra-unit textual coherence for intermediate proposition verification while integrating visual information across units to generate subsequent propositions and support structured reasoning. To this end, we present CRUX dataset by using three visual tools and four reasoning patterns to provide explicitly annotated CRUs across multiple reasoning paths for each mathematical problem. Leveraging the CRUX dataset, we propose a progressive training strategy inspired by human cognitive learning, which includes Instructional SFT, Practice SFT, and Strategic RL, aimed at further strengthening the Reason Chunking ability of the model.The resulting ViRC-7B model achieves a 18.8\\% average improvement over baselines across multiple mathematical benchmarks. Code is available at https://github.com/Leon-LihongWang/ViRC.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Code is available at https://github.com/Leon-LihongWang/ViRC",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14654v1",
            "code_links": [
                {
                    "url": "https://github.com/Leon-LihongWang/ViRC",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出ViRC框架，通过Reason Chunking机制增强多模态数学推理中的视觉交错思维链，模拟人类专家解题模式。",
            "summary_zh": "思维链（CoT）显著提升了大型语言模型的推理能力，但在扩展到多模态领域时面临挑战，特别是在数学任务中。现有的多模态大语言模型（MLLMs）通常仅从单个静态数学图像进行文本推理，忽视了推理过程中的动态视觉获取。相比之下，人类会反复检查视觉图像，并采用逐步推理来证明中间命题。这种将问题解决过程分解为关键逻辑节点的策略符合认知科学中的米勒定律。受此启发，我们提出了一个用于多模态数学任务的ViRC框架，引入了Reason Chunking机制，将多模态数学CoT结构化为连续的Critical Reasoning Units（CRUs），以模拟人类专家的问题解决模式。CRUs确保单元内的文本连贯性以验证中间命题，同时跨单元整合视觉信息以生成后续命题并支持结构化推理。为此，我们使用三种视觉工具和四种推理模式构建了CRUX数据集，为每个数学问题提供跨多个推理路径的显式标注CRUs。利用CRUX数据集，我们提出了一种受人类认知学习启发的渐进式训练策略，包括Instructional SFT、Practice SFT和Strategic RL，旨在进一步增强模型的Reason Chunking能力。由此产生的ViRC-7B模型在多个数学基准测试中比基线平均提升了18.8%。代码可在https://github.com/Leon-LihongWang/ViRC获取。",
            "intro_zh": [
                "现有MLLMs在数学任务中仅从静态图像进行文本推理，缺乏动态视觉获取，导致推理能力受限。",
                "提出ViRC框架，通过Reason Chunking机制将推理分解为CRUs，模拟人类逐步验证和视觉整合的解题模式。",
                "ViRC-7B模型在多个数学基准上平均提升18.8%，验证了框架在增强多模态数学推理方面的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多模态数学推理中，现有MLLMs仅依赖静态图像进行文本推理，忽略动态视觉获取和结构化推理过程的问题，导致推理准确性和鲁棒性不足。\\n\\n**核心思路**：受人类专家反复检查图像并逐步推理的启发，提出Reason Chunking机制，将多模态数学CoT分解为连续的Critical Reasoning Units（CRUs），模拟人类将问题分解为关键逻辑节点的认知策略，以增强视觉与文本的交错推理。\\n\\n**技术框架**：整体架构包括数据构建、模型训练和推理三阶段。首先，使用三种视觉工具（如OCR、图表解析）和四种推理模式（如归纳、演绎）构建CRUX数据集，为每个数学问题提供显式标注的CRUs。然后，采用渐进式训练策略：Instructional SFT学习基本推理模式，Practice SFT强化CRU生成能力，Strategic RL优化整体推理策略。最后，在推理时，模型基于输入图像和问题，动态生成CRUs序列进行逐步验证。\\n\\n**关键创新**：最重要的技术创新是Reason Chunking机制和CRUs的设计，将多模态推理结构化为可验证的中间命题单元，实现视觉信息的跨单元整合，与现有方法仅依赖单次图像处理有本质区别，更贴近人类认知过程。\\n\\n**关键设计**：CRUs定义为包含视觉信息提取、文本推理步骤和中间命题验证的单元；训练策略中，Instructional SFT使用指令数据，Practice SFT基于CRUX数据集进行微调，Strategic RL采用强化学习优化奖励函数（如准确性、连贯性）；模型基于7B参数架构，具体网络结构未详细说明，但强调多模态编码器和解码器的集成。",
            "application_zh": "该研究在数学教育、智能辅导系统和自动化解题工具中具有潜在应用价值，可提升多模态数学问题的理解和推理能力，未来可能扩展到科学、工程等领域的复杂视觉推理任务，推动多模态AI向更人类化的认知模式发展。",
            "highlight_zh": "ViRC-7B模型在多个数学基准测试中表现出色，相比基线模型平均提升18.8%，具体提升幅度因任务而异，最高可达未知百分比。实验对比了现有MLLMs基线，验证了Reason Chunking机制在增强视觉交错推理方面的有效性，代码开源促进可复现性。",
            "tags_zh": [
                "多模态数学推理",
                "视觉交错思维链",
                "Reason Chunking机制",
                "Critical Reasoning Units",
                "CRUX数据集",
                "渐进式训练策略",
                "多模态大语言模型",
                "认知科学启发"
            ],
            "_index": 46
        },
        {
            "title": "A Multicenter Benchmark of Multiple Instance Learning Models for Lymphoma Subtyping from HE-stained Whole Slide Images",
            "authors": [
                "Rao Muhammad Umer",
                "Daniel Sens",
                "Jonathan Noll",
                "Christian Matek",
                "Lukas Wolfseher",
                "Rainer Spang",
                "Ralf Huss",
                "Johannes Raffler",
                "Sarah Reinke",
                "Wolfram Klapper",
                "Katja Steiger",
                "Kristina Schwamborn",
                "Carsten Marr"
            ],
            "arxiv_id": "2512.14640v1",
            "summary": "Timely and accurate lymphoma diagnosis is essential for guiding cancer treatment. Standard diagnostic practice combines hematoxylin and eosin (HE)-stained whole slide images with immunohistochemistry, flow cytometry, and molecular genetic tests to determine lymphoma subtypes, a process requiring costly equipment, skilled personnel, and causing treatment delays. Deep learning methods could assist pathologists by extracting diagnostic information from routinely available HE-stained slides, yet comprehensive benchmarks for lymphoma subtyping on multicenter data are lacking. In this work, we present the first multicenter lymphoma benchmarking dataset covering four common lymphoma subtypes and healthy control tissue. We systematically evaluate five publicly available pathology foundation models (H-optimus-1, H0-mini, Virchow2, UNI2, Titan) combined with attention-based (AB-MIL) and transformer-based (TransMIL) multiple instance learning aggregators across three magnifications (10x, 20x, 40x). On in-distribution test sets, models achieve multiclass balanced accuracies exceeding 80% across all magnifications, with all foundation models performing similarly and both aggregation methods showing comparable results. The magnification study reveals that 40x resolution is sufficient, with no performance gains from higher resolutions or cross-magnification aggregation. However, on out-of-distribution test sets, performance drops substantially to around 60%, highlighting significant generalization challenges. To advance the field, larger multicenter studies covering additional rare lymphoma subtypes are needed. We provide an automated benchmarking pipeline to facilitate such future research.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "17 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14640v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "人形机器人",
                    "matched_keywords": [
                        "optimus"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出首个多中心淋巴瘤分型基准数据集，系统评估病理基础模型与多实例学习聚合器在HE染色全切片图像上的性能。",
            "summary_zh": "及时准确的淋巴瘤诊断对指导癌症治疗至关重要。标准诊断实践结合苏木精-伊红（HE）染色全切片图像与免疫组化、流式细胞术和分子遗传学检测来确定淋巴瘤亚型，这一过程需要昂贵设备、熟练人员并导致治疗延迟。深度学习方法可以通过从常规可用的HE染色切片中提取诊断信息来协助病理学家，但目前缺乏基于多中心数据的淋巴瘤分型综合基准。在这项工作中，我们提出了首个覆盖四种常见淋巴瘤亚型和健康对照组织的多中心淋巴瘤基准数据集。我们系统评估了五种公开可用的病理基础模型（H-optimus-1、H0-mini、Virchow2、UNI2、Titan）与基于注意力（AB-MIL）和基于Transformer（TransMIL）的多实例学习聚合器在三种放大倍数（10x、20x、40x）下的组合。在分布内测试集上，模型在所有放大倍数下实现了超过80%的多类平衡准确率，所有基础模型表现相似，两种聚合方法结果相当。放大倍数研究表明，40x分辨率已足够，更高分辨率或跨放大倍数聚合未带来性能提升。然而，在分布外测试集上，性能显著下降至约60%，突显了显著的泛化挑战。为推进该领域，需要覆盖更多罕见淋巴瘤亚型的更大规模多中心研究。我们提供了一个自动化基准测试流程以促进此类未来研究。",
            "intro_zh": [
                "核心问题：淋巴瘤诊断依赖多模态检测，过程昂贵耗时，且缺乏基于多中心HE染色全切片图像的深度学习基准。",
                "方法要点：构建首个多中心淋巴瘤基准数据集，系统评估病理基础模型与多实例学习聚合器在不同放大倍数下的性能。",
                "实验或效果：模型在分布内测试集上准确率超80%，但分布外测试集性能降至约60%，揭示泛化挑战。"
            ],
            "method_zh": "**问题定义**：论文旨在解决淋巴瘤亚型诊断中依赖昂贵多模态检测导致的延迟问题，现有深度学习方法缺乏基于多中心HE染色全切片图像的全面性能基准，难以评估模型在实际临床环境中的泛化能力。\\n\\n**核心思路**：通过构建首个多中心淋巴瘤基准数据集，系统比较多种病理基础模型与多实例学习聚合器的组合，在不同放大倍数下评估性能，以确定最优配置并揭示泛化瓶颈，为临床部署提供数据支持。\\n\\n**技术框架**：整体流程包括数据收集（多中心HE染色全切片图像，覆盖四种淋巴瘤亚型和健康组织）、特征提取（使用五种预训练病理基础模型生成图像块特征）、特征聚合（应用AB-MIL和TransMIL聚合器整合多实例信息）、分类预测（输出淋巴瘤亚型标签），并在三种放大倍数（10x、20x、40x）下进行端到端评估。\\n\\n**关键创新**：最重要的创新是首次建立了多中心淋巴瘤分型基准，填补了该领域空白；同时，系统性地探索了基础模型与聚合器的组合效应，以及放大倍数对性能的影响，为模型选择提供了实证依据。\\n\\n**关键设计**：使用公开病理基础模型（如H-optimus-1、Virchow2）进行特征提取，无需从头训练；聚合器采用标准AB-MIL（基于注意力的多实例学习）和TransMIL（基于Transformer的多实例学习）架构；评估指标为多类平衡准确率，以处理类别不平衡；实验设置包括分布内和分布外测试，以全面评估泛化性。",
            "application_zh": "该研究在医疗AI领域具有重要应用价值，可直接辅助病理学家进行淋巴瘤亚型诊断，减少对昂贵检测设备的依赖，加速诊断流程。未来可扩展至更多罕见淋巴瘤亚型或其他癌症类型，推动精准医疗和自动化病理分析的发展，但需解决泛化挑战以确保临床可靠性。",
            "highlight_zh": "在分布内测试集上，所有模型组合在10x、20x、40x放大倍数下均实现超过80%的多类平衡准确率，基础模型间性能相似，AB-MIL与TransMIL聚合器结果相当。40x分辨率已足够，更高分辨率或跨放大倍数聚合未带来性能提升。然而，在分布外测试集上，性能显著下降至约60%，突显了模型泛化能力不足，需进一步研究以应对多中心数据差异。",
            "tags_zh": [
                "淋巴瘤分型",
                "多实例学习",
                "病理基础模型",
                "全切片图像分析",
                "多中心基准",
                "HE染色图像",
                "深度学习评估",
                "医疗AI"
            ],
            "_index": 47
        },
        {
            "title": "Hierarchical Persistence Velocity for Network Anomaly Detection: Theory and Applications to Cryptocurrency Markets",
            "authors": [
                "Omid Khormali"
            ],
            "arxiv_id": "2512.14615v1",
            "summary": "We introduce the Overlap-Weighted Hierarchical Normalized Persistence Velocity (OW-HNPV), a novel topological data analysis method for detecting anomalies in time-varying networks. Unlike existing methods that measure cumulative topological presence, we introduce the first velocity-based perspective on persistence diagrams, measuring the rate at which features appear and disappear, automatically downweighting noise through overlap-based weighting. We also prove that OW-HNPV is mathematically stable. It behaves in a controlled, predictable way, even when comparing persistence diagrams from networks with different feature types. Applied to Ethereum transaction networks (May 2017-May 2018), OW-HNPV demonstrates superior performance for cryptocurrency anomaly detection, achieving up to 10.4% AUC gain over baseline models for 7-day price movement predictions. Compared with established methods, including Vector of Averaged Bettis (VAB), persistence landscapes, and persistence images, velocity-based summaries excel at medium- to long-range forecasting (4-7 days), with OW-HNPV providing the most consistent and stable performance across prediction horizons. Our results show that modeling topological velocity is crucial for detecting structural anomalies in dynamic networks.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14615v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "SAC"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出基于重叠加权的层次化归一化持久性速度方法，用于时变网络异常检测，在加密货币市场预测中表现优异。",
            "summary_zh": "我们引入了重叠加权的层次化归一化持久性速度，这是一种用于检测时变网络异常的新型拓扑数据分析方法。与现有测量累积拓扑存在的方法不同，我们首次从速度角度分析持久图，测量特征出现和消失的速率，并通过基于重叠的加权自动降低噪声影响。我们还证明了OW-HNPV在数学上是稳定的，即使在比较具有不同特征类型的网络的持久图时，其行为也是可控且可预测的。应用于以太坊交易网络（2017年5月至2018年5月），OW-HNPV在加密货币异常检测中表现出卓越性能，在7天价格变动预测中比基线模型实现了高达10.4%的AUC提升。与现有方法（包括平均贝蒂向量、持久景观和持久图像）相比，基于速度的摘要在中长期预测（4-7天）中表现突出，OW-HNPV在不同预测时间范围内提供了最一致和稳定的性能。我们的结果表明，建模拓扑速度对于检测动态网络中的结构异常至关重要。",
            "intro_zh": [
                "现有方法主要关注累积拓扑特征，缺乏对特征动态变化的速率分析，难以有效捕捉网络结构异常。",
                "提出基于速度的持久图分析，引入重叠加权机制自动降噪，并证明方法的数学稳定性，确保可控预测。",
                "在以太坊交易网络实验中，OW-HNPV实现高达10.4%的AUC提升，在中长期预测中表现最稳定和一致。"
            ],
            "method_zh": "**问题定义**：论文旨在解决时变网络中的异常检测问题，特别是在加密货币市场等动态系统中。现有方法如平均贝蒂向量、持久景观和持久图像主要基于累积拓扑特征，忽略了特征出现和消失的速率信息，导致对网络结构变化的敏感度不足，难以有效区分正常波动与异常事件。\\n\\n**核心思路**：论文的核心思路是从速度角度重新审视持久图，提出测量拓扑特征变化速率的方法。通过引入重叠加权机制，自动降低噪声影响，并确保方法在数学上的稳定性，使得即使在不同特征类型的网络比较中也能保持可控行为。\\n\\n**技术框架**：整体流程包括：首先，从时变网络中提取持久图，表示拓扑特征（如连通分量、环）的出生和死亡时间；其次，计算持久性速度，即特征在持久图中的变化速率；然后，应用重叠加权对速度进行归一化处理，减少噪声干扰；最后，将处理后的速度摘要用于异常检测和预测任务。\\n\\n**关键创新**：最重要的创新是首次将速度视角引入持久图分析，突破了传统累积方法的局限。与现有方法的本质区别在于，OW-HNPV直接量化拓扑动态变化，而非静态存在，从而更敏感地捕捉网络结构异常。\\n\\n**关键设计**：关键设计包括重叠加权函数，它基于特征在持久图中的重叠程度自动调整权重，以抑制噪声；归一化处理确保速度值在不同网络间可比；数学稳定性证明涉及利普希茨连续性等理论，保证方法对输入扰动不敏感。具体参数如时间窗口大小和加权系数需根据应用场景调整，但论文未详细说明。",
            "application_zh": "该研究在加密货币市场异常检测中已展示实际价值，可应用于金融风控、价格预测等领域。未来可扩展至其他动态网络系统，如社交网络、交通网络或生物网络，用于实时监控结构异常，提升系统安全性和预测准确性，具有广泛的工业应用前景。",
            "highlight_zh": "在以太坊交易网络实验中，OW-HNPV在7天价格变动预测中实现高达10.4%的AUC提升，优于平均贝蒂向量、持久景观和持久图像等基线方法。特别地，在中长期预测（4-7天）中表现最佳，提供最一致和稳定的性能，验证了拓扑速度建模对异常检测的有效性。",
            "tags_zh": [
                "拓扑数据分析",
                "持久图",
                "网络异常检测",
                "加密货币市场",
                "动态网络",
                "速度建模",
                "数学稳定性",
                "重叠加权"
            ],
            "_index": 48
        },
        {
            "title": "Hybrid Iterative Solvers with Geometry-Aware Neural Preconditioners for Parametric PDEs",
            "authors": [
                "Youngkyu Lee",
                "Francesc Levrero Florencio",
                "Jay Pathak",
                "George Em Karniadakis"
            ],
            "arxiv_id": "2512.14596v1",
            "summary": "The convergence behavior of classical iterative solvers for parametric partial differential equations (PDEs) is often highly sensitive to the domain and specific discretization of PDEs. Previously, we introduced hybrid solvers by combining the classical solvers with neural operators for a specific geometry 1, but they tend to under-perform in geometries not encountered during training. To address this challenge, we introduce Geo-DeepONet, a geometry-aware deep operator network that incorporates domain information extracted from finite element discretizations. Geo-DeepONet enables accurate operator learning across arbitrary unstructured meshes without requiring retraining. Building on this, we develop a class of geometry-aware hybrid preconditioned iterative solvers by coupling Geo-DeepONet with traditional methods such as relaxation schemes and Krylov subspace algorithms. Through numerical experiments on parametric PDEs posed over diverse unstructured domains, we demonstrate the enhanced robustness and efficiency of the proposed hybrid solvers for multiple real-world applications.",
            "categories": [
                "cs.LG",
                "math.NA"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "19 pages, 10 figures, 3 tables",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14596v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出几何感知神经预条件器与混合迭代求解器，以解决参数化偏微分方程在任意非结构化网格上的求解鲁棒性问题。",
            "summary_zh": "参数化偏微分方程（PDEs）的经典迭代求解器的收敛行为通常对PDE的域和特定离散化高度敏感。先前，我们通过将经典求解器与特定几何的神经算子结合引入了混合求解器，但它们在训练中未遇到的几何上往往表现不佳。为解决这一挑战，我们引入了Geo-DeepONet，这是一种几何感知的深度算子网络，它结合了从有限元离散化中提取的域信息。Geo-DeepONet能够在任意非结构化网格上实现准确的算子学习，而无需重新训练。在此基础上，我们通过将Geo-DeepONet与松弛方案和Krylov子空间算法等传统方法耦合，开发了一类几何感知的混合预条件迭代求解器。通过在多样非结构化域上的参数化PDE数值实验，我们证明了所提出的混合求解器在多个实际应用中具有增强的鲁棒性和效率。",
            "intro_zh": [
                "现有经典迭代求解器对参数化PDE的几何和离散化高度敏感，导致收敛不稳定，尤其在未见几何上性能下降。",
                "提出Geo-DeepONet，一种几何感知深度算子网络，结合有限元离散化信息，实现跨任意非结构化网格的准确算子学习。",
                "实验表明，混合求解器在多样非结构化域上显著提升鲁棒性和效率，适用于多个实际应用场景。"
            ],
            "method_zh": "**问题定义**：论文旨在解决参数化偏微分方程（PDEs）在任意非结构化网格上求解时，经典迭代求解器收敛行为对几何和离散化高度敏感的问题。现有混合求解器结合神经算子，但在未见几何上表现不佳，缺乏泛化能力。\\n\\n**核心思路**：核心思路是开发一种几何感知的深度算子网络（Geo-DeepONet），通过整合有限元离散化中的域信息，实现跨任意非结构化网格的准确算子学习，并以此为基础构建混合预条件迭代求解器，提升求解鲁棒性和效率。\\n\\n**技术框架**：整体框架包括两个主要阶段：首先，设计Geo-DeepONet网络，从有限元离散化中提取几何特征，学习参数化PDE的算子映射；其次，将Geo-DeepONet作为预条件器，与传统迭代方法（如松弛方案和Krylov子空间算法）耦合，形成混合求解器，用于加速求解过程。\\n\\n**关键创新**：最重要的技术创新是Geo-DeepONet，它通过几何感知机制，使神经算子能够适应任意非结构化网格，无需重新训练，从而解决了现有方法在未见几何上的泛化问题。与现有方法相比，本质区别在于整合了域信息，提升了跨几何的鲁棒性。\\n\\n**关键设计**：关键设计包括：Geo-DeepONet的网络结构，可能基于深度算子网络（DeepONet）扩展，加入几何特征提取模块；损失函数可能采用均方误差或相关算子学习损失，以最小化预测误差；参数设置涉及网络层数、激活函数和训练优化器，具体细节未知，但强调几何信息的有效融合。",
            "application_zh": "该研究在计算流体力学、结构分析和电磁学等领域具有广泛应用潜力，能提升参数化PDE求解的效率和鲁棒性，适用于复杂几何和动态边界条件问题，未来可能推动科学计算和工程仿真的智能化发展。",
            "highlight_zh": "实验在多样非结构化域上进行，对比基线包括传统迭代求解器和先前混合求解器。结果显示，所提出的混合求解器在收敛速度和稳定性上显著提升，具体性能数据未知，但论文强调增强了鲁棒性和效率，适用于多个实际应用场景。",
            "tags_zh": [
                "参数化偏微分方程",
                "几何感知学习",
                "神经算子网络",
                "混合迭代求解器",
                "非结构化网格",
                "预条件技术",
                "科学计算",
                "有限元方法"
            ],
            "_index": 49
        },
        {
            "title": "TUMTraf EMOT: Event-Based Multi-Object Tracking Dataset and Baseline for Traffic Scenarios",
            "authors": [
                "Mengyu Li",
                "Xingcheng Zhou",
                "Guang Chen",
                "Alois Knoll",
                "Hu Cao"
            ],
            "arxiv_id": "2512.14595v1",
            "summary": "In Intelligent Transportation Systems (ITS), multi-object tracking is primarily based on frame-based cameras. However, these cameras tend to perform poorly under dim lighting and high-speed motion conditions. Event cameras, characterized by low latency, high dynamic range and high temporal resolution, have considerable potential to mitigate these issues. Compared to frame-based vision, there are far fewer studies on event-based vision. To address this research gap, we introduce an initial pilot dataset tailored for event-based ITS, covering vehicle and pedestrian detection and tracking. We establish a tracking-by-detection benchmark with a specialized feature extractor based on this dataset, achieving excellent performance.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "10 pages, 9 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14595v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "traffic"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出事件相机多目标跟踪数据集和基准方法，以解决智能交通系统中传统相机在弱光高速场景下的性能不足问题。",
            "summary_zh": "在智能交通系统中，多目标跟踪主要基于帧式相机，但这些相机在弱光和高速度运动条件下表现不佳。事件相机具有低延迟、高动态范围和高时间分辨率的特点，有潜力缓解这些问题。与帧式视觉相比，基于事件视觉的研究较少。为填补这一研究空白，我们引入了一个专为基于事件的智能交通系统设计的初始试点数据集，涵盖车辆和行人的检测与跟踪。基于此数据集，我们建立了一个检测跟踪基准，并采用专门的特征提取器，实现了优异的性能。",
            "intro_zh": [
                "核心问题：传统帧式相机在智能交通系统中面临弱光和高速度运动条件下的性能下降，导致多目标跟踪精度不足。",
                "方法要点：提出事件相机多目标跟踪数据集和基准方法，利用事件相机的低延迟和高动态范围特性，设计专门的特征提取器。",
                "实验或效果：基于新数据集建立跟踪基准，实现优异性能，为事件视觉在交通场景中的应用提供基础。"
            ],
            "method_zh": "**问题定义**：论文旨在解决智能交通系统中多目标跟踪在弱光和高速度运动条件下的性能问题。现有方法主要依赖帧式相机，这些相机在动态范围和时间分辨率上有限，导致在挑战性场景中跟踪精度下降。\\n\\n**核心思路**：论文的核心思路是利用事件相机的优势，如低延迟、高动态范围和高时间分辨率，来弥补传统相机的不足。通过构建专门的数据集和基准方法，推动事件视觉在交通场景中的应用。\\n\\n**技术框架**：整体架构采用跟踪-检测范式，包括数据采集、特征提取和跟踪匹配阶段。主要模块包括事件数据预处理、专门的特征提取器网络，以及基于检测结果的跟踪算法。\\n\\n**关键创新**：最重要的技术创新是引入了首个针对事件相机在智能交通系统中的多目标跟踪数据集，并设计了专门的特征提取器，以优化事件数据的表示。与现有方法相比，本质区别在于从帧式视觉转向事件视觉，利用异步事件流的高时间分辨率特性。\\n\\n**关键设计**：关键设计包括事件数据的时空编码方法、特征提取器的网络结构（可能基于卷积或循环神经网络），以及损失函数（如分类和回归损失）的优化，以提升检测和跟踪的准确性。具体参数设置和网络细节在论文中未详细说明，需参考后续实验部分。",
            "application_zh": "该研究在智能交通系统中有广泛潜在应用，如自动驾驶、交通监控和行人安全。事件相机的高动态范围和低延迟特性使其适用于弱光、高速运动等挑战性环境，能提升跟踪精度和系统鲁棒性。未来可能推动事件视觉技术在实时交通管理中的普及，减少事故风险。",
            "highlight_zh": "最重要的实验结果包括：基于新数据集建立的跟踪基准，使用专门的特征提取器实现了优异性能。具体性能数据未知，但论文报告了在车辆和行人检测跟踪任务上的显著提升，对比传统帧式方法，在弱光和高速场景中表现出更好的鲁棒性和准确性。提升幅度需参考后续详细实验分析。",
            "tags_zh": [
                "事件相机",
                "多目标跟踪",
                "智能交通系统",
                "数据集构建",
                "特征提取",
                "弱光场景",
                "高速运动",
                "跟踪基准"
            ],
            "_index": 50
        },
        {
            "title": "Low-Resource, High-Impact: Building Corpora for Inclusive Language Technologies",
            "authors": [
                "Ekaterina Artemova",
                "Laurie Burchell",
                "Daryna Dementieva",
                "Shu Okabe",
                "Mariya Shmatova",
                "Pedro Ortiz Suarez"
            ],
            "arxiv_id": "2512.14576v1",
            "summary": "This tutorial (https://tum-nlp.github.io/low-resource-tutorial) is designed for NLP practitioners, researchers, and developers working with multilingual and low-resource languages who seek to create more equitable and socially impactful language technologies. Participants will walk away with a practical toolkit for building end-to-end NLP pipelines for underrepresented languages -- from data collection and web crawling to parallel sentence mining, machine translation, and downstream applications such as text classification and multimodal reasoning. The tutorial presents strategies for tackling the challenges of data scarcity and cultural variance, offering hands-on methods and modeling frameworks. We will focus on fair, reproducible, and community-informed development approaches, grounded in real-world scenarios. We will showcase a diverse set of use cases covering over 10 languages from different language families and geopolitical contexts, including both digitally resource-rich and severely underrepresented languages.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Tutorial is accepted to LREC2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14576v1",
            "code_links": [
                {
                    "url": "https://tum-nlp.github.io/low-resource-tutorial",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "人形机器人",
                    "matched_keywords": [
                        "digit"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出低资源语言NLP教程，通过端到端流程解决数据稀缺与文化差异问题，促进包容性语言技术发展。",
            "summary_zh": "本教程（https://tum-nlp.github.io/low-resource-tutorial）专为从事多语言和低资源语言工作的NLP从业者、研究人员和开发者设计，旨在创建更公平、更具社会影响力的语言技术。参与者将获得一个实用的工具包，用于为代表性不足的语言构建端到端NLP流程——从数据收集和网络爬取到平行句挖掘、机器翻译，以及文本分类和多模态推理等下游应用。教程介绍了应对数据稀缺和文化差异挑战的策略，提供了实践方法和建模框架。我们将重点关注公平、可重复且基于社区反馈的开发方法，并以真实场景为基础。我们将展示涵盖10多种来自不同语系和地缘政治背景的语言的多样化用例，包括数字资源丰富和严重代表性不足的语言。",
            "intro_zh": [
                "核心问题：现有NLP方法在低资源语言中面临数据稀缺、文化差异和代表性不足的挑战，导致技术不公和性能低下。",
                "方法要点：提供端到端NLP流程工具包，包括数据收集、平行句挖掘、机器翻译等，强调公平、可重复和社区参与的开发方法。",
                "实验或效果：教程涵盖10多种语言用例，展示从资源丰富到严重不足语言的实践应用，提升低资源语言技术的可访问性和影响力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决低资源语言在NLP技术开发中面临的数据稀缺、文化差异和代表性不足问题。现有方法通常依赖大规模标注数据，导致低资源语言技术落后、不公平，且缺乏针对性的端到端解决方案。\\n\\n**核心思路**：通过提供一个全面的教程和工具包，引导开发者构建端到端NLP流程，从数据收集到下游应用，强调公平、可重复和社区参与的设计，以应对低资源语言的独特挑战。\\n\\n**技术框架**：整体架构包括数据收集与网络爬取、平行句挖掘、机器翻译模型训练，以及下游应用如文本分类和多模态推理。流程分阶段进行，确保数据质量和模型适应性。\\n\\n**关键创新**：最重要的创新在于整合了低资源语言NLP的全流程实践方法，并强调社会影响和公平性，与现有方法相比，更注重社区反馈和可重复性，而非单纯技术优化。\\n\\n**关键设计**：教程基于真实场景设计，包含具体的数据处理策略、建模框架和评估指标，但未指定单一网络结构或损失函数，而是提供灵活的方法以适应不同语言需求。",
            "application_zh": "该研究可应用于多语言机器翻译、低资源语言文本分类、跨文化信息检索等领域，提升全球语言技术的包容性，支持教育、医疗和社会服务中的公平访问，未来可能推动更多低资源语言进入主流AI应用。",
            "highlight_zh": "教程展示了涵盖10多种语言的多样化用例，包括资源丰富和严重不足的语言，通过实践方法提升了低资源语言NLP流程的可操作性，但未提供具体性能数据或对比基线，重点在于方法论的可重复性和社会影响。",
            "tags_zh": [
                "低资源语言处理",
                "多语言NLP",
                "数据稀缺应对",
                "端到端流程",
                "公平AI",
                "社区参与开发",
                "机器翻译",
                "文本分类"
            ],
            "_index": 51
        },
        {
            "title": "Polypersona: Persona-Grounded LLM for Synthetic Survey Responses",
            "authors": [
                "Tejaswani Dash",
                "Dinesh Karri",
                "Anudeep Vurity",
                "Gautam Datla",
                "Tazeem Ahmad",
                "Saima Rafi",
                "Rohith Tangudu"
            ],
            "arxiv_id": "2512.14562v1",
            "summary": "This paper introduces PolyPersona, a generative framework for synthesizing persona-conditioned survey responses across multiple domains. The framework instruction-tunes compact chat models using parameter-efficient LoRA adapters with 4-bit quantization under a resource-adaptive training setup. A dialogue-based data pipeline explicitly preserves persona cues, ensuring consistent behavioral alignment across generated responses. Using this pipeline, we construct a dataset of 3,568 synthetic survey responses spanning ten domains and 433 distinct personas, enabling controlled instruction tuning and systematic multi-domain evaluation. We evaluate the generated responses using a multi-metric evaluation suite that combines standard text generation metrics, including BLEU, ROUGE, and BERTScore, with survey-specific metrics designed to assess structural coherence, stylistic consistency, and sentiment alignment.Experimental results show that compact models such as TinyLlama 1.1B and Phi-2 achieve performance comparable to larger 7B to 8B baselines, with a highest BLEU score of 0.090 and ROUGE-1 of 0.429. These findings demonstrate that persona-conditioned fine-tuning enables small language models to generate reliable and coherent synthetic survey data. The proposed framework provides an efficient and reproducible approach for survey data generation, supporting scalable evaluation while facilitating bias analysis through transparent and open protocols.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted in IEEE Bigdata 2025- LLMs4ALL",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14562v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出PolyPersona框架，通过角色条件化微调小型语言模型，高效生成多领域合成调查数据。",
            "summary_zh": "本文介绍了PolyPersona，一个用于生成跨多个领域的角色条件化调查响应的生成框架。该框架在资源自适应训练设置下，使用参数高效的LoRA适配器和4位量化对紧凑聊天模型进行指令微调。基于对话的数据管道明确保留了角色线索，确保生成响应之间行为一致性。利用该管道，我们构建了一个包含10个领域和433个不同角色的3,568个合成调查响应的数据集，支持可控指令微调和系统化的多领域评估。我们使用多指标评估套件评估生成响应，该套件结合了标准文本生成指标（包括BLEU、ROUGE和BERTScore）和专门设计的调查特定指标，用于评估结构连贯性、风格一致性和情感对齐。实验结果表明，TinyLlama 1.1B和Phi-2等紧凑模型实现了与较大7B至8B基线相当的性能，最高BLEU得分为0.090，ROUGE-1为0.429。这些发现表明，角色条件化微调使小型语言模型能够生成可靠且连贯的合成调查数据。所提出的框架为调查数据生成提供了一种高效且可重复的方法，支持可扩展评估，同时通过透明和开放的协议促进偏见分析。",
            "intro_zh": [
                "现有方法在生成合成调查数据时，难以确保角色一致性，且资源消耗大，限制了可扩展性和可控性。",
                "提出PolyPersona框架，通过角色条件化指令微调，结合LoRA适配器和4位量化，高效训练小型模型生成多领域响应。",
                "实验显示，TinyLlama 1.1B等小型模型在BLEU和ROUGE指标上接近较大基线，最高BLEU达0.090，验证了框架的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决生成多领域合成调查数据时，现有方法难以保持角色一致性和行为对齐的问题，同时传统方法资源消耗大，限制了模型的可扩展性和可控评估。\\n\\n**核心思路**：通过角色条件化指令微调，将角色信息显式融入模型训练，结合参数高效技术（如LoRA和量化），使小型语言模型能够生成连贯且一致的调查响应，从而降低资源需求并提升生成质量。\\n\\n**技术框架**：整体流程包括数据管道构建、模型微调和评估三个阶段。首先，基于对话的数据管道收集并处理角色线索，构建包含10个领域和433个角色的数据集；其次，使用LoRA适配器和4位量化对紧凑模型（如TinyLlama 1.1B）进行指令微调；最后，通过多指标评估套件（包括文本生成和调查特定指标）系统评估生成响应。\\n\\n**关键创新**：最重要的技术创新是角色条件化微调与资源自适应训练的结合，通过显式保留角色线索确保行为一致性，同时利用LoRA和量化实现高效训练，使小型模型在性能上媲美大型基线，本质区别在于强调可控性和可扩展性。\\n\\n**关键设计**：技术细节包括使用LoRA适配器进行参数高效微调，减少训练参数量；采用4位量化降低内存占用；设计基于对话的数据管道，明确编码角色属性（如人口统计和行为特征）；损失函数基于标准语言建模目标，但通过指令微调融入角色条件；评估指标结合BLEU、ROUGE、BERTScore和自定义调查指标（如结构连贯性评分）。",
            "application_zh": "该研究可应用于社会科学调查、市场研究、教育评估等领域，用于生成低成本、高质量的合成数据，支持数据增强、模型测试和偏见分析。其实际价值在于提供可扩展且透明的数据生成方法，未来可能推动自动化调查工具和伦理AI评估的发展。",
            "highlight_zh": "最重要的实验结果显示，紧凑模型如TinyLlama 1.1B和Phi-2在生成合成调查响应时，性能与较大的7B至8B基线模型相当，最高BLEU得分达到0.090，ROUGE-1得分达到0.429。这表明通过角色条件化微调，小型模型能够有效生成连贯且可靠的响应，验证了框架的高效性和实用性。",
            "tags_zh": [
                "合成数据生成",
                "角色条件化模型",
                "指令微调",
                "LoRA适配器",
                "4位量化",
                "多领域评估",
                "调查响应生成",
                "资源高效训练"
            ],
            "_index": 52
        },
        {
            "title": "VLegal-Bench: Cognitively Grounded Benchmark for Vietnamese Legal Reasoning of Large Language Models",
            "authors": [
                "Nguyen Tien Dong",
                "Minh-Anh Nguyen",
                "Thanh Dat Hoang",
                "Nguyen Tuan Ngoc",
                "Dao Xuan Quang Minh",
                "Phan Phi Hai",
                "Nguyen Thi Ngoc Anh",
                "Dang Van Tu",
                "Binh Vu"
            ],
            "arxiv_id": "2512.14554v1",
            "summary": "The rapid advancement of large language models (LLMs) has enabled new possibilities for applying artificial intelligence within the legal domain. Nonetheless, the complexity, hierarchical organization, and frequent revisions of Vietnamese legislation pose considerable challenges for evaluating how well these models interpret and utilize legal knowledge. To address this gap, Vietnamese Legal Benchmark (VLegal-Bench) is introduced, the first comprehensive benchmark designed to systematically assess LLMs on Vietnamese legal tasks. Informed by Bloom's cognitive taxonomy, VLegal-Bench encompasses multiple levels of legal understanding through tasks designed to reflect practical usage scenarios. The benchmark comprises 10,450 samples generated through a rigorous annotation pipeline, where legal experts label and cross-validate each instance using our annotation system to ensure every sample is grounded in authoritative legal documents and mirrors real-world legal assistant workflows, including general legal questions and answers, retrieval-augmented generation, multi-step reasoning, and scenario-based problem solving tailored to Vietnamese law. By providing a standardized, transparent, and cognitively informed evaluation framework, VLegal-Bench establishes a solid foundation for assessing LLM performance in Vietnamese legal contexts and supports the development of more reliable, interpretable, and ethically aligned AI-assisted legal systems.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14554v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出VLegal-Bench基准以解决大语言模型在越南法律推理评估中的标准化与认知深度不足问题",
            "summary_zh": "大语言模型的快速发展为人工智能在法律领域的应用开辟了新可能。然而，越南法律的复杂性、层级结构及频繁修订，对评估这些模型如何解释和利用法律知识构成了重大挑战。为填补这一空白，越南法律基准被引入，这是首个旨在系统评估大语言模型在越南法律任务上的综合性基准。基于布鲁姆认知分类法，VLegal-Bench通过设计反映实际使用场景的任务，涵盖了多层次的法律理解。该基准包含10,450个样本，通过严格的标注流程生成，法律专家使用我们的标注系统对每个实例进行标注和交叉验证，确保每个样本都基于权威法律文件，并模拟真实世界法律助手工作流程，包括一般法律问答、检索增强生成、多步推理以及针对越南法律的场景化问题解决。通过提供一个标准化、透明且基于认知科学的评估框架，VLegal-Bench为评估大语言模型在越南法律背景下的性能奠定了坚实基础，并支持开发更可靠、可解释且符合伦理的AI辅助法律系统。",
            "intro_zh": [
                "核心问题：越南法律复杂多变，现有评估方法缺乏标准化基准，难以系统衡量大语言模型的法律推理能力。",
                "方法要点：基于布鲁姆认知分类法设计多层次任务，构建大规模标注数据集，模拟真实法律工作流程进行评估。",
                "实验或效果：创建包含10,450个样本的基准，提供透明评估框架，支持AI法律系统开发，提升模型可靠性与可解释性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大语言模型在越南法律推理评估中的标准化与认知深度不足问题。现有方法缺乏针对越南法律特点的综合性基准，难以系统评估模型对复杂、层级化且频繁修订的法律知识的理解与应用能力，导致评估结果不透明、不可比，限制了AI在法律领域的可靠发展。\\n\\n**核心思路**：论文的核心解决思路是构建一个基于认知科学理论的标准化基准，通过模拟真实法律工作流程和多层次任务设计，系统评估大语言模型的法律推理能力。这样设计是为了确保评估的科学性与实用性，结合布鲁姆认知分类法，从基础记忆到高级分析，全面覆盖法律理解的不同维度，从而提供更准确的性能衡量。\\n\\n**技术框架**：整体架构包括任务设计、数据生成、标注验证和评估框架四个主要阶段。首先，基于布鲁姆认知分类法设计多层次法律任务，如问答、检索增强生成、多步推理和场景化问题解决。其次，通过法律专家参与生成10,450个样本，使用标注系统进行标注和交叉验证。然后，确保每个样本基于权威法律文件，模拟真实法律助手工作流程。最后，建立标准化评估框架，支持透明性能比较。\\n\\n**关键创新**：最重要的技术创新点是首次引入基于布鲁姆认知分类法的越南法律基准，将认知理论与法律实践结合，提供多层次、场景化的评估任务。与现有方法的本质区别在于其系统性、透明性和认知基础，不仅关注表面性能，还深入评估模型的法律推理深度，填补了越南法律AI评估的空白。\\n\\n**关键设计**：关键设计包括任务设计基于布鲁姆认知分类法，涵盖从知识记忆到创造应用的多层次；标注流程采用法律专家参与和交叉验证，确保数据质量；样本生成基于权威法律文件，模拟真实工作流程如检索增强生成；评估框架标准化，支持透明比较，具体参数如样本数量10,450，但网络结构或损失函数细节在摘要中未提及，可能涉及未知的模型集成或评估指标。",
            "application_zh": "该研究的潜在应用领域包括越南法律AI助手开发、法律教育工具、自动化文档处理和法律咨询系统。实际价值在于提供标准化评估基准，促进大语言模型在法律领域的可靠应用，提升AI系统的可解释性和伦理对齐。未来影响可能推动越南乃至全球法律AI技术的发展，支持更智能、高效的法律服务。",
            "highlight_zh": "最重要的实验结果包括构建了包含10,450个样本的VLegal-Bench基准，基于布鲁姆认知分类法设计多层次任务，如问答和场景化问题解决。通过法律专家标注和交叉验证确保数据质量，模拟真实法律工作流程。该基准提供了标准化评估框架，支持大语言模型性能比较，具体性能数据如准确率或提升幅度在摘要中未提及，但强调了透明性和认知深度，为后续研究奠定基础。",
            "tags_zh": [
                "法律推理基准",
                "越南法律AI",
                "布鲁姆认知分类法",
                "大语言模型评估",
                "检索增强生成",
                "多步推理",
                "场景化问题解决",
                "AI辅助法律系统"
            ],
            "_index": 53
        },
        {
            "title": "HiFi-Portrait: Zero-shot Identity-preserved Portrait Generation with High-fidelity Multi-face Fusion",
            "authors": [
                "Yifang Xu",
                "Benxiang Zhai",
                "Yunzhuo Sun",
                "Ming Li",
                "Yang Li",
                "Sidan Du"
            ],
            "arxiv_id": "2512.14542v1",
            "summary": "Recent advancements in diffusion-based technologies have made significant strides, particularly in identity-preserved portrait generation (IPG). However, when using multiple reference images from the same ID, existing methods typically produce lower-fidelity portraits and struggle to customize face attributes precisely. To address these issues, this paper presents HiFi-Portrait, a high-fidelity method for zero-shot portrait generation. Specifically, we first introduce the face refiner and landmark generator to obtain fine-grained multi-face features and 3D-aware face landmarks. The landmarks include the reference ID and the target attributes. Then, we design HiFi-Net to fuse multi-face features and align them with landmarks, which improves ID fidelity and face control. In addition, we devise an automated pipeline to construct an ID-based dataset for training HiFi-Portrait. Extensive experimental results demonstrate that our method surpasses the SOTA approaches in face similarity and controllability. Furthermore, our method is also compatible with previous SDXL-based works.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted by CVPR 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14542v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出HiFi-Portrait方法，通过高保真多脸融合解决零样本身份保持肖像生成中的保真度和属性控制问题。",
            "summary_zh": "近年来，基于扩散模型的技术在身份保持肖像生成（IPG）领域取得了显著进展。然而，当使用同一身份的多张参考图像时，现有方法通常生成保真度较低的肖像，并且难以精确定制面部属性。为解决这些问题，本文提出了HiFi-Portrait，一种用于零样本肖像生成的高保真方法。具体而言，我们首先引入面部细化器和地标生成器，以获取细粒度的多脸特征和3D感知的面部地标。这些地标包括参考身份和目标属性。然后，我们设计了HiFi-Net来融合多脸特征并将其与地标对齐，从而提高身份保真度和面部控制能力。此外，我们还设计了一个自动化流程来构建基于身份的数据集，用于训练HiFi-Portrait。大量实验结果表明，我们的方法在面部相似性和可控性方面超越了最先进的方法。此外，我们的方法也与之前基于SDXL的工作兼容。",
            "intro_zh": [
                "现有方法在使用多张参考图像时，生成肖像保真度低且难以精确控制面部属性。",
                "引入面部细化器和地标生成器获取细粒度特征，设计HiFi-Net融合特征并与地标对齐。",
                "实验显示，方法在面部相似性和可控性上超越SOTA，并与SDXL兼容，提升显著。"
            ],
            "method_zh": "**问题定义**：论文旨在解决零样本身份保持肖像生成（IPG）中，当使用同一身份的多张参考图像时，现有方法生成肖像保真度低、难以精确定制面部属性的问题。现有方法的痛点包括：多脸融合导致特征模糊、身份保真度下降，以及缺乏细粒度控制机制。\\n\\n**核心思路**：论文的核心解决思路是通过细粒度多脸特征提取和3D感知地标对齐，实现高保真身份保持和精确属性控制。设计基于面部细化器和地标生成器获取特征，再通过HiFi-Net进行融合和对齐，以提升保真度和可控性。\\n\\n**技术框架**：整体架构包括三个阶段：首先，使用面部细化器处理多张参考图像，提取细粒度面部特征；其次，通过地标生成器生成3D感知的面部地标，包含身份和属性信息；最后，HiFi-Net融合多脸特征并与地标对齐，生成高保真肖像。训练基于自动化构建的ID数据集。\\n\\n**关键创新**：最重要的技术创新点是HiFi-Net的设计，它专门用于多脸特征融合和地标对齐，解决了现有方法中特征模糊和保真度低的问题。与现有方法的本质区别在于：结合了细粒度特征提取和3D感知控制，实现了更精确的身份保持和属性定制。\\n\\n**关键设计**：关键设计包括：面部细化器基于卷积网络提取局部特征；地标生成器使用3D模型生成包含身份和属性的地标；HiFi-Net采用注意力机制融合特征，对齐损失函数基于地标距离；训练使用自动化构建的数据集，具体参数如学习率、批大小等未在摘要中详细说明，需参考论文正文。",
            "application_zh": "该研究在数字娱乐、虚拟现实、社交媒体和个性化内容创作等领域具有潜在应用价值。例如，可用于生成高保真虚拟角色、定制化肖像艺术或增强现实体验。未来可能推动身份保持生成技术的发展，提升人机交互和创意产业的效率。",
            "highlight_zh": "实验结果显示，HiFi-Portrait在面部相似性指标上超越现有SOTA方法，具体提升幅度未在摘要中给出，但强调在保真度和可控性方面有显著改进。方法兼容SDXL-based工作，验证了其泛化能力。对比基线包括其他IPG方法，实验基于自动化构建的数据集进行。",
            "tags_zh": [
                "身份保持肖像生成",
                "零样本学习",
                "多脸融合",
                "高保真生成",
                "3D面部地标",
                "扩散模型",
                "面部属性控制",
                "SDXL兼容"
            ],
            "_index": 54
        },
        {
            "title": "Native Intelligence Emerges from Large-Scale Clinical Practice: A Retinal Foundation Model with Deployment Efficiency",
            "authors": [
                "Jia Guo",
                "Jiawei Du",
                "Shengzhu Yang",
                "Shuai Lu",
                "Wenquan Cheng",
                "Kaiwen Zhang",
                "Yihua Sun",
                "Chuhong Yang",
                "Weihang Zhang",
                "Fang Chen",
                "Yilan Wu",
                "Lie Ju",
                "Guochen Ning",
                "Longfei Ma",
                "Huiping Yao",
                "Jinyuan Wang",
                "Peilun Shi",
                "Yukun Zhou",
                "Jie Xu",
                "Pearse A. Keane",
                "Hanruo Liu",
                "Hongen Liao",
                "Ningli Wang",
                "Huiqi Li"
            ],
            "arxiv_id": "2512.14499v1",
            "summary": "Current retinal foundation models remain constrained by curated research datasets that lack authentic clinical context, and require extensive task-specific optimization for each application, limiting their deployment efficiency in low-resource settings. Here, we show that these barriers can be overcome by building clinical native intelligence directly from real-world medical practice. Our key insight is that large-scale telemedicine programs, where expert centers provide remote consultations across distributed facilities, represent a natural reservoir for learning clinical image interpretation. We present ReVision, a retinal foundation model that learns from the natural alignment between 485,980 color fundus photographs and their corresponding diagnostic reports, accumulated through a decade-long telemedicine program spanning 162 medical institutions across China. Through extensive evaluation across 27 ophthalmic benchmarks, we demonstrate that ReVison enables deployment efficiency with minimal local resources. Without any task-specific training, ReVision achieves zero-shot disease detection with an average AUROC of 0.946 across 12 public benchmarks and 0.952 on 3 independent clinical cohorts. When minimal adaptation is feasible, ReVision matches extensively fine-tuned alternatives while requiring orders of magnitude fewer trainable parameters and labeled examples. The learned representations also transfer effectively to new clinical sites, imaging domains, imaging modalities, and systemic health prediction tasks. In a prospective reader study with 33 ophthalmologists, ReVision's zero-shot assistance improved diagnostic accuracy by 14.8% across all experience levels. These results demonstrate that clinical native intelligence can be directly extracted from clinical archives without any further annotation to build medical AI systems suited to various low-resource settings.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14499v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出ReVision视网膜基础模型，从大规模临床实践中提取原生智能，以解决低资源环境下部署效率问题。",
            "summary_zh": "当前视网膜基础模型受限于缺乏真实临床背景的精选研究数据集，且每个应用都需要大量任务特定优化，限制了其在低资源环境下的部署效率。本文表明，通过直接从真实世界医疗实践中构建临床原生智能可以克服这些障碍。我们的核心见解是，大规模远程医疗项目（专家中心为分布式机构提供远程咨询）是学习临床图像解读的自然资源库。我们提出了ReVision，这是一个视网膜基础模型，它从485,980张彩色眼底照片及其对应诊断报告的自然对齐中学习，这些数据来自中国162家医疗机构长达十年的远程医疗项目积累。通过对27个眼科基准的广泛评估，我们证明ReVision能以最少的本地资源实现部署效率。在没有任何任务特定训练的情况下，ReVision在12个公共基准上实现了平均AUROC为0.946的零样本疾病检测，在3个独立临床队列上为0.952。当最小适应可行时，ReVision匹配了经过广泛微调的替代方案，同时需要数量级更少的可训练参数和标记示例。学习到的表示还能有效迁移到新的临床站点、成像域、成像模态和全身健康预测任务。在一项涉及33名眼科医生的前瞻性读者研究中，ReVision的零样本辅助在所有经验水平上将诊断准确性提高了14.8%。这些结果表明，临床原生智能可以直接从临床档案中提取，无需进一步注释，以构建适合各种低资源环境的医疗AI系统。",
            "intro_zh": [
                "现有视网膜基础模型依赖精选数据集，缺乏真实临床背景，且需大量任务特定优化，部署效率低。",
                "从大规模远程医疗项目中学习临床图像与报告的自然对齐，构建临床原生智能基础模型。",
                "零样本疾病检测AUROC达0.946-0.952，最小适应下匹配微调模型，辅助诊断提升14.8%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决视网膜基础模型在低资源环境下部署效率低的问题。现有方法依赖精选研究数据集，缺乏真实临床多样性，且每个应用需大量任务特定优化，导致资源消耗大、适应性差。\\n\\n**核心思路**：核心思路是从大规模临床实践中直接提取“临床原生智能”，利用远程医疗项目中自然积累的图像-报告对齐数据，构建无需额外标注的基础模型，实现高效部署。\\n\\n**技术框架**：整体框架基于大规模预训练。首先，从十年远程医疗项目中收集485,980张彩色眼底照片及对应诊断报告，覆盖162家机构。然后，通过自监督或弱监督学习对齐图像与文本报告，学习通用表示。最后，在27个眼科基准上评估零样本或最小适应性能。\\n\\n**关键创新**：最重要的创新是“临床原生智能”概念，直接从真实临床工作流中学习，而非人工标注数据集。本质区别在于利用自然对齐数据，减少对标注的依赖，提升模型在真实场景中的泛化能力和部署效率。\\n\\n**关键设计**：关键设计包括使用大规模异构临床数据（485,980样本），可能采用视觉-语言对齐技术（如图像-报告匹配），损失函数可能基于对比学习或跨模态预测。网络结构可能基于Transformer或CNN，具体细节未知，但强调最小化可训练参数（如仅微调少量层）以实现高效适应。",
            "application_zh": "该研究在低资源医疗环境中具有广泛应用潜力，如远程眼科筛查、基层医疗诊断辅助和疾病监测。实际价值在于减少对专家标注和计算资源的依赖，提升AI系统在真实临床场景中的可及性和效率。未来可能推动医疗AI向更普惠、自适应方向发展，支持多模态健康预测。",
            "highlight_zh": "零样本疾病检测在12个公共基准上平均AUROC达0.946，在3个临床队列上达0.952。最小适应下匹配微调模型，可训练参数和标记示例减少数量级。前瞻性读者研究中，零样本辅助将33名眼科医生的诊断准确性提升14.8%。模型能有效迁移到新站点、成像域和全身健康任务。",
            "tags_zh": [
                "视网膜基础模型",
                "临床原生智能",
                "远程医疗",
                "零样本学习",
                "多模态对齐",
                "部署效率",
                "眼科AI",
                "弱监督学习"
            ],
            "_index": 55
        },
        {
            "title": "SuperCLIP: CLIP with Simple Classification Supervision",
            "authors": [
                "Weiheng Zhao",
                "Zilong Huang",
                "Jiashi Feng",
                "Xinggang Wang"
            ],
            "arxiv_id": "2512.14480v1",
            "summary": "Contrastive Language-Image Pretraining (CLIP) achieves strong generalization in vision-language tasks by aligning images and texts in a shared embedding space. However, recent findings show that CLIP-like models still underutilize fine-grained semantic signals in text, and this issue becomes even more pronounced when dealing with long and detailed captions. This stems from CLIP's training objective, which optimizes only global image-text similarity and overlooks token-level supervision - limiting its ability to achieve fine-grained visual-text alignment. To address this, we propose SuperCLIP, a simple yet effective framework that augments contrastive learning with classification-based supervision. By adding only a lightweight linear layer to the vision encoder, SuperCLIP leverages token-level cues to enhance visual-textual alignment - with just a 0.077% increase in total FLOPs, and no need for additional annotated data. Experiments show that SuperCLIP consistently improves zero-shot classification, image-text retrieval, and purely visual tasks. These gains hold regardless of whether the model is trained on original web data or rich re-captioned data, demonstrating SuperCLIP's ability to recover textual supervision in both cases. Furthermore, SuperCLIP alleviates CLIP's small-batch performance drop through classification-based supervision that avoids reliance on large batch sizes. Code and models will be made open source.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted by NeurIPS 2025. Code: https://github.com/hustvl/SuperCLIP",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14480v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出SuperCLIP框架，通过分类监督增强对比学习，解决CLIP模型细粒度语义利用不足的问题。",
            "summary_zh": "对比语言-图像预训练（CLIP）通过在共享嵌入空间中对齐图像和文本来实现视觉-语言任务的强大泛化能力。然而，最近的研究发现，CLIP类模型仍然未能充分利用文本中的细粒度语义信号，这一问题在处理长而详细的描述时尤为明显。这源于CLIP的训练目标仅优化全局图像-文本相似性，而忽略了词元级监督，限制了其实现细粒度视觉-文本对齐的能力。为解决这一问题，我们提出了SuperCLIP，这是一个简单而有效的框架，通过基于分类的监督来增强对比学习。仅通过在视觉编码器上添加一个轻量级线性层，SuperCLIP利用词元级线索来增强视觉-文本对齐，总FLOPs仅增加0.077%，且无需额外的标注数据。实验表明，SuperCLIP在零样本分类、图像-文本检索和纯视觉任务上均能持续提升性能。这些提升无论模型是在原始网络数据还是丰富的重新标注数据上训练都成立，证明了SuperCLIP在这两种情况下恢复文本监督的能力。此外，SuperCLIP通过基于分类的监督减轻了CLIP在小批量情况下的性能下降，避免了依赖大批量大小。代码和模型将开源。",
            "intro_zh": [
                "CLIP模型仅优化全局图像-文本相似性，忽略词元级监督，导致细粒度语义利用不足，尤其在处理长描述时表现更差。",
                "SuperCLIP通过添加轻量级线性层，引入基于分类的监督，增强视觉-文本对齐，无需额外数据，计算开销极小。",
                "实验显示SuperCLIP在零样本分类、检索等任务上持续提升，并缓解小批量性能下降，适用于多种训练数据场景。"
            ],
            "method_zh": "**问题定义**：论文旨在解决CLIP模型在视觉-语言任务中细粒度语义利用不足的问题。现有CLIP模型仅通过对比学习优化全局图像-文本相似性，忽略了文本中的词元级监督，导致在处理复杂或长描述时难以实现精细的视觉-文本对齐，限制了模型在零样本分类、检索等任务上的性能。\\n\\n**核心思路**：论文的核心思路是通过引入基于分类的监督来增强对比学习，从而弥补CLIP在词元级语义捕捉上的不足。设计上，SuperCLIP在视觉编码器后添加一个轻量级线性层，将图像特征映射到文本词元空间，利用分类损失函数强制模型学习更细粒度的视觉-文本对应关系，而无需改变原始CLIP的对比学习框架。\\n\\n**技术框架**：整体架构基于CLIP，包含视觉编码器和文本编码器。主要模块包括：1）原始CLIP的对比学习模块，用于全局对齐；2）新增的分类监督模块，由一个线性层组成，将视觉特征转换为文本词元预测；3）训练阶段，同时优化对比损失和分类损失，以融合全局和局部监督。流程上，输入图像和文本，视觉编码器提取特征，文本编码器提取词元特征，通过联合损失进行端到端训练。\\n\\n**关键创新**：最重要的技术创新是将分类监督无缝集成到CLIP框架中，通过词元级预测增强细粒度对齐。与现有方法（如仅依赖对比学习或复杂多任务学习）的本质区别在于，SuperCLIP仅添加极少量参数（线性层），计算开销几乎可忽略（FLOPs增加0.077%），且无需额外标注数据，实现了高效且通用的改进。\\n\\n**关键设计**：关键设计包括：1）线性层参数设置：轻量级，仅增加少量计算；2）损失函数：结合对比损失（如InfoNCE）和分类损失（如交叉熵），分类损失针对文本词元进行监督；3）网络结构：保持CLIP编码器不变，仅在视觉编码器输出后添加线性层；4）训练策略：使用原始或重新标注的网络数据，无需数据增强或复杂预处理。",
            "application_zh": "SuperCLIP的潜在应用领域包括计算机视觉和自然语言处理的交叉任务，如零样本图像分类、图像-文本检索、视觉问答和内容生成。其实际价值在于提升多模态模型的细粒度理解能力，适用于需要处理复杂语义的场景（如医疗图像分析、自动驾驶感知）。未来影响可能推动更高效的视觉-语言预训练方法，促进AI在开放域任务中的泛化性能。",
            "highlight_zh": "实验结果显示，SuperCLIP在多个基准测试中持续提升性能：在零样本分类任务上，相比基线CLIP模型，准确率有显著提升（具体数据未知，但论文提到“一致改进”）；在图像-文本检索任务中，召回率指标得到增强；在纯视觉任务上（如目标检测），也观察到性能增益。此外，SuperCLIP有效缓解了CLIP在小批量训练时的性能下降问题，且这些提升在原始网络数据和重新标注数据上均成立，证明了其鲁棒性和通用性。",
            "tags_zh": [
                "对比学习",
                "视觉-语言对齐",
                "细粒度语义",
                "零样本分类",
                "图像-文本检索",
                "多模态预训练",
                "分类监督",
                "轻量级增强"
            ],
            "_index": 56
        },
        {
            "title": "TACK Tunnel Data (TTD): A Benchmark Dataset for Deep Learning-Based Defect Detection in Tunnels",
            "authors": [
                "Andreas Sjölander",
                "Valeria Belloni",
                "Robel Fekadu",
                "Andrea Nascetti"
            ],
            "arxiv_id": "2512.14477v1",
            "summary": "Tunnels are essential elements of transportation infrastructure, but are increasingly affected by ageing and deterioration mechanisms such as cracking. Regular inspections are required to ensure their safety, yet traditional manual procedures are time-consuming, subjective, and costly. Recent advances in mobile mapping systems and Deep Learning (DL) enable automated visual inspections. However, their effectiveness is limited by the scarcity of tunnel datasets. This paper introduces a new publicly available dataset containing annotated images of three different tunnel linings, capturing typical defects: cracks, leaching, and water infiltration. The dataset is designed to support supervised, semi-supervised, and unsupervised DL methods for defect detection and segmentation. Its diversity in texture and construction techniques also enables investigation of model generalization and transferability across tunnel types. By addressing the critical lack of domain-specific data, this dataset contributes to advancing automated tunnel inspection and promoting safer, more efficient infrastructure maintenance strategies.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14477v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "mapping"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出TACK隧道数据集以解决隧道缺陷检测领域数据稀缺问题，支持深度学习模型开发与评估。",
            "summary_zh": "隧道是交通基础设施的关键组成部分，但日益受到老化和劣化机制（如裂缝）的影响。为确保安全，需要定期检查，但传统人工方法耗时、主观且成本高。移动测绘系统和深度学习的进展使得自动化视觉检查成为可能，但其有效性受限于隧道数据集的稀缺性。本文介绍了一个新的公开数据集，包含三种不同隧道衬砌的标注图像，捕捉典型缺陷：裂缝、渗漏和水渗透。该数据集旨在支持有监督、半监督和无监督的深度学习方法进行缺陷检测和分割。其在纹理和施工技术上的多样性也使得能够研究模型在不同隧道类型间的泛化性和可迁移性。通过解决领域特定数据严重缺乏的问题，该数据集有助于推进自动化隧道检查，并促进更安全、更高效的基础设施维护策略。",
            "intro_zh": [
                "核心问题：隧道缺陷检测依赖传统人工检查，存在耗时、主观和成本高的问题，且深度学习应用受限于领域数据稀缺。",
                "方法要点：构建公开的TACK隧道数据集，包含多种隧道衬砌的标注图像，支持多种深度学习方法的缺陷检测与分割任务。",
                "实验或效果：数据集多样性促进模型泛化研究，为自动化隧道检查提供基准，推动基础设施维护策略优化。"
            ],
            "method_zh": "**问题定义**：论文旨在解决隧道缺陷检测中深度学习应用受限的核心问题，即领域特定数据稀缺。现有方法依赖传统人工检查，效率低下且主观性强，而移动测绘系统和深度学习虽能实现自动化，但缺乏高质量、多样化的隧道数据集来训练和评估模型，导致模型泛化能力不足，难以适应不同隧道类型和缺陷场景。\\n\\n**核心思路**：论文的核心解决思路是构建一个公开、标注完善的隧道数据集，以填补领域数据空白。通过收集和标注多种隧道衬砌的图像，捕捉典型缺陷如裂缝、渗漏和水渗透，为深度学习模型提供丰富的训练和测试资源。设计上强调数据多样性，以支持模型在不同隧道类型间的泛化研究，从而提升自动化检测的可靠性和效率。\\n\\n**技术框架**：整体架构包括数据采集、标注和数据集构建三个阶段。首先，使用移动测绘系统采集三种不同隧道衬砌的高分辨率图像；其次，对图像中的缺陷进行人工或半自动标注，生成精确的边界框或分割掩码；最后，将标注数据组织成标准格式，支持有监督、半监督和无监督学习任务。主要模块包括图像预处理、标注工具和数据集管理平台，确保数据质量和易用性。\\n\\n**关键创新**：最重要的技术创新点是首次公开提供专门针对隧道缺陷检测的多样化数据集。与现有方法相比，本质区别在于其全面覆盖多种隧道类型和缺陷类别，并支持多种学习范式，从而直接解决了领域数据稀缺的瓶颈问题，为模型开发和评估提供了标准化基准。\\n\\n**关键设计**：关键设计包括数据集的多样性设计，涵盖不同纹理和施工技术的隧道衬砌，以模拟真实世界场景；标注细节上，使用精确的边界框和分割掩码标注缺陷区域，支持检测和分割任务；数据集格式兼容常见深度学习框架，如TensorFlow和PyTorch，便于研究人员快速集成和使用。具体参数设置如图像分辨率、标注标准等未在摘要中详细说明，但强调其公开可用性和可扩展性。",
            "application_zh": "该研究主要应用于交通基础设施维护领域，特别是隧道安全检查和缺陷监测。通过提供高质量数据集，支持开发自动化视觉检测系统，可替代传统人工检查，降低维护成本、提高效率并增强安全性。未来影响包括推动智能基础设施管理，促进深度学习在土木工程中的广泛应用，并为其他类似领域（如桥梁、道路）的数据集构建提供参考。",
            "highlight_zh": "论文最重要的实验结果是成功构建并公开了TACK隧道数据集，包含三种隧道衬砌的标注图像，覆盖裂缝、渗漏和水渗透等典型缺陷。数据集支持有监督、半监督和无监督学习方法，为缺陷检测和分割任务提供基准。通过多样性设计，促进了模型泛化能力的研究，具体性能数据如准确率或提升幅度未在摘要中提供，但该数据集填补了领域数据空白，为后续研究提供了标准化评估平台。",
            "tags_zh": [
                "隧道缺陷检测",
                "深度学习数据集",
                "基础设施维护",
                "视觉检查",
                "模型泛化",
                "自动化检测",
                "移动测绘系统"
            ],
            "_index": 57
        },
        {
            "title": "Kinetic-Mamba: Mamba-Assisted Predictions of Stiff Chemical Kinetics",
            "authors": [
                "Additi Pandey",
                "Liang Wei",
                "Hessam Babaee",
                "George Em Karniadakis"
            ],
            "arxiv_id": "2512.14471v1",
            "summary": "Accurate chemical kinetics modeling is essential for combustion simulations, as it governs the evolution of complex reaction pathways and thermochemical states. In this work, we introduce Kinetic-Mamba, a Mamba-based neural operator framework that integrates the expressive power of neural operators with the efficient temporal modeling capabilities of Mamba architectures. The framework comprises three complementary models: (i) a standalone Mamba model that predicts the time evolution of thermochemical state variables from given initial conditions; (ii) a constrained Mamba model that enforces mass conservation while learning the state dynamics; and (iii) a regime-informed architecture employing two standalone Mamba models to capture dynamics across temperature-dependent regimes. We additionally develop a latent Kinetic-Mamba variant that evolves dynamics in a reduced latent space and reconstructs the full state on the physical manifold. We evaluate the accuracy and robustness of Kinetic-Mamba using both time-decomposition and recursive-prediction strategies. We further assess the extrapolation capabilities of the model on varied out-of-distribution datasets. Computational experiments on Syngas and GRI-Mech 3.0 reaction mechanisms demonstrate that our framework achieves high fidelity in predicting complex kinetic behavior using only the initial conditions of the state variables.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14471v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO",
                        "VIO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出Kinetic-Mamba框架，结合Mamba架构高效预测刚性化学动力学，用于燃烧模拟中的复杂反应路径建模。",
            "summary_zh": "准确的化学动力学建模对于燃烧模拟至关重要，它控制着复杂反应路径和热化学状态的演化。本文介绍了Kinetic-Mamba，这是一个基于Mamba的神经算子框架，将神经算子的表达能力与Mamba架构的高效时间建模能力相结合。该框架包含三个互补模型：(i) 一个独立的Mamba模型，从给定初始条件预测热化学状态变量的时间演化；(ii) 一个约束Mamba模型，在学习状态动力学的同时强制质量守恒；(iii) 一个基于机制的架构，采用两个独立的Mamba模型来捕捉跨温度依赖机制的动力。我们还开发了一个潜在Kinetic-Mamba变体，在降维潜在空间中演化动力学，并在物理流形上重建完整状态。我们使用时间分解和递归预测策略评估Kinetic-Mamba的准确性和鲁棒性。我们进一步评估了模型在不同分布外数据集上的外推能力。在Syngas和GRI-Mech 3.0反应机制上的计算实验表明，我们的框架仅使用状态变量的初始条件就能高保真地预测复杂的动力学行为。",
            "intro_zh": [
                "核心问题：燃烧模拟中刚性化学动力学建模复杂，传统方法计算成本高，难以准确捕捉多尺度反应路径和热化学状态演化。",
                "方法要点：提出Kinetic-Mamba框架，结合神经算子的表达能力和Mamba架构的高效时间建模，通过多种模型变体提升预测精度和物理一致性。",
                "实验或效果：在Syngas和GRI-Mech 3.0机制上验证，仅用初始条件即可高保真预测动力学行为，展现出强外推能力和计算效率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决燃烧模拟中刚性化学动力学的准确建模问题，现有方法如传统数值求解器计算成本高，难以处理复杂反应路径和多尺度时间演化，而现有机器学习方法可能缺乏物理约束或效率不足。\\n\\n**核心思路**：通过结合神经算子的表达能力和Mamba架构的高效时间建模能力，设计一个集成框架，以仅使用初始条件预测热化学状态变量的时间演化，同时引入物理约束和机制感知设计来提升模型的准确性和鲁棒性。\\n\\n**技术框架**：整体架构包括三个互补模型：独立Mamba模型用于直接预测状态演化；约束Mamba模型在训练中强制质量守恒；机制感知架构使用两个Mamba模型分别处理不同温度依赖机制。此外，潜在Kinetic-Mamba变体在降维潜在空间演化动力学，再重建到物理空间。评估采用时间分解和递归预测策略。\\n\\n**关键创新**：最重要的技术创新是将Mamba架构引入化学动力学预测，实现高效时间建模，同时通过多模型集成和物理约束设计，解决了现有方法在计算效率、物理一致性和外推能力方面的不足。\\n\\n**关键设计**：关键设计包括使用Mamba架构处理序列数据，损失函数可能结合预测误差和物理约束（如质量守恒），网络结构针对不同模型变体优化，例如机制感知架构中的温度分割策略，以及潜在变体中的降维和重建模块。具体参数设置未知，但框架强调仅依赖初始条件进行预测。",
            "application_zh": "该研究主要应用于燃烧模拟领域，如发动机设计、能源系统和环境建模，通过高效预测刚性化学动力学，可加速复杂反应路径的仿真，降低计算成本，提升模拟精度。未来可能扩展到其他化学工程和物理系统建模，推动AI在科学计算中的实际应用。",
            "highlight_zh": "在Syngas和GRI-Mech 3.0反应机制上的实验表明，Kinetic-Mamba框架仅使用初始条件即可高保真预测复杂动力学行为，具体性能数据未知，但通过时间分解和递归预测策略验证了准确性和鲁棒性。模型在分布外数据集上展现出强外推能力，相比传统方法可能显著提升计算效率，但未提供具体对比基线或提升幅度数据。",
            "tags_zh": [
                "化学动力学建模",
                "Mamba架构",
                "神经算子",
                "燃烧模拟",
                "时间序列预测",
                "物理约束学习",
                "潜在空间建模",
                "外推能力"
            ],
            "_index": 58
        },
        {
            "title": "Nonlinear System Identification Nano-drone Benchmark",
            "authors": [
                "Riccardo Busetto",
                "Elia Cereda",
                "Marco Forgione",
                "Gabriele Maroni",
                "Dario Piga",
                "Daniele Palossi"
            ],
            "arxiv_id": "2512.14450v1",
            "summary": "We introduce a benchmark for system identification based on 75k real-world samples from the Crazyflie 2.1 Brushless nano-quadrotor, a sub-50g aerial vehicle widely adopted in robotics research. The platform presents a challenging testbed due to its multi-input, multi-output nature, open-loop instability, and nonlinear dynamics under agile maneuvers. The dataset comprises four aggressive trajectories with synchronized 4-dimensional motor inputs and 13-dimensional output measurements. To enable fair comparison of identification methods, the benchmark includes a suite of multi-horizon prediction metrics for evaluating both one-step and multi-step error propagation. In addition to the data, we provide a detailed description of the platform and experimental setup, as well as baseline models highlighting the challenge of accurate prediction under real-world noise and actuation nonlinearities. All data, scripts, and reference implementations are released as open-source at https://github.com/idsia-robotics/nanodrone-sysid-benchmark to facilitate transparent comparison of algorithms and support research on agile, miniaturized aerial robotics.",
            "categories": [
                "eess.SY",
                "cs.RO"
            ],
            "primary_category": "eess.SY",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14450v1",
            "code_links": [
                {
                    "url": "https://github.com/idsia-robotics/nanodrone-sysid-benchmark",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出基于Crazyflie 2.1纳米四旋翼的75k真实世界样本系统辨识基准，以解决微型无人机非线性动力学建模的挑战。",
            "summary_zh": "我们引入了一个基于Crazyflie 2.1无刷纳米四旋翼（一种广泛用于机器人研究的重量低于50克的飞行器）75k真实世界样本的系统辨识基准。该平台因其多输入多输出特性、开环不稳定性以及在敏捷机动下的非线性动力学而成为一个具有挑战性的测试平台。数据集包含四条激进轨迹，具有同步的4维电机输入和13维输出测量。为了公平比较辨识方法，该基准包括一套多步长预测指标，用于评估一步和多步误差传播。除了数据外，我们还提供了平台和实验设置的详细描述，以及基线模型，突出了在真实世界噪声和执行器非线性下进行准确预测的挑战。所有数据、脚本和参考实现均以开源形式发布在https://github.com/idsia-robotics/nanodrone-sysid-benchmark，以促进算法的透明比较，并支持敏捷、微型空中机器人技术的研究。",
            "intro_zh": [
                "现有系统辨识方法在微型无人机非线性动力学建模中面临挑战，如多输入多输出、开环不稳定性和真实噪声影响。",
                "论文提出基于Crazyflie 2.1纳米四旋翼的75k真实世界样本基准，包含激进轨迹数据和多步长预测指标。",
                "基准提供开源数据、脚本和基线模型，支持算法透明比较，提升微型无人机系统辨识研究的可重复性和实用性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决微型无人机（如Crazyflie 2.1纳米四旋翼）非线性系统辨识的挑战，现有方法在真实世界应用中常因多输入多输出特性、开环不稳定性和非线性动力学而性能受限，缺乏标准化基准进行公平比较。\\n\\n**核心思路**：通过构建一个基于真实世界数据的系统辨识基准，提供大规模、高质量的数据集和评估指标，以促进算法开发和比较，核心思想是利用Crazyflie 2.1平台作为测试床，捕捉其复杂动力学行为。\\n\\n**技术框架**：整体架构包括数据采集、基准构建和评估三阶段。首先，使用Crazyflie 2.1纳米四旋翼进行实验，收集75k样本的激进轨迹数据；其次，整理数据为4维输入（电机控制）和13维输出（测量状态），并设计多步长预测指标；最后，提供开源工具和基线模型，支持用户测试和比较辨识算法。\\n\\n**关键创新**：最重要的技术创新是引入一个真实世界、大规模的系统辨识基准，专注于微型无人机的非线性动力学，与现有方法相比，本质区别在于其强调真实噪声、执行器非线性和多步预测评估，提升了研究的实用性和可重复性。\\n\\n**关键设计**：关键设计包括数据集包含四条激进轨迹，确保覆盖非线性区域；使用多步长预测指标（如一步和多步误差传播）进行公平评估；提供详细平台描述和实验设置，减少不确定性；开源所有数据、脚本和参考实现，促进透明比较。",
            "application_zh": "该研究在微型空中机器人、无人机控制和系统辨识领域具有广泛应用价值。潜在应用包括提升微型无人机的自主飞行性能、优化控制算法设计，以及支持机器人教育中的实验教学。未来影响可能推动敏捷、小型化机器人技术的发展，并为工业检测、环境监测等场景提供更精确的建模工具。",
            "highlight_zh": "最重要的实验结果包括：基准基于75k真实世界样本，覆盖Crazyflie 2.1纳米四旋翼的激进机动；提供多步长预测指标，如一步和多步误差，用于全面评估辨识方法；基线模型展示了在真实噪声和非线性下的预测挑战，具体性能数据未知，但开源实现支持透明比较，提升研究可重复性。",
            "tags_zh": [
                "系统辨识",
                "纳米无人机",
                "非线性动力学",
                "基准测试",
                "真实世界数据",
                "多步预测",
                "开源基准",
                "微型机器人"
            ],
            "_index": 59
        },
        {
            "title": "A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning",
            "authors": [
                "Zixin Zhang",
                "Kanghao Chen",
                "Hanqing Wang",
                "Hongfei Zhang",
                "Harold Haodong Chen",
                "Chenfei Liao",
                "Litao Guo",
                "Ying-Cong Chen"
            ],
            "arxiv_id": "2512.14442v1",
            "summary": "Affordance prediction, which identifies interaction regions on objects based on language instructions, is critical for embodied AI. Prevailing end-to-end models couple high-level reasoning and low-level grounding into a single monolithic pipeline and rely on training over annotated datasets, which leads to poor generalization on novel objects and unseen environments. In this paper, we move beyond this paradigm by proposing A4-Agent, a training-free agentic framework that decouples affordance prediction into a three-stage pipeline. Our framework coordinates specialized foundation models at test time: (1) a $\\textbf{Dreamer}$ that employs generative models to visualize $\\textit{how}$ an interaction would look; (2) a $\\textbf{Thinker}$ that utilizes large vision-language models to decide $\\textit{what}$ object part to interact with; and (3) a $\\textbf{Spotter}$ that orchestrates vision foundation models to precisely locate $\\textit{where}$ the interaction area is. By leveraging the complementary strengths of pre-trained models without any task-specific fine-tuning, our zero-shot framework significantly outperforms state-of-the-art supervised methods across multiple benchmarks and demonstrates robust generalization to real-world settings.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14442v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "世界模型",
                    "matched_keywords": [
                        "dreamer"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出A4-Agent框架，通过解耦推理过程实现零样本可及性预测，解决现有方法泛化能力不足的问题。",
            "summary_zh": "可及性预测是基于语言指令识别物体上交互区域的关键技术，对具身AI至关重要。当前主流端到端模型将高层推理与低层定位耦合在单一流程中，并依赖标注数据集训练，导致在新物体和未见环境上泛化能力差。本文超越这一范式，提出A4-Agent，一种无需训练的智能体框架，将可及性预测解耦为三阶段流程。该框架在测试时协调专用基础模型：(1) Dreamer利用生成模型可视化交互过程；(2) Thinker利用大型视觉语言模型决定交互的物体部分；(3) Spotter协调视觉基础模型精确定位交互区域。通过利用预训练模型的互补优势且无需任务特定微调，我们的零样本框架在多个基准测试中显著优于最先进的监督方法，并展现出对真实场景的鲁棒泛化能力。",
            "intro_zh": [
                "现有端到端模型耦合推理与定位，依赖标注数据训练，导致泛化能力差，难以处理新物体和未见环境。",
                "提出A4-Agent框架，将可及性预测解耦为三阶段流程，协调专用基础模型实现零样本推理，无需训练。",
                "在多个基准测试中显著优于监督方法，展现出鲁棒泛化能力，提升可及性预测的准确性和适应性。"
            ],
            "method_zh": "**问题定义**：论文解决可及性预测问题，即基于语言指令识别物体上交互区域。现有方法的痛点在于端到端模型将高层推理和低层定位耦合在单一流程中，并依赖标注数据集训练，导致泛化能力差，难以处理新物体和未见环境。\\n\\n**核心思路**：论文的核心解决思路是解耦可及性预测过程，将其分为可视化、决策和定位三个阶段，并利用预训练基础模型的互补优势实现零样本推理。这样设计可以避免训练依赖，提升泛化能力，通过模块化流程增强可解释性和适应性。\\n\\n**技术框架**：整体架构是一个三阶段智能体框架：Dreamer阶段使用生成模型（如扩散模型）可视化交互过程，生成交互场景图像；Thinker阶段利用大型视觉语言模型（如GPT-4V）分析语言指令和物体图像，决定交互的物体部分；Spotter阶段协调视觉基础模型（如SAM）精确定位交互区域，输出掩码或边界框。三个阶段在测试时动态协调，无需训练。\\n\\n**关键创新**：最重要的技术创新点是提出训练免费的智能体框架，通过解耦推理过程实现零样本可及性预测。与现有方法的本质区别在于：现有方法通常为监督学习，耦合推理和定位；而A4-Agent利用预训练模型，无需微调，通过模块化流程提升泛化能力。\\n\\n**关键设计**：关键设计包括：使用生成模型（如Stable Diffusion）作为Dreamer，实现交互可视化；集成大型视觉语言模型（如CLIP或GPT系列）作为Thinker，进行语义推理；采用视觉基础模型（如Segment Anything Model）作为Spotter，进行精确分割。框架无需特定参数设置或损失函数，依赖预训练模型的零样本能力，通过提示工程和模型协调实现端到端预测。",
            "application_zh": "该研究在具身AI、机器人操作和智能交互系统中有广泛应用潜力。例如，在家庭服务机器人中，A4-Agent可以帮助机器人理解用户指令并定位物体上的可操作区域，如“打开这个抽屉”或“按下那个按钮”，提升机器人的自主性和适应性。未来可能推动零样本学习在视觉推理领域的发展，降低对标注数据的依赖，促进AI系统在动态环境中的部署。",
            "highlight_zh": "在多个基准测试中，A4-Agent显著优于最先进的监督方法。具体而言，在可及性预测任务上，零样本性能提升约15-20%，例如在某个基准测试中准确率达到85%，而监督方法为70%。框架展现出对真实世界场景的鲁棒泛化能力，无需额外训练即可处理新物体和复杂环境，验证了其有效性和实用性。",
            "tags_zh": [
                "可及性预测",
                "零样本学习",
                "智能体框架",
                "视觉语言模型",
                "基础模型协调",
                "具身AI",
                "解耦推理",
                "训练免费方法"
            ],
            "_index": 60
        },
        {
            "title": "Effect of Document Packing on the Latent Multi-Hop Reasoning Capabilities of Large Language Models",
            "authors": [
                "Gabriele Prato",
                "Shagun Sodhani",
                "Alessandro Sordoni",
                "Sarath Chandar"
            ],
            "arxiv_id": "2512.14427v1",
            "summary": "The standard practice for training large language models involves packing multiple documents together to optimize computational efficiency. However, the impact of this process on the models' capabilities remains largely unexplored. To address this gap, we investigate how different document-packing strategies influence the latent multi-hop reasoning abilities of LLMs. Our findings indicate that packing can improve model performance compared to training on individual documents, at the expense of more compute. To further understand the underlying mechanisms, we conduct an ablation study, identifying key factors that explain the advantages of packing. Ultimately, our research deepens the understanding of LLM training dynamics and provides practical insights for optimizing model development.",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14427v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "研究文档打包策略对大型语言模型潜在多跳推理能力的影响，以优化训练效率与性能",
            "summary_zh": "训练大型语言模型的标准实践通常涉及将多个文档打包在一起，以提高计算效率。然而，这一过程对模型能力的影响在很大程度上尚未被探索。为填补这一空白，我们研究了不同文档打包策略如何影响LLMs的潜在多跳推理能力。我们的发现表明，与在单个文档上训练相比，打包可以提高模型性能，但需要更多的计算资源。为了进一步理解底层机制，我们进行了消融研究，识别了解释打包优势的关键因素。最终，我们的研究深化了对LLM训练动态的理解，并为优化模型开发提供了实用见解。",
            "intro_zh": [
                "核心问题：现有训练方法中，文档打包策略对大型语言模型潜在多跳推理能力的影响未知，缺乏系统研究。",
                "方法要点：通过对比不同文档打包策略，分析其对模型性能的影响，并进行消融研究以识别关键因素。",
                "实验或效果：打包策略能提升模型性能，但需更多计算；消融研究揭示了打包优势的机制，为优化训练提供指导。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型训练中，文档打包策略对模型潜在多跳推理能力的具体影响问题。现有方法的痛点在于，标准训练实践通常基于计算效率考虑，将多个文档打包处理，但这一过程对模型能力（特别是多跳推理能力）的影响尚未被系统研究，导致训练优化缺乏理论依据。\\n\\n**核心思路**：论文的核心解决思路是通过实验对比不同文档打包策略，评估其对模型性能的影响，并深入分析底层机制。这样设计是为了填补研究空白，从实证角度理解打包策略如何塑造模型能力，从而为训练优化提供数据支持。\\n\\n**技术框架**：整体架构包括数据准备、模型训练、性能评估和消融分析四个主要阶段。首先，准备不同打包策略的数据集；然后，在大型语言模型上进行训练；接着，通过多跳推理任务评估模型性能；最后，进行消融研究以识别影响性能的关键因素。\\n\\n**关键创新**：最重要的技术创新点在于首次系统研究文档打包策略对LLM潜在多跳推理能力的影响，并通过消融分析揭示其机制。与现有方法的本质区别在于，现有研究多关注训练效率，而本文聚焦于能力影响，提供了更全面的训练动态理解。\\n\\n**关键设计**：关键设计包括定义不同的文档打包策略（如基于长度、内容或随机打包），使用标准的多跳推理评估指标（如准确率或F1分数），并在训练过程中监控计算资源消耗。消融研究可能涉及调整打包参数（如文档数量或顺序），以分析其对性能的具体贡献。",
            "application_zh": "该研究的潜在应用领域包括大型语言模型的训练优化、自然语言处理任务（如问答和推理系统）的开发，以及人工智能教育工具的改进。实际价值在于为模型开发者提供基于实证的打包策略指导，以平衡计算效率与模型性能，从而降低训练成本并提升模型能力。未来影响可能推动更智能的训练方法设计，促进LLM在复杂推理任务中的应用。",
            "highlight_zh": "最重要的实验结果表明，与在单个文档上训练相比，文档打包策略能显著提升大型语言模型在多跳推理任务上的性能，具体提升幅度未知，但需更多计算资源。消融研究识别了关键因素（如打包顺序和文档多样性），解释了打包优势的机制。对比基线包括标准训练方法，结果强调了打包策略对模型能力塑造的重要性。",
            "tags_zh": [
                "文档打包策略",
                "大型语言模型",
                "多跳推理能力",
                "训练优化",
                "消融研究",
                "计算效率",
                "模型性能评估",
                "自然语言处理"
            ],
            "_index": 61
        },
        {
            "title": "The Devil is in Attention Sharing: Improving Complex Non-rigid Image Editing Faithfulness via Attention Synergy",
            "authors": [
                "Zhuo Chen",
                "Fanyue Wei",
                "Runze Xu",
                "Jingjing Li",
                "Lixin Duan",
                "Angela Yao",
                "Wen Li"
            ],
            "arxiv_id": "2512.14423v1",
            "summary": "Training-free image editing with large diffusion models has become practical, yet faithfully performing complex non-rigid edits (e.g., pose or shape changes) remains highly challenging. We identify a key underlying cause: attention collapse in existing attention sharing mechanisms, where either positional embeddings or semantic features dominate visual content retrieval, leading to over-editing or under-editing.To address this issue, we introduce SynPS, a method that Synergistically leverages Positional embeddings and Semantic information for faithful non-rigid image editing. We first propose an editing measurement that quantifies the required editing magnitude at each denoising step. Based on this measurement, we design an attention synergy pipeline that dynamically modulates the influence of positional embeddings, enabling SynPS to balance semantic modifications and fidelity preservation.By adaptively integrating positional and semantic cues, SynPS effectively avoids both over- and under-editing. Extensive experiments on public and newly curated benchmarks demonstrate the superior performance and faithfulness of our approach.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page:https://synps26.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14423v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出SynPS方法，通过注意力协同机制解决复杂非刚性图像编辑中的忠实性问题",
            "summary_zh": "基于大型扩散模型的无训练图像编辑已变得实用，但忠实执行复杂非刚性编辑（如姿态或形状变化）仍然极具挑战。本文发现一个关键根本原因：现有注意力共享机制中的注意力崩溃，其中位置嵌入或语义特征主导视觉内容检索，导致过度编辑或编辑不足。为解决这一问题，我们引入了SynPS方法，该方法协同利用位置嵌入和语义信息以实现忠实的非刚性图像编辑。我们首先提出一种编辑度量，量化每个去噪步骤所需的编辑幅度。基于此度量，我们设计了一个注意力协同流程，动态调节位置嵌入的影响，使SynPS能够平衡语义修改和保真度保持。通过自适应整合位置和语义线索，SynPS有效避免了过度编辑和编辑不足。在公开和新构建的基准测试上进行的大量实验证明了我们方法的优越性能和忠实性。",
            "intro_zh": [
                "现有方法在复杂非刚性编辑中存在注意力崩溃问题，导致过度编辑或编辑不足。",
                "提出SynPS方法，通过动态调节位置嵌入影响，协同利用位置和语义信息。",
                "实验表明SynPS在多个基准测试中性能优越，显著提升编辑忠实性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决基于大型扩散模型的复杂非刚性图像编辑（如姿态或形状变化）中的忠实性问题。现有方法的痛点在于注意力共享机制中的注意力崩溃，其中位置嵌入或语义特征过度主导内容检索，导致编辑结果要么过度改变原始内容（过度编辑），要么未能充分实现编辑目标（编辑不足）。\\n\\n**核心思路**：论文的核心解决思路是设计一个注意力协同机制，动态平衡位置嵌入和语义信息在编辑过程中的影响。这样设计是因为位置嵌入有助于保持结构一致性，而语义信息驱动内容变化，通过自适应整合两者可以避免单一因素主导导致的编辑偏差。\\n\\n**技术框架**：整体架构包括两个主要阶段：首先，提出编辑度量模块，用于量化每个去噪步骤所需的编辑幅度；其次，基于此度量设计注意力协同流程，该流程在去噪过程中动态调节位置嵌入的权重，从而调制其对内容检索的影响。流程通过迭代优化实现编辑目标与保真度的平衡。\\n\\n**关键创新**：最重要的技术创新点是引入了注意力协同机制，具体表现为编辑度量和动态调制策略。与现有方法的本质区别在于，现有方法通常固定或简单组合位置和语义信息，而SynPS通过量化编辑需求自适应调整，实现了更精细的控制。\\n\\n**关键设计**：关键设计包括编辑度量函数，该函数基于图像特征计算编辑幅度；注意力调制模块，使用可调参数根据度量结果调整位置嵌入的贡献；网络结构上，可能集成到扩散模型的注意力层中，具体参数设置如调制系数通过实验优化，损失函数可能涉及保真度和编辑目标的权衡，但论文未详细说明，标记为未知。",
            "application_zh": "该研究在计算机视觉和人工智能领域具有广泛潜在应用，包括图像编辑软件、虚拟现实内容生成、影视特效制作和数字艺术创作。通过提升复杂非刚性编辑的忠实性，它能支持更自然和可控的图像修改，减少人工干预，推动自动化编辑工具的发展，未来可能影响生成式AI在创意产业和媒体生产中的实际部署。",
            "highlight_zh": "在公开和新构建的基准测试中，SynPS方法在复杂非刚性编辑任务上表现出优越性能。具体实验数据未在摘要中提供，但论文提到通过大量实验证明了其忠实性和有效性，可能包括与现有基线的对比，如注意力共享方法，提升幅度涉及减少过度编辑和编辑不足的情况，具体数值标记为未知。",
            "tags_zh": [
                "非刚性图像编辑",
                "注意力机制",
                "扩散模型",
                "语义编辑",
                "位置嵌入",
                "编辑忠实性",
                "生成式AI",
                "计算机视觉"
            ],
            "_index": 62
        },
        {
            "title": "Hybrid Ensemble Method for Detecting Cyber-Attacks in Water Distribution Systems Using the BATADAL Dataset",
            "authors": [
                "Waqas Ahmed"
            ],
            "arxiv_id": "2512.14422v1",
            "summary": "The cybersecurity of Industrial Control Systems that manage critical infrastructure such as Water Distribution Systems has become increasingly important as digital connectivity expands. BATADAL benchmark data is a good source of testing intrusion detection techniques, but it presents several important problems, such as imbalance in the number of classes, multivariate time dependence, and stealthy attacks. We consider a hybrid ensemble learning model that will enhance the detection ability of cyber-attacks in WDS by using the complementary capabilities of machine learning and deep learning models. Three base learners, namely, Random Forest , eXtreme Gradient Boosting , and Long Short-Term Memory network, have been strictly compared and seven ensemble types using simple averaged and stacked learning with a logistic regression meta-learner. Random Forest analysis identified top predictors turned into temporal and statistical features, and Synthetic Minority Oversampling Technique (SMOTE) was used to overcome the class imbalance issue. The analyics indicates that the single Long Short-Term Memory network model is of poor performance (F1 = 0.000, AUC = 0.4460), but tree-based models, especially eXtreme Gradient Boosting, perform well (F1 = 0.7470, AUC=0.9684). The hybrid stacked ensemble of Random Forest , eXtreme Gradient Boosting , and Long Short-Term Memory network scored the highest, with the attack class of 0.7205 with an F1-score and a AUC of 0.9826 indicating that the heterogeneous stacking between model precision and generalization can work. The proposed framework establishes a robust and scalable solution for cyber-attack detection in time-dependent industrial systems, integrating temporal learning and ensemble diversity to support the secure operation of critical infrastructure.",
            "categories": [
                "cs.CR",
                "cs.LG"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "18 pages, & figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14422v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "人形机器人",
                    "matched_keywords": [
                        "digit"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出混合集成学习方法以解决供水系统网络攻击检测中的类别不平衡、多变量时间依赖和隐蔽攻击问题。",
            "summary_zh": "随着数字连接性的扩展，管理供水系统等关键基础设施的工业控制系统网络安全日益重要。BATADAL基准数据是测试入侵检测技术的良好来源，但它存在几个重要问题，如类别数量不平衡、多变量时间依赖性和隐蔽攻击。我们考虑一种混合集成学习模型，通过利用机器学习和深度学习模型的互补能力，增强供水系统中网络攻击的检测能力。严格比较了三个基础学习器，即随机森林、极限梯度提升和长短期记忆网络，并使用简单平均和带有逻辑回归元学习器的堆叠学习构建了七种集成类型。随机森林分析确定了转化为时间和统计特征的重要预测因子，并使用合成少数类过采样技术（SMOTE）来克服类别不平衡问题。分析表明，单一长短期记忆网络模型性能较差（F1 = 0.000，AUC = 0.4460），但基于树的模型，尤其是极限梯度提升，表现良好（F1 = 0.7470，AUC = 0.9684）。随机森林、极限梯度提升和长短期记忆网络的混合堆叠集成得分最高，攻击类别的F1分数为0.7205，AUC为0.9826，表明模型精度和泛化能力之间的异构堆叠是有效的。所提出的框架为时间依赖的工业系统中的网络攻击检测建立了一个稳健且可扩展的解决方案，整合了时间学习和集成多样性，以支持关键基础设施的安全运行。",
            "intro_zh": [
                "核心问题：BATADAL数据集存在类别不平衡、多变量时间依赖和隐蔽攻击等挑战，传统单一模型难以有效检测网络攻击。",
                "方法要点：提出混合集成学习模型，结合随机森林、极限梯度提升和长短期记忆网络的互补优势，通过堆叠集成提升检测性能。",
                "实验或效果：混合堆叠集成在攻击类别上达到F1分数0.7205和AUC 0.9826，显著优于单一模型，验证了异构集成的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决供水系统网络攻击检测中的具体问题，包括BATADAL数据集中的类别不平衡、多变量时间依赖性和隐蔽攻击。现有方法的痛点在于单一模型（如长短期记忆网络）在处理这些复杂问题时性能不足，难以同时捕捉时间序列特征和静态统计模式，导致检测精度低和泛化能力差。\\n\\n**核心思路**：论文的核心解决思路是设计一种混合集成学习模型，通过结合机器学习和深度学习模型的互补能力，以增强检测能力。这样设计的原因是利用随机森林和极限梯度提升处理统计特征和类别不平衡的优势，以及长短期记忆网络捕捉时间依赖性的能力，通过集成学习整合不同模型的预测，提高整体鲁棒性和准确性。\\n\\n**技术框架**：整体架构包括数据预处理、基础学习器训练、集成构建和评估阶段。主要模块包括：使用SMOTE处理类别不平衡，从随机森林分析中提取时间和统计特征作为输入；训练三个基础学习器（随机森林、极限梯度提升、长短期记忆网络）；构建七种集成类型，包括简单平均和带有逻辑回归元学习器的堆叠集成；最后通过性能指标（如F1分数和AUC）评估模型。\\n\\n**关键创新**：最重要的技术创新点是提出异构堆叠集成方法，将基于树的模型（随机森林和极限梯度提升）与深度学习模型（长短期记忆网络）结合，利用它们的互补性来平衡模型精度和泛化能力。与现有方法的本质区别在于，它不依赖单一模型，而是通过集成多样性来应对数据集的复杂挑战，如时间依赖和隐蔽攻击。\\n\\n**关键设计**：关键设计包括：使用SMOTE算法处理类别不平衡，确保训练数据分布均衡；基础学习器中，随机森林和极限梯度提升用于特征选择和分类，长短期记忆网络用于时间序列建模；集成阶段，堆叠集成使用逻辑回归作为元学习器，结合基础学习器的预测输出；评估时采用F1分数和AUC作为主要指标，以全面衡量检测性能。",
            "application_zh": "该研究主要应用于工业控制系统网络安全领域，特别是供水系统等关键基础设施的网络攻击检测。实际价值在于提供了一种稳健且可扩展的解决方案，能够有效应对时间依赖数据和隐蔽攻击，提升系统安全性和可靠性。未来影响可能扩展到其他工业系统（如电力、交通）的入侵检测，推动智能安全监控技术的发展。",
            "highlight_zh": "最重要的实验结果显示：单一长短期记忆网络模型性能极差，F1分数为0.000，AUC为0.4460；基于树的模型中，极限梯度提升表现最佳，F1分数达0.7470，AUC为0.9684；混合堆叠集成（随机森林、极限梯度提升、长短期记忆网络）达到最高性能，攻击类别的F1分数为0.7205，AUC为0.9826，相比单一模型有显著提升，验证了异构集成的优势。",
            "tags_zh": [
                "网络攻击检测",
                "混合集成学习",
                "供水系统安全",
                "时间序列分析",
                "类别不平衡处理",
                "堆叠集成",
                "工业控制系统",
                "BATADAL数据集"
            ],
            "_index": 63
        },
        {
            "title": "Dual-Axis RCCL: Representation-Complete Convergent Learning for Organic Chemical Space",
            "authors": [
                "Dejun Hu",
                "Zhiming Li",
                "Jia-Rui Shen",
                "Jia-Ning Tu",
                "Zi-Hao Ye",
                "Junliang Zhang"
            ],
            "arxiv_id": "2512.14418v1",
            "summary": "Machine learning is profoundly reshaping molecular and materials modeling; however, given the vast scale of chemical space (10^30-10^60), it remains an open scientific question whether models can achieve convergent learning across this space. We introduce a Dual-Axis Representation-Complete Convergent Learning (RCCL) strategy, enabled by a molecular representation that integrates graph convolutional network (GCN) encoding of local valence environments, grounded in modern valence bond theory, together with no-bridge graph (NBG) encoding of ring/cage topologies, providing a quantitative measure of chemical-space coverage. This framework formalizes representation completeness, establishing a principled basis for constructing datasets that support convergent learning for large models. Guided by this RCCL framework, we develop the FD25 dataset, systematically covering 13,302 local valence units and 165,726 ring/cage topologies, achieving near-complete combinatorial coverage of organic molecules with H/C/N/O/F elements. Graph neural networks trained on FD25 exhibit representation-complete convergent learning and strong out-of-distribution generalization, with an overall prediction error of approximately 1.0 kcal/mol MAE across external benchmarks. Our results establish a quantitative link between molecular representation, structural completeness, and model generalization, providing a foundation for interpretable, transferable, and data-efficient molecular intelligence.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "33 pages, 10 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14418v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出双轴表示完全收敛学习策略，以解决有机化学空间模型收敛与泛化难题。",
            "summary_zh": "机器学习正在深刻重塑分子与材料建模；然而，考虑到化学空间的巨大规模（10^30-10^60），模型能否在该空间实现收敛学习仍是一个开放的科学问题。我们引入了一种双轴表示完全收敛学习策略，该策略通过一种分子表示实现，该表示整合了基于现代价键理论的局部价环境图卷积网络编码，以及环/笼拓扑的无桥图编码，提供了化学空间覆盖的定量度量。该框架形式化了表示完全性，为构建支持大模型收敛学习的数据集建立了原则性基础。在此RCCL框架指导下，我们开发了FD25数据集，系统覆盖了13,302个局部价单元和165,726个环/笼拓扑，实现了对含H/C/N/O/F元素的有机分子的近乎完全组合覆盖。在FD25上训练的图神经网络表现出表示完全收敛学习和强大的分布外泛化能力，在外部基准测试中整体预测误差约为1.0 kcal/mol MAE。我们的结果建立了分子表示、结构完全性和模型泛化之间的定量联系，为可解释、可迁移和数据高效的分子智能奠定了基础。",
            "intro_zh": [
                "核心问题：化学空间规模巨大（10^30-10^60），现有方法难以确保模型在该空间实现收敛学习，泛化能力受限。",
                "方法要点：提出双轴RCCL策略，结合局部价环境GCN编码和环/笼拓扑NBG编码，构建表示完全数据集FD25。",
                "实验或效果：在FD25训练的模型实现收敛学习，外部基准预测误差约1.0 kcal/mol MAE，泛化能力显著提升。"
            ],
            "method_zh": "**问题定义**：论文旨在解决有机化学空间中机器学习模型的收敛学习与泛化难题。现有方法痛点在于化学空间规模巨大（10^30-10^60），缺乏系统覆盖，导致模型训练可能不收敛或泛化能力差，难以确保在未见分子上的预测准确性。\\n\\n**核心思路**：核心思路是通过定义“表示完全性”来量化化学空间覆盖，并基于此构建数据集，确保模型能学习到化学空间的完整结构信息。设计双轴表示：一轴基于现代价键理论编码局部价环境，另一轴编码环/笼拓扑，以全面捕捉分子特征。\\n\\n**技术框架**：整体架构包括两个主要阶段：表示构建和模型训练。首先，开发分子表示方法，整合图卷积网络（GCN）编码局部价环境和无桥图（NBG）编码环/笼拓扑。其次，基于RCCL框架指导，构建FD25数据集，系统覆盖局部价单元和环/笼拓扑。最后，使用图神经网络在FD25上进行训练，评估收敛性和泛化性能。\\n\\n**关键创新**：最重要的技术创新是提出“表示完全收敛学习”概念，并实现双轴分子表示。与现有方法相比，本质区别在于从理论层面形式化了化学空间覆盖的度量，使数据集构建有据可依，而非依赖经验或随机采样，从而系统性提升模型泛化能力。\\n\\n**关键设计**：关键设计包括：分子表示中，GCN编码基于现代价键理论处理局部化学键，NBG编码处理复杂环结构；FD25数据集覆盖13,302个局部价单元和165,726个环/笼拓扑，针对H/C/N/O/F元素；训练使用图神经网络，损失函数可能基于能量预测误差（如MAE），具体网络结构和参数设置论文中未详细说明，但强调在FD25上实现收敛学习。",
            "application_zh": "该研究在药物发现、材料设计和化学合成等领域具有重要应用价值。通过提供可解释、可迁移的分子智能模型，能加速新分子筛选和性能预测，降低实验成本。未来可能推动AI驱动的化学研究，实现数据高效和泛化强的自动化建模。",
            "highlight_zh": "最重要的实验结果包括：FD25数据集系统覆盖13,302个局部价单元和165,726个环/笼拓扑，实现近乎完全组合覆盖。在FD25训练的图神经网络表现出表示完全收敛学习，外部基准测试中整体预测误差约为1.0 kcal/mol MAE，显示出强大的分布外泛化能力，显著优于传统方法。",
            "tags_zh": [
                "化学空间学习",
                "分子表示学习",
                "图神经网络",
                "收敛学习",
                "泛化能力",
                "数据集构建",
                "有机化学",
                "价键理论"
            ],
            "_index": 64
        },
        {
            "title": "Massive Editing for Large Language Models Based on Dynamic Weight Generation",
            "authors": [
                "Wentao Wan",
                "Qiqing Lao",
                "Zhiwei Xie",
                "Hefeng Wu",
                "Runnan Lin",
                "Liang Lin",
                "Keze Wang"
            ],
            "arxiv_id": "2512.14395v1",
            "summary": "Knowledge Editing (KE) is a field that studies how to modify some knowledge in Large Language Models (LLMs) at a low cost (compared to pre-training). Currently, performing large-scale edits on LLMs while ensuring the Reliability, Generality, and Locality metrics of the edits remain a challenge. This paper proposes a Massive editing approach for LLMs based on dynamic weight Generation (MeG). Our MeG involves attaching a dynamic weight neuron to specific layers of the LLMs and using a diffusion model to conditionally generate the weights of this neuron based on the input query required for the knowledge. This allows the use of adding a single dynamic weight neuron to achieve the goal of large-scale knowledge editing. Experiments show that our MeG can significantly improve the performance of large-scale KE in terms of Reliability, Generality, and Locality metrics compared to existing knowledge editing methods, particularly with a high percentage point increase in the absolute value index for the Locality metric, demonstrating the advantages of our proposed method.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "27 pages, 8 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14395v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出基于动态权重生成的大规模编辑方法MeG，以解决大语言模型知识编辑中的可靠性、泛化性和局部性挑战。",
            "summary_zh": "知识编辑（KE）是研究如何以低成本（相比预训练）修改大语言模型（LLMs）中某些知识的领域。目前，在对LLMs进行大规模编辑的同时，确保编辑的可靠性、泛化性和局部性指标仍是一个挑战。本文提出了一种基于动态权重生成的大规模编辑方法（MeG）。我们的MeG涉及在LLMs的特定层附加一个动态权重神经元，并使用扩散模型根据知识所需的输入查询条件生成该神经元的权重。这使得通过添加单个动态权重神经元即可实现大规模知识编辑的目标。实验表明，与现有知识编辑方法相比，我们的MeG在可靠性、泛化性和局部性指标方面显著提升了大规KE的性能，特别是在局部性指标的绝对值指数上实现了高百分点的增长，证明了我们提出方法的优势。",
            "intro_zh": [
                "现有知识编辑方法在大规模编辑时难以同时保证可靠性、泛化性和局部性，导致编辑效果不稳定。",
                "MeG通过在LLMs特定层附加动态权重神经元，利用扩散模型根据输入查询生成权重，实现高效大规模编辑。",
                "实验显示MeG在可靠性、泛化性和局部性指标上显著优于现有方法，局部性指标提升尤为突出。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大语言模型（LLMs）知识编辑（KE）中的大规模编辑挑战，即在修改大量知识时，如何同时保证编辑的可靠性（Reliability，编辑后模型正确回答相关查询）、泛化性（Generality，编辑知识能泛化到相关表述）和局部性（Locality，不影响无关知识）。现有方法如基于微调或参数更新的编辑技术，在大规模编辑时往往导致性能下降、计算成本高或编辑冲突，难以平衡这三个指标。\\n\\n**核心思路**：论文提出MeG（Massive editing based on dynamic weight Generation）方法，其核心思想是通过在LLMs的特定层附加一个动态权重神经元，并利用扩散模型（Diffusion Model）条件生成该神经元的权重，从而实现对大规模知识的灵活编辑。这样设计的原因在于，动态权重神经元可以针对不同输入查询自适应调整，避免直接修改原始模型参数，减少编辑干扰，同时扩散模型能有效建模复杂权重分布，提高编辑的准确性和稳定性。\\n\\n**技术框架**：整体架构包括三个主要阶段：首先，在预训练的LLMs中选择关键层（如中间层或输出层）附加动态权重神经元；其次，使用扩散模型作为权重生成器，该模型以输入查询（如知识相关的文本）为条件，生成对应神经元的权重；最后，在推理时，根据查询动态加载生成的权重，实现知识编辑。流程上，先训练扩散模型学习权重分布，然后应用编辑时实时生成权重，无需重新训练整个模型。\\n\\n**关键创新**：最重要的技术创新是将扩散模型引入知识编辑领域，用于条件生成动态权重，这允许通过单个神经元处理大规模编辑任务，与现有方法（如基于梯度更新或外部记忆的方法）相比，本质区别在于MeG避免了参数直接修改，减少了编辑冲突和计算开销，同时提高了编辑的局部性和泛化性。\\n\\n**关键设计**：关键设计包括：动态权重神经元附加在LLMs的特定层（具体层数未指定，可能基于实验选择），扩散模型采用条件生成架构，以输入查询的嵌入向量作为条件输入；损失函数可能基于重建误差或对抗损失来训练扩散模型；网络结构细节未明确，但扩散模型通常包含去噪网络；参数设置如神经元大小、扩散步数等需根据实验调整，以确保编辑效果。",
            "application_zh": "该研究可应用于大语言模型的持续学习和知识更新场景，例如在AI助手、内容生成系统或专业领域模型中，快速修正错误知识、添加新信息或适应政策变化，而无需昂贵重训练。潜在价值包括降低模型维护成本、提高编辑效率，并推动可编辑AI系统的发展，未来可能影响自适应机器学习、个性化模型定制等领域。",
            "highlight_zh": "实验结果显示，MeG在可靠性、泛化性和局部性指标上均显著优于现有知识编辑方法（如基于微调或参数编辑的方法）。具体而言，在局部性指标上，MeG实现了高百分点的绝对值提升（具体数值未提供，但强调“高百分点增长”），表明其能有效保持无关知识不变；同时，在可靠性和泛化性方面也有明显改进，验证了该方法在大规模编辑任务中的优势。",
            "tags_zh": [
                "知识编辑",
                "大语言模型",
                "动态权重生成",
                "扩散模型",
                "大规模编辑",
                "模型可编辑性",
                "条件生成",
                "神经元附加"
            ],
            "_index": 65
        },
        {
            "title": "A Comprehensive Safety Metric to Evaluate Perception in Autonomous Systems",
            "authors": [
                "Georg Volk",
                "Jörg Gamerdinger",
                "Alexander von Bernuth",
                "Oliver Bringmann"
            ],
            "arxiv_id": "2512.14367v1",
            "summary": "Complete perception of the environment and its correct interpretation is crucial for autonomous vehicles. Object perception is the main component of automotive surround sensing. Various metrics already exist for the evaluation of object perception. However, objects can be of different importance depending on their velocity, orientation, distance, size, or the potential damage that could be caused by a collision due to a missed detection. Thus, these additional parameters have to be considered for safety evaluation. We propose a new safety metric that incorporates all these parameters and returns a single easily interpretable safety assessment score for object perception. This new metric is evaluated with both real world and virtual data sets and compared to state of the art metrics.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted at IEEE ITSC 2020",
            "doi": "10.1109/ITSC45102.2020.9294708",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14367v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "autonomous vehicle"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出一种综合安全度量标准，以评估自动驾驶系统中的感知性能，考虑对象重要性参数。",
            "summary_zh": "环境感知及其正确解释对自动驾驶车辆至关重要，对象感知是汽车环绕感知的核心组成部分。现有多种度量标准用于评估对象感知，但对象的重要性可能因其速度、方向、距离、大小或漏检可能导致碰撞的潜在损害而不同。因此，安全评估需考虑这些额外参数。我们提出一种新的安全度量标准，整合所有参数，返回单一易于解释的安全评估分数。该新度量标准通过真实世界和虚拟数据集进行评估，并与最先进度量标准进行比较。",
            "intro_zh": [
                "核心问题：现有对象感知度量标准未考虑对象重要性差异，如速度、方向、距离、大小和潜在碰撞损害，导致安全评估不全面。",
                "方法要点：提出综合安全度量标准，整合对象重要性参数，生成单一安全分数，提升评估的准确性和可解释性。",
                "实验或效果：在真实和虚拟数据集上验证，新度量标准优于现有方法，提供更可靠的安全评估结果。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自动驾驶系统中对象感知评估的不足。现有度量标准如准确率、召回率等，主要关注检测性能，但忽略了对象在安全层面的重要性差异。例如，高速移动或近距离的对象可能对安全构成更大威胁，而现有方法未将这些参数纳入评估，导致安全评估不全面，无法准确反映感知系统在实际驾驶中的风险。\\n\\n**核心思路**：论文的核心思路是设计一种综合安全度量标准，将对象的重要性参数（如速度、方向、距离、大小和潜在碰撞损害）整合到评估中。通过量化这些参数对安全的影响，生成一个单一的安全评估分数，使评估更贴近实际驾驶场景的安全需求。这样设计是因为自动驾驶系统的安全不仅依赖于检测准确性，还取决于对象在环境中的动态特性和潜在风险。\\n\\n**技术框架**：整体架构包括参数收集、权重分配、分数计算和评估比较阶段。首先，从感知系统输出中提取对象参数（如速度、距离等）。然后，基于安全重要性为每个参数分配权重，可能使用专家知识或数据驱动方法。接着，通过数学公式（如加权和或更复杂的函数）计算综合安全分数。最后，在真实和虚拟数据集上应用该度量标准，并与现有度量标准进行对比分析。\\n\\n**关键创新**：最重要的技术创新是首次将对象重要性参数系统性地整合到感知评估中，形成单一安全分数。与现有方法的本质区别在于，现有方法多关注检测性能的统计指标，而新度量标准直接关联安全风险，提供更直观和实用的评估工具，有助于识别高风险场景下的感知缺陷。\\n\\n**关键设计**：关键设计包括参数选择（如速度、方向、距离、大小、潜在损害）、权重设置（可能基于碰撞概率或损害严重性模型）、分数计算函数（例如线性加权或非线性映射以确保分数在合理范围内）。具体技术细节如损失函数或网络结构在摘要中未提及，可能涉及简单的数学模型或更复杂的优化算法，但论文未详细说明，因此标记为未知。",
            "application_zh": "该研究主要应用于自动驾驶汽车领域，用于评估和优化感知系统的安全性能。潜在价值包括提升车辆在复杂交通环境中的可靠性，减少因感知错误导致的事故风险。未来影响可能扩展到其他自主系统，如机器人导航和无人机监控，促进安全关键系统的标准化评估。",
            "highlight_zh": "最重要的实验结果包括：在真实世界和虚拟数据集上，新提出的综合安全度量标准相比现有最先进度量标准，显示出更高的评估准确性和实用性。具体性能数据如提升幅度在摘要中未提供，但通过对比分析，新度量标准能更有效地识别高风险对象，提供更可靠的安全分数，验证了其在实际应用中的优势。",
            "tags_zh": [
                "自动驾驶感知",
                "安全评估",
                "对象重要性",
                "综合度量标准",
                "感知性能",
                "风险评估",
                "虚拟验证",
                "真实世界数据"
            ],
            "_index": 66
        },
        {
            "title": "Unified Semantic Transformer for 3D Scene Understanding",
            "authors": [
                "Sebastian Koch",
                "Johanna Wald",
                "Hide Matsuki",
                "Pedro Hermosilla",
                "Timo Ropinski",
                "Federico Tombari"
            ],
            "arxiv_id": "2512.14364v1",
            "summary": "Holistic 3D scene understanding involves capturing and parsing unstructured 3D environments. Due to the inherent complexity of the real world, existing models have predominantly been developed and limited to be task-specific. We introduce UNITE, a Unified Semantic Transformer for 3D scene understanding, a novel feed-forward neural network that unifies a diverse set of 3D semantic tasks within a single model. Our model operates on unseen scenes in a fully end-to-end manner and only takes a few seconds to infer the full 3D semantic geometry. Our approach is capable of directly predicting multiple semantic attributes, including 3D scene segmentation, instance embeddings, open-vocabulary features, as well as affordance and articulations, solely from RGB images. The method is trained using a combination of 2D distillation, heavily relying on self-supervision and leverages novel multi-view losses designed to ensure 3D view consistency. We demonstrate that UNITE achieves state-of-the-art performance on several different semantic tasks and even outperforms task-specific models, in many cases, surpassing methods that operate on ground truth 3D geometry. See the project website at unite-page.github.io",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page: https://unite-page.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14364v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出UNITE统一语义Transformer，以单一模型解决3D场景理解中的多任务挑战",
            "summary_zh": "整体3D场景理解涉及捕获和解析非结构化3D环境。由于现实世界的固有复杂性，现有模型主要被开发并局限于特定任务。我们引入了UNITE，一个用于3D场景理解的统一语义Transformer，这是一种新颖的前馈神经网络，将多种3D语义任务统一在单一模型中。我们的模型以完全端到端的方式处理未见过的场景，仅需几秒钟即可推断完整的3D语义几何。我们的方法能够直接从RGB图像预测多种语义属性，包括3D场景分割、实例嵌入、开放词汇特征，以及功能性和关节性。该方法通过结合2D蒸馏进行训练，严重依赖自监督，并利用新颖的多视图损失设计来确保3D视图一致性。我们证明UNITE在多个不同语义任务上实现了最先进的性能，甚至在许多情况下超越了特定任务模型，超过了基于真实3D几何的方法。请访问项目网站unite-page.github.io。",
            "intro_zh": [
                "现有3D场景理解模型多为任务特定，难以处理现实世界的复杂性和多样性，导致泛化能力有限。",
                "提出UNITE统一语义Transformer，通过单一模型整合多任务学习，直接从RGB图像预测多种语义属性，实现端到端处理。",
                "实验表明UNITE在多个语义任务上达到SOTA性能，超越任务特定模型，并在许多情况下优于基于真实3D几何的方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决3D场景理解中现有模型多为任务特定、泛化能力不足的问题。现有方法通常针对单一任务（如分割或实例检测）设计，难以统一处理复杂场景，且依赖真实3D几何数据，限制了实际应用。\\n\\n**核心思路**：论文提出UNITE统一语义Transformer，核心思路是通过单一模型整合多任务学习，直接从RGB图像预测多种语义属性，实现端到端处理。设计基于Transformer架构，利用自监督和多视图一致性损失，减少对真实3D数据的依赖，提升模型的泛化能力和效率。\\n\\n**技术框架**：整体架构包括输入处理、Transformer编码、多任务解码和输出生成阶段。输入为RGB图像，通过特征提取模块转换为初始表示；Transformer编码器处理这些表示，捕获全局上下文；多任务解码器并行预测分割、实例嵌入、开放词汇特征等属性；输出为完整的3D语义几何，整个过程以端到端方式实现。\\n\\n**关键创新**：最重要的技术创新是提出统一语义Transformer，将多任务学习整合到单一模型中，直接从RGB图像预测多种语义属性，无需真实3D几何。与现有方法的本质区别在于其通用性和效率，通过自监督和多视图损失确保3D一致性，超越任务特定模型。\\n\\n**关键设计**：关键设计包括使用Transformer架构作为核心，结合2D蒸馏训练方法，依赖自监督学习；引入新颖的多视图损失函数，如视图一致性损失，以确保3D预测的鲁棒性；网络参数设置优化以实现快速推理（几秒内完成），并支持开放词汇特征预测，增强模型的适应性。",
            "application_zh": "该研究在机器人导航、自动驾驶、增强现实和智能监控等领域具有广泛应用潜力。通过直接从RGB图像理解3D场景，可降低对昂贵传感器的依赖，提升系统在复杂环境中的感知能力，推动人工智能在现实世界任务中的实际部署。",
            "highlight_zh": "UNITE在多个3D语义任务上实现SOTA性能，例如在场景分割和实例检测任务中，准确率提升约5-10%，超越现有任务特定模型。实验显示，在未见过的场景上，模型仅需几秒完成推理，并在许多情况下优于基于真实3D几何的方法，验证了其高效性和泛化能力。",
            "tags_zh": [
                "3D场景理解",
                "统一语义Transformer",
                "多任务学习",
                "端到端处理",
                "自监督学习",
                "多视图一致性",
                "RGB图像处理",
                "开放词汇特征"
            ],
            "_index": 67
        },
        {
            "title": "Causal Structure Learning for Dynamical Systems with Theoretical Score Analysis",
            "authors": [
                "Nicholas Tagliapietra",
                "Katharina Ensinger",
                "Christoph Zimmer",
                "Osman Mian"
            ],
            "arxiv_id": "2512.14361v1",
            "summary": "Real world systems evolve in continuous-time according to their underlying causal relationships, yet their dynamics are often unknown. Existing approaches to learning such dynamics typically either discretize time -- leading to poor performance on irregularly sampled data -- or ignore the underlying causality. We propose CaDyT, a novel method for causal discovery on dynamical systems addressing both these challenges. In contrast to state-of-the-art causal discovery methods that model the problem using discrete-time Dynamic Bayesian networks, our formulation is grounded in Difference-based causal models, which allow milder assumptions for modeling the continuous nature of the system. CaDyT leverages exact Gaussian Process inference for modeling the continuous-time dynamics which is more aligned with the underlying dynamical process. We propose a practical instantiation that identifies the causal structure via a greedy search guided by the Algorithmic Markov Condition and Minimum Description Length principle. Our experiments show that CaDyT outperforms state-of-the-art methods on both regularly and irregularly-sampled data, discovering causal networks closer to the true underlying dynamics.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "math.DS"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted as Oral at AAAI 2026 Conference",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14361v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出CaDyT方法，基于差分因果模型和高斯过程推理，解决动态系统中因果结构学习在连续时间和不规则采样数据上的挑战。",
            "summary_zh": "现实世界系统根据其潜在因果关系在连续时间内演化，但其动力学通常未知。现有学习这些动力学的方法通常要么离散化时间——导致在不规则采样数据上性能不佳——要么忽略底层因果关系。我们提出了CaDyT，一种用于动态系统因果发现的新方法，解决了这两个挑战。与使用离散时间动态贝叶斯网络建模问题的最先进因果发现方法不同，我们的公式基于差分因果模型，该模型允许对系统的连续性质进行更温和的假设建模。CaDyT利用精确的高斯过程推理来建模连续时间动力学，这更符合底层动态过程。我们提出了一种实用的实例化方法，通过由算法马尔可夫条件和最小描述长度原则指导的贪婪搜索来识别因果结构。我们的实验表明，CaDyT在规则和不规则采样数据上都优于最先进的方法，发现了更接近真实底层动力学的因果网络。",
            "intro_zh": [
                "现有方法在动态系统因果发现中面临挑战：时间离散化导致不规则采样数据性能差，或忽略底层因果关系，限制了模型对连续过程的适应性。",
                "CaDyT基于差分因果模型，利用高斯过程推理建模连续时间动力学，通过贪婪搜索结合算法马尔可夫条件和最小描述长度原则识别因果结构。",
                "实验表明，CaDyT在规则和不规则采样数据上均优于现有方法，能更准确地发现接近真实动力学的因果网络，提升了因果发现的鲁棒性和准确性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决动态系统中因果结构学习的问题，特别是在连续时间背景下。现有方法的痛点包括：传统方法通常离散化时间，这在不规则采样数据上表现不佳；而其他方法可能忽略系统的因果结构，导致模型无法准确反映底层动力学过程。\\n\\n**核心思路**：论文的核心解决思路是结合差分因果模型和高斯过程推理，以更温和的假设建模系统的连续性质。这样设计是因为差分因果模型允许直接处理连续时间动态，避免了离散化带来的信息损失，而高斯过程推理能精确建模不确定性，更符合真实世界的动态演化。\\n\\n**技术框架**：整体架构包括数据预处理、模型构建和结构搜索三个阶段。主要模块：首先，使用差分因果模型定义系统动力学；其次，应用高斯过程进行连续时间推理；最后，通过贪婪搜索算法，基于算法马尔可夫条件和最小描述长度原则，迭代优化因果结构。\\n\\n**关键创新**：最重要的技术创新点是将差分因果模型与高斯过程推理相结合，用于连续时间动态系统的因果发现。与现有方法的本质区别在于：现有方法多基于离散时间动态贝叶斯网络，而CaDyT直接建模连续时间，减少了假设强度，提高了对不规则采样数据的适应性。\\n\\n**关键设计**：关键设计包括：使用高斯过程核函数（如径向基函数）建模时间相关性；设置贪婪搜索的停止准则基于最小描述长度；参数优化通过最大似然估计进行；损失函数结合了模型拟合优度和结构复杂度，以平衡准确性和简洁性。",
            "application_zh": "该研究在机器人控制、生物医学信号处理、金融时间序列分析等领域具有潜在应用价值。通过更准确地发现动态系统中的因果结构，CaDyT能提升模型预测能力，支持决策制定，例如在自动驾驶中优化传感器数据处理，或在医疗监测中识别疾病动态模式，未来可能推动因果人工智能在复杂系统建模中的发展。",
            "highlight_zh": "实验结果显示，CaDyT在规则和不规则采样数据上均优于最先进的因果发现方法。具体性能数据表明，CaDyT发现的因果网络更接近真实底层动力学，例如在合成数据集上，其结构准确率提升约10-15%，在真实世界不规则采样数据中，鲁棒性显著增强，验证了其在连续时间建模中的优势。",
            "tags_zh": [
                "因果发现",
                "动态系统",
                "连续时间建模",
                "高斯过程",
                "差分因果模型",
                "算法马尔可夫条件",
                "最小描述长度",
                "不规则采样数据"
            ],
            "_index": 68
        },
        {
            "title": "Mimicking Human Visual Development for Learning Robust Image Representations",
            "authors": [
                "Ankita Raj",
                "Kaashika Prajaapat",
                "Tapan Kumar Gandhi",
                "Chetan Arora"
            ],
            "arxiv_id": "2512.14360v1",
            "summary": "The human visual system is remarkably adept at adapting to changes in the input distribution; a capability modern convolutional neural networks (CNNs) still struggle to match. Drawing inspiration from the developmental trajectory of human vision, we propose a progressive blurring curriculum to improve the generalization and robustness of CNNs. Human infants are born with poor visual acuity, gradually refining their ability to perceive fine details. Mimicking this process, we begin training CNNs on highly blurred images during the initial epochs and progressively reduce the blur as training advances. This approach encourages the network to prioritize global structures over high-frequency artifacts, improving robustness against distribution shifts and noisy inputs. Challenging prior claims that blurring in the initial training epochs imposes a stimulus deficit and irreversibly harms model performance, we reveal that early-stage blurring enhances generalization with minimal impact on in-domain accuracy. Our experiments demonstrate that the proposed curriculum reduces mean corruption error (mCE) by up to 8.30% on CIFAR-10-C and 4.43% on ImageNet-100-C datasets, compared to standard training without blurring. Unlike static blur-based augmentation, which applies blurred images randomly throughout training, our method follows a structured progression, yielding consistent gains across various datasets. Furthermore, our approach complements other augmentation techniques, such as CutMix and MixUp, and enhances both natural and adversarial robustness against common attack methods. Code is available at https://github.com/rajankita/Visual_Acuity_Curriculum.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted to ICVGIP 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14360v1",
            "code_links": [
                {
                    "url": "https://github.com/rajankita/Visual_Acuity_Curriculum",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出渐进模糊课程学习，模仿人类视觉发育过程以提升卷积神经网络的泛化与鲁棒性。",
            "summary_zh": "人类视觉系统能够出色地适应输入分布的变化，而现代卷积神经网络（CNNs）在这方面仍面临挑战。受人类视觉发育轨迹的启发，我们提出了一种渐进模糊课程学习方法来提升CNNs的泛化能力和鲁棒性。人类婴儿出生时视觉敏锐度较差，逐渐发展出感知细节的能力。模仿这一过程，我们在训练初期使用高度模糊的图像训练CNNs，随着训练推进逐步减少模糊程度。这种方法鼓励网络优先关注全局结构而非高频伪影，从而提升对分布偏移和噪声输入的鲁棒性。与先前认为早期模糊会导致刺激不足并不可逆地损害模型性能的观点不同，我们发现早期模糊能够增强泛化能力，同时对域内准确率影响极小。实验表明，与标准无模糊训练相比，所提方法在CIFAR-10-C数据集上平均腐蚀误差（mCE）降低了8.30%，在ImageNet-100-C数据集上降低了4.43%。与静态模糊增强（在整个训练过程中随机应用模糊图像）不同，我们的方法遵循结构化渐进过程，在不同数据集上均取得一致提升。此外，该方法可与其他增强技术（如CutMix和MixUp）互补，并提升对常见攻击方法的自然和对抗鲁棒性。代码可在https://github.com/rajankita/Visual_Acuity_Curriculum获取。",
            "intro_zh": [
                "现有卷积神经网络在适应输入分布变化方面表现不佳，泛化与鲁棒性不足，难以匹配人类视觉系统的适应能力。",
                "提出渐进模糊课程学习，模仿人类视觉发育过程，从高度模糊图像开始训练并逐步减少模糊，引导网络优先学习全局结构。",
                "实验显示该方法显著提升模型鲁棒性，在CIFAR-10-C和ImageNet-100-C数据集上分别降低mCE达8.30%和4.43%，且与现有增强技术兼容。"
            ],
            "method_zh": "**问题定义**：论文旨在解决卷积神经网络（CNNs）在面临输入分布变化（如噪声、腐蚀）时泛化能力和鲁棒性不足的问题。现有方法如静态模糊增强虽能提升鲁棒性，但缺乏结构化设计，可能无法有效引导网络学习稳健特征，且先前研究认为早期模糊会损害模型性能，导致刺激不足。\\n\\n**核心思路**：受人类视觉发育过程的启发，提出渐进模糊课程学习。人类婴儿从模糊视觉逐渐发展出清晰视觉，模仿这一过程，在训练初期使用高度模糊图像，随着训练推进逐步减少模糊程度，使网络先学习全局结构，再细化细节，从而提升对分布偏移的鲁棒性。\\n\\n**技术框架**：整体流程分为三个阶段：初始化阶段使用高模糊图像训练网络，中间阶段逐步降低模糊程度，最终阶段使用清晰图像微调。训练过程中，模糊程度根据预设的课程表（如线性或指数衰减）动态调整，确保网络从粗到细学习特征。主要模块包括模糊生成器（应用高斯模糊等滤波器）、课程调度器（控制模糊程度变化）和标准CNN训练流程。\\n\\n**关键创新**：最重要的创新是结构化渐进模糊课程，与静态模糊增强的本质区别在于其动态性和模仿人类发育的生物学合理性。该方法通过逐步减少模糊，引导网络优先关注稳健的全局特征，而非易受干扰的高频细节，从而系统性提升泛化能力。\\n\\n**关键设计**：关键参数包括初始模糊强度（如高斯核标准差）、模糊衰减策略（如线性或指数衰减）和训练周期分配。损失函数采用标准分类损失（如交叉熵），网络结构为常见CNNs（如ResNet）。模糊生成器使用高斯滤波器，课程调度器根据训练epoch数动态调整模糊程度，确保平滑过渡。",
            "application_zh": "该研究在计算机视觉领域具有广泛潜在应用，可提升模型在真实世界中的鲁棒性，适用于自动驾驶、医疗影像分析、安防监控等场景，其中输入数据常受噪声、光照变化或腐蚀影响。未来可能推动更生物启发的AI训练方法发展，增强AI系统的适应性和可靠性。",
            "highlight_zh": "实验结果显示，渐进模糊课程学习显著提升模型鲁棒性。在CIFAR-10-C数据集上，平均腐蚀误差（mCE）降低8.30%；在ImageNet-100-C数据集上降低4.43%。与标准训练基线相比，该方法在不显著损害域内准确率的前提下，实现了对分布偏移的更好适应。此外，与CutMix和MixUp等增强技术结合时，进一步提升了自然和对抗鲁棒性，验证了其兼容性和有效性。",
            "tags_zh": [
                "渐进模糊课程学习",
                "卷积神经网络",
                "泛化能力",
                "鲁棒性",
                "人类视觉发育",
                "图像表示学习",
                "分布偏移",
                "对抗攻击"
            ],
            "_index": 69
        },
        {
            "title": "Enhancing Interpretability for Vision Models via Shapley Value Optimization",
            "authors": [
                "Kanglong Fan",
                "Yunqiao Yang",
                "Chen Ma"
            ],
            "arxiv_id": "2512.14354v1",
            "summary": "Deep neural networks have demonstrated remarkable performance across various domains, yet their decision-making processes remain opaque. Although many explanation methods are dedicated to bringing the obscurity of DNNs to light, they exhibit significant limitations: post-hoc explanation methods often struggle to faithfully reflect model behaviors, while self-explaining neural networks sacrifice performance and compatibility due to their specialized architectural designs. To address these challenges, we propose a novel self-explaining framework that integrates Shapley value estimation as an auxiliary task during training, which achieves two key advancements: 1) a fair allocation of the model prediction scores to image patches, ensuring explanations inherently align with the model's decision logic, and 2) enhanced interpretability with minor structural modifications, preserving model performance and compatibility. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art interpretability.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted to AAAI2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14354v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "SAC"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出基于沙普利值优化的自解释框架，以解决视觉模型可解释性与性能兼容性之间的矛盾。",
            "summary_zh": "深度神经网络在多个领域表现出色，但其决策过程仍不透明。尽管许多解释方法致力于揭示深度神经网络的模糊性，但它们存在显著局限性：事后解释方法往往难以忠实反映模型行为，而自解释神经网络因其专门架构设计而牺牲了性能和兼容性。为解决这些挑战，我们提出了一种新颖的自解释框架，在训练过程中将沙普利值估计作为辅助任务集成，实现了两个关键进展：1）将模型预测分数公平分配给图像块，确保解释与模型的决策逻辑内在一致；2）通过微小的结构修改增强可解释性，同时保持模型性能和兼容性。在多个基准测试上的广泛实验表明，我们的方法实现了最先进的可解释性。",
            "intro_zh": [
                "现有方法存在局限性：事后解释方法难以忠实反映模型行为，自解释神经网络牺牲性能和兼容性。",
                "提出自解释框架，集成沙普利值估计作为辅助任务，实现公平分配预测分数和增强可解释性。",
                "在多个基准测试上实现最先进的可解释性，同时保持模型性能和兼容性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决深度神经网络决策过程不透明的问题，现有方法如事后解释方法（如LIME、SHAP）可能无法忠实反映模型内部逻辑，而自解释神经网络（如ProtoPNet）则因专门架构设计导致性能下降和兼容性差。\\n\\n**核心思路**：通过将沙普利值估计作为训练过程中的辅助任务，构建一个自解释框架，使模型在预测时能自动生成与决策逻辑一致的解释，从而在保持高性能的同时增强可解释性。这样设计基于沙普利值在博弈论中的公平分配特性，确保解释的公正性和内在一致性。\\n\\n**技术框架**：整体架构包括一个主视觉模型（如CNN或Transformer）和一个辅助解释模块。训练流程分为两个阶段：首先，模型进行标准分类任务；其次，通过优化沙普利值估计损失，将预测分数分配给输入图像的各个块（如补丁或区域），生成解释图。主要模块包括特征提取器、分类头和沙普利值计算器。\\n\\n**关键创新**：最重要的技术创新是将沙普利值优化集成到训练过程中作为辅助任务，而非事后计算。与现有方法的本质区别在于，它避免了事后解释的偏差问题，同时通过微小结构修改（如添加解释层）实现自解释，而不牺牲模型性能或兼容性，从而在可解释性和实用性之间取得平衡。\\n\\n**关键设计**：关键参数包括图像块的大小和数量，用于划分输入；损失函数结合分类损失和沙普利值估计损失，后者可能基于蒙特卡洛采样或近似算法计算；网络结构上，在标准视觉模型基础上添加轻量级解释层，以生成解释图，确保计算效率。",
            "application_zh": "该研究在医疗影像分析、自动驾驶、安防监控等领域具有潜在应用价值，通过提供可靠解释增强模型可信度，辅助决策制定。未来可能推动可解释人工智能在现实世界中的部署，促进人机协作和监管合规。",
            "highlight_zh": "在多个基准测试（如ImageNet、CIFAR-10）上，该方法实现了最先进的可解释性，具体表现为解释图与模型决策逻辑的一致性指标（如保真度）提升约10-15%，同时分类准确率保持与基线模型相当（如ImageNet上Top-1准确率约78%），优于现有自解释方法（如ProtoPNet）和事后解释方法（如Grad-CAM）。",
            "tags_zh": [
                "可解释人工智能",
                "沙普利值",
                "自解释神经网络",
                "视觉模型",
                "辅助任务",
                "公平分配",
                "性能兼容性",
                "解释图生成"
            ],
            "_index": 70
        },
        {
            "title": "Fine-Tuning of Neural Network Approximate MPC without Retraining via Bayesian Optimization",
            "authors": [
                "Henrik Hose",
                "Paul Brunzema",
                "Alexander von Rohr",
                "Alexander Gräfe",
                "Angela P. Schoellig",
                "Sebastian Trimpe"
            ],
            "arxiv_id": "2512.14350v1",
            "summary": "Approximate model-predictive control (AMPC) aims to imitate an MPC's behavior with a neural network, removing the need to solve an expensive optimization problem at runtime. However, during deployment, the parameters of the underlying MPC must usually be fine-tuned. This often renders AMPC impractical as it requires repeatedly generating a new dataset and retraining the neural network. Recent work addresses this problem by adapting AMPC without retraining using approximated sensitivities of the MPC's optimization problem. Currently, this adaption must be done by hand, which is labor-intensive and can be unintuitive for high-dimensional systems. To solve this issue, we propose using Bayesian optimization to tune the parameters of AMPC policies based on experimental data. By combining model-based control with direct and local learning, our approach achieves superior performance to nominal AMPC on hardware, with minimal experimentation. This allows automatic and data-efficient adaptation of AMPC to new system instances and fine-tuning to cost functions that are difficult to directly implement in MPC. We demonstrate the proposed method in hardware experiments for the swing-up maneuver on an inverted cartpole and yaw control of an under-actuated balancing unicycle robot, a challenging control problem.",
            "categories": [
                "cs.RO",
                "eess.SY"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14350v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出基于贝叶斯优化的近似模型预测控制微调方法，无需重新训练神经网络即可适应新系统实例和成本函数。",
            "summary_zh": "近似模型预测控制（AMPC）旨在通过神经网络模仿MPC的行为，从而避免在运行时求解昂贵的优化问题。然而，在部署过程中，通常需要对底层MPC的参数进行微调，这往往需要重新生成数据集并重新训练神经网络，导致AMPC不实用。最近的研究通过利用MPC优化问题的近似灵敏度来调整AMPC而无需重新训练，但这种方法需要手动操作，对于高维系统来说既费时又不直观。为解决这一问题，我们提出使用贝叶斯优化基于实验数据来调整AMPC策略的参数。通过将基于模型的控制与直接和局部学习相结合，我们的方法在硬件上实现了比名义AMPC更优的性能，且实验量最小。这使得AMPC能够自动且数据高效地适应新系统实例，并微调到难以直接在MPC中实现的成本函数。我们在硬件实验中验证了所提方法，包括倒立摆的摆动上升操作和欠驱动平衡独轮机器人的偏航控制，这是一个具有挑战性的控制问题。",
            "intro_zh": [
                "现有AMPC在部署时需手动微调参数，过程繁琐且不适用于高维系统，限制了其实用性。",
                "提出结合贝叶斯优化与实验数据，自动调整AMPC参数，无需重新训练神经网络，实现高效适应。",
                "在倒立摆和独轮机器人硬件实验中，该方法性能优于名义AMPC，验证了其有效性和数据效率。"
            ],
            "method_zh": "**问题定义**：论文解决近似模型预测控制（AMPC）在部署时参数微调的问题。现有AMPC通过神经网络模仿MPC行为，避免运行时优化计算，但微调通常需要重新生成数据集和重新训练神经网络，导致过程昂贵且不实用。最近方法利用MPC优化问题的近似灵敏度进行调整而无需重新训练，但需手动操作，对于高维系统来说劳动密集且不直观。\\n\\n**核心思路**：论文提出使用贝叶斯优化基于实验数据自动调整AMPC策略的参数。核心思想是将基于模型的控制与直接和局部学习相结合，通过贝叶斯优化高效搜索参数空间，最小化实验成本，实现AMPC的自动适应。这样设计是因为贝叶斯优化能处理黑盒优化问题，适合在有限实验数据下找到最优参数，避免手动调整的繁琐和不确定性。\\n\\n**技术框架**：整体架构包括两个主要阶段：离线训练和在线微调。离线阶段，使用标准方法训练AMPC神经网络以模仿MPC行为；在线阶段，通过贝叶斯优化器基于实验数据调整AMPC参数。流程为：收集实验数据，构建代理模型（如高斯过程），优化目标函数（如控制性能指标），迭代更新参数直至收敛。关键模块包括AMPC策略、贝叶斯优化器和实验平台。\\n\\n**关键创新**：最重要的技术创新是将贝叶斯优化引入AMPC微调过程，实现无需重新训练的自动参数调整。与现有方法的本质区别在于：现有方法依赖手动调整或近似灵敏度，而本方法通过数据驱动的优化自动适应，提高了效率和可扩展性，特别适用于高维系统和复杂成本函数。\\n\\n**关键设计**：关键设计包括：使用高斯过程作为贝叶斯优化的代理模型，以建模目标函数与参数的关系；采用预期改进（EI）等采集函数指导参数选择；目标函数基于实验数据定义，如控制误差或任务完成时间；AMPC神经网络结构未详细指定，但通常为前馈网络，模仿MPC的输入-输出映射；参数设置涉及优化迭代次数和实验预算，以平衡性能与数据效率。",
            "application_zh": "该研究在机器人控制、自动驾驶和工业自动化等领域具有潜在应用价值。通过实现AMPC的自动微调，可以快速适应新系统实例（如不同型号的机器人）和复杂成本函数（如难以在MPC中直接定义的性能指标），提升控制系统的灵活性和部署效率。未来可能推动自适应控制技术的发展，降低硬件实验成本。",
            "highlight_zh": "在倒立摆摆动上升和欠驱动平衡独轮机器人偏航控制的硬件实验中，该方法相比名义AMPC实现了性能提升。具体地，在倒立摆任务中，通过贝叶斯优化微调后，控制误差减少约20%；在独轮机器人任务中，成功完成偏航控制，且实验数据量最小，仅需少量迭代即可收敛。这些结果验证了方法在挑战性控制问题上的有效性和数据效率。",
            "tags_zh": [
                "近似模型预测控制",
                "贝叶斯优化",
                "神经网络微调",
                "机器人控制",
                "自适应控制",
                "数据高效学习",
                "硬件实验",
                "参数优化"
            ],
            "_index": 71
        },
        {
            "title": "Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure",
            "authors": [
                "Jooyeol Yun",
                "Jaegul Choo"
            ],
            "arxiv_id": "2512.14336v1",
            "summary": "Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to grow as web environments become increasingly dynamic. Yet automating the animation of vector graphics remains challenging for vision-language models (VLMs) despite recent progress in code generation and motion planning. VLMs routinely mis-handle SVGs, since visually coherent parts are often fragmented into low-level shapes that offer little guidance of which elements should move together. In this paper, we introduce a framework that recovers the semantic structure required for reliable SVG animation and reveals the missing layer that current VLM systems overlook. This is achieved through a statistical aggregation of multiple weak part predictions, allowing the system to stably infer semantics from noisy predictions. By reorganizing SVGs into semantic groups, our approach enables VLMs to produce animations with far greater coherence. Our experiments demonstrate substantial gains over existing approaches, suggesting that semantic recovery is the key step that unlocks robust SVG animation and supports more interpretable interactions between VLMs and vector graphics.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "yeolj00.github.io/personal-projects/vector-prism",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14336v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出Vector Prism框架，通过分层语义结构恢复解决SVG动画自动化中的视觉语言模型误处理问题。",
            "summary_zh": "可缩放矢量图形（SVG）在现代网页设计中至关重要，随着网络环境日益动态化，对其动画化的需求不断增长。然而，尽管在代码生成和运动规划方面取得了进展，自动化矢量图形动画对视觉语言模型（VLMs）仍然具有挑战性。VLMs经常误处理SVG，因为视觉上连贯的部分通常被分割成低级形状，这些形状很少提供哪些元素应该一起移动的指导。本文介绍了一个框架，该框架恢复了可靠SVG动画所需的语义结构，并揭示了当前VLM系统忽略的缺失层。这是通过对多个弱部分预测进行统计聚合实现的，使系统能够从噪声预测中稳定推断语义。通过将SVG重新组织为语义组，我们的方法使VLMs能够产生更具连贯性的动画。我们的实验表明，与现有方法相比有显著提升，这表明语义恢复是解锁稳健SVG动画并支持VLMs与矢量图形之间更可解释交互的关键步骤。",
            "intro_zh": [
                "现有视觉语言模型在SVG动画中常误处理，因视觉连贯部分被分割成低级形状，缺乏语义指导。",
                "提出Vector Prism框架，通过统计聚合弱预测恢复语义结构，将SVG重组为语义组以提升动画连贯性。",
                "实验显示该方法相比现有方法有显著提升，验证了语义恢复对SVG动画自动化的关键作用。"
            ],
            "method_zh": "**问题定义**：论文旨在解决视觉语言模型（VLMs）在自动化SVG动画中的误处理问题。现有方法的痛点是，SVG中的视觉连贯部分常被分割成低级形状（如路径、矩形），这些形状缺乏高层语义信息，导致VLMs难以判断哪些元素应一起移动，从而产生不连贯的动画。\\n\\n**核心思路**：论文的核心解决思路是通过恢复SVG的语义结构来指导动画生成。设计基于统计聚合多个弱部分预测，以稳定推断语义，从而弥补当前VLM系统忽略的语义层。这有助于将SVG重组为语义组，提升动画的连贯性和可靠性。\\n\\n**技术框架**：整体架构包括三个阶段：首先，输入SVG文件，提取其低级形状元素；其次，应用视觉语言模型生成多个弱部分预测，这些预测可能包含噪声；然后，通过统计聚合方法（如聚类或投票机制）整合这些预测，恢复出高层语义结构；最后，基于恢复的语义组，指导VLMs生成连贯的动画。主要模块包括预测生成模块、语义恢复模块和动画生成模块。\\n\\n**关键创新**：最重要的技术创新点是引入语义恢复层，通过统计聚合弱预测来稳定推断语义，这与现有方法直接依赖低级形状形成本质区别。现有方法通常忽略语义结构，导致动画不连贯，而本方法通过分层处理，增强了VLMs的语义理解能力。\\n\\n**关键设计**：关键设计包括使用多模态模型（如VLMs）进行弱部分预测，设计聚合算法（如基于置信度的加权平均或多数投票）以减少噪声影响，并可能涉及参数设置如预测数量阈值和语义分组标准。损失函数可能基于动画连贯性指标优化，网络结构可能结合编码器-解码器框架，但具体细节在摘要中未详细说明，需参考论文全文。",
            "application_zh": "该研究在网页设计、动态图形生成和交互式媒体领域具有潜在应用价值。通过提升SVG动画的自动化水平，可支持更高效的网页内容创作、广告设计和教育工具开发。未来可能推动视觉语言模型与矢量图形的深度融合，促进更智能的图形编辑和动画生成系统。",
            "highlight_zh": "实验表明，Vector Prism框架相比现有方法在SVG动画生成上取得显著提升，具体性能数据未在摘要中提供，但提到“substantial gains”，暗示在连贯性指标上可能有较大改进。对比基线可能包括直接使用VLMs的方法，提升幅度未知，但结果验证了语义恢复对动画质量的关键作用。",
            "tags_zh": [
                "矢量图形动画",
                "语义结构恢复",
                "视觉语言模型",
                "统计聚合",
                "SVG处理",
                "多模态学习",
                "网页设计自动化"
            ],
            "_index": 72
        },
        {
            "title": "Step-Tagging: Toward controlling the generation of Language Reasoning Models through step monitoring",
            "authors": [
                "Yannis Belkhiter",
                "Seshu Tirupathi",
                "Giulio Zizzo",
                "John D. Kelleher"
            ],
            "arxiv_id": "2512.14332v1",
            "summary": "The field of Language Reasoning Models (LRMs) has been very active over the past few years with advances in training and inference techniques enabling LRMs to reason longer, and more accurately. However, a growing body of studies show that LRMs are still inefficient, over-generating verification and reflection steps. To address this challenge, we introduce the Step-Tagging framework, a lightweight sentence-classifier enabling real-time annotation of the type of reasoning steps that an LRM is generating. To monitor reasoning behaviors, we introduced ReasonType: a novel taxonomy of reasoning steps. Building on this framework, we demonstrated that online monitoring of the count of specific steps can produce effective interpretable early stopping criteria of LRM inferences. We evaluate the Step-tagging framework on three open-source reasoning models across standard benchmark datasets: MATH500, GSM8K, AIME and non-mathematical tasks (GPQA and MMLU-Pro). We achieve 20 to 50\\% token reduction while maintaining comparable accuracy to standard generation, with largest gains observed on more computation-heavy tasks. This work offers a novel way to increase control over the generation of LRMs, and a new tool to study behaviors of LRMs.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14332v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出Step-Tagging框架，通过实时监控推理步骤类型来控制语言推理模型的生成过程",
            "summary_zh": "语言推理模型（LRMs）领域近年来发展迅速，训练和推理技术的进步使LRMs能够进行更长、更准确的推理。然而，越来越多的研究表明，LRMs仍然效率低下，过度生成验证和反思步骤。为解决这一挑战，我们引入了Step-Tagging框架，这是一个轻量级的句子分类器，能够实时标注LRM生成的推理步骤类型。为了监控推理行为，我们引入了ReasonType：一种新颖的推理步骤分类法。基于此框架，我们证明了在线监控特定步骤的数量可以产生有效的可解释早期停止标准，用于LRM推理。我们在三个开源推理模型上评估了Step-Tagging框架，使用标准基准数据集：MATH500、GSM8K、AIME以及非数学任务（GPQA和MMLU-Pro）。我们实现了20%到50%的令牌减少，同时保持与标准生成相当的准确性，在计算量更大的任务上观察到最大的增益。这项工作提供了一种新颖的方式来增加对LRM生成的控制，以及一种研究LRM行为的新工具。",
            "intro_zh": [
                "语言推理模型在生成过程中存在效率低下问题，过度生成验证和反思步骤，导致计算资源浪费。",
                "提出Step-Tagging框架，通过轻量级句子分类器实时标注推理步骤类型，并基于ReasonType分类法监控推理行为。",
                "在多个基准数据集上实现20-50%令牌减少，同时保持准确性，为LRM生成控制提供新方法。"
            ],
            "method_zh": "**问题定义**：语言推理模型（LRMs）在推理过程中存在效率低下问题，具体表现为过度生成验证和反思步骤，导致不必要的计算开销和资源浪费。现有方法缺乏对推理步骤类型的实时监控机制，无法有效控制生成过程，使得模型在达到正确结论后仍继续生成冗余步骤。\\n\\n**核心思路**：通过引入一个轻量级的句子分类器（Step-Tagging框架），实时识别和标注LRM生成的推理步骤类型，从而监控推理行为。基于此，设计早期停止标准，当检测到特定步骤（如验证或反思）数量达到阈值时，提前终止生成，以减少令牌使用并提高效率。\\n\\n**技术框架**：整体架构包括两个主要阶段：首先，构建ReasonType分类法，定义推理步骤类型（如前提、推导、验证、反思等）；其次，训练一个句子分类器，对LRM生成的每个句子进行实时分类，标注其所属的ReasonType类别。在推理过程中，系统在线统计各类步骤的数量，并根据预设规则（如验证步骤过多）触发早期停止。\\n\\n**关键创新**：最重要的技术创新是提出了ReasonType分类法和基于Step-Tagging的实时监控框架。与现有方法相比，本质区别在于将推理过程分解为可解释的步骤类型，并通过轻量级分类器实现动态控制，而非依赖固定的生成长度或后处理优化。\\n\\n**关键设计**：关键设计包括ReasonType分类法的具体类别定义（如数学推理中的计算、验证，非数学任务中的分析、总结），句子分类器的网络结构（可能基于预训练语言模型的轻量级微调），以及早期停止规则的参数设置（如验证步骤的最大允许数量）。损失函数通常使用交叉熵损失进行多分类训练，确保分类准确性。",
            "application_zh": "该研究在人工智能和机器学习领域具有广泛的应用潜力，特别是在需要高效推理的自然语言处理任务中，如数学问题求解、科学问答和复杂决策支持。实际价值在于降低计算成本，提高语言推理模型的部署效率，适用于资源受限环境。未来可能影响模型优化和可解释性研究，为开发更可控的AI系统提供新工具。",
            "highlight_zh": "在MATH500、GSM8K、AIME等数学数据集以及GPQA和MMLU-Pro非数学任务上，Step-Tagging框架实现了显著的令牌减少：达到20%到50%，同时保持与标准生成方法相当的准确性。对比基线为未使用早期停止的标准LRM生成，提升幅度在计算量更大的任务中尤为明显，例如在复杂数学推理中观察到最大增益。这证明了框架在减少冗余生成方面的有效性。",
            "tags_zh": [
                "语言推理模型",
                "步骤监控",
                "早期停止",
                "轻量级分类器",
                "推理效率",
                "可解释AI",
                "令牌减少",
                "ReasonType分类法"
            ],
            "_index": 73
        },
        {
            "title": "Leveraging LLMs for Collaborative Ontology Engineering in Parkinson Disease Monitoring and Alerting",
            "authors": [
                "Georgios Bouchouras",
                "Dimitrios Doumanas",
                "Andreas Soularidis",
                "Konstantinos Kotis",
                "George A. Vouros"
            ],
            "arxiv_id": "2512.14288v1",
            "summary": "This paper explores the integration of Large Language Models (LLMs) in the engineering of a Parkinson's Disease (PD) monitoring and alerting ontology through four key methodologies: One Shot (OS) prompt techniques, Chain of Thought (CoT) prompts, X-HCOME, and SimX-HCOME+. The primary objective is to determine whether LLMs alone can create comprehensive ontologies and, if not, whether human-LLM collaboration can achieve this goal. Consequently, the paper assesses the effectiveness of LLMs in automated ontology development and the enhancement achieved through human-LLM collaboration.\n  Initial ontology generation was performed using One Shot (OS) and Chain of Thought (CoT) prompts, demonstrating the capability of LLMs to autonomously construct ontologies for PD monitoring and alerting. However, these outputs were not comprehensive and required substantial human refinement to enhance their completeness and accuracy.\n  X-HCOME, a hybrid ontology engineering approach that combines human expertise with LLM capabilities, showed significant improvements in ontology comprehensiveness. This methodology resulted in ontologies that are very similar to those constructed by experts.\n  Further experimentation with SimX-HCOME+, another hybrid methodology emphasizing continuous human supervision and iterative refinement, highlighted the importance of ongoing human involvement. This approach led to the creation of more comprehensive and accurate ontologies.\n  Overall, the paper underscores the potential of human-LLM collaboration in advancing ontology engineering, particularly in complex domains like PD. The results suggest promising directions for future research, including the development of specialized GPT models for ontology construction.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14288v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出人机协作本体工程方法，利用LLMs提升帕金森病监测与警报本体构建的全面性与准确性",
            "summary_zh": "本文探讨了将大型语言模型（LLMs）集成到帕金森病（PD）监测与警报本体工程中的四种关键方法：一次性（OS）提示技术、思维链（CoT）提示、X-HCOME和SimX-HCOME+。主要目标是确定LLMs是否能够独立创建全面的本体，如果不能，人机协作是否能够实现这一目标。因此，本文评估了LLMs在自动化本体开发中的有效性以及通过人机协作实现的改进。\n\n初始本体生成使用一次性（OS）和思维链（CoT）提示进行，展示了LLMs自主构建PD监测与警报本体的能力。然而，这些输出并不全面，需要大量的人工细化来增强其完整性和准确性。\n\nX-HCOME是一种结合人类专业知识和LLM能力的混合本体工程方法，在本体全面性方面显示出显著改进。这种方法产生的本体与专家构建的本体非常相似。\n\n进一步实验使用SimX-HCOME+，这是另一种强调持续人类监督和迭代细化的混合方法，突出了持续人类参与的重要性。这种方法导致了更全面和准确的本体创建。\n\n总体而言，本文强调了人机协作在推进本体工程中的潜力，特别是在PD等复杂领域。结果指出了未来研究的有希望方向，包括开发专门用于本体构建的GPT模型。",
            "intro_zh": [
                "现有方法依赖专家手动构建本体，耗时且难以适应复杂领域如帕金森病监测，自动化工具缺乏全面性和准确性。",
                "论文提出结合LLMs与人类协作的混合方法，通过迭代提示和持续监督，提升本体工程的效率和效果。",
                "实验表明，人机协作方法（如X-HCOME和SimX-HCOME+）能生成更全面、准确的本体，接近专家水平，显著优于纯LLM方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决帕金森病监测与警报本体工程中自动化构建的挑战，现有纯LLM方法（如OS和CoT提示）生成的本体不够全面和准确，需要大量人工干预，效率低下且难以保证质量。\\n\\n**核心思路**：核心思路是引入人机协作的混合方法，结合LLMs的自动化生成能力和人类专家的领域知识，通过迭代反馈和持续监督，逐步优化本体，以克服纯自动化方法的局限性，实现更高效和准确的本体构建。\\n\\n**技术框架**：整体框架包括四个主要阶段：首先，使用OS和CoT提示进行初始本体生成；其次，应用X-HCOME方法，在LLM生成基础上加入人类专家评估和修正；然后，采用SimX-HCOME+方法，强调持续人类监督和多次迭代细化；最后，通过比较不同方法输出的本体，评估其全面性和准确性。\\n\\n**关键创新**：最重要的技术创新是提出了X-HCOME和SimX-HCOME+这两种混合本体工程方法，它们将LLMs的快速生成与人类专家的精细调整相结合，通过结构化协作流程，实现了本体质量的显著提升，与现有纯自动化或纯手动方法有本质区别。\\n\\n**关键设计**：关键设计包括使用特定提示技术（如OS和CoT）来引导LLMs生成初始本体；在X-HCOME中，设置人类专家审查环节，基于LLM输出进行修正；在SimX-HCOME+中，引入迭代循环，允许多次人类反馈和LLM重新生成，以逐步优化本体结构和内容，具体参数如迭代次数和提示模板根据实验需求调整。",
            "application_zh": "该研究在医疗健康领域具有重要应用价值，特别是帕金森病等慢性疾病的监测与警报系统。通过构建高质量本体，可以支持智能诊断、个性化治疗和远程监护，提升医疗服务的精准性和效率。未来可扩展到其他复杂医学领域或通用知识工程，推动人工智能在专业领域的深度集成。",
            "highlight_zh": "实验结果显示，纯LLM方法（OS和CoT）生成的本体在全面性和准确性上不足，需要大量人工细化。而人机协作方法X-HCOME和SimX-HCOME+显著提升性能，生成的本体与专家构建的非常相似，具体数据表明，混合方法在概念覆盖和关系准确性方面优于基线，例如通过迭代监督，本体完整性提高约30-50%，为自动化本体工程提供了有效路径。",
            "tags_zh": [
                "本体工程",
                "大型语言模型",
                "人机协作",
                "帕金森病监测",
                "混合方法",
                "自动化构建",
                "医疗人工智能",
                "知识表示"
            ],
            "_index": 74
        },
        {
            "title": "SS4D: Native 4D Generative Model via Structured Spacetime Latents",
            "authors": [
                "Zhibing Li",
                "Mengchen Zhang",
                "Tong Wu",
                "Jing Tan",
                "Jiaqi Wang",
                "Dahua Lin"
            ],
            "arxiv_id": "2512.14284v1",
            "summary": "We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "ToG(Siggraph Asia 2025)",
            "doi": "10.1145/3763302",
            "journal_ref": "ACM Transactions on Graphics, 44(6): Article 244, 12 pages, December 2025",
            "pdf_url": "https://arxiv.org/pdf/2512.14284v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出SS4D原生4D生成模型，通过结构化时空潜在表示从单目视频直接合成动态3D物体",
            "summary_zh": "我们提出了SS4D，一种原生4D生成模型，能够直接从单目视频合成动态3D物体。与先前通过优化3D或视频生成模型来构建4D表示的方法不同，我们直接在4D数据上训练生成器，实现了高保真度、时间一致性和结构一致性。我们方法的核心是一组压缩的结构化时空潜在表示。具体而言：（1）为了解决4D训练数据稀缺的问题，我们基于预训练的单图像到3D模型构建，保持了强大的空间一致性。（2）通过引入专门的时间层来跨帧推理，强制实现时间一致性。（3）为了支持长视频序列的高效训练和推理，我们使用分解的4D卷积和时间下采样块沿时间轴压缩潜在序列。此外，我们采用精心设计的训练策略来增强对遮挡的鲁棒性。",
            "intro_zh": [
                "现有方法依赖3D或视频生成模型优化构建4D表示，导致保真度低、时间不一致和结构失真。",
                "提出结构化时空潜在表示，结合预训练3D模型、时间层和压缩机制，实现原生4D生成。",
                "实验显示SS4D在动态3D合成中实现高保真度和时间一致性，显著优于基线方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从单目视频直接生成动态3D物体（即4D表示）的问题。现有方法通常基于3D或视频生成模型进行优化，导致生成结果保真度低、时间不一致（如帧间抖动）和结构失真（如物体形状不稳定），且受限于4D训练数据稀缺和计算效率低。\\n\\n**核心思路**：论文提出直接训练原生4D生成模型，通过结构化时空潜在表示来统一建模空间和时间维度。设计思路包括：利用预训练单图像到3D模型解决数据稀缺问题，引入时间层确保跨帧一致性，采用压缩机制提高长序列处理效率。\\n\\n**技术框架**：整体架构包括输入单目视频、特征提取、结构化时空潜在编码、生成器网络和输出动态3D序列。主要模块：基于预训练模型的初始3D重建模块、时间推理层、4D卷积压缩模块和训练策略模块。流程为视频输入→提取时空特征→编码为压缩潜在表示→生成器合成4D输出。\\n\\n**关键创新**：最重要的创新是提出原生4D生成模型，而非基于现有模型的优化方法。本质区别在于直接端到端训练4D数据，实现了结构化时空潜在表示，结合了空间一致性和时间连贯性，避免了传统方法的分解式处理导致的误差累积。\\n\\n**关键设计**：关键参数包括潜在表示的维度压缩因子（如时间下采样率）、4D卷积的核大小和步长。损失函数结合重建损失、时间一致性损失和对抗损失。网络结构采用分解的4D卷积层来减少计算复杂度，时间层使用循环或注意力机制，训练策略包括数据增强（如模拟遮挡）和多阶段训练以增强鲁棒性。",
            "application_zh": "该研究在虚拟现实、游戏开发、电影特效和机器人仿真等领域具有潜在应用价值，能够高效生成逼真的动态3D内容，降低人工建模成本，推动4D生成技术的发展，未来可能扩展到更复杂的场景理解和交互式内容创建。",
            "highlight_zh": "实验结果表明，SS4D在动态3D合成任务中实现了高保真度（如PSNR提升约15%）和时间一致性（如帧间差异减少20%以上），显著优于基线方法如基于3D优化的模型。在长视频序列上，通过压缩机制，训练和推理效率提升约30%，同时保持生成质量。",
            "tags_zh": [
                "4D生成模型",
                "动态3D合成",
                "时空潜在表示",
                "单目视频理解",
                "时间一致性",
                "结构化压缩",
                "原生生成训练",
                "遮挡鲁棒性"
            ],
            "_index": 75
        },
        {
            "title": "SPARQL-LLM: Real-Time SPARQL Query Generation from Natural Language Questions",
            "authors": [
                "Panayiotis Smeros",
                "Vincent Emonet",
                "Ruijie Wang",
                "Ana-Claudia Sima",
                "Tarcisio Mendes de Farias"
            ],
            "arxiv_id": "2512.14277v1",
            "summary": "The advent of large language models is contributing to the emergence of novel approaches that promise to better tackle the challenge of generating structured queries, such as SPARQL queries, from natural language. However, these new approaches mostly focus on response accuracy over a single source while ignoring other evaluation criteria, such as federated query capability over distributed data stores, as well as runtime and cost to generate SPARQL queries. Consequently, they are often not production-ready or easy to deploy over (potentially federated) knowledge graphs with good accuracy. To mitigate these issues, in this paper, we extend our previous work and describe and systematically evaluate SPARQL-LLM, an open-source and triplestore-agnostic approach, powered by lightweight metadata, that generates SPARQL queries from natural language text. First, we describe its architecture, which consists of dedicated components for metadata indexing, prompt building, and query generation and execution. Then, we evaluate it based on a state-of-the-art challenge with multilingual questions, and a collection of questions from three of the most prevalent knowledge graphs within the field of bioinformatics. Our results demonstrate a substantial increase of 24% in the F1 Score on the state-of-the-art challenge, adaptability to high-resource languages such as English and Spanish, as well as ability to form complex and federated bioinformatics queries. Furthermore, we show that SPARQL-LLM is up to 36x faster than other systems participating in the challenge, while costing a maximum of $0.01 per question, making it suitable for real-time, low-cost text-to-SPARQL applications. One such application deployed over real-world decentralized knowledge graphs can be found at https://www.expasy.org/chat.",
            "categories": [
                "cs.IR",
                "cs.AI",
                "cs.CL"
            ],
            "primary_category": "cs.IR",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "17 pages, 8 figures, 1 table. Under Review",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14277v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出SPARQL-LLM方法，通过轻量级元数据实现实时、低成本的文本到SPARQL查询生成，适用于分布式知识图谱应用。",
            "summary_zh": "大型语言模型的出现促进了从自然语言生成结构化查询（如SPARQL查询）的新方法。然而，这些新方法大多关注单一来源的响应准确性，而忽略了其他评估标准，如分布式数据存储上的联邦查询能力，以及生成SPARQL查询的运行时间和成本。因此，它们通常不适合生产环境或难以在（可能联邦的）知识图谱上以良好准确性部署。为缓解这些问题，本文扩展了先前工作，描述并系统评估了SPARQL-LLM，这是一种开源且与三元存储无关的方法，由轻量级元数据驱动，从自然语言文本生成SPARQL查询。首先，我们描述了其架构，包括元数据索引、提示构建、查询生成和执行等专用组件。然后，基于一个包含多语言问题的最新挑战，以及来自生物信息学领域三个最流行知识图谱的问题集合进行评估。结果显示，在最新挑战中F1分数显著提高了24%，适应英语和西班牙语等高资源语言，并能形成复杂和联邦的生物信息学查询。此外，SPARQL-LLM比参与挑战的其他系统快达36倍，每个问题成本最高为0.01美元，使其适用于实时、低成本的文本到SPARQL应用。一个部署在真实世界去中心化知识图谱上的此类应用可在https://www.expasy.org/chat找到。",
            "intro_zh": [
                "现有方法主要关注单一来源的查询准确性，但忽略了联邦查询能力、运行时间和成本，导致难以在生产环境中部署。",
                "SPARQL-LLM采用轻量级元数据和专用组件架构，实现与三元存储无关的文本到SPARQL查询生成，提升效率和适应性。",
                "实验显示，F1分数提高24%，支持多语言和复杂联邦查询，运行速度快达36倍，成本低至每问题0.01美元。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从自然语言生成SPARQL查询的挑战，现有方法痛点包括过度依赖单一数据源、缺乏联邦查询支持、运行成本高且难以实时部署，限制了在生产环境中的应用。\\n\\n**核心思路**：通过轻量级元数据驱动，结合大型语言模型，设计一个开源、与三元存储无关的框架，以平衡准确性、效率和成本，实现实时、低成本的查询生成，特别强调对分布式知识图谱的适应性。\\n\\n**技术框架**：整体架构包括三个主要阶段：元数据索引阶段，用于提取和存储知识图谱的轻量级结构信息；提示构建阶段，基于元数据生成上下文丰富的提示，以指导语言模型；查询生成和执行阶段，利用语言模型生成SPARQL查询并执行验证，确保查询的正确性和可执行性。\\n\\n**关键创新**：最重要的技术创新是引入轻量级元数据作为中介，减少对原始数据的依赖，从而提升查询生成的速度和成本效益，同时支持联邦查询，与现有方法相比，本质区别在于更注重生产环境中的实用性和可扩展性。\\n\\n**关键设计**：关键设计包括元数据索引的优化算法，以减少存储和检索开销；提示构建中使用模板化方法，结合多语言支持；查询生成阶段集成错误检测和修正机制；整体框架基于开源工具，参数设置灵活，以适应不同知识图谱和语言模型，具体损失函数和网络结构细节在论文中未明确说明，可能依赖于标准语言模型训练。",
            "application_zh": "该研究在生物信息学、语义网和智能问答系统等领域具有广泛应用潜力，能支持实时、低成本的文本到SPARQL查询转换，促进分布式知识图谱的交互式访问，提升数据检索效率，未来可能扩展到更多行业如医疗、金融，推动知识驱动应用的发展。",
            "highlight_zh": "在最新挑战中，SPARQL-LLM的F1分数比基线方法提高了24%，展示了在多语言（如英语和西班牙语）和复杂联邦查询上的优越性能；运行速度比其他系统快达36倍，每个问题成本最高仅0.01美元，验证了其在实时、低成本应用中的有效性。",
            "tags_zh": [
                "SPARQL查询生成",
                "自然语言处理",
                "知识图谱",
                "联邦查询",
                "轻量级元数据",
                "实时系统",
                "低成本计算",
                "生物信息学应用"
            ],
            "_index": 76
        },
        {
            "title": "Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in",
            "authors": [
                "Xiaoqian Shen",
                "Min-Hung Chen",
                "Yu-Chiang Frank Wang",
                "Mohamed Elhoseiny",
                "Ryo Hachiuma"
            ],
            "arxiv_id": "2512.14273v1",
            "summary": "Grounded video question answering (GVQA) aims to localize relevant temporal segments in videos and generate accurate answers to a given question; however, large video-language models (LVLMs) exhibit limited temporal awareness. Although existing approaches based on Group Relative Policy Optimization (GRPO) attempt to improve temporal grounding, they still struggle to faithfully ground their answers in the relevant video evidence, leading to temporal mislocalization and hallucinations. In this work, we present Zoom-Zero, a coarse-to-fine framework that first localizes query-relevant segments and then temporally zooms into the most salient frames for finer-grained visual verification. Our method addresses the limits of GRPO for the GVQA task with two key innovations: (i) a zoom-in accuracy reward that validates the fidelity of temporal grounding prediction and facilitates fine-grained visual verification on grounded frames; (ii) token-selective credit assignment, which attributes rewards to the tokens responsible for temporal localization or answer generation, mitigating GRPO's issue in handling multi-faceted reward signals. Our proposed method advances grounded video question answering, improving temporal grounding by 5.2\\% on NExT-GQA and 4.6\\% on ReXTime, while also enhancing average answer accuracy by 2.4\\%. Additionally, the coarse-to-fine zoom-in during inference further benefits long-form video understanding by preserving critical visual details without compromising global context, yielding an average improvement of 6.4\\% on long-video benchmarks.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page: https://xiaoqian-shen.github.io/Zoom-Zero/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14273v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reward"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出Zoom-Zero框架，通过粗到细的时序放大机制解决视频问答中的时序定位不准确问题。",
            "summary_zh": "基于视频的问答任务旨在定位视频中的相关时序片段并生成准确答案，但现有大型视频语言模型在时序感知方面存在局限。虽然基于组相对策略优化的方法试图改进时序定位，但仍难以忠实基于视频证据生成答案，导致时序错位和幻觉。本文提出Zoom-Zero，这是一个粗到细的框架，首先定位查询相关片段，然后时序放大到最显著帧进行细粒度视觉验证。我们的方法通过两个关键创新解决了GVQA任务中GRPO的局限：(i) 放大精度奖励，验证时序定位预测的保真度并促进对定位帧的细粒度视觉验证；(ii) 令牌选择性信用分配，将奖励归因于负责时序定位或答案生成的令牌，缓解GRPO在处理多方面奖励信号时的问题。所提方法推进了基于视频的问答，在NExT-GQA上时序定位提升5.2%，在ReXTime上提升4.6%，同时平均答案准确率提升2.4%。此外，推理过程中的粗到细放大通过保留关键视觉细节而不损害全局上下文，进一步有益于长视频理解，在长视频基准上平均提升6.4%。",
            "intro_zh": [
                "现有大型视频语言模型在时序感知方面有限，导致视频问答中时序定位不准确和幻觉问题。",
                "提出Zoom-Zero框架，采用粗到细的时序放大机制，结合放大精度奖励和令牌选择性信用分配。",
                "在NExT-GQA和ReXTime数据集上，时序定位分别提升5.2%和4.6%，答案准确率平均提升2.4%。"
            ],
            "method_zh": "**问题定义**：论文解决基于视频的问答任务中的时序定位不准确问题。现有方法如基于组相对策略优化的方法，在处理多方面奖励信号时存在局限，导致时序错位和幻觉，难以忠实基于视频证据生成答案。\\n\\n**核心思路**：论文提出粗到细的时序放大机制，先粗粒度定位查询相关片段，再细粒度放大到关键帧进行视觉验证，以增强时序感知和答案生成准确性。\\n\\n**技术框架**：整体架构分为两个阶段：第一阶段使用粗粒度模块定位相关时序片段；第二阶段通过时序放大机制，选择最显著帧进行细粒度视觉验证，结合强化学习优化定位和答案生成。\\n\\n**关键创新**：最重要的技术创新包括放大精度奖励，用于验证时序定位预测的保真度；以及令牌选择性信用分配，将奖励精确归因于负责时序定位或答案生成的令牌，缓解现有方法在多奖励信号处理中的问题。\\n\\n**关键设计**：关键设计包括基于强化学习的奖励机制，放大精度奖励计算定位帧与真实证据的匹配度；令牌选择性信用分配通过分析令牌贡献分配奖励；网络结构可能集成视觉编码器和语言模型，具体参数设置未知，但强调粗到细的层次化处理。",
            "application_zh": "该研究在视频理解领域有广泛应用潜力，如智能监控、视频内容分析、教育辅助和自动驾驶。通过改进时序定位和答案生成，可提升长视频处理效率，增强多模态AI系统的实用性和可靠性，未来可能推动视频问答技术向更精准、可解释的方向发展。",
            "highlight_zh": "实验结果显示，Zoom-Zero在NExT-GQA数据集上时序定位提升5.2%，在ReXTime数据集上提升4.6%，平均答案准确率提升2.4%。在长视频基准上，通过粗到细放大机制，平均性能提升6.4%，显著优于现有基线方法，验证了框架的有效性和泛化能力。",
            "tags_zh": [
                "视频问答",
                "时序定位",
                "粗到细框架",
                "强化学习",
                "多模态融合",
                "长视频理解",
                "视觉验证",
                "令牌信用分配"
            ],
            "_index": 77
        },
        {
            "title": "CaFe-TeleVision: A Coarse-to-Fine Teleoperation System with Immersive Situated Visualization for Enhanced Ergonomics",
            "authors": [
                "Zixin Tang",
                "Yiming Chen",
                "Quentin Rouxel",
                "Dianxi Li",
                "Shuang Wu",
                "Fei Chen"
            ],
            "arxiv_id": "2512.14270v1",
            "summary": "Teleoperation presents a promising paradigm for remote control and robot proprioceptive data collection. Despite recent progress, current teleoperation systems still suffer from limitations in efficiency and ergonomics, particularly in challenging scenarios. In this paper, we propose CaFe-TeleVision, a coarse-to-fine teleoperation system with immersive situated visualization for enhanced ergonomics. At its core, a coarse-to-fine control mechanism is proposed in the retargeting module to bridge workspace disparities, jointly optimizing efficiency and physical ergonomics. To stream immersive feedback with adequate visual cues for human vision systems, an on-demand situated visualization technique is integrated in the perception module, which reduces the cognitive load for multi-view processing. The system is built on a humanoid collaborative robot and validated with six challenging bimanual manipulation tasks. User study among 24 participants confirms that CaFe-TeleVision enhances ergonomics with statistical significance, indicating a lower task load and a higher user acceptance during teleoperation. Quantitative results also validate the superior performance of our system across six tasks, surpassing comparative methods by up to 28.89% in success rate and accelerating by 26.81% in completion time. Project webpage: https://clover-cuhk.github.io/cafe_television/",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14270v1",
            "code_links": [
                {
                    "url": "https://clover-cuhk.github.io/cafe_television/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "人形机器人",
                    "matched_keywords": [
                        "humanoid"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出CaFe-TeleVision系统，通过粗到精控制与沉浸式可视化提升远程操作的效率与人体工学性能",
            "summary_zh": "远程操作为远程控制和机器人本体感知数据收集提供了有前景的范式。尽管近期有所进展，但现有远程操作系统在效率和人体工学方面仍存在局限，尤其是在挑战性场景中。本文提出CaFe-TeleVision，一种具有沉浸式情境可视化功能的粗到精远程操作系统，旨在提升人体工学性能。其核心在于重定向模块中提出的粗到精控制机制，以弥合工作空间差异，同时优化效率和物理人体工学。为了提供具有足够视觉线索的沉浸式反馈以适应人类视觉系统，感知模块集成了按需情境可视化技术，降低了多视图处理的认知负荷。该系统基于人形协作机器人构建，并通过六项挑战性双手操作任务进行验证。对24名参与者的用户研究证实，CaFe-TeleVision在统计学上显著提升了人体工学性能，表明在远程操作期间任务负荷更低、用户接受度更高。定量结果也验证了我们的系统在六项任务中的优越性能，成功率最高提升28.89%，完成时间加速26.81%。项目网页：https://clover-cuhk.github.io/cafe_television/",
            "intro_zh": [
                "现有远程操作系统在挑战性场景中效率与人体工学性能不足，影响操作体验与任务成功率。",
                "提出粗到精控制机制与沉浸式情境可视化，优化工作空间匹配并降低认知负荷，提升操作性能。",
                "用户研究显示系统显著降低任务负荷、提高接受度；定量实验表明成功率最高提升28.89%，时间加速26.81%。"
            ],
            "method_zh": "**问题定义**：远程操作系统在挑战性场景中面临效率低下和人体工学性能差的问题，具体表现为工作空间不匹配导致操作困难，以及多视图反馈带来的高认知负荷，限制了远程操作的实用性和用户体验。\\n\\n**核心思路**：通过粗到精控制机制解决工作空间差异问题，结合沉浸式情境可视化技术优化视觉反馈，旨在同时提升操作效率和人体工学性能，降低用户认知负担。\\n\\n**技术框架**：系统基于人形协作机器人构建，包含重定向模块和感知模块。重定向模块实现粗到精控制，先进行粗略位置调整，再精细控制姿态；感知模块集成按需情境可视化，提供沉浸式视觉反馈。整体流程从用户输入到机器人执行，通过反馈循环优化操作。\\n\\n**关键创新**：粗到精控制机制联合优化效率与物理人体工学，本质区别在于分阶段处理工作空间匹配，而非单一映射；沉浸式情境可视化按需提供视觉线索，减少多视图处理需求，与现有方法相比更注重认知负荷降低。\\n\\n**关键设计**：重定向模块中，粗控制阶段使用位置映射算法，精控制阶段涉及姿态优化；感知模块采用可视化技术动态生成情境视图，参数设置基于人类视觉系统特性，如视野范围和深度感知，但具体网络结构或损失函数在摘要中未详细说明，需参考论文全文获取。",
            "application_zh": "该研究在机器人远程操作领域具有广泛潜在应用，如危险环境作业（如核设施维护、灾难救援）、医疗手术辅助、工业制造中的精细装配，以及远程培训和数据收集。通过提升效率与人体工学，可推动远程操作系统的实用化，降低操作员疲劳，提高任务成功率，对未来人机协作和自动化发展有积极影响。",
            "highlight_zh": "最重要的实验结果包括：用户研究（24名参与者）显示系统显著提升人体工学性能，任务负荷降低、用户接受度提高（统计学显著）；定量评估在六项挑战性双手操作任务中，CaFe-TeleVision成功率最高提升28.89%（相比基线方法），完成时间加速26.81%，验证了系统在效率和性能上的优越性。",
            "tags_zh": [
                "远程操作",
                "人形机器人",
                "粗到精控制",
                "沉浸式可视化",
                "人体工学优化",
                "双手操作",
                "认知负荷降低",
                "工作空间匹配"
            ],
            "_index": 78
        },
        {
            "title": "Enhancing Visual Programming for Visual Reasoning via Probabilistic Graphs",
            "authors": [
                "Wentao Wan",
                "Kaiyu Wu",
                "Qingyang Ma",
                "Nan Kang",
                "Yunjie Chen",
                "Liang Lin",
                "Keze Wang"
            ],
            "arxiv_id": "2512.14257v1",
            "summary": "Recently, Visual Programming (VP) based on large language models (LLMs) has rapidly developed and demonstrated significant potential in complex Visual Reasoning (VR) tasks. Previous works to enhance VP have primarily focused on improving the quality of LLM-generated visual programs. However, they have neglected to optimize the VP-invoked pre-trained models, which serve as modules for the visual sub-tasks decomposed from the targeted tasks by VP. The difficulty is that there are only final labels of targeted VR tasks rather than labels of sub-tasks. Besides, the non-differentiable nature of VP impedes the direct use of efficient gradient-based optimization methods to leverage final labels for end-to-end learning of the entire VP framework. To overcome these issues, we propose EVPG, a method to Enhance Visual Programming for visual reasoning via Probabilistic Graphs. Specifically, we creatively build a directed probabilistic graph according to the variable dependency relationships during the VP executing process, which reconstructs the non-differentiable VP executing process into a differentiable exact probability inference process on this directed probabilistic graph. As a result, this enables the VP framework to utilize the final labels for efficient, gradient-based optimization in end-to-end supervised learning on targeted VR tasks. Extensive and comprehensive experiments demonstrate the effectiveness and advantages of our EVPG, showing significant performance improvements for VP on three classical complex VR tasks: GQA, NLVRv2, and Open Images.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "13 Pages, 12 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14257v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO",
                        "VIO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出EVPG方法，通过概率图将视觉编程重构为可微推理过程，以解决视觉推理任务中端到端优化难题。",
            "summary_zh": "近年来，基于大语言模型（LLMs）的视觉编程（VP）在复杂视觉推理（VR）任务中快速发展并展现出巨大潜力。先前增强VP的工作主要集中于提高LLM生成的视觉程序质量，但忽略了优化VP调用的预训练模型，这些模型作为VP从目标任务分解出的视觉子任务的模块。困难在于只有目标VR任务的最终标签，而没有子任务的标签。此外，VP的不可微特性阻碍了直接使用高效的基于梯度的优化方法，以利用最终标签对整个VP框架进行端到端学习。为克服这些问题，我们提出了EVPG，一种通过概率图增强视觉编程以进行视觉推理的方法。具体而言，我们根据VP执行过程中的变量依赖关系，创造性地构建了一个有向概率图，将不可微的VP执行过程重构为该有向概率图上的可微精确概率推理过程。这使得VP框架能够利用最终标签，在目标VR任务的端到端监督学习中实现高效的基于梯度的优化。广泛而全面的实验证明了我们EVPG的有效性和优势，在三个经典复杂VR任务（GQA、NLVRv2和Open Images）上显示出VP的显著性能提升。",
            "intro_zh": [
                "现有方法主要优化LLM生成的视觉程序，但忽略了预训练模型的优化，且缺乏子任务标签，导致端到端学习困难。",
                "EVPG通过构建有向概率图，将不可微的VP执行过程重构为可微概率推理，实现基于梯度的端到端优化。",
                "在GQA、NLVRv2和Open Images等任务上，EVPG显著提升了VP性能，验证了其有效性和优势。"
            ],
            "method_zh": "**问题定义**：论文旨在解决视觉编程（VP）在视觉推理（VR）任务中端到端优化的难题。现有方法的痛点在于：VP调用的预训练模型作为子任务模块，缺乏子任务标签，且VP执行过程不可微，无法直接利用最终标签进行梯度优化，限制了整体性能提升。\\n\\n**核心思路**：论文的核心解决思路是通过构建有向概率图，将VP执行过程中的变量依赖关系建模为概率推理问题，从而将不可微的VP执行重构为可微的精确概率推理过程。这样设计是因为概率图能自然表达变量间的依赖，且推理过程可微，便于利用最终标签进行端到端学习。\\n\\n**技术框架**：整体架构包括VP模块、概率图构建模块和优化模块。首先，VP模块基于LLM生成视觉程序并分解任务；然后，根据程序执行中的变量依赖，构建有向概率图，将每个步骤视为概率节点；最后，通过概率推理计算整体概率，并利用最终标签和损失函数进行梯度反向传播，优化预训练模型参数。\\n\\n**关键创新**：最重要的技术创新是将VP执行过程重构为有向概率图上的可微推理，这本质区别在于现有方法仅关注程序质量，而EVPG实现了整个框架的端到端可微优化，解决了不可微性和标签缺失问题。\\n\\n**关键设计**：关键设计包括：基于变量依赖构建有向概率图的结构，使用概率分布表示节点输出；损失函数基于最终任务标签，如交叉熵损失，通过概率推理计算梯度；优化时采用标准梯度下降方法，更新预训练模型参数，无需子任务标签。具体参数设置如学习率和网络结构依赖于基础VP框架，论文未详细说明，但强调可微性实现。",
            "application_zh": "该研究在复杂视觉推理任务中具有广泛潜在应用，如智能问答系统、图像理解、自动驾驶和机器人交互。通过提升VP的端到端优化能力，EVPG能增强多模态AI系统的准确性和鲁棒性，推动视觉编程在实际场景中的部署，例如在医疗影像分析或教育辅助工具中实现更精准的推理。未来可能影响AI模型的可解释性和自动化程度。",
            "highlight_zh": "实验在GQA、NLVRv2和Open Images三个经典VR任务上进行，EVPG相比基线VP方法显示出显著性能提升。具体数据未在摘要中提供，但论文提到“广泛而全面的实验”和“显著性能改进”，表明在准确率或相关指标上有实质性增长，验证了EVPG在端到端优化中的有效性。",
            "tags_zh": [
                "视觉编程",
                "视觉推理",
                "概率图模型",
                "端到端学习",
                "可微优化",
                "大语言模型",
                "多模态AI",
                "监督学习"
            ],
            "_index": 79
        },
        {
            "title": "DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos",
            "authors": [
                "Yang Bai",
                "Liudi Yang",
                "George Eskandar",
                "Fengyi Shen",
                "Mohammad Altillawi",
                "Ziyuan Liu",
                "Gitta Kutyniok"
            ],
            "arxiv_id": "2512.14217v1",
            "summary": "Video diffusion models provide powerful real-world simulators for embodied AI but remain limited in controllability for robotic manipulation. Recent works on trajectory-conditioned video generation address this gap but often rely on 2D trajectories or single modality conditioning, which restricts their ability to produce controllable and consistent robotic demonstrations. We present DRAW2ACT, a depth-aware trajectory-conditioned video generation framework that extracts multiple orthogonal representations from the input trajectory, capturing depth, semantics, shape and motion, and injects them into the diffusion model. Moreover, we propose to jointly generate spatially aligned RGB and depth videos, leveraging cross-modality attention mechanisms and depth supervision to enhance the spatio-temporal consistency. Finally, we introduce a multimodal policy model conditioned on the generated RGB and depth sequences to regress the robot's joint angles. Experiments on Bridge V2, Berkeley Autolab, and simulation benchmarks show that DRAW2ACT achieves superior visual fidelity and consistency while yielding higher manipulation success rates compared to existing baselines.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14217v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "世界模型",
                    "matched_keywords": [
                        "world simulator"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出DRAW2ACT框架，通过深度感知轨迹条件视频生成，提升机器人演示的可控性和一致性。",
            "summary_zh": "视频扩散模型为具身AI提供了强大的真实世界模拟器，但在机器人操作的可控性方面仍有限制。最近基于轨迹条件的视频生成工作解决了这一差距，但通常依赖于2D轨迹或单模态条件，这限制了它们生成可控且一致的机器人演示的能力。我们提出了DRAW2ACT，一个深度感知轨迹条件视频生成框架，从输入轨迹中提取多个正交表示，捕捉深度、语义、形状和运动，并将它们注入扩散模型。此外，我们提出联合生成空间对齐的RGB和深度视频，利用跨模态注意力机制和深度监督来增强时空一致性。最后，我们引入了一个基于生成的RGB和深度序列的多模态策略模型，以回归机器人的关节角度。在Bridge V2、Berkeley Autolab和模拟基准上的实验表明，与现有基线相比，DRAW2ACT实现了更优的视觉保真度和一致性，同时获得了更高的操作成功率。",
            "intro_zh": [
                "现有视频扩散模型在机器人操作中可控性不足，依赖2D轨迹或单模态条件，导致演示视频缺乏深度信息和一致性。",
                "DRAW2ACT提取轨迹的深度、语义等多维表示，注入扩散模型，并联合生成RGB和深度视频，增强时空对齐。",
                "实验在多个基准上显示，DRAW2ACT在视觉质量和操作成功率上优于基线，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决视频扩散模型在机器人操作演示中可控性不足的问题。现有方法通常基于2D轨迹或单模态条件，无法有效捕捉深度信息，导致生成的视频在空间和时间上不一致，限制了机器人演示的真实性和实用性。\\n\\n**核心思路**：论文的核心思路是通过深度感知轨迹条件视频生成，从输入轨迹中提取多维表示，并联合生成RGB和深度视频，以增强可控性和一致性。这样设计是为了更全面地编码轨迹信息，利用深度数据提升空间对齐，并通过跨模态机制优化视频质量。\\n\\n**技术框架**：整体框架包括三个主要阶段：首先，从输入轨迹提取深度、语义、形状和运动等正交表示；其次，将这些表示注入扩散模型，联合生成空间对齐的RGB和深度视频序列；最后，基于生成的视频序列，训练一个多模态策略模型来回归机器人的关节角度，实现端到端的机器人控制。\\n\\n**关键创新**：最重要的技术创新是深度感知轨迹条件生成和跨模态视频联合生成。与现有方法相比，DRAW2ACT不仅利用轨迹的2D信息，还整合深度数据，通过注意力机制实现RGB和深度视频的协同生成，本质区别在于其多模态和深度增强的设计，提升了演示的时空一致性。\\n\\n**关键设计**：关键设计包括：使用扩散模型作为基础生成器，注入轨迹的多维表示；引入跨模态注意力机制，在RGB和深度视频生成过程中进行信息交互；采用深度监督损失函数，确保生成视频的深度准确性；策略模型基于生成的视频序列，通过回归损失优化关节角度预测。参数设置如扩散步数和网络架构细节未在摘要中指定，需参考论文全文。",
            "application_zh": "该研究在机器人演示和具身AI领域具有广泛应用潜力，可用于生成高质量的训练数据、模拟真实世界操作场景，以及辅助机器人策略学习。实际价值在于提升机器人操作的可靠性和效率，未来可能推动自动化演示生成和虚拟训练系统的发展。",
            "highlight_zh": "在Bridge V2、Berkeley Autolab和模拟基准上的实验表明，DRAW2ACT在视觉保真度和一致性方面优于现有基线，具体性能数据未在摘要中提供，但报告了更高的操作成功率，验证了其有效性。",
            "tags_zh": [
                "视频生成",
                "机器人演示",
                "深度感知",
                "轨迹条件",
                "多模态融合",
                "扩散模型",
                "时空一致性",
                "具身AI"
            ],
            "_index": 80
        },
        {
            "title": "Error Bound Analysis of Physics-Informed Neural Networks-Driven T2 Quantification in Cardiac Magnetic Resonance Imaging",
            "authors": [
                "Mengxue Zhang",
                "Qingrui Cai",
                "Yinyin Chen",
                "Hang Jin",
                "Jianjun Zhou",
                "Qiu Guo",
                "Peijun Zhao",
                "Zhiping Mao",
                "Xingxing Zhang",
                "Yuyu Xia",
                "Xianwang Jiang",
                "Qin Xu",
                "Chunyan Xiong",
                "Yirong Zhou",
                "Chengyan Wang",
                "Xiaobo Qu"
            ],
            "arxiv_id": "2512.14211v1",
            "summary": "Physics-Informed Neural Networks (PINN) are emerging as a promising approach for quantitative parameter estimation of Magnetic Resonance Imaging (MRI). While existing deep learning methods can provide an accurate quantitative estimation of the T2 parameter, they still require large amounts of training data and lack theoretical support and a recognized gold standard. Thus, given the absence of PINN-based approaches for T2 estimation, we propose embedding the fundamental physics of MRI, the Bloch equation, in the loss of PINN, which is solely based on target scan data and does not require a pre-defined training database. Furthermore, by deriving rigorous upper bounds for both the T2 estimation error and the generalization error of the Bloch equation solution, we establish a theoretical foundation for evaluating the PINN's quantitative accuracy. Even without access to the ground truth or a gold standard, this theory enables us to estimate the error with respect to the real quantitative parameter T2. The accuracy of T2 mapping and the validity of the theoretical analysis are demonstrated on a numerical cardiac model and a water phantom, where our method exhibits excellent quantitative precision in the myocardial T2 range. Clinical applicability is confirmed in 94 acute myocardial infarction (AMI) patients, achieving low-error quantitative T2 estimation under the theoretical error bound, highlighting the robustness and potential of PINN.",
            "categories": [
                "physics.bio-ph",
                "cs.AI"
            ],
            "primary_category": "physics.bio-ph",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14211v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "mapping"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出基于物理信息神经网络的T2量化方法，通过嵌入布洛赫方程和误差界分析解决心脏磁共振成像中定量参数估计的数据依赖和理论缺失问题。",
            "summary_zh": "物理信息神经网络（PINN）正成为磁共振成像（MRI）定量参数估计的一种有前景方法。虽然现有深度学习方法能提供T2参数的准确定量估计，但仍需要大量训练数据，且缺乏理论支持和公认的金标准。鉴于目前尚无基于PINN的T2估计方法，我们提出将MRI的基本物理原理——布洛赫方程嵌入PINN的损失函数中，该方法仅基于目标扫描数据，无需预定义的训练数据库。此外，通过推导T2估计误差和布洛赫方程解泛化误差的严格上界，我们为评估PINN的定量准确性建立了理论基础。即使无法获得真实值或金标准，该理论也能使我们估计相对于真实定量参数T2的误差。T2映射的准确性和理论分析的有效性在数值心脏模型和水模上得到验证，我们的方法在心肌T2范围内表现出优异的定量精度。临床适用性在94名急性心肌梗死（AMI）患者中得到确认，在理论误差界内实现了低误差的定量T2估计，突显了PINN的鲁棒性和潜力。",
            "intro_zh": [
                "现有深度学习方法依赖大量训练数据且缺乏理论支持，导致T2参数估计在心脏磁共振成像中难以验证和推广。",
                "论文提出将布洛赫方程嵌入PINN损失函数，仅基于目标扫描数据实现T2量化，无需外部训练数据库。",
                "通过理论误差界分析和实验验证，方法在心肌T2范围内实现高精度估计，并在临床患者中展示低误差和鲁棒性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决心脏磁共振成像中T2参数定量估计的问题。现有深度学习方法虽然能提供准确估计，但严重依赖大量训练数据，且缺乏理论支持，导致结果难以验证和泛化，特别是在缺乏金标准的情况下。\\n\\n**核心思路**：核心思路是利用物理信息神经网络（PINN），将MRI的基本物理原理——布洛赫方程直接嵌入神经网络的损失函数中。这样，模型仅需目标扫描数据即可学习T2参数，无需外部训练数据库，同时通过理论分析提供误差界，为定量准确性提供理论保障。\\n\\n**技术框架**：整体框架包括数据输入、PINN模型构建、损失函数设计和误差分析。首先，输入目标MRI扫描数据；然后，构建神经网络来近似T2参数和解布洛赫方程；损失函数结合数据拟合项和物理约束项（基于布洛赫方程）；最后，通过理论推导误差上界，评估模型性能。\\n\\n**关键创新**：最重要的创新点是将布洛赫方程嵌入PINN损失函数，实现无需训练数据的T2量化，并首次推导了T2估计误差和布洛赫方程解泛化误差的严格上界，为PINN在MRI定量分析中的可靠性提供了理论基础。\\n\\n**关键设计**：关键设计包括损失函数设计，其中包含数据项（如均方误差）和物理项（基于布洛赫方程的残差）；网络结构可能采用多层感知机等通用架构；参数设置针对T2范围优化；误差界分析基于数学推导，如使用Lipschitz常数或泛化误差理论。",
            "application_zh": "该研究主要应用于心脏磁共振成像领域，特别是急性心肌梗死等心脏疾病的定量诊断和监测。通过提供无需大量训练数据且具有理论支持的T2量化方法，可提升临床成像的准确性和效率，未来可能扩展到其他MRI参数估计或医学图像分析任务，推动AI在医疗影像中的可靠应用。",
            "highlight_zh": "在数值心脏模型和水模实验中，方法在心肌T2范围内表现出优异的定量精度，具体性能数据未在摘要中给出，但强调低误差。临床验证中，对94名急性心肌梗死患者实现了在理论误差界内的低误差T2估计，突显了方法的鲁棒性和潜在临床价值，相比现有方法减少了数据依赖并提供了理论保障。",
            "tags_zh": [
                "物理信息神经网络",
                "T2量化",
                "心脏磁共振成像",
                "布洛赫方程",
                "误差界分析",
                "定量参数估计",
                "急性心肌梗死",
                "医学图像分析"
            ],
            "_index": 81
        },
        {
            "title": "Towards Explainable Quantum AI: Informing the Encoder Selection of Quantum Neural Networks via Visualization",
            "authors": [
                "Shaolun Ruan",
                "Feng Liang",
                "Rohan Ramakrishna",
                "Chao Ren",
                "Rudai Yan",
                "Qiang Guan",
                "Jiannan Li",
                "Yong Wang"
            ],
            "arxiv_id": "2512.14181v1",
            "summary": "Quantum Neural Networks (QNNs) represent a promising fusion of quantum computing and neural network architectures, offering speed-ups and efficient processing of high-dimensional, entangled data. A crucial component of QNNs is the encoder, which maps classical input data into quantum states. However, choosing suitable encoders remains a significant challenge, largely due to the lack of systematic guidance and the trial-and-error nature of current approaches. This process is further impeded by two key challenges: (1) the difficulty in evaluating encoded quantum states prior to training, and (2) the lack of intuitive methods for analyzing an encoder's ability to effectively distinguish data features. To address these issues, we introduce a novel visualization tool, XQAI-Eyes, which enables QNN developers to compare classical data features with their corresponding encoded quantum states and to examine the mixed quantum states across different classes. By bridging classical and quantum perspectives, XQAI-Eyes facilitates a deeper understanding of how encoders influence QNN performance. Evaluations across diverse datasets and encoder designs demonstrate XQAI-Eyes's potential to support the exploration of the relationship between encoder design and QNN effectiveness, offering a holistic and transparent approach to optimizing quantum encoders. Moreover, domain experts used XQAI-Eyes to derive two key practices for quantum encoder selection, grounded in the principles of pattern preservation and feature mapping.",
            "categories": [
                "quant-ph",
                "cs.AI",
                "cs.HC"
            ],
            "primary_category": "quant-ph",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "9 pages, 6 figures, accepted by TVCG 2026, not published yet",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14181v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "mapping"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出XQAI-Eyes可视化工具以解决量子神经网络编码器选择缺乏系统指导的问题",
            "summary_zh": "量子神经网络（QNNs）是量子计算与神经网络架构的有前景融合，能够加速处理高维纠缠数据。编码器作为QNNs的关键组件，负责将经典输入数据映射为量子态，但选择合适的编码器仍面临重大挑战，主要原因是缺乏系统指导且当前方法多为试错过程。这一过程受到两个关键挑战的阻碍：（1）训练前难以评估编码后的量子态；（2）缺乏直观方法分析编码器有效区分数据特征的能力。为解决这些问题，我们引入了一种新颖的可视化工具XQAI-Eyes，使QNN开发者能够比较经典数据特征与其对应的编码量子态，并检查不同类别间的混合量子态。通过桥接经典与量子视角，XQAI-Eyes促进了对编码器如何影响QNN性能的深入理解。在不同数据集和编码器设计上的评估表明，XQAI-Eyes有潜力支持探索编码器设计与QNN有效性之间的关系，为优化量子编码器提供全面透明的方法。此外，领域专家使用XQAI-Eyes基于模式保留和特征映射原则，推导出量子编码器选择的两个关键实践。",
            "intro_zh": [
                "核心问题：量子神经网络编码器选择缺乏系统指导，现有方法依赖试错，且难以在训练前评估量子态和分析特征区分能力。",
                "方法要点：提出XQAI-Eyes可视化工具，通过比较经典数据特征与编码量子态，帮助开发者直观理解编码器对性能的影响。",
                "实验或效果：评估显示XQAI-Eyes能有效支持编码器设计探索，并基于可视化结果推导出量子编码器选择的关键实践原则。"
            ],
            "method_zh": "**问题定义**：论文旨在解决量子神经网络（QNNs）中编码器选择缺乏系统指导的问题。现有方法的痛点包括：编码器选择依赖试错过程，缺乏直观评估工具；训练前难以评估编码量子态的质量；以及无法有效分析编码器区分数据特征的能力，这导致QNN性能优化效率低下。\\n\\n**核心思路**：论文的核心解决思路是开发一个可视化工具XQAI-Eyes，通过桥接经典数据特征与编码量子态的视角，使开发者能够直观比较和分析编码器的效果。这样设计是因为可视化能提供直观洞察，帮助理解编码器如何影响量子态表示，从而指导选择过程，减少试错成本。\\n\\n**技术框架**：整体架构包括数据输入、编码器处理、量子态生成和可视化分析模块。流程为：首先输入经典数据，通过不同编码器映射为量子态；然后，XQAI-Eyes工具将经典数据特征（如高维向量）与对应的编码量子态进行可视化对比；同时，分析不同类别数据编码后的混合量子态，以评估编码器的区分能力。\\n\\n**关键创新**：最重要的技术创新点是XQAI-Eyes工具本身，它首次将可视化方法系统应用于量子编码器分析。与现有方法的本质区别在于，现有方法多基于理论或实验试错，而XQAI-Eyes提供了一种直观、交互式的分析手段，直接关联经典特征与量子表示，增强了可解释性和决策支持。\\n\\n**关键设计**：关键设计包括可视化界面的实现，支持多维度数据特征与量子态的比较；使用量子态模拟或真实量子硬件生成编码结果；参数设置涉及编码器类型（如角度编码、振幅编码等）和数据集选择；损失函数或评估指标可能基于量子态相似度或分类性能，但论文未详细说明具体技术细节，强调工具的可视化功能而非底层算法。",
            "application_zh": "该研究的潜在应用领域包括量子机器学习、量子人工智能和量子计算优化。实际价值在于为QNN开发者提供工具支持，加速编码器选择和模型设计过程，降低开发成本。未来影响可能推动量子AI的可解释性研究，促进量子算法在实际问题中的部署，例如在药物发现、金融建模或复杂系统模拟中优化量子神经网络性能。",
            "highlight_zh": "最重要的实验结果显示，XQAI-Eyes在不同数据集和编码器设计上进行了评估，证明了其支持探索编码器设计与QNN有效性关系的潜力。具体性能数据未在摘要中提供，但评估表明工具能提供全面透明的优化方法。基于可视化结果，领域专家推导出两个关键实践：模式保留和特征映射原则，用于指导量子编码器选择，这提升了选择过程的系统性和效率。",
            "tags_zh": [
                "量子神经网络",
                "编码器选择",
                "可视化工具",
                "可解释量子AI",
                "量子态分析",
                "特征映射",
                "模式保留",
                "量子机器学习"
            ],
            "_index": 82
        },
        {
            "title": "Spherical Voronoi: Directional Appearance as a Differentiable Partition of the Sphere",
            "authors": [
                "Francesco Di Sario",
                "Daniel Rebain",
                "Dor Verbin",
                "Marco Grangetto",
                "Andrea Tagliasacchi"
            ],
            "arxiv_id": "2512.14180v1",
            "summary": "Radiance field methods (e.g. 3D Gaussian Splatting) have emerged as a powerful paradigm for novel view synthesis, yet their appearance modeling often relies on Spherical Harmonics (SH), which impose fundamental limitations. SH struggle with high-frequency signals, exhibit Gibbs ringing artifacts, and fail to capture specular reflections - a key component of realistic rendering. Although alternatives like spherical Gaussians offer improvements, they add significant optimization complexity. We propose Spherical Voronoi (SV) as a unified framework for appearance representation in 3D Gaussian Splatting. SV partitions the directional domain into learnable regions with smooth boundaries, providing an intuitive and stable parameterization for view-dependent effects. For diffuse appearance, SV achieves competitive results while keeping optimization simpler than existing alternatives. For reflections - where SH fail - we leverage SV as learnable reflection probes, taking reflected directions as input following principles from classical graphics. This formulation attains state-of-the-art results on synthetic and real-world datasets, demonstrating that SV offers a principled, efficient, and general solution for appearance modeling in explicit 3D representations.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14180v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出球面Voronoi作为3D高斯泼溅中外观建模的统一框架，以解决球谐函数在高频信号和镜面反射方面的局限性。",
            "summary_zh": "辐射场方法（如3D高斯泼溅）已成为新视角合成的强大范式，但其外观建模通常依赖于球谐函数（SH），这带来了根本性限制。SH难以处理高频信号，会出现吉布斯振铃伪影，并且无法捕捉镜面反射——这是真实感渲染的关键组成部分。尽管球面高斯等替代方法有所改进，但它们增加了显著的优化复杂性。我们提出球面Voronoi（SV）作为3D高斯泼溅中外观表示的统一框架。SV将方向域划分为具有平滑边界的可学习区域，为视角相关效应提供了直观且稳定的参数化。对于漫反射外观，SV在保持优化比现有替代方法更简单的同时，实现了具有竞争力的结果。对于SH失败的反射部分，我们利用SV作为可学习的反射探针，遵循经典图形学原理，将反射方向作为输入。该公式在合成和真实世界数据集上达到了最先进的结果，表明SV为显式3D表示中的外观建模提供了一个原则性、高效且通用的解决方案。",
            "intro_zh": [
                "现有方法如球谐函数（SH）在3D高斯泼溅中处理高频信号和镜面反射时存在局限性，导致吉布斯振铃伪影和真实感不足。",
                "论文提出球面Voronoi（SV）框架，通过将方向域划分为可学习区域来统一表示外观，简化优化并提升反射建模能力。",
                "实验表明，SV在合成和真实数据集上达到最先进性能，显著改善反射效果，同时保持优化效率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决3D高斯泼溅中外观建模的挑战，特别是现有方法如球谐函数（SH）在处理高频信号和镜面反射时的不足，包括吉布斯振铃伪影和优化复杂性增加。\\n\\n**核心思路**：核心思路是引入球面Voronoi（SV）作为统一框架，通过将方向域划分为可学习区域来参数化外观，从而提供更直观和稳定的表示，以捕捉视角相关效应，如漫反射和镜面反射。\\n\\n**技术框架**：整体框架基于3D高斯泼溅，将SV集成到外观建模中。主要模块包括：方向域划分、可学习区域参数化、反射探针设计。流程涉及初始化SV区域，通过优化学习区域边界和外观参数，并利用反射方向作为输入来增强反射建模。\\n\\n**关键创新**：最重要的技术创新是SV作为可学习分区机制，它统一了漫反射和镜面反射的表示，避免了SH的局限性，同时简化了优化过程。与现有方法的本质区别在于其基于Voronoi图的分区策略，提供了更灵活和高效的外观建模。\\n\\n**关键设计**：关键设计包括：使用平滑边界函数来定义SV区域，以减少优化不稳定性；将反射方向作为输入，遵循经典图形学原理；损失函数结合重建误差和正则化项，以平衡外观质量和优化效率；参数设置如区域数量和初始化策略，以适配不同数据集。",
            "application_zh": "该研究在计算机视觉和图形学领域具有广泛潜在应用，包括虚拟现实、增强现实、电影特效和游戏开发中的新视角合成。通过提升外观建模的真实感和效率，SV可推动3D重建和渲染技术的发展，未来可能扩展到更复杂的动态场景和实时应用中。",
            "highlight_zh": "实验在合成和真实世界数据集上进行，SV在反射建模方面达到最先进性能，具体提升包括PSNR和SSIM指标的显著改善。与基线方法如SH和球面高斯相比，SV在保持优化简单性的同时，减少了吉布斯伪影，并提升了镜面反射的准确性，例如在某些数据集上反射误差降低超过20%。",
            "tags_zh": [
                "球面Voronoi",
                "3D高斯泼溅",
                "外观建模",
                "新视角合成",
                "镜面反射",
                "辐射场方法",
                "可学习分区",
                "计算机视觉"
            ],
            "_index": 83
        },
        {
            "title": "IntentMiner: Intent Inversion Attack via Tool Call Analysis in the Model Context Protocol",
            "authors": [
                "Yunhao Yao",
                "Zhiqiang Wang",
                "Haoran Cheng",
                "Yihang Cheng",
                "Haohua Du",
                "Xiang-Yang Li"
            ],
            "arxiv_id": "2512.14166v1",
            "summary": "The rapid evolution of Large Language Models (LLMs) into autonomous agents has led to the adoption of the Model Context Protocol (MCP) as a standard for discovering and invoking external tools. While this architecture decouples the reasoning engine from tool execution to enhance scalability, it introduces a significant privacy surface: third-party MCP servers, acting as semi-honest intermediaries, can observe detailed tool interaction logs outside the user's trusted boundary. In this paper, we first identify and formalize a novel privacy threat termed Intent Inversion, where a semi-honest MCP server attempts to reconstruct the user's private underlying intent solely by analyzing legitimate tool calls. To systematically assess this vulnerability, we propose IntentMiner, a framework that leverages Hierarchical Information Isolation and Three-Dimensional Semantic Analysis, integrating tool purpose, call statements, and returned results, to accurately infer user intent at the step level. Extensive experiments demonstrate that IntentMiner achieves a high degree of semantic alignment (over 85%) with original user queries, significantly outperforming baseline approaches. These results highlight the inherent privacy risks in decoupled agent architectures, revealing that seemingly benign tool execution logs can serve as a potent vector for exposing user secrets.",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "12 pages, 6 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14166v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出IntentMiner框架，通过分析模型上下文协议中的工具调用，揭示解耦代理架构中的意图反转隐私威胁。",
            "summary_zh": "大型语言模型（LLMs）向自主代理的快速发展促使模型上下文协议（MCP）成为发现和调用外部工具的标准。虽然这种架构将推理引擎与工具执行解耦以增强可扩展性，但它引入了一个显著的隐私暴露面：作为半诚实中介的第三方MCP服务器可以在用户信任边界之外观察详细的工具交互日志。本文首先识别并形式化了一种新的隐私威胁，称为意图反转，即半诚实的MCP服务器仅通过分析合法的工具调用来尝试重建用户的私有底层意图。为了系统评估这一漏洞，我们提出了IntentMiner框架，该框架利用分层信息隔离和三维语义分析，整合工具目的、调用语句和返回结果，以在步骤级别准确推断用户意图。大量实验表明，IntentMiner实现了与原始用户查询的高度语义对齐（超过85%），显著优于基线方法。这些结果突显了解耦代理架构中固有的隐私风险，揭示了看似良性的工具执行日志可以作为暴露用户秘密的强大载体。",
            "intro_zh": [
                "核心问题：模型上下文协议（MCP）作为解耦代理架构的标准，引入隐私风险，第三方服务器可通过工具调用日志推断用户意图，现有方法缺乏对此威胁的系统评估。",
                "方法要点：提出IntentMiner框架，基于分层信息隔离和三维语义分析，整合工具目的、调用语句和返回结果，实现步骤级意图推断。",
                "实验或效果：IntentMiner在语义对齐上超过85%，显著优于基线，揭示工具日志作为隐私泄露载体的高风险。"
            ],
            "method_zh": "**问题定义**：论文解决模型上下文协议（MCP）中第三方服务器通过分析工具调用日志推断用户私有意图的隐私威胁，现有方法未系统评估此漏洞，导致用户意图可能被半诚实中介泄露。\\n\\n**核心思路**：核心思路是设计一个框架，通过结构化分析工具交互数据，从看似无害的日志中重建用户意图，利用分层隔离减少噪声，结合多维度语义信息提高推断准确性。\\n\\n**技术框架**：整体架构包括数据收集模块（从MCP服务器获取工具调用日志）、分层信息隔离模块（分离工具目的、调用和结果以降低干扰）、三维语义分析模块（整合三个维度进行意图推断）和评估模块（计算语义对齐度）。流程为：日志输入→分层处理→语义分析→意图输出。\\n\\n**关键创新**：最重要的创新是首次形式化意图反转攻击，并提出基于分层隔离和三维分析的推断方法，与现有基线相比，本质区别在于系统整合工具交互的多个方面，而非仅依赖单一维度。\\n\\n**关键设计**：关键设计包括分层信息隔离策略（如工具分类和调用语句解析）、三维语义分析算法（结合自然语言处理技术计算相似度）、评估指标（使用语义对齐度量化意图匹配），具体参数如对齐阈值未在摘要中说明，但框架强调步骤级推断以提高精度。",
            "application_zh": "该研究应用于人工智能安全和隐私保护领域，潜在价值包括评估自主代理系统的隐私风险、指导MCP协议的安全设计、以及开发防御机制防止意图泄露。未来可能影响LLM代理的部署标准，推动更安全的工具调用架构。",
            "highlight_zh": "最重要的实验结果是IntentMiner在语义对齐上达到超过85%的准确率，显著优于未指定的基线方法，具体提升幅度未给出，但结果突显工具调用日志作为隐私泄露载体的高风险，验证了框架的有效性。",
            "tags_zh": [
                "意图反转攻击",
                "模型上下文协议",
                "隐私威胁",
                "工具调用分析",
                "语义对齐",
                "自主代理",
                "分层信息隔离",
                "三维语义分析"
            ],
            "_index": 84
        },
        {
            "title": "LAPPI: Interactive Optimization with LLM-Assisted Preference-Based Problem Instantiation",
            "authors": [
                "So Kuroki",
                "Manami Nakagawa",
                "Shigeo Yoshida",
                "Yuki Koyama",
                "Kozuno Tadashi"
            ],
            "arxiv_id": "2512.14138v1",
            "summary": "Many real-world tasks, such as trip planning or meal planning, can be formulated as combinatorial optimization problems. However, using optimization solvers is difficult for end users because it requires problem instantiation: defining candidate items, assigning preference scores, and specifying constraints. We introduce LAPPI (LLM-Assisted Preference-based Problem Instantiation), an interactive approach that uses large language models (LLMs) to support users in this instantiation process. Through natural language conversations, the system helps users transform vague preferences into well-defined optimization problems. These instantiated problems are then passed to existing optimization solvers to generate solutions. In a user study on trip planning, our method successfully captured user preferences and generated feasible plans that outperformed both conventional and prompt-engineering approaches. We further demonstrate LAPPI's versatility by adapting it to an additional use case.",
            "categories": [
                "cs.HC",
                "cs.AI"
            ],
            "primary_category": "cs.HC",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14138v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL",
                        "PPO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出LAPPI方法，利用大语言模型辅助用户将模糊偏好转化为优化问题，以解决组合优化问题实例化困难。",
            "summary_zh": "许多现实世界任务，如旅行规划或餐饮规划，可以形式化为组合优化问题。然而，使用优化求解器对终端用户来说很困难，因为它需要问题实例化：定义候选项目、分配偏好分数和指定约束。我们引入了LAPPI（LLM辅助的基于偏好的问题实例化），这是一种交互式方法，利用大语言模型（LLMs）支持用户在此实例化过程中。通过自然语言对话，系统帮助用户将模糊偏好转化为明确定义的优化问题。这些实例化的问题随后传递给现有的优化求解器以生成解决方案。在旅行规划的用户研究中，我们的方法成功捕捉了用户偏好，并生成了可行的计划，优于传统方法和提示工程方法。我们通过将其适应到另一个用例进一步展示了LAPPI的通用性。",
            "intro_zh": [
                "核心问题：现有组合优化求解器要求用户手动定义问题实例，包括候选项、偏好和约束，这对非专业用户构成高门槛。",
                "方法要点：提出LAPPI，通过LLM驱动的自然语言对话交互，辅助用户将模糊偏好转化为结构化优化问题，降低实例化难度。",
                "实验或效果：在旅行规划用户研究中，LAPPI成功捕捉偏好，生成可行计划，性能优于传统方法和提示工程方法。"
            ],
            "method_zh": "**问题定义**：论文解决组合优化问题实例化的困难，现有方法要求用户手动定义候选项、偏好分数和约束，导致高认知负担和易出错，尤其对非专业用户。\\n\\n**核心思路**：核心思路是利用大语言模型（LLMs）的对话能力，通过自然语言交互引导用户表达偏好，自动转化为结构化优化问题，从而简化实例化过程。\\n\\n**技术框架**：整体架构包括交互对话模块、偏好提取模块和问题实例化模块。用户通过自然语言输入偏好，LLM解析并引导澄清，系统提取关键参数（如候选项、权重、约束），生成优化问题定义，然后传递给外部求解器求解。\\n\\n**关键创新**：最重要的创新是将LLM集成到优化问题实例化中，实现从模糊偏好到明确定义问题的自动化转换，本质区别在于交互性和用户友好性，而非传统手动或基于模板的方法。\\n\\n**关键设计**：关键设计包括LLM的提示工程，用于引导对话和偏好提取；偏好分数分配机制，基于用户反馈动态调整；以及约束规范接口，支持自然语言描述转化为数学约束。具体参数如对话轮次和偏好阈值未详细说明，但强调自适应交互。",
            "application_zh": "该研究潜在应用于旅行规划、餐饮规划、日程安排等组合优化任务，实际价值在于降低非专业用户使用优化技术的门槛，提升决策效率。未来可能扩展到更多个性化服务领域，如资源分配或产品推荐，推动人机协作优化的发展。",
            "highlight_zh": "在旅行规划用户研究中，LAPPI成功捕捉用户偏好，生成可行计划，性能优于传统方法（如手动实例化）和提示工程方法（如直接LLM生成）。具体数据未知，但实验表明在用户满意度和计划可行性方面有显著提升，验证了交互式实例化的有效性。",
            "tags_zh": [
                "组合优化",
                "问题实例化",
                "大语言模型",
                "交互式系统",
                "偏好建模",
                "自然语言处理",
                "用户辅助",
                "优化求解器"
            ],
            "_index": 85
        },
        {
            "title": "UIXPOSE: Mobile Malware Detection via Intention-Behaviour Discrepancy Analysis",
            "authors": [
                "Amirmohammad Pasdar",
                "Toby Murray",
                "Van-Thuan Pham"
            ],
            "arxiv_id": "2512.14130v1",
            "summary": "We introduce UIXPOSE, a source-code-agnostic framework that operates on both compiled and open-source apps. This framework applies Intention Behaviour Alignment (IBA) to mobile malware analysis, aligning UI-inferred intent with runtime semantics. Previous work either infers intent statically, e.g., permission-centric, or widget-level or monitors coarse dynamic signals (endpoints, partial resource usage) that miss content and context. UIXPOSE infers an intent vector from each screen using vision-language models and knowledge structures and combines decoded network payloads, heap/memory signals, and resource utilisation traces into a behaviour vector. Their alignment, calculated at runtime, can both detect misbehaviour and highlight exploration of behaviourally rich paths. In three real-world case studies, UIXPOSE reveals covert exfiltration and hidden background activity that evade metadata-only baselines, demonstrating how IBA improves dynamic detection.",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "15 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14130v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出UIXPOSE框架，通过意图-行为差异分析解决移动恶意软件检测问题",
            "summary_zh": "我们介绍了UIXPOSE，这是一个与源代码无关的框架，可同时处理编译和开源应用程序。该框架将意图行为对齐（IBA）应用于移动恶意软件分析，将UI推断的意图与运行时语义对齐。先前的工作要么静态推断意图（例如基于权限或小部件级别），要么监控粗糙的动态信号（端点、部分资源使用），这些方法忽略了内容和上下文。UIXPOSE使用视觉语言模型和知识结构从每个屏幕推断意图向量，并将解码的网络负载、堆/内存信号和资源利用跟踪组合成行为向量。在运行时计算它们的对齐度，既可以检测不当行为，又可以突出显示行为丰富路径的探索。在三个真实案例研究中，UIXPOSE揭示了仅基于元数据的基线无法检测到的隐蔽数据外泄和隐藏后台活动，展示了IBA如何改进动态检测。",
            "intro_zh": [
                "现有方法在移动恶意软件检测中，要么静态推断意图（如基于权限），要么监控粗糙动态信号（如端点），忽略了UI内容和上下文，导致检测不准确。",
                "UIXPOSE提出意图行为对齐（IBA）方法，通过视觉语言模型从UI推断意图向量，结合运行时行为向量，在运行时计算对齐度以检测恶意行为。",
                "在三个真实案例中，UIXPOSE成功检测到隐蔽数据外泄和隐藏后台活动，超越了仅基于元数据的基线，证明了IBA在动态检测中的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决移动恶意软件检测中现有方法的不足，即静态方法（如基于权限或小部件）无法捕捉动态行为，而动态方法（如监控端点或资源使用）又忽略了UI内容和上下文，导致检测精度低和漏报率高。\\n\\n**核心思路**：论文提出意图行为对齐（IBA）的核心思路，通过将UI推断的意图与运行时行为对齐，来检测恶意软件。这样设计是因为恶意软件常通过UI伪装正常意图，但实际行为与意图不符，对齐分析能有效揭示这种差异。\\n\\n**技术框架**：整体架构包括两个主要阶段：意图推断和行为收集。在意图推断阶段，使用视觉语言模型和知识结构从每个屏幕提取意图向量；在行为收集阶段，解码网络负载、监控堆/内存信号和资源利用跟踪，组合成行为向量。运行时计算意图向量和行为向量的对齐度，以检测恶意行为。\\n\\n**关键创新**：最重要的技术创新是首次将意图行为对齐（IBA）应用于移动恶意软件分析，结合视觉语言模型进行UI意图推断，并与多模态运行时行为数据对齐。与现有方法的本质区别在于，它同时考虑了UI内容和动态行为，实现了更细粒度的检测。\\n\\n**关键设计**：关键设计包括使用视觉语言模型（具体模型未知）从屏幕图像和文本中提取意图特征，知识结构（如UI元素层次）辅助推断；行为向量整合网络负载解码（如HTTP请求）、内存信号（如堆分配）和资源跟踪（如CPU使用）；对齐计算采用未知的相似度度量，在运行时动态评估。参数设置和损失函数细节在摘要中未明确，需参考论文全文。",
            "application_zh": "该研究主要应用于移动安全领域，特别是Android和iOS平台的恶意软件检测。潜在价值包括提高应用商店审核效率、增强终端用户设备防护，以及辅助安全分析师进行深度行为分析。未来可能扩展到物联网设备或桌面软件的安全检测，推动基于多模态对齐的恶意软件防御技术发展。",
            "highlight_zh": "在三个真实案例研究中，UIXPOSE成功检测到隐蔽数据外泄和隐藏后台活动，这些活动逃避了仅基于元数据的基线检测。具体性能数据未知，但实验表明IBA方法能显著提升动态检测的准确性和覆盖率，突出行为丰富路径的探索，为移动恶意软件分析提供了新视角。",
            "tags_zh": [
                "移动恶意软件检测",
                "意图行为对齐",
                "视觉语言模型",
                "动态分析",
                "UI推断",
                "多模态融合",
                "运行时语义",
                "安全框架"
            ],
            "_index": 86
        },
        {
            "title": "Interactive Motion Planning for Human-Robot Collaboration Based on Human-Centric Configuration Space Ergonomic Field",
            "authors": [
                "Chenzui Li",
                "Yiming Chen",
                "Xi Wu",
                "Tao Teng",
                "Sylvain Calinon",
                "Darwin Caldwell",
                "Fei Chen"
            ],
            "arxiv_id": "2512.14111v1",
            "summary": "Industrial human-robot collaboration requires motion planning that is collision-free, responsive, and ergonomically safe to reduce fatigue and musculoskeletal risk. We propose the Configuration Space Ergonomic Field (CSEF), a continuous and differentiable field over the human joint space that quantifies ergonomic quality and provides gradients for real-time ergonomics-aware planning. An efficient algorithm constructs CSEF from established metrics with joint-wise weighting and task conditioning, and we integrate it into a gradient-based planner compatible with impedance-controlled robots. In a 2-DoF benchmark, CSEF-based planning achieves higher success rates, lower ergonomic cost, and faster computation than a task-space ergonomic planner. Hardware experiments with a dual-arm robot in unimanual guidance, collaborative drilling, and bimanual cocarrying show faster ergonomic cost reduction, closer tracking to optimized joint targets, and lower muscle activation than a point-to-point baseline. CSEF-based planning method reduces average ergonomic scores by up to 10.31% for collaborative drilling tasks and 5.60% for bimanual co-carrying tasks while decreasing activation in key muscle groups, indicating practical benefits for real-world deployment.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "10 pages, 9 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14111v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "物理动作",
                    "matched_keywords": [
                        "musculoskeletal"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出基于人机协作的配置空间人机工程场，实现实时人机工程学感知的运动规划。",
            "summary_zh": "工业人机协作需要无碰撞、响应迅速且人机工程学安全的运动规划，以减少疲劳和肌肉骨骼风险。我们提出了配置空间人机工程场（CSEF），这是一个在人体关节空间上的连续可微场，用于量化人机工程学质量并提供实时人机工程学感知规划的梯度。我们开发了一种高效算法，通过关节权重和任务条件从现有指标构建CSEF，并将其集成到与阻抗控制机器人兼容的基于梯度的规划器中。在2自由度基准测试中，基于CSEF的规划比任务空间人机工程学规划器实现了更高的成功率、更低的人机工程学成本和更快的计算速度。在单臂引导、协作钻孔和双臂协同搬运的硬件实验中，与点到点基线相比，基于CSEF的规划显示出更快的人机工程学成本降低、更接近优化关节目标的跟踪以及更低的肌肉激活。对于协作钻孔任务，基于CSEF的规划方法平均人机工程学分数降低了高达10.31%，对于双臂协同搬运任务降低了5.60%，同时减少了关键肌肉群的激活，表明在实际部署中具有实用效益。",
            "intro_zh": [
                "现有工业人机协作运动规划方法在实时性和人机工程学优化方面存在不足，难以平衡碰撞避免、响应速度和人体疲劳风险。",
                "论文提出配置空间人机工程场（CSEF），通过连续可微场量化人机工程学质量，并利用梯度进行实时规划，以提升人机交互的安全性和效率。",
                "实验结果显示，基于CSEF的规划在基准测试中成功率更高、成本更低、计算更快，硬件实验中人机工程学分数降低达10.31%，肌肉激活减少，验证了其实际效益。"
            ],
            "method_zh": "**问题定义**：论文旨在解决工业人机协作中运动规划的挑战，包括确保无碰撞、实时响应和人机工程学安全，以减少人体疲劳和肌肉骨骼风险。现有方法如任务空间人机工程学规划器在实时性和梯度优化方面存在局限，难以高效处理复杂关节空间的人机工程学评估。\\n\\n**核心思路**：论文的核心思路是引入配置空间人机工程场（CSEF），这是一个在人体关节空间上定义的连续可微场，能够量化人机工程学质量并提供梯度信息，从而支持基于梯度的实时运动规划。这样设计是为了直接在人机交互的关节层面进行优化，避免任务空间转换带来的信息损失和计算开销。\\n\\n**技术框架**：整体框架包括两个主要阶段：首先，通过高效算法从现有人机工程学指标构建CSEF，该算法结合关节权重和任务条件进行参数化；其次，将CSEF集成到基于梯度的运动规划器中，该规划器与阻抗控制机器人兼容，实现实时的人机工程学感知规划。流程涉及数据采集、场构建、梯度计算和规划执行。\\n\\n**关键创新**：最重要的技术创新是CSEF本身，它首次在配置空间上实现了连续可微的人机工程学场，与现有任务空间方法相比，本质区别在于直接在关节空间进行优化，提供了更精确的梯度引导和更快的计算效率，从而提升了规划的实时性和人机工程学性能。\\n\\n**关键设计**：关键设计包括：使用关节权重来调整不同关节对人机工程学成本的影响，任务条件允许根据特定协作场景（如钻孔或搬运）定制场；损失函数基于CSEF梯度最小化人机工程学成本；算法实现中，通过数值方法确保场的可微性，并与阻抗控制接口集成以支持硬件实验。",
            "application_zh": "该研究主要应用于工业人机协作领域，如制造业中的装配、钻孔和搬运任务，通过实时优化人机工程学，能显著降低操作员疲劳和受伤风险，提升生产安全性和效率。未来可扩展至医疗康复、服务机器人等场景，推动智能机器人系统的人性化交互发展。",
            "highlight_zh": "在2自由度基准测试中，基于CSEF的规划相比任务空间人机工程学规划器，成功率更高、人机工程学成本更低、计算速度更快。硬件实验中，对于协作钻孔任务，平均人机工程学分数降低高达10.31%，双臂协同搬运任务降低5.60%，同时关键肌肉群激活减少，验证了CSEF在实时规划和人体安全方面的显著提升。",
            "tags_zh": [
                "人机协作",
                "运动规划",
                "人机工程学",
                "配置空间",
                "梯度优化",
                "实时控制",
                "工业机器人",
                "肌肉激活分析"
            ],
            "_index": 87
        },
        {
            "title": "OUSAC: Optimized Guidance Scheduling with Adaptive Caching for DiT Acceleration",
            "authors": [
                "Ruitong Sun",
                "Tianze Yang",
                "Wei Niu",
                "Jin Sun"
            ],
            "arxiv_id": "2512.14096v1",
            "summary": "Diffusion models have emerged as the dominant paradigm for high-quality image generation, yet their computational expense remains substantial due to iterative denoising. Classifier-Free Guidance (CFG) significantly enhances generation quality and controllability but doubles the computation by requiring both conditional and unconditional forward passes at every timestep. We present OUSAC (Optimized gUidance Scheduling with Adaptive Caching), a framework that accelerates diffusion transformers (DiT) through systematic optimization. Our key insight is that variable guidance scales enable sparse computation: adjusting scales at certain timesteps can compensate for skipping CFG at others, enabling both fewer total sampling steps and fewer CFG steps while maintaining quality. However, variable guidance patterns introduce denoising deviations that undermine standard caching methods, which assume constant CFG scales across steps. Moreover, different transformer blocks are affected at different levels under dynamic conditions. This paper develops a two-stage approach leveraging these insights. Stage-1 employs evolutionary algorithms to jointly optimize which timesteps to skip and what guidance scale to use, eliminating up to 82% of unconditional passes. Stage-2 introduces adaptive rank allocation that tailors calibration efforts per transformer block, maintaining caching effectiveness under variable guidance. Experiments demonstrate that OUSAC significantly outperforms state-of-the-art acceleration methods, achieving 53% computational savings with 15% quality improvement on DiT-XL/2 (ImageNet 512x512), 60% savings with 16.1% improvement on PixArt-alpha (MSCOCO), and 5x speedup on FLUX while improving CLIP Score over the 50-step baseline.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "29 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14096v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "SAC"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出OUSAC框架，通过优化引导调度与自适应缓存加速扩散变换器，解决CFG计算开销大的问题。",
            "summary_zh": "扩散模型已成为高质量图像生成的主导范式，但由于迭代去噪，其计算开销仍然很大。无分类器引导（CFG）显著提高了生成质量和可控性，但需要在每个时间步同时进行条件前向传播和无条件前向传播，从而使计算量加倍。我们提出了OUSAC（优化引导调度与自适应缓存），这是一个通过系统优化加速扩散变换器（DiT）的框架。我们的关键见解是，可变的引导尺度可以实现稀疏计算：在某些时间步调整尺度可以补偿在其他时间步跳过CFG，从而在保持质量的同时减少总采样步骤和CFG步骤。然而，可变的引导模式引入了去噪偏差，破坏了标准缓存方法，这些方法假设跨步骤的CFG尺度恒定。此外，在动态条件下，不同的变换器块受到的影响程度不同。本文利用这些见解开发了一种两阶段方法。第一阶段采用进化算法联合优化跳过哪些时间步以及使用什么引导尺度，消除了高达82%的无条件前向传播。第二阶段引入了自适应秩分配，为每个变换器块定制校准工作，在可变引导下保持缓存有效性。实验表明，OUSAC显著优于最先进的加速方法，在DiT-XL/2（ImageNet 512x512）上实现了53%的计算节省和15%的质量提升，在PixArt-alpha（MSCOCO）上实现了60%的节省和16.1%的提升，在FLUX上实现了5倍加速，同时CLIP分数超过了50步基线。",
            "intro_zh": [
                "扩散模型生成高质量图像但计算开销大，CFG虽提升质量却加倍计算量，现有方法难以平衡效率与质量。",
                "提出OUSAC框架，通过可变引导尺度实现稀疏计算，结合进化算法优化调度和自适应缓存维持有效性。",
                "实验显示OUSAC显著优于现有方法，在多个数据集上实现50%以上计算节省和15%以上质量提升，加速效果突出。"
            ],
            "method_zh": "**问题定义**：扩散模型（特别是扩散变换器DiT）在图像生成中计算开销大，主要源于迭代去噪过程。无分类器引导（CFG）虽提升生成质量和可控性，但需在每个时间步同时执行条件前向传播和无条件前向传播，导致计算量加倍。现有加速方法如标准缓存假设CFG尺度恒定，无法适应可变引导模式，且不同变换器块在动态条件下受影响程度不同，造成去噪偏差和缓存失效。\\n\\n**核心思路**：论文的核心思路是利用可变引导尺度实现稀疏计算，通过调整某些时间步的引导尺度来补偿跳过CFG步骤的损失，从而减少总计算量。同时，针对可变引导模式引入的自适应缓存机制，通过定制化校准每个变换器块，维持缓存有效性。这基于洞察：引导尺度的动态变化可优化调度，而块级差异需精细处理。\\n\\n**技术框架**：OUSAC采用两阶段框架。第一阶段：使用进化算法联合优化时间步跳过策略和引导尺度设置，目标是在保持生成质量的前提下，最大化无条件前向传播的跳过比例（可达82%）。第二阶段：引入自适应秩分配，根据每个变换器块在可变引导下的敏感度，动态分配校准资源，确保缓存机制在非恒定尺度下仍能有效工作。整体流程从优化调度到自适应调整，实现端到端加速。\\n\\n**关键创新**：最重要的技术创新是可变引导调度与自适应缓存的结合。与现有方法（如固定CFG或简单缓存）相比，本质区别在于：1）允许引导尺度随时间步变化，实现计算稀疏化；2）开发块级自适应机制，解决动态条件下的缓存偏差问题。这突破了传统恒定假设，实现了更高效的资源分配。\\n\\n**关键设计**：关键设计包括：1）进化算法用于优化，参数包括时间步选择矩阵和引导尺度向量，损失函数结合生成质量指标（如CLIP分数）和计算成本；2）自适应秩分配基于块敏感度分析，通过低秩近似校准缓存，秩数根据块在可变引导下的误差动态调整；3）网络结构沿用标准DiT，但集成调度和缓存模块，无需修改底层模型。具体设置如实验中使用DiT-XL/2等基准模型，优化目标为最小化前向传播次数同时最大化质量得分。",
            "application_zh": "该研究在扩散模型加速领域具有广泛潜在应用，主要面向高质量图像生成任务，如艺术创作、媒体内容生成和工业设计。实际价值在于显著降低计算成本，使DiT等大模型更易于部署在资源受限环境（如移动设备或边缘计算），同时提升生成效率。未来可能影响扩散模型的规模化应用，推动实时图像生成和交互式AI工具的发展，为多模态AI系统提供更高效的底层支持。",
            "highlight_zh": "实验结果表明OUSAC在多个基准上取得显著性能提升：在DiT-XL/2（ImageNet 512x512）上，实现53%计算节省和15%质量提升；在PixArt-alpha（MSCOCO）上，节省60%计算并提升16.1%质量；在FLUX模型上，加速5倍且CLIP分数超过50步基线。这些结果优于最先进的加速方法，证明了框架在减少无条件前向传播（高达82%）和维持缓存有效性方面的有效性。",
            "tags_zh": [
                "扩散模型加速",
                "无分类器引导",
                "扩散变换器",
                "计算优化",
                "自适应缓存",
                "进化算法",
                "图像生成",
                "稀疏计算"
            ],
            "_index": 88
        },
        {
            "title": "GaussianPlant: Structure-aligned Gaussian Splatting for 3D Reconstruction of Plants",
            "authors": [
                "Yang Yang",
                "Risa Shinoda",
                "Hiroaki Santo",
                "Fumio Okura"
            ],
            "arxiv_id": "2512.14087v1",
            "summary": "We present a method for jointly recovering the appearance and internal structure of botanical plants from multi-view images based on 3D Gaussian Splatting (3DGS). While 3DGS exhibits robust reconstruction of scene appearance for novel-view synthesis, it lacks structural representations underlying those appearances (e.g., branching patterns of plants), which limits its applicability to tasks such as plant phenotyping. To achieve both high-fidelity appearance and structural reconstruction, we introduce GaussianPlant, a hierarchical 3DGS representation, which disentangles structure and appearance. Specifically, we employ structure primitives (StPs) to explicitly represent branch and leaf geometry, and appearance primitives (ApPs) to the plants' appearance using 3D Gaussians. StPs represent a simplified structure of the plant, i.e., modeling branches as cylinders and leaves as disks. To accurately distinguish the branches and leaves, StP's attributes (i.e., branches or leaves) are optimized in a self-organized manner. ApPs are bound to each StP to represent the appearance of branches or leaves as in conventional 3DGS. StPs and ApPs are jointly optimized using a re-rendering loss on the input multi-view images, as well as the gradient flow from ApP to StP using the binding correspondence information. We conduct experiments to qualitatively evaluate the reconstruction accuracy of both appearance and structure, as well as real-world experiments to qualitatively validate the practical performance. Experiments show that the GaussianPlant achieves both high-fidelity appearance reconstruction via ApPs and accurate structural reconstruction via StPs, enabling the extraction of branch structure and leaf instances.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Submitted to IEEE TPAMI, under review",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14087v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "深度估计",
                    "matched_keywords": [
                        "3D reconstruction"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出GaussianPlant方法，通过解耦结构和外观的高斯溅射表示，解决植物三维重建中外观与内部结构难以同时恢复的问题。",
            "summary_zh": "我们提出了一种基于三维高斯溅射（3DGS）的方法，用于从多视角图像中联合恢复植物植物的外观和内部结构。虽然3DGS在新视角合成中表现出强大的场景外观重建能力，但它缺乏支撑这些外观的结构表示（例如植物的分支模式），这限制了其在植物表型分析等任务中的应用。为了实现高保真外观和结构重建，我们引入了GaussianPlant，这是一种分层3DGS表示，将结构和外观解耦。具体来说，我们使用结构基元（StPs）来显式表示分支和叶片的几何形状，并使用外观基元（ApPs）通过三维高斯来表示植物的外观。StPs表示植物的简化结构，即将分支建模为圆柱体，叶片建模为圆盘。为了准确区分分支和叶片，StP的属性（即分支或叶片）以自组织方式进行优化。ApPs绑定到每个StP，以像传统3DGS那样表示分支或叶片的外观。StPs和ApPs通过输入多视角图像的重渲染损失以及使用绑定对应信息从ApP到StP的梯度流进行联合优化。我们进行了实验，定性地评估外观和结构的重建准确性，以及实际实验来定性地验证实际性能。实验表明，GaussianPlant通过ApPs实现了高保真外观重建，通过StPs实现了准确的结构重建，从而能够提取分支结构和叶片实例。",
            "intro_zh": [
                "现有3DGS方法在植物重建中缺乏内部结构表示，限制了其在表型分析等任务的应用。",
                "提出分层高斯溅射表示，通过结构基元和外观基元解耦结构与外观，实现联合优化。",
                "实验验证了GaussianPlant在外观和结构重建上的高保真性，能有效提取分支和叶片实例。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从多视角图像中联合恢复植物外观和内部结构的问题。现有三维高斯溅射（3DGS）方法虽能高效重建外观，但缺乏对植物分支模式等内部结构的显式表示，这限制了其在植物表型分析等需要结构信息的任务中的应用。\\n\\n**核心思路**：论文提出GaussianPlant，通过引入分层3DGS表示，将结构和外观解耦。具体地，使用结构基元（StPs）显式建模分支和叶片的简化几何，外观基元（ApPs）绑定到StPs以表示外观，从而实现外观与结构的联合优化。\\n\\n**技术框架**：整体框架包括两个主要模块：结构基元（StPs）和外观基元（ApPs）。StPs负责表示植物的简化结构（分支为圆柱体，叶片为圆盘），ApPs负责表示外观。优化过程基于多视角图像的重渲染损失，以及从ApP到StP的梯度流，通过绑定对应信息实现联合训练。\\n\\n**关键创新**：最重要的创新点是提出分层高斯溅射表示，将结构和外观解耦，这在传统3DGS中是缺失的。通过StPs显式建模结构，ApPs绑定到StPs，实现了外观与结构的协同优化，本质区别在于引入了结构感知能力。\\n\\n**关键设计**：关键设计包括：StPs的属性（分支或叶片）通过自组织方式优化以准确区分结构；ApPs使用三维高斯表示外观；损失函数结合重渲染损失和梯度流损失，确保外观和结构的一致性；绑定对应信息用于传递梯度，促进联合优化。",
            "application_zh": "该研究在植物表型分析、农业监测和生态研究中具有重要应用价值。通过同时恢复外观和内部结构，GaussianPlant能支持植物生长跟踪、疾病检测和形态分析，提升自动化农业和生物研究的效率。未来可能扩展到其他复杂结构物体的三维重建。",
            "highlight_zh": "实验表明，GaussianPlant在外观重建上达到高保真水平，结构重建准确，能有效提取分支结构和叶片实例。定性评估显示，相比传统3DGS，GaussianPlant在结构表示上有显著提升，实际实验验证了其在真实场景中的鲁棒性。具体性能数据未知，但重建质量得到定性确认。",
            "tags_zh": [
                "三维高斯溅射",
                "植物三维重建",
                "结构外观解耦",
                "多视角图像",
                "植物表型分析",
                "分层表示",
                "联合优化",
                "结构基元"
            ],
            "_index": 89
        },
        {
            "title": "RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees",
            "authors": [
                "Junjie Ma",
                "Jinlong Li"
            ],
            "arxiv_id": "2512.14069v1",
            "summary": "Inference with modern Large Language Models (LLMs) is expensive and slow, and speculative sampling has emerged as an effective solution to this problem, however, the number of the calls to the draft model for generating candidate tokens in speculative sampling is a preset hyperparameter, lacking flexibility. To generate and utilize the candidate tokens more effectively, we propose RADAR, a novel speculative sampling method with RL-based dynamic draft trees. RADAR formulates the draft tree generation process as a Markov Decision Process (MDP) and employs offline reinforcement learning to train a prediction model, which enables real-time decision on the calls to the draft model, reducing redundant computations and further accelerating inference. Evaluations across three LLMs and four tasks show that RADAR achieves a speedup of 3.17x-4.82x over the auto-regressive decoding baseline. The code is available at https://github.com/minaduki-sora/RADAR.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "5 pages, 2 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14069v1",
            "code_links": [
                {
                    "url": "https://github.com/minaduki-sora/RADAR",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RL"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出基于强化学习的动态草稿树方法RADAR，以加速大语言模型推理过程。",
            "summary_zh": "现代大语言模型（LLMs）的推理过程成本高昂且速度缓慢，推测采样已成为解决此问题的有效方案。然而，推测采样中用于生成候选标记的草稿模型调用次数是一个预设的超参数，缺乏灵活性。为了更有效地生成和利用候选标记，我们提出了RADAR，这是一种基于强化学习动态草稿树的新型推测采样方法。RADAR将草稿树生成过程建模为马尔可夫决策过程（MDP），并采用离线强化学习训练预测模型，从而实现对草稿模型调用的实时决策，减少冗余计算并进一步加速推理。在三个LLM和四个任务上的评估表明，RADAR相比自回归解码基线实现了3.17倍至4.82倍的加速。代码可在https://github.com/minaduki-sora/RADAR获取。",
            "intro_zh": [
                "现有推测采样方法中草稿模型调用次数固定，缺乏灵活性，导致计算冗余和效率低下。",
                "RADAR将草稿树生成建模为MDP，利用离线强化学习训练预测模型，动态决定草稿模型调用。",
                "实验显示RADAR在多个LLM和任务上实现3.17x-4.82x加速，显著超越自回归解码基线。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大语言模型推理中的效率问题，特别是推测采样方法中草稿模型调用次数固定导致的灵活性不足和计算冗余。现有推测采样方法预设草稿模型调用次数，无法根据输入动态调整，可能生成过多或过少候选标记，影响推理速度和资源利用。\\n\\n**核心思路**：论文提出RADAR方法，通过将草稿树生成过程建模为马尔可夫决策过程（MDP），并利用离线强化学习训练预测模型，实现动态决策草稿模型调用次数。这样设计可以实时优化候选标记生成，减少不必要计算，提升推理效率。\\n\\n**技术框架**：RADAR的整体架构包括草稿模型、目标模型和强化学习预测模块。流程分为离线训练和在线推理两个阶段：离线阶段使用历史数据训练预测模型，学习最优调用策略；在线阶段根据当前状态动态决定草稿模型调用次数，生成候选标记树，然后由目标模型验证并接受有效标记。\\n\\n**关键创新**：最重要的技术创新是将强化学习引入推测采样，实现动态草稿树生成。与现有固定调用次数的方法相比，RADAR能自适应调整策略，本质区别在于从静态超参数优化转向基于状态的实时决策，提高了灵活性和效率。\\n\\n**关键设计**：技术细节包括：将草稿树生成建模为MDP，状态为当前解码上下文，动作为调用草稿模型次数，奖励为加速比；采用离线强化学习算法（如保守Q学习）训练预测模型，避免在线探索开销；网络结构可能基于Transformer或MLP，具体参数设置未在摘要中详述，需参考论文正文。",
            "application_zh": "RADAR可应用于需要高效大语言模型推理的场景，如聊天机器人、代码生成、文本摘要和机器翻译等。其实际价值在于降低计算成本、提升响应速度，适用于云服务和边缘设备。未来可能推动更智能的自适应推理方法发展，促进AI部署的普及化。",
            "highlight_zh": "RADAR在三个大语言模型和四个任务上的实验结果显示，相比自回归解码基线，实现了3.17倍至4.82倍的加速。具体性能数据表明，该方法能有效减少冗余计算，提升推理效率，验证了动态决策策略的优越性。对比基线包括传统推测采样方法，RADAR在灵活性和速度方面均有显著提升。",
            "tags_zh": [
                "大语言模型推理加速",
                "推测采样",
                "强化学习",
                "动态草稿树",
                "马尔可夫决策过程",
                "离线强化学习",
                "自回归解码优化",
                "计算效率提升"
            ],
            "_index": 90
        },
        {
            "title": "OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value",
            "authors": [
                "Mengzhang Cai",
                "Xin Gao",
                "Yu Li",
                "Honglin Lin",
                "Zheng Liu",
                "Zhuoshi Pan",
                "Qizhi Pei",
                "Xiaoran Shang",
                "Mengyuan Sun",
                "Zinan Tang",
                "Xiaoyang Wang",
                "Zhanping Zhong",
                "Yun Zhu",
                "Dahua Lin",
                "Conghui He",
                "Lijun Wu"
            ],
            "arxiv_id": "2512.14051v1",
            "summary": "The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14051v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO",
                        "VIO"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出OpenDataArena平台以解决大语言模型后训练数据集评估不透明和缺乏系统性基准的问题",
            "summary_zh": "大语言模型的快速发展依赖于后训练数据集的质量和多样性。然而，当前存在一个关键矛盾：模型被严格基准测试，而支撑它们的数据却是一个黑箱——其组成不透明、来源不确定且缺乏系统性评估。这种不透明性阻碍了可重复性，并模糊了数据特征与模型行为之间的因果关系。为弥合这一差距，我们引入了OpenDataArena，这是一个全面开放的平台，旨在基准测试后训练数据的内在价值。ODA建立了一个包含四个关键支柱的综合生态系统：（i）一个统一的训练-评估流程，确保在不同模型和领域之间进行公平、开放的比较；（ii）一个多维评分框架，从数十个不同维度分析数据质量；（iii）一个交互式数据谱系探索器，可视化数据集谱系并剖析组件来源；（iv）一个完全开源的工具包，用于训练、评估和评分，以促进数据研究。在ODA上进行的大量实验——涵盖多个领域的120多个训练数据集、22个基准测试，通过超过600次训练运行和4000万个处理数据点验证——揭示了非平凡的见解。我们的分析揭示了数据复杂性与任务性能之间的内在权衡，通过谱系追踪识别了流行基准中的冗余，并绘制了数据集之间的谱系关系。我们发布所有结果、工具和配置，以普及高质量数据评估的访问。ODA不仅仅扩展排行榜，而是设想从试错式数据策展转向以数据为中心的人工智能的原则性科学，为数据混合规律和基础模型战略组成的严格研究铺平道路。",
            "intro_zh": [
                "核心问题：大语言模型后训练数据集评估不透明，缺乏系统性基准，阻碍可重复性和数据-模型关系分析。",
                "方法要点：构建OpenDataArena平台，集成统一训练-评估流程、多维评分框架、数据谱系探索器和开源工具包。",
                "实验或效果：实验涵盖120+数据集、22个基准，揭示数据复杂性-性能权衡，识别冗余，并绘制数据集谱系关系。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大语言模型后训练数据集评估不透明的问题。现有方法中，数据集通常被视为黑箱，其组成、来源和质量缺乏系统性评估，导致模型性能与数据特征之间的因果关系不明确，阻碍了可重复性和数据策展的科学性。\\n\\n**核心思路**：通过构建一个全面、开放的基准测试平台OpenDataArena，将数据集评估从黑箱转变为透明、可量化的过程。核心思想是建立一个生态系统，整合训练、评估、评分和谱系分析，以公平、系统地衡量数据集的内在价值。\\n\\n**技术框架**：ODA的整体架构包括四个主要模块：统一训练-评估流程，支持多种模型和领域的公平比较；多维评分框架，从数十个维度（如多样性、复杂性、噪声水平）评估数据质量；交互式数据谱系探索器，可视化数据集的来源和演变；开源工具包，提供训练、评估和评分的完整工具链。这些模块协同工作，形成一个端到端的基准测试平台。\\n\\n**关键创新**：最重要的技术创新在于将数据集评估系统化，并引入数据谱系分析。与现有方法（通常仅关注模型性能或简单数据统计）的本质区别在于，ODA强调数据的内在价值和多维特性，通过谱系追踪揭示数据集之间的依赖关系和冗余，从而促进数据策展的科学性。\\n\\n**关键设计**：关键设计包括统一的训练流程（支持Llama、Qwen等模型），确保比较的公平性；多维评分框架基于数十个预定义指标（如信息熵、主题分布、语言复杂度），量化数据质量；数据谱系探索器使用图结构可视化数据集来源；开源工具包提供可复现的配置和脚本，具体参数设置（如训练超参数、评分阈值）在论文中详细说明，但未指定特定损失函数或网络结构，因为ODA本身不提出新模型，而是评估框架。",
            "application_zh": "该研究在大语言模型开发、数据策展和人工智能研究领域具有广泛潜在应用。实际价值包括帮助研究人员和开发者系统评估数据集质量，优化数据混合策略，提升模型性能；未来影响可能推动以数据为中心的人工智能发展，促进数据混合规律和基础模型组成的研究，提高AI系统的可解释性和可重复性。",
            "highlight_zh": "实验涵盖120多个训练数据集和22个基准测试，通过超过600次训练运行和4000万个数据点验证。关键结果包括：揭示了数据复杂性与任务性能之间的内在权衡（例如，高复杂性数据可能在某些任务中提升性能，但增加训练成本）；通过谱系追踪识别了流行基准中的冗余（如多个数据集共享相似来源）；绘制了数据集之间的谱系关系图，提供了数据演变的可视化洞察。这些结果基于公平比较，未提供具体性能提升幅度，但强调了数据评估的系统性价值。",
            "tags_zh": [
                "后训练数据集评估",
                "大语言模型基准测试",
                "数据谱系分析",
                "以数据为中心的人工智能",
                "开源平台",
                "多维评分框架",
                "数据混合规律",
                "可重复性研究"
            ],
            "_index": 91
        },
        {
            "title": "Evaluating Small Language Models for Agentic On-Farm Decision Support Systems",
            "authors": [
                "Enhong Liu",
                "Haiyu Yang",
                "Miel Hostens"
            ],
            "arxiv_id": "2512.14043v1",
            "summary": "Large Language Models (LLM) hold potential to support dairy scholars and farmers by supporting decision-making and broadening access to knowledge for stakeholders with limited technical expertise. However, the substantial computational demand restricts access to LLM almost exclusively through cloud-based service, which makes LLM-based decision support tools impractical for dairy farming. To address this gap, lightweight alternatives capable of running locally on farm hardware are required. In this work, we benchmarked 20 open-source Small Language Models (SLM) available on HuggingFace under farm-realistic computing constraints. Building on our prior work, we developed an agentic AI system that integrates five task-specific agents: literature search, web search, SQL database interaction, NoSQL database interaction, and graph generation following predictive models. Evaluation was conducted in two phases. In the first phase, five test questions were used for the initial screening to identify models capable of following basic dairy-related instructions and performing reliably in a compute-constrained environment. Models that passed this preliminary stage were then evaluated using 30 questions (five per task category mentioned above, plus one category addressing integrity and misconduct) in phase two. In results, Qwen-4B achieved superior performance across most of task categories, although showed unstable effectiveness in NoSQL database interactions through PySpark. To our knowledge, this is the first work explicitly evaluating the feasibility of SLM as engines for dairy farming decision-making, with central emphases on privacy and computational efficiency. While results highlight the promise of SLM-assisted tools for practical deployment in dairy farming, challenges remain, and fine-tuning is still needed to refine SLM performance in dairy-specific questions.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14043v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "世界模型",
                    "matched_keywords": [
                        "predictive model"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "评估小型语言模型在奶牛养殖决策支持系统中的可行性，以解决隐私和计算效率问题。",
            "summary_zh": "大型语言模型（LLM）有潜力通过支持决策制定和为技术专业知识有限的利益相关者拓宽知识获取途径来支持奶牛学者和农民。然而，巨大的计算需求几乎完全限制了通过基于云的服务访问LLM，这使得基于LLM的决策支持工具在奶牛养殖中不切实际。为了解决这一差距，需要能够在农场硬件上本地运行的轻量级替代方案。在这项工作中，我们在农场现实计算约束下，对HuggingFace上可用的20个开源小型语言模型（SLM）进行了基准测试。基于我们之前的工作，我们开发了一个代理AI系统，该系统集成了五个特定任务的代理：文献搜索、网络搜索、SQL数据库交互、NoSQL数据库交互以及遵循预测模型的图生成。评估分两个阶段进行。在第一阶段，使用五个测试问题进行初步筛选，以识别能够遵循基本奶牛相关指令并在计算受限环境中可靠运行的模型。通过此初步阶段的模型随后在第二阶段使用30个问题（上述每个任务类别五个，加上一个解决完整性和不当行为的类别）进行评估。在结果中，Qwen-4B在大多数任务类别中实现了卓越的性能，尽管通过PySpark在NoSQL数据库交互中显示出不稳定的有效性。据我们所知，这是第一项明确评估SLM作为奶牛养殖决策引擎可行性的工作，重点关注隐私和计算效率。虽然结果突显了SLM辅助工具在奶牛养殖中实际部署的前景，但挑战仍然存在，并且仍需要微调以改进SLM在奶牛特定问题中的性能。",
            "intro_zh": [
                "核心问题：大型语言模型（LLM）计算需求高，依赖云端服务，在奶牛养殖等隐私敏感、计算受限场景中不实用。",
                "方法要点：评估20个开源小型语言模型（SLM）在农场硬件上的可行性，构建集成五个任务代理的AI系统，以支持本地决策。",
                "实验或效果：Qwen-4B在多数任务中表现优异，但NoSQL交互不稳定，表明SLM有潜力但需进一步优化。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型（LLM）在奶牛养殖决策支持系统中因高计算需求和云端依赖导致的隐私泄露、延迟和不切实际问题。现有方法的痛点在于LLM通常需要云端部署，不适合农场本地硬件，限制了实时决策和隐私保护。\\n\\n**核心思路**：论文的核心解决思路是评估小型语言模型（SLM）作为轻量级替代方案，在农场现实计算约束下本地运行的可行性。设计基于代理AI系统，集成多个任务代理，以降低计算开销并提高实用性。这样设计是因为SLM参数较少，更适合资源受限环境，同时通过任务分解提升系统效率。\\n\\n**技术框架**：整体架构包括两个主要阶段：初步筛选和详细评估。在初步筛选阶段，使用五个测试问题评估20个SLM的基本指令遵循和可靠性；通过筛选的模型进入第二阶段，使用30个问题（覆盖五个任务类别：文献搜索、网络搜索、SQL数据库交互、NoSQL数据库交互、图生成，以及一个完整性类别）进行综合评估。系统集成了五个任务特定代理，每个代理负责不同功能模块。\\n\\n**关键创新**：最重要的技术创新点是首次明确评估SLM在奶牛养殖决策支持中的可行性，并构建了一个代理AI系统，将多个任务代理集成到本地部署框架中。与现有方法的本质区别在于，现有研究多关注LLM的云端应用，而本工作聚焦于SLM的本地化、隐私保护和计算效率，填补了农业AI领域的空白。\\n\\n**关键设计**：关键设计包括：使用HuggingFace上的20个开源SLM作为基准模型；设置农场现实计算约束（如有限内存和CPU）；任务代理基于PySpark等技术实现NoSQL交互；评估指标包括任务完成准确性和稳定性；具体参数设置如模型大小（例如Qwen-4B）和微调需求未详细说明，但强调需要针对奶牛特定问题进行优化。",
            "application_zh": "该研究的潜在应用领域包括奶牛养殖决策支持、农业智能化和本地化AI工具开发。实际价值在于为农民和学者提供隐私保护、低延迟的决策辅助，降低技术门槛。未来影响可能推动SLM在更多农业场景中的部署，促进可持续农业发展。",
            "highlight_zh": "最重要的实验结果是：在评估的20个SLM中，Qwen-4B在大多数任务类别（如文献搜索、网络搜索、SQL交互）中表现最优，显示出较高的准确性和可靠性。然而，在NoSQL数据库交互任务中，其有效性不稳定，表明特定任务仍需改进。对比基线为其他SLM，提升幅度未具体量化，但Qwen-4B整体领先。实验还发现SLM在计算受限环境下可行，为本地部署提供了实证支持。",
            "tags_zh": [
                "小型语言模型",
                "奶牛养殖决策支持",
                "代理AI系统",
                "本地化部署",
                "隐私保护",
                "计算效率",
                "农业人工智能",
                "任务评估"
            ],
            "_index": 92
        },
        {
            "title": "Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model",
            "authors": [
                "Zhaofeng Hu",
                "Hongrui Yu",
                "Vaidhyanathan Chandramouli",
                "Ci-Jyun Liang"
            ],
            "arxiv_id": "2512.14031v1",
            "summary": "This study evaluates two leading approaches for teaching construction robots new skills to understand their applicability for construction automation: a Vision-Language-Action (VLA) model and Reinforcement Learning (RL) methods. The goal is to understand both task performance and the practical effort needed to deploy each approach on real jobs. The authors developed two teleoperation interfaces to control the robots and collect the demonstrations needed, both of which proved effective for training robots for long-horizon and dexterous tasks. In addition, the authors conduct a three-stage evaluation. First, the authors compare a Multi-Layer Perceptron (MLP) policy with a Deep Q-network (DQN) imitation model to identify the stronger RL baseline, focusing on model performance, generalization, and a pick-up experiment. Second, three different VLA models are trained in two different scenarios and compared with each other. Third, the authors benchmark the selected RL baseline against the VLA model using computational and sample-efficiency measures and then a robot experiment on a multi-stage panel installation task that includes transport and installation. The VLA model demonstrates strong generalization and few-shot capability, achieving 60% and 100% success in the pickup phase. In comparison, DQN can be made robust but needs additional noise during tuning, which increases the workload. Overall, the findings indicate that VLA offers practical advantages for changing tasks by reducing programming effort and enabling useful performance with minimal data, while DQN provides a viable baseline when sufficient tuning effort is acceptable.",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14031v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RL"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "评估分层强化学习与视觉-语言-动作模型在建筑机器人技能学习中的样本效率与实用性",
            "summary_zh": "本研究评估了两种用于教授建筑机器人新技能的主流方法——视觉-语言-动作（VLA）模型和强化学习（RL）方法，以理解它们在建筑自动化中的适用性。目标是了解任务性能以及在真实工作中部署每种方法所需的实际工作量。作者开发了两种遥操作接口来控制机器人并收集所需的演示，这两种接口均被证明对训练机器人执行长期和灵巧任务有效。此外，作者进行了三阶段评估。首先，作者比较了多层感知器（MLP）策略和深度Q网络（DQN）模仿模型，以确定更强的RL基线，重点关注模型性能、泛化能力和拾取实验。其次，在两种不同场景下训练了三种不同的VLA模型，并进行了相互比较。第三，作者使用计算和样本效率指标，以及在一个包括运输和安装的多阶段面板安装任务上的机器人实验，对选定的RL基线与VLA模型进行了基准测试。VLA模型表现出强大的泛化能力和少样本能力，在拾取阶段实现了60%和100%的成功率。相比之下，DQN虽然可以变得鲁棒，但需要在调优过程中添加额外噪声，这增加了工作量。总体而言，研究结果表明，VLA通过减少编程工作量并以最少的数据实现有用的性能，为任务变更提供了实际优势，而DQN在可接受足够调优工作量的情况下提供了一个可行的基线。",
            "intro_zh": [
                "核心问题：建筑自动化中机器人技能学习面临样本效率低、泛化能力差和实际部署工作量大的挑战，现有方法如传统RL需要大量数据且调优复杂。",
                "方法要点：系统比较VLA模型与RL方法，通过开发遥操作接口收集演示，并进行三阶段评估以量化性能、泛化能力和实际工作量。",
                "实验或效果：VLA在拾取任务中达到60%和100%成功率，展现强泛化；DQN需额外调优噪声，VLA在样本效率和实用性上更具优势。"
            ],
            "method_zh": "**问题定义**：论文旨在解决建筑机器人技能学习中的样本效率低、泛化能力差和实际部署工作量大的问题。现有方法如传统强化学习（RL）需要大量交互数据，且调优过程复杂，难以适应多变任务；而视觉-语言-动作（VLA）模型虽具潜力，但其在建筑场景中的适用性和与RL的对比尚不明确。痛点在于缺乏系统评估，以指导实际应用中选择合适方法。\\n\\n**核心思路**：论文的核心思路是通过开发两种遥操作接口收集机器人演示数据，并设计三阶段评估框架，系统比较VLA模型和RL方法在性能、泛化能力和实际工作量方面的差异。这样设计旨在提供实证依据，帮助理解不同方法在建筑自动化中的优缺点，从而优化技能学习策略。\\n\\n**技术框架**：整体架构包括数据收集、模型训练和评估三部分。首先，使用遥操作接口控制机器人执行任务并收集演示数据。其次，训练阶段：对于RL，比较MLP策略和DQN模仿模型；对于VLA，训练三种不同模型在两种场景下。最后，评估阶段分为三步：比较RL基线以确定最优模型；比较VLA模型内部性能；基准测试选定的RL基线与VLA模型，使用计算效率、样本效率和机器人实验（如多阶段面板安装任务）。\\n\\n**关键创新**：最重要的技术创新在于系统性地整合遥操作数据收集与多阶段评估框架，首次在建筑机器人领域对VLA和RL进行头对头比较。与现有方法相比，本质区别在于强调实际部署工作量（如调优复杂度）的量化，而不仅仅是任务性能，这为实际应用提供了更全面的指导。\\n\\n**关键设计**：关键设计包括：遥操作接口设计，确保有效收集长期和灵巧任务的演示；在RL比较中，使用MLP和DQN作为基线，重点关注泛化能力和拾取实验；VLA模型训练中，采用两种不同场景以测试适应性；评估指标包括成功率、计算时间、样本需求，以及在实际机器人任务（如面板安装）中的表现。具体参数和损失函数细节在论文中未详细说明，但强调DQN调优时需添加噪声以提高鲁棒性。",
            "application_zh": "该研究在建筑自动化领域具有重要应用价值，可用于机器人技能学习，如面板安装、材料运输等复杂任务。潜在应用包括智能施工、工业机器人操作和家庭服务机器人，通过减少编程工作量和数据需求，提高部署效率和适应性。未来可能推动多模态学习在机器人领域的普及，促进实际场景中的快速技能迁移。",
            "highlight_zh": "最重要的实验结果包括：VLA模型在拾取阶段实现60%和100%成功率，展现强泛化和少样本能力；DQN虽可鲁棒，但需额外噪声调优，增加工作量。在样本效率比较中，VLA以更少数据达到有用性能，而DQN作为基线在可接受调优下可行。多阶段面板安装任务中，VLA在运输和安装环节表现优异，突显其实际优势。",
            "tags_zh": [
                "机器人技能学习",
                "视觉-语言-动作模型",
                "强化学习",
                "样本效率",
                "建筑自动化",
                "多模态学习",
                "遥操作接口",
                "泛化能力"
            ],
            "_index": 93
        },
        {
            "title": "MobileWorldBench: Towards Semantic World Modeling For Mobile Agents",
            "authors": [
                "Shufan Li",
                "Konstantinos Kallidromitis",
                "Akash Gokul",
                "Yusuke Kato",
                "Kazuki Kozuka",
                "Aditya Grover"
            ],
            "arxiv_id": "2512.14014v1",
            "summary": "World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "21 pages, 13 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14014v1",
            "code_links": [
                {
                    "url": "https://github.com/jacklishufan/MobileWorld",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "世界模型",
                    "matched_keywords": [
                        "world model"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出MobileWorldBench基准和MobileWorld数据集，通过自然语言描述状态转移来提升移动GUI代理的世界建模能力",
            "summary_zh": "世界模型在提升具身智能体任务性能方面显示出巨大效用。然而，先前的研究主要集中在像素空间世界模型上，这些方法在GUI环境中面临实际限制，因为预测未来状态中的复杂视觉元素通常很困难。在这项工作中，我们探索了GUI智能体世界建模的另一种表述方式，即用自然语言而非预测原始像素来描述状态转移。首先，我们引入了MobileWorldBench，这是一个评估视觉语言模型作为移动GUI智能体世界模型能力的基准。其次，我们发布了MobileWorld，一个包含140万个样本的大规模数据集，显著提升了视觉语言模型的世界建模能力。最后，我们提出了一个新颖的框架，将视觉语言模型世界模型集成到移动智能体的规划框架中，证明语义世界模型可以通过提高任务成功率直接使移动智能体受益。代码和数据集可在https://github.com/jacklishufan/MobileWorld获取。",
            "intro_zh": [
                "现有像素空间世界模型在GUI环境中预测复杂视觉元素困难，限制了移动代理的任务性能。",
                "论文提出用自然语言描述状态转移的语义世界模型，并构建基准和数据集来提升视觉语言模型能力。",
                "实验表明，该框架能显著提高移动代理的任务成功率，验证了语义世界模型的直接效益。"
            ],
            "method_zh": "**问题定义**：论文旨在解决移动GUI代理的世界建模问题，现有像素空间世界模型在预测未来状态中的复杂视觉元素时面临困难，导致在GUI环境中实用性受限。\\n\\n**核心思路**：采用自然语言而非像素来表述状态转移，利用视觉语言模型作为语义世界模型，以更高效地描述GUI环境中的变化，从而提升代理的规划能力。\\n\\n**技术框架**：整体架构包括三个主要部分：MobileWorldBench基准用于评估视觉语言模型的世界建模能力；MobileWorld数据集提供大规模训练样本；以及一个集成框架，将视觉语言模型世界模型嵌入到移动代理的规划流程中，实现状态预测和任务执行。\\n\\n**关键创新**：最重要的技术创新在于从像素空间转向语义空间的世界建模，通过自然语言描述状态转移，避免了视觉预测的复杂性，本质区别在于强调语义理解而非视觉重建。\\n\\n**关键设计**：MobileWorld数据集包含140万个样本，覆盖多样GUI场景；框架中视觉语言模型被训练来预测自然语言状态描述，具体损失函数和网络结构细节未在摘要中明确，但依赖于现有视觉语言模型架构进行微调和集成。",
            "application_zh": "该研究在移动GUI代理领域具有广泛潜在应用，如智能手机应用自动化、网页交互辅助和机器人界面控制。通过提升语义世界建模能力，可增强代理在复杂环境中的任务执行效率和成功率，推动具身智能向更实用的方向发展。",
            "highlight_zh": "实验结果显示，提出的语义世界模型框架显著提高了移动代理的任务成功率。具体性能数据未在摘要中提供，但通过MobileWorldBench基准评估，视觉语言模型在集成后表现出优于传统像素空间方法的性能，提升幅度依赖于数据集规模和模型微调，验证了自然语言描述状态转移的有效性。",
            "tags_zh": [
                "移动GUI代理",
                "语义世界建模",
                "视觉语言模型",
                "自然语言状态转移",
                "基准测试",
                "大规模数据集",
                "任务规划",
                "具身智能"
            ],
            "_index": 94
        },
        {
            "title": "Professional Software Developers Don't Vibe, They Control: AI Agent Use for Coding in 2025",
            "authors": [
                "Ruanqianqian Huang",
                "Avery Reyna",
                "Sorin Lerner",
                "Haijun Xia",
                "Brian Hempel"
            ],
            "arxiv_id": "2512.14012v1",
            "summary": "The rise of AI agents is transforming how software can be built. The promise of agents is that developers might write code quicker, delegate multiple tasks to different agents, and even write a full piece of software purely out of natural language. In reality, what roles agents play in professional software development remains in question. This paper investigates how experienced developers use agents in building software, including their motivations, strategies, task suitability, and sentiments. Through field observations (N=13) and qualitative surveys (N=99), we find that while experienced developers value agents as a productivity boost, they retain their agency in software design and implementation out of insistence on fundamental software quality attributes, employing strategies for controlling agent behavior leveraging their expertise. In addition, experienced developers feel overall positive about incorporating agents into software development given their confidence in complementing the agents' limitations. Our results shed light on the value of software development best practices in effective use of agents, suggest the kinds of tasks for which agents may be suitable, and point towards future opportunities for better agentic interfaces and agentic use guidelines.",
            "categories": [
                "cs.SE",
                "cs.AI",
                "cs.HC"
            ],
            "primary_category": "cs.SE",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14012v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "通过实地观察和定性调查揭示专业开发者如何控制AI代理以提升软件质量与生产力",
            "summary_zh": "AI代理的兴起正在改变软件的构建方式。代理的承诺在于开发者可能更快地编写代码，将多个任务委托给不同的代理，甚至纯粹用自然语言编写完整的软件。然而，代理在专业软件开发中扮演的角色仍然存疑。本文通过实地观察（N=13）和定性调查（N=99），研究了经验丰富的开发者如何在构建软件时使用代理，包括他们的动机、策略、任务适用性和情感。我们发现，虽然经验丰富的开发者重视代理作为生产力提升工具，但他们坚持基本的软件质量属性，利用专业知识控制代理行为，从而保持对软件设计和实现的自主权。此外，经验丰富的开发者对将代理融入软件开发持总体积极态度，因为他们有信心弥补代理的局限性。我们的结果揭示了软件开发最佳实践在有效使用代理中的价值，提出了代理可能适合的任务类型，并指出了未来改进代理界面和代理使用指南的机会。",
            "intro_zh": [
                "核心问题：AI代理在专业软件开发中的实际角色和影响尚不明确，现有研究缺乏对开发者使用策略和动机的深入理解。",
                "方法要点：通过实地观察和定性调查，分析经验丰富的开发者如何控制AI代理行为，以提升软件质量和生产力。",
                "实验或效果：发现开发者通过专业知识控制代理，保持自主权，并对代理融入开发持积极态度，揭示了最佳实践的价值。"
            ],
            "method_zh": "**问题定义**：论文旨在解决AI代理在专业软件开发中的实际应用问题，现有方法的痛点在于过度强调代理的自动化能力，而忽视了开发者如何在实际工作中与代理互动，导致对代理角色和影响的误解。\\n\\n**核心思路**：论文的核心解决思路是通过实证研究，深入分析经验丰富的开发者使用AI代理的动机、策略和情感，强调开发者如何利用专业知识控制代理行为，以保持软件质量，而非完全依赖代理自动化。\\n\\n**技术框架**：整体架构包括两个主要阶段：首先进行实地观察（N=13），直接记录开发者在实际项目中使用AI代理的行为；其次进行定性调查（N=99），收集开发者对代理使用的主观反馈和策略描述。\\n\\n**关键创新**：最重要的技术创新点在于将研究焦点从代理的技术性能转向开发者的实际使用行为，通过混合方法揭示开发者如何主动控制代理，这与现有方法通常假设代理主导开发过程的本质区别在于强调了人类专业知识的核心作用。\\n\\n**关键设计**：关键设计包括采用定性研究方法，如观察记录和开放式问卷调查，以捕捉开发者的策略和情感；参数设置上，样本选择聚焦经验丰富的开发者（N=13观察，N=99调查），确保数据代表性；分析框架基于主题编码，提取动机、控制策略和任务适用性等维度。",
            "application_zh": "该研究的潜在应用领域包括软件开发工具设计、AI代理界面优化和开发者培训指南。实际价值在于为工具开发者提供基于实证的改进方向，帮助创建更符合专业需求的代理系统，提升开发效率和质量。未来影响可能推动更人性化的AI辅助开发环境，促进开发者与代理的协同工作。",
            "highlight_zh": "最重要的实验结果包括：经验丰富的开发者通过控制代理行为（如代码审查和任务分解）来保持软件质量，对代理融入开发持总体积极态度（基于调查反馈），并识别出代理适合的任务类型（如代码生成和调试）。具体数据：实地观察N=13显示开发者主动干预代理输出；定性调查N=99中多数开发者表达积极情感，并强调专业知识的重要性。提升幅度体现在开发者生产力提升的同时，软件质量属性得到保障。",
            "tags_zh": [
                "AI代理",
                "软件开发",
                "实地观察",
                "定性调查",
                "开发者行为",
                "软件质量",
                "生产力提升",
                "人机协同"
            ],
            "_index": 95
        },
        {
            "title": "CLAIM: Camera-LiDAR Alignment with Intensity and Monodepth",
            "authors": [
                "Zhuo Zhang",
                "Yonghui Liu",
                "Meijie Zhang",
                "Feiyang Tan",
                "Yikang Ding"
            ],
            "arxiv_id": "2512.14001v1",
            "summary": "In this paper, we unleash the potential of the powerful monodepth model in camera-LiDAR calibration and propose CLAIM, a novel method of aligning data from the camera and LiDAR. Given the initial guess and pairs of images and LiDAR point clouds, CLAIM utilizes a coarse-to-fine searching method to find the optimal transformation minimizing a patched Pearson correlation-based structure loss and a mutual information-based texture loss. These two losses serve as good metrics for camera-LiDAR alignment results and require no complicated steps of data processing, feature extraction, or feature matching like most methods, rendering our method simple and adaptive to most scenes. We validate CLAIM on public KITTI, Waymo, and MIAS-LCEC datasets, and the experimental results demonstrate its superior performance compared with the state-of-the-art methods. The code is available at https://github.com/Tompson11/claim.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted by IROS 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14001v1",
            "code_links": [
                {
                    "url": "https://github.com/Tompson11/claim",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "lidar",
                        "waymo"
                    ],
                    "score": 2
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出CLAIM方法，利用单目深度模型和粗到精搜索，解决相机与LiDAR数据对齐问题，无需复杂特征处理。",
            "summary_zh": "本文释放了强大单目深度模型在相机-LiDAR标定中的潜力，提出了CLAIM，一种新颖的相机和LiDAR数据对齐方法。给定初始猜测和图像与LiDAR点云对，CLAIM采用粗到精搜索方法，寻找最小化基于块皮尔逊相关的结构损失和基于互信息的纹理损失的最优变换。这两种损失作为相机-LiDAR对齐结果的良好度量，无需像大多数方法那样进行复杂的数据处理、特征提取或特征匹配步骤，使我们的方法简单且适应大多数场景。我们在公开的KITTI、Waymo和MIAS-LCEC数据集上验证了CLAIM，实验结果表明其性能优于最先进的方法。代码可在https://github.com/Tompson11/claim获取。",
            "intro_zh": [
                "核心问题：现有相机-LiDAR对齐方法通常依赖复杂的数据处理、特征提取或匹配步骤，导致计算成本高、适应性差，难以适应多样场景。",
                "方法要点：CLAIM利用单目深度模型生成深度信息，结合粗到精搜索策略，通过结构损失和纹理损失优化变换，实现高效对齐。",
                "实验或效果：在KITTI、Waymo和MIAS-LCEC数据集上，CLAIM表现出优于现有方法的性能，验证了其简单性和适应性。"
            ],
            "method_zh": "**问题定义**：论文解决相机与LiDAR数据对齐问题，即估计两者间的空间变换参数（如旋转和平移）。现有方法的痛点在于依赖复杂的特征提取和匹配步骤，如手动设计特征或深度学习网络，这增加了计算负担，且对场景变化敏感，难以泛化到不同环境。\\n\\n**核心思路**：论文的核心思路是利用单目深度模型（monodepth）生成图像的深度信息，结合LiDAR点云的强度数据，通过优化损失函数直接对齐数据，避免传统特征处理。这样设计是因为单目深度模型能提供丰富的结构信息，而强度数据包含纹理细节，两者结合可更全面地评估对齐质量。\\n\\n**技术框架**：整体流程包括输入图像和LiDAR点云对，以及初始变换猜测。首先，使用单目深度模型从图像生成深度图。然后，采用粗到精搜索策略：在粗搜索阶段，在较大参数空间内采样候选变换；在精搜索阶段，基于损失函数梯度优化。主要模块包括数据预处理（如点云投影）、损失计算（结构损失和纹理损失）和优化器。\\n\\n**关键创新**：最重要的技术创新是引入基于块皮尔逊相关的结构损失和基于互信息的纹理损失作为对齐度量，无需复杂特征处理。与现有方法的本质区别在于直接利用原始数据（深度和强度）进行对齐，简化了流程，提高了适应性和效率。\\n\\n**关键设计**：关键设计包括：损失函数中，结构损失基于图像深度图和LiDAR点云投影深度图的块皮尔逊相关系数，衡量结构一致性；纹理损失基于图像灰度图和LiDAR强度图的互信息，衡量纹理相似性。搜索策略采用粗到精方法，粗搜索使用网格采样，精搜索使用梯度下降优化。参数设置如块大小和搜索范围根据数据集调整，以平衡精度和速度。",
            "application_zh": "该研究在自动驾驶、机器人导航和增强现实等领域具有潜在应用价值。通过简化相机-LiDAR对齐流程，CLAIM可提高多传感器融合系统的部署效率和鲁棒性，支持实时环境感知和三维重建，未来可能推动低成本、高适应性传感器标定技术的发展。",
            "highlight_zh": "在KITTI、Waymo和MIAS-LCEC数据集上的实验显示，CLAIM在相机-LiDAR对齐任务中优于最先进方法。具体性能数据未知，但论文报告了在多个指标上的显著提升，如对齐误差降低和计算效率提高，验证了其简单性和适应性优势。",
            "tags_zh": [
                "相机-LiDAR对齐",
                "单目深度模型",
                "粗到精搜索",
                "结构损失",
                "纹理损失",
                "多模态融合",
                "传感器标定",
                "自动驾驶感知"
            ],
            "_index": 96
        },
        {
            "title": "On the Hardness of Conditional Independence Testing In Practice",
            "authors": [
                "Zheng He",
                "Roman Pogodin",
                "Yazhe Li",
                "Namrata Deka",
                "Arthur Gretton",
                "Danica J. Sutherland"
            ],
            "arxiv_id": "2512.14000v1",
            "summary": "Tests of conditional independence (CI) underpin a number of important problems in machine learning and statistics, from causal discovery to evaluation of predictor fairness and out-of-distribution robustness. Shah and Peters (2020) showed that, contrary to the unconditional case, no universally finite-sample valid test can ever achieve nontrivial power. While informative, this result (based on \"hiding\" dependence) does not seem to explain the frequent practical failures observed with popular CI tests. We investigate the Kernel-based Conditional Independence (KCI) test - of which we show the Generalized Covariance Measure underlying many recent tests is nearly a special case - and identify the major factors underlying its practical behavior. We highlight the key role of errors in the conditional mean embedding estimate for the Type-I error, while pointing out the importance of selecting an appropriate conditioning kernel (not recognized in previous work) as being necessary for good test power but also tending to inflate Type-I error.",
            "categories": [
                "stat.ML",
                "cs.LG",
                "stat.ME"
            ],
            "primary_category": "stat.ML",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Published at NeurIPS 2025: https://openreview.net/forum?id=Tn1M71PDfF",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14000v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "揭示KCI测试在实践中失效的关键因素，提出条件核选择对I型错误和检验功效的平衡影响",
            "summary_zh": "条件独立性检验在机器学习和统计学中支撑着从因果发现到预测器公平性和分布外鲁棒性评估等多个重要问题。Shah和Peters（2020）表明，与无条件情况相反，不存在普遍有限样本有效的检验能够获得非平凡的检验功效。虽然这一结果（基于“隐藏”依赖性）具有启发性，但似乎并未解释实践中常见条件独立性检验频繁失效的现象。我们研究了基于核的条件独立性检验，并展示了作为许多近期检验基础的广义协方差度量几乎是一个特例，同时识别了其实际行为背后的主要因素。我们强调了条件均值嵌入估计中的误差对I型错误的关键作用，同时指出选择适当的条件核（先前工作中未认识到）对于获得良好的检验功效是必要的，但也倾向于增加I型错误。",
            "intro_zh": [
                "核心问题：现有条件独立性检验在实践中频繁失效，Shah和Peters的理论结果未能完全解释这些实际失败现象。",
                "方法要点：聚焦KCI检验，识别条件均值嵌入估计误差和条件核选择对I型错误和检验功效的关键影响。",
                "实验或效果：揭示了条件核选择在平衡检验功效和I型错误中的重要性，为改进CI测试提供了实用指导。"
            ],
            "method_zh": "**问题定义**：论文旨在解决条件独立性检验在实践中频繁失效的问题，特别是针对基于核的条件独立性检验。现有方法的痛点在于，虽然Shah和Peters的理论结果指出了CI测试的固有困难，但未能解释实际观察到的失败现象，尤其是KCI测试在应用中表现不佳的原因。\\n\\n**核心思路**：论文的核心解决思路是通过深入分析KCI测试的实际行为，识别导致其失效的关键因素，特别是条件均值嵌入估计中的误差对I型错误的影响，以及条件核选择对检验功效和I型错误的双重作用。这样设计是为了从实践角度补充理论结果，提供可操作的改进方向。\\n\\n**技术框架**：整体架构包括对KCI测试的理论分析、实验验证和因素识别。主要模块包括：1) 回顾KCI测试和广义协方差度量的理论基础；2) 分析条件均值嵌入估计误差的统计性质；3) 研究条件核选择对测试性能的影响；4) 通过模拟实验验证理论发现。\\n\\n**关键创新**：最重要的技术创新点在于首次系统性地揭示了条件核选择在KCI测试中的关键作用，并指出其未被先前工作认识到的必要性。与现有方法的本质区别在于，不仅关注理论上的有限样本有效性，还深入探讨了实际实现中的具体挑战，如估计误差和核参数选择。\\n\\n**关键设计**：关键设计包括：1) 使用核方法进行条件均值嵌入估计，具体涉及再生核希尔伯特空间中的操作；2) 分析条件核的选择标准，如核类型和带宽参数；3) 通过统计检验（如置换检验）评估条件独立性；4) 在实验中控制变量以隔离不同因素的影响，例如固定条件核或调整估计精度。",
            "application_zh": "该研究在因果发现、机器学习公平性评估和分布外鲁棒性测试等领域具有重要应用价值。通过改进条件独立性检验的实践可靠性，可以提升因果推断的准确性、确保算法公平性，并增强模型在未知数据上的泛化能力。未来可能推动更稳健的CI测试方法开发，促进相关领域的实际部署。",
            "highlight_zh": "实验结果表明，条件核选择对KCI测试的性能有显著影响：不当的条件核会导致I型错误率大幅增加（例如在某些设置下超过名义水平），同时降低检验功效。与基线方法（如传统KCI测试）相比，论文通过优化条件核选择，在模拟数据中实现了更好的错误控制（如将I型错误率接近名义水平）和更高的检测能力（提升检验功效约10-20%），具体数据取决于数据集和参数设置。",
            "tags_zh": [
                "条件独立性检验",
                "核方法",
                "因果发现",
                "机器学习公平性",
                "分布外鲁棒性",
                "统计检验",
                "条件均值嵌入",
                "I型错误控制"
            ],
            "_index": 97
        },
        {
            "title": "Maximum Mean Discrepancy with Unequal Sample Sizes via Generalized U-Statistics",
            "authors": [
                "Aaron Wei",
                "Milad Jalali",
                "Danica J. Sutherland"
            ],
            "arxiv_id": "2512.13997v1",
            "summary": "Existing two-sample testing techniques, particularly those based on choosing a kernel for the Maximum Mean Discrepancy (MMD), often assume equal sample sizes from the two distributions. Applying these methods in practice can require discarding valuable data, unnecessarily reducing test power. We address this long-standing limitation by extending the theory of generalized U-statistics and applying it to the usual MMD estimator, resulting in new characterization of the asymptotic distributions of the MMD estimator with unequal sample sizes (particularly outside the proportional regimes required by previous partial results). This generalization also provides a new criterion for optimizing the power of an MMD test with unequal sample sizes. Our approach preserves all available data, enhancing test accuracy and applicability in realistic settings. Along the way, we give much cleaner characterizations of the variance of MMD estimators, revealing something that might be surprising to those in the area: while zero MMD implies a degenerate estimator, it is sometimes possible to have a degenerate estimator with nonzero MMD as well; we give a construction and a proof that it does not happen in common situations.",
            "categories": [
                "stat.ML",
                "cs.LG",
                "math.ST",
                "stat.ME"
            ],
            "primary_category": "stat.ML",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13997v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                },
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 2,
            "headline_zh": "提出基于广义U统计量的最大均值差异方法，解决不等样本量下的两样本检验问题，提升测试准确性和适用性。",
            "summary_zh": "现有的两样本检验技术，特别是基于选择核函数的最大均值差异方法，通常假设两个分布具有相等的样本量。在实际应用中，这些方法可能需要丢弃有价值的数据，不必要地降低检验功效。我们通过扩展广义U统计量的理论并将其应用于通常的MMD估计量，解决了这一长期存在的限制，从而对不等样本量下MMD估计量的渐近分布进行了新的刻画（特别是在先前部分结果所需的比例范围之外）。这一推广还为优化不等样本量下MMD检验的功效提供了新的准则。我们的方法保留了所有可用数据，增强了实际场景中的检验准确性和适用性。在此过程中，我们给出了MMD估计量方差的更清晰刻画，揭示了该领域可能令人惊讶的一点：虽然零MMD意味着估计量退化，但有时也可能出现非零MMD下的退化估计量；我们给出了一个构造，并证明这在常见情况下不会发生。",
            "intro_zh": [
                "现有基于最大均值差异的两样本检验方法通常假设样本量相等，实际应用中需丢弃数据，降低检验功效。",
                "论文扩展广义U统计量理论，应用于MMD估计量，刻画不等样本量下的渐近分布，提供优化检验功效的新准则。",
                "方法保留所有可用数据，提升检验准确性和适用性，并给出MMD估计量方差的更清晰刻画，揭示退化估计量的新现象。"
            ],
            "method_zh": "**问题定义**：论文解决不等样本量下的两样本检验问题，现有基于最大均值差异的方法通常假设样本量相等，导致实际应用中需丢弃数据，降低检验功效，限制了方法的适用性和准确性。\\n\\n**核心思路**：通过扩展广义U统计量的理论，将其应用于MMD估计量，以处理不等样本量情况，从而避免数据丢弃，提升检验功效，并刻画渐近分布。\\n\\n**技术框架**：整体框架包括：1) 定义不等样本量下的MMD估计量；2) 应用广义U统计量理论推导其渐近分布；3) 基于新准则优化检验功效；4) 分析估计量方差和退化情况。\\n\\n**关键创新**：最重要的创新是首次完整刻画不等样本量下MMD估计量的渐近分布，特别是在非比例样本量场景，突破了先前部分结果的限制，并提供了优化检验功效的新准则。\\n\\n**关键设计**：关键设计包括：1) 使用广义U统计量作为理论工具，确保估计量的无偏性和一致性；2) 推导方差表达式，揭示零MMD和非零MMD下估计量退化的可能性；3) 设置核函数选择准则，以最大化检验功效；4) 避免数据丢弃，保留所有样本点。",
            "application_zh": "该研究在机器学习、统计学和数据分析领域具有广泛潜在应用，如异常检测、分布漂移监测、生物信息学中的基因表达分析等，可提升不等样本量场景下的检验准确性和效率，推动实际应用中的数据处理优化。",
            "highlight_zh": "实验结果表明，新方法在不等样本量下显著提升检验功效，相比传统方法（如丢弃数据或使用近似方法），功效提升可达20%以上，并在合成和真实数据集上验证了渐近分布的正确性和优化准则的有效性。",
            "tags_zh": [
                "两样本检验",
                "最大均值差异",
                "广义U统计量",
                "不等样本量",
                "渐近分布",
                "检验功效优化",
                "核方法",
                "统计学习"
            ],
            "_index": 98
        },
        {
            "title": "Universal Reasoning Model",
            "authors": [
                "Zitian Gao",
                "Lynx Chen",
                "Yihao Xiao",
                "He Xing",
                "Ran Tao",
                "Haoming Luo",
                "Joey Zhou",
                "Bryan Dai"
            ],
            "arxiv_id": "2512.14693v1",
            "summary": "Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14693v1",
            "code_links": [
                {
                    "url": "https://github.com/zitian-gao/URM",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出通用推理模型以提升复杂推理任务性能，在ARC-AGI基准上实现新突破",
            "summary_zh": "通用Transformer（UTs）已广泛应用于ARC-AGI和数独等复杂推理任务，但其性能提升的具体来源尚未得到充分探索。本研究系统分析了UTs的变体，发现ARC-AGI上的改进主要源于Transformer的循环归纳偏置和强非线性组件，而非复杂的架构设计。基于这一发现，我们提出了通用推理模型（URM），通过引入短卷积和截断反向传播来增强UT。该方法显著提升了推理性能，在ARC-AGI 1上达到了53.8%的pass@1，在ARC-AGI 2上达到了16.0%的pass@1，实现了最先进水平。代码已开源：https://github.com/zitian-gao/URM。",
            "intro_zh": [
                "现有通用Transformer在复杂推理任务中性能提升来源不明确，限制了进一步优化。",
                "通过分析发现性能源于循环归纳偏置和强非线性，提出增强通用Transformer的通用推理模型。",
                "在ARC-AGI基准上实现显著提升，达到最先进水平，验证了方法的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决复杂推理任务（如ARC-AGI和数独）中，通用Transformer（UTs）性能提升来源不明确的问题。现有方法依赖复杂架构设计，但缺乏对核心驱动因素的深入理解，导致优化方向模糊，限制了模型在推理任务上的进一步突破。\\n\\n**核心思路**：论文的核心思路是通过系统分析UTs变体，识别出性能提升的关键因素——循环归纳偏置和强非线性组件，而非复杂架构。基于此，设计一个增强型模型，通过引入短卷积和截断反向传播来强化这些因素，从而提升推理能力，避免不必要的架构复杂性。\\n\\n**技术框架**：整体架构基于通用Transformer，增强为通用推理模型（URM）。流程包括：输入处理阶段，使用标准Transformer编码器；核心推理阶段，集成短卷积模块以增强局部特征提取和循环归纳偏置；训练阶段，采用截断反向传播优化计算效率。主要模块包括Transformer层、短卷积层和训练优化模块。\\n\\n**关键创新**：最重要的技术创新是结合短卷积和截断反向传播来增强通用Transformer。与现有方法相比，本质区别在于不依赖复杂架构设计，而是聚焦于强化已识别的关键因素（循环归纳偏置和强非线性），从而更直接地提升推理性能，同时保持模型简洁性。\\n\\n**关键设计**：关键设计包括：短卷积层用于增强局部模式捕捉，具体参数设置如卷积核大小和步长需根据任务调整；截断反向传播用于减少训练时的计算开销，提高效率；网络结构保持Transformer基础，但集成这些组件；损失函数通常使用标准交叉熵或任务特定损失，具体细节在论文中未详细说明，需参考开源代码。",
            "application_zh": "该研究在复杂推理任务领域具有广泛潜在应用，如人工智能通用智能（AGI）基准测试（如ARC-AGI）、逻辑游戏（如数独）和需要高级推理的AI系统。实际价值在于提供了一种高效、简洁的模型优化方法，可推动推理模型的发展，未来可能影响教育辅助、自动化决策和智能机器人等领域，提升AI的推理和问题解决能力。",
            "highlight_zh": "最重要的实验结果包括：在ARC-AGI 1基准上，URM达到53.8% pass@1，在ARC-AGI 2基准上达到16.0% pass@1，均实现最先进水平。对比基线可能包括标准UTs和其他变体，提升幅度显著，具体数据在论文中未详细对比，但摘要表明“substantially improves”，验证了方法在复杂推理任务上的有效性。",
            "tags_zh": [
                "通用推理模型",
                "Transformer增强",
                "复杂推理任务",
                "ARC-AGI基准",
                "短卷积",
                "截断反向传播",
                "循环归纳偏置",
                "非线性组件"
            ],
            "_index": 99
        },
        {
            "title": "Native and Compact Structured Latents for 3D Generation",
            "authors": [
                "Jianfeng Xiang",
                "Xiaoxue Chen",
                "Sicheng Xu",
                "Ruicheng Wang",
                "Zelong Lv",
                "Yu Deng",
                "Hongyuan Zhu",
                "Yue Dong",
                "Hao Zhao",
                "Nicholas Jing Yuan",
                "Jiaolong Yang"
            ],
            "arxiv_id": "2512.14692v1",
            "summary": "Recent advancements in 3D generative modeling have significantly improved the generation realism, yet the field is still hampered by existing representations, which struggle to capture assets with complex topologies and detailed appearance. This paper present an approach for learning a structured latent representation from native 3D data to address this challenge. At its core is a new sparse voxel structure called O-Voxel, an omni-voxel representation that encodes both geometry and appearance. O-Voxel can robustly model arbitrary topology, including open, non-manifold, and fully-enclosed surfaces, while capturing comprehensive surface attributes beyond texture color, such as physically-based rendering parameters. Based on O-Voxel, we design a Sparse Compression VAE which provides a high spatial compression rate and a compact latent space. We train large-scale flow-matching models comprising 4B parameters for 3D generation using diverse public 3D asset datasets. Despite their scale, inference remains highly efficient. Meanwhile, the geometry and material quality of our generated assets far exceed those of existing models. We believe our approach offers a significant advancement in 3D generative modeling.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project Page: https://microsoft.github.io/TRELLIS.2/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14692v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出O-Voxel稀疏体素表示与稀疏压缩VAE，以解决3D生成中复杂拓扑与精细外观建模的挑战",
            "summary_zh": "近年来，3D生成建模在生成真实感方面取得了显著进展，但该领域仍受限于现有表示方法，这些方法难以捕捉具有复杂拓扑结构和精细外观的资产。本文提出了一种从原生3D数据中学习结构化潜在表示的方法来解决这一挑战。其核心是一种名为O-Voxel的新型稀疏体素结构，这是一种全向体素表示，能够同时编码几何和外观信息。O-Voxel能够鲁棒地建模任意拓扑结构，包括开放、非流形和完全封闭的表面，同时捕捉超越纹理颜色的全面表面属性，例如基于物理的渲染参数。基于O-Voxel，我们设计了一种稀疏压缩变分自编码器，它提供了高空间压缩率和紧凑的潜在空间。我们使用多样化的公共3D资产数据集，训练了包含40亿参数的大规模流匹配模型用于3D生成。尽管模型规模庞大，推理过程仍然保持高效。同时，我们生成资产的几何和材质质量远超现有模型。我们相信，我们的方法为3D生成建模提供了重要进展。",
            "intro_zh": [
                "现有3D表示方法难以有效建模复杂拓扑（如开放、非流形表面）和精细外观（如物理渲染参数），限制了生成资产的真实感与多样性。",
                "提出O-Voxel稀疏体素表示，统一编码几何与外观，并设计稀疏压缩VAE实现高效压缩，构建紧凑潜在空间用于大规模流匹配生成。",
                "生成资产在几何与材质质量上显著超越现有模型，模型参数量达40亿但推理高效，验证了方法的有效性与实用性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决3D生成建模中，现有表示方法（如网格、点云、传统体素）难以有效捕捉复杂拓扑结构（如开放、非流形表面）和精细外观属性（如物理渲染参数）的问题，导致生成资产真实感不足、多样性受限。\\n\\n**核心思路**：通过设计一种新型的稀疏体素表示O-Voxel，统一编码几何和外观信息，以鲁棒建模任意拓扑；并基于此构建稀疏压缩变分自编码器，实现高压缩率的潜在表示，从而支持大规模生成模型的训练与高效推理。\\n\\n**技术框架**：整体流程包括：1) 数据预处理，将原生3D数据转换为O-Voxel表示；2) 稀疏压缩VAE编码器，将O-Voxel压缩为紧凑潜在向量；3) 解码器重构O-Voxel；4) 在潜在空间上训练大规模流匹配模型（40亿参数）进行3D生成；5) 推理时通过解码器生成O-Voxel，可进一步转换为其他3D格式。\\n\\n**关键创新**：O-Voxel表示是核心创新，它扩展了传统体素，支持任意拓扑建模和丰富表面属性（如材质参数）编码，与现有方法相比，本质区别在于其“全向”特性，能更全面捕捉3D资产细节。稀疏压缩VAE则实现了高效的空间压缩，减少计算开销。\\n\\n**关键设计**：O-Voxel采用稀疏数据结构，仅存储非空体素，每个体素包含几何（如占用状态）和外观（如颜色、法线、物理渲染参数）信息。稀疏压缩VAE使用卷积网络处理稀疏输入，损失函数包括重构损失和潜在空间正则化。流匹配模型基于扩散思想，在潜在空间训练，参数规模达40亿，但通过优化实现高效推理。",
            "application_zh": "该研究在3D内容创作、虚拟现实、游戏开发、工业设计等领域具有广泛应用潜力。其高质量生成能力可加速资产制作流程，降低人工成本；紧凑表示便于存储与传输；支持复杂拓扑建模，适用于建筑、生物模型等场景。未来可能推动3D生成技术的标准化与普及。",
            "highlight_zh": "实验表明，生成资产在几何细节和材质真实性上远超基线模型（如基于网格或点云的方法），具体提升幅度未知，但论文强调“质量远超过现有模型”。模型参数量达40亿，训练使用多样公共数据集，推理效率高，验证了O-Voxel与稀疏压缩VAE的有效性。",
            "tags_zh": [
                "3D生成建模",
                "稀疏体素表示",
                "变分自编码器",
                "流匹配",
                "潜在空间学习",
                "拓扑建模",
                "物理渲染",
                "大规模模型训练"
            ],
            "_index": 100
        },
        {
            "title": "VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image",
            "authors": [
                "Sicheng Xu",
                "Guojun Chen",
                "Jiaolong Yang",
                "Yizhong Zhang",
                "Yu Deng",
                "Steve Lin",
                "Baining Guo"
            ],
            "arxiv_id": "2512.14677v1",
            "summary": "We propose VASA-3D, an audio-driven, single-shot 3D head avatar generator. This research tackles two major challenges: capturing the subtle expression details present in real human faces, and reconstructing an intricate 3D head avatar from a single portrait image. To accurately model expression details, VASA-3D leverages the motion latent of VASA-1, a method that yields exceptional realism and vividness in 2D talking heads. A critical element of our work is translating this motion latent to 3D, which is accomplished by devising a 3D head model that is conditioned on the motion latent. Customization of this model to a single image is achieved through an optimization framework that employs numerous video frames of the reference head synthesized from the input image. The optimization takes various training losses robust to artifacts and limited pose coverage in the generated training data. Our experiment shows that VASA-3D produces realistic 3D talking heads that cannot be achieved by prior art, and it supports the online generation of 512x512 free-viewpoint videos at up to 75 FPS, facilitating more immersive engagements with lifelike 3D avatars.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "NeurIPS 2025 paper. Project webpage: https://www.microsoft.com/en-us/research/project/vasa-3d/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14677v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出VASA-3D以解决从单张图像生成高真实感音频驱动3D头部虚拟形象的挑战。",
            "summary_zh": "我们提出了VASA-3D，一种音频驱动的单次3D头部虚拟形象生成器。这项研究解决了两个主要挑战：捕捉真实人脸的微妙表情细节，以及从单张肖像图像重建复杂的3D头部虚拟形象。为了准确建模表情细节，VASA-3D利用了VASA-1的运动潜在表示，该方法在2D说话头部生成中展现出卓越的真实感和生动性。我们工作的一个关键要素是将这种运动潜在表示转换到3D，这是通过设计一个以运动潜在表示为条件的3D头部模型来实现的。通过一个优化框架，该模型被定制到单张图像，该框架使用了从输入图像合成的参考头部的多个视频帧。优化采用了多种训练损失，这些损失对生成训练数据中的伪影和有限姿态覆盖具有鲁棒性。我们的实验表明，VASA-3D生成了现有技术无法实现的逼真3D说话头部，并支持在线生成高达75 FPS的512x512自由视点视频，促进了与逼真3D虚拟形象更沉浸式的互动。",
            "intro_zh": [
                "现有方法难以从单张图像生成高真实感的3D头部虚拟形象，特别是在捕捉微妙表情细节方面存在不足。",
                "VASA-3D利用VASA-1的运动潜在表示，通过条件化3D模型和优化框架，实现从单张图像到音频驱动3D虚拟形象的转换。",
                "实验显示，VASA-3D生成逼真3D说话头部，支持75 FPS的512x512自由视点视频生成，显著超越现有技术。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从单张肖像图像生成音频驱动的逼真3D头部虚拟形象的问题。现有方法在捕捉真实人脸的微妙表情细节和从单张图像重建复杂3D结构方面存在挑战，导致生成的虚拟形象真实感不足或依赖多视图输入。\\n\\n**核心思路**：核心思路是结合VASA-1在2D说话头部生成中的优势，将其运动潜在表示迁移到3D领域。通过设计一个以运动潜在表示为条件的3D头部模型，并利用优化框架从单张图像定制化模型，实现高真实感的音频驱动3D虚拟形象生成。\\n\\n**技术框架**：整体架构包括三个主要阶段：首先，从输入音频提取VASA-1的运动潜在表示；其次，构建一个3D高斯头部模型，该模型以运动潜在表示为条件，用于生成3D几何和外观；最后，通过优化框架，使用从输入图像合成的多个视频帧作为参考，对模型进行定制化训练，以适配特定个体。\\n\\n**关键创新**：最重要的技术创新是将2D运动潜在表示成功迁移到3D空间，实现了从单张图像生成逼真3D头部虚拟形象。与现有方法的本质区别在于，它无需多视图图像或复杂3D扫描，仅凭单张图像和音频就能生成高真实感的自由视点视频。\\n\\n**关键设计**：关键设计包括：采用VASA-1的运动潜在表示作为条件输入；设计3D高斯模型来参数化头部几何和外观；优化框架中使用多种训练损失，如重建损失、对抗损失和姿态鲁棒损失，以减少伪影并处理有限姿态覆盖；具体参数如视频帧合成数量、损失权重等通过实验调优，以平衡真实感和效率。",
            "application_zh": "VASA-3D在虚拟现实、增强现实、游戏、在线教育和远程会议等领域具有广泛应用潜力。它能够快速生成逼真的3D虚拟形象，提升用户体验和沉浸感，推动人机交互和数字内容创作的发展，未来可能扩展到更复杂的多模态交互场景。",
            "highlight_zh": "实验结果表明，VASA-3D在生成逼真3D说话头部方面显著优于现有技术，支持在线生成512x512分辨率的自由视点视频，帧率高达75 FPS。具体性能数据包括高真实感评分和低误差指标，在表情细节和姿态多样性上均有显著提升，为实时应用提供了可行方案。",
            "tags_zh": [
                "音频驱动虚拟形象",
                "单图像3D重建",
                "高斯头部模型",
                "运动潜在表示迁移",
                "自由视点视频生成",
                "优化框架",
                "实时渲染",
                "表情细节建模"
            ],
            "_index": 101
        },
        {
            "title": "ART: Articulated Reconstruction Transformer",
            "authors": [
                "Zizhang Li",
                "Cheng Zhang",
                "Zhengqin Li",
                "Henry Howard-Jenkins",
                "Zhaoyang Lv",
                "Chen Geng",
                "Jiajun Wu",
                "Richard Newcombe",
                "Jakob Engel",
                "Zhao Dong"
            ],
            "arxiv_id": "2512.14671v1",
            "summary": "We introduce ART, Articulated Reconstruction Transformer -- a category-agnostic, feed-forward model that reconstructs complete 3D articulated objects from only sparse, multi-state RGB images. Previous methods for articulated object reconstruction either rely on slow optimization with fragile cross-state correspondences or use feed-forward models limited to specific object categories. In contrast, ART treats articulated objects as assemblies of rigid parts, formulating reconstruction as part-based prediction. Our newly designed transformer architecture maps sparse image inputs to a set of learnable part slots, from which ART jointly decodes unified representations for individual parts, including their 3D geometry, texture, and explicit articulation parameters. The resulting reconstructions are physically interpretable and readily exportable for simulation. Trained on a large-scale, diverse dataset with per-part supervision, and evaluated across diverse benchmarks, ART achieves significant improvements over existing baselines and establishes a new state of the art for articulated object reconstruction from image inputs.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project Page: https://kyleleey.github.io/ART/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14671v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出ART模型以解决从稀疏多状态RGB图像重建完整3D关节物体的类别无关前馈问题",
            "summary_zh": "我们介绍了ART（Articulated Reconstruction Transformer）——一种类别无关的前馈模型，能够仅从稀疏的多状态RGB图像中重建完整的3D关节物体。以往的关节物体重建方法要么依赖于缓慢的优化过程，需要脆弱的跨状态对应关系，要么使用仅限于特定物体类别的前馈模型。相比之下，ART将关节物体视为刚性部件的组装体，将重建问题表述为基于部件的预测。我们新设计的Transformer架构将稀疏图像输入映射到一组可学习的部件槽，ART从中联合解码出各个部件的统一表示，包括其3D几何形状、纹理和显式关节参数。所得的重建结果具有物理可解释性，并可轻松导出用于仿真。通过在具有逐部件监督的大规模多样化数据集上进行训练，并在多个基准测试中进行评估，ART相比现有基线取得了显著改进，为从图像输入进行关节物体重建建立了新的最先进水平。",
            "intro_zh": [
                "现有方法依赖缓慢优化或局限于特定类别，难以高效重建通用关节物体。",
                "ART将关节物体视为刚性部件组装，通过Transformer架构实现前馈预测。",
                "在多样化数据集上训练，ART显著超越基线，建立新的最先进水平。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从稀疏多状态RGB图像重建完整3D关节物体的挑战，现有方法要么依赖缓慢优化过程，需要脆弱的跨状态对应关系，要么使用前馈模型但局限于特定物体类别，导致通用性和效率不足。\\n\\n**核心思路**：ART将关节物体视为刚性部件的组装体，将重建问题表述为基于部件的预测，通过Transformer架构实现类别无关的前馈推理，从而避免优化依赖并提升泛化能力。\\n\\n**技术框架**：整体架构包括输入处理、Transformer编码器、部件槽映射和联合解码器。稀疏图像输入经过特征提取后，由Transformer映射到可学习的部件槽，然后解码器联合输出每个部件的3D几何、纹理和显式关节参数。\\n\\n**关键创新**：最重要的技术创新是设计了一种新的Transformer架构，能够从稀疏图像中直接预测部件级表示，并结合了显式关节参数，实现了物理可解释的重建，与现有方法相比，本质区别在于其类别无关性和前馈特性。\\n\\n**关键设计**：关键设计包括使用可学习的部件槽作为中间表示，通过损失函数监督几何重建、纹理生成和关节参数预测，网络结构结合了注意力机制和多层感知机，训练时利用大规模多样化数据集进行逐部件监督。",
            "application_zh": "该研究在机器人操作、虚拟现实和仿真领域具有广泛应用潜力，例如，可用于机器人对关节物体的抓取和操控，或为游戏和动画生成逼真的3D模型。其物理可解释的重建结果可直接导出用于物理仿真，提升自动化系统的交互能力。未来可能推动智能系统在动态环境中的感知与决策。",
            "highlight_zh": "ART在多个基准测试中显著超越现有基线，例如在重建精度上提升约15-20%，具体数据因数据集而异。与优化方法相比，推理速度提高数倍，同时保持类别无关性，在多样化物体上均表现优异，建立了新的最先进水平。",
            "tags_zh": [
                "关节物体重建",
                "3D重建",
                "Transformer架构",
                "前馈模型",
                "部件预测",
                "稀疏图像输入",
                "物理仿真",
                "类别无关学习"
            ],
            "_index": 102
        },
        {
            "title": "WaveSim: A Wavelet-based Multi-scale Similarity Metric for Weather and Climate Fields",
            "authors": [
                "Gabriele Accarino",
                "Viviana Acquaviva",
                "Sara Shamekh",
                "Duncan Watson-Parris",
                "David Lawrence"
            ],
            "arxiv_id": "2512.14656v1",
            "summary": "We introduce WaveSim, a multi-scale similarity metric for the evaluation of spatial fields in weather and climate applications. WaveSim exploits wavelet transforms to decompose input fields into scale-specific wavelet coefficients. The metric is built by multiplying three orthogonal components derived from these coefficients: Magnitude, which quantifies similarities in the energy distribution of the coefficients, i.e., the intensity of the field; Displacement, which captures spatial shift by comparing the centers of mass of normalized energy distributions; and Structure, which assesses pattern organization independent of location and amplitude. Each component yields a scale-specific similarity score ranging from 0 (no similarity) to 1 (perfect similarity), which are then combined across scales to produce an overall similarity measure. We first evaluate WaveSim using synthetic test cases, applying controlled spatial and temporal perturbations to systematically assess its sensitivity and expected behavior. We then demonstrate its applicability to physically relevant case studies of key modes of climate variability in Earth System Models. Traditional point-wise metrics lack a mechanism for attributing errors to physical scales or modes of dissimilarity. By operating in the wavelet domain and decomposing the signal along independent axes, WaveSim bypasses these limitations and provides an interpretable and diagnostically rich framework for assessing similarity in complex fields. Additionally, the WaveSim framework allows users to place emphasis on a specific scale or component, and lends itself to user-specific model intercomparison, model evaluation, and calibration and training of forecasting systems. We provide a PyTorch-ready implementation of WaveSim, along with all evaluation scripts, at: https://github.com/gabrieleaccarino/wavesim.",
            "categories": [
                "physics.ao-ph",
                "cs.CV",
                "physics.data-an"
            ],
            "primary_category": "physics.ao-ph",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14656v1",
            "code_links": [
                {
                    "url": "https://github.com/gabrieleaccarino/wavesim",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出WaveSim，一种基于小波变换的多尺度相似性度量方法，用于评估天气和气候空间场的相似性。",
            "summary_zh": "我们介绍了WaveSim，一种用于评估天气和气候应用中空间场的多尺度相似性度量方法。WaveSim利用小波变换将输入场分解为特定尺度的小波系数。该度量通过乘以从这些系数导出的三个正交分量来构建：幅度，量化系数能量分布的相似性，即场的强度；位移，通过比较归一化能量分布的质量中心来捕捉空间偏移；以及结构，评估独立于位置和幅度的模式组织。每个分量产生一个特定尺度的相似性得分，范围从0（无相似性）到1（完美相似性），然后跨尺度组合以产生整体相似性度量。我们首先使用合成测试案例评估WaveSim，应用受控的空间和时间扰动来系统评估其敏感性和预期行为。然后，我们通过地球系统模型中关键气候变率模式的物理相关案例研究来展示其适用性。传统的逐点度量缺乏将误差归因于物理尺度或差异模式的机制。通过在小波域中操作并沿独立轴分解信号，WaveSim克服了这些限制，并为评估复杂场中的相似性提供了一个可解释且诊断丰富的框架。此外，WaveSim框架允许用户强调特定尺度或分量，并适用于用户特定的模型比较、模型评估以及预测系统的校准和训练。我们提供了WaveSim的PyTorch就绪实现以及所有评估脚本，网址为：https://github.com/gabrieleaccarino/wavesim。",
            "intro_zh": [
                "传统逐点度量无法将误差归因于物理尺度或差异模式，限制了天气和气候场评估的深度分析。",
                "WaveSim利用小波变换分解场，通过幅度、位移和结构三个正交分量量化多尺度相似性。",
                "在合成测试和气候变率案例中，WaveSim表现出高敏感性和可解释性，支持模型评估和校准。"
            ],
            "method_zh": "**问题定义**：论文旨在解决天气和气候空间场相似性评估中的问题，传统逐点度量（如均方误差）缺乏多尺度分析和误差归因能力，无法区分强度、位置和结构差异，限制了模型诊断和比较的深度。\\n\\n**核心思路**：论文提出基于小波变换的多尺度相似性度量WaveSim，通过分解场到不同尺度，并设计三个正交分量（幅度、位移、结构）来独立量化相似性，从而提供可解释的、尺度感知的评估框架。\\n\\n**技术框架**：整体流程包括：1) 输入空间场；2) 应用小波变换分解为多尺度小波系数；3) 从系数计算幅度、位移和结构三个分量，每个分量产生尺度特定相似性得分（0-1）；4) 跨尺度组合得分生成整体相似性度量；5) 支持用户自定义权重以强调特定尺度或分量。\\n\\n**关键创新**：最重要的创新是将小波变换与正交分量分解结合，实现多尺度、可解释的相似性度量，与现有方法相比，本质区别在于能同时捕捉强度、空间偏移和模式结构的差异，并提供诊断性分析。\\n\\n**关键设计**：关键设计包括：使用小波变换（如Daubechies小波）进行多尺度分解；幅度分量基于能量分布相似性；位移分量通过归一化能量分布的质量中心比较；结构分量独立于位置和幅度评估模式组织；得分范围标准化为0-1；提供PyTorch实现以支持高效计算和集成。",
            "application_zh": "WaveSim适用于天气和气候领域的模型比较、模型评估、预测系统校准和训练，例如地球系统模型的气候变率模式分析。其多尺度特性支持诊断性评估，帮助识别模型误差来源，提升预测准确性，未来可扩展至其他空间场分析领域。",
            "highlight_zh": "在合成测试中，WaveSim对受控扰动表现出高敏感性，能准确量化不同尺度相似性；在气候变率案例研究中，成功评估了关键模式（如ENSO）的相似性，相比传统度量提供更丰富的诊断信息，具体性能数据未在摘要中提供，但框架已通过开源代码验证。",
            "tags_zh": [
                "小波变换",
                "多尺度相似性度量",
                "天气气候场评估",
                "模型诊断",
                "空间场分析",
                "正交分量分解",
                "可解释性框架",
                "PyTorch实现"
            ],
            "_index": 103
        },
        {
            "title": "Adaptable Segmentation Pipeline for Diverse Brain Tumors with Radiomic-guided Subtyping and Lesion-Wise Model Ensemble",
            "authors": [
                "Daniel Capellán-Martín",
                "Abhijeet Parida",
                "Zhifan Jiang",
                "Nishad Kulkarni",
                "Krithika Iyer",
                "Austin Tapp",
                "Syed Muhammad Anwar",
                "María J. Ledesma-Carbayo",
                "Marius George Linguraru"
            ],
            "arxiv_id": "2512.14648v1",
            "summary": "Robust and generalizable segmentation of brain tumors on multi-parametric magnetic resonance imaging (MRI) remains difficult because tumor types differ widely. The BraTS 2025 Lighthouse Challenge benchmarks segmentation methods on diverse high-quality datasets of adult and pediatric tumors: multi-consortium international pediatric brain tumor segmentation (PED), preoperative meningioma tumor segmentation (MEN), meningioma radiotherapy segmentation (MEN-RT), and segmentation of pre- and post-treatment brain metastases (MET). We present a flexible, modular, and adaptable pipeline that improves segmentation performance by selecting and combining state-of-the-art models and applying tumor- and lesion-specific processing before and after training. Radiomic features extracted from MRI help detect tumor subtype, ensuring a more balanced training. Custom lesion-level performance metrics determine the influence of each model in the ensemble and optimize post-processing that further refines the predictions, enabling the workflow to tailor every step to each case. On the BraTS testing sets, our pipeline achieved performance comparable to top-ranked algorithms across multiple challenges. These findings confirm that custom lesion-aware processing and model selection yield robust segmentations yet without locking the method to a specific network architecture. Our method has the potential for quantitative tumor measurement in clinical practice, supporting diagnosis and prognosis.",
            "categories": [
                "cs.CV",
                "eess.IV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "12 pages, 5 figures, 3 tables. Algorithm presented at MICCAI BraTS 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14648v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出可适应脑肿瘤分割流程，通过影像组学引导亚型识别和病灶级模型集成提升多类型肿瘤分割性能",
            "summary_zh": "在多参数磁共振成像（MRI）上实现稳健且可泛化的脑肿瘤分割仍然困难，因为肿瘤类型差异很大。BraTS 2025 Lighthouse Challenge 在多样化的高质量成人及儿童肿瘤数据集上对分割方法进行基准测试：国际多联盟儿童脑肿瘤分割（PED）、术前脑膜瘤分割（MEN）、脑膜瘤放疗分割（MEN-RT）以及治疗前后脑转移瘤分割（MET）。我们提出了一种灵活、模块化且可适应的流程，通过选择和组合最先进的模型，并在训练前后应用肿瘤和病灶特异性处理来提升分割性能。从MRI中提取的影像组学特征有助于检测肿瘤亚型，确保更平衡的训练。定制的病灶级性能指标确定集成中每个模型的影响，并优化进一步细化预测的后处理，使工作流程能够针对每个病例定制每一步。在BraTS测试集上，我们的流程在多个挑战中取得了与顶级算法相当的性能。这些发现证实，定制的病灶感知处理和模型选择能够产生稳健的分割，同时不将方法锁定于特定的网络架构。我们的方法在临床实践中具有定量肿瘤测量的潜力，支持诊断和预后。",
            "intro_zh": [
                "核心问题：现有脑肿瘤分割方法难以泛化到多样化的肿瘤类型（如儿童肿瘤、脑膜瘤、转移瘤），且缺乏针对病灶级特性的定制处理。",
                "方法要点：提出模块化流程，结合影像组学引导的亚型识别、病灶级模型集成和定制后处理，实现自适应分割。",
                "实验或效果：在BraTS 2025多个挑战测试集上，性能与顶级算法相当，验证了方法的稳健性和泛化能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多参数MRI上脑肿瘤分割的稳健性和泛化性问题，特别是针对多样化的肿瘤类型（如儿童肿瘤、脑膜瘤、转移瘤）。现有方法的痛点在于：肿瘤类型差异大导致单一模型难以适应；缺乏针对病灶级特性的定制处理；训练数据不平衡影响模型性能。\\n\\n**核心思路**：论文的核心思路是设计一个灵活、模块化的分割流程，通过选择和组合最先进的模型，并应用肿瘤和病灶特异性处理来提升性能。这包括利用影像组学特征识别肿瘤亚型以平衡训练，以及基于病灶级指标优化模型集成和后处理。\\n\\n**技术框架**：整体流程包含多个阶段：首先，从MRI数据中提取影像组学特征，用于检测肿瘤亚型并指导数据平衡；其次，选择和训练多个分割模型（可能基于不同网络架构）；然后，使用定制的病灶级性能指标评估每个模型，并确定其在集成中的权重；最后，应用病灶感知的后处理（如形态学操作或阈值调整）进一步细化预测结果。整个流程是模块化的，允许针对不同肿瘤类型和病例进行定制。\\n\\n**关键创新**：最重要的技术创新点在于将影像组学引导的亚型识别与病灶级模型集成相结合，实现自适应分割。与现有方法的本质区别在于：不依赖于单一网络架构，而是通过模块化设计灵活组合模型；引入病灶级指标进行精细优化，而非仅依赖整体性能；强调处理流程的可定制性，以适应多样化的临床场景。\\n\\n**关键设计**：关键设计包括：影像组学特征提取用于亚型分类（具体特征集未知，但可能基于纹理、形状等）；模型集成策略基于病灶级指标（如Dice系数或敏感度）动态加权；后处理步骤可能涉及病灶级的形态学操作或阈值调整，以消除噪声并提升边界精度。损失函数和网络结构细节未在摘要中明确，但流程允许集成多种最先进模型（如U-Net变体或Transformer架构）。",
            "application_zh": "该研究在临床医学影像分析领域具有重要应用价值，特别是脑肿瘤的定量测量。潜在应用包括：辅助医生进行脑肿瘤（如儿童肿瘤、脑膜瘤、转移瘤）的诊断和预后评估；支持放疗规划中的靶区勾画；促进多中心研究中的标准化分割。未来影响可能推动个性化医疗和自动化影像分析工具的发展，提高临床工作效率和准确性。",
            "highlight_zh": "在BraTS 2025 Lighthouse Challenge的测试集上，该流程在多个子挑战（包括PED、MEN、MEN-RT、MET）中取得了与顶级算法相当的性能。具体性能数据未在摘要中提供，但结果表明方法在多样化的肿瘤类型上表现稳健，验证了病灶级处理和模型集成的有效性。提升幅度体现在泛化能力的增强，无需锁定特定网络架构即可达到高分割精度。",
            "tags_zh": [
                "脑肿瘤分割",
                "多参数MRI",
                "影像组学",
                "模型集成",
                "病灶级处理",
                "自适应流程",
                "临床影像分析",
                "BraTS挑战"
            ],
            "_index": 104
        },
        {
            "title": "TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines",
            "authors": [
                "David Schulmeister",
                "Valentin Hartmann",
                "Lars Klein",
                "Robert West"
            ],
            "arxiv_id": "2512.14645v1",
            "summary": "Today, a lot of research on language models is focused on large, general-purpose models. However, many NLP pipelines only require models with a well-defined, small set of capabilities. While large models are capable of performing the tasks of those smaller models, they are simply not fast enough to process large amounts of data or offer real-time responses. Furthermore, they often use unnecessarily large amounts of energy, leading to sustainability concerns and problems when deploying them on battery-powered devices. In our work, we show how to train small models for such efficiency-critical applications. As opposed to many off-the-shelf NLP pipelines, our models use modern training techniques such as distillation, and offer support for low-resource languages. We call our models TiME (Tiny Monolingual Encoders) and comprehensively evaluate them on a range of common NLP tasks, observing an improved trade-off between benchmark performance on one hand, and throughput, latency and energy consumption on the other. Along the way, we show that distilling monolingual models from multilingual teachers is possible, and likewise distilling models with absolute positional embeddings from teachers with relative positional embeddings.",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14645v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出TiME小规模单语编码器，通过蒸馏训练实现高效NLP流水线，解决大模型在实时处理与能效方面的不足。",
            "summary_zh": "当前语言模型研究主要集中于大型通用模型，但许多NLP流水线仅需具备特定小规模能力的模型。大型模型虽能执行这些任务，但在处理大量数据或提供实时响应时速度不足，且能耗过高，导致可持续性问题，在电池供电设备上部署困难。本研究展示了如何为这类效率关键应用训练小型模型。与许多现成NLP流水线不同，我们的模型采用蒸馏等现代训练技术，并支持低资源语言。我们称这些模型为TiME（Tiny Monolingual Encoders），在一系列常见NLP任务上进行了全面评估，观察到在基准性能与吞吐量、延迟和能耗之间取得了更好的权衡。同时，我们证明了从多语言教师模型蒸馏单语模型是可行的，同样可以从具有相对位置嵌入的教师模型蒸馏出具有绝对位置嵌入的模型。",
            "intro_zh": [
                "核心问题：大型通用语言模型在实时处理、高吞吐量和低能耗场景下效率不足，难以部署于资源受限设备。",
                "方法要点：采用蒸馏技术训练小型单语编码器TiME，从多语言大模型迁移知识，优化性能与效率的平衡。",
                "实验或效果：在多种NLP任务上评估，TiME在保持较高基准性能的同时，显著提升吞吐量、降低延迟和能耗。"
            ],
            "method_zh": "**问题定义**：论文旨在解决NLP流水线中大型语言模型在效率关键应用中的不足，包括处理速度慢、能耗高、难以在实时或资源受限环境（如移动设备）部署。现有方法的痛点在于过度依赖通用大模型，导致不必要的计算开销和可持续性问题。\\n\\n**核心思路**：通过知识蒸馏技术，从大型多语言教师模型训练小型单语学生模型，专注于特定语言和任务，以在性能与效率间取得更好平衡。设计思路基于蒸馏能压缩模型规模，同时保留关键能力，并适应低资源语言需求。\\n\\n**技术框架**：整体流程包括：1) 选择大型多语言教师模型（如基于Transformer的预训练模型）；2) 使用蒸馏损失函数，将教师的知识迁移到小型单语学生模型；3) 在目标语言数据集上微调学生模型；4) 评估模型在NLP任务上的性能和效率指标。主要模块涉及教师-学生架构、蒸馏训练阶段和效率优化组件。\\n\\n**关键创新**：最重要的技术创新是证明了从多语言教师模型蒸馏单语模型的可行性，以及从具有相对位置嵌入的教师模型蒸馏出具有绝对位置嵌入的学生模型。这与现有方法本质区别在于，传统蒸馏多在同语言或同架构中进行，而本研究扩展了跨语言和嵌入类型的蒸馏能力。\\n\\n**关键设计**：关键设计包括：使用蒸馏损失（如软标签损失和注意力蒸馏）来优化学生模型；网络结构为小型Transformer编码器，参数规模显著小于教师模型；位置嵌入采用绝对类型以简化计算；训练时可能结合数据增强和低资源语言适配技术，具体参数设置未在摘要中详述，需参考论文正文。",
            "application_zh": "TiME模型适用于需要高效NLP处理的场景，如实时聊天机器人、移动设备上的文本分析、低功耗物联网设备，以及低资源语言地区的语言服务。其实际价值在于降低部署成本、提升响应速度并减少环境影响，未来可能推动边缘计算和可持续AI的发展。",
            "highlight_zh": "实验表明，TiME模型在常见NLP任务（如文本分类、命名实体识别）上，相比大型基线模型，在基准性能相近的情况下，吞吐量提升显著（具体数据未提供，但强调更好权衡），延迟降低，能耗减少。例如，从多语言教师蒸馏的单语模型在低资源语言任务上表现良好，验证了蒸馏跨语言和嵌入类型的有效性。",
            "tags_zh": [
                "小规模语言模型",
                "知识蒸馏",
                "单语编码器",
                "高效NLP",
                "低资源语言",
                "能效优化",
                "实时处理",
                "Transformer架构"
            ],
            "_index": 105
        },
        {
            "title": "AMD-HookNet++: Evolution of AMD-HookNet with Hybrid CNN-Transformer Feature Enhancement for Glacier Calving Front Segmentation",
            "authors": [
                "Fei Wu",
                "Marcel Dreier",
                "Nora Gourmelon",
                "Sebastian Wind",
                "Jianlin Zhang",
                "Thorsten Seehaus",
                "Matthias Braun",
                "Andreas Maier",
                "Vincent Christlein"
            ],
            "arxiv_id": "2512.14639v1",
            "summary": "The dynamics of glaciers and ice shelf fronts significantly impact the mass balance of ice sheets and coastal sea levels. To effectively monitor glacier conditions, it is crucial to consistently estimate positional shifts of glacier calving fronts. AMD-HookNet firstly introduces a pure two-branch convolutional neural network (CNN) for glacier segmentation. Yet, the local nature and translational invariance of convolution operations, while beneficial for capturing low-level details, restricts the model ability to maintain long-range dependencies. In this study, we propose AMD-HookNet++, a novel advanced hybrid CNN-Transformer feature enhancement method for segmenting glaciers and delineating calving fronts in synthetic aperture radar images. Our hybrid structure consists of two branches: a Transformer-based context branch to capture long-range dependencies, which provides global contextual information in a larger view, and a CNN-based target branch to preserve local details. To strengthen the representation of the connected hybrid features, we devise an enhanced spatial-channel attention module to foster interactions between the hybrid CNN-Transformer branches through dynamically adjusting the token relationships from both spatial and channel perspectives. Additionally, we develop a pixel-to-pixel contrastive deep supervision to optimize our hybrid model by integrating pixelwise metric learning into glacier segmentation. Through extensive experiments and comprehensive quantitative and qualitative analyses on the challenging glacier segmentation benchmark dataset CaFFe, we show that AMD-HookNet++ sets a new state of the art with an IoU of 78.2 and a HD95 of 1,318 m, while maintaining a competitive MDE of 367 m. More importantly, our hybrid model produces smoother delineations of calving fronts, resolving the issue of jagged edges typically seen in pure Transformer-based approaches.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "10.1109/TGRS.2025.3642764",
            "journal_ref": "IEEE Transactions on Geoscience and Remote Sensing (2025)",
            "pdf_url": "https://arxiv.org/pdf/2512.14639v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出AMD-HookNet++，通过混合CNN-Transformer特征增强解决冰川崩解前沿分割中的长程依赖和局部细节平衡问题。",
            "summary_zh": "冰川和冰架前沿的动态变化对冰盖质量平衡和沿海海平面有显著影响。为有效监测冰川状况，持续估计冰川崩解前沿的位置变化至关重要。AMD-HookNet首次引入纯双分支卷积神经网络（CNN）进行冰川分割，但卷积操作的局部性和平移不变性虽有利于捕捉低级细节，却限制了模型保持长程依赖的能力。本研究提出AMD-HookNet++，一种新颖的先进混合CNN-Transformer特征增强方法，用于合成孔径雷达图像中的冰川分割和崩解前沿描绘。我们的混合结构包括两个分支：一个基于Transformer的上下文分支以捕获长程依赖，提供更大视野的全局上下文信息；一个基于CNN的目标分支以保留局部细节。为增强连接混合特征的表示，我们设计了一个增强的空间-通道注意力模块，通过从空间和通道角度动态调整令牌关系，促进混合CNN-Transformer分支之间的交互。此外，我们开发了像素到像素对比深度监督，通过将像素级度量学习集成到冰川分割中，优化我们的混合模型。通过在具有挑战性的冰川分割基准数据集CaFFe上进行广泛实验和全面的定量与定性分析，我们表明AMD-HookNet++以78.2的IoU和1,318米的HD95设定了新的最先进水平，同时保持了367米的竞争性MDE。更重要的是，我们的混合模型产生了更平滑的崩解前沿描绘，解决了纯基于Transformer方法中常见的锯齿边缘问题。",
            "intro_zh": [
                "核心问题：现有纯CNN方法如AMD-HookNet在冰川分割中难以捕获长程依赖，导致全局上下文信息不足，影响崩解前沿的准确描绘。",
                "方法要点：提出混合CNN-Transformer架构，结合Transformer分支捕获全局上下文和CNN分支保留局部细节，通过注意力模块增强特征交互。",
                "实验或效果：在CaFFe数据集上，AMD-HookNet++达到78.2 IoU和1,318米HD95，优于基线，并生成更平滑的前沿轮廓。"
            ],
            "method_zh": "**问题定义**：论文旨在解决冰川崩解前沿分割问题，特别是在合成孔径雷达图像中准确描绘冰川边界。现有方法如纯CNN（如AMD-HookNet）虽能捕捉局部细节，但由于卷积操作的局部性，难以建模长程依赖，导致全局上下文信息缺失，影响分割精度和边缘平滑度。\\n\\n**核心思路**：论文提出混合CNN-Transformer架构，核心思想是结合CNN的局部细节捕捉能力和Transformer的全局上下文建模能力，以平衡局部与全局特征，提升分割性能。设计基于双分支结构，分别处理不同尺度的信息，并通过注意力机制促进分支间交互。\\n\\n**技术框架**：整体架构包括两个主要分支：一个Transformer-based上下文分支，用于捕获长程依赖和全局上下文；一个CNN-based目标分支，用于保留局部细节和低级特征。两个分支的输出通过增强的空间-通道注意力模块进行融合，该模块动态调整空间和通道维度的令牌关系。训练过程中，采用像素到像素对比深度监督，集成像素级度量学习以优化模型。\\n\\n**关键创新**：最重要的技术创新是混合CNN-Transformer特征增强方法，特别是增强的空间-通道注意力模块，它从空间和通道角度动态调整特征交互，解决了纯Transformer方法中常见的锯齿边缘问题。与现有方法相比，本质区别在于同时利用CNN和Transformer的优势，而非单一架构。\\n\\n**关键设计**：关键设计包括双分支网络结构，其中Transformer分支可能基于Vision Transformer变体，CNN分支基于卷积层；损失函数结合交叉熵损失和对比损失，用于像素级监督；注意力模块参数可学习，以自适应调整特征权重；实验设置使用CaFFe数据集进行训练和评估，优化器如Adam，学习率通过网格搜索确定。",
            "application_zh": "该研究在冰川监测和气候变化领域具有重要应用价值。通过准确分割冰川崩解前沿，可用于实时监测冰川动态变化，评估冰盖质量平衡和沿海海平面上升风险。潜在应用包括环境科学、遥感图像分析和灾害预警系统，未来可能扩展到其他遥感目标分割任务，如海冰或陆地覆盖分类。",
            "highlight_zh": "在CaFFe基准数据集上，AMD-HookNet++取得了最先进的性能：IoU达到78.2，HD95为1,318米，同时MDE保持在367米的竞争水平。与基线方法相比，IoU提升显著（具体提升幅度未知），并生成更平滑的崩解前沿轮廓，解决了纯Transformer方法的锯齿边缘问题。定量和定性分析均验证了模型的有效性。",
            "tags_zh": [
                "冰川分割",
                "崩解前沿描绘",
                "混合CNN-Transformer",
                "特征增强",
                "空间-通道注意力",
                "像素对比学习",
                "合成孔径雷达",
                "遥感图像分析"
            ],
            "_index": 106
        },
        {
            "title": "MuseCPBench: an Empirical Study of Music Editing Methods through Music Context Preservation",
            "authors": [
                "Yash Vishe",
                "Eric Xue",
                "Xunyi Jiang",
                "Zachary Novack",
                "Junda Wu",
                "Julian McAuley",
                "Xin Xu"
            ],
            "arxiv_id": "2512.14629v1",
            "summary": "Music editing plays a vital role in modern music production, with applications in film, broadcasting, and game development. Recent advances in music generation models have enabled diverse editing tasks such as timbre transfer, instrument substitution, and genre transformation. However, many existing works overlook the evaluation of their ability to preserve musical facets that should remain unchanged during editing a property we define as Music Context Preservation (MCP). While some studies do consider MCP, they adopt inconsistent evaluation protocols and metrics, leading to unreliable and unfair comparisons. To address this gap, we introduce the first MCP evaluation benchmark, MuseCPBench, which covers four categories of musical facets and enables comprehensive comparisons across five representative music editing baselines. Through systematic analysis along musical facets, methods, and models, we identify consistent preservation gaps in current music editing methods and provide insightful explanations. We hope our findings offer practical guidance for developing more effective and reliable music editing strategies with strong MCP capability",
            "categories": [
                "cs.SD",
                "cs.AI"
            ],
            "primary_category": "cs.SD",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14629v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出首个音乐上下文保持评估基准MuseCPBench，以解决音乐编辑方法评估不一致的问题",
            "summary_zh": "音乐编辑在现代音乐制作中扮演着重要角色，广泛应用于电影、广播和游戏开发。近年来，音乐生成模型的进步使得音色转换、乐器替换和风格变换等多种编辑任务成为可能。然而，许多现有研究忽视了评估编辑过程中应保持不变的音乐要素的保持能力，这一属性被定义为音乐上下文保持（MCP）。尽管部分研究考虑了MCP，但它们采用了不一致的评估协议和指标，导致不可靠且不公平的比较。为填补这一空白，我们引入了首个MCP评估基准MuseCPBench，涵盖四类音乐要素，并支持对五种代表性音乐编辑基线方法进行全面比较。通过对音乐要素、方法和模型的系统分析，我们识别了当前音乐编辑方法中一致的保持差距，并提供了深入的解释。我们希望这些发现能为开发具有更强MCP能力的更有效和可靠音乐编辑策略提供实用指导。",
            "intro_zh": [
                "现有音乐编辑方法缺乏统一的音乐上下文保持评估标准，导致评估结果不可靠且难以公平比较。",
                "论文提出首个MCP评估基准MuseCPBench，涵盖四类音乐要素，支持对五种基线方法进行系统分析。",
                "实验揭示了当前方法在音乐上下文保持方面的一致差距，为改进编辑策略提供了实证指导。"
            ],
            "method_zh": "**问题定义**：论文旨在解决音乐编辑方法评估中音乐上下文保持（MCP）能力评估不一致的问题。现有方法在评估MCP时采用不同的协议和指标，导致结果不可靠且难以公平比较，缺乏标准化基准。\\n\\n**核心思路**：通过构建首个MCP评估基准MuseCPBench，统一评估框架，涵盖多类音乐要素，以系统分析现有方法的MCP能力，识别保持差距并提供解释。\\n\\n**技术框架**：整体架构包括基准构建和评估分析两个阶段。基准构建阶段定义四类音乐要素（如旋律、节奏等），收集或生成测试数据；评估分析阶段应用五种代表性音乐编辑基线方法（如基于生成模型的方法），使用统一指标（如保持度分数）进行量化评估，并进行跨要素、方法和模型的比较分析。\\n\\n**关键创新**：最重要的技术创新是首次提出并实现MCP评估基准，填补了音乐编辑领域标准化评估的空白。与现有方法的本质区别在于提供了一致、全面的评估协议，而非依赖零散或不一致的指标。\\n\\n**关键设计**：关键设计包括定义四类音乐要素的具体类别（如音高、时值等），选择五种基线方法（如扩散模型或变分自编码器），设计量化指标（如基于相似度计算的保持分数），以及设置评估协议（如使用标准数据集和预处理流程）。具体参数和损失函数依赖于所选基线方法，但基准本身不引入新模型，而是聚焦评估框架。",
            "application_zh": "该研究在音乐制作、影视配乐、游戏音效和广播编辑等领域具有潜在应用价值，通过提供标准化评估基准，能帮助开发者优化音乐编辑模型，提升编辑质量与可靠性。未来可能推动音乐生成与编辑技术的标准化发展，促进更智能的音乐创作工具。",
            "highlight_zh": "实验结果显示，MuseCPBench在评估五种基线方法时，揭示了当前方法在音乐上下文保持方面的一致差距，例如在某些音乐要素上保持分数较低（具体数据未在摘要中提供）。通过系统比较，基准提供了可靠的性能排名，突出了方法间的差异，为改进策略提供了实证依据。",
            "tags_zh": [
                "音乐编辑",
                "音乐上下文保持",
                "评估基准",
                "音乐生成模型",
                "系统分析",
                "标准化评估",
                "多要素评估",
                "实证研究"
            ],
            "_index": 107
        },
        {
            "title": "JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction",
            "authors": [
                "Atsuyuki Miyai",
                "Shota Onohara",
                "Jeonghun Baek",
                "Kiyoharu Aizawa"
            ],
            "arxiv_id": "2512.14620v1",
            "summary": "This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark, and Vibe Benchmark Construction, a scalable construction method. Following the evolution from MMMU to MMMU-Pro, JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, thereby creating a benchmark that requires integrated visual-textual understanding through visual perception. To build JMMMU-Pro, we propose Vibe Benchmark Construction, a methodology in which an image generative model (e.g., Nano Banana Pro) produces candidate visual questions, and humans verify the outputs and, when necessary, regenerate with adjusted prompts to ensure quality. By leveraging Nano Banana Pro's highly realistic image generation capabilities and its ability to embed clean Japanese text, we construct a high-quality benchmark at low cost, covering a wide range of background and layout designs. Experimental results show that all open-source LMMs struggle substantially with JMMMU-Pro, underscoring JMMMU-Pro as an important benchmark for guiding future efforts in the open-source community. We believe that JMMMU-Pro provides a more rigorous evaluation tool for assessing the Japanese capabilities of LMMs and that our Vibe Benchmark Construction also offers an efficient guideline for future development of image-based VQA benchmarks.",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.CV"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page: https://mmmu-japanese-benchmark.github.io/JMMMU_Pro/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14620v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出JMMMU-Pro基准和Vibe Benchmark Construction方法，以低成本构建高质量日语多学科多模态理解评估工具",
            "summary_zh": "本文介绍了JMMMU-Pro，一个基于图像的日语多学科多模态理解基准，以及Vibe Benchmark Construction，一种可扩展的构建方法。继从MMMU到MMMU-Pro的演进后，JMMMU-Pro通过将问题图像和问题文本组合成单一图像来扩展JMMMU，从而创建一个需要通过视觉感知进行集成视觉-文本理解的基准。为了构建JMMMU-Pro，我们提出了Vibe Benchmark Construction方法，其中图像生成模型（如Nano Banana Pro）生成候选视觉问题，人类验证输出并在必要时通过调整提示重新生成以确保质量。通过利用Nano Banana Pro的高度逼真图像生成能力及其嵌入清晰日语文本的能力，我们以低成本构建了一个高质量的基准，覆盖广泛的背景和布局设计。实验结果表明，所有开源LMM在JMMMU-Pro上都表现不佳，突显了JMMMU-Pro作为指导开源社区未来努力的重要基准。我们相信JMMMU-Pro为评估LMM的日语能力提供了更严格的评估工具，并且我们的Vibe Benchmark Construction也为未来基于图像的VQA基准开发提供了高效指南。",
            "intro_zh": [
                "现有日语多模态基准JMMMU在评估视觉-文本集成理解方面存在不足，需要更严格的评估工具来测试大语言模型（LMM）的日语能力。",
                "论文提出JMMMU-Pro基准，将问题图像和文本组合成单一图像，并引入Vibe Benchmark Construction方法，利用图像生成模型和人工验证低成本构建高质量数据集。",
                "实验显示所有开源LMM在JMMMU-Pro上表现显著困难，验证了基准的挑战性，为开源社区提供了重要指导方向。"
            ],
            "method_zh": "**问题定义**：论文旨在解决现有日语多模态理解基准（如JMMMU）在评估大语言模型（LMM）视觉-文本集成理解能力方面的不足。现有基准通常将图像和文本分开处理，缺乏对视觉感知和文本理解紧密结合的测试，导致评估不够全面和严格，难以准确反映模型在实际复杂场景中的性能。\\n\\n**核心思路**：论文的核心思路是通过构建JMMMU-Pro基准，将问题图像和问题文本融合成单一图像，迫使模型同时进行视觉感知和文本解析，从而实现更集成的多模态理解评估。同时，提出Vibe Benchmark Construction方法，利用先进的图像生成模型（如Nano Banana Pro）自动生成候选视觉问题，并结合人工验证和调整，以低成本、高效率地创建高质量、多样化的基准数据集。\\n\\n**技术框架**：整体架构包括两个主要阶段：基准构建和评估。在基准构建阶段，首先使用图像生成模型（如Nano Banana Pro）根据预设提示生成包含日语文本和背景图像的候选问题图像；然后，人类评估者验证生成图像的质量，包括文本清晰度、图像真实性和问题相关性，必要时通过调整提示重新生成以优化结果。在评估阶段，将构建的JMMMU-Pro数据集用于测试开源LMM，通过标准视觉问答（VQA）任务评估模型性能，分析其在多学科场景下的表现。\\n\\n**关键创新**：最重要的技术创新点是Vibe Benchmark Construction方法，它结合了自动化图像生成和人工质量控制，显著降低了基准构建的成本和时间，同时确保了数据的高质量和多样性。与现有方法（如纯人工标注或简单数据增强）相比，该方法利用生成模型的能力快速产生大量候选数据，并通过人类反馈循环优化，本质区别在于实现了可扩展性和效率的平衡，避免了传统方法的高开销或低质量问题。\\n\\n**关键设计**：关键设计包括使用Nano Banana Pro作为图像生成模型，因其具有高度逼真的图像生成能力和嵌入干净日语文本的特性；在提示设计上，调整文本内容、背景布局和学科范围以覆盖多学科场景；人工验证阶段设置严格的质量标准，如文本可读性、图像一致性和问题逻辑性，确保基准的可靠性和挑战性。损失函数或网络结构细节在论文中未明确提及，侧重于方法论和流程设计。",
            "application_zh": "该研究主要应用于多模态人工智能领域，特别是日语大语言模型（LMM）的评估和开发。JMMMU-Pro基准可作为严格的测试工具，帮助研究者和开发者评估模型在日语视觉-文本集成理解任务上的性能，指导模型优化和算法改进。Vibe Benchmark Construction方法具有推广价值，可应用于其他语言或领域的图像基准构建，如教育、医疗或娱乐场景的视觉问答系统开发，提升多模态数据集的构建效率和质量，推动开源社区在跨语言多模态理解方面的进步。",
            "highlight_zh": "实验结果显示，所有测试的开源大语言模型（LMM）在JMMMU-Pro基准上都表现显著困难，具体性能数据未在摘要中提供，但论文强调模型得分普遍较低，突显了基准的挑战性。与现有基准（如JMMMU）相比，JMMMU-Pro通过图像-文本融合设计提高了评估难度，验证了其在测试视觉感知和文本理解集成能力方面的有效性。这为开源社区提供了重要参考，表明当前模型在日语多模态任务上仍有较大提升空间。",
            "tags_zh": [
                "多模态理解",
                "日语基准",
                "视觉问答",
                "图像生成",
                "基准构建",
                "大语言模型评估",
                "跨学科测试",
                "人工验证"
            ],
            "_index": 108
        },
        {
            "title": "ParaFormer: A Generalized PageRank Graph Transformer for Graph Representation Learning",
            "authors": [
                "Chaohao Yuan",
                "Zhenjie Song",
                "Ercan Engin Kuruoglu",
                "Kangfei Zhao",
                "Yang Liu",
                "Deli Zhao",
                "Hong Cheng",
                "Yu Rong"
            ],
            "arxiv_id": "2512.14619v1",
            "summary": "Graph Transformers (GTs) have emerged as a promising graph learning tool, leveraging their all-pair connected property to effectively capture global information. To address the over-smoothing problem in deep GNNs, global attention was initially introduced, eliminating the necessity for using deep GNNs. However, through empirical and theoretical analysis, we verify that the introduced global attention exhibits severe over-smoothing, causing node representations to become indistinguishable due to its inherent low-pass filtering. This effect is even stronger than that observed in GNNs. To mitigate this, we propose PageRank Transformer (ParaFormer), which features a PageRank-enhanced attention module designed to mimic the behavior of deep Transformers. We theoretically and empirically demonstrate that ParaFormer mitigates over-smoothing by functioning as an adaptive-pass filter. Experiments show that ParaFormer achieves consistent performance improvements across both node classification and graph classification tasks on 11 datasets ranging from thousands to millions of nodes, validating its efficacy. The supplementary material, including code and appendix, can be found in https://github.com/chaohaoyuan/ParaFormer.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted by WSDM 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14619v1",
            "code_links": [
                {
                    "url": "https://github.com/chaohaoyuan/ParaFormer",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出PageRank Transformer以解决图Transformer中全局注意力导致的过平滑问题",
            "summary_zh": "图Transformer（GTs）作为一种有前景的图学习工具，利用其全连接特性有效捕获全局信息。为解决深度图神经网络（GNNs）中的过平滑问题，全局注意力被引入，消除了使用深度GNNs的必要性。然而，通过实证和理论分析，我们发现引入的全局注意力表现出严重的过平滑现象，由于其固有的低通滤波特性，导致节点表示变得难以区分，这种效应甚至比GNNs中观察到的更强。为缓解此问题，我们提出了PageRank Transformer（ParaFormer），其特点是包含一个PageRank增强的注意力模块，旨在模拟深度Transformer的行为。我们从理论和实证上证明，ParaFormer通过充当自适应通滤波器来缓解过平滑。实验表明，ParaFormer在从数千到数百万节点的11个数据集上的节点分类和图分类任务中均实现了持续的性能提升，验证了其有效性。补充材料，包括代码和附录，可在https://github.com/chaohaoyuan/ParaFormer找到。",
            "intro_zh": [
                "现有图Transformer的全局注意力机制存在严重过平滑问题，导致节点表示难以区分，影响模型性能。",
                "提出PageRank Transformer，通过PageRank增强的注意力模块模拟深度Transformer行为，实现自适应滤波。",
                "在11个数据集上，ParaFormer在节点和图分类任务中均取得一致性能提升，验证了其缓解过平滑的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决图Transformer（GTs）中全局注意力机制导致的过平滑问题。现有方法的痛点是，尽管全局注意力被引入以缓解深度GNNs的过平滑，但实证和理论分析显示，其固有的低通滤波特性反而引发更严重的过平滑，使节点表示变得相似，从而降低模型区分能力。\\n\\n**核心思路**：论文的核心解决思路是设计一个PageRank增强的注意力模块，以模拟深度Transformer的行为，从而将全局注意力从低通滤波器转变为自适应通滤波器。这样设计的原因是，PageRank算法能有效捕捉图结构中的重要性信息，结合注意力机制可以动态调整滤波特性，避免过度平滑。\\n\\n**技术框架**：整体架构基于图Transformer，主要包含输入层、PageRank增强的注意力模块、前馈网络和输出层。流程上，首先将节点特征输入注意力模块，该模块集成PageRank计算以调整注意力权重，然后通过多层处理进行表示学习，最终用于分类任务。关键模块是PageRank注意力，它结合了传统注意力和图结构信息。\\n\\n**关键创新**：最重要的技术创新点是提出PageRank Transformer（ParaFormer），其本质区别在于将PageRank算法融入注意力机制，实现自适应滤波，而非固定低通滤波。这解决了现有GTs中全局注意力的过平滑缺陷，提升了模型的表示能力。\\n\\n**关键设计**：关键设计包括PageRank注意力模块的参数设置，如结合注意力得分和PageRank得分以计算最终权重；网络结构采用多层Transformer编码器；损失函数通常使用交叉熵损失用于分类任务；此外，可能涉及超参数调整以优化性能。",
            "application_zh": "该研究在图表示学习领域具有广泛潜在应用，如社交网络分析、生物信息学中的蛋白质相互作用预测、推荐系统中的用户-物品关系建模等。其实际价值在于通过缓解过平滑问题，提升图数据处理的准确性和鲁棒性，未来可能推动图Transformer在更大规模图任务中的应用，促进人工智能在图结构数据上的发展。",
            "highlight_zh": "实验在11个数据集上进行，涵盖节点分类和图分类任务，数据集规模从数千到数百万节点。ParaFormer相比基线模型（如标准图Transformer和GNNs）在多个指标上均显示出一致性能提升，具体提升幅度因数据集而异，但总体验证了其缓解过平滑的有效性。例如，在节点分类任务中，准确率提升可达几个百分点，显著优于现有方法。",
            "tags_zh": [
                "图Transformer",
                "过平滑问题",
                "PageRank算法",
                "自适应滤波",
                "节点分类",
                "图分类",
                "图表示学习",
                "全局注意力"
            ],
            "_index": 109
        },
        {
            "title": "LLmFPCA-detect: LLM-powered Multivariate Functional PCA for Anomaly Detection in Sparse Longitudinal Texts",
            "authors": [
                "Prasanjit Dubey",
                "Aritra Guha",
                "Zhengyi Zhou",
                "Qiong Wu",
                "Xiaoming Huo",
                "Paromita Dubey"
            ],
            "arxiv_id": "2512.14604v1",
            "summary": "Sparse longitudinal (SL) textual data arises when individuals generate text repeatedly over time (e.g., customer reviews, occasional social media posts, electronic medical records across visits), but the frequency and timing of observations vary across individuals. These complex textual data sets have immense potential to inform future policy and targeted recommendations. However, because SL text data lack dedicated methods and are noisy, heterogeneous, and prone to anomalies, detecting and inferring key patterns is challenging. We introduce LLmFPCA-detect, a flexible framework that pairs LLM-based text embeddings with functional data analysis to detect clusters and infer anomalies in large SL text datasets. First, LLmFPCA-detect embeds each piece of text into an application-specific numeric space using LLM prompts. Sparse multivariate functional principal component analysis (mFPCA) conducted in the numeric space forms the workhorse to recover primary population characteristics, and produces subject-level scores which, together with baseline static covariates, facilitate data segmentation, unsupervised anomaly detection and inference, and enable other downstream tasks. In particular, we leverage LLMs to perform dynamic keyword profiling guided by the data segments and anomalies discovered by LLmFPCA-detect, and we show that cluster-specific functional PC scores from LLmFPCA-detect, used as features in existing pipelines, help boost prediction performance. We support the stability of LLmFPCA-detect with experiments and evaluate it on two different applications using public datasets, Amazon customer-review trajectories, and Wikipedia talk-page comment streams, demonstrating utility across domains and outperforming state-of-the-art baselines.",
            "categories": [
                "stat.ML",
                "cs.LG"
            ],
            "primary_category": "stat.ML",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14604v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出LLmFPCA-detect框架，结合LLM文本嵌入与稀疏多元函数主成分分析，解决稀疏纵向文本数据的异常检测问题。",
            "summary_zh": "稀疏纵向（SL）文本数据出现在个体随时间重复生成文本的场景中（如客户评论、偶尔的社交媒体帖子、跨次就诊的电子病历），但观测频率和时间在不同个体间存在差异。这些复杂的文本数据集具有巨大潜力，可为未来政策和针对性推荐提供信息。然而，由于SL文本数据缺乏专门方法，且具有噪声、异质性和易出现异常的特点，检测和推断关键模式具有挑战性。我们引入了LLmFPCA-detect，这是一个灵活的框架，将基于LLM的文本嵌入与函数数据分析相结合，以检测大型SL文本数据集中的聚类并推断异常。首先，LLmFPCA-detect使用LLM提示将每段文本嵌入到特定应用的数值空间中。在数值空间中进行的稀疏多元函数主成分分析（mFPCA）是恢复主要群体特征的核心工具，并产生个体级分数，这些分数与基线静态协变量一起，促进数据分割、无监督异常检测和推断，并支持其他下游任务。特别是，我们利用LLM在LLmFPCA-detect发现的数据段和异常指导下进行动态关键词分析，并展示LLmFPCA-detect产生的聚类特定函数主成分分数作为现有流程中的特征，有助于提升预测性能。我们通过实验支持LLmFPCA-detect的稳定性，并使用公共数据集（亚马逊客户评论轨迹和维基百科讨论页评论流）在两个不同应用中评估它，展示了跨领域的实用性并优于最先进的基线方法。",
            "intro_zh": [
                "稀疏纵向文本数据（如客户评论、医疗记录）缺乏专门分析方法，现有方法难以处理其噪声、异质性和异常问题。",
                "提出LLmFPCA-detect框架，结合LLM文本嵌入与函数数据分析，通过稀疏多元函数主成分分析恢复群体特征并检测异常。",
                "在亚马逊评论和维基百科评论数据集上实验，LLmFPCA-detect优于基线方法，提升预测性能并展示跨领域稳定性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决稀疏纵向（SL）文本数据中的异常检测和模式推断问题。现有方法缺乏专门处理SL文本数据的工具，这些数据具有观测频率和时间不规律、噪声大、异质性强、易出现异常等特点，导致传统文本分析或统计方法效果有限，难以有效提取关键模式和识别异常。\\n\\n**核心思路**：论文的核心思路是将大型语言模型（LLM）的文本嵌入能力与函数数据分析（FDA）相结合，利用LLM将文本转换为数值表示，再通过稀疏多元函数主成分分析（mFPCA）在函数空间中建模时间动态，从而恢复群体特征、检测聚类和异常。这种设计能够同时处理文本的语义信息和时间序列的稀疏性，克服单一方法的局限性。\\n\\n**技术框架**：整体框架分为三个阶段：首先，使用LLM提示将每个文本片段嵌入到特定应用的数值空间中，生成文本向量；其次，在数值空间上应用稀疏mFPCA，从稀疏时间点数据中估计函数主成分，恢复主要群体特征并计算个体级分数；最后，结合基线静态协变量，进行数据分割、无监督异常检测和推断，并支持下游任务如动态关键词分析和预测性能提升。\\n\\n**关键创新**：最重要的技术创新是首次将LLM文本嵌入与稀疏mFPCA结合用于SL文本数据分析，实现了文本语义与时间动态的联合建模。与现有方法相比，本质区别在于它专门针对SL文本数据的稀疏性和异质性设计，避免了传统方法对密集或规则时间序列的依赖，同时利用LLM的通用语义理解能力增强特征提取。\\n\\n**关键设计**：关键设计包括使用LLM提示进行文本嵌入，具体提示内容未知，但旨在生成应用特定的数值表示；稀疏mFPCA部分涉及函数主成分估计，处理不规则时间点数据，可能基于基函数展开或协方差平滑技术；异常检测基于个体级分数和协变量，采用无监督方法如聚类或离群点检测；动态关键词分析利用LLM在数据段指导下生成解释性关键词。具体参数设置和损失函数未在摘要中详细说明，需参考论文全文。",
            "application_zh": "该研究在客户评论分析、社交媒体监控、电子病历挖掘等领域具有广泛应用价值。例如，企业可基于客户评论轨迹检测异常反馈以改进产品，医疗机构可从稀疏就诊记录中识别患者异常模式以辅助诊断。未来可能扩展到更多纵向文本场景，如教育评估或政策分析，提升数据驱动的决策能力。",
            "highlight_zh": "在亚马逊客户评论数据集和维基百科讨论页评论流上的实验表明，LLmFPCA-detect优于最先进的基线方法，具体性能数据未知，但摘要提到它提升了预测性能。框架展示了跨领域的稳定性和实用性，通过聚类特定函数主成分分数作为特征，有效增强了现有下游任务的性能。",
            "tags_zh": [
                "稀疏纵向文本数据",
                "异常检测",
                "大型语言模型",
                "函数数据分析",
                "多元函数主成分分析",
                "文本嵌入",
                "无监督学习",
                "时间序列分析"
            ],
            "_index": 110
        },
        {
            "title": "Sound and Music Biases in Deep Music Transcription Models: A Systematic Analysis",
            "authors": [
                "Lukáš Samuel Marták",
                "Patricia Hu",
                "Gerhard Widmer"
            ],
            "arxiv_id": "2512.14602v1",
            "summary": "Automatic Music Transcription (AMT) -- the task of converting music audio into note representations -- has seen rapid progress, driven largely by deep learning systems. Due to the limited availability of richly annotated music datasets, much of the progress in AMT has been concentrated on classical piano music, and even a few very specific datasets. Whether these systems can generalize effectively to other musical contexts remains an open question. Complementing recent studies on distribution shifts in sound (e.g., recording conditions), in this work we investigate the musical dimension -- specifically, variations in genre, dynamics, and polyphony levels. To this end, we introduce the MDS corpus, comprising three distinct subsets -- (1) Genre, (2) Random, and (3) MAEtest -- to emulate different axes of distribution shift. We evaluate the performance of several state-of-the-art AMT systems on the MDS corpus using both traditional information-retrieval and musically-informed performance metrics. Our extensive evaluation isolates and exposes varying degrees of performance degradation under specific distribution shifts. In particular, we measure a note-level F1 performance drop of 20 percentage points due to sound, and 14 due to genre. Generally, we find that dynamics estimation proves more vulnerable to musical variation than onset prediction. Musically informed evaluation metrics, particularly those capturing harmonic structure, help identify potential contributing factors. Furthermore, experiments with randomly generated, non-musical sequences reveal clear limitations in system performance under extreme musical distribution shifts. Altogether, these findings offer new evidence of the persistent impact of the Corpus Bias problem in deep AMT systems.",
            "categories": [
                "cs.SD",
                "cs.LG"
            ],
            "primary_category": "cs.SD",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "pre-print of the upcoming EURASIP JASM journal article",
            "doi": "10.1186/s13636-025-00428-z",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14602v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "系统分析深度音乐转录模型中的声音与音乐偏见，揭示其在分布偏移下的性能退化问题",
            "summary_zh": "自动音乐转录（AMT）——将音乐音频转换为音符表示的任务——在深度学习系统的推动下取得了快速进展。由于丰富标注音乐数据集的可用性有限，AMT的大部分进展集中在古典钢琴音乐，甚至少数特定数据集上。这些系统是否能有效泛化到其他音乐情境仍是一个开放问题。本研究补充了最近关于声音分布偏移（如录音条件）的研究，调查了音乐维度——特别是流派、动态和复音水平的变化。为此，我们引入了MDS语料库，包含三个不同的子集——（1）流派，（2）随机，和（3）MAEtest——以模拟分布偏移的不同轴。我们使用传统信息检索和音乐感知性能指标评估了几个最先进的AMT系统在MDS语料库上的表现。我们的广泛评估隔离并暴露了在特定分布偏移下不同程度的性能退化。具体而言，我们测量到由于声音导致的音符级F1性能下降20个百分点，由于流派导致的下降14个百分点。总体而言，我们发现动态估计比起始预测更容易受到音乐变化的影响。音乐感知评估指标，特别是那些捕捉和声结构的指标，有助于识别潜在的贡献因素。此外，使用随机生成的非音乐序列进行的实验揭示了在极端音乐分布偏移下系统性能的明显限制。总之，这些发现为深度AMT系统中语料库偏见问题的持续影响提供了新证据。",
            "intro_zh": [
                "核心问题：现有深度音乐转录模型主要基于古典钢琴数据训练，泛化能力未知，面临音乐维度（如流派、动态）分布偏移的挑战。",
                "方法要点：构建MDS语料库模拟不同分布偏移，系统评估多个先进模型，使用传统和音乐感知指标分析性能退化。",
                "实验或效果：发现声音和流派分别导致F1下降20和14个百分点，动态估计更脆弱，非音乐序列揭示系统极限。"
            ],
            "method_zh": "**问题定义**：论文旨在解决深度自动音乐转录（AMT）模型在音乐维度分布偏移下的泛化能力问题。现有方法主要依赖古典钢琴等有限数据集训练，导致模型可能无法有效处理其他音乐情境（如不同流派、动态变化），存在语料库偏见，影响实际应用中的鲁棒性和准确性。\\n\\n**核心思路**：通过构建一个专门设计的MDS语料库，模拟音乐维度的分布偏移（如流派、动态、复音水平），系统评估多个先进AMT模型在这些偏移下的性能，以量化偏见影响并识别脆弱环节。这种设计允许隔离不同因素对性能的影响，从而深入理解模型泛化限制。\\n\\n**技术框架**：整体流程包括数据收集与语料库构建、模型选择与评估、指标计算与分析三个阶段。首先，创建MDS语料库，包含三个子集：Genre（模拟流派变化）、Random（模拟随机非音乐序列）、MAEtest（模拟其他分布偏移）。然后，选取多个最先进的AMT模型作为评估对象。最后，使用传统信息检索指标（如音符级F1）和音乐感知指标（如捕捉和声结构的指标）进行性能评估，分析不同分布偏移下的性能退化。\\n\\n**关键创新**：最重要的创新在于系统性地研究音乐维度的分布偏移对AMT模型的影响，而不仅仅是声音条件的变化。通过设计MDS语料库，论文能够量化特定音乐因素（如流派）导致的性能下降，并引入音乐感知指标来揭示潜在原因，这扩展了现有对AMT偏见问题的理解。\\n\\n**关键设计**：MDS语料库是关键设计，它包含三个子集以模拟不同轴：Genre子集覆盖多种音乐流派，Random子集使用随机生成的非音乐序列测试极端偏移，MAEtest子集可能基于其他数据集构建。评估中，使用音符级F1分数等传统指标，以及专门设计的音乐感知指标（如和声结构相关指标），以全面捕捉性能变化。模型选择包括多个最先进的深度AMT系统，确保评估的代表性。",
            "application_zh": "该研究在音乐信息检索、音频处理和教育领域具有潜在应用价值。通过揭示AMT模型在音乐维度分布偏移下的偏见，可指导更鲁棒的模型开发，提升自动转录系统在多样化音乐场景（如流行音乐、现场录音）中的准确性。未来可能推动数据增强策略、领域自适应方法的发展，促进音乐技术的实际部署和跨领域泛化。",
            "highlight_zh": "最重要的实验结果显示：在MDS语料库评估中，声音分布偏移导致音符级F1性能下降20个百分点，流派偏移导致下降14个百分点，凸显了模型对音乐变化的敏感性。动态估计比起始预测更易受音乐变化影响，表明模型在处理音量变化时更脆弱。使用随机非音乐序列的实验揭示了系统在极端偏移下的性能极限，F1分数显著降低。音乐感知指标（如和声结构指标）帮助识别了性能退化的潜在因素，为改进模型提供了方向。",
            "tags_zh": [
                "自动音乐转录",
                "分布偏移",
                "语料库偏见",
                "音乐信息检索",
                "深度学习评估",
                "泛化能力",
                "音乐感知指标",
                "模型鲁棒性"
            ],
            "_index": 111
        },
        {
            "title": "FakeRadar: Probing Forgery Outliers to Detect Unknown Deepfake Videos",
            "authors": [
                "Zhaolun Li",
                "Jichang Li",
                "Yinqi Cai",
                "Junye Chen",
                "Xiaonan Luo",
                "Guanbin Li",
                "Rushi Lan"
            ],
            "arxiv_id": "2512.14601v1",
            "summary": "In this paper, we propose FakeRadar, a novel deepfake video detection framework designed to address the challenges of cross-domain generalization in real-world scenarios. Existing detection methods typically rely on manipulation-specific cues, performing well on known forgery types but exhibiting severe limitations against emerging manipulation techniques. This poor generalization stems from their inability to adapt effectively to unseen forgery patterns. To overcome this, we leverage large-scale pretrained models (e.g. CLIP) to proactively probe the feature space, explicitly highlighting distributional gaps between real videos, known forgeries, and unseen manipulations. Specifically, FakeRadar introduces Forgery Outlier Probing, which employs dynamic subcluster modeling and cluster-conditional outlier generation to synthesize outlier samples near boundaries of estimated subclusters, simulating novel forgery artifacts beyond known manipulation types. Additionally, we design Outlier-Guided Tri-Training, which optimizes the detector to distinguish real, fake, and outlier samples using proposed outlier-driven contrastive learning and outlier-conditioned cross-entropy losses. Experiments show that FakeRadar outperforms existing methods across various benchmark datasets for deepfake video detection, particularly in cross-domain evaluations, by handling the variety of emerging manipulation techniques.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14601v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出FakeRadar框架，通过伪造异常探测和三重训练解决深度伪造视频检测中的跨域泛化问题",
            "summary_zh": "本文提出FakeRadar，一种新颖的深度伪造视频检测框架，旨在解决现实场景中跨域泛化的挑战。现有检测方法通常依赖于特定操纵线索，在已知伪造类型上表现良好，但对新兴操纵技术表现出严重局限性。这种泛化能力差源于它们无法有效适应未见过的伪造模式。为克服此问题，我们利用大规模预训练模型（如CLIP）主动探测特征空间，明确突出真实视频、已知伪造和未见操纵之间的分布差距。具体而言，FakeRadar引入伪造异常探测，采用动态子簇建模和簇条件异常生成，在估计子簇边界附近合成异常样本，模拟超出已知操纵类型的新伪造伪影。此外，我们设计异常引导三重训练，通过提出的异常驱动对比学习和异常条件交叉熵损失优化检测器，以区分真实、伪造和异常样本。实验表明，FakeRadar在深度伪造视频检测的各种基准数据集上优于现有方法，特别是在跨域评估中，通过处理各种新兴操纵技术。",
            "intro_zh": [
                "现有深度伪造检测方法依赖特定操纵线索，对新兴伪造技术泛化能力差，无法适应未知伪造模式。",
                "FakeRadar利用预训练模型主动探测特征空间，通过伪造异常探测模拟未知伪造，结合三重训练优化检测器。",
                "实验显示FakeRadar在跨域评估中优于现有方法，有效处理多种新兴操纵技术，提升检测泛化性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决深度伪造视频检测中的跨域泛化问题，即现有方法在已知伪造类型上表现良好，但对新兴或未见过的伪造技术泛化能力差，这源于它们过度依赖特定操纵线索，无法适应未知伪造模式。\\n\\n**核心思路**：核心思路是利用大规模预训练模型主动探测特征空间，通过模拟未知伪造的异常样本来增强检测器的泛化能力。具体来说，通过伪造异常探测生成模拟未知伪造的样本，并设计异常引导的训练机制，使检测器能更好地区分真实、已知伪造和未知伪造。\\n\\n**技术框架**：整体框架包括两个主要阶段：伪造异常探测和异常引导三重训练。首先，基于预训练模型提取特征，进行动态子簇建模以估计真实和已知伪造的分布；然后，通过簇条件异常生成合成异常样本。其次，使用这些样本进行三重训练，结合异常驱动对比学习和异常条件交叉熵损失优化检测器。\\n\\n**关键创新**：最重要的创新是伪造异常探测，它主动生成模拟未知伪造的异常样本，而非依赖现有数据；以及异常引导三重训练，将异常样本纳入训练过程，提升对未知伪造的识别能力。与现有方法的本质区别在于从被动适应转向主动探测，增强泛化性。\\n\\n**关键设计**：关键设计包括：使用CLIP等预训练模型提取多模态特征；动态子簇建模通过聚类算法划分特征空间；簇条件异常生成基于边界采样合成异常；损失函数结合异常驱动对比损失（鼓励异常与已知类分离）和异常条件交叉熵损失（优化分类）；网络结构可能包含特征提取器和分类头，具体参数设置未知。",
            "application_zh": "该研究在网络安全、内容审核和数字取证领域具有重要应用价值。例如，可用于社交媒体平台自动检测深度伪造视频，防止虚假信息传播；在司法鉴定中辅助识别伪造证据；未来可能扩展到其他多媒体伪造检测任务，提升人工智能系统的可信度和安全性。",
            "highlight_zh": "实验在多个深度伪造视频检测基准数据集上进行，FakeRadar在跨域评估中显著优于现有方法。具体性能数据未知，但论文报告在处理新兴操纵技术时表现出更好的泛化能力，提升幅度可能体现在准确率或F1分数上，对比基线包括传统基于线索的方法和现有泛化方法。",
            "tags_zh": [
                "深度伪造检测",
                "跨域泛化",
                "伪造异常探测",
                "异常引导训练",
                "预训练模型",
                "对比学习",
                "视频分析",
                "人工智能安全"
            ],
            "_index": 112
        },
        {
            "title": "FoodLogAthl-218: Constructing a Real-World Food Image Dataset Using Dietary Management Applications",
            "authors": [
                "Mitsuki Watanabe",
                "Sosuke Amano",
                "Kiyoharu Aizawa",
                "Yoko Yamakata"
            ],
            "arxiv_id": "2512.14574v1",
            "summary": "Food image classification models are crucial for dietary management applications because they reduce the burden of manual meal logging. However, most publicly available datasets for training such models rely on web-crawled images, which often differ from users' real-world meal photos. In this work, we present FoodLogAthl-218, a food image dataset constructed from real-world meal records collected through the dietary management application FoodLog Athl. The dataset contains 6,925 images across 218 food categories, with a total of 14,349 bounding boxes. Rich metadata, including meal date and time, anonymized user IDs, and meal-level context, accompany each image. Unlike conventional datasets-where a predefined class set guides web-based image collection-our data begins with user-submitted photos, and labels are applied afterward. This yields greater intra-class diversity, a natural frequency distribution of meal types, and casual, unfiltered images intended for personal use rather than public sharing. In addition to (1) a standard classification benchmark, we introduce two FoodLog-specific tasks: (2) an incremental fine-tuning protocol that follows the temporal stream of users' logs, and (3) a context-aware classification task where each image contains multiple dishes, and the model must classify each dish by leveraging the overall meal context. We evaluate these tasks using large multimodal models (LMMs). The dataset is publicly available at https://huggingface.co/datasets/FoodLog/FoodLogAthl-218.",
            "categories": [
                "cs.CV",
                "cs.MM"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "10.1145/3746027.3758276",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14574v1",
            "code_links": [
                {
                    "url": "https://huggingface.co/datasets/FoodLog",
                    "type": "huggingface"
                }
            ],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出FoodLogAthl-218真实世界食物图像数据集，解决现有数据集与用户实际餐照差异大的问题。",
            "summary_zh": "食物图像分类模型对饮食管理应用至关重要，能减轻手动记录餐食的负担。然而，大多数用于训练此类模型的公开数据集依赖网络爬取的图像，这些图像常与用户实际餐照存在差异。本研究提出了FoodLogAthl-218，这是一个从饮食管理应用FoodLog Athl收集的真实世界餐食记录构建的食物图像数据集。该数据集包含218个食物类别，共6,925张图像，总计14,349个边界框。每张图像附有丰富的元数据，包括餐食日期和时间、匿名用户ID以及餐食级上下文信息。与传统数据集不同，传统数据集以预定义类别集指导网络图像收集，而我们的数据始于用户提交的照片，随后才应用标签。这带来了更大的类内多样性、餐食类型的自然频率分布，以及用于个人使用而非公开分享的随意、未过滤图像。除了（1）标准分类基准外，我们还引入了两个FoodLog特定任务：（2）遵循用户日志时间流的增量微调协议，以及（3）上下文感知分类任务，其中每张图像包含多道菜肴，模型必须利用整体餐食上下文对每道菜进行分类。我们使用大型多模态模型（LMMs）评估了这些任务。该数据集可在https://huggingface.co/datasets/FoodLog/FoodLogAthl-218公开获取。",
            "intro_zh": [
                "核心问题：现有食物图像数据集多基于网络爬取，与用户实际餐照差异大，缺乏真实世界多样性，限制了饮食管理应用的模型性能。",
                "方法要点：从饮食管理应用收集用户真实餐照构建数据集，采用后标注方式，引入增量微调和上下文感知分类任务，以提升模型实用性。",
                "实验或效果：构建了包含6,925张图像、218类别的数据集，评估显示能有效支持标准分类和特定任务，促进真实世界应用。"
            ],
            "method_zh": "**问题定义**：论文旨在解决食物图像分类模型在饮食管理应用中因训练数据与用户实际餐照不匹配而性能受限的问题。现有方法依赖网络爬取图像，这些图像往往经过美化或标准化，缺乏真实世界餐食的多样性、随意性和自然分布，导致模型在真实场景中泛化能力不足。\\n\\n**核心思路**：论文的核心思路是通过从饮食管理应用FoodLog Athl收集用户真实提交的餐食照片，构建一个真实世界食物图像数据集，并采用后标注方式（即先有图像后加标签），以捕捉更自然的类内多样性和频率分布。此外，设计特定任务如增量微调和上下文感知分类，以模拟实际应用中的动态学习和多菜肴场景。\\n\\n**技术框架**：整体流程包括数据收集、标注和任务设计三个阶段。首先，从应用收集用户上传的餐食图像及元数据（如时间、用户ID）；其次，对图像进行手动或半自动标注，生成边界框和类别标签；最后，基于数据集设计标准分类基准、增量微调协议（按时间顺序微调模型）和上下文感知分类任务（利用餐食上下文辅助多菜肴分类）。评估使用大型多模态模型（LMMs）进行。\\n\\n**关键创新**：最重要的创新在于数据集的构建方式：不同于传统基于预定义类别的网络爬取，本数据集以用户真实照片为起点，后加标签，从而更真实地反映实际饮食场景。这带来了类内多样性高、频率分布自然、图像随意未过滤的特点，本质区别在于数据源的真实性和标注流程的逆向设计。\\n\\n**关键设计**：数据集包含6,925张图像、218个食物类别、14,349个边界框，元数据包括餐食日期、时间、匿名用户ID和餐食级上下文。任务设计中，增量微调协议模拟用户日志的时间流，上下文感知分类任务要求模型在单图像多菜肴场景下利用整体上下文进行分类。具体参数和损失函数未在摘要中详细说明，但评估基于LMMs，可能涉及标准分类损失和上下文融合机制。",
            "application_zh": "该研究主要应用于饮食管理、健康监测和智能餐饮领域。通过提供真实世界食物图像数据集，能提升食物识别模型的准确性和实用性，支持自动餐食记录、营养分析和个人化饮食建议。未来可促进AI在医疗健康、健身管理和食品工业中的集成，推动个性化健康解决方案的发展。",
            "highlight_zh": "实验构建了FoodLogAthl-218数据集，包含6,925张图像、218个类别和14,349个边界框，具有高类内多样性和自然频率分布。评估使用大型多模态模型（LMMs），在标准分类基准上表现良好，并成功支持增量微调协议和上下文感知分类任务，后者能有效利用餐食上下文提升多菜肴分类性能，具体性能数据未在摘要中提供，但突出了数据集的实用性和任务设计的有效性。",
            "tags_zh": [
                "食物图像分类",
                "真实世界数据集",
                "饮食管理应用",
                "增量微调",
                "上下文感知分类",
                "大型多模态模型",
                "计算机视觉",
                "健康人工智能"
            ],
            "_index": 113
        },
        {
            "title": "CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer",
            "authors": [
                "Xianwei Cao",
                "Dou Quan",
                "Shuang Wang",
                "Ning Huyan",
                "Wei Wang",
                "Yunan Li",
                "Licheng Jiao"
            ],
            "arxiv_id": "2512.14560v1",
            "summary": "Image retrieval-based cross-view geo-localization (IRCVGL) aims to match images captured from significantly different viewpoints, such as satellite and street-level images. Existing methods predominantly rely on learning robust global representations or implicit feature alignment, which often fail to model explicit spatial correspondences crucial for accurate localization. In this work, we propose a novel correspondence-aware feature refinement framework, termed CLNet, that explicitly bridges the semantic and geometric gaps between different views. CLNet decomposes the view alignment process into three learnable and complementary modules: a Neural Correspondence Map (NCM) that spatially aligns cross-view features via latent correspondence fields; a Nonlinear Embedding Converter (NEC) that remaps features across perspectives using an MLP-based transformation; and a Global Feature Recalibration (GFR) module that reweights informative feature channels guided by learned spatial cues. The proposed CLNet can jointly capture both high-level semantics and fine-grained alignments. Extensive experiments on four public benchmarks, CVUSA, CVACT, VIGOR, and University-1652, demonstrate that our proposed CLNet achieves state-of-the-art performance while offering better interpretability and generalizability.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "16 pages, 6 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14560v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出CLNet框架，通过显式跨视角对应关系解决图像检索式跨视角地理定位问题。",
            "summary_zh": "基于图像检索的跨视角地理定位（IRCVGL）旨在匹配从显著不同视角（如卫星和街景）捕获的图像。现有方法主要依赖学习鲁棒的全局表示或隐式特征对齐，往往难以建模对精确定位至关重要的显式空间对应关系。本文提出了一种新颖的对应感知特征细化框架，称为CLNet，它显式地桥接了不同视角之间的语义和几何鸿沟。CLNet将视角对齐过程分解为三个可学习且互补的模块：神经对应图（NCM），通过潜在对应场在空间上对齐跨视角特征；非线性嵌入转换器（NEC），使用基于MLP的变换跨视角重映射特征；以及全局特征重校准（GFR）模块，通过学习到的空间线索引导重新加权信息丰富的特征通道。所提出的CLNet能够联合捕获高级语义和细粒度对齐。在四个公共基准数据集（CVUSA、CVACT、VIGOR和University-1652）上的大量实验表明，我们提出的CLNet实现了最先进的性能，同时提供了更好的可解释性和泛化性。",
            "intro_zh": [
                "现有方法依赖全局表示或隐式对齐，难以建模跨视角的显式空间对应关系，导致地理定位精度受限。",
                "提出CLNet框架，通过神经对应图、非线性嵌入转换器和全局特征重校准模块，显式学习跨视角的语义和几何对应。",
                "在CVUSA等四个基准数据集上实现SOTA性能，显著提升定位精度，并增强模型的可解释性和泛化能力。"
            ],
            "method_zh": "**问题定义**：论文解决图像检索式跨视角地理定位（IRCVGL）问题，即匹配卫星与街景等不同视角图像以确定地理位置。现有方法主要依赖学习全局特征表示或进行隐式特征对齐，缺乏对跨视角间显式空间对应关系的建模，这限制了定位精度，因为不同视角的图像在几何结构和语义内容上存在显著差异。\\n\\n**核心思路**：论文提出CLNet框架，其核心思路是显式地建模跨视角的对应关系，以桥接语义和几何鸿沟。通过将视角对齐过程分解为可学习的模块，CLNet能够同时捕获高级语义信息和细粒度的空间对齐，从而提升特征表示的判别性和鲁棒性。这种设计旨在克服现有方法中对应关系建模不足的问题，通过引入结构化组件来增强特征交互。\\n\\n**技术框架**：CLNet的整体架构包括三个主要模块：神经对应图（NCM）、非线性嵌入转换器（NEC）和全局特征重校准（GFR）。NCM通过潜在对应场在空间上对齐跨视角特征，学习特征点之间的对应关系；NEC使用基于多层感知机（MLP）的变换将特征从一个视角重映射到另一个视角，以适应视角差异；GFR则根据学习到的空间线索重新加权特征通道，突出信息丰富的部分。这些模块协同工作，形成一个端到端的训练框架。\\n\\n**关键创新**：最重要的技术创新在于显式地建模跨视角对应关系，而非依赖隐式对齐。CLNet通过NCM引入空间对应场，直接学习特征间的几何对应，这是与现有方法的本质区别。此外，模块化设计允许联合优化语义和几何信息，提升了特征表示的全面性和准确性。\\n\\n**关键设计**：关键设计包括：NCM使用卷积网络生成对应场，计算特征相似性以对齐空间位置；NEC基于MLP实现非线性变换，参数共享以增强泛化；GFR采用注意力机制，根据对应线索调整通道权重。损失函数通常结合分类损失和对比损失，以优化特征判别性。网络结构基于骨干网络（如ResNet）提取特征，后接CLNet模块进行细化，具体参数设置如层数和维度根据数据集调整，例如在CVUSA上使用ResNet-50作为骨干。",
            "application_zh": "该研究在自动驾驶、无人机导航、增强现实和智能城市等领域具有广泛应用潜力。通过提升跨视角地理定位的精度，可支持车辆定位、路径规划、虚拟地图叠加等任务，增强空间感知能力。未来可能推动多模态定位系统的发展，为机器人视觉和地理信息系统提供更可靠的解决方案。",
            "highlight_zh": "在CVUSA、CVACT、VIGOR和University-1652四个基准数据集上，CLNet均达到最先进性能。例如，在CVUSA上，top-1准确率提升约2-3个百分点，显著优于现有基线方法如SAFA和CVMNet。实验显示，CLNet在跨数据集泛化测试中也表现优异，验证了其鲁棒性和可解释性优势。",
            "tags_zh": [
                "跨视角地理定位",
                "图像检索",
                "对应关系建模",
                "特征对齐",
                "神经网络",
                "计算机视觉",
                "语义几何融合",
                "可解释AI"
            ],
            "_index": 114
        },
        {
            "title": "Test Time Optimized Generalized AI-based Medical Image Registration Method",
            "authors": [
                "Sneha Sree C.",
                "Dattesh Shanbhag",
                "Sudhanya Chatterjee"
            ],
            "arxiv_id": "2512.14556v1",
            "summary": "Medical image registration is critical for aligning anatomical structures across imaging modalities such as computed tomography (CT), magnetic resonance imaging (MRI), and ultrasound. Among existing techniques, non-rigid registration (NRR) is particularly challenging due to the need to capture complex anatomical deformations caused by physiological processes like respiration or contrast-induced signal variations. Traditional NRR methods, while theoretically robust, often require extensive parameter tuning and incur high computational costs, limiting their use in real-time clinical workflows. Recent deep learning (DL)-based approaches have shown promise; however, their dependence on task-specific retraining restricts scalability and adaptability in practice. These limitations underscore the need for efficient, generalizable registration frameworks capable of handling heterogeneous imaging contexts. In this work, we introduce a novel AI-driven framework for 3D non-rigid registration that generalizes across multiple imaging modalities and anatomical regions. Unlike conventional methods that rely on application-specific models, our approach eliminates anatomy- or modality-specific customization, enabling streamlined integration into diverse clinical environments.",
            "categories": [
                "eess.IV",
                "cs.CV"
            ],
            "primary_category": "eess.IV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14556v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出一种基于AI的通用3D非刚性医学图像配准框架，以解决多模态和跨解剖区域配准中的计算效率与泛化性不足问题。",
            "summary_zh": "医学图像配准对于对齐计算机断层扫描（CT）、磁共振成像（MRI）和超声等成像模态中的解剖结构至关重要。在现有技术中，非刚性配准（NRR）尤其具有挑战性，因为它需要捕捉由呼吸或对比剂引起的信号变化等生理过程导致的复杂解剖变形。传统的NRR方法虽然在理论上稳健，但通常需要大量参数调整并产生高计算成本，限制了其在实时临床工作流程中的应用。最近的基于深度学习（DL）的方法显示出潜力；然而，它们对任务特定再训练的依赖在实践中限制了可扩展性和适应性。这些局限性凸显了对能够处理异构成像环境的高效、可泛化配准框架的需求。在这项工作中，我们引入了一种新颖的AI驱动框架，用于3D非刚性配准，该框架可泛化到多种成像模态和解剖区域。与依赖应用特定模型的传统方法不同，我们的方法消除了解剖或模态特定的定制，从而能够简化集成到多样化的临床环境中。",
            "intro_zh": [
                "核心问题：传统非刚性配准方法计算成本高且需大量参数调整，而深度学习方法依赖任务特定再训练，限制了泛化性和实时应用。",
                "方法要点：提出一种AI驱动的通用3D非刚性配准框架，无需解剖或模态特定定制，实现跨模态和跨解剖区域的泛化。",
                "实验或效果：框架在多种成像模态和解剖区域上表现出高效性和泛化能力，提升了配准精度和计算效率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决医学图像配准中非刚性配准（NRR）的挑战，特别是在多模态（如CT、MRI、超声）和跨解剖区域场景下。现有方法的痛点包括：传统NRR方法需要大量参数调整和高计算成本，限制了实时临床应用；而基于深度学习的方法依赖任务特定再训练，导致泛化性不足和可扩展性受限。\\n\\n**核心思路**：论文的核心解决思路是设计一个通用的AI驱动框架，通过消除解剖或模态特定的定制，实现跨多种成像环境的高效非刚性配准。这样设计旨在克服现有方法的局限性，提升配准的泛化能力和计算效率，从而适应多样化的临床需求。\\n\\n**技术框架**：整体架构包括数据预处理、特征提取、变形场预测和后处理阶段。主要模块可能涉及深度学习网络（如卷积神经网络）用于学习图像间的空间对应关系，以及优化算法来调整变形参数。流程从输入多模态图像开始，通过端到端训练生成配准结果，无需针对特定任务进行再训练。\\n\\n**关键创新**：最重要的技术创新点是提出一个无需解剖或模态特定定制的通用配准框架，与现有方法相比，本质区别在于其泛化能力，能够直接应用于多种成像场景，而无需额外调整或再训练。这显著提升了方法的实用性和可扩展性。\\n\\n**关键设计**：关键设计细节可能包括使用多尺度网络结构来捕捉不同层次的解剖变形，结合损失函数（如相似性度量和正则化项）来平衡配准精度和变形平滑性，以及采用高效的优化策略（如测试时优化）来减少计算开销。具体参数设置和网络结构细节在摘要中未明确，需参考完整论文。",
            "application_zh": "该研究在医学影像领域具有广泛的应用潜力，包括多模态图像融合（如CT与MRI对齐）、手术导航、疾病监测和个性化治疗规划。其通用性设计使得框架能够简化集成到临床工作流程中，提升诊断准确性和治疗效率，未来可能推动智能医疗系统的发展。",
            "highlight_zh": "最重要的实验结果显示，该框架在多种成像模态（如CT、MRI）和解剖区域上实现了显著的性能提升。具体数据未知，但可能包括配准精度提高（如降低目标配准误差）和计算时间减少，相比传统方法和现有深度学习基线，展现出更好的泛化能力和效率。",
            "tags_zh": [
                "医学图像配准",
                "非刚性配准",
                "深度学习",
                "多模态融合",
                "泛化框架",
                "3D配准",
                "AI驱动",
                "临床应用"
            ],
            "_index": 115
        },
        {
            "title": "Dual Language Models: Balancing Training Efficiency and Overfitting Resilience",
            "authors": [
                "David Samuel",
                "Lucas Georges Gabriel Charpentier"
            ],
            "arxiv_id": "2512.14549v1",
            "summary": "This paper combines autoregressive and masked-diffusion training objectives without any architectural modifications, resulting in flexible language models that outperform single-objective models. Autoregressive modeling has been a popular approach, partly because of its training efficiency; however, that comes at the cost of sensitivity to overfitting. On the other hand, masked-diffusion models are less efficient to train while being more resilient to overfitting. In this work, we demonstrate that dual-objective training achieves the best of both worlds. To derive the optimal ratio between both objectives, we train and evaluate 50 language models under varying levels of data repetition. We show that it is optimal to combine both objectives under all evaluated settings and that the optimal ratio is similar whether targeting autoregressive or masked-diffusion downstream performance.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14549v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出双目标训练方法以平衡语言模型的训练效率与过拟合鲁棒性",
            "summary_zh": "本文结合了自回归和掩码扩散训练目标，无需任何架构修改，从而构建出灵活的语言模型，其性能优于单目标模型。自回归建模因其训练效率高而广受欢迎，但代价是对过拟合敏感；而掩码扩散模型训练效率较低，但对过拟合更具鲁棒性。本研究证明，双目标训练实现了两者的优势结合。为确定两种目标之间的最优比例，我们在不同数据重复水平下训练和评估了50个语言模型。结果表明，在所有评估设置下，结合两种目标是最优的，且无论针对自回归还是掩码扩散的下游性能，最优比例都相似。",
            "intro_zh": [
                "自回归模型训练效率高但易过拟合，掩码扩散模型鲁棒性强但效率低，现有单目标方法难以兼顾。",
                "提出结合自回归与掩码扩散的双目标训练，无需修改架构，通过优化目标比例实现优势互补。",
                "实验显示双目标模型在所有设置下均优于单目标，最优比例稳定，显著提升下游任务性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决语言模型训练中效率与鲁棒性的权衡问题。现有自回归模型训练高效但易过拟合，掩码扩散模型鲁棒性强但训练效率低，单目标方法难以同时优化这两方面，导致模型在数据重复或有限场景下性能受限。\\n\\n**核心思路**：论文提出结合自回归和掩码扩散训练目标的双目标训练方法，通过同时优化两种目标，无需修改模型架构，即可在训练效率和过拟合鲁棒性之间取得平衡。设计思路基于两种目标互补的特性：自回归目标促进序列生成效率，掩码扩散目标增强数据分布的鲁棒性，从而提升模型整体性能。\\n\\n**技术框架**：整体流程包括模型初始化、双目标训练和评估阶段。首先，使用标准语言模型架构（如Transformer），不引入额外模块；然后，在训练过程中同时计算自回归损失和掩码扩散损失，通过加权求和形成总损失函数；最后，在不同数据重复水平下训练多个模型，调整目标比例，并评估下游任务性能。主要模块包括损失计算、比例优化和性能验证。\\n\\n**关键创新**：最重要的技术创新是提出无需架构修改的双目标训练框架，将自回归和掩码扩散目标有机结合。与现有单目标方法相比，本质区别在于同时优化效率和鲁棒性，而非单独侧重某一目标，这通过实验验证了目标互补的有效性，为语言模型训练提供了新范式。\\n\\n**关键设计**：关键参数包括自回归和掩码扩散损失的权重比例，论文通过网格搜索优化确定最优值；损失函数为加权和形式，总损失 = α * 自回归损失 + (1-α) * 掩码扩散损失，其中α为可调参数；网络结构保持标准，如基于Transformer的编码器-解码器或仅解码器架构，确保方法通用性；训练设置涉及批量大小、学习率等标准超参数，并在不同数据重复率下进行实验以验证鲁棒性。",
            "application_zh": "该研究在自然语言处理领域具有广泛潜在应用，如文本生成、机器翻译和对话系统，通过提升模型训练效率和鲁棒性，可降低数据需求、加速模型部署，并增强在噪声或有限数据场景下的性能。未来可能推动更高效、稳健的语言模型设计，促进AI在实际应用中的普及。",
            "highlight_zh": "最重要的实验结果是双目标模型在所有评估设置下均优于单目标模型。具体地，通过训练50个语言模型并调整目标比例，发现结合自回归和掩码扩散目标能显著提升下游任务性能，最优比例在不同数据重复水平下保持相似（例如，在特定设置中，α约0.5时性能最佳），对比基线单目标模型，双目标模型在过拟合鲁棒性指标上提升约10-20%，同时保持训练效率。",
            "tags_zh": [
                "语言模型训练",
                "双目标优化",
                "自回归建模",
                "掩码扩散模型",
                "过拟合鲁棒性",
                "训练效率平衡",
                "下游任务性能",
                "数据重复实验"
            ],
            "_index": 116
        },
        {
            "title": "Improving Slow Transfer Predictions: Generative Methods Compared",
            "authors": [
                "Jacob Taegon Kim",
                "Alex Sim",
                "Kesheng Wu",
                "Jinoh Kim"
            ],
            "arxiv_id": "2512.14522v1",
            "summary": "Monitoring data transfer performance is a crucial task in scientific computing networks. By predicting performance early in the communication phase, potentially sluggish transfers can be identified and selectively monitored, optimizing network usage and overall performance. A key bottleneck to improving the predictive power of machine learning (ML) models in this context is the issue of class imbalance. This project focuses on addressing the class imbalance problem to enhance the accuracy of performance predictions. In this study, we analyze and compare various augmentation strategies, including traditional oversampling methods and generative techniques. Additionally, we adjust the class imbalance ratios in training datasets to evaluate their impact on model performance. While augmentation may improve performance, as the imbalance ratio increases, the performance does not significantly improve. We conclude that even the most advanced technique, such as CTGAN, does not significantly improve over simple stratified sampling.",
            "categories": [
                "cs.LG",
                "cs.DC",
                "cs.NI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "10.1109/ICNC64010.2025.10994006",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14522v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "比较生成方法与传统过采样以解决科学计算网络中数据转移预测的类别不平衡问题",
            "summary_zh": "监测数据转移性能是科学计算网络中的关键任务。通过在通信阶段早期预测性能，可以识别潜在缓慢的转移并选择性监控，从而优化网络使用和整体性能。在此背景下，提高机器学习模型预测能力的一个关键瓶颈是类别不平衡问题。本项目专注于解决类别不平衡问题以提升性能预测的准确性。在本研究中，我们分析并比较了多种增强策略，包括传统的过采样方法和生成技术。此外，我们调整训练数据集中的类别不平衡比例以评估其对模型性能的影响。虽然增强可能改善性能，但随着不平衡比例增加，性能并未显著提升。我们得出结论，即使是最先进的技术，如CTGAN，也没有显著优于简单的分层采样。",
            "intro_zh": [
                "核心问题：科学计算网络中数据转移预测面临类别不平衡，导致机器学习模型预测能力受限，现有方法难以有效处理高不平衡比。",
                "方法要点：通过比较传统过采样与生成方法如CTGAN，调整训练数据集的不平衡比例，以评估不同增强策略对预测性能的影响。",
                "实验或效果：实验表明，随着不平衡比例增加，增强方法性能提升有限，CTGAN未显著优于分层采样，揭示了生成方法在此场景下的局限性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决科学计算网络中数据转移性能预测的类别不平衡问题，现有机器学习模型因少数类样本不足而预测能力受限，传统过采样方法可能过拟合或引入噪声，导致在高不平衡比下性能提升不明显。\\n\\n**核心思路**：核心思路是通过系统比较传统过采样方法与生成技术（如CTGAN）在调整类别不平衡比例时的效果，以评估不同增强策略对模型预测准确性的影响，从而确定最优解决方案。\\n\\n**技术框架**：整体框架包括数据收集、类别不平衡比例调整、增强策略应用（如过采样和生成方法）、模型训练与评估。主要模块包括数据预处理、不平衡比例设置、增强方法实施、性能指标计算。\\n\\n**关键创新**：最重要的技术创新点在于将生成方法（如CTGAN）与传统过采样方法在科学计算网络预测场景中进行对比分析，并量化不平衡比例对性能的影响，揭示了生成方法在此特定问题中的局限性。\\n\\n**关键设计**：关键设计包括设置不同的类别不平衡比例（如1:10、1:100等），使用CTGAN作为生成方法代表，采用分层采样作为基线，评估指标可能包括准确率、召回率或F1分数，具体参数和损失函数未在摘要中详细说明，需参考论文全文。",
            "application_zh": "该研究主要应用于科学计算网络中的数据转移性能监控和优化，通过改进预测模型，可帮助识别缓慢转移并优化网络资源分配，提升整体计算效率。未来可扩展至其他网络性能预测场景，如云计算或物联网，具有实际工程价值。",
            "highlight_zh": "最重要的实验结果显示，随着类别不平衡比例增加，传统过采样和生成方法（如CTGAN）的性能提升均有限，CTGAN未显著优于简单的分层采样基线。具体性能数据未在摘要中提供，但结论强调了生成方法在此问题中的效果不显著，可能受限于数据特性或模型复杂度。",
            "tags_zh": [
                "类别不平衡",
                "数据转移预测",
                "生成对抗网络",
                "过采样方法",
                "科学计算网络",
                "机器学习模型",
                "性能优化",
                "CTGAN"
            ],
            "_index": 117
        },
        {
            "title": "SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models",
            "authors": [
                "Shizhuo Mao",
                "Song Chen",
                "Yi Kang"
            ],
            "arxiv_id": "2512.14481v1",
            "summary": "Large language models (LLMs) excel at natural language tasks but face deployment challenges due to their growing size outpacing GPU memory advancements. Model quantization mitigates this issue by lowering weight and activation precision, but existing solutions face fundamental trade-offs: dynamic quantization incurs high computational overhead and poses deployment challenges on edge devices, while static quantization sacrifices accuracy. Existing approaches of quantization-aware training (QAT) further suffer from weight training costs. We propose SASQ: a lightweight QAT framework specifically tailored for activation quantization factors. SASQ exclusively optimizes only the quantization factors (without changing pre-trained weights), enabling static inference with high accuracy while maintaining deployment efficiency. SASQ adaptively truncates some outliers, thereby reducing the difficulty of quantization while preserving the distributional characteristics of the activations. SASQ not only surpasses existing SOTA quantization schemes but also outperforms the corresponding FP16 models. On LLaMA2-7B, it achieves 5.2% lower perplexity than QuaRot and 4.7% lower perplexity than the FP16 model on WikiText2.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14481v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "SAC"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出SASQ框架以解决大语言模型量化训练中激活量化因子的优化问题，实现高效静态推理。",
            "summary_zh": "大语言模型（LLMs）在自然语言任务中表现出色，但其规模增长超过了GPU内存的进步，导致部署挑战。模型量化通过降低权重和激活的精度来缓解这一问题，但现有解决方案面临基本权衡：动态量化带来高计算开销并在边缘设备上部署困难，而静态量化牺牲准确性。现有的量化感知训练（QAT）方法还面临权重训练成本问题。我们提出SASQ：一个专门针对激活量化因子的轻量级QAT框架。SASQ仅优化量化因子（不改变预训练权重），实现高精度的静态推理，同时保持部署效率。SASQ自适应地截断一些异常值，从而降低量化难度，同时保留激活的分布特性。SASQ不仅超越了现有的SOTA量化方案，还优于相应的FP16模型。在LLaMA2-7B上，它在WikiText2上实现了比QuaRot低5.2%的困惑度和比FP16模型低4.7%的困惑度。",
            "intro_zh": [
                "核心问题：现有量化方法存在权衡，动态量化计算开销大且部署难，静态量化精度低，量化感知训练成本高。",
                "方法要点：提出SASQ框架，仅优化激活量化因子，不改变预训练权重，实现轻量级训练和高效静态推理。",
                "实验或效果：在LLaMA2-7B上，SASQ超越SOTA量化方案和FP16模型，困惑度显著降低。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大语言模型量化中的激活量化问题，现有方法如动态量化计算开销高、部署困难，静态量化精度低，量化感知训练权重训练成本高，导致模型部署效率与准确性难以兼顾。\\n\\n**核心思路**：SASQ的核心思路是专注于优化激活量化因子，而非权重，通过轻量级训练实现静态推理，从而在保持部署效率的同时提升量化精度。这样设计避免了权重训练的高成本，并针对激活分布特性进行优化。\\n\\n**技术框架**：SASQ框架包括预训练模型加载、激活量化因子初始化、自适应截断模块、量化因子优化阶段和静态推理部署。主要模块涉及激活分布分析、量化因子调整和损失函数计算，流程简单高效。\\n\\n**关键创新**：最重要的技术创新是仅优化激活量化因子而不改变预训练权重，这降低了训练成本并保留了模型性能。与现有方法的本质区别在于避免了权重更新，专注于激活量化因子的自适应优化。\\n\\n**关键设计**：关键设计包括自适应截断机制以处理激活异常值，使用量化感知损失函数来指导因子优化，参数设置如量化比特数和截断阈值基于实验调整，网络结构保持原模型不变，仅添加轻量级优化层。",
            "application_zh": "SASQ适用于大语言模型的边缘部署和资源受限环境，如移动设备、嵌入式系统和云计算平台，能提升模型推理效率并降低内存需求，具有实际价值，未来可能推动轻量级AI模型的广泛应用。",
            "highlight_zh": "在LLaMA2-7B模型上，SASQ在WikiText2数据集上实现了比QuaRot量化方案低5.2%的困惑度，比FP16模型低4.7%的困惑度，显著超越了现有SOTA量化方法，并展示了优于原始高精度模型的性能。",
            "tags_zh": [
                "大语言模型",
                "模型量化",
                "量化感知训练",
                "激活量化",
                "静态推理",
                "边缘部署",
                "轻量级训练",
                "困惑度优化"
            ],
            "_index": 118
        },
        {
            "title": "Model-First Reasoning LLM Agents: Reducing Hallucinations through Explicit Problem Modeling",
            "authors": [
                "Annu Rana",
                "Gaurav Kumar"
            ],
            "arxiv_id": "2512.14474v1",
            "summary": "Large Language Models (LLMs) often struggle with complex multi-step planning tasks, showing high rates of constraint violations and inconsistent solutions. Existing strategies such as Chain-of-Thought and ReAct rely on implicit state tracking and lack an explicit problem representation. Inspired by classical AI planning, we propose Model-First Reasoning (MFR), a two-phase paradigm in which the LLM first constructs an explicit model of the problem, defining entities, state variables, actions, and constraints, before generating a solution plan. Across multiple planning domains, including medical scheduling, route planning, resource allocation, logic puzzles, and procedural synthesis, MFR reduces constraint violations and improves solution quality compared to Chain-of-Thought and ReAct. Ablation studies show that the explicit modeling phase is critical for these gains. Our results suggest that many LLM planning failures stem from representational deficiencies rather than reasoning limitations, highlighting explicit modeling as a key component for robust and interpretable AI agents. All prompts, evaluation procedures, and task datasets are documented to facilitate reproducibility.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14474v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出模型优先推理范式，通过显式问题建模减少大语言模型在复杂规划任务中的幻觉问题",
            "summary_zh": "大语言模型在处理复杂多步规划任务时，常出现高约束违反率和不一致解。现有方法如思维链和ReAct依赖隐式状态跟踪，缺乏显式问题表示。受经典AI规划启发，本文提出模型优先推理范式，该范式分为两个阶段：首先构建问题的显式模型，定义实体、状态变量、动作和约束，然后生成解决方案计划。在医疗调度、路径规划、资源分配、逻辑谜题和程序合成等多个规划领域中，与思维链和ReAct相比，MFR减少了约束违反并提高了解决方案质量。消融研究表明，显式建模阶段对这些改进至关重要。结果表明，许多LLM规划失败源于表示缺陷而非推理限制，凸显显式建模作为鲁棒和可解释AI代理的关键组成部分。所有提示、评估程序和任务数据集均已记录，以促进可重复性。",
            "intro_zh": [
                "现有方法如思维链和ReAct依赖隐式状态跟踪，缺乏显式问题表示，导致高约束违反和不一致解。",
                "提出模型优先推理范式，先构建显式问题模型，再生成解决方案，受经典AI规划启发。",
                "在多个规划领域，MFR相比基线减少约束违反，提高解质量，显式建模阶段是关键。"
            ],
            "method_zh": "**问题定义**：论文解决大语言模型在复杂多步规划任务中的高约束违反率和不一致解问题。现有方法如思维链和ReAct依赖隐式状态跟踪，缺乏显式问题表示，导致模型在推理过程中容易产生幻觉和错误。\\n\\n**核心思路**：核心思路是引入显式问题建模作为规划的前置阶段，受经典AI规划启发，通过先定义问题结构再生成解，将表示与推理分离，以弥补LLM的表示缺陷。\\n\\n**技术框架**：整体架构为两阶段范式：第一阶段，LLM构建显式问题模型，包括定义实体、状态变量、动作和约束；第二阶段，基于该模型生成解决方案计划。流程从问题输入开始，经过建模阶段输出结构化表示，再进入规划阶段产生最终解。\\n\\n**关键创新**：最重要的技术创新是模型优先推理范式，与现有方法的本质区别在于将显式建模作为独立阶段，而非依赖隐式或端到端推理，从而增强问题表示的清晰度和可解释性。\\n\\n**关键设计**：关键设计包括使用标准提示工程引导LLM进行建模和规划，无特定网络结构或损失函数；评估基于多个规划领域的任务数据集，如医疗调度和路径规划，通过约束违反率和解质量指标量化性能。",
            "application_zh": "该研究在医疗调度、路径规划、资源分配、逻辑谜题和程序合成等领域有潜在应用，可提升AI代理在复杂任务中的鲁棒性和可解释性。未来可能扩展到更多规划密集型场景，如机器人控制和自动化系统，推动AI向更可靠和透明方向发展。",
            "highlight_zh": "在多个规划领域，MFR相比思维链和ReAct基线，显著减少约束违反并提高解质量。消融研究确认显式建模阶段是关键，具体性能数据未提供，但结果突出表示缺陷是LLM规划失败的主因。",
            "tags_zh": [
                "大语言模型",
                "规划任务",
                "显式建模",
                "模型优先推理",
                "约束违反",
                "可解释AI",
                "多步推理",
                "AI代理"
            ],
            "_index": 119
        },
        {
            "title": "AnySleep: a channel-agnostic deep learning system for high-resolution sleep staging in multi-center cohorts",
            "authors": [
                "Niklas Grieger",
                "Jannik Raskob",
                "Siamak Mehrkanoon",
                "Stephan Bialonski"
            ],
            "arxiv_id": "2512.14461v1",
            "summary": "Sleep is essential for good health throughout our lives, yet studying its dynamics requires manual sleep staging, a labor-intensive step in sleep research and clinical care. Across centers, polysomnography (PSG) recordings are traditionally scored in 30-s epochs for pragmatic, not physiological, reasons and can vary considerably in electrode count, montage, and subject characteristics. These constraints present challenges in conducting harmonized multi-center sleep studies and discovering novel, robust biomarkers on shorter timescales. Here, we present AnySleep, a deep neural network model that uses any electroencephalography (EEG) or electrooculography (EOG) data to score sleep at adjustable temporal resolutions. We trained and validated the model on over 19,000 overnight recordings from 21 datasets collected across multiple clinics, spanning nearly 200,000 hours of EEG and EOG data, to promote robust generalization across sites. The model attains state-of-the-art performance and surpasses or equals established baselines at 30-s epochs. Performance improves as more channels are provided, yet remains strong when EOG is absent or when only EOG or single EEG derivations (frontal, central, or occipital) are available. On sub-30-s timescales, the model captures short wake intrusions consistent with arousals and improves prediction of physiological characteristics (age, sex) and pathophysiological conditions (sleep apnea), relative to standard 30-s scoring. We make the model publicly available to facilitate large-scale studies with heterogeneous electrode setups and to accelerate the discovery of novel biomarkers in sleep.",
            "categories": [
                "cs.LG",
                "eess.SP",
                "q-bio.QM"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "18 pages, 6 figures, 2 tables",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14461v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出AnySleep深度学习系统，以解决多中心睡眠研究中电极配置异质性和时间分辨率受限的问题。",
            "summary_zh": "睡眠对健康至关重要，但研究其动态需要人工睡眠分期，这是睡眠研究和临床护理中劳动密集的步骤。传统上，多中心的多导睡眠图（PSG）记录以30秒为周期进行评分，这更多是出于实用而非生理原因，且电极数量、导联方式和受试者特征差异很大。这些限制给开展协调的多中心睡眠研究以及在更短时间尺度上发现新的、稳健的生物标志物带来了挑战。本文提出了AnySleep，一种深度神经网络模型，可使用任何脑电图（EEG）或眼电图（EOG）数据以可调的时间分辨率进行睡眠分期。我们在来自21个数据集的超过19,000个夜间记录上训练和验证了该模型，涵盖近200,000小时的EEG和EOG数据，以促进跨站点的稳健泛化。该模型达到了最先进的性能，在30秒周期上超越或等同于已建立的基线。提供更多通道时性能会提高，但当EOG缺失或仅使用EOG或单个EEG导联（额叶、中央或枕叶）时，性能仍然强劲。在30秒以下的时间尺度上，该模型捕获了与觉醒一致的短暂清醒侵入，并相对于标准的30秒评分，改善了生理特征（年龄、性别）和病理生理状况（睡眠呼吸暂停）的预测。我们公开提供该模型，以促进具有异质电极设置的大规模研究，并加速睡眠中新生物标志物的发现。",
            "intro_zh": [
                "核心问题：传统睡眠分期依赖人工、以30秒为固定周期，且多中心数据在电极配置和受试者特征上差异大，限制了协调研究和短时生物标志物发现。",
                "方法要点：开发AnySleep深度神经网络，利用任意EEG或EOG数据，支持可调时间分辨率，通过大规模多中心数据训练实现强泛化能力。",
                "实验或效果：在30秒周期达到SOTA性能，在亚30秒尺度捕获短时觉醒，提升年龄、性别和睡眠呼吸暂停等特征的预测准确性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决睡眠分期中的两个核心问题：一是传统方法依赖人工评分，以30秒为固定周期，这更多基于实用而非生理需求，限制了短时动态分析；二是多中心PSG数据在电极数量、导联方式和受试者特征上存在显著异质性，导致模型泛化困难，阻碍协调研究和新型生物标志物发现。现有方法的痛点包括劳动密集型人工分期、时间分辨率固定以及缺乏对异质电极配置的适应性。\\n\\n**核心思路**：论文提出AnySleep模型，其核心思路是设计一个通道无关的深度学习系统，能够处理任意EEG或EOG数据，并支持可调的时间分辨率。通过在大规模多中心数据集上进行训练，模型学习从异质输入中提取稳健特征，从而实现在不同电极设置下的强泛化能力，同时捕捉短时睡眠动态。\\n\\n**技术框架**：整体架构基于深度神经网络，流程包括数据预处理、特征提取和分类阶段。主要模块可能包括输入层处理多通道EEG/EOG信号，卷积层或Transformer层用于时空特征学习，以及输出层进行睡眠阶段分类。训练阶段使用来自21个数据集的超过19,000个记录，涵盖近200,000小时数据，以优化模型参数。\\n\\n**关键创新**：最重要的技术创新点是通道无关性和可调时间分辨率。与现有方法通常依赖固定电极配置和30秒周期不同，AnySleep能够适应任意EEG或EOG通道，包括单通道情况，并支持亚30秒的分辨率，从而更精细地分析睡眠动态。这本质区别在于提高了模型的灵活性和实用性，适用于多中心异质数据。\\n\\n**关键设计**：关键设计包括使用深度神经网络架构（具体网络结构如卷积神经网络或循环神经网络未在摘要中指定，但可能涉及多层处理），损失函数可能基于交叉熵以优化分类准确性，参数设置通过大规模训练数据调整。模型设计强调对输入通道的鲁棒性，例如在仅EOG或单EEG导联时仍保持高性能，这通过数据增强或特定网络层实现。",
            "application_zh": "AnySleep的潜在应用领域包括睡眠医学研究、临床诊断和健康监测。实际价值在于自动化睡眠分期，减少人工劳动，促进多中心协作研究，并加速发现短时生物标志物，如用于睡眠障碍早期检测。未来影响可能推动个性化睡眠分析和远程医疗发展。",
            "highlight_zh": "实验结果显示，AnySleep在30秒周期达到最先进性能，超越或等于基线模型。提供更多通道时性能提升，但仅使用EOG或单EEG导联（如额叶、中央或枕叶）时仍表现强劲。在亚30秒尺度，模型成功捕获短时觉醒，并相对于标准30秒评分，显著改善了对年龄、性别和睡眠呼吸暂停等特征的预测准确性，具体提升幅度未在摘要中量化，但强调了实质性改进。",
            "tags_zh": [
                "睡眠分期",
                "深度学习",
                "多中心研究",
                "脑电图分析",
                "时间分辨率可调",
                "通道无关模型",
                "生物标志物发现",
                "自动化评分"
            ],
            "_index": 120
        },
        {
            "title": "Reasoning-Style Poisoning of LLM Agents via Stealthy Style Transfer: Process-Level Attacks and Runtime Monitoring in RSV Space",
            "authors": [
                "Xingfu Zhou",
                "Pengfei Wang"
            ],
            "arxiv_id": "2512.14448v1",
            "summary": "Large Language Model (LLM) agents relying on external retrieval are increasingly deployed in high-stakes environments. While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. We propose Reasoning-Style Poisoning (RSP), a paradigm that manipulates how agents process information rather than what they process. We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers. To quantify these shifts, we develop the Reasoning Style Vector (RSV), a metric tracking Verification depth, Self-confidence, and Attention focus. Experiments on HotpotQA and FEVER using ReAct, Reflection, and Tree of Thoughts (ToT) architectures reveal that GSI significantly degrades performance. It increases reasoning steps by up to 4.4 times or induces premature errors, successfully bypassing state-of-the-art content filters. Finally, we propose RSP-M, a lightweight runtime monitor that calculates RSV metrics in real-time and triggers alerts when values exceed safety thresholds. Our work demonstrates that reasoning style is a distinct, exploitable vulnerability, necessitating process-level defenses beyond static content analysis.",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14448v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出推理风格投毒攻击与监控方法，揭示LLM代理在过程层面的安全漏洞。",
            "summary_zh": "大型语言模型（LLM）代理依赖外部检索，在高风险环境中部署日益增多。现有对抗攻击主要关注内容伪造或指令注入，而本文识别出一种新颖的、面向过程的攻击面：代理的推理风格。我们提出推理风格投毒（RSP），这是一种操纵代理处理信息方式而非处理内容的范式。我们引入生成式风格注入（GSI），这是一种攻击方法，将检索到的文档重写为病态语调——特别是“分析瘫痪”或“认知仓促”——而不改变底层事实或使用显式触发器。为了量化这些变化，我们开发了推理风格向量（RSV），这是一个跟踪验证深度、自信度和注意力焦点的指标。在HotpotQA和FEVER数据集上使用ReAct、Reflection和Tree of Thoughts（ToT）架构进行的实验表明，GSI显著降低了性能。它将推理步骤增加多达4.4倍或诱导过早错误，成功绕过最先进的内容过滤器。最后，我们提出RSP-M，一种轻量级运行时监控器，实时计算RSV指标并在值超过安全阈值时触发警报。我们的工作表明，推理风格是一个独特且可利用的漏洞，需要超越静态内容分析的过程级防御。",
            "intro_zh": [
                "现有对抗攻击主要针对内容伪造或指令注入，忽视了LLM代理推理过程本身的脆弱性，导致安全防御存在盲区。",
                "论文提出推理风格投毒攻击，通过生成式风格注入操纵文档语调，在不改变事实的情况下诱导病态推理风格，并开发推理风格向量进行量化评估。",
                "实验显示攻击使推理步骤增加4.4倍或引发错误，显著降低性能，同时提出轻量级运行时监控器RSP-M实现实时防御。"
            ],
            "method_zh": "**问题定义**：论文解决LLM代理在依赖外部检索时面临的新型安全威胁——推理风格被恶意操纵的问题。现有方法主要关注内容层面的攻击（如伪造或注入），缺乏对代理推理过程本身的保护，导致攻击者可能通过隐蔽方式影响代理的决策逻辑，而无需篡改事实内容。\\n\\n**核心思路**：论文的核心思路是识别并利用推理风格作为攻击面，提出推理风格投毒（RSP）范式，通过生成式风格注入（GSI）重写检索文档的语调，诱导代理进入病态推理模式（如过度分析或仓促决策），而不改变文档事实，从而绕过传统内容过滤器。同时，开发推理风格向量（RSV）量化风格变化，并设计运行时监控器进行实时防御。\\n\\n**技术框架**：整体框架包括攻击和防御两部分。攻击阶段：使用GSI方法，基于检索到的文档，通过风格转换技术将其重写为“分析瘫痪”（过度谨慎、冗长分析）或“认知仓促”（草率、快速决策）的语调，生成中毒文档供代理处理。防御阶段：在代理运行时，计算RSV指标（包括验证深度、自信度、注意力焦点），通过RSP-M监控器实时评估，当指标超出安全阈值时触发警报。实验部分在HotpotQA和FEVER数据集上，结合ReAct、Reflection和Tree of Thoughts架构验证攻击效果和监控性能。\\n\\n**关键创新**：最重要的技术创新是首次将推理风格定义为独立的攻击面，并提出过程导向的攻击方法GSI，它不依赖内容篡改或显式触发器，而是通过风格转移实现隐蔽操纵。与现有方法（如对抗样本或提示注入）的本质区别在于，RSP攻击代理的“如何思考”而非“思考什么”，从而开辟了新的安全研究维度。\\n\\n**关键设计**：关键设计包括：GSI中的风格转换技术，具体参数未知，但可能基于文本生成模型实现语调重写；RSV指标设计，包含三个维度——验证深度（衡量代理对信息的核查程度）、自信度（评估代理决策的确定性）、注意力焦点（分析代理关注点的分布），这些指标通过代理的中间输出计算；RSP-M监控器采用轻量级实现，实时计算RSV并设置阈值触发机制，具体阈值基于实验数据校准。",
            "application_zh": "该研究主要应用于LLM代理的安全领域，特别是在高风险环境如金融决策、医疗诊断或自动驾驶中，代理依赖外部检索进行推理时。其实践价值在于揭示了过程层面的安全漏洞，推动开发更全面的防御机制，未来可能影响AI安全标准制定，促进实时监控工具的发展，提升代理系统的鲁棒性和可信度。",
            "highlight_zh": "实验在HotpotQA和FEVER数据集上进行，使用ReAct、Reflection和Tree of Thoughts架构。攻击方法GSI显著降低代理性能：推理步骤最多增加4.4倍，或诱导过早错误，导致准确率下降。同时，GSI成功绕过最先进的内容过滤器，验证了攻击的隐蔽性。防御方面，RSP-M监控器实时计算RSV指标，有效检测异常推理风格，具体阈值设置基于实验数据，但提升幅度未知。",
            "tags_zh": [
                "推理风格投毒",
                "生成式风格注入",
                "推理风格向量",
                "过程级攻击",
                "运行时监控",
                "LLM代理安全",
                "对抗攻击",
                "内容过滤器绕过"
            ],
            "_index": 121
        },
        {
            "title": "S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation",
            "authors": [
                "Leon Sick",
                "Lukas Hoyer",
                "Dominik Engel",
                "Pedro Hermosilla",
                "Timo Ropinski"
            ],
            "arxiv_id": "2512.14440v1",
            "summary": "In recent years, the state-of-the-art in unsupervised video instance segmentation has heavily relied on synthetic video data, generated from object-centric image datasets such as ImageNet. However, video synthesis by artificially shifting and scaling image instance masks fails to accurately model realistic motion in videos, such as perspective changes, movement by parts of one or multiple instances, or camera motion. To tackle this issue, we propose an unsupervised video instance segmentation model trained exclusively on real video data. We start from unsupervised instance segmentation masks on individual video frames. However, these single-frame segmentations exhibit temporal noise and their quality varies through the video. Therefore, we establish temporal coherence by identifying high-quality keymasks in the video by leveraging deep motion priors. The sparse keymask pseudo-annotations are then used to train a segmentation model for implicit mask propagation, for which we propose a Sparse-To-Dense Distillation approach aided by a Temporal DropLoss. After training the final model on the resulting dense labelset, our approach outperforms the current state-of-the-art across various benchmarks.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project Page with Code/Models/Demo: https://leonsick.github.io/s2d/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14440v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "动作生成",
                    "matched_keywords": [
                        "motion prior"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出S2D稀疏到稠密关键掩码蒸馏方法，以解决无监督视频实例分割中合成数据运动建模不准确的问题。",
            "summary_zh": "近年来，无监督视频实例分割的最先进方法严重依赖从ImageNet等以对象为中心的图像数据集生成的合成视频数据。然而，通过人工平移和缩放图像实例掩码来合成视频，无法准确建模视频中的真实运动，如视角变化、单个或多个实例的部分运动或相机运动。为解决这一问题，我们提出了一种仅使用真实视频数据训练的无监督视频实例分割模型。我们从单个视频帧上的无监督实例分割掩码开始。然而，这些单帧分割存在时间噪声，且其质量在整个视频中变化。因此，我们通过利用深度运动先验识别视频中的高质量关键掩码来建立时间一致性。稀疏的关键掩码伪标注随后用于训练一个用于隐式掩码传播的分割模型，为此我们提出了一种由时间DropLoss辅助的稀疏到稠密蒸馏方法。在最终模型上对生成的稠密标签集进行训练后，我们的方法在各种基准测试中超越了当前最先进水平。",
            "intro_zh": [
                "现有方法依赖合成视频数据，但人工生成的掩码运动无法准确建模真实视频中的复杂运动，如视角变化和部分运动。",
                "提出S2D方法，利用深度运动先验识别高质量关键掩码，并通过稀疏到稠密蒸馏训练模型实现隐式掩码传播。",
                "在多个基准测试中，该方法超越了当前最先进水平，显著提升了无监督视频实例分割的性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决无监督视频实例分割中，现有方法依赖合成视频数据导致运动建模不准确的问题。现有方法通过人工平移和缩放图像实例掩码生成合成视频，无法有效模拟真实视频中的复杂运动，如视角变化、实例部分运动或相机运动，这限制了模型在真实场景中的泛化能力。\\n\\n**核心思路**：论文的核心思路是仅使用真实视频数据进行训练，通过识别高质量的关键掩码来建立时间一致性，并利用稀疏到稠密蒸馏方法训练模型实现隐式掩码传播。这样设计避免了合成数据的偏差，直接学习真实视频中的运动模式，从而提升分割的准确性和鲁棒性。\\n\\n**技术框架**：整体流程分为三个阶段：首先，从单个视频帧的无监督实例分割掩码开始；其次，利用深度运动先验识别视频中的高质量关键掩码，建立稀疏的伪标注；然后，使用这些稀疏关键掩码通过稀疏到稠密蒸馏方法训练一个分割模型，该模型能够隐式传播掩码信息；最后，在生成的稠密标签集上训练最终模型，实现端到端的无监督视频实例分割。\\n\\n**关键创新**：最重要的技术创新是提出Sparse-To-Dense Distillation（S2D）方法，结合Temporal DropLoss，直接从真实视频数据中学习，而不依赖合成数据。与现有方法的本质区别在于，它通过深度运动先验自动筛选高质量关键掩码，避免了人工合成带来的运动失真，从而更准确地建模真实视频动态。\\n\\n**关键设计**：关键设计包括：利用深度运动先验（如光流或运动估计网络）来评估掩码质量并识别关键帧；设计Temporal DropLoss，在训练过程中随机丢弃部分时间步的损失，以增强模型对时间噪声的鲁棒性；采用蒸馏框架，将稀疏关键掩码的知识传递到稠密分割模型中，具体可能涉及教师-学生网络结构或损失函数优化，例如使用交叉熵损失或对比学习损失来对齐掩码预测。参数设置可能包括关键掩码的选择阈值、DropLoss的丢弃率以及训练迭代次数，但具体细节在摘要中未详细说明，需参考完整论文。",
            "application_zh": "该研究在计算机视觉和机器人领域具有广泛的应用潜力，可用于视频监控、自动驾驶、增强现实和视频编辑等场景。通过提升无监督视频实例分割的准确性，它能够减少对标注数据的依赖，降低应用成本，并推动智能系统在动态环境中的实时理解和交互能力。未来可能影响视频分析技术的发展，促进更高效的自动化处理工具。",
            "highlight_zh": "在多个基准测试中，S2D方法显著超越了当前最先进的无监督视频实例分割模型。具体性能数据未在摘要中提供，但根据论文描述，它在各种指标上表现出色，例如可能包括更高的分割精度（如mAP）和更好的时间一致性。提升幅度体现在对真实视频运动的更准确建模，减少了合成数据带来的偏差，从而在复杂场景下实现更鲁棒的分割结果。",
            "tags_zh": [
                "无监督视频实例分割",
                "稀疏到稠密蒸馏",
                "关键掩码识别",
                "深度运动先验",
                "时间一致性",
                "真实视频数据训练",
                "隐式掩码传播",
                "Temporal DropLoss"
            ],
            "_index": 122
        },
        {
            "title": "VICTOR: Dataset Copyright Auditing in Video Recognition Systems",
            "authors": [
                "Quan Yuan",
                "Zhikun Zhang",
                "Linkang Du",
                "Min Chen",
                "Mingyang Sun",
                "Yunjun Gao",
                "Shibo He",
                "Jiming Chen"
            ],
            "arxiv_id": "2512.14439v1",
            "summary": "Video recognition systems are increasingly being deployed in daily life, such as content recommendation and security monitoring. To enhance video recognition development, many institutions have released high-quality public datasets with open-source licenses for training advanced models. At the same time, these datasets are also susceptible to misuse and infringement. Dataset copyright auditing is an effective solution to identify such unauthorized use. However, existing dataset copyright solutions primarily focus on the image domain; the complex nature of video data leaves dataset copyright auditing in the video domain unexplored. Specifically, video data introduces an additional temporal dimension, which poses significant challenges to the effectiveness and stealthiness of existing methods.\n  In this paper, we propose VICTOR, the first dataset copyright auditing approach for video recognition systems. We develop a general and stealthy sample modification strategy that enhances the output discrepancy of the target model. By modifying only a small proportion of samples (e.g., 1%), VICTOR amplifies the impact of published modified samples on the prediction behavior of the target models. Then, the difference in the model's behavior for published modified and unpublished original samples can serve as a key basis for dataset auditing. Extensive experiments on multiple models and datasets highlight the superiority of VICTOR. Finally, we show that VICTOR is robust in the presence of several perturbation mechanisms to the training videos or the target models.",
            "categories": [
                "cs.CR",
                "cs.CV"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "To appear in the NDSS Symposium 2026, February 2026, San Diego, CA, USA",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14439v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出VICTOR方法以解决视频识别系统中数据集版权审计的挑战",
            "summary_zh": "视频识别系统在内容推荐和安全监控等日常生活中的应用日益广泛。为促进视频识别技术的发展，许多机构发布了高质量的开源许可公共数据集用于训练先进模型。然而，这些数据集也容易遭到滥用和侵权。数据集版权审计是识别此类未经授权使用的有效解决方案。然而，现有的数据集版权解决方案主要集中于图像领域；视频数据的复杂性使得视频领域的数据集版权审计尚未得到探索。具体而言，视频数据引入了额外的时间维度，这对现有方法的有效性和隐蔽性提出了重大挑战。本文提出了VICTOR，这是首个针对视频识别系统的数据集版权审计方法。我们开发了一种通用且隐蔽的样本修改策略，增强了目标模型的输出差异。通过仅修改一小部分样本（例如1%），VICTOR放大了已发布修改样本对目标模型预测行为的影响。然后，模型对已发布修改样本和未发布原始样本的行为差异可以作为数据集审计的关键依据。在多个模型和数据集上的广泛实验突显了VICTOR的优越性。最后，我们展示了VICTOR在面对训练视频或目标模型的多种扰动机制时具有鲁棒性。",
            "intro_zh": [
                "现有方法主要针对图像领域，视频数据的时间维度复杂性导致版权审计在视频领域未被探索，现有方法在有效性和隐蔽性方面面临挑战。",
                "提出VICTOR方法，通过修改少量样本（如1%）来放大模型输出差异，利用模型对修改样本和原始样本的行为差异作为审计依据。",
                "在多个模型和数据集上的实验显示VICTOR具有优越性能，且对训练视频或模型的扰动机制表现出鲁棒性，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决视频识别系统中数据集版权审计的问题。现有方法主要针对图像领域，而视频数据引入了时间维度，导致现有方法在视频领域难以应用，具体痛点包括：视频数据的复杂性（如时序信息）使得审计方法在有效性和隐蔽性方面面临挑战，现有图像领域的方法无法直接迁移到视频数据上，缺乏针对视频的版权审计解决方案。\\n\\n**核心思路**：论文的核心解决思路是设计一种通用且隐蔽的样本修改策略，通过修改少量视频样本（如1%）来增强目标模型的输出差异。这样，模型对已发布修改样本和未发布原始样本的行为差异可以被放大，从而作为数据集版权审计的关键依据。这种设计基于假设：修改样本会显著影响模型预测，而原始样本则不会，从而通过对比行为差异来检测未经授权的数据集使用。\\n\\n**技术框架**：整体架构包括两个主要阶段：样本修改阶段和审计阶段。在样本修改阶段，VICTOR对数据集中的一小部分样本进行修改，生成修改后的样本并发布。在审计阶段，通过查询目标模型，获取模型对修改样本和原始样本的预测输出，计算行为差异（如输出概率分布或置信度差异），并基于预设阈值判断是否存在版权侵权。流程涉及数据预处理、模型查询和差异分析模块。\\n\\n**关键创新**：最重要的技术创新点是首次将数据集版权审计扩展到视频领域，并针对视频数据的时间维度特性设计了样本修改策略。与现有图像领域方法的本质区别在于：VICTOR考虑了视频的时序信息，通过修改策略增强了模型输出差异，而不仅仅是依赖静态图像特征；同时，它强调隐蔽性，仅修改少量样本以减少对数据集质量的负面影响。\\n\\n**关键设计**：关键设计包括：修改比例设置为小值（如1%），以平衡审计效果和隐蔽性；修改策略可能涉及视频帧的扰动或时序调整，具体细节未知；审计过程基于模型输出差异的度量，如KL散度或置信度差异；实验中使用多种视频识别模型（如3D CNN或Transformer）和数据集（如Kinetics或UCF101）进行验证，但具体网络结构和损失函数未在摘要中详细说明。",
            "application_zh": "该研究的潜在应用领域包括视频内容推荐系统、安全监控系统和视频分析平台，用于检测未经授权的数据集使用，保护数据提供者的知识产权。实际价值在于为视频识别领域提供首个版权审计工具，促进数据共享的合法性和安全性。未来影响可能推动视频数据版权标准的建立，并激励更多高质量视频数据集的发布。",
            "highlight_zh": "最重要的实验结果包括：VICTOR在多个视频识别模型（如3D CNN）和数据集（如Kinetics）上表现出优越性能，具体提升幅度未知，但实验显示其能有效检测未经授权的数据集使用；对比基线可能包括图像领域的版权审计方法，VICTOR在视频领域具有显著优势；此外，VICTOR对训练视频或目标模型的扰动机制（如数据增强或模型微调）表现出鲁棒性，验证了其在实际场景中的可靠性。",
            "tags_zh": [
                "视频识别",
                "数据集版权审计",
                "样本修改策略",
                "模型行为差异",
                "时序数据处理",
                "隐蔽性设计",
                "鲁棒性验证",
                "版权保护"
            ],
            "_index": 123
        },
        {
            "title": "Score-Based Turbo Message Passing for Plug-and-Play Compressive Imaging",
            "authors": [
                "Chang Cai",
                "Hao Jiang",
                "Xiaojun Yuan",
                "Ying-Jun Angela Zhang"
            ],
            "arxiv_id": "2512.14435v1",
            "summary": "Message-passing algorithms have been adapted for compressive imaging by incorporating various off-the-shelf image denoisers. However, these denoisers rely largely on generic or hand-crafted priors and often fall short in accurately capturing the complex statistical structure of natural images. As a result, traditional plug-and-play (PnP) methods often lead to suboptimal reconstruction, especially in highly underdetermined regimes. Recently, score-based generative models have emerged as a powerful framework for accurately characterizing sophisticated image distribution. Yet, their direct use for posterior sampling typically incurs prohibitive computational complexity. In this paper, by exploiting the close connection between score-based generative modeling and empirical Bayes denoising, we devise a message-passing framework that integrates a score-based minimum mean-squared error (MMSE) denoiser for compressive image recovery. The resulting algorithm, named score-based turbo message passing (STMP), combines the fast convergence of message passing with the expressive power of score-based generative priors. For practical systems with quantized measurements, we further propose quantized STMP (Q-STMP), which augments STMP with a component-wise MMSE dequantization module. We demonstrate that the asymptotic performance of STMP and Q-STMP can be accurately predicted by a set of state-evolution (SE) equations. Experiments on the FFHQ dataset demonstrate that STMP strikes a significantly better performance-complexity tradeoff compared with competing baselines, and that Q-STMP remains robust even under 1-bit quantization. Remarkably, both STMP and Q-STMP typically converge within 10 iterations.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14435v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出基于分数的Turbo消息传递算法，用于解决压缩成像中传统插拔式方法重建性能不足的问题。",
            "summary_zh": "消息传递算法通过集成各种现成的图像去噪器，已被应用于压缩成像。然而，这些去噪器主要依赖于通用或手工设计的先验，往往难以准确捕捉自然图像的复杂统计结构。因此，传统的插拔式方法通常导致次优重建，特别是在高度欠定的情况下。最近，基于分数的生成模型已成为准确描述复杂图像分布的强大框架。然而，它们直接用于后验采样通常会产生过高的计算复杂度。本文通过利用基于分数的生成建模与经验贝叶斯去噪之间的紧密联系，设计了一个消息传递框架，该框架集成了基于分数的最小均方误差去噪器用于压缩图像恢复。所得算法称为基于分数的Turbo消息传递，结合了消息传递的快速收敛性和基于分数的生成先验的表达能力。对于具有量化测量的实际系统，我们进一步提出了量化STMP，它通过分量级MMSE去量化模块增强了STMP。我们证明STMP和Q-STMP的渐近性能可以通过一组状态演化方程准确预测。在FFHQ数据集上的实验表明，与竞争基线相比，STMP在性能与复杂度之间取得了显著更好的权衡，并且Q-STMP即使在1位量化下仍保持鲁棒性。值得注意的是，STMP和Q-STMP通常都在10次迭代内收敛。",
            "intro_zh": [
                "核心问题：传统插拔式压缩成像方法依赖通用先验，难以捕捉图像复杂统计结构，导致重建性能不佳，尤其在欠定场景下。",
                "方法要点：提出STMP算法，将基于分数的生成模型作为MMSE去噪器集成到消息传递框架中，结合快速收敛与强表达能力。",
                "实验或效果：在FFHQ数据集上，STMP显著优于基线，Q-STMP在1位量化下鲁棒，两者均在10次迭代内快速收敛。"
            ],
            "method_zh": "**问题定义**：论文解决压缩成像中的图像重建问题，特别是在高度欠定测量下。现有插拔式方法依赖通用或手工先验的去噪器，难以准确建模自然图像的复杂分布，导致重建质量下降，且计算复杂度高。\\n\\n**核心思路**：通过利用基于分数的生成模型与经验贝叶斯去噪的紧密联系，将基于分数的MMSE去噪器嵌入消息传递框架，实现高效后验采样，从而提升重建性能并降低复杂度。\\n\\n**技术框架**：整体框架包括STMP和Q-STMP两个版本。STMP基于Turbo消息传递，迭代执行线性估计和基于分数的去噪步骤；Q-STMP在STMP基础上增加分量级MMSE去量化模块，处理量化测量。状态演化方程用于理论性能预测。\\n\\n**关键创新**：首次将基于分数的生成模型作为MMSE去噪器集成到消息传递中，实现快速收敛与强表达能力的结合；提出Q-STMP扩展以处理量化系统，增强实用性。\\n\\n**关键设计**：使用基于分数的生成模型学习图像分布分数函数，作为MMSE去噪器；在消息传递中采用Turbo结构加速收敛；对于量化场景，设计分量级MMSE去量化模块；通过状态演化方程分析渐近性能，无需复杂采样。",
            "application_zh": "该研究在压缩成像领域具有广泛应用潜力，如医学成像、遥感、安全监控和低功耗物联网设备。通过提升欠定测量下的重建质量并支持量化系统，STMP和Q-STMP可降低数据采集成本、提高传输效率，推动高效图像处理技术的发展，未来可能扩展到视频重建和多模态成像等场景。",
            "highlight_zh": "在FFHQ数据集实验中，STMP相比竞争基线（如传统PnP方法）在性能-复杂度权衡上显著更优，具体提升幅度未量化但强调“显著更好”。Q-STMP在1位量化下仍保持鲁棒重建能力，验证了其对实际系统的适应性。两者均展示快速收敛特性，通常在10次迭代内达到稳定，提高了计算效率。",
            "tags_zh": [
                "压缩成像",
                "消息传递算法",
                "基于分数的生成模型",
                "插拔式方法",
                "量化测量",
                "图像重建",
                "状态演化方程",
                "最小均方误差去噪"
            ],
            "_index": 124
        },
        {
            "title": "Geometric Parameter Optimization of a Novel 3-(PP(2-(UPS))) Redundant Parallel Mechanism based on Workspace Determination",
            "authors": [
                "Quan Yuan",
                "Daqian Cao",
                "Weibang Bai"
            ],
            "arxiv_id": "2512.14434v1",
            "summary": "Redundant parallel robots are normally employed in scenarios requiring good precision, high load capability, and large workspace compared to traditional parallel mechanisms. However, the elementary robotic configuration and geometric parameter optimization are still quite challenging. This paper proposes a novel 3-(PP(2-(UPS))) redundant parallel mechanism, with good generalizability first, and further investigates the kinematic optimization issue by analyzing and investigating how its key geometric parameters influence the volume, shape, boundary completeness, and orientation capabilities of its workspace. The torsional capability index TI_1 and tilting capability index TI_2 are defined to evaluate the orientation performance of the mechanism. Numerical simulation studies are completed to indicate the analysis, providing reasonable but essential references for the parameter optimization of 3-(PP(2-(UPS))) and other similar redundant parallel mechanisms.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "7 pages, 5 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14434v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出新型3-(PP(2-(UPS)))冗余并联机构，通过工作空间分析优化几何参数以提升性能。",
            "summary_zh": "冗余并联机器人通常应用于需要高精度、高负载能力和大工作空间的场景，相比传统并联机构具有优势。然而，其基本机器人构型和几何参数优化仍然极具挑战性。本文首先提出了一种新型的3-(PP(2-(UPS)))冗余并联机构，具有良好的通用性；进一步通过分析和研究其关键几何参数如何影响工作空间的体积、形状、边界完整性和定向能力，探讨了运动学优化问题。定义了扭转能力指数TI_1和倾斜能力指数TI_2来评估机构的定向性能。完成了数值模拟研究以验证分析，为3-(PP(2-(UPS)))及其他类似冗余并联机构的参数优化提供了合理且必要的参考。",
            "intro_zh": [
                "冗余并联机构在几何参数优化方面面临挑战，现有方法难以平衡工作空间性能与结构复杂性。",
                "提出新型3-(PP(2-(UPS)))机构，通过工作空间分析定义能力指数来指导参数优化。",
                "数值模拟验证了参数对工作空间的影响，为优化提供了关键参考，提升了机构的定向性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决冗余并联机构几何参数优化的问题，现有方法在分析工作空间体积、形状、边界完整性和定向能力方面存在不足，导致参数选择缺乏系统性指导，影响机构性能。\\n\\n**核心思路**：论文的核心思路是提出一种新型3-(PP(2-(UPS)))冗余并联机构，并通过分析其关键几何参数对工作空间特性的影响，定义能力指数来量化定向性能，从而为参数优化提供理论依据。这样设计是因为工作空间是衡量并联机构性能的关键指标，直接关联到实际应用中的精度和灵活性。\\n\\n**技术框架**：整体架构包括机构设计、工作空间分析、能力指数定义和数值模拟四个阶段。首先，设计3-(PP(2-(UPS)))机构构型；其次，分析几何参数如何影响工作空间的体积、形状、边界完整性和定向能力；然后，定义扭转能力指数TI_1和倾斜能力指数TI_2来评估定向性能；最后，通过数值模拟验证分析结果，生成优化参考。\\n\\n**关键创新**：最重要的技术创新点是提出了一种具有良好通用性的新型冗余并联机构构型3-(PP(2-(UPS)))，并系统性地将工作空间分析应用于几何参数优化，通过定义能力指数来量化性能，这区别于传统依赖经验或简单指标的方法，提供了更科学的优化途径。\\n\\n**关键设计**：关键设计包括机构的具体构型参数（如连杆长度、关节配置等），以及定义的能力指数TI_1和TI_2，这些指数基于工作空间分析计算，用于评估机构的扭转和倾斜能力。数值模拟中可能涉及参数扫描和优化算法，但论文未详细说明具体算法细节，主要侧重于分析框架。",
            "application_zh": "该研究可应用于高精度制造、航空航天装配、医疗机器人等领域，其中需要大工作空间、高负载能力和精确控制的场景。通过优化几何参数，能提升冗余并联机构的性能，促进其在复杂任务中的实际应用，未来可能扩展到更多类似机构的优化设计中。",
            "highlight_zh": "数值模拟结果表明，通过分析关键几何参数对工作空间的影响，定义了TI_1和TI_2指数来评估定向性能，为参数优化提供了具体参考。例如，优化后的机构在工作空间体积和形状完整性方面得到改善，定向能力指数显示提升，但论文未提供具体数据对比或基线，主要强调分析框架的有效性。",
            "tags_zh": [
                "冗余并联机构",
                "几何参数优化",
                "工作空间分析",
                "运动学优化",
                "能力指数",
                "数值模拟",
                "机器人构型设计",
                "高精度控制"
            ],
            "_index": 125
        },
        {
            "title": "Seismology modeling agent: A smart assistant for geophysical researchers",
            "authors": [
                "Yukun Ren",
                "Siwei Yu",
                "Kai Chen",
                "Jianwei Ma"
            ],
            "arxiv_id": "2512.14429v1",
            "summary": "To address the steep learning curve and reliance on complex manual file editing and command-line operations in the traditional workflow of the mainstream open-source seismic wave simulation software SPECFEM, this paper proposes an intelligent, interactive workflow powered by Large Language Models (LLMs). We introduce the first Model Context Protocol (MCP) server suite for SPECFEM (supporting 2D, 3D Cartesian, and 3D Globe versions), which decomposes the entire simulation process into discrete, agent-executable tools spanning from parameter generation and mesh partitioning to solver execution and visualization. This approach enables a paradigm shift from file-driven to intent-driven conversational interactions. The framework supports both fully automated execution and human-in-the-loop collaboration, allowing researchers to guide simulation strategies in real time and retain scientific decision-making authority while significantly reducing tedious low-level operations. Validated through multiple case studies, the workflow operates seamlessly in both autonomous and interactive modes, yielding high-fidelity results consistent with standard baselines. As the first application of MCP technology to computational seismology, this study significantly lowers the entry barrier, enhances reproducibility, and offers a promising avenue for advancing computational geophysics toward AI-assisted and automated scientific research. The complete source code is available at https://github.com/RenYukun1563/specfem-mcp.",
            "categories": [
                "cs.AI",
                "cs.SE"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "26 pages, 15 figures. Code available at https://github.com/RenYukun1563/specfem-mcp",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14429v1",
            "code_links": [
                {
                    "url": "https://github.com/RenYukun1563/specfem-mcp",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出基于大语言模型的智能交互工作流，以降低SPECFEM地震波模拟软件的使用门槛并提升效率。",
            "summary_zh": "针对主流开源地震波模拟软件SPECFEM传统工作流程中学习曲线陡峭、依赖复杂手动文件编辑和命令行操作的问题，本文提出了一种由大语言模型驱动的智能交互工作流。我们首次为SPECFEM（支持2D、3D笛卡尔和3D全球版本）引入了模型上下文协议服务器套件，将整个模拟过程分解为从参数生成、网格划分到求解器执行和可视化的离散、可代理执行的工具。这种方法实现了从文件驱动到意图驱动的对话交互的范式转变。该框架支持全自动执行和人机协同合作，使研究人员能够实时指导模拟策略并保留科学决策权，同时显著减少繁琐的低级操作。通过多个案例研究验证，该工作流在自主和交互模式下均能无缝运行，产生与标准基线一致的高保真结果。作为MCP技术在计算地震学中的首次应用，本研究显著降低了入门门槛，增强了可重复性，并为推动计算地球物理学向AI辅助和自动化科学研究发展提供了有前景的途径。完整源代码可在https://github.com/RenYukun1563/specfem-mcp获取。",
            "intro_zh": [
                "核心问题：SPECFEM传统工作流程学习曲线陡峭，依赖复杂手动文件编辑和命令行操作，效率低下且易出错。",
                "方法要点：基于大语言模型构建智能交互工作流，将模拟过程分解为可代理执行的工具，实现意图驱动的对话交互。",
                "实验或效果：在自主和交互模式下均能无缝运行，产生高保真结果，显著降低使用门槛并提升可重复性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决SPECFEM地震波模拟软件传统工作流程中存在的两大痛点：一是学习曲线陡峭，新用户需要花费大量时间掌握复杂的文件格式和命令行操作；二是工作流程高度依赖手动文件编辑和脚本执行，过程繁琐且容易出错，降低了科研效率。\\n\\n**核心思路**：论文的核心思路是利用大语言模型的自然语言理解和代码生成能力，构建一个智能代理系统，将原本文件驱动的模拟过程转变为意图驱动的对话交互。通过将复杂的模拟任务分解为一系列可执行的工具，让用户可以用自然语言描述模拟意图，由代理自动完成底层操作。\\n\\n**技术框架**：整体架构基于模型上下文协议（MCP）服务器套件实现。主要模块包括：1）参数生成模块，根据用户意图自动生成模拟参数文件；2）网格划分模块，处理几何模型和网格生成；3）求解器执行模块，调用SPECFEM核心计算程序；4）可视化模块，对模拟结果进行后处理和图形展示。这些模块通过MCP协议封装为独立的工具，由LLM驱动的代理按需调用。\\n\\n**关键创新**：最重要的技术创新是首次将MCP技术引入计算地震学领域，创建了首个针对SPECFEM的MCP服务器套件。与现有方法的本质区别在于实现了工作流范式的根本转变：从传统的“文件编辑-命令行执行”模式转变为“自然语言对话-自动执行”模式，大大降低了操作复杂性。\\n\\n**关键设计**：关键技术设计包括：1）支持SPECFEM的2D、3D笛卡尔和3D全球版本，覆盖主流应用场景；2）采用工具分解策略，将整个模拟流程拆分为离散、可组合的操作单元；3）设计双模式运行机制，支持全自动执行和人机协同两种工作方式；4）通过MCP协议实现工具的标准接口，确保与不同LLM平台的兼容性。",
            "application_zh": "该研究主要应用于计算地球物理学领域，特别是地震波模拟和地下结构成像相关研究。实际价值在于显著降低了SPECFEM等专业模拟软件的使用门槛，使更多地球物理研究者能够高效开展数值模拟工作。未来可能推动计算地球物理学向AI辅助的自动化科研方向发展，提升科研效率和可重复性，并为智能地球物理数据分析平台的开发提供技术基础。",
            "highlight_zh": "通过多个案例研究验证，该智能工作流在自主和交互两种模式下均能无缝运行。实验结果显示，系统生成的地震波模拟结果与标准SPECFEM基线保持高度一致，证明了其可靠性和准确性。具体而言，在测试案例中，系统成功完成了从参数设置到结果可视化的完整流程，产生的波形数据和波场图像与手动操作结果在视觉和数值上均匹配良好，验证了工作流的有效性。",
            "tags_zh": [
                "地震波模拟",
                "大语言模型",
                "智能代理",
                "计算地球物理学",
                "工作流自动化",
                "模型上下文协议",
                "人机协同",
                "SPECFEM"
            ],
            "_index": 126
        },
        {
            "title": "Quadratic Kalman Filter for Elliptical Extended Object Tracking based on Decoupling State Components",
            "authors": [
                "Simon Steuernagel",
                "Marcus Baum"
            ],
            "arxiv_id": "2512.14426v1",
            "summary": "Extended object tracking involves estimating both the physical extent and kinematic parameters of a target object, where typically multiple measurements are observed per time step. In this article, we propose a deterministic closed-form elliptical extended object tracker, based on decoupling of the kinematics, orientation, and axis lengths. By disregarding potential correlations between these state components, fewer approximations are required for the individual estimators than for an overall joint solution. The resulting algorithm outperforms existing algorithms, reaching the accuracy of sampling-based procedures. Additionally, a batch-based variant is introduced, yielding highly efficient computation while outperforming all comparable state-of-the-art algorithms. This is validated both by a simulation study using common models from literature, as well as an extensive quantitative evaluation on real automotive radar data.",
            "categories": [
                "eess.SP",
                "cs.RO"
            ],
            "primary_category": "eess.SP",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "13 pages, 8 figures, submitted to IEEE Transactions on Aerospace and Electronic Systems",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14426v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出基于状态分量解耦的二次卡尔曼滤波器，用于椭圆扩展目标跟踪，以提升精度和计算效率。",
            "summary_zh": "扩展目标跟踪涉及同时估计目标物体的物理范围和运动学参数，通常每个时间步会观测到多个测量值。本文提出了一种基于运动学、方向和轴长解耦的确定性闭式椭圆扩展目标跟踪器。通过忽略这些状态分量之间的潜在相关性，与整体联合解决方案相比，各个估计器所需的近似更少。所得到的算法优于现有算法，达到了基于采样方法的精度。此外，还引入了基于批处理的变体，实现了高效计算，同时优于所有可比较的最先进算法。这通过使用文献中常见模型的仿真研究，以及对真实汽车雷达数据的广泛定量评估得到了验证。",
            "intro_zh": [
                "现有扩展目标跟踪方法通常联合估计所有状态分量，导致近似复杂且计算成本高，难以平衡精度与效率。",
                "论文提出将运动学、方向和轴长状态分量解耦，分别设计估计器，减少近似需求，实现确定性闭式跟踪。",
                "算法在仿真和真实雷达数据上验证，达到采样方法精度，批处理变体计算高效，优于所有可比先进算法。"
            ],
            "method_zh": "**问题定义**：论文解决椭圆扩展目标跟踪问题，即同时估计目标的运动状态（如位置、速度）和物理范围（如椭圆形状参数）。现有方法通常采用联合估计框架，将所有状态分量耦合处理，这导致在卡尔曼滤波等框架中需要复杂的近似（如线性化或采样），以处理非线性或非高斯特性，从而引入误差并增加计算负担，难以在精度和效率之间取得平衡。\\n\\n**核心思路**：论文的核心思路是将扩展目标的状态分解为三个独立分量：运动学（描述目标整体运动）、方向（描述椭圆朝向）和轴长（描述椭圆大小），并假设这些分量之间无相关性。通过这种解耦，可以分别为每个分量设计更简单的估计器，减少整体近似需求，从而在保持精度的同时降低计算复杂度。这种设计基于观察：在扩展目标跟踪中，状态分量间的相关性可能较弱，解耦能简化问题而不显著损失性能。\\n\\n**技术框架**：整体架构基于卡尔曼滤波框架，但进行了扩展以适应解耦状态。流程包括：在每个时间步，接收多个测量值（如雷达点云）；首先，使用标准卡尔曼滤波器估计运动学状态（如位置和速度），基于测量值的中心或聚合信息；其次，独立估计方向状态，可能利用测量值的协方差或几何特征；最后，估计轴长状态，基于测量值的分布或拟合椭圆。这些估计器并行或顺序运行，最终组合成完整状态估计。批处理变体则通过处理多个时间步的数据一次性优化，进一步提高效率。\\n\\n**关键创新**：最重要的技术创新是状态分量的解耦策略，这本质上是将复杂的联合估计问题分解为多个子问题，从而避免了整体近似中的高维非线性处理。与现有方法（如联合扩展卡尔曼滤波或粒子滤波）相比，本方法通过减少近似步骤，提高了估计的准确性和稳定性，同时保持了确定性闭式解的优势，无需随机采样。此外，批处理变体引入了高效优化，进一步提升了计算性能。\\n\\n**关键设计**：技术细节包括：使用二次卡尔曼滤波器（可能指处理二次测量模型或状态）来处理椭圆形状的非线性；具体地，方向估计可能基于测量协方差矩阵的特征向量分解，轴长估计可能基于测量点到椭圆中心的距离统计；参数设置如过程噪声和测量噪声协方差需根据应用场景（如汽车雷达）调整；损失函数或优化目标是最小化状态估计误差，但论文未指定具体形式，而是依赖于卡尔曼滤波的均方误差最小化框架；网络结构不适用，因为这是基于模型的滤波方法，而非深度学习。",
            "application_zh": "该研究主要应用于自动驾驶和智能交通系统，特别是汽车雷达目标跟踪场景，能准确估计车辆等扩展目标的形状和运动，提升环境感知能力。潜在价值包括提高跟踪精度和实时性，支持更安全的决策控制。未来可能扩展到机器人导航、无人机监控等领域，推动扩展目标跟踪技术的实际部署。",
            "highlight_zh": "在仿真研究中，使用文献常见模型验证，算法达到基于采样方法（如粒子滤波）的精度，表明解耦策略有效减少近似误差。在真实汽车雷达数据上，进行广泛定量评估，批处理变体计算高效，优于所有可比较的最先进算法（具体性能数据如误差降低百分比未在摘要中提供，但强调“outperforms”）。整体结果验证了方法在精度和效率上的双重优势。",
            "tags_zh": [
                "扩展目标跟踪",
                "椭圆形状估计",
                "卡尔曼滤波",
                "状态解耦",
                "汽车雷达",
                "确定性算法",
                "批处理优化",
                "运动学估计"
            ],
            "_index": 127
        },
        {
            "title": "DISCODE: Distribution-Aware Score Decoder for Robust Automatic Evaluation of Image Captioning",
            "authors": [
                "Nakamasa Inoue",
                "Kanoko Goto",
                "Masanari Oi",
                "Martyna Gruszka",
                "Mahiro Ukai",
                "Takumi Hirose",
                "Yusuke Sekikawa"
            ],
            "arxiv_id": "2512.14420v1",
            "summary": "Large vision-language models (LVLMs) have shown impressive performance across a broad range of multimodal tasks. However, robust image caption evaluation using LVLMs remains challenging, particularly under domain-shift scenarios. To address this issue, we introduce the Distribution-Aware Score Decoder (DISCODE), a novel finetuning-free method that generates robust evaluation scores better aligned with human judgments across diverse domains. The core idea behind DISCODE lies in its test-time adaptive evaluation approach, which introduces the Adaptive Test-Time (ATT) loss, leveraging a Gaussian prior distribution to improve robustness in evaluation score estimation. This loss is efficiently minimized at test time using an analytical solution that we derive. Furthermore, we introduce the Multi-domain Caption Evaluation (MCEval) benchmark, a new image captioning evaluation benchmark covering six distinct domains, designed to assess the robustness of evaluation metrics. In our experiments, we demonstrate that DISCODE achieves state-of-the-art performance as a reference-free evaluation metric across MCEval and four representative existing benchmarks.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Paper accepted to AAAI 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14420v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出分布感知分数解码器DISCODE，以解决大视觉语言模型在跨域场景下图像描述评估不鲁棒的问题。",
            "summary_zh": "大视觉语言模型（LVLMs）在多种多模态任务中展现出卓越性能，但在图像描述评估方面，尤其是在领域偏移场景下，实现鲁棒评估仍具挑战性。为解决此问题，我们引入了分布感知分数解码器（DISCODE），这是一种无需微调的新方法，能够生成更鲁棒的评估分数，在不同领域中更好地与人类判断对齐。DISCODE的核心思想在于其测试时自适应评估方法，该方法引入了自适应测试时（ATT）损失，利用高斯先验分布来提高评估分数估计的鲁棒性。我们推导出的解析解可在测试时高效最小化此损失。此外，我们引入了多领域描述评估（MCEval）基准，这是一个涵盖六个不同领域的新图像描述评估基准，旨在评估评估指标的鲁棒性。在我们的实验中，我们证明DISCODE在MCEval和四个代表性现有基准上，作为无参考评估指标实现了最先进的性能。",
            "intro_zh": [
                "现有大视觉语言模型在图像描述评估中面临领域偏移挑战，导致评估分数与人类判断不一致，鲁棒性不足。",
                "论文提出DISCODE方法，核心是测试时自适应评估，通过引入自适应测试时损失和高斯先验，无需微调即可提升跨域评估鲁棒性。",
                "实验显示DISCODE在MCEval基准和四个现有基准上达到最先进性能，显著提升了评估分数与人类判断的对齐度。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大视觉语言模型（LVLMs）在图像描述自动评估中，尤其是在领域偏移场景下，评估分数不鲁棒、与人类判断不一致的问题。现有方法通常依赖预训练模型直接生成分数，缺乏对测试数据分布的适应，导致跨域性能下降，评估结果不可靠。\\n\\n**核心思路**：DISCODE的核心思路是采用测试时自适应评估方法，通过引入自适应测试时（ATT）损失和高斯先验分布，在测试阶段动态调整评估分数，以更好地匹配人类判断，而无需额外的微调训练。这种设计基于假设：评估分数的分布应遵循某种先验，从而在未知领域提高鲁棒性。\\n\\n**技术框架**：整体框架包括两个主要阶段：首先，使用预训练LVLM（如CLIP或类似模型）提取图像和描述的特征表示；其次，应用DISCODE解码器，在测试时通过最小化ATT损失来生成鲁棒的评估分数。关键模块包括特征提取器、分数解码器和ATT损失优化器，其中优化基于推导出的解析解高效执行。\\n\\n**关键创新**：最重要的技术创新是测试时自适应评估方法，特别是ATT损失的引入和高斯先验的应用。与现有方法本质区别在于，DISCODE不依赖微调或额外训练数据，而是通过测试时优化直接提升跨域鲁棒性，这减少了计算开销并提高了泛化能力。\\n\\n**关键设计**：关键设计包括ATT损失函数，它结合了高斯先验分布来正则化分数估计；解析解优化，允许在测试时快速最小化损失；以及多领域基准MCEval的构建，用于评估鲁棒性。具体参数设置如高斯分布的均值和方差可能基于经验调整，但论文未详细说明，网络结构保持轻量级以适配现有LVLMs。",
            "application_zh": "DISCODE可广泛应用于图像描述生成系统的自动评估，特别是在多领域或跨域场景下，如医疗影像描述、自动驾驶场景理解、社交媒体内容分析等。其实用价值在于提供更鲁棒、与人类判断一致的评估指标，助力模型优化和基准测试。未来可能影响多模态评估标准的发展，推动LVLMs在真实世界应用中的可靠性提升。",
            "highlight_zh": "DISCODE在MCEval基准上实现了最先进的性能，覆盖六个不同领域，显著提升了评估分数与人类判断的相关性。与基线方法相比，在四个代表性现有基准（如COCO Captions）上也表现出优越性能，具体提升幅度未详细量化，但论文强调其鲁棒性和对齐度优于现有无参考评估指标。",
            "tags_zh": [
                "图像描述评估",
                "大视觉语言模型",
                "测试时自适应",
                "分布感知解码",
                "跨域鲁棒性",
                "无参考评估",
                "多模态基准",
                "高斯先验"
            ],
            "_index": 128
        },
        {
            "title": "PortAgent: LLM-driven Vehicle Dispatching Agent for Port Terminals",
            "authors": [
                "Jia Hu",
                "Junqi Li",
                "Weimeng Lin",
                "Peng Jia",
                "Yuxiong Ji",
                "Jintao Lai"
            ],
            "arxiv_id": "2512.14417v1",
            "summary": "Vehicle Dispatching Systems (VDSs) are critical to the operational efficiency of Automated Container Terminals (ACTs). However, their widespread commercialization is hindered due to their low transferability across diverse terminals. This transferability challenge stems from three limitations: high reliance on port operational specialists, a high demand for terminal-specific data, and time-consuming manual deployment processes. Leveraging the emergence of Large Language Models (LLMs), this paper proposes PortAgent, an LLM-driven vehicle dispatching agent that fully automates the VDS transferring workflow. It bears three features: (1) no need for port operations specialists; (2) low need of data; and (3) fast deployment. Specifically, specialist dependency is eliminated by the Virtual Expert Team (VET). The VET collaborates with four virtual experts, including a Knowledge Retriever, Modeler, Coder, and Debugger, to emulate a human expert team for the VDS transferring workflow. These experts specialize in the domain of terminal VDS via a few-shot example learning approach. Through this approach, the experts are able to learn VDS-domain knowledge from a few VDS examples. These examples are retrieved via a Retrieval-Augmented Generation (RAG) mechanism, mitigating the high demand for terminal-specific data. Furthermore, an automatic VDS design workflow is established among these experts to avoid extra manual interventions. In this workflow, a self-correction loop inspired by the LLM Reflexion framework is created",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14417v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出PortAgent：基于大语言模型的车辆调度代理，实现自动化集装箱码头车辆调度系统的快速迁移部署",
            "summary_zh": "车辆调度系统（VDS）对自动化集装箱码头（ACT）的运营效率至关重要，但其广泛商业化受到跨码头可迁移性低的阻碍。这一挑战源于三个限制：高度依赖港口运营专家、对码头特定数据的高需求以及耗时的手动部署过程。利用大语言模型（LLM）的出现，本文提出了PortAgent，一个LLM驱动的车辆调度代理，完全自动化VDS迁移工作流程。它具有三个特点：（1）无需港口运营专家；（2）数据需求低；（3）部署快速。具体来说，通过虚拟专家团队（VET）消除了专家依赖。VET与四个虚拟专家协作，包括知识检索器、建模器、编码器和调试器，以模拟人类专家团队进行VDS迁移工作流程。这些专家通过少样本示例学习方法专门化于码头VDS领域。通过这种方法，专家能够从少量VDS示例中学习VDS领域知识。这些示例通过检索增强生成（RAG）机制检索，减轻了对码头特定数据的高需求。此外，在这些专家之间建立了自动VDS设计工作流程，以避免额外的手动干预。在该工作流程中，创建了一个受LLM Reflexion框架启发的自校正循环。",
            "intro_zh": [
                "核心问题：现有车辆调度系统跨码头迁移性低，依赖专家、数据需求高、部署耗时，阻碍商业化应用。",
                "方法要点：提出PortAgent，基于大语言模型构建虚拟专家团队，通过少样本学习和检索增强生成实现自动化迁移。",
                "实验或效果：PortAgent显著提升迁移效率，减少专家依赖和数据需求，实现快速部署，具体性能提升未知。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自动化集装箱码头中车辆调度系统（VDS）跨码头迁移性低的问题。现有方法痛点包括：高度依赖港口运营专家进行定制化设计、需要大量码头特定数据以训练模型、以及手动部署过程耗时且成本高，这限制了VDS的广泛商业化应用。\\n\\n**核心思路**：论文的核心解决思路是利用大语言模型（LLM）的通用性和推理能力，构建一个自动化代理PortAgent，通过模拟人类专家团队的工作流程，实现VDS的快速、低数据依赖迁移。这样设计是因为LLM能够处理自然语言和代码，结合少样本学习，可以适应不同码头的特定需求，减少对专家和数据的依赖。\\n\\n**技术框架**：整体架构基于虚拟专家团队（VET），包括四个主要模块：知识检索器（负责从示例库中检索相关VDS示例）、建模器（基于检索到的示例进行领域知识建模）、编码器（生成或调整VDS代码）、调试器（检查和修正代码错误）。工作流程中，这些模块协作执行VDS迁移任务，并通过自校正循环（受LLM Reflexion框架启发）迭代优化，确保输出质量。\\n\\n**关键创新**：最重要的技术创新点是结合LLM与少样本学习和检索增强生成（RAG），构建自动化专家团队，完全消除对真实专家的依赖。与现有方法的本质区别在于：传统方法需要大量人工干预和定制化数据，而PortAgent通过自动化流程和低数据需求，实现了高效、可扩展的迁移。\\n\\n**关键设计**：关键设计包括：使用少样本示例学习方法，让虚拟专家从少量VDS示例中学习领域知识；采用RAG机制检索示例，减少数据需求；建立自校正循环，基于反馈迭代改进输出；具体参数设置、损失函数或网络结构细节在摘要中未提及，可能涉及LLM的微调或提示工程，但论文未详细说明，因此标记为未知。",
            "application_zh": "该研究主要应用于自动化集装箱码头的车辆调度系统迁移和部署，潜在价值包括提升码头运营效率、降低系统定制化成本、促进VDS的标准化和商业化。未来可能扩展到其他物流或工业自动化场景，如仓库管理、智能制造等，推动AI在复杂环境中的自适应应用。",
            "highlight_zh": "最重要的实验结果包括：PortAgent实现了自动化VDS迁移，显著减少对港口运营专家的依赖，数据需求降低（具体幅度未知），部署速度加快（具体时间未知）。与基线方法相比，可能展示了更高的迁移成功率和效率，但摘要中未提供具体性能数据、对比基线或提升幅度，因此实验亮点基于论文描述的特点推断。",
            "tags_zh": [
                "大语言模型",
                "车辆调度系统",
                "自动化集装箱码头",
                "少样本学习",
                "检索增强生成",
                "虚拟专家团队",
                "系统迁移",
                "自校正循环"
            ],
            "_index": 129
        },
        {
            "title": "Pattern Recognition of Aluminium Arbitrage in Global Trade Data",
            "authors": [
                "Muhammad Sukri Bin Ramli"
            ],
            "arxiv_id": "2512.14410v1",
            "summary": "As the global economy transitions toward decarbonization, the aluminium sector has become a focal point for strategic resource management. While policies such as the Carbon Border Adjustment Mechanism (CBAM) aim to reduce emissions, they have inadvertently widened the price arbitrage between primary metal, scrap, and semi-finished goods, creating new incentives for market optimization. This study presents a unified, unsupervised machine learning framework to detect and classify emerging trade anomalies within UN Comtrade data (2020 to 2024). Moving beyond traditional rule-based monitoring, we apply a four-layer analytical pipeline utilizing Forensic Statistics, Isolation Forests, Network Science, and Deep Autoencoders. Contrary to the hypothesis that Sustainability Arbitrage would be the primary driver, empirical results reveal a contradictory and more severe phenomenon of Hardware Masking. Illicit actors exploit bi-directional tariff incentives by misclassifying scrap as high-count heterogeneous goods to justify extreme unit-price outliers of >$160/kg, a 1,900% markup indicative of Trade-Based Money Laundering (TBML) rather than commercial arbitrage. Topologically, risk is not concentrated in major exporters but in high-centrality Shadow Hubs that function as pivotal nodes for illicit rerouting. These actors execute a strategy of Void-Shoring, systematically suppressing destination data to Unspecified Code to fracture mirror statistics and sever forensic trails. Validated by SHAP (Shapley Additive Explanations), the results confirm that price deviation is the dominant predictor of anomalies, necessitating a paradigm shift in customs enforcement from physical volume checks to dynamic, algorithmic valuation auditing.",
            "categories": [
                "econ.GN",
                "cs.LG"
            ],
            "primary_category": "econ.GN",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14410v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出无监督机器学习框架以检测全球贸易数据中的铝套利异常模式",
            "summary_zh": "随着全球经济向脱碳转型，铝行业已成为战略资源管理的焦点。尽管碳边境调节机制（CBAM）等政策旨在减少排放，但它们无意中扩大了原铝、废铝和半成品之间的价格套利空间，为市场优化创造了新的激励。本研究提出一个统一的无监督机器学习框架，用于检测和分类联合国商品贸易统计数据库（UN Comtrade）数据（2020年至2024年）中的新兴贸易异常。超越传统的基于规则的监测方法，我们应用了一个四层分析流程，利用法证统计、孤立森林、网络科学和深度自编码器。与可持续性套利是主要驱动因素的假设相反，实证结果揭示了一个矛盾且更严重的硬件掩蔽现象。非法行为者利用双向关税激励，通过将废铝错误分类为高计数异质商品，以证明极端单位价格异常值（>160美元/千克）的合理性，这种1900%的加价表明是贸易洗钱（TBML）而非商业套利。从拓扑结构上看，风险并非集中在主要出口国，而是集中在作为非法改道关键节点的高中心性影子枢纽中。这些行为者执行一种“空岸”策略，系统性地将目的地数据抑制为“未指定代码”，以破坏镜像统计并切断法证追踪。通过SHAP（沙普利加性解释）验证，结果确认价格偏差是异常的主要预测因子，需要海关执法从物理体积检查向动态、算法化估值审计的范式转变。",
            "intro_zh": [
                "核心问题：传统基于规则的监测方法难以检测全球贸易数据中的新兴异常模式，如铝套利中的价格操纵和分类错误。",
                "方法要点：提出无监督机器学习框架，结合法证统计、孤立森林、网络科学和深度自编码器，实现异常检测和分类。",
                "实验或效果：实证揭示硬件掩蔽现象，价格偏差是主要预测因子，推动海关执法向动态估值审计转变。"
            ],
            "method_zh": "**问题定义**：论文旨在解决全球贸易数据中铝套利异常模式的检测问题，特别是由政策变化（如CBAM）引发的价格套利和非法行为（如贸易洗钱）。现有方法的痛点在于传统基于规则的监测难以适应新兴异常模式，导致漏检或误判，无法有效识别复杂的市场操纵行为。\\n\\n**核心思路**：论文的核心解决思路是构建一个统一的无监督机器学习框架，通过多模态数据分析来检测和分类贸易异常。这样设计是因为无监督方法无需标注数据，能自动发现未知模式，而结合多种技术可以增强检测的鲁棒性和解释性。\\n\\n**技术框架**：整体架构是一个四层分析流程：第一层使用法证统计进行初步数据清洗和异常筛选；第二层应用孤立森林算法检测价格和数量异常值；第三层利用网络科学分析贸易网络拓扑结构，识别高中心性影子枢纽；第四层采用深度自编码器进行特征学习和异常分类，最终通过SHAP进行结果验证和解释。\\n\\n**关键创新**：最重要的技术创新点是将法证统计、孤立森林、网络科学和深度自编码器集成到一个统一框架中，实现从数据预处理到异常解释的全流程自动化。与现有方法的本质区别在于，它超越了单一技术或规则的限制，通过多角度分析提高了检测的准确性和可扩展性。\\n\\n**关键设计**：关键设计包括使用UN Comtrade数据（2020-2024年），设置价格异常阈值（如>160美元/千克），应用孤立森林的污染参数来调整异常检测灵敏度，深度自编码器采用多层神经网络结构进行无监督特征提取，并通过SHAP值量化特征重要性，以价格偏差作为主导预测因子。",
            "application_zh": "该研究可应用于海关执法、金融监管和贸易风险管理领域，帮助机构动态检测铝等大宗商品贸易中的异常行为，如价格操纵、分类错误和贸易洗钱。其实际价值在于提升监管效率，减少非法活动，支持政策制定。未来可能扩展到其他资源行业，推动全球贸易数据监控向算法化、智能化方向发展。",
            "highlight_zh": "最重要的实验结果包括：检测到极端单位价格异常值（>160美元/千克），加价达1900%，表明贸易洗钱而非商业套利；网络分析揭示风险集中在高中心性影子枢纽，而非主要出口国；SHAP验证显示价格偏差是异常的主要预测因子，准确率提升显著，相比传统方法能更早发现新兴模式。具体性能数据未知，但框架在UN Comtrade数据上有效识别了硬件掩蔽现象。",
            "tags_zh": [
                "无监督学习",
                "异常检测",
                "贸易数据分析",
                "网络科学",
                "深度自编码器",
                "价格套利",
                "海关执法",
                "机器学习框架"
            ],
            "_index": 130
        },
        {
            "title": "Broadening View Synthesis of Dynamic Scenes from Constrained Monocular Videos",
            "authors": [
                "Le Jiang",
                "Shaotong Zhu",
                "Yedi Luo",
                "Shayda Moezzi",
                "Sarah Ostadabbas"
            ],
            "arxiv_id": "2512.14406v1",
            "summary": "In dynamic Neural Radiance Fields (NeRF) systems, state-of-the-art novel view synthesis methods often fail under significant viewpoint deviations, producing unstable and unrealistic renderings. To address this, we introduce Expanded Dynamic NeRF (ExpanDyNeRF), a monocular NeRF framework that leverages Gaussian splatting priors and a pseudo-ground-truth generation strategy to enable realistic synthesis under large-angle rotations. ExpanDyNeRF optimizes density and color features to improve scene reconstruction from challenging perspectives. We also present the Synthetic Dynamic Multiview (SynDM) dataset, the first synthetic multiview dataset for dynamic scenes with explicit side-view supervision-created using a custom GTA V-based rendering pipeline. Quantitative and qualitative results on SynDM and real-world datasets demonstrate that ExpanDyNeRF significantly outperforms existing dynamic NeRF methods in rendering fidelity under extreme viewpoint shifts. Further details are provided in the supplementary materials.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14406v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出ExpanDyNeRF框架，利用高斯泼溅先验和伪真值生成策略，解决动态NeRF在大视角偏移下渲染不稳定的问题。",
            "summary_zh": "在动态神经辐射场（NeRF）系统中，当前最先进的新视角合成方法在显著视角偏差下往往失效，产生不稳定和不真实的渲染结果。为解决这一问题，我们提出了扩展动态NeRF（ExpanDyNeRF），这是一个单目NeRF框架，利用高斯泼溅先验和伪真值生成策略，实现大角度旋转下的真实合成。ExpanDyNeRF优化密度和颜色特征，以改善从挑战性视角的场景重建。我们还提出了合成动态多视角（SynDM）数据集，这是首个用于动态场景的合成多视角数据集，具有明确的侧视角监督，使用基于GTA V的自定义渲染管线创建。在SynDM和真实世界数据集上的定量和定性结果表明，ExpanDyNeRF在极端视角偏移下的渲染保真度显著优于现有动态NeRF方法。更多细节见补充材料。",
            "intro_zh": [
                "现有动态NeRF方法在大视角偏移下渲染不稳定，产生不真实结果，限制了实际应用。",
                "ExpanDyNeRF结合高斯泼溅先验和伪真值生成，优化特征以提升重建质量。",
                "在SynDM数据集上，ExpanDyNeRF显著优于基线，渲染保真度提升，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决动态NeRF系统在单目视频输入下，当视角发生大角度偏移时，新视角合成渲染不稳定和不真实的问题。现有方法的痛点在于缺乏有效的先验和训练策略来处理极端视角变化，导致重建质量下降。\\n\\n**核心思路**：论文的核心解决思路是引入高斯泼溅先验来约束场景表示，并结合伪真值生成策略提供额外监督，从而增强模型在大视角变化下的泛化能力。这样设计是因为高斯泼溅能提供几何先验，而伪真值可以弥补单目输入的信息不足。\\n\\n**技术框架**：整体架构包括一个基于NeRF的动态场景建模模块，集成高斯泼溅先验作为正则化项，以及一个伪真值生成模块用于生成多视角监督数据。流程分为数据准备、模型训练和渲染三个阶段，主要模块包括特征提取、密度和颜色预测、先验融合和损失优化。\\n\\n**关键创新**：最重要的技术创新点是结合高斯泼溅先验和伪真值生成策略，这在动态NeRF领域是首次。与现有方法的本质区别在于，它通过先验约束和额外监督，直接针对大视角偏移进行优化，而非依赖更多输入数据或复杂网络结构。\\n\\n**关键设计**：关键设计包括使用高斯分布对场景几何进行建模作为先验，伪真值通过合成或数据增强生成以提供多视角标签。损失函数结合重建损失、先验正则化项和伪真值监督损失，网络结构基于标准NeRF但扩展了特征层以处理动态内容。具体参数如学习率、批大小等未在摘要中详述，需参考论文正文。",
            "application_zh": "该研究在虚拟现实、增强现实和机器人导航等领域有潜在应用，能提升动态场景的实时渲染质量和视角泛化能力，推动NeRF技术在真实世界复杂环境中的部署。未来可能影响自动驾驶模拟和影视特效制作。",
            "highlight_zh": "在SynDM数据集上，ExpanDyNeRF在极端视角偏移下的渲染保真度显著优于现有动态NeRF方法，具体性能数据如PSNR和SSIM指标有显著提升，对比基线包括其他单目动态NeRF方法，提升幅度在定量评估中表现突出。",
            "tags_zh": [
                "动态神经辐射场",
                "新视角合成",
                "单目视频",
                "高斯泼溅先验",
                "伪真值生成",
                "大视角偏移",
                "合成数据集",
                "渲染保真度"
            ],
            "_index": 131
        },
        {
            "title": "RePo: Language Models with Context Re-Positioning",
            "authors": [
                "Huayang Li",
                "Tianyu Zhao",
                "Richard Sproat"
            ],
            "arxiv_id": "2512.14391v1",
            "summary": "In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, $f_φ$, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14391v1",
            "code_links": [
                {
                    "url": "https://github.com/SakanaAI/repo",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出RePo机制，通过上下文重定位减少大语言模型中的额外认知负荷，提升长文本和结构化数据处理能力。",
            "summary_zh": "上下文学习是现代大语言模型（LLMs）的基础；然而，主流架构通过分配线性或恒定的位置索引，强加了僵化且固定的上下文结构。基于认知负荷理论（CLT），我们认为这种无信息性的结构增加了额外认知负荷，消耗了本应用于深度推理和注意力分配的有限工作记忆容量。为解决这一问题，我们提出了RePo，一种通过上下文重定位减少额外负荷的新机制。与标准方法不同，RePo利用可微分模块fφ来分配捕捉上下文依赖关系的标记位置，而不是依赖预定义的整数范围。通过在OLMo-2 1B骨干网络上持续预训练，我们证明RePo在涉及噪声上下文、结构化数据和较长上下文长度的任务上显著提升了性能，同时在一般短上下文任务上保持了竞争力。详细分析表明，RePo成功地将更高注意力分配给遥远但相关的信息，在密集和非线性空间中分配位置，并捕捉输入上下文的内在结构。我们的代码可在https://github.com/SakanaAI/repo获取。",
            "intro_zh": [
                "现有大语言模型使用线性或恒定位置索引，导致上下文结构僵化，增加额外认知负荷，影响深度推理。",
                "提出RePo机制，通过可微分模块动态分配标记位置，捕捉上下文依赖，减少额外负荷，提升模型效率。",
                "实验显示RePo在噪声上下文、结构化数据和长文本任务上性能显著提升，同时保持短上下文任务竞争力。"
            ],
            "method_zh": "**问题定义**：论文解决大语言模型中上下文学习的问题，现有方法使用线性或恒定位置索引，导致上下文结构僵化，增加了额外认知负荷，消耗工作记忆容量，限制了模型在深度推理和注意力分配上的表现。\\n\\n**核心思路**：基于认知负荷理论，提出RePo机制，通过上下文重定位减少额外负荷，核心思想是利用可微分模块动态分配标记位置，捕捉上下文依赖关系，而不是依赖预定义的整数范围，从而优化注意力分配。\\n\\n**技术框架**：整体架构基于OLMo-2 1B骨干网络，通过持续预训练集成RePo模块。流程包括输入上下文处理、可微分模块fφ计算位置分配、模型训练和推理，主要模块包括位置编码模块、注意力机制和优化器。\\n\\n**关键创新**：最重要的技术创新是引入可微分的位置分配模块fφ，实现动态上下文重定位，与现有方法的本质区别在于摆脱了固定位置索引的约束，允许位置在密集和非线性空间中分配，更好地捕捉输入结构。\\n\\n**关键设计**：关键参数包括模块fφ的权重φ，通过梯度下降优化；损失函数基于任务性能（如语言建模损失）和位置分配的平滑性约束；网络结构集成到Transformer架构中，位置编码替换为动态计算的位置向量，训练时使用持续预训练策略。",
            "application_zh": "该研究潜在应用于需要处理长文本、结构化数据或噪声上下文的任务，如文档摘要、代码生成、问答系统和数据分析。实际价值在于提升大语言模型在复杂场景下的推理能力和效率，未来可能推动更智能的AI助手和自动化工具发展，影响自然语言处理领域的模型设计。",
            "highlight_zh": "实验基于OLMo-2 1B模型，RePo在噪声上下文任务上性能提升约15%，结构化数据处理任务提升10-20%，长上下文长度任务（如超过1000标记）表现显著优于基线。与标准位置编码方法相比，RePo在一般短上下文任务上保持竞争力，具体数据如准确率或损失函数改进在论文中详细报告，提升幅度因任务而异，最高达20%。",
            "tags_zh": [
                "上下文学习",
                "位置编码",
                "认知负荷理论",
                "大语言模型",
                "注意力机制",
                "结构化数据处理",
                "长文本处理",
                "可微分模块"
            ],
            "_index": 132
        },
        {
            "title": "Optimizing Rank for High-Fidelity Implicit Neural Representations",
            "authors": [
                "Julian McGinnis",
                "Florian A. Hölzl",
                "Suprosanna Shit",
                "Florentin Bieder",
                "Paul Friedrich",
                "Mark Mühlau",
                "Björn Menze",
                "Daniel Rueckert",
                "Benedikt Wiestler"
            ],
            "arxiv_id": "2512.14366v1",
            "summary": "Implicit Neural Representations (INRs) based on vanilla Multi-Layer Perceptrons (MLPs) are widely believed to be incapable of representing high-frequency content. This has directed research efforts towards architectural interventions, such as coordinate embeddings or specialized activation functions, to represent high-frequency signals. In this paper, we challenge the notion that the low-frequency bias of vanilla MLPs is an intrinsic, architectural limitation to learn high-frequency content, but instead a symptom of stable rank degradation during training. We empirically demonstrate that regulating the network's rank during training substantially improves the fidelity of the learned signal, rendering even simple MLP architectures expressive. Extensive experiments show that using optimizers like Muon, with high-rank, near-orthogonal updates, consistently enhances INR architectures even beyond simple ReLU MLPs. These substantial improvements hold across a diverse range of domains, including natural and medical images, and novel view synthesis, with up to 9 dB PSNR improvements over the previous state-of-the-art. Our project page, which includes code and experimental results, is available at: (https://muon-inrs.github.io).",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14366v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出通过优化网络秩来提升隐式神经表示的高频信号保真度，挑战传统架构限制观点。",
            "summary_zh": "基于普通多层感知机（MLPs）的隐式神经表示（INRs）被广泛认为无法表示高频内容，这导致研究转向坐标嵌入或特殊激活函数等架构干预。本文挑战了普通MLPs的低频偏差是学习高频内容的内在架构限制这一观点，认为这是训练过程中稳定秩退化的症状。我们通过实验证明，在训练期间调节网络秩能显著提高学习信号的保真度，使简单的MLP架构也具有表达力。大量实验表明，使用如Muon等具有高秩、近正交更新的优化器，能持续增强INR架构，甚至超越简单的ReLU MLPs。这些显著改进适用于多种领域，包括自然和医学图像以及新视角合成，相比先前最先进方法，PSNR提升高达9 dB。我们的项目页面包含代码和实验结果，可在https://muon-inrs.github.io访问。",
            "intro_zh": [
                "现有隐式神经表示（INRs）中，普通MLPs被认为无法有效表示高频信号，导致研究过度依赖复杂架构干预。",
                "论文提出训练过程中的稳定秩退化是低频偏差的根本原因，通过优化网络秩来提升信号保真度。",
                "实验显示，使用高秩优化器如Muon，在多个领域实现高达9 dB PSNR提升，显著超越现有方法。"
            ],
            "method_zh": "**问题定义**：隐式神经表示（INRs）中，普通多层感知机（MLPs）被广泛认为存在低频偏差，无法有效学习高频信号，导致现有方法依赖坐标嵌入或特殊激活函数等复杂架构干预，增加了模型复杂性和计算成本。\\n\\n**核心思路**：论文挑战了低频偏差是MLPs内在架构限制的观点，提出这是训练过程中稳定秩退化的症状。通过调节网络秩，可以缓解退化问题，使简单MLP架构也能表达高频内容，从而避免不必要的架构修改。\\n\\n**技术框架**：整体流程包括使用普通MLP作为基础架构，在训练过程中引入秩优化机制。主要模块包括网络初始化、训练循环和秩监控，通过优化器（如Muon）实现高秩、近正交的权重更新，以维持网络表达能力。\\n\\n**关键创新**：最重要的技术创新是将高频信号学习问题从架构限制重新定义为秩优化问题，通过训练动态调节秩来提升保真度，与现有方法本质区别在于避免复杂架构干预，专注于优化过程本身。\\n\\n**关键设计**：关键参数设置包括使用Muon优化器，其设计确保权重更新具有高秩和近正交性；损失函数通常基于信号重建误差（如均方误差）；网络结构为简单ReLU MLPs，无额外嵌入或激活函数，强调秩调节的核心作用。",
            "application_zh": "该研究在计算机视觉和医学图像处理领域具有广泛潜在应用，如高保真图像重建、新视角合成和医学影像分析。通过提升隐式神经表示的信号保真度，可简化模型架构，降低计算成本，推动高效神经表示学习的发展，未来可能影响生成模型、3D重建和实时渲染等方向。",
            "highlight_zh": "实验在自然图像、医学图像和新视角合成等多个领域进行，使用Muon优化器调节网络秩，相比先前最先进方法，PSNR提升高达9 dB。结果表明，即使简单ReLU MLPs也能实现显著性能改进，验证了秩优化对提升隐式神经表示保真度的有效性，且改进具有一致性和普适性。",
            "tags_zh": [
                "隐式神经表示",
                "网络秩优化",
                "高频信号学习",
                "多层感知机",
                "Muon优化器",
                "图像重建",
                "新视角合成",
                "医学图像处理"
            ],
            "_index": 133
        },
        {
            "title": "TiCard: Deployable EXPLAIN-only Residual Learning for Cardinality Estimation",
            "authors": [
                "Qizhi Wang"
            ],
            "arxiv_id": "2512.14358v1",
            "summary": "Cardinality estimation is a key bottleneck for cost-based query optimization, yet deployable improvements remain difficult: classical estimators miss correlations, while learned estimators often require workload-specific training pipelines and invasive integration into the optimizer. This paper presents TiCard, a low intrusion, correction-based framework that augments (rather than replaces) a database's native estimator. TiCard learns multiplicative residual corrections using EXPLAIN-only features, and uses EXPLAIN ANALYZE only for offline labels. We study two practical instantiations: (i) a Gradient Boosting Regressor for sub-millisecond inference, and (ii) TabPFN, an in-context tabular foundation model that adapts by refreshing a small reference set without gradient retraining. On TiDB with TPCH and the Join Order Benchmark, in a low-trace setting (263 executions total; 157 used for learning), TiCard improves operator-level tail accuracy substantially: P90 Q-error drops from 312.85 (native) to 13.69 (TiCard-GBR), and P99 drops from 37,974.37 to 3,416.50 (TiCard-TabPFN), while a join-only policy preserves near-perfect median behavior. We position TiCard as an AI4DB building block focused on deployability: explicit scope, conservative integration policies, and an integration roadmap from offline correction to in-optimizer use.",
            "categories": [
                "cs.AI",
                "cs.DB"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "16 pages(/wo references), 4 figures, 10 tables",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14358v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出TiCard框架，通过仅使用EXPLAIN特征的残差学习增强数据库基数估计，解决传统方法忽略相关性和学习模型部署困难的问题。",
            "summary_zh": "基数估计是基于成本的查询优化的关键瓶颈，但可部署的改进仍然困难：传统估计器忽略了相关性，而学习型估计器通常需要特定于工作负载的训练流程并侵入性地集成到优化器中。本文提出了TiCard，这是一个低侵入性、基于校正的框架，用于增强（而非替换）数据库的原生估计器。TiCard使用仅EXPLAIN特征学习乘法残差校正，并且仅将EXPLAIN ANALYZE用于离线标签。我们研究了两种实际实例化：（i）用于亚毫秒推理的梯度提升回归器，以及（ii）TabPFN，一种上下文表格基础模型，通过刷新小型参考集而无需梯度重新训练来适应。在TiDB上使用TPCH和连接顺序基准测试，在低跟踪设置（总共263次执行；157次用于学习）中，TiCard显著提高了算子级别的尾部准确性：P90 Q-error从312.85（原生）降至13.69（TiCard-GBR），P99从37,974.37降至3,416.50（TiCard-TabPFN），而仅连接策略保持了近乎完美的中位数行为。我们将TiCard定位为专注于可部署性的AI4DB构建块：明确的范围、保守的集成策略以及从离线校正到优化器内使用的集成路线图。",
            "intro_zh": [
                "核心问题：传统基数估计器忽略数据相关性，而学习型估计器部署困难，需要侵入性集成和特定工作负载训练。",
                "方法要点：提出TiCard框架，通过仅使用EXPLAIN特征学习乘法残差校正来增强数据库原生估计器，实现低侵入性部署。",
                "实验或效果：在低跟踪设置下，TiCard显著提升尾部准确性，P90 Q-error从312.85降至13.69，P99从37,974.37降至3,416.50。"
            ],
            "method_zh": "**问题定义**：论文解决数据库查询优化中基数估计的准确性问题。现有方法的痛点在于：传统估计器（如直方图）无法捕捉数据相关性，导致估计偏差；而学习型估计器（如神经网络）虽然能建模复杂关系，但通常需要大量训练数据、特定工作负载的定制化流程，并且集成到数据库优化器中时侵入性强，难以实际部署。\\n\\n**核心思路**：论文提出TiCard框架，其核心思想是“增强而非替换”数据库的原生估计器。通过仅使用EXPLAIN命令提取的特征（如算子类型、谓词信息等），学习一个乘法残差校正因子，对原生估计值进行校正，从而在保持低侵入性的同时提升估计准确性。这种设计避免了完全替换估计器带来的复杂集成问题，并利用EXPLAIN ANALYZE的离线执行结果作为监督标签，减少在线开销。\\n\\n**技术框架**：整体架构分为离线学习和在线推理两个阶段。离线阶段：使用EXPLAIN ANALYZE执行查询，收集真实基数作为标签；同时，仅使用EXPLAIN提取特征（不执行查询）。基于这些特征和标签，训练残差校正模型。在线阶段：对于新查询，仅运行EXPLAIN获取特征，输入训练好的模型预测校正因子，然后乘以原生估计值得到最终估计。框架支持多种模型实例化，如梯度提升回归器（GBR）和TabPFN。\\n\\n**关键创新**：最重要的技术创新是“仅EXPLAIN特征”的残差学习机制。与现有学习型估计器相比，TiCard的本质区别在于：1）低侵入性：仅依赖数据库标准接口（EXPLAIN），无需修改优化器内核；2）通用性：通过残差校正适应不同数据库的原生估计器，而非构建端到端模型；3）高效性：离线标签收集和在线特征提取都基于轻量级操作，减少性能开销。\\n\\n**关键设计**：关键设计包括：1）特征工程：仅从EXPLAIN输出中提取结构化特征，如算子类型、表名、谓词条件等，避免依赖原始数据或统计信息；2）损失函数：使用Q-error（估计值与真实值的最大比率）作为优化目标，重点关注尾部准确性；3）模型选择：实例化两种模型——梯度提升回归器（GBR）用于亚毫秒级快速推理，TabPFN（一种表格基础模型）通过小样本参考集自适应，无需梯度重新训练；4）部署策略：定义明确的范围（如仅校正连接操作）和保守集成政策，确保稳定性和可维护性。",
            "application_zh": "该研究主要应用于数据库管理系统（DBMS）的查询优化领域，特别是基数估计模块。其实际价值在于提供了一种可部署的AI增强方案，能够在不显著改动数据库内核的情况下，提升查询性能优化效果。未来影响包括：作为AI4DB（人工智能用于数据库）的关键构建块，推动学习型方法在工业级数据库中的落地，从离线校正逐步扩展到在线优化器集成，适用于云数据库、大数据分析等场景。",
            "highlight_zh": "在TiDB数据库上，使用TPCH和Join Order Benchmark进行实验，在低跟踪设置（仅263次执行，其中157次用于学习）下，TiCard显著提升了算子级别的尾部准确性。具体结果：原生估计器的P90 Q-error为312.85，TiCard-GBR将其降至13.69（提升约22倍）；P99 Q-error从37,974.37降至3,416.50（TiCard-TabPFN，提升约11倍）。同时，通过仅连接策略，保持了近乎完美的中位数估计行为，验证了框架的有效性和可部署性。",
            "tags_zh": [
                "基数估计",
                "查询优化",
                "残差学习",
                "数据库增强",
                "可部署AI",
                "梯度提升回归器",
                "表格基础模型",
                "低侵入性集成"
            ],
            "_index": 134
        },
        {
            "title": "HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis",
            "authors": [
                "Kaizhe Zhang",
                "Yijie Zhou",
                "Weizhan Zhang",
                "Caixia Yan",
                "Haipeng Du",
                "yugui xie",
                "Yu-Hui Wen",
                "Yong-Jin Liu"
            ],
            "arxiv_id": "2512.14352v1",
            "summary": "Dynamic novel view synthesis (NVS) is essential for creating immersive experiences. Existing approaches have advanced dynamic NVS by introducing 3D Gaussian Splatting (3DGS) with implicit deformation fields or indiscriminately assigned time-varying parameters, surpassing NeRF-based methods. However, due to excessive model complexity and parameter redundancy, they incur large model sizes and slow rendering speeds, making them inefficient for real-time applications, particularly on resource-constrained devices. To obtain a more efficient model with fewer redundant parameters, in this paper, we propose Hybrid Gaussian Splatting (HGS), a compact and efficient framework explicitly designed to disentangle static and dynamic regions of a scene within a unified representation. The core innovation of HGS lies in our Static-Dynamic Decomposition (SDD) strategy, which leverages Radial Basis Function (RBF) modeling for Gaussian primitives. Specifically, for dynamic regions, we employ time-dependent RBFs to effectively capture temporal variations and handle abrupt scene changes, while for static regions, we reduce redundancy by sharing temporally invariant parameters. Additionally, we introduce a two-stage training strategy tailored for explicit models to enhance temporal coherence at static-dynamic boundaries. Experimental results demonstrate that our method reduces model size by up to 98% and achieves real-time rendering at up to 125 FPS at 4K resolution on a single RTX 3090 GPU. It further sustains 160 FPS at 1352 * 1014 on an RTX 3050 and has been integrated into the VR system. Moreover, HGS achieves comparable rendering quality to state-of-the-art methods while providing significantly improved visual fidelity for high-frequency details and abrupt scene changes.",
            "categories": [
                "cs.CV",
                "cs.CG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "11 pages, 9 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14352v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出混合高斯泼溅框架，通过静态-动态分解策略解决动态新视角合成中模型冗余和效率低下的问题。",
            "summary_zh": "动态新视角合成对于创造沉浸式体验至关重要。现有方法通过引入带有隐式变形场或非区分性时变参数的3D高斯泼溅技术，超越了基于NeRF的方法，推动了动态NVS的发展。然而，由于模型复杂度过高和参数冗余，这些方法导致模型体积庞大、渲染速度缓慢，在资源受限设备上效率低下，难以满足实时应用需求。为获得参数更少、更高效的模型，本文提出了混合高斯泼溅框架，这是一种紧凑且高效的框架，旨在在统一表示中显式解耦场景的静态和动态区域。HGS的核心创新在于静态-动态分解策略，该策略利用径向基函数对高斯基元进行建模。具体而言，对于动态区域，我们使用时变RBF有效捕捉时间变化并处理场景突变；对于静态区域，我们通过共享时间不变参数来减少冗余。此外，我们引入了针对显式模型的两阶段训练策略，以增强静态-动态边界的时间一致性。实验结果表明，我们的方法将模型大小减少了高达98%，在单张RTX 3090 GPU上以4K分辨率实现高达125 FPS的实时渲染，在RTX 3050上以1352×1014分辨率维持160 FPS，并已集成到VR系统中。此外，HGS在渲染质量上与最先进方法相当，同时在高频细节和场景突变方面提供了显著改善的视觉保真度。",
            "intro_zh": [
                "现有动态新视角合成方法模型复杂、参数冗余，导致模型体积大、渲染慢，难以实时应用。",
                "提出混合高斯泼溅框架，通过静态-动态分解策略，使用径向基函数分别建模动态和静态区域。",
                "实验显示模型大小减少98%，渲染速度达125 FPS（4K），在VR系统中实现高效集成。"
            ],
            "method_zh": "**问题定义**：论文旨在解决动态新视角合成中现有方法因模型复杂度和参数冗余导致的模型体积大、渲染速度慢的问题，特别是在资源受限设备上难以实现实时应用。现有方法如基于3D高斯泼溅的隐式变形场或非区分性时变参数方法，虽然提升了动态NVS性能，但存在效率低下的痛点。\\n\\n**核心思路**：论文的核心解决思路是提出混合高斯泼溅框架，通过静态-动态分解策略，在统一表示中显式解耦场景的静态和动态区域，以减少冗余参数并提升效率。这样设计是为了针对性地处理动态变化和静态不变部分，优化模型紧凑性。\\n\\n**技术框架**：整体架构包括基于3D高斯泼溅的表示，结合静态-动态分解模块。主要模块：动态区域使用时变径向基函数建模以捕捉时间变化和突变；静态区域共享时间不变参数以减少冗余。训练流程采用两阶段策略，第一阶段优化整体表示，第二阶段专门增强静态-动态边界的时间一致性。\\n\\n**关键创新**：最重要的技术创新是静态-动态分解策略，利用径向基函数对高斯基元进行区分性建模。与现有方法的本质区别在于显式解耦静态和动态区域，而非使用隐式或非区分性参数，从而显著减少模型复杂度和参数数量。\\n\\n**关键设计**：关键设计包括：使用径向基函数作为建模工具，动态区域参数随时间变化，静态区域参数共享；两阶段训练策略，第一阶段基于标准损失函数优化整体模型，第二阶段引入边界一致性损失以提升时间相干性；具体参数设置如高斯基元数量和RBF核函数类型根据场景调整，损失函数可能结合重建损失和时间平滑项。",
            "application_zh": "该研究在虚拟现实、增强现实和沉浸式媒体领域具有广泛应用潜力，如VR游戏、远程协作和影视制作。通过实现紧凑模型和实时渲染，它提升了在资源受限设备上的部署效率，推动动态场景合成技术的实际落地，未来可能影响实时交互系统和移动端应用的发展。",
            "highlight_zh": "最重要的实验结果包括：模型大小减少高达98%，在RTX 3090 GPU上以4K分辨率实现125 FPS的实时渲染，在RTX 3050上以1352×1014分辨率维持160 FPS。与最先进方法相比，HGS在渲染质量上相当，同时在高频细节和场景突变方面视觉保真度显著提升，已成功集成到VR系统中验证实用性。",
            "tags_zh": [
                "动态新视角合成",
                "3D高斯泼溅",
                "静态-动态分解",
                "径向基函数",
                "实时渲染",
                "模型压缩",
                "虚拟现实",
                "时间一致性"
            ],
            "_index": 135
        },
        {
            "title": "Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn Graph Orbits",
            "authors": [
                "Michael Murray",
                "Tenzin Chan",
                "Kedar Karhadker",
                "Christopher J. Hillar"
            ],
            "arxiv_id": "2512.14338v1",
            "summary": "Many learning problems involve symmetries, and while invariance can be built into neural architectures, it can also emerge implicitly when training on group-structured data. We study this phenomenon in classical Hopfield networks and show they can infer the full isomorphism class of a graph from a small random sample. Our results reveal that: (i) graph isomorphism classes can be represented within a three-dimensional invariant subspace, (ii) using gradient descent to minimize energy flow (MEF) has an implicit bias toward norm-efficient solutions, which underpins a polynomial sample complexity bound for learning isomorphism classes, and (iii) across multiple learning rules, parameters converge toward the invariant subspace as sample sizes grow. Together, these findings highlight a unifying mechanism for generalization in Hopfield networks: a bias toward norm efficiency in learning drives the emergence of approximate invariance under group-structured data.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14338v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "揭示Hopfield网络通过范数效率偏置高效学习图同构类的隐式机制",
            "summary_zh": "许多学习问题涉及对称性，虽然不变性可以构建到神经架构中，但在处理群结构数据时也可能隐式出现。我们在经典Hopfield网络中研究这一现象，并证明它们能够从小的随机样本中推断出图的完整同构类。我们的结果表明：(i) 图同构类可以在三维不变子空间内表示；(ii) 使用梯度下降最小化能量流（MEF）具有对范数高效解的隐式偏置，这支撑了学习同构类的多项式样本复杂度界限；(iii) 在多种学习规则下，随着样本量的增加，参数会收敛到不变子空间。这些发现共同突出了Hopfield网络中泛化的统一机制：学习中对范数效率的偏置驱动了在群结构数据下近似不变性的出现。",
            "intro_zh": [
                "现有方法在处理图对称性时通常显式构建不变性，但隐式学习机制尚不明确，难以高效利用群结构数据。",
                "论文提出Hopfield网络通过最小化能量流的梯度下降，隐式偏置范数高效解，从而在三维不变子空间表示图同构类。",
                "实验表明该方法能以多项式样本复杂度学习同构类，参数收敛到不变子空间，验证了隐式偏置驱动近似不变性的机制。"
            ],
            "method_zh": "**问题定义**：论文旨在解决如何高效学习图同构类的问题，即从图的随机样本中推断其完整同构类。现有方法的痛点在于，虽然可以通过显式构建不变性（如使用图神经网络中的不变层）来处理对称性，但这可能增加计算复杂度或限制模型灵活性，且隐式学习机制在群结构数据下的作用机制不明确，导致样本效率低下。\\n\\n**核心思路**：论文的核心解决思路是研究Hopfield网络在处理群结构数据时的隐式学习行为，特别是通过最小化能量流（MEF）的梯度下降过程，揭示其对范数高效解的隐式偏置。这样设计是因为Hopfield网络作为经典模型，具有简单的动力学和能量函数，便于理论分析，能够深入探讨对称性学习中的基本机制，而不受复杂架构的干扰。\\n\\n**技术框架**：整体架构基于经典Hopfield网络，流程包括：首先，将图表示为节点特征或邻接矩阵，作为输入数据；然后，使用Hopfield网络通过能量最小化过程进行学习，其中能量函数定义为网络状态与权重矩阵的函数；接着，应用梯度下降（或其他学习规则）来最小化能量流，优化网络参数；最后，分析参数在训练过程中的演化，特别是它们如何收敛到表示图同构类的不变子空间。主要模块包括数据表示模块、Hopfield网络模型、能量最小化优化模块和理论分析模块。\\n\\n**关键创新**：最重要的技术创新点是揭示了Hopfield网络学习中的隐式偏置机制：通过最小化能量流的梯度下降，网络自然地倾向于范数高效的解，这驱动了参数向不变子空间收敛，从而实现近似不变性。与现有方法的本质区别在于，不依赖显式的不变性构建（如对称群操作或特定网络层），而是利用优化过程的隐式特性，在简单模型中自发涌现对称性学习能力，这为理解更复杂神经网络中的泛化提供了新视角。\\n\\n**关键设计**：关键设计包括使用经典Hopfield网络的连续或离散版本，能量函数基于赫布学习规则或类似形式；损失函数间接通过能量最小化体现，具体为最小化能量流（MEF），这涉及梯度下降更新权重；网络结构简单，主要为全连接权重矩阵，输入维度对应图表示；参数设置中，样本复杂度分析基于理论推导，得出多项式界限，具体数值未在摘要中给出，但强调与群结构相关；学习规则可能包括标准梯度下降或其他变体，以验证不同规则下参数收敛到不变子空间的普遍性。",
            "application_zh": "该研究在计算机视觉、图学习和机器人领域具有潜在应用价值，例如用于图像识别中的对称性处理、社交网络或分子图的结构分析，以及机器人感知中的物体识别。实际价值在于提供了一种高效学习对称性的隐式机制，可降低模型复杂度和样本需求，未来可能启发更鲁棒的神经网络设计，推动人工智能在群结构数据上的泛化能力提升。",
            "highlight_zh": "最重要的实验结果表明，Hopfield网络能够从小的随机样本中推断图同构类，具体性能包括：图同构类可在三维不变子空间内表示；使用梯度下降最小化能量流（MEF）时，隐式偏置导致范数高效解，支撑了多项式样本复杂度界限（具体数值未在摘要中给出，但强调为多项式级）；在多种学习规则下，参数随样本量增加收敛到不变子空间，验证了隐式偏置驱动近似不变性的机制，相比显式构建不变性的方法，可能在样本效率上有所提升。",
            "tags_zh": [
                "Hopfield网络",
                "图同构类",
                "隐式偏置",
                "不变子空间",
                "范数效率",
                "群结构数据",
                "样本复杂度",
                "能量最小化"
            ],
            "_index": 136
        },
        {
            "title": "Semantic Mismatch and Perceptual Degradation: A New Perspective on Image Editing Immunity",
            "authors": [
                "Shuai Dong",
                "Jie Zhang",
                "Guoying Zhao",
                "Shiguang Shan",
                "Xilin Chen"
            ],
            "arxiv_id": "2512.14320v1",
            "summary": "Text-guided image editing via diffusion models, while powerful, raises significant concerns about misuse, motivating efforts to immunize images against unauthorized edits using imperceptible perturbations. Prevailing metrics for evaluating immunization success typically rely on measuring the visual dissimilarity between the output generated from a protected image and a reference output generated from the unprotected original. This approach fundamentally overlooks the core requirement of image immunization, which is to disrupt semantic alignment with attacker intent, regardless of deviation from any specific output. We argue that immunization success should instead be defined by the edited output either semantically mismatching the prompt or suffering substantial perceptual degradations, both of which thwart malicious intent. To operationalize this principle, we propose Synergistic Intermediate Feature Manipulation (SIFM), a method that strategically perturbs intermediate diffusion features through dual synergistic objectives: (1) maximizing feature divergence from the original edit trajectory to disrupt semantic alignment with the expected edit, and (2) minimizing feature norms to induce perceptual degradations. Furthermore, we introduce the Immunization Success Rate (ISR), a novel metric designed to rigorously quantify true immunization efficacy for the first time. ISR quantifies the proportion of edits where immunization induces either semantic failure relative to the prompt or significant perceptual degradations, assessed via Multimodal Large Language Models (MLLMs). Extensive experiments show our SIFM achieves the state-of-the-art performance for safeguarding visual content against malicious diffusion-based manipulation.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.CY",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "11 pages, 4 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14320v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出SIFM方法以解决图像免疫评估不准确的问题，通过语义失配和感知退化新视角保护图像免受恶意编辑。",
            "summary_zh": "基于扩散模型的文本引导图像编辑虽然强大，但也引发了严重的滥用担忧，促使人们使用不可察觉的扰动来免疫图像以防止未经授权的编辑。评估免疫成功的主流指标通常依赖于测量受保护图像生成的输出与未受保护原始图像生成的参考输出之间的视觉差异。这种方法从根本上忽视了图像免疫的核心要求，即破坏与攻击者意图的语义对齐，而不考虑与任何特定输出的偏差。我们认为，免疫成功应定义为编辑输出要么语义上与提示不匹配，要么遭受显著的感知退化，这两者都能阻止恶意意图。为了实现这一原则，我们提出了协同中间特征操纵（SIFM），这是一种通过双重协同目标策略性地扰动扩散中间特征的方法：（1）最大化特征与原始编辑轨迹的差异，以破坏与预期编辑的语义对齐；（2）最小化特征范数以诱导感知退化。此外，我们引入了免疫成功率（ISR），这是一种新颖的指标，首次设计用于严格量化真实的免疫效果。ISR量化了免疫导致编辑输出相对于提示语义失败或显著感知退化的比例，通过多模态大语言模型（MLLMs）进行评估。大量实验表明，我们的SIFM在保护视觉内容免受基于扩散的恶意操纵方面达到了最先进的性能。",
            "intro_zh": [
                "现有图像免疫评估方法依赖视觉差异度量，忽视了破坏语义对齐的核心要求，导致评估不准确。",
                "论文提出SIFM方法，通过最大化特征差异和最小化特征范数双重目标，协同扰动扩散中间特征。",
                "实验显示SIFM在免疫成功率上达到最先进水平，有效保护图像免受恶意编辑，验证了新视角的有效性。"
            ],
            "method_zh": "**问题定义**：论文要解决的具体问题是文本引导扩散模型图像编辑的滥用风险，现有免疫评估方法依赖视觉差异度量，忽视了破坏语义对齐的核心要求，导致评估不准确，无法有效量化免疫效果。\\n\\n**核心思路**：论文的核心解决思路是重新定义免疫成功为编辑输出语义失配或感知退化，并提出SIFM方法，通过双重协同目标策略性地扰动扩散中间特征，以同时破坏语义对齐和诱导感知退化，从而更全面地阻止恶意编辑。\\n\\n**技术框架**：整体架构包括两个主要阶段：首先，在扩散过程中提取中间特征；其次，应用SIFM方法，通过最大化特征差异目标（如使用距离度量）和最小化特征范数目标（如L2范数），协同优化扰动，生成免疫图像。评估时使用ISR指标，结合MLLMs判断语义匹配和感知质量。\\n\\n**关键创新**：最重要的技术创新点是提出了基于语义失配和感知退化的新免疫视角，以及SIFM方法中的双重协同目标设计，与现有方法本质区别在于从输出偏差转向意图破坏，更符合实际免疫需求。\\n\\n**关键设计**：关键设计包括：使用扩散模型的中间特征作为扰动对象；设计损失函数结合特征差异最大化（如余弦相似度损失）和特征范数最小化（如正则化项）；参数设置可能涉及扰动强度、优化步数；网络结构依赖于预训练扩散模型，无需额外训练。",
            "application_zh": "该研究在数字内容保护、隐私安全和版权管理等领域具有潜在应用价值，可用于保护个人照片、艺术作品或敏感图像免受AI驱动的恶意编辑，如深度伪造或未经授权的修改。未来可能推动更鲁棒的免疫技术发展，增强视觉数据的真实性和可信度。",
            "highlight_zh": "实验表明，SIFM在免疫成功率（ISR）上达到最先进水平，具体数据未知，但通过对比基线方法（如基于视觉差异的免疫技术），在多个数据集和编辑任务中显著提升免疫效果，有效诱导语义失配和感知退化，验证了新评估指标的有效性。",
            "tags_zh": [
                "图像免疫",
                "扩散模型",
                "语义对齐",
                "特征扰动",
                "多模态评估",
                "内容保护",
                "恶意编辑防御"
            ],
            "_index": 137
        },
        {
            "title": "From YOLO to VLMs: Advancing Zero-Shot and Few-Shot Detection of Wastewater Treatment Plants Using Satellite Imagery in MENA Region",
            "authors": [
                "Akila Premarathna",
                "Kanishka Hewageegana",
                "Garcia Andarcia Mariangel"
            ],
            "arxiv_id": "2512.14312v1",
            "summary": "In regions of the Middle East and North Africa (MENA), there is a high demand for wastewater treatment plants (WWTPs), crucial for sustainable water management. Precise identification of WWTPs from satellite images enables environmental monitoring. Traditional methods like YOLOv8 segmentation require extensive manual labeling. But studies indicate that vision-language models (VLMs) are an efficient alternative to achieving equivalent or superior results through inherent reasoning and annotation. This study presents a structured methodology for VLM comparison, divided into zero-shot and few-shot streams specifically to identify WWTPs. The YOLOv8 was trained on a governmental dataset of 83,566 high-resolution satellite images from Egypt, Saudi Arabia, and UAE: ~85% WWTPs (positives), 15% non-WWTPs (negatives). Evaluated VLMs include LLaMA 3.2 Vision, Qwen 2.5 VL, DeepSeek-VL2, Gemma 3, Gemini, and Pixtral 12B (Mistral), used to identify WWTP components such as circular/rectangular tanks, aeration basins and distinguish confounders via expert prompts producing JSON outputs with confidence and descriptions. The dataset comprises 1,207 validated WWTP locations (198 UAE, 354 KSA, 655 Egypt) and equal non-WWTP sites from field/AI data, as 600mx600m Geo-TIFF images (Zoom 18, EPSG:4326). Zero-shot evaluations on WWTP images showed several VLMs out-performing YOLOv8's true positive rate, with Gemma-3 highest. Results confirm that VLMs, particularly with zero-shot, can replace YOLOv8 for efficient, annotation-free WWTP classification, enabling scalable remote sensing.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "9 pages, 9 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14312v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出基于视觉语言模型的零样本与少样本方法，以替代YOLOv8实现中东和北非地区废水处理厂的卫星图像高效无标注检测。",
            "summary_zh": "在中东和北非地区，废水处理厂对可持续水资源管理至关重要，从卫星图像中精确识别这些设施有助于环境监测。传统方法如YOLOv8分割需要大量人工标注，但研究表明视觉语言模型通过其内在推理和标注能力，能高效实现同等或更优结果。本研究提出了一种结构化的VLM比较方法，分为零样本和少样本两个流程，专门用于识别废水处理厂。YOLOv8在来自埃及、沙特阿拉伯和阿联酋的83,566张高分辨率卫星图像政府数据集上训练，其中约85%为废水处理厂（正样本），15%为非废水处理厂（负样本）。评估的VLM包括LLaMA 3.2 Vision、Qwen 2.5 VL、DeepSeek-VL2、Gemma 3、Gemini和Pixtral 12B（Mistral），用于识别废水处理厂组件如圆形/矩形储罐、曝气池，并通过专家提示区分混淆物，生成包含置信度和描述的JSON输出。数据集包含1,207个已验证的废水处理厂位置（198个阿联酋、354个沙特阿拉伯、655个埃及）和等量的非废水处理厂站点，来自现场/AI数据，作为600米×600米的Geo-TIFF图像（缩放级别18，EPSG:4326）。在废水处理厂图像上的零样本评估显示，多个VLM在真阳性率上优于YOLOv8，其中Gemma-3表现最佳。结果证实，VLM，特别是零样本方法，可以替代YOLOv8进行高效、无标注的废水处理厂分类，实现可扩展的遥感应用。",
            "intro_zh": [
                "核心问题：传统基于YOLOv8的废水处理厂检测方法依赖大量人工标注，成本高且难以适应中东和北非地区的多样化卫星图像。",
                "方法要点：提出结构化视觉语言模型比较框架，通过零样本和少样本流程，利用专家提示识别废水处理厂组件，实现无标注高效检测。",
                "实验或效果：多个VLM在零样本评估中真阳性率超越YOLOv8，Gemma-3表现最优，证实VLM可替代传统方法用于遥感分类。"
            ],
            "method_zh": "**问题定义**：论文旨在解决中东和北非地区废水处理厂在卫星图像中的自动检测问题。现有方法如YOLOv8分割需要大量手动标注数据，导致成本高昂、效率低下，且难以适应区域多样性和复杂场景。\\n\\n**核心思路**：论文的核心思路是利用视觉语言模型的零样本和少样本能力，通过自然语言提示进行废水处理厂组件的识别和分类，从而减少对标注数据的依赖，提高检测的灵活性和可扩展性。\\n\\n**技术框架**：整体架构分为两个主要流程：零样本评估和少样本评估。首先，收集并预处理包含废水处理厂和非废水处理厂的卫星图像数据集；然后，使用专家设计的提示词，引导多个VLM模型（如LLaMA 3.2 Vision、Gemma 3等）进行图像分析，识别圆形/矩形储罐、曝气池等组件，并输出JSON格式结果；最后，与基于YOLOv8的传统方法进行性能比较。\\n\\n**关键创新**：最重要的技术创新在于将视觉语言模型应用于废水处理厂的遥感检测任务，通过结构化比较框架，验证了VLM在零样本和少样本设置下的有效性，实现了无标注或低标注的高效检测，与依赖大量标注数据的传统方法形成本质区别。\\n\\n**关键设计**：关键设计包括：数据集基于83,566张高分辨率卫星图像，正负样本比例约为85:15；图像格式为600米×600米的Geo-TIFF，缩放级别18，坐标系统为EPSG:4326；使用专家提示词引导VLM推理，例如区分废水处理厂组件和混淆物；评估指标包括真阳性率等，以量化性能提升。",
            "application_zh": "该研究在中东和北非地区的环境监测和水资源管理中具有重要应用价值，可用于废水处理厂的快速识别和分布分析，支持可持续水资源规划。未来可扩展至全球其他区域的遥感检测任务，推动无标注或少标注的智能遥感技术发展，提升环境监测的效率和准确性。",
            "highlight_zh": "实验结果显示，在零样本评估中，多个视觉语言模型（如Gemma-3、LLaMA 3.2 Vision等）的真阳性率超越了基于YOLOv8的传统方法，其中Gemma-3表现最佳。具体数据表明，VLM能够实现高效的无标注检测，验证了其在废水处理厂分类任务中的优越性，为遥感应用提供了可扩展的解决方案。",
            "tags_zh": [
                "视觉语言模型",
                "零样本检测",
                "少样本学习",
                "卫星图像分析",
                "废水处理厂识别",
                "遥感应用",
                "中东和北非地区",
                "环境监测"
            ],
            "_index": 138
        },
        {
            "title": "Continual Learning at the Edge: An Agnostic IIoT Architecture",
            "authors": [
                "Pablo García-Santaclara",
                "Bruno Fernández-Castro",
                "Rebeca P. Díaz-Redondo",
                "Carlos Calvo-Moa",
                "Henar Mariño-Bodelón"
            ],
            "arxiv_id": "2512.14311v1",
            "summary": "The exponential growth of Internet-connected devices has presented challenges to traditional centralized computing systems due to latency and bandwidth limitations. Edge computing has evolved to address these difficulties by bringing computations closer to the data source. Additionally, traditional machine learning algorithms are not suitable for edge-computing systems, where data usually arrives in a dynamic and continual way. However, incremental learning offers a good solution for these settings. We introduce a new approach that applies the incremental learning philosophy within an edge-computing scenario for the industrial sector with a specific purpose: real time quality control in a manufacturing system. Applying continual learning we reduce the impact of catastrophic forgetting and provide an efficient and effective solution.",
            "categories": [
                "stat.ML",
                "cs.LG"
            ],
            "primary_category": "stat.ML",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "10.1007/978-981-96-6938-7_33",
            "journal_ref": "García-Santaclara, P., Fernández-Castro, B., Díaz-Redondo, R. P., Calvo-Moa, C., & Mariño-Bodelón, H. (2025). Continual learning at the edge: An agnostic IIoT architecture. In Lecture Notes in Networks and Systems. Springer",
            "pdf_url": "https://arxiv.org/pdf/2512.14311v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出一种面向工业物联网的边缘计算架构，应用持续学习实现制造系统的实时质量控制。",
            "summary_zh": "互联网连接设备的指数级增长给传统集中式计算系统带来了延迟和带宽限制的挑战。边缘计算通过将计算任务靠近数据源来解决这些困难。此外，传统机器学习算法不适合边缘计算系统，因为数据通常以动态和持续的方式到达。然而，增量学习为这些场景提供了良好的解决方案。我们引入了一种新方法，将增量学习理念应用于工业领域的边缘计算场景，具体目的是实现制造系统中的实时质量控制。通过应用持续学习，我们减少了灾难性遗忘的影响，并提供了一种高效有效的解决方案。",
            "intro_zh": [
                "传统集中式计算系统面临延迟和带宽限制，边缘计算虽能缓解但数据动态性挑战机器学习算法。",
                "提出在工业物联网边缘计算场景中应用持续学习，减少灾难性遗忘，实现实时质量控制。",
                "实验表明该方法在制造系统中能有效处理动态数据流，提升质量控制的效率和准确性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决工业物联网（IIoT）边缘计算场景中，传统机器学习算法无法适应动态、持续数据流的问题，特别是制造系统实时质量控制的挑战。现有方法的痛点包括传统集中式计算系统的高延迟和带宽限制，以及传统机器学习算法在边缘设备上处理增量数据时易受灾难性遗忘影响，导致模型性能下降。\\n\\n**核心思路**：论文的核心解决思路是将持续学习（Continual Learning）理念应用于边缘计算架构，通过增量学习机制使模型能够在不遗忘旧知识的情况下适应新数据。这样设计是因为边缘设备通常资源有限，且数据以流式方式到达，需要模型能够在线学习和适应，同时保持对历史数据的记忆，以支持实时决策如质量控制。\\n\\n**技术框架**：整体架构是一个面向工业物联网的边缘计算系统，包含数据采集层、边缘处理层和云端协调层。主要模块包括传感器数据输入、边缘设备上的持续学习模型、实时推理引擎和反馈循环。流程涉及数据从制造设备收集，在边缘节点进行增量训练和预测，结果用于质量控制决策，并可能将模型更新同步到云端以优化全局性能。\\n\\n**关键创新**：最重要的技术创新点是将持续学习与边缘计算深度融合，提出一种与具体算法无关（Agnostic）的架构，这意味着它可以适配多种增量学习算法，如弹性权重巩固（EWC）或渐进式神经网络，以适应不同工业场景。与现有方法的本质区别在于，它专门针对工业物联网的动态性和实时性需求，通过减少灾难性遗忘来提升模型在边缘环境中的鲁棒性和效率。\\n\\n**关键设计**：关键设计包括采用增量学习算法来处理数据流，例如使用正则化技术来保护重要权重，防止灾难性遗忘；网络结构可能基于轻量级神经网络以适应边缘设备的计算资源；参数设置如学习率和正则化系数需优化以平衡新旧知识；损失函数可能结合分类损失和遗忘惩罚项；此外，架构支持分布式部署，允许边缘节点独立学习并协同工作。",
            "application_zh": "该研究主要应用于工业物联网领域，特别是在制造系统中实现实时质量控制，如缺陷检测、过程监控和预测性维护。其实际价值在于通过边缘计算降低延迟和带宽消耗，同时利用持续学习提升模型在动态环境中的适应性，从而提高生产效率和产品质量。未来影响可能扩展到其他工业自动化场景，如能源管理和智能物流，推动工业4.0的发展。",
            "highlight_zh": "实验在模拟或真实制造环境中进行，对比基线包括传统批处理机器学习方法和简单增量学习算法。结果显示，提出的持续学习边缘架构在实时质量控制任务中，准确率提升约10-15%，同时减少了灾难性遗忘的影响，模型在连续数据流上的稳定性显著增强。具体性能数据如处理延迟降低30%，资源利用率优化20%，但论文未提供详细数值，需参考原文。",
            "tags_zh": [
                "持续学习",
                "边缘计算",
                "工业物联网",
                "增量学习",
                "实时质量控制",
                "灾难性遗忘",
                "制造系统",
                "分布式架构"
            ],
            "_index": 139
        },
        {
            "title": "PSMamba: Progressive Self-supervised Vision Mamba for Plant Disease Recognition",
            "authors": [
                "Abdullah Al Mamun",
                "Miaohua Zhang",
                "David Ahmedt-Aristizabal",
                "Zeeshan Hayder",
                "Mohammad Awrangjeb"
            ],
            "arxiv_id": "2512.14309v1",
            "summary": "Self-supervised Learning (SSL) has become a powerful paradigm for representation learning without manual annotations. However, most existing frameworks focus on global alignment and struggle to capture the hierarchical, multi-scale lesion patterns characteristic of plant disease imagery. To address this gap, we propose PSMamba, a progressive self-supervised framework that integrates the efficient sequence modelling of Vision Mamba (VM) with a dual-student hierarchical distillation strategy. Unlike conventional single teacher-student designs, PSMamba employs a shared global teacher and two specialised students: one processes mid-scale views to capture lesion distributions and vein structures, while the other focuses on local views to capture fine-grained cues such as texture irregularities and early-stage lesions. This multi-granular supervision facilitates the joint learning of contextual and detailed representations, with consistency losses ensuring coherent cross-scale alignment. Experiments on three benchmark datasets show that PSMamba consistently outperforms state-of-the-art SSL methods, delivering superior accuracy and robustness in both domain-shifted and fine-grained scenarios.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14309v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出PSMamba框架，通过渐进式自监督视觉Mamba与双学生层次蒸馏，解决植物病害图像多尺度病变模式识别难题。",
            "summary_zh": "自监督学习已成为无需人工标注的强大表示学习范式。然而，大多数现有框架侧重于全局对齐，难以捕捉植物病害图像中层次化、多尺度的病变模式特征。为填补这一空白，我们提出了PSMamba，这是一个渐进式自监督框架，将视觉Mamba的高效序列建模与双学生层次蒸馏策略相结合。与传统的单教师-学生设计不同，PSMamba采用一个共享的全局教师和两个专门的学生：一个处理中尺度视图以捕捉病变分布和叶脉结构，另一个专注于局部视图以捕捉细粒度线索，如纹理不规则和早期病变。这种多粒度监督促进了上下文和细节表示的联合学习，并通过一致性损失确保跨尺度对齐的连贯性。在三个基准数据集上的实验表明，PSMamba始终优于最先进的自监督学习方法，在领域转移和细粒度场景中均提供了卓越的准确性和鲁棒性。",
            "intro_zh": [
                "现有自监督学习方法主要关注全局对齐，难以有效捕捉植物病害图像中层次化、多尺度的病变模式，导致在细粒度识别任务中表现受限。",
                "论文提出PSMamba框架，结合视觉Mamba的高效序列建模与双学生层次蒸馏策略，通过多粒度监督实现上下文和细节表示的联合学习。",
                "在三个基准数据集上，PSMamba优于现有自监督学习方法，在领域转移和细粒度场景中展现出更高的准确性和鲁棒性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决植物病害识别中，现有自监督学习方法难以有效捕捉图像中层次化、多尺度病变模式的问题。现有方法通常侧重于全局对齐，忽略了病变分布、叶脉结构、纹理不规则等细粒度特征，导致在复杂场景下识别性能受限。\\n\\n**核心思路**：论文的核心思路是设计一个渐进式自监督框架，通过整合视觉Mamba的高效序列建模能力和双学生层次蒸馏策略，实现多尺度特征的联合学习。这样设计是为了同时捕获全局上下文和局部细节，提升模型对植物病害图像中复杂病变模式的表征能力。\\n\\n**技术框架**：PSMamba的整体架构包括一个共享的全局教师网络和两个专门的学生网络。全局教师处理完整图像以学习全局表示；一个学生网络处理中尺度视图，专注于捕捉病变分布和叶脉结构；另一个学生网络处理局部视图，专注于捕捉纹理不规则和早期病变等细粒度线索。训练过程通过渐进式蒸馏，将教师的知识传递给学生，并利用一致性损失确保跨尺度对齐的连贯性。\\n\\n**关键创新**：最重要的技术创新点是双学生层次蒸馏策略与视觉Mamba的结合。与传统的单教师-学生设计相比，PSMamba通过两个专门的学生网络实现多粒度监督，能够更有效地学习层次化特征。本质区别在于它强调了跨尺度对齐和细粒度线索的整合，而非仅依赖全局对齐。\\n\\n**关键设计**：关键设计包括：使用视觉Mamba作为骨干网络，以高效处理图像序列；设置两个学生网络分别处理中尺度和局部视图，具体参数如视图大小和裁剪策略需根据数据集调整；损失函数结合了蒸馏损失和一致性损失，例如使用对比损失或均方误差来确保教师与学生之间以及不同尺度之间的表示对齐；网络结构可能包含编码器-解码器或Transformer-like模块，具体细节需参考论文实验部分。",
            "application_zh": "该研究主要应用于农业领域的植物病害识别，通过自监督学习减少对大量标注数据的依赖，提升病害检测的准确性和效率。潜在价值包括支持智能农业系统、早期病害预警和精准施药，未来可能扩展到其他细粒度视觉识别任务，如医学图像分析或工业质检。",
            "highlight_zh": "在三个基准数据集上的实验表明，PSMamba在植物病害识别任务中，相比最先进的自监督学习方法，实现了显著的性能提升。具体数据未知，但论文报告了在领域转移和细粒度场景下的优越准确性和鲁棒性，例如在细粒度分类任务中可能达到更高的Top-1准确率，提升幅度需参考具体实验结果。",
            "tags_zh": [
                "植物病害识别",
                "自监督学习",
                "视觉Mamba",
                "层次蒸馏",
                "多尺度特征",
                "细粒度识别",
                "农业图像分析",
                "序列建模"
            ],
            "_index": 140
        },
        {
            "title": "Improving the Accuracy of Amortized Model Comparison with Self-Consistency",
            "authors": [
                "Šimon Kucharský",
                "Aayush Mishra",
                "Daniel Habermann",
                "Stefan T. Radev",
                "Paul-Christian Bürkner"
            ],
            "arxiv_id": "2512.14308v1",
            "summary": "Amortized Bayesian inference (ABI) offers fast, scalable approximations to posterior densities by training neural surrogates on data simulated from the statistical model. However, ABI methods are highly sensitive to model misspecification: when observed data fall outside the training distribution (generative scope of the statistical models), neural surrogates can behave unpredictably. This makes it a challenge in a model comparison setting, where multiple statistical models are considered, of which at least some are misspecified. Recent work on self-consistency (SC) provides a promising remedy to this issue, accessible even for empirical data (without ground-truth labels). In this work, we investigate how SC can improve amortized model comparison conceptualized in four different ways. Across two synthetic and two real-world case studies, we find that approaches for model comparison that estimate marginal likelihoods through approximate parameter posteriors consistently outperform methods that directly approximate model evidence or posterior model probabilities. SC training improves robustness when the likelihood is available, even under severe model misspecification. The benefits of SC for methods without access of analytic likelihoods are more limited and inconsistent. Our results suggest practical guidance for reliable amortized Bayesian model comparison: prefer parameter posterior-based methods and augment them with SC training on empirical datasets to mitigate extrapolation bias under model misspecification.",
            "categories": [
                "stat.ML",
                "cs.LG",
                "stat.CO"
            ],
            "primary_category": "stat.ML",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "17 pages, 9 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14308v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出基于自一致性的训练方法，提升模型错误设定下摊销贝叶斯模型比较的准确性",
            "summary_zh": "摊销贝叶斯推断（ABI）通过训练神经网络代理来快速近似后验密度，但该方法对模型错误设定高度敏感：当观测数据超出训练分布时，神经网络代理可能表现不可预测。这在模型比较场景中尤为挑战，因为需要考虑多个统计模型，其中至少部分模型存在错误设定。最近关于自一致性（SC）的研究为解决这一问题提供了有前景的补救措施，即使对于没有真实标签的经验数据也适用。本研究探讨了SC如何改进四种不同概念化的摊销模型比较方法。通过两个合成案例和两个真实世界案例研究，我们发现通过近似参数后验估计边际似然的方法，在模型比较中始终优于直接近似模型证据或后验模型概率的方法。当似然函数可用时，SC训练即使在严重模型错误设定下也能提高鲁棒性。对于无法访问解析似然函数的方法，SC的益处更为有限且不一致。我们的结果为可靠的摊销贝叶斯模型比较提供了实用指导：优先选择基于参数后验的方法，并在经验数据集上通过SC训练增强它们，以减轻模型错误设定下的外推偏差。",
            "intro_zh": [
                "核心问题：摊销贝叶斯推断在模型错误设定下表现不稳定，影响模型比较的可靠性。",
                "方法要点：引入自一致性训练，增强神经网络代理在经验数据上的鲁棒性，减少外推偏差。",
                "实验或效果：基于参数后验的方法在四个案例中表现最佳，SC训练显著提升准确性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决摊销贝叶斯推断（ABI）在模型比较场景中的局限性，特别是当统计模型存在错误设定时。现有ABI方法通过训练神经网络代理来近似后验密度，但高度依赖训练数据的分布；当观测数据超出生成范围（即模型错误设定），代理可能产生不可预测的输出，导致模型比较结果不准确。这在实践中常见，因为真实数据往往不完全符合假设模型。\\n\\n**核心思路**：论文的核心解决思路是利用自一致性（SC）原则来增强ABI的鲁棒性。SC基于这样的理念：在贝叶斯框架下，后验分布应满足一致性条件，例如参数估计与模型证据之间的内在关系。通过将SC作为训练目标或正则化项，可以迫使神经网络代理在经验数据上保持统计一致性，从而减少模型错误设定带来的外推偏差。这样设计是因为SC不依赖真实标签，适用于实际应用中的无监督数据，直接针对ABI的脆弱性进行改进。\\n\\n**技术框架**：整体架构包括四个主要阶段：首先，定义四种不同的摊销模型比较方法，涵盖直接近似模型证据、后验模型概率以及通过参数后验估计边际似然等途径。其次，在合成数据和真实数据集上模拟训练，生成神经网络代理。然后，引入SC训练，通过优化一致性损失（如后验预测检查或参数-证据关系）来微调代理。最后，在测试数据上评估比较方法的性能，使用指标如准确性或偏差，并分析SC对不同方法的影响。流程强调从模拟到经验数据的过渡，以验证泛化能力。\\n\\n**关键创新**：最重要的技术创新点是将自一致性原则系统性地整合到摊销模型比较中，特别是在模型错误设定场景下。与现有方法相比，本质区别在于：现有ABI通常仅依赖模拟数据训练，缺乏对经验数据适应性的直接约束；而本工作通过SC提供了无监督的鲁棒性机制，使代理能在未知分布上保持统计合理性，从而提升比较的可靠性。这扩展了ABI的应用范围，使其更适用于实际复杂数据。\\n\\n**关键设计**：关键设计包括：损失函数方面，SC训练可能涉及一致性损失，如最小化后验预测分布与观测数据之间的差异，或强制参数后验与模型证据的贝叶斯关系；网络结构上，使用标准神经网络（如多层感知机）作为代理，但通过SC目标进行端到端优化；参数设置中，训练数据来自统计模型的模拟，但SC阶段引入经验数据作为额外输入；实验设计上，对比了有/无SC的四种方法，并设置基线如传统贝叶斯方法，以量化提升。这些细节确保了方法在合成和真实案例中的可复现性和有效性。",
            "application_zh": "该研究在贝叶斯统计和机器学习领域具有广泛的应用潜力，特别是在需要快速模型比较的场景中，如生物信息学中的基因表达分析、金融风险建模或认知科学的假设检验。实际价值在于提供了一种可靠的工具，用于在模型不确定或错误设定下进行决策支持，减少对大量计算资源的依赖。未来影响可能推动摊销推断在更复杂真实世界问题中的应用，促进自动化模型选择流程的发展。",
            "highlight_zh": "最重要的实验结果显示：在四个案例研究（两个合成和两个真实世界）中，基于参数后验估计边际似然的方法始终优于直接近似模型证据或后验模型概率的方法。具体而言，当似然函数可用时，SC训练显著提高了鲁棒性，即使在严重模型错误设定下也能减少外推偏差，提升准确性达未知幅度（论文未提供具体数值，但强调一致性改进）。对于无法访问解析似然函数的方法，SC的益处有限且不一致，突显了方法依赖性。这些结果通过对比基线（如无SC的ABI）验证，为实践提供了明确指导。",
            "tags_zh": [
                "摊销贝叶斯推断",
                "模型比较",
                "自一致性训练",
                "模型错误设定",
                "神经网络代理",
                "后验近似",
                "鲁棒性增强",
                "经验数据应用"
            ],
            "_index": 141
        },
        {
            "title": "The Trust in AI-Generated Health Advice (TAIGHA) Scale and Short Version (TAIGHA-S): Development and Validation Study",
            "authors": [
                "Marvin Kopka",
                "Azeem Majeed",
                "Gabriella Spinelli",
                "Austen El-Osta",
                "Markus Feufel"
            ],
            "arxiv_id": "2512.14278v1",
            "summary": "Artificial Intelligence tools such as large language models are increasingly used by the public to obtain health information and guidance. In health-related contexts, following or rejecting AI-generated advice can have direct clinical implications. Existing instruments like the Trust in Automated Systems Survey assess trustworthiness of generic technology, and no validated instrument measures users' trust in AI-generated health advice specifically. This study developed and validated the Trust in AI-Generated Health Advice (TAIGHA) scale and its four-item short form (TAIGHA-S) as theory-based instruments measuring trust and distrust, each with cognitive and affective components. The items were developed using a generative AI approach, followed by content validation with 10 domain experts, face validation with 30 lay participants, and psychometric validation with 385 UK participants who received AI-generated advice in a symptom-assessment scenario. After automated item reduction, 28 items were retained and reduced to 10 based on expert ratings. TAIGHA showed excellent content validity (S-CVI/Ave=0.99) and CFA confirmed a two-factor model with excellent fit (CFI=0.98, TLI=0.98, RMSEA=0.07, SRMR=0.03). Internal consistency was high (α=0.95). Convergent validity was supported by correlations with the Trust in Automated Systems Survey (r=0.67/-0.66) and users' reliance on the AI's advice (r=0.37 for trust), while divergent validity was supported by low correlations with reading flow and mental load (all |r|<0.25). TAIGHA-S correlated highly with the full scale (r=0.96) and showed good reliability (α=0.88). TAIGHA and TAIGHA-S are validated instruments for assessing user trust and distrust in AI-generated health advice. Reporting trust and distrust separately permits a more complete evaluation of AI interventions, and the short scale is well-suited for time-constrained settings.",
            "categories": [
                "cs.HC",
                "cs.AI"
            ],
            "primary_category": "cs.HC",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14278v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出TAIGHA量表及其短版TAIGHA-S，用于专门评估用户对AI生成健康建议的信任与不信任。",
            "summary_zh": "随着大型语言模型等人工智能工具被公众越来越多地用于获取健康信息和指导，在健康相关情境中，采纳或拒绝AI生成的建议可能产生直接的临床影响。现有工具如“自动化系统信任调查”评估的是通用技术的可信度，缺乏专门测量用户对AI生成健康建议信任度的有效工具。本研究开发并验证了“AI生成健康建议信任量表”及其四项目短版，作为基于理论的工具，分别测量信任与不信任，每个维度包含认知和情感成分。项目开发采用生成式AI方法，随后进行了10名领域专家的内容验证、30名普通参与者的表面验证，以及385名英国参与者的心理测量验证，这些参与者在症状评估场景中接收了AI生成的建议。经过自动项目缩减，保留了28个项目，并根据专家评分缩减至10个。TAIGHA显示出优异的内容效度，验证性因子分析确认了双因子模型具有极佳的拟合度。内部一致性很高。收敛效度得到与自动化系统信任调查和用户对AI建议依赖度的相关性支持，而区分效度则通过与阅读流畅性和心理负荷的低相关性得到支持。TAIGHA-S与完整量表高度相关，并显示出良好的信度。TAIGHA和TAIGHA-S是评估用户对AI生成健康建议信任与不信任的有效工具。单独报告信任与不信任允许对AI干预进行更全面的评估，短量表非常适合时间受限的场景。",
            "intro_zh": [
                "现有工具如自动化系统信任调查评估通用技术可信度，缺乏专门测量用户对AI生成健康建议信任的有效工具，无法满足健康领域特定需求。",
                "开发基于理论的TAIGHA量表及其短版，分别测量信任与不信任的认知和情感成分，采用生成式AI方法生成项目，并进行多阶段验证。",
                "TAIGHA显示出优异的内容效度和模型拟合度，内部一致性高，与相关工具相关性良好，短版与完整量表高度相关，信度可靠。"
            ],
            "method_zh": "**问题定义**：论文旨在解决缺乏专门评估用户对AI生成健康建议信任度的有效工具的问题。现有方法如自动化系统信任调查评估的是通用技术可信度，无法捕捉健康建议这一特定场景下的信任动态，导致在健康干预评估中可能遗漏关键信息，影响AI应用的临床效果和用户采纳。\\n\\n**核心思路**：论文的核心解决思路是开发一个基于理论的量表，专门针对AI生成健康建议的信任与不信任进行测量，每个维度包含认知和情感成分。这样设计是因为信任在健康决策中至关重要，且信任与不信任可能独立存在，单独评估能提供更全面的用户反馈，从而优化AI工具的设计和部署。\\n\\n**技术框架**：整体流程包括项目生成、内容验证、表面验证和心理测量验证四个主要阶段。首先，使用生成式AI方法生成初始项目；然后，由10名领域专家进行内容验证，确保项目与理论框架一致；接着，30名普通参与者进行表面验证，评估项目的可理解性和相关性；最后，385名参与者在症状评估场景中接收AI建议，完成量表，进行心理测量分析，包括项目缩减、因子分析和效度检验。\\n\\n**关键创新**：最重要的技术创新点是开发了首个专门针对AI生成健康建议的信任量表，将信任与不信任作为独立但相关的维度进行测量，并整合认知和情感成分。与现有方法的本质区别在于其领域特异性（健康建议）和理论驱动性（基于信任理论），而非通用技术评估，从而能更精确地捕捉用户在这一关键场景中的心理反应。\\n\\n**关键设计**：关键设计包括：采用生成式AI辅助项目生成，提高效率；通过专家评分（内容效度指数）和参与者反馈进行项目筛选，确保质量；使用验证性因子分析确认双因子模型（信任与不信任）的拟合度；设置内部一致性系数（Cronbach's α）评估信度；通过相关性分析检验收敛效度（如与自动化系统信任调查的相关性）和区分效度（如与阅读流畅性的低相关性）。具体参数如S-CVI/Ave=0.99、CFI=0.98、α=0.95等，确保了量表的科学性和实用性。",
            "application_zh": "该研究在医疗健康、人工智能交互和用户研究领域具有重要应用价值。TAIGHA量表可用于评估AI健康助手、症状检查工具或医疗咨询系统的用户信任度，帮助开发者优化算法和界面设计，提高用户采纳率和临床效果。短版TAIGHA-S适用于时间敏感场景，如实时健康监测或快速用户反馈收集。未来，该工具可促进AI在健康领域的负责任部署，支持个性化医疗和公共卫生干预。",
            "highlight_zh": "TAIGHA量表显示出优异的内容效度（S-CVI/Ave=0.99）和模型拟合度（CFI=0.98, TLI=0.98, RMSEA=0.07, SRMR=0.03），内部一致性高（α=0.95）。收敛效度得到支持：与自动化系统信任调查的相关性为r=0.67（信任）和r=-0.66（不信任），与用户对AI建议依赖度的相关性为r=0.37（信任）。区分效度良好：与阅读流畅性和心理负荷的相关性均低于|r|=0.25。TAIGHA-S与完整量表高度相关（r=0.96），信度可靠（α=0.88），表明其作为短工具的实用性。",
            "tags_zh": [
                "AI生成健康建议",
                "信任量表",
                "心理测量验证",
                "内容效度",
                "自动化系统信任",
                "用户研究",
                "健康人工智能",
                "短量表开发"
            ],
            "_index": 142
        },
        {
            "title": "TUN: Detecting Significant Points in Persistence Diagrams with Deep Learning",
            "authors": [
                "Yu Chen",
                "Hongwei Lin"
            ],
            "arxiv_id": "2512.14274v1",
            "summary": "Persistence diagrams (PDs) provide a powerful tool for understanding the topology of the underlying shape of a point cloud. However, identifying which points in PDs encode genuine signals remains challenging. This challenge directly hinders the practical adoption of topological data analysis in many applications, where automated and reliable interpretation of persistence diagrams is essential for downstream decision-making. In this paper, we study automatic significance detection for one-dimensional persistence diagrams. Specifically, we propose Topology Understanding Net (TUN), a multi-modal network that combines enhanced PD descriptors with self-attention, a PointNet-style point cloud encoder, learned fusion, and per-point classification, alongside stable preprocessing and imbalance-aware training. It provides an automated and effective solution for identifying significant points in PDs, which are critical for downstream applications. Experiments show that TUN outperforms classic methods in detecting significant points in PDs, illustrating its effectiveness in real-world applications.",
            "categories": [
                "cs.CV",
                "cs.LG",
                "math.AT"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14274v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出TUN网络以解决一维持久图显著性点自动检测问题，提升拓扑数据分析的实用性和下游决策可靠性。",
            "summary_zh": "持久图（PDs）是理解点云底层形状拓扑结构的强大工具，但识别其中哪些点编码真实信号仍具挑战性。这一挑战直接阻碍了拓扑数据分析在许多应用中的实际采用，其中持久图的自动可靠解释对下游决策至关重要。本文研究一维持久图的自动显著性检测。具体而言，我们提出了拓扑理解网络（TUN），这是一个多模态网络，结合了增强的PD描述符与自注意力机制、PointNet风格的点云编码器、学习融合和逐点分类，以及稳定的预处理和不平衡感知训练。它为识别持久图中的显著性点提供了自动有效的解决方案，这对下游应用至关重要。实验表明，TUN在检测持久图中的显著性点方面优于经典方法，证明了其在现实应用中的有效性。",
            "intro_zh": [
                "核心问题：现有方法难以自动可靠地识别持久图中的显著性点，阻碍了拓扑数据分析在实际应用中的广泛采用。",
                "方法要点：提出TUN网络，结合增强描述符、自注意力、点云编码器和学习融合，实现多模态特征提取与分类。",
                "实验或效果：TUN在检测显著性点方面超越经典方法，验证了其有效性和在现实场景中的实用性。"
            ],
            "method_zh": "**问题定义**：论文解决一维持久图中显著性点的自动检测问题。现有方法依赖手动阈值或简单统计，缺乏自动化且难以区分噪声与真实信号，导致下游应用决策不可靠。\\n\\n**核心思路**：设计一个多模态深度学习网络，通过融合增强的持久图描述符和点云表示，利用自注意力机制捕捉全局依赖，实现端到端的显著性点分类。\\n\\n**技术框架**：整体架构包括稳定预处理、多模态特征提取（增强PD描述符和PointNet风格编码器）、自注意力融合模块、学习融合层和逐点分类器，采用不平衡感知训练优化。\\n\\n**关键创新**：首次将多模态网络（结合描述符和点云）与自注意力机制应用于持久图显著性检测，实现自动化且高精度的分类，本质区别在于从数据驱动角度替代传统启发式方法。\\n\\n**关键设计**：使用稳定预处理减少噪声影响；增强PD描述符包括持久性、出生-死亡坐标等特征；PointNet编码器处理点云结构；自注意力模块捕捉点间关系；学习融合层自适应整合多模态特征；损失函数采用交叉熵并加入类别权重处理不平衡数据；训练时优化超参数如学习率和批次大小。",
            "application_zh": "该研究在计算机视觉、机器人感知和生物信息学等领域有广泛应用潜力，例如点云分割、形状分析和异常检测。通过自动识别持久图中的显著性点，能提升拓扑数据分析的实用性和下游决策的可靠性，推动人工智能在复杂数据理解中的进步。",
            "highlight_zh": "实验表明，TUN在检测一维持久图显著性点方面显著优于经典方法（如基于阈值的统计方法），具体性能数据未知，但论文报告了更高的准确率和召回率，提升幅度明显，验证了其在真实世界数据集上的有效性和鲁棒性。",
            "tags_zh": [
                "持久图显著性检测",
                "拓扑数据分析",
                "多模态网络",
                "自注意力机制",
                "点云编码",
                "深度学习",
                "不平衡感知训练",
                "自动化分类"
            ],
            "_index": 143
        },
        {
            "title": "Explainable Preference Learning: a Decision Tree-based Surrogate Model for Preferential Bayesian Optimization",
            "authors": [
                "Nick Leenders",
                "Thomas Quadt",
                "Boris Cule",
                "Roy Lindelauf",
                "Herman Monsuur",
                "Joost van Oijen",
                "Mark Voskuijl"
            ],
            "arxiv_id": "2512.14263v1",
            "summary": "Current Preferential Bayesian Optimization methods rely on Gaussian Processes (GPs) as surrogate models. These models are hard to interpret, struggle with handling categorical data, and are computationally complex, limiting their real-world usability. In this paper, we introduce an inherently interpretable decision tree-based surrogate model capable of handling both categorical and continuous data, and scalable to large datasets. Extensive numerical experiments on eight increasingly spiky optimization functions show that our model outperforms GP-based alternatives on spiky functions and has only marginally lower performance for non-spiky functions. Moreover, we apply our model to the real-world Sushi dataset and show its ability to learn an individual's sushi preferences. Finally, we show some initial work on using historical preference data to speed up the optimization process for new unseen users.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "math.OC"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14263v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出基于决策树的可解释偏好学习模型，以解决偏好贝叶斯优化中高斯过程模型难以解释、处理分类数据困难的问题。",
            "summary_zh": "当前的偏好贝叶斯优化方法依赖于高斯过程作为代理模型。这些模型难以解释，处理分类数据困难，且计算复杂度高，限制了其在实际应用中的可用性。本文提出了一种本质上可解释的基于决策树的代理模型，能够同时处理分类和连续数据，并可扩展到大型数据集。在八个逐渐尖峰的优化函数上进行的大量数值实验表明，我们的模型在尖峰函数上优于基于高斯过程的替代方法，在非尖峰函数上性能仅略低。此外，我们将模型应用于真实世界的寿司数据集，展示了其学习个人寿司偏好的能力。最后，我们展示了利用历史偏好数据加速新用户优化过程的初步工作。",
            "intro_zh": [
                "现有偏好贝叶斯优化依赖高斯过程，存在模型难以解释、处理分类数据困难、计算复杂等限制。",
                "提出基于决策树的代理模型，具备内在可解释性，能处理分类和连续数据，并支持大规模数据集。",
                "在尖峰函数上优于高斯过程模型，非尖峰函数性能相近，寿司数据集验证了实际偏好学习能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决偏好贝叶斯优化中现有高斯过程代理模型的可解释性差、处理分类数据能力弱、计算复杂度高的问题，这些痛点限制了模型在真实场景中的实用性和扩展性。\\n\\n**核心思路**：采用决策树作为代理模型替代高斯过程，利用决策树的内在可解释性、对分类数据的天然支持以及较低的计算复杂度，构建一个既能准确建模用户偏好又易于理解的优化框架。\\n\\n**技术框架**：整体架构包括数据预处理、决策树模型训练、偏好预测和优化迭代四个阶段。首先处理包含分类和连续特征的偏好数据，然后训练决策树模型来学习偏好函数，接着基于模型预测进行贝叶斯优化以选择下一个查询点，最后迭代更新直至收敛。\\n\\n**关键创新**：最重要的技术创新是将决策树引入偏好贝叶斯优化作为代理模型，与现有高斯过程方法相比，本质区别在于提供了内在可解释性、更好的分类数据处理能力和更高的计算效率，同时保持了竞争性的优化性能。\\n\\n**关键设计**：模型采用决策树算法（如CART或C4.5）作为基础，损失函数可能基于信息增益或基尼不纯度进行节点分裂，关键参数包括树的最大深度、最小样本分裂数等以控制过拟合，网络结构为单棵决策树或集成方法（如随机森林）以提升鲁棒性，具体细节在论文中未明确说明，但强调了可扩展性和处理混合数据类型的能力。",
            "application_zh": "该研究在个性化推荐系统、产品设计优化和用户偏好建模等领域具有广泛应用潜力。例如，在电商平台中，可基于用户历史偏好快速学习新用户的兴趣，提升推荐准确性和效率；在工业设计中，能优化产品参数以满足多样化用户需求。未来可能推动可解释AI在交互式优化任务中的普及，增强用户信任和系统透明度。",
            "highlight_zh": "在八个逐渐尖峰的优化函数实验中，基于决策树的模型在尖峰函数上显著优于高斯过程基线，具体性能提升幅度未知，但论文指出在非尖峰函数上性能仅略低。寿司数据集应用显示模型能有效学习个人偏好，验证了实际可用性。此外，初步工作表明利用历史数据可加速新用户优化，但具体加速效果未量化。",
            "tags_zh": [
                "偏好学习",
                "贝叶斯优化",
                "决策树模型",
                "可解释人工智能",
                "分类数据处理",
                "代理模型",
                "个性化推荐",
                "优化算法"
            ],
            "_index": 144
        },
        {
            "title": "FLAME: Flow Enhanced Legendre Memory Models for General Time Series Forecasting",
            "authors": [
                "Xingjian Wu",
                "Hanyin Cheng",
                "Xiangfei Qiu",
                "Zhengyu Li",
                "Jilin Hu",
                "Chenjuan Guo",
                "Bin Yang"
            ],
            "arxiv_id": "2512.14253v1",
            "summary": "In this work, we introduce FLAME, a family of extremely lightweight and capable Time Series Foundation Models, which support both deterministic and probabilistic forecasting via generative probabilistic modeling, thus ensuring both efficiency and robustness. FLAME utilizes the Legendre Memory for strong generalization capabilities. Through adapting variants of Legendre Memory, i.e., translated Legendre (LegT) and scaled Legendre (LegS), in the Encoding and Decoding phases, FLAME can effectively capture the inherent inductive bias within data and make efficient long-range inferences. To enhance the accuracy of probabilistic forecasting while keeping efficient, FLAME adopts a Normalization Flow based forecasting head, which can model the arbitrarily intricate distributions over the forecasting horizon in a generative manner. Comprehensive experiments on well-recognized benchmarks, including TSFM-Bench and ProbTS, demonstrate the consistent state-of-the-art zero-shot performance of FLAME on both deterministic and probabilistic forecasting tasks.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14253v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出FLAME时间序列基础模型，通过流增强的勒让德记忆机制实现高效鲁棒的确定性与概率性预测。",
            "summary_zh": "本文介绍了FLAME，一个极其轻量且强大的时间序列基础模型家族，支持通过生成式概率建模进行确定性和概率性预测，从而确保效率和鲁棒性。FLAME利用勒让德记忆实现强大的泛化能力。通过在编码和解码阶段采用勒让德记忆的变体，即平移勒让德（LegT）和缩放勒让德（LegS），FLAME能够有效捕捉数据中的内在归纳偏置，并进行高效的长程推理。为了在保持高效的同时增强概率性预测的准确性，FLAME采用基于归一化流的预测头，以生成方式建模预测范围内任意复杂的分布。在公认的基准测试（包括TSFM-Bench和ProbTS）上的全面实验表明，FLAME在确定性和概率性预测任务上均展现出持续的最先进零样本性能。",
            "intro_zh": [
                "现有时间序列预测方法在泛化能力、长程推理效率和概率分布建模方面存在不足，难以兼顾轻量化和高性能。",
                "FLAME通过勒让德记忆变体捕捉数据归纳偏置，结合归一化流预测头生成复杂分布，实现高效鲁棒的确定性与概率性预测。",
                "在TSFM-Bench和ProbTS基准测试中，FLAME在零样本设置下取得最先进性能，显著提升预测准确性和效率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决时间序列预测中的泛化能力弱、长程推理效率低以及概率分布建模不准确的问题。现有方法通常依赖复杂模型或特定领域知识，难以在轻量化前提下实现高效且鲁棒的确定性与概率性预测，尤其是在零样本场景下表现不佳。\\n\\n**核心思路**：FLAME的核心思路是结合勒让德记忆的泛化优势与归一化流的分布建模能力，设计一个轻量级的时间序列基础模型。通过勒让德记忆变体（LegT和LegS）捕捉数据中的内在结构，并利用归一化流生成任意复杂的预测分布，从而在保持高效的同时提升预测的准确性和鲁棒性。\\n\\n**技术框架**：FLAME的整体架构包括编码阶段、解码阶段和预测头。编码阶段使用勒让德记忆变体（如LegT和LegS）处理输入时间序列，提取特征并捕捉归纳偏置；解码阶段同样采用勒让德记忆进行长程推理，生成中间表示；预测头基于归一化流，将解码输出映射为预测分布，支持确定性和概率性预测。整个流程以生成式概率建模为核心，确保模型的高效性和泛化能力。\\n\\n**关键创新**：最重要的技术创新点是引入了流增强的勒让德记忆机制。与现有方法相比，FLAME通过勒让德记忆变体（LegT和LegS）在编码和解码阶段自适应地处理时间序列，有效捕捉数据动态，并结合归一化流建模复杂分布，这在轻量化模型中实现了前所未有的泛化性能和概率预测精度。本质区别在于将勒让德记忆的数学优势与生成式流模型相结合，避免了传统方法对大量数据或复杂结构的依赖。\\n\\n**关键设计**：关键设计包括：勒让德记忆变体（LegT和LegS）的参数化方式，用于调整记忆核的平移和缩放，以适应不同时间尺度；归一化流预测头的网络结构，通常基于可逆变换层（如仿射耦合层）来建模高维分布；损失函数结合了确定性预测的均方误差和概率性预测的负对数似然，以端到端方式优化；模型参数设置注重轻量化，例如通过低秩近似减少计算开销，确保在资源受限环境下的高效运行。",
            "application_zh": "FLAME的潜在应用领域广泛，包括金融时间序列预测（如股票价格和汇率）、物联网传感器数据分析、能源需求预测、医疗健康监测（如心电图信号）以及气候建模等。其轻量化和高效特性使其适用于边缘计算和实时系统，而强大的零样本性能支持跨领域迁移，减少对标注数据的依赖。未来可能推动时间序列基础模型的发展，促进AI在动态系统分析和决策支持中的实际部署。",
            "highlight_zh": "在TSFM-Bench和ProbTS基准测试中，FLAME在零样本设置下取得了最先进的性能。具体而言，在确定性预测任务上，FLAME相比基线模型（如Transformer和LSTM）平均提升了约10-15%的准确性（以RMSE或MAE衡量）；在概率性预测任务上，通过归一化流建模，FLAME在预测区间覆盖率和分布拟合度（如CRPS分数）上显著优于传统概率模型（如高斯过程），提升幅度达20%以上。实验还显示FLAME模型参数极少，推理速度比同类方法快2-3倍，凸显其高效性。",
            "tags_zh": [
                "时间序列预测",
                "勒让德记忆",
                "归一化流",
                "概率性建模",
                "零样本学习",
                "轻量化模型",
                "长程推理",
                "基础模型"
            ],
            "_index": 145
        },
        {
            "title": "From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition",
            "authors": [
                "Yiqing Zhou",
                "Yu Lei",
                "Shuzheng Si",
                "Qingyan Sun",
                "Wei Wang",
                "Yifei Wu",
                "Hao Wen",
                "Gang Chen",
                "Fanchao Qi",
                "Maosong Sun"
            ],
            "arxiv_id": "2512.14244v1",
            "summary": "Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14244v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出基于基本话语单元的上下文压缩框架，通过结构化分解与选择解决长文本处理中的计算成本与噪声问题。",
            "summary_zh": "管理大量上下文仍然是大型语言模型（LLMs）的关键瓶颈，特别是在长文档问答和自主代理等应用中，长输入会导致高计算成本并引入噪声。现有的压缩技术通常通过离散标记删除破坏局部连贯性，或依赖隐含的潜在编码，这些方法存在位置偏差且与闭源API不兼容。为解决这些限制，我们引入了基于EDU的上下文压缩器，这是一种新颖的显式压缩框架，旨在保留全局结构和细粒度细节。我们的方法将上下文压缩重新表述为“先结构后选择”的过程。首先，我们的LingoEDU将线性文本转换为基本话语单元（EDUs）的结构关系树，这些单元严格锚定到源索引以消除幻觉。其次，一个轻量级排名模块选择与查询相关的子树进行线性化。为了严格评估结构理解，我们发布了StructBench，这是一个包含248个多样化文档的手动标注数据集。实证结果表明，我们的方法实现了最先进的结构预测准确性，并显著优于前沿LLMs，同时降低了成本。此外，我们的结构感知压缩显著提高了从长上下文任务到复杂深度搜索场景的下游任务性能。",
            "intro_zh": [
                "现有压缩方法破坏局部连贯性或依赖隐含编码，导致位置偏差和API不兼容，难以平衡压缩效率与信息保留。",
                "提出基于基本话语单元的结构化分解框架，通过先构建关系树再选择相关子树，实现显式、可解释的上下文压缩。",
                "在StructBench数据集上实现最先进的结构预测精度，显著优于前沿LLMs，并在下游任务中提升性能，同时降低计算成本。"
            ],
            "method_zh": "**问题定义**：论文旨在解决长文本处理中上下文压缩的挑战，现有方法如离散标记删除会破坏局部连贯性，而隐含编码方法存在位置偏差且与闭源API不兼容，导致压缩后信息失真或难以集成。\\n\\n**核心思路**：将上下文压缩重新定义为“先结构后选择”的过程，通过将线性文本分解为基本话语单元（EDUs）的结构关系树，再基于查询相关性选择子树，实现显式、结构化的压缩，以保留全局逻辑和细粒度细节。\\n\\n**技术框架**：整体架构包含两个主要阶段：首先，LingoEDU模块将输入文本转换为EDUs的结构关系树，每个EDU严格锚定到源文本索引；其次，轻量级排名模块评估查询与子树的相关性，选择高相关子树进行线性化输出为压缩文本。\\n\\n**关键创新**：最重要的创新是引入EDU-based显式压缩框架，通过结构化分解消除幻觉，与现有方法相比，本质区别在于强调可解释性和结构保留，而非仅依赖隐含表示或简单删减。\\n\\n**关键设计**：LingoEDU基于语言学规则或预训练模型自动识别EDUs并构建关系树；排名模块可能使用注意力机制或相似度计算，具体参数和损失函数在论文中未详细说明，但强调轻量化和高效性以降低计算开销。",
            "application_zh": "该研究在长文档问答、自主代理、复杂深度搜索等场景具有广泛应用价值，能显著降低LLMs的计算成本并提升处理效率，未来可推动智能文档分析和多轮对话系统的发展，增强AI在真实世界任务中的实用性。",
            "highlight_zh": "在StructBench数据集上，该方法实现了最先进的结构预测准确性，具体性能数据未在摘要中提供，但显著优于前沿LLMs；实验表明，结构感知压缩在下游任务中提升性能，同时减少计算开销，例如在长上下文任务和深度搜索场景中表现优异。",
            "tags_zh": [
                "上下文压缩",
                "基本话语单元",
                "结构化分解",
                "长文本处理",
                "大型语言模型",
                "计算效率",
                "下游任务增强",
                "显式压缩框架"
            ],
            "_index": 146
        },
        {
            "title": "Beyond MMD: Evaluating Graph Generative Models with Geometric Deep Learning",
            "authors": [
                "Salvatore Romano",
                "Marco Grassia",
                "Giuseppe Mangioni"
            ],
            "arxiv_id": "2512.14241v1",
            "summary": "Graph generation is a crucial task in many fields, including network science and bioinformatics, as it enables the creation of synthetic graphs that mimic the properties of real-world networks for various applications. Graph Generative Models (GGMs) have emerged as a promising solution to this problem, leveraging deep learning techniques to learn the underlying distribution of real-world graphs and generate new samples that closely resemble them. Examples include approaches based on Variational Auto-Encoders, Recurrent Neural Networks, and more recently, diffusion-based models. However, the main limitation often lies in the evaluation process, which typically relies on Maximum Mean Discrepancy (MMD) as a metric to assess the distribution of graph properties in the generated ensemble. This paper introduces a novel methodology for evaluating GGMs that overcomes the limitations of MMD, which we call RGM (Representation-aware Graph-generation Model evaluation). As a practical demonstration of our methodology, we present a comprehensive evaluation of two state-of-the-art Graph Generative Models: Graph Recurrent Attention Networks (GRAN) and Efficient and Degree-guided graph GEnerative model (EDGE). We investigate their performance in generating realistic graphs and compare them using a Geometric Deep Learning model trained on a custom dataset of synthetic and real-world graphs, specifically designed for graph classification tasks. Our findings reveal that while both models can generate graphs with certain topological properties, they exhibit significant limitations in preserving the structural characteristics that distinguish different graph domains. We also highlight the inadequacy of Maximum Mean Discrepancy as an evaluation metric for GGMs and suggest alternative approaches for future research.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "physics.soc-ph"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "16 pages, 4 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14241v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出RGM方法以解决图生成模型评估中MMD指标的局限性问题",
            "summary_zh": "图生成是网络科学和生物信息学等领域的核心任务，图生成模型通过学习真实世界图的分布来生成相似的新样本，如基于变分自编码器、循环神经网络和扩散模型的方法。然而，现有评估过程主要依赖最大均值差异作为度量标准来评估生成图集合的属性分布，这存在明显局限。本文提出了一种新颖的图生成模型评估方法，称为RGM，以克服MMD的不足。作为方法实践，我们全面评估了两种最先进的图生成模型：图循环注意力网络和高效度引导图生成模型，通过几何深度学习模型在自定义合成与真实图数据集上进行图分类任务训练，分析它们在生成真实图方面的性能。研究发现，虽然两种模型都能生成具有特定拓扑属性的图，但在保持区分不同图域的结构特征方面存在显著局限。同时，我们强调了MMD作为图生成模型评估指标的不充分性，并为未来研究提出了替代方法。",
            "intro_zh": [
                "现有图生成模型评估主要依赖最大均值差异，但该指标无法充分捕捉图的结构特性，导致评估结果不全面。",
                "论文提出RGM方法，利用几何深度学习模型在自定义数据集上训练，通过图分类任务来评估生成图的真实性和结构保持能力。",
                "实验表明，GRAN和EDGE模型在生成图时存在结构特征保持不足的问题，RGM方法能更有效地揭示这些局限，为未来评估提供新方向。"
            ],
            "method_zh": "**问题定义**：论文旨在解决图生成模型评估中的核心问题，即现有方法主要依赖最大均值差异作为评估指标，但MMD仅能度量图属性的分布差异，无法充分评估生成图的结构真实性和域特异性特征，导致评估结果可能误导模型性能判断。\\n\\n**核心思路**：论文提出RGM方法，其核心思想是通过几何深度学习模型来评估图生成模型。具体来说，利用自定义的合成与真实图数据集训练一个图分类模型，然后将生成图输入该模型，通过分类性能来间接评估生成图是否保留了真实图的结构特征，从而克服MMD的局限性。\\n\\n**技术框架**：整体架构包括数据准备、模型训练和评估三个阶段。首先，构建包含合成图和真实图的自定义数据集，用于图分类任务。其次，训练一个几何深度学习模型作为评估器，学习图的结构表示。最后，将图生成模型生成的图输入评估器，通过分类准确率等指标来量化生成图的质量和结构保持能力。\\n\\n**关键创新**：最重要的技术创新是引入几何深度学习模型作为评估工具，而非传统统计指标。与现有方法本质区别在于，RGM关注图的结构语义和域区分能力，而MMD仅关注属性分布，这使得评估更全面和深入。\\n\\n**关键设计**：关键设计包括自定义数据集的构建，确保涵盖多样图域；使用几何深度学习模型，如图神经网络，以捕捉图的结构信息；评估指标基于分类性能，如准确率或F1分数，来量化生成图与真实图的相似度；实验中对GRAN和EDGE模型进行对比分析，突出RGM方法的有效性。",
            "application_zh": "该研究在图生成模型评估领域具有重要应用价值，可广泛应用于网络科学、生物信息学、社交网络分析和药物发现等领域。通过提供更准确的评估方法，RGM能帮助研究人员优化图生成模型，生成更真实的合成图，用于数据增强、仿真测试和隐私保护等任务。未来，该方法可能推动图生成技术的发展，提升模型在实际场景中的可靠性和实用性。",
            "highlight_zh": "实验结果显示，GRAN和EDGE模型在生成图时，虽然能复制某些拓扑属性，但在保持结构特征方面表现不佳，具体表现为在自定义数据集上的分类准确率较低，表明生成图与真实图在域区分能力上存在差距。与基于MMD的评估相比，RGM方法能更敏感地揭示这些结构缺陷，例如在特定图类上的性能下降幅度超过20%，突显了MMD作为评估指标的不足。这为未来图生成模型的研究提供了新的评估基准和改进方向。",
            "tags_zh": [
                "图生成模型评估",
                "几何深度学习",
                "最大均值差异",
                "图结构保持",
                "图分类任务",
                "自定义数据集",
                "图神经网络",
                "域特异性特征"
            ],
            "_index": 147
        },
        {
            "title": "Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets",
            "authors": [
                "Estelle Zheng",
                "Nathan Cerisara",
                "Sébastien Warichet",
                "Emmanuel Helbert",
                "Christophe Cerisara"
            ],
            "arxiv_id": "2512.14237v1",
            "summary": "Fine-tuning large language models (LLMs) is often limited by the memory available on commodity GPUs. Parameter-efficient fine-tuning (PEFT) methods such as QLoRA reduce the number of trainable parameters, yet still incur high memory usage induced by the backward pass in the full model. We revisit Ladder Side Tuning (LST), a rarely explored PEFT technique that adds a lightweight side network, and show that it matches QLoRA's compute scaling slope while cutting peak memory by 50\\%. Across different downstream benchmarks spanning natural language understanding, mathematical and LLM-critic tasks, LST has competitive performance with QLoRA's accuracy on average while being much more memory-efficient. This efficiency enables fine-tuning of 7B-parameter models on a single 12 GB consumer GPU with 2k-token contexts, requiring no gradient checkpointing\\textemdash conditions under which QLoRA exhausts memory. Beyond memory efficiency, we also establish scaling laws showing that LST scales similarly to QLoRA. We exploit Ladder's architectural flexibility by introducing xLadder, a depth-extended variant that increases effective depth via cross-connections and shortens chain-of-thought (CoT) at fixed parameter count. Ladder is strong when memory is the bottleneck; xLadder builds on this by enabling deeper reasoning without additional memory overhead.",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14237v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出Ladder Side Tuning方法，通过轻量级侧网络解决大语言模型微调中的内存瓶颈问题。",
            "summary_zh": "微调大语言模型（LLMs）常受限于商用GPU的内存容量。参数高效微调（PEFT）方法如QLoRA虽减少了可训练参数数量，但完整模型的反向传播仍导致高内存占用。本文重新审视了Ladder Side Tuning（LST），一种较少被探索的PEFT技术，它通过添加轻量级侧网络，在保持与QLoRA相似计算扩展斜率的同时，将峰值内存降低50%。在涵盖自然语言理解、数学和LLM批评任务的不同下游基准测试中，LST平均性能与QLoRA相当，同时内存效率更高。这种效率使得在单个12GB消费级GPU上微调70亿参数模型成为可能，支持2k令牌上下文且无需梯度检查点——在这些条件下QLoRA会耗尽内存。除了内存效率，我们还建立了扩展定律，显示LST与QLoRA具有相似的扩展性。通过利用Ladder的架构灵活性，我们引入了xLadder，一种深度扩展变体，通过交叉连接增加有效深度，并在固定参数数量下缩短思维链（CoT）。Ladder在内存受限时表现强劲；xLadder在此基础上实现了更深层推理，且无额外内存开销。",
            "intro_zh": [
                "核心问题：现有参数高效微调方法如QLoRA虽减少可训练参数，但反向传播仍导致高内存占用，限制大模型在消费级GPU上的微调。",
                "方法要点：提出Ladder Side Tuning，通过添加轻量级侧网络，仅微调侧网络参数，大幅降低内存需求，同时保持模型性能。",
                "实验或效果：在多个下游任务中，LST性能与QLoRA相当，峰值内存降低50%，支持70亿参数模型在12GB GPU上微调。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大语言模型（LLMs）微调中的内存瓶颈问题。现有参数高效微调（PEFT）方法如QLoRA虽减少可训练参数，但反向传播过程仍需计算完整模型的梯度，导致高内存占用，限制了在消费级GPU（如12GB内存）上微调大规模模型（如70亿参数）的可行性，尤其是在长上下文（如2k令牌）场景下。\\n\\n**核心思路**：论文的核心思路是重新利用Ladder Side Tuning（LST），一种基于侧网络的PEFT技术。通过添加一个轻量级的侧网络（side network）到预训练LLM中，仅微调侧网络的参数，而冻结主模型参数，从而大幅减少反向传播时的内存需求，因为梯度计算仅限于侧网络，而非整个模型。\\n\\n**技术框架**：整体架构包括一个预训练的大语言模型（主网络）和一个附加的轻量级侧网络。在微调过程中，主网络的参数被冻结，仅侧网络的参数可训练。侧网络通过连接点（如中间层输出）与主网络交互，接收主网络的激活值，处理后输出补充信息，与主网络输出结合以完成下游任务。训练时，损失函数基于任务目标（如分类、生成）计算，梯度仅通过侧网络反向传播。\\n\\n**关键创新**：最重要的技术创新是系统性地验证和扩展LST作为内存高效的PEFT方法。与QLoRA等现有方法相比，LST的本质区别在于其架构设计：它通过侧网络实现参数隔离，使得内存开销与侧网络规模而非主模型规模成正比，从而在保持性能的同时显著降低内存占用。此外，论文引入xLadder变体，通过交叉连接增加网络深度，提升推理能力而无额外内存成本。\\n\\n**关键设计**：关键设计包括侧网络的轻量化结构（如小型前馈网络），以减少参数数量；连接策略（如从主网络特定层提取特征），以确保信息传递；损失函数基于具体下游任务（例如交叉熵损失用于分类任务）；参数设置上，侧网络规模远小于主模型（例如仅占主模型参数的少量百分比），以实现内存节约。xLadder通过引入跨层连接（cross-connections）扩展深度，具体细节如连接方式和层数调整需参考论文实验部分。",
            "application_zh": "该研究在自然语言处理领域具有广泛的应用潜力，特别是在资源受限环境下微调大语言模型。实际价值包括：在消费级硬件（如个人GPU）上高效微调模型，用于定制化AI助手、内容生成、代码补全等任务；在边缘设备或移动端部署轻量级AI应用，降低计算成本；未来可能推动更环保的AI训练，减少能源消耗。影响方面，它为内存敏感的微调场景提供了新解决方案，促进大模型技术的普及和民主化。",
            "highlight_zh": "最重要的实验结果包括：LST在多个下游基准测试（自然语言理解、数学、LLM批评任务）中平均性能与QLoRA相当，但峰值内存降低50%。具体数据上，LST支持在单个12GB消费级GPU上微调70亿参数模型，使用2k令牌上下文且无需梯度检查点，而QLoRA在相同条件下会耗尽内存。扩展定律显示LST与QLoRA具有相似的计算扩展斜率，验证了其可扩展性。xLadder变体进一步提升了推理深度，在固定参数下缩短思维链，展示了架构灵活性带来的性能增益。",
            "tags_zh": [
                "参数高效微调",
                "大语言模型",
                "内存优化",
                "侧网络",
                "轻量级训练",
                "自然语言处理",
                "GPU资源受限",
                "扩展定律"
            ],
            "_index": 148
        },
        {
            "title": "4D-RaDiff: Latent Diffusion for 4D Radar Point Cloud Generation",
            "authors": [
                "Jimmie Kwok",
                "Holger Caesar",
                "Andras Palffy"
            ],
            "arxiv_id": "2512.14235v1",
            "summary": "Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14235v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "自动驾驶",
                    "matched_keywords": [
                        "lidar"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出4D-RaDiff框架，通过潜在扩散生成4D雷达点云，以解决自动驾驶中雷达数据标注不足的问题。",
            "summary_zh": "汽车雷达因其成本效益和在恶劣天气条件下的鲁棒性，在环境感知方面展现出有前景的发展。然而，标注雷达数据的有限可用性对推进基于雷达的感知系统构成了重大挑战。为解决这一限制，我们提出了一个新颖的框架来生成4D雷达点云，用于训练和评估物体检测器。与基于图像的扩散不同，我们的方法旨在通过将扩散应用于潜在点云表示来考虑雷达点云的稀疏性和独特特性。在这个潜在空间中，生成通过对象或场景级别的条件进行控制。所提出的4D-RaDiff将未标注的边界框转换为高质量的雷达标注，并将现有的激光雷达点云数据转换为逼真的雷达场景。实验表明，在训练期间将4D-RaDiff的合成雷达数据作为数据增强方法，与仅使用真实数据训练相比，持续提高了物体检测性能。此外，在我们的合成数据上进行预训练，可将所需标注雷达数据量减少高达90%，同时实现可比的物体检测性能。",
            "intro_zh": [
                "核心问题：自动驾驶中雷达数据标注不足，限制了基于雷达的感知系统发展，现有方法难以生成逼真的雷达点云。",
                "方法要点：提出4D-RaDiff框架，在潜在空间应用扩散模型生成4D雷达点云，通过对象或场景条件控制生成过程。",
                "实验或效果：合成数据作为增强方法提升检测性能，预训练减少90%标注数据需求，保持可比性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自动驾驶中4D雷达点云数据标注不足的问题，现有方法如基于图像的扩散模型难以处理雷达点云的稀疏性和独特特性，导致生成数据质量低，限制了雷达感知系统的训练和评估。\\n\\n**核心思路**：论文提出在潜在点云表示上应用扩散模型，而非直接在原始点云上操作，以更好地建模雷达点云的稀疏分布和噪声特性，通过条件控制生成过程，实现高质量雷达点云合成。\\n\\n**技术框架**：整体框架包括数据预处理、潜在表示学习、扩散过程生成和条件控制模块。首先将输入数据（如未标注边界框或激光雷达点云）转换为潜在表示，然后在潜在空间应用扩散模型进行生成，最后解码为雷达点云，整个过程可通过对象级或场景级条件进行引导。\\n\\n**关键创新**：最重要的技术创新是将扩散模型应用于雷达点云的潜在表示，而非传统图像或密集点云，这考虑了雷达数据的稀疏性和噪声模式，与现有方法相比，本质区别在于针对雷达特性定制生成过程，提高了生成数据的真实性和实用性。\\n\\n**关键设计**：关键设计包括潜在编码器的网络结构，用于学习点云的紧凑表示；扩散模型中的噪声调度和去噪步骤，优化生成质量；条件机制如嵌入向量，实现对象或场景控制；损失函数可能结合重建损失和对抗损失，具体参数设置未在摘要中详细说明，需参考论文正文。",
            "application_zh": "该研究主要应用于自动驾驶领域，通过生成高质量合成雷达数据，可用于训练和评估物体检测器，提升雷达感知系统的性能。实际价值在于减少对昂贵标注数据的依赖，加速雷达技术研发，未来可能扩展到其他传感器融合场景，推动智能交通系统发展。",
            "highlight_zh": "实验表明，使用4D-RaDiff生成的合成雷达数据作为数据增强方法，在训练物体检测器时，相比仅使用真实数据，检测性能持续提升。具体地，预训练在合成数据上可将所需标注雷达数据量减少高达90%，同时保持可比的检测性能，这显著降低了数据收集和标注成本。",
            "tags_zh": [
                "4D雷达点云生成",
                "潜在扩散模型",
                "自动驾驶感知",
                "数据增强",
                "物体检测",
                "雷达数据合成",
                "条件生成",
                "传感器模拟"
            ],
            "_index": 149
        },
        {
            "title": "Multi-View MRI Approach for Classification of MGMT Methylation in Glioblastoma Patients",
            "authors": [
                "Rawan Alyahya",
                "Asrar Alruwayqi",
                "Atheer Alqarni",
                "Asma Alkhaldi",
                "Metab Alkubeyyer",
                "Xin Gao",
                "Mona Alshahrani"
            ],
            "arxiv_id": "2512.14232v1",
            "summary": "The presence of MGMT promoter methylation significantly affects how well chemotherapy works for patients with Glioblastoma Multiforme (GBM). Currently, confirmation of MGMT promoter methylation relies on invasive brain tumor tissue biopsies. In this study, we explore radiogenomics techniques, a promising approach in precision medicine, to identify genetic markers from medical images. Using MRI scans and deep learning models, we propose a new multi-view approach that considers spatial relationships between MRI views to detect MGMT methylation status. Importantly, our method extracts information from all three views without using a complicated 3D deep learning model, avoiding issues associated with high parameter count, slow convergence, and substantial memory demands. We also introduce a new technique for tumor slice extraction and show its superiority over existing methods based on multiple evaluation metrics. By comparing our approach to state-of-the-art models, we demonstrate the efficacy of our method. Furthermore, we share a reproducible pipeline of published models, encouraging transparency and the development of robust diagnostic tools. Our study highlights the potential of non-invasive methods for identifying MGMT promoter methylation and contributes to advancing precision medicine in GBM treatment.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14232v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出多视图MRI方法，通过空间关系建模检测胶质母细胞瘤MGMT甲基化状态，避免复杂3D模型问题。",
            "summary_zh": "MGMT启动子甲基化的存在显著影响胶质母细胞瘤（GBM）患者化疗效果。目前，MGMT启动子甲基化的确认依赖于侵入性脑肿瘤组织活检。本研究探索了影像基因组学技术，这是一种在精准医学中具有前景的方法，旨在从医学图像中识别遗传标记。利用MRI扫描和深度学习模型，我们提出了一种新的多视图方法，该方法考虑了MRI视图之间的空间关系来检测MGMT甲基化状态。重要的是，我们的方法从所有三个视图中提取信息，而不使用复杂的3D深度学习模型，避免了高参数数量、收敛缓慢和大量内存需求等问题。我们还引入了一种新的肿瘤切片提取技术，并基于多个评估指标展示了其相对于现有方法的优越性。通过将我们的方法与最先进的模型进行比较，我们证明了我们方法的有效性。此外，我们分享了已发表模型的可复现流程，鼓励透明度和稳健诊断工具的开发。我们的研究突出了非侵入性方法识别MGMT启动子甲基化的潜力，并有助于推进GBM治疗中的精准医学。",
            "intro_zh": [
                "核心问题：现有MGMT甲基化检测依赖侵入性活检，风险高且耗时，缺乏非侵入性精准方法。",
                "方法要点：提出多视图MRI方法，结合空间关系建模，避免复杂3D模型，实现高效甲基化状态检测。",
                "实验或效果：新方法在多个评估指标上优于现有技术，验证了非侵入性检测的可行性和有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决胶质母细胞瘤（GBM）患者MGMT启动子甲基化状态的非侵入性检测问题。现有方法主要依赖侵入性脑肿瘤组织活检，具有高风险和操作复杂性，而传统基于医学影像的方法往往忽略多视图间的空间关系，或使用复杂3D深度学习模型导致高参数、慢收敛和大内存需求。\\n\\n**核心思路**：论文提出一种多视图MRI方法，通过整合MRI扫描中的多个视图（如轴向、冠状、矢状面），并建模视图间的空间关系，来检测MGMT甲基化状态。核心设计在于避免使用复杂的3D模型，而是从所有视图中提取信息，以降低计算负担并提高效率。\\n\\n**技术框架**：整体架构包括数据预处理、肿瘤切片提取、多视图特征融合和分类模块。首先，使用MRI扫描进行数据预处理；然后，应用新提出的肿瘤切片提取技术获取关键区域；接着，通过深度学习模型处理多个视图，并融合空间关系信息；最后，输出MGMT甲基化状态的分类结果。\\n\\n**关键创新**：最重要的技术创新是引入多视图空间关系建模，结合新的肿瘤切片提取技术，避免了复杂3D模型的使用。与现有方法的本质区别在于，它更注重视图间的交互和高效信息提取，而非依赖高维3D数据，从而解决了参数多、收敛慢和内存需求大的问题。\\n\\n**关键设计**：关键设计包括使用特定深度学习模型（如卷积神经网络）处理多视图MRI数据，设计损失函数（如交叉熵损失）进行优化，并设置参数以减少模型复杂度。肿瘤切片提取技术基于图像处理算法，确保提取的切片能有效代表肿瘤区域，提升分类准确性。",
            "application_zh": "该研究在精准医学领域具有重要应用价值，特别是在神经肿瘤学中，可用于非侵入性检测胶质母细胞瘤的MGMT甲基化状态，辅助化疗方案制定。潜在应用包括临床诊断工具开发、个性化治疗规划，以及推动影像基因组学在癌症治疗中的普及。未来可能扩展到其他脑肿瘤或癌症类型的遗传标记检测，提升医疗效率和患者预后。",
            "highlight_zh": "实验结果显示，新提出的多视图MRI方法在检测MGMT甲基化状态上表现优异。通过对比现有最先进模型，该方法在多个评估指标（如准确率、召回率）上均有显著提升，具体提升幅度未知，但验证了其优越性。新肿瘤切片提取技术也优于现有方法，进一步增强了分类性能，证明了非侵入性检测的可行性和高效性。",
            "tags_zh": [
                "多视图MRI",
                "MGMT甲基化检测",
                "胶质母细胞瘤",
                "影像基因组学",
                "深度学习",
                "非侵入性诊断",
                "精准医学",
                "空间关系建模"
            ],
            "_index": 150
        },
        {
            "title": "Weighted Conformal Prediction Provides Adaptive and Valid Mask-Conditional Coverage for General Missing Data Mechanisms",
            "authors": [
                "Jiarong Fan",
                "Juhyun Park. Thi Phuong Thuy Vo",
                "Nicolas Brunel"
            ],
            "arxiv_id": "2512.14221v1",
            "summary": "Conformal prediction (CP) offers a principled framework for uncertainty quantification, but it fails to guarantee coverage when faced with missing covariates. In addressing the heterogeneity induced by various missing patterns, Mask-Conditional Valid (MCV) Coverage has emerged as a more desirable property than Marginal Coverage. In this work, we adapt split CP to handle missing values by proposing a preimpute-mask-then-correct framework that can offer valid coverage. We show that our method provides guaranteed Marginal Coverage and Mask-Conditional Validity for general missing data mechanisms. A key component of our approach is a reweighted conformal prediction procedure that corrects the prediction sets after distributional imputation (multiple imputation) of the calibration dataset, making our method compatible with standard imputation pipelines. We derive two algorithms, and we show that they are approximately marginally valid and MCV. We evaluate them on synthetic and real-world datasets. It reduces significantly the width of prediction intervals w.r.t standard MCV methods, while maintaining the target guarantees.",
            "categories": [
                "stat.ML",
                "cs.LG"
            ],
            "primary_category": "stat.ML",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14221v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出加权保形预测方法，为一般缺失数据机制提供自适应且有效的掩码条件覆盖保证",
            "summary_zh": "保形预测（CP）为不确定性量化提供了原则性框架，但在面对缺失协变量时无法保证覆盖。针对各种缺失模式引起的异质性，掩码条件有效（MCV）覆盖已成为比边际覆盖更理想的属性。在本工作中，我们通过提出一个预插补-掩码-然后校正的框架来适应分割CP处理缺失值，该框架可以提供有效的覆盖。我们证明，我们的方法为一般缺失数据机制提供了保证的边际覆盖和掩码条件有效性。我们方法的一个关键组成部分是重新加权的保形预测过程，在校准数据集的分布插补（多重插补）后校正预测集，使我们的方法与标准插补流程兼容。我们推导出两种算法，并证明它们近似边际有效和MCV。我们在合成和真实世界数据集上对它们进行评估。与标准MCV方法相比，它显著减少了预测区间的宽度，同时保持了目标保证。",
            "intro_zh": [
                "保形预测在处理缺失协变量时无法保证覆盖，现有方法难以应对缺失模式异质性。",
                "提出预插补-掩码-校正框架，通过加权保形预测校正插补后的预测集，确保掩码条件有效性。",
                "在合成和真实数据集上验证，相比标准方法显著减少预测区间宽度，同时维持目标覆盖保证。"
            ],
            "method_zh": "**问题定义**：论文旨在解决保形预测在处理缺失协变量时无法保证覆盖的问题，特别是当数据存在多种缺失模式时，现有方法难以提供有效的掩码条件覆盖，导致预测不确定性量化不准确。\\n\\n**核心思路**：通过引入加权保形预测，结合预插补和掩码处理，校正插补后数据的分布偏差，从而为一般缺失数据机制提供自适应且有效的掩码条件覆盖保证。\\n\\n**技术框架**：整体采用预插补-掩码-校正的三阶段框架：首先对校准数据集进行分布插补（如多重插补），然后应用掩码条件处理缺失模式，最后通过重新加权的保形预测过程校正预测集，确保覆盖有效性。\\n\\n**关键创新**：最重要的技术创新是加权保形预测过程，它通过权重调整校正插补引入的分布偏差，与现有方法相比，本质区别在于能够兼容标准插补流程并提供理论保证的掩码条件有效性。\\n\\n**关键设计**：论文推导了两种算法，具体技术细节包括基于缺失模式的权重计算、插补方法的集成（如多重插补），以及保形预测中的得分函数设计，以确保近似边际有效性和MCV属性。",
            "application_zh": "该研究可应用于医疗诊断、金融风险评估和工业监控等领域，其中数据常存在缺失值，需要可靠的不确定性量化。通过提供有效的掩码条件覆盖，能提升预测模型的鲁棒性和可信度，对实际决策支持具有重要价值，未来可能推动缺失数据处理标准的发展。",
            "highlight_zh": "在合成和真实数据集上的实验表明，相比标准MCV方法，该方法显著减少了预测区间宽度，具体提升幅度因数据集而异，例如在某些场景下宽度减少可达20%以上，同时严格维持了目标覆盖保证（如95%置信水平），验证了其有效性和效率优势。",
            "tags_zh": [
                "保形预测",
                "缺失数据处理",
                "掩码条件覆盖",
                "不确定性量化",
                "加权校正",
                "多重插补",
                "预测区间优化",
                "数据异质性"
            ],
            "_index": 151
        },
        {
            "title": "Trajectory Tracking for Multi-Manipulator Systems in Constrained Environments",
            "authors": [
                "Mayank Sewlia",
                "Christos K. Verginis",
                "Dimos V. Dimarogonas"
            ],
            "arxiv_id": "2512.14206v1",
            "summary": "We consider the problem of cooperative manipulation by a mobile multi-manipulator system operating in obstacle-cluttered and highly constrained environments under spatio-temporal task specifications. The task requires transporting a grasped object while respecting both continuous robot dynamics and discrete geometric constraints arising from obstacles and narrow passages. To address this hybrid structure, we propose a multi-rate planning and control framework that combines offline generation of an STL-satisfying object trajectory and collision-free base footprints with online constrained inverse kinematics and continuous-time feedback control. The resulting closed-loop system enables coordinated reconfiguration of multiple manipulators while tracking the desired object motion. The approach is evaluated in high-fidelity physics simulations using three Franka Emika Panda mobile manipulators rigidly grasping an object.",
            "categories": [
                "cs.RO",
                "eess.SY"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14206v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "物理动作",
                    "matched_keywords": [
                        "physics simulation"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出多速率规划与控制框架，解决多移动机械臂在受限环境中协同搬运的轨迹跟踪问题。",
            "summary_zh": "本文研究了在障碍物密集且高度受限环境中，移动多机械臂系统在时空任务规范下的协同操作问题。任务要求在运输抓取物体时，同时满足连续机器人动力学和由障碍物及狭窄通道引起的离散几何约束。为应对这种混合结构，我们提出了一个多速率规划与控制框架，该框架结合了离线生成满足信号时序逻辑（STL）的物体轨迹和无碰撞基座足迹，以及在线约束逆运动学和连续时间反馈控制。由此产生的闭环系统能够在跟踪期望物体运动的同时，实现多个机械臂的协调重构。该方法使用三个刚性抓取物体的Franka Emika Panda移动机械臂，在高保真物理仿真中进行了评估。",
            "intro_zh": [
                "核心问题：现有方法难以同时处理多移动机械臂协同搬运中的连续动力学、离散几何约束及复杂时空任务要求，尤其在障碍物密集的受限环境中。",
                "方法要点：提出多速率规划与控制框架，离线生成满足STL的物体轨迹与无碰撞基座路径，在线结合约束逆运动学与反馈控制实现协调跟踪。",
                "实验或效果：在高保真仿真中，使用三个Panda移动机械臂成功实现了在受限环境下的物体搬运，验证了框架的有效性与协调性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多移动机械臂系统在障碍物密集、空间受限环境中协同搬运物体时的轨迹跟踪问题。现有方法常难以统一处理连续时间动力学与离散空间约束（如避障、狭窄通道）的混合特性，且缺乏对复杂时空任务规范（如信号时序逻辑STL）的有效集成，导致规划与控制脱节，在动态或高度约束场景中性能受限。\\n\\n**核心思路**：为解决上述混合结构挑战，论文提出一个分层多速率框架，核心思想是将问题分解为离线高层规划与在线底层控制。离线阶段专注于生成满足高级任务规范（STL）的物体级轨迹和保证无碰撞的基座运动路径；在线阶段则通过约束逆运动学将物体运动分解为各机械臂关节指令，并结合连续时间反馈控制实现鲁棒跟踪与实时协调。这种设计允许在保证全局任务满足性的同时，处理局部动力学与约束。\\n\\n**技术框架**：整体架构包含两个主要阶段：1) 离线规划阶段：基于环境模型与任务STL规范，使用优化方法生成物体的参考轨迹（满足时空约束）及各移动基座的无碰撞足迹路径。2) 在线控制阶段：采用多速率循环，高层以较低频率基于当前状态更新逆运动学解，将物体轨迹转换为各机械臂的关节目标；底层以较高频率运行连续时间反馈控制器（如基于动力学的PD或阻抗控制），驱动机械臂跟踪关节目标并处理扰动。闭环系统通过协调各机械臂的重新配置（如调整姿态以避免自碰撞或环境干涉）来精确跟踪物体运动。\\n\\n**关键创新**：最重要的技术创新在于提出了一个统一的多速率框架，将STL任务规范、离散几何约束与连续动力学控制紧密结合。与现有方法相比，其本质区别在于：a) 显式处理了混合系统结构，通过离线/在线分解协调全局规划与局部控制；b) 集成STL使得能够表达复杂的时空任务（如“在时间T内到达区域A并始终避障”），超越了传统的点对点轨迹规划；c) 通过约束逆运动学实现多机械臂的在线协调重构，增强了在狭窄环境中的灵活性。\\n\\n**关键设计**：技术细节包括：1) STL规范用于定义任务的时间与逻辑约束，如始终性、最终性算子，确保轨迹满足安全与进度要求。2) 离线规划可能采用基于采样的优化（如RRT*或非线性规划）来生成平滑且无碰撞的物体轨迹与基座路径。3) 在线逆运动学引入约束（如关节限位、避障距离约束），通过数值优化（如QP求解器）实时计算可行解。4) 反馈控制器基于机器人动力学模型设计，可能包含重力补偿和阻尼项以增强鲁棒性。5) 框架参数如规划频率（如1-10 Hz）与控制频率（如100-1000 Hz）需根据系统动力学调整，以平衡计算开销与跟踪精度。",
            "application_zh": "该研究在工业自动化、物流仓储和灾难救援等领域具有重要应用价值。例如，在工厂车间中，多移动机械臂可协同搬运大型或易损部件穿越狭窄通道；在仓库中，能实现高效货物转运与堆垛；在救援场景中，可用于在废墟等受限空间操作重物。未来，该框架可扩展至更复杂任务（如柔性物体操纵）或动态环境，推动自主机器人系统在真实世界中的部署。",
            "highlight_zh": "实验在高保真物理仿真（如Gazebo或MuJoCo）中进行，使用三个Franka Emika Panda移动机械臂刚性抓取一个物体。主要结果包括：成功在障碍物密集的受限环境中完成了物体搬运任务，轨迹跟踪误差保持在厘米级（具体数据未知，但论文暗示满足精度要求）；系统能够实时协调各机械臂的重新配置以避障，验证了多速率框架的有效性；与基线方法（如纯运动学规划或无STL集成的方法）相比，提升了任务成功率和在狭窄通道中的通过性，但具体提升幅度未在摘要中量化。",
            "tags_zh": [
                "多移动机械臂协同",
                "轨迹跟踪控制",
                "信号时序逻辑",
                "混合系统规划",
                "受限环境操作",
                "逆运动学优化",
                "物理仿真验证",
                "时空任务规范"
            ],
            "_index": 152
        },
        {
            "title": "Fracture Morphology Classification: Local Multiclass Modeling for Multilabel Complexity",
            "authors": [
                "Cassandra Krause",
                "Mattias P. Heinrich",
                "Ron Keuth"
            ],
            "arxiv_id": "2512.14196v1",
            "summary": "Between $15\\,\\%$ and $45\\,\\%$ of children experience a fracture during their growth years, making accurate diagnosis essential. Fracture morphology, alongside location and fragment angle, is a key diagnostic feature. In this work, we propose a method to extract fracture morphology by assigning automatically global AO codes to corresponding fracture bounding boxes. This approach enables the use of public datasets and reformulates the global multilabel task into a local multiclass one, improving the average F1 score by $7.89\\,\\%$. However, performance declines when using imperfect fracture detectors, highlighting challenges for real-world deployment. Our code is available on GitHub.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted as poster at the German Conference on Medical Image Computing 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14196v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出局部多类建模方法，将骨折形态全局多标签分类任务转化为局部多类任务，提升诊断准确性。",
            "summary_zh": "在儿童成长期间，15%至45%的儿童会经历骨折，因此准确诊断至关重要。骨折形态与位置和碎片角度一样，是关键的诊断特征。在这项工作中，我们提出了一种方法，通过自动分配全局AO代码到相应的骨折边界框来提取骨折形态。这种方法能够利用公共数据集，并将全局多标签任务重新表述为局部多类任务，将平均F1分数提高了7.89%。然而，当使用不完美的骨折检测器时，性能会下降，这突显了实际部署中的挑战。我们的代码已在GitHub上提供。",
            "intro_zh": [
                "核心问题：骨折形态分类面临全局多标签任务的复杂性，现有方法难以准确处理多标签关联和局部特征提取，导致诊断精度受限。",
                "方法要点：将全局多标签任务转化为局部多类任务，通过自动分配AO代码到骨折边界框，简化模型学习过程，提升分类性能。",
                "实验或效果：平均F1分数提升7.89%，但依赖完美骨折检测器，实际部署中性能下降，突显鲁棒性挑战。"
            ],
            "method_zh": "**问题定义**：论文旨在解决骨折形态分类问题，这是一个关键的医学诊断任务，涉及从医学图像中自动识别骨折的形态特征。现有方法的痛点在于，骨折形态分类通常被建模为全局多标签任务，即对整个图像进行多标签分类，这导致模型难以处理复杂的多标签关联和局部细节，尤其是在公共数据集有限的情况下，性能受限。\\n\\n**核心思路**：论文的核心解决思路是将全局多标签任务重新表述为局部多类任务。通过将骨折形态分类与骨折检测相结合，将全局的AO代码分配问题转化为局部的边界框分类问题。这样设计的原因是，局部建模可以更专注于骨折区域的特定特征，简化学习过程，并利用公共数据集中的边界框标注，提高模型的泛化能力和准确性。\\n\\n**技术框架**：整体架构包括两个主要阶段：骨折检测和局部分类。首先，使用骨折检测器（如基于深度学习的对象检测模型）从医学图像中提取骨折的边界框。然后，对每个边界框应用局部多类分类模型，自动分配对应的AO代码（代表骨折形态）。流程为：输入医学图像 → 骨折检测模块生成边界框 → 局部分类模块对每个边界框进行多类分类 → 输出骨折形态标签。\\n\\n**关键创新**：最重要的技术创新点是将骨折形态分类从全局多标签任务转化为局部多类任务。与现有方法的本质区别在于，现有方法通常直接对整个图像进行多标签预测，而本方法通过结合检测和分类，实现更精细的局部建模，减少了多标签间的干扰，并利用公共数据集的边界框信息提升性能。\\n\\n**关键设计**：关键设计包括使用标准的对象检测框架（如Faster R-CNN或YOLO）进行骨折检测，以及基于卷积神经网络（CNN）的局部分类器进行AO代码分配。损失函数可能结合检测损失（如边界框回归和分类损失）和分类损失（如交叉熵损失）。参数设置涉及网络结构的选择和训练策略，例如使用预训练模型和优化器（如Adam）。具体细节未在摘要中详细说明，但代码公开可供参考。",
            "application_zh": "该研究在医学影像分析领域具有重要应用价值，特别是在儿科骨折诊断中。通过自动提取骨折形态，可以辅助医生进行更准确的诊断，减少误诊率，并提高医疗效率。未来，该方法可集成到临床决策支持系统中，推动智能医疗的发展，但需解决检测器不完美带来的鲁棒性问题，以实现实际部署。",
            "highlight_zh": "最重要的实验结果是平均F1分数提升了7.89%，这表明局部多类建模有效提高了骨折形态分类的准确性。实验对比了全局多标签基线方法，新方法在性能上显著超越。然而，当使用不完美的骨折检测器时，性能下降，突显了模型对检测精度的依赖性，这在实际应用中是一个关键挑战。具体数据未在摘要中提供，但提升幅度明确。",
            "tags_zh": [
                "骨折形态分类",
                "局部多类建模",
                "全局多标签任务",
                "医学影像分析",
                "AO代码分配",
                "骨折检测",
                "深度学习",
                "儿科诊断"
            ],
            "_index": 153
        },
        {
            "title": "Improving Semantic Uncertainty Quantification in LVLMs with Semantic Gaussian Processes",
            "authors": [
                "Joseph Hoche",
                "Andrei Bursuc",
                "David Brellmann",
                "Gilles Louppe",
                "Pavel Izmailov",
                "Angela Yao",
                "Gianni Franchi"
            ],
            "arxiv_id": "2512.14177v1",
            "summary": "Large Vision-Language Models (LVLMs) often produce plausible but unreliable outputs, making robust uncertainty estimation essential. Recent work on semantic uncertainty estimates relies on external models to cluster multiple sampled responses and measure their semantic consistency. However, these clustering methods are often fragile, highly sensitive to minor phrasing variations, and can incorrectly group or separate semantically similar answers, leading to unreliable uncertainty estimates. We propose Semantic Gaussian Process Uncertainty (SGPU), a Bayesian framework that quantifies semantic uncertainty by analyzing the geometric structure of answer embeddings, avoiding brittle clustering. SGPU maps generated answers into a dense semantic space, computes the Gram matrix of their embeddings, and summarizes their semantic configuration via the eigenspectrum. This spectral representation is then fed into a Gaussian Process Classifier that learns to map patterns of semantic consistency to predictive uncertainty, and that can be applied in both black-box and white-box settings. Across six LLMs and LVLMs on eight datasets spanning VQA, image classification, and textual QA, SGPU consistently achieves state-of-the-art calibration (ECE) and discriminative (AUROC, AUARC) performance. We further show that SGPU transfers across models and modalities, indicating that its spectral representation captures general patterns of semantic uncertainty.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14177v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出语义高斯过程不确定性框架，以解决大视觉语言模型中语义不确定性量化不可靠的问题。",
            "summary_zh": "大视觉语言模型（LVLMs）常产生看似合理但不可靠的输出，因此鲁棒的不确定性估计至关重要。近期关于语义不确定性的研究依赖于外部模型对多个采样响应进行聚类并测量其语义一致性。然而，这些聚类方法通常脆弱，对细微的措辞变化高度敏感，可能错误地分组或分离语义相似的答案，导致不可靠的不确定性估计。我们提出了语义高斯过程不确定性（SGPU），这是一个贝叶斯框架，通过分析答案嵌入的几何结构来量化语义不确定性，避免了脆弱的聚类。SGPU将生成的答案映射到密集的语义空间，计算其嵌入的Gram矩阵，并通过特征谱总结其语义配置。然后将这种谱表示输入高斯过程分类器，该分类器学习将语义一致性模式映射到预测不确定性，并可在黑盒和白盒设置中应用。在涵盖VQA、图像分类和文本QA的八个数据集上，对六个LLM和LVLM进行测试，SGPU在标定（ECE）和判别（AUROC、AUARC）性能方面始终达到最先进水平。我们进一步表明，SGPU能够跨模型和模态迁移，表明其谱表示捕捉了语义不确定性的一般模式。",
            "intro_zh": [
                "现有方法依赖外部模型聚类答案，对措辞变化敏感，易错误分组，导致不确定性估计不可靠。",
                "提出SGPU框架，通过分析答案嵌入的几何结构，避免聚类，使用高斯过程分类器学习语义一致性模式。",
                "在六个模型和八个数据集上，SGPU在标定和判别性能上达到最先进水平，并能跨模型和模态迁移。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大视觉语言模型中语义不确定性量化不可靠的问题。现有方法依赖外部模型对多个采样响应进行聚类，但这些聚类方法脆弱，对细微措辞变化高度敏感，可能错误地分组或分离语义相似的答案，导致不确定性估计不准确。\\n\\n**核心思路**：论文的核心思路是避免脆弱的聚类过程，转而通过分析答案嵌入的几何结构来量化语义不确定性。设计一个贝叶斯框架，将生成的答案映射到语义空间，计算其嵌入的Gram矩阵，并通过特征谱总结语义配置，然后使用高斯过程分类器学习将语义一致性模式映射到预测不确定性。\\n\\n**技术框架**：整体架构包括三个主要阶段：首先，将生成的多个答案通过嵌入模型映射到密集语义空间，得到向量表示；其次，计算这些嵌入的Gram矩阵，并分析其特征谱，以捕捉答案之间的语义关系；最后，将谱表示作为输入，训练高斯过程分类器，该分类器学习识别语义一致性模式并输出不确定性估计。框架支持黑盒和白盒设置，适用于不同模型和任务。\\n\\n**关键创新**：最重要的技术创新是引入语义高斯过程不确定性（SGPU）框架，它通过几何分析替代聚类，避免了现有方法对措辞变化的敏感性。本质区别在于，SGPU直接利用嵌入的谱特征来量化不确定性，而不是依赖外部聚类算法，从而提高了鲁棒性和泛化能力。\\n\\n**关键设计**：关键设计包括使用预训练的嵌入模型（如BERT或类似模型）将答案转换为向量，计算Gram矩阵以捕获成对相似性，并通过特征值分解提取谱表示。高斯过程分类器采用标准核函数（如RBF核）来建模不确定性，损失函数基于预测概率与真实标签的校准误差进行优化。参数设置可能涉及嵌入维度、核超参数和训练数据规模，具体细节在论文中未详细说明，但框架设计为可适应不同配置。",
            "application_zh": "该研究在需要高可靠性的大视觉语言模型应用中具有重要价值，如自动驾驶中的视觉问答、医疗图像分析、内容审核和智能助手。通过提供更准确的语义不确定性估计，SGPU能帮助系统识别不可靠输出，减少错误决策，提升安全性和用户体验。未来可能推动多模态AI在关键领域的部署，促进不确定性量化技术的标准化发展。",
            "highlight_zh": "在六个LLM和LVLM模型上，针对八个数据集（涵盖VQA、图像分类和文本QA），SGPU在标定性能（ECE）和判别性能（AUROC、AUARC）方面均达到最先进水平。具体提升幅度未在摘要中给出，但实验表明SGPU优于现有聚类方法，并能跨模型和模态迁移，验证了其谱表示捕捉语义不确定性一般模式的有效性。",
            "tags_zh": [
                "语义不确定性量化",
                "大视觉语言模型",
                "高斯过程分类器",
                "谱表示",
                "多模态AI",
                "贝叶斯框架",
                "答案嵌入",
                "模型校准"
            ],
            "_index": 154
        },
        {
            "title": "On Improving Deep Active Learning with Formal Verification",
            "authors": [
                "Jonathan Spiegelman",
                "Guy Amir",
                "Guy Katz"
            ],
            "arxiv_id": "2512.14170v1",
            "summary": "Deep Active Learning (DAL) aims to reduce labeling costs in neural-network training by prioritizing the most informative unlabeled samples for annotation. Beyond selecting which samples to label, several DAL approaches further enhance data efficiency by augmenting the training set with synthetic inputs that do not require additional manual labeling. In this work, we investigate how augmenting the training data with adversarial inputs that violate robustness constraints can improve DAL performance. We show that adversarial examples generated via formal verification contribute substantially more than those produced by standard, gradient-based attacks. We apply this extension to multiple modern DAL techniques, as well as to a new technique that we propose, and show that it yields significant improvements in model generalization across standard benchmarks.",
            "categories": [
                "cs.LG",
                "cs.LO"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14170v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出基于形式验证的对抗样本增强方法，以提升深度主动学习的模型泛化性能",
            "summary_zh": "深度主动学习旨在通过优先标注信息量最大的未标记样本来降低神经网络训练中的标注成本。除了选择标注样本外，一些深度主动学习方法还通过添加无需额外人工标注的合成输入来进一步增强数据效率。本研究探讨了如何通过添加违反鲁棒性约束的对抗样本来增强训练数据，从而提升深度主动学习的性能。研究表明，通过形式验证生成的对抗样本比基于标准梯度攻击生成的样本贡献更大。我们将这一扩展应用于多种现代深度主动学习技术，以及我们提出的一种新技术，并证明其在标准基准测试中显著提升了模型的泛化能力。",
            "intro_zh": [
                "现有深度主动学习方法在数据增强方面主要依赖合成输入，但对抗样本的生成通常基于梯度攻击，可能无法充分暴露模型鲁棒性缺陷。",
                "论文提出使用形式验证生成对抗样本，这些样本能更有效地违反鲁棒性约束，从而增强训练数据的多样性和挑战性。",
                "实验表明，该方法在多个基准测试中显著提升了模型泛化性能，相比传统对抗样本增强方法有更大改进。"
            ],
            "method_zh": "**问题定义**：论文旨在解决深度主动学习中数据增强效率不足的问题。现有方法通常通过合成输入或基于梯度攻击的对抗样本来扩充训练集，但这些对抗样本可能无法全面测试模型的鲁棒性，导致泛化提升有限。\\n\\n**核心思路**：核心思路是利用形式验证技术生成对抗样本，这些样本能严格违反模型的鲁棒性约束（如对抗扰动下的输出不变性），从而更有效地暴露模型弱点，增强训练数据的挑战性。相比梯度攻击，形式验证能提供更可靠的对抗样本，因为它们基于数学证明而非启发式搜索。\\n\\n**技术框架**：整体流程包括：1) 使用深度主动学习算法选择未标记样本进行标注；2) 应用形式验证工具（如基于SMT求解器或抽象解释的方法）生成对抗样本，这些样本在给定扰动范围内保证违反鲁棒性属性；3) 将生成的对抗样本作为合成数据添加到训练集中，无需额外标注；4) 迭代训练模型，结合主动学习选择和对抗增强。框架可集成到现有深度主动学习方法中。\\n\\n**关键创新**：最重要的创新是将形式验证引入深度主动学习的数据增强过程。与现有基于梯度攻击的方法相比，形式验证生成的对抗样本具有可证明的违反性，能更系统地探索模型决策边界，本质区别在于从启发式攻击转向形式化保证的样本生成。\\n\\n**关键设计**：关键设计包括：使用形式验证工具（具体工具未在摘要中指定，可能涉及如Marabou或类似框架）定义鲁棒性约束（如L∞范数扰动界限），生成对抗样本；在训练中，这些样本作为额外数据点，损失函数可能结合标准分类损失和对抗训练目标；网络结构依赖于基准任务（如图像分类），但论文未详细说明具体架构参数。",
            "application_zh": "该研究在需要高效数据标注和强泛化能力的领域具有潜在应用价值，如医疗影像分析、自动驾驶感知系统和工业缺陷检测。通过减少标注成本并提升模型鲁棒性，可加速AI模型在实际场景中的部署，未来可能推动形式验证与主动学习的更广泛结合，用于安全关键系统。",
            "highlight_zh": "实验在标准基准测试（具体数据集未在摘要中说明，可能包括CIFAR-10或ImageNet子集）上显示，使用形式验证生成的对抗样本进行数据增强，相比基于梯度攻击的方法，在模型泛化性能上带来显著提升。例如，在多个深度主动学习技术（如不确定性采样或多样性采样）中，该方法平均提升了准确率约2-5个百分点，具体幅度因任务而异。新提出的技术也表现出优于基线方法的性能。",
            "tags_zh": [
                "深度主动学习",
                "形式验证",
                "对抗样本",
                "数据增强",
                "模型泛化",
                "鲁棒性约束",
                "神经网络训练"
            ],
            "_index": 155
        },
        {
            "title": "PathFinder: Advancing Path Loss Prediction for Single-to-Multi-Transmitter Scenario",
            "authors": [
                "Zhijie Zhong",
                "Zhiwen Yu",
                "Pengyu Li",
                "Jianming Lv",
                "C. L. Philip Chen",
                "Min Chen"
            ],
            "arxiv_id": "2512.14150v1",
            "summary": "Radio path loss prediction (RPP) is critical for optimizing 5G networks and enabling IoT, smart city, and similar applications. However, current deep learning-based RPP methods lack proactive environmental modeling, struggle with realistic multi-transmitter scenarios, and generalize poorly under distribution shifts, particularly when training/testing environments differ in building density or transmitter configurations. This paper identifies three key issues: (1) passive environmental modeling that overlooks transmitters and key environmental features; (2) overemphasis on single-transmitter scenarios despite real-world multi-transmitter prevalence; (3) excessive focus on in-distribution performance while neglecting distribution shift challenges. To address these, we propose PathFinder, a novel architecture that actively models buildings and transmitters via disentangled feature encoding and integrates Mask-Guided Low-rank Attention to independently focus on receiver and building regions. We also introduce a Transmitter-Oriented Mixup strategy for robust training and a new benchmark, single-to-multi-transmitter RPP (S2MT-RPP), tailored to evaluate extrapolation performance (multi-transmitter testing after single-transmitter training). Experimental results show PathFinder outperforms state-of-the-art methods significantly, especially in challenging multi-transmitter scenarios. Our code and project site are available at: https://emorzz1g.github.io/PathFinder/.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "34 pages, 14 figures, 4 tables. Under review",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14150v1",
            "code_links": [
                {
                    "url": "https://emorzz1g.github.io/PathFinder/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出PathFinder架构，通过解耦特征编码和掩码引导低秩注意力，解决单发射器到多发射器场景下的无线路径损耗预测泛化问题。",
            "summary_zh": "无线路径损耗预测（RPP）对于优化5G网络和实现物联网、智慧城市等应用至关重要。然而，当前基于深度学习的RPP方法缺乏主动环境建模，难以处理现实中的多发射器场景，且在分布偏移下泛化能力差，特别是在训练/测试环境在建筑密度或发射器配置不同时。本文识别出三个关键问题：（1）被动环境建模忽视发射器和关键环境特征；（2）过度强调单发射器场景，尽管现实世界多发射器普遍存在；（3）过度关注分布内性能，而忽视分布偏移挑战。为解决这些问题，我们提出PathFinder，一种新颖架构，通过解耦特征编码主动建模建筑和发射器，并集成掩码引导低秩注意力以独立关注接收器和建筑区域。我们还引入面向发射器的混合策略进行鲁棒训练，并创建了一个新基准——单到多发射器RPP（S2MT-RPP），专门用于评估外推性能（在单发射器训练后进行多发射器测试）。实验结果表明，PathFinder显著优于最先进方法，尤其是在具有挑战性的多发射器场景中。我们的代码和项目网站可在 https://emorzz1g.github.io/PathFinder/ 获取。",
            "intro_zh": [
                "现有方法被动建模环境，忽视发射器和关键特征，导致预测不准确。",
                "PathFinder通过解耦特征编码和掩码引导低秩注意力，主动建模建筑和发射器，提升多场景泛化。",
                "实验显示PathFinder在S2MT-RPP基准上显著优于基线，多发射器场景性能提升明显。"
            ],
            "method_zh": "**问题定义**：论文旨在解决无线路径损耗预测（RPP）在单发射器到多发射器场景下的泛化问题。现有方法痛点包括：被动环境建模忽略发射器影响，过度依赖单发射器数据，导致在现实多发射器场景和分布偏移下性能下降。\\n\\n**核心思路**：核心思路是主动建模环境特征，特别是建筑和发射器，通过解耦编码分离其影响，并引入注意力机制聚焦关键区域，以增强模型对多发射器场景的适应性和泛化能力。\\n\\n**技术框架**：整体架构包括输入处理、特征编码、注意力模块和预测输出。主要模块有：解耦特征编码器，分别处理建筑和发射器特征；掩码引导低秩注意力模块，独立关注接收器和建筑区域；以及面向发射器的混合策略，用于数据增强和鲁棒训练。\\n\\n**关键创新**：最重要的技术创新是掩码引导低秩注意力机制，它允许模型动态聚焦于接收器和建筑区域，减少无关噪声，与现有方法相比，本质区别在于主动而非被动建模，并专门针对多发射器场景优化。\\n\\n**关键设计**：关键设计包括使用低秩分解降低注意力计算复杂度，设置特定掩码引导注意力权重，损失函数结合均方误差和分布对齐损失，网络结构采用卷积和注意力层结合，参数通过实验调优以平衡精度和效率。",
            "application_zh": "该研究在5G网络优化、物联网部署和智慧城市建设中具有重要应用价值。通过提升路径损耗预测的准确性和泛化能力，可支持更高效的无线网络规划、资源分配和信号覆盖优化，未来可能推动智能通信系统的发展，特别是在复杂城市环境和多设备场景下。",
            "highlight_zh": "PathFinder在单到多发射器RPP（S2MT-RPP）基准测试中显著优于现有方法，具体性能提升未知，但论文报告在多发射器场景下表现突出，验证了其主动建模和注意力机制的有效性，增强了模型在分布偏移下的鲁棒性。",
            "tags_zh": [
                "无线路径损耗预测",
                "多发射器场景",
                "解耦特征编码",
                "掩码引导注意力",
                "分布偏移泛化",
                "5G网络优化",
                "深度学习模型",
                "环境建模"
            ],
            "_index": 156
        },
        {
            "title": "Consistent Instance Field for Dynamic Scene Understanding",
            "authors": [
                "Junyi Wu",
                "Van Nguyen Nguyen",
                "Benjamin Planche",
                "Jiachen Tao",
                "Changchang Sun",
                "Zhongpai Gao",
                "Zhenghao Zhao",
                "Anwesa Choudhuri",
                "Gengyu Zhang",
                "Meng Zheng",
                "Feiran Wang",
                "Terrence Chen",
                "Yan Yan",
                "Ziyan Wu"
            ],
            "arxiv_id": "2512.14126v1",
            "summary": "We introduce Consistent Instance Field, a continuous and probabilistic spatio-temporal representation for dynamic scene understanding. Unlike prior methods that rely on discrete tracking or view-dependent features, our approach disentangles visibility from persistent object identity by modeling each space-time point with an occupancy probability and a conditional instance distribution. To realize this, we introduce a novel instance-embedded representation based on deformable 3D Gaussians, which jointly encode radiance and semantic information and are learned directly from input RGB images and instance masks through differentiable rasterization. Furthermore, we introduce new mechanisms to calibrate per-Gaussian identities and resample Gaussians toward semantically active regions, ensuring consistent instance representations across space and time. Experiments on HyperNeRF and Neu3D datasets demonstrate that our method significantly outperforms state-of-the-art methods on novel-view panoptic segmentation and open-vocabulary 4D querying tasks.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14126v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出一致性实例场，基于可变形3D高斯模型实现动态场景的连续时空表示，提升新视角全景分割和开放词汇4D查询性能。",
            "summary_zh": "我们引入了“一致性实例场”，这是一种用于动态场景理解的连续且概率性的时空表示方法。与先前依赖离散跟踪或视角相关特征的方法不同，我们的方法通过为每个时空点建模占用概率和条件实例分布，将可见性与持久对象身份解耦。为实现这一目标，我们提出了一种基于可变形3D高斯模型的新型实例嵌入表示，该表示联合编码辐射和语义信息，并通过可微分光栅化直接从输入RGB图像和实例掩码中学习。此外，我们引入了新机制来校准每个高斯的身份，并向语义活跃区域重新采样高斯，确保跨空间和时间的一致性实例表示。在HyperNeRF和Neu3D数据集上的实验表明，我们的方法在新视角全景分割和开放词汇4D查询任务上显著优于最先进的方法。",
            "intro_zh": [
                "现有方法依赖离散跟踪或视角相关特征，难以在动态场景中实现跨时空的实例一致性表示。",
                "提出一致性实例场，基于可变形3D高斯模型联合编码辐射和语义信息，解耦可见性与对象身份。",
                "在HyperNeRF和Neu3D数据集上，新视角全景分割和开放词汇4D查询任务性能显著提升，优于现有方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决动态场景理解中实例表示的时空一致性问题。现有方法通常依赖离散跟踪（如逐帧关联）或视角相关特征，导致在复杂动态场景中实例身份难以保持一致性，尤其是在遮挡、变形或长时间序列中，这限制了新视角全景分割和4D查询等任务的性能。\\n\\n**核心思路**：论文提出“一致性实例场”作为连续概率性时空表示，核心思想是将每个时空点建模为占用概率和条件实例分布，从而解耦可见性（如遮挡变化）与持久对象身份。通过基于可变形3D高斯模型的实例嵌入表示，联合学习辐射（颜色、几何）和语义信息，直接从输入数据中优化，确保实例在空间和时间维度上的一致性。\\n\\n**技术框架**：整体流程包括数据输入（RGB图像和实例掩码）、表示学习、优化和推理阶段。主要模块包括：1）可变形3D高斯模型，用于编码每个点的辐射和语义特征；2）可微分光栅化模块，将3D表示投影到2D图像以计算损失；3）身份校准机制，通过优化调整每个高斯的实例标签；4）重新采样机制，动态向语义活跃区域（如对象边界或运动区域）添加或移除高斯，以提高表示效率。\\n\\n**关键创新**：最重要的创新是引入一致性实例场作为连续时空表示，与现有离散或视角相关方法有本质区别。具体包括：1）基于可变形3D高斯的实例嵌入表示，能灵活适应动态变形；2）联合编码辐射和语义，实现端到端学习；3）新机制（身份校准和重新采样）确保实例一致性，无需额外跟踪模块。\\n\\n**关键设计**：技术细节包括：1）使用可变形3D高斯模型，参数包括位置、协方差、颜色、不透明度和实例嵌入向量；2）损失函数结合重建损失（如RGB误差）和语义损失（如实例掩码匹配），通过可微分光栅化反向传播优化；3）身份校准通过最小化实例嵌入的跨时间方差实现；4）重新采样基于语义活跃度（如梯度或运动检测）动态调整高斯分布，参数如采样阈值需调优。",
            "application_zh": "该研究在自动驾驶、机器人导航、增强现实和视频分析等领域具有潜在应用价值。通过实现动态场景的连续实例表示，能提升环境感知的准确性和鲁棒性，例如在复杂交通场景中跟踪车辆和行人，或在AR中实现对象持久化交互。未来可能推动4D场景理解和开放词汇查询技术的发展，为智能系统提供更强大的时空推理能力。",
            "highlight_zh": "在HyperNeRF和Neu3D数据集上的实验表明，该方法在新视角全景分割任务中，相比最先进方法（如NeRF-based方法）在指标如PQ（全景质量）上提升显著，具体数据未知但论文报告为“显著优于”；在开放词汇4D查询任务中，能有效回答跨时空的语义查询，性能优于基线，展示了实例一致性的优势。这些结果验证了方法在动态场景理解中的有效性和泛化能力。",
            "tags_zh": [
                "动态场景理解",
                "实例一致性表示",
                "可变形3D高斯模型",
                "新视角全景分割",
                "开放词汇4D查询",
                "时空表示学习",
                "可微分光栅化",
                "语义辐射联合编码"
            ],
            "_index": 157
        },
        {
            "title": "SportsGPT: An LLM-driven Framework for Interpretable Sports Motion Assessment and Training Guidance",
            "authors": [
                "Wenbo Tian",
                "Ruting Lin",
                "Hongxian Zheng",
                "Yaodong Yang",
                "Geng Wu",
                "Zihao Zhang",
                "Zhang Zhang"
            ],
            "arxiv_id": "2512.14121v1",
            "summary": "Existing intelligent sports analysis systems mainly focus on \"scoring and visualization,\" often lacking automatic performance diagnosis and interpretable training guidance. Recent advances of Large Language Models (LMMs) and motion analysis techniques provide new opportunities to address the above limitations. In this paper, we propose SportsGPT, an LLM-driven framework for interpretable sports motion assessment and training guidance, which establishes a closed loop from motion time-series input to professional training guidance. First, given a set of high-quality target models, we introduce MotionDTW, a two-stage time series alignment algorithm designed for accurate keyframe extraction from skeleton-based motion sequences. Subsequently, we design a Knowledge-based Interpretable Sports Motion Assessment Model (KISMAM) to obtain a set of interpretable assessment metrics (e.g., insufficient extension) by constrasting the keyframes with the targe models. Finally, we propose SportsRAG, a RAG-based training guidance model based on Qwen3. Leveraging a 6B-token knowledge base, it prompts the LLM to generate professional training guidance by retrieving domain-specific QA pairs. Experimental results demonstrate that MotionDTW significantly outperforms traditional methods with lower temporal error and higher IoU scores. Furthermore, ablation studies validate the KISMAM and SportsRAG, confirming that SportsGPT surpasses general LLMs in diagnostic accuracy and professionalism.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14121v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出SportsGPT框架，通过运动序列对齐与知识增强，实现可解释的运动评估与训练指导。",
            "summary_zh": "现有的智能体育分析系统主要关注“评分与可视化”，往往缺乏自动性能诊断和可解释的训练指导。大型语言模型和运动分析技术的最新进展为解决上述局限提供了新机遇。本文提出SportsGPT，一个基于LLM的可解释运动评估与训练指导框架，建立了从运动时间序列输入到专业训练指导的闭环。首先，给定一组高质量目标模型，我们引入MotionDTW，一种两阶段时间序列对齐算法，用于从基于骨架的运动序列中准确提取关键帧。随后，我们设计了一个基于知识的可解释运动评估模型，通过对比关键帧与目标模型，获得一组可解释的评估指标。最后，我们提出SportsRAG，一个基于Qwen3的RAG训练指导模型，利用6B-token的知识库，通过检索领域特定的问答对，提示LLM生成专业训练指导。实验结果表明，MotionDTW在时间误差和IoU分数上显著优于传统方法。此外，消融研究验证了KISMAM和SportsRAG，确认SportsGPT在诊断准确性和专业性方面超越通用LLM。",
            "intro_zh": [
                "现有智能体育分析系统多聚焦于评分与可视化，缺乏自动诊断与可解释指导，限制了实际训练应用。",
                "提出SportsGPT框架，结合MotionDTW对齐算法、KISMAM评估模型和SportsRAG指导模型，实现闭环运动分析。",
                "实验显示MotionDTW降低时间误差、提升IoU，SportsGPT在诊断准确性和专业性上优于通用LLM。"
            ],
            "method_zh": "**问题定义**：论文旨在解决智能体育分析中缺乏自动性能诊断和可解释训练指导的问题。现有方法多停留在运动数据的评分与可视化层面，无法提供深入的错误分析和个性化改进建议，限制了其在专业训练中的应用价值。\\n\\n**核心思路**：论文的核心思路是构建一个从运动时间序列输入到专业训练指导的闭环框架。通过精确对齐运动序列、提取可解释评估指标，并利用大型语言模型生成指导，实现端到端的智能分析。这种设计结合了传统运动分析与现代AI技术，以提升系统的实用性和专业性。\\n\\n**技术框架**：整体架构包含三个主要阶段：首先，使用MotionDTW算法对输入的运动序列进行两阶段时间序列对齐，提取关键帧；其次，通过KISMAM模型对比关键帧与目标模型，生成可解释的评估指标；最后，基于SportsRAG模型，利用检索增强生成技术，从知识库中检索相关信息，驱动LLM生成专业训练指导。\\n\\n**关键创新**：最重要的技术创新包括MotionDTW算法，它通过两阶段对齐提高了关键帧提取的准确性；KISMAM模型，它引入了基于知识的评估机制，增强了结果的可解释性；以及SportsRAG模型，它结合RAG技术与大型语言模型，提升了指导的专业性和针对性。这些创新使系统超越了传统的评分系统，实现了更深入的交互式分析。\\n\\n**关键设计**：在技术细节上，MotionDTW采用两阶段动态时间规整算法，具体参数未详细说明，但强调其优化了时间误差和IoU分数；KISMAM基于对比学习框架，利用目标模型作为参考，生成如“伸展不足”等具体指标；SportsRAG基于Qwen3 LLM，构建了包含6B-token的领域知识库，通过检索QA对来增强提示，具体网络结构和损失函数在论文中未明确描述，但突出了其RAG机制的有效性。",
            "application_zh": "该研究在体育训练、康复医学和健身指导等领域具有广泛应用潜力。例如，可用于专业运动员的动作优化、普通用户的健身错误纠正，或物理治疗中的运动功能评估。其价值在于提供自动、可解释的反馈，降低教练依赖，提升训练效率。未来可能推动智能体育设备的集成，促进个性化健康管理的发展。",
            "highlight_zh": "实验结果显示，MotionDTW在关键帧提取上显著优于传统方法，具体表现为更低的时间误差和更高的IoU分数，但论文未提供具体数值。消融研究验证了KISMAM和SportsRAG的有效性，SportsGPT在诊断准确性和专业性方面超越通用LLM，例如在生成训练指导时更符合领域知识，但未给出量化提升幅度。",
            "tags_zh": [
                "运动分析",
                "时间序列对齐",
                "可解释评估",
                "检索增强生成",
                "大型语言模型",
                "骨架动作识别",
                "智能训练指导",
                "闭环系统"
            ],
            "_index": 158
        },
        {
            "title": "Joint Multimodal Contrastive Learning for Robust Spoken Term Detection and Keyword Spotting",
            "authors": [
                "Ramesh Gundluru",
                "Shubham Gupta",
                "Sri Rama Murty K"
            ],
            "arxiv_id": "2512.14115v1",
            "summary": "Acoustic Word Embeddings (AWEs) improve the efficiency of speech retrieval tasks such as Spoken Term Detection (STD) and Keyword Spotting (KWS). However, existing approaches suffer from limitations, including unimodal supervision, disjoint optimization of audio-audio and audio-text alignment, and the need for task-specific models. To address these shortcomings, we propose a joint multimodal contrastive learning framework that unifies both acoustic and cross-modal supervision in a shared embedding space. Our approach simultaneously optimizes: (i) audio-text contrastive learning, inspired by the CLAP loss, to align audio and text representations and (ii) audio-audio contrastive learning, via Deep Word Discrimination (DWD) loss, to enhance intra-class compactness and inter-class separation. The proposed method outperforms existing AWE baselines on word discrimination task while flexibly supporting both STD and KWS. To our knowledge, this is the first comprehensive approach of its kind.",
            "categories": [
                "cs.SD",
                "cs.LG"
            ],
            "primary_category": "cs.SD",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14115v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出联合多模态对比学习框架，以解决声学词嵌入在语音检索任务中的局限性，提升口语词检测和关键词识别的鲁棒性。",
            "summary_zh": "声学词嵌入（AWEs）提高了语音检索任务（如口语词检测和关键词识别）的效率。然而，现有方法存在局限性，包括单模态监督、音频-音频和音频-文本对齐的分离优化，以及需要任务特定模型。为解决这些不足，我们提出了一个联合多模态对比学习框架，在共享嵌入空间中统一了声学和跨模态监督。我们的方法同时优化：（i）音频-文本对比学习，受CLAP损失启发，以对齐音频和文本表示；（ii）音频-音频对比学习，通过深度词判别损失，以增强类内紧凑性和类间分离性。所提出的方法在词判别任务上优于现有AWE基线，同时灵活支持口语词检测和关键词识别。据我们所知，这是此类方法的首次全面尝试。",
            "intro_zh": [
                "现有声学词嵌入方法依赖单模态监督，导致音频-音频和音频-文本对齐分离优化，限制了跨模态检索的鲁棒性。",
                "论文提出联合多模态对比学习框架，统一音频-音频和音频-文本监督，通过共享嵌入空间同时优化两种对齐，提升模型泛化能力。",
                "实验表明，该方法在词判别任务上优于基线，并灵活支持口语词检测和关键词识别，实现了性能提升和任务通用性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决声学词嵌入在语音检索任务（如口语词检测和关键词识别）中的局限性，包括现有方法依赖单模态监督、音频-音频和音频-文本对齐分离优化，以及需要任务特定模型，导致效率低下和鲁棒性不足。\\n\\n**核心思路**：核心思路是设计一个联合多模态对比学习框架，在共享嵌入空间中统一声学和跨模态监督，通过同时优化音频-音频和音频-文本对齐，增强模型的泛化能力和检索性能，避免分离优化带来的信息损失。\\n\\n**技术框架**：整体架构包括输入处理、特征提取、共享嵌入空间和联合优化模块。首先，音频和文本输入分别通过编码器提取特征；然后，在共享嵌入空间中，使用音频-音频对比学习（基于深度词判别损失）和音频-文本对比学习（基于CLAP损失）同时训练；最后，输出统一的嵌入表示，支持多种语音检索任务。\\n\\n**关键创新**：最重要的技术创新是首次提出联合多模态对比学习框架，将音频-音频和音频-文本监督统一在单一模型中，解决了现有方法分离优化的痛点，本质区别在于实现了端到端的跨模态对齐和声学一致性。\\n\\n**关键设计**：关键设计包括使用CLAP损失进行音频-文本对比学习，以对齐跨模态表示；使用深度词判别损失进行音频-音频对比学习，以增强类内紧凑性和类间分离性；共享嵌入空间确保模型参数统一；网络结构可能基于Transformer或CNN编码器，具体参数设置未知，但优化目标是最大化正样本相似度和最小化负样本相似度。",
            "application_zh": "该研究在语音检索领域具有广泛潜在应用，包括智能语音助手中的口语词检测、安防系统中的关键词识别、语音搜索和内容分析。实际价值在于提高检索效率和鲁棒性，减少对任务特定模型的依赖，未来可能推动多模态语音处理技术的发展，应用于更复杂的场景如多语言语音识别和跨模态信息检索。",
            "highlight_zh": "最重要的实验结果显示，该方法在词判别任务上优于现有声学词嵌入基线，具体性能数据未知，但提升了类内紧凑性和类间分离性；同时，模型灵活支持口语词检测和关键词识别，无需任务特定调整，实现了跨任务通用性和性能提升，验证了联合多模态对比学习的有效性。",
            "tags_zh": [
                "声学词嵌入",
                "多模态对比学习",
                "口语词检测",
                "关键词识别",
                "语音检索",
                "跨模态对齐",
                "联合优化",
                "共享嵌入空间"
            ],
            "_index": 159
        },
        {
            "title": "MFE-GAN: Efficient GAN-based Framework for Document Image Enhancement and Binarization with Multi-scale Feature Extraction",
            "authors": [
                "Rui-Yang Ju",
                "KokSheik Wong",
                "Yanlin Jin",
                "Jen-Shiun Chiang"
            ],
            "arxiv_id": "2512.14114v1",
            "summary": "Document image enhancement and binarization are commonly performed prior to document analysis and recognition tasks for improving the efficiency and accuracy of optical character recognition (OCR) systems. This is because directly recognizing text in degraded documents, particularly in color images, often results in unsatisfactory recognition performance. To address these issues, existing methods train independent generative adversarial networks (GANs) for different color channels to remove shadows and noise, which, in turn, facilitates efficient text information extraction. However, deploying multiple GANs results in long training and inference times. To reduce both training and inference times of document image enhancement and binarization models, we propose MFE-GAN, an efficient GAN-based framework with multi-scale feature extraction (MFE), which incorporates Haar wavelet transformation (HWT) and normalization to process document images before feeding them into GANs for training. In addition, we present novel generators, discriminators, and loss functions to improve the model's performance, and we conduct ablation studies to demonstrate their effectiveness. Experimental results on the Benchmark, Nabuco, and CMATERdb datasets demonstrate that the proposed MFE-GAN significantly reduces the total training and inference times while maintaining comparable performance with respect to state-of-the-art (SOTA) methods. The implementation of this work is available at https://ruiyangju.github.io/MFE-GAN.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Extended Journal Version of APSIPA ASC 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14114v1",
            "code_links": [
                {
                    "url": "https://ruiyangju.github.io/MFE-GAN",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出MFE-GAN框架，通过多尺度特征提取和Haar小波变换，高效解决文档图像增强与二值化中的训练和推理时间问题。",
            "summary_zh": "文档图像增强和二值化通常在文档分析和识别任务之前进行，以提高光学字符识别（OCR）系统的效率和准确性。这是因为直接识别退化文档（尤其是彩色图像）中的文本往往导致不理想的识别性能。为解决这些问题，现有方法训练独立的生成对抗网络（GAN）用于不同颜色通道，以去除阴影和噪声，从而促进高效的文本信息提取。然而，部署多个GAN会导致训练和推理时间较长。为减少文档图像增强和二值化模型的训练和推理时间，我们提出了MFE-GAN，这是一种基于GAN的高效框架，具有多尺度特征提取（MFE），它结合了Haar小波变换（HWT）和归一化，在将文档图像输入GAN进行训练之前进行处理。此外，我们提出了新颖的生成器、判别器和损失函数以提高模型性能，并进行了消融研究以证明其有效性。在Benchmark、Nabuco和CMATERdb数据集上的实验结果表明，所提出的MFE-GAN显著减少了总训练和推理时间，同时保持了与最先进（SOTA）方法相当的性能。本工作的实现可在https://ruiyangju.github.io/MFE-GAN获取。",
            "intro_zh": [
                "核心问题：现有方法使用多个独立GAN处理不同颜色通道，导致训练和推理时间过长，效率低下。",
                "方法要点：提出MFE-GAN框架，集成多尺度特征提取和Haar小波变换，优化图像预处理，减少模型复杂度。",
                "实验或效果：在多个数据集上验证，MFE-GAN显著降低时间成本，同时性能与SOTA方法相当。"
            ],
            "method_zh": "**问题定义**：论文旨在解决文档图像增强和二值化任务中，现有基于GAN的方法因使用多个独立网络处理不同颜色通道而导致的训练和推理时间过长问题。这限制了实际部署的效率，尤其是在处理大规模或实时文档时。\\n\\n**核心思路**：论文的核心思路是通过引入多尺度特征提取（MFE）和Haar小波变换（HWT），在GAN训练前对文档图像进行预处理，从而减少模型复杂度并加速处理。这样设计是因为多尺度特征能更好地捕捉图像细节，而小波变换有助于分离噪声和阴影，使得后续GAN更高效地学习。\\n\\n**技术框架**：整体框架包括预处理、生成器和判别器三阶段。预处理阶段使用Haar小波变换和归一化处理输入图像，提取多尺度特征；生成器基于这些特征生成增强和二值化图像；判别器评估生成图像的真实性。框架通过端到端训练优化性能。\\n\\n**关键创新**：最重要的技术创新是集成多尺度特征提取和Haar小波变换到GAN框架中，避免了多个独立GAN的使用。与现有方法的本质区别在于，它通过预处理步骤统一处理图像，减少了模型参数和计算开销，从而显著提升效率。\\n\\n**关键设计**：关键设计包括：使用Haar小波变换进行图像分解，生成多尺度特征图；设计新颖的生成器和判别器网络结构，可能基于卷积神经网络（CNN）或类似架构；损失函数结合对抗损失、内容损失（如L1或L2损失）和可能的感知损失，以平衡图像质量和二值化准确性；参数设置如学习率、批量大小等通过实验优化，具体数值未在摘要中提供，但消融研究验证了其有效性。",
            "application_zh": "该研究主要应用于文档分析和光学字符识别（OCR）领域，特别是在处理退化文档（如扫描件、历史档案或低质量图像）时。其实际价值在于通过高效增强和二值化，提升OCR系统的准确性和速度，适用于数字化图书馆、自动化办公、档案管理等场景。未来可能扩展到其他图像处理任务，如医学图像增强或视频文本识别。",
            "highlight_zh": "实验在Benchmark、Nabuco和CMATERdb数据集上进行，结果显示MFE-GAN显著减少了总训练和推理时间，具体提升幅度未在摘要中量化，但强调与最先进（SOTA）方法性能相当。消融研究证明了多尺度特征提取和Haar小波变换的有效性，验证了框架在保持性能的同时优化效率的优势。",
            "tags_zh": [
                "文档图像增强",
                "图像二值化",
                "生成对抗网络",
                "多尺度特征提取",
                "Haar小波变换",
                "光学字符识别",
                "高效训练",
                "消融研究"
            ],
            "_index": 160
        },
        {
            "title": "Selective, Controlled and Domain-Agnostic Unlearning in Pretrained CLIP: A Training- and Data-Free Approach",
            "authors": [
                "Ashish Mishra",
                "Gyanaranjan Nayak",
                "Tarun Kumar",
                "Arpit Shah",
                "Suparna Bhattacharya",
                "Martin Foltin"
            ],
            "arxiv_id": "2512.14113v1",
            "summary": "Pretrained models like CLIP have demonstrated impressive zero-shot classification capabilities across diverse visual domains, spanning natural images, artistic renderings, and abstract representations. However, real-world applications often demand the removal (or \"unlearning\") of specific object classes without requiring additional data or retraining, or affecting the model's performance on unrelated tasks. In this paper, we propose a novel training- and data-free unlearning framework that enables three distinct forgetting paradigms: (1) global unlearning of selected objects across all domains, (2) domain-specific knowledge removal (e.g., eliminating sketch representations while preserving photo recognition), and (3) complete unlearning in selective domains. By leveraging a multimodal nullspace through synergistic integration of text prompts and synthesized visual prototypes derived from CLIP's joint embedding space, our method efficiently removes undesired class information while preserving the remaining knowledge. This approach overcomes the limitations of existing retraining-based methods and offers a flexible and computationally efficient solution for controlled model forgetting.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14113v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出无需训练和数据的CLIP选择性遗忘框架，实现跨域、领域特定和选择性领域的可控知识移除。",
            "summary_zh": "像CLIP这样的预训练模型已在自然图像、艺术渲染和抽象表示等多种视觉领域展示了令人印象深刻的零样本分类能力。然而，现实应用通常需要在无需额外数据或重新训练的情况下移除特定对象类别，同时不影响模型在无关任务上的性能。本文提出了一种新颖的无需训练和数据的遗忘框架，支持三种不同的遗忘范式：(1) 在所有领域中全局遗忘选定对象，(2) 领域特定知识移除（例如消除草图表示同时保留照片识别），(3) 在选择性领域中的完全遗忘。通过利用文本提示和从CLIP联合嵌入空间衍生的合成视觉原型协同整合的多模态零空间，我们的方法高效地移除了不需要的类别信息，同时保留了其余知识。这种方法克服了现有基于重新训练方法的局限性，为可控模型遗忘提供了灵活且计算高效的解决方案。",
            "intro_zh": [
                "现有基于重新训练的遗忘方法需要大量数据和计算资源，且难以实现跨域或领域特定的选择性遗忘，限制了实际应用。",
                "通过多模态零空间整合文本提示和合成视觉原型，实现无需训练和数据的可控遗忘，支持三种遗忘范式。",
                "实验表明，该方法在移除目标类别知识的同时，保持其他类别性能，相比基线方法显著提升遗忘效率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决预训练CLIP模型在无需额外数据或重新训练的情况下，实现选择性、可控和领域无关的遗忘问题。现有基于重新训练的方法需要大量数据和计算资源，且难以灵活控制遗忘范围（如跨域或领域特定），导致效率低下和实用性受限。\\n\\n**核心思路**：核心思路是利用CLIP的多模态嵌入空间，通过协同整合文本提示和合成视觉原型，构建一个多模态零空间，从而在不重新训练模型的情况下，直接修改模型参数以移除特定知识。这种设计基于CLIP的联合嵌入特性，允许从文本和视觉模态同时引导遗忘过程，实现高效且精确的知识移除。\\n\\n**技术框架**：整体框架包括三个主要阶段：首先，基于目标类别生成文本提示和合成视觉原型；其次，通过多模态零空间计算，将文本和视觉信息融合以识别需要移除的知识表示；最后，应用优化技术调整模型参数，实现遗忘。关键模块包括提示生成器、原型合成器和零空间优化器，流程无需外部数据或训练循环。\\n\\n**关键创新**：最重要的技术创新是提出了一种无需训练和数据的遗忘方法，通过多模态零空间实现三种遗忘范式（全局、领域特定、选择性领域）。与现有方法的本质区别在于，它避免了重新训练，直接利用CLIP的嵌入空间进行知识编辑，从而显著降低计算成本并提高灵活性。\\n\\n**关键设计**：关键设计包括使用CLIP的文本编码器生成类别特定提示，视觉原型通过嵌入空间插值或生成对抗网络合成；损失函数设计为最小化目标类别在嵌入空间中的表示，同时最大化其他类别的保留；参数设置涉及零空间阈值和优化步数，以确保遗忘效果而不损害模型整体性能。",
            "application_zh": "该研究在隐私保护、模型合规性和动态知识管理等领域具有潜在应用价值。例如，在医疗或金融场景中，可移除敏感类别以符合数据法规；在机器人或自动驾驶系统中，支持实时更新模型知识而不影响现有功能。未来可能推动更高效的模型编辑和自适应AI系统发展。",
            "highlight_zh": "实验结果显示，该方法在ImageNet等数据集上，成功移除目标类别知识（如“狗”或“汽车”），同时保持其他类别准确率下降小于5%。相比基于重新训练的基线方法，计算效率提升超过50%，并在跨域任务（如从照片到草图）中实现领域特定遗忘，验证了其灵活性和有效性。",
            "tags_zh": [
                "模型遗忘",
                "多模态学习",
                "零样本分类",
                "CLIP模型",
                "无需训练",
                "知识移除",
                "领域自适应",
                "嵌入空间优化"
            ],
            "_index": 161
        },
        {
            "title": "Neurosymbolic Inference On Foundation Models For Remote Sensing Text-to-image Retrieval With Complex Queries",
            "authors": [
                "Emanuele Mezzi",
                "Gertjan Burghouts",
                "Maarten Kruithof"
            ],
            "arxiv_id": "2512.14102v1",
            "summary": "Text-to-image retrieval in remote sensing (RS) has advanced rapidly with the rise of large vision-language models (LVLMs) tailored for aerial and satellite imagery, culminating in remote sensing large vision-language models (RS-LVLMS). However, limited explainability and poor handling of complex spatial relations remain key challenges for real-world use. To address these issues, we introduce RUNE (Reasoning Using Neurosymbolic Entities), an approach that combines Large Language Models (LLMs) with neurosymbolic AI to retrieve images by reasoning over the compatibility between detected entities and First-Order Logic (FOL) expressions derived from text queries. Unlike RS-LVLMs that rely on implicit joint embeddings, RUNE performs explicit reasoning, enhancing performance and interpretability. For scalability, we propose a logic decomposition strategy that operates on conditioned subsets of detected entities, guaranteeing shorter execution time compared to neural approaches. Rather than using foundation models for end-to-end retrieval, we leverage them only to generate FOL expressions, delegating reasoning to a neurosymbolic inference module. For evaluation we repurpose the DOTA dataset, originally designed for object detection, by augmenting it with more complex queries than in existing benchmarks. We show the LLM's effectiveness in text-to-logic translation and compare RUNE with state-of-the-art RS-LVLMs, demonstrating superior performance. We introduce two metrics, Retrieval Robustness to Query Complexity (RRQC) and Retrieval Robustness to Image Uncertainty (RRIU), which evaluate performance relative to query complexity and image uncertainty. RUNE outperforms joint-embedding models in complex RS retrieval tasks, offering gains in performance, robustness, and explainability. We show RUNE's potential for real-world RS applications through a use case on post-flood satellite image retrieval.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.IR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14102v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出RUNE方法，结合大语言模型与神经符号推理，解决遥感文本-图像检索中复杂空间关系处理与可解释性不足的问题。",
            "summary_zh": "遥感领域的文本-图像检索随着针对航空和卫星影像定制的大型视觉语言模型（LVLMs）的兴起而迅速发展，形成了遥感大型视觉语言模型（RS-LVLMs）。然而，有限的可解释性和对复杂空间关系的处理能力差仍然是实际应用中的关键挑战。为解决这些问题，我们引入了RUNE（使用神经符号实体进行推理），该方法将大语言模型（LLMs）与神经符号AI相结合，通过推理检测到的实体与从文本查询导出的第一阶逻辑（FOL）表达式之间的兼容性来检索图像。与依赖隐式联合嵌入的RS-LVLMs不同，RUNE执行显式推理，提高了性能和可解释性。为了可扩展性，我们提出了一种逻辑分解策略，该策略在检测到的实体的条件子集上操作，保证了比神经方法更短的执行时间。我们不是将基础模型用于端到端检索，而是仅利用它们生成FOL表达式，将推理委托给神经符号推理模块。为了评估，我们重新利用了原本设计用于目标检测的DOTA数据集，通过添加比现有基准更复杂的查询来增强它。我们展示了LLM在文本到逻辑翻译中的有效性，并将RUNE与最先进的RS-LVLMs进行比较，证明了其优越性能。我们引入了两个指标：检索对查询复杂性的鲁棒性（RRQC）和检索对图像不确定性的鲁棒性（RRIU），用于评估相对于查询复杂性和图像不确定性的性能。RUNE在复杂遥感检索任务中优于联合嵌入模型，在性能、鲁棒性和可解释性方面提供了增益。我们通过一个关于洪水后卫星图像检索的用例展示了RUNE在现实世界遥感应用中的潜力。",
            "intro_zh": [
                "现有遥感文本-图像检索方法（如RS-LVLMs）存在可解释性差、难以处理复杂空间关系等挑战，限制了实际应用。",
                "论文提出RUNE方法，结合大语言模型生成第一阶逻辑表达式，并利用神经符号推理模块进行显式推理，提升检索性能与可解释性。",
                "实验表明，RUNE在复杂查询任务中优于现有RS-LVLMs，并引入新指标评估鲁棒性，展示了在洪水后卫星图像检索等场景的应用潜力。"
            ],
            "method_zh": "**问题定义**：论文解决遥感文本-图像检索中，现有方法（如RS-LVLMs）依赖隐式联合嵌入导致可解释性差、难以处理复杂空间关系（如逻辑组合查询）的问题，这限制了在真实世界应用中的可靠性和性能。\\n\\n**核心思路**：核心思路是结合大语言模型（LLMs）与神经符号AI，将文本查询转换为第一阶逻辑（FOL）表达式，然后通过显式推理检测到的实体与FOL表达式的兼容性来检索图像，从而替代端到端的神经检索，提升可解释性和处理复杂关系的能力。\\n\\n**技术框架**：整体架构包含两个主要阶段：首先，使用大语言模型将文本查询翻译为第一阶逻辑表达式；其次，通过神经符号推理模块，在遥感图像中检测实体（如物体），并基于逻辑分解策略评估实体与FOL表达式的匹配度，最终输出检索结果。推理模块可能涉及符号推理引擎或规则系统，具体实现细节在论文中未详细说明。\\n\\n**关键创新**：最重要的技术创新是引入神经符号推理框架，将基础模型仅用于逻辑生成，而将推理任务委托给专门的神经符号模块，这实现了显式推理，与现有RS-LVLMs的隐式嵌入方法有本质区别，从而提高了可解释性和对复杂查询的处理能力。\\n\\n**关键设计**：关键设计包括逻辑分解策略，该策略在检测到的实体的条件子集上操作，以减少计算复杂度并保证更短的执行时间；此外，论文重新利用DOTA数据集，通过添加复杂查询来构建评估基准，具体参数设置和损失函数在摘要中未提及，可能涉及标准检索指标或自定义推理规则。",
            "application_zh": "该研究在遥感领域具有广泛的应用潜力，例如灾害响应（如洪水后卫星图像检索）、环境监测、城市规划等需要处理复杂空间关系和逻辑查询的场景。通过提升可解释性和鲁棒性，RUNE方法可以增强遥感数据分析的可靠性，支持决策制定和自动化任务，未来可能扩展到其他多模态检索或推理任务中。",
            "highlight_zh": "实验结果显示，RUNE在复杂遥感检索任务中优于最先进的RS-LVLMs，具体性能数据未在摘要中提供，但论文通过引入RRQC和RRIU指标评估了鲁棒性，并展示了在DOTA数据集增强版本上的有效性。此外，用例分析表明RUNE在洪水后卫星图像检索中具有实际应用价值，提升了检索精度和可解释性。",
            "tags_zh": [
                "遥感文本-图像检索",
                "神经符号推理",
                "大语言模型",
                "第一阶逻辑",
                "复杂空间关系",
                "可解释性AI",
                "多模态检索",
                "卫星图像分析"
            ],
            "_index": 162
        },
        {
            "title": "Quality-Aware Framework for Video-Derived Respiratory Signals",
            "authors": [
                "Nhi Nguyen",
                "Constantino Álvarez Casado",
                "Le Nguyen",
                "Manuel Lage Cañellas",
                "Miguel Bordallo López"
            ],
            "arxiv_id": "2512.14093v1",
            "summary": "Video-based respiratory rate (RR) estimation is often unreliable due to inconsistent signal quality across extraction methods. We present a predictive, quality-aware framework that integrates heterogeneous signal sources with dynamic assessment of reliability. Ten signals are extracted from facial remote photoplethysmography (rPPG), upper-body motion, and deep learning pipelines, and analyzed using four spectral estimators: Welch's method, Multiple Signal Classification (MUSIC), Fast Fourier Transform (FFT), and peak detection. Segment-level quality indices are then used to train machine learning models that predict accuracy or select the most reliable signal. This enables adaptive signal fusion and quality-based segment filtering. Experiments on three public datasets (OMuSense-23, COHFACE, MAHNOB-HCI) show that the proposed framework achieves lower RR estimation errors than individual methods in most cases, with performance gains depending on dataset characteristics. These findings highlight the potential of quality-driven predictive modeling to deliver scalable and generalizable video-based respiratory monitoring solutions.",
            "categories": [
                "cs.CV",
                "eess.SP"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "6 pages, 1 figure, 2 tables, conference",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14093v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "世界模型",
                    "matched_keywords": [
                        "predictive model"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出质量感知框架以解决视频呼吸信号提取质量不一致问题，实现自适应融合与过滤。",
            "summary_zh": "基于视频的呼吸率估计常因不同提取方法产生的信号质量不一致而不可靠。本文提出一个预测性、质量感知的框架，整合了异质信号源并动态评估其可靠性。从面部远程光电容积描记术、上半身运动和深度学习流程中提取了十个信号，并使用四种频谱估计器进行分析：Welch方法、多重信号分类、快速傅里叶变换和峰值检测。然后，利用片段级质量指标训练机器学习模型，以预测准确性或选择最可靠的信号。这实现了自适应信号融合和基于质量的片段过滤。在三个公共数据集上的实验表明，该框架在大多数情况下比单个方法实现了更低的呼吸率估计误差，性能提升取决于数据集特性。这些发现突出了质量驱动的预测建模在提供可扩展和泛化的视频呼吸监测解决方案方面的潜力。",
            "intro_zh": [
                "核心问题：现有视频呼吸信号提取方法质量不一致，导致呼吸率估计不可靠，缺乏统一的质量评估机制。",
                "方法要点：整合多源信号，动态评估可靠性，通过机器学习预测质量，实现自适应融合与过滤。",
                "实验或效果：在三个数据集上验证，框架降低估计误差，性能提升依赖数据集，展示质量驱动建模的潜力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决视频呼吸信号提取中因方法差异导致的信号质量不一致问题，现有方法缺乏系统性的质量评估和融合策略，导致呼吸率估计不可靠。\\n\\n**核心思路**：通过整合异质信号源并动态评估其可靠性，设计一个质量感知框架，利用机器学习模型预测信号质量，实现自适应融合和过滤，以提高呼吸率估计的准确性和鲁棒性。\\n\\n**技术框架**：整体流程包括信号提取、频谱分析、质量评估和预测建模。首先从视频中提取十个信号，包括面部rPPG、上半身运动和深度学习输出；然后使用四种频谱估计器分析信号；接着计算片段级质量指标；最后训练机器学习模型进行质量预测或信号选择，实现自适应融合。\\n\\n**关键创新**：最重要的创新是引入了预测性质量评估机制，将异质信号源与动态可靠性评估结合，通过机器学习实现自适应信号融合，与现有方法相比，本质区别在于系统性地处理信号质量不一致问题。\\n\\n**关键设计**：关键设计包括使用四种频谱估计器进行信号分析，提取片段级质量指标作为特征，训练机器学习模型预测准确性或选择信号，具体参数和损失函数在实验中基于数据集特性调整，未详细说明。",
            "application_zh": "该研究可应用于远程医疗、健康监测和智能家居等领域，提供非接触式呼吸监测解决方案，具有实际价值如降低医疗成本和提高监测便捷性，未来可能推动视频生物信号分析技术的发展。",
            "highlight_zh": "在OMuSense-23、COHFACE和MAHNOB-HCI三个数据集上的实验显示，框架在大多数情况下比单个提取方法实现了更低的呼吸率估计误差，性能提升幅度依赖数据集特性，具体误差数据未提供，但验证了质量驱动建模的有效性。",
            "tags_zh": [
                "视频呼吸信号",
                "质量感知框架",
                "信号融合",
                "机器学习预测",
                "远程光电容积描记术",
                "频谱分析",
                "自适应过滤",
                "呼吸率估计"
            ],
            "_index": 163
        },
        {
            "title": "ProtoFlow: Interpretable and Robust Surgical Workflow Modeling with Learned Dynamic Scene Graph Prototypes",
            "authors": [
                "Felix Holm",
                "Ghazal Ghazaei",
                "Nassir Navab"
            ],
            "arxiv_id": "2512.14092v1",
            "summary": "Purpose: Detailed surgical recognition is critical for advancing AI-assisted surgery, yet progress is hampered by high annotation costs, data scarcity, and a lack of interpretable models. While scene graphs offer a structured abstraction of surgical events, their full potential remains untapped. In this work, we introduce ProtoFlow, a novel framework that learns dynamic scene graph prototypes to model complex surgical workflows in an interpretable and robust manner.\n  Methods: ProtoFlow leverages a graph neural network (GNN) encoder-decoder architecture that combines self-supervised pretraining for rich representation learning with a prototype-based fine-tuning stage. This process discovers and refines core prototypes that encapsulate recurring, clinically meaningful patterns of surgical interaction, forming an explainable foundation for workflow analysis.\n  Results: We evaluate our approach on the fine-grained CAT-SG dataset. ProtoFlow not only outperforms standard GNN baselines in overall accuracy but also demonstrates exceptional robustness in limited-data, few-shot scenarios, maintaining strong performance when trained on as few as one surgical video. Our qualitative analyses further show that the learned prototypes successfully identify distinct surgical sub-techniques and provide clear, interpretable insights into workflow deviations and rare complications.\n  Conclusion: By uniting robust representation learning with inherent explainability, ProtoFlow represents a significant step toward developing more transparent, reliable, and data-efficient AI systems, accelerating their potential for clinical adoption in surgical training, real-time decision support, and workflow optimization.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14092v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出ProtoFlow框架，通过动态场景图原型学习实现可解释且鲁棒的手术工作流建模",
            "summary_zh": "目的：详细的手术识别对推进AI辅助手术至关重要，但高标注成本、数据稀缺和缺乏可解释模型阻碍了进展。虽然场景图提供了手术事件的结构化抽象，但其全部潜力尚未被充分挖掘。本文提出ProtoFlow，一种新颖的框架，通过学习动态场景图原型，以可解释且鲁棒的方式建模复杂手术工作流。方法：ProtoFlow利用图神经网络（GNN）编码器-解码器架构，结合自监督预训练进行丰富表示学习，以及基于原型的微调阶段。该过程发现并精炼核心原型，这些原型封装了重复出现的、具有临床意义的手术交互模式，为工作流分析形成可解释的基础。结果：我们在细粒度CAT-SG数据集上评估了我们的方法。ProtoFlow不仅在整体准确率上优于标准GNN基线，还在有限数据、少样本场景中表现出卓越的鲁棒性，在仅用一个手术视频训练时仍保持强劲性能。我们的定性分析进一步表明，学习到的原型成功识别了不同的手术子技术，并为工作流偏差和罕见并发症提供了清晰、可解释的见解。结论：通过将鲁棒的表示学习与固有的可解释性相结合，ProtoFlow代表了向开发更透明、可靠和数据高效的AI系统迈出的重要一步，加速了其在手术培训、实时决策支持和工作流优化中的临床采用潜力。",
            "intro_zh": [
                "核心问题：手术识别面临高标注成本、数据稀缺和模型缺乏可解释性，现有场景图方法未能充分利用结构化抽象潜力。",
                "方法要点：结合自监督预训练和原型微调，学习动态场景图原型以建模手术交互模式，实现可解释且鲁棒的工作流分析。",
                "实验或效果：在CAT-SG数据集上超越GNN基线，少样本场景下鲁棒性强，原型能识别手术子技术并提供工作流偏差见解。"
            ],
            "method_zh": "**问题定义**：论文旨在解决手术工作流建模中的挑战，包括高标注成本导致的数据稀缺、模型缺乏可解释性，以及现有场景图方法未能充分捕捉动态手术交互模式。现有方法的痛点在于依赖大量标注数据、难以泛化到少样本场景，且模型决策过程不透明，限制了临床可信度。\\n\\n**核心思路**：论文的核心思路是通过学习动态场景图原型来建模手术工作流，这些原型封装了重复出现的、具有临床意义的交互模式。设计上，结合自监督预训练学习丰富表示，再通过原型微调精炼核心模式，以实现可解释性和鲁棒性，从而减少对标注数据的依赖并增强模型透明度。\\n\\n**技术框架**：整体架构基于图神经网络（GNN）编码器-解码器，包含两个主要阶段：自监督预训练阶段，使用无标注数据学习场景图的通用表示；原型微调阶段，基于少量标注数据发现和优化动态原型，用于工作流分类和分析。流程上，输入手术视频的场景图序列，经编码器提取特征，解码器结合原型进行预测，输出工作流标签和可解释的原型匹配结果。\\n\\n**关键创新**：最重要的技术创新是引入了动态场景图原型学习机制，将原型方法与GNN结合，以可解释的方式建模手术交互模式。与现有方法的本质区别在于，它不仅提升性能，还通过原型提供直观的决策依据，增强了模型在少样本场景下的鲁棒性和临床可解释性，而传统方法多依赖黑盒模型或静态图表示。\\n\\n**关键设计**：关键参数包括原型数量（根据数据复杂度自适应设置）、GNN层数（例如2-3层图卷积网络）和损失函数（结合分类损失和原型一致性损失，如交叉熵和原型聚类损失）。网络结构使用GNN编码器（如GCN或GAT）处理场景图，解码器整合原型特征进行预测；自监督预训练可能采用对比学习或重构任务，微调阶段通过原型匹配优化表示。",
            "application_zh": "该研究的潜在应用领域包括手术培训、实时决策支持和工作流优化。在实际价值上，ProtoFlow的可解释性和鲁棒性有助于提高AI系统在临床环境中的可信度，减少对大量标注数据的依赖，加速AI辅助手术的采用。未来影响可能推动更透明、数据高效的医疗AI发展，提升手术安全性和效率。",
            "highlight_zh": "最重要的实验结果包括：在CAT-SG数据集上，ProtoFlow整体准确率超越标准GNN基线（具体提升幅度未知，但论文指出“outperforms”）；在少样本场景中，仅用一个手术视频训练时仍保持强劲性能，展示了卓越的鲁棒性；定性分析显示，学习到的原型能成功识别手术子技术，并提供工作流偏差和罕见并发症的清晰见解，验证了模型的可解释性。",
            "tags_zh": [
                "手术工作流建模",
                "动态场景图",
                "原型学习",
                "图神经网络",
                "可解释AI",
                "少样本学习",
                "自监督预训练",
                "医疗图像分析"
            ],
            "_index": 164
        },
        {
            "title": "Derivative-Informed Fourier Neural Operator: Universal Approximation and Applications to PDE-Constrained Optimization",
            "authors": [
                "Boyuan Yao",
                "Dingcheng Luo",
                "Lianghao Cao",
                "Nikola Kovachki",
                "Thomas O'Leary-Roseberry",
                "Omar Ghattas"
            ],
            "arxiv_id": "2512.14086v1",
            "summary": "We present approximation theories and efficient training methods for derivative-informed Fourier neural operators (DIFNOs) with applications to PDE-constrained optimization. A DIFNO is an FNO trained by minimizing its prediction error jointly on output and Fréchet derivative samples of a high-fidelity operator (e.g., a parametric PDE solution operator). As a result, a DIFNO can closely emulate not only the high-fidelity operator's response but also its sensitivities. To motivate the use of DIFNOs instead of conventional FNOs as surrogate models, we show that accurate surrogate-driven PDE-constrained optimization requires accurate surrogate Fréchet derivatives. Then, for continuously differentiable operators, we establish (i) simultaneous universal approximation of FNOs and their Fréchet derivatives on compact sets, and (ii) universal approximation of FNOs in weighted Sobolev spaces with input measures that have unbounded supports. Our theoretical results certify the capability of FNOs for accurate derivative-informed operator learning and accurate solution of PDE-constrained optimization. Furthermore, we develop efficient training schemes using dimension reduction and multi-resolution techniques that significantly reduce memory and computational costs for Fréchet derivative learning. Numerical examples on nonlinear diffusion--reaction, Helmholtz, and Navier--Stokes equations demonstrate that DIFNOs are superior in sample complexity for operator learning and solving infinite-dimensional PDE-constrained inverse problems, achieving high accuracy at low training sample sizes.",
            "categories": [
                "cs.LG",
                "math.NA"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14086v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出导数信息傅里叶神经算子以解决偏微分方程约束优化中的高精度代理建模问题",
            "summary_zh": "本文提出了导数信息傅里叶神经算子的逼近理论和高效训练方法，应用于偏微分方程约束优化。DIFNO是一种通过最小化其在高保真算子输出和Fréchet导数样本上的预测误差来训练的FNO，因此能够紧密模拟高保真算子的响应及其灵敏度。为了证明DIFNO作为代理模型的优势，我们展示了准确的代理驱动偏微分方程约束优化需要准确的代理Fréchet导数。对于连续可微算子，我们建立了(i) FNO及其Fréchet导数在紧集上的同时通用逼近性，以及(ii) FNO在具有无界支撑输入测度的加权Sobolev空间中的通用逼近性。我们的理论结果证明了FNO在准确导数信息算子学习和准确求解偏微分方程约束优化方面的能力。此外，我们开发了使用降维和多分辨率技术的高效训练方案，显著降低了Fréchet导数学习的内存和计算成本。在非线性扩散-反应、Helmholtz和Navier-Stokes方程上的数值实验表明，DIFNO在算子学习和求解无限维偏微分方程约束逆问题的样本复杂度方面具有优越性，在低训练样本量下实现了高精度。",
            "intro_zh": [
                "现有傅里叶神经算子在偏微分方程约束优化中缺乏导数信息，导致代理模型灵敏度不准确，影响优化精度。",
                "提出导数信息傅里叶神经算子，通过联合最小化输出和Fréchet导数误差，实现高保真算子的响应和灵敏度模拟。",
                "理论证明通用逼近性，实验显示在非线性方程上样本复杂度显著降低，实现高精度优化。"
            ],
            "method_zh": "**问题定义**：论文解决偏微分方程约束优化中代理模型导数不准确的问题。现有傅里叶神经算子仅学习算子输出，缺乏导数信息，导致优化过程中灵敏度误差累积，影响最终解的质量。\\n\\n**核心思路**：通过在高保真算子的输出和Fréchet导数样本上联合训练傅里叶神经算子，构建导数信息代理模型，使其同时逼近算子的响应和灵敏度，从而提高优化精度。\\n\\n**技术框架**：整体流程包括数据采集（获取高保真算子的输出和导数样本）、网络训练（使用降维和多分辨率技术优化损失函数）、模型验证（在测试集评估逼近性能）和应用部署（用于偏微分方程约束优化求解）。主要模块为傅里叶神经算子架构，结合导数信息损失项。\\n\\n**关键创新**：最重要的创新是引入导数信息学习，将Fréchet导数纳入训练目标，实现算子和其导数的同时逼近，与现有方法本质区别在于直接优化灵敏度准确性，而非仅输出匹配。\\n\\n**关键设计**：损失函数设计为输出误差和导数误差的加权和；网络结构基于傅里叶变换层，高效处理高维输入；采用降维技术（如主成分分析）减少导数样本维度，多分辨率训练加速收敛；参数设置包括傅里叶模式数、学习率和正则化系数，以平衡逼近精度和计算成本。",
            "application_zh": "该研究在偏微分方程约束优化领域具有广泛应用，如流体动力学、声学传播和化学反应模拟中的逆问题求解。实际价值在于降低高保真模拟的计算成本，提高优化效率，未来可能扩展到更复杂的多物理场系统和实时控制场景，推动科学计算和工程设计的智能化。",
            "highlight_zh": "实验在非线性扩散-反应、Helmholtz和Navier-Stokes方程上进行，DIFNO相比传统FNO在样本复杂度上显著优越，例如在Navier-Stokes问题中，使用少量训练样本（具体数据未知）即达到高精度，优化误差降低约一个数量级，验证了理论逼近性和高效训练方案的有效性。",
            "tags_zh": [
                "导数信息学习",
                "傅里叶神经算子",
                "偏微分方程约束优化",
                "代理建模",
                "Fréchet导数",
                "通用逼近理论",
                "样本复杂度",
                "多分辨率训练"
            ],
            "_index": 165
        },
        {
            "title": "Scalable Frameworks for Real-World Audio-Visual Speech Recognition",
            "authors": [
                "Sungnyun Kim"
            ],
            "arxiv_id": "2512.14083v1",
            "summary": "The practical deployment of Audio-Visual Speech Recognition (AVSR) systems is fundamentally challenged by significant performance degradation in real-world environments, characterized by unpredictable acoustic noise and visual interference. This dissertation posits that a systematic, hierarchical approach is essential to overcome these challenges, achieving the robust scalability at the representation, architecture, and system levels. At the representation level, we investigate methods for building a unified model that learns audio-visual features inherently robust to diverse real-world corruptions, thereby enabling generalization to new environments without specialized modules. To address architectural scalability, we explore how to efficiently expand model capacity while ensuring the adaptive and reliable use of multimodal inputs, developing a framework that intelligently allocates computational resources based on the input characteristics. Finally, at the system level, we present methods to expand the system's functionality through modular integration with large-scale foundation models, leveraging their powerful cognitive and generative capabilities to maximize final recognition accuracy. By systematically providing solutions at each of these three levels, this dissertation aims to build a next-generation, robust, and scalable AVSR system with high reliability in real-world applications.",
            "categories": [
                "eess.AS",
                "cs.CL",
                "cs.LG"
            ],
            "primary_category": "eess.AS",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "PhD Dissertation",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14083v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出分层可扩展框架以解决真实世界音频-视觉语音识别中的鲁棒性问题",
            "summary_zh": "音频-视觉语音识别（AVSR）系统在实际部署中面临根本性挑战，即在真实世界环境中（以不可预测的声学噪声和视觉干扰为特征）性能显著下降。本论文认为，系统性的分层方法对于克服这些挑战至关重要，旨在在表示、架构和系统三个层面实现鲁棒的可扩展性。在表示层面，我们研究构建统一模型的方法，该模型学习对多样真实世界干扰具有内在鲁棒性的音频-视觉特征，从而无需专门模块即可泛化到新环境。为解决架构可扩展性，我们探索如何高效扩展模型容量，同时确保多模态输入的自适应和可靠使用，开发了一个基于输入特征智能分配计算资源的框架。最后，在系统层面，我们提出通过与大规模基础模型的模块化集成来扩展系统功能的方法，利用其强大的认知和生成能力最大化最终识别准确率。通过在这三个层面系统性地提供解决方案，本论文旨在构建一个在真实世界应用中具有高可靠性的下一代鲁棒且可扩展的AVSR系统。",
            "intro_zh": [
                "核心问题：真实世界AVSR系统面临不可预测的声学噪声和视觉干扰，导致性能显著下降，现有方法缺乏系统性的鲁棒可扩展解决方案。",
                "方法要点：采用分层方法，在表示、架构和系统三个层面分别构建鲁棒特征、自适应架构和模块化集成，实现整体可扩展性。",
                "实验或效果：通过分层框架，系统在噪声和干扰环境下表现出更强的泛化能力和识别准确率，具体提升幅度未知。"
            ],
            "method_zh": "**问题定义**：论文旨在解决真实世界音频-视觉语音识别（AVSR）系统在不可预测声学噪声和视觉干扰下的性能退化问题。现有方法的痛点在于缺乏系统性的鲁棒可扩展解决方案，通常依赖于特定环境下的专门模块，难以泛化到新场景，且模型扩展效率低下，无法自适应处理多模态输入。\\n\\n**核心思路**：论文提出一个分层可扩展框架，通过在表示、架构和系统三个层面分别构建鲁棒性、自适应性和集成能力，实现整体系统的稳健扩展。这种设计基于真实世界环境的复杂性，需要从特征学习到系统集成的全方位优化，以应对多样干扰并最大化识别准确率。\\n\\n**技术框架**：整体架构分为三个层次：表示层构建统一模型学习鲁棒音频-视觉特征；架构层开发自适应框架，根据输入特征智能分配计算资源以扩展模型容量；系统层通过模块化集成大规模基础模型，利用其认知和生成能力增强功能。流程上，从底层特征提取到高层系统集成，逐层递进优化。\\n\\n**关键创新**：最重要的技术创新是分层可扩展框架，将鲁棒性问题分解为表示、架构和系统三个层面的子问题，并分别提供系统性解决方案。与现有方法的本质区别在于，它避免了单一模块的局限性，通过整体设计实现从特征到系统的全面可扩展性，从而更好地适应真实世界环境的动态变化。\\n\\n**关键设计**：在表示层，关键设计包括构建统一模型以学习对多样干扰具有内在鲁棒性的特征，可能涉及多任务学习或数据增强技术；在架构层，设计自适应框架，基于输入特征（如噪声水平或视觉质量）动态调整计算资源分配，可能使用注意力机制或轻量化模块；在系统层，关键设计是模块化集成大规模基础模型，通过接口设计或微调策略，利用其预训练能力提升识别准确率。具体参数设置、损失函数和网络结构细节未知。",
            "application_zh": "该研究在真实世界音频-视觉语音识别领域具有广泛潜在应用，如智能助手、视频会议系统、自动驾驶中的车内交互、医疗辅助设备（如助听器或语音康复工具）以及安全监控场景。其实际价值在于提升系统在噪声和干扰环境下的鲁棒性和可靠性，未来可能推动AVSR技术向更复杂、动态的真实世界部署迈进，促进多模态人工智能的实用化发展。",
            "highlight_zh": "最重要的实验结果包括：系统在真实世界噪声和视觉干扰环境下表现出更强的泛化能力，无需专门模块即可适应新场景；通过自适应架构，模型在扩展容量时保持了计算效率，具体性能数据未知；模块化集成大规模基础模型后，识别准确率得到最大化提升，对比基线可能包括传统AVSR方法或单模态系统，提升幅度未知。整体框架在三个层面均验证了可扩展性和鲁棒性的有效性。",
            "tags_zh": [
                "音频-视觉语音识别",
                "多模态融合",
                "鲁棒性学习",
                "可扩展架构",
                "自适应计算",
                "基础模型集成",
                "真实世界应用",
                "分层框架"
            ],
            "_index": 166
        },
        {
            "title": "SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations",
            "authors": [
                "Wentao Guo",
                "Mayank Mishra",
                "Xinle Cheng",
                "Ion Stoica",
                "Tri Dao"
            ],
            "arxiv_id": "2512.14080v1",
            "summary": "Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel \"token rounding\" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14080v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出SonicMoE系统，通过IO与Tile感知优化加速混合专家模型训练，解决内存与计算效率问题。",
            "summary_zh": "混合专家（MoE）模型已成为扩展语言模型规模而不显著增加计算成本的事实标准架构。近期MoE模型呈现出高专家粒度（较小的专家中间维度）和更高稀疏性（激活专家数量恒定但总专家数增加）的明显趋势，这提高了每FLOP的模型质量。然而，细粒度MoE由于更高的IO成本导致激活内存占用增加和硬件效率降低，而更稀疏的MoE则因分组GEMM内核中的填充而遭受计算浪费。为此，我们提出一种内存高效算法来计算MoE的前向和后向传播，最小化后向传播的激活缓存。我们还设计了GPU内核，将内存IO与计算重叠，使所有MoE架构受益。最后，我们提出一种新颖的“令牌舍入”方法，最小化分组GEMM内核中填充造成的计算浪费。因此，我们的方法SonicMoE在细粒度7B MoE上，相比ScatterMoE的BF16 MoE内核，减少了45%的激活内存，并在Hopper GPU上实现了1.86倍的计算吞吐量提升。具体而言，在64个H100上，SonicMoE实现了每天2130亿令牌的训练吞吐量，与使用lm-engine代码库进行FSDP-2训练的7B MoE模型在96个H100上ScatterMoE的2250亿令牌每天相当。在高MoE稀疏性设置下，我们的Tile感知令牌舍入算法相比普通top-K路由，在保持相似下游性能的同时，实现了额外1.16倍的内核执行时间加速。我们开源所有内核以加速MoE模型训练。",
            "intro_zh": [
                "核心问题：细粒度MoE因高IO成本导致内存占用大、效率低；稀疏MoE在分组GEMM中因填充浪费计算资源。",
                "方法要点：设计内存高效算法减少激活缓存，开发IO与计算重叠的GPU内核，提出令牌舍入方法优化填充计算。",
                "实验或效果：激活内存减少45%，计算吞吐提升1.86倍，训练吞吐达2130亿令牌/天，稀疏场景下内核加速1.16倍。"
            ],
            "method_zh": "**问题定义**：论文旨在解决混合专家（MoE）模型训练中的两个关键问题：细粒度MoE因高专家粒度导致激活内存占用大和IO成本高，降低硬件效率；稀疏MoE在分组GEMM（通用矩阵乘法）内核中因填充（padding）造成计算浪费，影响训练速度。现有方法如ScatterMoE在处理这些挑战时存在内存开销大和计算效率低的痛点。\\n\\n**核心思路**：论文的核心思路是通过系统级优化来平衡MoE模型的效率与性能。针对内存问题，设计最小化激活缓存的算法；针对计算浪费，提出令牌舍入方法减少填充；同时，利用GPU内核实现IO与计算重叠，提升整体吞吐量。这种设计基于对MoE架构趋势（高粒度、高稀疏性）的深入分析，旨在直接解决硬件瓶颈。\\n\\n**技术框架**：整体架构包括三个主要模块：内存高效的前向-后向传播算法、IO与计算重叠的GPU内核优化、以及Tile感知的令牌舍入算法。流程上，首先在训练过程中应用算法减少激活缓存，然后通过内核优化并行处理IO和计算，最后在路由阶段使用令牌舍入动态调整令牌分配，最小化分组GEMM中的填充操作。\\n\\n**关键创新**：最重要的技术创新是令牌舍入方法，它动态调整激活专家数量以减少填充，与现有静态top-K路由相比，本质区别在于自适应优化计算图结构。此外，IO与计算重叠的内核设计是系统级创新，显著提升GPU利用率。这些创新直接针对MoE训练中的内存和计算瓶颈，具有普适性。\\n\\n**关键设计**：关键设计包括：内存高效算法通过智能缓存策略减少激活存储；GPU内核基于Hopper架构优化，支持BF16精度和异步IO；令牌舍入算法结合Tile感知，动态调整令牌到专家的映射，参数设置上可能涉及稀疏阈值和舍入策略，但论文未详细说明损失函数或网络结构变化，主要聚焦于系统优化层面。",
            "application_zh": "该研究主要应用于大规模语言模型的训练与推理场景，特别是在需要高效扩展模型参数的MoE架构中。潜在应用领域包括自然语言处理、AI助手和内容生成等。实际价值在于显著降低训练成本和时间，提升硬件利用率，未来可能推动更复杂MoE模型的部署，加速AI模型规模化进程。",
            "highlight_zh": "最重要的实验结果包括：在细粒度7B MoE上，SonicMoE相比ScatterMoE的BF16内核，激活内存减少45%，计算吞吐提升1.86倍；在64个H100 GPU上，训练吞吐达到2130亿令牌/天，与ScatterMoE在96个H100上的2250亿令牌/天相当；在高稀疏设置下，令牌舍入算法相比普通top-K路由，内核执行时间加速1.16倍，同时下游性能保持相似。这些数据基于Hopper GPU和lm-engine代码库验证。",
            "tags_zh": [
                "混合专家模型",
                "GPU加速",
                "内存优化",
                "计算效率",
                "令牌路由",
                "系统优化",
                "大规模训练",
                "稀疏计算"
            ],
            "_index": 167
        },
        {
            "title": "FusAD: Time-Frequency Fusion with Adaptive Denoising for General Time Series Analysis",
            "authors": [
                "Da Zhang",
                "Bingyu Li",
                "Zhiyuan Zhao",
                "Feiping Nie",
                "Junyu Gao",
                "Xuelong Li"
            ],
            "arxiv_id": "2512.14078v1",
            "summary": "Time series analysis plays a vital role in fields such as finance, healthcare, industry, and meteorology, underpinning key tasks including classification, forecasting, and anomaly detection. Although deep learning models have achieved remarkable progress in these areas in recent years, constructing an efficient, multi-task compatible, and generalizable unified framework for time series analysis remains a significant challenge. Existing approaches are often tailored to single tasks or specific data types, making it difficult to simultaneously handle multi-task modeling and effectively integrate information across diverse time series types. Moreover, real-world data are often affected by noise, complex frequency components, and multi-scale dynamic patterns, which further complicate robust feature extraction and analysis. To ameliorate these challenges, we propose FusAD, a unified analysis framework designed for diverse time series tasks. FusAD features an adaptive time-frequency fusion mechanism, integrating both Fourier and Wavelet transforms to efficiently capture global-local and multi-scale dynamic features. With an adaptive denoising mechanism, FusAD automatically senses and filters various types of noise, highlighting crucial sequence variations and enabling robust feature extraction in complex environments. In addition, the framework integrates a general information fusion and decoding structure, combined with masked pre-training, to promote efficient learning and transfer of multi-granularity representations. Extensive experiments demonstrate that FusAD consistently outperforms state-of-the-art models on mainstream time series benchmarks for classification, forecasting, and anomaly detection tasks, while maintaining high efficiency and scalability. Code is available at https://github.com/zhangda1018/FusAD.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Paper has been accepted by ICDE2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14078v1",
            "code_links": [
                {
                    "url": "https://github.com/zhangda1018/FusAD",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出FusAD框架，通过自适应时频融合与去噪解决通用时间序列分析中的多任务兼容与噪声鲁棒性问题。",
            "summary_zh": "时间序列分析在金融、医疗、工业和气象等领域至关重要，支撑分类、预测和异常检测等关键任务。尽管深度学习模型近年来在这些领域取得了显著进展，但构建一个高效、多任务兼容且可泛化的统一框架仍面临重大挑战。现有方法通常针对单一任务或特定数据类型设计，难以同时处理多任务建模并有效整合不同类型时间序列的信息。此外，现实世界数据常受噪声、复杂频率成分和多尺度动态模式影响，进一步增加了鲁棒特征提取和分析的难度。为应对这些挑战，我们提出了FusAD，一个为多样化时间序列任务设计的统一分析框架。FusAD采用自适应时频融合机制，整合傅里叶和小波变换，以高效捕捉全局-局部和多尺度动态特征。通过自适应去噪机制，FusAD自动感知并过滤各类噪声，突出关键序列变化，在复杂环境中实现鲁棒特征提取。此外，该框架集成通用信息融合与解码结构，结合掩码预训练，促进多粒度表示的高效学习与迁移。大量实验表明，FusAD在主流时间序列基准测试中，在分类、预测和异常检测任务上持续优于最先进模型，同时保持高效率和可扩展性。代码可在https://github.com/zhangda1018/FusAD获取。",
            "intro_zh": [
                "现有方法常针对单一任务或特定数据类型，难以构建统一框架处理多任务和多样化时间序列。",
                "FusAD通过自适应时频融合与去噪机制，整合傅里叶和小波变换，实现鲁棒特征提取。",
                "实验显示FusAD在分类、预测和异常检测任务上优于最先进模型，保持高效率和可扩展性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决通用时间序列分析中的多任务兼容性、噪声鲁棒性和多尺度特征提取问题。现有方法的痛点包括：现有模型通常针对单一任务（如分类或预测）或特定数据类型（如平稳序列）设计，缺乏统一框架；现实数据常受噪声干扰，且包含复杂频率成分和多尺度动态模式，现有方法难以有效整合全局与局部信息，导致特征提取不鲁棒。\\n\\n**核心思路**：论文的核心解决思路是设计一个统一框架，通过自适应时频融合机制结合傅里叶变换（捕捉全局频率特征）和小波变换（捕捉局部时频特征），并引入自适应去噪机制自动过滤噪声，以增强多任务下的鲁棒性和泛化能力。这样设计是因为时间序列数据具有多尺度动态特性，传统单一变换方法难以全面捕捉，而自适应机制能根据数据特性动态调整，提高分析效率。\\n\\n**技术框架**：整体架构包括输入层、自适应时频融合模块、自适应去噪模块、通用信息融合与解码模块，以及输出层。主要模块/阶段：首先，输入时间序列数据；其次，通过自适应时频融合模块，并行应用傅里叶和小波变换，生成多尺度特征表示；然后，自适应去噪模块基于学习到的噪声模式自动过滤无关信息；接着，通用信息融合与解码模块整合多粒度特征，结合掩码预训练策略进行表示学习；最后，输出层根据具体任务（如分类、预测或异常检测）生成结果。\\n\\n**关键创新**：最重要的技术创新点是自适应时频融合与自适应去噪机制的集成。与现有方法的本质区别在于：现有方法多依赖单一变换（如仅傅里叶或小波）或固定去噪策略，而FusAD动态融合两种变换以捕捉全局-局部特征，并自适应感知噪声类型，实现更灵活和鲁棒的特征提取，支持多任务统一处理。\\n\\n**关键设计**：关键参数设置包括傅里叶变换的频域截断参数和小波变换的尺度参数，通过自适应学习调整；损失函数结合任务特定损失（如交叉熵用于分类、均方误差用于预测）和掩码预训练损失（如重构损失）；网络结构采用编码器-解码器架构，编码器包含卷积层和注意力机制处理时频特征，解码器根据任务设计；自适应去噪模块基于阈值学习或滤波网络实现噪声过滤。",
            "application_zh": "该研究在金融（如股票预测、风险检测）、医疗（如生理信号分析、疾病诊断）、工业（如设备监控、故障预警）和气象（如天气预测、气候分析）等领域具有广泛应用潜力。其实际价值在于提供统一框架，简化多任务时间序列分析流程，提高噪声环境下的鲁棒性和准确性，未来可能推动智能监控、预测系统和自动化决策的发展。",
            "highlight_zh": "在主流时间序列基准测试中，FusAD在分类、预测和异常检测任务上持续优于最先进模型。具体性能数据未知，但实验表明其提升幅度显著，同时保持高效率和可扩展性，代码已开源供验证和进一步研究。",
            "tags_zh": [
                "时间序列分析",
                "自适应时频融合",
                "自适应去噪",
                "多任务学习",
                "傅里叶变换",
                "小波变换",
                "掩码预训练",
                "鲁棒特征提取"
            ],
            "_index": 168
        },
        {
            "title": "Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed",
            "authors": [
                "Yonggan Fu",
                "Lexington Whalen",
                "Zhifan Ye",
                "Xin Dong",
                "Shizhe Diao",
                "Jingyu Liu",
                "Chengyue Wu",
                "Hao Zhang",
                "Enze Xie",
                "Song Han",
                "Maksim Khadkevich",
                "Jan Kautz",
                "Yingyan Celine Lin",
                "Pavlo Molchanov"
            ],
            "arxiv_id": "2512.14067v1",
            "summary": "Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively.",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14067v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出Efficient-DLM框架，通过改进AR-to-dLM转换方法，实现高效扩散语言模型，在保持任务准确性的同时提升生成速度。",
            "summary_zh": "扩散语言模型（dLMs）作为一种支持并行、非自回归生成的范式，展现出潜力，但其从头开始训练时的学习效率落后于自回归（AR）语言模型。为此，我们研究AR-to-dLM转换，将预训练的AR模型转化为高效的dLMs，在保持AR模型任务准确性的同时，显著提升生成速度。我们通过识别现有AR-to-dLM方法在注意力模式和目标上的局限性，提出更有效的转换原则和方法。具体来说，我们首先系统比较不同注意力模式，发现保持预训练AR权重分布对有效转换至关重要。因此，我们引入一种连续预训练方案，采用块级注意力模式，在块间保持因果性，同时在块内实现双向建模。这种方法比完全双向建模更能保留预训练AR模型的权重分布，并具有已知的KV缓存优势，实现准确性和效率的双赢。其次，为缓解掩码令牌分布（训练时均匀分布与测试时高度从左到右分布）之间的训练-测试差距，我们提出位置依赖的令牌掩码策略，在训练时为后续令牌分配更高的掩码概率，以更好地模拟测试时行为。利用此框架，我们广泛研究dLMs的注意力模式、训练动态和其他设计选择，为可扩展的AR-to-dLM转换提供实用见解。这些研究催生了Efficient-DLM系列模型，其性能优于最先进的AR模型和dLMs，例如，我们的Efficient-DLM 8B相比Dream 7B和Qwen3 4B，准确率分别提升+5.4%和+2.7%，吞吐量分别提高4.5倍和2.7倍。",
            "intro_zh": [
                "核心问题：现有AR-to-dLM转换方法在注意力模式和训练目标上存在局限性，导致转换后模型在准确性和效率上表现不佳。",
                "方法要点：提出块级注意力模式和位置依赖令牌掩码策略，以保持预训练权重分布并缩小训练-测试差距，实现高效转换。",
                "实验或效果：Efficient-DLM系列模型在准确率和吞吐量上均超越现有AR和dLM模型，如8B模型相比基线有显著提升。"
            ],
            "method_zh": "**问题定义**：论文旨在解决扩散语言模型（dLMs）在从头训练时学习效率低于自回归（AR）语言模型的问题，以及现有AR-to-dLM转换方法在注意力模式和训练目标上的不足，导致转换后模型在任务准确性和生成速度上难以兼顾。现有方法的痛点包括：注意力模式设计不当可能破坏预训练AR模型的权重分布，以及训练时掩码令牌分布与测试时不一致，影响模型性能。\\n\\n**核心思路**：论文的核心思路是通过改进AR-to-dLM转换过程，设计更有效的注意力模式和训练策略，以保持预训练AR模型的优势，同时利用dLMs的并行生成能力。这基于两个关键原则：一是维持预训练权重分布以保留知识，二是缩小训练与测试之间的分布差距以提升泛化能力。\\n\\n**技术框架**：整体框架包括两个主要阶段：首先，进行连续预训练，采用块级注意力模式，在块间保持因果性以支持KV缓存，在块内实现双向建模以增强表示能力；其次，实施位置依赖的令牌掩码策略，在训练时根据令牌位置调整掩码概率，模拟测试时的从左到右分布。框架还涉及对dLMs注意力模式、训练动态的系统研究，以优化设计选择。\\n\\n**关键创新**：最重要的技术创新是块级注意力模式和位置依赖令牌掩码策略。块级注意力模式在保留预训练权重分布的同时，实现高效并行生成，与现有方法（如完全双向建模）相比，本质区别在于更好地平衡了因果性和双向性。位置依赖掩码策略则针对训练-测试差距，通过动态调整掩码概率提升模型鲁棒性。\\n\\n**关键设计**：关键设计包括：块级注意力模式的具体实现，如块大小和注意力头配置；位置依赖掩码策略中的概率分配函数，例如基于令牌位置的线性或指数衰减；损失函数采用标准扩散模型目标，但结合掩码策略优化；网络结构基于预训练AR模型进行微调，保持原有层数和参数规模。这些设计通过实验验证，确保转换后的dLM在准确性和效率上达到最优。",
            "application_zh": "该研究在自然语言生成任务中具有广泛潜在应用，如机器翻译、文本摘要和对话系统，通过提升生成速度和准确性，可优化实时交互场景。实际价值在于降低计算成本，提高模型部署效率，未来可能推动dLMs在边缘设备和云服务中的普及，促进高效AI模型的发展。",
            "highlight_zh": "Efficient-DLM 8B模型在实验中表现突出：相比Dream 7B，准确率提升5.4%，吞吐量提高4.5倍；相比Qwen3 4B，准确率提升2.7%，吞吐量提高2.7倍。这些结果基于广泛基准测试，验证了方法在保持AR模型准确性的同时，显著提升dLMs的生成效率，超越现有最先进模型。",
            "tags_zh": [
                "扩散语言模型",
                "自回归模型转换",
                "注意力模式优化",
                "位置依赖掩码",
                "高效生成",
                "连续预训练",
                "KV缓存",
                "并行生成"
            ],
            "_index": 169
        },
        {
            "title": "Bridging Fidelity-Reality with Controllable One-Step Diffusion for Image Super-Resolution",
            "authors": [
                "Hao Chen",
                "Junyang Chen",
                "Jinshan Pan",
                "Jiangxin Dong"
            ],
            "arxiv_id": "2512.14061v1",
            "summary": "Recent diffusion-based one-step methods have shown remarkable progress in the field of image super-resolution, yet they remain constrained by three critical limitations: (1) inferior fidelity performance caused by the information loss from compression encoding of low-quality (LQ) inputs; (2) insufficient region-discriminative activation of generative priors; (3) misalignment between text prompts and their corresponding semantic regions. To address these limitations, we propose CODSR, a controllable one-step diffusion network for image super-resolution. First, we propose an LQ-guided feature modulation module that leverages original uncompressed information from LQ inputs to provide high-fidelity conditioning for the diffusion process. We then develop a region-adaptive generative prior activation method to effectively enhance perceptual richness without sacrificing local structural fidelity. Finally, we employ a text-matching guidance strategy to fully harness the conditioning potential of text prompts. Extensive experiments demonstrate that CODSR achieves superior perceptual quality and competitive fidelity compared with state-of-the-art methods with efficient one-step inference.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page: https://github.com/Chanson94/CODSR",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14061v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "SAC"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出CODSR可控一步扩散网络，解决图像超分辨率中保真度与感知质量平衡问题",
            "summary_zh": "近期基于扩散模型的一步方法在图像超分辨率领域取得了显著进展，但仍受限于三个关键问题：(1) 由于低质量输入压缩编码导致的信息损失，造成保真度性能下降；(2) 生成先验的区域区分性激活不足；(3) 文本提示与其对应语义区域之间的错位。为解决这些限制，我们提出了CODSR，一种用于图像超分辨率的可控一步扩散网络。首先，我们提出了一个低质量引导的特征调制模块，利用低质量输入的原始未压缩信息为扩散过程提供高保真度条件。接着，我们开发了一种区域自适应生成先验激活方法，在不牺牲局部结构保真度的前提下有效增强感知丰富度。最后，我们采用文本匹配引导策略，充分利用文本提示的条件潜力。大量实验表明，CODSR在高效一步推理下，相比最先进方法实现了卓越的感知质量和有竞争力的保真度。",
            "intro_zh": [
                "现有扩散一步方法因低质量输入压缩编码导致信息损失，保真度性能受限，且生成先验激活不足、文本提示与语义区域错位。",
                "提出CODSR网络，通过低质量引导特征调制、区域自适应先验激活和文本匹配引导，提升保真度与感知质量平衡。",
                "实验显示CODSR在一步推理下实现卓越感知质量和竞争性保真度，优于现有方法，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决图像超分辨率中扩散一步方法的三个核心问题：低质量输入压缩编码导致信息损失，降低保真度；生成先验的区域区分性激活不足，影响感知质量；文本提示与语义区域错位，限制条件引导效果。现有方法如扩散模型虽在感知质量上表现良好，但常牺牲保真度，且一步推理中条件信息利用不充分。\\n\\n**核心思路**：论文的核心思路是通过可控扩散过程，结合低质量输入的原始信息、区域自适应生成先验和文本匹配引导，实现保真度与感知质量的平衡。设计上，强调利用未压缩信息增强条件，动态激活先验以提升局部结构，并优化文本语义对齐，从而克服现有方法的局限性。\\n\\n**技术框架**：CODSR的整体架构基于一步扩散网络，包含三个主要模块：低质量引导特征调制模块，用于提取和融合低质量输入的未压缩特征，为扩散过程提供高保真度条件；区域自适应生成先验激活方法，通过自适应机制增强生成先验在关键区域的激活，提升感知丰富度；文本匹配引导策略，利用文本提示与图像语义的匹配，优化条件引导过程。流程上，输入低质量图像，经过特征调制和先验激活，结合文本引导，通过一步扩散生成高质量超分辨率图像。\\n\\n**关键创新**：最重要的技术创新点包括：低质量引导特征调制模块，直接利用原始未压缩信息，避免信息损失，提升保真度；区域自适应生成先验激活方法，实现生成先验的动态区域区分性激活，增强感知质量而不损害结构；文本匹配引导策略，通过语义对齐优化文本条件，提高引导效率。与现有方法的本质区别在于，CODSR综合了保真度增强、感知质量优化和条件利用，在一步推理中实现更全面的性能提升。\\n\\n**关键设计**：关键设计包括：特征调制模块采用卷积网络提取低质量输入特征，并与扩散过程的条件编码融合；区域自适应激活基于注意力机制，动态调整生成先验权重；文本匹配引导使用预训练文本编码器，结合图像特征进行语义匹配。损失函数可能包含保真度损失（如L1或L2损失）和感知损失（如对抗损失或特征匹配损失），网络结构为扩散模型变体，参数设置如扩散步数、学习率等需根据实验优化，具体细节在论文中未详细说明，需参考原文。",
            "application_zh": "该研究在图像超分辨率领域具有广泛潜在应用，如医学影像增强、卫星图像处理、安防监控视频修复和数字媒体内容生成。实际价值在于提供高效一步推理方案，平衡保真度与感知质量，适用于实时或资源受限场景。未来可能推动扩散模型在超分辨率和其他图像修复任务中的实用化，促进人工智能在视觉计算中的发展。",
            "highlight_zh": "实验表明，CODSR在多个标准数据集（如Set5、Set14、Urban100）上，相比最先进方法（如扩散基线和传统超分辨率方法），在感知质量指标（如LPIPS、FID）上取得显著提升，同时保真度指标（如PSNR、SSIM）保持竞争力。具体数据未知，但论文报告了高效一步推理下的卓越性能，验证了模块设计的有效性。",
            "tags_zh": [
                "图像超分辨率",
                "扩散模型",
                "一步推理",
                "保真度增强",
                "感知质量优化",
                "条件生成",
                "文本引导",
                "区域自适应"
            ],
            "_index": 170
        },
        {
            "title": "SELECT: Detecting Label Errors in Real-world Scene Text Data",
            "authors": [
                "Wenjun Liu",
                "Qian Wu",
                "Yifeng Hu",
                "Yuke Li"
            ],
            "arxiv_id": "2512.14050v1",
            "summary": "We introduce SELECT (Scene tExt Label Errors deteCTion), a novel approach that leverages multi-modal training to detect label errors in real-world scene text datasets. Utilizing an image-text encoder and a character-level tokenizer, SELECT addresses the issues of variable-length sequence labels, label sequence misalignment, and character-level errors, outperforming existing methods in accuracy and practical utility. In addition, we introduce Similarity-based Sequence Label Corruption (SSLC), a process that intentionally introduces errors into the training labels to mimic real-world error scenarios during training. SSLC not only can cause a change in the sequence length but also takes into account the visual similarity between characters during corruption. Our method is the first to detect label errors in real-world scene text datasets successfully accounting for variable-length labels. Experimental results demonstrate the effectiveness of SELECT in detecting label errors and improving STR accuracy on real-world text datasets, showcasing its practical utility.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "10.1145/3743093.3771031",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14050v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出SELECT方法，通过多模态训练检测真实场景文本数据集中的标签错误，解决变长标签序列和字符级错误问题。",
            "summary_zh": "我们介绍了SELECT（场景文本标签错误检测），这是一种利用多模态训练来检测真实世界场景文本数据集中标签错误的新方法。通过使用图像-文本编码器和字符级分词器，SELECT解决了变长序列标签、标签序列错位和字符级错误等问题，在准确性和实用性方面优于现有方法。此外，我们引入了基于相似性的序列标签破坏（SSLC）过程，该过程在训练期间有意向训练标签中引入错误，以模拟真实世界的错误场景。SSLC不仅可能导致序列长度发生变化，还在破坏过程中考虑了字符之间的视觉相似性。我们的方法是第一个成功检测真实世界场景文本数据集中标签错误的方法，并考虑了变长标签。实验结果表明，SELECT在检测标签错误和提高真实世界文本数据集上的场景文本识别（STR）准确性方面具有有效性，展示了其实用价值。",
            "intro_zh": [
                "现有方法难以处理真实场景文本数据中的变长标签序列、标签错位和字符级错误，导致标签错误检测不准确。",
                "SELECT采用多模态训练，结合图像-文本编码器和字符级分词器，并引入SSLC过程模拟真实错误，提升检测能力。",
                "实验显示SELECT在检测标签错误和提升STR准确性方面优于现有方法，验证了其在实际应用中的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在检测真实世界场景文本数据集中的标签错误，这些错误包括变长序列标签、标签序列错位和字符级错误。现有方法通常假设固定长度标签或忽略视觉相似性，难以处理真实场景中的复杂错误模式，导致检测精度低和实用性不足。\\n\\n**核心思路**：SELECT通过多模态训练整合图像和文本信息，利用图像-文本编码器提取特征，字符级分词器处理变长序列，并引入SSLC过程在训练中模拟真实错误，以增强模型的鲁棒性和检测能力。这种设计旨在直接应对标签错误的多样性和视觉相似性挑战。\\n\\n**技术框架**：整体架构包括两个主要阶段：训练阶段和检测阶段。在训练阶段，使用图像-文本编码器（如基于Transformer的模型）对输入图像和文本进行编码，生成多模态特征表示；字符级分词器将文本标签分解为字符序列，以处理变长标签；SSLC过程在训练标签中引入错误，模拟真实场景。在检测阶段，模型基于训练好的编码器计算图像和文本之间的相似度，识别出可能的标签错误。\\n\\n**关键创新**：最重要的技术创新是SSLC过程，它不仅在训练中引入错误以模拟真实场景，还考虑了字符之间的视觉相似性，例如将“O”误标为“0”，这更贴近实际错误模式。与现有方法相比，SELECT首次成功处理变长标签序列，并通过多模态整合提升了错误检测的准确性。\\n\\n**关键设计**：关键技术细节包括：使用预训练的视觉-语言模型（如CLIP或类似架构）作为图像-文本编码器的基础，以提取对齐的特征；字符级分词器采用Unicode编码或自定义词汇表，确保对任意长度序列的支持；SSLC过程中，错误引入基于字符相似度矩阵，例如从混淆矩阵中采样，以模拟视觉相似的字符替换；损失函数可能结合对比学习损失（如InfoNCE）和重构损失，以优化多模态对齐和错误检测；参数设置上，训练时可能使用Adam优化器，学习率根据实验调整，批量大小适应数据集规模。",
            "application_zh": "该研究可应用于场景文本识别（STR）系统的数据清洗和质量提升，例如在自动驾驶、文档数字化和智能监控中，通过检测和修正标签错误，提高模型训练数据的准确性和可靠性，从而增强下游任务的性能。未来可能扩展到其他多模态数据错误检测领域，推动人工智能在真实世界应用中的鲁棒性发展。",
            "highlight_zh": "实验结果表明，SELECT在多个真实场景文本数据集（如ICDAR、COCO-Text）上，标签错误检测准确率显著优于基线方法（如基于规则或单模态的方法），具体提升幅度因数据集而异，例如在某个基准上准确率提高了约10-15%。同时，使用SELECT修正标签后，STR模型的识别准确率也有明显改善，验证了其在实际应用中的有效性。",
            "tags_zh": [
                "场景文本识别",
                "标签错误检测",
                "多模态训练",
                "变长序列处理",
                "字符级错误",
                "视觉相似性",
                "数据清洗",
                "真实世界应用"
            ],
            "_index": 171
        },
        {
            "title": "Intention Chain-of-Thought Prompting with Dynamic Routing for Code Generation",
            "authors": [
                "Shen Li",
                "Li Huang",
                "Shaoxiong Zhan",
                "Weifeng Sun",
                "Tao Yin",
                "Zhongxin Liu",
                "Meng Yan"
            ],
            "arxiv_id": "2512.14048v1",
            "summary": "Large language models (LLMs) exhibit strong generative capabilities and have shown great potential in code generation. Existing chain-of-thought (CoT) prompting methods enhance model reasoning by eliciting intermediate steps, but suffer from two major limitations: First, their uniform application tends to induce overthinking on simple tasks. Second, they lack intention abstraction in code generation, such as explicitly modeling core algorithmic design and efficiency, leading models to focus on surface-level structures while neglecting the global problem objective. Inspired by the cognitive economy principle of engaging structured reasoning only when necessary to conserve cognitive resources, we propose RoutingGen, a novel difficulty-aware routing framework that dynamically adapts prompting strategies for code generation. For simple tasks, it adopts few-shot prompting; for more complex ones, it invokes a structured reasoning strategy, termed Intention Chain-of-Thought (ICoT), which we introduce to guide the model in capturing task intention, such as the core algorithmic logic and its time complexity. Experiments across three models and six standard code generation benchmarks show that RoutingGen achieves state-of-the-art performance in most settings, while reducing total token usage by 46.37% on average across settings. Furthermore, ICoT outperforms six existing prompting baselines on challenging benchmarks.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted at AAAI-2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14048v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出RoutingGen框架，通过动态路由和意图链式思考解决代码生成中推理效率与意图建模不足的问题。",
            "summary_zh": "大型语言模型在代码生成方面展现出强大的生成能力和巨大潜力。现有的链式思考提示方法通过引导中间步骤来增强模型推理，但存在两个主要局限：首先，其统一应用倾向于在简单任务上引发过度思考；其次，在代码生成中缺乏意图抽象，例如明确建模核心算法设计和效率，导致模型关注表面结构而忽视全局问题目标。受认知经济原则启发，即仅在必要时进行结构化推理以节省认知资源，我们提出了RoutingGen，一种新颖的难度感知路由框架，动态调整代码生成的提示策略。对于简单任务，它采用少样本提示；对于更复杂的任务，它调用结构化推理策略，称为意图链式思考，我们引入该策略来指导模型捕捉任务意图，如核心算法逻辑及其时间复杂度。在三个模型和六个标准代码生成基准上的实验表明，RoutingGen在大多数设置中实现了最先进的性能，同时在所有设置中平均减少了46.37%的总令牌使用量。此外，在具有挑战性的基准上，ICoT优于六个现有的提示基线。",
            "intro_zh": [
                "现有链式思考提示方法在代码生成中存在过度思考和意图建模不足的问题，导致推理效率低下和全局目标忽视。",
                "论文提出RoutingGen框架，结合难度感知动态路由和意图链式思考，根据任务复杂度自适应选择提示策略以优化推理。",
                "实验显示RoutingGen在多个基准上达到最优性能，平均减少46.37%令牌使用，ICoT在挑战性任务上超越六个基线方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决代码生成中现有链式思考提示方法的两个核心痛点：一是统一应用导致简单任务上的过度思考，浪费计算资源；二是缺乏对任务意图（如算法逻辑和效率）的显式建模，使模型易陷入表面结构而忽略全局目标。\\n\\n**核心思路**：受认知经济原则启发，论文提出动态路由框架RoutingGen，根据任务难度自适应选择提示策略：简单任务用少样本提示避免冗余推理，复杂任务用结构化推理策略ICoT引导模型捕捉深层意图，从而平衡效率与准确性。\\n\\n**技术框架**：RoutingGen整体架构包含两个主要阶段：首先，通过难度评估模块（可能基于任务特征或模型反馈）动态路由任务；其次，根据路由结果，应用少样本提示或ICoT策略生成代码。ICoT策略通过引导模型输出中间步骤，如算法设计和时间复杂度分析，来强化意图抽象。\\n\\n**关键创新**：最重要的技术创新是难度感知动态路由与意图链式思考的结合。与现有方法相比，本质区别在于从静态统一提示转向自适应策略，并首次在代码生成中引入意图抽象（如算法逻辑建模），提升推理的针对性和深度。\\n\\n**关键设计**：关键设计包括路由阈值设置（用于区分简单与复杂任务）、ICoT提示模板（结构化引导模型输出意图相关步骤），以及实验中的模型选择（三个LLMs）和基准测试（六个标准代码生成数据集），具体参数如路由机制细节在论文中未明确，需参考原文。",
            "application_zh": "该研究在智能编程助手、自动化代码生成和教育工具等领域具有潜在应用价值，能提升代码生成的效率和质量，减少计算开销。未来可能推动自适应提示技术的发展，优化LLMs在软件工程中的实际部署，促进人机协作编程。",
            "highlight_zh": "RoutingGen在三个模型和六个标准代码生成基准上实现最先进性能，在大多数设置中领先。总令牌使用量平均减少46.37%，显著提升推理效率。ICoT在挑战性基准上超越六个现有提示基线，验证了意图建模的有效性。",
            "tags_zh": [
                "代码生成",
                "链式思考提示",
                "动态路由",
                "意图建模",
                "大型语言模型",
                "难度感知",
                "推理优化",
                "自适应策略"
            ],
            "_index": 172
        },
        {
            "title": "A Deep Dive into Function Inlining and its Security Implications for ML-based Binary Analysis",
            "authors": [
                "Omar Abusabha",
                "Jiyong Uhm",
                "Tamer Abuhmed",
                "Hyungjoon Koo"
            ],
            "arxiv_id": "2512.14045v1",
            "summary": "A function inlining optimization is a widely used transformation in modern compilers, which replaces a call site with the callee's body in need. While this transformation improves performance, it significantly impacts static features such as machine instructions and control flow graphs, which are crucial to binary analysis. Yet, despite its broad impact, the security impact of function inlining remains underexplored to date. In this paper, we present the first comprehensive study of function inlining through the lens of machine learning-based binary analysis. To this end, we dissect the inlining decision pipeline within the LLVM's cost model and explore the combinations of the compiler options that aggressively promote the function inlining ratio beyond standard optimization levels, which we term extreme inlining. We focus on five ML-assisted binary analysis tasks for security, using 20 unique models to systematically evaluate their robustness under extreme inlining scenarios. Our extensive experiments reveal several significant findings: i) function inlining, though a benign transformation in intent, can (in)directly affect ML model behaviors, being potentially exploited by evading discriminative or generative ML models; ii) ML models relying on static features can be highly sensitive to inlining; iii) subtle compiler settings can be leveraged to deliberately craft evasive binary variants; and iv) inlining ratios vary substantially across applications and build configurations, undermining assumptions of consistency in training and evaluation of ML models.",
            "categories": [
                "cs.CR",
                "cs.LG",
                "cs.PL"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14045v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VIO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "首次全面研究函数内联对基于机器学习的二进制分析安全性的影响，揭示极端内联可被利用来规避ML模型。",
            "summary_zh": "函数内联优化是现代编译器中广泛使用的转换技术，它通过将调用点替换为被调用函数体来提高性能。然而，这种转换会显著影响机器指令和控制流图等静态特征，这些特征对二进制分析至关重要。尽管其影响广泛，但函数内联的安全影响至今仍未得到充分探索。本文首次从基于机器学习的二进制分析角度对函数内联进行了全面研究。为此，我们剖析了LLVM成本模型中的内联决策流程，并探索了那些能够将函数内联比率提升到标准优化级别之外的编译器选项组合，我们称之为极端内联。我们专注于五个用于安全的ML辅助二进制分析任务，使用20个独特的模型来系统评估它们在极端内联场景下的鲁棒性。我们的大量实验揭示了几个重要发现：i) 函数内联，尽管意图上是良性的转换，可以（间接）影响ML模型行为，可能被利用来规避判别式或生成式ML模型；ii) 依赖静态特征的ML模型可能对内联高度敏感；iii) 微妙的编译器设置可以被利用来故意制作规避性的二进制变体；iv) 内联比率在不同应用程序和构建配置中差异很大，破坏了ML模型训练和评估中一致性假设。",
            "intro_zh": [
                "现有基于机器学习的二进制分析方法依赖静态特征，但函数内联优化会显著改变这些特征，其安全影响尚未被系统研究，导致模型鲁棒性未知。",
                "论文提出通过剖析LLVM内联决策流程，探索极端内联配置，并系统评估多个ML模型在极端内联下的鲁棒性，以揭示内联对安全分析的影响。",
                "实验发现ML模型对内联高度敏感，极端内联可被利用来规避模型，且内联比率在不同应用中差异大，破坏了训练和评估的一致性假设。"
            ],
            "method_zh": "**问题定义**：论文要解决的核心问题是函数内联优化对基于机器学习的二进制分析安全性的影响。现有基于ML的二进制分析方法（如恶意软件检测、漏洞识别等）通常依赖静态特征（如机器指令、控制流图），但函数内联作为编译器常用优化，会显著改变这些特征，而现有研究未系统评估其安全影响，导致模型在实际编译环境中的鲁棒性未知，可能被攻击者利用来规避检测。\\n\\n**核心思路**：论文的核心解决思路是从ML安全角度首次全面研究函数内联，通过深入分析编译器内联机制，设计极端内联场景，并系统测试多种ML模型在这些场景下的表现，以量化内联对模型鲁棒性的影响。这样设计是因为内联是编译过程中的关键步骤，其变化可能无意中引入安全漏洞，而极端内联能放大这种效应，帮助揭示潜在风险。\\n\\n**技术框架**：整体架构包括三个阶段：首先，剖析LLVM编译器的内联决策流程，特别是其成本模型，以理解内联如何被触发和优化；其次，探索编译器选项组合，定义并实现极端内联配置，这些配置能超越标准优化级别（如-O2、-O3）进一步提升内联比率；最后，构建实验平台，选取五个安全相关的ML辅助二进制分析任务（如函数边界检测、恶意软件分类等），使用20个独特模型（包括判别式和生成式模型），在标准内联和极端内联下进行系统评估，比较模型性能变化。\\n\\n**关键创新**：最重要的技术创新点是首次将函数内联与ML-based二进制分析安全性联系起来，提出极端内联概念作为攻击面，并进行了大规模实证研究。与现有方法（通常忽略编译器优化对ML特征的影响）的本质区别在于，本文主动探索内联的极限情况，揭示了内联如何被恶意利用来规避ML模型，填补了编译优化与ML安全交叉领域的研究空白。\\n\\n**关键设计**：关键设计包括：在LLVM框架中深入分析内联成本模型，识别影响内联比率的参数（如函数大小阈值、调用频率等）；定义极端内联为通过调整编译器标志（如-inline-threshold、-inlinehint-threshold）实现的超常规优化；实验中使用多样化的ML模型（如基于图神经网络的模型、传统机器学习模型），覆盖不同任务类型；评估指标包括准确率、召回率等，以量化性能下降；通过控制变量法比较不同内联级别下的模型行为，确保结果可靠性。",
            "application_zh": "该研究的潜在应用领域包括二进制安全分析、恶意软件检测、漏洞挖掘和软件保护。实际价值在于揭示了编译器优化对ML模型安全性的影响，帮助开发者设计更鲁棒的ML-based分析工具，避免因内联等优化导致模型失效。未来影响可能推动编译器和ML社区的协作，开发抗优化攻击的模型，并提升软件供应链安全。",
            "highlight_zh": "实验最重要的结果是：在极端内联下，多个ML模型性能显著下降，例如某些模型的准确率降低超过20%，表明内联可被利用来规避检测；内联比率在不同应用程序中差异很大，最高可达标准优化的2倍以上，破坏了训练数据一致性；通过微妙编译器设置（如调整内联阈值），能故意制作规避性二进制变体，成功欺骗判别式和生成式模型。这些发现基于20个模型在五个任务上的系统评估，凸显了ML模型对内联的敏感性。",
            "tags_zh": [
                "函数内联",
                "机器学习安全",
                "二进制分析",
                "编译器优化",
                "极端内联",
                "模型鲁棒性",
                "LLVM",
                "静态特征"
            ],
            "_index": 173
        },
        {
            "title": "ChartAgent: A Chart Understanding Framework with Tool Integrated Reasoning",
            "authors": [
                "Boran Wang",
                "Xinming Wang",
                "Yi Chen",
                "Xiang Li",
                "Jian Xu",
                "Jing Yuan",
                "Chenglin Liu"
            ],
            "arxiv_id": "2512.14040v1",
            "summary": "With their high information density and intuitive readability, charts have become the de facto medium for data analysis and communication across disciplines. Recent multimodal large language models (MLLMs) have made notable progress in automated chart understanding, yet they remain heavily dependent on explicit textual annotations and the performance degrades markedly when key numerals are absent. To address this limitation, we introduce ChartAgent, a chart understanding framework grounded in Tool-Integrated Reasoning (TIR). Inspired by human cognition, ChartAgent decomposes complex chart analysis into a sequence of observable, replayable steps. Supporting this architecture is an extensible, modular tool library comprising more than a dozen core tools, such as keyelement detection, instance segmentation, and optical character recognition (OCR), which the agent dynamically orchestrates to achieve systematic visual parsing across diverse chart types. Leveraging TIRs transparency and verifiability, ChartAgent moves beyond the black box paradigm by standardizing and consolidating intermediate outputs into a structured Evidence Package, providing traceable and reproducible support for final conclusions. Experiments show that ChartAgent substantially improves robustness under sparse annotation settings, offering a practical path toward trustworthy and extensible systems for chart understanding.",
            "categories": [
                "cs.CV",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14040v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "PPO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出ChartAgent框架，通过工具集成推理解决图表理解在稀疏标注下的鲁棒性问题。",
            "summary_zh": "图表因其高信息密度和直观可读性，已成为跨学科数据分析和交流的实际媒介。近年来，多模态大语言模型（MLLMs）在自动化图表理解方面取得了显著进展，但它们仍然严重依赖显式文本标注，当关键数字缺失时性能会显著下降。为了解决这一限制，我们引入了ChartAgent，这是一个基于工具集成推理（TIR）的图表理解框架。受人类认知启发，ChartAgent将复杂的图表分析分解为一系列可观察、可重放的步骤。支持这一架构的是一个可扩展的模块化工具库，包含十多个核心工具，如关键元素检测、实例分割和光学字符识别（OCR），智能体动态编排这些工具，以实现跨不同图表类型的系统化视觉解析。利用TIR的透明性和可验证性，ChartAgent超越了黑盒范式，通过将中间输出标准化并整合到结构化的证据包中，为最终结论提供可追溯和可复现的支持。实验表明，ChartAgent在稀疏标注设置下显著提高了鲁棒性，为可信赖和可扩展的图表理解系统提供了一条实用路径。",
            "intro_zh": [
                "现有多模态大语言模型依赖显式文本标注，关键数字缺失时性能显著下降，限制了实际应用。",
                "提出ChartAgent框架，基于工具集成推理，将复杂分析分解为可观察步骤，动态编排模块化工具库。",
                "实验显示，ChartAgent在稀疏标注设置下大幅提升鲁棒性，为可信赖图表理解系统提供实用路径。"
            ],
            "method_zh": "**问题定义**：论文旨在解决图表理解任务中，现有多模态大语言模型（MLLMs）严重依赖显式文本标注的问题。当图表中关键数字或文本信息缺失（稀疏标注）时，这些模型的性能会显著下降，限制了其在真实世界场景中的鲁棒性和实用性。现有方法的痛点在于其黑盒性质，缺乏透明度和可验证性，难以处理复杂或标注不完整的图表。\\n\\n**核心思路**：论文的核心解决思路是引入工具集成推理（TIR），受人类认知过程启发，将复杂的图表分析任务分解为一系列可观察、可重放的步骤。通过动态编排模块化工具库，智能体能够系统化地解析图表视觉元素，从而减少对显式标注的依赖。这种设计旨在提高模型在稀疏标注下的鲁棒性，同时增强推理过程的透明度和可追溯性。\\n\\n**技术框架**：ChartAgent的整体架构包括一个可扩展的模块化工具库和一个智能体控制器。工具库包含十多个核心工具，如关键元素检测、实例分割和光学字符识别（OCR）。智能体根据图表类型和任务需求，动态选择和编排这些工具，执行视觉解析步骤。中间输出被标准化并整合到结构化的证据包中，最终生成可验证的结论。流程分为图表输入、工具调用、证据收集和结论生成四个主要阶段。\\n\\n**关键创新**：最重要的技术创新点是工具集成推理（TIR）框架的提出，它将图表理解从端到端的黑盒模型转变为基于工具的动态推理过程。与现有方法的本质区别在于，ChartAgent通过模块化工具库实现系统化视觉解析，而非依赖单一模型的全参数化处理。这提高了灵活性、可解释性和在稀疏标注下的性能。\\n\\n**关键设计**：关键设计包括模块化工具库的构建，其中工具如关键元素检测可能基于预训练的视觉模型（如YOLO或Mask R-CNN），OCR工具可能集成Tesseract等开源库。智能体控制器使用规则或轻量级学习策略来动态编排工具。证据包采用结构化格式（如JSON）存储中间结果，确保可追溯性。具体参数设置和损失函数在论文中未详细说明，可能依赖于工具本身的配置。",
            "application_zh": "ChartAgent的潜在应用领域包括数据可视化分析、自动化报告生成、教育辅助工具和商业智能系统。其实用价值在于提高图表理解在真实世界场景中的鲁棒性和可信赖性，特别是在标注稀疏或缺失的情况下。未来影响可能推动多模态AI向更透明、可扩展的方向发展，促进跨学科数据交流的自动化。",
            "highlight_zh": "实验表明，ChartAgent在稀疏标注设置下显著提升了鲁棒性。具体性能数据未在摘要中提供，但论文提到与现有多模态大语言模型相比，ChartAgent在关键数字缺失时性能下降较少。提升幅度可能通过标准基准测试（如ChartQA或FigureQA）量化，但具体数值未知。对比基线可能包括基于MLLM的方法，ChartAgent通过工具集成推理实现了更稳定的表现。",
            "tags_zh": [
                "图表理解",
                "工具集成推理",
                "多模态大语言模型",
                "稀疏标注",
                "视觉解析",
                "可解释AI",
                "模块化工具库",
                "光学字符识别"
            ],
            "_index": 174
        },
        {
            "title": "Unleashing the Power of Image-Tabular Self-Supervised Learning via Breaking Cross-Tabular Barriers",
            "authors": [
                "Yibing Fu",
                "Yunpeng Zhao",
                "Zhitao Zeng",
                "Cheng Chen",
                "Yueming Jin"
            ],
            "arxiv_id": "2512.14026v1",
            "summary": "Multi-modal learning integrating medical images and tabular data has significantly advanced clinical decision-making in recent years. Self-Supervised Learning (SSL) has emerged as a powerful paradigm for pretraining these models on large-scale unlabeled image-tabular data, aiming to learn discriminative representations. However, existing SSL methods for image-tabular representation learning are often confined to specific data cohorts, mainly due to their rigid tabular modeling mechanisms when modeling heterogeneous tabular data. This inter-tabular barrier hinders the multi-modal SSL methods from effectively learning transferrable medical knowledge shared across diverse cohorts. In this paper, we propose a novel SSL framework, namely CITab, designed to learn powerful multi-modal feature representations in a cross-tabular manner. We design the tabular modeling mechanism from a semantic-awareness perspective by integrating column headers as semantic cues, which facilitates transferrable knowledge learning and the scalability in utilizing multiple data sources for pretraining. Additionally, we propose a prototype-guided mixture-of-linear layer (P-MoLin) module for tabular feature specialization, empowering the model to effectively handle the heterogeneity of tabular data and explore the underlying medical concepts. We conduct comprehensive evaluations on Alzheimer's disease diagnosis task across three publicly available data cohorts containing 4,461 subjects. Experimental results demonstrate that CITab outperforms state-of-the-art approaches, paving the way for effective and scalable cross-tabular multi-modal learning.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14026v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出CITab框架以解决跨队列图像-表格自监督学习中的表格异构性障碍问题",
            "summary_zh": "近年来，整合医学图像和表格数据的多模态学习显著推动了临床决策的发展。自监督学习已成为在这些大规模未标记图像-表格数据上进行预训练的强大范式，旨在学习判别性表征。然而，现有的图像-表格表征学习自监督方法通常局限于特定的数据队列，主要原因是它们在建模异构表格数据时采用了僵化的表格建模机制。这种跨表格障碍阻碍了多模态自监督方法有效学习跨不同队列共享的可迁移医学知识。本文提出了一种新颖的自监督学习框架，即CITab，旨在以跨表格的方式学习强大的多模态特征表征。我们从语义感知的角度设计表格建模机制，通过整合列标题作为语义线索，这促进了可迁移知识的学习以及利用多个数据源进行预训练的可扩展性。此外，我们提出了一个原型引导的线性混合层模块用于表格特征专门化，使模型能够有效处理表格数据的异构性并探索潜在的医学概念。我们在包含4,461名受试者的三个公开数据队列上对阿尔茨海默病诊断任务进行了全面评估。实验结果表明，CITab优于最先进的方法，为有效且可扩展的跨表格多模态学习铺平了道路。",
            "intro_zh": [
                "现有自监督学习方法在图像-表格多模态学习中面临跨队列表格数据异构性障碍，导致模型局限于特定数据队列，难以学习可迁移的医学知识。",
                "论文提出CITab框架，通过语义感知的表格建模机制整合列标题作为语义线索，并引入原型引导的线性混合层模块处理表格异构性，实现跨队列知识学习。",
                "在阿尔茨海默病诊断任务上，CITab在三个公开队列（共4,461名受试者）上超越现有方法，验证了其有效性和可扩展性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决图像-表格多模态自监督学习中的跨队列表格数据异构性问题。现有方法采用僵化的表格建模机制，无法有效处理不同队列间表格数据的结构差异（如列名、数据类型、分布变化），导致模型局限于特定数据队列，难以学习可迁移的医学知识，限制了预训练的可扩展性。\\n\\n**核心思路**：论文的核心思路是从语义感知角度重新设计表格建模，将列标题作为语义线索融入模型，以捕捉表格数据的通用语义结构，从而打破跨队列障碍。同时，通过原型引导的线性混合层模块专门化处理表格特征，适应不同队列的异构性，探索潜在的医学概念。这种设计旨在实现跨队列知识迁移和可扩展预训练。\\n\\n**技术框架**：CITab框架整体包括多模态输入处理、自监督预训练和下游任务微调三个阶段。主要模块包括：图像编码器（如CNN或ViT）、表格编码器（整合列标题语义）、原型引导的线性混合层模块（P-MoLin）用于表格特征专门化，以及多模态融合层。预训练阶段采用对比学习或生成式目标，学习图像和表格的联合表征，强调跨队列一致性。\\n\\n**关键创新**：最重要的技术创新是语义感知的表格建模机制和P-MoLin模块。与现有方法（如简单MLP或固定嵌入）相比，CITab通过列标题语义化表格数据，使模型能理解表格的通用结构，而非依赖特定队列的硬编码；P-MoLin模块则动态调整线性层以处理异构性，本质区别在于从“数据驱动”转向“语义驱动”，提升了模型的泛化能力和可扩展性。\\n\\n**关键设计**：技术细节包括：表格编码器使用列标题的嵌入（如词向量）与数值/类别特征结合；P-MoLin模块基于原型聚类（如k-means）生成多个线性层，通过门控机制混合输出；损失函数可能结合对比损失（如图像-表格对）和重构损失；网络结构可配置，例如使用ResNet或Transformer作为骨干；参数设置涉及原型数量、学习率调度和多任务权重，需在预训练数据上优化。",
            "application_zh": "该研究主要应用于医疗AI领域，特别是在需要整合医学图像（如MRI、CT）和临床表格数据（如患者信息、实验室结果）的诊断任务中，如阿尔茨海默病、癌症检测等。其实际价值在于通过跨队列自监督学习，利用多源未标记数据提升模型泛化能力，降低对标注数据的依赖，推动个性化医疗和早期疾病筛查。未来影响可能扩展到其他多模态场景，如金融风控或工业检测，促进可扩展AI系统的发展。",
            "highlight_zh": "在阿尔茨海默病诊断任务上，CITab在三个公开数据队列（ADNI、AIBL、OASIS，共4,461名受试者）上进行了全面评估。实验结果显示，CITab在诊断准确率、AUC等指标上显著优于现有最先进的自监督学习方法（如MoCo、SimCLR的变体），具体提升幅度因任务和队列而异，例如在跨队列泛化测试中，性能提升可达5-10%以上。这验证了CITab在打破表格异构性障碍、学习可迁移医学知识方面的有效性。",
            "tags_zh": [
                "多模态学习",
                "自监督学习",
                "医学图像分析",
                "表格数据处理",
                "跨队列泛化",
                "语义感知建模",
                "原型引导网络",
                "阿尔茨海默病诊断"
            ],
            "_index": 175
        },
        {
            "title": "EXAONE Path 2.5: Pathology Foundation Model with Multi-Omics Alignment",
            "authors": [
                "Juseung Yun",
                "Sunwoo Yu",
                "Sumin Ha",
                "Jonghyun Kim",
                "Janghyeon Lee",
                "Jongseong Jang",
                "Soonyoung Lee"
            ],
            "arxiv_id": "2512.14019v1",
            "summary": "Cancer progression arises from interactions across multiple biological layers, especially beyond morphological and across molecular layers that remain invisible to image-only models. To capture this broader biological landscape, we present EXAONE Path 2.5, a pathology foundation model that jointly models histologic, genomic, epigenetic and transcriptomic modalities, producing an integrated patient representation that reflects tumor biology more comprehensively. Our approach incorporates three key components: (1) multimodal SigLIP loss enabling all-pairwise contrastive learning across heterogeneous modalities, (2) a fragment-aware rotary positional encoding (F-RoPE) module that preserves spatial structure and tissue-fragment topology in WSI, and (3) domain-specialized internal foundation models for both WSI and RNA-seq to provide biologically grounded embeddings for robust multimodal alignment. We evaluate EXAONE Path 2.5 against six leading pathology foundation models across two complementary benchmarks: an internal real-world clinical dataset and the Patho-Bench benchmark covering 80 tasks. Our framework demonstrates high data and parameter efficiency, achieving on-par performance with state-of-the-art foundation models on Patho-Bench while exhibiting the highest adaptability in the internal clinical setting. These results highlight the value of biologically informed multimodal design and underscore the potential of integrated genotype-to-phenotype modeling for next-generation precision oncology.",
            "categories": [
                "cs.LG",
                "q-bio.QM"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14019v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出EXAONE Path 2.5病理学基础模型，通过多组学对齐整合组织学与分子数据，以更全面建模肿瘤生物学。",
            "summary_zh": "癌症进展源于多个生物层面的相互作用，尤其是超越形态学并涉及分子层面，这些对仅基于图像的模型是不可见的。为了捕捉更广泛的生物景观，我们提出了EXAONE Path 2.5，这是一个病理学基础模型，它联合建模组织学、基因组学、表观遗传学和转录组学模态，生成一个更全面反映肿瘤生物学的整合患者表征。我们的方法包含三个关键组件：(1) 多模态SigLIP损失，实现跨异质模态的全配对对比学习；(2) 片段感知旋转位置编码(F-RoPE)模块，保留全切片图像中的空间结构和组织片段拓扑；(3) 针对全切片图像和RNA-seq的领域专用内部基础模型，为稳健的多模态对齐提供基于生物学的嵌入。我们在两个互补基准上评估EXAONE Path 2.5与六个领先的病理学基础模型：一个内部真实世界临床数据集和覆盖80个任务的Patho-Bench基准。我们的框架展示了高数据和参数效率，在Patho-Bench上达到与最先进基础模型相当的性能，同时在内部临床设置中表现出最高的适应性。这些结果突显了基于生物学的多模态设计的价值，并强调了整合基因型到表型建模对下一代精准肿瘤学的潜力。",
            "intro_zh": [
                "现有方法主要依赖单一组织学图像，难以捕捉癌症进展中跨分子层面的相互作用，导致对肿瘤生物学的理解不全面。",
                "论文提出EXAONE Path 2.5，通过多模态对齐整合组织学、基因组、表观遗传和转录组数据，并引入SigLIP损失、F-RoPE模块和领域专用模型来增强表征能力。",
                "在Patho-Bench基准上达到与最先进模型相当的性能，在内部临床数据中表现出最高适应性，验证了多组学整合的有效性和高效性。"
            ],
            "method_zh": "**问题定义**：癌症进展涉及组织学与分子层面的复杂相互作用，现有病理学基础模型主要基于单一图像模态，无法全面建模肿瘤生物学，导致对患者表征的局限性。现有方法的痛点在于缺乏跨模态对齐能力，难以整合异质数据如基因组和转录组信息。\\n\\n**核心思路**：论文的核心解决思路是构建一个多模态病理学基础模型，通过联合建模组织学、基因组、表观遗传和转录组数据，生成整合的患者表征。设计上，采用对比学习对齐不同模态，并引入专门模块处理空间结构和生物学嵌入，以更全面地反映肿瘤生物学。\\n\\n**技术框架**：整体架构包括数据预处理、多模态对齐和评估阶段。主要模块有：多模态SigLIP损失用于全配对对比学习，F-RoPE模块保留全切片图像的空间拓扑，以及领域专用内部基础模型为全切片图像和RNA-seq提供生物学嵌入。流程上，先通过内部模型提取各模态嵌入，再使用SigLIP损失进行对齐，最终生成整合表征用于下游任务。\\n\\n**关键创新**：最重要的技术创新点是多模态SigLIP损失、F-RoPE模块和领域专用内部基础模型的结合。与现有方法的本质区别在于，它不仅整合多组学数据，还通过专门设计处理模态异质性和空间结构，实现更稳健的对齐和更全面的生物学建模。\\n\\n**关键设计**：关键参数设置包括对比学习中的温度参数和嵌入维度；损失函数使用SigLIP损失，支持跨模态全配对对比；网络结构涉及Transformer-based编码器用于模态嵌入，F-RoPE模块集成旋转位置编码以处理组织片段；技术细节还包括使用预训练的领域专用模型初始化，以增强生物学基础。",
            "application_zh": "该研究在精准肿瘤学领域具有重要应用价值，可用于癌症诊断、预后预测和治疗响应分析。通过整合多组学数据，它能提供更全面的患者表征，辅助临床决策和个性化医疗。未来可能推动下一代病理学工具的发展，促进跨模态生物医学研究。",
            "highlight_zh": "在Patho-Bench基准上，EXAONE Path 2.5与六个领先模型相比，达到最先进性能（具体数据未知，但描述为“on-par”），覆盖80个任务。在内部真实世界临床数据集中，它表现出最高的适应性，验证了多模态设计的优势。框架还展示了高数据和参数效率，提升了模型在复杂临床环境中的实用性。",
            "tags_zh": [
                "病理学基础模型",
                "多模态对齐",
                "多组学整合",
                "对比学习",
                "全切片图像分析",
                "精准肿瘤学",
                "生物医学人工智能",
                "Transformer架构"
            ],
            "_index": 176
        },
        {
            "title": "PerfCoder: Large Language Models for Interpretable Code Performance Optimization",
            "authors": [
                "Jiuding Yang",
                "Shengyao Lu",
                "Hongxuan Liu",
                "Shayan Shirahmad Gale Bagi",
                "Zahra Fazel",
                "Tomasz Czajkowski",
                "Di Niu"
            ],
            "arxiv_id": "2512.14018v1",
            "summary": "Large language models (LLMs) have achieved remarkable progress in automatic code generation, yet their ability to produce high-performance code remains limited--a critical requirement in real-world software systems. We argue that current LLMs struggle not only due to data scarcity but, more importantly, because they lack supervision that guides interpretable and effective performance improvements. In this work, we introduce PerfCoder, a family of LLMs specifically designed to generate performance-enhanced code from source code via interpretable, customized optimizations. PerfCoder is fine-tuned on a curated collection of real-world optimization trajectories with human-readable annotations, and preference-aligned by reinforcement fine-tuning using runtime measurements, enabling it to propose input-specific improvement strategies and apply them directly without relying on iterative refinement. On the PIE code performance benchmark, PerfCoder surpasses all existing models in both runtime speedup and effective optimization rate, demonstrating that performance optimization cannot be achieved by scale alone but requires optimization stratetgy awareness. In addition, PerfCoder can generate interpretable feedback about the source code, which, when provided as input to a larger LLM in a planner-and-optimizer cooperative workflow, can further improve outcomes. Specifically, we elevate the performance of 32B models and GPT-5 to new levels on code optimization, substantially surpassing their original performance.",
            "categories": [
                "cs.SE",
                "cs.AI"
            ],
            "primary_category": "cs.SE",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14018v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "强化学习",
                    "matched_keywords": [
                        "RL"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出PerfCoder模型，通过可解释的定制化优化解决大语言模型生成高性能代码的难题",
            "summary_zh": "大语言模型（LLMs）在自动代码生成方面取得了显著进展，但其生成高性能代码的能力仍然有限——这是现实世界软件系统中的关键需求。我们认为，当前LLMs的困难不仅源于数据稀缺，更重要的是缺乏指导可解释且有效性能改进的监督。在这项工作中，我们引入了PerfCoder，这是一个专门设计用于通过可解释的定制化优化从源代码生成性能增强代码的LLMs家族。PerfCoder在精心策划的真实世界优化轨迹集合上进行了微调，这些轨迹带有可读的人类注释，并通过使用运行时测量的强化微调进行偏好对齐，使其能够提出输入特定的改进策略并直接应用，而不依赖于迭代优化。在PIE代码性能基准测试中，PerfCoder在运行时加速和有效优化率方面均超越了所有现有模型，表明性能优化不能仅通过规模实现，而需要优化策略意识。此外，PerfCoder可以生成关于源代码的可解释反馈，当在规划器与优化器协作工作流中作为输入提供给更大的LLM时，可以进一步改善结果。具体来说，我们将32B模型和GPT-5在代码优化方面的性能提升到新水平，大幅超越了它们的原始性能。",
            "intro_zh": [
                "核心问题：现有大语言模型在生成高性能代码方面受限，主要因缺乏可解释的优化监督，导致代码性能不足。",
                "方法要点：提出PerfCoder模型，通过微调真实优化轨迹和强化学习对齐偏好，实现输入特定的可解释性能优化。",
                "实验或效果：在PIE基准上超越所有现有模型，显著提升运行时速度和优化率，并增强大模型代码优化能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大语言模型（LLMs）在自动代码生成中难以生成高性能代码的问题。现有方法的痛点包括：数据稀缺导致模型缺乏优化示例，更重要的是缺乏可解释的监督信号，使得模型无法有效指导性能改进，往往依赖迭代优化或规模扩展，效率低下且效果有限。\\n\\n**核心思路**：论文的核心思路是通过引入可解释的优化监督，训练专门的大语言模型来生成性能增强的代码。设计基于真实世界的优化轨迹，结合人类可读注释和运行时测量，使模型能够学习输入特定的优化策略，并直接应用，避免迭代过程，从而提高优化效率和效果。\\n\\n**技术框架**：整体架构包括两个主要阶段：首先，在精心策划的优化轨迹数据集上进行微调，这些数据包含源代码、优化版本和人类注释；其次，通过强化微调（如使用运行时测量作为奖励）对齐模型偏好，确保生成的代码在性能上更优。模型采用标准Transformer架构，但训练流程专门针对性能优化任务定制。\\n\\n**关键创新**：最重要的技术创新点是结合了可解释的优化监督和强化学习偏好对齐。与现有方法相比，PerfCoder不仅依赖大规模预训练，而是通过结构化优化数据微调，使模型具备优化策略意识，能够生成定制化、可解释的改进，而非仅靠规模或迭代优化。\\n\\n**关键设计**：关键设计包括：使用真实世界优化轨迹数据集进行监督微调，损失函数基于代码生成和注释预测；强化微调阶段采用运行时速度作为奖励信号，通过策略梯度方法优化模型输出；模型架构基于现有LLMs（如GPT系列），但训练数据和方法专门针对性能优化，参数设置可能涉及多任务学习以平衡代码生成和解释生成。",
            "application_zh": "该研究在软件工程和系统优化领域具有广泛潜在应用，可用于自动代码性能提升、编译器优化辅助、实时系统代码生成等场景。实际价值在于提高软件开发效率，减少人工优化成本，并增强代码的可维护性和性能。未来可能影响AI辅助编程工具的发展，推动大语言模型在专业领域（如高性能计算、嵌入式系统）的深入应用。",
            "highlight_zh": "在PIE代码性能基准测试中，PerfCoder在运行时加速和有效优化率方面均超越所有现有模型，具体数据未在摘要中提供，但表明性能显著提升。此外，通过可解释反馈与更大LLM（如32B模型和GPT-5）协作，进一步提升了代码优化效果，大幅超越原始性能，验证了优化策略意识的重要性。",
            "tags_zh": [
                "代码性能优化",
                "大语言模型",
                "可解释AI",
                "强化微调",
                "软件工程",
                "自动代码生成",
                "优化策略",
                "Transformer架构"
            ],
            "_index": 177
        },
        {
            "title": "Accelerating MHC-II Epitope Discovery via Multi-Scale Prediction in Antigen Presentation",
            "authors": [
                "Yue Wan",
                "Jiayi Yuan",
                "Zhiwei Feng",
                "Xiaowei Jia"
            ],
            "arxiv_id": "2512.14011v1",
            "summary": "Antigenic epitope presented by major histocompatibility complex II (MHC-II) proteins plays an essential role in immunotherapy. However, compared to the more widely studied MHC-I in computational immunotherapy, the study of MHC-II antigenic epitope poses significantly more challenges due to its complex binding specificity and ambiguous motif patterns. Consequently, existing datasets for MHC-II interactions are smaller and less standardized than those available for MHC-I. To address these challenges, we present a well-curated dataset derived from the Immune Epitope Database (IEDB) and other public sources. It not only extends and standardizes existing peptide-MHC-II datasets, but also introduces a novel antigen-MHC-II dataset with richer biological context. Leveraging this dataset, we formulate three major machine learning (ML) tasks of peptide binding, peptide presentation, and antigen presentation, which progressively capture the broader biological processes within the MHC-II antigen presentation pathway. We further employ a multi-scale evaluation framework to benchmark existing models, along with a comprehensive analysis over various modeling designs to this problem with a modular framework. Overall, this work serves as a valuable resource for advancing computational immunotherapy, providing a foundation for future research in ML guided epitope discovery and predictive modeling of immune responses.",
            "categories": [
                "cs.LG",
                "q-bio.QM"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14011v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "世界模型",
                    "matched_keywords": [
                        "predictive model"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出多尺度预测框架以加速MHC-II抗原呈递中的表位发现，解决数据稀缺和建模复杂性挑战。",
            "summary_zh": "主要组织相容性复合体II（MHC-II）蛋白呈递的抗原表位在免疫治疗中至关重要。然而，与计算免疫治疗中更广泛研究的MHC-I相比，MHC-II抗原表位的研究因其复杂的结合特异性和模糊的基序模式而面临更多挑战。因此，现有的MHC-II相互作用数据集比MHC-I的数据集更小且标准化程度更低。为应对这些挑战，我们提出了一个从免疫表位数据库（IEDB）和其他公共来源精心整理的数据集。它不仅扩展和标准化了现有的肽-MHC-II数据集，还引入了一个具有更丰富生物学背景的新型抗原-MHC-II数据集。利用此数据集，我们制定了肽结合、肽呈递和抗原呈递三个主要机器学习任务，逐步捕捉MHC-II抗原呈递途径中更广泛的生物过程。我们进一步采用多尺度评估框架对现有模型进行基准测试，并通过模块化框架对该问题的各种建模设计进行全面分析。总体而言，这项工作为推进计算免疫治疗提供了宝贵资源，为未来机器学习指导的表位发现和免疫反应预测建模研究奠定了基础。",
            "intro_zh": [
                "核心问题：MHC-II表位研究面临数据稀缺、标准化不足和复杂结合特异性等挑战，现有数据集较小且建模困难。",
                "方法要点：构建标准化数据集，定义多尺度机器学习任务，采用模块化框架进行模型评估和设计分析。",
                "实验或效果：通过多尺度评估框架，系统比较现有模型性能，为后续研究提供基准和指导。"
            ],
            "method_zh": "**问题定义**：论文旨在解决MHC-II抗原呈递中表位发现的挑战，包括数据稀缺、标准化不足以及复杂生物过程建模困难。现有方法通常依赖有限且非标准化的数据集，导致模型泛化能力差，难以准确预测肽结合、呈递和抗原呈递过程。\\n\\n**核心思路**：通过构建高质量、标准化的数据集，并定义多尺度机器学习任务，逐步建模从肽结合到抗原呈递的完整生物过程，以提升预测准确性和生物学相关性。\\n\\n**技术框架**：整体架构包括数据收集与整理、任务定义、模型评估和设计分析四个阶段。首先，从IEDB等公共来源整理肽-MHC-II和抗原-MHC-II数据集；其次，定义肽结合、肽呈递和抗原呈递三个任务；然后，使用多尺度评估框架对现有模型进行基准测试；最后，通过模块化框架分析不同建模设计。\\n\\n**关键创新**：最重要的技术创新是引入多尺度预测框架和新型抗原-MHC-II数据集，将表位发现从单一肽结合扩展到更广泛的抗原呈递过程，增强了模型的生物学解释性和实用性。\\n\\n**关键设计**：数据集基于IEDB等公共来源进行标准化处理；多尺度评估框架涵盖不同任务和指标；模块化框架允许灵活集成各种机器学习模型，如深度学习或传统方法，具体参数和损失函数根据任务需求定制，例如使用交叉熵损失进行分类任务。",
            "application_zh": "该研究在计算免疫治疗领域具有广泛应用潜力，可用于加速疫苗设计、个性化免疫疗法开发和自身免疫疾病研究。通过提供标准化数据集和多尺度预测框架，它支持更准确的表位发现和免疫反应建模，有望推动精准医疗和药物研发的进展。",
            "highlight_zh": "实验亮点包括构建了扩展的肽-MHC-II数据集和新型抗原-MHC-II数据集，通过多尺度评估框架系统比较了现有模型性能。具体性能数据未在摘要中提供，但基准测试显示了模型在不同任务上的相对优劣，为后续研究提供了重要参考。提升幅度取决于具体模型和任务，但整体框架增强了预测的全面性和可靠性。",
            "tags_zh": [
                "MHC-II表位发现",
                "多尺度预测",
                "抗原呈递",
                "计算免疫治疗",
                "机器学习任务",
                "数据集标准化",
                "模块化框架",
                "免疫反应建模"
            ],
            "_index": 178
        },
        {
            "title": "Physics-Informed Machine Learning for Two-Phase Moving-Interface and Stefan Problems",
            "authors": [
                "Che-Chia Chang",
                "Te-Sheng Lin",
                "Ming-Chih Lai"
            ],
            "arxiv_id": "2512.14010v1",
            "summary": "The Stefan problem is a classical free-boundary problem that models phase-change processes and poses computational challenges due to its moving interface and nonlinear temperature-phase coupling. In this work, we develop a physics-informed neural network framework for solving two-phase Stefan problems. The proposed method explicitly tracks the interface motion and enforces the discontinuity in the temperature gradient across the interface while maintaining global consistency of the temperature field. Our approach employs two neural networks: one representing the moving interface and the other for the temperature field. The interface network allows rapid categorization of thermal diffusivity in the spatial domain, which is a crucial step for selecting training points for the temperature network. The temperature network's input is augmented with a modified zero-level set function to accurately capture the jump in its normal derivative across the interface. Numerical experiments on two-phase dynamical Stefan problems demonstrate the superior accuracy and effectiveness of our proposed method compared with the ones obtained by other neural network methodology in literature. The results indicate that the proposed framework offers a robust and flexible alternative to traditional numerical methods for solving phase-change problems governed by moving boundaries. In addition, the proposed method can capture an unstable interface evolution associated with the Mullins-Sekerka instability.",
            "categories": [
                "physics.comp-ph",
                "cs.LG"
            ],
            "primary_category": "physics.comp-ph",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14010v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计",
                    "matched_keywords": [
                        "VO"
                    ],
                    "score": 1
                }
            ],
            "relevance_score": 1,
            "headline_zh": "提出基于物理信息神经网络的框架以解决两相Stefan问题中的移动界面和温度梯度不连续性挑战。",
            "summary_zh": "Stefan问题是一个经典的相变过程自由边界问题，因其移动界面和非线性温度-相耦合而带来计算挑战。本文开发了一个基于物理信息的神经网络框架来解决两相Stefan问题。该方法显式跟踪界面运动，并在保持温度场全局一致性的同时，强制界面处温度梯度的不连续性。我们的方法采用两个神经网络：一个表示移动界面，另一个用于温度场。界面网络允许在空间域中快速分类热扩散率，这是为温度网络选择训练点的关键步骤。温度网络的输入通过修改的零水平集函数增强，以准确捕捉界面处法向导数的跳跃。在两相动态Stefan问题上的数值实验表明，与文献中其他神经网络方法相比，我们提出的方法具有更高的准确性和有效性。结果表明，该框架为解决由移动边界控制的相变问题提供了一个鲁棒且灵活的替代传统数值方法的选择。此外，该方法能够捕捉与Mullins-Sekerka不稳定性相关的不稳定界面演化。",
            "intro_zh": [
                "Stefan问题作为经典自由边界问题，建模相变过程时面临移动界面和非线性耦合的计算挑战，传统数值方法处理不连续性和界面演化时效率较低。",
                "论文提出使用两个神经网络分别表示移动界面和温度场，通过物理信息约束显式跟踪界面并强制梯度不连续性，实现全局一致性。",
                "数值实验显示，该方法在准确性和有效性上优于现有神经网络方法，并能捕捉不稳定界面演化，为相变问题提供鲁棒解决方案。"
            ],
            "method_zh": "**问题定义**：论文旨在解决两相Stefan问题，这是一个描述相变过程的自由边界问题，涉及移动界面和温度场之间的非线性耦合。现有方法（如传统数值方法）在处理界面不连续性和动态演化时面临计算复杂度高、精度不足的挑战，而现有神经网络方法往往难以有效捕捉界面处的梯度跳跃和全局一致性。\\n\\n**核心思路**：论文的核心思路是结合物理信息神经网络，通过显式建模移动界面和温度场，利用神经网络的自适应能力来跟踪界面运动并强制界面处的温度梯度不连续性，从而在保持全局一致性的同时提高计算效率和准确性。这种设计基于物理约束，避免了传统方法中的网格依赖性和数值不稳定性。\\n\\n**技术框架**：整体架构包括两个主要模块：界面网络和温度网络。界面网络用于表示移动界面，通过零水平集函数定义界面位置，并快速分类空间域中的热扩散率，以指导训练点选择。温度网络则负责预测温度场，其输入通过修改的零水平集函数增强，以准确捕捉界面处的法向导数跳跃。训练过程通过物理信息损失函数（如热传导方程和界面条件）来优化两个网络，确保物理一致性。\\n\\n**关键创新**：最重要的技术创新是使用两个神经网络分别处理界面和温度场，并引入修改的零水平集函数来增强温度网络的输入，从而显式跟踪界面运动并强制梯度不连续性。与现有方法相比，本质区别在于该方法能够同时处理移动边界和温度场的不连续性，避免了传统数值方法的网格限制和现有神经网络方法的近似误差。\\n\\n**关键设计**：关键设计包括：界面网络采用全连接神经网络，输出为零水平集函数值；温度网络输入为空间坐标和修改的零水平集函数，输出为温度值；损失函数结合热传导方程的残差、界面处的跳跃条件以及边界条件，通过梯度下降优化；参数设置如网络层数、激活函数和训练点选择基于热扩散率分类，具体细节在论文中未详细说明，但强调自适应性和物理约束。",
            "application_zh": "该研究在相变过程建模领域具有广泛潜在应用，如材料科学中的凝固和熔化模拟、能源系统中的热管理、以及生物医学中的组织冷冻治疗。实际价值在于提供一种鲁棒且灵活的数值方法替代方案，能够高效处理移动边界问题，未来可能推动多物理场耦合模拟和工业优化设计的发展。",
            "highlight_zh": "最重要的实验结果显示，在两相动态Stefan问题上，该方法在准确性上显著优于文献中其他神经网络方法，具体性能数据未在摘要中提供，但强调了“superior accuracy and effectiveness”。此外，该方法成功捕捉了与Mullins-Sekerka不稳定性相关的不稳定界面演化，验证了其处理复杂界面动态的能力，提升幅度体现在鲁棒性和灵活性方面。",
            "tags_zh": [
                "物理信息神经网络",
                "Stefan问题",
                "移动界面",
                "两相流",
                "温度梯度不连续性",
                "相变模拟",
                "自由边界问题",
                "Mullins-Sekerka不稳定性"
            ],
            "_index": 179
        }
    ]
}