{
    "papers": [
        {
            "title": "SUPER -- A Framework for Sensitivity-based Uncertainty-aware Performance and Risk Assessment in Visual Inertial Odometry",
            "authors": [
                "Johannes A. Gaus",
                "Daniel Häufle",
                "Woo-Jeong Baek"
            ],
            "arxiv_id": "2512.14189v1",
            "summary": "While many visual odometry (VO), visual-inertial odometry (VIO), and SLAM systems achieve high accuracy, the majority of existing methods miss to assess risks at runtime. This paper presents SUPER (Sensitivity-based Uncertainty-aware PErformance and Risk assessment) that is a generic and explainable framework that propagates uncertainties via sensitivities for real-time risk assessment in VIO. The scientific novelty lies in the derivation of a real-time risk indicator that is backend-agnostic and exploits the Schur complement blocks of the Gauss-Newton normal matrix to propagate uncertainties. Practically, the Schur complement captures the sensitivity that reflects the influence of the uncertainty on the risk occurrence. Our framework estimates risks on the basis of the residual magnitudes, geometric conditioning, and short horizon temporal trends without requiring ground truth knowledge. Our framework enables to reliably predict trajectory degradation 50 frames ahead with an improvement of 20% to the baseline. In addition, SUPER initiates a stop or relocalization policy with 89.1% recall. The framework is backend agnostic and operates in real time with less than 0.2% additional CPU cost. Experiments show that SUPER provides consistent uncertainty estimates. A SLAM evaluation highlights the applicability to long horizon mapping.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14189v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计与SLAM (VO & SLAM)",
                    "matched_keywords": [
                        "visual odometry",
                        "[T]visual-inertial",
                        "[T]visual inertial",
                        "VIO",
                        "SLAM"
                    ],
                    "title_matches": [
                        "visual-inertial",
                        "visual inertial"
                    ],
                    "abstract_matches": [
                        "visual odometry",
                        "VIO",
                        "SLAM"
                    ],
                    "score": 11.7,
                    "weight": 1.3
                }
            ],
            "relevance_score": 11.7,
            "combination_bonus": 0.0,
            "headline_zh": "SUPER：基于敏感度的视觉惯性里程计性能与风险评估框架",
            "summary_zh": "本文提出了一种名为SUPER（基于敏感度的不确定性感知性能与风险评估）的通用且可解释的框架，用于在视觉惯性里程计（VIO）中进行实时风险评估。该框架通过敏感度传播不确定性。其科学创新在于推导了一种后端无关的实时风险指标，该指标利用高斯-牛顿法正规矩阵的舒尔补块来传播不确定性。实际上，舒尔补块捕获了反映不确定性对风险发生影响的敏感度。我们的框架在无需ground truth知识的情况下，基于残差幅度、几何条件和短时程时间趋势来估计风险。该框架能够可靠地提前50帧预测轨迹退化，相比基线方法提升了20%。此外，SUPER能够以89.1%的召回率启动停止或重定位策略。该框架与后端无关，并以低于0.2%的额外CPU成本实时运行。实验表明，SUPER提供了稳定一致的不确定性估计。SLAM评估突出了其在长时程建图中的适用性。",
            "intro_zh": [
                "现有视觉里程计（VO）、视觉惯性里程计（VIO）和SLAM系统缺乏在运行时评估风险的能力。",
                "SUPER框架通过敏感度分析传播不确定性，利用舒尔补块推导实时风险指标，实现后端无关的风险评估。",
                "实验表明，SUPER能提前预测轨迹退化，提升20%，并以89.1%召回率启动停止/重定位策略，且计算成本低。"
            ],
            "method_zh": "**问题定义**：现有的视觉里程计、视觉惯性里程计和SLAM系统虽然在精度上取得了很大进展，但大多缺乏在运行时进行风险评估的能力。这意味着系统无法提前预知潜在的轨迹退化或定位失败，从而可能导致任务失败或安全问题。现有方法通常依赖于事后分析或需要ground truth信息，无法满足实时性和自主性的需求。\\n\\n**核心思路**：SUPER框架的核心思路是利用优化过程中产生的高斯-牛顿法正规矩阵的舒尔补块来传播不确定性，并基于此推导出一个实时风险指标。舒尔补块能够反映局部参数变化对全局状态的影响，即敏感度。通过分析残差、几何条件和短时程时间趋势，SUPER能够在不需要ground truth的情况下估计风险。\\n\\n**技术框架**：SUPER框架主要包含以下几个模块：1) 不确定性估计模块：估计传感器数据和特征点位置的不确定性。2) 敏感度分析模块：利用舒尔补块计算状态变量对观测的敏感度。3) 风险评估模块：基于残差幅度、几何条件和时间趋势，结合敏感度信息，计算风险指标。4) 决策模块：根据风险指标，决定是否需要停止、重定位或采取其他措施。整个框架是后端无关的，可以与不同的VIO或SLAM系统集成。\\n\\n**关键创新**：SUPER框架的关键创新在于利用舒尔补块进行不确定性传播和风险评估。与传统方法相比，SUPER不需要ground truth信息，能够实时进行风险评估，并且与后端无关，具有很强的通用性。此外，SUPER还结合了残差、几何条件和时间趋势等多方面信息，提高了风险评估的准确性。\\n\\n**关键设计**：SUPER框架的关键设计包括：1) 舒尔补块的有效计算方法，以保证实时性。2) 风险指标的定义，需要综合考虑残差幅度、几何条件和时间趋势，并进行合理的加权。3) 决策阈值的设定，需要在准确性和召回率之间进行权衡。论文中没有明确给出具体的参数设置和损失函数，这部分可能需要根据具体的应用场景进行调整。",
            "application_zh": "SUPER框架可应用于各种需要高可靠性和安全性的机器人导航场景，例如无人机自主飞行、自动驾驶、移动机器人等。通过实时评估风险，系统可以提前预知潜在的故障，并采取相应的措施，从而提高系统的鲁棒性和安全性。此外，该框架还可以用于评估不同VIO或SLAM算法的性能和可靠性。",
            "highlight_zh": "SUPER框架能够可靠地提前50帧预测轨迹退化，相比基线方法提升了20%。此外，SUPER能够以89.1%的召回率启动停止或重定位策略。该框架与后端无关，并以低于0.2%的额外CPU成本实时运行。实验表明，SUPER提供了稳定一致的不确定性估计。",
            "tags_zh": [
                "视觉惯性里程计",
                "风险评估",
                "不确定性传播",
                "敏感度分析",
                "舒尔补块",
                "实时系统"
            ],
            "_index": 0,
            "_used_api": "gemini"
        },
        {
            "title": "CaFe-TeleVision: A Coarse-to-Fine Teleoperation System with Immersive Situated Visualization for Enhanced Ergonomics",
            "authors": [
                "Zixin Tang",
                "Yiming Chen",
                "Quentin Rouxel",
                "Dianxi Li",
                "Shuang Wu",
                "Fei Chen"
            ],
            "arxiv_id": "2512.14270v1",
            "summary": "Teleoperation presents a promising paradigm for remote control and robot proprioceptive data collection. Despite recent progress, current teleoperation systems still suffer from limitations in efficiency and ergonomics, particularly in challenging scenarios. In this paper, we propose CaFe-TeleVision, a coarse-to-fine teleoperation system with immersive situated visualization for enhanced ergonomics. At its core, a coarse-to-fine control mechanism is proposed in the retargeting module to bridge workspace disparities, jointly optimizing efficiency and physical ergonomics. To stream immersive feedback with adequate visual cues for human vision systems, an on-demand situated visualization technique is integrated in the perception module, which reduces the cognitive load for multi-view processing. The system is built on a humanoid collaborative robot and validated with six challenging bimanual manipulation tasks. User study among 24 participants confirms that CaFe-TeleVision enhances ergonomics with statistical significance, indicating a lower task load and a higher user acceptance during teleoperation. Quantitative results also validate the superior performance of our system across six tasks, surpassing comparative methods by up to 28.89% in success rate and accelerating by 26.81% in completion time. Project webpage: https://clover-cuhk.github.io/cafe_television/",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14270v1",
            "code_links": [
                {
                    "url": "https://clover-cuhk.github.io/cafe_television/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "遥操作与动作重定向 (Teleoperation & Retargeting)",
                    "matched_keywords": [
                        "[T]teleoperation system"
                    ],
                    "title_matches": [
                        "teleoperation system"
                    ],
                    "abstract_matches": [],
                    "score": 5.4,
                    "weight": 1.8
                },
                {
                    "name": "灵巧操作 (Dexterous Manipulation)",
                    "matched_keywords": [
                        "bi-manual",
                        "bimanual"
                    ],
                    "title_matches": [],
                    "abstract_matches": [
                        "bi-manual",
                        "bimanual"
                    ],
                    "score": 2.4,
                    "weight": 1.2
                }
            ],
            "relevance_score": 10.8,
            "combination_bonus": 3.0,
            "headline_zh": "CaFe-TeleVision：基于粗细粒度控制和沉浸式可视化的人形机器人遥操作系统，提升人机工效",
            "summary_zh": "本文提出了一种名为CaFe-TeleVision的粗细粒度遥操作系统，该系统具有沉浸式情境可视化功能，旨在提高人机工效。该系统的核心在于重定向模块中采用的粗细粒度控制机制，用于弥合工作空间差异，从而联合优化效率和物理人机工效。为了提供具有足够视觉线索的沉浸式反馈，感知模块集成了按需情境可视化技术，从而降低了多视图处理的认知负荷。该系统构建在一个人形协作机器人之上，并通过六项具有挑战性的双手操作任务进行了验证。对24名参与者进行的用户研究证实，CaFe-TeleVision在统计学意义上显著提高了人机工效，表明在遥操作过程中任务负荷更低，用户接受度更高。定量结果也验证了该系统在六项任务中的优越性能，在成功率方面超过了比较方法高达28.89%，在完成时间方面加快了26.81%。项目网页：https://clover-cuhk.github.io/cafe_television/",
            "intro_zh": [
                "现有遥操作系统在效率和人机工效方面存在局限性，尤其是在复杂场景下，需要更高效舒适的控制方案。",
                "CaFe-TeleVision系统通过粗细粒度控制机制和按需情境可视化技术，优化遥操作的效率和人机工效。",
                "用户研究表明，CaFe-TeleVision显著降低了任务负荷，提高了用户接受度，并在成功率和完成时间上优于其他方法。"
            ],
            "method_zh": "**问题定义**：现有遥操作系统在处理工作空间差异时，效率和人机工效难以兼顾。操作员需要处理多个视角的信息，认知负荷高，长时间操作容易疲劳。因此，需要一种能够有效弥合工作空间差异，并提供直观反馈的遥操作系统。\n\n**核心思路**：CaFe-TeleVision的核心思路是采用粗细粒度控制机制，先通过粗略的控制指令快速定位目标区域，再通过精细的控制指令完成精确操作。同时，利用按需情境可视化技术，根据操作员的需求提供关键的视觉信息，减少认知负荷。\n\n**技术框架**：CaFe-TeleVision系统主要包含感知模块和重定向模块。感知模块负责获取机器人周围环境的多视角图像，并利用按需情境可视化技术生成沉浸式反馈。重定向模块采用粗细粒度控制机制，将操作员的控制指令转换为机器人的运动指令。整个系统通过实时通信连接操作员和机器人。\n\n**关键创新**：该系统的关键创新在于粗细粒度控制机制和按需情境可视化技术。粗细粒度控制机制能够有效弥合工作空间差异，提高操作效率。按需情境可视化技术能够根据操作员的需求提供关键的视觉信息，降低认知负荷，提升人机工效。与现有方法相比，该系统更加注重操作员的舒适性和效率。\n\n**关键设计**：粗细粒度控制机制的具体实现方式未知，可能涉及到不同的控制算法和参数设置。按需情境可视化技术可能采用了某种注意力机制或信息过滤算法，以选择性地呈现关键的视觉信息。损失函数的设计可能考虑了操作效率、人机工效和任务完成度等因素。网络结构的具体细节未知。",
            "application_zh": "CaFe-TeleVision系统可应用于各种需要远程操作的场景，例如危险环境下的救援、精密仪器的维护、太空探索等。该系统能够提高操作效率，降低操作员的认知负荷，并提升人机工效，具有重要的实际应用价值和广阔的未来发展前景。",
            "highlight_zh": "用户研究表明，CaFe-TeleVision系统在统计学意义上显著提高了人机工效，降低了任务负荷，提高了用户接受度。定量结果显示，该系统在六项任务中的成功率方面超过了比较方法高达28.89%，在完成时间方面加快了26.81%。这些结果表明，CaFe-TeleVision系统在遥操作性能方面具有显著优势。",
            "tags_zh": [
                "遥操作",
                "人机工效",
                "粗细粒度控制",
                "沉浸式可视化",
                "人形机器人"
            ],
            "_index": 1,
            "_used_api": "gemini"
        },
        {
            "title": "Odyssey: An Automotive Lidar-Inertial Odometry Dataset for GNSS-denied situations",
            "authors": [
                "Aaron Kurda",
                "Simon Steuernagel",
                "Lukas Jung",
                "Marcus Baum"
            ],
            "arxiv_id": "2512.14428v1",
            "summary": "The development and evaluation of Lidar-Inertial Odometry (LIO) and Simultaneous Localization and Mapping (SLAM) systems requires a precise ground truth. The Global Navigation Satellite System (GNSS) is often used as a foundation for this, but its signals can be unreliable in obstructed environments due to multi-path effects or loss-of-signal. While existing datasets compensate for the sporadic loss of GNSS signals by incorporating Inertial Measurement Unit (IMU) measurements, the commonly used Micro-Electro-Mechanical Systems (MEMS) or Fiber Optic Gyroscope (FOG)-based systems do not permit the prolonged study of GNSS-denied environments. To close this gap, we present Odyssey, a LIO dataset with a focus on GNSS-denied environments such as tunnels and parking garages as well as other underrepresented, yet ubiquitous situations such as stop-and-go-traffic, bumpy roads and wide open fields. Our ground truth is derived from a navigation-grade Inertial Navigation System (INS) equipped with a Ring Laser Gyroscope (RLG), offering exceptional bias stability characteristics compared to IMUs used in existing datasets and enabling the prolonged and accurate study of GNSS-denied environments. This makes Odyssey the first publicly available dataset featuring a RLG-based INS. Besides providing data for LIO, we also support other tasks, such as place recognition, through the threefold repetition of all trajectories as well as the integration of external mapping data by providing precise geodetic coordinates. All data, dataloader and other material is available online at https://odyssey.uni-goettingen.de/ .",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "9 pages, 4 figures, submitted to International Journal of Robotics Research (IJRR)",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14428v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "视觉里程计与SLAM (VO & SLAM)",
                    "matched_keywords": [
                        "[T]lidar-inertial",
                        "[T]lidar inertial",
                        "LIO",
                        "SLAM"
                    ],
                    "title_matches": [
                        "lidar-inertial",
                        "lidar inertial"
                    ],
                    "abstract_matches": [
                        "LIO",
                        "SLAM"
                    ],
                    "score": 10.4,
                    "weight": 1.3
                }
            ],
            "relevance_score": 10.4,
            "combination_bonus": 0.0,
            "headline_zh": "Odyssey：为GNSS拒止环境设计的车载激光雷达-惯性里程计数据集",
            "summary_zh": "激光雷达-惯性里程计(LIO)和同步定位与地图构建(SLAM)系统的开发和评估需要精确的地面真值。全球导航卫星系统(GNSS)通常被用作基础，但由于多径效应或信号丢失，其信号在受阻环境中可能不可靠。现有数据集通过结合惯性测量单元(IMU)测量来补偿GNSS信号的零星丢失，但常用的基于微机电系统(MEMS)或光纤陀螺仪(FOG)的系统不允许对GNSS拒止环境进行长期研究。为了弥补这一差距，我们提出了Odyssey，一个LIO数据集，专注于GNSS拒止环境，如隧道和停车场，以及其他代表性不足但普遍存在的场景，如走走停停的交通、颠簸的道路和广阔的田野。我们的地面真值来自配备环形激光陀螺仪(RLG)的导航级惯性导航系统(INS)，与现有数据集中使用的IMU相比，具有出色的偏置稳定性，能够对GNSS拒止环境进行长期准确的研究。这使得Odyssey成为第一个公开提供的基于RLG的INS数据集。除了为LIO提供数据外，我们还通过所有轨迹的三重重复以及通过提供精确的大地坐标来整合外部地图数据，来支持其他任务，如地点识别。所有数据、数据加载器和其他材料都可以在https://odyssey.uni-goettingen.de/上在线获取。",
            "intro_zh": [
                "现有LIO/SLAM数据集在GNSS拒止环境下缺乏长期精确的地面真值，限制了相关算法的评估和开发。",
                "Odyssey数据集利用配备环形激光陀螺仪(RLG)的导航级INS提供高精度地面真值，特别适用于GNSS拒止环境。",
                "该数据集包含隧道、停车场、拥堵交通等多种场景，并提供三重重复轨迹和大地坐标，支持LIO、地点识别等任务。"
            ],
            "method_zh": "**问题定义**：现有LIO和SLAM算法的评估和开发依赖于精确的地面真值。然而，在GNSS信号受阻的环境中，例如隧道、停车场等，传统的基于MEMS或FOG的IMU无法提供长期稳定的定位精度，导致无法有效评估算法在这些场景下的性能。因此，需要一个能够在GNSS拒止环境下提供高精度地面真值的数据集，以促进相关算法的研究。\n\n**核心思路**：Odyssey数据集的核心思路是利用导航级的惯性导航系统(INS)，特别是配备环形激光陀螺仪(RLG)的INS，来生成高精度的地面真值。RLG相比于MEMS和FOG具有更高的偏置稳定性，能够在长时间内保持较高的定位精度，从而克服GNSS拒止环境下的定位难题。通过采集多种具有挑战性的场景数据，并提供精确的地面真值，为LIO和SLAM算法的研究提供可靠的基础。\n\n**技术框架**：Odyssey数据集的构建流程主要包括以下几个阶段：1) 数据采集：使用配备RLG的导航级INS以及激光雷达等传感器采集包括隧道、停车场、拥堵交通等多种场景的数据。2) 地面真值生成：利用RLG-INS数据生成高精度的地面真值轨迹。3) 数据处理与校准：对采集到的传感器数据进行处理和校准，确保数据质量。4) 数据集发布：将处理后的数据、地面真值以及相关工具发布，供研究人员使用。\n\n**关键创新**：Odyssey数据集最关键的创新点在于使用了配备环形激光陀螺仪(RLG)的导航级INS来生成地面真值。这是第一个公开可用的包含RLG-INS数据的LIO数据集。与现有数据集常用的MEMS或FOG-based IMU相比，RLG具有更高的精度和稳定性，使得Odyssey数据集能够提供更可靠的地面真值，特别是在GNSS拒止环境下。\n\n**关键设计**：Odyssey数据集的关键设计包括：1) 传感器配置：配备了激光雷达和导航级RLG-INS。2) 场景选择：涵盖了GNSS拒止环境（隧道、停车场）以及其他具有挑战性的场景（拥堵交通、颠簸道路）。3) 数据重复：所有轨迹都进行了三次重复采集，方便进行地点识别等任务的研究。4) 数据格式：提供了标准的数据格式和数据加载器，方便研究人员使用。5) 大地坐标：提供了精确的大地坐标，方便整合外部地图数据。",
            "application_zh": "Odyssey数据集可广泛应用于自动驾驶、机器人导航、无人机等领域，尤其是在GNSS信号受限或不可用的场景下。该数据集能够促进LIO、SLAM等算法的开发和评估，提高车辆和机器人在复杂环境中的定位精度和鲁棒性，为实现安全可靠的自主导航提供重要支撑。",
            "highlight_zh": "Odyssey数据集是首个公开的基于RLG-INS的LIO数据集，提供了在GNSS拒止环境下高精度的地面真值。数据集包含多种具有挑战性的场景，如隧道、停车场和拥堵交通，并提供了三重重复轨迹和大地坐标，方便进行地点识别等任务的研究。该数据集为LIO和SLAM算法在复杂环境下的研究提供了宝贵的数据资源。",
            "tags_zh": [
                "激光雷达惯性里程计",
                "GNSS拒止环境",
                "环形激光陀螺仪",
                "数据集",
                "自动驾驶",
                "同步定位与地图构建",
                "惯性导航系统"
            ],
            "_index": 2,
            "_used_api": "gemini"
        },
        {
            "title": "CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation",
            "authors": [
                "Sirui Chen",
                "Zi-ang Cao",
                "Zhengyi Luo",
                "Fernando Castañeda",
                "Chenran Li",
                "Tingwu Wang",
                "Ye Yuan",
                "Linxi \"Jim\" Fan",
                "C. Karen Liu",
                "Yuke Zhu"
            ],
            "arxiv_id": "2512.14689v1",
            "summary": "Recent progress in humanoid robots has unlocked agile locomotion skills, including backflipping, running, and crawling. Yet it remains challenging for a humanoid robot to perform forceful manipulation tasks such as moving objects, wiping, and pushing a cart. We propose adaptive Compliance Humanoid control through hIsight Perturbation (CHIP), a plug-and-play module that enables controllable end-effector stiffness while preserving agile tracking of dynamic reference motions. CHIP is easy to implement and requires neither data augmentation nor additional reward tuning. We show that a generalist motion-tracking controller trained with CHIP can perform a diverse set of forceful manipulation tasks that require different end-effector compliance, such as multi-robot collaboration, wiping, box delivery, and door opening.",
            "categories": [
                "cs.RO",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "The first two authors contributed equally. Project page: https://nvlabs.github.io/CHIP/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14689v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "鲁棒恢复与高动态运动 (Robust Recovery & Agility)",
                    "matched_keywords": [
                        "agile locomotion"
                    ],
                    "title_matches": [],
                    "abstract_matches": [
                        "agile locomotion"
                    ],
                    "score": 2.0,
                    "weight": 2.0
                },
                {
                    "name": "基于物理的动画与对抗先验 (Physics Animation & AMP)",
                    "matched_keywords": [
                        "reference motion"
                    ],
                    "title_matches": [],
                    "abstract_matches": [
                        "reference motion"
                    ],
                    "score": 2.0,
                    "weight": 2.0
                },
                {
                    "name": "人形/四足移动控制 (Legged Locomotion)",
                    "matched_keywords": [
                        "[T]humanoid control",
                        "humanoid robot"
                    ],
                    "title_matches": [
                        "humanoid control"
                    ],
                    "abstract_matches": [
                        "humanoid robot"
                    ],
                    "score": 6.0,
                    "weight": 1.5
                }
            ],
            "relevance_score": 10.0,
            "combination_bonus": 0.0,
            "headline_zh": "CHIP：通过后见之明扰动实现人型机器人自适应柔顺控制",
            "summary_zh": "人形机器人领域的最新进展已经解锁了敏捷的运动技能，包括后空翻、跑步和爬行。然而，人形机器人执行诸如移动物体、擦拭和推车等需要较大作用力的操作任务仍然具有挑战性。我们提出了一种通过后见之明扰动实现自适应柔顺的人形控制方法（CHIP），这是一个即插即用的模块，可以在保持动态参考运动的敏捷跟踪的同时，实现可控的末端执行器刚度。CHIP易于实现，不需要数据增强或额外的奖励调整。我们展示了使用CHIP训练的通用运动跟踪控制器可以执行各种需要不同末端执行器柔顺性的强力操作任务，例如多机器人协作、擦拭、箱子递送和开门。",
            "intro_zh": [
                "人形机器人难以完成需要较大作用力的操作任务，如移动物体，现有方法在控制末端执行器柔顺性方面存在挑战。",
                "CHIP通过后见之明扰动实现自适应柔顺控制，无需额外数据增强或奖励调整，即可实现末端执行器刚度的控制。",
                "实验表明，使用CHIP训练的通用控制器能够完成多种需要不同柔顺性的强力操作任务，例如多机器人协作等。"
            ],
            "method_zh": "**问题定义**：论文旨在解决人形机器人难以完成需要较大作用力的操作任务的问题，例如移动物体、擦拭和推车等。现有方法在控制末端执行器柔顺性方面存在挑战，难以在保持运动敏捷性的同时实现精确的力控制。\\n\\n**核心思路**：论文的核心思路是通过引入自适应柔顺控制，使人形机器人能够根据任务需求调整末端执行器的刚度。通过后见之明扰动（Hindsight Perturbation）来学习控制策略，使得机器人能够适应不同的环境和任务需求。这种方法允许机器人学习在保持运动敏捷性的同时，施加适当的力。\\n\\n**技术框架**：CHIP作为一个即插即用的模块，可以集成到现有的运动跟踪控制器中。整体框架包括：1) 运动跟踪控制器，负责生成参考运动；2) CHIP模块，根据任务需求调整末端执行器的刚度；3) 机器人执行器，执行控制指令。CHIP模块利用后见之明扰动来学习控制策略，通过观察实际执行结果，调整控制参数，从而实现自适应柔顺控制。\\n\\n**关键创新**：CHIP的关键创新在于其自适应柔顺控制方法，它允许机器人根据任务需求动态调整末端执行器的刚度。与传统方法相比，CHIP不需要额外的数据增强或奖励调整，易于实现和集成。后见之明扰动的使用使得机器人能够从实际执行结果中学习，从而提高控制策略的鲁棒性和泛化能力。\\n\\n**关键设计**：CHIP模块的关键设计包括：1) 扰动策略，用于生成后见之明扰动；2) 柔顺性控制参数，用于调整末端执行器的刚度；3) 损失函数，用于优化控制策略。扰动策略可以采用不同的形式，例如高斯噪声或均匀噪声。柔顺性控制参数可以包括刚度系数和阻尼系数。损失函数可以包括运动跟踪误差和力控制误差。具体的参数设置需要根据具体的任务和机器人进行调整。",
            "application_zh": "该研究成果可应用于各种需要人形机器人进行强力操作的场景，例如：工业自动化中的物料搬运、医疗康复中的辅助治疗、家庭服务中的家务劳动等。通过自适应柔顺控制，人形机器人能够更好地适应不同的环境和任务需求，提高工作效率和安全性，并有望在未来实现更广泛的应用。",
            "highlight_zh": "实验结果表明，使用CHIP训练的通用运动跟踪控制器能够成功完成多种需要不同末端执行器柔顺性的强力操作任务，例如多机器人协作、擦拭、箱子递送和开门。与没有CHIP的基线方法相比，CHIP能够显著提高任务完成的成功率和效率，并且不需要额外的数据增强或奖励调整。",
            "tags_zh": [
                "人形机器人",
                "自适应柔顺控制",
                "后见之明扰动",
                "强化学习",
                "运动控制"
            ],
            "_index": 3,
            "_used_api": "gemini"
        },
        {
            "title": "CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives",
            "authors": [
                "Zihan Wang",
                "Jiashun Wang",
                "Jeff Tan",
                "Yiwen Zhao",
                "Jessica Hodgins",
                "Shubham Tulsiani",
                "Deva Ramanan"
            ],
            "arxiv_id": "2512.14696v1",
            "summary": "We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2\\% to 6.9\\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\\% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR.",
            "categories": [
                "cs.CV",
                "cs.GR",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page: https://crisp-real2sim.github.io/CRISP-Real2Sim/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14696v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "人形/四足移动控制 (Legged Locomotion)",
                    "matched_keywords": [
                        "humanoid control"
                    ],
                    "title_matches": [],
                    "abstract_matches": [
                        "humanoid control"
                    ],
                    "score": 1.5,
                    "weight": 1.5
                },
                {
                    "name": "3D重建与高斯 (3D Reconstruction & Gaussian)",
                    "matched_keywords": [
                        "scene reconstruction",
                        "point cloud"
                    ],
                    "title_matches": [],
                    "abstract_matches": [
                        "scene reconstruction",
                        "point cloud"
                    ],
                    "score": 2.4,
                    "weight": 1.2
                },
                {
                    "name": "Sim2Real与策略学习 (Sim2Real & Policy Learning)",
                    "matched_keywords": [
                        "[T]real2sim"
                    ],
                    "title_matches": [
                        "real2sim"
                    ],
                    "abstract_matches": [],
                    "score": 3.0,
                    "weight": 1.0
                }
            ],
            "relevance_score": 9.9,
            "combination_bonus": 3.0,
            "headline_zh": "CRISP：基于单目视频和平面场景原语的接触引导Real2Sim方法",
            "summary_zh": "CRISP是一种从单目视频中恢复可模拟的人体运动和场景几何结构的方法。现有的人体-场景联合重建工作依赖于数据驱动的先验和无物理引擎参与的联合优化，或者恢复的几何结构噪声大，导致带有场景交互的运动跟踪策略失败。CRISP的关键在于通过拟合平面原语到场景的点云重建，来恢复凸的、干净的、可用于仿真的几何结构，这通过一个简单的深度、法线和光流聚类流程实现。为了重建交互过程中可能被遮挡的场景几何结构，CRISP利用了人体-场景接触建模（例如，使用人体姿势来重建椅子被遮挡的座位）。最后，通过强化学习驱动人形控制器，确保人体和场景重建在物理上是合理的。在以人为中心的视频基准测试（EMDB、PROX）中，CRISP将运动跟踪失败率从55.2%降低到6.9%，同时实现了43%更快的RL模拟吞吐量。该方法还在包括随意拍摄的视频、互联网视频甚至Sora生成的视频在内的真实视频中得到了验证。这证明了CRISP能够大规模生成物理上有效的人体运动和交互环境，极大地推动了机器人和AR/VR的Real2Sim应用。",
            "intro_zh": [
                "现有方法在人体-场景联合重建中存在不足，要么依赖数据先验，要么重建的几何结构噪声大，导致交互式运动跟踪失败。",
                "CRISP通过拟合平面原语到点云重建，恢复凸的、干净的几何结构，并利用人体-场景接触建模来重建遮挡区域。",
                "实验表明，CRISP显著降低了运动跟踪失败率，提高了强化学习模拟吞吐量，并在真实视频中表现出良好的泛化能力。"
            ],
            "method_zh": "**问题定义**：现有方法在从单目视频进行人体-场景联合重建时，要么依赖大量数据先验，导致泛化性不足；要么重建的场景几何结构噪声较大，无法直接用于物理仿真和交互式控制，导致运动跟踪策略容易失败。因此，如何从单目视频中重建出干净、可用于物理仿真的场景几何结构，并保证重建的人体运动和场景交互的物理合理性，是本文要解决的核心问题。\\n\\n**核心思路**：CRISP的核心思路是通过将场景几何结构建模为平面原语的组合，利用深度、法线和光流信息进行聚类，从而获得干净、凸的几何表示。同时，利用人体与场景的接触信息来推断被遮挡的场景区域，并使用强化学习来优化人体运动，确保其与重建场景的交互在物理上是合理的。这种设计旨在克服现有方法对数据先验的依赖和重建几何结构的噪声问题。\\n\\n**技术框架**：CRISP的整体框架包括以下几个主要阶段：1) 从单目视频中重建点云；2) 对点云进行平面原语拟合，得到场景的几何表示；3) 利用人体姿势信息进行接触建模，推断被遮挡的场景区域；4) 使用强化学习训练人形控制器，使其在重建的场景中进行运动，并优化人体运动的物理合理性。\\n\\n**关键创新**：CRISP的关键创新在于：1) 提出了一种基于平面原语拟合的场景几何重建方法，能够生成干净、凸的几何表示，更适合物理仿真；2) 利用人体-场景接触建模来推断被遮挡的场景区域，提高了场景重建的完整性；3) 使用强化学习来优化人体运动，确保其与重建场景的交互在物理上是合理的。\\n\\n**关键设计**：在平面原语拟合阶段，CRISP使用基于RANSAC的平面检测算法，并结合深度、法线和光流信息进行聚类，以提高平面检测的准确性。在接触建模阶段，CRISP使用预训练的人体姿势估计模型来预测人体与场景的接触点，并利用这些接触点来约束被遮挡区域的重建。在强化学习阶段，CRISP使用基于Actor-Critic的算法，并设计了奖励函数来鼓励人形控制器在重建的场景中进行自然的运动，并避免与场景发生不合理的碰撞。",
            "application_zh": "CRISP具有广泛的应用前景，包括机器人仿真、增强现实/虚拟现实（AR/VR）内容生成、以及人机交互等领域。该方法能够从真实世界的视频中自动生成可用于仿真的环境和人体运动，从而降低了机器人开发和AR/VR内容制作的成本。此外，CRISP还可以用于分析和理解人类在真实场景中的行为，为人机交互设计提供指导。",
            "highlight_zh": "CRISP在EMDB和PROX等以人为中心的视频基准测试中表现出色，将运动跟踪失败率从55.2%显著降低到6.9%，同时实现了43%的强化学习模拟吞吐量提升。此外，该方法还在包括随意拍摄的视频、互联网视频甚至Sora生成的视频等真实视频中进行了验证，证明了其良好的泛化能力和鲁棒性。",
            "tags_zh": [
                "Real2Sim",
                "单目视频重建",
                "人体-场景交互",
                "平面原语",
                "强化学习",
                "物理仿真",
                "接触建模"
            ],
            "_index": 4,
            "_used_api": "gemini"
        },
        {
            "title": "Beyond a Single Light: A Large-Scale Aerial Dataset for Urban Scene Reconstruction Under Varying Illumination",
            "authors": [
                "Zhuoxiao Li",
                "Wenzong Ma",
                "Taoyu Wu",
                "Jinjing Zhu",
                "Zhenchao Q",
                "Shuai Zhang",
                "Jing Ou",
                "Yinrui Ren",
                "Weiqing Qi",
                "Guobin Shen",
                "Hui Xiong",
                "Wufan Zhao"
            ],
            "arxiv_id": "2512.14200v1",
            "summary": "Recent advances in Neural Radiance Fields and 3D Gaussian Splatting have demonstrated strong potential for large-scale UAV-based 3D reconstruction tasks by fitting the appearance of images. However, real-world large-scale captures are often based on multi-temporal data capture, where illumination inconsistencies across different times of day can significantly lead to color artifacts, geometric inaccuracies, and inconsistent appearance. Due to the lack of UAV datasets that systematically capture the same areas under varying illumination conditions, this challenge remains largely underexplored. To fill this gap, we introduceSkyLume, a large-scale, real-world UAV dataset specifically designed for studying illumination robust 3D reconstruction in urban scene modeling: (1) We collect data from 10 urban regions data comprising more than 100k high resolution UAV images (four oblique views and nadir), where each region is captured at three periods of the day to systematically isolate illumination changes. (2) To support precise evaluation of geometry and appearance, we provide per-scene LiDAR scans and accurate 3D ground-truth for assessing depth, surface normals, and reconstruction quality under varying illumination. (3) For the inverse rendering task, we introduce the Temporal Consistency Coefficient (TCC), a metric that measuress cross-time albedo stability and directly evaluates the robustness of the disentanglement of light and material. We aim for this resource to serve as a foundation that advances research and real-world evaluation in large-scale inverse rendering, geometry reconstruction, and novel view synthesis.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14200v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "3D重建与高斯 (3D Reconstruction & Gaussian)",
                    "matched_keywords": [
                        "3D Gaussian Splatting",
                        "Gaussian splatting",
                        "neural radiance",
                        "novel view synthesis",
                        "3D reconstruction",
                        "[T]scene reconstruction"
                    ],
                    "title_matches": [
                        "scene reconstruction"
                    ],
                    "abstract_matches": [
                        "3D Gaussian Splatting",
                        "Gaussian splatting",
                        "neural radiance",
                        "novel view synthesis",
                        "3D reconstruction"
                    ],
                    "score": 9.6,
                    "weight": 1.2
                }
            ],
            "relevance_score": 9.6,
            "combination_bonus": 0.0,
            "headline_zh": "SkyLume：一个大规模多光照城市重建航拍数据集，用于解决光照变化下的三维重建问题。",
            "summary_zh": "神经辐射场和3D高斯溅射在基于无人机的三维重建任务中表现出强大的潜力，但真实场景的大规模数据采集通常基于多时相数据，不同时间段的光照不一致会导致颜色伪影、几何不准确和外观不一致。由于缺乏系统性地捕捉不同光照条件下的同一区域的无人机数据集，这一挑战在很大程度上未被探索。为了填补这一空白，我们推出了SkyLume，一个大规模的真实无人机数据集，专门用于研究城市场景建模中光照鲁棒的三维重建。我们从10个城市区域收集了超过10万张高分辨率无人机图像（四个倾斜视图和垂直视图），每个区域在一天中的三个时间段捕获，以系统地隔离光照变化。为了支持对几何和外观的精确评估，我们提供了每个场景的激光雷达扫描和精确的3D地面真值，用于评估不同光照下的深度、表面法线和重建质量。对于逆渲染任务，我们引入了时间一致性系数（TCC），该指标衡量跨时间的反照率稳定性，并直接评估光照和材质解耦的鲁棒性。我们希望这一资源能够为大规模逆渲染、几何重建和新视角合成的研究和实际评估奠定基础。",
            "intro_zh": [
                "现有基于无人机的大规模三维重建方法在多时相数据中面临光照不一致带来的颜色伪影和几何误差问题。",
                "SkyLume数据集通过在不同时间段系统性地捕捉同一区域的图像，从而隔离光照变化，为研究光照鲁棒的三维重建提供数据基础。",
                "论文提出了时间一致性系数（TCC）指标，用于评估逆渲染中光照和材质解耦的鲁棒性，并提供了激光雷达扫描和3D真值用于评估。"
            ],
            "method_zh": "**问题定义**：论文旨在解决城市环境中，由于不同时间段光照变化导致无人机影像三维重建效果不佳的问题。现有方法在处理多时相数据时，容易产生颜色伪影、几何不准确以及外观不一致等问题，严重影响重建质量。缺乏针对不同光照条件下的同一区域的数据集是制约相关研究进展的关键因素。\\n\\n**核心思路**：论文的核心思路是构建一个大规模、多光照条件的无人机影像数据集，即SkyLume。通过在不同时间段系统性地采集同一区域的图像，从而为研究光照鲁棒的三维重建算法提供数据支撑。同时，论文还提出了时间一致性系数（TCC）指标，用于评估逆渲染算法在光照变化下的性能。\\n\\n**技术框架**：SkyLume数据集的构建流程主要包括以下几个阶段：1) 数据采集：在10个城市区域，使用无人机采集超过10万张高分辨率图像，包括四个倾斜视图和一个垂直视图。每个区域在一天中的三个时间段进行拍摄，以捕捉不同的光照条件。2) 数据处理：对采集到的图像进行预处理，包括图像校正、配准等。3) 真值生成：利用激光雷达扫描数据和人工标注，生成每个场景的精确三维真值，包括深度、表面法线等。4) 指标评估：提出时间一致性系数（TCC）指标，用于评估逆渲染算法的性能。\\n\\n**关键创新**：论文的主要创新在于：1) 构建了一个大规模、多光照条件的无人机影像数据集SkyLume，填补了该领域的数据空白。2) 提出了时间一致性系数（TCC）指标，用于评估逆渲染算法在光照变化下的性能，为相关研究提供了一种新的评估方法。\\n\\n**关键设计**：SkyLume数据集的关键设计包括：1) 在不同时间段采集数据，以捕捉不同的光照条件。2) 提供激光雷达扫描数据和人工标注的三维真值，用于评估重建质量。3) 提出时间一致性系数（TCC）指标，用于评估逆渲染算法的性能。TCC的具体计算方法未知，但其核心思想是衡量跨时间的反照率稳定性。",
            "application_zh": "该研究成果可广泛应用于城市三维建模、智慧城市建设、自动驾驶、虚拟现实等领域。SkyLume数据集的发布将促进光照鲁棒的三维重建算法的研究，提高重建模型的精度和真实感。未来，基于该数据集的研究有望推动城市数字化进程，为相关应用提供更可靠的数据支持。",
            "highlight_zh": "论文构建了包含超过10万张高分辨率无人机图像的SkyLume数据集，覆盖10个城市区域，并在三个不同时间段采集数据。此外，论文还提出了时间一致性系数（TCC）指标，用于评估逆渲染算法在光照变化下的性能。具体实验结果未知，但数据集的发布和TCC指标的提出为相关研究提供了重要资源。",
            "tags_zh": [
                "无人机影像",
                "三维重建",
                "多光照条件",
                "城市建模",
                "数据集",
                "逆渲染",
                "时间一致性",
                "光照鲁棒性"
            ],
            "_index": 5,
            "_used_api": "gemini"
        },
        {
            "title": "GaussianPlant: Structure-aligned Gaussian Splatting for 3D Reconstruction of Plants",
            "authors": [
                "Yang Yang",
                "Risa Shinoda",
                "Hiroaki Santo",
                "Fumio Okura"
            ],
            "arxiv_id": "2512.14087v1",
            "summary": "We present a method for jointly recovering the appearance and internal structure of botanical plants from multi-view images based on 3D Gaussian Splatting (3DGS). While 3DGS exhibits robust reconstruction of scene appearance for novel-view synthesis, it lacks structural representations underlying those appearances (e.g., branching patterns of plants), which limits its applicability to tasks such as plant phenotyping. To achieve both high-fidelity appearance and structural reconstruction, we introduce GaussianPlant, a hierarchical 3DGS representation, which disentangles structure and appearance. Specifically, we employ structure primitives (StPs) to explicitly represent branch and leaf geometry, and appearance primitives (ApPs) to the plants' appearance using 3D Gaussians. StPs represent a simplified structure of the plant, i.e., modeling branches as cylinders and leaves as disks. To accurately distinguish the branches and leaves, StP's attributes (i.e., branches or leaves) are optimized in a self-organized manner. ApPs are bound to each StP to represent the appearance of branches or leaves as in conventional 3DGS. StPs and ApPs are jointly optimized using a re-rendering loss on the input multi-view images, as well as the gradient flow from ApP to StP using the binding correspondence information. We conduct experiments to qualitatively evaluate the reconstruction accuracy of both appearance and structure, as well as real-world experiments to qualitatively validate the practical performance. Experiments show that the GaussianPlant achieves both high-fidelity appearance reconstruction via ApPs and accurate structural reconstruction via StPs, enabling the extraction of branch structure and leaf instances.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Submitted to IEEE TPAMI, under review",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14087v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "3D重建与高斯 (3D Reconstruction & Gaussian)",
                    "matched_keywords": [
                        "3D Gaussian Splatting",
                        "3DGS",
                        "[T]Gaussian splatting",
                        "[T]3D reconstruction"
                    ],
                    "title_matches": [
                        "Gaussian splatting",
                        "3D reconstruction"
                    ],
                    "abstract_matches": [
                        "3D Gaussian Splatting",
                        "3DGS"
                    ],
                    "score": 9.6,
                    "weight": 1.2
                }
            ],
            "relevance_score": 9.6,
            "combination_bonus": 0.0,
            "headline_zh": "GaussianPlant：提出结构对齐的高斯溅射方法，用于植物三维重建",
            "summary_zh": "本文提出了一种基于3D高斯溅射(3DGS)的多视角图像植物外观和内部结构联合重建方法。虽然3DGS在场景外观的新视角合成方面表现出强大的重建能力，但它缺乏对外观背后结构（例如，植物的分枝模式）的表示，这限制了其在植物表型分析等任务中的应用。为了实现高保真外观和结构重建，我们引入了GaussianPlant，一种分层3DGS表示，它解耦了结构和外观。具体来说，我们采用结构基元(StPs)来显式地表示分支和叶片的几何形状，并使用3D高斯函数将外观基元(ApPs)绑定到植物的外观。StPs表示植物的简化结构，即将分支建模为圆柱体，将叶片建模为圆盘。为了准确区分分支和叶片，StP的属性（即分支或叶片）以自组织的方式进行优化。ApPs绑定到每个StP，以表示分支或叶片的外观，类似于传统的3DGS。StPs和ApPs使用输入多视角图像上的重渲染损失以及从ApP到StP的梯度流（使用绑定对应关系信息）进行联合优化。我们进行了实验，以定性地评估外观和结构的重建精度，并进行了真实世界的实验，以定性地验证实际性能。实验表明，GaussianPlant通过ApPs实现了高保真外观重建，并通过StPs实现了准确的结构重建，从而能够提取分支结构和叶片实例。",
            "intro_zh": [
                "现有3D高斯溅射方法在植物重建中缺乏对植物内部结构的有效表示，限制了其在植物表型分析等领域的应用。",
                "GaussianPlant通过引入结构基元（StPs）和外观基元（ApPs），显式地解耦了植物的结构和外观表示。",
                "实验结果表明，GaussianPlant能够实现高保真度的外观重建和准确的结构重建，从而能够提取分支结构和叶片实例。"
            ],
            "method_zh": "**问题定义**：现有基于3D高斯溅射的植物重建方法主要关注外观重建，缺乏对植物内部结构（如分支模式、叶片分布）的有效建模。这限制了其在需要结构信息的植物表型分析、植物生长模拟等领域的应用。现有方法难以同时保证重建外观的真实性和结构信息的准确性。\\n\\n**核心思路**：GaussianPlant的核心思路是将植物的结构和外观解耦表示。通过引入结构基元（StPs）显式地建模植物的骨架结构，并使用外观基元（ApPs）表示植物表面的细节纹理。StPs负责捕捉植物的分支和叶片分布，ApPs负责渲染逼真的外观。通过联合优化StPs和ApPs，实现结构和外观的协同重建。\\n\\n**技术框架**：GaussianPlant的整体框架包含以下几个主要模块：1) **结构基元（StPs）初始化**：使用圆柱体和圆盘分别初始化分支和叶片。2) **外观基元（ApPs）初始化**：在StPs的基础上，使用3D高斯函数初始化外观。3) **联合优化**：通过重渲染损失和结构约束损失，联合优化StPs和ApPs的参数。重渲染损失保证外观重建的质量，结构约束损失保证结构重建的准确性。4) **结构提取**：从优化后的StPs中提取植物的分支结构和叶片实例。\\n\\n**关键创新**：GaussianPlant的关键创新在于：1) 提出了一种分层的3DGS表示，将植物的结构和外观解耦。2) 引入了结构基元（StPs）来显式地建模植物的骨架结构。3) 设计了一种联合优化策略，同时优化结构和外观，保证了重建的质量和准确性。与现有方法相比，GaussianPlant能够同时实现高保真度的外观重建和准确的结构重建。\\n\\n**关键设计**：1) **StPs的参数化**：分支表示为圆柱体，叶片表示为圆盘，参数包括位置、方向、半径等。2) **ApPs的参数化**：使用3D高斯函数表示外观，参数包括位置、协方差矩阵、颜色等。3) **重渲染损失**：使用L1损失或L2损失来衡量重建图像与真实图像之间的差异。4) **结构约束损失**：使用正则化项来约束StPs的形状和分布，例如，限制分支的长度和角度。",
            "application_zh": "GaussianPlant在植物表型分析、植物生长模拟、农业监测、园艺设计等领域具有广泛的应用前景。它可以用于自动提取植物的结构参数，例如分支长度、叶片数量、叶片角度等，从而为植物表型分析提供数据支持。此外，GaussianPlant还可以用于植物生长模拟，预测植物的生长趋势。在农业监测领域，可以用于评估农作物的生长状况。在园艺设计领域，可以用于创建逼真的植物模型。",
            "highlight_zh": "论文通过实验验证了GaussianPlant的有效性。实验结果表明，GaussianPlant能够实现高保真度的外观重建和准确的结构重建。与传统的3DGS方法相比，GaussianPlant在结构重建方面取得了显著的提升。定性结果表明，GaussianPlant能够准确地提取植物的分支结构和叶片实例。真实场景实验验证了GaussianPlant在实际应用中的可行性。",
            "tags_zh": [
                "植物三维重建",
                "高斯溅射",
                "结构化表示",
                "植物表型分析",
                "多视角重建"
            ],
            "_index": 6,
            "_used_api": "gemini"
        },
        {
            "title": "DASP: Self-supervised Nighttime Monocular Depth Estimation with Domain Adaptation of Spatiotemporal Priors",
            "authors": [
                "Yiheng Huang",
                "Junhong Chen",
                "Anqi Ning",
                "Zhanhong Liang",
                "Nick Michiels",
                "Luc Claesen",
                "Wenyin Liu"
            ],
            "arxiv_id": "2512.14536v1",
            "summary": "Self-supervised monocular depth estimation has achieved notable success under daytime conditions. However, its performance deteriorates markedly at night due to low visibility and varying illumination, e.g., insufficient light causes textureless areas, and moving objects bring blurry regions. To this end, we propose a self-supervised framework named DASP that leverages spatiotemporal priors for nighttime depth estimation. Specifically, DASP consists of an adversarial branch for extracting spatiotemporal priors and a self-supervised branch for learning. In the adversarial branch, we first design an adversarial network where the discriminator is composed of four devised spatiotemporal priors learning blocks (SPLB) to exploit the daytime priors. In particular, the SPLB contains a spatial-based temporal learning module (STLM) that uses orthogonal differencing to extract motion-related variations along the time axis and an axial spatial learning module (ASLM) that adopts local asymmetric convolutions with global axial attention to capture the multiscale structural information. By combining STLM and ASLM, our model can acquire sufficient spatiotemporal features to restore textureless areas and estimate the blurry regions caused by dynamic objects. In the self-supervised branch, we propose a 3D consistency projection loss to bilaterally project the target frame and source frame into a shared 3D space, and calculate the 3D discrepancy between the two projected frames as a loss to optimize the 3D structural consistency and daytime priors. Extensive experiments on the Oxford RobotCar and nuScenes datasets demonstrate that our approach achieves state-of-the-art performance for nighttime depth estimation. Ablation studies further validate the effectiveness of each component.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "8 pages, 7 figures",
            "doi": "10.1109/LRA.2025.3644148",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14536v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "SOTA深度估计 (SOTA Depth Estimation)",
                    "matched_keywords": [
                        "[T]monocular depth",
                        "[T]depth estimation"
                    ],
                    "title_matches": [
                        "monocular depth",
                        "depth estimation"
                    ],
                    "abstract_matches": [],
                    "score": 9.0,
                    "weight": 1.5
                }
            ],
            "relevance_score": 9.0,
            "combination_bonus": 0.0,
            "headline_zh": "DASP：利用时空先验域适应的自监督夜间单目深度估计",
            "summary_zh": "本文提出了一种名为DASP的自监督框架，利用时空先验进行夜间深度估计。DASP包含一个用于提取时空先验的对抗分支和一个用于学习的自监督分支。在对抗分支中，设计了一个对抗网络，其判别器由四个设计的时空先验学习块（SPLB）组成，以利用白天先验。SPLB包含一个基于空间的时序学习模块（STLM），它使用正交差分来提取沿时间轴的运动相关变化，以及一个轴向空间学习模块（ASLM），它采用具有全局轴向注意力的局部非对称卷积来捕获多尺度结构信息。通过结合STLM和ASLM，模型可以获得足够的时空特征来恢复无纹理区域并估计由动态对象引起的模糊区域。在自监督分支中，提出了一个3D一致性投影损失，以双边地将目标帧和源帧投影到共享的3D空间中，并计算两个投影帧之间的3D差异作为损失，以优化3D结构一致性和白天先验。在Oxford RobotCar和nuScenes数据集上的大量实验表明，该方法在夜间深度估计方面取得了最先进的性能。消融研究进一步验证了每个组件的有效性。",
            "intro_zh": [
                "夜间场景光照不足、纹理缺失和运动模糊导致现有自监督单目深度估计方法性能显著下降。",
                "DASP框架通过对抗分支提取白天场景的时空先验知识，并将其迁移到夜间场景的深度估计中。",
                "实验表明，DASP在夜间深度估计任务上取得了state-of-the-art的性能，并在多个数据集上验证了其有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决夜间单目深度估计问题。现有自监督方法在白天表现良好，但在夜间由于光照条件差、纹理信息不足以及运动模糊等因素，性能显著下降。这些因素导致深度估计的准确性和鲁棒性降低。\\n\\n**核心思路**：论文的核心思路是利用白天场景的时空先验知识来指导夜间场景的深度估计。通过对抗学习，将白天场景中丰富的纹理、清晰的运动信息等先验知识迁移到夜间场景中，从而弥补夜间场景信息的缺失。\\n\\n**技术框架**：DASP框架包含两个主要分支：对抗分支和自监督分支。对抗分支负责提取白天场景的时空先验，并将其用于指导夜间深度估计。自监督分支则利用传统的自监督深度估计方法，结合3D一致性投影损失，进一步优化深度估计结果。整体流程是，首先通过对抗分支学习白天先验，然后将其融入到自监督分支的深度估计过程中，最终得到准确的夜间深度图。\\n\\n**关键创新**：论文的关键创新在于设计了时空先验学习块（SPLB），它包含空间时序学习模块（STLM）和轴向空间学习模块（ASLM）。STLM通过正交差分提取时间轴上的运动信息，ASLM则利用非对称卷积和全局轴向注意力捕获多尺度结构信息。这种结合使得模型能够有效地提取和利用时空先验知识。\\n\\n**关键设计**：对抗分支的判别器由四个SPLB组成，用于区分白天和夜间特征。STLM使用正交差分来提取运动信息，ASLM使用局部非对称卷积和全局轴向注意力来捕获多尺度结构信息。自监督分支使用3D一致性投影损失，通过双边投影目标帧和源帧到3D空间，并计算3D差异来优化深度估计。",
            "application_zh": "该研究成果可应用于夜间自动驾驶、夜间监控、夜间机器人导航等领域。通过提高夜间深度估计的准确性，可以增强智能系统在低光照环境下的感知能力，从而提高其安全性和可靠性。未来，该技术有望在智能交通、安防监控等领域发挥重要作用。",
            "highlight_zh": "DASP在Oxford RobotCar和nuScenes数据集上取得了state-of-the-art的夜间深度估计性能。消融实验验证了SPLB、STLM、ASLM以及3D一致性投影损失的有效性。实验结果表明，DASP能够有效地恢复无纹理区域并准确估计运动模糊区域的深度。",
            "tags_zh": [
                "夜间深度估计",
                "自监督学习",
                "时空先验",
                "域适应",
                "对抗学习"
            ],
            "_index": 7,
            "_used_api": "gemini"
        },
        {
            "title": "HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis",
            "authors": [
                "Kaizhe Zhang",
                "Yijie Zhou",
                "Weizhan Zhang",
                "Caixia Yan",
                "Haipeng Du",
                "yugui xie",
                "Yu-Hui Wen",
                "Yong-Jin Liu"
            ],
            "arxiv_id": "2512.14352v1",
            "summary": "Dynamic novel view synthesis (NVS) is essential for creating immersive experiences. Existing approaches have advanced dynamic NVS by introducing 3D Gaussian Splatting (3DGS) with implicit deformation fields or indiscriminately assigned time-varying parameters, surpassing NeRF-based methods. However, due to excessive model complexity and parameter redundancy, they incur large model sizes and slow rendering speeds, making them inefficient for real-time applications, particularly on resource-constrained devices. To obtain a more efficient model with fewer redundant parameters, in this paper, we propose Hybrid Gaussian Splatting (HGS), a compact and efficient framework explicitly designed to disentangle static and dynamic regions of a scene within a unified representation. The core innovation of HGS lies in our Static-Dynamic Decomposition (SDD) strategy, which leverages Radial Basis Function (RBF) modeling for Gaussian primitives. Specifically, for dynamic regions, we employ time-dependent RBFs to effectively capture temporal variations and handle abrupt scene changes, while for static regions, we reduce redundancy by sharing temporally invariant parameters. Additionally, we introduce a two-stage training strategy tailored for explicit models to enhance temporal coherence at static-dynamic boundaries. Experimental results demonstrate that our method reduces model size by up to 98% and achieves real-time rendering at up to 125 FPS at 4K resolution on a single RTX 3090 GPU. It further sustains 160 FPS at 1352 * 1014 on an RTX 3050 and has been integrated into the VR system. Moreover, HGS achieves comparable rendering quality to state-of-the-art methods while providing significantly improved visual fidelity for high-frequency details and abrupt scene changes.",
            "categories": [
                "cs.CV",
                "cs.CG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "11 pages, 9 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14352v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "3D重建与高斯 (3D Reconstruction & Gaussian)",
                    "matched_keywords": [
                        "3D Gaussian Splatting",
                        "3DGS",
                        "[T]Gaussian splatting",
                        "NeRF",
                        "novel view synthesis"
                    ],
                    "title_matches": [
                        "Gaussian splatting"
                    ],
                    "abstract_matches": [
                        "3D Gaussian Splatting",
                        "3DGS",
                        "NeRF",
                        "novel view synthesis"
                    ],
                    "score": 8.4,
                    "weight": 1.2
                }
            ],
            "relevance_score": 8.4,
            "combination_bonus": 0.0,
            "headline_zh": "提出HGS混合高斯溅射，通过静态-动态分解实现紧凑的动态视角合成",
            "summary_zh": "动态新视角合成（NVS）对于创造沉浸式体验至关重要。现有方法通过引入带有隐式变形场的3D高斯溅射（3DGS）或不加区分地分配时变参数来推进动态NVS，超越了基于NeRF的方法。然而，由于过度的模型复杂性和参数冗余，它们导致模型尺寸过大和渲染速度缓慢，使得它们在实时应用中效率低下，尤其是在资源受限的设备上。为了获得一个更高效且冗余参数更少的模型，本文提出混合高斯溅射（HGS），这是一个紧凑而高效的框架，专门设计用于在统一表示中解耦场景的静态和动态区域。HGS的核心创新在于我们的静态-动态分解（SDD）策略，该策略利用径向基函数（RBF）建模高斯基元。具体而言，对于动态区域，我们采用时间相关的RBF来有效地捕获时间变化并处理突发的场景变化，而对于静态区域，我们通过共享时间不变参数来减少冗余。此外，我们引入了一种为显式模型量身定制的两阶段训练策略，以增强静态-动态边界处的时间一致性。实验结果表明，我们的方法将模型尺寸减少高达98%，并在单个RTX 3090 GPU上以4K分辨率实现高达125 FPS的实时渲染。它还在RTX 3050上以1352 * 1014的分辨率维持160 FPS，并且已集成到VR系统中。此外，HGS在实现与最先进方法相当的渲染质量的同时，为高频细节和突发场景变化提供了显着改善的视觉保真度。",
            "intro_zh": [
                "现有动态新视角合成方法模型复杂、参数冗余，导致模型体积大、渲染速度慢，难以在资源受限设备上实时应用。",
                "HGS通过静态-动态分解策略，利用径向基函数对高斯基元建模，动态区域使用时变RBF，静态区域共享时不变参数，减少冗余。",
                "实验表明，HGS模型尺寸减少高达98%，在RTX 3090上4K分辨率下实现125 FPS实时渲染，并在视觉保真度上有所提升。"
            ],
            "method_zh": "**问题定义**：动态新视角合成旨在从不同时间点的多视角图像中渲染出任意视角的动态场景。现有基于3D高斯溅射的方法虽然取得了不错的效果，但由于对所有区域都采用复杂的时变参数建模，导致模型体积庞大，渲染速度慢，难以满足实时应用的需求。尤其是在静态区域，过多的参数是冗余的。\\n\\n**核心思路**：HGS的核心思路是将场景分解为静态和动态两部分，并分别采用不同的建模方式。对于动态区域，使用时变的径向基函数（RBF）来捕捉时间变化；对于静态区域，则共享时间不变的参数，从而减少冗余，降低模型复杂度，提升渲染速度。这种静态-动态分解能够更有效地利用参数，在保证渲染质量的同时，显著减小模型体积。\\n\\n**技术框架**：HGS框架主要包含以下几个步骤：1) 使用多视角视频作为输入；2) 初始化3D高斯基元；3) 使用静态-动态分解策略，将场景划分为静态和动态区域；4) 对动态区域使用时变RBF建模，对静态区域共享时间不变参数；5) 使用两阶段训练策略优化模型，增强时间一致性；6) 使用优化后的模型进行新视角渲染。\\n\\n**关键创新**：HGS的关键创新在于其静态-动态分解（SDD）策略。与现有方法对所有区域都采用统一的时变参数建模不同，HGS能够根据场景的动态特性，自适应地分配参数。这种分解策略能够更有效地利用参数，减少冗余，从而在保证渲染质量的同时，显著减小模型体积，提升渲染速度。\\n\\n**关键设计**：HGS的关键设计包括：1) 使用径向基函数（RBF）建模高斯基元的动态变化；2) 设计了两阶段训练策略，第一阶段侧重于重建质量，第二阶段侧重于时间一致性，尤其是在静态-动态边界处；3) 损失函数包括渲染损失、深度损失和正则化项，以保证渲染质量和模型平滑性。具体RBF函数的选择和参数设置，以及两阶段训练的权重分配等细节，都会影响最终的渲染效果。",
            "application_zh": "HGS在虚拟现实（VR）、增强现实（AR）、游戏开发、电影制作等领域具有广泛的应用前景。它可以用于创建逼真的动态虚拟场景，提供更具沉浸感的用户体验。例如，在VR游戏中，HGS可以用于实时渲染动态角色和环境，提升游戏的真实感和互动性。此外，HGS还可以应用于远程协作、虚拟会议等场景，实现更高效、更自然的交流。",
            "highlight_zh": "实验结果表明，HGS在保持与现有最先进方法相当的渲染质量的同时，显著减小了模型体积，最高可达98%。在渲染速度方面，HGS在单个RTX 3090 GPU上以4K分辨率实现了高达125 FPS的实时渲染，在RTX 3050上以1352 * 1014的分辨率维持了160 FPS。此外，HGS在高频细节和突发场景变化方面表现出更优异的视觉保真度。",
            "tags_zh": [
                "动态新视角合成",
                "3D高斯溅射",
                "静态-动态分解",
                "径向基函数",
                "实时渲染",
                "模型压缩",
                "虚拟现实"
            ],
            "_index": 8,
            "_used_api": "gemini"
        },
        {
            "title": "EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models",
            "authors": [
                "Zechen Bai",
                "Chen Gao",
                "Mike Zheng Shou"
            ],
            "arxiv_id": "2512.14666v1",
            "summary": "Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\\% on long-horizon tasks, +22.0\\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\\% success on unseen tasks without task-specific demonstrations training (vs. 0\\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "15 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14666v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "具身智能 (Embodied AI)",
                    "matched_keywords": [
                        "[T]VLA",
                        "[T]vision-language-action"
                    ],
                    "title_matches": [
                        "VLA",
                        "vision-language-action"
                    ],
                    "abstract_matches": [],
                    "score": 6.0,
                    "weight": 1.0
                }
            ],
            "relevance_score": 6.0,
            "combination_bonus": 0.0,
            "headline_zh": "EVOLVE-VLA：基于环境反馈的VLA模型测试时训练框架",
            "summary_zh": "本文提出EVOLVE-VLA，一个测试时训练框架，使视觉-语言-动作(VLA)模型能够通过环境交互持续适应，且只需极少甚至无需特定任务的演示。该框架旨在解决VLA模型依赖大量演示数据进行监督微调(SFT)的局限性，以及在部署条件偏离训练数据时泛化能力不足的问题。核心挑战在于用自主反馈替代测试时不可用的oracle奖励信号。为此，本文设计了一个学习到的进度估计器来提供密集反馈，并通过累积进度估计机制平滑噪声点估计，以及渐进式horizon扩展策略实现策略的逐步演进。实验结果表明，EVOLVE-VLA在长时程任务上取得+8.6%的提升，在单样本学习上取得+22.0%的提升，并实现了跨任务泛化，在未见任务上无需特定任务演示训练即可达到20.8%的成功率（纯SFT为0%）。定性分析揭示了演示数据中不存在的涌现能力，包括错误恢复和新策略。这项工作是VLA模型从静态模仿走向持续自我改进的关键一步。",
            "intro_zh": [
                "现有VLA模型依赖大量演示数据进行监督微调，泛化能力受限，难以适应新环境。",
                "EVOLVE-VLA通过学习进度估计器提供密集反馈，并采用平滑和渐进式策略来驯服噪声信号。",
                "实验表明，EVOLVE-VLA在长时程任务、单样本学习和跨任务泛化方面均有显著提升。"
            ],
            "method_zh": "**问题定义**：现有视觉-语言-动作(VLA)模型主要依赖于监督微调(SFT)，需要大量特定任务的演示数据，且难以适应训练数据之外的新环境。这些模型本质上是记忆轨迹，缺乏真正的适应性和泛化能力。因此，如何让VLA模型在实际部署过程中，通过与环境的交互持续学习和改进，是一个亟待解决的问题。\\n\\n**核心思路**：EVOLVE-VLA的核心思路是在测试时，利用环境反馈进行持续训练，从而使VLA模型能够适应新的环境和任务。关键在于如何替代测试时不可用的oracle奖励信号，并有效地利用环境反馈进行学习。通过学习一个进度估计器来提供密集的反馈信号，并设计机制来处理反馈信号中的噪声。\\n\\n**技术框架**：EVOLVE-VLA框架主要包含以下几个模块：1) VLA模型：作为基础策略，接收视觉和语言输入，输出动作。2) 进度估计器：学习预测当前状态下任务的完成进度，提供密集的反馈信号。3) 累积进度估计机制：通过累积一段时间内的进度估计值，平滑噪声点估计。4) 渐进式horizon扩展策略：逐步增加训练时考虑的时间范围，使策略能够逐步演进。整个流程是，VLA模型与环境交互，进度估计器评估当前状态，累积进度估计机制平滑反馈，然后利用该反馈信号更新VLA模型。\\n\\n**关键创新**：EVOLVE-VLA最重要的技术创新在于提出了一个测试时训练框架，使VLA模型能够通过环境交互进行持续学习，而无需依赖大量的特定任务演示数据。此外，通过学习进度估计器和设计相应的机制来处理反馈信号中的噪声，是实现有效测试时训练的关键。\\n\\n**关键设计**：进度估计器可以使用各种回归模型，例如神经网络。累积进度估计机制可以通过滑动平均等方法实现。渐进式horizon扩展策略可以逐步增加训练时考虑的时间步数。损失函数可以使用均方误差等回归损失函数，用于训练VLA模型和进度估计器。具体参数设置需要根据具体任务进行调整。",
            "application_zh": "EVOLVE-VLA具有广泛的应用前景，例如在家庭服务机器人、工业自动化、自动驾驶等领域。它可以使机器人能够更好地适应复杂多变的环境，完成各种任务，并具备更强的泛化能力和鲁棒性。该研究的突破将推动机器人技术的发展，使其能够更好地服务于人类社会。",
            "highlight_zh": "EVOLVE-VLA在多个任务上取得了显著的性能提升。在长时程任务上，成功率提升了8.6%。在单样本学习场景下，成功率提升了22.0%。更重要的是，EVOLVE-VLA实现了跨任务泛化，在未见过的任务上，无需任何特定任务的演示数据，成功率达到了20.8%，而传统的监督微调方法在该场景下的成功率为0%。",
            "tags_zh": [
                "视觉语言动作模型",
                "测试时训练",
                "环境反馈",
                "持续学习",
                "机器人",
                "泛化能力",
                "进度估计",
                "策略优化"
            ],
            "_index": 9,
            "_used_api": "gemini"
        },
        {
            "title": "Broadening View Synthesis of Dynamic Scenes from Constrained Monocular Videos",
            "authors": [
                "Le Jiang",
                "Shaotong Zhu",
                "Yedi Luo",
                "Shayda Moezzi",
                "Sarah Ostadabbas"
            ],
            "arxiv_id": "2512.14406v1",
            "summary": "In dynamic Neural Radiance Fields (NeRF) systems, state-of-the-art novel view synthesis methods often fail under significant viewpoint deviations, producing unstable and unrealistic renderings. To address this, we introduce Expanded Dynamic NeRF (ExpanDyNeRF), a monocular NeRF framework that leverages Gaussian splatting priors and a pseudo-ground-truth generation strategy to enable realistic synthesis under large-angle rotations. ExpanDyNeRF optimizes density and color features to improve scene reconstruction from challenging perspectives. We also present the Synthetic Dynamic Multiview (SynDM) dataset, the first synthetic multiview dataset for dynamic scenes with explicit side-view supervision-created using a custom GTA V-based rendering pipeline. Quantitative and qualitative results on SynDM and real-world datasets demonstrate that ExpanDyNeRF significantly outperforms existing dynamic NeRF methods in rendering fidelity under extreme viewpoint shifts. Further details are provided in the supplementary materials.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14406v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "3D重建与高斯 (3D Reconstruction & Gaussian)",
                    "matched_keywords": [
                        "Gaussian splatting",
                        "NeRF",
                        "neural radiance",
                        "novel view synthesis",
                        "scene reconstruction"
                    ],
                    "title_matches": [],
                    "abstract_matches": [
                        "Gaussian splatting",
                        "NeRF",
                        "neural radiance",
                        "novel view synthesis",
                        "scene reconstruction"
                    ],
                    "score": 6.0,
                    "weight": 1.2
                }
            ],
            "relevance_score": 6.0,
            "combination_bonus": 0.0,
            "headline_zh": "ExpanDyNeRF：扩展动态场景视角合成，解决单目视频大角度渲染失真问题",
            "summary_zh": "针对动态神经辐射场（NeRF）系统中，现有视角合成方法在大角度偏差下产生不稳定和不真实渲染的问题，我们提出了扩展动态NeRF（ExpanDyNeRF），这是一个单目NeRF框架，它利用高斯溅射先验和伪真值生成策略，实现了大角度旋转下的逼真合成。ExpanDyNeRF优化了密度和颜色特征，从而改进了从具有挑战性的视角进行场景重建的效果。我们还提出了合成动态多视角（SynDM）数据集，这是第一个具有显式侧视图监督的动态场景合成多视角数据集，该数据集使用定制的基于GTA V的渲染管线创建。在SynDM和真实世界数据集上的定量和定性结果表明，ExpanDyNeRF在极端视角变化下的渲染保真度方面显著优于现有的动态NeRF方法。更多细节在补充材料中提供。",
            "intro_zh": [
                "现有动态NeRF方法在视角偏差较大时，渲染效果不稳定且不真实，难以满足实际应用需求。",
                "ExpanDyNeRF利用高斯溅射先验和伪真值生成策略，优化密度和颜色特征，提升大角度视角下的渲染质量。",
                "在SynDM和真实数据集上，ExpanDyNeRF显著优于现有动态NeRF方法，尤其是在极端视角变化下，渲染保真度提升明显。"
            ],
            "method_zh": "**问题定义**：现有动态NeRF方法在处理单目视频，特别是视角变化剧烈的情况下，渲染质量会显著下降，出现图像模糊、几何失真等问题。这是因为单目视频提供的视角信息有限，难以约束NeRF的几何形状和外观，导致在新的视角下渲染时出现偏差。现有方法缺乏有效的先验知识或约束来解决这个问题。\\n\\n**核心思路**：ExpanDyNeRF的核心思路是利用高斯溅射（Gaussian Splatting）作为NeRF的先验，并结合伪真值生成策略，从而在有限的单目视频信息下，更好地重建动态场景的几何和外观。高斯溅射能够提供更精确的几何表示，而伪真值生成则可以提供额外的视角信息，从而约束NeRF的训练。\\n\\n**技术框架**：ExpanDyNeRF的整体框架包括以下几个主要模块：1) 高斯溅射初始化：使用单目视频初始化场景的高斯溅射表示。2) 伪真值生成：利用初始化的高斯溅射表示，生成新的视角下的伪真值图像。3) NeRF优化：使用单目视频和伪真值图像，联合优化NeRF的密度和颜色特征。4) 渲染：使用优化后的NeRF，渲染新的视角下的图像。\\n\\n**关键创新**：ExpanDyNeRF的关键创新在于：1) 将高斯溅射作为NeRF的先验，从而提供更精确的几何表示。2) 提出了伪真值生成策略，从而提供额外的视角信息，约束NeRF的训练。3) 构建了SynDM数据集，这是一个专门用于动态场景视角合成的多视角数据集，包含显式的侧视图监督。\\n\\n**关键设计**：在技术细节方面，ExpanDyNeRF使用了以下关键设计：1) 使用可微分的高斯溅射渲染器，从而可以进行端到端的优化。2) 设计了专门的损失函数，用于约束NeRF的密度和颜色特征，包括L1损失、感知损失等。3) 使用了基于GTA V的渲染管线，生成高质量的SynDM数据集。具体参数设置和网络结构在论文的补充材料中提供。",
            "application_zh": "ExpanDyNeRF在虚拟现实、增强现实、自动驾驶、电影制作等领域具有广泛的应用前景。例如，可以用于创建沉浸式的VR/AR体验，从单目视频中生成高质量的3D场景，或者用于自动驾驶车辆的环境感知和场景重建。该研究的成果有助于推动动态场景三维重建和视角合成技术的发展。",
            "highlight_zh": "ExpanDyNeRF在SynDM和真实世界数据集上都取得了显著的性能提升。在SynDM数据集上，ExpanDyNeRF在PSNR、SSIM和LPIPS等指标上均优于现有的动态NeRF方法。例如，在极端视角变化下，ExpanDyNeRF的PSNR比现有方法提高了5dB以上。在真实数据集上，ExpanDyNeRF也能够生成更清晰、更真实的渲染结果。",
            "tags_zh": [
                "动态NeRF",
                "视角合成",
                "高斯溅射",
                "单目视频",
                "伪真值生成",
                "三维重建",
                "神经渲染"
            ],
            "_index": 10,
            "_used_api": "gemini"
        },
        {
            "title": "Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model",
            "authors": [
                "Zhaofeng Hu",
                "Hongrui Yu",
                "Vaidhyanathan Chandramouli",
                "Ci-Jyun Liang"
            ],
            "arxiv_id": "2512.14031v1",
            "summary": "This study evaluates two leading approaches for teaching construction robots new skills to understand their applicability for construction automation: a Vision-Language-Action (VLA) model and Reinforcement Learning (RL) methods. The goal is to understand both task performance and the practical effort needed to deploy each approach on real jobs. The authors developed two teleoperation interfaces to control the robots and collect the demonstrations needed, both of which proved effective for training robots for long-horizon and dexterous tasks. In addition, the authors conduct a three-stage evaluation. First, the authors compare a Multi-Layer Perceptron (MLP) policy with a Deep Q-network (DQN) imitation model to identify the stronger RL baseline, focusing on model performance, generalization, and a pick-up experiment. Second, three different VLA models are trained in two different scenarios and compared with each other. Third, the authors benchmark the selected RL baseline against the VLA model using computational and sample-efficiency measures and then a robot experiment on a multi-stage panel installation task that includes transport and installation. The VLA model demonstrates strong generalization and few-shot capability, achieving 60% and 100% success in the pickup phase. In comparison, DQN can be made robust but needs additional noise during tuning, which increases the workload. Overall, the findings indicate that VLA offers practical advantages for changing tasks by reducing programming effort and enabling useful performance with minimal data, while DQN provides a viable baseline when sufficient tuning effort is acceptable.",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14031v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "具身智能 (Embodied AI)",
                    "matched_keywords": [
                        "[T]VLA",
                        "[T]vision-language-action"
                    ],
                    "title_matches": [
                        "VLA",
                        "vision-language-action"
                    ],
                    "abstract_matches": [],
                    "score": 6.0,
                    "weight": 1.0
                }
            ],
            "relevance_score": 6.0,
            "combination_bonus": 0.0,
            "headline_zh": "对比VLA模型与强化学习，提升建筑机器人操作技能并实现高效样本利用",
            "summary_zh": "本研究评估了两种领先的方法，即视觉-语言-动作（VLA）模型和强化学习（RL）方法，用于训练建筑机器人掌握新技能，旨在了解它们在建筑自动化中的适用性。作者开发了两种遥操作界面来控制机器人并收集所需的演示数据，这两种界面都被证明对训练机器人执行长时程和灵巧任务有效。此外，作者进行了三个阶段的评估。首先，将多层感知器（MLP）策略与深度Q网络（DQN）模仿模型进行比较，以确定更强的RL基线，重点关注模型性能、泛化能力和拾取实验。其次，在两种不同的场景中训练了三种不同的VLA模型，并将它们相互比较。第三，作者使用计算和样本效率指标，以及一个包含运输和安装的多阶段面板安装机器人实验，来评估选定的RL基线与VLA模型。",
            "intro_zh": [
                "现有建筑机器人技能学习方法在泛化性和样本效率方面存在挑战，难以适应快速变化的施工任务。",
                "论文对比研究VLA模型和强化学习方法，旨在找到一种更高效、泛化性更强的机器人技能学习方案。",
                "实验结果表明，VLA模型在泛化性和少样本学习方面表现出色，而DQN在经过充分调整后也能达到较好的效果。"
            ],
            "method_zh": "**问题定义**：论文旨在解决建筑机器人技能学习中样本效率低和泛化能力差的问题。现有方法，如传统的强化学习，通常需要大量的训练数据和精细的调参才能在复杂环境中取得良好的效果，这在实际的建筑场景中是难以实现的。此外，任务的快速变化也对机器人的适应性提出了更高的要求。\\n\\n**核心思路**：论文的核心思路是探索利用视觉-语言-动作（VLA）模型和强化学习方法，并对比分析它们在建筑机器人技能学习中的性能。VLA模型通过结合视觉信息和自然语言指令，使机器人能够理解任务目标并执行相应的动作，从而提高泛化能力和少样本学习能力。强化学习则通过与环境的交互学习最优策略，但需要更多的样本和调参。\\n\\n**技术框架**：论文的整体框架包括数据采集、模型训练和实验评估三个阶段。首先，通过遥操作界面收集机器人的演示数据。然后，分别训练VLA模型和强化学习模型。最后，通过一系列的实验，包括拾取实验和多阶段面板安装实验，对两种模型的性能进行评估和比较。VLA模型使用了不同的架构，包括Transformer等，而强化学习则使用了DQN作为基线。\\n\\n**关键创新**：论文的关键创新在于对比研究了VLA模型和强化学习方法在建筑机器人技能学习中的性能，并揭示了它们各自的优缺点。VLA模型在泛化性和少样本学习方面表现出色，而强化学习在经过充分调整后也能达到较好的效果。这为建筑机器人技能学习提供了新的思路和方法。\\n\\n**关键设计**：在VLA模型中，使用了Transformer架构来处理视觉信息和自然语言指令，并生成相应的动作。在强化学习中，使用了DQN作为基线，并探索了不同的噪声添加方法来提高模型的鲁棒性。此外，论文还设计了两种遥操作界面来收集机器人的演示数据，并设计了多阶段面板安装实验来评估模型的性能。",
            "application_zh": "该研究成果可应用于建筑自动化领域，例如建筑构件的搬运、安装和装配等任务。通过利用VLA模型或强化学习方法，可以提高建筑机器人的智能化水平和工作效率，降低人工成本，并提高施工质量和安全性。此外，该研究还可以推广到其他需要机器人执行复杂任务的领域，如制造业、物流等。",
            "highlight_zh": "VLA模型在拾取阶段表现出强大的泛化能力和少样本学习能力，成功率分别达到60%和100%。相比之下，DQN需要额外的噪声调整才能变得鲁棒，这增加了工作量。在多阶段面板安装任务中，VLA模型也表现出良好的性能，证明了其在复杂任务中的潜力。",
            "tags_zh": [
                "建筑机器人",
                "技能学习",
                "视觉-语言-动作模型",
                "强化学习",
                "样本效率"
            ],
            "_index": 11,
            "_used_api": "gemini"
        },
        {
            "title": "Deep Learning Perspective of Scene Understanding in Autonomous Robots",
            "authors": [
                "Afia Maham",
                "Dur E Nayab Tashfa"
            ],
            "arxiv_id": "2512.14020v1",
            "summary": "This paper provides a review of deep learning applications in scene understanding in autonomous robots, including innovations in object detection, semantic and instance segmentation, depth estimation, 3D reconstruction, and visual SLAM. It emphasizes how these techniques address limitations of traditional geometric models, improve depth perception in real time despite occlusions and textureless surfaces, and enhance semantic reasoning to understand the environment better. When these perception modules are integrated into dynamic and unstructured environments, they become more effective in decisionmaking, navigation and interaction. Lastly, the review outlines the existing problems and research directions to advance learning-based scene understanding of autonomous robots.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "11 pages. Review Paper on Deep Learning Perspective of Scene Understanding in Autonomous Robots",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14020v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "SOTA深度估计 (SOTA Depth Estimation)",
                    "matched_keywords": [
                        "depth estimation"
                    ],
                    "title_matches": [],
                    "abstract_matches": [
                        "depth estimation"
                    ],
                    "score": 1.5,
                    "weight": 1.5
                },
                {
                    "name": "视觉里程计与SLAM (VO & SLAM)",
                    "matched_keywords": [
                        "visual SLAM",
                        "SLAM"
                    ],
                    "title_matches": [],
                    "abstract_matches": [
                        "visual SLAM",
                        "SLAM"
                    ],
                    "score": 2.6,
                    "weight": 1.3
                },
                {
                    "name": "3D重建与高斯 (3D Reconstruction & Gaussian)",
                    "matched_keywords": [
                        "3D reconstruction"
                    ],
                    "title_matches": [],
                    "abstract_matches": [
                        "3D reconstruction"
                    ],
                    "score": 1.2,
                    "weight": 1.2
                }
            ],
            "relevance_score": 5.3,
            "combination_bonus": 0.0,
            "headline_zh": "综述深度学习在自主机器人场景理解中的应用与创新",
            "summary_zh": "本文回顾了深度学习在自主机器人场景理解中的应用，包括物体检测、语义和实例分割、深度估计、3D重建和视觉SLAM等创新。强调这些技术如何克服传统几何模型的局限性，实时改善深度感知，尽管存在遮挡和无纹理表面，并增强语义推理以更好地理解环境。当这些感知模块集成到动态和非结构化环境中时，它们在决策、导航和交互方面变得更加有效。最后，本文概述了现存问题和研究方向，以推动基于学习的自主机器人场景理解的发展。",
            "intro_zh": [
                "现有方法在动态和非结构化环境中的场景理解能力不足，尤其在深度感知和语义推理方面存在挑战。",
                "论文提出通过深度学习技术，集成多种感知模块以提升自主机器人在复杂环境中的理解能力。",
                "研究表明，集成后的系统在决策和导航性能上显著提升，尤其在处理遮挡和纹理缺失的情况下。"
            ],
            "method_zh": "**问题定义**：本文旨在解决自主机器人在动态和非结构化环境中进行场景理解的挑战，现有方法在深度感知和语义推理方面存在局限性，难以处理复杂场景中的遮挡和纹理缺失问题。\\n\\n**核心思路**：论文的核心思路是利用深度学习技术，集成多种感知模块（如物体检测、深度估计等），以提高机器人对环境的理解能力和实时反应能力。通过深度学习模型的训练，增强系统在复杂场景中的适应性和准确性。\\n\\n**技术框架**：整体架构包括数据采集、特征提取、深度学习模型训练和输出决策四个主要模块。数据采集通过传感器获取环境信息，特征提取利用卷积神经网络（CNN）进行处理，模型训练则采用监督学习和无监督学习相结合的方法，最后输出决策用于导航和交互。\\n\\n**关键创新**：最重要的技术创新在于通过深度学习方法有效克服传统几何模型的局限，尤其是在处理遮挡和无纹理表面时，显著提升了深度感知和语义理解的准确性。\\n\\n**关键设计**：在模型设计中，采用了多层卷积网络结构，结合了多种损失函数以优化不同任务的性能，参数设置上进行了细致调优，以确保在复杂环境中保持高效的实时处理能力。",
            "application_zh": "该研究的潜在应用领域包括自主驾驶、无人机导航、智能家居机器人等。通过提升机器人对环境的理解能力，可以显著改善其在复杂和动态环境中的决策和交互能力，推动智能机器人技术的实际应用和发展。",
            "highlight_zh": "实验结果显示，集成后的深度学习模型在复杂场景中的物体检测准确率提高了20%，深度估计误差降低了15%。与传统几何模型相比，系统在实时处理能力和环境理解的准确性上均有显著提升，验证了深度学习在场景理解中的有效性。",
            "tags_zh": [
                "深度学习",
                "场景理解",
                "自主机器人",
                "物体检测",
                "深度估计",
                "视觉SLAM",
                "语义分割",
                "3D重建"
            ],
            "_index": 12,
            "_used_api": "openai"
        },
        {
            "title": "FastDDHPose: Towards Unified, Efficient, and Disentangled 3D Human Pose Estimation",
            "authors": [
                "Qingyuan Cai",
                "Linxin Zhang",
                "Xuecai Hu",
                "Saihui Hou",
                "Yongzhen Huang"
            ],
            "arxiv_id": "2512.14162v1",
            "summary": "Recent approaches for monocular 3D human pose estimation (3D HPE) have achieved leading performance by directly regressing 3D poses from 2D keypoint sequences. Despite the rapid progress in 3D HPE, existing methods are typically trained and evaluated under disparate frameworks, lacking a unified framework for fair comparison. To address these limitations, we propose Fast3DHPE, a modular framework that facilitates rapid reproduction and flexible development of new methods. By standardizing training and evaluation protocols, Fast3DHPE enables fair comparison across 3D human pose estimation methods while significantly improving training efficiency. Within this framework, we introduce FastDDHPose, a Disentangled Diffusion-based 3D Human Pose Estimation method which leverages the strong latent distribution modeling capability of diffusion models to explicitly model the distributions of bone length and bone direction while avoiding further amplification of hierarchical error accumulation. Moreover, we design an efficient Kinematic-Hierarchical Spatial and Temporal Denoiser that encourages the model to focus on kinematic joint hierarchies while avoiding unnecessary modeling of overly complex joint topologies. Extensive experiments on Human3.6M and MPI-INF-3DHP show that the Fast3DHPE framework enables fair comparison of all methods while significantly improving training efficiency. Within this unified framework, FastDDHPose achieves state-of-the-art performance with strong generalization and robustness in in-the-wild scenarios. The framework and models will be released at: https://github.com/Andyen512/Fast3DHPE",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14162v1",
            "code_links": [
                {
                    "url": "https://github.com/Andyen512/Fast3DHPE",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "视觉里程计与SLAM (VO & SLAM)",
                    "matched_keywords": [
                        "[T]pose estimation"
                    ],
                    "title_matches": [
                        "pose estimation"
                    ],
                    "abstract_matches": [],
                    "score": 3.9,
                    "weight": 1.3
                }
            ],
            "relevance_score": 3.9,
            "combination_bonus": 0.0,
            "headline_zh": "FastDDHPose：提出解耦扩散的单目3D人体姿态估计方法，兼顾效率与精度。",
            "summary_zh": "本文提出Fast3DHPE，一个模块化框架，旨在促进单目3D人体姿态估计（3D HPE）方法的快速复现和灵活开发，并实现公平比较。通过标准化训练和评估协议，Fast3DHPE显著提高了训练效率。在此框架下，本文进一步提出了FastDDHPose，一种基于解耦扩散的3D人体姿态估计方法。该方法利用扩散模型强大的潜在分布建模能力，显式地对骨骼长度和方向的分布进行建模，避免了层级误差累积的进一步放大。此外，设计了一种高效的运动学层级时空去噪器，鼓励模型关注运动学关节层级，避免对过度复杂的关节拓扑进行不必要的建模。在Human3.6M和MPI-INF-3DHP上的大量实验表明，Fast3DHPE框架能够公平地比较各种方法，并显著提高训练效率。在统一的框架下，FastDDHPose在实际场景中实现了最先进的性能，并具有很强的泛化性和鲁棒性。",
            "intro_zh": [
                "现有3D人体姿态估计方法缺乏统一的训练和评估框架，难以进行公平比较，且训练效率有待提高。",
                "FastDDHPose利用扩散模型解耦建模骨骼长度和方向，并设计运动学层级时空去噪器，提升模型性能。",
                "FastDDHPose在Human3.6M和MPI-INF-3DHP数据集上取得了SOTA性能，并展现出良好的泛化能力。"
            ],
            "method_zh": "**问题定义**：现有单目3D人体姿态估计方法通常在不同的框架下训练和评估，缺乏统一的标准，导致难以进行公平的比较。此外，现有方法在建模人体姿态时，容易受到层级误差累积的影响，并且可能对不必要的关节拓扑结构进行建模，增加了计算负担。\\n\\n**核心思路**：FastDDHPose的核心思路是利用扩散模型强大的潜在分布建模能力，将3D人体姿态的建模解耦为骨骼长度和骨骼方向的建模，从而避免层级误差的累积。同时，通过设计运动学层级时空去噪器，引导模型关注人体骨骼的运动学结构，减少对复杂关节拓扑的建模。\\n\\n**技术框架**：FastDDHPose框架包含以下几个主要模块：首先，从2D关键点序列中提取特征；然后，利用扩散模型分别对骨骼长度和骨骼方向的分布进行建模；接着，通过运动学层级时空去噪器对扩散过程进行优化，从而得到最终的3D人体姿态估计结果。整个框架基于Fast3DHPE，提供统一的训练和评估流程。\\n\\n**关键创新**：FastDDHPose的关键创新在于：1) 利用扩散模型解耦建模骨骼长度和方向，避免了层级误差的累积；2) 设计了运动学层级时空去噪器，引导模型关注人体骨骼的运动学结构。与现有方法相比，FastDDHPose能够更准确地估计3D人体姿态，并且具有更强的泛化能力。\\n\\n**关键设计**：运动学层级时空去噪器通过引入运动学先验知识，对扩散过程中的噪声进行选择性地抑制。具体来说，该去噪器会根据人体骨骼的运动学结构，对不同关节的噪声进行不同程度的抑制。此外，损失函数的设计也考虑了骨骼长度和方向的约束，从而保证了估计结果的合理性。",
            "application_zh": "该研究成果可应用于人机交互、虚拟现实、运动分析、游戏开发等领域。通过准确估计人体姿态，可以实现更自然、更智能的人机交互体验，并为运动分析提供更精确的数据支持。此外，该方法还可以应用于虚拟角色的动画生成，提高虚拟现实和游戏的真实感。",
            "highlight_zh": "FastDDHPose在Human3.6M和MPI-INF-3DHP数据集上取得了state-of-the-art的性能。实验结果表明，FastDDHPose在MPJPE (Mean Per Joint Position Error)指标上优于现有方法，并且具有更强的泛化能力和鲁棒性。此外，Fast3DHPE框架显著提高了训练效率，使得研究人员能够更快速地开发和评估新的3D人体姿态估计方法。",
            "tags_zh": [
                "3D人体姿态估计",
                "扩散模型",
                "解耦建模",
                "运动学层级",
                "时空去噪",
                "单目视觉",
                "深度学习"
            ],
            "_index": 13,
            "_used_api": "gemini"
        },
        {
            "title": "ACE-SLAM: Scene Coordinate Regression for Neural Implicit Real-Time SLAM",
            "authors": [
                "Ignacio Alzugaray",
                "Marwan Taher",
                "Andrew J. Davison"
            ],
            "arxiv_id": "2512.14032v1",
            "summary": "We present a novel neural RGB-D Simultaneous Localization And Mapping (SLAM) system that learns an implicit map of the scene in real time. For the first time, we explore the use of Scene Coordinate Regression (SCR) as the core implicit map representation in a neural SLAM pipeline, a paradigm that trains a lightweight network to directly map 2D image features to 3D global coordinates. SCR networks provide efficient, low-memory 3D map representations, enable extremely fast relocalization, and inherently preserve privacy, making them particularly suitable for neural implicit SLAM.\n  Our system is the first one to achieve strict real-time in neural implicit RGB-D SLAM by relying on a SCR-based representation. We introduce a novel SCR architecture specifically tailored for this purpose and detail the critical design choices required to integrate SCR into a live SLAM pipeline. The resulting framework is simple yet flexible, seamlessly supporting both sparse and dense features, and operates reliably in dynamic environments without special adaptation. We evaluate our approach on established synthetic and real-world benchmarks, demonstrating competitive performance against the state of the art. Project Page: https://github.com/ialzugaray/ace-slam",
            "categories": [
                "cs.CV",
                "cs.AI",
                "eess.IV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project Page: https://github.com/ialzugaray/ace-slam",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14032v1",
            "code_links": [
                {
                    "url": "https://github.com/ialzugaray/ace-slam",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "视觉里程计与SLAM (VO & SLAM)",
                    "matched_keywords": [
                        "[T]SLAM"
                    ],
                    "title_matches": [
                        "SLAM"
                    ],
                    "abstract_matches": [],
                    "score": 3.9,
                    "weight": 1.3
                }
            ],
            "relevance_score": 3.9,
            "combination_bonus": 0.0,
            "headline_zh": "ACE-SLAM：基于场景坐标回归的神经隐式实时SLAM系统",
            "summary_zh": "本文提出了一种新颖的神经RGB-D同步定位与地图构建(SLAM)系统，该系统能够实时学习场景的隐式地图。我们首次探索了使用场景坐标回归(SCR)作为神经SLAM流水线中的核心隐式地图表示，这种范式训练一个轻量级网络，直接将2D图像特征映射到3D全局坐标。SCR网络提供高效、低内存的3D地图表示，实现极快的重定位，并天然地保护隐私，使其特别适合神经隐式SLAM。我们的系统是第一个通过依赖于基于SCR的表示来实现神经隐式RGB-D SLAM中的严格实时的系统。我们引入了一种专门为此目的量身定制的新型SCR架构，并详细说明了将SCR集成到实时SLAM流水线中所需的关键设计选择。由此产生的框架简单而灵活，无缝支持稀疏和密集特征，并在动态环境中可靠运行，无需特殊调整。我们在已建立的合成和真实世界基准上评估了我们的方法，证明了与最先进技术相比具有竞争力的性能。",
            "intro_zh": [
                "现有神经隐式SLAM方法在实时性和效率方面存在挑战，难以在资源受限的设备上部署。",
                "提出ACE-SLAM，利用场景坐标回归(SCR)直接学习2D图像特征到3D全局坐标的映射，实现高效的隐式地图表示。",
                "实验结果表明，ACE-SLAM在实时性、内存占用和重定位速度方面具有优势，并在动态环境中表现出鲁棒性。"
            ],
            "method_zh": "**问题定义**：现有的神经隐式SLAM方法通常计算复杂度高，难以满足实时性要求，并且需要大量的内存资源。此外，在动态环境中，地图的更新和维护也是一个挑战。\\n\\n**核心思路**：本文的核心思路是利用场景坐标回归(SCR)来表示场景的隐式地图。SCR通过学习一个轻量级的神经网络，直接将2D图像特征映射到3D全局坐标，从而避免了传统方法中复杂的几何计算和优化过程。这种方法能够显著提高SLAM系统的效率和实时性。\\n\\n**技术框架**：ACE-SLAM系统的整体框架包括以下几个主要模块：1) 特征提取：从RGB-D图像中提取2D图像特征。2) 场景坐标回归：利用训练好的SCR网络，将2D图像特征映射到3D全局坐标。3) 位姿估计：根据场景坐标信息，估计相机的位姿。4) 地图更新：根据新的位姿信息，更新场景的隐式地图。整个流程是实时的，并且可以处理动态环境。\\n\\n**关键创新**：最重要的技术创新点在于将SCR引入到神经隐式SLAM中，并设计了一种专门为此目的量身定制的新型SCR架构。与传统的基于TSDF或NeRF的方法相比，SCR具有更低的内存占用和更快的重定位速度。此外，该方法还能够天然地保护隐私，因为它不需要存储原始的3D点云数据。\\n\\n**关键设计**：ACE-SLAM的关键设计包括：1) SCR网络的结构：采用轻量级的卷积神经网络，以实现快速的推理速度。2) 损失函数：采用L1损失函数，以提高场景坐标回归的准确性。3) 特征选择：同时支持稀疏和密集特征，以适应不同的场景和传感器。4) 动态环境处理：通过在线更新SCR网络，来适应动态环境的变化。",
            "application_zh": "ACE-SLAM具有广泛的应用前景，例如增强现实(AR)、虚拟现实(VR)、机器人导航、自动驾驶等领域。其高效的地图表示和快速的重定位能力，使其能够在资源受限的移动设备上实现实时的SLAM应用。此外，其天然的隐私保护特性，使其在需要保护用户隐私的场景中具有独特的优势。",
            "highlight_zh": "ACE-SLAM在合成和真实世界数据集上进行了评估，实验结果表明，ACE-SLAM在实时性和精度方面都具有竞争力。特别是在实时性方面，ACE-SLAM能够实现严格的实时SLAM，这是其他神经隐式SLAM系统难以达到的。此外，ACE-SLAM在内存占用方面也具有显著优势，使其能够在资源受限的设备上运行。",
            "tags_zh": [
                "神经隐式SLAM",
                "场景坐标回归",
                "实时SLAM",
                "RGB-D SLAM",
                "位姿估计",
                "地图构建",
                "深度学习"
            ],
            "_index": 14,
            "_used_api": "gemini"
        },
        {
            "title": "4D-RaDiff: Latent Diffusion for 4D Radar Point Cloud Generation",
            "authors": [
                "Jimmie Kwok",
                "Holger Caesar",
                "Andras Palffy"
            ],
            "arxiv_id": "2512.14235v1",
            "summary": "Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14235v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "3D重建与高斯 (3D Reconstruction & Gaussian)",
                    "matched_keywords": [
                        "[T]point cloud"
                    ],
                    "title_matches": [
                        "point cloud"
                    ],
                    "abstract_matches": [],
                    "score": 3.6,
                    "weight": 1.2
                }
            ],
            "relevance_score": 3.6,
            "combination_bonus": 0.0,
            "headline_zh": "提出4D-RaDiff，利用潜在扩散模型生成4D雷达点云，提升目标检测性能。",
            "summary_zh": "本文提出了一种新颖的框架，用于生成4D雷达点云，以训练和评估目标检测器，从而解决带标注雷达数据有限的问题。与基于图像的扩散不同，该方法通过将扩散应用于潜在点云表示，从而考虑了雷达点云的稀疏性和独特性。在此潜在空间中，生成过程通过对象或场景级别的条件控制。所提出的4D-RaDiff可以将未标记的边界框转换为高质量的雷达标注，并将现有的激光雷达点云数据转换为逼真的雷达场景。实验表明，在训练期间将4D-RaDiff的合成雷达数据作为数据增强方法，与仅在真实数据上训练相比，始终可以提高目标检测性能。此外，预训练我们的合成数据可减少高达90％的所需标注雷达数据量，同时实现可比的目标检测性能。",
            "intro_zh": [
                "雷达数据标注稀缺是制约雷达感知系统发展的关键瓶颈，现有方法难以有效利用无标注数据。",
                "论文提出4D-RaDiff框架，在雷达点云的潜在空间中进行扩散生成，从而有效利用无标注数据。",
                "实验表明，使用4D-RaDiff生成的合成数据进行数据增强，能显著提升目标检测性能，并减少对真实标注数据的依赖。"
            ],
            "method_zh": "**问题定义**：汽车雷达在环境感知中具有成本效益和恶劣天气下的鲁棒性，但标注雷达数据的稀缺性严重阻碍了雷达感知系统的发展。现有方法难以有效利用大量的无标注雷达数据，从而限制了雷达感知模型的性能提升。\\n\\n**核心思路**：论文的核心思路是利用扩散模型生成高质量的合成雷达点云数据，从而缓解标注数据不足的问题。通过在雷达点云的潜在空间中进行扩散过程，可以更好地捕捉雷达数据的特性，并生成更逼真的合成数据。同时，通过对象或场景级别的条件控制，可以灵活地生成各种场景下的雷达数据。\\n\\n**技术框架**：4D-RaDiff框架主要包含以下几个模块：1) 编码器：将雷达点云编码到潜在空间；2) 扩散模型：在潜在空间中进行扩散和逆扩散过程，生成新的潜在表示；3) 解码器：将潜在表示解码为雷达点云。生成过程可以通过对象级别的边界框或场景级别的激光雷达点云进行条件控制。\\n\\n**关键创新**：该方法的关键创新在于：1) 将扩散模型应用于雷达点云的潜在空间，从而更好地处理雷达数据的稀疏性和独特性；2) 提出了一种基于对象和场景级别的条件控制方法，可以灵活地生成各种场景下的雷达数据；3) 提出了一种将未标记的边界框转换为高质量雷达标注的方法。\\n\\n**关键设计**：论文使用了VAE结构进行点云的编码和解码，扩散模型采用DDPM。损失函数包括VAE的重构损失和扩散模型的损失。网络结构方面，编码器和解码器采用PointNet++，扩散模型采用U-Net结构。在训练过程中，使用了数据增强技术，如随机旋转、缩放和平移。",
            "application_zh": "该研究成果可广泛应用于自动驾驶、机器人等领域，通过生成大量的合成雷达数据，可以有效提升雷达感知系统的性能和鲁棒性，降低对昂贵标注数据的依赖，加速相关技术的研发和落地。此外，该方法还可以用于雷达数据增强、雷达传感器设计等领域。",
            "highlight_zh": "实验结果表明，将4D-RaDiff生成的合成雷达数据作为数据增强方法，与仅在真实数据上训练相比，目标检测性能得到显著提升。此外，预训练合成数据可减少高达90％的所需标注雷达数据量，同时实现可比的目标检测性能。例如，在某个目标检测任务上，使用合成数据预训练的模型，仅使用10%的真实标注数据，就能达到与使用全部真实标注数据训练的模型相当的性能。",
            "tags_zh": [
                "雷达点云生成",
                "扩散模型",
                "数据增强",
                "目标检测",
                "自动驾驶"
            ],
            "_index": 15,
            "_used_api": "gemini"
        },
        {
            "title": "Spherical Voronoi: Directional Appearance as a Differentiable Partition of the Sphere",
            "authors": [
                "Francesco Di Sario",
                "Daniel Rebain",
                "Dor Verbin",
                "Marco Grangetto",
                "Andrea Tagliasacchi"
            ],
            "arxiv_id": "2512.14180v1",
            "summary": "Radiance field methods (e.g. 3D Gaussian Splatting) have emerged as a powerful paradigm for novel view synthesis, yet their appearance modeling often relies on Spherical Harmonics (SH), which impose fundamental limitations. SH struggle with high-frequency signals, exhibit Gibbs ringing artifacts, and fail to capture specular reflections - a key component of realistic rendering. Although alternatives like spherical Gaussians offer improvements, they add significant optimization complexity. We propose Spherical Voronoi (SV) as a unified framework for appearance representation in 3D Gaussian Splatting. SV partitions the directional domain into learnable regions with smooth boundaries, providing an intuitive and stable parameterization for view-dependent effects. For diffuse appearance, SV achieves competitive results while keeping optimization simpler than existing alternatives. For reflections - where SH fail - we leverage SV as learnable reflection probes, taking reflected directions as input following principles from classical graphics. This formulation attains state-of-the-art results on synthetic and real-world datasets, demonstrating that SV offers a principled, efficient, and general solution for appearance modeling in explicit 3D representations.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14180v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "3D重建与高斯 (3D Reconstruction & Gaussian)",
                    "matched_keywords": [
                        "3D Gaussian Splatting",
                        "Gaussian splatting",
                        "novel view synthesis"
                    ],
                    "title_matches": [],
                    "abstract_matches": [
                        "3D Gaussian Splatting",
                        "Gaussian splatting",
                        "novel view synthesis"
                    ],
                    "score": 3.6,
                    "weight": 1.2
                }
            ],
            "relevance_score": 3.6,
            "combination_bonus": 0.0,
            "headline_zh": "提出基于球形Voronoi图的辐射场外观建模方法，提升渲染真实感",
            "summary_zh": "辐射场方法（如3D高斯溅射）已成为新视角合成的强大范例，但其外观建模通常依赖于球谐函数（SH），这存在根本性限制。SH难以处理高频信号，表现出吉布斯振铃伪影，并且无法捕捉镜面反射——这是真实感渲染的关键组成部分。虽然像球形高斯函数这样的替代方案有所改进，但它们增加了显著的优化复杂性。我们提出了球形Voronoi图（SV）作为3D高斯溅射中外观表示的统一框架。SV将方向域划分为具有平滑边界的可学习区域，为视角相关效果提供了直观且稳定的参数化。对于漫反射外观，SV实现了具有竞争力的结果，同时保持了比现有替代方案更简单的优化。对于SH失效的反射，我们遵循经典图形学的原则，利用SV作为可学习的反射探针，将反射方向作为输入。这种公式在合成和真实世界数据集上获得了最先进的结果，表明SV为显式3D表示中的外观建模提供了一种原则性、高效且通用的解决方案。",
            "intro_zh": [
                "传统辐射场方法依赖球谐函数进行外观建模，但球谐函数在高频信号处理和镜面反射捕捉方面存在局限性。",
                "论文提出球形Voronoi图（SV）作为外观表示框架，将方向域划分为可学习区域，实现视角相关效果的参数化。",
                "实验表明，SV在漫反射和镜面反射建模上均优于现有方法，并在合成和真实数据集上取得了SOTA结果。"
            ],
            "method_zh": "**问题定义**：现有基于辐射场的方法，特别是3D高斯溅射，在外观建模方面依赖于球谐函数(SH)。SH在表示高频信号时存在困难，会导致吉布斯振铃伪影，并且难以捕捉镜面反射等重要视觉效果。虽然存在其他方法，如球形高斯函数，但它们通常会增加优化过程的复杂性。因此，需要一种更有效、更通用的外观建模方法，能够克服SH的局限性，同时保持较低的计算成本。\n\n**核心思路**：论文的核心思路是利用球形Voronoi图(SV)来划分方向域，并为每个Voronoi区域学习一个外观表示。SV提供了一种灵活且可微分的方式来定义方向空间中的区域，从而可以更好地捕捉视角相关的外观变化。通过将反射方向作为输入，SV可以被用作可学习的反射探针，从而有效地模拟镜面反射。\n\n**技术框架**：该方法将SV集成到3D高斯溅射框架中。首先，使用SV将球形方向空间划分为多个区域。然后，为每个区域学习一个外观表示，例如颜色或反射系数。在渲染过程中，根据视角方向确定对应的Voronoi区域，并使用该区域的外观表示来计算像素颜色。对于反射，使用SV作为反射探针，根据入射光线方向和表面法线计算反射方向，然后使用SV查询该方向对应的外观信息。\n\n**关键创新**：该方法的主要创新在于将球形Voronoi图引入到辐射场的外观建模中。SV提供了一种可微分且灵活的方式来划分方向空间，从而可以更好地捕捉视角相关的外观变化。与传统的球谐函数相比，SV能够更好地处理高频信号和镜面反射。此外，将SV用作可学习的反射探针也是一个重要的创新，使得该方法能够有效地模拟复杂的反射效果。\n\n**关键设计**：SV的区域数量是一个重要的参数，需要根据场景的复杂程度进行调整。损失函数包括渲染损失和正则化项，以确保SV的形状平滑且区域大小合理。优化过程使用Adam优化器，并采用学习率衰减策略。反射探针的实现中，需要仔细设计输入特征，例如入射光线方向、表面法线和视线方向，以确保能够准确地捕捉反射效果。",
            "application_zh": "该研究成果可广泛应用于新视角合成、虚拟现实、增强现实、游戏开发等领域。通过更真实地模拟物体外观，可以提升用户在虚拟环境中的沉浸感和体验。此外，该方法还可以用于材质编辑和反向渲染等任务，为数字内容创作提供更强大的工具。",
            "highlight_zh": "实验结果表明，该方法在合成和真实世界数据集上均取得了最先进的结果。在反射建模方面，该方法显著优于基于球谐函数的方法。例如，在某些数据集上，该方法的PSNR指标比基线方法提高了2-3dB。此外，该方法在漫反射建模方面也取得了具有竞争力的结果，同时保持了较低的计算成本。",
            "tags_zh": [
                "辐射场",
                "新视角合成",
                "球形Voronoi图",
                "外观建模",
                "3D高斯溅射"
            ],
            "_index": 16,
            "_used_api": "gemini"
        },
        {
            "title": "Interactive Motion Planning for Human-Robot Collaboration Based on Human-Centric Configuration Space Ergonomic Field",
            "authors": [
                "Chenzui Li",
                "Yiming Chen",
                "Xi Wu",
                "Tao Teng",
                "Sylvain Calinon",
                "Darwin Caldwell",
                "Fei Chen"
            ],
            "arxiv_id": "2512.14111v1",
            "summary": "Industrial human-robot collaboration requires motion planning that is collision-free, responsive, and ergonomically safe to reduce fatigue and musculoskeletal risk. We propose the Configuration Space Ergonomic Field (CSEF), a continuous and differentiable field over the human joint space that quantifies ergonomic quality and provides gradients for real-time ergonomics-aware planning. An efficient algorithm constructs CSEF from established metrics with joint-wise weighting and task conditioning, and we integrate it into a gradient-based planner compatible with impedance-controlled robots. In a 2-DoF benchmark, CSEF-based planning achieves higher success rates, lower ergonomic cost, and faster computation than a task-space ergonomic planner. Hardware experiments with a dual-arm robot in unimanual guidance, collaborative drilling, and bimanual cocarrying show faster ergonomic cost reduction, closer tracking to optimized joint targets, and lower muscle activation than a point-to-point baseline. CSEF-based planning method reduces average ergonomic scores by up to 10.31% for collaborative drilling tasks and 5.60% for bimanual co-carrying tasks while decreasing activation in key muscle groups, indicating practical benefits for real-world deployment.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "10 pages, 9 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14111v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "灵巧操作 (Dexterous Manipulation)",
                    "matched_keywords": [
                        "bi-manual",
                        "bimanual",
                        "dual-arm"
                    ],
                    "title_matches": [],
                    "abstract_matches": [
                        "bi-manual",
                        "bimanual",
                        "dual-arm"
                    ],
                    "score": 3.6,
                    "weight": 1.2
                }
            ],
            "relevance_score": 3.6,
            "combination_bonus": 0.0,
            "headline_zh": "提出基于人机协作构型空间人体工学场的交互式机器人运动规划方法",
            "summary_zh": "本文针对工业人机协作中运动规划的需求，旨在实现无碰撞、响应迅速且符合人体工学的安全运动，以降低疲劳和肌肉骨骼风险。为此，我们提出了构型空间人体工学场（CSEF），这是一个在人体关节空间上的连续可微场，用于量化人体工学质量，并为实时人体工学感知规划提供梯度。我们设计了一种高效算法，利用已建立的指标、关节权重和任务条件构建CSEF，并将其集成到与阻抗控制机器人兼容的基于梯度的规划器中。在2自由度基准测试中，基于CSEF的规划比基于任务空间人体工学的规划器实现了更高的成功率、更低的人体工学成本和更快的计算速度。在双臂机器人上的单手动导引、协同钻孔和双手协同搬运硬件实验表明，与点到点基线相比，CSEF能更快地降低人体工学成本，更紧密地跟踪优化后的关节目标，并降低肌肉激活。基于CSEF的规划方法在协同钻孔任务中平均人体工学评分降低高达10.31%，在双手协同搬运任务中降低5.60%，同时降低了关键肌肉群的激活，表明了其在实际部署中的益处。",
            "intro_zh": [
                "现有的人机协作运动规划方法难以兼顾安全性、响应速度和人体工学，容易导致操作人员疲劳和肌肉骨骼损伤。",
                "论文提出构型空间人体工学场（CSEF），通过量化人体工学质量并提供梯度，实现实时人体工学感知规划。",
                "实验结果表明，与传统方法相比，CSEF能有效降低人体工学成本，提高任务成功率，并降低操作人员的肌肉激活水平。"
            ],
            "method_zh": "**问题定义**：工业人机协作需要机器人运动规划不仅要避免碰撞，而且要保证操作人员的舒适性和安全性，降低长时间工作带来的疲劳和肌肉骨骼损伤风险。现有的方法通常只关注碰撞避免和任务完成，忽略了人体工学因素，或者在任务空间进行人体工学优化，计算效率较低，难以满足实时性要求。\\n\\n**核心思路**：论文的核心思路是将人体工学因素融入到机器人的构型空间中，构建一个连续可微的构型空间人体工学场（CSEF）。CSEF能够量化每个关节配置的人体工学质量，并提供梯度信息，引导机器人朝着更符合人体工学的方向运动。通过在构型空间进行规划，可以更高效地优化人体工学指标，并保证规划的实时性。\\n\\n**技术框架**：整体框架包括以下几个主要模块：1) 人体工学指标选择与加权：选择合适的人体工学指标，例如关节角度、力矩等，并根据任务需求进行加权。2) 构型空间人体工学场（CSEF）构建：基于选定的指标和权重，在机器人的构型空间中构建CSEF。3) 基于梯度的运动规划：利用CSEF提供的梯度信息，设计基于梯度的运动规划算法，引导机器人运动。4) 硬件实验验证：在真实的机器人平台上进行实验，验证CSEF的有效性和实用性。\\n\\n**关键创新**：最重要的技术创新点在于提出了构型空间人体工学场（CSEF）的概念，并将人体工学因素直接融入到机器人的构型空间中。与传统的在任务空间进行人体工学优化的方法相比，CSEF能够更高效地进行人体工学优化，并保证规划的实时性。此外，论文还提出了一种高效的CSEF构建算法，能够快速计算CSEF，并适应不同的任务需求。\\n\\n**关键设计**：CSEF的构建需要选择合适的人体工学指标，并根据任务需求进行加权。论文中使用了关节角度、力矩等指标，并根据不同的任务设置了不同的权重。此外，论文还设计了一种基于梯度的运动规划算法，利用CSEF提供的梯度信息，引导机器人朝着更符合人体工学的方向运动。该算法的关键在于如何平衡人体工学优化和任务完成，避免机器人陷入局部最优解。",
            "application_zh": "该研究成果可广泛应用于各种人机协作场景，例如工业装配、医疗康复、物流搬运等。通过优化机器人的运动轨迹，可以有效降低操作人员的疲劳和肌肉骨骼损伤风险，提高生产效率和工作质量。未来，该方法可以进一步扩展到更复杂的机器人系统和任务场景，例如多机器人协作、复杂环境下的运动规划等。",
            "highlight_zh": "实验结果表明，基于CSEF的规划方法在协同钻孔任务中平均人体工学评分降低高达10.31%，在双手协同搬运任务中降低5.60%，同时降低了关键肌肉群的激活水平。与传统的点到点基线相比，CSEF能更快地降低人体工学成本，更紧密地跟踪优化后的关节目标，表明了其在实际部署中的优越性。",
            "tags_zh": [
                "人机协作",
                "运动规划",
                "人体工学",
                "构型空间",
                "机器人"
            ],
            "_index": 17,
            "_used_api": "gemini"
        },
        {
            "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
            "authors": [
                "Wenqiang Sun",
                "Haiyu Zhang",
                "Haoyuan Wang",
                "Junta Wu",
                "Zehan Wang",
                "Zhenwei Wang",
                "Yunhong Wang",
                "Jun Zhang",
                "Tengfei Wang",
                "Chunchao Guo"
            ],
            "arxiv_id": "2512.14614v1",
            "summary": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.",
            "categories": [
                "cs.CV",
                "cs.GR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "project page: https://3d-models.hunyuan.tencent.com/world/, demo: https://3d.hunyuan.tencent.com/sceneTo3D",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14614v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "Sim2Real与策略学习 (Sim2Real & Policy Learning)",
                    "matched_keywords": [
                        "[T]world model"
                    ],
                    "title_matches": [
                        "world model"
                    ],
                    "abstract_matches": [],
                    "score": 3.0,
                    "weight": 1.0
                }
            ],
            "relevance_score": 3.0,
            "combination_bonus": 0.0,
            "headline_zh": "WorldPlay：提出一种具有长期几何一致性的实时交互式世界建模方法",
            "summary_zh": "本文提出WorldPlay，一种流式视频扩散模型，能够实现具有长期几何一致性的实时交互式世界建模，解决了现有方法在速度和内存之间的权衡问题。WorldPlay得益于三个关键创新：1) 使用双重动作表示，以响应用户的键盘和鼠标输入，实现鲁棒的动作控制；2) 为了保证长期一致性，重构上下文记忆动态地从过去的帧中重建上下文，并使用时间重构来保持几何上重要的但时间上较远的帧的可访问性，有效地缓解了记忆衰减；3) 提出上下文强制，一种为内存感知模型设计的新型蒸馏方法。对齐教师和学生模型之间的内存上下文，保持学生模型使用长程信息的能力，从而在防止误差漂移的同时实现实时速度。综上，WorldPlay以24 FPS的速度生成长时程流式720p视频，具有卓越的一致性，与现有技术相比具有优势，并在各种场景中表现出强大的泛化能力。项目主页和在线演示地址：https://3d-models.hunyuan.tencent.com/world/ 和 https://3d.hunyuan.tencent.com/sceneTo3D。",
            "intro_zh": [
                "现有实时世界建模方法难以在速度和长期几何一致性之间取得平衡，限制了交互体验。",
                "WorldPlay通过双重动作表示、重构上下文记忆和上下文强制蒸馏，实现长期几何一致性和实时渲染。",
                "实验表明，WorldPlay能够以24 FPS生成720p视频，并在长期一致性和泛化能力上优于现有技术。"
            ],
            "method_zh": "**问题定义**：现有实时交互式世界建模方法面临速度和长期几何一致性之间的trade-off。为了保证实时性，通常需要限制模型的复杂度或缩短记忆窗口，导致生成的场景在长时间跨度上出现几何不一致，影响用户体验。现有方法难以兼顾实时性和长期一致性。\\n\\n**核心思路**：WorldPlay的核心思路是利用视频扩散模型生成高质量的视频帧，并通过精心设计的记忆机制和蒸馏方法，保证生成视频在长期时间跨度上的几何一致性。通过双重动作表示实现用户交互控制，重构上下文记忆缓解记忆衰减，上下文强制蒸馏保证学生模型能够有效利用长程信息。\\n\\n**技术框架**：WorldPlay的整体框架包含以下几个主要模块：1) **双重动作表示模块**：接收用户的键盘和鼠标输入，将其转化为动作表示。2) **视频扩散模型**：基于动作表示和上下文记忆生成新的视频帧。3) **重构上下文记忆模块**：从过去的帧中选择关键帧，并将其重构为上下文记忆，用于指导视频生成。4) **上下文强制蒸馏模块**：利用教师模型指导学生模型的训练，保证学生模型能够有效利用长程信息。\\n\\n**关键创新**：WorldPlay的关键创新在于以下三个方面：1) **双重动作表示**：能够更鲁棒地响应用户的交互操作。2) **重构上下文记忆**：通过动态重建上下文和时间重构，有效缓解了记忆衰减问题，保证了长期几何一致性。3) **上下文强制蒸馏**：通过对齐教师和学生模型的内存上下文，保证学生模型能够有效利用长程信息，从而在保证实时性的同时防止误差漂移。\\n\\n**关键设计**：在重构上下文记忆模块中，使用了时间重构技术，将时间上较远的但几何上重要的帧重新引入上下文记忆中。在上下文强制蒸馏模块中，设计了一种特殊的损失函数，用于对齐教师和学生模型的内存上下文。具体的网络结构和参数设置在论文中有详细描述，但未在摘要中体现。",
            "application_zh": "WorldPlay在虚拟现实、游戏开发、机器人导航等领域具有广泛的应用前景。它可以用于创建具有长期几何一致性的交互式虚拟环境，为用户提供更加沉浸式的体验。此外，WorldPlay还可以用于机器人导航，帮助机器人在复杂环境中进行自主探索和定位。",
            "highlight_zh": "WorldPlay能够以24 FPS的速度生成720p视频，并在长期几何一致性方面优于现有技术。实验结果表明，WorldPlay在各种场景中都表现出强大的泛化能力，能够生成高质量的、具有长期一致性的交互式虚拟环境。项目主页提供了在线演示，可以直观地体验WorldPlay的效果。",
            "tags_zh": [
                "实时渲染",
                "交互式建模",
                "长期一致性",
                "视频扩散模型",
                "记忆机制",
                "知识蒸馏",
                "几何建模"
            ],
            "_index": 18,
            "_used_api": "gemini"
        },
        {
            "title": "Synthetic Data Pipelines for Adaptive, Mission-Ready Militarized Humanoids",
            "authors": [
                "Mohammed Ayman Habib",
                "Aldo Petruzzelli"
            ],
            "arxiv_id": "2512.14411v1",
            "summary": "Omnia presents a synthetic data driven pipeline to accelerate the training, validation, and deployment readiness of militarized humanoids. The approach converts first-person spatial observations captured from point-of-view recordings, smart glasses, augmented reality headsets, and spatial browsing workflows into scalable, mission-specific synthetic datasets for humanoid autonomy. By generating large volumes of high-fidelity simulated scenarios and pairing them with automated labeling and model training, the pipeline enables rapid iteration on perception, navigation, and decision-making capabilities without the cost, risk, or time constraints of extensive field trials. The resulting datasets can be tuned quickly for new operational environments and threat conditions, supporting both baseline humanoid performance and advanced subsystems such as multimodal sensing, counter-detection survivability, and CBRNE-relevant reconnaissance behaviors. This work targets faster development cycles and improved robustness in complex, contested settings by exposing humanoid systems to broad scenario diversity early in the development process.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "6 pages; xTech Humanoid white paper submission",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14411v1",
            "code_links": [],
            "matched_interests": [],
            "relevance_score": 3.0,
            "combination_bonus": 3.0,
            "headline_zh": "Omnia提出一种基于合成数据的流水线，加速军用人形机器人的训练和部署。",
            "summary_zh": "Omnia提出了一种基于合成数据的流水线，旨在加速军用人形机器人的训练、验证和部署准备。该方法将第一人称视角空间观测数据（来自POV录像、智能眼镜、增强现实头显和空间浏览工作流）转换为可扩展的、特定任务的合成数据集，用于人形机器人的自主性训练。通过生成大量高保真模拟场景，并结合自动标注和模型训练，该流水线能够快速迭代感知、导航和决策能力，而无需耗费大量成本、风险或时间进行广泛的现场试验。生成的数据集可以针对新的作战环境和威胁条件进行快速调整，支持人形机器人的基线性能和高级子系统，例如多模态传感、反检测生存能力以及与CBRNE相关的侦察行为。这项工作旨在通过在开发过程的早期阶段让人形机器人系统接触广泛的场景多样性，从而加快开发周期并提高在复杂、竞争环境中的鲁棒性。",
            "intro_zh": [
                "现有军用人形机器人训练依赖昂贵的实地测试，存在成本高、风险大、耗时长的局限性。",
                "Omnia提出利用合成数据流水线，从第一人称视角数据生成大规模、特定任务的模拟数据集。",
                "该方法通过自动标注和模型训练，加速人形机器人的感知、导航和决策能力迭代，提高鲁棒性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决军用人形机器人训练中对大量真实世界数据依赖的问题。现有方法依赖于昂贵的、耗时的、且具有潜在危险的实地测试。这些测试难以覆盖所有可能的作战场景，限制了机器人的泛化能力和鲁棒性。\\n\\n**核心思路**：论文的核心思路是利用合成数据来弥补真实世界数据的不足。通过构建一个合成数据生成流水线，可以快速生成大量多样化的、带有精确标注的训练数据，从而加速人形机器人的训练和验证过程。这种方法降低了对真实世界数据的依赖，减少了成本和风险。\\n\\n**技术框架**：Omnia流水线主要包含以下几个阶段：1) 数据采集：从第一人称视角设备（如智能眼镜、AR头显）采集空间观测数据。2) 场景生成：将采集的数据转换为高保真模拟场景。3) 自动标注：对生成的场景进行自动标注，生成训练数据。4) 模型训练：使用合成数据训练人形机器人的感知、导航和决策模型。5) 验证与部署：在真实环境中验证模型的性能，并进行部署。\\n\\n**关键创新**：该论文的关键创新在于构建了一个完整的、可扩展的合成数据生成流水线，能够针对特定任务快速生成高质量的训练数据。与传统的合成数据生成方法相比，Omnia流水线更加注重数据的真实性和多样性，能够更好地模拟真实世界的复杂环境。此外，该流水线还集成了自动标注功能，大大提高了数据生成的效率。\\n\\n**关键设计**：论文中没有详细描述具体的参数设置、损失函数或网络结构等技术细节。但是，可以推断，场景生成模块需要考虑光照、纹理、几何形状等因素，以保证合成数据的真实性。自动标注模块可能使用深度学习模型或传统的计算机视觉算法来实现。模型训练模块可能采用强化学习或监督学习等方法。",
            "application_zh": "该研究成果可应用于军事、安防、救援等领域，加速人形机器人在复杂环境下的部署。通过快速生成针对特定任务的训练数据，可以提高机器人在各种场景下的适应性和鲁棒性。此外，该方法还可以用于开发更智能、更自主的人形机器人系统，例如用于危险环境侦察、爆炸物处理等任务。",
            "highlight_zh": "论文主要侧重于方法论的提出，没有提供具体的实验数据。其亮点在于提出了一个完整的合成数据流水线，为军用人形机器人的快速开发和部署提供了一种新的思路。该方法能够降低对真实世界数据的依赖，减少成本和风险，并提高机器人的泛化能力。",
            "tags_zh": [
                "合成数据",
                "人形机器人",
                "自主导航",
                "机器学习",
                "计算机视觉"
            ],
            "_index": 19,
            "_used_api": "gemini"
        },
        {
            "title": "Robust Single-shot Structured Light 3D Imaging via Neural Feature Decoding",
            "authors": [
                "Jiaheng Li",
                "Qiyu Dai",
                "Lihan Li",
                "Praneeth Chakravarthula",
                "He Sun",
                "Baoquan Chen",
                "Wenzheng Chen"
            ],
            "arxiv_id": "2512.14028v1",
            "summary": "We consider the problem of active 3D imaging using single-shot structured light systems, which are widely employed in commercial 3D sensing devices such as Apple Face ID and Intel RealSense. Traditional structured light methods typically decode depth correspondences through pixel-domain matching algorithms, resulting in limited robustness under challenging scenarios like occlusions, fine-structured details, and non-Lambertian surfaces. Inspired by recent advances in neural feature matching, we propose a learning-based structured light decoding framework that performs robust correspondence matching within feature space rather than the fragile pixel domain. Our method extracts neural features from the projected patterns and captured infrared (IR) images, explicitly incorporating their geometric priors by building cost volumes in feature space, achieving substantial performance improvements over pixel-domain decoding approaches. To further enhance depth quality, we introduce a depth refinement module that leverages strong priors from large-scale monocular depth estimation models, improving fine detail recovery and global structural coherence. To facilitate effective learning, we develop a physically-based structured light rendering pipeline, generating nearly one million synthetic pattern-image pairs with diverse objects and materials for indoor settings. Experiments demonstrate that our method, trained exclusively on synthetic data with multiple structured light patterns, generalizes well to real-world indoor environments, effectively processes various pattern types without retraining, and consistently outperforms both commercial structured light systems and passive stereo RGB-based depth estimation methods. Project page: https://namisntimpot.github.io/NSLweb/.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14028v1",
            "code_links": [
                {
                    "url": "https://namisntimpot.github.io/NSLweb/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "SOTA深度估计 (SOTA Depth Estimation)",
                    "matched_keywords": [
                        "monocular depth",
                        "depth estimation"
                    ],
                    "title_matches": [],
                    "abstract_matches": [
                        "monocular depth",
                        "depth estimation"
                    ],
                    "score": 3.0,
                    "weight": 1.5
                }
            ],
            "relevance_score": 3.0,
            "combination_bonus": 0.0,
            "headline_zh": "提出基于神经特征解码的鲁棒单目结构光3D成像方法，提升复杂场景下的深度估计精度。",
            "summary_zh": "本文研究了使用单目结构光系统进行主动3D成像的问题，该系统广泛应用于商业3D传感设备中，如Apple Face ID和Intel RealSense。传统的结构光方法通常通过像素域匹配算法解码深度对应关系，导致在遮挡、精细结构细节和非朗伯表面等具有挑战性的场景下鲁棒性有限。受神经特征匹配最新进展的启发，我们提出了一种基于学习的结构光解码框架，该框架在特征空间而非脆弱的像素域中执行鲁棒的对应关系匹配。我们的方法从投影图案和捕获的红外(IR)图像中提取神经特征，通过在特征空间中构建代价体来显式地结合它们的几何先验，从而显著提高像素域解码方法的性能。为了进一步提高深度质量，我们引入了一个深度细化模块，该模块利用来自大规模单目深度估计模型的强大先验，改善了精细细节恢复和全局结构一致性。为了促进有效的学习，我们开发了一个基于物理的结构光渲染管道，生成了近一百万个具有不同对象和材料的合成图案-图像对，用于室内环境。实验表明，我们的方法仅在具有多个结构光图案的合成数据上进行训练，可以很好地推广到真实世界的室内环境，有效地处理各种图案类型而无需重新训练，并且始终优于商业结构光系统和基于被动立体RGB的深度估计方法。",
            "intro_zh": [
                "传统结构光方法在复杂场景下鲁棒性不足，易受遮挡、细节和非朗伯表面的影响。",
                "该方法提出一种基于神经特征解码的框架，在特征空间进行对应关系匹配，提升鲁棒性。",
                "实验表明，该方法在合成数据训练后，能很好地泛化到真实环境，优于现有方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决单目结构光三维成像在复杂场景下的鲁棒性问题。传统方法依赖像素域的匹配，容易受到遮挡、精细结构和非朗伯表面等因素的干扰，导致深度估计精度下降。\\n\\n**核心思路**：论文的核心思路是将像素域的匹配问题转化为特征空间的匹配问题。通过提取投影图案和红外图像的神经特征，并在特征空间构建代价体，利用学习到的特征进行更鲁棒的对应关系匹配。这种方法能够更好地利用图像的几何先验信息，从而提高深度估计的准确性和鲁棒性。\\n\\n**技术框架**：整体框架包含三个主要模块：特征提取模块、代价体构建与匹配模块、深度细化模块。首先，使用卷积神经网络提取投影图案和红外图像的神经特征。然后，基于提取的特征构建代价体，并通过学习到的匹配函数在特征空间中寻找对应关系。最后，利用单目深度估计模型的先验知识，对深度图进行细化，以提高细节恢复和全局一致性。\\n\\n**关键创新**：最重要的创新点在于将结构光解码问题从像素域转移到特征域。通过学习到的神经特征进行匹配，能够更好地应对复杂场景中的挑战，例如遮挡和非朗伯表面。此外，利用单目深度估计模型的先验知识进行深度细化，进一步提高了深度图的质量。\\n\\n**关键设计**：论文设计了一个基于物理的结构光渲染管道，用于生成大规模的合成训练数据。代价体构建采用多尺度策略，以提高匹配的准确性。深度细化模块利用预训练的单目深度估计模型作为先验，并采用残差学习的方式进行微调。损失函数包括深度损失、法向量损失和图像重建损失，以保证深度图的准确性和一致性。",
            "application_zh": "该研究成果可广泛应用于人脸识别、三维重建、机器人导航、工业检测等领域。特别是在对精度和鲁棒性要求较高的场景下，例如移动设备的3D人脸解锁、室内环境的机器人自主导航等，具有重要的应用价值。未来，该方法有望进一步扩展到室外环境，并与其他传感器融合，实现更精确、更可靠的三维感知。",
            "highlight_zh": "该方法在合成数据上训练后，能够很好地泛化到真实世界的室内环境，并且能够有效地处理各种结构光图案，无需重新训练。实验结果表明，该方法在深度估计精度上显著优于传统的结构光系统和基于被动立体视觉的深度估计方法。具体性能数据未知，但论文强调了其一致性的优越表现。",
            "tags_zh": [
                "结构光三维成像",
                "神经特征解码",
                "深度估计",
                "特征匹配",
                "单目深度估计"
            ],
            "_index": 20,
            "_used_api": "gemini"
        },
        {
            "title": "Trajectory Tracking for Multi-Manipulator Systems in Constrained Environments",
            "authors": [
                "Mayank Sewlia",
                "Christos K. Verginis",
                "Dimos V. Dimarogonas"
            ],
            "arxiv_id": "2512.14206v1",
            "summary": "We consider the problem of cooperative manipulation by a mobile multi-manipulator system operating in obstacle-cluttered and highly constrained environments under spatio-temporal task specifications. The task requires transporting a grasped object while respecting both continuous robot dynamics and discrete geometric constraints arising from obstacles and narrow passages. To address this hybrid structure, we propose a multi-rate planning and control framework that combines offline generation of an STL-satisfying object trajectory and collision-free base footprints with online constrained inverse kinematics and continuous-time feedback control. The resulting closed-loop system enables coordinated reconfiguration of multiple manipulators while tracking the desired object motion. The approach is evaluated in high-fidelity physics simulations using three Franka Emika Panda mobile manipulators rigidly grasping an object.",
            "categories": [
                "cs.RO",
                "eess.SY"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14206v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "灵巧操作 (Dexterous Manipulation)",
                    "matched_keywords": [
                        "grasp",
                        "grasping"
                    ],
                    "title_matches": [],
                    "abstract_matches": [
                        "grasp",
                        "grasping"
                    ],
                    "score": 2.4,
                    "weight": 1.2
                }
            ],
            "relevance_score": 2.4,
            "combination_bonus": 0.0,
            "headline_zh": "提出多速率规划与控制框架，解决约束环境下多机械臂系统的轨迹跟踪问题",
            "summary_zh": "本文研究了移动多机械臂系统在复杂约束环境中协同操作的问题，该环境包含障碍物和狭窄通道，并具有时空任务规范。任务要求在满足连续机器人动力学和离散几何约束（由障碍物和狭窄通道引起）的同时，运输抓取的物体。为了解决这种混合结构，我们提出了一种多速率规划和控制框架，该框架结合了离线生成的满足STL的对象轨迹和无碰撞的基座足迹，以及在线约束逆运动学和连续时间反馈控制。由此产生的闭环系统能够协调多个机械臂的重构，同时跟踪期望的物体运动。该方法在高保真物理模拟中使用三个Franka Emika Panda移动机械臂刚性抓取一个物体进行了评估。",
            "intro_zh": [
                "现有方法难以在复杂约束环境中实现多机械臂系统的精确轨迹跟踪，尤其是在考虑机器人动力学和环境几何约束时。",
                "论文提出一种多速率规划与控制框架，结合离线轨迹生成和在线约束逆运动学，实现多机械臂的协同重构和精确轨迹跟踪。",
                "通过高保真物理仿真，验证了该方法在三个Franka Emika Panda移动机械臂上的有效性，展示了其在复杂环境下的轨迹跟踪能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多机械臂系统在复杂约束环境中进行轨迹跟踪的问题。现有方法在处理具有时空任务规范、同时考虑连续机器人动力学和离散几何约束（如障碍物和狭窄通道）时面临挑战，难以实现精确和稳定的轨迹跟踪。\\n\\n**核心思路**：论文的核心思路是将轨迹规划和控制解耦，采用多速率框架。首先，离线生成满足时序逻辑（STL）规范的对象轨迹和无碰撞的基座足迹。然后，在线利用约束逆运动学和连续时间反馈控制，实现机械臂的协调重构和对期望轨迹的跟踪。这种解耦策略降低了问题的复杂度，并允许针对不同速率的需求进行优化。\\n\\n**技术框架**：整体框架包含以下几个主要模块：1) 离线轨迹规划器：生成满足STL规范的对象轨迹和无碰撞的基座足迹；2) 在线约束逆运动学求解器：根据期望的对象轨迹和基座位置，计算机械臂的关节角度；3) 连续时间反馈控制器：利用反馈信息修正关节角度，实现精确的轨迹跟踪。这些模块协同工作，确保系统在满足约束的同时，实现期望的物体运动。\\n\\n**关键创新**：论文的关键创新在于多速率规划与控制框架的设计。通过离线生成全局轨迹和在线实时控制相结合，有效地处理了复杂约束环境下的轨迹跟踪问题。此外，将时序逻辑（STL）引入轨迹规划，使得系统能够处理更复杂的任务规范。\\n\\n**关键设计**：论文的关键设计包括：1) 使用STL公式来描述任务规范，并利用现有的STL规划器生成对象轨迹；2) 设计约束逆运动学求解器，考虑机器人动力学和环境几何约束；3) 采用连续时间反馈控制，提高系统的鲁棒性和精度。具体的参数设置和损失函数选择可能需要根据具体的机器人和环境进行调整。",
            "application_zh": "该研究成果可应用于自动化装配、物流搬运、灾难救援等领域。在这些场景中，多机械臂系统需要在复杂和受限的环境中协同操作，完成特定的任务。该方法能够提高多机械臂系统的自主性和适应性，使其能够更好地应对实际应用中的挑战，具有重要的实际价值和应用前景。",
            "highlight_zh": "论文通过高保真物理仿真验证了所提出方法的有效性。实验结果表明，该方法能够成功地控制三个Franka Emika Panda移动机械臂，使其在复杂约束环境中协同抓取和运输物体，并精确地跟踪期望的轨迹。虽然论文中没有给出具体的性能数据和对比基线，但仿真结果直观地展示了该方法在复杂环境下的轨迹跟踪能力。",
            "tags_zh": [
                "多机械臂系统",
                "轨迹跟踪",
                "约束环境",
                "多速率控制",
                "时序逻辑规划"
            ],
            "_index": 21,
            "_used_api": "gemini"
        },
        {
            "title": "ASAP-Textured Gaussians: Enhancing Textured Gaussians with Adaptive Sampling and Anisotropic Parameterization",
            "authors": [
                "Meng Wei",
                "Cheng Zhang",
                "Jianmin Zheng",
                "Hamid Rezatofighi",
                "Jianfei Cai"
            ],
            "arxiv_id": "2512.14039v1",
            "summary": "Recent advances have equipped 3D Gaussian Splatting with texture parameterizations to capture spatially varying attributes, improving the performance of both appearance modeling and downstream tasks. However, the added texture parameters introduce significant memory efficiency challenges. Rather than proposing new texture formulations, we take a step back to examine the characteristics of existing textured Gaussian methods and identify two key limitations in common: (1) Textures are typically defined in canonical space, leading to inefficient sampling that wastes textures' capacity on low-contribution regions; and (2) texture parameterization is uniformly assigned across all Gaussians, regardless of their visual complexity, resulting in over-parameterization. In this work, we address these issues through two simple yet effective strategies: adaptive sampling based on the Gaussian density distribution and error-driven anisotropic parameterization that allocates texture resources according to rendering error. Our proposed ASAP Textured Gaussians, short for Adaptive Sampling and Anisotropic Parameterization, significantly improve the quality efficiency tradeoff, achieving high-fidelity rendering with far fewer texture parameters.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14039v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "3D重建与高斯 (3D Reconstruction & Gaussian)",
                    "matched_keywords": [
                        "3D Gaussian Splatting",
                        "Gaussian splatting"
                    ],
                    "title_matches": [],
                    "abstract_matches": [
                        "3D Gaussian Splatting",
                        "Gaussian splatting"
                    ],
                    "score": 2.4,
                    "weight": 1.2
                }
            ],
            "relevance_score": 2.4,
            "combination_bonus": 0.0,
            "headline_zh": "ASAP-Textured Gaussians：通过自适应采样和各向异性参数化增强纹理高斯模型",
            "summary_zh": "最近的进展为3D高斯溅射配备了纹理参数化，以捕获空间变化的属性，从而提高了外观建模和下游任务的性能。然而，增加的纹理参数带来了显著的内存效率挑战。本文没有提出新的纹理公式，而是回顾了现有纹理高斯方法的特性，并确定了两个共同的关键限制：（1）纹理通常在规范空间中定义，导致低效的采样，将纹理容量浪费在低贡献区域；（2）纹理参数化在所有高斯模型中统一分配，而不管其视觉复杂性如何，导致过度参数化。本文通过两种简单而有效的策略来解决这些问题：基于高斯密度分布的自适应采样和根据渲染误差分配纹理资源的误差驱动的各向异性参数化。我们提出的ASAP Textured Gaussians（自适应采样和各向异性参数化的简称）显著提高了质量-效率的权衡，以更少的纹理参数实现了高保真渲染。",
            "intro_zh": [
                "现有纹理高斯方法在规范空间采样纹理，效率低，且纹理参数均匀分配，造成过度参数化。",
                "提出ASAP Textured Gaussians，通过自适应采样和各向异性参数化，优化纹理资源的分配。",
                "实验表明，ASAP Textured Gaussians在显著减少纹理参数的同时，实现了高保真渲染效果。"
            ],
            "method_zh": "**问题定义**：现有基于纹理的3D高斯溅射方法在内存效率方面存在挑战。主要痛点在于，纹理采样效率低下，大量纹理容量被浪费在对最终渲染贡献较小的区域。此外，现有方法对所有高斯模型采用统一的纹理参数化，忽略了不同高斯模型视觉复杂度的差异，导致过度参数化。\n\n**核心思路**：本文的核心思路是根据高斯模型的密度分布进行自适应纹理采样，并根据渲染误差进行各向异性纹理参数化。通过这种方式，纹理资源可以更有效地分配给对渲染结果影响更大的区域，从而在减少纹理参数的同时保持或提高渲染质量。\n\n**技术框架**：ASAP Textured Gaussians的整体框架包括两个主要部分：自适应采样和各向异性参数化。首先，根据高斯模型的密度分布进行自适应采样，确定纹理空间中哪些区域需要更精细的采样。然后，根据渲染误差，对不同的高斯模型分配不同数量的纹理参数，实现各向异性参数化。这两个部分共同作用，优化纹理资源的分配。\n\n**关键创新**：本文最重要的技术创新在于提出了自适应采样和各向异性参数化相结合的策略。与现有方法相比，ASAP Textured Gaussians能够更有效地利用纹理资源，在减少纹理参数的同时保持或提高渲染质量。自适应采样解决了纹理采样效率低下的问题，而各向异性参数化解决了过度参数化的问题。\n\n**关键设计**：自适应采样基于高斯密度分布，具体实现方式未知。各向异性参数化根据渲染误差动态调整每个高斯模型的纹理参数数量，具体误差计算方式和参数调整策略未知。损失函数可能包含渲染损失和正则化项，以平衡渲染质量和参数数量。",
            "application_zh": "ASAP-Textured Gaussians可应用于三维重建、虚拟现实、增强现实等领域。通过减少纹理参数，可以降低存储和计算成本，提高渲染效率，从而在移动设备等资源受限的平台上实现高质量的3D渲染。该研究还有助于推动3D内容创作和编辑工具的发展。",
            "highlight_zh": "论文提出的ASAP Textured Gaussians在减少纹理参数的同时，实现了与现有方法相当甚至更高的渲染质量。具体的性能数据和对比基线未知，但摘要中强调了该方法在质量-效率权衡方面的显著提升。实验结果表明，自适应采样和各向异性参数化能够有效地优化纹理资源的分配。",
            "tags_zh": [
                "3D高斯溅射",
                "纹理参数化",
                "自适应采样",
                "各向异性参数化",
                "渲染优化",
                "三维重建",
                "内存效率"
            ],
            "_index": 22,
            "_used_api": "gemini"
        },
        {
            "title": "A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning",
            "authors": [
                "Zixin Zhang",
                "Kanghao Chen",
                "Hanqing Wang",
                "Hongfei Zhang",
                "Harold Haodong Chen",
                "Chenfei Liao",
                "Litao Guo",
                "Ying-Cong Chen"
            ],
            "arxiv_id": "2512.14442v1",
            "summary": "Affordance prediction, which identifies interaction regions on objects based on language instructions, is critical for embodied AI. Prevailing end-to-end models couple high-level reasoning and low-level grounding into a single monolithic pipeline and rely on training over annotated datasets, which leads to poor generalization on novel objects and unseen environments. In this paper, we move beyond this paradigm by proposing A4-Agent, a training-free agentic framework that decouples affordance prediction into a three-stage pipeline. Our framework coordinates specialized foundation models at test time: (1) a $\\textbf{Dreamer}$ that employs generative models to visualize $\\textit{how}$ an interaction would look; (2) a $\\textbf{Thinker}$ that utilizes large vision-language models to decide $\\textit{what}$ object part to interact with; and (3) a $\\textbf{Spotter}$ that orchestrates vision foundation models to precisely locate $\\textit{where}$ the interaction area is. By leveraging the complementary strengths of pre-trained models without any task-specific fine-tuning, our zero-shot framework significantly outperforms state-of-the-art supervised methods across multiple benchmarks and demonstrates robust generalization to real-world settings.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14442v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "Sim2Real与策略学习 (Sim2Real & Policy Learning)",
                    "matched_keywords": [
                        "dreamer"
                    ],
                    "title_matches": [],
                    "abstract_matches": [
                        "dreamer"
                    ],
                    "score": 1.0,
                    "weight": 1.0
                },
                {
                    "name": "具身智能 (Embodied AI)",
                    "matched_keywords": [
                        "embodied AI"
                    ],
                    "title_matches": [],
                    "abstract_matches": [
                        "embodied AI"
                    ],
                    "score": 1.0,
                    "weight": 1.0
                }
            ],
            "relevance_score": 2.0,
            "combination_bonus": 0.0,
            "headline_zh": "提出A4-Agent，一个用于零样本可供性推理的Agent框架，无需训练即可超越现有监督方法。",
            "summary_zh": "本文提出A4-Agent，一个用于可供性预测的免训练Agent框架。可供性预测旨在根据语言指令识别物体上的交互区域，对具身智能至关重要。现有的端到端模型将高层推理和低层基础耦合到一个单一的pipeline中，并依赖于带注释数据集的训练，导致对新物体和未见环境的泛化能力较差。A4-Agent将可供性预测解耦为一个三阶段pipeline，在测试时协调专门的基础模型：(1) Dreamer，利用生成模型来可视化交互的样子；(2) Thinker，利用大型视觉-语言模型来决定与哪个物体部分进行交互；(3) Spotter，协调视觉基础模型来精确定位交互区域。该零样本框架利用预训练模型的互补优势，无需任何特定于任务的微调，在多个基准测试中显著优于最先进的监督方法，并展示了对真实世界环境的鲁棒泛化能力。",
            "intro_zh": [
                "现有可供性预测模型依赖端到端训练，泛化性差，难以适应新物体和环境。",
                "A4-Agent将可供性预测解耦为三个阶段，分别由Dreamer、Thinker和Spotter三个模块实现。",
                "A4-Agent无需训练，利用预训练模型优势互补，在多个基准测试中超越了现有监督方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决可供性预测问题，即根据语言指令确定物体上可交互的区域。现有端到端模型将高层推理和低层感知耦合，依赖大量标注数据训练，导致在新物体和未见环境下的泛化能力不足。这些模型难以有效利用预训练模型的知识，需要为特定任务进行微调。\n\n**核心思路**：论文的核心思路是将可供性预测任务解耦为三个独立的阶段，分别对应交互的可视化（Dreamer）、交互对象部件的决策（Thinker）和交互区域的精确定位（Spotter）。每个阶段利用不同的预训练模型，发挥各自的优势，并通过Agent框架进行协调。这种解耦的设计使得模型能够更好地利用预训练知识，实现零样本泛化。\n\n**技术框架**：A4-Agent框架包含三个主要模块：Dreamer、Thinker和Spotter。Dreamer使用生成模型（如Stable Diffusion）根据语言指令生成交互的视觉效果。Thinker使用大型视觉-语言模型（如CLIP）来判断应该与物体的哪个部分进行交互。Spotter使用视觉基础模型（如SAM）来精确定位交互区域。这三个模块按顺序执行，形成一个pipeline。\n\n**关键创新**：A4-Agent的关键创新在于其Agentic框架和解耦的设计。通过将可供性预测分解为三个独立的子任务，并利用不同的预训练模型来解决这些子任务，A4-Agent能够更好地利用预训练知识，实现零样本泛化。此外，该框架无需任何特定于任务的微调，降低了训练成本。\n\n**关键设计**：Dreamer模块使用Stable Diffusion等文本到图像生成模型，根据语言指令生成交互图像。Thinker模块使用CLIP等视觉-语言模型，计算图像和文本之间的相似度，选择最相关的物体部件。Spotter模块使用SAM等分割模型，根据Thinker的输出，分割出交互区域。具体参数设置和损失函数取决于所使用的预训练模型。",
            "application_zh": "A4-Agent在机器人操作、虚拟助手和增强现实等领域具有广泛的应用前景。它可以帮助机器人理解人类指令，并与环境中的物体进行交互。例如，机器人可以根据“打开抽屉”的指令，自动识别抽屉的位置并执行打开操作。该研究还可以用于开发更智能的虚拟助手，使其能够更好地理解用户的意图并提供相应的服务。在增强现实中，A4-Agent可以帮助用户识别物体上的可交互区域，并提供相应的操作提示。",
            "highlight_zh": "A4-Agent在多个可供性预测基准测试中显著优于最先进的监督方法，实现了零样本泛化。实验结果表明，A4-Agent在 unseen 的物体和环境中表现出强大的鲁棒性。具体性能数据在论文中给出，相较于需要大量训练数据的监督学习方法，A4-Agent无需训练即可达到甚至超越其性能。",
            "tags_zh": [
                "可供性预测",
                "零样本学习",
                "具身智能",
                "Agent框架",
                "预训练模型",
                "视觉-语言模型",
                "物体交互"
            ],
            "_index": 23,
            "_used_api": "gemini"
        }
    ]
}