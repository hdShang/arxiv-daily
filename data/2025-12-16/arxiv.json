{
  "count": 24,
  "papers": [
    {
      "title": "SUPER -- A Framework for Sensitivity-based Uncertainty-aware Performance and Risk Assessment in Visual Inertial Odometry",
      "authors": [
        "Johannes A. Gaus",
        "Daniel Häufle",
        "Woo-Jeong Baek"
      ],
      "arxiv_id": "2512.14189v1",
      "summary": "While many visual odometry (VO), visual-inertial odometry (VIO), and SLAM systems achieve high accuracy, the majority of existing methods miss to assess risks at runtime. This paper presents SUPER (Sensitivity-based Uncertainty-aware PErformance and Risk assessment) that is a generic and explainable framework that propagates uncertainties via sensitivities for real-time risk assessment in VIO. The scientific novelty lies in the derivation of a real-time risk indicator that is backend-agnostic and exploits the Schur complement blocks of the Gauss-Newton normal matrix to propagate uncertainties. Practically, the Schur complement captures the sensitivity that reflects the influence of the uncertainty on the risk occurrence. Our framework estimates risks on the basis of the residual magnitudes, geometric conditioning, and short horizon temporal trends without requiring ground truth knowledge. Our framework enables to reliably predict trajectory degradation 50 frames ahead with an improvement of 20% to the baseline. In addition, SUPER initiates a stop or relocalization policy with 89.1% recall. The framework is backend agnostic and operates in real time with less than 0.2% additional CPU cost. Experiments show that SUPER provides consistent uncertainty estimates. A SLAM evaluation highlights the applicability to long horizon mapping.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-16",
      "updated": "2025-12-16",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14189v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "视觉里程计与SLAM (VO & SLAM)",
          "matched_keywords": [
            "visual odometry",
            "[T]visual-inertial",
            "[T]visual inertial",
            "VIO",
            "SLAM"
          ],
          "title_matches": [
            "visual-inertial",
            "visual inertial"
          ],
          "abstract_matches": [
            "visual odometry",
            "VIO",
            "SLAM"
          ],
          "score": 11.7,
          "weight": 1.3
        }
      ],
      "relevance_score": 11.7,
      "combination_bonus": 0.0
    },
    {
      "title": "CaFe-TeleVision: A Coarse-to-Fine Teleoperation System with Immersive Situated Visualization for Enhanced Ergonomics",
      "authors": [
        "Zixin Tang",
        "Yiming Chen",
        "Quentin Rouxel",
        "Dianxi Li",
        "Shuang Wu",
        "Fei Chen"
      ],
      "arxiv_id": "2512.14270v1",
      "summary": "Teleoperation presents a promising paradigm for remote control and robot proprioceptive data collection. Despite recent progress, current teleoperation systems still suffer from limitations in efficiency and ergonomics, particularly in challenging scenarios. In this paper, we propose CaFe-TeleVision, a coarse-to-fine teleoperation system with immersive situated visualization for enhanced ergonomics. At its core, a coarse-to-fine control mechanism is proposed in the retargeting module to bridge workspace disparities, jointly optimizing efficiency and physical ergonomics. To stream immersive feedback with adequate visual cues for human vision systems, an on-demand situated visualization technique is integrated in the perception module, which reduces the cognitive load for multi-view processing. The system is built on a humanoid collaborative robot and validated with six challenging bimanual manipulation tasks. User study among 24 participants confirms that CaFe-TeleVision enhances ergonomics with statistical significance, indicating a lower task load and a higher user acceptance during teleoperation. Quantitative results also validate the superior performance of our system across six tasks, surpassing comparative methods by up to 28.89% in success rate and accelerating by 26.81% in completion time. Project webpage: https://clover-cuhk.github.io/cafe_television/",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-16",
      "updated": "2025-12-16",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14270v1",
      "code_links": [
        {
          "url": "https://clover-cuhk.github.io/cafe_television/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "遥操作与动作重定向 (Teleoperation & Retargeting)",
          "matched_keywords": [
            "[T]teleoperation system"
          ],
          "title_matches": [
            "teleoperation system"
          ],
          "abstract_matches": [],
          "score": 5.4,
          "weight": 1.8
        },
        {
          "name": "灵巧操作 (Dexterous Manipulation)",
          "matched_keywords": [
            "bi-manual",
            "bimanual"
          ],
          "title_matches": [],
          "abstract_matches": [
            "bi-manual",
            "bimanual"
          ],
          "score": 2.4,
          "weight": 1.2
        }
      ],
      "relevance_score": 10.8,
      "combination_bonus": 3.0
    },
    {
      "title": "Odyssey: An Automotive Lidar-Inertial Odometry Dataset for GNSS-denied situations",
      "authors": [
        "Aaron Kurda",
        "Simon Steuernagel",
        "Lukas Jung",
        "Marcus Baum"
      ],
      "arxiv_id": "2512.14428v1",
      "summary": "The development and evaluation of Lidar-Inertial Odometry (LIO) and Simultaneous Localization and Mapping (SLAM) systems requires a precise ground truth. The Global Navigation Satellite System (GNSS) is often used as a foundation for this, but its signals can be unreliable in obstructed environments due to multi-path effects or loss-of-signal. While existing datasets compensate for the sporadic loss of GNSS signals by incorporating Inertial Measurement Unit (IMU) measurements, the commonly used Micro-Electro-Mechanical Systems (MEMS) or Fiber Optic Gyroscope (FOG)-based systems do not permit the prolonged study of GNSS-denied environments. To close this gap, we present Odyssey, a LIO dataset with a focus on GNSS-denied environments such as tunnels and parking garages as well as other underrepresented, yet ubiquitous situations such as stop-and-go-traffic, bumpy roads and wide open fields. Our ground truth is derived from a navigation-grade Inertial Navigation System (INS) equipped with a Ring Laser Gyroscope (RLG), offering exceptional bias stability characteristics compared to IMUs used in existing datasets and enabling the prolonged and accurate study of GNSS-denied environments. This makes Odyssey the first publicly available dataset featuring a RLG-based INS. Besides providing data for LIO, we also support other tasks, such as place recognition, through the threefold repetition of all trajectories as well as the integration of external mapping data by providing precise geodetic coordinates. All data, dataloader and other material is available online at https://odyssey.uni-goettingen.de/ .",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-16",
      "updated": "2025-12-16",
      "comment": "9 pages, 4 figures, submitted to International Journal of Robotics Research (IJRR)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14428v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "视觉里程计与SLAM (VO & SLAM)",
          "matched_keywords": [
            "[T]lidar-inertial",
            "[T]lidar inertial",
            "LIO",
            "SLAM"
          ],
          "title_matches": [
            "lidar-inertial",
            "lidar inertial"
          ],
          "abstract_matches": [
            "LIO",
            "SLAM"
          ],
          "score": 10.4,
          "weight": 1.3
        }
      ],
      "relevance_score": 10.4,
      "combination_bonus": 0.0
    },
    {
      "title": "CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation",
      "authors": [
        "Sirui Chen",
        "Zi-ang Cao",
        "Zhengyi Luo",
        "Fernando Castañeda",
        "Chenran Li",
        "Tingwu Wang",
        "Ye Yuan",
        "Linxi \"Jim\" Fan",
        "C. Karen Liu",
        "Yuke Zhu"
      ],
      "arxiv_id": "2512.14689v1",
      "summary": "Recent progress in humanoid robots has unlocked agile locomotion skills, including backflipping, running, and crawling. Yet it remains challenging for a humanoid robot to perform forceful manipulation tasks such as moving objects, wiping, and pushing a cart. We propose adaptive Compliance Humanoid control through hIsight Perturbation (CHIP), a plug-and-play module that enables controllable end-effector stiffness while preserving agile tracking of dynamic reference motions. CHIP is easy to implement and requires neither data augmentation nor additional reward tuning. We show that a generalist motion-tracking controller trained with CHIP can perform a diverse set of forceful manipulation tasks that require different end-effector compliance, such as multi-robot collaboration, wiping, box delivery, and door opening.",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-16",
      "updated": "2025-12-16",
      "comment": "The first two authors contributed equally. Project page: https://nvlabs.github.io/CHIP/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14689v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "鲁棒恢复与高动态运动 (Robust Recovery & Agility)",
          "matched_keywords": [
            "agile locomotion"
          ],
          "title_matches": [],
          "abstract_matches": [
            "agile locomotion"
          ],
          "score": 2.0,
          "weight": 2.0
        },
        {
          "name": "基于物理的动画与对抗先验 (Physics Animation & AMP)",
          "matched_keywords": [
            "reference motion"
          ],
          "title_matches": [],
          "abstract_matches": [
            "reference motion"
          ],
          "score": 2.0,
          "weight": 2.0
        },
        {
          "name": "人形/四足移动控制 (Legged Locomotion)",
          "matched_keywords": [
            "[T]humanoid control",
            "humanoid robot"
          ],
          "title_matches": [
            "humanoid control"
          ],
          "abstract_matches": [
            "humanoid robot"
          ],
          "score": 6.0,
          "weight": 1.5
        }
      ],
      "relevance_score": 10.0,
      "combination_bonus": 0.0
    },
    {
      "title": "CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives",
      "authors": [
        "Zihan Wang",
        "Jiashun Wang",
        "Jeff Tan",
        "Yiwen Zhao",
        "Jessica Hodgins",
        "Shubham Tulsiani",
        "Deva Ramanan"
      ],
      "arxiv_id": "2512.14696v1",
      "summary": "We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2\\% to 6.9\\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\\% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR.",
      "categories": [
        "cs.CV",
        "cs.GR",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-16",
      "updated": "2025-12-16",
      "comment": "Project page: https://crisp-real2sim.github.io/CRISP-Real2Sim/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14696v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "人形/四足移动控制 (Legged Locomotion)",
          "matched_keywords": [
            "humanoid control"
          ],
          "title_matches": [],
          "abstract_matches": [
            "humanoid control"
          ],
          "score": 1.5,
          "weight": 1.5
        },
        {
          "name": "3D重建与高斯 (3D Reconstruction & Gaussian)",
          "matched_keywords": [
            "scene reconstruction",
            "point cloud"
          ],
          "title_matches": [],
          "abstract_matches": [
            "scene reconstruction",
            "point cloud"
          ],
          "score": 2.4,
          "weight": 1.2
        },
        {
          "name": "Sim2Real与策略学习 (Sim2Real & Policy Learning)",
          "matched_keywords": [
            "[T]real2sim"
          ],
          "title_matches": [
            "real2sim"
          ],
          "abstract_matches": [],
          "score": 3.0,
          "weight": 1.0
        }
      ],
      "relevance_score": 9.9,
      "combination_bonus": 3.0
    },
    {
      "title": "Beyond a Single Light: A Large-Scale Aerial Dataset for Urban Scene Reconstruction Under Varying Illumination",
      "authors": [
        "Zhuoxiao Li",
        "Wenzong Ma",
        "Taoyu Wu",
        "Jinjing Zhu",
        "Zhenchao Q",
        "Shuai Zhang",
        "Jing Ou",
        "Yinrui Ren",
        "Weiqing Qi",
        "Guobin Shen",
        "Hui Xiong",
        "Wufan Zhao"
      ],
      "arxiv_id": "2512.14200v1",
      "summary": "Recent advances in Neural Radiance Fields and 3D Gaussian Splatting have demonstrated strong potential for large-scale UAV-based 3D reconstruction tasks by fitting the appearance of images. However, real-world large-scale captures are often based on multi-temporal data capture, where illumination inconsistencies across different times of day can significantly lead to color artifacts, geometric inaccuracies, and inconsistent appearance. Due to the lack of UAV datasets that systematically capture the same areas under varying illumination conditions, this challenge remains largely underexplored. To fill this gap, we introduceSkyLume, a large-scale, real-world UAV dataset specifically designed for studying illumination robust 3D reconstruction in urban scene modeling: (1) We collect data from 10 urban regions data comprising more than 100k high resolution UAV images (four oblique views and nadir), where each region is captured at three periods of the day to systematically isolate illumination changes. (2) To support precise evaluation of geometry and appearance, we provide per-scene LiDAR scans and accurate 3D ground-truth for assessing depth, surface normals, and reconstruction quality under varying illumination. (3) For the inverse rendering task, we introduce the Temporal Consistency Coefficient (TCC), a metric that measuress cross-time albedo stability and directly evaluates the robustness of the disentanglement of light and material. We aim for this resource to serve as a foundation that advances research and real-world evaluation in large-scale inverse rendering, geometry reconstruction, and novel view synthesis.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-16",
      "updated": "2025-12-16",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14200v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "3D重建与高斯 (3D Reconstruction & Gaussian)",
          "matched_keywords": [
            "3D Gaussian Splatting",
            "Gaussian splatting",
            "neural radiance",
            "novel view synthesis",
            "3D reconstruction",
            "[T]scene reconstruction"
          ],
          "title_matches": [
            "scene reconstruction"
          ],
          "abstract_matches": [
            "3D Gaussian Splatting",
            "Gaussian splatting",
            "neural radiance",
            "novel view synthesis",
            "3D reconstruction"
          ],
          "score": 9.6,
          "weight": 1.2
        }
      ],
      "relevance_score": 9.6,
      "combination_bonus": 0.0
    },
    {
      "title": "GaussianPlant: Structure-aligned Gaussian Splatting for 3D Reconstruction of Plants",
      "authors": [
        "Yang Yang",
        "Risa Shinoda",
        "Hiroaki Santo",
        "Fumio Okura"
      ],
      "arxiv_id": "2512.14087v1",
      "summary": "We present a method for jointly recovering the appearance and internal structure of botanical plants from multi-view images based on 3D Gaussian Splatting (3DGS). While 3DGS exhibits robust reconstruction of scene appearance for novel-view synthesis, it lacks structural representations underlying those appearances (e.g., branching patterns of plants), which limits its applicability to tasks such as plant phenotyping. To achieve both high-fidelity appearance and structural reconstruction, we introduce GaussianPlant, a hierarchical 3DGS representation, which disentangles structure and appearance. Specifically, we employ structure primitives (StPs) to explicitly represent branch and leaf geometry, and appearance primitives (ApPs) to the plants' appearance using 3D Gaussians. StPs represent a simplified structure of the plant, i.e., modeling branches as cylinders and leaves as disks. To accurately distinguish the branches and leaves, StP's attributes (i.e., branches or leaves) are optimized in a self-organized manner. ApPs are bound to each StP to represent the appearance of branches or leaves as in conventional 3DGS. StPs and ApPs are jointly optimized using a re-rendering loss on the input multi-view images, as well as the gradient flow from ApP to StP using the binding correspondence information. We conduct experiments to qualitatively evaluate the reconstruction accuracy of both appearance and structure, as well as real-world experiments to qualitatively validate the practical performance. Experiments show that the GaussianPlant achieves both high-fidelity appearance reconstruction via ApPs and accurate structural reconstruction via StPs, enabling the extraction of branch structure and leaf instances.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-16",
      "updated": "2025-12-16",
      "comment": "Submitted to IEEE TPAMI, under review",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14087v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "3D重建与高斯 (3D Reconstruction & Gaussian)",
          "matched_keywords": [
            "3D Gaussian Splatting",
            "3DGS",
            "[T]Gaussian splatting",
            "[T]3D reconstruction"
          ],
          "title_matches": [
            "Gaussian splatting",
            "3D reconstruction"
          ],
          "abstract_matches": [
            "3D Gaussian Splatting",
            "3DGS"
          ],
          "score": 9.6,
          "weight": 1.2
        }
      ],
      "relevance_score": 9.6,
      "combination_bonus": 0.0
    },
    {
      "title": "DASP: Self-supervised Nighttime Monocular Depth Estimation with Domain Adaptation of Spatiotemporal Priors",
      "authors": [
        "Yiheng Huang",
        "Junhong Chen",
        "Anqi Ning",
        "Zhanhong Liang",
        "Nick Michiels",
        "Luc Claesen",
        "Wenyin Liu"
      ],
      "arxiv_id": "2512.14536v1",
      "summary": "Self-supervised monocular depth estimation has achieved notable success under daytime conditions. However, its performance deteriorates markedly at night due to low visibility and varying illumination, e.g., insufficient light causes textureless areas, and moving objects bring blurry regions. To this end, we propose a self-supervised framework named DASP that leverages spatiotemporal priors for nighttime depth estimation. Specifically, DASP consists of an adversarial branch for extracting spatiotemporal priors and a self-supervised branch for learning. In the adversarial branch, we first design an adversarial network where the discriminator is composed of four devised spatiotemporal priors learning blocks (SPLB) to exploit the daytime priors. In particular, the SPLB contains a spatial-based temporal learning module (STLM) that uses orthogonal differencing to extract motion-related variations along the time axis and an axial spatial learning module (ASLM) that adopts local asymmetric convolutions with global axial attention to capture the multiscale structural information. By combining STLM and ASLM, our model can acquire sufficient spatiotemporal features to restore textureless areas and estimate the blurry regions caused by dynamic objects. In the self-supervised branch, we propose a 3D consistency projection loss to bilaterally project the target frame and source frame into a shared 3D space, and calculate the 3D discrepancy between the two projected frames as a loss to optimize the 3D structural consistency and daytime priors. Extensive experiments on the Oxford RobotCar and nuScenes datasets demonstrate that our approach achieves state-of-the-art performance for nighttime depth estimation. Ablation studies further validate the effectiveness of each component.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-16",
      "updated": "2025-12-16",
      "comment": "8 pages, 7 figures",
      "doi": "10.1109/LRA.2025.3644148",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14536v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "SOTA深度估计 (SOTA Depth Estimation)",
          "matched_keywords": [
            "[T]monocular depth",
            "[T]depth estimation"
          ],
          "title_matches": [
            "monocular depth",
            "depth estimation"
          ],
          "abstract_matches": [],
          "score": 9.0,
          "weight": 1.5
        }
      ],
      "relevance_score": 9.0,
      "combination_bonus": 0.0
    },
    {
      "title": "HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis",
      "authors": [
        "Kaizhe Zhang",
        "Yijie Zhou",
        "Weizhan Zhang",
        "Caixia Yan",
        "Haipeng Du",
        "yugui xie",
        "Yu-Hui Wen",
        "Yong-Jin Liu"
      ],
      "arxiv_id": "2512.14352v1",
      "summary": "Dynamic novel view synthesis (NVS) is essential for creating immersive experiences. Existing approaches have advanced dynamic NVS by introducing 3D Gaussian Splatting (3DGS) with implicit deformation fields or indiscriminately assigned time-varying parameters, surpassing NeRF-based methods. However, due to excessive model complexity and parameter redundancy, they incur large model sizes and slow rendering speeds, making them inefficient for real-time applications, particularly on resource-constrained devices. To obtain a more efficient model with fewer redundant parameters, in this paper, we propose Hybrid Gaussian Splatting (HGS), a compact and efficient framework explicitly designed to disentangle static and dynamic regions of a scene within a unified representation. The core innovation of HGS lies in our Static-Dynamic Decomposition (SDD) strategy, which leverages Radial Basis Function (RBF) modeling for Gaussian primitives. Specifically, for dynamic regions, we employ time-dependent RBFs to effectively capture temporal variations and handle abrupt scene changes, while for static regions, we reduce redundancy by sharing temporally invariant parameters. Additionally, we introduce a two-stage training strategy tailored for explicit models to enhance temporal coherence at static-dynamic boundaries. Experimental results demonstrate that our method reduces model size by up to 98% and achieves real-time rendering at up to 125 FPS at 4K resolution on a single RTX 3090 GPU. It further sustains 160 FPS at 1352 * 1014 on an RTX 3050 and has been integrated into the VR system. Moreover, HGS achieves comparable rendering quality to state-of-the-art methods while providing significantly improved visual fidelity for high-frequency details and abrupt scene changes.",
      "categories": [
        "cs.CV",
        "cs.CG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-16",
      "updated": "2025-12-16",
      "comment": "11 pages, 9 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14352v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "3D重建与高斯 (3D Reconstruction & Gaussian)",
          "matched_keywords": [
            "3D Gaussian Splatting",
            "3DGS",
            "[T]Gaussian splatting",
            "NeRF",
            "novel view synthesis"
          ],
          "title_matches": [
            "Gaussian splatting"
          ],
          "abstract_matches": [
            "3D Gaussian Splatting",
            "3DGS",
            "NeRF",
            "novel view synthesis"
          ],
          "score": 8.4,
          "weight": 1.2
        }
      ],
      "relevance_score": 8.4,
      "combination_bonus": 0.0
    },
    {
      "title": "EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models",
      "authors": [
        "Zechen Bai",
        "Chen Gao",
        "Mike Zheng Shou"
      ],
      "arxiv_id": "2512.14666v1",
      "summary": "Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\\% on long-horizon tasks, +22.0\\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\\% success on unseen tasks without task-specific demonstrations training (vs. 0\\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-16",
      "updated": "2025-12-16",
      "comment": "15 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14666v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "具身智能 (Embodied AI)",
          "matched_keywords": [
            "[T]VLA",
            "[T]vision-language-action"
          ],
          "title_matches": [
            "VLA",
            "vision-language-action"
          ],
          "abstract_matches": [],
          "score": 6.0,
          "weight": 1.0
        }
      ],
      "relevance_score": 6.0,
      "combination_bonus": 0.0
    },
    {
      "title": "Broadening View Synthesis of Dynamic Scenes from Constrained Monocular Videos",
      "authors": [
        "Le Jiang",
        "Shaotong Zhu",
        "Yedi Luo",
        "Shayda Moezzi",
        "Sarah Ostadabbas"
      ],
      "arxiv_id": "2512.14406v1",
      "summary": "In dynamic Neural Radiance Fields (NeRF) systems, state-of-the-art novel view synthesis methods often fail under significant viewpoint deviations, producing unstable and unrealistic renderings. To address this, we introduce Expanded Dynamic NeRF (ExpanDyNeRF), a monocular NeRF framework that leverages Gaussian splatting priors and a pseudo-ground-truth generation strategy to enable realistic synthesis under large-angle rotations. ExpanDyNeRF optimizes density and color features to improve scene reconstruction from challenging perspectives. We also present the Synthetic Dynamic Multiview (SynDM) dataset, the first synthetic multiview dataset for dynamic scenes with explicit side-view supervision-created using a custom GTA V-based rendering pipeline. Quantitative and qualitative results on SynDM and real-world datasets demonstrate that ExpanDyNeRF significantly outperforms existing dynamic NeRF methods in rendering fidelity under extreme viewpoint shifts. Further details are provided in the supplementary materials.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-16",
      "updated": "2025-12-16",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14406v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "3D重建与高斯 (3D Reconstruction & Gaussian)",
          "matched_keywords": [
            "Gaussian splatting",
            "NeRF",
            "neural radiance",
            "novel view synthesis",
            "scene reconstruction"
          ],
          "title_matches": [],
          "abstract_matches": [
            "Gaussian splatting",
            "NeRF",
            "neural radiance",
            "novel view synthesis",
            "scene reconstruction"
          ],
          "score": 6.0,
          "weight": 1.2
        }
      ],
      "relevance_score": 6.0,
      "combination_bonus": 0.0
    },
    {
      "title": "Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model",
      "authors": [
        "Zhaofeng Hu",
        "Hongrui Yu",
        "Vaidhyanathan Chandramouli",
        "Ci-Jyun Liang"
      ],
      "arxiv_id": "2512.14031v1",
      "summary": "This study evaluates two leading approaches for teaching construction robots new skills to understand their applicability for construction automation: a Vision-Language-Action (VLA) model and Reinforcement Learning (RL) methods. The goal is to understand both task performance and the practical effort needed to deploy each approach on real jobs. The authors developed two teleoperation interfaces to control the robots and collect the demonstrations needed, both of which proved effective for training robots for long-horizon and dexterous tasks. In addition, the authors conduct a three-stage evaluation. First, the authors compare a Multi-Layer Perceptron (MLP) policy with a Deep Q-network (DQN) imitation model to identify the stronger RL baseline, focusing on model performance, generalization, and a pick-up experiment. Second, three different VLA models are trained in two different scenarios and compared with each other. Third, the authors benchmark the selected RL baseline against the VLA model using computational and sample-efficiency measures and then a robot experiment on a multi-stage panel installation task that includes transport and installation. The VLA model demonstrates strong generalization and few-shot capability, achieving 60% and 100% success in the pickup phase. In comparison, DQN can be made robust but needs additional noise during tuning, which increases the workload. Overall, the findings indicate that VLA offers practical advantages for changing tasks by reducing programming effort and enabling useful performance with minimal data, while DQN provides a viable baseline when sufficient tuning effort is acceptable.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-16",
      "updated": "2025-12-16",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14031v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "具身智能 (Embodied AI)",
          "matched_keywords": [
            "[T]VLA",
            "[T]vision-language-action"
          ],
          "title_matches": [
            "VLA",
            "vision-language-action"
          ],
          "abstract_matches": [],
          "score": 6.0,
          "weight": 1.0
        }
      ],
      "relevance_score": 6.0,
      "combination_bonus": 0.0
    },
    {
      "title": "Deep Learning Perspective of Scene Understanding in Autonomous Robots",
      "authors": [
        "Afia Maham",
        "Dur E Nayab Tashfa"
      ],
      "arxiv_id": "2512.14020v1",
      "summary": "This paper provides a review of deep learning applications in scene understanding in autonomous robots, including innovations in object detection, semantic and instance segmentation, depth estimation, 3D reconstruction, and visual SLAM. It emphasizes how these techniques address limitations of traditional geometric models, improve depth perception in real time despite occlusions and textureless surfaces, and enhance semantic reasoning to understand the environment better. When these perception modules are integrated into dynamic and unstructured environments, they become more effective in decisionmaking, navigation and interaction. Lastly, the review outlines the existing problems and research directions to advance learning-based scene understanding of autonomous robots.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-16",
      "updated": "2025-12-16",
      "comment": "11 pages. Review Paper on Deep Learning Perspective of Scene Understanding in Autonomous Robots",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14020v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "SOTA深度估计 (SOTA Depth Estimation)",
          "matched_keywords": [
            "depth estimation"
          ],
          "title_matches": [],
          "abstract_matches": [
            "depth estimation"
          ],
          "score": 1.5,
          "weight": 1.5
        },
        {
          "name": "视觉里程计与SLAM (VO & SLAM)",
          "matched_keywords": [
            "visual SLAM",
            "SLAM"
          ],
          "title_matches": [],
          "abstract_matches": [
            "visual SLAM",
            "SLAM"
          ],
          "score": 2.6,
          "weight": 1.3
        },
        {
          "name": "3D重建与高斯 (3D Reconstruction & Gaussian)",
          "matched_keywords": [
            "3D reconstruction"
          ],
          "title_matches": [],
          "abstract_matches": [
            "3D reconstruction"
          ],
          "score": 1.2,
          "weight": 1.2
        }
      ],
      "relevance_score": 5.3,
      "combination_bonus": 0.0
    },
    {
      "title": "FastDDHPose: Towards Unified, Efficient, and Disentangled 3D Human Pose Estimation",
      "authors": [
        "Qingyuan Cai",
        "Linxin Zhang",
        "Xuecai Hu",
        "Saihui Hou",
        "Yongzhen Huang"
      ],
      "arxiv_id": "2512.14162v1",
      "summary": "Recent approaches for monocular 3D human pose estimation (3D HPE) have achieved leading performance by directly regressing 3D poses from 2D keypoint sequences. Despite the rapid progress in 3D HPE, existing methods are typically trained and evaluated under disparate frameworks, lacking a unified framework for fair comparison. To address these limitations, we propose Fast3DHPE, a modular framework that facilitates rapid reproduction and flexible development of new methods. By standardizing training and evaluation protocols, Fast3DHPE enables fair comparison across 3D human pose estimation methods while significantly improving training efficiency. Within this framework, we introduce FastDDHPose, a Disentangled Diffusion-based 3D Human Pose Estimation method which leverages the strong latent distribution modeling capability of diffusion models to explicitly model the distributions of bone length and bone direction while avoiding further amplification of hierarchical error accumulation. Moreover, we design an efficient Kinematic-Hierarchical Spatial and Temporal Denoiser that encourages the model to focus on kinematic joint hierarchies while avoiding unnecessary modeling of overly complex joint topologies. Extensive experiments on Human3.6M and MPI-INF-3DHP show that the Fast3DHPE framework enables fair comparison of all methods while significantly improving training efficiency. Within this unified framework, FastDDHPose achieves state-of-the-art performance with strong generalization and robustness in in-the-wild scenarios. The framework and models will be released at: https://github.com/Andyen512/Fast3DHPE",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-16",
      "updated": "2025-12-16",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14162v1",
      "code_links": [
        {
          "url": "https://github.com/Andyen512/Fast3DHPE",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "视觉里程计与SLAM (VO & SLAM)",
          "matched_keywords": [
            "[T]pose estimation"
          ],
          "title_matches": [
            "pose estimation"
          ],
          "abstract_matches": [],
          "score": 3.9,
          "weight": 1.3
        }
      ],
      "relevance_score": 3.9,
      "combination_bonus": 0.0
    },
    {
      "title": "ACE-SLAM: Scene Coordinate Regression for Neural Implicit Real-Time SLAM",
      "authors": [
        "Ignacio Alzugaray",
        "Marwan Taher",
        "Andrew J. Davison"
      ],
      "arxiv_id": "2512.14032v1",
      "summary": "We present a novel neural RGB-D Simultaneous Localization And Mapping (SLAM) system that learns an implicit map of the scene in real time. For the first time, we explore the use of Scene Coordinate Regression (SCR) as the core implicit map representation in a neural SLAM pipeline, a paradigm that trains a lightweight network to directly map 2D image features to 3D global coordinates. SCR networks provide efficient, low-memory 3D map representations, enable extremely fast relocalization, and inherently preserve privacy, making them particularly suitable for neural implicit SLAM.\n  Our system is the first one to achieve strict real-time in neural implicit RGB-D SLAM by relying on a SCR-based representation. We introduce a novel SCR architecture specifically tailored for this purpose and detail the critical design choices required to integrate SCR into a live SLAM pipeline. The resulting framework is simple yet flexible, seamlessly supporting both sparse and dense features, and operates reliably in dynamic environments without special adaptation. We evaluate our approach on established synthetic and real-world benchmarks, demonstrating competitive performance against the state of the art. Project Page: https://github.com/ialzugaray/ace-slam",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-16",
      "updated": "2025-12-16",
      "comment": "Project Page: https://github.com/ialzugaray/ace-slam",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14032v1",
      "code_links": [
        {
          "url": "https://github.com/ialzugaray/ace-slam",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "视觉里程计与SLAM (VO & SLAM)",
          "matched_keywords": [
            "[T]SLAM"
          ],
          "title_matches": [
            "SLAM"
          ],
          "abstract_matches": [],
          "score": 3.9,
          "weight": 1.3
        }
      ],
      "relevance_score": 3.9,
      "combination_bonus": 0.0
    },
    {
      "title": "4D-RaDiff: Latent Diffusion for 4D Radar Point Cloud Generation",
      "authors": [
        "Jimmie Kwok",
        "Holger Caesar",
        "Andras Palffy"
      ],
      "arxiv_id": "2512.14235v1",
      "summary": "Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-16",
      "updated": "2025-12-16",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14235v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "3D重建与高斯 (3D Reconstruction & Gaussian)",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "title_matches": [
            "point cloud"
          ],
          "abstract_matches": [],
          "score": 3.6,
          "weight": 1.2
        }
      ],
      "relevance_score": 3.6,
      "combination_bonus": 0.0
    },
    {
      "title": "Spherical Voronoi: Directional Appearance as a Differentiable Partition of the Sphere",
      "authors": [
        "Francesco Di Sario",
        "Daniel Rebain",
        "Dor Verbin",
        "Marco Grangetto",
        "Andrea Tagliasacchi"
      ],
      "arxiv_id": "2512.14180v1",
      "summary": "Radiance field methods (e.g. 3D Gaussian Splatting) have emerged as a powerful paradigm for novel view synthesis, yet their appearance modeling often relies on Spherical Harmonics (SH), which impose fundamental limitations. SH struggle with high-frequency signals, exhibit Gibbs ringing artifacts, and fail to capture specular reflections - a key component of realistic rendering. Although alternatives like spherical Gaussians offer improvements, they add significant optimization complexity. We propose Spherical Voronoi (SV) as a unified framework for appearance representation in 3D Gaussian Splatting. SV partitions the directional domain into learnable regions with smooth boundaries, providing an intuitive and stable parameterization for view-dependent effects. For diffuse appearance, SV achieves competitive results while keeping optimization simpler than existing alternatives. For reflections - where SH fail - we leverage SV as learnable reflection probes, taking reflected directions as input following principles from classical graphics. This formulation attains state-of-the-art results on synthetic and real-world datasets, demonstrating that SV offers a principled, efficient, and general solution for appearance modeling in explicit 3D representations.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-16",
      "updated": "2025-12-16",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14180v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "3D重建与高斯 (3D Reconstruction & Gaussian)",
          "matched_keywords": [
            "3D Gaussian Splatting",
            "Gaussian splatting",
            "novel view synthesis"
          ],
          "title_matches": [],
          "abstract_matches": [
            "3D Gaussian Splatting",
            "Gaussian splatting",
            "novel view synthesis"
          ],
          "score": 3.6,
          "weight": 1.2
        }
      ],
      "relevance_score": 3.6,
      "combination_bonus": 0.0
    },
    {
      "title": "Interactive Motion Planning for Human-Robot Collaboration Based on Human-Centric Configuration Space Ergonomic Field",
      "authors": [
        "Chenzui Li",
        "Yiming Chen",
        "Xi Wu",
        "Tao Teng",
        "Sylvain Calinon",
        "Darwin Caldwell",
        "Fei Chen"
      ],
      "arxiv_id": "2512.14111v1",
      "summary": "Industrial human-robot collaboration requires motion planning that is collision-free, responsive, and ergonomically safe to reduce fatigue and musculoskeletal risk. We propose the Configuration Space Ergonomic Field (CSEF), a continuous and differentiable field over the human joint space that quantifies ergonomic quality and provides gradients for real-time ergonomics-aware planning. An efficient algorithm constructs CSEF from established metrics with joint-wise weighting and task conditioning, and we integrate it into a gradient-based planner compatible with impedance-controlled robots. In a 2-DoF benchmark, CSEF-based planning achieves higher success rates, lower ergonomic cost, and faster computation than a task-space ergonomic planner. Hardware experiments with a dual-arm robot in unimanual guidance, collaborative drilling, and bimanual cocarrying show faster ergonomic cost reduction, closer tracking to optimized joint targets, and lower muscle activation than a point-to-point baseline. CSEF-based planning method reduces average ergonomic scores by up to 10.31% for collaborative drilling tasks and 5.60% for bimanual co-carrying tasks while decreasing activation in key muscle groups, indicating practical benefits for real-world deployment.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-16",
      "updated": "2025-12-16",
      "comment": "10 pages, 9 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14111v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "灵巧操作 (Dexterous Manipulation)",
          "matched_keywords": [
            "bi-manual",
            "bimanual",
            "dual-arm"
          ],
          "title_matches": [],
          "abstract_matches": [
            "bi-manual",
            "bimanual",
            "dual-arm"
          ],
          "score": 3.6,
          "weight": 1.2
        }
      ],
      "relevance_score": 3.6,
      "combination_bonus": 0.0
    },
    {
      "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
      "authors": [
        "Wenqiang Sun",
        "Haiyu Zhang",
        "Haoyuan Wang",
        "Junta Wu",
        "Zehan Wang",
        "Zhenwei Wang",
        "Yunhong Wang",
        "Jun Zhang",
        "Tengfei Wang",
        "Chunchao Guo"
      ],
      "arxiv_id": "2512.14614v1",
      "summary": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-16",
      "updated": "2025-12-16",
      "comment": "project page: https://3d-models.hunyuan.tencent.com/world/, demo: https://3d.hunyuan.tencent.com/sceneTo3D",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14614v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "Sim2Real与策略学习 (Sim2Real & Policy Learning)",
          "matched_keywords": [
            "[T]world model"
          ],
          "title_matches": [
            "world model"
          ],
          "abstract_matches": [],
          "score": 3.0,
          "weight": 1.0
        }
      ],
      "relevance_score": 3.0,
      "combination_bonus": 0.0
    },
    {
      "title": "Synthetic Data Pipelines for Adaptive, Mission-Ready Militarized Humanoids",
      "authors": [
        "Mohammed Ayman Habib",
        "Aldo Petruzzelli"
      ],
      "arxiv_id": "2512.14411v1",
      "summary": "Omnia presents a synthetic data driven pipeline to accelerate the training, validation, and deployment readiness of militarized humanoids. The approach converts first-person spatial observations captured from point-of-view recordings, smart glasses, augmented reality headsets, and spatial browsing workflows into scalable, mission-specific synthetic datasets for humanoid autonomy. By generating large volumes of high-fidelity simulated scenarios and pairing them with automated labeling and model training, the pipeline enables rapid iteration on perception, navigation, and decision-making capabilities without the cost, risk, or time constraints of extensive field trials. The resulting datasets can be tuned quickly for new operational environments and threat conditions, supporting both baseline humanoid performance and advanced subsystems such as multimodal sensing, counter-detection survivability, and CBRNE-relevant reconnaissance behaviors. This work targets faster development cycles and improved robustness in complex, contested settings by exposing humanoid systems to broad scenario diversity early in the development process.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-16",
      "updated": "2025-12-16",
      "comment": "6 pages; xTech Humanoid white paper submission",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14411v1",
      "code_links": [],
      "matched_interests": [],
      "relevance_score": 3.0,
      "combination_bonus": 3.0
    },
    {
      "title": "Robust Single-shot Structured Light 3D Imaging via Neural Feature Decoding",
      "authors": [
        "Jiaheng Li",
        "Qiyu Dai",
        "Lihan Li",
        "Praneeth Chakravarthula",
        "He Sun",
        "Baoquan Chen",
        "Wenzheng Chen"
      ],
      "arxiv_id": "2512.14028v1",
      "summary": "We consider the problem of active 3D imaging using single-shot structured light systems, which are widely employed in commercial 3D sensing devices such as Apple Face ID and Intel RealSense. Traditional structured light methods typically decode depth correspondences through pixel-domain matching algorithms, resulting in limited robustness under challenging scenarios like occlusions, fine-structured details, and non-Lambertian surfaces. Inspired by recent advances in neural feature matching, we propose a learning-based structured light decoding framework that performs robust correspondence matching within feature space rather than the fragile pixel domain. Our method extracts neural features from the projected patterns and captured infrared (IR) images, explicitly incorporating their geometric priors by building cost volumes in feature space, achieving substantial performance improvements over pixel-domain decoding approaches. To further enhance depth quality, we introduce a depth refinement module that leverages strong priors from large-scale monocular depth estimation models, improving fine detail recovery and global structural coherence. To facilitate effective learning, we develop a physically-based structured light rendering pipeline, generating nearly one million synthetic pattern-image pairs with diverse objects and materials for indoor settings. Experiments demonstrate that our method, trained exclusively on synthetic data with multiple structured light patterns, generalizes well to real-world indoor environments, effectively processes various pattern types without retraining, and consistently outperforms both commercial structured light systems and passive stereo RGB-based depth estimation methods. Project page: https://namisntimpot.github.io/NSLweb/.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-16",
      "updated": "2025-12-16",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14028v1",
      "code_links": [
        {
          "url": "https://namisntimpot.github.io/NSLweb/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "SOTA深度估计 (SOTA Depth Estimation)",
          "matched_keywords": [
            "monocular depth",
            "depth estimation"
          ],
          "title_matches": [],
          "abstract_matches": [
            "monocular depth",
            "depth estimation"
          ],
          "score": 3.0,
          "weight": 1.5
        }
      ],
      "relevance_score": 3.0,
      "combination_bonus": 0.0
    },
    {
      "title": "Trajectory Tracking for Multi-Manipulator Systems in Constrained Environments",
      "authors": [
        "Mayank Sewlia",
        "Christos K. Verginis",
        "Dimos V. Dimarogonas"
      ],
      "arxiv_id": "2512.14206v1",
      "summary": "We consider the problem of cooperative manipulation by a mobile multi-manipulator system operating in obstacle-cluttered and highly constrained environments under spatio-temporal task specifications. The task requires transporting a grasped object while respecting both continuous robot dynamics and discrete geometric constraints arising from obstacles and narrow passages. To address this hybrid structure, we propose a multi-rate planning and control framework that combines offline generation of an STL-satisfying object trajectory and collision-free base footprints with online constrained inverse kinematics and continuous-time feedback control. The resulting closed-loop system enables coordinated reconfiguration of multiple manipulators while tracking the desired object motion. The approach is evaluated in high-fidelity physics simulations using three Franka Emika Panda mobile manipulators rigidly grasping an object.",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-16",
      "updated": "2025-12-16",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14206v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "灵巧操作 (Dexterous Manipulation)",
          "matched_keywords": [
            "grasp",
            "grasping"
          ],
          "title_matches": [],
          "abstract_matches": [
            "grasp",
            "grasping"
          ],
          "score": 2.4,
          "weight": 1.2
        }
      ],
      "relevance_score": 2.4,
      "combination_bonus": 0.0
    },
    {
      "title": "ASAP-Textured Gaussians: Enhancing Textured Gaussians with Adaptive Sampling and Anisotropic Parameterization",
      "authors": [
        "Meng Wei",
        "Cheng Zhang",
        "Jianmin Zheng",
        "Hamid Rezatofighi",
        "Jianfei Cai"
      ],
      "arxiv_id": "2512.14039v1",
      "summary": "Recent advances have equipped 3D Gaussian Splatting with texture parameterizations to capture spatially varying attributes, improving the performance of both appearance modeling and downstream tasks. However, the added texture parameters introduce significant memory efficiency challenges. Rather than proposing new texture formulations, we take a step back to examine the characteristics of existing textured Gaussian methods and identify two key limitations in common: (1) Textures are typically defined in canonical space, leading to inefficient sampling that wastes textures' capacity on low-contribution regions; and (2) texture parameterization is uniformly assigned across all Gaussians, regardless of their visual complexity, resulting in over-parameterization. In this work, we address these issues through two simple yet effective strategies: adaptive sampling based on the Gaussian density distribution and error-driven anisotropic parameterization that allocates texture resources according to rendering error. Our proposed ASAP Textured Gaussians, short for Adaptive Sampling and Anisotropic Parameterization, significantly improve the quality efficiency tradeoff, achieving high-fidelity rendering with far fewer texture parameters.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-16",
      "updated": "2025-12-16",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14039v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "3D重建与高斯 (3D Reconstruction & Gaussian)",
          "matched_keywords": [
            "3D Gaussian Splatting",
            "Gaussian splatting"
          ],
          "title_matches": [],
          "abstract_matches": [
            "3D Gaussian Splatting",
            "Gaussian splatting"
          ],
          "score": 2.4,
          "weight": 1.2
        }
      ],
      "relevance_score": 2.4,
      "combination_bonus": 0.0
    },
    {
      "title": "A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning",
      "authors": [
        "Zixin Zhang",
        "Kanghao Chen",
        "Hanqing Wang",
        "Hongfei Zhang",
        "Harold Haodong Chen",
        "Chenfei Liao",
        "Litao Guo",
        "Ying-Cong Chen"
      ],
      "arxiv_id": "2512.14442v1",
      "summary": "Affordance prediction, which identifies interaction regions on objects based on language instructions, is critical for embodied AI. Prevailing end-to-end models couple high-level reasoning and low-level grounding into a single monolithic pipeline and rely on training over annotated datasets, which leads to poor generalization on novel objects and unseen environments. In this paper, we move beyond this paradigm by proposing A4-Agent, a training-free agentic framework that decouples affordance prediction into a three-stage pipeline. Our framework coordinates specialized foundation models at test time: (1) a $\\textbf{Dreamer}$ that employs generative models to visualize $\\textit{how}$ an interaction would look; (2) a $\\textbf{Thinker}$ that utilizes large vision-language models to decide $\\textit{what}$ object part to interact with; and (3) a $\\textbf{Spotter}$ that orchestrates vision foundation models to precisely locate $\\textit{where}$ the interaction area is. By leveraging the complementary strengths of pre-trained models without any task-specific fine-tuning, our zero-shot framework significantly outperforms state-of-the-art supervised methods across multiple benchmarks and demonstrates robust generalization to real-world settings.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-16",
      "updated": "2025-12-16",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14442v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "Sim2Real与策略学习 (Sim2Real & Policy Learning)",
          "matched_keywords": [
            "dreamer"
          ],
          "title_matches": [],
          "abstract_matches": [
            "dreamer"
          ],
          "score": 1.0,
          "weight": 1.0
        },
        {
          "name": "具身智能 (Embodied AI)",
          "matched_keywords": [
            "embodied AI"
          ],
          "title_matches": [],
          "abstract_matches": [
            "embodied AI"
          ],
          "score": 1.0,
          "weight": 1.0
        }
      ],
      "relevance_score": 2.0,
      "combination_bonus": 0.0
    }
  ],
  "filtered": true
}