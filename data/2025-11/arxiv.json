{
  "count": 1026,
  "papers": [
    {
      "title": "VIRAL: Visual Sim-to-Real at Scale for Humanoid Loco-Manipulation",
      "authors": [
        "Tairan He",
        "Zi Wang",
        "Haoru Xue",
        "Qingwei Ben",
        "Zhengyi Luo",
        "Wenli Xiao",
        "Ye Yuan",
        "Xingye Da",
        "Fernando Castañeda",
        "Shankar Sastry",
        "Changliu Liu",
        "Guanya Shi",
        "Linxi Fan",
        "Yuke Zhu"
      ],
      "arxiv_id": "2511.15200v2",
      "summary": "A key barrier to the real-world deployment of humanoid robots is the lack of autonomous loco-manipulation skills. We introduce VIRAL, a visual sim-to-real framework that learns humanoid loco-manipulation entirely in simulation and deploys it zero-shot to real hardware. VIRAL follows a teacher-student design: a privileged RL teacher, operating on full state, learns long-horizon loco-manipulation using a delta action space and reference state initialization. A vision-based student policy is then distilled from the teacher via large-scale simulation with tiled rendering, trained with a mixture of online DAgger and behavior cloning. We find that compute scale is critical: scaling simulation to tens of GPUs (up to 64) makes both teacher and student training reliable, while low-compute regimes often fail. To bridge the sim-to-real gap, VIRAL combines large-scale visual domain randomization over lighting, materials, camera parameters, image quality, and sensor delays--with real-to-sim alignment of the dexterous hands and cameras. Deployed on a Unitree G1 humanoid, the resulting RGB-based policy performs continuous loco-manipulation for up to 54 cycles, generalizing to diverse spatial and appearance variations without any real-world fine-tuning, and approaching expert-level teleoperation performance. Extensive ablations dissect the key design choices required to make RGB-based humanoid loco-manipulation work in practice.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-19",
      "updated": "2025-11-27",
      "comment": "Project website: https://viral-humanoid.github.io/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15200v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]manipulation",
            "[T]loco-manipulation",
            "dexterous hand",
            "[T]sim-to-real",
            "domain randomization",
            "teleoperation",
            "unitree"
          ],
          "score": 34.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "behavior cloning",
            "teacher-student"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 37.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "MOBIUS: A Multi-Modal Bipedal Robot that can Walk, Crawl, Climb, and Roll",
      "authors": [
        "Alexander Schperberg",
        "Yusuke Tanaka",
        "Stefano Di Cairano",
        "Dennis Hong"
      ],
      "arxiv_id": "2511.01774v1",
      "summary": "This article presents a Multi-Modal Bipedal Intelligent Urban Scout robot (MOBIUS) capable of walking, crawling, climbing, and rolling. MOBIUS features four limbs--two 6-DoF arms with two-finger grippers for manipulation and climbing, and two 4-DoF legs for locomotion--enabling smooth transitions across diverse terrains without reconfiguration. A hybrid control architecture combines reinforcement learning-based locomotion with model-based predictive and admittance control enhanced for safety by a Reference Governor toward compliant contact interactions. A high-level MIQCP planner autonomously selects locomotion modes to balance stability and energy efficiency. Hardware experiments demonstrate robust gait transitions, dynamic climbing, and full-body load support via pinch grasp. Overall, MOBIUS demonstrates the importance of tight integration between morphology, high-level planning, and control to enable mobile loco-manipulation and grasping, substantially expanding its interaction capabilities, workspace, and traversability.",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "23 pages, 20 figures. Collaborative work between the Robotics and Mechanisms Laboratory (RoMeLa) and Mitsubishi Electric Research Laboratories (MERL)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01774v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bipedal",
            "[T]bipedal robot",
            "[T]biped",
            "locomotion",
            "gait",
            "walking",
            "manipulation",
            "loco-manipulation",
            "grasping",
            "grasp"
          ],
          "score": 32.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "traversability"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 35.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Sim-to-Real Transfer in Deep Reinforcement Learning for Bipedal Locomotion",
      "authors": [
        "Lingfan Bao",
        "Tianhu Peng",
        "Chengxu Zhou"
      ],
      "arxiv_id": "2511.06465v1",
      "summary": "This chapter addresses the critical challenge of simulation-to-reality (sim-to-real) transfer for deep reinforcement learning (DRL) in bipedal locomotion. After contextualizing the problem within various control architectures, we dissect the ``curse of simulation'' by analyzing the primary sources of sim-to-real gap: robot dynamics, contact modeling, state estimation, and numerical solvers. Building on this diagnosis, we structure the solutions around two complementary philosophies. The first is to shrink the gap through model-centric strategies that systematically improve the simulator's physical fidelity. The second is to harden the policy, a complementary approach that uses in-simulation robustness training and post-deployment adaptation to make the policy inherently resilient to model inaccuracies. The chapter concludes by synthesizing these philosophies into a strategic framework, providing a clear roadmap for developing and evaluating robust sim-to-real solutions.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-09",
      "updated": "2025-11-09",
      "comment": "Sim-to-real for bipedal locomotion chapter",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06465v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bipedal",
            "[T]biped",
            "[T]locomotion",
            "[T]sim-to-real"
          ],
          "score": 24.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "[T]deep reinforcement learning"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 33.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "H-Zero: Cross-Humanoid Locomotion Pretraining Enables Few-shot Novel Embodiment Transfer",
      "authors": [
        "Yunfeng Lin",
        "Minghuan Liu",
        "Yufei Xue",
        "Ming Zhou",
        "Yong Yu",
        "Jiangmiao Pang",
        "Weinan Zhang"
      ],
      "arxiv_id": "2512.00971v1",
      "summary": "The rapid advancement of humanoid robotics has intensified the need for robust and adaptable controllers to enable stable and efficient locomotion across diverse platforms. However, developing such controllers remains a significant challenge because existing solutions are tailored to specific robot designs, requiring extensive tuning of reward functions, physical parameters, and training hyperparameters for each embodiment. To address this challenge, we introduce H-Zero, a cross-humanoid locomotion pretraining pipeline that learns a generalizable humanoid base policy. We show that pretraining on a limited set of embodiments enables zero-shot and few-shot transfer to novel humanoid robots with minimal fine-tuning. Evaluations show that the pretrained policy maintains up to 81% of the full episode duration on unseen robots in simulation while enabling few-shot transfer to unseen humanoids and upright quadrupeds within 30 minutes of fine-tuning.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "in submission, under review",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00971v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped",
            "[T]humanoid",
            "humanoid robot",
            "[T]humanoid locomotion",
            "[T]locomotion"
          ],
          "score": 22.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "[T]embodiment transfer"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 31.0,
      "hit_pillars": [
        "1_robot_core",
        "7_retargeting"
      ]
    },
    {
      "title": "Whole-Body Control With Terrain Estimation of A 6-DoF Wheeled Bipedal Robot",
      "authors": [
        "Cong Wen",
        "Yunfei Li",
        "Kexin Liu",
        "Yixin Qiu",
        "Xuanhong Liao",
        "Tianyu Wang",
        "Dingchuan Liu",
        "Tao Zhang",
        "Ximin Lyu"
      ],
      "arxiv_id": "2511.06397v1",
      "summary": "Wheeled bipedal robots have garnered increasing attention in exploration and inspection. However, most research simplifies calculations by ignoring leg dynamics, thereby restricting the robot's full motion potential. Additionally, robots face challenges when traversing uneven terrain. To address the aforementioned issue, we develop a complete dynamics model and design a whole-body control framework with terrain estimation for a novel 6 degrees of freedom wheeled bipedal robot. This model incorporates the closed-loop dynamics of the robot and a ground contact model based on the estimated ground normal vector. We use a LiDAR inertial odometry framework and improved Principal Component Analysis for terrain estimation. Task controllers, including PD control law and LQR, are employed for pose control and centroidal dynamics-based balance control, respectively. Furthermore, a hierarchical optimization approach is used to solve the whole-body control problem. We validate the performance of the terrain estimation algorithm and demonstrate the algorithm's robustness and ability to traverse uneven terrain through both simulation and real-world experiments.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-09",
      "updated": "2025-11-09",
      "comment": "8 pages, 8 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06397v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bipedal",
            "[T]bipedal robot",
            "[T]biped",
            "[T]whole-body control",
            "centroidal dynamics",
            "balance control"
          ],
          "score": 28.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "lidar-inertial"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 30.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "A Hierarchical Framework for Humanoid Locomotion with Supernumerary Limbs",
      "authors": [
        "Bowen Zhi"
      ],
      "arxiv_id": "2512.00077v1",
      "summary": "The integration of Supernumerary Limbs (SLs) on humanoid robots poses a significant stability challenge due to the dynamic perturbations they introduce. This thesis addresses this issue by designing a novel hierarchical control architecture to improve humanoid locomotion stability with SLs. The core of this framework is a decoupled strategy that combines learning-based locomotion with model-based balancing. The low-level component consists of a walking gait for a Unitree H1 humanoid through imitation learning and curriculum learning. The high-level component actively utilizes the SLs for dynamic balancing. The effectiveness of the system is evaluated in a physics-based simulation under three conditions: baseline gait for an unladen humanoid (baseline walking), walking with a static SL payload (static payload), and walking with the active dynamic balancing controller (dynamic balancing). Our evaluation shows that the dynamic balancing controller improves stability. Compared to the static payload condition, the balancing strategy yields a gait pattern closer to the baseline and decreases the Dynamic Time Warping (DTW) distance of the CoM trajectory by 47\\%. The balancing controller also improves the re-stabilization within gait cycles and achieves a more coordinated anti-phase pattern of Ground Reaction Forces (GRF). The results demonstrate that a decoupled, hierarchical design can effectively mitigate the internal dynamic disturbances arising from the mass and movement of the SLs, enabling stable locomotion for humanoids equipped with functional limbs. Code and videos are available here: https://github.com/heyzbw/HuSLs.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00077v1",
      "code_links": [
        {
          "url": "https://github.com/heyzbw/HuSLs",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]humanoid locomotion",
            "[T]locomotion",
            "gait",
            "walking",
            "unitree"
          ],
          "score": 26.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning",
            "curriculum learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 29.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "MS-PPO: Morphological-Symmetry-Equivariant Policy for Legged Robot Locomotion",
      "authors": [
        "Sizhe Wei",
        "Xulin Chen",
        "Fengze Xie",
        "Garrett Ethan Katz",
        "Zhenyu Gan",
        "Lu Gan"
      ],
      "arxiv_id": "2512.00727v1",
      "summary": "Reinforcement learning has recently enabled impressive locomotion capabilities on legged robots; however, most policy architectures remain morphology- and symmetry-agnostic, leading to inefficient training and limited generalization. This work introduces MS-PPO, a morphological-symmetry-equivariant policy learning framework that encodes robot kinematic structure and morphological symmetries directly into the policy network. We construct a morphology-informed graph neural architecture that is provably equivariant with respect to the robot's morphological symmetry group actions, ensuring consistent policy responses under symmetric states while maintaining invariance in value estimation. This design eliminates the need for tedious reward shaping or costly data augmentation, which are typically required to enforce symmetry. We evaluate MS-PPO in simulation on Unitree Go2 and Xiaomi CyberDog2 robots across diverse locomotion tasks, including trotting, pronking, slope walking, and bipedal turning, and further deploy the learned policies on hardware. Extensive experiments show that MS-PPO achieves superior training stability, symmetry generalization ability, and sample efficiency in challenging locomotion tasks, compared to state-of-the-art baselines. These findings demonstrate that embedding both kinematic structure and morphological symmetry into policy learning provides a powerful inductive bias for legged robot locomotion control. Our code will be made publicly available at https://lunarlab-gatech.github.io/MS-PPO/.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00727v1",
      "code_links": [
        {
          "url": "https://lunarlab-gatech.github.io/MS-PPO/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]legged robot",
            "bipedal",
            "biped",
            "[T]locomotion",
            "walking",
            "unitree"
          ],
          "score": 20.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "policy learning",
            "[T]PPO",
            "reward shaping"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 29.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "BFM-Zero: A Promptable Behavioral Foundation Model for Humanoid Control Using Unsupervised Reinforcement Learning",
      "authors": [
        "Yitang Li",
        "Zhengyi Luo",
        "Tonghe Zhang",
        "Cunxi Dai",
        "Anssi Kanervisto",
        "Andrea Tirinzoni",
        "Haoyang Weng",
        "Kris Kitani",
        "Mateusz Guzek",
        "Ahmed Touati",
        "Alessandro Lazaric",
        "Matteo Pirotta",
        "Guanya Shi"
      ],
      "arxiv_id": "2511.04131v1",
      "summary": "Building Behavioral Foundation Models (BFMs) for humanoid robots has the potential to unify diverse control tasks under a single, promptable generalist policy. However, existing approaches are either exclusively deployed on simulated humanoid characters, or specialized to specific tasks such as tracking. We propose BFM-Zero, a framework that learns an effective shared latent representation that embeds motions, goals, and rewards into a common space, enabling a single policy to be prompted for multiple downstream tasks without retraining. This well-structured latent space in BFM-Zero enables versatile and robust whole-body skills on a Unitree G1 humanoid in the real world, via diverse inference methods, including zero-shot motion tracking, goal reaching, and reward optimization, and few-shot optimization-based adaptation. Unlike prior on-policy reinforcement learning (RL) frameworks, BFM-Zero builds upon recent advancements in unsupervised RL and Forward-Backward (FB) models, which offer an objective-centric, explainable, and smooth latent representation of whole-body motions. We further extend BFM-Zero with critical reward shaping, domain randomization, and history-dependent asymmetric learning to bridge the sim-to-real gap. Those key design choices are quantitatively ablated in simulation. A first-of-its-kind model, BFM-Zero establishes a step toward scalable, promptable behavioral foundation models for whole-body humanoid control.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04131v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]humanoid control",
            "whole-body motion",
            "sim-to-real",
            "domain randomization",
            "unitree"
          ],
          "score": 22.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "reward shaping"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 28.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System",
      "authors": [
        "Yanjie Ze",
        "Siheng Zhao",
        "Weizhuo Wang",
        "Angjoo Kanazawa",
        "Rocky Duan",
        "Pieter Abbeel",
        "Guanya Shi",
        "Jiajun Wu",
        "C. Karen Liu"
      ],
      "arxiv_id": "2511.02832v1",
      "summary": "Large-scale data has driven breakthroughs in robotics, from language models to vision-language-action models in bimanual manipulation. However, humanoid robotics lacks equally effective data collection frameworks. Existing humanoid teleoperation systems either use decoupled control or depend on expensive motion capture setups. We introduce TWIST2, a portable, mocap-free humanoid teleoperation and data collection system that preserves full whole-body control while advancing scalability. Our system leverages PICO4U VR for obtaining real-time whole-body human motions, with a custom 2-DoF robot neck (cost around $250) for egocentric vision, enabling holistic human-to-humanoid control. We demonstrate long-horizon dexterous and mobile humanoid skills and we can collect 100 demonstrations in 15 minutes with an almost 100% success rate. Building on this pipeline, we propose a hierarchical visuomotor policy framework that autonomously controls the full humanoid body based on egocentric vision. Our visuomotor policy successfully demonstrates whole-body dexterous manipulation and dynamic kicking tasks. The entire system is fully reproducible and open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also open-sourced at https://twist-data.github.io .",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-04",
      "updated": "2025-11-04",
      "comment": "Website: https://yanjieze.com/TWIST2",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02832v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "humanoid control",
            "whole-body control",
            "manipulation",
            "dexterous manipulation",
            "bi-manual",
            "bimanual",
            "teleoperation"
          ],
          "score": 22.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "human-to-humanoid",
            "human to humanoid"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 28.0,
      "hit_pillars": [
        "1_robot_core",
        "7_retargeting"
      ]
    },
    {
      "title": "Descriptive Model-based Learning and Control for Bipedal Locomotion",
      "authors": [
        "Suraj Kumar",
        "Andy Ruina"
      ],
      "arxiv_id": "2511.00512v1",
      "summary": "Bipedal balance is challenging due to its multi-phase, hybrid nature and high-dimensional state space. Traditional balance control approaches for bipedal robots rely on low-dimensional models for locomotion planning and reactive control, constraining the full robot to behave like these simplified models. This involves tracking preset reference paths for the Center of Mass and upper body obtained through low-dimensional models, often resulting in inefficient walking patterns with bent knees. However, we observe that bipedal balance is inherently low-dimensional and can be effectively described with simple state and action descriptors in a low-dimensional state space. This allows the robot's motion to evolve freely in its high-dimensional state space, only constraining its projection in the low-dimensional state space. In this work, we propose a novel control approach that avoids prescribing a low-dimensional model to the full model. Instead, our control framework uses a descriptive model with the minimum degrees of freedom necessary to maintain balance, allowing the remaining degrees of freedom to evolve freely in the high-dimensional space. This results in an efficient human-like walking gait and improved robustness.",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-01",
      "updated": "2025-11-01",
      "comment": "8 pages, 15 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00512v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bipedal",
            "bipedal robot",
            "[T]biped",
            "[T]locomotion",
            "gait",
            "walking",
            "motion planning",
            "balance control"
          ],
          "score": 28.0
        }
      ],
      "relevance_score": 28.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Gallant: Voxel Grid-based Humanoid Locomotion and Local-navigation across 3D Constrained Terrains",
      "authors": [
        "Qingwei Ben",
        "Botian Xu",
        "Kailin Li",
        "Feiyu Jia",
        "Wentao Zhang",
        "Jingping Wang",
        "Jingbo Wang",
        "Dahua Lin",
        "Jiangmiao Pang"
      ],
      "arxiv_id": "2511.14625v1",
      "summary": "Robust humanoid locomotion requires accurate and globally consistent perception of the surrounding 3D environment. However, existing perception modules, mainly based on depth images or elevation maps, offer only partial and locally flattened views of the environment, failing to capture the full 3D structure. This paper presents Gallant, a voxel-grid-based framework for humanoid locomotion and local navigation in 3D constrained terrains. It leverages voxelized LiDAR data as a lightweight and structured perceptual representation, and employs a z-grouped 2D CNN to map this representation to the control policy, enabling fully end-to-end optimization. A high-fidelity LiDAR simulation that dynamically generates realistic observations is developed to support scalable, LiDAR-based training and ensure sim-to-real consistency. Experimental results show that Gallant's broader perceptual coverage facilitates the use of a single policy that goes beyond the limitations of previous methods confined to ground-level obstacles, extending to lateral clutter, overhead constraints, multi-level structures, and narrow passages. Gallant also firstly achieves near 100% success rates in challenging scenarios such as stair climbing and stepping onto elevated platforms through improved end-to-end optimization.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14625v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid locomotion",
            "[T]locomotion",
            "sim-to-real"
          ],
          "score": 20.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "elevation map",
            "[T]navigation"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 28.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Kinematics-Aware Multi-Policy Reinforcement Learning for Force-Capable Humanoid Loco-Manipulation",
      "authors": [
        "Kaiyan Xiao",
        "Zihan Xu",
        "Cheng Zhe",
        "Chengju Liu",
        "Qijun Chen"
      ],
      "arxiv_id": "2511.21169v1",
      "summary": "Humanoid robots, with their human-like morphology, hold great potential for industrial applications. However, existing loco-manipulation methods primarily focus on dexterous manipulation, falling short of the combined requirements for dexterity and proactive force interaction in high-load industrial scenarios. To bridge this gap, we propose a reinforcement learning-based framework with a decoupled three-stage training pipeline, consisting of an upper-body policy, a lower-body policy, and a delta-command policy. To accelerate upper-body training, a heuristic reward function is designed. By implicitly embedding forward kinematics priors, it enables the policy to converge faster and achieve superior performance. For the lower body, a force-based curriculum learning strategy is developed, enabling the robot to actively exert and regulate interaction forces with the environment.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21169v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]manipulation",
            "[T]loco-manipulation",
            "dexterous manipulation"
          ],
          "score": 22.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "curriculum learning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 28.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Whole-Body Inverse Dynamics MPC for Legged Loco-Manipulation",
      "authors": [
        "Lukas Molnar",
        "Jin Cheng",
        "Gabriele Fadini",
        "Dongho Kang",
        "Fatemeh Zargarbashi",
        "Stelian Coros"
      ],
      "arxiv_id": "2511.19709v1",
      "summary": "Loco-manipulation demands coordinated whole-body motion to manipulate objects effectively while maintaining locomotion stability, presenting significant challenges for both planning and control. In this work, we propose a whole-body model predictive control (MPC) framework that directly optimizes joint torques through full-order inverse dynamics, enabling unified motion and force planning and execution within a single predictive layer. This approach allows emergent, physically consistent whole-body behaviors that account for the system's dynamics and physical constraints. We implement our MPC formulation using open software frameworks (Pinocchio and CasADi), along with the state-of-the-art interior-point solver Fatrop. In real-world experiments on a Unitree B2 quadruped equipped with a Unitree Z1 manipulator arm, our MPC formulation achieves real-time performance at 80 Hz. We demonstrate loco-manipulation tasks that demand fine control over the end-effector's position and force to perform real-world interactions like pulling heavy loads, pushing boxes, and wiping whiteboards.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "9 pages, 6 figures, to be published in IEEE Robotics and Automation Letters (Special Issue: Advancements in MPC and Learning Algorithms for Legged Robots)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19709v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped",
            "whole-body motion",
            "locomotion",
            "[T]manipulation",
            "[T]loco-manipulation",
            "[T]MPC",
            "model predictive control",
            "unitree"
          ],
          "score": 28.0
        }
      ],
      "relevance_score": 28.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Learning Adaptive Neural Teleoperation for Humanoid Robots: From Inverse Kinematics to End-to-End Control",
      "authors": [
        "Sanjar Atamuradov"
      ],
      "arxiv_id": "2511.12390v1",
      "summary": "Virtual reality (VR) teleoperation has emerged as a promising approach for controlling humanoid robots in complex manipulation tasks. However, traditional teleoperation systems rely on inverse kinematics (IK) solvers and hand-tuned PD controllers, which struggle to handle external forces, adapt to different users, and produce natural motions under dynamic conditions. In this work, we propose a learning-based neural teleoperation framework that replaces the conventional IK+PD pipeline with learned policies trained via reinforcement learning. Our approach learns to directly map VR controller inputs to robot joint commands while implicitly handling force disturbances, producing smooth trajectories, and adapting to user preferences. We train our policies in simulation using demonstrations collected from IK-based teleoperation as initialization, then fine-tune them with force randomization and trajectory smoothness rewards. Experiments on the Unitree G1 humanoid robot demonstrate that our learned policies achieve 34% lower tracking error, 45% smoother motions, and superior force adaptation compared to the IK baseline, while maintaining real-time performance (50Hz control frequency). We validate our approach on manipulation tasks including object pick-and-place, door opening, and bimanual coordination. These results suggest that learning-based approaches can significantly improve the naturalness and robustness of humanoid teleoperation systems.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-15",
      "updated": "2025-11-15",
      "comment": "9 pages, 5 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12390v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot",
            "manipulation",
            "bi-manual",
            "bimanual",
            "[T]teleoperation",
            "unitree"
          ],
          "score": 26.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 27.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Beyond Egocentric Limits: Multi-View Depth-Based Learning for Robust Quadrupedal Locomotion",
      "authors": [
        "Rémy Rahem",
        "Wael Suleiman"
      ],
      "arxiv_id": "2511.22744v1",
      "summary": "Recent progress in legged locomotion has allowed highly dynamic and parkour-like behaviors for robots, similar to their biological counterparts. Yet, these methods mostly rely on egocentric (first-person) perception, limiting their performance, especially when the viewpoint of the robot is occluded. A promising solution would be to enhance the robot's environmental awareness by using complementary viewpoints, such as multiple actors exchanging perceptual information. Inspired by this idea, this work proposes a multi-view depth-based locomotion framework that combines egocentric and exocentric observations to provide richer environmental context during agile locomotion. Using a teacher-student distillation approach, the student policy learns to fuse proprioception with dual depth streams while remaining robust to real-world sensing imperfections. To further improve robustness, we introduce extensive domain randomization, including stochastic remote-camera dropouts and 3D positional perturbations that emulate aerial-ground cooperative sensing. Simulation results show that multi-viewpoints policies outperform single-viewpoint baseline in gap crossing, step descent, and other dynamic maneuvers, while maintaining stability when the exocentric camera is partially or completely unavailable. Additional experiments show that moderate viewpoint misalignment is well tolerated when incorporated during training. This study demonstrates that heterogeneous visual feedback improves robustness and agility in quadrupedal locomotion. Furthermore, to support reproducibility, the implementation accompanying this work is publicly available at https://anonymous.4open.science/r/multiview-parkour-6FB8",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "12 pages, 6 figures, code available at https://anonymous.4open.science/r/multiview-parkour-6FB8",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22744v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]quadruped",
            "[T]quadrupedal",
            "legged locomotion",
            "[T]locomotion",
            "parkour",
            "agile locomotion",
            "domain randomization"
          ],
          "score": 26.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "teacher-student"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 27.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Towards Adaptive Humanoid Control via Multi-Behavior Distillation and Reinforced Fine-Tuning",
      "authors": [
        "Yingnan Zhao",
        "Xinmiao Wang",
        "Dewei Wang",
        "Xinzhe Liu",
        "Dan Lu",
        "Qilong Han",
        "Peng Liu",
        "Chenjia Bai"
      ],
      "arxiv_id": "2511.06371v2",
      "summary": "Humanoid robots are promising to learn a diverse set of human-like locomotion behaviors, including standing up, walking, running, and jumping. However, existing methods predominantly require training independent policies for each skill, yielding behavior-specific controllers that exhibit limited generalization and brittle performance when deployed on irregular terrains and in diverse situations. To address this challenge, we propose Adaptive Humanoid Control (AHC) that adopts a two-stage framework to learn an adaptive humanoid locomotion controller across different skills and terrains. Specifically, we first train several primary locomotion policies and perform a multi-behavior distillation process to obtain a basic multi-behavior controller, facilitating adaptive behavior switching based on the environment. Then, we perform reinforced fine-tuning by collecting online feedback in performing adaptive behaviors on more diverse terrains, enhancing terrain adaptability for the controller. We conduct experiments in both simulation and real-world experiments in Unitree G1 robots. The results show that our method exhibits strong adaptability across various situations and terrains. Project website: https://ahc-humanoid.github.io.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-09",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06371v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "[T]humanoid control",
            "humanoid locomotion",
            "locomotion",
            "walking",
            "running",
            "jumping",
            "unitree"
          ],
          "score": 26.0
        }
      ],
      "relevance_score": 26.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Rad-GS: Radar-Vision Integration for 3D Gaussian Splatting SLAM in Outdoor Environments",
      "authors": [
        "Renxiang Xiao",
        "Wei Liu",
        "Yuanfan Zhang",
        "Yushuai Chen",
        "Jinming Chen",
        "Zilu Wang",
        "Liang Hu"
      ],
      "arxiv_id": "2511.16091v1",
      "summary": "We present Rad-GS, a 4D radar-camera SLAM system designed for kilometer-scale outdoor environments, utilizing 3D Gaussian as a differentiable spatial representation. Rad-GS combines the advantages of raw radar point cloud with Doppler information and geometrically enhanced point cloud to guide dynamic object masking in synchronized images, thereby alleviating rendering artifacts and improving localization accuracy. Additionally, unsynchronized image frames are leveraged to globally refine the 3D Gaussian representation, enhancing texture consistency and novel view synthesis fidelity. Furthermore, the global octree structure coupled with a targeted Gaussian primitive management strategy further suppresses noise and significantly reduces memory consumption in large-scale environments. Extensive experiments and ablation studies demonstrate that Rad-GS achieves performance comparable to traditional 3D Gaussian methods based on camera or LiDAR inputs, highlighting the feasibility of robust outdoor mapping using 4D mmWave radar. Real-world reconstruction at kilometer scale validates the potential of Rad-GS for large-scale scene reconstruction.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "10.1109/LRA.2025.3630875",
      "journal_ref": "IEEE Robotics and Automation Letters 10(12), 13359-13366 (2025)",
      "pdf_url": "https://arxiv.org/pdf/2511.16091v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]SLAM",
            "[T]3D gaussian splatting",
            "[T]gaussian splatting",
            "novel view synthesis",
            "scene reconstruction",
            "point cloud",
            "localization"
          ],
          "score": 26.0
        }
      ],
      "relevance_score": 26.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "iGaussian: Real-Time Camera Pose Estimation via Feed-Forward 3D Gaussian Splatting Inversion",
      "authors": [
        "Hao Wang",
        "Linqing Zhao",
        "Xiuwei Xu",
        "Jiwen Lu",
        "Haibin Yan"
      ],
      "arxiv_id": "2511.14149v1",
      "summary": "Recent trends in SLAM and visual navigation have embraced 3D Gaussians as the preferred scene representation, highlighting the importance of estimating camera poses from a single image using a pre-built Gaussian model. However, existing approaches typically rely on an iterative \\textit{render-compare-refine} loop, where candidate views are first rendered using NeRF or Gaussian Splatting, then compared against the target image, and finally, discrepancies are used to update the pose. This multi-round process incurs significant computational overhead, hindering real-time performance in robotics. In this paper, we propose iGaussian, a two-stage feed-forward framework that achieves real-time camera pose estimation through direct 3D Gaussian inversion. Our method first regresses a coarse 6DoF pose using a Gaussian Scene Prior-based Pose Regression Network with spatial uniform sampling and guided attention mechanisms, then refines it through feature matching and multi-model fusion. The key contribution lies in our cross-correlation module that aligns image embeddings with 3D Gaussian attributes without differentiable rendering, coupled with a Weighted Multiview Predictor that fuses features from Multiple strategically sampled viewpoints. Experimental results on the NeRF Synthetic, Mip-NeRF 360, and T\\&T+DB datasets demonstrate a significant performance improvement over previous methods, reducing median rotation errors to 0.2° while achieving 2.87 FPS tracking on mobile robots, which is an impressive 10 times speedup compared to optimization-based approaches. Code: https://github.com/pythongod-exe/iGaussian",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "IROS 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14149v1",
      "code_links": [
        {
          "url": "https://github.com/pythongod-exe/iGaussian",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "SLAM",
            "[T]3D gaussian splatting",
            "[T]gaussian splatting",
            "NeRF",
            "[T]pose estimation",
            "navigation"
          ],
          "score": 24.0
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "feature matching"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 26.0,
      "hit_pillars": [
        "3_perception_slam",
        "6_video_extraction"
      ]
    },
    {
      "title": "Scaling Cross-Embodiment World Models for Dexterous Manipulation",
      "authors": [
        "Zihao He",
        "Bo Ai",
        "Tongzhou Mu",
        "Yulin Liu",
        "Weikang Wan",
        "Jiawei Fu",
        "Yilun Du",
        "Henrik I. Christensen",
        "Hao Su"
      ],
      "arxiv_id": "2511.01177v2",
      "summary": "Cross-embodiment learning seeks to build generalist robots that operate across diverse morphologies, but differences in action spaces and kinematics hinder data sharing and policy transfer. This raises a central question: Is there any invariance that allows actions to transfer across embodiments? We conjecture that environment dynamics are embodiment-invariant, and that world models capturing these dynamics can provide a unified interface across embodiments. To learn such a unified world model, the crucial step is to design state and action representations that abstract away embodiment-specific details while preserving control relevance. To this end, we represent different embodiments (e.g., human hands and robot hands) as sets of 3D particles and define actions as particle displacements, creating a shared representation for heterogeneous data and control problems. A graph-based world model is then trained on exploration data from diverse simulated robot hands and real human hands, and integrated with model-based planning for deployment on novel hardware. Experiments on rigid and deformable manipulation tasks reveal three findings: (i) scaling to more training embodiments improves generalization to unseen ones, (ii) co-training on both simulated and real data outperforms training on either alone, and (iii) the learned models enable effective control on robots with varied degrees of freedom. These results establish world models as a promising interface for cross-embodiment dexterous manipulation.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-11-09",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01177v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]dexterous manipulation"
          ],
          "score": 12.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "[T]cross-embodiment"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 25.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "7_retargeting"
      ]
    },
    {
      "title": "UMIGen: A Unified Framework for Egocentric Point Cloud Generation and Cross-Embodiment Robotic Imitation Learning",
      "authors": [
        "Yan Huang",
        "Shoujie Li",
        "Xingting Li",
        "Wenbo Ding"
      ],
      "arxiv_id": "2511.09302v1",
      "summary": "Data-driven robotic learning faces an obvious dilemma: robust policies demand large-scale, high-quality demonstration data, yet collecting such data remains a major challenge owing to high operational costs, dependence on specialized hardware, and the limited spatial generalization capability of current methods. The Universal Manipulation Interface (UMI) relaxes the strict hardware requirements for data collection, but it is restricted to capturing only RGB images of a scene and omits the 3D geometric information on which many tasks rely. Inspired by DemoGen, we propose UMIGen, a unified framework that consists of two key components: (1) Cloud-UMI, a handheld data collection device that requires no visual SLAM and simultaneously records point cloud observation-action pairs; and (2) a visibility-aware optimization mechanism that extends the DemoGen pipeline to egocentric 3D observations by generating only points within the camera's field of view. These two components enable efficient data generation that aligns with real egocentric observations and can be directly transferred across different robot embodiments without any post-processing. Experiments in both simulated and real-world settings demonstrate that UMIGen supports strong cross-embodiment generalization and accelerates data collection in diverse manipulation tasks.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09302v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "visual SLAM",
            "SLAM",
            "[T]point cloud"
          ],
          "score": 10.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "[T]cross-embodiment"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 25.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam",
        "7_retargeting"
      ]
    },
    {
      "title": "Sampling-Based Optimization with Parallelized Physics Simulator for Bimanual Manipulation",
      "authors": [
        "Iryna Hurova",
        "Alinjar Dan",
        "Karl Kruusamäe",
        "Arun Kumar Singh"
      ],
      "arxiv_id": "2511.21264v1",
      "summary": "In recent years, dual-arm manipulation has become an area of strong interest in robotics, with end-to-end learning emerging as the predominant strategy for solving bimanual tasks. A critical limitation of such learning-based approaches, however, is their difficulty in generalizing to novel scenarios, especially within cluttered environments. This paper presents an alternative paradigm: a sampling-based optimization framework that utilizes a GPU-accelerated physics simulator as its world model. We demonstrate that this approach can solve complex bimanual manipulation tasks in the presence of static obstacles. Our contribution is a customized Model Predictive Path Integral Control (MPPI) algorithm, \\textbf{guided by carefully designed task-specific cost functions,} that uses GPU-accelerated MuJoCo for efficiently evaluating robot-object interaction. We apply this method to solve significantly more challenging versions of tasks from the PerAct$^{2}$ benchmark, such as requiring the point-to-point transfer of a ball through an obstacle course. Furthermore, we establish that our method achieves real-time performance on commodity GPUs and facilitates successful sim-to-real transfer by leveraging unique features within MuJoCo. The paper concludes with a statistical analysis of the sample complexity and robustness, quantifying the performance of our approach. The project website is available at: https://sites.google.com/view/bimanualakslabunitartu .",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "9 pages, 5 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21264v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]bi-manual",
            "[T]bimanual",
            "dual-arm",
            "sim-to-real"
          ],
          "score": 22.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "world model",
            "MuJoCo"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 25.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Heuristic Step Planning for Learning Dynamic Bipedal Locomotion: A Comparative Study of Model-Based and Model-Free Approaches",
      "authors": [
        "William Suliman",
        "Ekaterina Chaikovskaia",
        "Egor Davydenko",
        "Roman Gorbachev"
      ],
      "arxiv_id": "2511.00840v2",
      "summary": "This work presents an extended framework for learning-based bipedal locomotion that incorporates a heuristic step-planning strategy guided by desired torso velocity tracking. The framework enables precise interaction between a humanoid robot and its environment, supporting tasks such as crossing gaps and accurately approaching target objects. Unlike approaches based on full or simplified dynamics, the proposed method avoids complex step planners and analytical models. Step planning is primarily driven by heuristic commands, while a Raibert-type controller modulates the foot placement length based on the error between desired and actual torso velocity. We compare our method with a model-based step-planning approach -- the Linear Inverted Pendulum Model (LIPM) controller. Experimental results demonstrate that our approach attains comparable or superior accuracy in maintaining target velocity (up to 80%), significantly greater robustness on uneven terrain (over 50% improvement), and improved energy efficiency. These results suggest that incorporating complex analytical, model-based components into the training architecture may be unnecessary for achieving stable and robust bipedal walking, even in unstructured environments.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-02",
      "updated": "2025-11-27",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00840v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid",
            "humanoid robot",
            "[T]bipedal",
            "[T]biped",
            "[T]locomotion",
            "walking"
          ],
          "score": 24.0
        }
      ],
      "relevance_score": 24.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "DecARt Leg: Design and Evaluation of a Novel Humanoid Robot Leg with Decoupled Actuation for Agile Locomotion",
      "authors": [
        "Egor Davydenko",
        "Andrei Volchenkov",
        "Vladimir Gerasimov",
        "Roman Gorbachev"
      ],
      "arxiv_id": "2511.10021v1",
      "summary": "In this paper, we propose a novel design of an electrically actuated robotic leg, called the DecARt (Decoupled Actuation Robot) Leg, aimed at performing agile locomotion. This design incorporates several new features, such as the use of a quasi-telescopic kinematic structure with rotational motors for decoupled actuation, a near-anthropomorphic leg appearance with a forward facing knee, and a novel multi-bar system for ankle torque transmission from motors placed above the knee. To analyze the agile locomotion capabilities of the design numerically, we propose a new descriptive metric, called the `Fastest Achievable Swing Time` (FAST), and perform a quantitative evaluation of the proposed design and compare it with other designs. Then we evaluate the performance of the DecARt Leg-based robot via extensive simulation and preliminary hardware experiments.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10021v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot",
            "[T]locomotion",
            "[T]agile locomotion"
          ],
          "score": 24.0
        }
      ],
      "relevance_score": 24.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "SplatSearch: Instance Image Goal Navigation for Mobile Robots using 3D Gaussian Splatting and Diffusion Models",
      "authors": [
        "Siddarth Narasimhan",
        "Matthew Lisondra",
        "Haitong Wang",
        "Goldie Nejat"
      ],
      "arxiv_id": "2511.12972v1",
      "summary": "The Instance Image Goal Navigation (IIN) problem requires mobile robots deployed in unknown environments to search for specific objects or people of interest using only a single reference goal image of the target. This problem can be especially challenging when: 1) the reference image is captured from an arbitrary viewpoint, and 2) the robot must operate with sparse-view scene reconstructions. In this paper, we address the IIN problem, by introducing SplatSearch, a novel architecture that leverages sparse-view 3D Gaussian Splatting (3DGS) reconstructions. SplatSearch renders multiple viewpoints around candidate objects using a sparse online 3DGS map, and uses a multi-view diffusion model to complete missing regions of the rendered images, enabling robust feature matching against the goal image. A novel frontier exploration policy is introduced which uses visual context from the synthesized viewpoints with semantic context from the goal image to evaluate frontier locations, allowing the robot to prioritize frontiers that are semantically and visually relevant to the goal image. Extensive experiments in photorealistic home and real-world environments validate the higher performance of SplatSearch against current state-of-the-art methods in terms of Success Rate and Success Path Length. An ablation study confirms the design choices of SplatSearch.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "Project Page: https://splat-search.github.io/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12972v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting",
            "scene reconstruction",
            "[T]navigation"
          ],
          "score": 22.0
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "feature matching"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 24.0,
      "hit_pillars": [
        "3_perception_slam",
        "6_video_extraction"
      ]
    },
    {
      "title": "AutoOdom: Learning Auto-regressive Proprioceptive Odometry for Legged Locomotion",
      "authors": [
        "Changsheng Luo",
        "Yushi Wang",
        "Wenhan Cai",
        "Mingguo Zhao"
      ],
      "arxiv_id": "2511.18857v1",
      "summary": "Accurate proprioceptive odometry is fundamental for legged robot navigation in GPS-denied and visually degraded environments where conventional visual odometry systems fail. Current approaches face critical limitations: analytical filtering methods suffer from modeling uncertainties and cumulative drift, hybrid learning-filtering approaches remain constrained by their analytical components, while pure learning-based methods struggle with simulation-to-reality transfer and demand extensive real-world data collection. This paper introduces AutoOdom, a novel autoregressive proprioceptive odometry system that overcomes these challenges through an innovative two-stage training paradigm. Stage 1 employs large-scale simulation data to learn complex nonlinear dynamics and rapidly changing contact states inherent in legged locomotion, while Stage 2 introduces an autoregressive enhancement mechanism using limited real-world data to effectively bridge the sim-to-real gap. The key innovation lies in our autoregressive training approach, where the model learns from its own predictions to develop resilience against sensor noise and improve robustness in highly dynamic environments. Comprehensive experimental validation on the Booster T1 humanoid robot demonstrates that AutoOdom significantly outperforms state-of-the-art methods across all evaluation metrics, achieving 57.2% improvement in absolute trajectory error, 59.2% improvement in Umeyama-aligned error, and 36.2% improvement in relative pose error compared to the Legolas baseline. Extensive ablation studies provide critical insights into sensor modality selection and temporal modeling, revealing counterintuitive findings about IMU acceleration data and validating our systematic design choices for robust proprioceptive odometry in challenging locomotion scenarios.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18857v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "legged robot",
            "[T]legged locomotion",
            "humanoid",
            "humanoid robot",
            "[T]locomotion",
            "sim-to-real"
          ],
          "score": 20.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "visual odometry",
            "navigation"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 24.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Depth-Consistent 3D Gaussian Splatting via Physical Defocus Modeling and Multi-View Geometric Supervision",
      "authors": [
        "Yu Deng",
        "Baozhu Zhao",
        "Junyan Su",
        "Xiaohan Zhang",
        "Qi Liu"
      ],
      "arxiv_id": "2511.10316v1",
      "summary": "Three-dimensional reconstruction in scenes with extreme depth variations remains challenging due to inconsistent supervisory signals between near-field and far-field regions. Existing methods fail to simultaneously address inaccurate depth estimation in distant areas and structural degradation in close-range regions. This paper proposes a novel computational framework that integrates depth-of-field supervision and multi-view consistency supervision to advance 3D Gaussian Splatting. Our approach comprises two core components: (1) Depth-of-field Supervision employs a scale-recovered monocular depth estimator (e.g., Metric3D) to generate depth priors, leverages defocus convolution to synthesize physically accurate defocused images, and enforces geometric consistency through a novel depth-of-field loss, thereby enhancing depth fidelity in both far-field and near-field regions; (2) Multi-View Consistency Supervision employing LoFTR-based semi-dense feature matching to minimize cross-view geometric errors and enforce depth consistency via least squares optimization of reliable matched points. By unifying defocus physics with multi-view geometric constraints, our method achieves superior depth fidelity, demonstrating a 0.8 dB PSNR improvement over the state-of-the-art method on the Waymo Open Dataset. This framework bridges physical imaging principles and learning-based depth regularization, offering a scalable solution for complex depth stratification in urban environments.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10316v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation",
            "monocular depth",
            "[T]3D gaussian splatting",
            "[T]gaussian splatting",
            "Metric3D"
          ],
          "score": 18.0
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "feature matching"
          ],
          "score": 2.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "geometric consistency"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 23.0,
      "hit_pillars": [
        "3_perception_slam",
        "6_video_extraction",
        "7_retargeting"
      ]
    },
    {
      "title": "HAFO: A Force-Adaptive Control Framework for Humanoid Robots in Intense Interaction Environments",
      "authors": [
        "Chenhui Dong",
        "Haozhe Xu",
        "Wenhao Feng",
        "Zhipeng Wang",
        "Yanmin Zhou",
        "Yifei Zhao",
        "Bin He"
      ],
      "arxiv_id": "2511.20275v3",
      "summary": "Reinforcement learning (RL) controllers have made impressive progress in humanoid locomotion and light-weight object manipulation. However, achieving robust and precise motion control with intense force interaction remains a significant challenge. To address these limitations, this paper proposes HAFO, a dual-agent reinforcement learning framework that concurrently optimizes both a robust locomotion strategy and a precise upper-body manipulation strategy via coupled training in environments with external disturbances. The external pulling disturbances are explicitly modeled using a spring-damper system, allowing for fine-grained force control through manipulation of the virtual spring. In this process, the reinforcement learning policy autonomously generates a disturbance-rejection response by utilizing environmental feedback. Furthermore, HAFO employs an asymmetric Actor-Critic framework in which the Critic network's access to privileged external forces guides the actor network to acquire generalizable force adaptation for resisting external disturbances. The experimental results demonstrate that HAFO achieves whole-body control for humanoid robots across diverse force-interaction environments, delivering outstanding performance in load-bearing tasks and maintaining stable operation even under rope suspension state.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-25",
      "updated": "2025-12-04",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20275v3",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot",
            "humanoid locomotion",
            "whole-body control",
            "locomotion",
            "manipulation"
          ],
          "score": 20.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "actor-critic"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 23.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "RGMP: Recurrent Geometric-prior Multimodal Policy for Generalizable Humanoid Robot Manipulation",
      "authors": [
        "Xuetao Li",
        "Wenke Huang",
        "Nengyuan Pan",
        "Kaiyan Zhao",
        "Songhua Yang",
        "Yiming Wang",
        "Mengde Li",
        "Mang Ye",
        "Jifeng Xuan",
        "Miao Li"
      ],
      "arxiv_id": "2511.09141v1",
      "summary": "Humanoid robots exhibit significant potential in executing diverse human-level skills. However, current research predominantly relies on data-driven approaches that necessitate extensive training datasets to achieve robust multimodal decision-making capabilities and generalizable visuomotor control. These methods raise concerns due to the neglect of geometric reasoning in unseen scenarios and the inefficient modeling of robot-target relationships within the training data, resulting in significant waste of training resources. To address these limitations, we present the Recurrent Geometric-prior Multimodal Policy (RGMP), an end-to-end framework that unifies geometric-semantic skill reasoning with data-efficient visuomotor control. For perception capabilities, we propose the Geometric-prior Skill Selector, which infuses geometric inductive biases into a vision language model, producing adaptive skill sequences for unseen scenes with minimal spatial common sense tuning. To achieve data-efficient robotic motion synthesis, we introduce the Adaptive Recursive Gaussian Network, which parameterizes robot-object interactions as a compact hierarchy of Gaussian processes that recursively encode multi-scale spatial relationships, yielding dexterous, data-efficient motion synthesis even from sparse demonstrations. Evaluated on both our humanoid robot and desktop dual-arm robot, the RGMP framework achieves 87% task success in generalization tests and exhibits 5x greater data efficiency than the state-of-the-art model. This performance underscores its superior cross-domain generalization, enabled by geometric-semantic reasoning and recursive-Gaussion adaptation.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "",
      "doi": "",
      "journal_ref": "Proceedings of the AAAI conference on artificial intelligence, 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.09141v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot",
            "[T]manipulation",
            "dual-arm"
          ],
          "score": 20.0
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "motion synthesis"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 22.5,
      "hit_pillars": [
        "1_robot_core",
        "4_motion_diffusion"
      ]
    },
    {
      "title": "MILE: A Mechanically Isomorphic Exoskeleton Data Collection System with Fingertip Visuotactile Sensing for Dexterous Manipulation",
      "authors": [
        "Jinda Du",
        "Jieji Ren",
        "Qiaojun Yu",
        "Ningbin Zhang",
        "Yu Deng",
        "Xingyu Wei",
        "Yufei Liu",
        "Guoying Gu",
        "Xiangyang Zhu"
      ],
      "arxiv_id": "2512.00324v1",
      "summary": "Imitation learning provides a promising approach to dexterous hand manipulation, but its effectiveness is limited by the lack of large-scale, high-fidelity data. Existing data-collection pipelines suffer from inaccurate motion retargeting, low data-collection efficiency, and missing high-resolution fingertip tactile sensing. We address this gap with MILE, a mechanically isomorphic teleoperation and data-collection system co-designed from human hand to exoskeleton to robotic hand. The exoskeleton is anthropometrically derived from the human hand, and the robotic hand preserves one-to-one joint-position isomorphism, eliminating nonlinear retargeting and enabling precise, natural control. The exoskeleton achieves a multi-joint mean absolute angular error below one degree, while the robotic hand integrates compact fingertip visuotactile modules that provide high-resolution tactile observations. Built on this retargeting-free interface, we teleoperate complex, contact-rich in-hand manipulation and efficiently collect a multimodal dataset comprising high-resolution fingertip visuotactile signals, RGB-D images, and joint positions. The teleoperation pipeline achieves a mean success rate improvement of 64%. Incorporating fingertip tactile observations further increases the success rate by an average of 25% over the vision-only baseline, validating the fidelity and utility of the dataset. Further details are available at: https://sites.google.com/view/mile-system.",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-29",
      "updated": "2025-11-29",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00324v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "dexterous hand",
            "[T]dexterous manipulation",
            "in-hand manipulation",
            "teleoperation"
          ],
          "score": 18.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "motion retargeting"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 22.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "7_retargeting"
      ]
    },
    {
      "title": "SMF-VO: Direct Ego-Motion Estimation via Sparse Motion Fields",
      "authors": [
        "Sangheon Yang",
        "Yeongin Yoon",
        "Hong Mo Jung",
        "Jongwoo Lim"
      ],
      "arxiv_id": "2511.09072v1",
      "summary": "Traditional Visual Odometry (VO) and Visual Inertial Odometry (VIO) methods rely on a 'pose-centric' paradigm, which computes absolute camera poses from the local map thus requires large-scale landmark maintenance and continuous map optimization. This approach is computationally expensive, limiting their real-time performance on resource-constrained devices. To overcome these limitations, we introduce Sparse Motion Field Visual Odometry (SMF-VO), a lightweight, 'motion-centric' framework. Our approach directly estimates instantaneous linear and angular velocity from sparse optical flow, bypassing the need for explicit pose estimation or expensive landmark tracking. We also employed a generalized 3D ray-based motion field formulation that works accurately with various camera models, including wide-field-of-view lenses. SMF-VO demonstrates superior efficiency and competitive accuracy on benchmark datasets, achieving over 100 FPS on a Raspberry Pi 5 using only a CPU. Our work establishes a scalable and efficient alternative to conventional methods, making it highly suitable for mobile robotics and wearable devices.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09072v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "visual odometry",
            "[T]VO",
            "VIO",
            "visual-inertial",
            "optical flow",
            "[T]ego-motion",
            "pose estimation"
          ],
          "score": 22.0
        }
      ],
      "relevance_score": 22.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Dual-Arm Whole-Body Motion Planning: Leveraging Overlapping Kinematic Chains",
      "authors": [
        "Richard Cheng",
        "Peter Werner",
        "Carolyn Matl"
      ],
      "arxiv_id": "2511.08778v1",
      "summary": "High degree-of-freedom dual-arm robots are becoming increasingly common due to their morphology enabling them to operate effectively in human environments. However, motion planning in real-time within unknown, changing environments remains a challenge for such robots due to the high dimensionality of the configuration space and the complex collision-avoidance constraints that must be obeyed. In this work, we propose a novel way to alleviate the curse of dimensionality by leveraging the structure imposed by shared joints (e.g. torso joints) in a dual-arm robot. First, we build two dynamic roadmaps (DRM) for each kinematic chain (i.e. left arm + torso, right arm + torso) with specific structure induced by the shared joints. Then, we show that we can leverage this structure to efficiently search through the composition of the two roadmaps and largely sidestep the curse of dimensionality. Finally, we run several experiments in a real-world grocery store with this motion planner on a 19 DoF mobile manipulation robot executing a grocery fulfillment task, achieving 0.4s average planning times with 99.9% success rate across more than 2000 motion plans.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "Published in Humanoids 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08778v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]whole-body motion",
            "manipulation",
            "mobile manipulation",
            "[T]dual-arm",
            "[T]motion planning"
          ],
          "score": 22.0
        }
      ],
      "relevance_score": 22.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Integration of Visual SLAM into Consumer-Grade Automotive Localization",
      "authors": [
        "Luis Diener",
        "Jens Kalkkuhl",
        "Markus Enzweiler"
      ],
      "arxiv_id": "2511.06919v1",
      "summary": "Accurate ego-motion estimation in consumer-grade vehicles currently relies on proprioceptive sensors, i.e. wheel odometry and IMUs, whose performance is limited by systematic errors and calibration. While visual-inertial SLAM has become a standard in robotics, its integration into automotive ego-motion estimation remains largely unexplored. This paper investigates how visual SLAM can be integrated into consumer-grade vehicle localization systems to improve performance. We propose a framework that fuses visual SLAM with a lateral vehicle dynamics model to achieve online gyroscope calibration under realistic driving conditions. Experimental results demonstrate that vision-based integration significantly improves gyroscope calibration accuracy and thus enhances overall localization performance, highlighting a promising path toward higher automotive localization accuracy. We provide results on both proprietary and public datasets, showing improved performance and superior localization accuracy on a public benchmark compared to state-of-the-art methods.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "This manuscript has been submitted to the IEEE for possible publication",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06919v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]visual SLAM",
            "[T]SLAM",
            "visual-inertial",
            "ego-motion",
            "[T]localization"
          ],
          "score": 22.0
        }
      ],
      "relevance_score": 22.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "ViTaMIn-B: A Reliable and Efficient Visuo-Tactile Bimanual Manipulation Interface",
      "authors": [
        "Chuanyu Li",
        "Chaoyi Liu",
        "Daotan Wang",
        "Shuyu Zhang",
        "Lusong Li",
        "Zecui Zeng",
        "Fangchen Liu",
        "Jing Xu",
        "Rui Chen"
      ],
      "arxiv_id": "2511.05858v2",
      "summary": "Handheld devices have opened up unprecedented opportunities to collect large-scale, high-quality demonstrations efficiently. However, existing systems often lack robust tactile sensing or reliable pose tracking to handle complex interaction scenarios, especially for bimanual and contact-rich tasks. In this work, we propose ViTaMIn-B, a more capable and efficient handheld data collection system for such tasks. We first design DuoTact, a novel compliant visuo-tactile sensor built with a flexible frame to withstand large contact forces during manipulation while capturing high-resolution contact geometry. To enhance the cross-sensor generalizability, we propose reconstructing the sensor's global deformation as a 3D point cloud and using it as the policy input. We further develop a robust, unified 6-DoF bimanual pose acquisition process using Meta Quest controllers, which eliminates the trajectory drift issue in common SLAM-based methods. Comprehensive user studies confirm the efficiency and high usability of ViTaMIn-B among novice and expert operators. Furthermore, experiments on four bimanual manipulation tasks demonstrate its superior task performance relative to existing systems. Project page: https://chuanyune.github.io/ViTaMIn-B_page/",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-08",
      "updated": "2025-12-02",
      "comment": "Project page: https://chuanyune.github.io/ViTaMIn-B_page/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05858v2",
      "code_links": [
        {
          "url": "https://chuanyune.github.io/ViTaMIn-B_page/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]bi-manual",
            "[T]bimanual"
          ],
          "score": 18.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "SLAM",
            "point cloud"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 22.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action",
      "authors": [
        "Xiaoquan Sun",
        "Ruijian Zhang",
        "Kang Pang",
        "Bingchen Miao",
        "Yuxiang Tan",
        "Zhen Yang",
        "Ming Li",
        "Jiayu Chen"
      ],
      "arxiv_id": "2511.14161v2",
      "summary": "Household tidying is an important application area, yet current benchmarks neither model user preferences nor support mobility, and they generalize poorly, making it hard to comprehensively assess integrated language-to-action capabilities. To address this, we propose RoboTidy, a unified benchmark for language-guided household tidying that supports Vision-Language-Action (VLA) and Vision-Language-Navigation (VLN) training and evaluation. RoboTidy provides 500 photorealistic 3D Gaussian Splatting (3DGS) household scenes (covering 500 objects and containers) with collisions, formulates tidying as an \"Action (Object, Container)\" list, and supplies 6.4k high-quality manipulation demonstration trajectories and 1.5k naviagtion trajectories to support both few-shot and large-scale training. We also deploy RoboTidy in the real world for object tidying, establishing an end-to-end benchmark for household tidying. RoboTidy offers a scalable platform and bridges a key gap in embodied AI by enabling holistic and realistic evaluation of language-guided robots.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-18",
      "updated": "2025-11-19",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14161v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting",
            "[T]navigation"
          ],
          "score": 20.0
        }
      ],
      "relevance_score": 22.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "RoboAfford++: A Generative AI-Enhanced Dataset for Multimodal Affordance Learning in Robotic Manipulation and Navigation",
      "authors": [
        "Xiaoshuai Hao",
        "Yingbo Tang",
        "Lingfeng Zhang",
        "Yanbiao Ma",
        "Yunfeng Diao",
        "Ziyu Jia",
        "Wenbo Ding",
        "Hangjun Ye",
        "Long Chen"
      ],
      "arxiv_id": "2511.12436v1",
      "summary": "Robotic manipulation and navigation are fundamental capabilities of embodied intelligence, enabling effective robot interactions with the physical world. Achieving these capabilities requires a cohesive understanding of the environment, including object recognition to localize target objects, object affordances to identify potential interaction areas and spatial affordances to discern optimal areas for both object placement and robot movement. While Vision-Language Models (VLMs) excel at high-level task planning and scene understanding, they often struggle to infer actionable positions for physical interaction, such as functional grasping points and permissible placement regions. This limitation stems from the lack of fine-grained annotations for object and spatial affordances in their training datasets. To tackle this challenge, we introduce RoboAfford++, a generative AI-enhanced dataset for multimodal affordance learning for both robotic manipulation and navigation. Our dataset comprises 869,987 images paired with 2.0 million question answering (QA) annotations, covering three critical tasks: object affordance recognition to identify target objects based on attributes and spatial relationships, object affordance prediction to pinpoint functional parts for manipulation, and spatial affordance localization to identify free space for object placement and robot navigation. Complementing this dataset, we propose RoboAfford-Eval, a comprehensive benchmark for assessing affordance-aware prediction in real-world scenarios, featuring 338 meticulously annotated samples across the same three tasks. Extensive experimental results reveal the deficiencies of existing VLMs in affordance learning, while fine-tuning on the RoboAfford++ dataset significantly enhances their ability to reason about object and spatial affordances, validating the dataset's effectiveness.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-16",
      "updated": "2025-11-16",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12436v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "grasping",
            "grasp"
          ],
          "score": 10.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding",
            "affordance prediction",
            "localization",
            "[T]navigation"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 22.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "GS-Checker: Tampering Localization for 3D Gaussian Splatting",
      "authors": [
        "Haoliang Han",
        "Ziyuan Luo",
        "Jun Qi",
        "Anderson Rocha",
        "Renjie Wan"
      ],
      "arxiv_id": "2511.20354v1",
      "summary": "Recent advances in editing technologies for 3D Gaussian Splatting (3DGS) have made it simple to manipulate 3D scenes. However, these technologies raise concerns about potential malicious manipulation of 3D content. To avoid such malicious applications, localizing tampered regions becomes crucial. In this paper, we propose GS-Checker, a novel method for locating tampered areas in 3DGS models. Our approach integrates a 3D tampering attribute into the 3D Gaussian parameters to indicate whether the Gaussian has been tampered. Additionally, we design a 3D contrastive mechanism by comparing the similarity of key attributes between 3D Gaussians to seek tampering cues at 3D level. Furthermore, we introduce a cyclic optimization strategy to refine the 3D tampering attribute, enabling more accurate tampering localization. Notably, our approach does not require expensive 3D labels for supervision. Extensive experimental results demonstrate the effectiveness of our proposed method to locate the tampered 3DGS area.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "Accepted by AAAI2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20354v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting",
            "[T]localization"
          ],
          "score": 20.0
        }
      ],
      "relevance_score": 22.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Splatblox: Traversability-Aware Gaussian Splatting for Outdoor Robot Navigation",
      "authors": [
        "Samarth Chopra",
        "Jing Liang",
        "Gershom Seneviratne",
        "Yonghan Lee",
        "Jaehoon Choi",
        "Jianyu An",
        "Stephen Cheng",
        "Dinesh Manocha"
      ],
      "arxiv_id": "2511.18525v1",
      "summary": "We present Splatblox, a real-time system for autonomous navigation in outdoor environments with dense vegetation, irregular obstacles, and complex terrain. Our method fuses segmented RGB images and LiDAR point clouds using Gaussian Splatting to construct a traversability-aware Euclidean Signed Distance Field (ESDF) that jointly encodes geometry and semantics. Updated online, this field enables semantic reasoning to distinguish traversable vegetation (e.g., tall grass) from rigid obstacles (e.g., trees), while LiDAR ensures 360-degree geometric coverage for extended planning horizons. We validate Splatblox on a quadruped robot and demonstrate transfer to a wheeled platform. In field trials across vegetation-rich scenarios, it outperforms state-of-the-art methods with over 50% higher success rate, 40% fewer freezing incidents, 5% shorter paths, and up to 13% faster time to goal, while supporting long-range missions up to 100 meters. Experiment videos and more details can be found on our project page: https://splatblox.github.io",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-23",
      "updated": "2025-11-23",
      "comment": "Submitted to ICRA 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18525v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]gaussian splatting",
            "[T]traversability",
            "point cloud",
            "[T]navigation"
          ],
          "score": 20.0
        }
      ],
      "relevance_score": 22.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Autonomous Grasping On Quadruped Robot With Task Level Interaction",
      "authors": [
        "Muhtadin",
        "Mochammad Hilmi Rusydiansyah",
        "Mauridhi Hery Purnomo",
        "I Ketut Eddy Purnama",
        "Chastine Fatichah"
      ],
      "arxiv_id": "2512.01052v1",
      "summary": "Quadruped robots are increasingly used in various applications due to their high mobility and ability to operate in diverse terrains. However, most available quadruped robots are primarily focused on mobility without object manipulation capabilities. Equipping a quadruped robot with a robotic arm and gripper introduces a challenge in manual control, especially in remote scenarios that require complex commands. This research aims to develop an autonomous grasping system on a quadruped robot using a task-level interaction approach. The system includes hardware integration of a robotic arm and gripper onto the quadruped robot's body, a layered control system designed using ROS, and a web-based interface for human-robot interaction. The robot is capable of autonomously performing tasks such as navigation, object detection, and grasping using GraspNet. Testing was conducted through real-world scenarios to evaluate navigation, object selection and grasping, and user experience. The results show that the robot can perform tasks accurately and consistently, achieving a grasping success rate of 75 % from 12 trials. Therefore, the system demonstrates significant potential in enhancing the capabilities of quadruped robots as service robots in real-world environments.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01052v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]quadruped",
            "manipulation",
            "[T]grasping",
            "[T]grasp"
          ],
          "score": 20.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 22.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "FreeMusco: Motion-Free Learning of Latent Control for Morphology-Adaptive Locomotion in Musculoskeletal Characters",
      "authors": [
        "Minkwan Kim",
        "Yoonsang Lee"
      ],
      "arxiv_id": "2511.14205v1",
      "summary": "We propose FreeMusco, a motion-free framework that jointly learns latent representations and control policies for musculoskeletal characters. By leveraging the musculoskeletal model as a strong prior, our method enables energy-aware and morphology-adaptive locomotion to emerge without motion data. The framework generalizes across human, non-human, and synthetic morphologies, where distinct energy-efficient strategies naturally appear--for example, quadrupedal gaits in Chimanoid versus bipedal gaits in Humanoid. The latent space and corresponding control policy are constructed from scratch, without demonstration, and enable downstream tasks such as goal navigation and path following--representing, to our knowledge, the first motion-free method to provide such capabilities. FreeMusco learns diverse and physically plausible locomotion behaviors through model-based reinforcement learning, guided by the locomotion objective that combines control, balancing, and biomechanical terms. To better capture the periodic structure of natural gait, we introduce the temporally averaged loss formulation, which compares simulated and target states over a time window rather than on a per-frame basis. We further encourage behavioral diversity by randomizing target poses and energy levels during training, enabling locomotion to be flexibly modulated in both form and intensity at runtime. Together, these results demonstrate that versatile and adaptive locomotion control can emerge without motion capture, offering a new direction for simulating movement in characters where data collection is impractical or impossible.",
      "categories": [
        "cs.GR"
      ],
      "primary_category": "cs.GR",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "SIGGRAPH Asia 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14205v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped",
            "quadrupedal",
            "humanoid",
            "bipedal",
            "biped",
            "[T]locomotion",
            "gait"
          ],
          "score": 18.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 21.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "SafeFall: Learning Protective Control for Humanoid Robots",
      "authors": [
        "Ziyu Meng",
        "Tengyu Liu",
        "Le Ma",
        "Yingying Wu",
        "Ran Song",
        "Wei Zhang",
        "Siyuan Huang"
      ],
      "arxiv_id": "2511.18509v1",
      "summary": "Bipedal locomotion makes humanoid robots inherently prone to falls, causing catastrophic damage to the expensive sensors, actuators, and structural components of full-scale robots. To address this critical barrier to real-world deployment, we present \\method, a framework that learns to predict imminent, unavoidable falls and execute protective maneuvers to minimize hardware damage. SafeFall is designed to operate seamlessly alongside existing nominal controller, ensuring no interference during normal operation. It combines two synergistic components: a lightweight, GRU-based fall predictor that continuously monitors the robot's state, and a reinforcement learning policy for damage mitigation. The protective policy remains dormant until the predictor identifies a fall as unavoidable, at which point it activates to take control and execute a damage-minimizing response. This policy is trained with a novel, damage-aware reward function that incorporates the robot's specific structural vulnerabilities, learning to shield critical components like the head and hands while absorbing energy with more robust parts of its body. Validated on a full-scale Unitree G1 humanoid, SafeFall demonstrated significant performance improvements over unprotected falls. It reduced peak contact forces by 68.3\\%, peak joint torques by 78.4\\%, and eliminated 99.3\\% of collisions with vulnerable components. By enabling humanoids to fail safely, SafeFall provides a crucial safety net that allows for more aggressive experiments and accelerates the deployment of these robots in complex, real-world environments.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-23",
      "updated": "2025-11-23",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18509v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot",
            "bipedal",
            "biped",
            "locomotion",
            "unitree"
          ],
          "score": 20.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 21.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Learning Natural and Robust Hexapod Locomotion over Complex Terrains via Motion Priors based on Deep Reinforcement Learning",
      "authors": [
        "Xin Liu",
        "Jinze Wu",
        "Yinghui Li",
        "Chenkun Qi",
        "Yufei Xue",
        "Feng Gao"
      ],
      "arxiv_id": "2511.03167v1",
      "summary": "Multi-legged robots offer enhanced stability to navigate complex terrains with their multiple legs interacting with the environment. However, how to effectively coordinate the multiple legs in a larger action exploration space to generate natural and robust movements is a key issue. In this paper, we introduce a motion prior-based approach, successfully applying deep reinforcement learning algorithms to a real hexapod robot. We generate a dataset of optimized motion priors, and train an adversarial discriminator based on the priors to guide the hexapod robot to learn natural gaits. The learned policy is then successfully transferred to a real hexapod robot, and demonstrate natural gait patterns and remarkable robustness without visual information in complex terrains. This is the first time that a reinforcement learning controller has been used to achieve complex terrain walking on a real hexapod robot.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-05",
      "updated": "2025-11-05",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03167v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "legged robot",
            "[T]locomotion",
            "gait",
            "walking"
          ],
          "score": 12.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "[T]deep reinforcement learning"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 21.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Dual-Agent Reinforcement Learning for Adaptive and Cost-Aware Visual-Inertial Odometry",
      "authors": [
        "Feiyang Pan",
        "Shenghe Zheng",
        "Chunyan Yin",
        "Guangbin Dou"
      ],
      "arxiv_id": "2511.21083v1",
      "summary": "Visual-Inertial Odometry (VIO) is a critical component for robust ego-motion estimation, enabling foundational capabilities such as autonomous navigation in robotics and real-time 6-DoF tracking for augmented reality. Existing methods face a well-known trade-off: filter-based approaches are efficient but prone to drift, while optimization-based methods, though accurate, rely on computationally prohibitive Visual-Inertial Bundle Adjustment (VIBA) that is difficult to run on resource-constrained platforms. Rather than removing VIBA altogether, we aim to reduce how often and how heavily it must be invoked. To this end, we cast two key design choices in modern VIO, when to run the visual frontend and how strongly to trust its output, as sequential decision problems, and solve them with lightweight reinforcement learning (RL) agents. Our framework introduces a lightweight, dual-pronged RL policy that serves as our core contribution: (1) a Select Agent intelligently gates the entire VO pipeline based only on high-frequency IMU data; and (2) a composite Fusion Agent that first estimates a robust velocity state via a supervised network, before an RL policy adaptively fuses the full (p, v, q) state. Experiments on the EuRoC MAV and TUM-VI datasets show that, in our unified evaluation, the proposed method achieves a more favorable accuracy-efficiency-memory trade-off than prior GPU-based VO/VIO systems: it attains the best average ATE while running up to 1.77 times faster and using less GPU memory. Compared to classical optimization-based VIO systems, our approach maintains competitive trajectory accuracy while substantially reducing computational load.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21083v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "running"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "VO",
            "VIO",
            "[T]visual-inertial",
            "ego-motion",
            "navigation"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 20.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "ACE-F: A Cross Embodiment Foldable System with Force Feedback for Dexterous Teleoperation",
      "authors": [
        "Rui Yan",
        "Jiajian Fu",
        "Shiqi Yang",
        "Lars Paulsen",
        "Xuxin Cheng",
        "Xiaolong Wang"
      ],
      "arxiv_id": "2511.20887v1",
      "summary": "Teleoperation systems are essential for efficiently collecting diverse and high-quality robot demonstration data, especially for complex, contact-rich tasks. However, current teleoperation platforms typically lack integrated force feedback, cross-embodiment generalization, and portable, user-friendly designs, limiting their practical deployment. To address these limitations, we introduce ACE-F, a cross embodiment foldable teleoperation system with integrated force feedback. Our approach leverages inverse kinematics (IK) combined with a carefully designed human-robot interface (HRI), enabling users to capture precise and high-quality demonstrations effortlessly. We further propose a generalized soft-controller pipeline integrating PD control and inverse dynamics to ensure robot safety and precise motion control across diverse robotic embodiments. Critically, to achieve cross-embodiment generalization of force feedback without additional sensors, we innovatively interpret end-effector positional deviations as virtual force signals, which enhance data collection and enable applications in imitation learning. Extensive teleoperation experiments confirm that ACE-F significantly simplifies the control of various robot embodiments, making dexterous manipulation tasks as intuitive as operating a computer mouse. The system is open-sourced at: https://acefoldable.github.io/",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20887v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "dexterous manipulation",
            "[T]teleoperation"
          ],
          "score": 10.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "[T]cross-embodiment"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 20.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "7_retargeting"
      ]
    },
    {
      "title": "FastGS: Training 3D Gaussian Splatting in 100 Seconds",
      "authors": [
        "Shiwei Ren",
        "Tianci Wen",
        "Yongchun Fang",
        "Biao Lu"
      ],
      "arxiv_id": "2511.04283v3",
      "summary": "The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to properly regulate the number of Gaussians during training, causing redundant computational time overhead. In this paper, we propose FastGS, a novel, simple, and general acceleration framework that fully considers the importance of each Gaussian based on multi-view consistency, efficiently solving the trade-off between training time and rendering quality. We innovatively design a densification and pruning strategy based on multi-view consistency, dispensing with the budgeting mechanism. Extensive experiments on Mip-NeRF 360, Tanks & Temples, and Deep Blending datasets demonstrate that our method significantly outperforms the state-of-the-art methods in training speed, achieving a 3.32$\\times$ training acceleration and comparable rendering quality compared with DashGaussian on the Mip-NeRF 360 dataset and a 15.45$\\times$ acceleration compared with vanilla 3DGS on the Deep Blending dataset. We demonstrate that FastGS exhibits strong generality, delivering 2-7$\\times$ training acceleration across various tasks, including dynamic scene reconstruction, surface reconstruction, sparse-view reconstruction, large-scale reconstruction, and simultaneous localization and mapping. The project page is available at https://fastgs.github.io/",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-06",
      "updated": "2025-12-06",
      "comment": "Project page: https://fastgs.github.io/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04283v3",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting",
            "NeRF",
            "scene reconstruction",
            "localization"
          ],
          "score": 20.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Dual-MPC Footstep Planning for Robust Quadruped Locomotion",
      "authors": [
        "Byeong-Il Ham",
        "Hyun-Bin Kim",
        "Jeonguk Kang",
        "Keun Ha Choi",
        "Kyung-Soo Kim"
      ],
      "arxiv_id": "2511.07921v1",
      "summary": "In this paper, we propose a footstep planning strategy based on model predictive control (MPC) that enables robust regulation of body orientation against undesired body rotations by optimizing footstep placement. Model-based locomotion approaches typically adopt heuristic methods or planning based on the linear inverted pendulum model. These methods account for linear velocity in footstep planning, while excluding angular velocity, which leads to angular momentum being handled exclusively via ground reaction force (GRF). Footstep planning based on MPC that takes angular velocity into account recasts the angular momentum control problem as a dual-input approach that coordinates GRFs and footstep placement, instead of optimizing GRFs alone, thereby improving tracking performance. A mutual-feedback loop couples the footstep planner and the GRF MPC, with each using the other's solution to iteratively update footsteps and GRFs. The use of optimal solutions reduces body oscillation and enables extended stance and swing phases. The method is validated on a quadruped robot, demonstrating robust locomotion with reduced oscillations, longer stance and swing phases across various terrains.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "9 pages, 9 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07921v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]quadruped",
            "[T]locomotion",
            "[T]MPC",
            "model predictive control"
          ],
          "score": 20.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Koopman global linearization of contact dynamics for robot locomotion and manipulation enables elaborate control",
      "authors": [
        "Cormac O'Neill",
        "Jasmine Terrones",
        "H. Harry Asada"
      ],
      "arxiv_id": "2511.06515v1",
      "summary": "Controlling robots that dynamically engage in contact with their environment is a pressing challenge. Whether a legged robot making-and-breaking contact with a floor, or a manipulator grasping objects, contact is everywhere. Unfortunately, the switching of dynamics at contact boundaries makes control difficult. Predictive controllers face non-convex optimization problems when contact is involved. Here, we overcome this difficulty by applying Koopman operators to subsume the segmented dynamics due to contact changes into a unified, globally-linear model in an embedding space. We show that viscoelastic contact at robot-environment interactions underpins the use of Koopman operators without approximation to control inputs. This methodology enables the convex Model Predictive Control of a legged robot, and the real-time control of a manipulator engaged in dynamic pushing. In this work, we show that our method allows robots to discover elaborate control strategies in real-time over time horizons with multiple contact changes, and the method is applicable to broad fields beyond robotics.",
      "categories": [
        "cs.RO",
        "math.DS"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-09",
      "updated": "2025-11-09",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06515v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "legged robot",
            "[T]locomotion",
            "[T]manipulation",
            "grasping",
            "grasp",
            "model predictive control"
          ],
          "score": 20.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Beyond Darkness: Thermal-Supervised 3D Gaussian Splatting for Low-Light Novel View Synthesis",
      "authors": [
        "Qingsen Ma",
        "Chen Zou",
        "Dianyun Wang",
        "Jia Wang",
        "Liuyu Xiang",
        "Zhaofeng He"
      ],
      "arxiv_id": "2511.13011v1",
      "summary": "Under extremely low-light conditions, novel view synthesis (NVS) faces severe degradation in terms of geometry, color consistency, and radiometric stability. Standard 3D Gaussian Splatting (3DGS) pipelines fail when applied directly to underexposed inputs, as independent enhancement across views causes illumination inconsistencies and geometric distortion. To address this, we present DTGS, a unified framework that tightly couples Retinex-inspired illumination decomposition with thermal-guided 3D Gaussian Splatting for illumination-invariant reconstruction. Unlike prior approaches that treat enhancement as a pre-processing step, DTGS performs joint optimization across enhancement, geometry, and thermal supervision through a cyclic enhancement-reconstruction mechanism. A thermal supervisory branch stabilizes both color restoration and geometry learning by dynamically balancing enhancement, structural, and thermal losses. Moreover, a Retinex-based decomposition module embedded within the 3DGS loop provides physically interpretable reflectance-illumination separation, ensuring consistent color and texture across viewpoints. To evaluate our method, we construct RGBT-LOW, a new multi-view low-light thermal dataset capturing severe illumination degradation. Extensive experiments show that DTGS significantly outperforms existing low-light enhancement and 3D reconstruction baselines, achieving superior radiometric consistency, geometric fidelity, and color stability under extreme illumination.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13011v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting",
            "[T]novel view synthesis"
          ],
          "score": 20.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "DiffuDepGrasp: Diffusion-based Depth Noise Modeling Empowers Sim2Real Robotic Grasping",
      "authors": [
        "Yingting Zhou",
        "Wenbo Cui",
        "Weiheng Liu",
        "Guixing Chen",
        "Haoran Li",
        "Dongbin Zhao"
      ],
      "arxiv_id": "2511.12912v1",
      "summary": "Transferring the depth-based end-to-end policy trained in simulation to physical robots can yield an efficient and robust grasping policy, yet sensor artifacts in real depth maps like voids and noise establish a significant sim2real gap that critically impedes policy transfer. Training-time strategies like procedural noise injection or learned mappings suffer from data inefficiency due to unrealistic noise simulation, which is often ineffective for grasping tasks that require fine manipulation or dependency on paired datasets heavily. Furthermore, leveraging foundation models to reduce the sim2real gap via intermediate representations fails to mitigate the domain shift fully and adds computational overhead during deployment. This work confronts dual challenges of data inefficiency and deployment complexity. We propose DiffuDepGrasp, a deploy-efficient sim2real framework enabling zero-shot transfer through simulation-exclusive policy training. Its core innovation, the Diffusion Depth Generator, synthesizes geometrically pristine simulation depth with learned sensor-realistic noise via two synergistic modules. The first Diffusion Depth Module leverages temporal geometric priors to enable sample-efficient training of a conditional diffusion model that captures complex sensor noise distributions, while the second Noise Grafting Module preserves metric accuracy during perceptual artifact injection. With only raw depth inputs during deployment, DiffuDepGrasp eliminates computational overhead and achieves a 95.7% average success rate on 12-object grasping with zero-shot transfer and strong generalization to unseen objects.Project website: https://diffudepgrasp.github.io/.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12912v1",
      "code_links": [
        {
          "url": "https://diffudepgrasp.github.io/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "[T]grasping",
            "[T]grasp",
            "[T]sim2real"
          ],
          "score": 20.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "DensifyBeforehand: LiDAR-assisted Content-aware Densification for Efficient and Quality 3D Gaussian Splatting",
      "authors": [
        "Phurtivilai Patt",
        "Leyang Huang",
        "Yinqiang Zhang",
        "Yang Lei"
      ],
      "arxiv_id": "2511.19294v1",
      "summary": "This paper addresses the limitations of existing 3D Gaussian Splatting (3DGS) methods, particularly their reliance on adaptive density control, which can lead to floating artifacts and inefficient resource usage. We propose a novel densify beforehand approach that enhances the initialization of 3D scenes by combining sparse LiDAR data with monocular depth estimation from corresponding RGB images. Our ROI-aware sampling scheme prioritizes semantically and geometrically important regions, yielding a dense point cloud that improves visual fidelity and computational efficiency. This densify beforehand approach bypasses the adaptive density control that may introduce redundant Gaussians in the original pipeline, allowing the optimization to focus on the other attributes of 3D Gaussian primitives, reducing overlap while enhancing visual quality. Our method achieves comparable results to state-of-the-art techniques while significantly lowering resource consumption and training time. We validate our approach through extensive comparisons and ablation studies on four newly collected datasets, showcasing its effectiveness in preserving regions of interest in complex scenes.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19294v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation",
            "monocular depth",
            "[T]3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting",
            "point cloud"
          ],
          "score": 20.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Reference-Free Sampling-Based Model Predictive Control",
      "authors": [
        "Fabian Schramm",
        "Pierre Fabre",
        "Nicolas Perrin-Gilbert",
        "Justin Carpentier"
      ],
      "arxiv_id": "2511.19204v1",
      "summary": "We present a sampling-based model predictive control (MPC) framework that enables emergent locomotion without relying on handcrafted gait patterns or predefined contact sequences. Our method discovers diverse motion patterns, ranging from trotting to galloping, robust standing policies, jumping, and handstand balancing, purely through the optimization of high-level objectives. Building on model predictive path integral (MPPI), we propose a dual-space spline parameterization that operates on position and velocity control points. Our approach enables contact-making and contact-breaking strategies that adapt automatically to task requirements, requiring only a limited number of sampled trajectories. This sample efficiency allows us to achieve real-time control on standard CPU hardware, eliminating the need for GPU acceleration typically required by other state-of-the-art MPPI methods. We validate our approach on the Go2 quadrupedal robot, demonstrating various emergent gaits and basic jumping capabilities. In simulation, we further showcase more complex behaviors, such as backflips, dynamic handstand balancing and locomotion on a Humanoid, all without requiring reference tracking or offline pre-training.",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19204v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped",
            "quadrupedal",
            "humanoid",
            "locomotion",
            "gait",
            "jumping",
            "MPC",
            "[T]model predictive control"
          ],
          "score": 20.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "SENTINEL: A Fully End-to-End Language-Action Model for Humanoid Whole Body Control",
      "authors": [
        "Yuxuan Wang",
        "Haobin Jiang",
        "Shiqing Yao",
        "Ziluo Ding",
        "Zongqing Lu"
      ],
      "arxiv_id": "2511.19236v1",
      "summary": "Existing humanoid control systems often rely on teleoperation or modular generation pipelines that separate language understanding from physical execution. However, the former is entirely human-driven, and the latter lacks tight alignment between language commands and physical behaviors. In this paper, we present SENTINEL, a fully end-to-end language-action model for humanoid whole-body control. We construct a large-scale dataset by tracking human motions in simulation using a pretrained whole body controller, combined with their text annotations. The model directly maps language commands and proprioceptive inputs to low-level actions without any intermediate representation. The model generates action chunks using flow matching, which can be subsequently refined by a residual action head for real-world deployment. Our method exhibits strong semantic understanding and stable execution on humanoid robots in both simulation and real-world deployment, and also supports multi-modal extensions by converting inputs into texts.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "23 pages, 8 figures, 11 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19236v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "humanoid control",
            "[T]whole-body control",
            "teleoperation"
          ],
          "score": 18.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "flow matching"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 19.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Opening the Sim-to-Real Door for Humanoid Pixel-to-Action Policy Transfer",
      "authors": [
        "Haoru Xue",
        "Tairan He",
        "Zi Wang",
        "Qingwei Ben",
        "Wenli Xiao",
        "Zhengyi Luo",
        "Xingye Da",
        "Fernando Castañeda",
        "Guanya Shi",
        "Shankar Sastry",
        "Linxi \"Jim\" Fan",
        "Yuke Zhu"
      ],
      "arxiv_id": "2512.01061v1",
      "summary": "Recent progress in GPU-accelerated, photorealistic simulation has opened a scalable data-generation path for robot learning, where massive physics and visual randomization allow policies to generalize beyond curated environments. Building on these advances, we develop a teacher-student-bootstrap learning framework for vision-based humanoid loco-manipulation, using articulated-object interaction as a representative high-difficulty benchmark. Our approach introduces a staged-reset exploration strategy that stabilizes long-horizon privileged-policy training, and a GRPO-based fine-tuning procedure that mitigates partial observability and improves closed-loop consistency in sim-to-real RL. Trained entirely on simulation data, the resulting policy achieves robust zero-shot performance across diverse door types and outperforms human teleoperators by up to 31.7% in task completion time under the same whole-body control stack. This represents the first humanoid sim-to-real policy capable of diverse articulated loco-manipulation using pure RGB perception.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "https://doorman-humanoid.github.io/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01061v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "whole-body control",
            "manipulation",
            "loco-manipulation",
            "[T]sim-to-real"
          ],
          "score": 18.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "teacher-student"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 19.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "SasMamba: A Lightweight Structure-Aware Stride State Space Model for 3D Human Pose Estimation",
      "authors": [
        "Hu Cui",
        "Wenqiang Hua",
        "Renjing Huang",
        "Shurui Jia",
        "Tessai Hayama"
      ],
      "arxiv_id": "2511.08872v1",
      "summary": "Recently, the Mamba architecture based on State Space Models (SSMs) has gained attention in 3D human pose estimation due to its linear complexity and strong global modeling capability. However, existing SSM-based methods typically apply manually designed scan operations to flatten detected 2D pose sequences into purely temporal sequences, either locally or globally. This approach disrupts the inherent spatial structure of human poses and entangles spatial and temporal features, making it difficult to capture complex pose dependencies. To address these limitations, we propose the Skeleton Structure-Aware Stride SSM (SAS-SSM), which first employs a structure-aware spatiotemporal convolution to dynamically capture essential local interactions between joints, and then applies a stride-based scan strategy to construct multi-scale global structural representations. This enables flexible modeling of both local and global pose information while maintaining linear computational complexity. Built upon SAS-SSM, our model SasMamba achieves competitive 3D pose estimation performance with significantly fewer parameters compared to existing hybrid models. The source code is available at https://hucui2022.github.io/sasmamba_proj/.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "8pages, WACV2026 accepted",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08872v1",
      "code_links": [
        {
          "url": "https://hucui2022.github.io/sasmamba_proj/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba",
            "SSM",
            "[T]state space model"
          ],
          "score": 10.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]pose estimation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "3D pose estimation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 18.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam",
        "6_video_extraction"
      ]
    },
    {
      "title": "Socially aware navigation for mobile robots: a survey on deep reinforcement learning approaches",
      "authors": [
        "Ibrahim Khalil Kabir",
        "Muhammad Faizan Mysorewala"
      ],
      "arxiv_id": "2512.00049v1",
      "summary": "Socially aware navigation is a fast-evolving research area in robotics that enables robots to move within human environments while adhering to the implicit human social norms. The advent of Deep Reinforcement Learning (DRL) has accelerated the development of navigation policies that enable robots to incorporate these social conventions while effectively reaching their objectives. This survey offers a comprehensive overview of DRL-based approaches to socially aware navigation, highlighting key aspects such as proxemics, human comfort, naturalness, trajectory and intention prediction, which enhance robot interaction in human environments. This work critically analyzes the integration of value-based, policy-based, and actor-critic reinforcement learning algorithms alongside neural network architectures, such as feedforward, recurrent, convolutional, graph, and transformer networks, for enhancing agent learning and representation in socially aware navigation. Furthermore, we examine crucial evaluation mechanisms, including metrics, benchmark datasets, simulation environments, and the persistent challenges of sim-to-real transfer. Our comparative analysis of the literature reveals that while DRL significantly improves safety, and human acceptance over traditional approaches, the field still faces setback due to non-uniform evaluation mechanisms, absence of standardized social metrics, computational burdens that limit scalability, and difficulty in transferring simulation to real robotic hardware applications. We assert that future progress will depend on hybrid approaches that leverage the strengths of multiple approaches and producing benchmarks that balance technical efficiency with human-centered evaluation.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00049v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "sim-to-real"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "[T]deep reinforcement learning",
            "actor-critic"
          ],
          "score": 10.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 18.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "GraspView: Active Perception Scoring and Best-View Optimization for Robotic Grasping in Cluttered Environments",
      "authors": [
        "Shenglin Wang",
        "Mingtong Dai",
        "Jingxuan Su",
        "Lingbo Liu",
        "Chunjie Chen",
        "Xinyu Wu",
        "Liang Lin"
      ],
      "arxiv_id": "2511.04199v1",
      "summary": "Robotic grasping is a fundamental capability for autonomous manipulation, yet remains highly challenging in cluttered environments where occlusion, poor perception quality, and inconsistent 3D reconstructions often lead to unstable or failed grasps. Conventional pipelines have widely relied on RGB-D cameras to provide geometric information, which fail on transparent or glossy objects and degrade at close range. We present GraspView, an RGB-only robotic grasping pipeline that achieves accurate manipulation in cluttered environments without depth sensors. Our framework integrates three key components: (i) global perception scene reconstruction, which provides locally consistent, up-to-scale geometry from a single RGB view and fuses multi-view projections into a coherent global 3D scene; (ii) a render-and-score active perception strategy, which dynamically selects next-best-views to reveal occluded regions; and (iii) an online metric alignment module that calibrates VGGT predictions against robot kinematics to ensure physical scale consistency. Building on these tailor-designed modules, GraspView performs best-view global grasping, fusing multi-view reconstructions and leveraging GraspNet for robust execution. Experiments on diverse tabletop objects demonstrate that GraspView significantly outperforms both RGB-D and single-view RGB baselines, especially under heavy occlusion, near-field sensing, and with transparent objects. These results highlight GraspView as a practical and versatile alternative to RGB-D pipelines, enabling reliable grasping in unstructured real-world environments.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04199v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "[T]grasping",
            "[T]grasp"
          ],
          "score": 14.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene reconstruction",
            "VGGT"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Whole-body motion planning and safety-critical control for aerial manipulation",
      "authors": [
        "Lin Yang",
        "Jinwoo Lee",
        "Domenico Campolo",
        "H. Jin Kim",
        "Jeonghyun Byun"
      ],
      "arxiv_id": "2511.02342v2",
      "summary": "Aerial manipulation combines the maneuverability of multirotors with the dexterity of robotic arms to perform complex tasks in cluttered spaces. Yet planning safe, dynamically feasible trajectories remains difficult due to whole-body collision avoidance and the conservativeness of common geometric abstractions such as bounding boxes or ellipsoids. We present a whole-body motion planning and safety-critical control framework for aerial manipulators built on superquadrics (SQs). Using an SQ-plus-proxy representation, we model both the vehicle and obstacles with differentiable, geometry-accurate surfaces. Leveraging this representation, we introduce a maximum-clearance planner that fuses Voronoi diagrams with an equilibrium-manifold formulation to generate smooth, collision-aware trajectories. We further design a safety-critical controller that jointly enforces thrust limits and collision avoidance via high-order control barrier functions. In simulation, our approach outperforms sampling-based planners in cluttered environments, producing faster, safer, and smoother trajectories and exceeding ellipsoid-based baselines in geometric fidelity. Actual experiments on a physical aerial-manipulation platform confirm feasibility and robustness, demonstrating consistent performance across simulation and hardware settings. The video can be found at https://youtu.be/hQYKwrWf1Ak.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-04",
      "updated": "2025-11-10",
      "comment": "Submitted to 2026 IFAC World Congress with the Journal option (MECHATRONICS)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02342v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]whole-body motion",
            "[T]manipulation",
            "[T]motion planning"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Object-Centric 3D Gaussian Splatting for Strawberry Plant Reconstruction and Phenotyping",
      "authors": [
        "Jiajia Li",
        "Keyi Zhu",
        "Qianwen Zhang",
        "Dong Chen",
        "Qi Sun",
        "Zhaojian Li"
      ],
      "arxiv_id": "2511.02207v1",
      "summary": "Strawberries are among the most economically significant fruits in the United States, generating over $2 billion in annual farm-gate sales and accounting for approximately 13% of the total fruit production value. Plant phenotyping plays a vital role in selecting superior cultivars by characterizing plant traits such as morphology, canopy structure, and growth dynamics. However, traditional plant phenotyping methods are time-consuming, labor-intensive, and often destructive. Recently, neural rendering techniques, notably Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have emerged as powerful frameworks for high-fidelity 3D reconstruction. By capturing a sequence of multi-view images or videos around a target plant, these methods enable non-destructive reconstruction of complex plant architectures. Despite their promise, most current applications of 3DGS in agricultural domains reconstruct the entire scene, including background elements, which introduces noise, increases computational costs, and complicates downstream trait analysis. To address this limitation, we propose a novel object-centric 3D reconstruction framework incorporating a preprocessing pipeline that leverages the Segment Anything Model v2 (SAM-2) and alpha channel background masking to achieve clean strawberry plant reconstructions. This approach produces more accurate geometric representations while substantially reducing computational time. With a background-free reconstruction, our algorithm can automatically estimate important plant traits, such as plant height and canopy width, using DBSCAN clustering and Principal Component Analysis (PCA). Experimental results show that our method outperforms conventional pipelines in both accuracy and efficiency, offering a scalable and non-destructive solution for strawberry plant phenotyping.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-04",
      "updated": "2025-11-04",
      "comment": "11 pages, 4 figures, 3 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02207v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting",
            "NeRF",
            "neural radiance"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "DualVision ArthroNav: Investigating Opportunities to Enhance Localization and Reconstruction in Image-based Arthroscopy Navigation via External Cameras",
      "authors": [
        "Hongchao Shu",
        "Lalithkumar Seenivasan",
        "Mingxu Liu",
        "Yunseo Hwang",
        "Yu-Chun Ku",
        "Jonathan Knopf",
        "Alejandro Martin-Gomez",
        "Mehran Armand",
        "Mathias Unberath"
      ],
      "arxiv_id": "2511.10699v1",
      "summary": "Arthroscopic procedures can greatly benefit from navigation systems that enhance spatial awareness, depth perception, and field of view. However, existing optical tracking solutions impose strict workspace constraints and disrupt surgical workflow. Vision-based alternatives, though less invasive, often rely solely on the monocular arthroscope camera, making them prone to drift, scale ambiguity, and sensitivity to rapid motion or occlusion. We propose DualVision ArthroNav, a multi-camera arthroscopy navigation system that integrates an external camera rigidly mounted on the arthroscope. The external camera provides stable visual odometry and absolute localization, while the monocular arthroscope video enables dense scene reconstruction. By combining these complementary views, our system resolves the scale ambiguity and long-term drift inherent in monocular SLAM and ensures robust relocalization. Experiments demonstrate that our system effectively compensates for calibration errors, achieving an average absolute trajectory error of 1.09 mm. The reconstructed scenes reach an average target registration error of 2.16 mm, with high visual fidelity (SSIM = 0.69, PSNR = 22.19). These results indicate that our system provides a practical and cost-efficient solution for arthroscopic navigation, bridging the gap between optical tracking and purely vision-based systems, and paving the way toward clinically deployable, fully vision-based arthroscopic guidance.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "eess.IV",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10699v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "visual odometry",
            "SLAM",
            "scene reconstruction",
            "[T]localization",
            "[T]navigation"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "InEKFormer: A Hybrid State Estimator for Humanoid Robots",
      "authors": [
        "Lasse Hohmeyer",
        "Mihaela Popescu",
        "Ivan Bergonzani",
        "Dennis Mronga",
        "Frank Kirchner"
      ],
      "arxiv_id": "2511.16306v1",
      "summary": "Humanoid robots have great potential for a wide range of applications, including industrial and domestic use, healthcare, and search and rescue missions. However, bipedal locomotion in different environments is still a challenge when it comes to performing stable and dynamic movements. This is where state estimation plays a crucial role, providing fast and accurate feedback of the robot's floating base state to the motion controller. Although classical state estimation methods such as Kalman filters are widely used in robotics, they require expert knowledge to fine-tune the noise parameters. Due to recent advances in the field of machine learning, deep learning methods are increasingly used for state estimation tasks. In this work, we propose the InEKFormer, a novel hybrid state estimation method that incorporates an invariant extended Kalman filter (InEKF) and a Transformer network. We compare our method with the InEKF and the KalmanNet approaches on datasets obtained from the humanoid robot RH5. The results indicate the potential of Transformers in humanoid state estimation, but also highlight the need for robust autoregressive training in these high-dimensional problems.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "Accepted at The 22nd International Conference on Advanced Robotics (ICAR 2025)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16306v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot",
            "bipedal",
            "biped",
            "locomotion"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "HMC: Learning Heterogeneous Meta-Control for Contact-Rich Loco-Manipulation",
      "authors": [
        "Lai Wei",
        "Xuanbin Peng",
        "Ri-Zhao Qiu",
        "Tianshu Huang",
        "Xuxin Cheng",
        "Xiaolong Wang"
      ],
      "arxiv_id": "2511.14756v1",
      "summary": "Learning from real-world robot demonstrations holds promise for interacting with complex real-world environments. However, the complexity and variability of interaction dynamics often cause purely positional controllers to struggle with contacts or varying payloads. To address this, we propose a Heterogeneous Meta-Control (HMC) framework for Loco-Manipulation that adaptively stitches multiple control modalities: position, impedance, and hybrid force-position. We first introduce an interface, HMC-Controller, for blending actions from different control profiles continuously in the torque space. HMC-Controller facilitates both teleoperation and policy deployment. Then, to learn a robust force-aware policy, we propose HMC-Policy to unify different controllers into a heterogeneous architecture. We adopt a mixture-of-experts style routing to learn from large-scale position-only data and fine-grained force-aware demonstrations. Experiments on a real humanoid robot show over 50% relative improvement vs. baselines on challenging tasks such as compliant table wiping and drawer opening, demonstrating the efficacy of HMC.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14756v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid",
            "humanoid robot",
            "[T]manipulation",
            "[T]loco-manipulation",
            "teleoperation"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Design of an Adaptive Modular Anthropomorphic Dexterous Hand for Human-like Manipulation",
      "authors": [
        "Zelong Zhou",
        "Wenrui Chen",
        "Zeyun Hu",
        "Qiang Diao",
        "Qixin Gao",
        "Yaonan Wang"
      ],
      "arxiv_id": "2511.22100v1",
      "summary": "Biological synergies have emerged as a widely adopted paradigm for dexterous hand design, enabling human-like manipulation with a small number of actuators. Nonetheless, excessive coupling tends to diminish the dexterity of hands. This paper tackles the trade-off between actuation complexity and dexterity by proposing an anthropomorphic finger topology with 4 DoFs driven by 2 actuators, and by developing an adaptive, modular dexterous hand based on this finger topology. We explore the biological basis of hand synergies and human gesture analysis, translating joint-level coordination and structural attributes into a modular finger architecture. Leveraging these biomimetic mappings, we design a five-finger modular hand and establish its kinematic model to analyze adaptive grasping and in-hand manipulation. Finally, we construct a physical prototype and conduct preliminary experiments, which validate the effectiveness of the proposed design and analysis.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "7 pages, 8 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22100v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]dexterous hand",
            "in-hand manipulation",
            "grasping",
            "grasp"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "OVAL-Grasp: Open-Vocabulary Affordance Localization for Task Oriented Grasping",
      "authors": [
        "Edmond Tong",
        "Advaith Balaji",
        "Anthony Opipari",
        "Stanley Lewis",
        "Zhen Zeng",
        "Odest Chadwicke Jenkins"
      ],
      "arxiv_id": "2511.20841v1",
      "summary": "To manipulate objects in novel, unstructured environments, robots need task-oriented grasps that target object parts based on the given task. Geometry-based methods often struggle with visually defined parts, occlusions, and unseen objects. We introduce OVAL-Grasp, a zero-shot open-vocabulary approach to task-oriented, affordance based grasping that uses large-language models and vision-language models to allow a robot to grasp objects at the correct part according to a given task. Given an RGB image and a task, OVAL-Grasp identifies parts to grasp or avoid with an LLM, segments them with a VLM, and generates a 2D heatmap of actionable regions on the object. During our evaluations, we found that our method outperformed two task oriented grasping baselines on experiments with 20 household objects with 3 unique tasks for each. OVAL-Grasp successfully identifies and segments the correct object part 95% of the time and grasps the correct actionable area 78.3% of the time in real-world experiments with the Fetch mobile manipulator. Additionally, OVAL-Grasp finds correct object parts under partial occlusions, demonstrating a part selection success rate of 80% in cluttered scenes. We also demonstrate OVAL-Grasp's efficacy in scenarios that rely on visual features for part selection, and show the benefit of a modular design through our ablation experiments. Our project webpage is available at https://ekjt.github.io/OVAL-Grasp/",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "10 pages, 7 figures, 3 tables. Presented at the 2025 International Symposium on Experimental Robotics (ISER)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20841v1",
      "code_links": [
        {
          "url": "https://ekjt.github.io/OVAL-Grasp/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]grasping",
            "[T]grasp"
          ],
          "score": 12.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "A Virtual Mechanical Interaction Layer Enables Resilient Human-to-Robot Object Handovers",
      "authors": [
        "Omar Faris",
        "Sławomir Tadeja",
        "Fulvio Forni"
      ],
      "arxiv_id": "2511.19543v1",
      "summary": "Object handover is a common form of interaction that is widely present in collaborative tasks. However, achieving it efficiently remains a challenge. We address the problem of ensuring resilient robotic actions that can adapt to complex changes in object pose during human-to-robot object handovers. We propose the use of Virtual Model Control to create an interaction layer that controls the robot and adapts to the dynamic changes in the handover process. Additionally, we propose the use of augmented reality to facilitate bidirectional communication between humans and robots during handovers. We assess the performance of our controller in a set of experiments that demonstrate its resilience to various sources of uncertainties, including complex changes to the object's pose during the handover. Finally, we performed a user study with 16 participants to understand human preferences for different robot control profiles and augmented reality visuals in object handovers. Our results showed a general preference for the proposed approach and revealed insights that can guide further development in adapting the interaction with the user.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19543v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "[T]human-to-robot",
            "[T]human to robot"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "7_retargeting"
      ]
    },
    {
      "title": "Neural Texture Splatting: Expressive 3D Gaussian Splatting for View Synthesis, Geometry, and Dynamic Reconstruction",
      "authors": [
        "Yiming Wang",
        "Shaofei Wang",
        "Marko Mihajlovic",
        "Siyu Tang"
      ],
      "arxiv_id": "2511.18873v1",
      "summary": "3D Gaussian Splatting (3DGS) has emerged as a leading approach for high-quality novel view synthesis, with numerous variants extending its applicability to a broad spectrum of 3D and 4D scene reconstruction tasks. Despite its success, the representational capacity of 3DGS remains limited by the use of 3D Gaussian kernels to model local variations. Recent works have proposed to augment 3DGS with additional per-primitive capacity, such as per-splat textures, to enhance its expressiveness. However, these per-splat texture approaches primarily target dense novel view synthesis with a reduced number of Gaussian primitives, and their effectiveness tends to diminish when applied to more general reconstruction scenarios. In this paper, we aim to achieve concrete performance improvement over state-of-the-art 3DGS variants across a wide range of reconstruction tasks, including novel view synthesis, geometry and dynamic reconstruction, under both sparse and dense input settings. To this end, we introduce Neural Texture Splatting (NTS). At the core of our approach is a global neural field (represented as a hybrid of a tri-plane and a neural decoder) that predicts local appearance and geometric fields for each primitive. By leveraging this shared global representation that models local texture fields across primitives, we significantly reduce model size and facilitate efficient global information exchange, demonstrating strong generalization across tasks. Furthermore, our neural modeling of local texture fields introduces expressive view- and time-dependent effects, a critical aspect that existing methods fail to account for. Extensive experiments show that Neural Texture Splatting consistently improves models and achieves state-of-the-art results across multiple benchmarks.",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "SIGGRAPH Asia 2025 (conference track), Project page: https://19reborn.github.io/nts/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18873v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting",
            "novel view synthesis",
            "scene reconstruction"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Head Stabilization for Wheeled Bipedal Robots via Force-Estimation-Based Admittance Control",
      "authors": [
        "Tianyu Wang",
        "Chunxiang Yan",
        "Xuanhong Liao",
        "Tao Zhang",
        "Ping Wang",
        "Cong Wen",
        "Dingchuan Liu",
        "Haowen Yu",
        "Ximin Lyu"
      ],
      "arxiv_id": "2511.18712v1",
      "summary": "Wheeled bipedal robots are emerging as flexible platforms for field exploration. However, head instability induced by uneven terrain can degrade the accuracy of onboard sensors or damage fragile payloads. Existing research primarily focuses on stabilizing the mobile platform but overlooks active stabilization of the head in the world frame, resulting in vertical oscillations that undermine overall stability. To address this challenge, we developed a model-based ground force estimation method for our 6-degree-of-freedom wheeled bipedal robot. Leveraging these force estimates, we implemented an admittance control algorithm to enhance terrain adaptability. Simulation experiments validated the real-time performance of the force estimator and the robot's robustness when traversing uneven terrain.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18712v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]bipedal",
            "[T]bipedal robot",
            "[T]biped"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Observer Actor: Active Vision Imitation Learning with Sparse View Gaussian Splatting",
      "authors": [
        "Yilong Wang",
        "Cheng Qian",
        "Ruomeng Fan",
        "Edward Johns"
      ],
      "arxiv_id": "2511.18140v1",
      "summary": "We propose Observer Actor (ObAct), a novel framework for active vision imitation learning in which the observer moves to optimal visual observations for the actor. We study ObAct on a dual-arm robotic system equipped with wrist-mounted cameras. At test time, ObAct dynamically assigns observer and actor roles: the observer arm constructs a 3D Gaussian Splatting (3DGS) representation from three images, virtually explores this to find an optimal camera pose, then moves to this pose; the actor arm then executes a policy using the observer's observations. This formulation enhances the clarity and visibility of both the object and the gripper in the policy's observations. As a result, we enable the training of ambidextrous policies on observations that remain closer to the occlusion-free training distribution, leading to more robust policies. We study this formulation with two existing imitation learning methods -- trajectory transfer and behavior cloning -- and experiments show that ObAct significantly outperforms static-camera setups: trajectory transfer improves by 145% without occlusion and 233% with occlusion, while behavior cloning improves by 75% and 143%, respectively. Videos are available at https://obact.github.io.",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-22",
      "updated": "2025-11-22",
      "comment": "Videos are available on our project webpage at https://obact.github.io",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18140v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "dual-arm"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning",
            "behavior cloning"
          ],
          "score": 6.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Smol-GS: Compact Representations for Abstract 3D Gaussian Splatting",
      "authors": [
        "Haishan Wang",
        "Mohammad Hassan Vali",
        "Arno Solin"
      ],
      "arxiv_id": "2512.00850v1",
      "summary": "We present Smol-GS, a novel method for learning compact representations for 3D Gaussian Splatting (3DGS). Our approach learns highly efficient encodings in 3D space that integrate both spatial and semantic information. The model captures the coordinates of the splats through a recursive voxel hierarchy, while splat-wise features store abstracted cues, including color, opacity, transformation, and material properties. This design allows the model to compress 3D scenes by orders of magnitude without loss of flexibility. Smol-GS achieves state-of-the-art compression on standard benchmarks while maintaining high rendering quality. Beyond visual fidelity, the discrete representations could potentially serve as a foundation for downstream tasks such as navigation, planning, and broader 3D scene understanding.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00850v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting",
            "scene understanding",
            "navigation"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots",
      "authors": [
        "Yushi Wang",
        "Changsheng Luo",
        "Penghui Chen",
        "Jianran Liu",
        "Weijian Sun",
        "Tong Guo",
        "Kechang Yang",
        "Biao Hu",
        "Yangang Zhang",
        "Mingguo Zhao"
      ],
      "arxiv_id": "2511.03996v1",
      "summary": "Humanoid soccer poses a representative challenge for embodied intelligence, requiring robots to operate within a tightly coupled perception-action loop. However, existing systems typically rely on decoupled modules, resulting in delayed responses and incoherent behaviors in dynamic environments, while real-world perceptual limitations further exacerbate these issues. In this work, we present a unified reinforcement learning-based controller that enables humanoid robots to acquire reactive soccer skills through the direct integration of visual perception and motion control. Our approach extends Adversarial Motion Priors to perceptual settings in real-world dynamic environments, bridging motion imitation and visually grounded dynamic control. We introduce an encoder-decoder architecture combined with a virtual perception system that models real-world visual characteristics, allowing the policy to recover privileged states from imperfect observations and establish active coordination between perception and action. The resulting controller demonstrates strong reactivity, consistently executing coherent and robust soccer behaviors across various scenarios, including real RoboCup matches.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "Project page: https://humanoid-kick.github.io",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03996v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot"
          ],
          "score": 12.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "adversarial motion priors",
            "adversarial motion prior"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 17.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "8_physics_animation"
      ]
    },
    {
      "title": "Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations",
      "authors": [
        "Irmak Guzey",
        "Haozhi Qi",
        "Julen Urain",
        "Changhao Wang",
        "Jessica Yin",
        "Krishna Bodduluri",
        "Mike Lambeta",
        "Lerrel Pinto",
        "Akshara Rai",
        "Jitendra Malik",
        "Tingfan Wu",
        "Akash Sharma",
        "Homanga Bharadhwaj"
      ],
      "arxiv_id": "2511.16661v1",
      "summary": "Learning multi-fingered robot policies from humans performing daily tasks in natural environments has long been a grand goal in the robotics community. Achieving this would mark significant progress toward generalizable robot manipulation in human environments, as it would reduce the reliance on labor-intensive robot data collection. Despite substantial efforts, progress toward this goal has been bottle-necked by the embodiment gap between humans and robots, as well as by difficulties in extracting relevant contextual and motion cues that enable learning of autonomous policies from in-the-wild human videos. We claim that with simple yet sufficiently powerful hardware for obtaining human data and our proposed framework AINA, we are now one significant step closer to achieving this dream. AINA enables learning multi-fingered policies from data collected by anyone, anywhere, and in any environment using Aria Gen 2 glasses. These glasses are lightweight and portable, feature a high-resolution RGB camera, provide accurate on-board 3D head and hand poses, and offer a wide stereo view that can be leveraged for depth estimation of the scene. This setup enables the learning of 3D point-based policies for multi-fingered hands that are robust to background changes and can be deployed directly without requiring any robot data (including online corrections, reinforcement learning, or simulation). We compare our framework against prior human-to-robot policy learning approaches, ablate our design choices, and demonstrate results across nine everyday manipulation tasks. Robot rollouts are best viewed on our website: https://aina-robot.github.io.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16661v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "policy learning"
          ],
          "score": 3.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "human-to-robot",
            "human to robot"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 17.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam",
        "7_retargeting"
      ]
    },
    {
      "title": "SparseSurf: Sparse-View 3D Gaussian Splatting for Surface Reconstruction",
      "authors": [
        "Meiying Gu",
        "Jiawei Zhang",
        "Jiahe Li",
        "Xiaohan Yu",
        "Haonan Luo",
        "Jin Zheng",
        "Xiao Bai"
      ],
      "arxiv_id": "2511.14633v1",
      "summary": "Recent advances in optimizing Gaussian Splatting for scene geometry have enabled efficient reconstruction of detailed surfaces from images. However, when input views are sparse, such optimization is prone to overfitting, leading to suboptimal reconstruction quality. Existing approaches address this challenge by employing flattened Gaussian primitives to better fit surface geometry, combined with depth regularization to alleviate geometric ambiguities under limited viewpoints. Nevertheless, the increased anisotropy inherent in flattened Gaussians exacerbates overfitting in sparse-view scenarios, hindering accurate surface fitting and degrading novel view synthesis performance. In this paper, we propose \\net{}, a method that reconstructs more accurate and detailed surfaces while preserving high-quality novel view rendering. Our key insight is to introduce Stereo Geometry-Texture Alignment, which bridges rendering quality and geometry estimation, thereby jointly enhancing both surface reconstruction and view synthesis. In addition, we present a Pseudo-Feature Enhanced Geometry Consistency that enforces multi-view geometric consistency by incorporating both training and unseen views, effectively mitigating overfitting caused by sparse supervision. Extensive experiments on the DTU, BlendedMVS, and Mip-NeRF360 datasets demonstrate that our method achieves the state-of-the-art performance.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "Accepted at AAAI 2026. Project page: https://miya-oi.github.io/SparseSurf-project",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14633v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "[T]gaussian splatting",
            "novel view synthesis"
          ],
          "score": 14.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "geometric consistency"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 17.0,
      "hit_pillars": [
        "3_perception_slam",
        "7_retargeting"
      ]
    },
    {
      "title": "MA-SLAM: Active SLAM in Large-Scale Unknown Environment using Map Aware Deep Reinforcement Learning",
      "authors": [
        "Yizhen Yin",
        "Yuhua Qi",
        "Dapeng Feng",
        "Hongbo Chen",
        "Hongjun Ma",
        "Jin Wu",
        "Yi Jiang"
      ],
      "arxiv_id": "2511.14330v1",
      "summary": "Active Simultaneous Localization and Mapping (Active SLAM) involves the strategic planning and precise control of a robotic system's movement in order to construct a highly accurate and comprehensive representation of its surrounding environment, which has garnered significant attention within the research community. While the current methods demonstrate efficacy in small and controlled settings, they face challenges when applied to large-scale and diverse environments, marked by extended periods of exploration and suboptimal paths of discovery. In this paper, we propose MA-SLAM, a Map-Aware Active SLAM system based on Deep Reinforcement Learning (DRL), designed to address the challenge of efficient exploration in large-scale environments. In pursuit of this objective, we put forward a novel structured map representation. By discretizing the spatial data and integrating the boundary points and the historical trajectory, the structured map succinctly and effectively encapsulates the visited regions, thereby serving as input for the deep reinforcement learning based decision module. Instead of sequentially predicting the next action step within the decision module, we have implemented an advanced global planner to optimize the exploration path by leveraging long-range target points. We conducted experiments in three simulation environments and deployed in a real unmanned ground vehicle (UGV), the results demonstrate that our approach significantly reduces both the duration and distance of exploration compared with state-of-the-art methods.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14330v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "[T]deep reinforcement learning"
          ],
          "score": 9.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]SLAM",
            "localization"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 17.0,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Lightweight Learning from Actuation-Space Demonstrations via Flow Matching for Whole-Body Soft Robotic Grasping",
      "authors": [
        "Liudi Yang",
        "Yang Bai",
        "Yuhao Wang",
        "Ibrahim Alsarraj",
        "Gitta Kutyniok",
        "Zhanchi Wang",
        "Ke Wu"
      ],
      "arxiv_id": "2511.01770v1",
      "summary": "Robotic grasping under uncertainty remains a fundamental challenge due to its uncertain and contact-rich nature. Traditional rigid robotic hands, with limited degrees of freedom and compliance, rely on complex model-based and heavy feedback controllers to manage such interactions. Soft robots, by contrast, exhibit embodied mechanical intelligence: their underactuated structures and passive flexibility of their whole body, naturally accommodate uncertain contacts and enable adaptive behaviors. To harness this capability, we propose a lightweight actuation-space learning framework that infers distributional control representations for whole-body soft robotic grasping, directly from deterministic demonstrations using a flow matching model (Rectified Flow),without requiring dense sensing or heavy control loops. Using only 30 demonstrations (less than 8% of the reachable workspace), the learned policy achieves a 97.5% grasp success rate across the whole workspace, generalizes to grasped-object size variations of +-33%, and maintains stable performance when the robot's dynamic response is directly adjusted by scaling the execution time from 20% to 200%. These results demonstrate that actuation-space learning, by leveraging its passive redundant DOFs and flexibility, converts the body's mechanics into functional control intelligence and substantially reduces the burden on central controllers for this uncertain-rich task.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01770v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]grasping",
            "[T]grasp"
          ],
          "score": 12.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]flow matching"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 16.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Improving Robustness to Out-of-Distribution States in Imitation Learning via Deep Koopman-Boosted Diffusion Policy",
      "authors": [
        "Dianye Huang",
        "Nassir Navab",
        "Zhongliang Jiang"
      ],
      "arxiv_id": "2511.00555v1",
      "summary": "Integrating generative models with action chunking has shown significant promise in imitation learning for robotic manipulation. However, the existing diffusion-based paradigm often struggles to capture strong temporal dependencies across multiple steps, particularly when incorporating proprioceptive input. This limitation can lead to task failures, where the policy overfits to proprioceptive cues at the expense of capturing the visually derived features of the task. To overcome this challenge, we propose the Deep Koopman-boosted Dual-branch Diffusion Policy (D3P) algorithm. D3P introduces a dual-branch architecture to decouple the roles of different sensory modality combinations. The visual branch encodes the visual observations to indicate task progression, while the fused branch integrates both visual and proprioceptive inputs for precise manipulation. Within this architecture, when the robot fails to accomplish intermediate goals, such as grasping a drawer handle, the policy can dynamically switch to execute action chunks generated by the visual branch, allowing recovery to previously observed states and facilitating retrial of the task. To further enhance visual representation learning, we incorporate a Deep Koopman Operator module that captures structured temporal dynamics from visual inputs. During inference, we use the test-time loss of the generative model as a confidence signal to guide the aggregation of the temporally overlapping predicted action chunks, thereby enhancing the reliability of policy execution. In simulation experiments across six RLBench tabletop tasks, D3P outperforms the state-of-the-art diffusion policy by an average of 14.6\\%. On three real-world robotic manipulation tasks, it achieves a 15.0\\% improvement. Code: https://github.com/dianyeHuang/D3P.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-01",
      "updated": "2025-11-01",
      "comment": "Accepted by IEEE T-RO",
      "doi": "10.1109/TRO.2025.3629819",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00555v1",
      "code_links": [
        {
          "url": "https://github.com/dianyeHuang/D3P",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "grasping",
            "grasp"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning",
            "[T]diffusion policy",
            "representation learning"
          ],
          "score": 10.5
        }
      ],
      "relevance_score": 16.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting",
      "authors": [
        "Aymen Mir",
        "Jian Wang",
        "Riza Alp Guler",
        "Chuan Guo",
        "Gerard Pons-Moll",
        "Bing Zhou"
      ],
      "arxiv_id": "2511.09827v1",
      "summary": "We present a novel framework for animating humans in 3D scenes using 3D Gaussian Splatting (3DGS), a neural scene representation that has recently achieved state-of-the-art photorealistic results for novel-view synthesis but remains under-explored for human-scene animation and interaction. Unlike existing animation pipelines that use meshes or point clouds as the underlying 3D representation, our approach introduces the use of 3DGS as the 3D representation to the problem of animating humans in scenes. By representing humans and scenes as Gaussians, our approach allows for geometry-consistent free-viewpoint rendering of humans interacting with 3D scenes. Our key insight is that the rendering can be decoupled from the motion synthesis and each sub-problem can be addressed independently, without the need for paired human-scene data. Central to our method is a Gaussian-aligned motion module that synthesizes motion without explicit scene geometry, using opacity-based cues and projected Gaussian structures to guide human placement and pose alignment. To ensure natural interactions, we further propose a human-scene Gaussian refinement optimization that enforces realistic contact and navigation. We evaluate our approach on scenes from Scannet++ and the SuperSplat library, and on avatars reconstructed from sparse and dense multi-view human capture. Finally, we demonstrate that our framework allows for novel applications such as geometry-consistent free-viewpoint rendering of edited monocular RGB videos with new animated humans, showcasing the unique advantage of 3DGS for monocular video-based human animation.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09827v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting",
            "point cloud",
            "navigation"
          ],
          "score": 14.0
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "motion synthesis"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 16.5,
      "hit_pillars": [
        "3_perception_slam",
        "4_motion_diffusion"
      ]
    },
    {
      "title": "SAC-MoE: Reinforcement Learning with Mixture-of-Experts for Control of Hybrid Dynamical Systems with Uncertainty",
      "authors": [
        "Leroy D'Souza",
        "Akash Karthikeyan",
        "Yash Vardhan Pant",
        "Sebastian Fischmeister"
      ],
      "arxiv_id": "2511.12361v1",
      "summary": "Hybrid dynamical systems result from the interaction of continuous-variable dynamics with discrete events and encompass various systems such as legged robots, vehicles and aircrafts. Challenges arise when the system's modes are characterized by unobservable (latent) parameters and the events that cause system dynamics to switch between different modes are also unobservable. Model-based control approaches typically do not account for such uncertainty in the hybrid dynamics, while standard model-free RL methods fail to account for abrupt mode switches, leading to poor generalization.\n  To overcome this, we propose SAC-MoE which models the actor of the Soft Actor-Critic (SAC) framework as a Mixture-of-Experts (MoE) with a learned router that adaptively selects among learned experts. To further improve robustness, we develop a curriculum-based training algorithm to prioritize data collection in challenging settings, allowing better generalization to unseen modes and switching locations. Simulation studies in hybrid autonomous racing and legged locomotion tasks show that SAC-MoE outperforms baselines (up to 6x) in zero-shot generalization to unseen environments. Our curriculum strategy consistently improves performance across all evaluated policies. Qualitative analysis shows that the interpretable MoE router activates different experts for distinct latent modes.",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-15",
      "updated": "2025-11-15",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12361v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "legged robot",
            "legged locomotion",
            "locomotion"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "[T]SAC",
            "actor-critic"
          ],
          "score": 10.5
        }
      ],
      "relevance_score": 16.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "CLM: Removing the GPU Memory Barrier for 3D Gaussian Splatting",
      "authors": [
        "Hexu Zhao",
        "Xiwen Min",
        "Xiaoteng Liu",
        "Moonjun Gong",
        "Yiming Li",
        "Ang Li",
        "Saining Xie",
        "Jinyang Li",
        "Aurojit Panda"
      ],
      "arxiv_id": "2511.04951v1",
      "summary": "3D Gaussian Splatting (3DGS) is an increasingly popular novel view synthesis approach due to its fast rendering time, and high-quality output. However, scaling 3DGS to large (or intricate) scenes is challenging due to its large memory requirement, which exceed most GPU's memory capacity. In this paper, we describe CLM, a system that allows 3DGS to render large scenes using a single consumer-grade GPU, e.g., RTX4090. It does so by offloading Gaussians to CPU memory, and loading them into GPU memory only when necessary. To reduce performance and communication overheads, CLM uses a novel offloading strategy that exploits observations about 3DGS's memory access pattern for pipelining, and thus overlap GPU-to-CPU communication, GPU computation and CPU computation. Furthermore, we also exploit observation about the access pattern to reduce communication volume. Our evaluation shows that the resulting implementation can render a large scene that requires 100 million Gaussians on a single RTX4090 and achieve state-of-the-art reconstruction quality.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-07",
      "updated": "2025-11-07",
      "comment": "Accepted to appear in the 2026 ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04951v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting",
            "novel view synthesis"
          ],
          "score": 16.0
        }
      ],
      "relevance_score": 16.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Integrating Ergonomics and Manipulability for Upper Limb Postural Optimization in Bimanual Human-Robot Collaboration",
      "authors": [
        "Chenzui Li",
        "Yiming Chen",
        "Xi Wu",
        "Giacinto Barresi",
        "Fei Chen"
      ],
      "arxiv_id": "2511.04009v1",
      "summary": "This paper introduces an upper limb postural optimization method for enhancing physical ergonomics and force manipulability during bimanual human-robot co-carrying tasks. Existing research typically emphasizes human safety or manipulative efficiency, whereas our proposed method uniquely integrates both aspects to strengthen collaboration across diverse conditions (e.g., different grasping postures of humans, and different shapes of objects). Specifically, the joint angles of a simplified human skeleton model are optimized by minimizing the cost function to prioritize safety and manipulative capability. To guide humans towards the optimized posture, the reference end-effector poses of the robot are generated through a transformation module. A bimanual model predictive impedance controller (MPIC) is proposed for our human-like robot, CURI, to recalibrate the end effector poses through planned trajectories. The proposed method has been validated through various subjects and objects during human-human collaboration (HHC) and human-robot collaboration (HRC). The experimental results demonstrate significant improvement in muscle conditions by comparing the activation of target muscles before and after optimization.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "7 pages, 7 figures, IROS 2025 accepted",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04009v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "grasping",
            "grasp",
            "[T]bi-manual",
            "[T]bimanual"
          ],
          "score": 16.0
        }
      ],
      "relevance_score": 16.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "DentalSplat: Dental Occlusion Novel View Synthesis from Sparse Intra-Oral Photographs",
      "authors": [
        "Yiyi Miao",
        "Taoyu Wu",
        "Tong Chen",
        "Sihao Li",
        "Ji Jiang",
        "Youpeng Yang",
        "Angelos Stefanidis",
        "Limin Yu",
        "Jionglong Su"
      ],
      "arxiv_id": "2511.03099v1",
      "summary": "In orthodontic treatment, particularly within telemedicine contexts, observing patients' dental occlusion from multiple viewpoints facilitates timely clinical decision-making. Recent advances in 3D Gaussian Splatting (3DGS) have shown strong potential in 3D reconstruction and novel view synthesis. However, conventional 3DGS pipelines typically rely on densely captured multi-view inputs and precisely initialized camera poses, limiting their practicality. Orthodontic cases, in contrast, often comprise only three sparse images, specifically, the anterior view and bilateral buccal views, rendering the reconstruction task especially challenging. The extreme sparsity of input views severely degrades reconstruction quality, while the absence of camera pose information further complicates the process. To overcome these limitations, we propose DentalSplat, an effective framework for 3D reconstruction from sparse orthodontic imagery. Our method leverages a prior-guided dense stereo reconstruction model to initialize the point cloud, followed by a scale-adaptive pruning strategy to improve the training efficiency and reconstruction quality of 3DGS. In scenarios with extremely sparse viewpoints, we further incorporate optical flow as a geometric constraint, coupled with gradient regularization, to enhance rendering fidelity. We validate our approach on a large-scale dataset comprising 950 clinical cases and an additional video-based test set of 195 cases designed to simulate real-world remote orthodontic imaging conditions. Experimental results demonstrate that our method effectively handles sparse input scenarios and achieves superior novel view synthesis quality for dental occlusion visualization, outperforming state-of-the-art techniques.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-05",
      "updated": "2025-11-05",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03099v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "3DGS",
            "gaussian splatting",
            "[T]novel view synthesis",
            "point cloud",
            "optical flow"
          ],
          "score": 16.0
        }
      ],
      "relevance_score": 16.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "WorldPlanner: Monte Carlo Tree Search and MPC with Action-Conditioned Visual World Models",
      "authors": [
        "R. Khorrambakht",
        "Joaquim Ortiz-Haro",
        "Joseph Amigo",
        "Omar Mostafa",
        "Daniel Dugas",
        "Franziska Meier",
        "Ludovic Righetti"
      ],
      "arxiv_id": "2511.03077v1",
      "summary": "Robots must understand their environment from raw sensory inputs and reason about the consequences of their actions in it to solve complex tasks. Behavior Cloning (BC) leverages task-specific human demonstrations to learn this knowledge as end-to-end policies. However, these policies are difficult to transfer to new tasks, and generating training data is challenging because it requires careful demonstrations and frequent environment resets. In contrast to such policy-based view, in this paper we take a model-based approach where we collect a few hours of unstructured easy-to-collect play data to learn an action-conditioned visual world model, a diffusion-based action sampler, and optionally a reward model. The world model -- in combination with the action sampler and a reward model -- is then used to optimize long sequences of actions with a Monte Carlo Tree Search (MCTS) planner. The resulting plans are executed on the robot via a zeroth-order Model Predictive Controller (MPC). We show that the action sampler mitigates hallucinations of the world model during planning and validate our approach on 3 real-world robotic tasks with varying levels of planning and modeling complexity. Our experiments support the hypothesis that planning leads to a significant improvement over BC baselines on a standard manipulation test environment.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-04",
      "updated": "2025-11-04",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03077v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "[T]MPC",
            "model predictive control"
          ],
          "score": 10.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "behavior cloning",
            "[T]world model"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 16.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Embodiment Transfer Learning for Vision-Language-Action Models",
      "authors": [
        "Chengmeng Li",
        "Yaxin Peng"
      ],
      "arxiv_id": "2511.01224v1",
      "summary": "Vision-language-action (VLA) models have significantly advanced robotic learning, enabling training on large-scale, cross-embodiment data and fine-tuning for specific robots. However, state-of-the-art autoregressive VLAs struggle with multi-robot collaboration. We introduce embodiment transfer learning, denoted as ET-VLA, a novel framework for efficient and effective transfer of pre-trained VLAs to multi-robot. ET-VLA's core is Synthetic Continued Pretraining (SCP), which uses synthetically generated data to warm up the model for the new embodiment, bypassing the need for real human demonstrations and reducing data collection costs. SCP enables the model to learn correct actions and precise action token numbers. Following SCP, the model is fine-tuned on target embodiment data. To further enhance the model performance on multi-embodiment, we present the Embodied Graph-of-Thought technique, a novel approach that formulates each sub-task as a node, that allows the VLA model to distinguish the functionalities and roles of each embodiment during task execution. Our work considers bimanual robots, a simple version of multi-robot to verify our approaches. We validate the effectiveness of our method on both simulation benchmarks and real robots covering three different bimanual embodiments. In particular, our proposed ET-VLA \\space can outperform OpenVLA on six real-world tasks over 53.2%. We will open-source all codes to support the community in advancing VLA models for robot learning.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01224v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "bi-manual",
            "bimanual"
          ],
          "score": 4.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "cross-embodiment",
            "[T]embodiment transfer"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 16.0,
      "hit_pillars": [
        "1_robot_core",
        "7_retargeting"
      ]
    },
    {
      "title": "LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping",
      "authors": [
        "Lijie Wang",
        "Lianjie Guo",
        "Ziyi Xu",
        "Qianhao Wang",
        "Fei Gao",
        "Xieyuanli Chen"
      ],
      "arxiv_id": "2511.01186v1",
      "summary": "Reconstructing large-scale colored point clouds is an important task in robotics, supporting perception, navigation, and scene understanding. Despite advances in LiDAR inertial visual odometry (LIVO), its performance remains highly sensitive to extrinsic calibration. Meanwhile, 3D vision foundation models, such as VGGT, suffer from limited scalability in large environments and inherently lack metric scale. To overcome these limitations, we propose LiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry with the state-of-the-art VGGT model through a two-stage coarse- to-fine fusion pipeline: First, a pre-fusion module with robust initialization refinement efficiently estimates VGGT poses and point clouds with coarse metric scale within each session. Then, a post-fusion module enhances cross-modal 3D similarity transformation, using bounding-box-based regularization to reduce scale distortions caused by inconsistent FOVs between LiDAR and camera sensors. Extensive experiments across multiple datasets demonstrate that LiDAR-VGGT achieves dense, globally consistent colored point clouds and outperforms both VGGT-based methods and LIVO baselines. The implementation of our proposed novel color point cloud evaluation toolkit will be released as open source.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01186v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "visual odometry",
            "lidar-inertial",
            "scene understanding",
            "point cloud",
            "navigation",
            "[T]VGGT"
          ],
          "score": 16.0
        }
      ],
      "relevance_score": 16.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control",
      "authors": [
        "Zhengyi Luo",
        "Ye Yuan",
        "Tingwu Wang",
        "Chenran Li",
        "Sirui Chen",
        "Fernando Castañeda",
        "Zi-Ang Cao",
        "Jiefeng Li",
        "David Minor",
        "Qingwei Ben",
        "Xingye Da",
        "Runyu Ding",
        "Cyrus Hogg",
        "Lina Song",
        "Edy Lim",
        "Eugene Jeong",
        "Tairan He",
        "Haoru Xue",
        "Wenli Xiao",
        "Zi Wang",
        "Simon Yuen",
        "Jan Kautz",
        "Yan Chang",
        "Umar Iqbal",
        "Linxi \"Jim\" Fan",
        "Yuke Zhu"
      ],
      "arxiv_id": "2511.07820v2",
      "summary": "Despite the rise of billion-parameter foundation models trained across thousands of GPUs, similar scaling gains have not been shown for humanoid control. Current neural controllers for humanoids remain modest in size, target a limited set of behaviors, and are trained on a handful of GPUs over several days. We show that scaling up model capacity, data, and compute yields a generalist humanoid controller capable of creating natural and robust whole-body movements. Specifically, we posit motion tracking as a natural and scalable task for humanoid control, leveraging dense supervision from diverse motion-capture data to acquire human motion priors without manual reward engineering. We build a foundation model for motion tracking by scaling along three axes: network size (from 1.2M to 42M parameters), dataset volume (over 100M frames, 700 hours of high-quality motion data), and compute (9k GPU hours). Beyond demonstrating the benefits of scale, we show the practical utility of our model through two mechanisms: (1) a real-time universal kinematic planner that bridges motion tracking to downstream task execution, enabling natural and interactive control, and (2) a unified token space that supports various motion input interfaces, such as VR teleoperation devices, human videos, and vision-language-action (VLA) models, all using the same policy. Scaling motion tracking exhibits favorable properties: performance improves steadily with increased compute and data diversity, and learned representations generalize to unseen motions, establishing motion tracking at scale as a practical foundation for humanoid control.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.GR",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-11",
      "updated": "2025-12-04",
      "comment": "Project page: https://nvlabs.github.io/SONIC/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07820v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid control",
            "[T]whole-body control",
            "teleoperation"
          ],
          "score": 16.0
        }
      ],
      "relevance_score": 16.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Robust and High-Fidelity 3D Gaussian Splatting: Fusing Pose Priors and Geometry Constraints for Texture-Deficient Outdoor Scenes",
      "authors": [
        "Meijun Guo",
        "Yongliang Shi",
        "Caiyun Liu",
        "Yixiao Feng",
        "Ming Ma",
        "Tinghai Yan",
        "Weining Lu",
        "Bin Liang"
      ],
      "arxiv_id": "2511.06765v1",
      "summary": "3D Gaussian Splatting (3DGS) has emerged as a key rendering pipeline for digital asset creation due to its balance between efficiency and visual quality. To address the issues of unstable pose estimation and scene representation distortion caused by geometric texture inconsistency in large outdoor scenes with weak or repetitive textures, we approach the problem from two aspects: pose estimation and scene representation. For pose estimation, we leverage LiDAR-IMU Odometry to provide prior poses for cameras in large-scale environments. These prior pose constraints are incorporated into COLMAP's triangulation process, with pose optimization performed via bundle adjustment. Ensuring consistency between pixel data association and prior poses helps maintain both robustness and accuracy. For scene representation, we introduce normal vector constraints and effective rank regularization to enforce consistency in the direction and shape of Gaussian primitives. These constraints are jointly optimized with the existing photometric loss to enhance the map quality. We evaluate our approach using both public and self-collected datasets. In terms of pose optimization, our method requires only one-third of the time while maintaining accuracy and robustness across both datasets. In terms of scene representation, the results show that our method significantly outperforms conventional 3DGS pipelines. Notably, on self-collected datasets characterized by weak or repetitive textures, our approach demonstrates enhanced visualization capabilities and achieves superior overall performance. Codes and data will be publicly available at https://github.com/justinyeah/normal_shape.git.",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "7 pages, 3 figures. Accepted by IROS 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06765v1",
      "code_links": [
        {
          "url": "https://github.com/justinyeah/normal_shape.git",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting",
            "pose estimation"
          ],
          "score": 16.0
        }
      ],
      "relevance_score": 16.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Building temporally coherent 3D maps with VGGT for memory-efficient Semantic SLAM",
      "authors": [
        "Gergely Dinya",
        "Péter Halász",
        "András Lőrincz",
        "Kristóf Karacs",
        "Anna Gelencsér-Horváth"
      ],
      "arxiv_id": "2511.16282v2",
      "summary": "We present a fast, spatio-temporal scene understanding framework based on Visual Geometry Grounded Transformer (VGGT). The proposed pipeline is designed to enable efficient, close to real-time performance, supporting applications including assistive navigation. To achieve continuous updates of the 3D scene representation, we process the image flow with a sliding window, aligning submaps, thereby overcoming VGGT's high memory demands. We exploit the VGGT tracking head to aggregate 2D semantic instance masks into 3D objects. To allow for temporal consistency and richer contextual reasoning the system stores timestamps and instance-level identities, thereby enabling the detection of changes in the environment. We evaluate the approach on well-known benchmarks and custom datasets specifically designed for assistive navigation scenarios. The results demonstrate the applicability of the framework to real-world scenarios.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-27",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16282v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]SLAM",
            "scene understanding",
            "navigation",
            "[T]VGGT"
          ],
          "score": 16.0
        }
      ],
      "relevance_score": 16.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Gaussian Blending: Rethinking Alpha Blending in 3D Gaussian Splatting",
      "authors": [
        "Junseo Koo",
        "Jinseo Jeong",
        "Gunhee Kim"
      ],
      "arxiv_id": "2511.15102v1",
      "summary": "The recent introduction of 3D Gaussian Splatting (3DGS) has significantly advanced novel view synthesis. Several studies have further improved the rendering quality of 3DGS, yet they still exhibit noticeable visual discrepancies when synthesizing views at sampling rates unseen during training. Specifically, they suffer from (i) erosion-induced blurring artifacts when zooming in and (ii) dilation-induced staircase artifacts when zooming out. We speculate that these artifacts arise from the fundamental limitation of the alpha blending adopted in 3DGS methods. Instead of the conventional alpha blending that computes alpha and transmittance as scalar quantities over a pixel, we propose to replace it with our novel Gaussian Blending that treats alpha and transmittance as spatially varying distributions. Thus, transmittances can be updated considering the spatial distribution of alpha values across the pixel area, allowing nearby background splats to contribute to the final rendering. Our Gaussian Blending maintains real-time rendering speed and requires no additional memory cost, while being easily integrated as a drop-in replacement into existing 3DGS-based or other NVS frameworks. Extensive experiments demonstrate that Gaussian Blending effectively captures fine details at various sampling rates unseen during training, consistently outperforming existing novel view synthesis models across both unseen and seen sampling rates.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15102v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting",
            "novel view synthesis"
          ],
          "score": 16.0
        }
      ],
      "relevance_score": 16.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Opt3DGS: Optimizing 3D Gaussian Splatting with Adaptive Exploration and Curvature-Aware Exploitation",
      "authors": [
        "Ziyang Huang",
        "Jiagang Chen",
        "Jin Liu",
        "Shunping Ji"
      ],
      "arxiv_id": "2511.13571v1",
      "summary": "3D Gaussian Splatting (3DGS) has emerged as a leading framework for novel view synthesis, yet its core optimization challenges remain underexplored. We identify two key issues in 3DGS optimization: entrapment in suboptimal local optima and insufficient convergence quality. To address these, we propose Opt3DGS, a robust framework that enhances 3DGS through a two-stage optimization process of adaptive exploration and curvature-guided exploitation. In the exploration phase, an Adaptive Weighted Stochastic Gradient Langevin Dynamics (SGLD) method enhances global search to escape local optima. In the exploitation phase, a Local Quasi-Newton Direction-guided Adam optimizer leverages curvature information for precise and efficient convergence. Extensive experiments on diverse benchmark datasets demonstrate that Opt3DGS achieves state-of-the-art rendering quality by refining the 3DGS optimization process without modifying its underlying representation.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "Accepted at AAAI 2026 as a Conference Paper",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13571v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting",
            "novel view synthesis"
          ],
          "score": 16.0
        }
      ],
      "relevance_score": 16.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting",
      "authors": [
        "Zihan Li",
        "Tengfei Wang",
        "Wentian Gan",
        "Hao Zhan",
        "Xin Wang",
        "Zongqian Zhan"
      ],
      "arxiv_id": "2511.13278v2",
      "summary": "Lightweight building surface models are crucial for digital city, navigation, and fast geospatial analytics, yet conventional multi-view geometry pipelines remain cumbersome and quality-sensitive due to their reliance on dense reconstruction, meshing, and subsequent simplification. This work presents SF-Recon, a method that directly reconstructs lightweight building surfaces from multi-view images without post-hoc mesh simplification. We first train an initial 3D Gaussian Splatting (3DGS) field to obtain a view-consistent representation. Building structure is then distilled by a normal-gradient-guided Gaussian optimization that selects primitives aligned with roof and wall boundaries, followed by multi-view edge-consistency pruning to enhance structural sharpness and suppress non-structural artifacts without external supervision. Finally, a multi-view depth-constrained Delaunay triangulation converts the structured Gaussian field into a lightweight, structurally faithful building mesh. Based on a proposed SF dataset, the experimental results demonstrate that our SF-Recon can directly reconstruct lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency. Website:https://lzh282140127-cell.github.io/SF-Recon-project/",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-21",
      "comment": "This paper has been submitted to the 2026 ISPRS Congress",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13278v2",
      "code_links": [
        {
          "url": "https://lzh282140127-cell.github.io/SF-Recon-project/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting",
            "navigation"
          ],
          "score": 16.0
        }
      ],
      "relevance_score": 16.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "SymGS : Leveraging Local Symmetries for 3D Gaussian Splatting Compression",
      "authors": [
        "Keshav Gupta",
        "Akshat Sanghvi",
        "Shreyas Reddy Palley",
        "Astitva Srivastava",
        "Charu Sharma",
        "Avinash Sharma"
      ],
      "arxiv_id": "2511.13264v2",
      "summary": "3D Gaussian Splatting has emerged as a transformative technique in novel view synthesis, primarily due to its high rendering speed and photorealistic fidelity. However, its memory footprint scales rapidly with scene complexity, often reaching several gigabytes. Existing methods address this issue by introducing compression strategies that exploit primitive-level redundancy through similarity detection and quantization. We aim to surpass the compression limits of such methods by incorporating symmetry-aware techniques, specifically targeting mirror symmetries to eliminate redundant primitives. We propose a novel compression framework, SymGS, introducing learnable mirrors into the scene, thereby eliminating local and global reflective redundancies for compression. Our framework functions as a plug-and-play enhancement to state-of-the-art compression methods, (e.g. HAC) to achieve further compression. Compared to HAC, we achieve $1.66 \\times$ compression across benchmark datasets (upto $3\\times$ on large-scale scenes). On an average, SymGS enables $\\bf{108\\times}$ compression of a 3DGS scene, while preserving rendering quality. The project page and supplementary can be found at symgs.github.io",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-19",
      "comment": "Project Page: https://symgs.github.io/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13264v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting",
            "novel view synthesis"
          ],
          "score": 16.0
        }
      ],
      "relevance_score": 16.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "IE-SRGS: An Internal-External Knowledge Fusion Framework for High-Fidelity 3D Gaussian Splatting Super-Resolution",
      "authors": [
        "Xiang Feng",
        "Tieshi Zhong",
        "Shuo Chang",
        "Weiliu Wang",
        "Chengkai Wang",
        "Yifei Chen",
        "Yuhe Wang",
        "Zhenzhong Kuang",
        "Xuefei Yin",
        "Yanming Zhu"
      ],
      "arxiv_id": "2511.22233v1",
      "summary": "Reconstructing high-resolution (HR) 3D Gaussian Splatting (3DGS) models from low-resolution (LR) inputs remains challenging due to the lack of fine-grained textures and geometry. Existing methods typically rely on pre-trained 2D super-resolution (2DSR) models to enhance textures, but suffer from 3D Gaussian ambiguity arising from cross-view inconsistencies and domain gaps inherent in 2DSR models. We propose IE-SRGS, a novel 3DGS SR paradigm that addresses this issue by jointly leveraging the complementary strengths of external 2DSR priors and internal 3DGS features. Specifically, we use 2DSR and depth estimation models to generate HR images and depth maps as external knowledge, and employ multi-scale 3DGS models to produce cross-view consistent, domain-adaptive counterparts as internal knowledge. A mask-guided fusion strategy is introduced to integrate these two sources and synergistically exploit their complementary strengths, effectively guiding the 3D Gaussian optimization toward high-fidelity reconstruction. Extensive experiments on both synthetic and real-world benchmarks show that IE-SRGS consistently outperforms state-of-the-art methods in both quantitative accuracy and visual fidelity.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22233v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation",
            "[T]3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting"
          ],
          "score": 16.0
        }
      ],
      "relevance_score": 16.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "SP-VINS: A Hybrid Stereo Visual Inertial Navigation System based on Implicit Environmental Map",
      "authors": [
        "Xueyu Du",
        "Lilian Zhang",
        "Fuan Duan",
        "Xincan Luo",
        "Maosong Wang",
        "Wenqi Wu",
        "JunMao"
      ],
      "arxiv_id": "2511.18756v1",
      "summary": "Filter-based visual inertial navigation system (VINS) has attracted mobile-robot researchers for the good balance between accuracy and efficiency, but its limited mapping quality hampers long-term high-accuracy state estimation. To this end, we first propose a novel filter-based stereo VINS, differing from traditional simultaneous localization and mapping (SLAM) systems based on 3D map, which performs efficient loop closure constraints with implicit environmental map composed of keyframes and 2D keypoints. Secondly, we proposed a hybrid residual filter framework that combines landmark reprojection and ray constraints to construct a unified Jacobian matrix for measurement updates. Finally, considering the degraded environment, we incorporated the camera-IMU extrinsic parameters into visual description to achieve online calibration. Benchmark experiments demonstrate that the proposed SP-VINS achieves high computational efficiency while maintaining long-term high-accuracy localization performance, and is superior to existing state-of-the-art (SOTA) methods.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18756v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "SLAM",
            "[T]visual-inertial",
            "localization",
            "[T]navigation"
          ],
          "score": 16.0
        }
      ],
      "relevance_score": 16.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization",
      "authors": [
        "Youngsik Yun",
        "Dongjun Gu",
        "Youngjung Uh"
      ],
      "arxiv_id": "2511.17918v1",
      "summary": "Despite 3D Gaussian Splatting (3DGS) excelling in most configurations, it lacks generalization across novel viewpoints in a few-shot scenario because it overfits to the sparse observations. We revisit 3DGS optimization from a machine learning perspective, framing novel view synthesis as a generalization problem to unseen viewpoints-an underexplored direction. We propose Frequency-Adaptive Sharpness Regularization (FASR), which reformulates the 3DGS training objective, thereby guiding 3DGS to converge toward a better generalization solution. Although Sharpness-Aware Minimization (SAM) similarly reduces the sharpness of the loss landscape to improve generalization of classification models, directly employing it to 3DGS is suboptimal due to the discrepancy between the tasks. Specifically, it hinders reconstructing high-frequency details due to excessive regularization, while reducing its strength leads to under-penalizing sharpness. To address this, we reflect the local frequency of images to set the regularization weight and the neighborhood radius when estimating the local sharpness. It prevents floater artifacts in novel viewpoints and reconstructs fine details that SAM tends to oversmooth. Across datasets with various configurations, our method consistently improves a wide range of baselines. Code will be available at https://bbangsik13.github.io/FASR.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-22",
      "updated": "2025-11-22",
      "comment": "Project page: https://bbangsik13.github.io/FASR",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17918v1",
      "code_links": [
        {
          "url": "https://bbangsik13.github.io/FASR",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting",
            "novel view synthesis"
          ],
          "score": 16.0
        }
      ],
      "relevance_score": 16.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Constant-Time Motion Planning with Manipulation Behaviors",
      "authors": [
        "Nayesha Gandotra",
        "Itamar Mishani",
        "Maxim Likhachev"
      ],
      "arxiv_id": "2512.00939v1",
      "summary": "Recent progress in contact-rich robotic manipulation has been striking, yet most deployed systems remain confined to simple, scripted routines. One of the key barriers is the lack of motion planning algorithms that can provide verifiable guarantees for safety, efficiency and reliability. To address this, a family of algorithms called Constant-Time Motion Planning (CTMP) was introduced, which leverages a preprocessing phase to enable collision-free motion queries in a fixed, user-specified time budget (e.g., 10 milliseconds). However, existing CTMP methods do not explicitly incorporate the manipulation behaviors essential for object handling. To bridge this gap, we introduce the \\textit{Behavioral Constant-Time Motion Planner} (B-CTMP), an algorithm that extends CTMP to solve a broad class of two-step manipulation tasks: (1) a collision-free motion to a behavior initiation state, followed by (2) execution of a manipulation behavior (such as grasping or insertion) to reach the goal. By precomputing compact data structures, B-CTMP guarantees constant-time query in mere milliseconds while ensuring completeness and successful task execution over a specified set of states. We evaluate B-CTMP on two canonical manipulation tasks in simulation, shelf picking and plug insertion,and demonstrate its effectiveness on a real robot. Our results show that B-CTMP unifies collision-free planning and object manipulation within a single constant-time framework, providing provable guarantees of speed and success for manipulation in semi-structured environments.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "In submission",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00939v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "grasping",
            "grasp",
            "[T]motion planning"
          ],
          "score": 16.0
        }
      ],
      "relevance_score": 16.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction",
      "authors": [
        "Qingzhou Lu",
        "Yao Feng",
        "Baiyu Shi",
        "Michael Piseno",
        "Zhenan Bao",
        "C. Karen Liu"
      ],
      "arxiv_id": "2511.04679v1",
      "summary": "Humanoid robots are expected to operate in human-centered environments where safe and natural physical interaction is essential. However, most recent reinforcement learning (RL) policies emphasize rigid tracking and suppress external forces. Existing impedance-augmented approaches are typically restricted to base or end-effector control and focus on resisting extreme forces rather than enabling compliance. We introduce GentleHumanoid, a framework that integrates impedance control into a whole-body motion tracking policy to achieve upper-body compliance. At its core is a unified spring-based formulation that models both resistive contacts (restoring forces when pressing against surfaces) and guiding contacts (pushes or pulls sampled from human motion data). This formulation ensures kinematically consistent forces across the shoulder, elbow, and wrist, while exposing the policy to diverse interaction scenarios. Safety is further supported through task-adjustable force thresholds. We evaluate our approach in both simulation and on the Unitree G1 humanoid across tasks requiring different levels of compliance, including gentle hugging, sit-to-stand assistance, and safe object manipulation. Compared to baselines, our policy consistently reduces peak contact forces while maintaining task success, resulting in smoother and more natural interactions. These results highlight a step toward humanoid robots that can safely and effectively collaborate with humans and handle objects in real-world environments.",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "Home page: https://gentle-humanoid.axell.top",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04679v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "whole-body motion",
            "manipulation",
            "unitree"
          ],
          "score": 14.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 15.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Depth Anything 3: Recovering the Visual Space from Any Views",
      "authors": [
        "Haotong Lin",
        "Sili Chen",
        "Junhao Liew",
        "Donny Y. Chen",
        "Zhenyu Li",
        "Guang Shi",
        "Jiashi Feng",
        "Bingyi Kang"
      ],
      "arxiv_id": "2511.10647v1",
      "summary": "We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "https://depth-anything-3.github.io/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10647v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "teacher-student"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation",
            "monocular depth",
            "pose estimation",
            "[T]Depth Anything",
            "VGGT"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 15.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "HDCNet: A Hybrid Depth Completion Network for Grasping Transparent and Reflective Objects",
      "authors": [
        "Guanghu Xie",
        "Mingxu Li",
        "Songwei Wu",
        "Yang Liu",
        "Zongwu Xie",
        "Baoshi Cao",
        "Hong Liu"
      ],
      "arxiv_id": "2511.07081v1",
      "summary": "Depth perception of transparent and reflective objects has long been a critical challenge in robotic manipulation.Conventional depth sensors often fail to provide reliable measurements on such surfaces, limiting the performance of robots in perception and grasping tasks. To address this issue, we propose a novel depth completion network,HDCNet,which integrates the complementary strengths of Transformer,CNN and Mamba architectures.Specifically,the encoder is designed as a dual-branch Transformer-CNN framework to extract modality-specific features. At the shallow layers of the encoder, we introduce a lightweight multimodal fusion module to effectively integrate low-level features. At the network bottleneck,a Transformer-Mamba hybrid fusion module is developed to achieve deep integration of high-level semantic and global contextual information, significantly enhancing depth completion accuracy and robustness. Extensive evaluations on multiple public datasets demonstrate that HDCNet achieves state-of-the-art(SOTA) performance in depth completion tasks.Furthermore,robotic grasping experiments show that HDCNet substantially improves grasp success rates for transparent and reflective objects,achieving up to a 60% increase.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07081v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "[T]grasping",
            "[T]grasp"
          ],
          "score": 14.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "Mamba"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 15.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "EOGS++: Earth Observation Gaussian Splatting with Internal Camera Refinement and Direct Panchromatic Rendering",
      "authors": [
        "Pierrick Bournez",
        "Luca Savant Aira",
        "Thibaud Ehret",
        "Gabriele Facciolo"
      ],
      "arxiv_id": "2511.16542v1",
      "summary": "Recently, 3D Gaussian Splatting has been introduced as a compelling alternative to NeRF for Earth observation, offering com- petitive reconstruction quality with significantly reduced training times. In this work, we extend the Earth Observation Gaussian Splatting (EOGS) framework to propose EOGS++, a novel method tailored for satellite imagery that directly operates on raw high-resolution panchromatic data without requiring external preprocessing. Furthermore, leveraging optical flow techniques we embed bundle adjustment directly within the training process, avoiding reliance on external optimization tools while improving camera pose estimation. We also introduce several improvements to the original implementation, including early stopping and TSDF post-processing, all contributing to sharper reconstructions and better geometric accuracy. Experiments on the IARPA 2016 and DFC2019 datasets demonstrate that EOGS++ achieves state-of-the-art performance in terms of reconstruction quality and effi- ciency, outperforming the original EOGS method and other NeRF-based methods while maintaining the computational advantages of Gaussian Splatting. Our model demonstrates an improvement from 1.33 to 1.19 mean MAE errors on buildings compared to the original EOGS models",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "8 pages, ISPRS",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16542v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "MAE"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "[T]gaussian splatting",
            "NeRF",
            "optical flow",
            "pose estimation"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 15.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos",
      "authors": [
        "Seungjae Lee",
        "Yoonkyo Jung",
        "Inkook Chun",
        "Yao-Chih Lee",
        "Zikui Cai",
        "Hongjia Huang",
        "Aayush Talreja",
        "Tan Dat Dao",
        "Yongyuan Liang",
        "Jia-Bin Huang",
        "Furong Huang"
      ],
      "arxiv_id": "2511.21690v1",
      "summary": "Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D \"trace-space\" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21690v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "[T]cross-embodiment"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "7_retargeting"
      ]
    },
    {
      "title": "APEX: Action Priors Enable Efficient Exploration for Robust Motion Tracking on Legged Robots",
      "authors": [
        "Shivam Sood",
        "Laukik Nakhwa",
        "Sun Ge",
        "Yuhong Cao",
        "Jin Cheng",
        "Fatemah Zargarbashi",
        "Taerim Yoon",
        "Sungjoon Choi",
        "Stelian Coros",
        "Guillaume Sartoretti"
      ],
      "arxiv_id": "2511.09091v2",
      "summary": "Learning natural, animal-like locomotion from demonstrations has become a core paradigm in legged robotics. Despite the recent advancements in motion tracking, most existing methods demand extensive tuning and rely on reference data during deployment, limiting adaptability. We present APEX (Action Priors enable Efficient Exploration), a plug-and-play extension to state-of-the-art motion tracking algorithms that eliminates any dependence on reference data during deployment, improves sample efficiency, and reduces parameter tuning effort. APEX integrates expert demonstrations directly into reinforcement learning (RL) by incorporating decaying action priors, which initially bias exploration toward expert demonstrations but gradually allow the policy to explore independently. This is combined with a multi-critic framework that balances task performance with motion style. Moreover, APEX enables a single policy to learn diverse motions and transfer reference-like styles across different terrains and velocities, while remaining robust to variations in reward design. We validate the effectiveness of our method through extensive experiments in both simulation and on a Unitree Go2 robot. By leveraging demonstrations to guide exploration during RL training, without imposing explicit bias toward them, APEX enables legged robots to learn with greater stability, efficiency, and generalization. We believe this approach paves the way for guidance-driven RL to boost natural skill acquisition in a wide array of robotic tasks, from locomotion to manipulation. Website and code: https://marmotlab.github.io/APEX/.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-12",
      "updated": "2025-11-19",
      "comment": "This work was intended as a replacement of arXiv:2505.10022 and any subsequent updates will appear there",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09091v2",
      "code_links": [
        {
          "url": "https://marmotlab.github.io/APEX/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]legged robot",
            "locomotion",
            "manipulation",
            "unitree"
          ],
          "score": 12.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "reward design"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Gentle Manipulation Policy Learning via Demonstrations from VLM Planned Atomic Skills",
      "authors": [
        "Jiayu Zhou",
        "Qiwei Wu",
        "Jian Li",
        "Zhe Chen",
        "Xiaogang Xiong",
        "Renjing Xu"
      ],
      "arxiv_id": "2511.05855v1",
      "summary": "Autonomous execution of long-horizon, contact-rich manipulation tasks traditionally requires extensive real-world data and expert engineering, posing significant cost and scalability challenges. This paper proposes a novel framework integrating hierarchical semantic decomposition, reinforcement learning (RL), visual language models (VLMs), and knowledge distillation to overcome these limitations. Complex tasks are decomposed into atomic skills, with RL-trained policies for each primitive exclusively in simulation. Crucially, our RL formulation incorporates explicit force constraints to prevent object damage during delicate interactions. VLMs perform high-level task decomposition and skill planning, generating diverse expert demonstrations. These are distilled into a unified policy via Visual-Tactile Diffusion Policy for end-to-end execution. We conduct comprehensive ablation studies exploring different VLM-based task planners to identify optimal demonstration generation pipelines, and systematically compare imitation learning algorithms for skill distillation. Extensive simulation experiments and physical deployment validate that our approach achieves policy learning for long-horizon manipulation without costly human demonstrations, while the VLM-guided atomic skill framework enables scalable generalization to diverse tasks.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-08",
      "updated": "2025-11-08",
      "comment": "Accepted for the 40th Annual AAAI Conference on Artificial Intelligence (2026)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05855v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "[T]policy learning",
            "imitation learning",
            "diffusion policy"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "CuriGS: Curriculum-Guided Gaussian Splatting for Sparse View Synthesis",
      "authors": [
        "Zijian Wu",
        "Mingfeng Jiang",
        "Zidian Lin",
        "Ying Song",
        "Hanjie Ma",
        "Qun Wu",
        "Dongping Zhang",
        "Guiyang Pu"
      ],
      "arxiv_id": "2511.16030v1",
      "summary": "3D Gaussian Splatting (3DGS) has recently emerged as an efficient, high-fidelity representation for real-time scene reconstruction and rendering. However, extending 3DGS to sparse-view settings remains challenging because of supervision scarcity and overfitting caused by limited viewpoint coverage. In this paper, we present CuriGS, a curriculum-guided framework for sparse-view 3D reconstruction using 3DGS. CuriGS addresses the core challenge of sparse-view synthesis by introducing student views: pseudo-views sampled around ground-truth poses (teacher). For each teacher, we generate multiple groups of student views with different perturbation levels. During training, we follow a curriculum schedule that gradually unlocks higher perturbation level, randomly sampling candidate students from the active level to assist training. Each sampled student is regularized via depth-correlation and co-regularization, and evaluated using a multi-signal metric that combines SSIM, LPIPS, and an image-quality measure. For every teacher and perturbation level, we periodically retain the best-performing students and promote those that satisfy a predefined quality threshold to the training set, resulting in a stable augmentation of sparse training views. Experimental results show that CuriGS outperforms state-of-the-art baselines in both rendering fidelity and geometric consistency across various synthetic and real sparse-view scenes. Project page: https://zijian1026.github.io/CuriGS/",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16030v1",
      "code_links": [
        {
          "url": "https://zijian1026.github.io/CuriGS/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting",
            "scene reconstruction"
          ],
          "score": 12.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "geometric consistency"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "3_perception_slam",
        "7_retargeting"
      ]
    },
    {
      "title": "Simultaneous Localization and 3D-Semi Dense Mapping for Micro Drones Using Monocular Camera and Inertial Sensors",
      "authors": [
        "Jeryes Danial",
        "Yosi Ben Asher",
        "Itzik Klein"
      ],
      "arxiv_id": "2511.14335v2",
      "summary": "Monocular simultaneous localization and mapping (SLAM) algorithms estimate drone poses and build a 3D map using a single camera. Current algorithms include sparse methods that lack detailed geometry, while learning-driven approaches produce dense maps but are computationally intensive. Monocular SLAM also faces scale ambiguities, which affect its accuracy. To address these challenges, we propose an edge-aware lightweight monocular SLAM system combining sparse keypoint-based pose estimation with dense edge reconstruction. Our method employs deep learning-based depth prediction and edge detection, followed by optimization to refine keypoints and edges for geometric consistency, without relying on global loop closure or heavy neural computations. We fuse inertial data with vision by using an extended Kalman filter to resolve scale ambiguity and improve accuracy. The system operates in real time on low-power platforms, as demonstrated on a DJI Tello drone with a monocular camera and inertial sensors. In addition, we demonstrate robust autonomous navigation and obstacle avoidance in indoor corridors and on the TUM RGBD dataset. Our approach offers an effective, practical solution to real-time mapping and navigation in resource-constrained environments.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-18",
      "updated": "2025-11-23",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14335v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "SLAM",
            "pose estimation",
            "[T]localization",
            "navigation"
          ],
          "score": 12.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "geometric consistency"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "3_perception_slam",
        "7_retargeting"
      ]
    },
    {
      "title": "IMTalker: Efficient Audio-driven Talking Face Generation with Implicit Motion Transfer",
      "authors": [
        "Bo Chen",
        "Tao Liu",
        "Qi Chen",
        "Xie Chen",
        "Zilong Zheng"
      ],
      "arxiv_id": "2511.22167v1",
      "summary": "Talking face generation aims to synthesize realistic speaking portraits from a single image, yet existing methods often rely on explicit optical flow and local warping, which fail to model complex global motions and cause identity drift. We present IMTalker, a novel framework that achieves efficient and high-fidelity talking face generation through implicit motion transfer. The core idea is to replace traditional flow-based warping with a cross-attention mechanism that implicitly models motion discrepancy and identity alignment within a unified latent space, enabling robust global motion rendering. To further preserve speaker identity during cross-identity reenactment, we introduce an identity-adaptive module that projects motion latents into personalized spaces, ensuring clear disentanglement between motion and identity. In addition, a lightweight flow-matching motion generator produces vivid and controllable implicit motion vectors from audio, pose, and gaze cues. Extensive experiments demonstrate that IMTalker surpasses prior methods in motion accuracy, identity preservation, and audio-lip synchronization, achieving state-of-the-art quality with superior efficiency, operating at 40 FPS for video-driven and 42 FPS for audio-driven generation on an RTX 4090 GPU. We will release our code and pre-trained models to facilitate applications and future research.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "11 pages, 5 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22167v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "flow matching"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "optical flow"
          ],
          "score": 2.0
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "motion latent"
          ],
          "score": 2.5
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "[T]motion transfer"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam",
        "4_motion_diffusion",
        "7_retargeting"
      ]
    },
    {
      "title": "Maglev-Pentabot: Magnetic Levitation System for Non-Contact Manipulation using Deep Reinforcement Learning",
      "authors": [
        "Guoming Huang",
        "Qingyi Zhou",
        "Dianjing Liu",
        "Shuai Zhang",
        "Ming Zhou",
        "Zongfu Yu"
      ],
      "arxiv_id": "2511.21149v1",
      "summary": "Non-contact manipulation has emerged as a transformative approach across various industrial fields. However, current flexible 2D and 3D non-contact manipulation techniques are often limited to microscopic scales, typically controlling objects in the milligram range. In this paper, we present a magnetic levitation system, termed Maglev-Pentabot, designed to address this limitation. The Maglev-Pentabot leverages deep reinforcement learning (DRL) to develop complex control strategies for manipulating objects in the gram range. Specifically, we propose an electromagnet arrangement optimized through numerical analysis to maximize controllable space. Additionally, an action remapping method is introduced to address sample sparsity issues caused by the strong nonlinearity in magnetic field intensity, hence allowing the DRL controller to converge. Experimental results demonstrate flexible manipulation capabilities, and notably, our system can generalize to transport tasks it has not been explicitly trained for. Furthermore, our approach can be scaled to manipulate heavier objects using larger electromagnets, offering a reference framework for industrial-scale robotic applications.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21149v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "[T]deep reinforcement learning"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "ReAlign: Text-to-Motion Generation via Step-Aware Reward-Guided Alignment",
      "authors": [
        "Wanjiang Weng",
        "Xiaofeng Tan",
        "Junbo Wang",
        "Guo-Sen Xie",
        "Pan Zhou",
        "Hongsong Wang"
      ],
      "arxiv_id": "2511.19217v1",
      "summary": "Text-to-motion generation, which synthesizes 3D human motions from text inputs, holds immense potential for applications in gaming, film, and robotics. Recently, diffusion-based methods have been shown to generate more diversity and realistic motion. However, there exists a misalignment between text and motion distributions in diffusion models, which leads to semantically inconsistent or low-quality motions. To address this limitation, we propose Reward-guided sampling Alignment (ReAlign), comprising a step-aware reward model to assess alignment quality during the denoising sampling and a reward-guided strategy that directs the diffusion process toward an optimally aligned distribution. This reward model integrates step-aware tokens and combines a text-aligned module for semantic consistency and a motion-aligned module for realism, refining noisy motions at each timestep to balance probability density and alignment. Extensive experiments of both motion generation and retrieval tasks demonstrate that our approach significantly improves text-motion alignment and motion quality compared to existing state-of-the-art methods.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "Accepted by AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19217v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "[T]text-to-motion",
            "[T]motion generation"
          ],
          "score": 15.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "4_motion_diffusion"
      ]
    },
    {
      "title": "Switch-JustDance: Benchmarking Whole Body Motion Tracking Policies Using a Commercial Console Game",
      "authors": [
        "Jeonghwan Kim",
        "Wontaek Kim",
        "Yidan Lu",
        "Jin Cheng",
        "Fatemeh Zargarbashi",
        "Zicheng Zeng",
        "Zekun Qi",
        "Zhiyang Dou",
        "Nitish Sontakke",
        "Donghoon Baek",
        "Sehoon Ha",
        "Tianyu Li"
      ],
      "arxiv_id": "2511.17925v2",
      "summary": "Recent advances in whole-body robot control have enabled humanoid and legged robots to perform increasingly agile and coordinated motions. However, standardized benchmarks for evaluating these capabilities in real-world settings, and in direct comparison to humans, remain scarce. Existing evaluations often rely on pre-collected human motion datasets or simulation-based experiments, which limit reproducibility, overlook hardware factors, and hinder fair human-robot comparisons. We present Switch-JustDance, a low-cost and reproducible benchmarking pipeline that leverages motion-sensing console games, Just Dance on the Nintendo Switch, to evaluate robot whole-body control. Using Just Dance on the Nintendo Switch as a representative platform, Switch-JustDance converts in-game choreography into robot-executable motions through streaming, motion reconstruction, and motion retargeting modules and enables users to evaluate controller performance through the game's built-in scoring system. We first validate the evaluation properties of Just Dance, analyzing its reliability, validity, sensitivity, and potential sources of bias. Our results show that the platform provides consistent and interpretable performance measures, making it a suitable tool for benchmarking embodied AI. Building on this foundation, we benchmark three state-of-the-art humanoid whole-body controllers on hardware and provide insights into their relative strengths and limitations.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-22",
      "updated": "2025-12-08",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17925v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "legged robot",
            "humanoid",
            "whole-body control",
            "[T]whole-body motion"
          ],
          "score": 12.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "motion retargeting"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 15.0,
      "hit_pillars": [
        "1_robot_core",
        "7_retargeting"
      ]
    },
    {
      "title": "Unveiling the Impact of Data and Model Scaling on High-Level Control for Humanoid Robots",
      "authors": [
        "Yuxi Wei",
        "Zirui Wang",
        "Kangning Yin",
        "Yue Hu",
        "Jingbo Wang",
        "Siheng Chen"
      ],
      "arxiv_id": "2511.09241v2",
      "summary": "Data scaling has long remained a critical bottleneck in robot learning. For humanoid robots, human videos and motion data are abundant and widely available, offering a free and large-scale data source. Besides, the semantics related to the motions enable modality alignment and high-level robot control learning. However, how to effectively mine raw video, extract robot-learnable representations, and leverage them for scalable learning remains an open problem. To address this, we introduce Humanoid-Union, a large-scale dataset generated through an autonomous pipeline, comprising over 260 hours of diverse, high-quality humanoid robot motion data with semantic annotations derived from human motion videos. The dataset can be further expanded via the same pipeline. Building on this data resource, we propose SCHUR, a scalable learning framework designed to explore the impact of large-scale data on high-level control in humanoid robots. Experimental results demonstrate that SCHUR achieves high robot motion generation quality and strong text-motion alignment under data and model scaling, with 37\\% reconstruction improvement under MPJPE and 25\\% alignment improvement under FID comparing with previous methods. Its effectiveness is further validated through deployment in real-world humanoid robot.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-12",
      "updated": "2025-12-07",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09241v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]humanoid robot"
          ],
          "score": 12.0
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "motion generation"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 14.5,
      "hit_pillars": [
        "1_robot_core",
        "4_motion_diffusion"
      ]
    },
    {
      "title": "PressTrack-HMR: Pressure-Based Top-Down Multi-Person Global Human Mesh Recovery",
      "authors": [
        "Jiayue Yuan",
        "Fangting Xie",
        "Guangwen Ouyang",
        "Changhai Ma",
        "Ziyu Wu",
        "Heyu Ding",
        "Quan Wan",
        "Yi Ke",
        "Yuchen Wu",
        "Xiaohui Cai"
      ],
      "arxiv_id": "2511.09147v2",
      "summary": "Multi-person global human mesh recovery (HMR) is crucial for understanding crowd dynamics and interactions. Traditional vision-based HMR methods sometimes face limitations in real-world scenarios due to mutual occlusions, insufficient lighting, and privacy concerns. Human-floor tactile interactions offer an occlusion-free and privacy-friendly alternative for capturing human motion. Existing research indicates that pressure signals acquired from tactile mats can effectively estimate human pose in single-person scenarios. However, when multiple individuals walk randomly on the mat simultaneously, how to distinguish intermingled pressure signals generated by different persons and subsequently acquire individual temporal pressure data remains a pending challenge for extending pressure-based HMR to the multi-person situation. In this paper, we present \\textbf{PressTrack-HMR}, a top-down pipeline that recovers multi-person global human meshes solely from pressure signals. This pipeline leverages a tracking-by-detection strategy to first identify and segment each individual's pressure signal from the raw pressure data, and subsequently performs HMR for each extracted individual signal. Furthermore, we build a multi-person interaction pressure dataset \\textbf{MIP}, which facilitates further research into pressure-based human motion analysis in multi-person scenarios. Experimental results demonstrate that our method excels in multi-person HMR using pressure data, with 89.2 $mm$ MPJPE and 112.6 $mm$ WA-MPJPE$_{100}$, and these showcase the potential of tactile mats for ubiquitous, privacy-preserving multi-person action recognition. Our dataset & code are available at https://github.com/Jiayue-Yuan/PressTrack-HMR.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-12",
      "updated": "2025-11-13",
      "comment": "Accepted by AAAI-2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09147v2",
      "code_links": [
        {
          "url": "https://github.com/Jiayue-Yuan/PressTrack-HMR",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "multi-person interaction"
          ],
          "score": 2.5
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "[T]human mesh recovery",
            "[T]HMR"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 14.5,
      "hit_pillars": [
        "5_interaction_reaction",
        "6_video_extraction"
      ]
    },
    {
      "title": "Isaac Lab: A GPU-Accelerated Simulation Framework for Multi-Modal Robot Learning",
      "authors": [
        "NVIDIA",
        ":",
        "Mayank Mittal",
        "Pascal Roth",
        "James Tigue",
        "Antoine Richard",
        "Octi Zhang",
        "Peter Du",
        "Antonio Serrano-Muñoz",
        "Xinjie Yao",
        "René Zurbrügg",
        "Nikita Rudin",
        "Lukasz Wawrzyniak",
        "Milad Rakhsha",
        "Alain Denzler",
        "Eric Heiden",
        "Ales Borovicka",
        "Ossama Ahmed",
        "Iretiayo Akinola",
        "Abrar Anwar",
        "Mark T. Carlson",
        "Ji Yuan Feng",
        "Animesh Garg",
        "Renato Gasoto",
        "Lionel Gulich",
        "Yijie Guo",
        "M. Gussert",
        "Alex Hansen",
        "Mihir Kulkarni",
        "Chenran Li",
        "Wei Liu",
        "Viktor Makoviychuk",
        "Grzegorz Malczyk",
        "Hammad Mazhar",
        "Masoud Moghani",
        "Adithyavairavan Murali",
        "Michael Noseworthy",
        "Alexander Poddubny",
        "Nathan Ratliff",
        "Welf Rehberg",
        "Clemens Schwarke",
        "Ritvik Singh",
        "James Latham Smith",
        "Bingjie Tang",
        "Ruchik Thaker",
        "Matthew Trepte",
        "Karl Van Wyk",
        "Fangzhou Yu",
        "Alex Millane",
        "Vikram Ramasamy",
        "Remo Steiner",
        "Sangeeta Subramanian",
        "Clemens Volk",
        "CY Chen",
        "Neel Jawale",
        "Ashwin Varghese Kuruttukulam",
        "Michael A. Lin",
        "Ajay Mandlekar",
        "Karsten Patzwaldt",
        "John Welsh",
        "Huihua Zhao",
        "Fatima Anes",
        "Jean-Francois Lafleche",
        "Nicolas Moënne-Loccoz",
        "Soowan Park",
        "Rob Stepinski",
        "Dirk Van Gelder",
        "Chris Amevor",
        "Jan Carius",
        "Jumyung Chang",
        "Anka He Chen",
        "Pablo de Heras Ciechomski",
        "Gilles Daviet",
        "Mohammad Mohajerani",
        "Julia von Muralt",
        "Viktor Reutskyy",
        "Michael Sauter",
        "Simon Schirm",
        "Eric L. Shi",
        "Pierre Terdiman",
        "Kenny Vilella",
        "Tobias Widmer",
        "Gordon Yeoman",
        "Tiffany Chen",
        "Sergey Grizan",
        "Cathy Li",
        "Lotus Li",
        "Connor Smith",
        "Rafael Wiltz",
        "Kostas Alexis",
        "Yan Chang",
        "David Chu",
        "Linxi \"Jim\" Fan",
        "Farbod Farshidian",
        "Ankur Handa",
        "Spencer Huang",
        "Marco Hutter",
        "Yashraj Narang",
        "Soha Pouya",
        "Shiwei Sheng",
        "Yuke Zhu",
        "Miles Macklin",
        "Adam Moravanszky",
        "Philipp Reist",
        "Yunrong Guo",
        "David Hoeller",
        "Gavriel State"
      ],
      "arxiv_id": "2511.04831v1",
      "summary": "We present Isaac Lab, the natural successor to Isaac Gym, which extends the paradigm of GPU-native robotics simulation into the era of large-scale multi-modal learning. Isaac Lab combines high-fidelity GPU parallel physics, photorealistic rendering, and a modular, composable architecture for designing environments and training robot policies. Beyond physics and rendering, the framework integrates actuator models, multi-frequency sensor simulation, data collection pipelines, and domain randomization tools, unifying best practices for reinforcement and imitation learning at scale within a single extensible platform. We highlight its application to a diverse set of challenges, including whole-body control, cross-embodiment mobility, contact-rich and dexterous manipulation, and the integration of human demonstrations for skill acquisition. Finally, we discuss upcoming integration with the differentiable, GPU-accelerated Newton physics engine, which promises new opportunities for scalable, data-efficient, and gradient-based approaches to robot learning. We believe Isaac Lab's combination of advanced simulation capabilities, rich sensing, and data-center scale execution will help unlock the next generation of breakthroughs in robotics research.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "Code and documentation are available here: https://github.com/isaac-sim/IsaacLab",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04831v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "whole-body control",
            "manipulation",
            "dexterous manipulation",
            "domain randomization"
          ],
          "score": 8.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning",
            "Isaac Gym"
          ],
          "score": 3.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "cross-embodiment"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "7_retargeting"
      ]
    },
    {
      "title": "ScheduleStream: Temporal Planning with Samplers for GPU-Accelerated Multi-Arm Task and Motion Planning & Scheduling",
      "authors": [
        "Caelan Garrett",
        "Fabio Ramos"
      ],
      "arxiv_id": "2511.04758v1",
      "summary": "Bimanual and humanoid robots are appealing because of their human-like ability to leverage multiple arms to efficiently complete tasks. However, controlling multiple arms at once is computationally challenging due to the growth in the hybrid discrete-continuous action space. Task and Motion Planning (TAMP) algorithms can efficiently plan in hybrid spaces but generally produce plans, where only one arm is moving at a time, rather than schedules that allow for parallel arm motion. In order to extend TAMP to produce schedules, we present ScheduleStream, the first general-purpose framework for planning & scheduling with sampling operations. ScheduleStream models temporal dynamics using hybrid durative actions, which can be started asynchronously and persist for a duration that's a function of their parameters. We propose domain-independent algorithms that solve ScheduleStream problems without any application-specific mechanisms. We apply ScheduleStream to Task and Motion Planning & Scheduling (TAMPAS), where we use GPU acceleration within samplers to expedite planning. We compare ScheduleStream algorithms to several ablations in simulation and find that they produce more efficient solutions. We demonstrate ScheduleStream on several real-world bimanual robot tasks at https://schedulestream.github.io.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "Project website: https://schedulestream.github.io",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04758v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid",
            "humanoid robot",
            "bi-manual",
            "bimanual",
            "[T]motion planning"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations",
      "authors": [
        "Maximus A. Pace",
        "Prithwish Dan",
        "Chuanruo Ning",
        "Atiksh Bhardwaj",
        "Audrey Du",
        "Edward W. Duan",
        "Wei-Chiu Ma",
        "Kushal Kedia"
      ],
      "arxiv_id": "2511.04671v1",
      "summary": "Human videos can be recorded quickly and at scale, making them an appealing source of training data for robot learning. However, humans and robots differ fundamentally in embodiment, resulting in mismatched action execution. Direct kinematic retargeting of human hand motion can therefore produce actions that are physically infeasible for robots. Despite these low-level differences, human demonstrations provide valuable motion cues about how to manipulate and interact with objects. Our key idea is to exploit the forward diffusion process: as noise is added to actions, low-level execution differences fade while high-level task guidance is preserved. We present X-Diffusion, a principled framework for training diffusion policies that maximally leverages human data without learning dynamically infeasible motions. X-Diffusion first trains a classifier to predict whether a noisy action is executed by a human or robot. Then, a human action is incorporated into policy training only after adding sufficient noise such that the classifier cannot discern its embodiment. Actions consistent with robot execution supervise fine-grained denoising at low noise levels, while mismatched human actions provide only coarse guidance at higher noise levels. Our experiments show that naive co-training under execution mismatches degrades policy performance, while X-Diffusion consistently improves it. Across five manipulation tasks, X-Diffusion achieves a 16% higher average success rate than the best baseline. The project website is available at https://portal-cornell.github.io/X-Diffusion/.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04671v1",
      "code_links": [
        {
          "url": "https://portal-cornell.github.io/X-Diffusion/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "kinematic retargeting",
            "[T]cross-embodiment"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core",
        "7_retargeting"
      ]
    },
    {
      "title": "CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation",
      "authors": [
        "Yuwen Tao",
        "Kanglei Zhou",
        "Xin Tan",
        "Yuan Xie"
      ],
      "arxiv_id": "2511.03992v1",
      "summary": "Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret free-form language expressions and localize the corresponding 3D regions in Gaussian fields. While recent advances have introduced cross-modal alignment between language and 3D geometry, existing pipelines still struggle with cross-view consistency due to their reliance on 2D rendered pseudo supervision and view specific feature learning. In this work, we present Camera Aware Referring Field (CaRF), a fully differentiable framework that operates directly in the 3D Gaussian space and achieves multi view consistency. Specifically, CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates camera geometry into Gaussian text interactions to explicitly model view dependent variations and enhance geometric reasoning. Building on this, In Training Paired View Supervision (ITPVS) is proposed to align per Gaussian logits across calibrated views during training, effectively mitigating single view overfitting and exposing inter view discrepancies for optimization. Extensive experiments on three representative benchmarks demonstrate that CaRF achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of the art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively. Moreover, this work promotes more reliable and view consistent 3D scene understanding, with potential benefits for embodied AI, AR/VR interaction, and autonomous perception.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03992v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "[T]gaussian splatting",
            "scene understanding"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Source-Free Bistable Fluidic Gripper for Size-Selective and Stiffness-Adaptive Grasping",
      "authors": [
        "Zhihang Qin",
        "Yueheng Zhang",
        "Wan Su",
        "Linxin Hou",
        "Shenghao Zhou",
        "Zhijun Chen",
        "Yu Jun Tan",
        "Cecilia Laschi"
      ],
      "arxiv_id": "2511.03691v1",
      "summary": "Conventional fluid-driven soft grippers typically depend on external sources, which limit portability and long-term autonomy. This work introduces a self-contained soft gripper with fixed size that operates solely through internal liquid redistribution among three interconnected bistable snap-through chambers. When the top sensing chamber deforms upon contact, the displaced liquid triggers snap-through expansion of the grasping chambers, enabling stable and size-selective grasping without continuous energy input. The internal hydraulic feedback further allows passive adaptation of gripping pressure to object stiffness. This source-free and compact design opens new possibilities for lightweight, stiffness-adaptive fluid-driven manipulation in soft robotics, providing a feasible approach for targeted size-specific sampling and operation in underwater and field environments.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-05",
      "updated": "2025-11-05",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03691v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "[T]grasping",
            "[T]grasp"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single Panoramic Camera",
      "authors": [
        "Hao Shi",
        "Ze Wang",
        "Shangwei Guo",
        "Mengfei Duan",
        "Song Wang",
        "Teng Chen",
        "Kailun Yang",
        "Lin Wang",
        "Kaiwei Wang"
      ],
      "arxiv_id": "2511.03571v1",
      "summary": "Robust 3D semantic occupancy is crucial for legged/humanoid robots, yet most semantic scene completion (SSC) systems target wheeled platforms with forward-facing sensors. We present OneOcc, a vision-only panoramic SSC framework designed for gait-introduced body jitter and 360° continuity. OneOcc combines: (i) Dual-Projection fusion (DP-ER) to exploit the annular panorama and its equirectangular unfolding, preserving 360° continuity and grid alignment; (ii) Bi-Grid Voxelization (BGV) to reason in Cartesian and cylindrical-polar spaces, reducing discretization bias and sharpening free/occupied boundaries; (iii) a lightweight decoder with Hierarchical AMoE-3D for dynamic multi-scale fusion and better long-range/occlusion reasoning; and (iv) plug-and-play Gait Displacement Compensation (GDC) learning feature-level motion correction without extra sensors. We also release two panoramic occupancy benchmarks: QuadOcc (real quadruped, first-person 360°) and Human360Occ (H3O) (CARLA human-ego 360° with RGB, Depth, semantic occupancy; standardized within-/cross-city splits). OneOcc sets new state-of-the-art (SOTA): on QuadOcc it beats strong vision baselines and popular LiDAR ones; on H3O it gains +3.83 mIoU (within-city) and +8.08 (cross-city). Modules are lightweight, enabling deployable full-surround perception for legged/humanoid robots. Datasets and code will be publicly available at https://github.com/MasterHow/OneOcc.",
      "categories": [
        "cs.RO",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-05",
      "updated": "2025-11-05",
      "comment": "Datasets and code will be publicly available at https://github.com/MasterHow/OneOcc",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03571v1",
      "code_links": [
        {
          "url": "https://github.com/MasterHow/OneOcc",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped",
            "[T]legged robot",
            "humanoid",
            "humanoid robot",
            "gait"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Phy-Tac: Toward Human-Like Grasping via Physics-Conditioned Tactile Goals",
      "authors": [
        "Shipeng Lyu",
        "Lijie Sheng",
        "Fangyuan Wang",
        "Wenyao Zhang",
        "Weiwei Lin",
        "Zhenzhong Jia",
        "David Navarro-Alarcon",
        "Guodong Guo"
      ],
      "arxiv_id": "2511.01520v1",
      "summary": "Humans naturally grasp objects with minimal level required force for stability, whereas robots often rely on rigid, over-squeezing control. To narrow this gap, we propose a human-inspired physics-conditioned tactile method (Phy-Tac) for force-optimal stable grasping (FOSG) that unifies pose selection, tactile prediction, and force regulation. A physics-based pose selector first identifies feasible contact regions with optimal force distribution based on surface geometry. Then, a physics-conditioned latent diffusion model (Phy-LDM) predicts the tactile imprint under FOSG target. Last, a latent-space LQR controller drives the gripper toward this tactile imprint with minimal actuation, preventing unnecessary compression. Trained on a physics-conditioned tactile dataset covering diverse objects and contact conditions, the proposed Phy-LDM achieves superior tactile prediction accuracy, while the Phy-Tac outperforms fixed-force and GraspNet-based baselines in grasp stability and force efficiency. Experiments on classical robotic platforms demonstrate force-efficient and adaptive manipulation that bridges the gap between robotic and human grasping.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "9 papges, 10 figures, 3 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01520v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "[T]grasping",
            "[T]grasp"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Adaptive and Multi-object Grasping via Deformable Origami Modules",
      "authors": [
        "Peiyi Wang",
        "Paul A. M. Lefeuvre",
        "Shangwei Zou",
        "Zhenwei Ni",
        "Daniela Rus",
        "Cecilia Laschi"
      ],
      "arxiv_id": "2511.00516v1",
      "summary": "Soft robotics gripper have shown great promise in handling fragile and geometrically complex objects. However, most existing solutions rely on bulky actuators, complex control strategies, or advanced tactile sensing to achieve stable and reliable grasping performance. In this work, we present a multi-finger hybrid gripper featuring passively deformable origami modules that generate constant force and torque output. Each finger composed of parallel origami modules is driven by a 1-DoF actuator mechanism, enabling passive shape adaptability and stable grasping force without active sensing or feedback control. More importantly, we demonstrate an interesting capability in simultaneous multi-object grasping, which allows stacked objects of varied shape and size to be picked, transported and placed independently at different states, significantly improving manipulation efficiency compared to single-object grasping. These results highlight the potential of origami-based compliant structures as scalable modules for adaptive, stable and efficient multi-object manipulation in domestic and industrial pick-and-place scenarios.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-01",
      "updated": "2025-11-01",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00516v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "[T]grasping",
            "[T]grasp"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "ICD-Net: Inertial Covariance Displacement Network for Drone Visual-Inertial SLAM",
      "authors": [
        "Tali Orlev Shapira",
        "Itzik Klein"
      ],
      "arxiv_id": "2512.00037v1",
      "summary": "Visual-inertial SLAM systems often exhibit suboptimal performance due to multiple confounding factors including imperfect sensor calibration, noisy measurements, rapid motion dynamics, low illumination, and the inherent limitations of traditional inertial navigation integration methods. These issues are particularly problematic in drone applications where robust and accurate state estimation is critical for safe autonomous operation. In this work, we present ICD-Net, a novel framework that enhances visual-inertial SLAM performance by learning to process raw inertial measurements and generating displacement estimates with associated uncertainty quantification. Rather than relying on analytical inertial sensor models that struggle with real-world sensor imperfections, our method directly extracts displacement maps from sensor data while simultaneously predicting measurement covariances that reflect estimation confidence. We integrate ICD-Net outputs as additional residual constraints into the VINS-Fusion optimization framework, where the predicted uncertainties appropriately weight the neural network contributions relative to traditional visual and inertial terms. The learned displacement constraints provide complementary information that compensates for various error sources in the SLAM pipeline. Our approach can be used under both normal operating conditions and in situations of camera inconsistency or visual degradation. Experimental evaluation on challenging high-speed drone sequences demonstrated that our approach significantly improved trajectory estimation accuracy compared to standard VINS-Fusion, with more than 38% improvement in mean APE and uncertainty estimates proving crucial for maintaining system robustness. Our method shows that neural network enhancement can effectively address multiple sources of SLAM degradation while maintaining real-time performance requirements.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00037v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]SLAM",
            "[T]visual-inertial",
            "navigation"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "PALMS+: Modular Image-Based Floor Plan Localization Leveraging Depth Foundation Model",
      "authors": [
        "Yunqian Cheng",
        "Benjamin Princen",
        "Roberto Manduchi"
      ],
      "arxiv_id": "2511.09724v1",
      "summary": "Indoor localization in GPS-denied environments is crucial for applications like emergency response and assistive navigation. Vision-based methods such as PALMS enable infrastructure-free localization using only a floor plan and a stationary scan, but are limited by the short range of smartphone LiDAR and ambiguity in indoor layouts. We propose PALMS$+$, a modular, image-based system that addresses these challenges by reconstructing scale-aligned 3D point clouds from posed RGB images using a foundation monocular depth estimation model (Depth Pro), followed by geometric layout matching via convolution with the floor plan. PALMS$+$ outputs a posterior over the location and orientation, usable for direct or sequential localization. Evaluated on the Structured3D and a custom campus dataset consisting of 80 observations across four large campus buildings, PALMS$+$ outperforms PALMS and F3Loc in stationary localization accuracy -- without requiring any training. Furthermore, when integrated with a particle filter for sequential localization on 33 real-world trajectories, PALMS$+$ achieved lower localization errors compared to other methods, demonstrating robustness for camera-free tracking and its potential for infrastructure-free applications. Code and data are available at https://github.com/Head-inthe-Cloud/PALMS-Plane-based-Accessible-Indoor-Localization-Using-Mobile-Smartphones",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "Accepted to IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026, Application Track. Main paper: 8 pages, 5 figures. Supplementary material included",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09724v1",
      "code_links": [
        {
          "url": "https://github.com/Head-inthe-Cloud/PALMS-Plane-based-Accessible-Indoor-Localization-Using-Mobile-Smartphones",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation",
            "monocular depth",
            "point cloud",
            "[T]localization",
            "navigation"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "IFG: Internet-Scale Guidance for Functional Grasping Generation",
      "authors": [
        "Ray Muxin Liu",
        "Mingxuan Li",
        "Kenneth Shaw",
        "Deepak Pathak"
      ],
      "arxiv_id": "2511.09558v1",
      "summary": "Large Vision Models trained on internet-scale data have demonstrated strong capabilities in segmenting and semantically understanding object parts, even in cluttered, crowded scenes. However, while these models can direct a robot toward the general region of an object, they lack the geometric understanding required to precisely control dexterous robotic hands for 3D grasping. To overcome this, our key insight is to leverage simulation with a force-closure grasping generation pipeline that understands local geometries of the hand and object in the scene. Because this pipeline is slow and requires ground-truth observations, the resulting data is distilled into a diffusion model that operates in real-time on camera point clouds. By combining the global semantic understanding of internet-scale models with the geometric precision of a simulation-based locally-aware force-closure, \\our achieves high-performance semantic grasping without any manually collected training data. For visualizations of this please visit our website at https://ifgrasping.github.io/",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.GR",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "Website at https://ifgrasping.github.io/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09558v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]grasping",
            "[T]grasp"
          ],
          "score": 12.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "ScaleADFG: Affordance-based Dexterous Functional Grasping via Scalable Dataset",
      "authors": [
        "Sizhe Wang",
        "Yifan Yang",
        "Yongkang Luo",
        "Daheng Li",
        "Wei Wei",
        "Yan Zhang",
        "Peiying Hu",
        "Yunjin Fu",
        "Haonan Duan",
        "Jia Sun",
        "Peng Wang"
      ],
      "arxiv_id": "2511.09602v1",
      "summary": "Dexterous functional tool-use grasping is essential for effective robotic manipulation of tools. However, existing approaches face significant challenges in efficiently constructing large-scale datasets and ensuring generalizability to everyday object scales. These issues primarily arise from size mismatches between robotic and human hands, and the diversity in real-world object scales. To address these limitations, we propose the ScaleADFG framework, which consists of a fully automated dataset construction pipeline and a lightweight grasp generation network. Our dataset introduce an affordance-based algorithm to synthesize diverse tool-use grasp configurations without expert demonstrations, allowing flexible object-hand size ratios and enabling large robotic hands (compared to human hands) to grasp everyday objects effectively. Additionally, we leverage pre-trained models to generate extensive 3D assets and facilitate efficient retrieval of object affordances. Our dataset comprising five object categories, each containing over 1,000 unique shapes with 15 scale variations. After filtering, the dataset includes over 60,000 grasps for each 2 dexterous robotic hands. On top of this dataset, we train a lightweight, single-stage grasp generation network with a notably simple loss design, eliminating the need for post-refinement. This demonstrates the critical importance of large-scale datasets and multi-scale object variant for effective training. Extensive experiments in simulation and on real robot confirm that the ScaleADFG framework exhibits strong adaptability to objects of varying scales, enhancing functional grasp stability, diversity, and generalizability. Moreover, our network exhibits effective zero-shot transfer to real-world objects. Project page is available at https://sizhe-wang.github.io/ScaleADFG_webpage",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "Accepted by IEEE Robotics and Automation Letters",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09602v1",
      "code_links": [
        {
          "url": "https://sizhe-wang.github.io/ScaleADFG_webpage",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "[T]grasping",
            "[T]grasp"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Learning Omnidirectional Locomotion for a Salamander-Like Quadruped Robot",
      "authors": [
        "Zhiang Liu",
        "Yang Liu",
        "Yongchun Fang",
        "Xian Guo"
      ],
      "arxiv_id": "2511.08299v1",
      "summary": "Salamander-like quadruped robots are designed inspired by the skeletal structure of their biological counterparts. However, existing controllers cannot fully exploit these morphological features and largely rely on predefined gait patterns or joint trajectories, which prevents the generation of diverse and flexible locomotion and limits their applicability in real-world scenarios. In this paper, we propose a learning framework that enables the robot to acquire a diverse repertoire of omnidirectional gaits without reference motions. Each body part is controlled by a phase variable capable of forward and backward evolution, with a phase coverage reward to promote the exploration of the leg phase space. Additionally, morphological symmetry of the robot is incorporated via data augmentation, improving sample efficiency and enforcing both motion-level and task-level symmetry in learned behaviors. Extensive experiments show that the robot successfully acquires 22 omnidirectional gaits exhibiting both dynamic and symmetric movements, demonstrating the effectiveness of the proposed learning framework.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08299v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]quadruped",
            "[T]locomotion",
            "gait"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Perceptual Quality Assessment of 3D Gaussian Splatting: A Subjective Dataset and Prediction Metric",
      "authors": [
        "Zhaolin Wan",
        "Yining Diao",
        "Jingqi Xu",
        "Hao Wang",
        "Zhiyang Li",
        "Xiaopeng Fan",
        "Wangmeng Zuo",
        "Debin Zhao"
      ],
      "arxiv_id": "2511.08032v1",
      "summary": "With the rapid advancement of 3D visualization, 3D Gaussian Splatting (3DGS) has emerged as a leading technique for real-time, high-fidelity rendering. While prior research has emphasized algorithmic performance and visual fidelity, the perceptual quality of 3DGS-rendered content, especially under varying reconstruction conditions, remains largely underexplored. In practice, factors such as viewpoint sparsity, limited training iterations, point downsampling, noise, and color distortions can significantly degrade visual quality, yet their perceptual impact has not been systematically studied. To bridge this gap, we present 3DGS-QA, the first subjective quality assessment dataset for 3DGS. It comprises 225 degraded reconstructions across 15 object types, enabling a controlled investigation of common distortion factors. Based on this dataset, we introduce a no-reference quality prediction model that directly operates on native 3D Gaussian primitives, without requiring rendered images or ground-truth references. Our model extracts spatial and photometric cues from the Gaussian representation to estimate perceived quality in a structure-aware manner. We further benchmark existing quality assessment methods, spanning both traditional and learning-based approaches. Experimental results show that our method consistently achieves superior performance, highlighting its robustness and effectiveness for 3DGS content evaluation. The dataset and code are made publicly available at https://github.com/diaoyn/3DGSQA to facilitate future research in 3DGS quality assessment.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08032v1",
      "code_links": [
        {
          "url": "https://github.com/diaoyn/3DGSQA",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "TouchWalker: Real-Time Avatar Locomotion from Touchscreen Finger Walking",
      "authors": [
        "Geuntae Park",
        "Jiwon Yi",
        "Taehyun Rhee",
        "Kwanguk Kim",
        "Yoonsang Lee"
      ],
      "arxiv_id": "2511.07860v1",
      "summary": "We present TouchWalker, a real-time system for controlling full-body avatar locomotion using finger-walking gestures on a touchscreen. The system comprises two main components: TouchWalker-MotionNet, a neural motion generator that synthesizes full-body avatar motion on a per-frame basis from temporally sparse two-finger input, and TouchWalker-UI, a compact touch interface that interprets user touch input to avatar-relative foot positions. Unlike prior systems that rely on symbolic gesture triggers or predefined motion sequences, TouchWalker uses its neural component to generate continuous, context-aware full-body motion on a per-frame basis-including airborne phases such as running, even without input during mid-air steps-enabling more expressive and immediate interaction. To ensure accurate alignment between finger contacts and avatar motion, it employs a MoE-GRU architecture with a dedicated foot-alignment loss. We evaluate TouchWalker in a user study comparing it to a virtual joystick baseline with predefined motion across diverse locomotion tasks. Results show that TouchWalker improves users' sense of embodiment, enjoyment, and immersion.",
      "categories": [
        "cs.HC",
        "cs.GR"
      ],
      "primary_category": "cs.HC",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "Accepted to ISMAR 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07860v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]locomotion",
            "[T]walking",
            "running"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "YoNoSplat: You Only Need One Model for Feedforward 3D Gaussian Splatting",
      "authors": [
        "Botao Ye",
        "Boqi Chen",
        "Haofei Xu",
        "Daniel Barath",
        "Marc Pollefeys"
      ],
      "arxiv_id": "2511.07321v1",
      "summary": "Fast and flexible 3D scene reconstruction from unstructured image collections remains a significant challenge. We present YoNoSplat, a feedforward model that reconstructs high-quality 3D Gaussian Splatting representations from an arbitrary number of images. Our model is highly versatile, operating effectively with both posed and unposed, calibrated and uncalibrated inputs. YoNoSplat predicts local Gaussians and camera poses for each view, which are aggregated into a global representation using either predicted or provided poses. To overcome the inherent difficulty of jointly learning 3D Gaussians and camera parameters, we introduce a novel mixing training strategy. This approach mitigates the entanglement between the two tasks by initially using ground-truth poses to aggregate local Gaussians and gradually transitioning to a mix of predicted and ground-truth poses, which prevents both training instability and exposure bias. We further resolve the scale ambiguity problem by a novel pairwise camera-distance normalization scheme and by embedding camera intrinsics into the network. Moreover, YoNoSplat also predicts intrinsic parameters, making it feasible for uncalibrated inputs. YoNoSplat demonstrates exceptional efficiency, reconstructing a scene from 100 views (at 280x518 resolution) in just 2.69 seconds on an NVIDIA GH200 GPU. It achieves state-of-the-art performance on standard benchmarks in both pose-free and pose-dependent settings. Our project page is at https://botaoye.github.io/yonosplat/.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07321v1",
      "code_links": [
        {
          "url": "https://botaoye.github.io/yonosplat/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "[T]gaussian splatting",
            "scene reconstruction"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Sparse4DGS: 4D Gaussian Splatting for Sparse-Frame Dynamic Scene Reconstruction",
      "authors": [
        "Changyue Shi",
        "Chuxiao Yang",
        "Xinyuan Hu",
        "Minghao Chen",
        "Wenwen Pan",
        "Yan Yang",
        "Jiajun Ding",
        "Zhou Yu",
        "Jun Yu"
      ],
      "arxiv_id": "2511.07122v1",
      "summary": "Dynamic Gaussian Splatting approaches have achieved remarkable performance for 4D scene reconstruction. However, these approaches rely on dense-frame video sequences for photorealistic reconstruction. In real-world scenarios, due to equipment constraints, sometimes only sparse frames are accessible. In this paper, we propose Sparse4DGS, the first method for sparse-frame dynamic scene reconstruction. We observe that dynamic reconstruction methods fail in both canonical and deformed spaces under sparse-frame settings, especially in areas with high texture richness. Sparse4DGS tackles this challenge by focusing on texture-rich areas. For the deformation network, we propose Texture-Aware Deformation Regularization, which introduces a texture-based depth alignment loss to regulate Gaussian deformation. For the canonical Gaussian field, we introduce Texture-Aware Canonical Optimization, which incorporates texture-based noise into the gradient descent process of canonical Gaussians. Extensive experiments show that when taking sparse frames as inputs, our method outperforms existing dynamic or few-shot techniques on NeRF-Synthetic, HyperNeRF, NeRF-DS, and our iPhone-4D datasets.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07122v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]gaussian splatting",
            "NeRF",
            "[T]scene reconstruction"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Semi-distributed Cross-modal Air-Ground Relative Localization",
      "authors": [
        "Weining Lu",
        "Deer Bin",
        "Lian Ma",
        "Ming Ma",
        "Zhihao Ma",
        "Xiangyang Chen",
        "Longfei Wang",
        "Yixiao Feng",
        "Zhouxian Jiang",
        "Yongliang Shi",
        "Bin Liang"
      ],
      "arxiv_id": "2511.06749v1",
      "summary": "Efficient, accurate, and flexible relative localization is crucial in air-ground collaborative tasks. However, current approaches for robot relative localization are primarily realized in the form of distributed multi-robot SLAM systems with the same sensor configuration, which are tightly coupled with the state estimation of all robots, limiting both flexibility and accuracy. To this end, we fully leverage the high capacity of Unmanned Ground Vehicle (UGV) to integrate multiple sensors, enabling a semi-distributed cross-modal air-ground relative localization framework. In this work, both the UGV and the Unmanned Aerial Vehicle (UAV) independently perform SLAM while extracting deep learning-based keypoints and global descriptors, which decouples the relative localization from the state estimation of all agents. The UGV employs a local Bundle Adjustment (BA) with LiDAR, camera, and an IMU to rapidly obtain accurate relative pose estimates. The BA process adopts sparse keypoint optimization and is divided into two stages: First, optimizing camera poses interpolated from LiDAR-Inertial Odometry (LIO), followed by estimating the relative camera poses between the UGV and UAV. Additionally, we implement an incremental loop closure detection algorithm using deep learning-based descriptors to maintain and retrieve keyframes efficiently. Experimental results demonstrate that our method achieves outstanding performance in both accuracy and efficiency. Unlike traditional multi-robot SLAM approaches that transmit images or point clouds, our method only transmits keypoint pixels and their descriptors, effectively constraining the communication bandwidth under 0.3 Mbps. Codes and data will be publicly available on https://github.com/Ascbpiac/cross-model-relative-localization.git.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "7 pages, 3 figures. Accepted by IROS 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06749v1",
      "code_links": [
        {
          "url": "https://github.com/Ascbpiac/cross-model-relative-localization.git",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "SLAM",
            "lidar-inertial",
            "LIO",
            "point cloud",
            "[T]localization"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "LEGO-SLAM: Language-Embedded Gaussian Optimization SLAM",
      "authors": [
        "Sibaek Lee",
        "Seongbo Ha",
        "Kyeongsu Kang",
        "Joonyeol Choi",
        "Seungjun Tak",
        "Hyeonwoo Yu"
      ],
      "arxiv_id": "2511.16144v1",
      "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled Simultaneous Localization and Mapping (SLAM) systems to build photorealistic maps. However, these maps lack the open-vocabulary semantic understanding required for advanced robotic interaction. Integrating language features into SLAM remains a significant challenge, as storing high-dimensional features demands excessive memory and rendering overhead, while existing methods with static models lack adaptability for novel environments. To address these limitations, we propose LEGO-SLAM (Language-Embedded Gaussian Optimization SLAM), the first framework to achieve real-time, open-vocabulary mapping within a 3DGS-based SLAM system. At the core of our method is a scene-adaptive encoder-decoder that distills high-dimensional language embeddings into a compact 16-dimensional feature space. This design reduces the memory per Gaussian and accelerates rendering, enabling real-time performance. Unlike static approaches, our encoder adapts online to unseen scenes. These compact features also enable a language-guided pruning strategy that identifies semantic redundancy, reducing the map's Gaussian count by over 60\\% while maintaining rendering quality. Furthermore, we introduce a language-based loop detection approach that reuses these mapping features, eliminating the need for a separate detection model. Extensive experiments demonstrate that LEGO-SLAM achieves competitive mapping quality and tracking accuracy, all while providing open-vocabulary capabilities at 15 FPS.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "18 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16144v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]SLAM",
            "3D gaussian splatting",
            "3DGS",
            "gaussian splatting",
            "localization"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "RTS-Mono: A Real-Time Self-Supervised Monocular Depth Estimation Method for Real-World Deployment",
      "authors": [
        "Zeyu Cheng",
        "Tongfei Liu",
        "Tao Lei",
        "Xiang Hua",
        "Yi Zhang",
        "Chengkai Tang"
      ],
      "arxiv_id": "2511.14107v1",
      "summary": "Depth information is crucial for autonomous driving and intelligent robot navigation. The simplicity and flexibility of self-supervised monocular depth estimation are conducive to its role in these fields. However, most existing monocular depth estimation models consume many computing resources. Although some methods have reduced the model's size and improved computing efficiency, the performance deteriorates, seriously hindering the real-world deployment of self-supervised monocular depth estimation models in the real world. To address this problem, we proposed a real-time self-supervised monocular depth estimation method and implemented it in the real world. It is called RTS-Mono, which is a lightweight and efficient encoder-decoder architecture. The encoder is based on Lite-Encoder, and the decoder is designed with a multi-scale sparse fusion framework to minimize redundancy, ensure performance, and improve inference speed. RTS-Mono achieved state-of-the-art (SoTA) performance in high and low resolutions with extremely low parameter counts (3 M) in experiments based on the KITTI dataset. Compared with lightweight methods, RTS-Mono improved Abs Rel and Sq Rel by 5.6% and 9.8% at low resolution and improved Sq Rel and RMSE by 6.1% and 1.9% at high resolution. In real-world deployment experiments, RTS-Mono has extremely high accuracy and can perform real-time inference on Nvidia Jetson Orin at a speed of 49 FPS. Source code is available at https://github.com/ZYCheng777/RTS-Mono.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "14 pages, 10 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14107v1",
      "code_links": [
        {
          "url": "https://github.com/ZYCheng777/RTS-Mono",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]depth estimation",
            "[T]monocular depth",
            "navigation"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models",
      "authors": [
        "Yushuo Zheng",
        "Jiangyong Ying",
        "Huiyu Duan",
        "Chunyi Li",
        "Zicheng Zhang",
        "Jing Liu",
        "Xiaohong Liu",
        "Guangtao Zhai"
      ],
      "arxiv_id": "2511.13259v1",
      "summary": "Large multimodal models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks, however their knowledge and abilities in the cross-view geo-localization and pose estimation domains remain unexplored, despite potential benefits for navigation, autonomous driving, outdoor robotics, \\textit{etc}. To bridge this gap, we introduce \\textbf{GeoX-Bench}, a comprehensive \\underline{Bench}mark designed to explore and evaluate the capabilities of LMMs in \\underline{cross}-view \\underline{Geo}-localization and pose estimation. Specifically, GeoX-Bench contains 10,859 panoramic-satellite image pairs spanning 128 cities in 49 countries, along with corresponding 755,976 question-answering (QA) pairs. Among these, 42,900 QA pairs are designated for benchmarking, while the remaining are intended to enhance the capabilities of LMMs. Based on GeoX-Bench, we evaluate the capabilities of 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and further explore the empowered capabilities of instruction-tuning. Our benchmark demonstrate that while current LMMs achieve impressive performance in geo-localization tasks, their effectiveness declines significantly on the more complex pose estimation tasks, highlighting a critical area for future improvement, and instruction-tuning LMMs on the training data of GeoX-Bench can significantly improve the cross-view geo-sense abilities. The GeoX-Bench is available at \\textcolor{magenta}{https://github.com/IntMeGroup/GeoX-Bench}.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13259v1",
      "code_links": [
        {
          "url": "https://github.com/IntMeGroup/GeoX-Bench",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]pose estimation",
            "[T]localization",
            "navigation"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Collision-Free Navigation of Mobile Robots via Quadtree-Based Model Predictive Control",
      "authors": [
        "Osama Al Sheikh Ali",
        "Sotiris Koutsoftas",
        "Ze Zhang",
        "Knut Akesson",
        "Emmanuel Dean"
      ],
      "arxiv_id": "2511.13188v1",
      "summary": "This paper presents an integrated navigation framework for Autonomous Mobile Robots (AMRs) that unifies environment representation, trajectory generation, and Model Predictive Control (MPC). The proposed approach incorporates a quadtree-based method to generate structured, axis-aligned collision-free regions from occupancy maps. These regions serve as both a basis for developing safe corridors and as linear constraints within the MPC formulation, enabling efficient and reliable navigation without requiring direct obstacle encoding. The complete pipeline combines safe-area extraction, connectivity graph construction, trajectory generation, and B-spline smoothing into one coherent system. Experimental results demonstrate consistent success and superior performance compared to baseline approaches across complex environments.",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "This paper has been accepted by IEEE SII 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13188v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "MPC",
            "[T]model predictive control"
          ],
          "score": 8.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "TR-Gaussians: High-fidelity Real-time Rendering of Planar Transmission and Reflection with 3D Gaussian Splatting",
      "authors": [
        "Yong Liu",
        "Keyang Ye",
        "Tianjia Shao",
        "Kun Zhou"
      ],
      "arxiv_id": "2511.13009v1",
      "summary": "We propose Transmission-Reflection Gaussians (TR-Gaussians), a novel 3D-Gaussian-based representation for high-fidelity rendering of planar transmission and reflection, which are ubiquitous in indoor scenes. Our method combines 3D Gaussians with learnable reflection planes that explicitly model the glass planes with view-dependent reflectance strengths. Real scenes and transmission components are modeled by 3D Gaussians and the reflection components are modeled by the mirrored Gaussians with respect to the reflection plane. The transmission and reflection components are blended according to a Fresnel-based, view-dependent weighting scheme, allowing for faithful synthesis of complex appearance effects under varying viewpoints. To effectively optimize TR-Gaussians, we develop a multi-stage optimization framework incorporating color and geometry constraints and an opacity perturbation mechanism. Experiments on different datasets demonstrate that TR-Gaussians achieve real-time, high-fidelity novel view synthesis in scenes with planar transmission and reflection, and outperform state-of-the-art approaches both quantitatively and qualitatively.",
      "categories": [
        "cs.GR",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "15 pages, 12 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13009v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "[T]gaussian splatting",
            "novel view synthesis"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Neo: Real-Time On-Device 3D Gaussian Splatting with Reuse-and-Update Sorting Acceleration",
      "authors": [
        "Changhun Oh",
        "Seongryong Oh",
        "Jinwoo Hwang",
        "Yoonsung Kim",
        "Hardik Sharma",
        "Jongse Park"
      ],
      "arxiv_id": "2511.12930v1",
      "summary": "3D Gaussian Splatting (3DGS) rendering in real-time on resource-constrained devices is essential for delivering immersive augmented and virtual reality (AR/VR) experiences. However, existing solutions struggle to achieve high frame rates, especially for high-resolution rendering. Our analysis identifies the sorting stage in the 3DGS rendering pipeline as the major bottleneck due to its high memory bandwidth demand. This paper presents Neo, which introduces a reuse-and-update sorting algorithm that exploits temporal redundancy in Gaussian ordering across consecutive frames, and devises a hardware accelerator optimized for this algorithm. By efficiently tracking and updating Gaussian depth ordering instead of re-sorting from scratch, Neo significantly reduces redundant computations and memory bandwidth pressure. Experimental results show that Neo achieves up to 10.0x and 5.6x higher throughput than state-of-the-art edge GPU and ASIC solution, respectively, while reducing DRAM traffic by 94.5% and 81.3%. These improvements make high-quality and low-latency on-device 3D rendering more practical.",
      "categories": [
        "cs.AR",
        "cs.CV"
      ],
      "primary_category": "cs.AR",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12930v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "BINDER: Instantly Adaptive Mobile Manipulation with Open-Vocabulary Commands",
      "authors": [
        "Seongwon Cho",
        "Daechul Ahn",
        "Donghyun Shin",
        "Hyeonbeom Choi",
        "San Kim",
        "Jonghyun Choi"
      ],
      "arxiv_id": "2511.22364v1",
      "summary": "Open-vocabulary mobile manipulation (OVMM) requires robots to follow language instructions, navigate, and manipulate while updating their world representation under dynamic environmental changes. However, most prior approaches update their world representation only at discrete update points such as navigation targets, waypoints, or the end of an action step, leaving robots blind between updates and causing cascading failures: overlooked objects, late error detection, and delayed replanning. To address this limitation, we propose BINDER (Bridging INstant and DEliberative Reasoning), a dual process framework that decouples strategic planning from continuous environment monitoring. Specifically, BINDER integrates a Deliberative Response Module (DRM, a multimodal LLM for task planning) with an Instant Response Module (IRM, a VideoLLM for continuous monitoring). The two modules play complementary roles: the DRM performs strategic planning with structured 3D scene updates and guides what the IRM attends to, while the IRM analyzes video streams to update memory, correct ongoing actions, and trigger replanning when necessary. Through this bidirectional coordination, the modules address the trade off between maintaining awareness and avoiding costly updates, enabling robust adaptation under dynamic conditions. Evaluated in three real world environments with dynamic object placement, BINDER achieves substantially higher success and efficiency than SoTA baselines, demonstrating its effectiveness for real world deployment.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "12 pages, 8 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22364v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]mobile manipulation"
          ],
          "score": 12.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Can Protective Watermarking Safeguard the Copyright of 3D Gaussian Splatting?",
      "authors": [
        "Wenkai Huang",
        "Yijia Guo",
        "Gaolei Li",
        "Lei Ma",
        "Hang Zhang",
        "Liwen Hu",
        "Jiazheng Wang",
        "Jianhua Li",
        "Tiejun Huang"
      ],
      "arxiv_id": "2511.22262v1",
      "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation for 3D scenes, widely adopted due to its exceptional efficiency and high-fidelity visual quality. Given the significant value of 3DGS assets, recent works have introduced specialized watermarking schemes to ensure copyright protection and ownership verification. However, can existing 3D Gaussian watermarking approaches genuinely guarantee robust protection of the 3D assets? In this paper, for the first time, we systematically explore and validate possible vulnerabilities of 3DGS watermarking frameworks. We demonstrate that conventional watermark removal techniques designed for 2D images do not effectively generalize to the 3DGS scenario due to the specialized rendering pipeline and unique attributes of each gaussian primitives. Motivated by this insight, we propose GSPure, the first watermark purification framework specifically for 3DGS watermarking representations. By analyzing view-dependent rendering contributions and exploiting geometrically accurate feature clustering, GSPure precisely isolates and effectively removes watermark-related Gaussian primitives while preserving scene integrity. Extensive experiments demonstrate that our GSPure achieves the best watermark purification performance, reducing watermark PSNR by up to 16.34dB while minimizing degradation to original scene fidelity with less than 1dB PSNR loss. Moreover, it consistently outperforms existing methods in both effectiveness and generalization.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "Accepted by AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22262v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "RemedyGS: Defend 3D Gaussian Splatting against Computation Cost Attacks",
      "authors": [
        "Yanping Li",
        "Zhening Liu",
        "Zijian Li",
        "Zehong Lin",
        "Jun Zhang"
      ],
      "arxiv_id": "2511.22147v1",
      "summary": "As a mainstream technique for 3D reconstruction, 3D Gaussian splatting (3DGS) has been applied in a wide range of applications and services. Recent studies have revealed critical vulnerabilities in this pipeline and introduced computation cost attacks that lead to malicious resource occupancies and even denial-of-service (DoS) conditions, thereby hindering the reliable deployment of 3DGS. In this paper, we propose the first effective and comprehensive black-box defense framework, named RemedyGS, against such computation cost attacks, safeguarding 3DGS reconstruction systems and services. Our pipeline comprises two key components: a detector to identify the attacked input images with poisoned textures and a purifier to recover the benign images from their attacked counterparts, mitigating the adverse effects of these attacks. Moreover, we incorporate adversarial training into the purifier to enforce distributional alignment between the recovered and original natural images, thereby enhancing the defense efficacy. Experimental results demonstrate that our framework effectively defends against white-box, black-box, and adaptive attacks in 3DGS systems, achieving state-of-the-art performance in both safety and utility.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22147v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "$Δ$-NeRF: Incremental Refinement of Neural Radiance Fields through Residual Control and Knowledge Transfer",
      "authors": [
        "Kriti Ghosh",
        "Devjyoti Chakraborty",
        "Lakshmish Ramaswamy",
        "Suchendra M. Bhandarkar",
        "In Kee Kim",
        "Nancy O'Hare",
        "Deepak Mishra"
      ],
      "arxiv_id": "2511.20804v1",
      "summary": "Neural Radiance Fields (NeRFs) have demonstrated remarkable capabilities in 3D reconstruction and novel view synthesis. However, most existing NeRF frameworks require complete retraining when new views are introduced incrementally, limiting their applicability in domains where data arrives sequentially. This limitation is particularly problematic in satellite-based terrain analysis, where regions are repeatedly observed over time. Incremental refinement of NeRFs remains underexplored, and naive approaches suffer from catastrophic forgetting when past data is unavailable. We propose $Δ$-NeRF, a unique modular residual framework for incremental NeRF refinement. $Δ$-NeRF introduces several novel techniques including: (1) a residual controller that injects per-layer corrections into a frozen base NeRF, enabling refinement without access to past data; (2) an uncertainty-aware gating mechanism that prevents overcorrection by adaptively combining base and refined predictions; and (3) a view selection strategy that reduces training data by up to 47\\% while maintaining performance. Additionally, we employ knowledge distillation to compress the enhanced model into a compact student network (20\\% of original size). Experiments on satellite imagery demonstrate that $Δ$-NeRF achieves performance comparable to joint training while reducing training time by 30-42\\%. $Δ$-NeRF consistently outperforms existing baselines, achieving an improvement of up to 43.5\\% in PSNR over naive fine-tuning and surpassing joint training on some metrics.",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20804v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]NeRF",
            "[T]neural radiance",
            "novel view synthesis"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "DeLightMono: Enhancing Self-Supervised Monocular Depth Estimation in Endoscopy by Decoupling Uneven Illumination",
      "authors": [
        "Mingyang Ou",
        "Haojin Li",
        "Yifeng Zhang",
        "Ke Niu",
        "Zhongxi Qiu",
        "Heng Li",
        "Jiang Liu"
      ],
      "arxiv_id": "2511.20058v1",
      "summary": "Self-supervised monocular depth estimation serves as a key task in the development of endoscopic navigation systems. However, performance degradation persists due to uneven illumination inherent in endoscopic images, particularly in low-intensity regions. Existing low-light enhancement techniques fail to effectively guide the depth network. Furthermore, solutions from other fields, like autonomous driving, require well-lit images, making them unsuitable and increasing data collection burdens. To this end, we present DeLight-Mono - a novel self-supervised monocular depth estimation framework with illumination decoupling. Specifically, endoscopic images are represented by a designed illumination-reflectance-depth model, and are decomposed with auxiliary networks. Moreover, a self-supervised joint-optimizing framework with novel losses leveraging the decoupled components is proposed to mitigate the effects of uneven illumination on depth estimation. The effectiveness of the proposed methods was rigorously verified through extensive comparisons and an ablation study performed on two public datasets.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20058v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]depth estimation",
            "[T]monocular depth",
            "navigation"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "AIA-UltraNeRF:Acoustic-Impedance-Aware Neural Radiance Field with Hash Encodings for Robotic Ultrasound Reconstruction and Localization",
      "authors": [
        "Shuai Zhang",
        "Jingsong Mu",
        "Cancan Zhao",
        "Leiqi Tian",
        "Zhijun Xing",
        "Bo Ouyang",
        "Xiang Li"
      ],
      "arxiv_id": "2511.18293v1",
      "summary": "Neural radiance field (NeRF) is a promising approach for reconstruction and new view synthesis. However, previous NeRF-based reconstruction methods overlook the critical role of acoustic impedance in ultrasound imaging. Localization methods face challenges related to local minima due to the selection of initial poses. In this study, we design a robotic ultrasound system (RUSS) with an acoustic-impedance-aware ultrasound NeRF (AIA-UltraNeRF) to decouple the scanning and diagnostic processes. Specifically, AIA-UltraNeRF models a continuous function of hash-encoded spatial coordinates for the 3D ultrasound map, allowing for the storage of acoustic impedance without dense sampling. This approach accelerates both reconstruction and inference speeds. We then propose a dual-supervised network that leverages teacher and student models to hash-encode the rendered ultrasound images from the reconstructed map. AIA-UltraNeRF retrieves the most similar hash values without the need to render images again, providing an offline initial image position for localization. Moreover, we develop a RUSS with a spherical remote center of motion mechanism to hold the probe, implementing operator-independent scanning modes that separate image acquisition from diagnostic workflows. Experimental results on a phantom and human subjects demonstrate the effectiveness of acoustic impedance in implicitly characterizing the color of ultrasound images. AIAUltraNeRF achieves both reconstruction and localization with inference speeds that are 9.9 faster than those of vanilla NeRF.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-23",
      "updated": "2025-11-23",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18293v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "NeRF",
            "[T]neural radiance",
            "[T]localization"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Off-Road Navigation via Implicit Neural Representation of Terrain Traversability",
      "authors": [
        "Yixuan Jia",
        "Qingyuan Li",
        "Jonathan P. How"
      ],
      "arxiv_id": "2511.18183v1",
      "summary": "Autonomous off-road navigation requires robots to estimate terrain traversability from onboard sensors and plan accordingly. Conventional approaches typically rely on sampling-based planners such as MPPI to generate short-term control actions that aim to minimize traversal time and risk measures derived from the traversability estimates. These planners can react quickly but optimize only over a short look-ahead window, limiting their ability to reason about the full path geometry, which is important for navigating in challenging off-road environments. Moreover, they lack the ability to adjust speed based on the terrain bumpiness, which is important for smooth navigation on challenging terrains. In this paper, we introduce TRAIL (Traversability with an Implicit Learned Representation), an off-road navigation framework that leverages an implicit neural representation to continuously parameterize terrain properties. This representation yields spatial gradients that enable integration with a novel gradient-based trajectory optimization method that adapts the path geometry and speed profile based on terrain traversability.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-22",
      "updated": "2025-11-22",
      "comment": "9 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18183v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "trajectory optimization"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]traversability",
            "[T]navigation"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "EchoVLA: Robotic Vision-Language-Action Model with Synergistic Declarative Memory for Mobile Manipulation",
      "authors": [
        "Min Lin",
        "Xiwen Liang",
        "Bingqian Lin",
        "Liu Jingzhi",
        "Zijian Jiao",
        "Kehan Li",
        "Yuhan Ma",
        "Yuecheng Liu",
        "Shen Zhao",
        "Yuzheng Zhuang",
        "Xiaodan Liang"
      ],
      "arxiv_id": "2511.18112v1",
      "summary": "Recent progress in Vision-Language-Action (VLA) models has enabled embodied agents to interpret multimodal instructions and perform complex tasks. However, existing VLAs are mostly confined to short-horizon, table-top manipulation, lacking the memory and reasoning capability required for long-horizon mobile manipulation, where agents must coordinate navigation and manipulation under changing spatial contexts. In this work, we present EchoVLA, a memory-aware VLA model for long-horizon mobile manipulation. EchoVLA incorporates a synergistic declarative memory inspired by the human brain, consisting of a scene memory that maintains a collection of spatial-semantic maps and an episodic memory that stores task-level experiences with multimodal contextual features. During both training and inference, the two memories are individually stored, updated, and retrieved based on current observations, task history, and instructions, and their retrieved representations are fused via coarse- and fine-grained attention to guide mobile-arm diffusion policies. To support large-scale training and evaluation, we further introduce MoMani, an automated benchmark that generates expert-level long-horizon trajectories through multimodal large language model (MLLM)-guided planning and feedback-driven refinement, supplemented with real-robot demonstrations. Experiments in simulated and real-world settings show that EchoVLA improves long-horizon performance, reaching 0.52 SR on manipulation/navigation and 0.31 on mobile manipulation, exceeding $π_{0.5}$ by +0.08 and +0.11.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-22",
      "updated": "2025-11-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18112v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]mobile manipulation"
          ],
          "score": 12.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Unobservable Subspace Evolution and Alignment for Consistent Visual-Inertial Navigation",
      "authors": [
        "Chungeng Tian",
        "Fenghua He",
        "Ning Hao"
      ],
      "arxiv_id": "2511.17992v1",
      "summary": "The inconsistency issue in the Visual-Inertial Navigation System (VINS) is a long-standing and fundamental challenge. While existing studies primarily attribute the inconsistency to observability mismatch, these analyses are often based on simplified theoretical formulations that consider only prediction and SLAM correction. Such formulations fail to cover the non-standard estimation steps, such as MSCKF correction and delayed initialization, which are critical for practical VINS estimators. Furthermore, the lack of a comprehensive understanding of how inconsistency dynamically emerges across estimation steps has hindered the development of precise and efficient solutions. As a result, current approaches often face a trade-off between estimator accuracy, consistency, and implementation complexity. To address these limitations, this paper proposes a novel analysis framework termed Unobservable Subspace Evolution (USE), which systematically characterizes how the unobservable subspace evolves throughout the entire estimation pipeline by explicitly tracking changes in its evaluation points. This perspective sheds new light on how individual estimation steps contribute to inconsistency. Our analysis reveals that observability misalignment induced by certain steps is the antecedent of observability mismatch. Guided by this insight, we propose a simple yet effective solution paradigm, Unobservable Subspace Alignment (USA), which eliminates inconsistency by selectively intervening only in those estimation steps that induce misalignment. We design two USA methods: transformation-based and re-evaluation-based, both offering accurate and computationally lightweight solutions. Extensive simulations and real-world experiments validate the effectiveness of the proposed methods.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-22",
      "updated": "2025-11-22",
      "comment": "20 pages, 16 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17992v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "SLAM",
            "[T]visual-inertial",
            "[T]navigation"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Feed-Forward 3D Gaussian Splatting Compression with Long-Context Modeling",
      "authors": [
        "Zhening Liu",
        "Rui Song",
        "Yushi Huang",
        "Yingdong Hu",
        "Xinjie Zhang",
        "Jiawei Shao",
        "Zehong Lin",
        "Jun Zhang"
      ],
      "arxiv_id": "2512.00877v1",
      "summary": "3D Gaussian Splatting (3DGS) has emerged as a revolutionary 3D representation. However, its substantial data size poses a major barrier to widespread adoption. While feed-forward 3DGS compression offers a practical alternative to costly per-scene per-train compressors, existing methods struggle to model long-range spatial dependencies, due to the limited receptive field of transform coding networks and the inadequate context capacity in entropy models. In this work, we propose a novel feed-forward 3DGS compression framework that effectively models long-range correlations to enable highly compact and generalizable 3D representations. Central to our approach is a large-scale context structure that comprises thousands of Gaussians based on Morton serialization. We then design a fine-grained space-channel auto-regressive entropy model to fully leverage this expansive context. Furthermore, we develop an attention-based transform coding model to extract informative latent priors by aggregating features from a wide range of neighboring Gaussians. Our method yields a $20\\times$ compression ratio for 3DGS in a feed-forward inference and achieves state-of-the-art performance among generalizable codecs.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00877v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Asset-Driven Sematic Reconstruction of Dynamic Scene with Multi-Human-Object Interactions",
      "authors": [
        "Sandika Biswas",
        "Qianyi Wu",
        "Biplab Banerjee",
        "Hamid Rezatofighi"
      ],
      "arxiv_id": "2512.00547v1",
      "summary": "Real-world human-built environments are highly dynamic, involving multiple humans and their complex interactions with surrounding objects. While 3D geometry modeling of such scenes is crucial for applications like AR/VR, gaming, and embodied AI, it remains underexplored due to challenges like diverse motion patterns and frequent occlusions. Beyond novel view rendering, 3D Gaussian Splatting (GS) has demonstrated remarkable progress in producing detailed, high-quality surface geometry with fast optimization of the underlying structure. However, very few GS-based methods address multihuman, multiobject scenarios, primarily due to the above-mentioned inherent challenges. In a monocular setup, these challenges are further amplified, as maintaining structural consistency under severe occlusion becomes difficult when the scene is optimized solely based on GS-based rendering loss. To tackle the challenges of such a multihuman, multiobject dynamic scene, we propose a hybrid approach that effectively combines the advantages of 1) 3D generative models for generating high-fidelity meshes of the scene elements, 2) Semantic-aware deformation, \\ie rigid transformation of the rigid objects and LBS-based deformation of the humans, and mapping of the deformed high-fidelity meshes in the dynamic scene, and 3) GS-based optimization of the individual elements for further refining their alignments in the scene. Such a hybrid approach helps maintain the object structures even under severe occlusion and can produce multiview and temporally consistent geometry. We choose HOI-M3 for evaluation, as, to the best of our knowledge, this is the only dataset featuring multihuman, multiobject interactions in a dynamic scene. Our method outperforms the state-of-the-art method in producing better surface reconstruction of such scenes.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-29",
      "updated": "2025-11-29",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00547v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "gaussian splatting"
          ],
          "score": 4.0
        },
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "[T]human-object interaction",
            "HOI"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "3_perception_slam",
        "5_interaction_reaction"
      ]
    },
    {
      "title": "Cross-Temporal 3D Gaussian Splatting for Sparse-View Guided Scene Update",
      "authors": [
        "Zeyuan An",
        "Yanghang Xiao",
        "Zhiying Leng",
        "Frederick W. B. Li",
        "Xiaohui Liang"
      ],
      "arxiv_id": "2512.00534v1",
      "summary": "Maintaining consistent 3D scene representations over time is a significant challenge in computer vision. Updating 3D scenes from sparse-view observations is crucial for various real-world applications, including urban planning, disaster assessment, and historical site preservation, where dense scans are often unavailable or impractical. In this paper, we propose Cross-Temporal 3D Gaussian Splatting (Cross-Temporal 3DGS), a novel framework for efficiently reconstructing and updating 3D scenes across different time periods, using sparse images and previously captured scene priors. Our approach comprises three stages: 1) Cross-temporal camera alignment for estimating and aligning camera poses across different timestamps; 2) Interference-based confidence initialization to identify unchanged regions between timestamps, thereby guiding updates; and 3) Progressive cross-temporal optimization, which iteratively integrates historical prior information into the 3D scene to enhance reconstruction quality. Our method supports non-continuous capture, enabling not only updates using new sparse views to refine existing scenes, but also recovering past scenes from limited data with the help of current captures. Furthermore, we demonstrate the potential of this approach to achieve temporal changes using only sparse images, which can later be reconstructed into detailed 3D representations as needed. Experimental results show significant improvements over baseline methods in reconstruction quality and data efficiency, making this approach a promising solution for scene versioning, cross-temporal digital twins, and long-term spatial documentation.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-29",
      "updated": "2025-11-29",
      "comment": "AAAI2026 accepted",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00534v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting"
          ],
          "score": 14.0
        }
      ],
      "relevance_score": 14.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "CloudMamba: Grouped Selective State Spaces for Point Cloud Analysis",
      "authors": [
        "Kanglin Qu",
        "Pan Gao",
        "Qun Dai",
        "Zhanzhi Ye",
        "Rui Ye",
        "Yuanhao Sun"
      ],
      "arxiv_id": "2511.07823v1",
      "summary": "Due to the long-range modeling ability and linear complexity property, Mamba has attracted considerable attention in point cloud analysis. Despite some interesting progress, related work still suffers from imperfect point cloud serialization, insufficient high-level geometric perception, and overfitting of the selective state space model (S6) at the core of Mamba. To this end, we resort to an SSM-based point cloud network termed CloudMamba to address the above challenges. Specifically, we propose sequence expanding and sequence merging, where the former serializes points along each axis separately and the latter serves to fuse the corresponding higher-order features causally inferred from different sequences, enabling unordered point sets to adapt more stably to the causal nature of Mamba without parameters. Meanwhile, we design chainedMamba that chains the forward and backward processes in the parallel bidirectional Mamba, capturing high-level geometric information during scanning. In addition, we propose a grouped selective state space model (GS6) via parameter sharing on S6, alleviating the overfitting problem caused by the computational mode in S6. Experiments on various point cloud tasks validate CloudMamba's ability to achieve state-of-the-art results with significantly less complexity.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "Accepted by AAAI '26",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07823v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba",
            "SSM",
            "state space model"
          ],
          "score": 7.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Unified Humanoid Fall-Safety Policy from a Few Demonstrations",
      "authors": [
        "Zhengjie Xu",
        "Ye Li",
        "Kwan-yee Lin",
        "Stella X. Yu"
      ],
      "arxiv_id": "2511.07407v1",
      "summary": "Falling is an inherent risk of humanoid mobility. Maintaining stability is thus a primary safety focus in robot control and learning, yet no existing approach fully averts loss of balance. When instability does occur, prior work addresses only isolated aspects of falling: avoiding falls, choreographing a controlled descent, or standing up afterward. Consequently, humanoid robots lack integrated strategies for impact mitigation and prompt recovery when real falls defy these scripts. We aim to go beyond keeping balance to make the entire fall-and-recovery process safe and autonomous: prevent falls when possible, reduce impact when unavoidable, and stand up when fallen. By fusing sparse human demonstrations with reinforcement learning and an adaptive diffusion-based memory of safe reactions, we learn adaptive whole-body behaviors that unify fall prevention, impact mitigation, and rapid recovery in one policy. Experiments in simulation and on a Unitree G1 demonstrate robust sim-to-real transfer, lower impact forces, and consistently fast recovery across diverse disturbances, pointing towards safer, more resilient humanoids in real environments. Videos are available at https://firm2025.github.io/.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07407v1",
      "code_links": [
        {
          "url": "https://firm2025.github.io/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "sim-to-real",
            "unitree"
          ],
          "score": 12.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Text2Loc++: Generalizing 3D Point Cloud Localization from Natural Language",
      "authors": [
        "Yan Xia",
        "Letian Shi",
        "Yilin Di",
        "Joao F. Henriques",
        "Daniel Cremers"
      ],
      "arxiv_id": "2511.15308v1",
      "summary": "We tackle the problem of localizing 3D point cloud submaps using complex and diverse natural language descriptions, and present Text2Loc++, a novel neural network designed for effective cross-modal alignment between language and point clouds in a coarse-to-fine localization pipeline. To support benchmarking, we introduce a new city-scale dataset covering both color and non-color point clouds from diverse urban scenes, and organize location descriptions into three levels of linguistic complexity. In the global place recognition stage, Text2Loc++ combines a pretrained language model with a Hierarchical Transformer with Max pooling (HTM) for sentence-level semantics, and employs an attention-based point cloud encoder for spatial understanding. We further propose Masked Instance Training (MIT) to filter out non-aligned objects and improve multimodal robustness. To enhance the embedding space, we introduce Modality-aware Hierarchical Contrastive Learning (MHCL), incorporating cross-modal, submap-, text-, and instance-level losses. In the fine localization stage, we completely remove explicit text-instance matching and design a lightweight yet powerful framework based on Prototype-based Map Cloning (PMC) and a Cascaded Cross-Attention Transformer (CCAT). Extensive experiments on the KITTI360Pose dataset show that Text2Loc++ outperforms existing methods by up to 15%. In addition, the proposed model exhibits robust generalization when evaluated on the new dataset, effectively handling complex linguistic expressions and a wide variety of urban environments. The code and dataset will be made publicly available.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "This paper builds upon and extends our earlier conference paper Text2Loc presented at CVPR 2024",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15308v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "contrastive learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud",
            "[T]localization"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Symmetry-Breaking in Multi-Agent Navigation: Winding Number-Aware MPC with a Learned Topological Strategy",
      "authors": [
        "Tomoki Nakao",
        "Kazumi Kasaura",
        "Tadashi Kozuno"
      ],
      "arxiv_id": "2511.15239v1",
      "summary": "We address the fundamental challenge of resolving symmetry-induced deadlocks in distributed multi-agent navigation by proposing a new hierarchical navigation method. When multiple agents interact, it is inherently difficult for them to autonomously break the symmetry of deciding how to pass each other. To tackle this problem, we introduce an approach that quantifies cooperative symmetry-breaking strategies using a topological invariant called the winding number, and learns the strategies themselves through reinforcement learning. Our method features a hierarchical policy consisting of a learning-based Planner, which plans topological cooperative strategies, and a model-based Controller, which executes them. Through reinforcement learning, the Planner learns to produce two types of parameters for the Controller: one is the topological cooperative strategy represented by winding numbers, and the other is a set of dynamic weights that determine which agent interaction to prioritize in dense scenarios where multiple agents cross simultaneously. The Controller then generates collision-free and efficient motions based on the strategy and weights provided by the Planner. This hierarchical structure combines the flexible decision-making ability of learning-based methods with the reliability of model-based approaches. Simulation and real-world robot experiments demonstrate that our method outperforms existing baselines, particularly in dense environments, by efficiently avoiding collisions and deadlocks while achieving superior navigation performance. The code for the experiments is available at https://github.com/omron-sinicx/WNumMPC.",
      "categories": [
        "cs.RO",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "11 pages, 5 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15239v1",
      "code_links": [
        {
          "url": "https://github.com/omron-sinicx/WNumMPC",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]MPC"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "MARVO: Marine-Adaptive Radiance-aware Visual Odometry",
      "authors": [
        "Sacchin Sundar",
        "Atman Kikani",
        "Aaliya Alam",
        "Sumukh Shrote",
        "A. Nayeemulla Khan",
        "A. Shahina"
      ],
      "arxiv_id": "2511.22860v1",
      "summary": "Underwater visual localization remains challenging due to wavelength-dependent attenuation, poor texture, and non-Gaussian sensor noise. We introduce MARVO, a physics-aware, learning-integrated odometry framework that fuses underwater image formation modeling, differentiable matching, and reinforcement-learning optimization. At the front-end, we extend transformer-based feature matcher with a Physics Aware Radiance Adapter that compensates for color channel attenuation and contrast loss, yielding geometrically consistent feature correspondences under turbidity. These semi dense matches are combined with inertial and pressure measurements inside a factor-graph backend, where we formulate a keyframe-based visual-inertial-barometric estimator using GTSAM library. Each keyframe introduces (i) Pre-integrated IMU motion factors, (ii) MARVO-derived visual pose factors, and (iii) barometric depth priors, giving a full-state MAP estimate in real time. Lastly, we introduce a Reinforcement-Learningbased Pose-Graph Optimizer that refines global trajectories beyond local minima of classical least-squares solvers by learning optimal retraction actions on SE(2).",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-28",
      "updated": "2025-11-28",
      "comment": "10 pages, 5 figures, 3 tables, Submitted to CVPR2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22860v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]visual odometry",
            "visual-inertial",
            "metric depth",
            "localization"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "DAPointMamba: Domain Adaptive Point Mamba for Point Cloud Completion",
      "authors": [
        "Yinghui Li",
        "Qianyu Zhou",
        "Di Shao",
        "Hao Yang",
        "Ye Zhu",
        "Richard Dazeley",
        "Xuequan Lu"
      ],
      "arxiv_id": "2511.20278v1",
      "summary": "Domain adaptive point cloud completion (DA PCC) aims to narrow the geometric and semantic discrepancies between the labeled source and unlabeled target domains. Existing methods either suffer from limited receptive fields or quadratic complexity due to using CNNs or vision Transformers. In this paper, we present the first work that studies the adaptability of State Space Models (SSMs) in DA PCC and find that directly applying SSMs to DA PCC will encounter several challenges: directly serializing 3D point clouds into 1D sequences often disrupts the spatial topology and local geometric features of the target domain. Besides, the overlook of designs in the learning domain-agnostic representations hinders the adaptation performance. To address these issues, we propose a novel framework, DAPointMamba for DA PCC, that exhibits strong adaptability across domains and has the advantages of global receptive fields and efficient linear complexity. It has three novel modules. In particular, Cross-Domain Patch-Level Scanning introduces patch-level geometric correspondences, enabling effective local alignment. Cross-Domain Spatial SSM Alignment further strengthens spatial consistency by modulating patch features based on cross-domain similarity, effectively mitigating fine-grained structural discrepancies. Cross-Domain Channel SSM Alignment actively addresses global semantic gaps by interleaving and aligning feature channels. Extensive experiments on both synthetic and real-world benchmarks demonstrate that our DAPointMamba outperforms state-of-the-art methods with less computational complexity and inference latency.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "Accepted to AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20278v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba",
            "SSM",
            "state space model"
          ],
          "score": 7.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "GigaWorld-0: World Models as Data Engine to Empower Embodied AI",
      "authors": [
        "GigaWorld Team",
        "Angen Ye",
        "Boyuan Wang",
        "Chaojun Ni",
        "Guan Huang",
        "Guosheng Zhao",
        "Haoyun Li",
        "Jiagang Zhu",
        "Kerui Li",
        "Mengyuan Xu",
        "Qiuping Deng",
        "Siting Wang",
        "Wenkang Qin",
        "Xinze Chen",
        "Xiaofeng Wang",
        "Yankai Wang",
        "Yu Cao",
        "Yifan Chang",
        "Yuan Xu",
        "Yun Ye",
        "Yang Wang",
        "Yukun Zhou",
        "Zhengyuan Zhang",
        "Zhehao Dong",
        "Zheng Zhu"
      ],
      "arxiv_id": "2511.19861v2",
      "summary": "World models are emerging as a foundational paradigm for scalable, data-efficient embodied AI. In this work, we present GigaWorld-0, a unified world model framework designed explicitly as a data engine for Vision-Language-Action (VLA) learning. GigaWorld-0 integrates two synergistic components: GigaWorld-0-Video, which leverages large-scale video generation to produce diverse, texture-rich, and temporally coherent embodied sequences under fine-grained control of appearance, camera viewpoint, and action semantics; and GigaWorld-0-3D, which combines 3D generative modeling, 3D Gaussian Splatting reconstruction, physically differentiable system identification, and executable motion planning to ensure geometric consistency and physical realism. Their joint optimization enables the scalable synthesis of embodied interaction data that is visually compelling, spatially coherent, physically plausible, and instruction-aligned. Training at scale is made feasible through our efficient GigaTrain framework, which exploits FP8-precision and sparse attention to drastically reduce memory and compute requirements. We conduct comprehensive evaluations showing that GigaWorld-0 generates high-quality, diverse, and controllable data across multiple dimensions. Critically, VLA model (e.g., GigaBrain-0) trained on GigaWorld-0-generated data achieve strong real-world performance, significantly improving generalization and task success on physical robots without any real-world interaction during training.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-30",
      "comment": "Project Page: https://giga-world-0.github.io/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19861v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "motion planning"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "gaussian splatting"
          ],
          "score": 4.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "geometric consistency"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam",
        "7_retargeting"
      ]
    },
    {
      "title": "Accelerating Reinforcement Learning via Error-Related Human Brain Signals",
      "authors": [
        "Suzie Kim",
        "Hye-Bin Shin",
        "Hyo-Jeong Jang"
      ],
      "arxiv_id": "2511.18878v1",
      "summary": "In this work, we investigate how implicit neural feed back can accelerate reinforcement learning in complex robotic manipulation settings. While prior electroencephalogram (EEG) guided reinforcement learning studies have primarily focused on navigation or low-dimensional locomotion tasks, we aim to understand whether such neural evaluative signals can improve policy learning in high-dimensional manipulation tasks involving obstacles and precise end-effector control. We integrate error related potentials decoded from offline-trained EEG classifiers into reward shaping and systematically evaluate the impact of human-feedback weighting. Experiments on a 7-DoF manipulator in an obstacle-rich reaching environment show that neural feedback accelerates reinforcement learning and, depending on the human-feedback weighting, can yield task success rates that at times exceed those of sparse-reward baselines. Moreover, when applying the best-performing feedback weighting across all sub jects, we observe consistent acceleration of reinforcement learning relative to the sparse-reward setting. Furthermore, leave-one subject-out evaluations confirm that the proposed framework remains robust despite the intrinsic inter-individual variability in EEG decodability. Our findings demonstrate that EEG-based reinforcement learning can scale beyond locomotion tasks and provide a viable pathway for human-aligned manipulation skill acquisition.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18878v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "locomotion",
            "manipulation"
          ],
          "score": 4.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "policy learning",
            "reward shaping"
          ],
          "score": 7.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "DPNet: Doppler LiDAR Motion Planning for Highly-Dynamic Environments",
      "authors": [
        "Wei Zuo",
        "Zeyi Ren",
        "Chengyang Li",
        "Yikun Wang",
        "Mingle Zhao",
        "Shuai Wang",
        "Wei Sui",
        "Fei Gao",
        "Yik-Chung Wu",
        "Chengzhong Xu"
      ],
      "arxiv_id": "2512.00375v1",
      "summary": "Existing motion planning methods often struggle with rapid-motion obstacles due to an insufficient understanding of environmental changes. To address this limitation, we propose integrating motion planners with Doppler LiDARs which provide not only ranging measurements but also instantaneous point velocities. However, this integration is nontrivial due to the dual requirements of high accuracy and high frequency. To this end, we introduce Doppler Planning Network (DPNet), which tracks and reacts to rapid obstacles using Doppler model-based learning. Particularly, we first propose a Doppler Kalman neural network (D-KalmanNet) to track the future states of obstacles under partially observable Gaussian state space model. We then leverage the estimated motions to construct a Doppler-tuned model predictive control (DT-MPC) framework for ego-motion planning, enabling runtime auto-tuning of the controller parameters. These two model-based learners allow DPNet to maintain lightweight while learning fast environmental changes using minimum data, and achieve both high frequency and high accuracy in tracking and planning. Experiments on both high-fidelity simulator and real-world datasets demonstrate the superiority of DPNet over extensive benchmark schemes.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-29",
      "updated": "2025-11-29",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00375v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "MPC",
            "model predictive control",
            "[T]motion planning"
          ],
          "score": 10.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "state space model"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "ego-motion"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Gaussian See, Gaussian Do: Semantic 3D Motion Transfer from Multiview Video",
      "authors": [
        "Yarin Bekor",
        "Gal Michael Harari",
        "Or Perel",
        "Or Litany"
      ],
      "arxiv_id": "2511.14848v1",
      "summary": "We present Gaussian See, Gaussian Do, a novel approach for semantic 3D motion transfer from multiview video. Our method enables rig-free, cross-category motion transfer between objects with semantically meaningful correspondence. Building on implicit motion transfer techniques, we extract motion embeddings from source videos via condition inversion, apply them to rendered frames of static target shapes, and use the resulting videos to supervise dynamic 3D Gaussian Splatting reconstruction. Our approach introduces an anchor-based view-aware motion embedding mechanism, ensuring cross-view consistency and accelerating convergence, along with a robust 4D reconstruction pipeline that consolidates noisy supervision videos. We establish the first benchmark for semantic 3D motion transfer and demonstrate superior motion fidelity and structural consistency compared to adapted baselines. Code and data for this paper available at https://gsgd-motiontransfer.github.io/",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "SIGGRAPH Asia 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14848v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "gaussian splatting"
          ],
          "score": 4.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "[T]motion transfer"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 13.0,
      "hit_pillars": [
        "3_perception_slam",
        "7_retargeting"
      ]
    },
    {
      "title": "Collaborative Multi-Robot Non-Prehensile Manipulation via Flow-Matching Co-Generation",
      "authors": [
        "Yorai Shaoul",
        "Zhe Chen",
        "Mohamed Naveed Gul Mohamed",
        "Federico Pecora",
        "Maxim Likhachev",
        "Jiaoyang Li"
      ],
      "arxiv_id": "2511.10874v1",
      "summary": "Coordinating a team of robots to reposition multiple objects in cluttered environments requires reasoning jointly about where robots should establish contact, how to manipulate objects once contact is made, and how to navigate safely and efficiently at scale. Prior approaches typically fall into two extremes -- either learning the entire task or relying on privileged information and hand-designed planners -- both of which struggle to handle diverse objects in long-horizon tasks. To address these challenges, we present a unified framework for collaborative multi-robot, multi-object non-prehensile manipulation that integrates flow-matching co-generation with anonymous multi-robot motion planning. Within this framework, a generative model co-generates contact formations and manipulation trajectories from visual observations, while a novel motion planner conveys robots at scale. Crucially, the same planner also supports coordination at the object level, assigning manipulated objects to larger target structures and thereby unifying robot- and object-level reasoning within a single algorithmic framework. Experiments in challenging simulated environments demonstrate that our approach outperforms baselines in both motion planning and manipulation tasks, highlighting the benefits of generative co-design and integrated planning for scaling collaborative manipulation to complex multi-agent, multi-object settings. Visit gco-paper.github.io for code and demonstrations.",
      "categories": [
        "cs.RO",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10874v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "motion planning"
          ],
          "score": 8.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]flow matching"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 12.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Dynamics-Decoupled Trajectory Alignment for Sim-to-Real Transfer in Reinforcement Learning for Autonomous Driving",
      "authors": [
        "Thomas Steinecker",
        "Alexander Bienemann",
        "Denis Trescher",
        "Thorsten Luettel",
        "Mirko Maehlisch"
      ],
      "arxiv_id": "2511.07155v1",
      "summary": "Reinforcement learning (RL) has shown promise in robotics, but deploying RL on real vehicles remains challenging due to the complexity of vehicle dynamics and the mismatch between simulation and reality. Factors such as tire characteristics, road surface conditions, aerodynamic disturbances, and vehicle load make it infeasible to model real-world dynamics accurately, which hinders direct transfer of RL agents trained in simulation. In this paper, we present a framework that decouples motion planning from vehicle control through a spatial and temporal alignment strategy between a virtual vehicle and the real system. An RL agent is first trained in simulation using a kinematic bicycle model to output continuous control actions. Its behavior is then distilled into a trajectory-predicting agent that generates finite-horizon ego-vehicle trajectories, enabling synchronization between virtual and real vehicles. At deployment, a Stanley controller governs lateral dynamics, while longitudinal alignment is maintained through adaptive update mechanisms that compensate for deviations between virtual and real trajectories. We validate our approach on a real vehicle and demonstrate that the proposed alignment strategy enables robust zero-shot transfer of RL-based motion planning from simulation to reality, successfully decoupling high-level trajectory generation from low-level vehicle control.",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07155v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]sim-to-real",
            "motion planning"
          ],
          "score": 8.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 12.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "TiS-TSL: Image-Label Supervised Surgical Video Stereo Matching via Time-Switchable Teacher-Student Learning",
      "authors": [
        "Rui Wang",
        "Ying Zhou",
        "Hao Wang",
        "Wenwei Zhang",
        "Qiang Li",
        "Zhiwei Wang"
      ],
      "arxiv_id": "2511.06817v3",
      "summary": "Stereo matching in minimally invasive surgery (MIS) is essential for next-generation navigation and augmented reality. Yet, dense disparity supervision is nearly impossible due to anatomical constraints, typically limiting annotations to only a few image-level labels acquired before the endoscope enters deep body cavities. Teacher-Student Learning (TSL) offers a promising solution by leveraging a teacher trained on sparse labels to generate pseudo labels and associated confidence maps from abundant unlabeled surgical videos. However, existing TSL methods are confined to image-level supervision, providing only spatial confidence and lacking temporal consistency estimation. This absence of spatio-temporal reliability results in unstable disparity predictions and severe flickering artifacts across video frames. To overcome these challenges, we propose TiS-TSL, a novel time-switchable teacher-student learning framework for video stereo matching under minimal supervision. At its core is a unified model that operates in three distinct modes: Image-Prediction (IP), Forward Video-Prediction (FVP), and Backward Video-Prediction (BVP), enabling flexible temporal modeling within a single architecture. Enabled by this unified model, TiS-TSL adopts a two-stage learning strategy. The Image-to-Video (I2V) stage transfers sparse image-level knowledge to initialize temporal modeling. The subsequent Video-to-Video (V2V) stage refines temporal disparity predictions by comparing forward and backward predictions to calculate bidirectional spatio-temporal consistency. This consistency identifies unreliable regions across frames, filters noisy video-level pseudo labels, and enforces temporal coherence. Experimental results on two public datasets demonstrate that TiS-TSL exceeds other image-based state-of-the-arts by improving TEPE and EPE by at least 2.11% and 4.54%, respectively.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-12",
      "comment": "8 pages, 4 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06817v3",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]teacher-student"
          ],
          "score": 4.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]stereo matching",
            "navigation"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 12.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "ZeroDexGrasp: Zero-Shot Task-Oriented Dexterous Grasp Synthesis with Prompt-Based Multi-Stage Semantic Reasoning",
      "authors": [
        "Juntao Jian",
        "Yi-Lin Wei",
        "Chengjie Mou",
        "Yuhao Lin",
        "Xing Zhu",
        "Yujun Shen",
        "Wei-Shi Zheng",
        "Ruizhen Hu"
      ],
      "arxiv_id": "2511.13327v1",
      "summary": "Task-oriented dexterous grasping holds broad application prospects in robotic manipulation and human-object interaction. However, most existing methods still struggle to generalize across diverse objects and task instructions, as they heavily rely on costly labeled data to ensure task-specific semantic alignment. In this study, we propose \\textbf{ZeroDexGrasp}, a zero-shot task-oriented dexterous grasp synthesis framework integrating Multimodal Large Language Models with grasp refinement to generate human-like grasp poses that are well aligned with specific task objectives and object affordances. Specifically, ZeroDexGrasp employs prompt-based multi-stage semantic reasoning to infer initial grasp configurations and object contact information from task and object semantics, then exploits contact-guided grasp optimization to refine these poses for physical feasibility and task alignment. Experimental results demonstrate that ZeroDexGrasp enables high-quality zero-shot dexterous grasping on diverse unseen object categories and complex task requirements, advancing toward more generalizable and intelligent robotic grasping.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13327v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "grasping",
            "[T]grasp"
          ],
          "score": 10.0
        },
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "human-object interaction"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 12.5,
      "hit_pillars": [
        "1_robot_core",
        "5_interaction_reaction"
      ]
    },
    {
      "title": "Learning to Generate Human-Human-Object Interactions from Textual Descriptions",
      "authors": [
        "Jeonghyeon Na",
        "Sangwon Baik",
        "Inhee Lee",
        "Junyoung Lee",
        "Hanbyul Joo"
      ],
      "arxiv_id": "2511.20446v1",
      "summary": "The way humans interact with each other, including interpersonal distances, spatial configuration, and motion, varies significantly across different situations. To enable machines to understand such complex, context-dependent behaviors, it is essential to model multiple people in relation to the surrounding scene context. In this paper, we present a novel research problem to model the correlations between two people engaged in a shared interaction involving an object. We refer to this formulation as Human-Human-Object Interactions (HHOIs). To overcome the lack of dedicated datasets for HHOIs, we present a newly captured HHOIs dataset and a method to synthesize HHOI data by leveraging image generative models. As an intermediary, we obtain individual human-object interaction (HOIs) and human-human interaction (HHIs) from the HHOIs, and with these data, we train an text-to-HOI and text-to-HHI model using score-based diffusion model. Finally, we present a unified generative framework that integrates the two individual model, capable of synthesizing complete HHOIs in a single advanced sampling process. Our method extends HHOI generation to multi-human settings, enabling interactions involving more than two individuals. Experimental results show that our method generates realistic HHOIs conditioned on textual descriptions, outperforming previous approaches that focus only on single-human HOIs. Furthermore, we introduce multi-human motion generation involving objects as an application of our framework.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "Project Page: https://tlb-miss.github.io/hhoi/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20446v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "motion generation"
          ],
          "score": 2.5
        },
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "[T]human-object interaction",
            "HOI"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 12.5,
      "hit_pillars": [
        "4_motion_diffusion",
        "5_interaction_reaction"
      ]
    },
    {
      "title": "MotionDuet: Dual-Conditioned 3D Human Motion Generation with Video-Regularized Text Learning",
      "authors": [
        "Yi-Yang Zhang",
        "Tengjiao Sun",
        "Pengcheng Fang",
        "Deng-Bao Wang",
        "Xiaohao Cai",
        "Min-Ling Zhang",
        "Hansung Kim"
      ],
      "arxiv_id": "2511.18209v1",
      "summary": "3D Human motion generation is pivotal across film, animation, gaming, and embodied intelligence. Traditional 3D motion synthesis relies on costly motion capture, while recent work shows that 2D videos provide rich, temporally coherent observations of human behavior. Existing approaches, however, either map high-level text descriptions to motion or rely solely on video conditioning, leaving a gap between generated dynamics and real-world motion statistics. We introduce MotionDuet, a multimodal framework that aligns motion generation with the distribution of video-derived representations. In this dual-conditioning paradigm, video cues extracted from a pretrained model (e.g., VideoMAE) ground low-level motion dynamics, while textual prompts provide semantic intent. To bridge the distribution gap across modalities, we propose Dual-stream Unified Encoding and Transformation (DUET) and a Distribution-Aware Structural Harmonization (DASH) loss. DUET fuses video-informed cues into the motion latent space via unified encoding and dynamic attention, while DASH aligns motion trajectories with both distributional and structural statistics of video features. An auto-guidance mechanism further balances textual and visual signals by leveraging a weakened copy of the model, enhancing controllability without sacrificing diversity. Extensive experiments demonstrate that MotionDuet generates realistic and controllable human motions, surpassing strong state-of-the-art baselines.",
      "categories": [
        "cs.GR"
      ],
      "primary_category": "cs.GR",
      "published": "2025-11-22",
      "updated": "2025-11-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18209v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "motion synthesis",
            "[T]motion generation",
            "motion latent"
          ],
          "score": 12.5
        }
      ],
      "relevance_score": 12.5,
      "hit_pillars": [
        "4_motion_diffusion"
      ]
    },
    {
      "title": "Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions",
      "authors": [
        "Kaifeng Zhang",
        "Shuo Sha",
        "Hanxiao Jiang",
        "Matthew Loper",
        "Hyunjong Song",
        "Guangyan Cai",
        "Zhuo Xu",
        "Xiaochen Hu",
        "Changxi Zheng",
        "Yunzhu Li"
      ],
      "arxiv_id": "2511.04665v2",
      "summary": "Robotic manipulation policies are advancing rapidly, but their direct evaluation in the real world remains costly, time-consuming, and difficult to reproduce, particularly for tasks involving deformable objects. Simulation provides a scalable and systematic alternative, yet existing simulators often fail to capture the coupled visual and physical complexity of soft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from real-world videos and renders robots, objects, and environments with photorealistic fidelity using 3D Gaussian Splatting. We validate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and T-block pushing, demonstrating that simulated rollouts correlate strongly with real-world execution performance and reveal key behavioral patterns of learned policies. Our results suggest that combining physics-informed reconstruction with high-quality rendering enables reproducible, scalable, and accurate evaluation of robotic manipulation policies. Website: https://real2sim-eval.github.io/",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-06",
      "updated": "2025-11-10",
      "comment": "The first two authors contributed equally. Website: https://real2sim-eval.github.io/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04665v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "real2sim"
          ],
          "score": 4.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "[T]gaussian splatting"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "BoRe-Depth: Self-supervised Monocular Depth Estimation with Boundary Refinement for Embedded Systems",
      "authors": [
        "Chang Liu",
        "Juan Li",
        "Sheng Zhang",
        "Chang Liu",
        "Jie Li",
        "Xu Zhang"
      ],
      "arxiv_id": "2511.04388v1",
      "summary": "Depth estimation is one of the key technologies for realizing 3D perception in unmanned systems. Monocular depth estimation has been widely researched because of its low-cost advantage, but the existing methods face the challenges of poor depth estimation performance and blurred object boundaries on embedded systems. In this paper, we propose a novel monocular depth estimation model, BoRe-Depth, which contains only 8.7M parameters. It can accurately estimate depth maps on embedded systems and significantly improves boundary quality. Firstly, we design an Enhanced Feature Adaptive Fusion Module (EFAF) which adaptively fuses depth features to enhance boundary detail representation. Secondly, we integrate semantic knowledge into the encoder to improve the object recognition and boundary perception capabilities. Finally, BoRe-Depth is deployed on NVIDIA Jetson Orin, and runs efficiently at 50.7 FPS. We demonstrate that the proposed model significantly outperforms previous lightweight models on multiple challenging datasets, and we provide detailed ablation studies for the proposed methods. The code is available at https://github.com/liangxiansheng093/BoRe-Depth.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "8 pages, 5 figures, published to IROS 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04388v1",
      "code_links": [
        {
          "url": "https://github.com/liangxiansheng093/BoRe-Depth",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]depth estimation",
            "[T]monocular depth"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "MacroNav: Multi-Task Context Representation Learning Enables Efficient Navigation in Unknown Environments",
      "authors": [
        "Kuankuan Sima",
        "Longbin Tang",
        "Haozhe Ma",
        "Lin Zhao"
      ],
      "arxiv_id": "2511.04320v1",
      "summary": "Autonomous navigation in unknown environments requires compact yet expressive spatial understanding under partial observability to support high-level decision making. Existing approaches struggle to balance rich contextual representation with navigation efficiency. We present MacroNav, a learning-based navigation framework featuring two key components: (1) a lightweight context encoder trained via multi-task self-supervised learning to capture multi-scale, navigation-centric spatial representations; and (2) a reinforcement learning policy that seamlessly integrates these representations with graph-based reasoning for efficient action selection. Extensive experiments demonstrate the context encoder's efficient and robust environmental understanding. Real-world deployments further validate MacroNav's effectiveness, yielding significant gains over state-of-the-art navigation methods in both Success Rate (SR) and Success weighted by Path Length (SPL), while maintaining low computational cost. Code will be released upon acceptance.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04320v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "[T]representation learning"
          ],
          "score": 6.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "TurboMap: GPU-Accelerated Local Mapping for Visual SLAM",
      "authors": [
        "Parsa Hosseininejad",
        "Kimia Khabiri",
        "Shishir Gopinath",
        "Soudabeh Mohammadhashemi",
        "Karthik Dantu",
        "Steven Y. Ko"
      ],
      "arxiv_id": "2511.02036v1",
      "summary": "This paper presents TurboMap, a GPU-accelerated and CPU-optimized local mapping module for visual SLAM systems. We identify key performance bottlenecks in the local mapping process for visual SLAM and address them through targeted GPU and CPU optimizations. Specifically, we offload map point triangulation and fusion to the GPU, accelerate redundant keyframe culling on the CPU, and integrate a GPU-accelerated solver to speed up local bundle adjustment. Our implementation is built on top of ORB-SLAM3 and leverages CUDA for GPU programming. The experimental results show that TurboMap achieves an average speedup of 1.3x in the EuRoC dataset and 1.6x in the TUM-VI dataset in the local mapping module, on both desktop and embedded platforms, while maintaining the accuracy of the original system.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "Submitted to ICRA 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02036v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]visual SLAM",
            "[T]SLAM"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Floor Plan-Guided Visual Navigation Incorporating Depth and Directional Cues",
      "authors": [
        "Wei Huang",
        "Jiaxin Li",
        "Zang Wan",
        "Huijun Di",
        "Wei Liang",
        "Zhu Yang"
      ],
      "arxiv_id": "2511.01493v2",
      "summary": "Guiding an agent to a specific target in indoor environments based solely on RGB inputs and a floor plan is a promising yet challenging problem. Although existing methods have made significant progress, two challenges remain unresolved. First, the modality gap between egocentric RGB observations and the floor plan hinders the integration of visual and spatial information for both local obstacle avoidance and global planning. Second, accurate localization is critical for navigation performance, but remains challenging at deployment in unseen environments due to the lack of explicit geometric alignment between RGB inputs and floor plans. We propose a novel diffusion-based policy, denoted as GlocDiff, which integrates global path planning from the floor plan with local depth-aware features derived from RGB observations. The floor plan offers explicit global guidance, while the depth features provide implicit geometric cues, collectively enabling precise prediction of optimal navigation directions and robust obstacle avoidance. Moreover, GlocDiff introduces noise perturbation during training to enhance robustness against pose estimation errors, and we find that combining this with a relatively stable VO module during inference results in significantly improved navigation performance. Extensive experiments on the FloNa benchmark demonstrate GlocDiff's efficiency and effectiveness in achieving superior navigation performance, and the success of real-world deployments also highlights its potential for widespread practical applications.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01493v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "VO",
            "pose estimation",
            "localization",
            "[T]navigation"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "CM-LIUW-Odometry: Robust and High-Precision LiDAR-Inertial-UWB-Wheel Odometry for Extreme Degradation Coal Mine Tunnels",
      "authors": [
        "Kun Hu",
        "Menggang Li",
        "Zhiwen Jin",
        "Chaoquan Tang",
        "Eryi Hu",
        "Gongbo Zhou"
      ],
      "arxiv_id": "2511.01379v1",
      "summary": "Simultaneous Localization and Mapping (SLAM) in large-scale, complex, and GPS-denied underground coal mine environments presents significant challenges. Sensors must contend with abnormal operating conditions: GPS unavailability impedes scene reconstruction and absolute geographic referencing, uneven or slippery terrain degrades wheel odometer accuracy, and long, feature-poor tunnels reduce LiDAR effectiveness. To address these issues, we propose CoalMine-LiDAR-IMU-UWB-Wheel-Odometry (CM-LIUW-Odometry), a multimodal SLAM framework based on the Iterated Error-State Kalman Filter (IESKF). First, LiDAR-inertial odometry is tightly fused with UWB absolute positioning constraints to align the SLAM system with a global coordinate. Next, wheel odometer is integrated through tight coupling, enhanced by nonholonomic constraints (NHC) and vehicle lever arm compensation, to address performance degradation in areas beyond UWB measurement range. Finally, an adaptive motion mode switching mechanism dynamically adjusts the robot's motion mode based on UWB measurement range and environmental degradation levels. Experimental results validate that our method achieves superior accuracy and robustness in real-world underground coal mine scenarios, outperforming state-of-the-art approaches. We open source our code of this work on Github to benefit the robotics community.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "Accepted by IROS 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01379v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "SLAM",
            "[T]lidar-inertial",
            "scene reconstruction",
            "localization"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "TSPE-GS: Probabilistic Depth Extraction for Semi-Transparent Surface Reconstruction via 3D Gaussian Splatting",
      "authors": [
        "Zhiyuan Xu",
        "Nan Min",
        "Yuhang Guo",
        "Tong Wei"
      ],
      "arxiv_id": "2511.09944v1",
      "summary": "3D Gaussian Splatting offers a strong speed-quality trade-off but struggles to reconstruct semi-transparent surfaces because most methods assume a single depth per pixel, which fails when multiple surfaces are visible. We propose TSPE-GS (Transparent Surface Probabilistic Extraction for Gaussian Splatting), which uniformly samples transmittance to model a pixel-wise multi-modal distribution of opacity and depth, replacing the prior single-peak assumption and resolving cross-surface depth ambiguity. By progressively fusing truncated signed distance functions, TSPE-GS reconstructs external and internal surfaces separately within a unified framework. The method generalizes to other Gaussian-based reconstruction pipelines without extra training overhead. Extensive experiments on public and self-collected semi-transparent and opaque datasets show TSPE-GS significantly improves semi-transparent geometry reconstruction while maintaining performance on opaque scenes.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "AAAI26 Poster",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09944v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "[T]gaussian splatting"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Understanding the Representation of Older Adults in Motion Capture Locomotion Datasets",
      "authors": [
        "Yunkai Yu",
        "Yingying Wang",
        "Rong Zheng"
      ],
      "arxiv_id": "2511.11713v1",
      "summary": "The Internet of Things (IoT) sensors have been widely employed to capture human locomotions to enable applications such as activity recognition, human pose estimation, and fall detection. Motion capture (MoCap) systems are frequently used to generate ground truth annotations for human poses when training models with data from wearable or ambient sensors, and have been shown to be effective to synthesize data in these modalities. However, the representation of older adults, an increasingly important demographic in healthcare, in existing MoCap locomotion datasets has not been thoroughly examined. This work surveyed 41 publicly available datasets, identifying eight that include older adult motions and four that contain motions performed by younger actors annotated as old style. Older adults represent a small portion of participants overall, and few datasets provide full-body motion data for this group. To assess the fidelity of old-style walking motions, quantitative metrics are introduced, defining high fidelity as the ability to capture age-related differences relative to normative walking. Using gait parameters that are age-sensitive, robust to noise, and resilient to data scarcity, we found that old-style walking motions often exhibit overly controlled patterns and fail to faithfully characterize aging. These findings highlight the need for improved representation of older adults in motion datasets and establish a method to quantitatively evaluate the quality of old-style walking motions.",
      "categories": [
        "cs.CY",
        "cs.CV"
      ],
      "primary_category": "cs.CY",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "8 pages,4 figures, to be published in IEEE AIOT 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11713v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]locomotion",
            "gait",
            "walking"
          ],
          "score": 10.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "pose estimation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "OUGS: Active View Selection via Object-aware Uncertainty Estimation in 3DGS",
      "authors": [
        "Haiyi Li",
        "Qi Chen",
        "Denis Kalkofen",
        "Hsiang-Ting Chen"
      ],
      "arxiv_id": "2511.09397v2",
      "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have achieved state-of-the-art results for novel view synthesis. However, efficiently capturing high-fidelity reconstructions of specific objects within complex scenes remains a significant challenge. A key limitation of existing active reconstruction methods is their reliance on scene-level uncertainty metrics, which are often biased by irrelevant background clutter and lead to inefficient view selection for object-centric tasks. We present OUGS, a novel framework that addresses this challenge with a more principled, physically-grounded uncertainty formulation for 3DGS. Our core innovation is to derive uncertainty directly from the explicit physical parameters of the 3D Gaussian primitives (e.g., position, scale, rotation). By propagating the covariance of these parameters through the rendering Jacobian, we establish a highly interpretable uncertainty model. This foundation allows us to then seamlessly integrate semantic segmentation masks to produce a targeted, object-aware uncertainty score that effectively disentangles the object from its environment. This allows for a more effective active view selection strategy that prioritizes views critical to improving object fidelity. Experimental evaluations on public datasets demonstrate that our approach significantly improves the efficiency of the 3DGS reconstruction process and achieves higher quality for targeted objects compared to existing state-of-the-art methods, while also serving as a robust uncertainty estimator for the global scene.",
      "categories": [
        "cs.CV",
        "cs.CG",
        "cs.GR",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-12",
      "updated": "2025-12-15",
      "comment": "Conditionally accepted to Eurographics 2026 (five reviewers)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09397v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "[T]3DGS",
            "gaussian splatting",
            "novel view synthesis"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "RAPTR: Radar-based 3D Pose Estimation using Transformer",
      "authors": [
        "Sorachi Kato",
        "Ryoma Yataka",
        "Pu Perry Wang",
        "Pedro Miraldo",
        "Takuya Fujihashi",
        "Petros Boufounos"
      ],
      "arxiv_id": "2511.08387v1",
      "summary": "Radar-based indoor 3D human pose estimation typically relied on fine-grained 3D keypoint labels, which are costly to obtain especially in complex indoor settings involving clutter, occlusions, or multiple people. In this paper, we propose \\textbf{RAPTR} (RAdar Pose esTimation using tRansformer) under weak supervision, using only 3D BBox and 2D keypoint labels which are considerably easier and more scalable to collect. Our RAPTR is characterized by a two-stage pose decoder architecture with a pseudo-3D deformable attention to enhance (pose/joint) queries with multi-view radar features: a pose decoder estimates initial 3D poses with a 3D template loss designed to utilize the 3D BBox labels and mitigate depth ambiguities; and a joint decoder refines the initial poses with 2D keypoint labels and a 3D gravity loss. Evaluated on two indoor radar datasets, RAPTR outperforms existing methods, reducing joint position error by $34.3\\%$ on HIBER and $76.9\\%$ on MMVR. Our implementation is available at https://github.com/merlresearch/radar-pose-transformer.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "26 pages, Accepted to NeurIPS 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08387v1",
      "code_links": [
        {
          "url": "https://github.com/merlresearch/radar-pose-transformer",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]pose estimation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "[T]3D pose estimation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam",
        "6_video_extraction"
      ]
    },
    {
      "title": "WEDepth: Efficient Adaptation of World Knowledge for Monocular Depth Estimation",
      "authors": [
        "Gongshu Wang",
        "Zhirui Wang",
        "Kan Yang"
      ],
      "arxiv_id": "2511.08036v1",
      "summary": "Monocular depth estimation (MDE) has widely applicable but remains highly challenging due to the inherently ill-posed nature of reconstructing 3D scenes from single 2D images. Modern Vision Foundation Models (VFMs), pre-trained on large-scale diverse datasets, exhibit remarkable world understanding capabilities that benefit for various vision tasks. Recent studies have demonstrated significant improvements in MDE through fine-tuning these VFMs. Inspired by these developments, we propose WEDepth, a novel approach that adapts VFMs for MDE without modi-fying their structures and pretrained weights, while effec-tively eliciting and leveraging their inherent priors. Our method employs the VFM as a multi-level feature en-hancer, systematically injecting prior knowledge at differ-ent representation levels. Experiments on NYU-Depth v2 and KITTI datasets show that WEDepth establishes new state-of-the-art (SOTA) performance, achieving competi-tive results compared to both diffusion-based approaches (which require multiple forward passes) and methods pre-trained on relative depth. Furthermore, we demonstrate our method exhibits strong zero-shot transfer capability across diverse scenarios.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08036v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]depth estimation",
            "[T]monocular depth"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "UltraGS: Gaussian Splatting for Ultrasound Novel View Synthesis",
      "authors": [
        "Yuezhe Yang",
        "Wenjie Cai",
        "Dexin Yang",
        "Yufang Dong",
        "Xingbo Dong",
        "Zhe Jin"
      ],
      "arxiv_id": "2511.07743v1",
      "summary": "Ultrasound imaging is a cornerstone of non-invasive clinical diagnostics, yet its limited field of view complicates novel view synthesis. We propose \\textbf{UltraGS}, a Gaussian Splatting framework optimized for ultrasound imaging. First, we introduce a depth-aware Gaussian splatting strategy, where each Gaussian is assigned a learnable field of view, enabling accurate depth prediction and precise structural representation. Second, we design SH-DARS, a lightweight rendering function combining low-order spherical harmonics with ultrasound-specific wave physics, including depth attenuation, reflection, and scattering, to model tissue intensity accurately. Third, we contribute the Clinical Ultrasound Examination Dataset, a benchmark capturing diverse anatomical scans under real-world clinical protocols. Extensive experiments on three datasets demonstrate UltraGS's superiority, achieving state-of-the-art results in PSNR (up to 29.55), SSIM (up to 0.89), and MSE (as low as 0.002) while enabling real-time synthesis at 64.69 fps. The code and dataset are open-sourced at: https://github.com/Bean-Young/UltraGS.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "Under Review",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07743v1",
      "code_links": [
        {
          "url": "https://github.com/Bean-Young/UltraGS",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]gaussian splatting",
            "[T]novel view synthesis"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "LLM-GROP: Visually Grounded Robot Task and Motion Planning with Large Language Models",
      "authors": [
        "Xiaohan Zhang",
        "Yan Ding",
        "Yohei Hayamizu",
        "Zainab Altaweel",
        "Yifeng Zhu",
        "Yuke Zhu",
        "Peter Stone",
        "Chris Paxton",
        "Shiqi Zhang"
      ],
      "arxiv_id": "2511.07727v1",
      "summary": "Task planning and motion planning are two of the most important problems in robotics, where task planning methods help robots achieve high-level goals and motion planning methods maintain low-level feasibility. Task and motion planning (TAMP) methods interleave the two processes of task planning and motion planning to ensure goal achievement and motion feasibility. Within the TAMP context, we are concerned with the mobile manipulation (MoMa) of multiple objects, where it is necessary to interleave actions for navigation and manipulation.\n  In particular, we aim to compute where and how each object should be placed given underspecified goals, such as ``set up dinner table with a fork, knife and plate.'' We leverage the rich common sense knowledge from large language models (LLMs), e.g., about how tableware is organized, to facilitate both task-level and motion-level planning. In addition, we use computer vision methods to learn a strategy for selecting base positions to facilitate MoMa behaviors, where the base position corresponds to the robot's ``footprint'' and orientation in its operating space. Altogether, this article provides a principled TAMP framework for MoMa tasks that accounts for common sense about object rearrangement and is adaptive to novel situations that include many objects that need to be moved. We performed quantitative experiments in both real-world settings and simulated environments. We evaluated the success rate and efficiency in completing long-horizon object rearrangement tasks. While the robot completed 84.4\\% real-world object rearrangement trials, subjective human evaluations indicated that the robot's performance is still lower than experienced human waiters.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "10.1177/02783649251378196",
      "journal_ref": "The International Journal of Robotics Research, 2025, Vol. 0(0), pp. 1-19",
      "pdf_url": "https://arxiv.org/pdf/2511.07727v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "mobile manipulation",
            "[T]motion planning"
          ],
          "score": 10.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "GFix: Perceptually Enhanced Gaussian Splatting Video Compression",
      "authors": [
        "Siyue Teng",
        "Ge Gao",
        "Duolikun Danier",
        "Yuxuan Jiang",
        "Fan Zhang",
        "Thomas Davis",
        "Zoe Liu",
        "David Bull"
      ],
      "arxiv_id": "2511.06953v1",
      "summary": "3D Gaussian Splatting (3DGS) enhances 3D scene reconstruction through explicit representation and fast rendering, demonstrating potential benefits for various low-level vision tasks, including video compression. However, existing 3DGS-based video codecs generally exhibit more noticeable visual artifacts and relatively low compression ratios. In this paper, we specifically target the perceptual enhancement of 3DGS-based video compression, based on the assumption that artifacts from 3DGS rendering and quantization resemble noisy latents sampled during diffusion training. Building on this premise, we propose a content-adaptive framework, GFix, comprising a streamlined, single-step diffusion model that serves as an off-the-shelf neural enhancer. Moreover, to increase compression efficiency, We propose a modulated LoRA scheme that freezes the low-rank decompositions and modulates the intermediate hidden states, thereby achieving efficient adaptation of the diffusion backbone with highly compressible updates. Experimental results show that GFix delivers strong perceptual quality enhancement, outperforming GSVC with up to 72.1% BD-rate savings in LPIPS and 21.4% in FID.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06953v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting",
            "scene reconstruction"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Inpaint360GS: Efficient Object-Aware 3D Inpainting via Gaussian Splatting for 360° Scenes",
      "authors": [
        "Shaoxiang Wang",
        "Shihong Zhang",
        "Christen Millerdurai",
        "Rüdiger Westermann",
        "Didier Stricker",
        "Alain Pagani"
      ],
      "arxiv_id": "2511.06457v1",
      "summary": "Despite recent advances in single-object front-facing inpainting using NeRF and 3D Gaussian Splatting (3DGS), inpainting in complex 360° scenes remains largely underexplored. This is primarily due to three key challenges: (i) identifying target objects in the 3D field of 360° environments, (ii) dealing with severe occlusions in multi-object scenes, which makes it hard to define regions to inpaint, and (iii) maintaining consistent and high-quality appearance across views effectively. To tackle these challenges, we propose Inpaint360GS, a flexible 360° editing framework based on 3DGS that supports multi-object removal and high-fidelity inpainting in 3D space. By distilling 2D segmentation into 3D and leveraging virtual camera views for contextual guidance, our method enables accurate object-level editing and consistent scene completion. We further introduce a new dataset tailored for 360° inpainting, addressing the lack of ground truth object-free scenes. Experiments demonstrate that Inpaint360GS outperforms existing baselines and achieves state-of-the-art performance. Project page: https://dfki-av.github.io/inpaint360gs/",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-09",
      "updated": "2025-11-09",
      "comment": "WACV 2026, project page: https://dfki-av.github.io/inpaint360gs/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06457v1",
      "code_links": [
        {
          "url": "https://dfki-av.github.io/inpaint360gs/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting",
            "NeRF"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "VDNeRF: Vision-only Dynamic Neural Radiance Field for Urban Scenes",
      "authors": [
        "Zhengyu Zou",
        "Jingfeng Li",
        "Hao Li",
        "Xiaolei Hou",
        "Jinwen Hu",
        "Jingkun Chen",
        "Lechao Cheng",
        "Dingwen Zhang"
      ],
      "arxiv_id": "2511.06408v1",
      "summary": "Neural Radiance Fields (NeRFs) implicitly model continuous three-dimensional scenes using a set of images with known camera poses, enabling the rendering of photorealistic novel views. However, existing NeRF-based methods encounter challenges in applications such as autonomous driving and robotic perception, primarily due to the difficulty of capturing accurate camera poses and limitations in handling large-scale dynamic environments. To address these issues, we propose Vision-only Dynamic NeRF (VDNeRF), a method that accurately recovers camera trajectories and learns spatiotemporal representations for dynamic urban scenes without requiring additional camera pose information or expensive sensor data. VDNeRF employs two separate NeRF models to jointly reconstruct the scene. The static NeRF model optimizes camera poses and static background, while the dynamic NeRF model incorporates the 3D scene flow to ensure accurate and consistent reconstruction of dynamic objects. To address the ambiguity between camera motion and independent object motion, we design an effective and powerful training framework to achieve robust camera pose estimation and self-supervised decomposition of static and dynamic elements in a scene. Extensive evaluations on mainstream urban driving datasets demonstrate that VDNeRF surpasses state-of-the-art NeRF-based pose-free methods in both camera pose estimation and dynamic novel view synthesis.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-09",
      "updated": "2025-11-09",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06408v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "NeRF",
            "[T]neural radiance",
            "novel view synthesis",
            "pose estimation"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Physics-Informed Deformable Gaussian Splatting: Towards Unified Constitutive Laws for Time-Evolving Material Field",
      "authors": [
        "Haoqin Hong",
        "Ding Fan",
        "Fubin Dou",
        "Zhi-Li Zhou",
        "Haoran Sun",
        "Congcong Zhu",
        "Jingrun Chen"
      ],
      "arxiv_id": "2511.06299v3",
      "summary": "Recently, 3D Gaussian Splatting (3DGS), an explicit scene representation technique, has shown significant promise for dynamic novel-view synthesis from monocular video input. However, purely data-driven 3DGS often struggles to capture the diverse physics-driven motion patterns in dynamic scenes. To fill this gap, we propose Physics-Informed Deformable Gaussian Splatting (PIDG), which treats each Gaussian particle as a Lagrangian material point with time-varying constitutive parameters and is supervised by 2D optical flow via motion projection. Specifically, we adopt static-dynamic decoupled 4D decomposed hash encoding to reconstruct geometry and motion efficiently. Subsequently, we impose the Cauchy momentum residual as a physics constraint, enabling independent prediction of each particle's velocity and constitutive stress via a time-evolving material field. Finally, we further supervise data fitting by matching Lagrangian particle flow to camera-compensated optical flow, which accelerates convergence and improves generalization. Experiments on a custom physics-driven dataset as well as on standard synthetic and real-world datasets demonstrate significant gains in physical consistency and monocular dynamic reconstruction quality.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-09",
      "updated": "2025-11-22",
      "comment": "Accepted by AAAI-26",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06299v3",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting",
            "optical flow"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Affordance-Guided Coarse-to-Fine Exploration for Base Placement in Open-Vocabulary Mobile Manipulation",
      "authors": [
        "Tzu-Jung Lin",
        "Jia-Fong Yeh",
        "Hung-Ting Su",
        "Chung-Yi Lin",
        "Yi-Ting Chen",
        "Winston H. Hsu"
      ],
      "arxiv_id": "2511.06240v1",
      "summary": "In open-vocabulary mobile manipulation (OVMM), task success often hinges on the selection of an appropriate base placement for the robot. Existing approaches typically navigate to proximity-based regions without considering affordances, resulting in frequent manipulation failures. We propose Affordance-Guided Coarse-to-Fine Exploration, a zero-shot framework for base placement that integrates semantic understanding from vision-language models (VLMs) with geometric feasibility through an iterative optimization process. Our method constructs cross-modal representations, namely Affordance RGB and Obstacle Map+, to align semantics with spatial context. This enables reasoning that extends beyond the egocentric limitations of RGB perception. To ensure interaction is guided by task-relevant affordances, we leverage coarse semantic priors from VLMs to guide the search toward task-relevant regions and refine placements with geometric constraints, thereby reducing the risk of convergence to local optima. Evaluated on five diverse open-vocabulary mobile manipulation tasks, our system achieves an 85% success rate, significantly outperforming classical geometric planners and VLM-based methods. This demonstrates the promise of affordance-aware and multimodal reasoning for generalizable, instruction-conditioned planning in OVMM.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-09",
      "updated": "2025-11-09",
      "comment": "Accepted to AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06240v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]mobile manipulation"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Lite Any Stereo: Efficient Zero-Shot Stereo Matching",
      "authors": [
        "Junpeng Jing",
        "Weixun Luo",
        "Ye Mao",
        "Krystian Mikolajczyk"
      ],
      "arxiv_id": "2511.16555v1",
      "summary": "Recent advances in stereo matching have focused on accuracy, often at the cost of significantly increased model size. Traditionally, the community has regarded efficient models as incapable of zero-shot ability due to their limited capacity. In this paper, we introduce Lite Any Stereo, a stereo depth estimation framework that achieves strong zero-shot generalization while remaining highly efficient. To this end, we design a compact yet expressive backbone to ensure scalability, along with a carefully crafted hybrid cost aggregation module. We further propose a three-stage training strategy on million-scale data to effectively bridge the sim-to-real gap. Together, these components demonstrate that an ultra-light model can deliver strong generalization, ranking 1st across four widely used real-world benchmarks. Remarkably, our model attains accuracy comparable to or exceeding state-of-the-art non-prior-based accurate methods while requiring less than 1% computational cost, setting a new standard for efficient stereo matching.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16555v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "sim-to-real"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation",
            "stereo depth",
            "[T]stereo matching"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Physics-Informed Machine Learning for Efficient Sim-to-Real Data Augmentation in Micro-Object Pose Estimation",
      "authors": [
        "Zongcai Tan",
        "Lan Wei",
        "Dandan Zhang"
      ],
      "arxiv_id": "2511.16494v1",
      "summary": "Precise pose estimation of optical microrobots is essential for enabling high-precision object tracking and autonomous biological studies. However, current methods rely heavily on large, high-quality microscope image datasets, which are difficult and costly to acquire due to the complexity of microrobot fabrication and the labour-intensive labelling. Digital twin systems offer a promising path for sim-to-real data augmentation, yet existing techniques struggle to replicate complex optical microscopy phenomena, such as diffraction artifacts and depth-dependent imaging.This work proposes a novel physics-informed deep generative learning framework that, for the first time, integrates wave optics-based physical rendering and depth alignment into a generative adversarial network (GAN), to synthesise high-fidelity microscope images for microrobot pose estimation efficiently. Our method improves the structural similarity index (SSIM) by 35.6% compared to purely AI-driven methods, while maintaining real-time rendering speeds (0.022 s/frame).The pose estimator (CNN backbone) trained on our synthetic data achieves 93.9%/91.9% (pitch/roll) accuracy, just 5.0%/5.4% (pitch/roll) below that of an estimator trained exclusively on real data. Furthermore, our framework generalises to unseen poses, enabling data augmentation and robust pose estimation for novel microrobot configurations without additional training data.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16494v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]sim-to-real"
          ],
          "score": 6.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]pose estimation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "EGSA-PT:Edge-Guided Spatial Attention with Progressive Training for Monocular Depth Estimation and Segmentation of Transparent Objects",
      "authors": [
        "Gbenga Omotara",
        "Ramy Farag",
        "Seyed Mohamad Ali Tousi",
        "G. N. DeSouza"
      ],
      "arxiv_id": "2511.14970v1",
      "summary": "Transparent object perception remains a major challenge in computer vision research, as transparency confounds both depth estimation and semantic segmentation. Recent work has explored multi-task learning frameworks to improve robustness, yet negative cross-task interactions often hinder performance. In this work, we introduce Edge-Guided Spatial Attention (EGSA), a fusion mechanism designed to mitigate destructive interactions by incorporating boundary information into the fusion between semantic and geometric features. On both Syn-TODD and ClearPose benchmarks, EGSA consistently improved depth accuracy over the current state of the art method (MODEST), while preserving competitive segmentation performance, with the largest improvements appearing in transparent regions. Besides our fusion design, our second contribution is a multi-modal progressive training strategy, where learning transitions from edges derived from RGB images to edges derived from predicted depth images. This approach allows the system to bootstrap learning from the rich textures contained in RGB images, and then switch to more relevant geometric content in depth maps, while it eliminates the need for ground-truth depth at training time. Together, these contributions highlight edge-guided fusion as a robust approach capable of improving transparent object perception.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14970v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]depth estimation",
            "[T]monocular depth"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Cheating Stereo Matching in Full-scale: Physical Adversarial Attack against Binocular Depth Estimation in Autonomous Driving",
      "authors": [
        "Kangqiao Zhao",
        "Shuo Huai",
        "Xurui Song",
        "Jun Luo"
      ],
      "arxiv_id": "2511.14386v3",
      "summary": "Though deep neural models adopted to realize the perception of autonomous driving have proven vulnerable to adversarial examples, known attacks often leverage 2D patches and target mostly monocular perception. Therefore, the effectiveness of Physical Adversarial Examples (PAEs) on stereo-based binocular depth estimation remains largely unexplored. To this end, we propose the first texture-enabled physical adversarial attack against stereo matching models in the context of autonomous driving. Our method employs a 3D PAE with global camouflage texture rather than a local 2D patch-based one, ensuring both visual consistency and attack effectiveness across different viewpoints of stereo cameras. To cope with the disparity effect of these cameras, we also propose a new 3D stereo matching rendering module that allows the PAE to be aligned with real-world positions and headings in binocular vision. We further propose a novel merging attack that seamlessly blends the target into the environment through fine-grained PAE optimization. It has significantly enhanced stealth and lethality upon existing hiding attacks that fail to get seamlessly merged into the background. Extensive evaluations show that our PAEs can successfully fool the stereo models into producing erroneous depth information.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-26",
      "comment": "AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14386v3",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]depth estimation",
            "[T]stereo matching"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "IBGS: Image-Based Gaussian Splatting",
      "authors": [
        "Hoang Chuong Nguyen",
        "Wei Mao",
        "Jose M. Alvarez",
        "Miaomiao Liu"
      ],
      "arxiv_id": "2511.14357v1",
      "summary": "3D Gaussian Splatting (3DGS) has recently emerged as a fast, high-quality method for novel view synthesis (NVS). However, its use of low-degree spherical harmonics limits its ability to capture spatially varying color and view-dependent effects such as specular highlights. Existing works augment Gaussians with either a global texture map, which struggles with complex scenes, or per-Gaussian texture maps, which introduces high storage overhead. We propose Image-Based Gaussian Splatting, an efficient alternative that leverages high-resolution source images for fine details and view-specific color modeling. Specifically, we model each pixel color as a combination of a base color from standard 3DGS rendering and a learned residual inferred from neighboring training images. This promotes accurate surface alignment and enables rendering images of high-frequency details and accurate view-dependent effects. Experiments on standard NVS benchmarks show that our method significantly outperforms prior Gaussian Splatting approaches in rendering quality, without increasing the storage footprint.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "Accepted to NeurIPS 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14357v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting",
            "novel view synthesis"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "ManipShield: A Unified Framework for Image Manipulation Detection, Localization and Explanation",
      "authors": [
        "Zitong Xu",
        "Huiyu Duan",
        "Xiaoyu Wang",
        "Zhaolin Cai",
        "Kaiwei Zhang",
        "Qiang Hu",
        "Jing Liu",
        "Xiongkuo Min",
        "Guangtao Zhai"
      ],
      "arxiv_id": "2511.14259v2",
      "summary": "With the rapid advancement of generative models, powerful image editing methods now enable diverse and highly realistic image manipulations that far surpass traditional deepfake techniques, posing new challenges for manipulation detection. Existing image manipulation detection and localization (IMDL) benchmarks suffer from limited content diversity, narrow generative-model coverage, and insufficient interpretability, which hinders the generalization and explanation capabilities of current manipulation detection methods. To address these limitations, we introduce \\textbf{ManipBench}, a large-scale benchmark for image manipulation detection and localization focusing on AI-edited images. ManipBench contains over 450K manipulated images produced by 25 state-of-the-art image editing models across 12 manipulation categories, among which 100K images are further annotated with bounding boxes, judgment cues, and textual explanations to support interpretable detection. Building upon ManipBench, we propose \\textbf{ManipShield}, an all-in-one model based on a Multimodal Large Language Model (MLLM) that leverages contrastive LoRA fine-tuning and task-specific decoders to achieve unified image manipulation detection, localization, and explanation. Extensive experiments on ManipBench and several public datasets demonstrate that ManipShield achieves state-of-the-art performance and exhibits strong generality to unseen manipulation models. Both ManipBench and ManipShield will be released upon publication.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14259v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "LIO-MARS: Non-uniform Continuous-time Trajectories for Real-time LiDAR-Inertial-Odometry",
      "authors": [
        "Jan Quenzel",
        "Sven Behnke"
      ],
      "arxiv_id": "2511.13985v1",
      "summary": "Autonomous robotic systems heavily rely on environment knowledge to safely navigate. For search & rescue, a flying robot requires robust real-time perception, enabled by complementary sensors. IMU data constrains acceleration and rotation, whereas LiDAR measures accurate distances around the robot. Building upon the LiDAR odometry MARS, our LiDAR-inertial odometry (LIO) jointly aligns multi-resolution surfel maps with a Gaussian mixture model (GMM) using a continuous-time B-spline trajectory. Our new scan window uses non-uniform temporal knot placement to ensure continuity over the whole trajectory without additional scan delay. Moreover, we accelerate essential covariance and GMM computations with Kronecker sums and products by a factor of 3.3. An unscented transform de-skews surfels, while a splitting into intra-scan segments facilitates motion compensation during spline optimization. Complementary soft constraints on relative poses and preintegrated IMU pseudo-measurements further improve robustness and accuracy. Extensive evaluation showcases the state-of-the-art quality of our LIO-MARS w.r.t. recent LIO systems on various handheld, ground and aerial vehicle-based datasets.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "submitted to T-RO, 19 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13985v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]lidar-inertial",
            "[T]LIO"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "ActiveGrasp: Information-Guided Active Grasping with Calibrated Energy-based Model",
      "authors": [
        "Boshu Lei",
        "Wen Jiang",
        "Kostas Daniilidis"
      ],
      "arxiv_id": "2511.12795v1",
      "summary": "Grasping in a densely cluttered environment is a challenging task for robots. Previous methods tried to solve this problem by actively gathering multiple views before grasp pose generation. However, they either overlooked the importance of the grasp distribution for information gain estimation or relied on the projection of the grasp distribution, which ignores the structure of grasp poses on the SE(3) manifold. To tackle these challenges, we propose a calibrated energy-based model for grasp pose generation and an active view selection method that estimates information gain from grasp distribution. Our energy-based model captures the multi-modality nature of grasp distribution on the SE(3) manifold. The energy level is calibrated to the success rate of grasps so that the predicted distribution aligns with the real distribution. The next best view is selected by estimating the information gain for grasp from the calibrated distribution conditioned on the reconstructed environment, which could efficiently drive the robot to explore affordable parts of the target object. Experiments on simulated environments and real robot setups demonstrate that our model could successfully grasp objects in a cluttered environment with limited view budgets compared to previous state-of-the-art models. Our simulated environment can serve as a reproducible platform for future research on active grasping. The source code of our paper will be made public when the paper is released to the public.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-16",
      "updated": "2025-11-16",
      "comment": "under review",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12795v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]grasping",
            "[T]grasp"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry",
      "authors": [
        "Cheng Liao"
      ],
      "arxiv_id": "2511.12653v1",
      "summary": "Deep learning-based Visual SLAM (vSLAM) systems exhibit exceptional geometric reasoning capabilities, yet their prohibitive computational overhead severely restricts deployment on resource-constrained autonomous platforms. This paper presents a hierarchical quantization optimization framework, DPVO-QAT++ (DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry). Through the synergistic integration of learnable scale parameterization, a heterogeneous precision design for the Visual Odometry (VO) front-end and back-end (front-end floating-point fake quantization with FP16/FP32; back-end full precision), and GPU-native kernel fusion for fake quantization (custom CUDA kernels), our framework significantly reduces memory footprint and increases processing speed while preserving the trajectory accuracy of the original model. On the TartanAir dataset, our framework achieves an average FPS increase of 52.1%, a 29.1% reduction in median latency, and a 64.9% reduction in peak GPU memory reservation, while maintaining trajectory accuracy (ATE) comparable to the original DPVO model across 32 validation sequences. On the EuRoC dataset, it realizes an average FPS increase of 30.1%, a 23.1% reduction in median latency, and a 37.7% reduction in peak GPU memory reservation, maintaining comparable trajectory accuracy (ATE) across 11 validation sequences. Experimental results demonstrate that DPVO-QAT++ effectively bridges the gap between high-precision deep VO and the efficiency requirements for practical deployment, offering a viable engineering paradigm for the application of this technology on real-world embedded platforms.\n  Keywords: Visual Odometry, Heterogeneous Precision Architecture, Quantization-Aware Training, CUDA Kernel Fusion, Scale-Only Training, Deep Patch Visual Odometry, GPU-Native Kernel Fusion.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-16",
      "updated": "2025-11-16",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12653v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]visual odometry",
            "visual SLAM",
            "SLAM",
            "VO"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Multi-modal On-Device Learning for Monocular Depth Estimation on Ultra-low-power MCUs",
      "authors": [
        "Davide Nadalini",
        "Manuele Rusci",
        "Elia Cereda",
        "Luca Benini",
        "Francesco Conti",
        "Daniele Palossi"
      ],
      "arxiv_id": "2512.00086v1",
      "summary": "Monocular depth estimation (MDE) plays a crucial role in enabling spatially-aware applications in Ultra-low-power (ULP) Internet-of-Things (IoT) platforms. However, the limited number of parameters of Deep Neural Networks for the MDE task, designed for IoT nodes, results in severe accuracy drops when the sensor data observed in the field shifts significantly from the training dataset. To address this domain shift problem, we present a multi-modal On-Device Learning (ODL) technique, deployed on an IoT device integrating a Greenwaves GAP9 MicroController Unit (MCU), a 80 mW monocular camera and a 8 x 8 pixel depth sensor, consuming $\\approx$300mW. In its normal operation, this setup feeds a tiny 107 k-parameter $μ$PyD-Net model with monocular images for inference. The depth sensor, usually deactivated to minimize energy consumption, is only activated alongside the camera to collect pseudo-labels when the system is placed in a new environment. Then, the fine-tuning task is performed entirely on the MCU, using the new data. To optimize our backpropagation-based on-device training, we introduce a novel memory-driven sparse update scheme, which minimizes the fine-tuning memory to 1.2 MB, 2.2x less than a full update, while preserving accuracy (i.e., only 2% and 1.5% drops on the KITTI and NYUv2 datasets). Our in-field tests demonstrate, for the first time, that ODL for MDE can be performed in 17.8 minutes on the IoT node, reducing the root mean squared error from 4.9 to 0.6m with only 3 k self-labeled samples, collected in a real-life deployment scenario.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "14 pages, 9 figures, 3 tables. Associated open-source release available at: https://github.com/dnadalini/ondevice_learning_for_monocular_depth_estimation",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00086v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]depth estimation",
            "[T]monocular depth"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "From Passive Perception to Active Memory: A Weakly Supervised Image Manipulation Localization Framework Driven by Coarse-Grained Annotations",
      "authors": [
        "Zhiqing Guo",
        "Dongdong Xi",
        "Songlin Li",
        "Gaobo Yang"
      ],
      "arxiv_id": "2511.20359v1",
      "summary": "Image manipulation localization (IML) faces a fundamental trade-off between minimizing annotation cost and achieving fine-grained localization accuracy. Existing fully-supervised IML methods depend heavily on dense pixel-level mask annotations, which limits scalability to large datasets or real-world deployment.In contrast, the majority of existing weakly-supervised IML approaches are based on image-level labels, which greatly reduce annotation effort but typically lack precise spatial localization. To address this dilemma, we propose BoxPromptIML, a novel weakly-supervised IML framework that effectively balances annotation cost and localization performance. Specifically, we propose a coarse region annotation strategy, which can generate relatively accurate manipulation masks at lower cost. To improve model efficiency and facilitate deployment, we further design an efficient lightweight student model, which learns to perform fine-grained localization through knowledge distillation from a fixed teacher model based on the Segment Anything Model (SAM). Moreover, inspired by the human subconscious memory mechanism, our feature fusion module employs a dual-guidance strategy that actively contextualizes recalled prototypical patterns with real-time observational cues derived from the input. Instead of passive feature extraction, this strategy enables a dynamic process of knowledge recollection, where long-term memory is adapted to the specific context of the current image, significantly enhancing localization accuracy and robustness. Extensive experiments across both in-distribution and out-of-distribution datasets show that BoxPromptIML outperforms or rivals fully-supervised models, while maintaining strong generalization, low annotation cost, and efficient deployment characteristics.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "Accepted by AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20359v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Redefining Radar Segmentation: Simultaneous Static-Moving Segmentation and Ego-Motion Estimation using Radar Point Clouds",
      "authors": [
        "Simin Zhu",
        "Satish Ravindran",
        "Alexander Yarovoy",
        "Francesco Fioranelli"
      ],
      "arxiv_id": "2511.20003v1",
      "summary": "Conventional radar segmentation research has typically focused on learning category labels for different moving objects. Although fundamental differences between radar and optical sensors lead to differences in the reliability of predicting accurate and consistent category labels, a review of common radar perception tasks in automotive reveals that determining whether an object is moving or static is a prerequisite for most tasks. To fill this gap, this study proposes a neural network based solution that can simultaneously segment static and moving objects from radar point clouds. Furthermore, since the measured radial velocity of static objects is correlated with the motion of the radar, this approach can also estimate the instantaneous 2D velocity of the moving platform or vehicle (ego motion). However, despite performing dual tasks, the proposed method employs very simple yet effective building blocks for feature extraction: multi layer perceptrons (MLPs) and recurrent neural networks (RNNs). In addition to being the first of its kind in the literature, the proposed method also demonstrates the feasibility of extracting the information required for the dual task directly from unprocessed point clouds, without the need for cloud aggregation, Doppler compensation, motion compensation, or any other intermediate signal processing steps. To measure its performance, this study introduces a set of novel evaluation metrics and tests the proposed method using a challenging real world radar dataset, RadarScenes. The results show that the proposed method not only performs well on the dual tasks, but also has broad application potential in other radar perception tasks.",
      "categories": [
        "eess.SP",
        "cs.CV"
      ],
      "primary_category": "eess.SP",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "16 pages, 9 figures, under review at IEEE Transactions on Radar Systems",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20003v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud",
            "[T]ego-motion"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "IDSplat: Instance-Decomposed 3D Gaussian Splatting for Driving Scenes",
      "authors": [
        "Carl Lindström",
        "Mahan Rafidashti",
        "Maryam Fatemi",
        "Lars Hammarstrand",
        "Martin R. Oswald",
        "Lennart Svensson"
      ],
      "arxiv_id": "2511.19235v1",
      "summary": "Reconstructing dynamic driving scenes is essential for developing autonomous systems through sensor-realistic simulation. Although recent methods achieve high-fidelity reconstructions, they either rely on costly human annotations for object trajectories or use time-varying representations without explicit object-level decomposition, leading to intertwined static and dynamic elements that hinder scene separation. We present IDSplat, a self-supervised 3D Gaussian Splatting framework that reconstructs dynamic scenes with explicit instance decomposition and learnable motion trajectories, without requiring human annotations. Our key insight is to model dynamic objects as coherent instances undergoing rigid transformations, rather than unstructured time-varying primitives. For instance decomposition, we employ zero-shot, language-grounded video tracking anchored to 3D using lidar, and estimate consistent poses via feature correspondences. We introduce a coordinated-turn smoothing scheme to obtain temporally and physically consistent motion trajectories, mitigating pose misalignments and tracking failures, followed by joint optimization of object poses and Gaussian parameters. Experiments on the Waymo Open Dataset demonstrate that our method achieves competitive reconstruction quality while maintaining instance-level decomposition and generalizes across diverse sequences and view densities without retraining, making it practical for large-scale autonomous driving applications. Code will be released.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19235v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "[T]gaussian splatting"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "NVGS: Neural Visibility for Occlusion Culling in 3D Gaussian Splatting",
      "authors": [
        "Brent Zoomers",
        "Florian Hahlbohm",
        "Joni Vanherck",
        "Lode Jorissen",
        "Marcus Magnor",
        "Nick Michiels"
      ],
      "arxiv_id": "2511.19202v1",
      "summary": "3D Gaussian Splatting can exploit frustum culling and level-of-detail strategies to accelerate rendering of scenes containing a large number of primitives. However, the semi-transparent nature of Gaussians prevents the application of another highly effective technique: occlusion culling. We address this limitation by proposing a novel method to learn the viewpoint-dependent visibility function of all Gaussians in a trained model using a small, shared MLP across instances of an asset in a scene. By querying it for Gaussians within the viewing frustum prior to rasterization, our method can discard occluded primitives during rendering. Leveraging Tensor Cores for efficient computation, we integrate these neural queries directly into a novel instanced software rasterizer. Our approach outperforms the current state of the art for composed scenes in terms of VRAM usage and image quality, utilizing a combination of our instanced rasterizer and occlusion culling MLP, and exhibits complementary properties to existing LoD techniques.",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "15 pages, 13 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19202v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]3D gaussian splatting",
            "[T]gaussian splatting"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Novel View Synthesis from A Few Glimpses via Test-Time Natural Video Completion",
      "authors": [
        "Yan Xu",
        "Yixing Wang",
        "Stella X. Yu"
      ],
      "arxiv_id": "2511.17932v1",
      "summary": "Given just a few glimpses of a scene, can you imagine the movie playing out as the camera glides through it? That's the lens we take on \\emph{sparse-input novel view synthesis}, not only as filling spatial gaps between widely spaced views, but also as \\emph{completing a natural video} unfolding through space.\n  We recast the task as \\emph{test-time natural video completion}, using powerful priors from \\emph{pretrained video diffusion models} to hallucinate plausible in-between views. Our \\emph{zero-shot, generation-guided} framework produces pseudo views at novel camera poses, modulated by an \\emph{uncertainty-aware mechanism} for spatial coherence. These synthesized frames densify supervision for \\emph{3D Gaussian Splatting} (3D-GS) for scene reconstruction, especially in under-observed regions. An iterative feedback loop lets 3D geometry and 2D view synthesis inform each other, improving both the scene reconstruction and the generated views.\n  The result is coherent, high-fidelity renderings from sparse inputs \\emph{without any scene-specific training or fine-tuning}. On LLFF, DTU, DL3DV, and MipNeRF-360, our method significantly outperforms strong 3D-GS baselines under extreme sparsity.",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-22",
      "updated": "2025-11-22",
      "comment": "Accepted to NeurIPS 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17932v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "gaussian splatting",
            "[T]novel view synthesis",
            "scene reconstruction"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "CycleManip: Enabling Cyclic Task Manipulation via Effective Historical Perception and Understanding",
      "authors": [
        "Yi-Lin Wei",
        "Haoran Liao",
        "Yuhao Lin",
        "Pengyue Wang",
        "Zhizhao Liang",
        "Guiliang Liu",
        "Wei-Shi Zheng"
      ],
      "arxiv_id": "2512.01022v1",
      "summary": "In this paper, we explore an important yet underexplored task in robot manipulation: cycle-based manipulation, where robots need to perform cyclic or repetitive actions with an expected terminal time. These tasks are crucial in daily life, such as shaking a bottle or knocking a nail. However, few prior works have explored this task, leading to two main challenges: 1) the imitation methods often fail to complete these tasks within the expected terminal time due to the ineffective utilization of history; 2) the absence of a benchmark with sufficient data and automatic evaluation tools hinders development of effective solutions in this area. To address these challenges, we first propose the CycleManip framework to achieve cycle-based task manipulation in an end-to-end imitation manner without requiring any extra models, hierarchical structure or significant computational overhead. The core insight is to enhance effective history perception by a cost-aware sampling strategy and to improve historical understanding by multi-task learning. Second, we introduce a cycle-based task manipulation benchmark, which provides diverse cycle-based tasks, and an automatic evaluation method. Extensive experiments conducted in both simulation and real-world settings demonstrate that our method achieves high success rates in cycle-based task manipulation. The results further show strong adaptability performance in general manipulation, and the plug-and-play ability on imitation policies such as Vision-Language-Action (VLA) models. Moreover, the results show that our approach can be applied across diverse robotic platforms, including bi-arm grippers, dexterous hands, and humanoid robots.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "Project page: https://isee-laboratory.github.io/OmniDexGrasp/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01022v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid",
            "humanoid robot",
            "[T]manipulation",
            "dexterous hand"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Magnetic Tactile-Driven Soft Actuator for Intelligent Grasping and Firmness Evaluation",
      "authors": [
        "Chengjin Du",
        "Federico Bernabei",
        "Zhengyin Du",
        "Sergio Decherchi",
        "Matteo Lo Preti",
        "Lucia Beccai"
      ],
      "arxiv_id": "2512.00907v2",
      "summary": "Soft robots are powerful tools for manipulating delicate objects, yet their adoption is hindered by two gaps: the lack of integrated tactile sensing and sensor signal distortion caused by actuator deformations. This paper addresses these challenges by introducing the SoftMag actuator: a magnetic tactile-sensorized soft actuator. Unlike systems relying on attached sensors or treating sensing and actuation separately, SoftMag unifies them through a shared architecture while confronting the mechanical parasitic effect, where deformations corrupt tactile signals. A multiphysics simulation framework models this coupling, and a neural-network-based decoupling strategy removes the parasitic component, restoring sensing fidelity. Experiments including indentation, quasi-static and step actuation, and fatigue tests validate the actuator's performance and decoupling effectiveness. Building upon this foundation, the system is extended into a two-finger SoftMag gripper, where a multi-task neural network enables real-time prediction of tri-axial contact forces and position. Furthermore, a probing-based strategy estimates object firmness during grasping. Validation on apricots shows a strong correlation (Pearson r over 0.8) between gripper-estimated firmness and reference measurements, confirming the system's capability for non-destructive quality assessment. Results demonstrate that combining integrated magnetic sensing, learning-based correction, and real-time inference enables a soft robotic platform that adapts its grasp and quantifies material properties. The framework offers an approach for advancing sensorized soft actuators toward intelligent, material-aware robotics.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-30",
      "updated": "2025-12-02",
      "comment": "25 pages, 24 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00907v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]grasping",
            "[T]grasp"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "GenDexHand: Generative Simulation for Dexterous Hands",
      "authors": [
        "Feng Chen",
        "Zhuxiu Xu",
        "Tianzhe Chu",
        "Xunzhe Zhou",
        "Li Sun",
        "Zewen Wu",
        "Shenghua Gao",
        "Zhongyu Li",
        "Yanchao Yang",
        "Yi Ma"
      ],
      "arxiv_id": "2511.01791v1",
      "summary": "Data scarcity remains a fundamental bottleneck for embodied intelligence. Existing approaches use large language models (LLMs) to automate gripper-based simulation generation, but they transfer poorly to dexterous manipulation, which demands more specialized environment design. Meanwhile, dexterous manipulation tasks are inherently more difficult due to their higher degrees of freedom. Massively generating feasible and trainable dexterous hand tasks remains an open challenge. To this end, we present GenDexHand, a generative simulation pipeline that autonomously produces diverse robotic tasks and environments for dexterous manipulation. GenDexHand introduces a closed-loop refinement process that adjusts object placements and scales based on vision-language model (VLM) feedback, substantially improving the average quality of generated environments. Each task is further decomposed into sub-tasks to enable sequential reinforcement learning, reducing training time and increasing success rates. Our work provides a viable path toward scalable training of diverse dexterous hand behaviors in embodied intelligence by offering a simulation-based solution to synthetic data generation. Our website: https://winniechen2002.github.io/GenDexHand/.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01791v1",
      "code_links": [
        {
          "url": "https://winniechen2002.github.io/GenDexHand/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "[T]dexterous hand",
            "dexterous manipulation"
          ],
          "score": 10.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 11.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation",
      "authors": [
        "Yufeng Jin",
        "Niklas Funk",
        "Vignesh Prasad",
        "Zechu Li",
        "Mathias Franzius",
        "Jan Peters",
        "Georgia Chalvatzaki"
      ],
      "arxiv_id": "2511.01501v1",
      "summary": "Object pose estimation is a fundamental problem in robotics and computer vision, yet it remains challenging due to partial observability, occlusions, and object symmetries, which inevitably lead to pose ambiguity and multiple hypotheses consistent with the same observation. While deterministic deep networks achieve impressive performance under well-constrained conditions, they are often overconfident and fail to capture the multi-modality of the underlying pose distribution. To address these challenges, we propose a novel probabilistic framework that leverages flow matching on the SE(3) manifold for estimating 6D object pose distributions. Unlike existing methods that regress a single deterministic output, our approach models the full pose distribution with a sample-based estimate and enables reasoning about uncertainty in ambiguous cases such as symmetric objects or severe occlusions. We achieve state-of-the-art results on Real275, YCB-V, and LM-O, and demonstrate how our sample-based pose estimates can be leveraged in downstream robotic manipulation tasks such as active perception for disambiguating uncertain viewpoints or guiding grasp synthesis in an uncertainty-aware manner.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01501v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "grasp"
          ],
          "score": 8.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "flow matching"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "pose estimation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 11.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "POMA-3D: The Point Map Way to 3D Scene Understanding",
      "authors": [
        "Ye Mao",
        "Weixun Luo",
        "Ranran Huang",
        "Junpeng Jing",
        "Krystian Mikolajczyk"
      ],
      "arxiv_id": "2511.16567v2",
      "summary": "In this paper, we introduce POMA-3D, the first self-supervised 3D representation model learned from point maps. Point maps encode explicit 3D coordinates on a structured 2D grid, preserving global 3D geometry while remaining compatible with the input format of 2D foundation models. To transfer rich 2D priors into POMA-3D, a view-to-scene alignment strategy is designed. Moreover, as point maps are view-dependent with respect to a canonical space, we introduce POMA-JEPA, a joint embedding-predictive architecture that enforces geometrically consistent point map features across multiple views. Additionally, we introduce ScenePoint, a point map dataset constructed from 6.5K room-level RGB-D scenes and 1M 2D image scenes to facilitate large-scale POMA-3D pretraining. Experiments show that POMA-3D serves as a strong backbone for both specialist and generalist 3D understanding. It benefits diverse tasks, including 3D question answering, embodied navigation, scene retrieval, and embodied localization, all achieved using only geometric inputs (i.e., 3D coordinates). Overall, our POMA-3D explores a point map way to 3D scene understanding, addressing the scarcity of pretrained priors and limited data in 3D representation learning. Project Page: https://matchlab-imperial.github.io/poma3d/",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-21",
      "comment": "11 pages, 6 tables, 5 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16567v2",
      "code_links": [
        {
          "url": "https://matchlab-imperial.github.io/poma3d/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "representation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]scene understanding",
            "localization",
            "navigation"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 11.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "B2F: End-to-End Body-to-Face Motion Generation with Style Reference",
      "authors": [
        "Bokyung Jang",
        "Eunho Jung",
        "Yoonsang Lee"
      ],
      "arxiv_id": "2511.13988v1",
      "summary": "Human motion naturally integrates body movements and facial expressions, forming a unified perception. If a virtual character's facial expression does not align well with its body movements, it may weaken the perception of the character as a cohesive whole. Motivated by this, we propose B2F, a model that generates facial motions aligned with body movements. B2F takes a facial style reference as input, generating facial animations that reflect the provided style while maintaining consistency with the associated body motion. To achieve this, B2F learns a disentangled representation of content and style, using alignment and consistency-based objectives. We represent style using discrete latent codes learned via the Gumbel-Softmax trick, enabling diverse expression generation with a structured latent representation. B2F outputs facial motion in the FLAME format, making it compatible with SMPL-X characters, and supports ARKit-style avatars through a dedicated conversion module. Our evaluations show that B2F generates expressive and engaging facial animations that synchronize with body movements and style intent, while mitigating perceptual dissonance from mismatched cues, and generalizing across diverse characters and styles.",
      "categories": [
        "cs.GR"
      ],
      "primary_category": "cs.GR",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "Pacific Graphics 2025",
      "doi": "10.2312/pg.20251256",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13988v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "[T]motion generation"
          ],
          "score": 7.5
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "SMPL",
            "SMPL-X"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 11.5,
      "hit_pillars": [
        "4_motion_diffusion",
        "6_video_extraction"
      ]
    },
    {
      "title": "FineXtrol: Controllable Motion Generation via Fine-Grained Text",
      "authors": [
        "Keming Shen",
        "Bizhu Wu",
        "Junliang Chen",
        "Xiaoqin Wang",
        "Linlin Shen"
      ],
      "arxiv_id": "2511.18927v1",
      "summary": "Recent works have sought to enhance the controllability and precision of text-driven motion generation. Some approaches leverage large language models (LLMs) to produce more detailed texts, while others incorporate global 3D coordinate sequences as additional control signals. However, the former often introduces misaligned details and lacks explicit temporal cues, and the latter incurs significant computational cost when converting coordinates to standard motion representations. To address these issues, we propose FineXtrol, a novel control framework for efficient motion generation guided by temporally-aware, precise, user-friendly, and fine-grained textual control signals that describe specific body part movements over time. In support of this framework, we design a hierarchical contrastive learning module that encourages the text encoder to produce more discriminative embeddings for our novel control signals, thereby improving motion controllability. Quantitative results show that FineXtrol achieves strong performance in controllable motion generation, while qualitative analysis demonstrates its flexibility in directing specific body part movements.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "20 pages, 14 figures, AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18927v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "contrastive learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "text-driven motion",
            "[T]motion generation"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 11.5,
      "hit_pillars": [
        "2_algo_arch",
        "4_motion_diffusion"
      ]
    },
    {
      "title": "Partially Equivariant Reinforcement Learning in Symmetry-Breaking Environments",
      "authors": [
        "Junwoo Chang",
        "Minwoo Park",
        "Joohwan Seo",
        "Roberto Horowitz",
        "Jongmin Lee",
        "Jongeun Choi"
      ],
      "arxiv_id": "2512.00915v1",
      "summary": "Group symmetries provide a powerful inductive bias for reinforcement learning (RL), enabling efficient generalization across symmetric states and actions via group-invariant Markov Decision Processes (MDPs). However, real-world environments almost never realize fully group-invariant MDPs; dynamics, actuation limits, and reward design usually break symmetries, often only locally. Under group-invariant Bellman backups for such cases, local symmetry-breaking introduces errors that propagate across the entire state-action space, resulting in global value estimation errors. To address this, we introduce Partially group-Invariant MDP (PI-MDP), which selectively applies group-invariant or standard Bellman backups depending on where symmetry holds. This framework mitigates error propagation from locally broken symmetries while maintaining the benefits of equivariance, thereby enhancing sample efficiency and generalizability. Building on this framework, we present practical RL algorithms -- Partially Equivariant (PE)-DQN for discrete control and PE-SAC for continuous control -- that combine the benefits of equivariance with robustness to symmetry-breaking. Experiments across Grid-World, locomotion, and manipulation benchmarks demonstrate that PE-DQN and PE-SAC significantly outperform baselines, highlighting the importance of selective symmetry exploitation for robust and sample-efficient RL.",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "27 pages, 10 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00915v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "locomotion",
            "manipulation"
          ],
          "score": 4.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "SAC",
            "reward design"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 11.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "MVSMamba: Multi-View Stereo with State Space Model",
      "authors": [
        "Jianfei Jiang",
        "Qiankun Liu",
        "Hongyuan Liu",
        "Haochen Yu",
        "Liyong Wang",
        "Jiansheng Chen",
        "Huimin Ma"
      ],
      "arxiv_id": "2511.01315v1",
      "summary": "Robust feature representations are essential for learning-based Multi-View Stereo (MVS), which relies on accurate feature matching. Recent MVS methods leverage Transformers to capture long-range dependencies based on local features extracted by conventional feature pyramid networks. However, the quadratic complexity of Transformer-based MVS methods poses challenges to balance performance and efficiency. Motivated by the global modeling capability and linear complexity of the Mamba architecture, we propose MVSMamba, the first Mamba-based MVS network. MVSMamba enables efficient global feature aggregation with minimal computational overhead. To fully exploit Mamba's potential in MVS, we propose a Dynamic Mamba module (DM-module) based on a novel reference-centered dynamic scanning strategy, which enables: (1) Efficient intra- and inter-view feature interaction from the reference to source views, (2) Omnidirectional multi-view feature representations, and (3) Multi-scale global feature aggregation. Extensive experimental results demonstrate MVSMamba outperforms state-of-the-art MVS methods on the DTU dataset and the Tanks-and-Temples benchmark with both superior performance and efficiency. The source code is available at https://github.com/JianfeiJ/MVSMamba.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "Accepted by NeurIPS 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01315v1",
      "code_links": [
        {
          "url": "https://github.com/JianfeiJ/MVSMamba",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba",
            "[T]state space model"
          ],
          "score": 9.0
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "feature matching"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 11.0,
      "hit_pillars": [
        "2_algo_arch",
        "6_video_extraction"
      ]
    },
    {
      "title": "MambaTrack3D: A State Space Model Framework for LiDAR-Based Object Tracking under High Temporal Variation",
      "authors": [
        "Shengjing Tian",
        "Yinan Han",
        "Xiantong Zhao",
        "Xuehu Liu",
        "Qi Lang"
      ],
      "arxiv_id": "2511.15077v1",
      "summary": "Dynamic outdoor environments with high temporal variation (HTV) pose significant challenges for 3D single object tracking in LiDAR point clouds. Existing memory-based trackers often suffer from quadratic computational complexity, temporal redundancy, and insufficient exploitation of geometric priors. To address these issues, we propose MambaTrack3D, a novel HTV-oriented tracking framework built upon the state space model Mamba. Specifically, we design a Mamba-based Inter-frame Propagation (MIP) module that replaces conventional single-frame feature extraction with efficient inter-frame propagation, achieving near-linear complexity while explicitly modeling spatial relations across historical frames. Furthermore, a Grouped Feature Enhancement Module (GFEM) is introduced to separate foreground and background semantics at the channel level, thereby mitigating temporal redundancy in the memory bank. Extensive experiments on KITTI-HTV and nuScenes-HTV benchmarks demonstrate that MambaTrack3D consistently outperforms both HTV-oriented and normal-scenario trackers, achieving improvements of up to 6.5 success and 9.5 precision over HVTrack under moderate temporal gaps. On the standard KITTI dataset, MambaTrack3D remains highly competitive with state-of-the-art normal-scenario trackers, confirming its strong generalization ability. Overall, MambaTrack3D achieves a superior accuracy-efficiency trade-off, delivering robust performance across both specialized HTV and conventional tracking scenarios.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "This work has been submitted to a journal for possible publication",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15077v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba",
            "[T]state space model"
          ],
          "score": 9.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 11.0,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "LiDAR-GS++:Improving LiDAR Gaussian Reconstruction via Diffusion Priors",
      "authors": [
        "Qifeng Chen",
        "Jiarun Liu",
        "Rengan Xie",
        "Tao Tang",
        "Sicong Du",
        "Yiru Zhao",
        "Yuchi Huo",
        "Sheng Yang"
      ],
      "arxiv_id": "2511.12304v1",
      "summary": "Recent GS-based rendering has made significant progress for LiDAR, surpassing Neural Radiance Fields (NeRF) in both quality and speed. However, these methods exhibit artifacts in extrapolated novel view synthesis due to the incomplete reconstruction from single traversal scans. To address this limitation, we present LiDAR-GS++, a LiDAR Gaussian Splatting reconstruction method enhanced by diffusion priors for real-time and high-fidelity re-simulation on public urban roads. Specifically, we introduce a controllable LiDAR generation model conditioned on coarsely extrapolated rendering to produce extra geometry-consistent scans and employ an effective distillation mechanism for expansive reconstruction. By extending reconstruction to under-fitted regions, our approach ensures global geometric consistency for extrapolative novel views while preserving detailed scene surfaces captured by sensors. Experiments on multiple public datasets demonstrate that LiDAR-GS++ achieves state-of-the-art performance for both interpolated and extrapolated viewpoints, surpassing existing GS and NeRF-based methods.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-15",
      "updated": "2025-11-15",
      "comment": "Accepted by AAAI-26",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12304v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "gaussian splatting",
            "NeRF",
            "neural radiance",
            "novel view synthesis"
          ],
          "score": 8.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "geometric consistency"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 11.0,
      "hit_pillars": [
        "3_perception_slam",
        "7_retargeting"
      ]
    },
    {
      "title": "Learning When to Ask: Simulation-Trained Humanoids for Mental-Health Diagnosis",
      "authors": [
        "Filippo Cenacchi",
        "Deborah Richards",
        "Longbing Cao"
      ],
      "arxiv_id": "2512.08952v1",
      "summary": "Testing humanoid robots with users is slow, causes wear, and limits iteration and diversity. Yet screening agents must master conversational timing, prosody, backchannels, and what to attend to in faces and speech for Depression and PTSD. Most simulators omit policy learning with nonverbal dynamics; many controllers chase task accuracy while underweighting trust, pacing, and rapport. We virtualise the humanoid as a conversational agent to train without hardware burden. Our agent-centred, simulation-first pipeline turns interview data into 276 Unreal Engine MetaHuman patients with synchronised speech, gaze/face, and head-torso poses, plus PHQ-8 and PCL-C flows. A perception-fusion-policy loop decides what and when to speak, when to backchannel, and how to avoid interruptions, under a safety shield. Training uses counterfactual replay (bounded nonverbal perturbations) and an uncertainty-aware turn manager that probes to reduce diagnostic ambiguity. Results are simulation-only; the humanoid is the transfer target. In comparing three controllers, a custom TD3 (Twin Delayed DDPG) outperformed PPO and CEM, achieving near-ceiling coverage with steadier pace at comparable rewards. Decision-quality analyses show negligible turn overlap, aligned cut timing, fewer clarification prompts, and shorter waits. Performance stays stable under modality dropout and a renderer swap, and rankings hold on a held-out patient split. Contributions: (1) an agent-centred simulator that turns interviews into 276 interactive patients with bounded nonverbal counterfactuals; (2) a safe learning loop that treats timing and rapport as first-class control variables; (3) a comparative study (TD3 vs PPO/CEM) with clear gains in completeness and social timing; and (4) ablations and robustness analyses explaining the gains and enabling clinician-supervised humanoid pilots.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-28",
      "updated": "2025-11-28",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08952v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot"
          ],
          "score": 8.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning",
            "PPO"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 11.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "SocialNav: Training Human-Inspired Foundation Model for Socially-Aware Embodied Navigation",
      "authors": [
        "Ziyi Chen",
        "Yingnan Guo",
        "Zedong Chu",
        "Minghua Luo",
        "Yanfen Shen",
        "Mingchao Sun",
        "Junjun Hu",
        "Shichao Xie",
        "Kuan Yang",
        "Pei Shi",
        "Zhining Gu",
        "Lu Liu",
        "Honglin Han",
        "Xiaolong Wu",
        "Mu Xu",
        "Yu Zhang"
      ],
      "arxiv_id": "2511.21135v1",
      "summary": "Embodied navigation that adheres to social norms remains an open research challenge. Our \\textbf{SocialNav} is a foundational model for socially-aware navigation with a hierarchical \"brain-action\" architecture, capable of understanding high-level social norms and generating low-level, socially compliant trajectories. To enable such dual capabilities, we construct the SocNav Dataset, a large-scale collection of 7 million samples, comprising (1) a Cognitive Activation Dataset providing social reasoning signals such as chain-of-thought explanations and social traversability prediction, and (2) an Expert Trajectories Pyramid aggregating diverse navigation demonstrations from internet videos, simulated environments, and real-world robots. A multi-stage training pipeline is proposed to gradually inject and refine navigation intelligence: we first inject general navigation skills and social norms understanding into the model via imitation learning, and then refine such skills through a deliberately designed Socially-Aware Flow Exploration GRPO (SAFE-GRPO), the first flow-based reinforcement learning framework for embodied navigation that explicitly rewards socially compliant behaviors. SocialNav achieves +38% success rate and +46% social compliance rate compared to the state-of-the-art method, demonstrating strong gains in both navigation performance and social compliance. Our project page: https://amap-eai.github.io/SocialNav/",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21135v1",
      "code_links": [
        {
          "url": "https://amap-eai.github.io/SocialNav/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "imitation learning"
          ],
          "score": 3.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "traversability",
            "[T]navigation"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 11.0,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "3D-Aware Multi-Task Learning with Cross-View Correlations for Dense Scene Understanding",
      "authors": [
        "Xiaoye Wang",
        "Chen Tang",
        "Xiangyu Yue",
        "Wei-Hong Li"
      ],
      "arxiv_id": "2511.20646v1",
      "summary": "This paper addresses the challenge of training a single network to jointly perform multiple dense prediction tasks, such as segmentation and depth estimation, i.e., multi-task learning (MTL). Current approaches mainly capture cross-task relations in the 2D image space, often leading to unstructured features lacking 3D-awareness. We argue that 3D-awareness is vital for modeling cross-task correlations essential for comprehensive scene understanding. We propose to address this problem by integrating correlations across views, i.e., cost volume, as geometric consistency in the MTL network. Specifically, we introduce a lightweight Cross-view Module (CvM), shared across tasks, to exchange information across views and capture cross-view correlations, integrated with a feature from MTL encoder for multi-task predictions. This module is architecture-agnostic and can be applied to both single and multi-view data. Extensive results on NYUv2 and PASCAL-Context demonstrate that our method effectively injects geometric consistency into existing MTL methods to improve performance.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "3D-aware Multi-task Learning, Cross-view Correlations, Code will be available at https://github.com/WeiHongLee/CrossView3DMTL",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20646v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation",
            "[T]scene understanding"
          ],
          "score": 8.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "geometric consistency"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 11.0,
      "hit_pillars": [
        "3_perception_slam",
        "7_retargeting"
      ]
    },
    {
      "title": "DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification",
      "authors": [
        "Yujie Yang",
        "Shuang Li",
        "Jun Ye",
        "Neng Dong",
        "Fan Li",
        "Huafeng Li"
      ],
      "arxiv_id": "2511.04281v1",
      "summary": "Video-based Visible-Infrared person re-identification (VVI-ReID) aims to retrieve the same pedestrian across visible and infrared modalities from video sequences. Existing methods tend to exploit modality-invariant visual features but largely overlook gait features, which are not only modality-invariant but also rich in temporal dynamics, thus limiting their ability to model the spatiotemporal consistency essential for cross-modal video matching. To address these challenges, we propose a DINOv2-Driven Gait Representation Learning (DinoGRL) framework that leverages the rich visual priors of DINOv2 to learn gait features complementary to appearance cues, facilitating robust sequence-level representations for cross-modal retrieval. Specifically, we introduce a Semantic-Aware Silhouette and Gait Learning (SASGL) model, which generates and enhances silhouette representations with general-purpose semantic priors from DINOv2 and jointly optimizes them with the ReID objective to achieve semantically enriched and task-adaptive gait feature learning. Furthermore, we develop a Progressive Bidirectional Multi-Granularity Enhancement (PBMGE) module, which progressively refines feature representations by enabling bidirectional interactions between gait and appearance streams across multiple spatial granularities, fully leveraging their complementarity to enhance global representations with rich local details and produce highly discriminative features. Extensive experiments on HITSZ-VCM and BUPT datasets demonstrate the superiority of our approach, significantly outperforming existing state-of-the-art methods.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04281v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]gait"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]representation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Simple 3D Pose Features Support Human and Machine Social Scene Understanding",
      "authors": [
        "Wenshuo Qin",
        "Leyla Isik"
      ],
      "arxiv_id": "2511.03988v1",
      "summary": "Humans can quickly and effortlessly extract a variety of information about others' social interactions from visual input, ranging from visuospatial cues like whether two people are facing each other to higher-level information. Yet, the computations supporting these abilities remain poorly understood, and social interaction recognition continues to challenge even the most advanced AI vision systems. Here, we hypothesized that humans rely on 3D visuospatial pose information to make social interaction judgments, which is absent in most AI vision models. To test this, we combined state-of-the-art pose and depth estimation algorithms to extract 3D joint positions of people in short video clips depicting everyday human actions and compared their ability to predict human social interaction judgments with current AI vision models. Strikingly, 3D joint positions outperformed most current AI vision models, revealing that key social information is available in explicit body position but not in the learned features of most vision models, including even the layer-wise embeddings of the pose models used to extract joint positions. To uncover the critical pose features humans use to make social judgments, we derived a compact set of 3D social pose features describing only the 3D position and direction of faces in the videos. We found that these minimal descriptors matched the predictive strength of the full set of 3D joints and significantly improved the performance of off-the-shelf AI vision models when combined with their embeddings. Moreover, the degree to which 3D social pose features were represented in each off-the-shelf AI vision model predicted the model's ability to match human social judgments. Together, our findings provide strong evidence that human social scene understanding relies on explicit representations of 3D pose and can be supported by simple, structured visuospatial primitives.",
      "categories": [
        "cs.CV",
        "q-bio.NC"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "28 pages, 6 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03988v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation",
            "[T]scene understanding"
          ],
          "score": 8.0
        },
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "social interaction"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "3_perception_slam",
        "5_interaction_reaction"
      ]
    },
    {
      "title": "Dynamic Sparsity: Challenging Common Sparsity Assumptions for Learning World Models in Robotic Reinforcement Learning Benchmarks",
      "authors": [
        "Muthukumar Pandaram",
        "Jakob Hollenstein",
        "David Drexel",
        "Samuele Tosatto",
        "Antonio Rodríguez-Sánchez",
        "Justus Piater"
      ],
      "arxiv_id": "2511.08086v2",
      "summary": "The use of learned dynamics models, also known as world models, can improve the sample efficiency of reinforcement learning. Recent work suggests that the underlying causal graphs of such dynamics models are sparsely connected, with each of the future state variables depending only on a small subset of the current state variables, and that learning may therefore benefit from sparsity priors. Similarly, temporal sparsity, i.e. sparsely and abruptly changing local dynamics, has also been proposed as a useful inductive bias.\n  In this work, we critically examine these assumptions by analyzing ground-truth dynamics from a set of robotic reinforcement learning environments in the MuJoCo Playground benchmark suite, aiming to determine whether the proposed notions of state and temporal sparsity actually tend to hold in typical reinforcement learning tasks.\n  We study (i) whether the causal graphs of environment dynamics are sparse, (ii) whether such sparsity is state-dependent, and (iii) whether local system dynamics change sparsely.\n  Our results indicate that global sparsity is rare, but instead the tasks show local, state-dependent sparsity in their dynamics and this sparsity exhibits distinct structures, appearing in temporally localized clusters (e.g., during contact events) and affecting specific subsets of state dimensions. These findings challenge common sparsity prior assumptions in dynamics learning, emphasizing the need for grounded inductive biases that reflect the state-dependent sparsity structure of real-world dynamics.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-11",
      "updated": "2025-11-14",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08086v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "[T]world model",
            "MuJoCo"
          ],
          "score": 10.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Learning from the Right Patches: A Two-Stage Wavelet-Driven Masked Autoencoder for Histopathology Representation Learning",
      "authors": [
        "Raneen Younis",
        "Louay Hamdi",
        "Lukas Chavez",
        "Zahra Ahmadi"
      ],
      "arxiv_id": "2511.06958v2",
      "summary": "Whole-slide images are central to digital pathology, yet their extreme size and scarce annotations make self-supervised learning essential. Masked Autoencoders (MAEs) with Vision Transformer backbones have recently shown strong potential for histopathology representation learning. However, conventional random patch sampling during MAE pretraining often includes irrelevant or noisy regions, limiting the model's ability to capture meaningful tissue patterns. In this paper, we present a lightweight and domain-adapted framework that brings structure and biological relevance into MAE-based learning through a wavelet-informed patch selection strategy. WISE-MAE applies a two-step coarse-to-fine process: wavelet-based screening at low magnification to locate structurally rich regions, followed by high-resolution extraction for detailed modeling. This approach mirrors the diagnostic workflow of pathologists and improves the quality of learned representations. Evaluations across multiple cancer datasets, including lung, renal, and colorectal tissues, show that WISE-MAE achieves competitive representation quality and downstream classification performance while maintaining efficiency under weak supervision.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-19",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06958v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]representation learning",
            "[T]masked autoencoder",
            "MAE"
          ],
          "score": 10.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Nonholonomic Narrow Dead-End Escape with Deep Reinforcement Learning",
      "authors": [
        "Denghan Xiong",
        "Yanzhe Zhao",
        "Yutong Chen",
        "Zichun Wang"
      ],
      "arxiv_id": "2511.22338v1",
      "summary": "Nonholonomic constraints restrict feasible velocities without reducing configuration-space dimension, which makes collision-free geometric paths generally non-executable for car-like robots. Ackermann steering further imposes curvature bounds and forbids in-place rotation, so escaping from narrow dead ends typically requires tightly sequenced forward and reverse maneuvers. Classical planners that decouple global search and local steering struggle in these settings because narrow passages occupy low-measure regions and nonholonomic reachability shrinks the set of valid connections, which degrades sampling efficiency and increases sensitivity to clearances. We study nonholonomic narrow dead-end escape for Ackermann vehicles and contribute three components. First, we construct a generator that samples multi-phase forward-reverse trajectories compatible with Ackermann kinematics and inflates their envelopes to synthesize families of narrow dead ends that are guaranteed to admit at least one feasible escape. Second, we construct a training environment that enforces kinematic constraints and train a policy using the soft actor-critic algorithm. Third, we evaluate against representative classical planners that combine global search with nonholonomic steering. Across parameterized dead-end families, the learned policy solves a larger fraction of instances, reduces maneuver count, and maintains comparable path length and planning time while under the same sensing and control limits. We provide our project as open source at https://github.com/gitagitty/cisDRL-RobotNav.git",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "14 pages, 5 figures, 1 table, submitted to arXiv",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22338v1",
      "code_links": [
        {
          "url": "https://github.com/gitagitty/cisDRL-RobotNav.git",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "[T]deep reinforcement learning",
            "actor-critic"
          ],
          "score": 10.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood Inverse Reinforcement Learning",
      "authors": [
        "Junsung Park"
      ],
      "arxiv_id": "2511.22210v1",
      "summary": "Offline inverse reinforcement learning (IRL) aims to recover a reward function that explains expert behavior using only fixed demonstration data, without any additional online interaction. We propose BiCQL-ML, a policy-free offline IRL algorithm that jointly optimizes a reward function and a conservative Q-function in a bi-level framework, thereby avoiding explicit policy learning. The method alternates between (i) learning a conservative Q-function via Conservative Q-Learning (CQL) under the current reward, and (ii) updating the reward parameters to maximize the expected Q-values of expert actions while suppressing over-generalization to out-of-distribution actions. This procedure can be viewed as maximum likelihood estimation under a soft value matching principle. We provide theoretical guarantees that BiCQL-ML converges to a reward function under which the expert policy is soft-optimal. Empirically, we show on standard offline RL benchmarks that BiCQL-ML improves both reward recovery and downstream policy performance compared to existing offline IRL baselines.",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "8 pages, 3 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22210v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "policy learning",
            "[T]inverse reinforcement learning"
          ],
          "score": 10.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Hyper-GoalNet: Goal-Conditioned Manipulation Policy Learning with HyperNetworks",
      "authors": [
        "Pei Zhou",
        "Wanting Yao",
        "Qian Luo",
        "Xunzhe Zhou",
        "Yanchao Yang"
      ],
      "arxiv_id": "2512.00085v1",
      "summary": "Goal-conditioned policy learning for robotic manipulation presents significant challenges in maintaining performance across diverse objectives and environments. We introduce Hyper-GoalNet, a framework that generates task-specific policy network parameters from goal specifications using hypernetworks. Unlike conventional methods that simply condition fixed networks on goal-state pairs, our approach separates goal interpretation from state processing -- the former determines network parameters while the latter applies these parameters to current observations. To enhance representation quality for effective policy generation, we implement two complementary constraints on the latent space: (1) a forward dynamics model that promotes state transition predictability, and (2) a distance-based constraint ensuring monotonic progression toward goal states. We evaluate our method on a comprehensive suite of manipulation tasks with varying environmental randomization. Results demonstrate significant performance improvements over state-of-the-art methods, particularly in high-variability conditions. Real-world robotic experiments further validate our method's robustness to sensor noise and physical uncertainties. Code is available at: https://github.com/wantingyao/hyper-goalnet.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00085v1",
      "code_links": [
        {
          "url": "https://github.com/wantingyao/hyper-goalnet",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]policy learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "MFM-point: Multi-scale Flow Matching for Point Cloud Generation",
      "authors": [
        "Petr Molodyk",
        "Jaemoo Choi",
        "David W. Romero",
        "Ming-Yu Liu",
        "Yongxin Chen"
      ],
      "arxiv_id": "2511.20041v1",
      "summary": "In recent years, point cloud generation has gained significant attention in 3D generative modeling. Among existing approaches, point-based methods directly generate point clouds without relying on other representations such as latent features, meshes, or voxels. These methods offer low training cost and algorithmic simplicity, but often underperform compared to representation-based approaches. In this paper, we propose MFM-Point, a multi-scale Flow Matching framework for point cloud generation that substantially improves the scalability and performance of point-based methods while preserving their simplicity and efficiency. Our multi-scale generation algorithm adopts a coarse-to-fine generation paradigm, enhancing generation quality and scalability without incurring additional training or inference overhead. A key challenge in developing such a multi-scale framework lies in preserving the geometric structure of unordered point clouds while ensuring smooth and consistent distributional transitions across resolutions. To address this, we introduce a structured downsampling and upsampling strategy that preserves geometry and maintains alignment between coarse and fine resolutions. Our experimental results demonstrate that MFM-Point achieves best-in-class performance among point-based methods and challenges the best representation-based methods. In particular, MFM-point demonstrates strong results in multi-category and high-resolution generation tasks.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20041v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]flow matching"
          ],
          "score": 4.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Manifold-constrained Hamilton-Jacobi Reachability Learning for Decentralized Multi-Agent Motion Planning",
      "authors": [
        "Qingyi Chen",
        "Ruiqi Ni",
        "Jun Kim",
        "Ahmed H. Qureshi"
      ],
      "arxiv_id": "2511.03591v1",
      "summary": "Safe multi-agent motion planning (MAMP) under task-induced constraints is a critical challenge in robotics. Many real-world scenarios require robots to navigate dynamic environments while adhering to manifold constraints imposed by tasks. For example, service robots must carry cups upright while avoiding collisions with humans or other robots. Despite recent advances in decentralized MAMP for high-dimensional systems, incorporating manifold constraints remains difficult. To address this, we propose a manifold-constrained Hamilton-Jacobi reachability (HJR) learning framework for decentralized MAMP. Our method solves HJR problems under manifold constraints to capture task-aware safety conditions, which are then integrated into a decentralized trajectory optimization planner. This enables robots to generate motion plans that are both safe and task-feasible without requiring assumptions about other agents' policies. Our approach generalizes across diverse manifold-constrained tasks and scales effectively to high-dimensional multi-agent manipulation problems. Experiments show that our method outperforms existing constrained motion planners and operates at speeds suitable for real-world applications. Video demonstrations are available at https://youtu.be/RYcEHMnPTH8 .",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-05",
      "updated": "2025-11-05",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03591v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "trajectory optimization",
            "[T]motion planning"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "MoSa: Motion Generation with Scalable Autoregressive Modeling",
      "authors": [
        "Mengyuan Liu",
        "Sheng Yan",
        "Yong Wang",
        "Yingjie Li",
        "Gui-Bin Bian",
        "Hong Liu"
      ],
      "arxiv_id": "2511.01200v1",
      "summary": "We introduce MoSa, a novel hierarchical motion generation framework for text-driven 3D human motion generation that enhances the Vector Quantization-guided Generative Transformers (VQ-GT) paradigm through a coarse-to-fine scalable generation process. In MoSa, we propose a Multi-scale Token Preservation Strategy (MTPS) integrated into a hierarchical residual vector quantization variational autoencoder (RQ-VAE). MTPS employs interpolation at each hierarchical quantization to effectively retain coarse-to-fine multi-scale tokens. With this, the generative transformer supports Scalable Autoregressive (SAR) modeling, which predicts scale tokens, unlike traditional methods that predict only one token at each step. Consequently, MoSa requires only 10 inference steps, matching the number of RQ-VAE quantization layers. To address potential reconstruction degradation from frequent interpolation, we propose CAQ-VAE, a lightweight yet expressive convolution-attention hybrid VQ-VAE. CAQ-VAE enhances residual block design and incorporates attention mechanisms to better capture global dependencies. Extensive experiments show that MoSa achieves state-of-the-art generation quality and efficiency, outperforming prior methods in both fidelity and speed. On the Motion-X dataset, MoSa achieves an FID of 0.06 (versus MoMask's 0.20) while reducing inference time by 27 percent. Moreover, MoSa generalizes well to downstream tasks such as motion editing, requiring no additional fine-tuning. The code is available at https://mosa-web.github.io/MoSa-web",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01200v1",
      "code_links": [
        {
          "url": "https://mosa-web.github.io/MoSa-web",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "[T]motion generation",
            "MoMask"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "4_motion_diffusion"
      ]
    },
    {
      "title": "Learning from Watching: Scalable Extraction of Manipulation Trajectories from Human Videos",
      "authors": [
        "X. Hu",
        "G. Ye"
      ],
      "arxiv_id": "2512.00024v1",
      "summary": "Collecting high-quality data for training large-scale robotic models typically relies on real robot platforms, which is labor-intensive and costly, whether via teleoperation or scripted demonstrations. To scale data collection, many researchers have turned to leveraging human manipulation videos available online. However, current methods predominantly focus on hand detection or object pose estimation, failing to fully exploit the rich interaction cues embedded in these videos. In this work, we propose a novel approach that combines large foundation models for video understanding with point tracking techniques to extract dense trajectories of all task-relevant keypoints during manipulation. This enables more comprehensive utilization of Internet-scale human demonstration videos. Experimental results demonstrate that our method can accurately track keypoints throughout the entire manipulation process, paving the way for more scalable and data-efficient robot learning.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "Accepted to RSS 2025 Workshop",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00024v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "teleoperation"
          ],
          "score": 8.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "pose estimation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles",
      "authors": [
        "Hyungtae Lim",
        "Daebeom Kim",
        "Hyun Myung"
      ],
      "arxiv_id": "2511.00635v1",
      "summary": "As various 3D light detection and ranging (LiDAR) sensors have been introduced to the market, research on multi-session simultaneous localization and mapping (MSS) using heterogeneous LiDAR sensors has been actively conducted. Existing MSS methods mostly rely on loop closure detection for inter-session alignment; however, the performance of loop closure detection can be potentially degraded owing to the differences in the density and field of view (FoV) of the sensors used in different sessions. In this study, we challenge the existing paradigm that relies heavily on loop detection modules and propose a novel MSS framework, called Multi-Mapcher, that employs large-scale map-to-map registration to perform inter-session initial alignment, which is commonly assumed to be infeasible, by leveraging outlier-robust 3D point cloud registration. Next, after finding inter-session loops by radius search based on the assumption that the inter-session initial alignment is sufficiently precise, anchor node-based robust pose graph optimization is employed to build a consistent global map. As demonstrated in our experiments, our approach shows substantially better MSS performance for various LiDAR sensors used to capture the sessions and is faster than state-of-the-art approaches. Our code is available at https://github.com/url-kaist/multi-mapcher.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-01",
      "updated": "2025-11-01",
      "comment": "13 pages, 12 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00635v1",
      "code_links": [
        {
          "url": "https://github.com/url-kaist/multi-mapcher",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]SLAM",
            "point cloud",
            "localization"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback",
      "authors": [
        "Kai Luo",
        "Hao Shi",
        "Kunyu Peng",
        "Fei Teng",
        "Sheng Wu",
        "Kaiwei Wang",
        "Kailun Yang"
      ],
      "arxiv_id": "2511.00510v1",
      "summary": "This paper investigates Multi-Object Tracking (MOT) in panoramic imagery, which introduces unique challenges including a 360° Field of View (FoV), resolution dilution, and severe view-dependent distortions. Conventional MOT methods designed for narrow-FoV pinhole cameras generalize unsatisfactorily under these conditions. To address panoramic distortion, large search space, and identity ambiguity under a 360° FoV, OmniTrack++ adopts a feedback-driven framework that progressively refines perception with trajectory cues. A DynamicSSM block first stabilizes panoramic features, implicitly alleviating geometric distortion. On top of normalized representations, FlexiTrack Instances use trajectory-informed feedback for flexible localization and reliable short-term association. To ensure long-term robustness, an ExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts design, enabling recovery from fragmented tracks and reducing identity drift. Finally, a Tracklet Management module adaptively switches between end-to-end and tracking-by-detection modes according to scene dynamics, offering a balanced and scalable solution for panoramic MOT. To support rigorous evaluation, we establish the EmboTrack benchmark, a comprehensive dataset for panoramic MOT that includes QuadTrack, captured with a quadruped robot, and BipTrack, collected with a bipedal wheel-legged robot. Together, these datasets span wide-angle environments and diverse motion patterns, providing a challenging testbed for real-world panoramic perception. Extensive experiments on JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art performance, yielding substantial HOTA improvements of +25.5% on JRDB and +43.07% on QuadTrack over the original OmniTrack. Datasets and code will be made publicly available at https://github.com/xifen523/OmniTrack.",
      "categories": [
        "cs.CV",
        "cs.RO",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-01",
      "updated": "2025-11-01",
      "comment": "Extended version of CVPR 2025 paper arXiv:2503.04565. Datasets and code will be made publicly available at https://github.com/xifen523/OmniTrack",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00510v1",
      "code_links": [
        {
          "url": "https://github.com/xifen523/OmniTrack",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped",
            "legged robot",
            "bipedal",
            "biped"
          ],
          "score": 8.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "STORM: Segment, Track, and Object Re-Localization from a Single Image",
      "authors": [
        "Yu Deng",
        "Teng Cao",
        "Hikaru Shindo",
        "Jiahong Xue",
        "Quentin Delfosse",
        "Kristian Kersting"
      ],
      "arxiv_id": "2511.09771v2",
      "summary": "Accurate 6D pose estimation and tracking are fundamental capabilities for physical AI systems such as robots. However, existing approaches typically require a pre-defined 3D model of the target and rely on a manually annotated segmentation mask in the first frame, which is labor-intensive and leads to reduced performance when faced with occlusions or rapid movement. To address these limitations, we propose STORM (Segment, Track, and Object Re-localization from a single iMage), an open-source robust real-time 6D pose estimation system that requires no manual annotation. STORM employs a novel three-stage pipeline combining vision-language understanding with feature matching: contextual object descriptions guide localization, self-cross-attention mechanisms identify candidate regions, and produce precise masks and 3D models for accurate pose estimation. Another key innovation is our automatic re-registration mechanism that detects tracking failures through feature similarity monitoring and recovers from severe occlusions or rapid motion. STORM achieves state-of-the-art accuracy on challenging industrial datasets featuring multi-object occlusions, high-speed motion, and varying illumination, while operating at real-time speeds without additional training. This annotation-free approach significantly reduces deployment overhead, providing a practical solution for modern applications, such as flexible manufacturing and intelligent quality control.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-12",
      "updated": "2025-12-01",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09771v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "pose estimation",
            "[T]localization"
          ],
          "score": 8.0
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "feature matching"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "3_perception_slam",
        "6_video_extraction"
      ]
    },
    {
      "title": "MirrorLimb: Implementing hand pose acquisition and robot teleoperation based on RealMirror",
      "authors": [
        "Cong Tai",
        "Hansheng Wu",
        "Haixu Long",
        "Zhengbin Long",
        "Zhaoyu Zheng",
        "Haodong Xiang",
        "Tao Shen"
      ],
      "arxiv_id": "2511.08865v1",
      "summary": "In this work, we present a PICO-based robot remote operating framework that enables low-cost, real-time acquisition of hand motion and pose data, outperforming mainstream visual tracking and motion capture solutions in terms of cost-effectiveness. The framework is natively compatible with the RealMirror ecosystem, offering ready-to-use functionality for stable and precise robotic trajectory recording within the Isaac simulation environment, thereby facilitating the construction of Vision-Language-Action (VLA) datasets. Additionally, the system supports real-time teleoperation of a variety of end-effector-equipped robots, including dexterous hands and robotic grippers. This work aims to lower the technical barriers in the study of upper-limb robotic manipulation, thereby accelerating advancements in VLA-related research.",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08865v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "dexterous hand",
            "[T]teleoperation"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "RePose-NeRF: Robust Radiance Fields for Mesh Reconstruction under Noisy Camera Poses",
      "authors": [
        "Sriram Srinivasan",
        "Gautam Ramachandra"
      ],
      "arxiv_id": "2511.08545v1",
      "summary": "Accurate 3D reconstruction from multi-view images is essential for downstream robotic tasks such as navigation, manipulation, and environment understanding. However, obtaining precise camera poses in real-world settings remains challenging, even when calibration parameters are known. This limits the practicality of existing NeRF-based methods that rely heavily on accurate extrinsic estimates. Furthermore, their implicit volumetric representations differ significantly from the widely adopted polygonal meshes, making rendering and manipulation inefficient in standard 3D software. In this work, we propose a robust framework that reconstructs high-quality, editable 3D meshes directly from multi-view images with noisy extrinsic parameters. Our approach jointly refines camera poses while learning an implicit scene representation that captures fine geometric detail and photorealistic appearance. The resulting meshes are compatible with common 3D graphics and robotics tools, enabling efficient downstream use. Experiments on standard benchmarks demonstrate that our method achieves accurate and robust 3D reconstruction under pose uncertainty, bridging the gap between neural implicit representations and practical robotic applications.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "Several figures are included to illustrate the reconstruction and rendering quality of the proposed method, which is why the submission exceeds the 50MB file size limit. > Several figures are included to illustrate the reconstruction and rendering quality of the proposed method, which is why the submission exceeds the 50,000 KB file size limit (Now this has been resolved)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08545v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]NeRF",
            "navigation"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering",
      "authors": [
        "Laura Bragagnolo",
        "Leonardo Barcellona",
        "Stefano Ghidoni"
      ],
      "arxiv_id": "2511.08294v2",
      "summary": "Accurate 3D human pose estimation is fundamental for applications such as augmented reality and human-robot interaction. State-of-the-art multi-view methods learn to fuse predictions across views by training on large annotated datasets, leading to poor generalization when the test scenario differs. To overcome these limitations, we propose SkelSplat, a novel framework for multi-view 3D human pose estimation based on differentiable Gaussian rendering. Human pose is modeled as a skeleton of 3D Gaussians, one per joint, optimized via differentiable rendering to enable seamless fusion of arbitrary camera views without 3D ground-truth supervision. Since Gaussian Splatting was originally designed for dense scene reconstruction, we propose a novel one-hot encoding scheme that enables independent optimization of human joints. SkelSplat outperforms approaches that do not rely on 3D ground truth in Human3.6M and CMU, while reducing the cross-dataset error up to 47.8% compared to learning-based methods. Experiments on Human3.6M-Occ and Occlusion-Person demonstrate robustness to occlusions, without scenario-specific fine-tuning. Our project page is available here: https://skelsplat.github.io.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-12-02",
      "comment": "WACV 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08294v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "gaussian splatting",
            "scene reconstruction",
            "[T]pose estimation"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "ViPRA: Video Prediction for Robot Actions",
      "authors": [
        "Sandeep Routray",
        "Hengkai Pan",
        "Unnat Jain",
        "Shikhar Bahl",
        "Deepak Pathak"
      ],
      "arxiv_id": "2511.07732v1",
      "summary": "Can we turn a video prediction model into a robot policy? Videos, including those of humans or teleoperated robots, capture rich physical interactions. However, most of them lack labeled actions, which limits their use in robot learning. We present Video Prediction for Robot Actions (ViPRA), a simple pretraining-finetuning framework that learns continuous robot control from these actionless videos. Instead of directly predicting actions, we train a video-language model to predict both future visual observations and motion-centric latent actions, which serve as intermediate representations of scene dynamics. We train these latent actions using perceptual losses and optical flow consistency to ensure they reflect physically grounded behavior. For downstream control, we introduce a chunked flow matching decoder that maps latent actions to robot-specific continuous action sequences, using only 100 to 200 teleoperated demonstrations. This approach avoids expensive action annotation, supports generalization across embodiments, and enables smooth, high-frequency continuous control upto 22 Hz via chunked action decoding. Unlike prior latent action works that treat pretraining as autoregressive policy learning, explicitly models both what changes and how. Our method outperforms strong baselines, with a 16% gain on the SIMPLER benchmark and a 13% improvement across real world manipulation tasks. We will release models and code at https://vipra-project.github.io",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "Website: https://vipra-project.github.io",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07732v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning",
            "flow matching"
          ],
          "score": 3.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "optical flow"
          ],
          "score": 2.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "cross-embodiment"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam",
        "7_retargeting"
      ]
    },
    {
      "title": "Time-Aware Policy Learning for Adaptive and Punctual Robot Control",
      "authors": [
        "Yinsen Jia",
        "Boyuan Chen"
      ],
      "arxiv_id": "2511.07654v1",
      "summary": "Temporal awareness underlies intelligent behavior in both animals and humans, guiding how actions are sequenced, paced, and adapted to changing goals and environments. Yet most robot learning algorithms remain blind to time. We introduce time-aware policy learning, a reinforcement learning framework that enables robots to explicitly perceive and reason with time as a first-class variable. The framework augments conventional reinforcement policies with two complementary temporal signals, the remaining time and a time ratio, which allow a single policy to modulate its behavior continuously from rapid and dynamic to cautious and precise execution. By jointly optimizing punctuality and stability, the robot learns to balance efficiency, robustness, resiliency, and punctuality without re-training or reward adjustment. Across diverse manipulation domains from long-horizon pick and place, to granular-media pouring, articulated-object handling, and multi-agent object delivery, the time-aware policy produces adaptive behaviors that outperform standard reinforcement learning baselines by up to 48% in efficiency, 8 times more robust in sim-to-real transfer, and 90% in acoustic quietness while maintaining near-perfect success rates. Explicit temporal reasoning further enables real-time human-in-the-loop control and multi-agent coordination, allowing robots to recover from disturbances, re-synchronize after delays, and align motion tempo with human intent. By treating time not as a constraint but as a controllable dimension of behavior, time-aware policy learning provides a unified foundation for efficient, robust, resilient, and human-aligned robot autonomy.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07654v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "sim-to-real"
          ],
          "score": 4.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "[T]policy learning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Lightning Grasp: High Performance Procedural Grasp Synthesis with Contact Fields",
      "authors": [
        "Zhao-Heng Yin",
        "Pieter Abbeel"
      ],
      "arxiv_id": "2511.07418v1",
      "summary": "Despite years of research, real-time diverse grasp synthesis for dexterous hands remains an unsolved core challenge in robotics and computer graphics. We present Lightning Grasp, a novel high-performance procedural grasp synthesis algorithm that achieves orders-of-magnitude speedups over state-of-the-art approaches, while enabling unsupervised grasp generation for irregular, tool-like objects. The method avoids many limitations of prior approaches, such as the need for carefully tuned energy functions and sensitive initialization. This breakthrough is driven by a key insight: decoupling complex geometric computation from the search process via a simple, efficient data structure - the Contact Field. This abstraction collapses the problem complexity, enabling a procedural search at unprecedented speeds. We open-source our system to propel further innovation in robotic manipulation.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.DC",
        "cs.GR"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "Code: https://github.com/zhaohengyin/lightning-grasp",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07418v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "dexterous hand",
            "[T]grasp"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Robot Learning from a Physical World Model",
      "authors": [
        "Jiageng Mao",
        "Sicheng He",
        "Hao-Ning Wu",
        "Yang You",
        "Shuyang Sun",
        "Zhicheng Wang",
        "Yanan Bao",
        "Huizhong Chen",
        "Leonidas Guibas",
        "Vitor Guizilini",
        "Howard Zhou",
        "Yue Wang"
      ],
      "arxiv_id": "2511.07416v1",
      "summary": "We introduce PhysWorld, a framework that enables robot learning from video generation through physical world modeling. Recent video generation models can synthesize photorealistic visual demonstrations from language commands and images, offering a powerful yet underexplored source of training signals for robotics. However, directly retargeting pixel motions from generated videos to robots neglects physics, often resulting in inaccurate manipulations. PhysWorld addresses this limitation by coupling video generation with physical world reconstruction. Given a single image and a task command, our method generates task-conditioned videos and reconstructs the underlying physical world from the videos, and the generated video motions are grounded into physically accurate actions through object-centric residual reinforcement learning with the physical world model. This synergy transforms implicit visual guidance into physically executable robotic trajectories, eliminating the need for real robot data collection and enabling zero-shot generalizable robotic manipulation. Experiments on diverse real-world tasks demonstrate that PhysWorld substantially improves manipulation accuracy compared to previous approaches. Visit \\href{https://pointscoder.github.io/PhysWorld_Web/}{the project webpage} for details.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "Project page: https://pointscoder.github.io/PhysWorld_Web/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07416v1",
      "code_links": [
        {
          "url": "https://pointscoder.github.io/PhysWorld_Web/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "[T]world model"
          ],
          "score": 6.0
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "learning from video"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "6_video_extraction"
      ]
    },
    {
      "title": "TrueCity: Real and Simulated Urban Data for Cross-Domain 3D Scene Understanding",
      "authors": [
        "Duc Nguyen",
        "Yan-Ling Lai",
        "Qilin Zhang",
        "Prabin Gyawali",
        "Benedikt Schwab",
        "Olaf Wysocki",
        "Thomas H. Kolbe"
      ],
      "arxiv_id": "2511.07007v1",
      "summary": "3D semantic scene understanding remains a long-standing challenge in the 3D computer vision community. One of the key issues pertains to limited real-world annotated data to facilitate generalizable models. The common practice to tackle this issue is to simulate new data. Although synthetic datasets offer scalability and perfect labels, their designer-crafted scenes fail to capture real-world complexity and sensor noise, resulting in a synthetic-to-real domain gap. Moreover, no benchmark provides synchronized real and simulated point clouds for segmentation-oriented domain shift analysis. We introduce TrueCity, the first urban semantic segmentation benchmark with cm-accurate annotated real-world point clouds, semantic 3D city models, and annotated simulated point clouds representing the same city. TrueCity proposes segmentation classes aligned with international 3D city modeling standards, enabling consistent evaluation of synthetic-to-real gap. Our extensive experiments on common baselines quantify domain shift and highlight strategies for exploiting synthetic data to enhance real-world 3D scene understanding. We are convinced that the TrueCity dataset will foster further development of sim-to-real gap quantification and enable generalizable data-driven models. The data, code, and 3D models are available online: https://tum-gis.github.io/TrueCity/",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "The paper accepted for 3DV 2026 (International Conference on 3D Vision 2026)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07007v1",
      "code_links": [
        {
          "url": "https://tum-gis.github.io/TrueCity/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "sim-to-real"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]scene understanding",
            "point cloud"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Human-Level Actuation for Humanoids",
      "authors": [
        "MD-Nazmus Sunbeam"
      ],
      "arxiv_id": "2511.06796v1",
      "summary": "Claims that humanoid robots achieve ``human-level'' actuation are common but rarely quantified. Peak torque or speed specifications tell us little about whether a joint can deliver the right combination of torque, power, and endurance at task-relevant postures and rates. We introduce a comprehensive framework that makes ``human-level'' measurable and comparable across systems. Our approach has three components. First, a kinematic \\emph{DoF atlas} standardizes joint coordinate systems and ranges of motion using ISB-based conventions, ensuring that human and robot joints are compared in the same reference frames. Second, \\emph{Human-Equivalence Envelopes (HEE)} define per-joint requirements by measuring whether a robot meets human torque \\emph{and} power simultaneously at the same joint angle and rate $(q,ω)$, weighted by positive mechanical work in task-specific bands (walking, stairs, lifting, reaching, and hand actions). Third, the \\emph{Human-Level Actuation Score (HLAS)} aggregates six physically grounded factors: workspace coverage (ROM and DoF), HEE coverage, torque-mode bandwidth, efficiency, and thermal sustainability. We provide detailed measurement protocols using dynamometry, electrical power monitoring, and thermal testing that yield every HLAS input from reproducible experiments. A worked example demonstrates HLAS computation for a multi-joint humanoid, showing how the score exposes actuator trade-offs (gearing ratio versus bandwidth and efficiency) that peak-torque specifications obscure. The framework serves as both a design specification for humanoid development and a benchmarking standard for comparing actuation systems, with all components grounded in published human biomechanics data.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "61 pages, 8 figures, 7 tables, and 12 numbered equations",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06796v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "humanoid robot",
            "walking"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Rethinking Rainy 3D Scene Reconstruction via Perspective Transforming and Brightness Tuning",
      "authors": [
        "Qianfeng Yang",
        "Xiang Chen",
        "Pengpeng Li",
        "Qiyuan Guan",
        "Guiyue Jin",
        "Jiyu Jin"
      ],
      "arxiv_id": "2511.06734v1",
      "summary": "Rain degrades the visual quality of multi-view images, which are essential for 3D scene reconstruction, resulting in inaccurate and incomplete reconstruction results. Existing datasets often overlook two critical characteristics of real rainy 3D scenes: the viewpoint-dependent variation in the appearance of rain streaks caused by their projection onto 2D images, and the reduction in ambient brightness resulting from cloud coverage during rainfall. To improve data realism, we construct a new dataset named OmniRain3D that incorporates perspective heterogeneity and brightness dynamicity, enabling more faithful simulation of rain degradation in 3D scenes. Based on this dataset, we propose an end-to-end reconstruction framework named REVR-GSNet (Rain Elimination and Visibility Recovery for 3D Gaussian Splatting). Specifically, REVR-GSNet integrates recursive brightness enhancement, Gaussian primitive optimization, and GS-guided rain elimination into a unified architecture through joint alternating optimization, achieving high-fidelity reconstruction of clean 3D scenes from rain-degraded inputs. Extensive experiments show the effectiveness of our dataset and method. Our dataset and method provide a foundation for future research on multi-view image deraining and rainy 3D scene reconstruction.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "Accepted by AAAI 2026 (Oral)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06734v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "gaussian splatting",
            "[T]scene reconstruction"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "From Words to Safety: Language-Conditioned Safety Filtering for Robot Navigation",
      "authors": [
        "Zeyuan Feng",
        "Haimingyue Zhang",
        "Somil Bansal"
      ],
      "arxiv_id": "2511.05889v1",
      "summary": "As robots become increasingly integrated into open-world, human-centered environments, their ability to interpret natural language instructions and adhere to safety constraints is critical for effective and trustworthy interaction. Existing approaches often focus on mapping language to reward functions instead of safety specifications or address only narrow constraint classes (e.g., obstacle avoidance), limiting their robustness and applicability. We propose a modular framework for language-conditioned safety in robot navigation. Our framework is composed of three core components: (1) a large language model (LLM)-based module that translates free-form instructions into structured safety specifications, (2) a perception module that grounds these specifications by maintaining object-level 3D representations of the environment, and (3) a model predictive control (MPC)-based safety filter that enforces both semantic and geometric constraints in real time. We evaluate the effectiveness of the proposed framework through both simulation studies and hardware experiments, demonstrating that it robustly interprets and enforces diverse language-specified constraints across a wide range of environments and scenarios.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-08",
      "updated": "2025-11-08",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05889v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "MPC",
            "model predictive control"
          ],
          "score": 4.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "CRISTAL: Real-time Camera Registration in Static LiDAR Scans using Neural Rendering",
      "authors": [
        "Joni Vanherck",
        "Steven Moonen",
        "Brent Zoomers",
        "Kobe Werner",
        "Jeroen Put",
        "Lode Jorissen",
        "Nick Michiels"
      ],
      "arxiv_id": "2511.16349v1",
      "summary": "Accurate camera localization is crucial for robotics and Extended Reality (XR), enabling reliable navigation and alignment of virtual and real content. Existing visual methods often suffer from drift, scale ambiguity, and depend on fiducials or loop closure. This work introduces a real-time method for localizing a camera within a pre-captured, highly accurate colored LiDAR point cloud. By rendering synthetic views from this cloud, 2D-3D correspondences are established between live frames and the point cloud. A neural rendering technique narrows the domain gap between synthetic and real images, reducing occlusion and background artifacts to improve feature matching. The result is drift-free camera tracking with correct metric scale in the global LiDAR coordinate system. Two real-time variants are presented: Online Render and Match, and Prebuild and Localize. We demonstrate improved results on the ScanNet++ dataset and outperform existing SLAM pipelines.",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16349v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "SLAM",
            "point cloud",
            "localization",
            "navigation"
          ],
          "score": 8.0
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "feature matching"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "3_perception_slam",
        "6_video_extraction"
      ]
    },
    {
      "title": "NMPC-based Motion Planning with Adaptive Weighting for Dynamic Object Interception",
      "authors": [
        "Chen Cai",
        "Saksham Kohli",
        "Steven Liu"
      ],
      "arxiv_id": "2511.15532v1",
      "summary": "Catching fast-moving objects serves as a benchmark for robotic agility, posing significant coordination challenges for cooperative manipulator systems holding a catcher, particularly due to inherent closed-chain constraints. This paper presents a nonlinear model predictive control (MPC)-based motion planner that bridges high-level interception planning with real-time joint space control, enabling dynamic object interception for systems comprising two cooperating arms. We introduce an Adaptive- Terminal (AT) MPC formulation featuring cost shaping, which contrasts with a simpler Primitive-Terminal (PT) approach relying heavily on terminal penalties for rapid convergence. The proposed AT formulation is shown to effectively mitigate issues related to actuator power limit violations frequently encountered with the PT strategy, yielding trajectories and significantly reduced control effort. Experimental results on a robotic platform with two cooperative arms, demonstrating excellent real time performance, with an average planner cycle computation time of approximately 19 ms-less than half the 40 ms system sampling time. These results indicate that the AT formulation achieves significantly improved motion quality and robustness with minimal computational overhead compared to the PT baseline, making it well-suited for dynamic, cooperative interception tasks.",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "This work has been submitted to the IFAC World Congress for possible publication. Under review",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15532v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "MPC",
            "model predictive control",
            "[T]motion planning"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "UniHOI: Unified Human-Object Interaction Understanding via Unified Token Space",
      "authors": [
        "Panqi Yang",
        "Haodong Jing",
        "Nanning Zheng",
        "Yongqiang Ma"
      ],
      "arxiv_id": "2511.15046v1",
      "summary": "In the field of human-object interaction (HOI), detection and generation are two dual tasks that have traditionally been addressed separately, hindering the development of comprehensive interaction understanding. To address this, we propose UniHOI, which jointly models HOI detection and generation via a unified token space, thereby effectively promoting knowledge sharing and enhancing generalization. Specifically, we introduce a symmetric interaction-aware attention module and a unified semi-supervised learning paradigm, enabling effective bidirectional mapping between images and interaction semantics even under limited annotations. Extensive experiments demonstrate that UniHOI achieves state-of-the-art performance in both HOI detection and generation. Specifically, UniHOI improves accuracy by 4.9% on long-tailed HOI detection and boosts interaction metrics by 42.0% on open-vocabulary generation tasks.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "Accepted by AAAI 2026,9 pages, 4 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15046v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "[T]human-object interaction",
            "HOI"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "5_interaction_reaction"
      ]
    },
    {
      "title": "Dental3R: Geometry-Aware Pairing for Intraoral 3D Reconstruction from Sparse-View Photographs",
      "authors": [
        "Yiyi Miao",
        "Taoyu Wu",
        "Tong Chen",
        "Ji Jiang",
        "Zhe Tang",
        "Zhengyong Jiang",
        "Angelos Stefanidis",
        "Limin Yu",
        "Jionglong Su"
      ],
      "arxiv_id": "2511.14315v1",
      "summary": "Intraoral 3D reconstruction is fundamental to digital orthodontics, yet conventional methods like intraoral scanning are inaccessible for remote tele-orthodontics, which typically relies on sparse smartphone imagery. While 3D Gaussian Splatting (3DGS) shows promise for novel view synthesis, its application to the standard clinical triad of unposed anterior and bilateral buccal photographs is challenging. The large view baselines, inconsistent illumination, and specular surfaces common in intraoral settings can destabilize simultaneous pose and geometry estimation. Furthermore, sparse-view photometric supervision often induces a frequency bias, leading to over-smoothed reconstructions that lose critical diagnostic details. To address these limitations, we propose \\textbf{Dental3R}, a pose-free, graph-guided pipeline for robust, high-fidelity reconstruction from sparse intraoral photographs. Our method first constructs a Geometry-Aware Pairing Strategy (GAPS) to intelligently select a compact subgraph of high-value image pairs. The GAPS focuses on correspondence matching, thereby improving the stability of the geometry initialization and reducing memory usage. Building on the recovered poses and point cloud, we train the 3DGS model with a wavelet-regularized objective. By enforcing band-limited fidelity using a discrete wavelet transform, our approach preserves fine enamel boundaries and interproximal edges while suppressing high-frequency artifacts. We validate our approach on a large-scale dataset of 950 clinical cases and an additional video-based test set of 195 cases. Experimental results demonstrate that Dental3R effectively handles sparse, unposed inputs and achieves superior novel view synthesis quality for dental occlusion visualization, outperforming state-of-the-art methods.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14315v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "3DGS",
            "gaussian splatting",
            "novel view synthesis",
            "point cloud"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Enhancing Generalization of Depth Estimation Foundation Model via Weakly-Supervised Adaptation with Regularization",
      "authors": [
        "Yan Huang",
        "Yongyi Su",
        "Xin Lin",
        "Le Zhang",
        "Xun Xu"
      ],
      "arxiv_id": "2511.14238v1",
      "summary": "The emergence of foundation models has substantially advanced zero-shot generalization in monocular depth estimation (MDE), as exemplified by the Depth Anything series. However, given access to some data from downstream tasks, a natural question arises: can the performance of these models be further improved? To this end, we propose WeSTAR, a parameter-efficient framework that performs Weakly supervised Self-Training Adaptation with Regularization, designed to enhance the robustness of MDE foundation models in unseen and diverse domains. We first adopt a dense self-training objective as the primary source of structural self-supervision. To further improve robustness, we introduce semantically-aware hierarchical normalization, which exploits instance-level segmentation maps to perform more stable and multi-scale structural normalization. Beyond dense supervision, we introduce a cost-efficient weak supervision in the form of pairwise ordinal depth annotations to further guide the adaptation process, which enforces informative ordinal constraints to mitigate local topological errors. Finally, a weight regularization loss is employed to anchor the LoRA updates, ensuring training stability and preserving the model's generalizable knowledge. Extensive experiments on both realistic and corrupted out-of-distribution datasets under diverse and challenging scenarios demonstrate that WeSTAR consistently improves generalization and achieves state-of-the-art performance across a wide range of benchmarks.",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "Accepted by AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14238v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]depth estimation",
            "monocular depth",
            "Depth Anything"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "GRLoc: Geometric Representation Regression for Visual Localization",
      "authors": [
        "Changyang Li",
        "Xuejian Ma",
        "Lixiang Liu",
        "Zhan Li",
        "Qingan Yan",
        "Yi Xu"
      ],
      "arxiv_id": "2511.13864v1",
      "summary": "Absolute Pose Regression (APR) has emerged as a compelling paradigm for visual localization. However, APR models typically operate as black boxes, directly regressing a 6-DoF pose from a query image, which can lead to memorizing training views rather than understanding 3D scene geometry. In this work, we propose a geometrically-grounded alternative. Inspired by novel view synthesis, which renders images from intermediate geometric representations, we reformulate APR as its inverse that regresses the underlying 3D representations directly from the image, and we name this paradigm Geometric Representation Regression (GRR). Our model explicitly predicts two disentangled geometric representations in the world coordinate system: (1) a ray bundle's directions to estimate camera rotation, and (2) a corresponding pointmap to estimate camera translation. The final 6-DoF camera pose is then recovered from these geometric components using a differentiable deterministic solver. This disentangled approach, which separates the learned visual-to-geometry mapping from the final pose calculation, introduces a strong geometric prior into the network. We find that the explicit decoupling of rotation and translation predictions measurably boosts performance. We demonstrate state-of-the-art performance on 7-Scenes and Cambridge Landmarks datasets, validating that modeling the inverse rendering process is a more robust path toward generalizable absolute pose estimation.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13864v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "novel view synthesis",
            "pose estimation",
            "[T]localization"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "From Power to Precision: Learning Fine-grained Dexterity for Multi-fingered Robotic Hands",
      "authors": [
        "Jianglong Ye",
        "Lai Wei",
        "Guangqi Jiang",
        "Changwei Jing",
        "Xueyan Zou",
        "Xiaolong Wang"
      ],
      "arxiv_id": "2511.13710v1",
      "summary": "Human grasps can be roughly categorized into two types: power grasps and precision grasps. Precision grasping enables tool use and is believed to have influenced human evolution. Today's multi-fingered robotic hands are effective in power grasps, but for tasks requiring precision, parallel grippers are still more widely adopted. This contrast highlights a key limitation in current robotic hand design: the difficulty of achieving both stable power grasps and precise, fine-grained manipulation within a single, versatile system. In this work, we bridge this gap by jointly optimizing the control and hardware design of a multi-fingered dexterous hand, enabling both power and precision manipulation. Rather than redesigning the entire hand, we introduce a lightweight fingertip geometry modification, represent it as a contact plane, and jointly optimize its parameters along with the corresponding control. Our control strategy dynamically switches between power and precision manipulation and simplifies precision control into parallel thumb-index motions, which proves robust for sim-to-real transfer. On the design side, we leverage large-scale simulation to optimize the fingertip geometry using a differentiable neural-physics surrogate model. We validate our approach through extensive experiments in both sim-to-real and real-to-real settings. Our method achieves an 82.5% zero-shot success rate on unseen objects in sim-to-real precision grasping, and a 93.3% success rate in challenging real-world tasks involving bread pinching. These results demonstrate that our co-design framework can significantly enhance the fine-grained manipulation ability of multi-fingered hands without reducing their ability for power grasps. Our project page is at https://jianglongye.com/power-to-precision",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "Project page: https://jianglongye.com/power-to-precision",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13710v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "dexterous hand",
            "grasping",
            "grasp",
            "sim-to-real"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "GaRLILEO: Gravity-aligned Radar-Leg-Inertial Enhanced Odometry",
      "authors": [
        "Chiyun Noh",
        "Sangwoo Jung",
        "Hanjun Kim",
        "Yafei Hu",
        "Laura Herlant",
        "Ayoung Kim"
      ],
      "arxiv_id": "2511.13216v1",
      "summary": "Deployment of legged robots for navigating challenging terrains (e.g., stairs, slopes, and unstructured environments) has gained increasing preference over wheel-based platforms. In such scenarios, accurate odometry estimation is a preliminary requirement for stable locomotion, localization, and mapping. Traditional proprioceptive approaches, which rely on leg kinematics sensor modalities and inertial sensing, suffer from irrepressible vertical drift caused by frequent contact impacts, foot slippage, and vibrations, particularly affected by inaccurate roll and pitch estimation. Existing methods incorporate exteroceptive sensors such as LiDAR or cameras. Further enhancement has been introduced by leveraging gravity vector estimation to add additional observations on roll and pitch, thereby increasing the accuracy of vertical pose estimation. However, these approaches tend to degrade in feature-sparse or repetitive scenes and are prone to errors from double-integrated IMU acceleration. To address these challenges, we propose GaRLILEO, a novel gravity-aligned continuous-time radar-leg-inertial odometry framework. GaRLILEO decouples velocity from the IMU by building a continuous-time ego-velocity spline from SoC radar Doppler and leg kinematics information, enabling seamless sensor fusion which mitigates odometry distortion. In addition, GaRLILEO can reliably capture accurate gravity vectors leveraging a novel soft S2-constrained gravity factor, improving vertical pose accuracy without relying on LiDAR or cameras. Evaluated on a self-collected real-world dataset with diverse indoor-outdoor trajectories, GaRLILEO demonstrates state-of-the-art accuracy, particularly in vertical odometry estimation on stairs and slopes. We open-source both our dataset and algorithm to foster further research in legged robot odometry and SLAM. https://garlileo.github.io/GaRLILEO",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13216v1",
      "code_links": [
        {
          "url": "https://garlileo.github.io/GaRLILEO",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "legged robot",
            "locomotion"
          ],
          "score": 4.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "SLAM",
            "pose estimation",
            "localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Uni-Inter: Unifying 3D Human Motion Synthesis Across Diverse Interaction Contexts",
      "authors": [
        "Sheng Liu",
        "Yuanzhi Liang",
        "Jiepeng Wang",
        "Sidan Du",
        "Chi Zhang",
        "Xuelong Li"
      ],
      "arxiv_id": "2511.13032v1",
      "summary": "We present Uni-Inter, a unified framework for human motion generation that supports a wide range of interaction scenarios: including human-human, human-object, and human-scene-within a single, task-agnostic architecture. In contrast to existing methods that rely on task-specific designs and exhibit limited generalization, Uni-Inter introduces the Unified Interactive Volume (UIV), a volumetric representation that encodes heterogeneous interactive entities into a shared spatial field. This enables consistent relational reasoning and compound interaction modeling. Motion generation is formulated as joint-wise probabilistic prediction over the UIV, allowing the model to capture fine-grained spatial dependencies and produce coherent, context-aware behaviors. Experiments across three representative interaction tasks demonstrate that Uni-Inter achieves competitive performance and generalizes well to novel combinations of entities. These results suggest that unified modeling of compound interactions offers a promising direction for scalable motion synthesis in complex environments.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13032v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "[T]motion synthesis",
            "motion generation"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "4_motion_diffusion"
      ]
    },
    {
      "title": "PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos",
      "authors": [
        "Dianbing Xi",
        "Guoyuan An",
        "Jingsen Zhu",
        "Zhijian Liu",
        "Yuan Liu",
        "Ruiyuan Zhang",
        "Jiayuan Lu",
        "Yuchi Huo",
        "Rui Wang"
      ],
      "arxiv_id": "2511.12935v2",
      "summary": "We propose PFAvatar (Pose-Fusion Avatar), a new method that reconstructs high-quality 3D avatars from Outfit of the Day(OOTD) photos, which exhibit diverse poses, occlusions, and complex backgrounds. Our method consists of two stages: (1) fine-tuning a pose-aware diffusion model from few-shot OOTD examples and (2) distilling a 3D avatar represented by a neural radiance field (NeRF). In the first stage, unlike previous methods that segment images into assets (e.g., garments, accessories) for 3D assembly, which is prone to inconsistency, we avoid decomposition and directly model the full-body appearance. By integrating a pre-trained ControlNet for pose estimation and a novel Condition Prior Preservation Loss (CPPL), our method enables end-to-end learning of fine details while mitigating language drift in few-shot training. Our method completes personalization in just 5 minutes, achieving a 48x speed-up compared to previous approaches. In the second stage, we introduce a NeRF-based avatar representation optimized by canonical SMPL-X space sampling and Multi-Resolution 3D-SDS. Compared to mesh-based representations that suffer from resolution-dependent discretization and erroneous occluded geometry, our continuous radiance field can preserve high-frequency textures (e.g., hair) and handle occlusions correctly through transmittance. Experiments demonstrate that PFAvatar outperforms state-of-the-art methods in terms of reconstruction fidelity, detail preservation, and robustness to occlusions/truncations, advancing practical 3D avatar generation from real-world OOTD albums. In addition, the reconstructed 3D avatar supports downstream applications such as virtual try-on, animation, and human video reenactment, further demonstrating the versatility and practical value of our approach.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-18",
      "comment": "Accepted by AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12935v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "NeRF",
            "neural radiance",
            "pose estimation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "SMPL",
            "SMPL-X"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "3_perception_slam",
        "6_video_extraction"
      ]
    },
    {
      "title": "MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory",
      "authors": [
        "Bo Wang",
        "Jiehong Lin",
        "Chenzhi Liu",
        "Xinting Hu",
        "Yifei Yu",
        "Tianjia Liu",
        "Zhongrui Wang",
        "Xiaojuan Qi"
      ],
      "arxiv_id": "2511.22609v1",
      "summary": "We present MG-Nav (Memory-Guided Navigation), a dual-scale framework for zero-shot visual navigation that unifies global memory-guided planning with local geometry-enhanced control. At its core is the Sparse Spatial Memory Graph (SMG), a compact, region-centric memory where each node aggregates multi-view keyframe and object semantics, capturing both appearance and spatial structure while preserving viewpoint diversity. At the global level, the agent is localized on SMG and a goal-conditioned node path is planned via an image-to-instance hybrid retrieval, producing a sequence of reachable waypoints for long-horizon guidance. At the local level, a navigation foundation policy executes these waypoints in point-goal mode with obstacle-aware control, and switches to image-goal mode when navigating from the final node towards the visual target. To further enhance viewpoint alignment and goal recognition, we introduce VGGT-adapter, a lightweight geometric module built on the pre-trained VGGT model, which aligns observation and goal features in a shared 3D-aware space. MG-Nav operates global planning and local control at different frequencies, using periodic re-localization to correct errors. Experiments on HM3D Instance-Image-Goal and MP3D Image-Goal benchmarks demonstrate that MG-Nav achieves state-of-the-art zero-shot performance and remains robust under dynamic rearrangements and unseen scene conditions.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "10pages, 5 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22609v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization",
            "[T]navigation",
            "VGGT"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Unlocking Zero-shot Potential of Semi-dense Image Matching via Gaussian Splatting",
      "authors": [
        "Juncheng Chen",
        "Chao Xu",
        "Yanjun Cao"
      ],
      "arxiv_id": "2511.21265v1",
      "summary": "Learning-based image matching critically depends on large-scale, diverse, and geometrically accurate training data. 3D Gaussian Splatting (3DGS) enables photorealistic novel-view synthesis and thus is attractive for data generation. However, its geometric inaccuracies and biased depth rendering currently prevent robust correspondence labeling. To address this, we introduce MatchGS, the first framework designed to systematically correct and leverage 3DGS for robust, zero-shot image matching. Our approach is twofold: (1) a geometrically-faithful data generation pipeline that refines 3DGS geometry to produce highly precise correspondence labels, enabling the synthesis of a vast and diverse range of viewpoints without compromising rendering fidelity; and (2) a 2D-3D representation alignment strategy that infuses 3DGS' explicit 3D knowledge into the 2D matcher, guiding 2D semi-dense matchers to learn viewpoint-invariant 3D representations. Our generated ground-truth correspondences reduce the epipolar error by up to 40 times compared to existing datasets, enable supervision under extreme viewpoint changes, and provide self-supervisory signals through Gaussian attributes. Consequently, state-of-the-art matchers trained solely on our data achieve significant zero-shot performance gains on public benchmarks, with improvements of up to 17.7%. Our work demonstrates that with proper geometric refinement, 3DGS can serve as a scalable, high-fidelity, and structurally-rich data source, paving the way for a new generation of robust zero-shot image matchers.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21265v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Material-informed Gaussian Splatting for 3D World Reconstruction in a Digital Twin",
      "authors": [
        "Andy Huynh",
        "João Malheiro Silva",
        "Holger Caesar",
        "Tong Duy Son"
      ],
      "arxiv_id": "2511.20348v2",
      "summary": "3D reconstruction for Digital Twins often relies on LiDAR-based methods, which provide accurate geometry but lack the semantics and textures naturally captured by cameras. Traditional LiDAR-camera fusion approaches require complex calibration and still struggle with certain materials like glass, which are visible in images but poorly represented in point clouds. We propose a camera-only pipeline that reconstructs scenes using 3D Gaussian Splatting from multi-view images, extracts semantic material masks via vision models, converts Gaussian representations to mesh surfaces with projected material labels, and assigns physics-based material properties for accurate sensor simulation in modern graphics engines and simulators. This approach combines photorealistic reconstruction with physics-based material assignment, providing sensor simulation fidelity comparable to LiDAR-camera fusion while eliminating hardware complexity and calibration requirements. We validate our camera-only method using an internal dataset from an instrumented test vehicle, leveraging LiDAR as ground truth for reflectivity validation alongside image similarity metrics.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-28",
      "comment": "8 pages, 5 figures. Submitted to IEEE Intelligent Vehicles Symposium (IV) 2026 for possible publication. Revised version (v2) to correct author order",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20348v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "[T]gaussian splatting",
            "point cloud"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Map-World: Masked Action planning and Path-Integral World Model for Autonomous Driving",
      "authors": [
        "Bin Hu",
        "Zijian Lu",
        "Haicheng Liao",
        "Chengran Yuan",
        "Bin Rao",
        "Yongkang Li",
        "Guofa Li",
        "Zhiyong Cui",
        "Cheng-zhong Xu",
        "Zhenning Li"
      ],
      "arxiv_id": "2511.20156v1",
      "summary": "Motion planning for autonomous driving must handle multiple plausible futures while remaining computationally efficient. Recent end-to-end systems and world-model-based planners predict rich multi-modal trajectories, but typically rely on handcrafted anchors or reinforcement learning to select a single best mode for training and control. This selection discards information about alternative futures and complicates optimization. We propose MAP-World, a prior-free multi-modal planning framework that couples masked action planning with a path-weighted world model. The Masked Action Planning (MAP) module treats future ego motion as masked sequence completion: past waypoints are encoded as visible tokens, future waypoints are represented as mask tokens, and a driving-intent path provides a coarse scaffold. A compact latent planning state is expanded into multiple trajectory queries with injected noise, yielding diverse, temporally consistent modes without anchor libraries or teacher policies. A lightweight world model then rolls out future BEV semantics conditioned on each candidate trajectory. During training, semantic losses are computed as an expectation over modes, using trajectory probabilities as discrete path weights, so the planner learns from the full distribution of plausible futures instead of a single selected path. On NAVSIM, our method matches anchor-based approaches and achieves state-of-the-art performance among world-model-based methods, while avoiding reinforcement learning and maintaining real-time inference latency.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20156v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "motion planning"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "[T]world model"
          ],
          "score": 6.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "ego-motion"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "VGGT4D: Mining Motion Cues in Visual Geometry Transformers for 4D Scene Reconstruction",
      "authors": [
        "Yu Hu",
        "Chong Cheng",
        "Sicheng Yu",
        "Xiaoyang Guo",
        "Hao Wang"
      ],
      "arxiv_id": "2511.19971v1",
      "summary": "Reconstructing dynamic 4D scenes is challenging, as it requires robust disentanglement of dynamic objects from the static background. While 3D foundation models like VGGT provide accurate 3D geometry, their performance drops markedly when moving objects dominate. Existing 4D approaches often rely on external priors, heavy post-optimization, or require fine-tuning on 4D datasets. In this paper, we propose VGGT4D, a training-free framework that extends the 3D foundation model VGGT for robust 4D scene reconstruction. Our approach is motivated by the key finding that VGGT's global attention layers already implicitly encode rich, layer-wise dynamic cues. To obtain masks that decouple static and dynamic elements, we mine and amplify global dynamic cues via gram similarity and aggregate them across a temporal window. To further sharpen mask boundaries, we introduce a refinement strategy driven by projection gradient. We then integrate these precise masks into VGGT's early-stage inference, effectively mitigating motion interference in both pose estimation and geometric reconstruction. Across six datasets, our method achieves superior performance in dynamic object segmentation, camera pose estimation, and dense reconstruction. It also supports single-pass inference on sequences longer than 500 frames.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19971v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]scene reconstruction",
            "pose estimation",
            "VGGT"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation",
      "authors": [
        "Juntao Gao",
        "Feiyang Ye",
        "Jing Zhang",
        "Wenjing Qian"
      ],
      "arxiv_id": "2511.18950v1",
      "summary": "Vision-Language-Action (VLA) models have emerged as a powerful paradigm in Embodied AI. However, the significant computational overhead of processing redundant visual tokens remains a critical bottleneck for real-time robotic deployment. While standard token pruning techniques can alleviate this, these task-agnostic methods struggle to preserve task-critical visual information. To address this challenge, simultaneously preserving both the holistic context and fine-grained details for precise action, we propose Compressor-VLA, a novel hybrid instruction-conditioned token compression framework designed for efficient, task-oriented compression of visual information in VLA models. The proposed Compressor-VLA framework consists of two token compression modules: a Semantic Task Compressor (STC) that distills holistic, task-relevant context, and a Spatial Refinement Compressor (SRC) that preserves fine-grained spatial details. This compression is dynamically modulated by the natural language instruction, allowing for the adaptive condensation of task-relevant visual information. Experimentally, extensive evaluations demonstrate that Compressor-VLA achieves a competitive success rate on the LIBERO benchmark while reducing FLOPs by 59% and the visual token count by over 3x compared to its baseline. The real-robot deployments on a dual-arm robot platform validate the model's sim-to-real transferability and practical applicability. Moreover, qualitative analyses reveal that our instruction guidance dynamically steers the model's perceptual focus toward task-relevant objects, thereby validating the effectiveness of our approach.",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "11 pages, 5 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18950v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "dual-arm",
            "sim-to-real"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Mitigating Long-Tail Bias in HOI Detection via Adaptive Diversity Cache",
      "authors": [
        "Yuqiu Jiang",
        "Xiaozhen Qiao",
        "Tianyu Mei",
        "Haojian Huang",
        "Yifan Chen",
        "Ye Zheng",
        "Zhe Sun"
      ],
      "arxiv_id": "2511.18811v1",
      "summary": "Human-Object Interaction (HOI) detection is a fundamental task in computer vision, empowering machines to comprehend human-object relationships in diverse real-world scenarios. Recent advances in VLMs have significantly improved HOI detection by leveraging rich cross-modal representations. However, most existing VLM-based approaches rely heavily on additional training or prompt tuning, resulting in substantial computational overhead and limited scalability, particularly in long-tailed scenarios where rare interactions are severely underrepresented. In this paper, we propose the Adaptive Diversity Cache (ADC) module, a novel training-free and plug-and-play mechanism designed to mitigate long-tail bias in HOI detection. ADC constructs class-specific caches that accumulate high-confidence and diverse feature representations during inference. The method incorporates frequency-aware cache adaptation that favors rare categories and is designed to enable robust prediction calibration without requiring additional training or fine-tuning. Extensive experiments on HICO-DET and V-COCO datasets show that ADC consistently improves existing HOI detectors, achieving up to +8.57\\% mAP gain on rare categories and +4.39\\% on the full dataset, demonstrating its effectiveness in mitigating long-tail bias while preserving overall performance.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18811v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "human-object interaction",
            "[T]HOI"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "5_interaction_reaction"
      ]
    },
    {
      "title": "Expanding the Workspace of Electromagnetic Navigation Systems Using Dynamic Feedback for Single- and Multi-agent Control",
      "authors": [
        "Jasan Zughaibi",
        "Denis von Arx",
        "Maurus Derungs",
        "Florian Heemeyer",
        "Luca A. Antonelli",
        "Quentin Boehler",
        "Michael Muehlebach",
        "Bradley J. Nelson"
      ],
      "arxiv_id": "2511.18486v1",
      "summary": "Electromagnetic navigation systems (eMNS) enable a number of magnetically guided surgical procedures. A challenge in magnetically manipulating surgical tools is that the effective workspace of an eMNS is often severely constrained by power and thermal limits. We show that system-level control design significantly expands this workspace by reducing the currents needed to achieve a desired motion. We identified five key system approaches that enable this expansion: (i) motion-centric torque/force objectives, (ii) energy-optimal current allocation, (iii) real-time pose estimation, (iv) dynamic feedback, and (v) high-bandwidth eMNS components. As a result, we stabilize a 3D inverted pendulum on an eight-coil OctoMag eMNS with significantly lower currents (0.1-0.2 A vs. 8-14 A), by replacing a field-centric field-alignment strategy with a motion-centric torque/force-based approach. We generalize to multi-agent control by simultaneously stabilizing two inverted pendulums within a shared workspace, exploiting magnetic-field nonlinearity and coil redundancy for independent actuation. A structured analysis compares the electromagnetic workspaces of both paradigms and examines current-allocation strategies that map motion objectives to coil currents. Cross-platform evaluation of the clinically oriented Navion eMNS further demonstrates substantial workspace expansion by maintaining stable balancing at distances up to 50 cm from the coils. The results demonstrate that feedback is a practical path to scalable, efficient, and clinically relevant magnetic manipulation.",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-23",
      "updated": "2025-11-23",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18486v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "pose estimation",
            "[T]navigation"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "ReCoGS: Real-time ReColoring for Gaussian Splatting scenes",
      "authors": [
        "Lorenzo Rutayisire",
        "Nicola Capodieci",
        "Fabio Pellacini"
      ],
      "arxiv_id": "2511.18441v1",
      "summary": "Gaussian Splatting has emerged as a leading method for novel view synthesis, offering superior training efficiency and real-time inference compared to NeRF approaches, while still delivering high-quality reconstructions. Beyond view synthesis, this 3D representation has also been explored for editing tasks. Many existing methods leverage 2D diffusion models to generate multi-view datasets for training, but they often suffer from limitations such as view inconsistencies, lack of fine-grained control, and high computational demand. In this work, we focus specifically on the editing task of recoloring. We introduce a user-friendly pipeline that enables precise selection and recoloring of regions within a pre-trained Gaussian Splatting scene. To demonstrate the real-time performance of our method, we also present an interactive tool that allows users to experiment with the pipeline in practice. Code is available at https://github.com/loryruta/recogs.",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-23",
      "updated": "2025-11-23",
      "comment": "Project page is available at https://github.com/loryruta/recogs",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18441v1",
      "code_links": [
        {
          "url": "https://github.com/loryruta/recogs",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]gaussian splatting",
            "NeRF",
            "novel view synthesis"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "A Coordinated Dual-Arm Framework for Delicate Snap-Fit Assemblies",
      "authors": [
        "Shreyas Kumar",
        "Barat S",
        "Debojit Das",
        "Yug Desai",
        "Siddhi Jain",
        "Rajesh Kumar",
        "Harish J. Palanthandalam-Madapusi"
      ],
      "arxiv_id": "2511.18153v1",
      "summary": "Delicate snap-fit assemblies, such as inserting a lens into an eye-wear frame or during electronics assembly, demand timely engagement detection and rapid force attenuation to prevent overshoot-induced component damage or assembly failure. We address these challenges with two key contributions. First, we introduce SnapNet, a lightweight neural network that detects snap-fit engagement from joint-velocity transients in real-time, showing that reliable detection can be achieved using proprioceptive signals without external sensors. Second, we present a dynamical-systems-based dual-arm coordination framework that integrates SnapNet driven detection with an event-triggered impedance modulation, enabling accurate alignment and compliant insertion during delicate snap-fit assemblies. Experiments across diverse geometries on a heterogeneous bimanual platform demonstrate high detection accuracy (over 96% recall) and up to a 30% reduction in peak impact forces compared to standard impedance control.",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-22",
      "updated": "2025-11-22",
      "comment": "10 pages, 9 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18153v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "bi-manual",
            "bimanual",
            "[T]dual-arm"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Efficient and Scalable Monocular Human-Object Interaction Motion Reconstruction",
      "authors": [
        "Boran Wen",
        "Ye Lu",
        "Keyan Wan",
        "Sirui Wang",
        "Jiahong Zhou",
        "Junxuan Liang",
        "Xinpeng Liu",
        "Bang Xiao",
        "Dingbang Huang",
        "Ruiyang Liu",
        "Yong-Lu Li"
      ],
      "arxiv_id": "2512.00960v2",
      "summary": "Generalized robots must learn from diverse, large-scale human-object interactions (HOI) to operate robustly in the real world. Monocular internet videos offer a nearly limitless and readily available source of data, capturing an unparalleled diversity of human activities, objects, and environments. However, accurately and scalably extracting 4D interaction data from these in-the-wild videos remains a significant and unsolved challenge. Thus, in this work, we introduce 4DHOISolver, a novel and efficient optimization framework that constrains the ill-posed 4D HOI reconstruction problem by leveraging sparse, human-in-the-loop contact point annotations, while maintaining high spatio-temporal coherence and physical plausibility. Leveraging this framework, we introduce Open4DHOI, a new large-scale 4D HOI dataset featuring a diverse catalog of 144 object types and 103 actions. Furthermore, we demonstrate the effectiveness of our reconstructions by enabling an RL-based agent to imitate the recovered motions. However, a comprehensive benchmark of existing 3D foundation models indicates that automatically predicting precise human-object contact correspondences remains an unsolved problem, underscoring the immediate necessity of our human-in-the-loop strategy while posing an open challenge to the community. Data and code will be publicly available at https://wenboran2002.github.io/open4dhoi/",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-30",
      "updated": "2025-12-06",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00960v2",
      "code_links": [
        {
          "url": "https://wenboran2002.github.io/open4dhoi/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "[T]human-object interaction",
            "HOI"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "5_interaction_reaction"
      ]
    },
    {
      "title": "PolarGS: Polarimetric Cues for Ambiguity-Free Gaussian Splatting with Accurate Geometry Recovery",
      "authors": [
        "Bo Guo",
        "Sijia Wen",
        "Yifan Zhao",
        "Jia Li",
        "Zhiming Zheng"
      ],
      "arxiv_id": "2512.00794v1",
      "summary": "Recent advances in surface reconstruction for 3D Gaussian Splatting (3DGS) have enabled remarkable geometric accuracy. However, their performance degrades in photometrically ambiguous regions such as reflective and textureless surfaces, where unreliable cues disrupt photometric consistency and hinder accurate geometry estimation. Reflected light is often partially polarized in a manner that reveals surface orientation, making polarization an optic complement to photometric cues in resolving such ambiguities. Therefore, we propose PolarGS, an optics-aware extension of RGB-based 3DGS that leverages polarization as an optical prior to resolve photometric ambiguities and enhance reconstruction accuracy. Specifically, we introduce two complementary modules: a polarization-guided photometric correction strategy, which ensures photometric consistency by identifying reflective regions via the Degree of Linear Polarization (DoLP) and refining reflective Gaussians with Color Refinement Maps; and a polarization-enhanced Gaussian densification mechanism for textureless area geometry recovery, which integrates both Angle and Degree of Linear Polarization (A/DoLP) into a PatchMatch-based depth completion process. This enables the back-projection and fusion of new Gaussians, leading to more complete reconstruction. PolarGS is framework-agnostic and achieves superior geometric accuracy compared to state-of-the-art methods.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00794v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "3DGS",
            "[T]gaussian splatting"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes",
      "authors": [
        "Xiaoshan Wu",
        "Yifei Yu",
        "Xiaoyang Lyu",
        "Yihua Huang",
        "Bo Wang",
        "Baoheng Zhang",
        "Zhongrui Wang",
        "Xiaojuan Qi"
      ],
      "arxiv_id": "2512.00771v1",
      "summary": "Robust 3D geometry estimation from videos is critical for applications such as autonomous navigation, SLAM, and 3D scene reconstruction. Recent methods like DUSt3R demonstrate that regressing dense pointmaps from image pairs enables accurate and efficient pose-free reconstruction. However, existing RGB-only approaches struggle under real-world conditions involving dynamic objects and extreme illumination, due to the inherent limitations of conventional cameras. In this paper, we propose EAG3R, a novel geometry estimation framework that augments pointmap-based reconstruction with asynchronous event streams. Built upon the MonST3R backbone, EAG3R introduces two key innovations: (1) a retinex-inspired image enhancement module and a lightweight event adapter with SNR-aware fusion mechanism that adaptively combines RGB and event features based on local reliability; and (2) a novel event-based photometric consistency loss that reinforces spatiotemporal coherence during global optimization. Our method enables robust geometry estimation in challenging dynamic low-light scenes without requiring retraining on night-time data. Extensive experiments demonstrate that EAG3R significantly outperforms state-of-the-art RGB-only baselines across monocular depth estimation, camera pose tracking, and dynamic reconstruction tasks.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "Accepted at NeurIPS 2025 (spotlight)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00771v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "SLAM",
            "depth estimation",
            "monocular depth",
            "scene reconstruction",
            "navigation"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "3D Gaussian Point Encoders",
      "authors": [
        "Jim James",
        "Ben Wilson",
        "Simon Lucey",
        "James Hays"
      ],
      "arxiv_id": "2511.04797v1",
      "summary": "In this work, we introduce the 3D Gaussian Point Encoder, an explicit per-point embedding built on mixtures of learned 3D Gaussians. This explicit geometric representation for 3D recognition tasks is a departure from widely used implicit representations such as PointNet. However, it is difficult to learn 3D Gaussian encoders in end-to-end fashion with standard optimizers. We develop optimization techniques based on natural gradients and distillation from PointNets to find a Gaussian Basis that can reconstruct PointNet activations. The resulting 3D Gaussian Point Encoders are faster and more parameter efficient than traditional PointNets. As in the 3D reconstruction literature where there has been considerable interest in the move from implicit (e.g., NeRF) to explicit (e.g., Gaussian Splatting) representations, we can take advantage of computational geometry heuristics to accelerate 3D Gaussian Point Encoders further. We extend filtering techniques from 3D Gaussian Splatting to construct encoders that run 2.7 times faster as a comparable accuracy PointNet while using 46% less memory and 88% fewer FLOPs. Furthermore, we demonstrate the effectiveness of 3D Gaussian Point Encoders as a component in Mamba3D, running 1.27 times faster and achieving a reduction in memory and FLOPs by 42% and 54% respectively. 3D Gaussian Point Encoders are lightweight enough to achieve high framerates on CPU-only devices.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "10 pages, 3 figures, 3 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04797v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "running"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "Mamba"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "gaussian splatting",
            "NeRF"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 9.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "ForeRobo: Unlocking Infinite Simulation Data for 3D Goal-driven Robotic Manipulation",
      "authors": [
        "Dexin wang",
        "Faliang Chang",
        "Chunsheng Liu"
      ],
      "arxiv_id": "2511.04381v1",
      "summary": "Efficiently leveraging simulation to acquire advanced manipulation skills is both challenging and highly significant. We introduce \\textit{ForeRobo}, a generative robotic agent that utilizes generative simulations to autonomously acquire manipulation skills driven by envisioned goal states. Instead of directly learning low-level policies, we advocate integrating generative paradigms with classical control. Our approach equips a robotic agent with a self-guided \\textit{propose-generate-learn-actuate} cycle. The agent first proposes the skills to be acquired and constructs the corresponding simulation environments; it then configures objects into appropriate arrangements to generate skill-consistent goal states (\\textit{ForeGen}). Subsequently, the virtually infinite data produced by ForeGen are used to train the proposed state generation model (\\textit{ForeFormer}), which establishes point-wise correspondences by predicting the 3D goal position of every point in the current state, based on the scene state and task instructions. Finally, classical control algorithms are employed to drive the robot in real-world environments to execute actions based on the envisioned goal states. Compared with end-to-end policy learning methods, ForeFormer offers superior interpretability and execution efficiency. We train and benchmark ForeFormer across a variety of rigid-body and articulated-object manipulation tasks, and observe an average improvement of 56.32\\% over the state-of-the-art state generation models, demonstrating strong generality across different manipulation patterns. Moreover, in real-world evaluations involving more than 20 robotic tasks, ForeRobo achieves zero-shot sim-to-real transfer and exhibits remarkable generalization capabilities, attaining an average success rate of 79.28\\%.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04381v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "sim-to-real"
          ],
          "score": 8.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 9.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Can Context Bridge the Reality Gap? Sim-to-Real Transfer of Context-Aware Policies",
      "authors": [
        "Marco Iannotta",
        "Yuxuan Yang",
        "Johannes A. Stork",
        "Erik Schaffernicht",
        "Todor Stoyanov"
      ],
      "arxiv_id": "2511.04249v1",
      "summary": "Sim-to-real transfer remains a major challenge in reinforcement learning (RL) for robotics, as policies trained in simulation often fail to generalize to the real world due to discrepancies in environment dynamics. Domain Randomization (DR) mitigates this issue by exposing the policy to a wide range of randomized dynamics during training, yet leading to a reduction in performance. While standard approaches typically train policies agnostic to these variations, we investigate whether sim-to-real transfer can be improved by conditioning the policy on an estimate of the dynamics parameters -- referred to as context. To this end, we integrate a context estimation module into a DR-based RL framework and systematically compare SOTA supervision strategies. We evaluate the resulting context-aware policies in both a canonical control benchmark and a real-world pushing task using a Franka Emika Panda robot. Results show that context-aware policies outperform the context-agnostic baseline across all settings, although the best supervision strategy depends on the task.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04249v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]sim-to-real",
            "domain randomization"
          ],
          "score": 8.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 9.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Self-Supervised Moving Object Segmentation of Sparse and Noisy Radar Point Clouds",
      "authors": [
        "Leon Schwarzer",
        "Matthias Zeller",
        "Daniel Casado Herraez",
        "Simon Dierl",
        "Michael Heidingsfeld",
        "Cyrill Stachniss"
      ],
      "arxiv_id": "2511.02395v1",
      "summary": "Moving object segmentation is a crucial task for safe and reliable autonomous mobile systems like self-driving cars, improving the reliability and robustness of subsequent tasks like SLAM or path planning. While the segmentation of camera or LiDAR data is widely researched and achieves great results, it often introduces an increased latency by requiring the accumulation of temporal sequences to gain the necessary temporal context. Radar sensors overcome this problem with their ability to provide a direct measurement of a point's Doppler velocity, which can be exploited for single-scan moving object segmentation. However, radar point clouds are often sparse and noisy, making data annotation for use in supervised learning very tedious, time-consuming, and cost-intensive. To overcome this problem, we address the task of self-supervised moving object segmentation of sparse and noisy radar point clouds. We follow a two-step approach of contrastive self-supervised representation learning with subsequent supervised fine-tuning using limited amounts of annotated data. We propose a novel clustering-based contrastive loss function with cluster refinement based on dynamic points removal to pretrain the network to produce motion-aware representations of the radar data. Our method improves label efficiency after fine-tuning, effectively boosting state-of-the-art performance by self-supervised pretraining.",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-04",
      "updated": "2025-11-04",
      "comment": "Accepted for publication at IEEE International Conference on Intelligent Transportation Systems (ITSC 2025), 8 pages, 3 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02395v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "representation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "SLAM",
            "[T]point cloud"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 9.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Bootstrap Off-policy with World Model",
      "authors": [
        "Guojian Zhan",
        "Likun Wang",
        "Xiangteng Zhang",
        "Jiaxin Gao",
        "Masayoshi Tomizuka",
        "Shengbo Eben Li"
      ],
      "arxiv_id": "2511.00423v2",
      "summary": "Online planning has proven effective in reinforcement learning (RL) for improving sample efficiency and final performance. However, using planning for environment interaction inevitably introduces a divergence between the collected data and the policy's actual behaviors, degrading both model learning and policy improvement. To address this, we propose BOOM (Bootstrap Off-policy with WOrld Model), a framework that tightly integrates planning and off-policy learning through a bootstrap loop: the policy initializes the planner, and the planner refines actions to bootstrap the policy through behavior alignment. This loop is supported by a jointly learned world model, which enables the planner to simulate future trajectories and provides value targets to facilitate policy improvement. The core of BOOM is a likelihood-free alignment loss that bootstraps the policy using the planner's non-parametric action distribution, combined with a soft value-weighted mechanism that prioritizes high-return behaviors and mitigates variability in the planner's action quality within the replay buffer. Experiments on the high-dimensional DeepMind Control Suite and Humanoid-Bench show that BOOM achieves state-of-the-art results in both training stability and final performance. The code is accessible at https://github.com/molumitu/BOOM_MBRL.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-01",
      "updated": "2025-11-21",
      "comment": "NeurIPS 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00423v2",
      "code_links": [
        {
          "url": "https://github.com/molumitu/BOOM_MBRL",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "policy learning",
            "[T]world model"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 9.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Robot Crash Course: Learning Soft and Stylized Falling",
      "authors": [
        "Pascal Strauch",
        "David Müller",
        "Sammy Christen",
        "Agon Serifi",
        "Ruben Grandia",
        "Espen Knoop",
        "Moritz Bächer"
      ],
      "arxiv_id": "2511.10635v1",
      "summary": "Despite recent advances in robust locomotion, bipedal robots operating in the real world remain at risk of falling. While most research focuses on preventing such events, we instead concentrate on the phenomenon of falling itself. Specifically, we aim to reduce physical damage to the robot while providing users with control over a robot's end pose. To this end, we propose a robot agnostic reward function that balances the achievement of a desired end pose with impact minimization and the protection of critical robot parts during reinforcement learning. To make the policy robust to a broad range of initial falling conditions and to enable the specification of an arbitrary and unseen end pose at inference time, we introduce a simulation-based sampling strategy of initial and end poses. Through simulated and real-world experiments, our work demonstrates that even bipedal robots can perform controlled, soft falls.",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10635v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "bipedal",
            "bipedal robot",
            "biped",
            "locomotion"
          ],
          "score": 8.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 9.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "High-Altitude Balloon Station-Keeping with First Order Model Predictive Control",
      "authors": [
        "Myles Pasetsky",
        "Jiawei Lin",
        "Bradley Guo",
        "Sarah Dean"
      ],
      "arxiv_id": "2511.07761v1",
      "summary": "High-altitude balloons (HABs) are common in scientific research due to their wide range of applications and low cost. Because of their nonlinear, underactuated dynamics and the partial observability of wind fields, prior work has largely relied on model-free reinforcement learning (RL) methods to design near-optimal control schemes for station-keeping. These methods often compare only against hand-crafted heuristics, dismissing model-based approaches as impractical given the system complexity and uncertain wind forecasts. We revisit this assumption about the efficacy of model-based control for station-keeping by developing First-Order Model Predictive Control (FOMPC). By implementing the wind and balloon dynamics as differentiable functions in JAX, we enable gradient-based trajectory optimization for online planning. FOMPC outperforms a state-of-the-art RL policy, achieving a 24% improvement in time-within-radius (TWR) without requiring offline training, though at the cost of greater online computation per control step. Through systematic ablations of modeling assumptions and control factors, we show that online planning is effective across many configurations, including under simplified wind and dynamics models.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07761v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]model predictive control",
            "trajectory optimization"
          ],
          "score": 8.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 9.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Gait Recognition via Collaborating Discriminative and Generative Diffusion Models",
      "authors": [
        "Haijun Xiong",
        "Bin Feng",
        "Bang Wang",
        "Xinggang Wang",
        "Wenyu Liu"
      ],
      "arxiv_id": "2511.06245v1",
      "summary": "Gait recognition offers a non-intrusive biometric solution by identifying individuals through their walking patterns. Although discriminative models have achieved notable success in this domain, the full potential of generative models remains largely underexplored. In this paper, we introduce \\textbf{CoD$^2$}, a novel framework that combines the data distribution modeling capabilities of diffusion models with the semantic representation learning strengths of discriminative models to extract robust gait features. We propose a Multi-level Conditional Control strategy that incorporates both high-level identity-aware semantic conditions and low-level visual details. Specifically, the high-level condition, extracted by the discriminative extractor, guides the generation of identity-consistent gait sequences, whereas low-level visual details, such as appearance and motion, are preserved to enhance consistency. Furthermore, the generated sequences facilitate the discriminative extractor's learning, enabling it to capture more comprehensive high-level semantic features. Extensive experiments on four datasets (SUSTech1K, CCPG, GREW, and Gait3D) demonstrate that CoD$^2$ achieves state-of-the-art performance and can be seamlessly integrated with existing discriminative methods, yielding consistent improvements.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-09",
      "updated": "2025-11-09",
      "comment": "14 pages, 4figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06245v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]gait",
            "walking"
          ],
          "score": 8.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "representation learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 9.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Mem-MLP: Real-Time 3D Human Motion Generation from Sparse Inputs",
      "authors": [
        "Sinan Mutlu",
        "Georgios F. Angelis",
        "Savas Ozkan",
        "Paul Wisbey",
        "Anastasios Drosou",
        "Mete Ozay"
      ],
      "arxiv_id": "2511.16264v1",
      "summary": "Realistic and smooth full-body tracking is crucial for immersive AR/VR applications. Existing systems primarily track head and hands via Head Mounted Devices (HMDs) and controllers, making the 3D full-body reconstruction in-complete. One potential approach is to generate the full-body motions from sparse inputs collected from limited sensors using a Neural Network (NN) model. In this paper, we propose a novel method based on a multi-layer perceptron (MLP) backbone that is enhanced with residual connections and a novel NN-component called Memory-Block. In particular, Memory-Block represents missing sensor data with trainable code-vectors, which are combined with the sparse signals from previous time instances to improve the temporal consistency. Furthermore, we formulate our solution as a multi-task learning problem, allowing our MLP-backbone to learn robust representations that boost accuracy. Our experiments show that our method outperforms state-of-the-art baselines by substantially reducing prediction errors. Moreover, it achieves 72 FPS on mobile HMDs that ultimately improves the accuracy-running time tradeoff.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16264v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "running"
          ],
          "score": 2.0
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "[T]motion generation"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 9.5,
      "hit_pillars": [
        "1_robot_core",
        "4_motion_diffusion"
      ]
    },
    {
      "title": "GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization",
      "authors": [
        "Yikun Wang",
        "Zuyan Liu",
        "Ziyi Wang",
        "Pengfei Liu",
        "Han Hu",
        "Yongming Rao"
      ],
      "arxiv_id": "2511.15705v1",
      "summary": "Current research on agentic visual reasoning enables deep multimodal understanding but primarily focuses on image manipulation tools, leaving a gap toward more general-purpose agentic models. In this work, we revisit the geolocalization task, which requires not only nuanced visual grounding but also web search to confirm or refine hypotheses during reasoning. Since existing geolocalization benchmarks fail to meet the need for high-resolution imagery and the localization challenge for deep agentic reasoning, we curate GeoBench, a benchmark that includes photos and panoramas from around the world, along with a subset of satellite images of different cities to rigorously evaluate the geolocalization ability of agentic models. We also propose GeoVista, an agentic model that seamlessly integrates tool invocation within the reasoning loop, including an image-zoom-in tool to magnify regions of interest and a web-search tool to retrieve related web information. We develop a complete training pipeline for it, including a cold-start supervised fine-tuning (SFT) stage to learn reasoning patterns and tool-use priors, followed by a reinforcement learning (RL) stage to further enhance reasoning ability. We adopt a hierarchical reward to leverage multi-level geographical information and improve overall geolocalization performance. Experimental results show that GeoVista surpasses other open-source agentic models on the geolocalization task greatly and achieves performance comparable to closed-source models such as Gemini-2.5-flash and GPT-5 on most metrics.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15705v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 9.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "In-N-On: Scaling Egocentric Manipulation with in-the-wild and on-task Data",
      "authors": [
        "Xiongyi Cai",
        "Ri-Zhao Qiu",
        "Geng Chen",
        "Lai Wei",
        "Isabella Liu",
        "Tianshu Huang",
        "Xuxin Cheng",
        "Xiaolong Wang"
      ],
      "arxiv_id": "2511.15704v1",
      "summary": "Egocentric videos are a valuable and scalable data source to learn manipulation policies. However, due to significant data heterogeneity, most existing approaches utilize human data for simple pre-training, which does not unlock its full potential. This paper first provides a scalable recipe for collecting and using egocentric data by categorizing human data into two categories: in-the-wild and on-task alongside with systematic analysis on how to use the data. We first curate a dataset, PHSD, which contains over 1,000 hours of diverse in-the-wild egocentric data and over 20 hours of on-task data directly aligned to the target manipulation tasks. This enables learning a large egocentric language-conditioned flow matching policy, Human0. With domain adaptation techniques, Human0 minimizes the gap between humans and humanoids. Empirically, we show Human0 achieves several novel properties from scaling human data, including language following of instructions from only human data, few-shot learning, and improved robustness using on-task data. Project website: https://xiongyicai.github.io/In-N-On/",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "Project webpage: https://xiongyicai.github.io/In-N-On/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15704v1",
      "code_links": [
        {
          "url": "https://xiongyicai.github.io/In-N-On/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid",
            "[T]manipulation"
          ],
          "score": 8.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "flow matching"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 9.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Improving Robotic Manipulation Robustness via NICE Scene Surgery",
      "authors": [
        "Sajjad Pakdamansavoji",
        "Mozhgan Pourkeshavarz",
        "Adam Sigal",
        "Zhiyuan Li",
        "Rui Heng Yang",
        "Amir Rasouli"
      ],
      "arxiv_id": "2511.22777v1",
      "summary": "Learning robust visuomotor policies for robotic manipulation remains a challenge in real-world settings, where visual distractors can significantly degrade performance and safety. In this work, we propose an effective and scalable framework, Naturalistic Inpainting for Context Enhancement (NICE). Our method minimizes out-of-distribution (OOD) gap in imitation learning by increasing visual diversity through construction of new experiences using existing demonstrations. By utilizing image generative frameworks and large language models, NICE performs three editing operations, object replacement, restyling, and removal of distracting (non-target) objects. These changes preserve spatial relationships without obstructing target objects and maintain action-label consistency. Unlike previous approaches, NICE requires no additional robot data collection, simulator access, or custom model training, making it readily applicable to existing robotic datasets.\n  Using real-world scenes, we showcase the capability of our framework in producing photo-realistic scene enhancement. For downstream tasks, we use NICE data to finetune a vision-language model (VLM) for spatial affordance prediction and a vision-language-action (VLA) policy for object manipulation. Our evaluations show that NICE successfully minimizes OOD gaps, resulting in over 20% improvement in accuracy for affordance prediction in highly cluttered scenes. For manipulation tasks, success rate increases on average by 11% when testing in environments populated with distractors in different quantities. Furthermore, we show that our method improves visual robustness, lowering target confusion by 6%, and enhances safety by reducing collision rate by 7%.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "11 figures, 3 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22777v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "affordance prediction"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 9.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "RealD$^2$iff: Bridging Real-World Gap in Robot Manipulation via Depth Diffusion",
      "authors": [
        "Xiujian Liang",
        "Jiacheng Liu",
        "Mingyang Sun",
        "Qichen He",
        "Cewu Lu",
        "Jianhua Sun"
      ],
      "arxiv_id": "2511.22505v2",
      "summary": "Robot manipulation in the real world is fundamentally constrained by the visual sim2real gap, where depth observations collected in simulation fail to reflect the complex noise patterns inherent to real sensors. In this work, inspired by the denoising capability of diffusion models, we invert the conventional perspective and propose a clean-to-noisy paradigm that learns to synthesize noisy depth, thereby bridging the visual sim2real gap through purely simulation-driven robotic learning. Building on this idea, we introduce RealD$^2$iff, a hierarchical coarse-to-fine diffusion framework that decomposes depth noise into global structural distortions and fine-grained local perturbations. To enable progressive learning of these components, we further develop two complementary strategies: Frequency-Guided Supervision (FGS) for global structure modeling and Discrepancy-Guided Optimization (DGO) for localized refinement. To integrate RealD$^2$iff seamlessly into imitation learning, we construct a pipeline that spans six stages. We provide comprehensive empirical and experimental validation demonstrating the effectiveness of this paradigm. RealD$^2$iff enables two key applications: (1) generating real-world-like depth to construct clean-noisy paired datasets without manual sensor data collection. (2) Achieving zero-shot sim2real robot manipulation, substantially improving real-world performance without additional fine-tuning.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-27",
      "updated": "2025-12-08",
      "comment": "We are the author team of the paper \"RealD$^2$iff: Bridging Real-World Gap in Robot Manipulation via Depth Diffusion\". After self-examination, our team discovered inappropriate wording in the citation of related work, the introduction, and the contribution statement, which may affect the contribution of other related works. Therefore, we have decided to revise the paper and request its withdrawal",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22505v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "sim2real"
          ],
          "score": 8.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 9.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Scaling Foundation Models for Radar Scene Understanding",
      "authors": [
        "Pushkal Mishra",
        "Kshitiz Bansal",
        "Dinesh Bharadia"
      ],
      "arxiv_id": "2511.21105v1",
      "summary": "Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21105v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "contrastive learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]scene understanding",
            "localization"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 9.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Wanderland: Geometrically Grounded Simulation for Open-World Embodied AI",
      "authors": [
        "Xinhao Liu",
        "Jiaqi Li",
        "Youming Deng",
        "Ruxin Chen",
        "Yingjia Zhang",
        "Yifei Ma",
        "Li Guo",
        "Yiming Li",
        "Jing Zhang",
        "Chen Feng"
      ],
      "arxiv_id": "2511.20620v1",
      "summary": "Reproducible closed-loop evaluation remains a major bottleneck in Embodied AI such as visual navigation. A promising path forward is high-fidelity simulation that combines photorealistic sensor rendering with geometrically grounded interaction in complex, open-world urban environments. Although recent video-3DGS methods ease open-world scene capturing, they are still unsuitable for benchmarking due to large visual and geometric sim-to-real gaps. To address these challenges, we introduce Wanderland, a real-to-sim framework that features multi-sensor capture, reliable reconstruction, accurate geometry, and robust view synthesis. Using this pipeline, we curate a diverse dataset of indoor-outdoor urban scenes and systematically demonstrate how image-only pipelines scale poorly, how geometry quality impacts novel view synthesis, and how all of these adversely affect navigation policy learning and evaluation reliability. Beyond serving as a trusted testbed for embodied navigation, Wanderland's rich raw sensor data further allows benchmarking of 3D reconstruction and novel view synthesis models. Our work establishes a new foundation for reproducible research in open-world embodied AI. Project website is at https://ai4ce.github.io/wanderland/.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20620v1",
      "code_links": [
        {
          "url": "https://ai4ce.github.io/wanderland/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "sim-to-real"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3DGS",
            "novel view synthesis",
            "navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 9.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "ACIT: Attention-Guided Cross-Modal Interaction Transformer for Pedestrian Crossing Intention Prediction",
      "authors": [
        "Yuanzhe Li",
        "Steffen Müller"
      ],
      "arxiv_id": "2511.20020v1",
      "summary": "Predicting pedestrian crossing intention is crucial for autonomous vehicles to prevent pedestrian-related collisions. However, effectively extracting and integrating complementary cues from different types of data remains one of the major challenges. This paper proposes an attention-guided cross-modal interaction Transformer (ACIT) for pedestrian crossing intention prediction. ACIT leverages six visual and motion modalities, which are grouped into three interaction pairs: (1) Global semantic map and global optical flow, (2) Local RGB image and local optical flow, and (3) Ego-vehicle speed and pedestrian's bounding box. Within each visual interaction pair, a dual-path attention mechanism enhances salient regions within the primary modality through intra-modal self-attention and facilitates deep interactions with the auxiliary modality (i.e., optical flow) via optical flow-guided attention. Within the motion interaction pair, cross-modal attention is employed to model the cross-modal dynamics, enabling the effective extraction of complementary motion features. Beyond pairwise interactions, a multi-modal feature fusion module further facilitates cross-modal interactions at each time step. Furthermore, a Transformer-based temporal feature aggregation module is introduced to capture sequential dependencies. Experimental results demonstrate that ACIT outperforms state-of-the-art methods, achieving accuracy rates of 70% and 89% on the JAADbeh and JAADall datasets, respectively. Extensive ablation studies are further conducted to investigate the contribution of different modules of ACIT.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20020v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "optical flow"
          ],
          "score": 2.0
        },
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "[T]interaction transformer"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 9.5,
      "hit_pillars": [
        "3_perception_slam",
        "5_interaction_reaction"
      ]
    },
    {
      "title": "HAVEN: Hierarchical Adversary-aware Visibility-Enabled Navigation with Cover Utilization using Deep Transformer Q-Networks",
      "authors": [
        "Mihir Chauhan",
        "Damon Conover",
        "Aniket Bera"
      ],
      "arxiv_id": "2512.00592v1",
      "summary": "Autonomous navigation in partially observable environments requires agents to reason beyond immediate sensor input, exploit occlusion, and ensure safety while progressing toward a goal. These challenges arise in many robotics domains, from urban driving and warehouse automation to defense and surveillance. Classical path planning approaches and memoryless reinforcement learning often fail under limited fields of view (FoVs) and occlusions, committing to unsafe or inefficient maneuvers. We propose a hierarchical navigation framework that integrates a Deep Transformer Q-Network (DTQN) as a high-level subgoal selector with a modular low-level controller for waypoint execution. The DTQN consumes short histories of task-aware features, encoding odometry, goal direction, obstacle proximity, and visibility cues, and outputs Q-values to rank candidate subgoals. Visibility-aware candidate generation introduces masking and exposure penalties, rewarding the use of cover and anticipatory safety. A low-level potential field controller then tracks the selected subgoal, ensuring smooth short-horizon obstacle avoidance. We validate our approach in 2D simulation and extend it directly to a 3D Unity-ROS environment by projecting point-cloud perception into the same feature schema, enabling transfer without architectural changes. Results show consistent improvements over classical planners and RL baselines in success rate, safety margins, and time to goal, with ablations confirming the value of temporal memory and visibility-aware candidate design. These findings highlight a generalizable framework for safe navigation under uncertainty, with broad relevance across robotic platforms.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-29",
      "updated": "2025-11-29",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00592v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud",
            "[T]navigation"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 9.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "PUL-SLAM: Path-Uncertainty Co-Optimization with Lightweight Stagnation Detection for Efficient Robotic Exploration",
      "authors": [
        "Yizhen Yin",
        "Dapeng Feng",
        "Hongbo Chen",
        "Yuhua Qi"
      ],
      "arxiv_id": "2511.04180v2",
      "summary": "Existing Active SLAM methodologies face issues such as slow exploration speed and suboptimal paths. To address these limitations, we propose a hybrid framework combining a Path-Uncertainty Co-Optimization Deep Reinforcement Learning framework and a Lightweight Stagnation Detection mechanism. The Path-Uncertainty Co-Optimization framework jointly optimizes travel distance and map uncertainty through a dual-objective reward function, balancing exploration and exploitation. The Lightweight Stagnation Detection reduces redundant exploration through Lidar Static Anomaly Detection and Map Update Stagnation Detection, terminating episodes on low expansion rates. Experimental results show that compared with the frontier-based method and RRT method, our approach shortens exploration time by up to 65% and reduces path distance by up to 42%, significantly improving exploration efficiency in complex environments while maintaining reliable map completeness. Ablation studies confirm that the collaborative mechanism accelerates training convergence. Empirical validation on a physical robotic platform demonstrates the algorithm's practical applicability and its successful transferability from simulation to real-world environments.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-06",
      "updated": "2025-12-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04180v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "deep reinforcement learning"
          ],
          "score": 3.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]SLAM"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Optimizing Earth-Moon Transfer and Cislunar Navigation: Integrating Low-Energy Trajectories, AI Techniques and GNSS-R Technologies",
      "authors": [
        "Arsalan Muhammad",
        "Wasiu Akande Ahmed",
        "Omada Friday Ojonugwa",
        "Paul Puspendu Biswas"
      ],
      "arxiv_id": "2511.03173v1",
      "summary": "The rapid growth of cislunar activities, including lunar landings, the Lunar Gateway, and in-space refueling stations, requires advances in cost-efficient trajectory design and reliable integration of navigation and remote sensing. Traditional Earth-Moon transfers suffer from rigid launch windows and high propellant demands, while Earth-based GNSS systems provide little to no coverage beyond geostationary orbit. This limits autonomy and environmental awareness in cislunar space. This review compares four major transfer strategies by evaluating velocity requirements, flight durations, and fuel efficiency, and by identifying their suitability for both crewed and robotic missions. The emerging role of artificial intelligence and machine learning is highlighted: convolutional neural networks support automated crater recognition and digital terrain model generation, while deep reinforcement learning enables adaptive trajectory refinement during descent and landing to reduce risk and decision latency. The study also examines how GNSS-Reflectometry and advanced Positioning, Navigation, and Timing architectures can extend navigation capabilities beyond current limits. GNSS-R can act as a bistatic radar for mapping lunar ice, soil properties, and surface topography, while PNT systems support autonomous rendezvous, Lagrange point station-keeping, and coordinated satellite swarm operations. Combining these developments establishes a scalable framework for sustainable cislunar exploration and long-term human and robotic presence.",
      "categories": [
        "astro-ph.EP",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "astro-ph.EP",
      "published": "2025-11-05",
      "updated": "2025-11-05",
      "comment": "",
      "doi": "",
      "journal_ref": "Published in the Proceedings of 2nd IAASPAICE 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.03173v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "deep reinforcement learning"
          ],
          "score": 3.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Deployable Vision-driven UAV River Navigation via Human-in-the-loop Preference Alignment",
      "authors": [
        "Zihan Wang",
        "Jianwen Li",
        "Li-Fan Wu",
        "Nina Mahmoudian"
      ],
      "arxiv_id": "2511.01083v1",
      "summary": "Rivers are critical corridors for environmental monitoring and disaster response, where Unmanned Aerial Vehicles (UAVs) guided by vision-driven policies can provide fast, low-cost coverage. However, deployment exposes simulation-trained policies with distribution shift and safety risks and requires efficient adaptation from limited human interventions. We study human-in-the-loop (HITL) learning with a conservative overseer who vetoes unsafe or inefficient actions and provides statewise preferences by comparing the agent's proposal with a corrective override. We introduce Statewise Hybrid Preference Alignment for Robotics (SPAR-H), which fuses direct preference optimization on policy logits with a reward-based pathway that trains an immediate-reward estimator from the same preferences and updates the policy using a trust-region surrogate. With five HITL rollouts collected from a fixed novice policy, SPAR-H achieves the highest final episodic reward and the lowest variance across initial conditions among tested methods. The learned reward model aligns with human-preferred actions and elevates nearby non-intervened choices, supporting stable propagation of improvements. We benchmark SPAR-H against imitation learning (IL), direct preference variants, and evaluative reinforcement learning (RL) in the HITL setting, and demonstrate real-world feasibility of continual preference alignment for UAV river following. Overall, dual statewise preferences empirically provide a practical route to data-efficient online adaptation in riverine navigation.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-02",
      "updated": "2025-11-02",
      "comment": "Submitted to ICRA 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01083v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "imitation learning"
          ],
          "score": 3.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Opinion: Towards Unified Expressive Policy Optimization for Robust Robot Learning",
      "authors": [
        "Haidong Huang",
        "Haiyue Zhu. Jiayu Song",
        "Xixin Zhao",
        "Yaohua Zhou",
        "Jiayi Zhang",
        "Yuze Zhai",
        "Xiaocong Li"
      ],
      "arxiv_id": "2511.10087v1",
      "summary": "Offline-to-online reinforcement learning (O2O-RL) has emerged as a promising paradigm for safe and efficient robotic policy deployment but suffers from two fundamental challenges: limited coverage of multimodal behaviors and distributional shifts during online adaptation. We propose UEPO, a unified generative framework inspired by large language model pretraining and fine-tuning strategies. Our contributions are threefold: (1) a multi-seed dynamics-aware diffusion policy that efficiently captures diverse modalities without training multiple models; (2) a dynamic divergence regularization mechanism that enforces physically meaningful policy diversity; and (3) a diffusion-based data augmentation module that enhances dynamics model generalization. On the D4RL benchmark, UEPO achieves +5.9\\% absolute improvement over Uni-O4 on locomotion tasks and +12.4\\% on dexterous manipulation, demonstrating strong generalization and scalability.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "Accepted by NeurIPS 2025 Workshop on Embodied World Models for Decision Making",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10087v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "locomotion",
            "manipulation",
            "dexterous manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "diffusion policy"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "ReIDMamba: Learning Discriminative Features with Visual State Space Model for Person Re-Identification",
      "authors": [
        "Hongyang Gu",
        "Qisong Yang",
        "Lei Pu",
        "Siming Han",
        "Yao Ding"
      ],
      "arxiv_id": "2511.07948v1",
      "summary": "Extracting robust discriminative features is a critical challenge in person re-identification (ReID). While Transformer-based methods have successfully addressed some limitations of convolutional neural networks (CNNs), such as their local processing nature and information loss resulting from convolution and downsampling operations, they still face the scalability issue due to the quadratic increase in memory and computational requirements with the length of the input sequence. To overcome this, we propose a pure Mamba-based person ReID framework named ReIDMamba. Specifically, we have designed a Mamba-based strong baseline that effectively leverages fine-grained, discriminative global features by introducing multiple class tokens. To further enhance robust features learning within Mamba, we have carefully designed two novel techniques. First, the multi-granularity feature extractor (MGFE) module, designed with a multi-branch architecture and class token fusion, effectively forms multi-granularity features, enhancing both discrimination ability and fine-grained coverage. Second, the ranking-aware triplet regularization (RATR) is introduced to reduce redundancy in features from multiple branches, enhancing the diversity of multi-granularity features by incorporating both intra-class and inter-class diversity constraints, thus ensuring the robustness of person features. To our knowledge, this is the pioneering work that integrates a purely Mamba-driven approach into ReID research. Our proposed ReIDMamba model boasts only one-third the parameters of TransReID, along with lower GPU memory usage and faster inference throughput. Experimental results demonstrate ReIDMamba's superior and promising performance, achieving state-of-the-art performance on five person ReID benchmarks. Code is available at https://github.com/GuHY777/ReIDMamba.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "11 pages, 8 figures. Accepted to IEEE Transactions on Multimedia (TMM). Accepted Manuscript version uploaded",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07948v1",
      "code_links": [
        {
          "url": "https://github.com/GuHY777/ReIDMamba",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba",
            "[T]state space model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "DiLO: Disentangled Latent Optimization for Learning Shape and Deformation in Grouped Deforming 3D Objects",
      "authors": [
        "Mostofa Rafid Uddin",
        "Jana Armouti",
        "Umong Sain",
        "Md Asib Rahman",
        "Xingjian Li",
        "Min Xu"
      ],
      "arxiv_id": "2511.06115v1",
      "summary": "In this work, we propose a disentangled latent optimization-based method for parameterizing grouped deforming 3D objects into shape and deformation factors in an unsupervised manner. Our approach involves the joint optimization of a generator network along with the shape and deformation factors, supported by specific regularization techniques. For efficient amortized inference of disentangled shape and deformation codes, we train two order-invariant PoinNet-based encoder networks in the second stage of our method. We demonstrate several significant downstream applications of our method, including unsupervised deformation transfer, deformation classification, and explainability analysis. Extensive experiments conducted on 3D human, animal, and facial expression datasets demonstrate that our simple approach is highly effective in these downstream tasks, comparable or superior to existing methods with much higher complexity.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-08",
      "updated": "2025-11-08",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06115v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "[T]latent optimization"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "7_retargeting"
      ]
    },
    {
      "title": "LAOF: Robust Latent Action Learning with Optical Flow Constraints",
      "authors": [
        "Xizhou Bu",
        "Jiexi Lyu",
        "Fulei Sun",
        "Ruichen Yang",
        "Zhiqiang Ma",
        "Wei Li"
      ],
      "arxiv_id": "2511.16407v1",
      "summary": "Learning latent actions from large-scale videos is crucial for the pre-training of scalable embodied foundation models, yet existing methods often struggle with action-irrelevant distractors. Although incorporating action supervision can alleviate these distractions, its effectiveness is restricted by the scarcity of available action labels. Optical flow represents pixel-level motion between consecutive frames, naturally suppressing background elements and emphasizing moving objects. Motivated by this, we propose robust Latent Action learning with Optical Flow constraints, called LAOF, a pseudo-supervised framework that leverages the agent's optical flow as an action-driven signal to learn latent action representations robust to distractors. Experimental results show that the latent representations learned by LAOF outperform existing methods on downstream imitation learning and reinforcement learning tasks. This superior performance arises from optical flow constraints, which substantially stabilize training and improve the quality of latent representations under extremely label-scarce conditions, while remaining effective as the proportion of action labels increases to 10 percent. Importantly, even without action supervision, LAOF matches or surpasses action-supervised methods trained with 1 percent of action labels.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "Code can be found at https://github.com/XizoB/LAOF",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16407v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "imitation learning"
          ],
          "score": 3.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]optical flow"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Motion Transfer-Enhanced StyleGAN for Generating Diverse Macaque Facial Expressions",
      "authors": [
        "Takuya Igaue",
        "Catia Correia-Caeiro",
        "Akito Yoshida",
        "Takako Miyabe-Nishiwaki",
        "Ryusuke Hayashi"
      ],
      "arxiv_id": "2511.16711v1",
      "summary": "Generating animal faces using generative AI techniques is challenging because the available training images are limited both in quantity and variation, particularly for facial expressions across individuals. In this study, we focus on macaque monkeys, widely studied in systems neuroscience and evolutionary research, and propose a method to generate their facial expressions using a style-based generative image model (i.e., StyleGAN2). To address data limitations, we implemented: 1) data augmentation by synthesizing new facial expression images using a motion transfer to animate still images with computer graphics, 2) sample selection based on the latent representation of macaque faces from an initially trained StyleGAN2 model to ensure the variation and uniform sampling in training dataset, and 3) loss function refinement to ensure the accurate reproduction of subtle movements, such as eye movements. Our results demonstrate that the proposed method enables the generation of diverse facial expressions for multiple macaque individuals, outperforming models trained solely on original still images. Additionally, we show that our model is effective for style-based image editing, where specific style parameters correspond to distinct facial movements. These findings underscore the model's potential for disentangling motion components as style parameters, providing a valuable tool for research on macaque facial expressions.",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16711v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "[T]motion transfer"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "7_retargeting"
      ]
    },
    {
      "title": "Eq.Bot: Enhance Robotic Manipulation Learning via Group Equivariant Canonicalization",
      "authors": [
        "Jian Deng",
        "Yuandong Wang",
        "Yangfu Zhu",
        "Tao Feng",
        "Tianyu Wo",
        "Zhenzhou Shao"
      ],
      "arxiv_id": "2511.15194v1",
      "summary": "Robotic manipulation systems are increasingly deployed across diverse domains. Yet existing multi-modal learning frameworks lack inherent guarantees of geometric consistency, struggling to handle spatial transformations such as rotations and translations. While recent works attempt to introduce equivariance through bespoke architectural modifications, these methods suffer from high implementation complexity, computational cost, and poor portability. Inspired by human cognitive processes in spatial reasoning, we propose Eq.Bot, a universal canonicalization framework grounded in SE(2) group equivariant theory for robotic manipulation learning. Our framework transforms observations into a canonical space, applies an existing policy, and maps the resulting actions back to the original space. As a model-agnostic solution, Eq.Bot aims to endow models with spatial equivariance without requiring architectural modifications. Extensive experiments demonstrate the superiority of Eq.Bot under both CNN-based (e.g., CLIPort) and Transformer-based (e.g., OpenVLA-OFT) architectures over existing methods on various robotic manipulation tasks, where the most significant improvement can reach 50.0%.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "12 pages, 4 figures and 3 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15194v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "geometric consistency"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "1_robot_core",
        "7_retargeting"
      ]
    },
    {
      "title": "WinMamba: Multi-Scale Shifted Windows in State Space Model for 3D Object Detection",
      "authors": [
        "Longhui Zheng",
        "Qiming Xia",
        "Xiaolu Chen",
        "Zhaoliang Liu",
        "Chenglu Wen"
      ],
      "arxiv_id": "2511.13138v1",
      "summary": "3D object detection is critical for autonomous driving, yet it remains fundamentally challenging to simultaneously maximize computational efficiency and capture long-range spatial dependencies. We observed that Mamba-based models, with their linear state-space design, capture long-range dependencies at lower cost, offering a promising balance between efficiency and accuracy. However, existing methods rely on axis-aligned scanning within a fixed window, inevitably discarding spatial information. To address this problem, we propose WinMamba, a novel Mamba-based 3D feature-encoding backbone composed of stacked WinMamba blocks. To enhance the backbone with robust multi-scale representation, the WinMamba block incorporates a window-scale-adaptive module that compensates voxel features across varying resolutions during sampling. Meanwhile, to obtain rich contextual cues within the linear state space, we equip the WinMamba layer with a learnable positional encoding and a window-shift strategy. Extensive experiments on the KITTI and Waymo datasets demonstrate that WinMamba significantly outperforms the baseline. Ablation studies further validate the individual contributions of the WSF and AWF modules in improving detection accuracy. The code will be made publicly available.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "9 pages, 3 figures,",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13138v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba",
            "[T]state space model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "AD-R1: Closed-Loop Reinforcement Learning for End-to-End Autonomous Driving with Impartial World Models",
      "authors": [
        "Tianyi Yan",
        "Tao Tang",
        "Xingtai Gui",
        "Yongkang Li",
        "Jiasen Zhesng",
        "Weiyao Huang",
        "Lingdong Kong",
        "Wencheng Han",
        "Xia Zhou",
        "Xueyang Zhang",
        "Yifei Zhan",
        "Kun Zhan",
        "Cheng-zhong Xu",
        "Jianbing Shen"
      ],
      "arxiv_id": "2511.20325v1",
      "summary": "End-to-end models for autonomous driving hold the promise of learning complex behaviors directly from sensor data, but face critical challenges in safety and handling long-tail events. Reinforcement Learning (RL) offers a promising path to overcome these limitations, yet its success in autonomous driving has been elusive. We identify a fundamental flaw hindering this progress: a deep seated optimistic bias in the world models used for RL. To address this, we introduce a framework for post-training policy refinement built around an Impartial World Model. Our primary contribution is to teach this model to be honest about danger. We achieve this with a novel data synthesis pipeline, Counterfactual Synthesis, which systematically generates a rich curriculum of plausible collisions and off-road events. This transforms the model from a passive scene completer into a veridical forecaster that remains faithful to the causal link between actions and outcomes. We then integrate this Impartial World Model into our closed-loop RL framework, where it serves as an internal critic. During refinement, the agent queries the critic to ``dream\" of the outcomes for candidate actions. We demonstrate through extensive experiments, including on a new Risk Foreseeing Benchmark, that our model significantly outperforms baselines in predicting failures. Consequently, when used as a critic, it enables a substantial reduction in safety violations in challenging simulations, proving that teaching a model to dream of danger is a critical step towards building truly safe and intelligent autonomous agents.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20325v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "[T]world model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Motion Marionette: Rethinking Rigid Motion Transfer via Prior Guidance",
      "authors": [
        "Haoxuan Wang",
        "Jiachen Tao",
        "Junyi Wu",
        "Gaowen Liu",
        "Ramana Rao Kompella",
        "Yan Yan"
      ],
      "arxiv_id": "2511.19909v1",
      "summary": "We present Motion Marionette, a zero-shot framework for rigid motion transfer from monocular source videos to single-view target images. Previous works typically employ geometric, generative, or simulation priors to guide the transfer process, but these external priors introduce auxiliary constraints that lead to trade-offs between generalizability and temporal consistency. To address these limitations, we propose guiding the motion transfer process through an internal prior that exclusively captures the spatial-temporal transformations and is shared between the source video and any transferred target video. Specifically, we first lift both the source video and the target image into a unified 3D representation space. Motion trajectories are then extracted from the source video to construct a spatial-temporal (SpaT) prior that is independent of object geometry and semantics, encoding relative spatial variations over time. This prior is further integrated with the target object to synthesize a controllable velocity field, which is subsequently refined using Position-Based Dynamics to mitigate artifacts and enhance visual coherence. The resulting velocity field can be flexibly employed for efficient video production. Empirical results demonstrate that Motion Marionette generalizes across diverse objects, produces temporally consistent videos that align well with the source motion, and supports controllable video generation.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19909v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "[T]motion transfer"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "7_retargeting"
      ]
    },
    {
      "title": "Walking the Schrödinger Bridge: A Direct Trajectory for Text-to-3D Generation",
      "authors": [
        "Ziying Li",
        "Xuequan Lu",
        "Xinkui Zhao",
        "Guanjie Cheng",
        "Shuiguang Deng",
        "Jianwei Yin"
      ],
      "arxiv_id": "2511.05609v1",
      "summary": "Recent advancements in optimization-based text-to-3D generation heavily rely on distilling knowledge from pre-trained text-to-image diffusion models using techniques like Score Distillation Sampling (SDS), which often introduce artifacts such as over-saturation and over-smoothing into the generated 3D assets. In this paper, we address this essential problem by formulating the generation process as learning an optimal, direct transport trajectory between the distribution of the current rendering and the desired target distribution, thereby enabling high-quality generation with smaller Classifier-free Guidance (CFG) values. At first, we theoretically establish SDS as a simplified instance of the Schrödinger Bridge framework. We prove that SDS employs the reverse process of an Schrödinger Bridge, which, under specific conditions (e.g., a Gaussian noise as one end), collapses to SDS's score function of the pre-trained diffusion model. Based upon this, we introduce Trajectory-Centric Distillation (TraCe), a novel text-to-3D generation framework, which reformulates the mathematically trackable framework of Schrödinger Bridge to explicitly construct a diffusion bridge from the current rendering to its text-conditioned, denoised target, and trains a LoRA-adapted model on this trajectory's score dynamics for robust 3D optimization. Comprehensive experiments demonstrate that TraCe consistently achieves superior quality and fidelity to state-of-the-art techniques.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "NeurIPS 2025; https://github.com/emmaleee789/TraCe.git",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05609v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]walking"
          ],
          "score": 6.0
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "classifier-free guidance"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 8.5,
      "hit_pillars": [
        "1_robot_core",
        "4_motion_diffusion"
      ]
    },
    {
      "title": "Dexterous Robotic Piano Playing at Scale",
      "authors": [
        "Le Chen",
        "Yi Zhao",
        "Jan Schneider",
        "Quankai Gao",
        "Simon Guist",
        "Cheng Qian",
        "Juho Kannala",
        "Bernhard Schölkopf",
        "Joni Pajarinen",
        "Dieter Büchler"
      ],
      "arxiv_id": "2511.02504v1",
      "summary": "Endowing robot hands with human-level dexterity has been a long-standing goal in robotics. Bimanual robotic piano playing represents a particularly challenging task: it is high-dimensional, contact-rich, and requires fast, precise control. We present OmniPianist, the first agent capable of performing nearly one thousand music pieces via scalable, human-demonstration-free learning. Our approach is built on three core components. First, we introduce an automatic fingering strategy based on Optimal Transport (OT), allowing the agent to autonomously discover efficient piano-playing strategies from scratch without demonstrations. Second, we conduct large-scale Reinforcement Learning (RL) by training more than 2,000 agents, each specialized in distinct music pieces, and aggregate their experience into a dataset named RP1M++, consisting of over one million trajectories for robotic piano playing. Finally, we employ a Flow Matching Transformer to leverage RP1M++ through large-scale imitation learning, resulting in the OmniPianist agent capable of performing a wide range of musical pieces. Extensive experiments and ablation studies highlight the effectiveness and scalability of our approach, advancing dexterous robotic piano playing at scale.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-04",
      "updated": "2025-11-04",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02504v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "bi-manual",
            "bimanual"
          ],
          "score": 4.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "imitation learning",
            "flow matching"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 8.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "DensePercept-NCSSD: Vision Mamba towards Real-time Dense Visual Perception with Non-Causal State Space Duality",
      "authors": [
        "Tushar Anand",
        "Advik Sinha",
        "Abhijit Das"
      ],
      "arxiv_id": "2511.12671v1",
      "summary": "In this work, we propose an accurate and real-time optical flow and disparity estimation model by fusing pairwise input images in the proposed non-causal selective state space for dense perception tasks. We propose a non-causal Mamba block-based model that is fast and efficient and aptly manages the constraints present in a real-time applications. Our proposed model reduces inference times while maintaining high accuracy and low GPU usage for optical flow and disparity map generation. The results and analysis, and validation in real-life scenario justify that our proposed model can be used for unified real-time and accurate 3D dense perception estimation tasks. The code, along with the models, can be found at https://github.com/vimstereo/DensePerceptNCSSD",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-16",
      "updated": "2025-11-16",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12671v1",
      "code_links": [
        {
          "url": "https://github.com/vimstereo/DensePerceptNCSSD",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba"
          ],
          "score": 4.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "disparity estimation",
            "optical flow"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 8.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "LAHNet: Local Attentive Hashing Network for Point Cloud Registration",
      "authors": [
        "Wentao Qu",
        "Xiaoshui Huang",
        "Liang Xiao"
      ],
      "arxiv_id": "2512.00927v1",
      "summary": "Most existing learning-based point cloud descriptors for point cloud registration focus on perceiving local information of point clouds to generate distinctive features. However, a reasonable and broader receptive field is essential for enhancing feature distinctiveness. In this paper, we propose a Local Attentive Hashing Network for point cloud registration, called LAHNet, which introduces a local attention mechanism with the inductive bias of locality of convolution-like operators into point cloud descriptors. Specifically, a Group Transformer is designed to capture reasonable long-range context between points. This employs a linear neighborhood search strategy, Locality-Sensitive Hashing, enabling uniformly partitioning point clouds into non-overlapping windows. Meanwhile, an efficient cross-window strategy is adopted to further expand the reasonable feature receptive field. Furthermore, building on this effective windowing strategy, we propose an Interaction Transformer to enhance the feature interactions of the overlap regions within point cloud pairs. This computes an overlap matrix to match overlap regions between point cloud pairs by representing each window as a global signal. Extensive results demonstrate that LAHNet can learn robust and distinctive features, achieving significant registration results on real-world indoor and outdoor benchmarks.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00927v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        },
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "interaction transformer"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 8.5,
      "hit_pillars": [
        "3_perception_slam",
        "5_interaction_reaction"
      ]
    },
    {
      "title": "Audio-Visual World Models: Towards Multisensory Imagination in Sight and Sound",
      "authors": [
        "Jiahua Wang",
        "Shannan Yan",
        "Leqi Zheng",
        "Jialong Wu",
        "Yaoxin Mao"
      ],
      "arxiv_id": "2512.00883v1",
      "summary": "World models simulate environmental dynamics to enable agents to plan and reason about future states. While existing approaches have primarily focused on visual observations, real-world perception inherently involves multiple sensory modalities. Audio provides crucial spatial and temporal cues such as sound source localization and acoustic scene properties, yet its integration into world models remains largely unexplored. No prior work has formally defined what constitutes an audio-visual world model or how to jointly capture binaural spatial audio and visual dynamics under precise action control with task reward prediction. This work presents the first formal framework for Audio-Visual World Models (AVWM), formulating multimodal environment simulation as a partially observable Markov decision process with synchronized audio-visual observations, fine-grained actions, and task rewards. To address the lack of suitable training data, we construct AVW-4k, a dataset comprising 30 hours of binaural audio-visual trajectories with action annotations and reward signals across 76 indoor environments. We propose AV-CDiT, an Audio-Visual Conditional Diffusion Transformer with a novel modality expert architecture that balances visual and auditory learning, optimized through a three-stage training strategy for effective multimodal integration. Extensive experiments demonstrate that AV-CDiT achieves high-fidelity multimodal prediction across visual and auditory modalities with reward. Furthermore, we validate its practical utility in continuous audio-visual navigation tasks, where AVWM significantly enhances the agent's performance.",
      "categories": [
        "cs.MM",
        "cs.CV",
        "cs.SD"
      ],
      "primary_category": "cs.MM",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00883v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization",
            "navigation"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 8.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction",
      "authors": [
        "Chen Shi",
        "Shaoshuai Shi",
        "Xiaoyang Lyu",
        "Chunyang Liu",
        "Kehua Sheng",
        "Bo Zhang",
        "Li Jiang"
      ],
      "arxiv_id": "2511.04595v1",
      "summary": "Feed-forward 3D reconstruction for autonomous driving has advanced rapidly, yet existing methods struggle with the joint challenges of sparse, non-overlapping camera views and complex scene dynamics. We present UniSplat, a general feed-forward framework that learns robust dynamic scene reconstruction through unified latent spatio-temporal fusion. UniSplat constructs a 3D latent scaffold, a structured representation that captures geometric and semantic scene context by leveraging pretrained foundation models. To effectively integrate information across spatial views and temporal frames, we introduce an efficient fusion mechanism that operates directly within the 3D scaffold, enabling consistent spatio-temporal alignment. To ensure complete and detailed reconstructions, we design a dual-branch decoder that generates dynamic-aware Gaussians from the fused scaffold by combining point-anchored refinement with voxel-based generation, and maintain a persistent memory of static Gaussians to enable streaming scene completion beyond current camera coverage. Extensive experiments on real-world datasets demonstrate that UniSplat achieves state-of-the-art performance in novel view synthesis, while providing robust and high-quality renderings even for viewpoints outside the original camera coverage.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04595v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "novel view synthesis",
            "[T]scene reconstruction"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Enhancing Fault-Tolerant Space Computing: Guidance Navigation and Control (GNC) and Landing Vision System (LVS) Implementations on Next-Gen Multi-Core Processors",
      "authors": [
        "Kyongsik Yun",
        "David Bayard",
        "Gerik Kubiak",
        "Austin Owens",
        "Andrew Johnson",
        "Ryan Johnson",
        "Dan Scharf",
        "Thomas Lu"
      ],
      "arxiv_id": "2511.04052v1",
      "summary": "Future planetary exploration missions demand high-performance, fault-tolerant computing to enable autonomous Guidance, Navigation, and Control (GNC) and Lander Vision System (LVS) operations during Entry, Descent, and Landing (EDL). This paper evaluates the deployment of GNC and LVS algorithms on next-generation multi-core processors--HPSC, Snapdragon VOXL2, and AMD Xilinx Versal--demonstrating up to 15x speedup for LVS image processing and over 250x speedup for Guidance for Fuel-Optimal Large Divert (GFOLD) trajectory optimization compared to legacy spaceflight hardware. To ensure computational reliability, we present ARBITER (Asynchronous Redundant Behavior Inspection for Trusted Execution and Recovery), a Multi-Core Voting (MV) mechanism that performs real-time fault detection and correction across redundant cores. ARBITER is validated in both static optimization tasks (GFOLD) and dynamic closed-loop control (Attitude Control System). A fault injection study further identifies the gradient computation stage in GFOLD as the most sensitive to bit-level errors, motivating selective protection strategies and vector-based output arbitration. This work establishes a scalable and energy-efficient architecture for future missions, including Mars Sample Return, Enceladus Orbilander, and Ceres Sample Return, where onboard autonomy, low latency, and fault resilience are critical.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04052v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "trajectory optimization"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures",
      "authors": [
        "Florence Klitzner",
        "Blanca Inigo",
        "Benjamin D. Killeen",
        "Lalithkumar Seenivasan",
        "Michelle Song",
        "Axel Krieger",
        "Mathias Unberath"
      ],
      "arxiv_id": "2511.03882v1",
      "summary": "Imitation learning-based robot control policies are enjoying renewed interest in video-based robotics. However, it remains unclear whether this approach applies to X-ray-guided procedures, such as spine instrumentation. This is because interpretation of multi-view X-rays is complex. We examine opportunities and challenges for imitation policy learning in bi-plane-guided cannula insertion. We develop an in silico sandbox for scalable, automated simulation of X-ray-guided spine procedures with a high degree of realism. We curate a dataset of correct trajectories and corresponding bi-planar X-ray sequences that emulate the stepwise alignment of providers. We then train imitation learning policies for planning and open-loop control that iteratively align a cannula solely based on visual information. This precisely controlled setup offers insights into limitations and capabilities of this method. Our policy succeeded on the first attempt in 68.5% of cases, maintaining safe intra-pedicular trajectories across diverse vertebral levels. The policy generalized to complex anatomy, including fractures, and remained robust to varied initializations. Rollouts on real bi-planar X-rays further suggest that the model can produce plausible trajectories, despite training exclusively in simulation. While these preliminary results are promising, we also identify limitations, especially in entry point precision. Full closed-look control will require additional considerations around how to provide sufficiently frequent feedback. With more robust priors and domain knowledge, such models may provide a foundation for future efforts toward lightweight and CT-free robotic intra-operative spinal navigation.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-05",
      "updated": "2025-11-05",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03882v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]policy learning",
            "imitation learning"
          ],
          "score": 6.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Development of the Bioinspired Tendon-Driven DexHand 021 with Proprioceptive Compliance Control",
      "authors": [
        "Jianbo Yuan",
        "Haohua Zhu",
        "Jing Dai",
        "Sheng Yi"
      ],
      "arxiv_id": "2511.03481v2",
      "summary": "The human hand plays a vital role in daily life and industrial applications, yet replicating its multifunctional capabilities-including motion, sensing, and coordinated manipulation with robotic systems remains a formidable challenge. Developing a dexterous robotic hand requires balancing human-like agility with engineering constraints such as complexity, size-to-weight ratio, durability, and force-sensing performance. This letter presents Dex-Hand 021, a high-performance, cable-driven five-finger robotic hand with 12 active and 7 passive degrees of freedom (DoFs), achieving 19 DoFs dexterity in a lightweight 1 kg design. We propose a proprioceptive force-sensing-based admittance control method to enhance manipulation. Experimental results demonstrate its superior performance: a single-finger load capacity exceeding 10 N, fingertip repeatability under 0.001 m, and force estimation errors below 0.2 N. Compared to PID control, joint torques in multi-object grasping are reduced by 31.19%, significantly improves force-sensing capability while preventing overload during collisions. The hand excels in both power and precision grasps, successfully executing 33 GRASP taxonomy motions and complex manipulation tasks. This work advances the design of lightweight, industrial-grade dexterous hands and enhances proprioceptive control, contributing to robotic manipulation and intelligent manufacturing.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-05",
      "updated": "2025-11-09",
      "comment": "8 pages 18 fogures, IEEE RAL accept",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03481v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "dexterous hand",
            "grasping",
            "grasp"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Monocular absolute depth estimation from endoscopy via domain-invariant feature learning and latent consistency",
      "authors": [
        "Hao Li",
        "Daiwei Lu",
        "Jesse d'Almeida",
        "Dilara Isik",
        "Ehsan Khodapanah Aghdam",
        "Nick DiSanto",
        "Ayberk Acar",
        "Susheela Sharma",
        "Jie Ying Wu",
        "Robert J. Webster",
        "Ipek Oguz"
      ],
      "arxiv_id": "2511.02247v1",
      "summary": "Monocular depth estimation (MDE) is a critical task to guide autonomous medical robots. However, obtaining absolute (metric) depth from an endoscopy camera in surgical scenes is difficult, which limits supervised learning of depth on real endoscopic images. Current image-level unsupervised domain adaptation methods translate synthetic images with known depth maps into the style of real endoscopic frames and train depth networks using these translated images with their corresponding depth maps. However a domain gap often remains between real and translated synthetic images. In this paper, we present a latent feature alignment method to improve absolute depth estimation by reducing this domain gap in the context of endoscopic videos of the central airway. Our methods are agnostic to the image translation process and focus on the depth estimation itself. Specifically, the depth network takes translated synthetic and real endoscopic frames as input and learns latent domain-invariant features via adversarial learning and directional feature consistency. The evaluation is conducted on endoscopic videos of central airway phantoms with manually aligned absolute depth maps. Compared to state-of-the-art MDE methods, our approach achieves superior performance on both absolute and relative depth metrics, and consistently improves results across various backbones and pretrained weights. Our code is available at https://github.com/MedICL-VU/MDE.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-04",
      "updated": "2025-11-04",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02247v1",
      "code_links": [
        {
          "url": "https://github.com/MedICL-VU/MDE",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]depth estimation",
            "monocular depth"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "A Quantitative Comparison of Centralised and Distributed Reinforcement Learning-Based Control for Soft Robotic Arms",
      "authors": [
        "Linxin Hou",
        "Qirui Wu",
        "Zhihang Qin",
        "Neil Banerjee",
        "Yongxin Guo",
        "Cecilia Laschi"
      ],
      "arxiv_id": "2511.02192v1",
      "summary": "This paper presents a quantitative comparison between centralised and distributed multi-agent reinforcement learning (MARL) architectures for controlling a soft robotic arm modelled as a Cosserat rod in simulation. Using PyElastica and the OpenAI Gym interface, we train both a global Proximal Policy Optimisation (PPO) controller and a Multi-Agent PPO (MAPPO) under identical budgets. Both approaches are based on the arm having $n$ number of controlled sections. The study systematically varies $n$ and evaluates the performance of the arm to reach a fixed target in three scenarios: default baseline condition, recovery from external disturbance, and adaptation to actuator failure. Quantitative metrics used for the evaluation are mean action magnitude, mean final distance, mean episode length, and success rate. The results show that there are no significant benefits of the distributed policy when the number of controlled sections $n\\le4$. In very simple systems, when $n\\le2$, the centralised policy outperforms the distributed one. When $n$ increases to $4< n\\le 12$, the distributed policy shows a high sample efficiency. In these systems, distributed policy promotes a stronger success rate, resilience, and robustness under local observability and yields faster convergence given the same sample size. However, centralised policies achieve much higher time efficiency during training as it takes much less time to train the same size of samples. These findings highlight the trade-offs between centralised and distributed policy in reinforcement learning-based control for soft robotic systems and provide actionable design guidance for future sim-to-real transfer in soft rod-like manipulators.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-04",
      "updated": "2025-11-04",
      "comment": "7 pages, 4 figures, 2 tables, submitted to RoboSoft 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02192v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "sim-to-real"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "PPO"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning",
      "authors": [
        "Mengtan Zhang",
        "Zizhan Guo",
        "Hongbo Zhao",
        "Yi Feng",
        "Zuyi Xiong",
        "Yue Wang",
        "Shaoyi Du",
        "Hanli Wang",
        "Rui Fan"
      ],
      "arxiv_id": "2511.01502v1",
      "summary": "Unsupervised learning of depth and ego-motion, two fundamental 3D perception tasks, has made significant strides in recent years. However, most methods treat ego-motion as an auxiliary task, either mixing all motion types or excluding depth-independent rotational motions in supervision. Such designs limit the incorporation of strong geometric constraints, reducing reliability and robustness under diverse conditions. This study introduces a discriminative treatment of motion components, leveraging the geometric regularities of their respective rigid flows to benefit both depth and ego-motion estimation. Given consecutive video frames, network outputs first align the optical axes and imaging planes of the source and target cameras. Optical flows between frames are transformed through these alignments, and deviations are quantified to impose geometric constraints individually on each ego-motion component, enabling more targeted refinement. These alignments further reformulate the joint learning process into coaxial and coplanar forms, where depth and each translation component can be mutually derived through closed-form geometric relationships, introducing complementary constraints that improve depth robustness. DiMoDE, a general depth and ego-motion joint learning framework incorporating these designs, achieves state-of-the-art performance on multiple public datasets and a newly collected diverse real-world dataset, particularly under challenging conditions. Our source code will be publicly available at mias.group/DiMoDE upon publication.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "18 pages, 14 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01502v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "optical flow",
            "[T]ego-motion"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Tackling the Kidnapped Robot Problem via Sparse Feasible Hypothesis Sampling and Reliable Batched Multi-Stage Inference",
      "authors": [
        "Muhua Zhang",
        "Lei Ma",
        "Ying Wu",
        "Kai Shen",
        "Deqing Huang",
        "Henry Leung"
      ],
      "arxiv_id": "2511.01219v2",
      "summary": "This paper addresses the Kidnapped Robot Problem (KRP), a core localization challenge of relocalizing a robot in a known map without prior pose estimate when localization loss or at SLAM initialization. For this purpose, a passive 2-D global relocalization framework is proposed. It estimates the global pose efficiently and reliably from a single LiDAR scan and an occupancy grid map while the robot remains stationary, thereby enhancing the long-term autonomy of mobile robots. The proposed framework casts global relocalization as a non-convex problem and solves it via the multi-hypothesis scheme with batched multi-stage inference and early termination, balancing completeness and efficiency. The Rapidly-exploring Random Tree (RRT), under traversability constraints, asymptotically covers the reachable space to generate sparse, uniformly distributed feasible positional hypotheses, fundamentally reducing the sampling space. The hypotheses are preliminarily ordered by the proposed Scan Mean Absolute Difference (SMAD), a coarse beam-error level metric that facilitates the early termination by prioritizing high-likelihood candidates. The SMAD computation is optimized for non-panoramic scans. The Translation-Affinity Scan-to-Map Alignment Metric (TAM) is proposed for reliable orientation selection at hypothesized positions and accurate final pose evaluation to mitigate degradation in conventional likelihood-field metrics under translational uncertainty induced by sparse hypotheses, as well as non-panoramic LiDAR scan and environmental changes. Real-world experiments on a resource-constrained mobile robot with non-panoramic LiDAR scans show that the proposed framework achieves competitive performance in both global relocalization success rate and computational efficiency.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-11-11",
      "comment": "10 pages, 8 figures. This work has been submitted to the IEEE for possible publication",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01219v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "SLAM",
            "occupancy grid",
            "traversability",
            "localization"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "OmniVGGT: Omni-Modality Driven Visual Geometry Grounded Transformer",
      "authors": [
        "Haosong Peng",
        "Hao Li",
        "Yalun Dai",
        "Yushi Lan",
        "Yihang Luo",
        "Tianyu Qi",
        "Zhengshen Zhang",
        "Yufeng Zhan",
        "Junfei Zhang",
        "Wenchao Xu",
        "Ziwei Liu"
      ],
      "arxiv_id": "2511.10560v2",
      "summary": "General 3D foundation models have started to lead the trend of unifying diverse vision tasks, yet most assume RGB-only inputs and ignore readily available geometric cues (e.g., camera intrinsics, poses, and depth maps). To address this issue, we introduce OmniVGGT, a novel framework that can effectively benefit from an arbitrary number of auxiliary geometric modalities during both training and inference. In our framework, a GeoAdapter is proposed to encode depth and camera intrinsics/extrinsics into a spatial foundation model. It employs zero-initialized convolutions to progressively inject geometric information without disrupting the foundation model's representation space. This design ensures stable optimization with negligible overhead, maintaining inference speed comparable to VGGT even with multiple additional inputs. Additionally, a stochastic multimodal fusion regimen is proposed, which randomly samples modality subsets per instance during training. This enables an arbitrary number of modality inputs during testing and promotes learning robust spatial representations instead of overfitting to auxiliary cues. Comprehensive experiments on monocular/multi-view depth estimation, multi-view stereo, and camera pose estimation demonstrate that OmniVGGT outperforms prior methods with auxiliary inputs and achieves state-of-the-art results even with RGB-only input. To further highlight its practical utility, we integrated OmniVGGT into vision-language-action (VLA) models. The enhanced VLA model by OmniVGGT not only outperforms the vanilla point-cloud-based baseline on mainstream benchmarks, but also effectively leverages accessible auxiliary inputs to achieve consistent gains on robotic tasks.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-14",
      "comment": "Project Page: https://livioni.github.io/OmniVGGT-official/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10560v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation",
            "point cloud",
            "pose estimation",
            "VGGT"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization",
      "authors": [
        "Ashutosh Anshul",
        "Shreyas Gopal",
        "Deepu Rajan",
        "Eng Siong Chng"
      ],
      "arxiv_id": "2511.10212v1",
      "summary": "Recent multimodal deepfake detection methods designed for generalization conjecture that single-stage supervised training struggles to generalize across unseen manipulations and datasets. However, such approaches that target generalization require pretraining over real samples. Additionally, these methods primarily focus on detecting audio-visual inconsistencies and may overlook intra-modal artifacts causing them to fail against manipulations that preserve audio-visual alignment. To address these limitations, we propose a single-stage training framework that enhances generalization by incorporating next-frame prediction for both uni-modal and cross-modal features. Additionally, we introduce a window-level attention mechanism to capture discrepancies between predicted and actual frames, enabling the model to detect local artifacts around every frame, which is crucial for accurately classifying fully manipulated videos and effectively localizing deepfake segments in partially spoofed samples. Our model, evaluated on multiple benchmark datasets, demonstrates strong generalization and precise temporal localization.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "Under Review, Multimodal Deepfake detection",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10212v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "A Shared-Autonomy Construction Robotic System for Overhead Works",
      "authors": [
        "David Minkwan Kim",
        "K. M. Brian Lee",
        "Yong Hyeok Seo",
        "Nikola Raicevic",
        "Runfa Blark Li",
        "Kehan Long",
        "Chan Seon Yoon",
        "Dong Min Kang",
        "Byeong Jo Lim",
        "Young Pyoung Kim",
        "Nikolay Atanasov",
        "Truong Nguyen",
        "Se Woong Jun",
        "Young Wook Kim"
      ],
      "arxiv_id": "2511.09695v1",
      "summary": "We present the ongoing development of a robotic system for overhead work such as ceiling drilling. The hardware platform comprises a mobile base with a two-stage lift, on which a bimanual torso is mounted with a custom-designed drilling end effector and RGB-D cameras. To support teleoperation in dynamic environments with limited visibility, we use Gaussian splatting for online 3D reconstruction and introduce motion parameters to model moving objects. For safe operation around dynamic obstacles, we developed a neural configuration-space barrier approach for planning and control. Initial feasibility studies demonstrate the capability of the hardware in drilling, bolting, and anchoring, and the software in safe teleoperation in a dynamic environment.",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "4pages, 8 figures, ICRA construction workshop",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09695v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "bi-manual",
            "bimanual",
            "teleoperation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "gaussian splatting"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "WMPO: World Model-based Policy Optimization for Vision-Language-Action Models",
      "authors": [
        "Fangqi Zhu",
        "Zhengyang Yan",
        "Zicong Hong",
        "Quanxin Shou",
        "Xiao Ma",
        "Song Guo"
      ],
      "arxiv_id": "2511.09515v1",
      "summary": "Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the \"imagined\" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "project website: https://wm-po.github.io",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09515v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "[T]world model"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "DreamPose3D: Hallucinative Diffusion with Prompt Learning for 3D Human Pose Estimation",
      "authors": [
        "Jerrin Bright",
        "Yuhao Chen",
        "John S. Zelek"
      ],
      "arxiv_id": "2511.09502v1",
      "summary": "Accurate 3D human pose estimation remains a critical yet unresolved challenge, requiring both temporal coherence across frames and fine-grained modeling of joint relationships. However, most existing methods rely solely on geometric cues and predict each 3D pose independently, which limits their ability to resolve ambiguous motions and generalize to real-world scenarios. Inspired by how humans understand and anticipate motion, we introduce DreamPose3D, a diffusion-based framework that combines action-aware reasoning with temporal imagination for 3D pose estimation. DreamPose3D dynamically conditions the denoising process using task-relevant action prompts extracted from 2D pose sequences, capturing high-level intent. To model the structural relationships between joints effectively, we introduce a representation encoder that incorporates kinematic joint affinity into the attention mechanism. Finally, a hallucinative pose decoder predicts temporally coherent 3D pose sequences during training, simulating how humans mentally reconstruct motion trajectories to resolve ambiguity in perception. Extensive experiments on benchmarked Human3.6M and MPI-3DHP datasets demonstrate state-of-the-art performance across all metrics. To further validate DreamPose3D's robustness, we tested it on a broadcast baseball dataset, where it demonstrated strong performance despite ambiguous and noisy 2D inputs, effectively handling temporal consistency and intent-driven motion variations.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09502v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]pose estimation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "3D pose estimation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam",
        "6_video_extraction"
      ]
    },
    {
      "title": "BronchOpt : Vision-Based Pose Optimization with Fine-Tuned Foundation Models for Accurate Bronchoscopy Navigation",
      "authors": [
        "Hongchao Shu",
        "Roger D. Soberanis-Mukul",
        "Jiru Xu",
        "Hao Ding",
        "Morgan Ringel",
        "Mali Shen",
        "Saif Iftekar Sayed",
        "Hedyeh Rafii-Tari",
        "Mathias Unberath"
      ],
      "arxiv_id": "2511.09443v1",
      "summary": "Accurate intra-operative localization of the bronchoscope tip relative to patient anatomy remains challenging due to respiratory motion, anatomical variability, and CT-to-body divergence that cause deformation and misalignment between intra-operative views and pre-operative CT. Existing vision-based methods often fail to generalize across domains and patients, leading to residual alignment errors. This work establishes a generalizable foundation for bronchoscopy navigation through a robust vision-based framework and a new synthetic benchmark dataset that enables standardized and reproducible evaluation. We propose a vision-based pose optimization framework for frame-wise 2D-3D registration between intra-operative endoscopic views and pre-operative CT anatomy. A fine-tuned modality- and domain-invariant encoder enables direct similarity computation between real endoscopic RGB frames and CT-rendered depth maps, while a differentiable rendering module iteratively refines camera poses through depth consistency. To enhance reproducibility, we introduce the first public synthetic benchmark dataset for bronchoscopy navigation, addressing the lack of paired CT-endoscopy data. Trained exclusively on synthetic data distinct from the benchmark, our model achieves an average translational error of 2.65 mm and a rotational error of 0.19 rad, demonstrating accurate and stable localization. Qualitative results on real patient data further confirm strong cross-domain generalization, achieving consistent frame-wise 2D-3D alignment without domain-specific adaptation. Overall, the proposed framework achieves robust, domain-invariant localization through iterative vision-based optimization, while the new benchmark provides a foundation for standardized progress in vision-based bronchoscopy navigation.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09443v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization",
            "[T]navigation"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "LODESTAR: Degeneracy-Aware LiDAR-Inertial Odometry with Adaptive Schmidt-Kalman Filter and Data Exploitation",
      "authors": [
        "Eungchang Mason Lee",
        "Kevin Christiansen Marsim",
        "Hyun Myung"
      ],
      "arxiv_id": "2511.09142v1",
      "summary": "LiDAR-inertial odometry (LIO) has been widely used in robotics due to its high accuracy. However, its performance degrades in degenerate environments, such as long corridors and high-altitude flights, where LiDAR measurements are imbalanced or sparse, leading to ill-posed state estimation. In this letter, we present LODESTAR, a novel LIO method that addresses these degeneracies through two key modules: degeneracy-aware adaptive Schmidt-Kalman filter (DA-ASKF) and degeneracy-aware data exploitation (DA-DE). DA-ASKF employs a sliding window to utilize past states and measurements as additional constraints. Specifically, it introduces degeneracy-aware sliding modes that adaptively classify states as active or fixed based on their degeneracy level. Using Schmidt-Kalman update, it partially optimizes active states while preserving fixed states. These fixed states influence the update of active states via their covariances, serving as reference anchors--akin to a lodestar. Additionally, DA-DE prunes less-informative measurements from active states and selectively exploits measurements from fixed states, based on their localizability contribution and the condition number of the Jacobian matrix. Consequently, DA-ASKF enables degeneracy-aware constrained optimization and mitigates measurement sparsity, while DA-DE addresses measurement imbalance. Experimental results show that LODESTAR outperforms existing LiDAR-based odometry methods and degeneracy-aware modules in terms of accuracy and robustness under various degenerate conditions.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "8 pages, 5 figures, 6 tables, accepted for the publication in IEEE Robotics and Automation Letters",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09142v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]lidar-inertial",
            "LIO"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Spatio-Temporal Data Enhanced Vision-Language Model for Traffic Scene Understanding",
      "authors": [
        "Jingtian Ma",
        "Jingyuan Wang",
        "Wayne Xin Zhao",
        "Guoping Liu",
        "Xiang Wen"
      ],
      "arxiv_id": "2511.08978v1",
      "summary": "Nowadays, navigation and ride-sharing apps have collected numerous images with spatio-temporal data. A core technology for analyzing such images, associated with spatiotemporal information, is Traffic Scene Understanding (TSU), which aims to provide a comprehensive description of the traffic scene. Unlike traditional spatio-temporal data analysis tasks, the dependence on both spatio-temporal and visual-textual data introduces distinct challenges to TSU task. However, recent research often treats TSU as a common image understanding task, ignoring the spatio-temporal information and overlooking the interrelations between different aspects of the traffic scene. To address these issues, we propose a novel SpatioTemporal Enhanced Model based on CILP (ST-CLIP) for TSU. Our model uses the classic vision-language model, CLIP, as the backbone, and designs a Spatio-temporal Context Aware Multiaspect Prompt (SCAMP) learning method to incorporate spatiotemporal information into TSU. The prompt learning method consists of two components: A dynamic spatio-temporal context representation module that extracts representation vectors of spatio-temporal data for each traffic scene image, and a bi-level ST-aware multi-aspect prompt learning module that integrates the ST-context representation vectors into word embeddings of prompts for the CLIP model. The second module also extracts low-level visual features and image-wise high-level semantic features to exploit interactive relations among different aspects of traffic scenes. To the best of our knowledge, this is the first attempt to integrate spatio-temporal information into visionlanguage models to facilitate TSU task. Experiments on two realworld datasets demonstrate superior performance in the complex scene understanding scenarios with a few-shot learning strategy.",
      "categories": [
        "cs.MM",
        "cs.CV"
      ],
      "primary_category": "cs.MM",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08978v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]scene understanding",
            "navigation"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Low-cost Multi-agent Fleet for Acoustic Cooperative Localization Research",
      "authors": [
        "Nelson Durrant",
        "Braden Meyers",
        "Matthew McMurray",
        "Clayton Smith",
        "Brighton Anderson",
        "Tristan Hodgins",
        "Kalliyan Velasco",
        "Joshua G. Mangelson"
      ],
      "arxiv_id": "2511.08822v1",
      "summary": "Real-world underwater testing for multi-agent autonomy presents substantial financial and engineering challenges. In this work, we introduce the Configurable Underwater Group of Autonomous Robots (CoUGARs) as a low-cost, configurable autonomous-underwater-vehicle (AUV) platform for multi-agent autonomy research. The base design costs less than $3,000 USD (as of May 2025) and is based on commercially-available and 3D-printed parts, enabling quick customization for various sensor payloads and configurations. Our current expanded model is equipped with a doppler velocity log (DVL) and ultra-short-baseline (USBL) acoustic array/transducer to support research on acoustic-based cooperative localization. State estimation, navigation, and acoustic communications software has been developed and deployed using a containerized software stack and is tightly integrated with the HoloOcean simulator. The system was tested both in simulation and via in-situ field trials in Utah lakes and reservoirs.",
      "categories": [
        "cs.RO",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08822v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization",
            "navigation"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "SeFA-Policy: Fast and Accurate Visuomotor Policy Learning with Selective Flow Alignment",
      "authors": [
        "Rong Xue",
        "Jiageng Mao",
        "Mingtong Zhang",
        "Yue Wang"
      ],
      "arxiv_id": "2511.08583v1",
      "summary": "Developing efficient and accurate visuomotor policies poses a central challenge in robotic imitation learning. While recent rectified flow approaches have advanced visuomotor policy learning, they suffer from a key limitation: After iterative distillation, generated actions may deviate from the ground-truth actions corresponding to the current visual observation, leading to accumulated error as the reflow process repeats and unstable task execution. We present Selective Flow Alignment (SeFA), an efficient and accurate visuomotor policy learning framework. SeFA resolves this challenge by a selective flow alignment strategy, which leverages expert demonstrations to selectively correct generated actions and restore consistency with observations, while preserving multimodality. This design introduces a consistency correction mechanism that ensures generated actions remain observation-aligned without sacrificing the efficiency of one-step flow inference. Extensive experiments across both simulated and real-world manipulation tasks show that SeFA Policy surpasses state-of-the-art diffusion-based and flow-based policies, achieving superior accuracy and robustness while reducing inference latency by over 98%. By unifying rectified flow efficiency with observation-consistent action generation, SeFA provides a scalable and dependable solution for real-time visuomotor policy learning. Code is available on https://github.com/RongXueZoe/SeFA.",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08583v1",
      "code_links": [
        {
          "url": "https://github.com/RongXueZoe/SeFA",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]policy learning",
            "imitation learning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Real-time Point Cloud Data Transmission via L4S for 5G-Edge-Assisted Robotics",
      "authors": [
        "Gerasimos Damigos",
        "Achilleas Santi Seisa",
        "Nikolaos Stathoulopoulos",
        "Sara Sandberg",
        "George Nikolakopoulos"
      ],
      "arxiv_id": "2511.15677v1",
      "summary": "This article presents a novel framework for real-time Light Detection and Ranging (LiDAR) data transmission that leverages rate-adaptive technologies and point cloud encoding methods to ensure low-latency, and low-loss data streaming. The proposed framework is intended for, but not limited to, robotic applications that require real-time data transmission over the internet for offloaded processing. Specifically, the Low Latency, Low Loss, Scalable Throughput L4S-enabled SCReAM v2 transmission framework is extended to incorporate the Draco geometry compression algorithm, enabling dynamic compression of high-bitrate 3D LiDAR data according to the sensed channel capacity and network load. The low-latency 3D LiDAR streaming system is designed to maintain minimal end-to-end delay while constraining encoding errors to meet the accuracy requirements of robotic applications. We demonstrate the effectiveness of the proposed method through real-world experiments conducted over a public 5G network across multi-kilometer urban environments. The low-latency and low-loss requirements are preserved, while real-time offloading and evaluation of 3D SLAM algorithms are used to validate the framework's performance in practical use cases.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "IFAC Submission",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15677v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "SLAM",
            "[T]point cloud"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Multi-modal Deepfake Detection and Localization with FPN-Transformer",
      "authors": [
        "Chende Zheng",
        "Ruiqi Suo",
        "Zhoulin Ji",
        "Jingyi Deng",
        "Fangbin Yi",
        "Chenhao Lin",
        "Chao Shen"
      ],
      "arxiv_id": "2511.08031v1",
      "summary": "The rapid advancement of generative adversarial networks (GANs) and diffusion models has enabled the creation of highly realistic deepfake content, posing significant threats to digital trust across audio-visual domains. While unimodal detection methods have shown progress in identifying synthetic media, their inability to leverage cross-modal correlations and precisely localize forged segments limits their practicality against sophisticated, fine-grained manipulations. To address this, we introduce a multi-modal deepfake detection and localization framework based on a Feature Pyramid-Transformer (FPN-Transformer), addressing critical gaps in cross-modal generalization and temporal boundary regression. The proposed approach utilizes pre-trained self-supervised models (WavLM for audio, CLIP for video) to extract hierarchical temporal features. A multi-scale feature pyramid is constructed through R-TLM blocks with localized attention mechanisms, enabling joint analysis of cross-context temporal dependencies. The dual-branch prediction head simultaneously predicts forgery probabilities and refines temporal offsets of manipulated segments, achieving frame-level localization precision. We evaluate our approach on the test set of the IJCAI'25 DDL-AV benchmark, showing a good performance with a final score of 0.7535 for cross-modal deepfake detection and localization in challenging environments. Experimental results confirm the effectiveness of our approach and provide a novel way for generalized deepfake detection. Our code is available at https://github.com/Zig-HS/MM-DDL",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08031v1",
      "code_links": [
        {
          "url": "https://github.com/Zig-HS/MM-DDL",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Model Predictive Control via Probabilistic Inference: A Tutorial",
      "authors": [
        "Kohei Honda"
      ],
      "arxiv_id": "2511.08019v1",
      "summary": "Model Predictive Control (MPC) is a fundamental framework for optimizing robot behavior over a finite future horizon. While conventional numerical optimization methods can efficiently handle simple dynamics and cost structures, they often become intractable for the nonlinear or non-differentiable systems commonly encountered in robotics. This article provides a tutorial on probabilistic inference-based MPC, presenting a unified theoretical foundation and a comprehensive overview of representative methods. Probabilistic inference-based MPC approaches, such as Model Predictive Path Integral (MPPI) control, have gained significant attention by reinterpreting optimal control as a problem of probabilistic inference. Rather than relying on gradient-based numerical optimization, these methods estimate optimal control distributions through sampling-based techniques, accommodating arbitrary cost functions and dynamics. We first derive the optimal control distribution from the standard optimal control problem, elucidating its probabilistic interpretation and key characteristics. The widely used MPPI algorithm is then derived as a practical example, followed by discussions on prior and variational distribution design, tuning principles, and theoretical aspects. This article aims to serve as a systematic guide for researchers and practitioners seeking to understand, implement, and extend these methods in robotics and beyond.",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "15 pages, 7 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08019v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "MPC",
            "[T]model predictive control"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "EAGLE: Episodic Appearance- and Geometry-aware Memory for Unified 2D-3D Visual Query Localization in Egocentric Vision",
      "authors": [
        "Yifei Cao",
        "Yu Liu",
        "Guolong Wang",
        "Zhu Liu",
        "Kai Wang",
        "Xianjie Zhang",
        "Jizhe Yu",
        "Xun Tu"
      ],
      "arxiv_id": "2511.08007v2",
      "summary": "Egocentric visual query localization is vital for embodied AI and VR/AR, yet remains challenging due to camera motion, viewpoint changes, and appearance variations. We present EAGLE, a novel framework that leverages episodic appearance- and geometry-aware memory to achieve unified 2D-3D visual query localization in egocentric vision. Inspired by avian memory consolidation, EAGLE synergistically integrates segmentation guided by an appearance-aware meta-learning memory (AMM), with tracking driven by a geometry-aware localization memory (GLM). This memory consolidation mechanism, through structured appearance and geometry memory banks, stores high-confidence retrieval samples, effectively supporting both long- and short-term modeling of target appearance variations. This enables precise contour delineation with robust spatial discrimination, leading to significantly improved retrieval accuracy. Furthermore, by integrating the VQL-2D output with a visual geometry grounded Transformer (VGGT), we achieve a efficient unification of 2D and 3D tasks, enabling rapid and accurate back-projection into 3D space. Our method achieves state-ofthe-art performance on the Ego4D-VQ benchmark.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-12",
      "comment": "13 Pages, accepted by AAAI-2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08007v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization",
            "VGGT"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Virtual Traffic Lights for Multi-Robot Navigation: Decentralized Planning with Centralized Conflict Resolution",
      "authors": [
        "Sagar Gupta",
        "Thanh Vinh Nguyen",
        "Thieu Long Phan",
        "Vidul Attri",
        "Archit Gupta",
        "Niroshinie Fernando",
        "Kevin Lee",
        "Seng W. Loke",
        "Ronny Kutadinata",
        "Benjamin Champion",
        "Akansel Cosgun"
      ],
      "arxiv_id": "2511.07811v1",
      "summary": "We present a hybrid multi-robot coordination framework that combines decentralized path planning with centralized conflict resolution. In our approach, each robot autonomously plans its path and shares this information with a centralized node. The centralized system detects potential conflicts and allows only one of the conflicting robots to proceed at a time, instructing others to stop outside the conflicting area to avoid deadlocks. Unlike traditional centralized planning methods, our system does not dictate robot paths but instead provides stop commands, functioning as a virtual traffic light. In simulation experiments with multiple robots, our approach increased the success rate of robots reaching their goals while reducing deadlocks. Furthermore, we successfully validated the system in real-world experiments with two quadruped robots and separately with wheeled Duckiebots.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07811v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "A QP Framework for Improving Data Collection: Quantifying Device-Controller Performance in Robot Teleoperation",
      "authors": [
        "Yuxuan Zhao",
        "Yuanchen Tang",
        "Jindi Zhang",
        "Hongyu Yu"
      ],
      "arxiv_id": "2511.07720v1",
      "summary": "Robot learning empowers the robot system with human brain-like intelligence to autonomously acquire and adapt skills through experience, enhancing flexibility and adaptability in various environments. Aimed at achieving a similar level of capability in large language models (LLMs) for embodied intelligence, data quality plays a crucial role in training a foundational model with diverse robot skills. In this study, we investigate the collection of data for manipulation tasks using teleoperation devices. Different devices yield varying effects when paired with corresponding controller strategies, including position-based inverse kinematics (IK) control, torque-based inverse dynamics (ID) control, and optimization-based compliance control. In this paper, we develop a teleoperation pipeline that is compatible with different teleoperation devices and manipulator controllers. Within the pipeline, we construct the optimal QP formulation with the dynamic nullspace and the impedance tracking as the novel optimal controller to achieve compliant pose tracking and singularity avoidance. Regarding the optimal controller, it adaptively adjusts the weights assignment depending on the robot joint manipulability that reflects the state of joint configuration for the pose tracking in the form of impedance control and singularity avoidance with nullspace tracking. Analysis of quantitative experimental results suggests the quality of the teleoperated trajectory data, including tracking error, occurrence of singularity, and the smoothness of the joints' trajectory, with different combinations of teleoperation interface and the motion controller.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07720v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "[T]teleoperation"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Exact Smooth Reformulations for Trajectory Optimization Under Signal Temporal Logic Specifications",
      "authors": [
        "Shaohang Han",
        "Joris Verhagen",
        "Jana Tumova"
      ],
      "arxiv_id": "2511.07375v1",
      "summary": "We study motion planning under Signal Temporal Logic (STL), a useful formalism for specifying spatial-temporal requirements. We pose STL synthesis as a trajectory optimization problem leveraging the STL robustness semantics. To obtain a differentiable problem without approximation error, we introduce an exact reformulation of the max and min operators. The resulting method is exact, smooth, and sound. We validate it in numerical simulations, demonstrating its practical performance.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07375v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]trajectory optimization",
            "motion planning"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "MUGSQA: Novel Multi-Uncertainty-Based Gaussian Splatting Quality Assessment Method, Dataset, and Benchmarks",
      "authors": [
        "Tianang Chen",
        "Jian Jin",
        "Shilv Cai",
        "Zhuangzi Li",
        "Weisi Lin"
      ],
      "arxiv_id": "2511.06830v1",
      "summary": "Gaussian Splatting (GS) has recently emerged as a promising technique for 3D object reconstruction, delivering high-quality rendering results with significantly improved reconstruction speed. As variants continue to appear, assessing the perceptual quality of 3D objects reconstructed with different GS-based methods remains an open challenge. To address this issue, we first propose a unified multi-distance subjective quality assessment method that closely mimics human viewing behavior for objects reconstructed with GS-based methods in actual applications, thereby better collecting perceptual experiences. Based on it, we also construct a novel GS quality assessment dataset named MUGSQA, which is constructed considering multiple uncertainties of the input data. These uncertainties include the quantity and resolution of input views, the view distance, and the accuracy of the initial point cloud. Moreover, we construct two benchmarks: one to evaluate the robustness of various GS-based reconstruction methods under multiple uncertainties, and the other to evaluate the performance of existing quality assessment metrics. Our dataset and benchmark code will be released soon.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06830v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]gaussian splatting",
            "point cloud"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "ConeGS: Error-Guided Densification Using Pixel Cones for Improved Reconstruction with Fewer Primitives",
      "authors": [
        "Bartłomiej Baranowski",
        "Stefano Esposito",
        "Patricia Gschoßmann",
        "Anpei Chen",
        "Andreas Geiger"
      ],
      "arxiv_id": "2511.06810v1",
      "summary": "3D Gaussian Splatting (3DGS) achieves state-of-the-art image quality and real-time performance in novel view synthesis but often suffers from a suboptimal spatial distribution of primitives. This issue stems from cloning-based densification, which propagates Gaussians along existing geometry, limiting exploration and requiring many primitives to adequately cover the scene. We present ConeGS, an image-space-informed densification framework that is independent of existing scene geometry state. ConeGS first creates a fast Instant Neural Graphics Primitives (iNGP) reconstruction as a geometric proxy to estimate per-pixel depth. During the subsequent 3DGS optimization, it identifies high-error pixels and inserts new Gaussians along the corresponding viewing cones at the predicted depth values, initializing their size according to the cone diameter. A pre-activation opacity penalty rapidly removes redundant Gaussians, while a primitive budgeting strategy controls the total number of primitives, either by a fixed budget or by adapting to scene complexity, ensuring high reconstruction quality. Experiments show that ConeGS consistently enhances reconstruction quality and rendering performance across Gaussian budgets, with especially strong gains under tight primitive constraints where efficient placement is crucial.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06810v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "3DGS",
            "gaussian splatting",
            "novel view synthesis"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "DIAL-GS: Dynamic Instance Aware Reconstruction for Label-free Street Scenes with 4D Gaussian Splatting",
      "authors": [
        "Chenpeng Su",
        "Wenhua Wu",
        "Chensheng Peng",
        "Tianchen Deng",
        "Zhe Liu",
        "Hesheng Wang"
      ],
      "arxiv_id": "2511.06632v1",
      "summary": "Urban scene reconstruction is critical for autonomous driving, enabling structured 3D representations for data synthesis and closed-loop testing. Supervised approaches rely on costly human annotations and lack scalability, while current self-supervised methods often confuse static and dynamic elements and fail to distinguish individual dynamic objects, limiting fine-grained editing. We propose DIAL-GS, a novel dynamic instance-aware reconstruction method for label-free street scenes with 4D Gaussian Splatting. We first accurately identify dynamic instances by exploiting appearance-position inconsistency between warped rendering and actual observation. Guided by instance-level dynamic perception, we employ instance-aware 4D Gaussians as the unified volumetric representation, realizing dynamic-adaptive and instance-aware reconstruction. Furthermore, we introduce a reciprocal mechanism through which identity and dynamics reinforce each other, enhancing both integrity and consistency. Experiments on urban driving scenarios show that DIAL-GS surpasses existing self-supervised baselines in reconstruction quality and instance-level editing, offering a concise yet powerful solution for urban scene modeling.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06632v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]gaussian splatting",
            "scene reconstruction"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Adaptive PID Control for Robotic Systems via Hierarchical Meta-Learning and Reinforcement Learning with Physics-Based Data Augmentation",
      "authors": [
        "JiaHao Wu",
        "ShengWen Yu"
      ],
      "arxiv_id": "2511.06500v1",
      "summary": "Proportional-Integral-Derivative (PID) controllers remain the predominant choice in industrial robotics due to their simplicity and reliability. However, manual tuning of PID parameters for diverse robotic platforms is time-consuming and requires extensive domain expertise. This paper presents a novel hierarchical control framework that combines meta-learning for PID initialization and reinforcement learning (RL) for online adaptation. To address the sample efficiency challenge, a \\textit{physics-based data augmentation} strategy is introduced that generates virtual robot configurations by systematically perturbing physical parameters, enabling effective meta-learning with limited real robot data. The proposed approach is evaluated on two heterogeneous platforms: a 9-DOF Franka Panda manipulator and a 12-DOF Laikago quadruped robot. Experimental results demonstrate that the proposed method achieves 16.6\\% average improvement on Franka Panda (6.26° MAE), with exceptional gains in high-load joints (J2: 80.4\\% improvement from 12.36° to 2.42°). Critically, this work discovers the \\textit{optimization ceiling effect}: RL achieves dramatic improvements when meta-learning exhibits localized high-error joints, but provides no benefit (0.0\\%) when baseline performance is uniformly strong, as observed in Laikago. The method demonstrates robust performance under disturbances (parameter uncertainty: +19.2\\%, no disturbance: +16.6\\%, average: +10.0\\%) with only 10 minutes of training time. Multi-seed analysis across 100 random initializations confirms stable performance (4.81+/-1.64\\% average). These results establish that RL effectiveness is highly dependent on meta-learning baseline quality and error distribution, providing important design guidance for hierarchical control systems.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-09",
      "updated": "2025-11-09",
      "comment": "21 pages,12 tables, 6 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06500v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "MAE"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "ArtReg: Visuo-Tactile based Pose Tracking and Manipulation of Unseen Articulated Objects",
      "authors": [
        "Prajval Kumar Murali",
        "Mohsen Kaboli"
      ],
      "arxiv_id": "2511.06378v1",
      "summary": "Robots operating in real-world environments frequently encounter unknown objects with complex structures and articulated components, such as doors, drawers, cabinets, and tools. The ability to perceive, track, and manipulate these objects without prior knowledge of their geometry or kinematic properties remains a fundamental challenge in robotics. In this work, we present a novel method for visuo-tactile-based tracking of unseen objects (single, multiple, or articulated) during robotic interaction without assuming any prior knowledge regarding object shape or dynamics. Our novel pose tracking approach termed ArtReg (stands for Articulated Registration) integrates visuo-tactile point clouds in an unscented Kalman Filter formulation in the SE(3) Lie Group for point cloud registration. ArtReg is used to detect possible articulated joints in objects using purposeful manipulation maneuvers such as pushing or hold-pulling with a two-robot team. Furthermore, we leverage ArtReg to develop a closed-loop controller for goal-driven manipulation of articulated objects to move the object into the desired pose configuration. We have extensively evaluated our approach on various types of unknown objects through real robot experiments. We also demonstrate the robustness of our method by evaluating objects with varying center of mass, low-light conditions, and with challenging visual backgrounds. Furthermore, we benchmarked our approach on a standard dataset of articulated objects and demonstrated improved performance in terms of pose accuracy compared to state-of-the-art methods. Our experiments indicate that robust and accurate pose tracking leveraging visuo-tactile information enables robots to perceive and interact with unseen complex articulated objects (with revolute or prismatic joints).",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-09",
      "updated": "2025-11-09",
      "comment": "Under review",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06378v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Optimizing 3D Gaussian Splattering for Mobile GPUs",
      "authors": [
        "Md Musfiqur Rahman Sanim",
        "Zhihao Shu",
        "Bahram Afsharmanesh",
        "AmirAli Mirian",
        "Jiexiong Guan",
        "Wei Niu",
        "Bin Ren",
        "Gagan Agrawal"
      ],
      "arxiv_id": "2511.16298v1",
      "summary": "Image-based 3D scene reconstruction, which transforms multi-view images into a structured 3D representation of the surrounding environment, is a common task across many modern applications. 3D Gaussian Splatting (3DGS) is a new paradigm to address this problem and offers considerable efficiency as compared to the previous methods. Motivated by this, and considering various benefits of mobile device deployment (data privacy, operating without internet connectivity, and potentially faster responses), this paper develops Texture3dgs, an optimized mapping of 3DGS for a mobile GPU. A critical challenge in this area turns out to be optimizing for the two-dimensional (2D) texture cache, which needs to be exploited for faster executions on mobile GPUs. As a sorting method dominates the computations in 3DGS on mobile platforms, the core of Texture3dgs is a novel sorting algorithm where the processing, data movement, and placement are highly optimized for 2D memory. The properties of this algorithm are analyzed in view of a cost model for the texture cache. In addition, we accelerate other steps of the 3DGS algorithm through improved variable layout design and other optimizations. End-to-end evaluation shows that Texture3dgs delivers up to 4.1$\\times$ and 1.7$\\times$ speedup for the sorting and overall 3D scene reconstruction, respectively -- while also reducing memory usage by up to 1.6$\\times$ -- demonstrating the effectiveness of our design for efficient mobile 3D scene reconstruction.",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16298v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "3DGS",
            "gaussian splatting",
            "scene reconstruction"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Box6D : Zero-shot Category-level 6D Pose Estimation of Warehouse Boxes",
      "authors": [
        "Yintao Ma",
        "Sajjad Pakdamansavoji",
        "Amir Rasouli",
        "Tongtong Cao"
      ],
      "arxiv_id": "2511.15884v1",
      "summary": "Accurate and efficient 6D pose estimation of novel objects under clutter and occlusion is critical for robotic manipulation across warehouse automation, bin picking, logistics, and e-commerce fulfillment. There are three main approaches in this domain; Model-based methods assume an exact CAD model at inference but require high-resolution meshes and transfer poorly to new environments; Model-free methods that rely on a few reference images or videos are more flexible, however often fail under challenging conditions; Category-level approaches aim to balance flexibility and accuracy but many are overly general and ignore environment and object priors, limiting their practicality in industrial settings.\n  To this end, we propose Box6d, a category-level 6D pose estimation method tailored for storage boxes in the warehouse context. From a single RGB-D observation, Box6D infers the dimensions of the boxes via a fast binary search and estimates poses using a category CAD template rather than instance-specific models. Suing a depth-based plausibility filter and early-stopping strategy, Box6D then rejects implausible hypotheses, lowering computational cost. We conduct evaluations on real-world storage scenarios and public benchmarks, and show that our approach delivers competitive or superior 6D pose precision while reducing inference time by approximately 76%.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15884v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]pose estimation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "WALDO: Where Unseen Model-based 6D Pose Estimation Meets Occlusion",
      "authors": [
        "Sajjad Pakdamansavoji",
        "Yintao Ma",
        "Amir Rasouli",
        "Tongtong Cao"
      ],
      "arxiv_id": "2511.15874v1",
      "summary": "Accurate 6D object pose estimation is vital for robotics, augmented reality, and scene understanding. For seen objects, high accuracy is often attainable via per-object fine-tuning but generalizing to unseen objects remains a challenge. To address this problem, past arts assume access to CAD models at test time and typically follow a multi-stage pipeline to estimate poses: detect and segment the object, propose an initial pose, and then refine it. Under occlusion, however, the early-stage of such pipelines are prone to errors, which can propagate through the sequential processing, and consequently degrade the performance. To remedy this shortcoming, we propose four novel extensions to model-based 6D pose estimation methods: (i) a dynamic non-uniform dense sampling strategy that focuses computation on visible regions, reducing occlusion-induced errors; (ii) a multi-hypothesis inference mechanism that retains several confidence-ranked pose candidates, mitigating brittle single-path failures; (iii) iterative refinement to progressively improve pose accuracy; and (iv) series of occlusion-focused training augmentations that strengthen robustness and generalization. Furthermore, we propose a new weighted by visibility metric for evaluation under occlusion to minimize the bias in the existing protocols. Via extensive empirical evaluations, we show that our proposed approach achieves more than 5% improvement in accuracy on ICBIN and more than 2% on BOP dataset benchmarks, while achieving approximately 3 times faster inference.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15874v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding",
            "[T]pose estimation"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "CompTrack: Information Bottleneck-Guided Low-Rank Dynamic Token Compression for Point Cloud Tracking",
      "authors": [
        "Sifan Zhou",
        "Yichao Cao",
        "Jiahao Nie",
        "Yuqian Fu",
        "Ziyu Zhao",
        "Xiaobo Lu",
        "Shuo Wang"
      ],
      "arxiv_id": "2511.15580v3",
      "summary": "3D single object tracking (SOT) in LiDAR point clouds is a critical task in computer vision and autonomous driving. Despite great success having been achieved, the inherent sparsity of point clouds introduces a dual-redundancy challenge that limits existing trackers: (1) vast spatial redundancy from background noise impairs accuracy, and (2) informational redundancy within the foreground hinders efficiency. To tackle these issues, we propose CompTrack, a novel end-to-end framework that systematically eliminates both forms of redundancy in point clouds. First, CompTrack incorporates a Spatial Foreground Predictor (SFP) module to filter out irrelevant background noise based on information entropy, addressing spatial redundancy. Subsequently, its core is an Information Bottleneck-guided Dynamic Token Compression (IB-DTC) module that eliminates the informational redundancy within the foreground. Theoretically grounded in low-rank approximation, this module leverages an online SVD analysis to adaptively compress the redundant foreground into a compact and highly informative set of proxy tokens. Extensive experiments on KITTI, nuScenes and Waymo datasets demonstrate that CompTrack achieves top-performing tracking performance with superior efficiency, running at a real-time 90 FPS on a single RTX 3090 GPU.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-19",
      "updated": "2025-11-22",
      "comment": "Accepted by AAAI 2026 (Oral)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15580v3",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "running"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Interaction-Aware 4D Gaussian Splatting for Dynamic Hand-Object Interaction Reconstruction",
      "authors": [
        "Hao Tian",
        "Chenyangguang Zhang",
        "Rui Liu",
        "Wen Shen",
        "Xiaolin Qin"
      ],
      "arxiv_id": "2511.14540v1",
      "summary": "This paper focuses on a challenging setting of simultaneously modeling geometry and appearance of hand-object interaction scenes without any object priors. We follow the trend of dynamic 3D Gaussian Splatting based methods, and address several significant challenges. To model complex hand-object interaction with mutual occlusion and edge blur, we present interaction-aware hand-object Gaussians with newly introduced optimizable parameters aiming to adopt piecewise linear hypothesis for clearer structural representation. Moreover, considering the complementarity and tightness of hand shape and object shape during interaction dynamics, we incorporate hand information into object deformation field, constructing interaction-aware dynamic fields to model flexible motions. To further address difficulties in the optimization process, we propose a progressive strategy that handles dynamic regions and static background step by step. Correspondingly, explicit regularizations are designed to stabilize the hand-object representations for smooth motion transition, physical interaction reality, and coherent lighting. Experiments show that our approach surpasses existing dynamic 3D-GS-based methods and achieves state-of-the-art performance in reconstructing dynamic hand-object interaction.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "11 pages, 6 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14540v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "[T]gaussian splatting"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Self-Supervised Multisensory Pretraining for Contact-Rich Robot Reinforcement Learning",
      "authors": [
        "Rickmer Krohn",
        "Vignesh Prasad",
        "Gabriele Tiboni",
        "Georgia Chalvatzaki"
      ],
      "arxiv_id": "2511.14427v1",
      "summary": "Effective contact-rich manipulation requires robots to synergistically leverage vision, force, and proprioception. However, Reinforcement Learning agents struggle to learn in such multisensory settings, especially amidst sensory noise and dynamic changes. We propose MultiSensory Dynamic Pretraining (MSDP), a novel framework for learning expressive multisensory representations tailored for task-oriented policy learning. MSDP is based on masked autoencoding and trains a transformer-based encoder by reconstructing multisensory observations from only a subset of sensor embeddings, leading to cross-modal prediction and sensor fusion. For downstream policy learning, we introduce a novel asymmetric architecture, where a cross-attention mechanism allows the critic to extract dynamic, task-specific features from the frozen embeddings, while the actor receives a stable pooled representation to guide its actions. Our method demonstrates accelerated learning and robust performance under diverse perturbations, including sensor noise, and changes in object dynamics. Evaluations in multiple challenging, contact-rich robot manipulation tasks in simulation and the real world showcase the effectiveness of MSDP. Our approach exhibits strong robustness to perturbations and achieves high success rates on the real robot with as few as 6,000 online interactions, offering a simple yet powerful solution for complex multisensory robotic control.",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "9 pages, 10 figures, preprint",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14427v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "policy learning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "PuzzlePoles: Cylindrical Fiducial Markers Based on the PuzzleBoard Pattern",
      "authors": [
        "Juri Zach",
        "Peer Stelldinger"
      ],
      "arxiv_id": "2511.19448v1",
      "summary": "Reliable perception of the environment is a key enabler for autonomous systems, where calibration and localization tasks often rely on robust visual markers. We introduce the PuzzlePole, a new type of fiducial markers derived from the recently proposed PuzzleBoard calibration pattern. The PuzzlePole is a cylindrical marker, enabling reliable recognition and pose estimation from 360° viewing direction. By leveraging the unique combinatorial structure of the PuzzleBoard pattern, PuzzlePoles provide a high accuracy in localization and orientation while being robust to occlusions. The design offers flexibility for deployment in diverse autonomous systems scenarios, ranging from robot navigation and SLAM to tangible interfaces.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19448v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "SLAM",
            "pose estimation",
            "localization",
            "navigation"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Multi-Timescale Model Predictive Control for Slow-Fast Systems",
      "authors": [
        "Lukas Schroth",
        "Daniel Morton",
        "Amon Lahr",
        "Daniele Gammelli",
        "Andrea Carron",
        "Marco Pavone"
      ],
      "arxiv_id": "2511.14311v1",
      "summary": "Model Predictive Control (MPC) has established itself as the primary methodology for constrained control, enabling autonomy across diverse applications. While model fidelity is crucial in MPC, solving the corresponding optimization problem in real time remains challenging when combining long horizons with high-fidelity models that capture both short-term dynamics and long-term behavior. Motivated by results on the Exponential Decay of Sensitivities (EDS), which imply that, under certain conditions, the influence of modeling inaccuracies decreases exponentially along the prediction horizon, this paper proposes a multi-timescale MPC scheme for fast-sampled control. Tailored to systems with both fast and slow dynamics, the proposed approach improves computational efficiency by i) switching to a reduced model that captures only the slow, dominant dynamics and ii) exponentially increasing integration step sizes to progressively reduce model detail along the horizon. We evaluate the method on three practically motivated robotic control problems in simulation and observe speed-ups of up to an order of magnitude.",
      "categories": [
        "eess.SY",
        "cs.RO"
      ],
      "primary_category": "eess.SY",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14311v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "MPC",
            "[T]model predictive control"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Translating Cultural Choreography from Humanoid Forms to Robotic Arm",
      "authors": [
        "Chelsea-Xi Chen",
        "Zhe Zhang",
        "Aven-Le Zhou"
      ],
      "arxiv_id": "2511.17603v1",
      "summary": "Robotic arm choreography often reproduces trajectories while missing cultural semantics. This study examines whether symbolic posture transfer with joint space compatible notation can preserve semantic fidelity on a six-degree-of-freedom arm and remain portable across morphologies. We implement ROPERA, a three-stage pipeline for encoding culturally codified postures, composing symbolic sequences, and decoding to servo commands. A scene from Kunqu opera, \\textit{The Peony Pavilion}, serves as the material for evaluation. The procedure includes corpus-based posture selection, symbolic scoring, direct joint angle execution, and a visual layer with light painting and costume-informed colors. Results indicate reproducible execution with intended timing and cultural legibility reported by experts and audiences. The study points to non-anthropocentric cultural preservation and portable authoring workflows. Future work will design dance-informed transition profiles, extend the notation to locomotion with haptic, musical, and spatial cues, and test portability across platforms.",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17603v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "locomotion"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "RSPose: Ranking Based Losses for Human Pose Estimation",
      "authors": [
        "Muhammed Can Keles",
        "Bedrettin Cetinkaya",
        "Sinan Kalkan",
        "Emre Akbas"
      ],
      "arxiv_id": "2511.13857v1",
      "summary": "While heatmap-based human pose estimation methods have shown strong performance, they suffer from three main problems: (P1) \"Commonly used Mean Squared Error (MSE)\" Loss may not always improve joint localization because it penalizes all pixel deviations equally, without focusing explicitly on sharpening and correctly localizing the peak corresponding to the joint; (P2) heatmaps are spatially and class-wise imbalanced; and, (P3) there is a discrepancy between the evaluation metric (i.e., mAP) and the loss functions.\n  We propose ranking-based losses to address these issues.\n  Both theoretically and empirically, we show that our proposed losses are superior to commonly used heatmap losses (MSE, KL-Divergence). Our losses considerably increase the correlation between confidence scores and localization qualities, which is desirable because higher correlation leads to more accurate instance selection during Non-Maximum Suppression (NMS) and better Average Precision (mAP) performance. We refer to the models trained with our losses as RSPose.\n  We show the effectiveness of RSPose across two different modes: one-dimensional and two-dimensional heatmaps, on three different datasets (COCO, CrowdPose, MPII).\n  To the best of our knowledge, we are the first to propose losses that align with the evaluation metric (mAP) for human pose estimation.\n  RSPose outperforms the previous state of the art on the COCO-val set and achieves an mAP score of 79.9 with ViTPose-H, a vision transformer model for human pose estimation.\n  We also improve SimCC Resnet-50, a coordinate classification-based pose estimation method, by 1.5 AP on the COCO-val set, achieving 73.6 AP.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13857v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]pose estimation",
            "localization"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Contact-Safe Reinforcement Learning with ProMP Reparameterization and Energy Awareness",
      "authors": [
        "Bingkun Huang",
        "Yuhe Gong",
        "Zewen Yang",
        "Tianyu Ren",
        "Luis Figueredo"
      ],
      "arxiv_id": "2511.13459v1",
      "summary": "Reinforcement learning (RL) approaches based on Markov Decision Processes (MDPs) are predominantly applied in the robot joint space, often relying on limited task-specific information and partial awareness of the 3D environment. In contrast, episodic RL has demonstrated advantages over traditional MDP-based methods in terms of trajectory consistency, task awareness, and overall performance in complex robotic tasks. Moreover, traditional step-wise and episodic RL methods often neglect the contact-rich information inherent in task-space manipulation, especially considering the contact-safety and robustness. In this work, contact-rich manipulation tasks are tackled using a task-space, energy-safe framework, where reliable and safe task-space trajectories are generated through the combination of Proximal Policy Optimization (PPO) and movement primitives. Furthermore, an energy-aware Cartesian Impedance Controller objective is incorporated within the proposed framework to ensure safe interactions between the robot and the environment. Our experimental results demonstrate that the proposed framework outperforms existing methods in handling tasks on various types of surfaces in 3D environments, achieving high success rates as well as smooth trajectories and energy-safe interactions.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13459v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "PPO"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation",
      "authors": [
        "Lingfeng Zhang",
        "Yuchen Zhang",
        "Hongsheng Li",
        "Haoxiang Fu",
        "Yingbo Tang",
        "Hangjun Ye",
        "Long Chen",
        "Xiaojun Liang",
        "Xiaoshuai Hao",
        "Wenbo Ding"
      ],
      "arxiv_id": "2511.13269v1",
      "summary": "Vision-Language Models (VLMs), leveraging their powerful visual perception and reasoning capabilities, have been widely applied in Unmanned Aerial Vehicle (UAV) tasks. However, the spatial intelligence capabilities of existing VLMs in UAV scenarios remain largely unexplored, raising concerns about their effectiveness in navigating and interpreting dynamic environments. To bridge this gap, we introduce SpatialSky-Bench, a comprehensive benchmark specifically designed to evaluate the spatial intelligence capabilities of VLMs in UAV navigation. Our benchmark comprises two categories-Environmental Perception and Scene Understanding-divided into 13 subcategories, including bounding boxes, color, distance, height, and landing safety analysis, among others. Extensive evaluations of various mainstream open-source and closed-source VLMs reveal unsatisfactory performance in complex UAV navigation scenarios, highlighting significant gaps in their spatial capabilities. To address this challenge, we developed the SpatialSky-Dataset, a comprehensive dataset containing 1M samples with diverse annotations across various scenarios. Leveraging this dataset, we introduce Sky-VLM, a specialized VLM designed for UAV spatial reasoning across multiple granularities and contexts. Extensive experimental results demonstrate that Sky-VLM achieves state-of-the-art performance across all benchmark tasks, paving the way for the development of VLMs suitable for UAV scenarios. The source code is available at https://github.com/linglingxiansen/SpatialSKy.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13269v1",
      "code_links": [
        {
          "url": "https://github.com/linglingxiansen/SpatialSKy",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding",
            "[T]navigation"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "CloseUpShot: Close-up Novel View Synthesis from Sparse-views via Point-conditioned Diffusion Model",
      "authors": [
        "Yuqi Zhang",
        "Guanying Chen",
        "Jiaxing Chen",
        "Chuanyu Fu",
        "Chuan Huang",
        "Shuguang Cui"
      ],
      "arxiv_id": "2511.13121v1",
      "summary": "Reconstructing 3D scenes and synthesizing novel views from sparse input views is a highly challenging task. Recent advances in video diffusion models have demonstrated strong temporal reasoning capabilities, making them a promising tool for enhancing reconstruction quality under sparse-view settings. However, existing approaches are primarily designed for modest viewpoint variations, which struggle in capturing fine-grained details in close-up scenarios since input information is severely limited. In this paper, we present a diffusion-based framework, called CloseUpShot, for close-up novel view synthesis from sparse inputs via point-conditioned video diffusion. Specifically, we observe that pixel-warping conditioning suffers from severe sparsity and background leakage in close-up settings. To address this, we propose hierarchical warping and occlusion-aware noise suppression, enhancing the quality and completeness of the conditioning images for the video diffusion model. Furthermore, we introduce global structure guidance, which leverages a dense fused point cloud to provide consistent geometric context to the diffusion process, to compensate for the lack of globally consistent 3D constraints in sparse conditioning inputs. Extensive experiments on multiple datasets demonstrate that our method outperforms existing approaches, especially in close-up novel view synthesis, clearly validating the effectiveness of our design.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "Project Link: https://zyqz97.github.io/CloseUpShot/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13121v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]novel view synthesis",
            "point cloud"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Reconstructing 3D Scenes in Native High Dynamic Range",
      "authors": [
        "Kaixuan Zhang",
        "Minxian Li",
        "Mingwu Ren",
        "Jiankang Deng",
        "Xiatian Zhu"
      ],
      "arxiv_id": "2511.12895v1",
      "summary": "High Dynamic Range (HDR) imaging is essential for professional digital media creation, e.g., filmmaking, virtual production, and photorealistic rendering. However, 3D scene reconstruction has primarily focused on Low Dynamic Range (LDR) data, limiting its applicability to professional workflows. Existing approaches that reconstruct HDR scenes from LDR observations rely on multi-exposure fusion or inverse tone-mapping, which increase capture complexity and depend on synthetic supervision. With the recent emergence of cameras that directly capture native HDR data in a single exposure, we present the first method for 3D scene reconstruction that directly models native HDR observations. We propose {\\bf Native High dynamic range 3D Gaussian Splatting (NH-3DGS)}, which preserves the full dynamic range throughout the reconstruction pipeline. Our key technical contribution is a novel luminance-chromaticity decomposition of the color representation that enables direct optimization from native HDR camera data. We demonstrate on both synthetic and real multi-view HDR datasets that NH-3DGS significantly outperforms existing methods in reconstruction quality and dynamic range preservation, enabling professional-grade 3D reconstruction directly from native HDR captures. Code and datasets will be made available.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12895v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "3DGS",
            "gaussian splatting",
            "scene reconstruction"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Structured Imitation Learning of Interactive Policies through Inverse Games",
      "authors": [
        "Max M. Sun",
        "Todd Murphey"
      ],
      "arxiv_id": "2511.12848v1",
      "summary": "Generative model-based imitation learning methods have recently achieved strong results in learning high-complexity motor skills from human demonstrations. However, imitation learning of interactive policies that coordinate with humans in shared spaces without explicit communication remains challenging, due to the significantly higher behavioral complexity in multi-agent interactions compared to non-interactive tasks. In this work, we introduce a structured imitation learning framework for interactive policies by combining generative single-agent policy learning with a flexible yet expressive game-theoretic structure. Our method explicitly separates learning into two steps: first, we learn individual behavioral patterns from multi-agent demonstrations using standard imitation learning; then, we structurally learn inter-agent dependencies by solving an inverse game problem. Preliminary results in a synthetic 5-agent social navigation task show that our method significantly improves non-interactive policies and performs comparably to the ground truth interactive policy using only 50 demonstrations. These results highlight the potential of structured imitation learning in interactive settings.",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "Presented at the \"Workshop on Generative Modeling Meets Human-Robot Interaction\" at Robotics: Science and Systems 2025. Workshop website: https://sites.google.com/view/gai-hri/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12848v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning",
            "[T]imitation learning"
          ],
          "score": 6.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "DR. Nav: Semantic-Geometric Representations for Proactive Dead-End Recovery and Navigation",
      "authors": [
        "Vignesh Rajagopal",
        "Kasun Weerakoon Kulathun Mudiyanselage",
        "Gershom Devake Seneviratne",
        "Pon Aswin Sankaralingam",
        "Mohamed Elnoor",
        "Jing Liang",
        "Rohan Chandra",
        "Dinesh Manocha"
      ],
      "arxiv_id": "2511.12778v1",
      "summary": "We present DR. Nav (Dead-End Recovery-aware Navigation), a novel approach to autonomous navigation in scenarios where dead-end detection and recovery are critical, particularly in unstructured environments where robots must handle corners, vegetation occlusions, and blocked junctions. DR. Nav introduces a proactive strategy for navigation in unmapped environments without prior assumptions. Our method unifies dead-end prediction and recovery by generating a single, continuous, real-time semantic cost map. Specifically, DR. Nav leverages cross-modal RGB-LiDAR fusion with attention-based filtering to estimate per-cell dead-end likelihoods and recovery points, which are continuously updated through Bayesian inference to enhance robustness. Unlike prior mapping methods that only encode traversability, DR. Nav explicitly incorporates recovery-aware risk into the navigation cost map, enabling robots to anticipate unsafe regions and plan safer alternative trajectories. We evaluate DR. Nav across multiple dense indoor and outdoor scenarios and demonstrate an increase of 83.33% in accuracy in detection, a 52.4% reduction in time-to-goal (path efficiency), compared to state-of-the-art planners such as DWA, MPPI, and Nav2 DWB. Furthermore, the dead-end classifier functions",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-16",
      "updated": "2025-11-16",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12778v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "traversability",
            "[T]navigation"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Are LLMs The Way Forward? A Case Study on LLM-Guided Reinforcement Learning for Decentralized Autonomous Driving",
      "authors": [
        "Timur Anvar",
        "Jeffrey Chen",
        "Yuyan Wang",
        "Rohan Chandra"
      ],
      "arxiv_id": "2511.12751v1",
      "summary": "Autonomous vehicle navigation in complex environments such as dense and fast-moving highways and merging scenarios remains an active area of research. A key limitation of RL is its reliance on well-specified reward functions, which often fail to capture the full semantic and social complexity of diverse, out-of-distribution situations. As a result, a rapidly growing line of research explores using Large Language Models (LLMs) to replace or supplement RL for direct planning and control, on account of their ability to reason about rich semantic context. However, LLMs present significant drawbacks: they can be unstable in zero-shot safety-critical settings, produce inconsistent outputs, and often depend on expensive API calls with network latency. This motivates our investigation into whether small, locally deployed LLMs (< 14B parameters) can meaningfully support autonomous highway driving through reward shaping rather than direct control. We present a case study comparing RL-only, LLM-only, and hybrid approaches, where LLMs augment RL rewards by scoring state-action transitions during training, while standard RL policies execute at test time. Our findings reveal that RL-only agents achieve moderate success rates (73-89%) with reasonable efficiency, LLM-only agents can reach higher success rates (up to 94%) but with severely degraded speed performance, and hybrid approaches consistently fall between these extremes. Critically, despite explicit efficiency instructions, LLM-influenced approaches exhibit systematic conservative bias with substantial model-dependent variability, highlighting important limitations of current small LLMs for safety-critical control tasks.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-16",
      "updated": "2025-11-16",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12751v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "reward shaping"
          ],
          "score": 6.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "OPFormer: Object Pose Estimation leveraging foundation model with geometric encoding",
      "authors": [
        "Artem Moroz",
        "Vít Zeman",
        "Martin Mikšík",
        "Elizaveta Isianova",
        "Miroslav David",
        "Pavel Burget",
        "Varun Burde"
      ],
      "arxiv_id": "2511.12614v1",
      "summary": "We introduce a unified, end-to-end framework that seamlessly integrates object detection and pose estimation with a versatile onboarding process. Our pipeline begins with an onboarding stage that generates object representations from either traditional 3D CAD models or, in their absence, by rapidly reconstructing a high-fidelity neural representation (NeRF) from multi-view images. Given a test image, our system first employs the CNOS detector to localize target objects. For each detection, our novel pose estimation module, OPFormer, infers the precise 6D pose. The core of OPFormer is a transformer-based architecture that leverages a foundation model for robust feature extraction. It uniquely learns a comprehensive object representation by jointly encoding multiple template views and enriches these features with explicit 3D geometric priors using Normalized Object Coordinate Space (NOCS). A decoder then establishes robust 2D-3D correspondences to determine the final pose. Evaluated on the challenging BOP benchmarks, our integrated system demonstrates a strong balance between accuracy and efficiency, showcasing its practical applicability in both model-based and model-free scenarios.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-16",
      "updated": "2025-11-16",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12614v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "NeRF",
            "[T]pose estimation"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Variable Impedance Control for Floating-Base Supernumerary Robotic Leg in Walking Assistance",
      "authors": [
        "Jun Huo",
        "Kehan Xu",
        "Chengyao Li",
        "Yu Cao",
        "Jie Zuo",
        "Xinxing Chen",
        "Jian Huang"
      ],
      "arxiv_id": "2511.12184v1",
      "summary": "In human-robot systems, ensuring safety during force control in the presence of both internal and external disturbances is crucial. As a typical loosely coupled floating-base robot system, the supernumerary robotic leg (SRL) system is particularly susceptible to strong internal disturbances. To address the challenge posed by floating base, we investigated the dynamics model of the loosely coupled SRL and designed a hybrid position/force impedance controller to fit dynamic torque input. An efficient variable impedance control (VIC) method is developed to enhance human-robot interaction, particularly in scenarios involving external force disturbances. By dynamically adjusting impedance parameters, VIC improves the dynamic switching between rigidity and flexibility, so that it can adapt to unknown environmental disturbances in different states. An efficient real-time stability guaranteed impedance parameters generating network is specifically designed for the proposed SRL, to achieve shock mitigation and high rigidity supporting. Simulations and experiments validate the system's effectiveness, demonstrating its ability to maintain smooth signal transitions in flexible states while providing strong support forces in rigid states. This approach provides a practical solution for accommodating individual gait variations in interaction, and significantly advances the safety and adaptability of human-robot systems.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-15",
      "updated": "2025-11-15",
      "comment": "",
      "doi": "10.1109/LRA.2025.3588400",
      "journal_ref": "IEEE Robotics and Automation Letters, vol. 10, no. 9, pp. 8698-8705, Sept. 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.12184v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "gait",
            "[T]walking"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "CAPE: Context-Aware Diffusion Policy Via Proximal Mode Expansion for Collision Avoidance",
      "authors": [
        "Rui Heng Yang",
        "Xuan Zhao",
        "Leo Maxime Brunswic",
        "Montgomery Alban",
        "Mateo Clemente",
        "Tongtong Cao",
        "Jun Jin",
        "Amir Rasouli"
      ],
      "arxiv_id": "2511.22773v1",
      "summary": "In robotics, diffusion models can capture multi-modal trajectories from demonstrations, making them a transformative approach in imitation learning. However, achieving optimal performance following this regiment requires a large-scale dataset, which is costly to obtain, especially for challenging tasks, such as collision avoidance. In those tasks, generalization at test time demands coverage of many obstacles types and their spatial configurations, which are impractical to acquire purely via data. To remedy this problem, we propose Context-Aware diffusion policy via Proximal mode Expansion (CAPE), a framework that expands trajectory distribution modes with context-aware prior and guidance at inference via a novel prior-seeded iterative guided refinement procedure. The framework generates an initial trajectory plan and executes a short prefix trajectory, and then the remaining trajectory segment is perturbed to an intermediate noise level, forming a trajectory prior. Such a prior is context-aware and preserves task intent. Repeating the process with context-aware guided denoising iteratively expands mode support to allow finding smoother, less collision-prone trajectories. For collision avoidance, CAPE expands trajectory distribution modes with collision-aware context, enabling the sampling of collision-free trajectories in previously unseen environments while maintaining goal consistency. We evaluate CAPE on diverse manipulation tasks in cluttered unseen simulated and real-world settings and show up to 26% and 80% higher success rates respectively compared to SOTA methods, demonstrating better generalization to unseen environments.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "4 tables, 9 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22773v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning",
            "[T]diffusion policy"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Splat-SAP: Feed-Forward Gaussian Splatting for Human-Centered Scene with Scale-Aware Point Map Reconstruction",
      "authors": [
        "Boyao Zhou",
        "Shunyuan Zheng",
        "Zhanfeng Liao",
        "Zihan Ma",
        "Hanzhang Tu",
        "Boning Liu",
        "Yebin Liu"
      ],
      "arxiv_id": "2511.22704v1",
      "summary": "We present Splat-SAP, a feed-forward approach to render novel views of human-centered scenes from binocular cameras with large sparsity. Gaussian Splatting has shown its promising potential in rendering tasks, but it typically necessitates per-scene optimization with dense input views. Although some recent approaches achieve feed-forward Gaussian Splatting rendering through geometry priors obtained by multi-view stereo, such approaches still require largely overlapped input views to establish the geometry prior. To bridge this gap, we leverage pixel-wise point map reconstruction to represent geometry which is robust to large sparsity for its independent view modeling. In general, we propose a two-stage learning strategy. In stage 1, we transform the point map into real space via an iterative affinity learning process, which facilitates camera control in the following. In stage 2, we project point maps of two input views onto the target view plane and refine such geometry via stereo matching. Furthermore, we anchor Gaussian primitives on this refined plane in order to render high-quality images. As a metric representation, the scale-aware point map in stage 1 is trained in a self-supervised manner without 3D supervision and stage 2 is supervised with photo-metric loss. We collect multi-view human-centered data and demonstrate that our method improves both the stability of point map reconstruction and the visual quality of free-viewpoint rendering.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "Accepted by AAAI 2026. Project page: https://yaourtb.github.io/Splat-SAP",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22704v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "stereo matching",
            "[T]gaussian splatting"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Visual-Geometry Diffusion Policy: Robust Generalization via Complementarity-Aware Multimodal Fusion",
      "authors": [
        "Yikai Tang",
        "Haoran Geng",
        "Sheng Zang",
        "Pieter Abbeel",
        "Jitendra Malik"
      ],
      "arxiv_id": "2511.22445v1",
      "summary": "Imitation learning has emerged as a crucial ap proach for acquiring visuomotor skills from demonstrations, where designing effective observation encoders is essential for policy generalization. However, existing methods often struggle to generalize under spatial and visual randomizations, instead tending to overfit. To address this challenge, we propose Visual Geometry Diffusion Policy (VGDP), a multimodal imitation learning framework built around a Complementarity-Aware Fusion Module where modality-wise dropout enforces balanced use of RGB and point-cloud cues, with cross-attention serving only as a lightweight interaction layer. Our experiments show that the expressiveness of the fused latent space is largely induced by the enforced complementarity from modality-wise dropout, with cross-attention serving primarily as a lightweight interaction mechanism rather than the main source of robustness. Across a benchmark of 18 simulated tasks and 4 real-world tasks, VGDP outperforms seven baseline policies with an average performance improvement of 39.1%. More importantly, VGDP demonstrates strong robustness under visual and spatial per turbations, surpassing baselines with an average improvement of 41.5% in different visual conditions and 15.2% in different spatial settings.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22445v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning",
            "[T]diffusion policy"
          ],
          "score": 6.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "3D Affordance Keypoint Detection for Robotic Manipulation",
      "authors": [
        "Zhiyang Liu",
        "Ruiteng Zhao",
        "Lei Zhou",
        "Chengran Yuan",
        "Yuwei Wu",
        "Sheng Guo",
        "Zhengshen Zhang",
        "Chenchen Liu",
        "Marcelo H Ang"
      ],
      "arxiv_id": "2511.22195v1",
      "summary": "This paper presents a novel approach for affordance-informed robotic manipulation by introducing 3D keypoints to enhance the understanding of object parts' functionality. The proposed approach provides direct information about what the potential use of objects is, as well as guidance on where and how a manipulator should engage, whereas conventional methods treat affordance detection as a semantic segmentation task, focusing solely on answering the what question. To address this gap, we propose a Fusion-based Affordance Keypoint Network (FAKP-Net) by introducing 3D keypoint quadruplet that harnesses the synergistic potential of RGB and Depth image to provide information on execution position, direction, and extent. Benchmark testing demonstrates that FAKP-Net outperforms existing models by significant margins in affordance segmentation task and keypoint detection task. Real-world experiments also showcase the reliability of our method in accomplishing manipulation tasks with previously unseen objects.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "Accepted to IROS 2024",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22195v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "affordance detection"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "PFF-Net: Patch Feature Fitting for Point Cloud Normal Estimation",
      "authors": [
        "Qing Li",
        "Huifang Feng",
        "Kanle Shi",
        "Yue Gao",
        "Yi Fang",
        "Yu-Shen Liu",
        "Zhizhong Han"
      ],
      "arxiv_id": "2511.21365v1",
      "summary": "Estimating the normal of a point requires constructing a local patch to provide center-surrounding context, but determining the appropriate neighborhood size is difficult when dealing with different data or geometries. Existing methods commonly employ various parameter-heavy strategies to extract a full feature description from the input patch. However, they still have difficulties in accurately and efficiently predicting normals for various point clouds. In this work, we present a new idea of feature extraction for robust normal estimation of point clouds. We use the fusion of multi-scale features from different neighborhood sizes to address the issue of selecting reasonable patch sizes for various data or geometries. We seek to model a patch feature fitting (PFF) based on multi-scale features to approximate the optimal geometric description for normal estimation and implement the approximation process via multi-scale feature aggregation and cross-scale feature compensation. The feature aggregation module progressively aggregates the patch features of different scales to the center of the patch and shrinks the patch size by removing points far from the center. It not only enables the network to precisely capture the structure characteristic in a wide range, but also describes highly detailed geometries. The feature compensation module ensures the reusability of features from earlier layers of large scales and reveals associated information in different patch sizes. Our approximation strategy based on aggregating the features of multiple scales enables the model to achieve scale adaptation of varying local patches and deliver the optimal feature description. Extensive experiments demonstrate that our method achieves state-of-the-art performance on both synthetic and real-world datasets with fewer network parameters and running time.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "Accepted by TVCG",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21365v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "running"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "HTTM: Head-wise Temporal Token Merging for Faster VGGT",
      "authors": [
        "Weitian Wang",
        "Lukas Meiner",
        "Rai Shubham",
        "Cecilia De La Parra",
        "Akash Kumar"
      ],
      "arxiv_id": "2511.21317v1",
      "summary": "The Visual Geometry Grounded Transformer (VGGT) marks a significant leap forward in 3D scene reconstruction, as it is the first model that directly infers all key 3D attributes (camera poses, depths, and dense geometry) jointly in one pass. However, this joint inference mechanism requires global attention layers that perform all-to-all attention computation on tokens from all views. For reconstruction of large scenes with long-sequence inputs, this causes a significant latency bottleneck. In this paper, we propose head-wise temporal merging (HTTM), a training-free 3D token merging method for accelerating VGGT. Existing merging techniques merge tokens uniformly across different attention heads, resulting in identical tokens in the layers' output, which hinders the model's representational ability. HTTM tackles this problem by merging tokens in multi-head granularity, which preserves the uniqueness of feature tokens after head concatenation. Additionally, this enables HTTM to leverage the spatial locality and temporal correspondence observed at the head level to achieve higher merging ratios with lower merging costs compared to existing methods. Thus, HTTM achieves up to 7x acceleration with negligible performance drops in a GPU-based inference.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21317v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene reconstruction",
            "[T]VGGT"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "3-Tracer: A Tri-level Temporal-Aware Framework for Audio Forgery Detection and Localization",
      "authors": [
        "Shuhan Xia",
        "Xuannan Liu",
        "Xing Cui",
        "Peipei Li"
      ],
      "arxiv_id": "2511.21237v2",
      "summary": "Recently, partial audio forgery has emerged as a new form of audio manipulation. Attackers selectively modify partial but semantically critical frames while preserving the overall perceptual authenticity, making such forgeries particularly difficult to detect. Existing methods focus on independently detecting whether a single frame is forged, lacking the hierarchical structure to capture both transient and sustained anomalies across different temporal levels. To address these limitations, We identify three key levels relevant to partial audio forgery detection and present T3-Tracer, the first framework that jointly analyzes audio at the frame, segment, and audio levels to comprehensively detect forgery traces. T3-Tracer consists of two complementary core modules: the Frame-Audio Feature Aggregation Module (FA-FAM) and the Segment-level Multi-Scale Discrepancy-Aware Module (SMDAM). FA-FAM is designed to detect the authenticity of each audio frame. It combines both frame-level and audio-level temporal information to detect intra-frame forgery cues and global semantic inconsistencies. To further refine and correct frame detection, we introduce SMDAM to detect forgery boundaries at the segment level. It adopts a dual-branch architecture that jointly models frame features and inter-frame differences across multi-scale temporal windows, effectively identifying abrupt anomalies that appeared on the forged boundaries. Extensive experiments conducted on three challenging datasets demonstrate that our approach achieves state-of-the-art performance.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-12-01",
      "comment": "The experimental results in this paper have been further improved and updated; the baseline results do not match existing results, therefore the paper needs to be retracted",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21237v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Efficient Greedy Algorithms for Feature Selection in Robot Visual Localization",
      "authors": [
        "Vivek Pandey",
        "Amirhossein Mollaei",
        "Nader Motee"
      ],
      "arxiv_id": "2511.20894v1",
      "summary": "Robot localization is a fundamental component of autonomous navigation in unknown environments. Among various sensing modalities, visual input from cameras plays a central role, enabling robots to estimate their position by tracking point features across image frames. However, image frames often contain a large number of features, many of which are redundant or uninformative for localization. Processing all features can introduce significant computational latency and inefficiency. This motivates the need for intelligent feature selection, identifying a subset of features that are most informative for localization over a prediction horizon. In this work, we propose two fast and memory-efficient feature selection algorithms that enable robots to actively evaluate the utility of visual features in real time. Unlike existing approaches with high computational and memory demands, the proposed methods are explicitly designed to reduce both time and memory complexity while achieving a favorable trade-off between computational efficiency and localization accuracy.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20894v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization",
            "navigation"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "MODEST: Multi-Optics Depth-of-Field Stereo Dataset",
      "authors": [
        "Nisarg K. Trivedi",
        "Vinayak A. Belludi",
        "Li-Yun Wang",
        "Pardis Taghavi",
        "Dante Lok"
      ],
      "arxiv_id": "2511.20853v2",
      "summary": "Reliable depth estimation under real optical conditions remains a core challenge for camera vision in systems such as autonomous robotics and augmented reality. Despite recent progress in depth estimation and depth-of-field rendering, research remains constrained by the lack of large-scale, high-fidelity, real stereo DSLR datasets, limiting real-world generalization and evaluation of models trained on synthetic data as shown extensively in literature. We present the first high-resolution (5472$\\times$3648px) stereo DSLR dataset with 18000 images, systematically varying focal length and aperture across complex real scenes and capturing the optical realism and complexity of professional camera systems. For 9 scenes with varying scene complexity, lighting and background, images are captured with two identical camera assemblies at 10 focal lengths (28-70mm) and 5 apertures (f/2.8-f/22), spanning 50 optical configurations in 2000 images per scene. This full-range optics coverage enables controlled analysis of geometric and optical effects for monocular and stereo depth estimation, shallow depth-of-field rendering, deblurring, 3D scene reconstruction and novel view synthesis. Each focal configuration has a dedicated calibration image set, supporting evaluation of classical and learning based methods for intrinsic and extrinsic calibration. The dataset features challenging visual elements such as multi-scale optical illusions, reflective surfaces, mirrors, transparent glass walls, fine-grained details, and natural / artificial ambient light variations. This work attempts to bridge the realism gap between synthetic training data and real camera optics, and demonstrates challenges with the current state-of-the-art monocular, stereo depth and depth-of-field methods. We release the dataset, calibration files, and evaluation code to support reproducible research on real-world optical generalization.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-12-13",
      "comment": "Website, dataset and software tools now available for purely non-commercial, academic research purposes",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20853v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation",
            "stereo depth",
            "novel view synthesis",
            "scene reconstruction"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Conceptual Evaluation of Deep Visual Stereo Odometry for the MARWIN Radiation Monitoring Robot in Accelerator Tunnels",
      "authors": [
        "André Dehne",
        "Juri Zach",
        "Peer Stelldinger"
      ],
      "arxiv_id": "2512.00080v1",
      "summary": "The MARWIN robot operates at the European XFEL to perform autonomous radiation monitoring in long, monotonous accelerator tunnels where conventional localization approaches struggle. Its current navigation concept combines lidar-based edge detection, wheel/lidar odometry with periodic QR-code referencing, and fuzzy control of wall distance, rotation, and longitudinal position. While robust in predefined sections, this design lacks flexibility for unknown geometries and obstacles. This paper explores deep visual stereo odometry (DVSO) with 3D-geometric constraints as a focused alternative. DVSO is purely vision-based, leveraging stereo disparity, optical flow, and self-supervised learning to jointly estimate depth and ego-motion without labeled data. For global consistency, DVSO can subsequently be fused with absolute references (e.g., landmarks) or other sensors. We provide a conceptual evaluation for accelerator tunnel environments, using the European XFEL as a case study. Expected benefits include reduced scale drift via stereo, low-cost sensing, and scalable data collection, while challenges remain in low-texture surfaces, lighting variability, computational load, and robustness under radiation. The paper defines a research agenda toward enabling MARWIN to navigate more autonomously in constrained, safety-critical infrastructures.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00080v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "optical flow",
            "ego-motion",
            "localization",
            "navigation"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "DinoLizer: Learning from the Best for Generative Inpainting Localization",
      "authors": [
        "Minh Thong Doi",
        "Jan Butora",
        "Vincent Itier",
        "Jérémie Boulanger",
        "Patrick Bas"
      ],
      "arxiv_id": "2511.20722v1",
      "summary": "We introduce DinoLizer, a DINOv2-based model for localizing manipulated regions in generative inpainting. Our method builds on a DINOv2 model pretrained to detect synthetic images on the B-Free dataset. We add a linear classification head on top of the Vision Transformer's patch embeddings to predict manipulations at a $14\\times 14$ patch resolution. The head is trained to focus on semantically altered regions, treating non-semantic edits as part of the original content. Because the ViT accepts only fixed-size inputs, we use a sliding-window strategy to aggregate predictions over larger images; the resulting heatmaps are post-processed to refine the estimated binary manipulation masks. Empirical results show that DinoLizer surpasses state-of-the-art local manipulation detectors on a range of inpainting datasets derived from different generative models. It remains robust to common post-processing operations such as resizing, noise addition, and JPEG (double) compression. On average, DinoLizer achieves a 12\\% higher Intersection-over-Union (IoU) than the next best model, with even greater gains after post-processing. Our experiments with off-the-shelf DINOv2 demonstrate the strong representational power of Vision Transformers for this task. Finally, extensive ablation studies comparing DINOv2 and its successor, DINOv3, in deepfake localization confirm DinoLizer's superiority. The code will be publicly available upon acceptance of the paper.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20722v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Development of a Testbed for Autonomous Vehicles: Integrating MPC Control with Monocular Camera Lane Detection",
      "authors": [
        "Shantanu Rahman",
        "Nayeb Hasin",
        "Mainul Islam",
        "Golam Sarowar"
      ],
      "arxiv_id": "2511.19655v2",
      "summary": "Autonomous vehicles are becoming popular day by day not only for autonomous road traversal but also for industrial automation, farming and military. Most of the standard vehicles follow the Ackermann style steering mechanism. This has become to de facto standard for large and long faring vehicles. The local planner of an autonomous vehicle controls the low-level vehicle movement upon which the vehicle will perform its motor actuation. In our work, we focus on autonomous vehicles in road and perform experiments to analyze the effect of low-level controllers in the simulation and a real environment. To increase the precision and stability of trajectory tracking in autonomous cars, a novel method that combines lane identification with Model Predictive Control (MPC) is presented. The research focuses on camera-equipped autonomous vehicles and uses methods like edge recognition, sliding window-based straight-line identification for lane line extraction, and dynamic region of interest (ROI) extraction. Next, to follow the identified lane line, an MPC built on a bicycle vehicle dynamics model is created. A single-lane road simulation model is built using ROS Gazebo and tested in order to verify the controller's performance. The root mean square error between the optimal tracking trajectory and the target trajectory was reduced by 27.65% in the simulation results, demonstrating the high robustness and flexibility of the developed controller.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-24",
      "updated": "2025-11-26",
      "comment": "49 pages, 23 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19655v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]MPC",
            "model predictive control"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Multi-Agent Monocular Dense SLAM With 3D Reconstruction Priors",
      "authors": [
        "Yuchen Zhou",
        "Haihang Wu"
      ],
      "arxiv_id": "2511.19031v2",
      "summary": "Monocular Simultaneous Localization and Mapping (SLAM) aims to estimate a robot's pose while simultaneously reconstructing an unknown 3D scene using a single camera. While existing monocular SLAM systems generate detailed 3D geometry through dense scene representations, they are computationally expensive due to the need for iterative optimization. To address this challenge, MASt3R-SLAM utilizes learned 3D reconstruction priors, enabling more efficient and accurate estimation of both 3D structures and camera poses. However, MASt3R-SLAM is limited to single-agent operation. In this paper, we extend MASt3R-SLAM to introduce the first multi-agent monocular dense SLAM system. Each agent performs local SLAM using a 3D reconstruction prior, and their individual maps are fused into a globally consistent map through a loop-closure-based map fusion mechanism. Our approach improves computational efficiency compared to state-of-the-art methods, while maintaining similar mapping accuracy when evaluated on real-world datasets.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-24",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19031v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]SLAM",
            "localization"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "AuViRe: Audio-visual Speech Representation Reconstruction for Deepfake Temporal Localization",
      "authors": [
        "Christos Koutlis",
        "Symeon Papadopoulos"
      ],
      "arxiv_id": "2511.18993v1",
      "summary": "With the rapid advancement of sophisticated synthetic audio-visual content, e.g., for subtle malicious manipulations, ensuring the integrity of digital media has become paramount. This work presents a novel approach to temporal localization of deepfakes by leveraging Audio-Visual Speech Representation Reconstruction (AuViRe). Specifically, our approach reconstructs speech representations from one modality (e.g., lip movements) based on the other (e.g., audio waveform). Cross-modal reconstruction is significantly more challenging in manipulated video segments, leading to amplified discrepancies, thereby providing robust discriminative cues for precise temporal forgery localization. AuViRe outperforms the state of the art by +8.9 AP@0.95 on LAV-DF, +9.6 AP@0.5 on AV-Deepfake1M, and +5.1 AUC on an in-the-wild experiment. Code available at https://github.com/mever-team/auvire.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "WACV 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18993v1",
      "code_links": [
        {
          "url": "https://github.com/mever-team/auvire",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "MapRF: Weakly Supervised Online HD Map Construction via NeRF-Guided Self-Training",
      "authors": [
        "Hongyu Lyu",
        "Thomas Monninger",
        "Julie Stephany Berrio Perez",
        "Mao Shan",
        "Zhenxing Ming",
        "Stewart Worrall"
      ],
      "arxiv_id": "2511.19527v1",
      "summary": "Autonomous driving systems benefit from high-definition (HD) maps that provide critical information about road infrastructure. The online construction of HD maps offers a scalable approach to generate local maps from on-board sensors. However, existing methods typically rely on costly 3D map annotations for training, which limits their generalization and scalability across diverse driving environments. In this work, we propose MapRF, a weakly supervised framework that learns to construct 3D maps using only 2D image labels. To generate high-quality pseudo labels, we introduce a novel Neural Radiance Fields (NeRF) module conditioned on map predictions, which reconstructs view-consistent 3D geometry and semantics. These pseudo labels are then iteratively used to refine the map network in a self-training manner, enabling progressive improvement without additional supervision. Furthermore, to mitigate error accumulation during self-training, we propose a Map-to-Ray Matching strategy that aligns map predictions with camera rays derived from 2D labels. Extensive experiments on the Argoverse 2 and nuScenes datasets demonstrate that MapRF achieves performance comparable to fully supervised methods, attaining around 75% of the baseline while surpassing several approaches using only 2D labels. This highlights the potential of MapRF to enable scalable and cost-effective online HD map construction for autonomous driving.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19527v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]NeRF",
            "neural radiance"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Asynchronous Distributed Multi-Robot Motion Planning Under Imperfect Communication",
      "authors": [
        "Ardalan Tajbakhsh",
        "Augustinos Saravanos",
        "James Zhu",
        "Evangelos A. Theodorou",
        "Lorenz T. Biegler",
        "Aaron M. Johnson"
      ],
      "arxiv_id": "2511.18703v1",
      "summary": "This paper addresses the challenge of coordinating multi-robot systems under realistic communication delays using distributed optimization. We focus on consensus ADMM as a scalable framework for generating collision-free, dynamically feasible motion plans in both trajectory optimization and receding-horizon control settings. In practice, however, these algorithms are sensitive to penalty tuning or adaptation schemes (e.g. residual balancing and adaptive parameter heuristics) that do not explicitly consider delays. To address this, we introduce a Delay-Aware ADMM (DA-ADMM) variant that adapts penalty parameters based on real-time delay statistics, allowing agents to down-weight stale information and prioritize recent updates during consensus and dual updates. Through extensive simulations in 2D and 3D environments with double-integrator, Dubins-car, and drone dynamics, we show that DA-ADMM significantly improves robustness, success rate, and solution quality compared to fixed-parameter, residual-balancing, and fixed-constraint baselines. Our results highlight that performance degradation is not solely determined by delay length or frequency, but by the optimizer's ability to contextually reason over delayed information. The proposed DA-ADMM achieves consistently better coordination performance across a wide range of delay conditions, offering a principled and efficient mechanism for resilient multi-robot motion planning under imperfect communication.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "9 pages, 5 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18703v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "trajectory optimization",
            "[T]motion planning"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Sphinx: Efficiently Serving Novel View Synthesis using Regression-Guided Selective Refinement",
      "authors": [
        "Yuchen Xia",
        "Souvik Kundu",
        "Mosharaf Chowdhury",
        "Nishil Talati"
      ],
      "arxiv_id": "2511.18672v1",
      "summary": "Novel View Synthesis (NVS) is the task of generating new images of a scene from viewpoints that were not part of the original input. Diffusion-based NVS can generate high-quality, temporally consistent images, however, remains computationally prohibitive. Conversely, regression-based NVS offers suboptimal generation quality despite requiring significantly lower compute; leaving the design objective of a high-quality, inference-efficient NVS framework an open challenge. To close this critical gap, we present Sphinx, a training-free hybrid inference framework that achieves diffusion-level fidelity at a significantly lower compute. Sphinx proposes to use regression-based fast initialization to guide and reduce the denoising workload for the diffusion model. Additionally, it integrates selective refinement with adaptive noise scheduling, allowing more compute to uncertain regions and frames. This enables Sphinx to provide flexible navigation of the performance-quality trade-off, allowing adaptation to latency and fidelity requirements for dynamically changing inference scenarios. Our evaluation shows that Sphinx achieves an average 1.8x speedup over diffusion model inference with negligible perceptual degradation of less than 5%, establishing a new Pareto frontier between quality and latency in NVS serving.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18672v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]novel view synthesis",
            "navigation"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations",
      "authors": [
        "Litian Gong",
        "Fatemeh Bahrani",
        "Yutai Zhou",
        "Amin Banayeeanzade",
        "Jiachen Li",
        "Erdem Bıyık"
      ],
      "arxiv_id": "2511.18617v2",
      "summary": "AutoFocus-IL is a simple yet effective method to improve data efficiency and generalization in visual imitation learning by guiding policies to attend to task-relevant features rather than distractors and spurious correlations. Although saliency regularization has emerged as a promising way to achieve this, existing approaches typically require costly supervision such as human gaze data or manual saliency annotations. In contrast, AutoFocus-IL leverages vision-language models (VLMs) to automatically identify and track key objects in demonstrations, generating temporal saliency maps that highlight causal visual signals while suppressing distractors. These maps are then used to regularize behavior cloning policies, yielding stronger alignment between visual attention and task-relevant cues. Experiments in both the CARLA simulator and real-robot manipulation tasks demonstrate that AutoFocus-IL not only outperforms standard behavior cloning but also surpasses state-of-the-art baselines that assume privileged access to human supervision, such as gaze data. Code, datasets, and trained policy videos are available at https://AutoFocus-IL.github.io/.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-23",
      "updated": "2025-11-25",
      "comment": "8 pages, 6 figures. Code and datasets available at http://autofocus-il.github.io/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18617v2",
      "code_links": [
        {
          "url": "https://AutoFocus-IL.github.io/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning",
            "behavior cloning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "PhysGS: Bayesian-Inferred Gaussian Splatting for Physical Property Estimation",
      "authors": [
        "Samarth Chopra",
        "Jing Liang",
        "Gershom Seneviratne",
        "Dinesh Manocha"
      ],
      "arxiv_id": "2511.18570v1",
      "summary": "Understanding physical properties such as friction, stiffness, hardness, and material composition is essential for enabling robots to interact safely and effectively with their surroundings. However, existing 3D reconstruction methods focus on geometry and appearance and cannot infer these underlying physical properties. We present PhysGS, a Bayesian-inferred extension of 3D Gaussian Splatting that estimates dense, per-point physical properties from visual cues and vision--language priors. We formulate property estimation as Bayesian inference over Gaussian splats, where material and property beliefs are iteratively refined as new observations arrive. PhysGS also models aleatoric and epistemic uncertainties, enabling uncertainty-aware object and scene interpretation. Across object-scale (ABO-500), indoor, and outdoor real-world datasets, PhysGS improves accuracy of the mass estimation by up to 22.8%, reduces Shore hardness error by up to 61.2%, and lowers kinetic friction error by up to 18.1% compared to deterministic baselines. Our results demonstrate that PhysGS unifies 3D reconstruction, uncertainty modeling, and physical reasoning in a single, spatially continuous framework for dense physical property estimation. Additional results are available at https://samchopra2003.github.io/physgs.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-23",
      "updated": "2025-11-23",
      "comment": "Submitted to CVPR 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18570v1",
      "code_links": [
        {
          "url": "https://samchopra2003.github.io/physgs",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "[T]gaussian splatting"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "SegSplat: Feed-forward Gaussian Splatting and Open-Set Semantic Segmentation",
      "authors": [
        "Peter Siegel",
        "Federico Tombari",
        "Marc Pollefeys",
        "Daniel Barath"
      ],
      "arxiv_id": "2511.18386v1",
      "summary": "We have introduced SegSplat, a novel framework designed to bridge the gap between rapid, feed-forward 3D reconstruction and rich, open-vocabulary semantic understanding. By constructing a compact semantic memory bank from multi-view 2D foundation model features and predicting discrete semantic indices alongside geometric and appearance attributes for each 3D Gaussian in a single pass, SegSplat efficiently imbues scenes with queryable semantics. Our experiments demonstrate that SegSplat achieves geometric fidelity comparable to state-of-the-art feed-forward 3D Gaussian Splatting methods while simultaneously enabling robust open-set semantic segmentation, crucially \\textit{without} requiring any per-scene optimization for semantic feature integration. This work represents a significant step towards practical, on-the-fly generation of semantically aware 3D environments, vital for advancing robotic interaction, augmented reality, and other intelligent systems.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-23",
      "updated": "2025-11-23",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18386v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "[T]gaussian splatting"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Alias-free 4D Gaussian Splatting",
      "authors": [
        "Zilong Chen",
        "Huan-ang Gao",
        "Delin Qu",
        "Haohan Chi",
        "Hao Tang",
        "Kai Zhang",
        "Hao Zhao"
      ],
      "arxiv_id": "2511.18367v1",
      "summary": "Existing dynamic scene reconstruction methods based on Gaussian Splatting enable real-time rendering and generate realistic images. However, adjusting the camera's focal length or the distance between Gaussian primitives and the camera to modify rendering resolution often introduces strong artifacts, stemming from the frequency constraints of 4D Gaussians and Gaussian scale mismatch induced by the 2D dilated filter. To address this, we derive a maximum sampling frequency formulation for 4D Gaussian Splatting and introduce a 4D scale-adaptive filter and scale loss, which flexibly regulates the sampling frequency of 4D Gaussian Splatting. Our approach eliminates high-frequency artifacts under increased rendering frequencies while effectively reducing redundant Gaussians in multi-view video reconstruction. We validate the proposed method through monocular and multi-view video reconstruction experiments.Ours project page: https://4d-alias-free.github.io/4D-Alias-free/",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-23",
      "updated": "2025-11-23",
      "comment": "Project page: https://4d-alias-free.github.io/4D-Alias-free/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18367v1",
      "code_links": [
        {
          "url": "https://4d-alias-free.github.io/4D-Alias-free/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]gaussian splatting",
            "scene reconstruction"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "APULSE: A Scalable Hybrid Algorithm for the RCSPP on Large-Scale Dense Graphs",
      "authors": [
        "Nuno Soares",
        "António Grilo"
      ],
      "arxiv_id": "2511.18236v2",
      "summary": "The resource-constrained shortest path problem (RCSPP) is a fundamental NP-hard optimization challenge with broad applications, from network routing to autonomous navigation. This problem involves finding a path that minimizes a primary cost subject to a budget on a secondary resource. While various RCSPP solvers exist, they often face critical scalability limitations when applied to the large, dense graphs characteristic of complex, real-world scenarios, making them impractical for time-critical planning. This challenge is particularly acute in domains like mission planning for unmanned ground vehicles (UGVs), which demand solutions on large-scale terrain graphs. This paper introduces APULSE, a hybrid label-setting algorithm designed to efficiently solve the RCSPP on such challenging graphs. APULSE integrates a best-first search guided by an A* heuristic with aggressive, Pulse-style pruning mechanisms and a time-bucketing strategy for effective state-space reduction. A computational study, using a large-scale UGV planning scenario, benchmarks APULSE against state-of-the-art algorithms. The results demonstrate that APULSE consistently finds near-optimal solutions while being orders of magnitude faster and more robust, particularly on large problem instances where competing methods fail. This superior scalability establishes APULSE as an effective solution for RCSPP in complex, large-scale environments, enabling capabilities such as interactive decision support and dynamic replanning.",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-23",
      "updated": "2025-11-30",
      "comment": "This version corrects keywords and reference [9]. 9 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18236v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "[T]PULSE"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam",
        "8_physics_animation"
      ]
    },
    {
      "title": "Time-aware Motion Planning in Dynamic Environments with Conformal Prediction",
      "authors": [
        "Kaier Liang",
        "Licheng Luo",
        "Yixuan Wang",
        "Mingyu Cai",
        "Cristian Ioan Vasile"
      ],
      "arxiv_id": "2511.18170v1",
      "summary": "Safe navigation in dynamic environments remains challenging due to uncertain obstacle behaviors and the lack of formal prediction guarantees. We propose two motion planning frameworks that leverage conformal prediction (CP): a global planner that integrates Safe Interval Path Planning (SIPP) for uncertainty-aware trajectory generation, and a local planner that performs online reactive planning. The global planner offers distribution-free safety guarantees for long-horizon navigation, while the local planner mitigates inaccuracies in obstacle trajectory predictions through adaptive CP, enabling robust and responsive motion in dynamic environments. To further enhance trajectory feasibility, we introduce an adaptive quantile mechanism in the CP-based uncertainty quantification. Instead of using a fixed confidence level, the quantile is automatically tuned to the optimal value that preserves trajectory feasibility, allowing the planner to adaptively tighten safety margins in regions with higher uncertainty. We validate the proposed framework through numerical experiments conducted in dynamic and cluttered environments. The project page is available at https://time-aware-planning.github.io",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-22",
      "updated": "2025-11-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18170v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]motion planning"
          ],
          "score": 6.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "RoboLoc: A Benchmark Dataset for Point Place Recognition and Localization in Indoor-Outdoor Integrated Environments",
      "authors": [
        "Jaejin Jeon",
        "Seonghoon Ryoo",
        "Sang-Duck Lee",
        "Soomok Lee",
        "Seungwoo Jeong"
      ],
      "arxiv_id": "2512.01194v1",
      "summary": "Robust place recognition is essential for reliable localization in robotics, particularly in complex environments with fre- quent indoor-outdoor transitions. However, existing LiDAR-based datasets often focus on outdoor scenarios and lack seamless domain shifts. In this paper, we propose RoboLoc, a benchmark dataset designed for GPS-free place recognition in indoor-outdoor environments with floor transitions. RoboLoc features real-world robot trajectories, diverse elevation profiles, and transitions between structured indoor and unstructured outdoor domains. We benchmark a variety of state-of-the-art models, point-based, voxel-based, and BEV-based architectures, highlighting their generalizability domain shifts. RoboLoc provides a realistic testbed for developing multi-domain localization systems in robotics and autonomous navigation",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01194v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization",
            "navigation"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "FOM-Nav: Frontier-Object Maps for Object Goal Navigation",
      "authors": [
        "Thomas Chabal",
        "Shizhe Chen",
        "Jean Ponce",
        "Cordelia Schmid"
      ],
      "arxiv_id": "2512.01009v1",
      "summary": "This paper addresses the Object Goal Navigation problem, where a robot must efficiently find a target object in an unknown environment. Existing implicit memory-based methods struggle with long-term memory retention and planning, while explicit map-based approaches lack rich semantic information. To address these challenges, we propose FOM-Nav, a modular framework that enhances exploration efficiency through Frontier-Object Maps and vision-language models. Our Frontier-Object Maps are built online and jointly encode spatial frontiers and fine-grained object information. Using this representation, a vision-language model performs multimodal scene understanding and high-level goal prediction, which is executed by a low-level planner for efficient trajectory generation. To train FOM-Nav, we automatically construct large-scale navigation datasets from real-world scanned environments. Extensive experiments validate the effectiveness of our model design and constructed dataset. FOM-Nav achieves state-of-the-art performance on the MP3D and HM3D benchmarks, particularly in navigation efficiency metric SPL, and yields promising results on a real robot.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "Project page: https://www.di.ens.fr/willow/research/fom-nav/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01009v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding",
            "[T]navigation"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Silhouette-based Gait Foundation Model",
      "authors": [
        "Dingqiang Ye",
        "Chao Fan",
        "Kartik Narayan",
        "Bingzhe Wu",
        "Chengwen Luo",
        "Jianqiang Li",
        "Vishal M. Patel"
      ],
      "arxiv_id": "2512.00691v1",
      "summary": "Gait patterns play a critical role in human identification and healthcare analytics, yet current progress remains constrained by small, narrowly designed models that fail to scale or generalize. Building a unified gait foundation model requires addressing two longstanding barriers: (a) Scalability. Why have gait models historically failed to follow scaling laws? (b) Generalization. Can one model serve the diverse gait tasks that have traditionally been studied in isolation? We introduce FoundationGait, the first scalable, self-supervised pretraining framework for gait understanding. Its largest version has nearly 0.13 billion parameters and is pretrained on 12 public gait datasets comprising over 2 million walking sequences. Extensive experiments demonstrate that FoundationGait, with or without fine-tuning, performs robustly across a wide spectrum of gait datasets, conditions, tasks (e.g., human identification, scoliosis screening, depression prediction, and attribute estimation), and even input modality. Notably, it achieves 48.0% zero-shot rank-1 accuracy on the challenging in-the-wild Gait3D dataset (1,000 test subjects) and 64.5% on the largest in-the-lab OU-MVLP dataset (5,000+ test subjects), setting a new milestone in robust gait recognition. Coming code and model: https://github.com/ShiqiYu/OpenGait.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00691v1",
      "code_links": [
        {
          "url": "https://github.com/ShiqiYu/OpenGait",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]gait",
            "walking"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer",
      "authors": [
        "Dong In Lee",
        "Hyungjun Doh",
        "Seunggeun Chi",
        "Runlin Duan",
        "Sangpil Kim",
        "Karthik Ramani"
      ],
      "arxiv_id": "2512.00677v1",
      "summary": "Recent progress in 4D representations, such as Dynamic NeRF and 4D Gaussian Splatting (4DGS), has enabled dynamic 4D scene reconstruction. However, text-driven 4D scene editing remains under-explored due to the challenge of ensuring both multi-view and temporal consistency across space and time during editing. Existing studies rely on 2D diffusion models that edit frames independently, often causing motion distortion, geometric drift, and incomplete editing. We introduce Dynamic-eDiTor, a training-free text-driven 4D editing framework leveraging Multimodal Diffusion Transformer (MM-DiT) and 4DGS. This mechanism consists of Spatio-Temporal Sub-Grid Attention (STGA) for locally consistent cross-view and temporal fusion, and Context Token Propagation (CTP) for global propagation via token inheritance and optical-flow-guided token replacement. Together, these components allow Dynamic-eDiTor to perform seamless, globally consistent multi-view video without additional training and directly optimize pre-trained source 4DGS. Extensive experiments on multi-view video dataset DyNeRF demonstrate that our method achieves superior editing fidelity and both multi-view and temporal consistency prior approaches. Project page for results and code: https://di-lee.github.io/dynamic-eDiTor/",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "4D Scene Editing",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00677v1",
      "code_links": [
        {
          "url": "https://di-lee.github.io/dynamic-eDiTor/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "gaussian splatting",
            "NeRF",
            "scene reconstruction",
            "optical flow"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "SplatFont3D: Structure-Aware Text-to-3D Artistic Font Generation with Part-Level Style Control",
      "authors": [
        "Ji Gan",
        "Lingxu Chen",
        "Jiaxu Leng",
        "Xinbo Gao"
      ],
      "arxiv_id": "2512.00413v1",
      "summary": "Artistic font generation (AFG) can assist human designers in creating innovative artistic fonts. However, most previous studies primarily focus on 2D artistic fonts in flat design, leaving personalized 3D-AFG largely underexplored. 3D-AFG not only enables applications in immersive 3D environments such as video games and animations, but also may enhance 2D-AFG by rendering 2D fonts of novel views. Moreover, unlike general 3D objects, 3D fonts exhibit precise semantics with strong structural constraints and also demand fine-grained part-level style control. To address these challenges, we propose SplatFont3D, a novel structure-aware text-to-3D AFG framework with 3D Gaussian splatting, which enables the creation of 3D artistic fonts from diverse style text prompts with precise part-level style control. Specifically, we first introduce a Glyph2Cloud module, which progressively enhances both the shapes and styles of 2D glyphs (or components) and produces their corresponding 3D point clouds for Gaussian initialization. The initialized 3D Gaussians are further optimized through interaction with a pretrained 2D diffusion model using score distillation sampling. To enable part-level control, we present a dynamic component assignment strategy that exploits the geometric priors of 3D Gaussians to partition components, while alleviating drift-induced entanglement during 3D Gaussian optimization. Our SplatFont3D provides more explicit and effective part-level style control than NeRF, attaining faster rendering efficiency. Experiments show that our SplatFont3D outperforms existing 3D models for 3D-AFG in style-text consistency, visual quality, and rendering efficiency.",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-29",
      "updated": "2025-11-29",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00413v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "gaussian splatting",
            "NeRF",
            "point cloud"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Unified Multimodal Diffusion Forcing for Forceful Manipulation",
      "authors": [
        "Zixuan Huang",
        "Huaidian Hou",
        "Dmitry Berenson"
      ],
      "arxiv_id": "2511.04812v1",
      "summary": "Given a dataset of expert trajectories, standard imitation learning approaches typically learn a direct mapping from observations (e.g., RGB images) to actions. However, such methods often overlook the rich interplay between different modalities, i.e., sensory inputs, actions, and rewards, which is crucial for modeling robot behavior and understanding task outcomes. In this work, we propose Multimodal Diffusion Forcing, a unified framework for learning from multimodal robot trajectories that extends beyond action generation. Rather than modeling a fixed distribution, MDF applies random partial masking and trains a diffusion model to reconstruct the trajectory. This training objective encourages the model to learn temporal and cross-modal dependencies, such as predicting the effects of actions on force signals or inferring states from partial observations. We evaluate MDF on contact-rich, forceful manipulation tasks in simulated and real-world environments. Our results show that MDF not only delivers versatile functionalities, but also achieves strong performance, and robustness under noisy observations. More visualizations can be found on our website https://unified-df.github.io",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "Project website: https://unified-df.github.io",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04812v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies",
      "authors": [
        "Maëlic Neau",
        "Zoe Falomir",
        "Paulo E. Santos",
        "Anne-Gwenn Bosser",
        "Cédric Buche"
      ],
      "arxiv_id": "2511.04357v1",
      "summary": "Deploying autonomous robots that can learn new skills from demonstrations is an important challenge of modern robotics. Existing solutions often apply end-to-end imitation learning with Vision-Language Action (VLA) models or symbolic approaches with Action Model Learning (AML). On the one hand, current VLA models are limited by the lack of high-level symbolic planning, which hinders their abilities in long-horizon tasks. On the other hand, symbolic approaches in AML lack generalization and scalability perspectives. In this paper we present a new neuro-symbolic approach, GraSP-VLA, a framework that uses a Continuous Scene Graph representation to generate a symbolic representation of human demonstrations. This representation is used to generate new planning domains during inference and serves as an orchestrator for low-level VLA policies, scaling up the number of actions that can be reproduced in a row. Our results show that GraSP-VLA is effective for modeling symbolic representations on the task of automatic planning domain generation from observations. In addition, results on real-world experiments show the potential of our Continuous Scene Graph representation to orchestrate low-level VLA policies in long-horizon tasks.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04357v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]grasp"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "CBMC-V3: A CNS-inspired Control Framework Towards Manipulation Agility with SNN",
      "authors": [
        "Yanbo Pang",
        "Qingkai Li",
        "Mingguo Zhao"
      ],
      "arxiv_id": "2511.04109v1",
      "summary": "As robotic arm applications extend beyond industrial settings into healthcare, service, and daily life, existing control algorithms struggle to achieve the agile manipulation required for complex environments with dynamic trajectories, unpredictable interactions, and diverse objects. This paper presents a biomimetic control framework based on Spiking Neural Networks (SNN), inspired by the human Central Nervous System (CNS), to achieve agile control in such environments. The proposed framework features five control modules (cerebral cortex, cerebellum, thalamus, brainstem, spinal cord), three hierarchical control levels (first-order, second-order, third-order), and two information pathways (ascending, descending). Each module is fully implemented using SNN. The spinal cord module uses spike encoding and Leaky Integrate-and-Fire (LIF) neurons for feedback control. The brainstem module employs a network of LIF and non-spiking LIF neurons to dynamically adjust spinal cord parameters via reinforcement learning. The thalamus module similarly adjusts the cerebellum's torque outputs. The cerebellum module uses a recurrent SNN to learn the robotic arm's dynamics through regression, providing feedforward gravity compensation torques. The framework is validated both in simulation and on real-world robotic arm platform under various loads and trajectories. Results demonstrate that our method outperforms the industrial-grade position control in manipulation agility.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04109v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Part-Aware Bottom-Up Group Reasoning for Fine-Grained Social Interaction Detection",
      "authors": [
        "Dongkeun Kim",
        "Minsu Cho",
        "Suha Kwak"
      ],
      "arxiv_id": "2511.03666v1",
      "summary": "Social interactions often emerge from subtle, fine-grained cues such as facial expressions, gaze, and gestures. However, existing methods for social interaction detection overlook such nuanced cues and primarily rely on holistic representations of individuals. Moreover, they directly detect social groups without explicitly modeling the underlying interactions between individuals. These drawbacks limit their ability to capture localized social signals and introduce ambiguity when group configurations should be inferred from social interactions grounded in nuanced cues. In this work, we propose a part-aware bottom-up group reasoning framework for fine-grained social interaction detection. The proposed method infers social groups and their interactions using body part features and their interpersonal relations. Our model first detects individuals and enhances their features using part-aware cues, and then infers group configuration by associating individuals via similarity-based reasoning, which considers not only spatial relations but also subtle social cues that signal interactions, leading to more accurate group inference. Experiments on the NVI dataset demonstrate that our method outperforms prior methods, achieving the new state of the art.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-05",
      "updated": "2025-11-05",
      "comment": "Accepted to NeurIPS 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03666v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "[T]social interaction"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "5_interaction_reaction"
      ]
    },
    {
      "title": "FoldPath: End-to-End Object-Centric Motion Generation via Modulated Implicit Paths",
      "authors": [
        "Paolo Rabino",
        "Gabriele Tiboni",
        "Tatiana Tommasi"
      ],
      "arxiv_id": "2511.01407v1",
      "summary": "Object-Centric Motion Generation (OCMG) is instrumental in advancing automated manufacturing processes, particularly in domains requiring high-precision expert robotic motions, such as spray painting and welding. To realize effective automation, robust algorithms are essential for generating extended, object-aware trajectories across intricate 3D geometries. However, contemporary OCMG techniques are either based on ad-hoc heuristics or employ learning-based pipelines that are still reliant on sensitive post-processing steps to generate executable paths. We introduce FoldPath, a novel, end-to-end, neural field based method for OCMG. Unlike prior deep learning approaches that predict discrete sequences of end-effector waypoints, FoldPath learns the robot motion as a continuous function, thus implicitly encoding smooth output paths. This paradigm shift eliminates the need for brittle post-processing steps that concatenate and order the predicted discrete waypoints. Particularly, our approach demonstrates superior predictive performance compared to recently proposed learning-based methods, and attains generalization capabilities even in real industrial settings, where only a limited amount of 70 expert samples are provided. We validate FoldPath through comprehensive experiments in a realistic simulation environment and introduce new, rigorous metrics designed to comprehensively evaluate long-horizon robotic paths, thus advancing the OCMG task towards practical maturity.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "Accepted at 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01407v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "[T]motion generation"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "4_motion_diffusion"
      ]
    },
    {
      "title": "GauDP: Reinventing Multi-Agent Collaboration through Gaussian-Image Synergy in Diffusion Policies",
      "authors": [
        "Ziye Wang",
        "Li Kang",
        "Yiran Qin",
        "Jiahua Ma",
        "Zhanglin Peng",
        "Lei Bai",
        "Ruimao Zhang"
      ],
      "arxiv_id": "2511.00998v1",
      "summary": "Recently, effective coordination in embodied multi-agent systems has remained a fundamental challenge, particularly in scenarios where agents must balance individual perspectives with global environmental awareness. Existing approaches often struggle to balance fine-grained local control with comprehensive scene understanding, resulting in limited scalability and compromised collaboration quality. In this paper, we present GauDP, a novel Gaussian-image synergistic representation that facilitates scalable, perception-aware imitation learning in multi-agent collaborative systems. Specifically, GauDP constructs a globally consistent 3D Gaussian field from decentralized RGB observations, then dynamically redistributes 3D Gaussian attributes to each agent's local perspective. This enables all agents to adaptively query task-critical features from the shared scene representation while maintaining their individual viewpoints. This design facilitates both fine-grained control and globally coherent behavior without requiring additional sensing modalities (e.g., 3D point cloud). We evaluate GauDP on the RoboFactory benchmark, which includes diverse multi-arm manipulation tasks. Our method achieves superior performance over existing image-based methods and approaches the effectiveness of point-cloud-driven methods, while maintaining strong scalability as the number of agents increases.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-02",
      "updated": "2025-11-02",
      "comment": "Accepted by NeurIPS 2025. Project page: https://ziyeeee.github.io/gaudp.io/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00998v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding",
            "point cloud"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Real-Time Learning of Predictive Dynamic Obstacle Models for Robotic Motion Planning",
      "authors": [
        "Stella Kombo",
        "Masih Haseli",
        "Skylar Wei",
        "Joel W. Burdick"
      ],
      "arxiv_id": "2511.00814v1",
      "summary": "Autonomous systems often must predict the motions of nearby agents from partial and noisy data. This paper asks and answers the question: \"can we learn, in real-time, a nonlinear predictive model of another agent's motions?\" Our online framework denoises and forecasts such dynamics using a modified sliding-window Hankel Dynamic Mode Decomposition (Hankel-DMD). Partial noisy measurements are embedded into a Hankel matrix, while an associated Page matrix enables singular-value hard thresholding (SVHT) to estimate the effective rank. A Cadzow projection enforces structured low-rank consistency, yielding a denoised trajectory and local noise variance estimates. From this representation, a time-varying Hankel-DMD lifted linear predictor is constructed for multi-step forecasts. The residual analysis provides variance-tracking signals that can support downstream estimators and risk-aware planning. We validate the approach in simulation under Gaussian and heavy-tailed noise, and experimentally on a dynamic crane testbed. Results show that the method achieves stable variance-aware denoising and short-horizon prediction suitable for integration into real-time control frameworks.",
      "categories": [
        "cs.RO",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-02",
      "updated": "2025-11-02",
      "comment": "10 pages, 6 figures, submitted to IEEE International Conference on Robotics and Automation (ICRA) 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00814v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]motion planning"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "predictive model"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Mitigating Error Accumulation in Co-Speech Motion Generation via Global Rotation Diffusion and Multi-Level Constraints",
      "authors": [
        "Xiangyue Zhang",
        "Jianfang Li",
        "Jianqiang Ren",
        "Jiaxu Zhang"
      ],
      "arxiv_id": "2511.10076v1",
      "summary": "Reliable co-speech motion generation requires precise motion representation and consistent structural priors across all joints. Existing generative methods typically operate on local joint rotations, which are defined hierarchically based on the skeleton structure. This leads to cumulative errors during generation, manifesting as unstable and implausible motions at end-effectors. In this work, we propose GlobalDiff, a diffusion-based framework that operates directly in the space of global joint rotations for the first time, fundamentally decoupling each joint's prediction from upstream dependencies and alleviating hierarchical error accumulation. To compensate for the absence of structural priors in global rotation space, we introduce a multi-level constraint scheme. Specifically, a joint structure constraint introduces virtual anchor points around each joint to better capture fine-grained orientation. A skeleton structure constraint enforces angular consistency across bones to maintain structural integrity. A temporal structure constraint utilizes a multi-scale variational encoder to align the generated motion with ground-truth temporal patterns. These constraints jointly regularize the global diffusion process and reinforce structural awareness. Extensive evaluations on standard co-speech benchmarks show that GlobalDiff generates smooth and accurate motions, improving the performance by 46.0 % compared to the current SOTA under multiple speaker identities.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "AAAI 2026",
      "doi": "",
      "journal_ref": "AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.10076v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "[T]motion generation"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "4_motion_diffusion"
      ]
    },
    {
      "title": "Hierarchical Direction Perception via Atomic Dot-Product Operators for Rotation-Invariant Point Clouds Learning",
      "authors": [
        "Chenyu Hu",
        "Xiaotong Li",
        "Hao Zhu",
        "Biao Hou"
      ],
      "arxiv_id": "2511.08240v1",
      "summary": "Point cloud processing has become a cornerstone technology in many 3D vision tasks. However, arbitrary rotations introduce variations in point cloud orientations, posing a long-standing challenge for effective representation learning. The core of this issue is the disruption of the point cloud's intrinsic directional characteristics caused by rotational perturbations. Recent methods attempt to implicitly model rotational equivariance and invariance, preserving directional information and propagating it into deep semantic spaces. Yet, they often fall short of fully exploiting the multiscale directional nature of point clouds to enhance feature representations. To address this, we propose the Direction-Perceptive Vector Network (DiPVNet). At its core is an atomic dot-product operator that simultaneously encodes directional selectivity and rotation invariance--endowing the network with both rotational symmetry modeling and adaptive directional perception. At the local level, we introduce a Learnable Local Dot-Product (L2DP) Operator, which enables interactions between a center point and its neighbors to adaptively capture the non-uniform local structures of point clouds. At the global level, we leverage generalized harmonic analysis to prove that the dot-product between point clouds and spherical sampling vectors is equivalent to a direction-aware spherical Fourier transform (DASFT). This leads to the construction of a global directional response spectrum for modeling holistic directional structures. We rigorously prove the rotation invariance of both operators. Extensive experiments on challenging scenarios involving noise and large-angle rotations demonstrate that DiPVNet achieves state-of-the-art performance on point cloud classification and segmentation tasks. Our code is available at https://github.com/wxszreal0/DiPVNet.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "Accepted to AAAI 2026. Code is available at: https://github.com/wxszreal0/DiPVNet",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08240v1",
      "code_links": [
        {
          "url": "https://github.com/wxszreal0/DiPVNet",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "representation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Non-Aligned Reference Image Quality Assessment for Novel View Synthesis",
      "authors": [
        "Abhijay Ghildyal",
        "Rajesh Sureddi",
        "Nabajeet Barman",
        "Saman Zadtootaghaj",
        "Alan Bovik"
      ],
      "arxiv_id": "2511.08155v1",
      "summary": "Evaluating the perceptual quality of Novel View Synthesis (NVS) images remains a key challenge, particularly in the absence of pixel-aligned ground truth references. Full-Reference Image Quality Assessment (FR-IQA) methods fail under misalignment, while No-Reference (NR-IQA) methods struggle with generalization. In this work, we introduce a Non-Aligned Reference (NAR-IQA) framework tailored for NVS, where it is assumed that the reference view shares partial scene content but lacks pixel-level alignment. We constructed a large-scale image dataset containing synthetic distortions targeting Temporal Regions of Interest (TROI) to train our NAR-IQA model. Our model is built on a contrastive learning framework that incorporates LoRA-enhanced DINOv2 embeddings and is guided by supervision from existing IQA methods. We train exclusively on synthetically generated distortions, deliberately avoiding overfitting to specific real NVS samples and thereby enhancing the model's generalization capability. Our model outperforms state-of-the-art FR-IQA, NR-IQA, and NAR-IQA methods, achieving robust performance on both aligned and non-aligned references. We also conducted a novel user study to gather data on human preferences when viewing non-aligned references in NVS. We find strong correlation between our proposed quality prediction model and the collected subjective ratings. For dataset and code, please visit our project page: https://stootaghaj.github.io/nova-project/",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08155v1",
      "code_links": [
        {
          "url": "https://stootaghaj.github.io/nova-project/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "contrastive learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]novel view synthesis"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Multi-Modal Assistance for Unsupervised Domain Adaptation on Point Cloud 3D Object Detection",
      "authors": [
        "Shenao Zhao",
        "Pengpeng Liang",
        "Zhoufan Yang"
      ],
      "arxiv_id": "2511.07966v1",
      "summary": "Unsupervised domain adaptation for LiDAR-based 3D object detection (3D UDA) based on the teacher-student architecture with pseudo labels has achieved notable improvements in recent years. Although it is quite popular to collect point clouds and images simultaneously, little attention has been paid to the usefulness of image data in 3D UDA when training the models. In this paper, we propose an approach named MMAssist that improves the performance of 3D UDA with multi-modal assistance. A method is designed to align 3D features between the source domain and the target domain by using image and text features as bridges. More specifically, we project the ground truth labels or pseudo labels to the images to get a set of 2D bounding boxes. For each 2D box, we extract its image feature from a pre-trained vision backbone. A large vision-language model (LVLM) is adopted to extract the box's text description, and a pre-trained text encoder is used to obtain its text feature. During the training of the model in the source domain and the student model in the target domain, we align the 3D features of the predicted boxes with their corresponding image and text features, and the 3D features and the aligned features are fused with learned weights for the final prediction. The features between the student branch and the teacher branch in the target domain are aligned as well. To enhance the pseudo labels, we use an off-the-shelf 2D object detector to generate 2D bounding boxes from images and estimate their corresponding 3D boxes with the aid of point cloud, and these 3D boxes are combined with the pseudo labels generated by the teacher model. Experimental results show that our approach achieves promising performance compared with state-of-the-art methods in three domain adaptation tasks on three popular 3D object detection datasets. The code is available at https://github.com/liangp/MMAssist.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "Accepted to AAAI-26",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07966v1",
      "code_links": [
        {
          "url": "https://github.com/liangp/MMAssist",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "teacher-student"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Human Motion Synthesis in 3D Scenes via Unified Scene Semantic Occupancy",
      "authors": [
        "Gong Jingyu",
        "Tong Kunkun",
        "Chen Zhuoran",
        "Yuan Chuanhan",
        "Chen Mingang",
        "Zhang Zhizhong",
        "Tan Xin",
        "Xie Yuan"
      ],
      "arxiv_id": "2511.07819v1",
      "summary": "Human motion synthesis in 3D scenes relies heavily on scene comprehension, while current methods focus mainly on scene structure but ignore the semantic understanding. In this paper, we propose a human motion synthesis framework that take an unified Scene Semantic Occupancy (SSO) for scene representation, termed SSOMotion. We design a bi-directional tri-plane decomposition to derive a compact version of the SSO, and scene semantics are mapped to an unified feature space via CLIP encoding and shared linear dimensionality reduction. Such strategy can derive the fine-grained scene semantic structures while significantly reduce redundant computations. We further take these scene hints and movement direction derived from instructions for motion control via frame-wise scene query. Extensive experiments and ablation studies conducted on cluttered scenes using ShapeNet furniture, as well as scanned scenes from PROX and Replica datasets, demonstrate its cutting-edge performance while validating its effectiveness and generalization ability. Code will be publicly available at https://github.com/jingyugong/SSOMotion.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07819v1",
      "code_links": [
        {
          "url": "https://github.com/jingyugong/SSOMotion",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "[T]motion synthesis"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "4_motion_diffusion"
      ]
    },
    {
      "title": "DIMO: Diverse 3D Motion Generation for Arbitrary Objects",
      "authors": [
        "Linzhan Mou",
        "Jiahui Lei",
        "Chen Wang",
        "Lingjie Liu",
        "Kostas Daniilidis"
      ],
      "arxiv_id": "2511.07409v1",
      "summary": "We present DIMO, a generative approach capable of generating diverse 3D motions for arbitrary objects from a single image. The core idea of our work is to leverage the rich priors in well-trained video models to extract the common motion patterns and then embed them into a shared low-dimensional latent space. Specifically, we first generate multiple videos of the same object with diverse motions. We then embed each motion into a latent vector and train a shared motion decoder to learn the distribution of motions represented by a structured and compact motion representation, i.e., neural key point trajectories. The canonical 3D Gaussians are then driven by these key points and fused to model the geometry and appearance. During inference time with learned latent space, we can instantly sample diverse 3D motions in a single-forward pass and support several interesting applications including 3D motion interpolation and language-guided motion generation. Our project page is available at https://linzhanm.github.io/dimo.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "Published in ICCV 2025, project page https://linzhanm.github.io/dimo",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07409v1",
      "code_links": [
        {
          "url": "https://linzhanm.github.io/dimo",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "[T]motion generation"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "4_motion_diffusion"
      ]
    },
    {
      "title": "OpenVLN: Open-world Aerial Vision-Language Navigation",
      "authors": [
        "Peican Lin",
        "Gan Sun",
        "Chenxi Liu",
        "Fazeng Li",
        "Weihong Ren",
        "Yang Cong"
      ],
      "arxiv_id": "2511.06182v2",
      "summary": "Vision-language models (VLMs) have been widely-applied in ground-based vision-language navigation (VLN). However, the vast complexity of outdoor aerial environments compounds data acquisition challenges and imposes long-horizon trajectory planning requirements on Unmanned Aerial Vehicles (UAVs), introducing novel complexities for aerial VLN. To address these challenges, we propose a data-efficient Open-world aerial Vision-Language Navigation (i.e., OpenVLN) framework, which could execute language-guided flight with limited data constraints and enhance long-horizon trajectory planning capabilities in complex aerial environments. Specifically, we reconfigure a reinforcement learning framework to optimize the VLM for UAV navigation tasks, which can efficiently fine-tune VLM by using rule-based policies under limited training data. Concurrently, we introduce a long-horizon planner for trajectory synthesis that dynamically generates precise UAV actions via value-based rewards. To the end, we conduct sufficient navigation experiments on the TravelUAV benchmark with dataset scaling across diverse reward settings. Our method demonstrates consistent performance gains of up to 4.34% in Success Rate, 6.19% in Oracle Success Rate, and 4.07% in Success weighted by Path Length over baseline methods, validating its deployment efficacy for long-horizon UAV navigation in complex aerial environments.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-09",
      "updated": "2025-11-21",
      "comment": "Content: 8 pages 4 figures, conference paper under review",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06182v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Adaptive Agent Selection and Interaction Network for Image-to-point cloud Registration",
      "authors": [
        "Zhixin Cheng",
        "Xiaotian Yin",
        "Jiacheng Deng",
        "Bohao Liao",
        "Yujia Chen",
        "Xu Zhou",
        "Baoqun Yin",
        "Tianzhu Zhang"
      ],
      "arxiv_id": "2511.05965v1",
      "summary": "Typical detection-free methods for image-to-point cloud registration leverage transformer-based architectures to aggregate cross-modal features and establish correspondences. However, they often struggle under challenging conditions, where noise disrupts similarity computation and leads to incorrect correspondences. Moreover, without dedicated designs, it remains difficult to effectively select informative and correlated representations across modalities, thereby limiting the robustness and accuracy of registration. To address these challenges, we propose a novel cross-modal registration framework composed of two key modules: the Iterative Agents Selection (IAS) module and the Reliable Agents Interaction (RAI) module. IAS enhances structural feature awareness with phase maps and employs reinforcement learning principles to efficiently select reliable agents. RAI then leverages these selected agents to guide cross-modal interactions, effectively reducing mismatches and improving overall robustness. Extensive experiments on the RGB-D Scenes v2 and 7-Scenes benchmarks demonstrate that our method consistently achieves state-of-the-art performance.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-08",
      "updated": "2025-11-08",
      "comment": "Accepted by AAAI2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05965v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation",
      "authors": [
        "Zhenyuan Qin",
        "Xincheng Shuai",
        "Henghui Ding"
      ],
      "arxiv_id": "2511.16666v1",
      "summary": "Controllable image generation has attracted increasing attention in recent years, enabling users to manipulate visual content such as identity and style. However, achieving simultaneous control over the 9D poses (location, size, and orientation) of multiple objects remains an open challenge. Despite recent progress, existing methods often suffer from limited controllability and degraded quality, falling short of comprehensive multi-object 9D pose control. To address these limitations, we propose SceneDesigner, a method for accurate and flexible multi-object 9-DoF pose manipulation. SceneDesigner incorporates a branched network to the pre-trained base model and leverages a new representation, CNOCS map, which encodes 9D pose information from the camera view. This representation exhibits strong geometric interpretation properties, leading to more efficient and stable training. To support training, we construct a new dataset, ObjectPose9D, which aggregates images from diverse sources along with 9D pose annotations. To further address data imbalance issues, particularly performance degradation on low-frequency poses, we introduce a two-stage training strategy with reinforcement learning, where the second stage fine-tunes the model using a reward-based objective on rebalanced data. At inference time, we propose Disentangled Object Sampling, a technique that mitigates insufficient object generation and concept confusion in complex multi-object scenes. Moreover, by integrating user-specific personalization weights, SceneDesigner enables customized pose control for reference subjects. Extensive qualitative and quantitative experiments demonstrate that SceneDesigner significantly outperforms existing approaches in both controllability and quality. Code is publicly available at https://github.com/FudanCVL/SceneDesigner.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "NeurIPS 2025 (Spotlight), Project Page: https://henghuiding.com/SceneDesigner/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16666v1",
      "code_links": [
        {
          "url": "https://github.com/FudanCVL/SceneDesigner",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Can MLLMs Read the Room? A Multimodal Benchmark for Assessing Deception in Multi-Party Social Interactions",
      "authors": [
        "Caixin Kang",
        "Yifei Huang",
        "Liangyang Ouyang",
        "Mingfang Zhang",
        "Ruicong Liu",
        "Yoichi Sato"
      ],
      "arxiv_id": "2511.16221v1",
      "summary": "Despite their advanced reasoning capabilities, state-of-the-art Multimodal Large Language Models (MLLMs) demonstrably lack a core component of human intelligence: the ability to `read the room' and assess deception in complex social interactions. To rigorously quantify this failure, we introduce a new task, Multimodal Interactive Deception Assessment (MIDA), and present a novel multimodal dataset providing synchronized video and text with verifiable ground-truth labels for every statement. We establish a comprehensive benchmark evaluating 12 state-of-the-art open- and closed-source MLLMs, revealing a significant performance gap: even powerful models like GPT-4o struggle to distinguish truth from falsehood reliably. Our analysis of failure modes indicates that these models fail to effectively ground language in multimodal social cues and lack the ability to model what others know, believe, or intend, highlighting the urgent need for novel approaches to building more perceptive and trustworthy AI systems. To take a step forward, we design a Social Chain-of-Thought (SoCoT) reasoning pipeline and a Dynamic Social Epistemic Memory (DSEM) module. Our framework yields performance improvement on this challenging task, demonstrating a promising new path toward building MLLMs capable of genuine human-like social reasoning.",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16221v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "[T]social interaction"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "5_interaction_reaction"
      ]
    },
    {
      "title": "Simba: Towards High-Fidelity and Geometrically-Consistent Point Cloud Completion via Transformation Diffusion",
      "authors": [
        "Lirui Zhang",
        "Zhengkai Zhao",
        "Zhi Zuo",
        "Pan Gao",
        "Jie Qin"
      ],
      "arxiv_id": "2511.16161v1",
      "summary": "Point cloud completion is a fundamental task in 3D vision. A persistent challenge in this field is simultaneously preserving fine-grained details present in the input while ensuring the global structural integrity of the completed shape. While recent works leveraging local symmetry transformations via direct regression have significantly improved the preservation of geometric structure details, these methods suffer from two major limitations: (1) These regression-based methods are prone to overfitting which tend to memorize instant-specific transformations instead of learning a generalizable geometric prior. (2) Their reliance on point-wise transformation regression lead to high sensitivity to input noise, severely degrading their robustness and generalization. To address these challenges, we introduce Simba, a novel framework that reformulates point-wise transformation regression as a distribution learning problem. Our approach integrates symmetry priors with the powerful generative capabilities of diffusion models, avoiding instance-specific memorization while capturing robust geometric structures. Additionally, we introduce a hierarchical Mamba-based architecture to achieve high-fidelity upsampling. Extensive experiments across the PCN, ShapeNet, and KITTI benchmarks validate our method's state-of-the-art (SOTA) performance.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "Accepted for publication at the 40th AAAI Conference on Artificial Intelligence (AAAI-26)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16161v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "Mamba"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Learning Depth from Past Selves: Self-Evolution Contrast for Robust Depth Estimation",
      "authors": [
        "Jing Cao",
        "Kui Jiang",
        "Shenyi Li",
        "Xiaocheng Feng",
        "Yong Huang"
      ],
      "arxiv_id": "2511.15167v1",
      "summary": "Self-supervised depth estimation has gained significant attention in autonomous driving and robotics. However, existing methods exhibit substantial performance degradation under adverse weather conditions such as rain and fog, where reduced visibility critically impairs depth prediction. To address this issue, we propose a novel self-evolution contrastive learning framework called SEC-Depth for self-supervised robust depth estimation tasks. Our approach leverages intermediate parameters generated during training to construct temporally evolving latency models. Using these, we design a self-evolution contrastive scheme to mitigate performance loss under challenging conditions. Concretely, we first design a dynamic update strategy of latency models for the depth estimation task to capture optimization states across training stages. To effectively leverage latency models, we introduce a self-evolution contrastive Loss (SECL) that treats outputs from historical latency models as negative samples. This mechanism adaptively adjusts learning objectives while implicitly sensing weather degradation severity, reducing the needs for manual intervention. Experiments show that our method integrates seamlessly into diverse baseline models and significantly enhances robustness in zero-shot evaluations.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15167v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "contrastive learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]depth estimation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization",
      "authors": [
        "Jian-Ting Guo",
        "Yu-Cheng Chen",
        "Ping-Chun Hsieh",
        "Kuo-Hao Ho",
        "Po-Wei Huang",
        "Ti-Rong Wu",
        "I-Chen Wu"
      ],
      "arxiv_id": "2511.15055v1",
      "summary": "Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "Accepted by the Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15055v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]trajectory optimization"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Reinforcement Learning from Implicit Neural Feedback for Human-Aligned Robot Control",
      "authors": [
        "Suzie Kim"
      ],
      "arxiv_id": "2512.00050v1",
      "summary": "Conventional reinforcement learning (RL) approaches often struggle to learn effective policies under sparse reward conditions, necessitating the manual design of complex, task-specific reward functions. To address this limitation, reinforcement learning from human feedback (RLHF) has emerged as a promising strategy that complements hand-crafted rewards with human-derived evaluation signals. However, most existing RLHF methods depend on explicit feedback mechanisms such as button presses or preference labels, which disrupt the natural interaction process and impose a substantial cognitive load on the user. We propose a novel reinforcement learning from implicit human feedback (RLIHF) framework that utilizes non-invasive electroencephalography (EEG) signals, specifically error-related potentials (ErrPs), to provide continuous, implicit feedback without requiring explicit user intervention. The proposed method adopts a pre-trained decoder to transform raw EEG signals into probabilistic reward components, enabling effective policy learning even in the presence of sparse external rewards. We evaluate our approach in a simulation environment built on the MuJoCo physics engine, using a Kinova Gen2 robotic arm to perform a complex pick-and-place task that requires avoiding obstacles while manipulating target objects. The results show that agents trained with decoded EEG feedback achieve performance comparable to those trained with dense, manually designed rewards. These findings validate the potential of using implicit neural feedback for scalable and human-aligned reinforcement learning in interactive robotics.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "Master's thesis, Korea University, 2025. arXiv admin note: substantial text overlap with arXiv:2507.13171",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00050v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "policy learning",
            "MuJoCo"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "PIGEON: VLM-Driven Object Navigation via Points of Interest Selection",
      "authors": [
        "Cheng Peng",
        "Zhenzhe Zhang",
        "Cheng Chi",
        "Xiaobao Wei",
        "Yanhao Zhang",
        "Heng Wang",
        "Pengwei Wang",
        "Zhongyuan Wang",
        "Jing Liu",
        "Shanghang Zhang"
      ],
      "arxiv_id": "2511.13207v1",
      "summary": "Navigating to a specified object in an unknown environment is a fundamental yet challenging capability of embodied intelligence. However, current methods struggle to balance decision frequency with intelligence, resulting in decisions lacking foresight or discontinuous actions. In this work, we propose PIGEON: Point of Interest Guided Exploration for Object Navigation with VLM, maintaining a lightweight and semantically aligned snapshot memory during exploration as semantic input for the exploration strategy. We use a large Visual-Language Model (VLM), named PIGEON-VL, to select Points of Interest (PoI) formed during exploration and then employ a lower-level planner for action output, increasing the decision frequency. Additionally, this PoI-based decision-making enables the generation of Reinforcement Learning with Verifiable Reward (RLVR) data suitable for simulators. Experiments on classic object navigation benchmarks demonstrate that our zero-shot transfer method achieves state-of-the-art performance, while RLVR further enhances the model's semantic guidance capabilities, enabling deep reasoning during real-time navigation.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13207v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "X-VMamba: Explainable Vision Mamba",
      "authors": [
        "Mohamed A. Mabrok",
        "Yalda Zafari"
      ],
      "arxiv_id": "2511.12694v1",
      "summary": "State Space Models (SSMs), particularly the Mamba architecture, have recently emerged as powerful alternatives to Transformers for sequence modeling, offering linear computational complexity while achieving competitive performance. Yet, despite their effectiveness, understanding how these Vision SSMs process spatial information remains challenging due to the lack of transparent, attention-like mechanisms. To address this gap, we introduce a controllability-based interpretability framework that quantifies how different parts of the input sequence (tokens or patches) influence the internal state dynamics of SSMs. We propose two complementary formulations: a Jacobian-based method applicable to any SSM architecture that measures influence through the full chain of state propagation, and a Gramian-based approach for diagonal SSMs that achieves superior speed through closed-form analytical solutions. Both methods operate in a single forward pass with linear complexity, requiring no architectural modifications or hyperparameter tuning. We validate our framework through experiments on three diverse medical imaging modalities, demonstrating that SSMs naturally implement hierarchical feature refinement from diffuse low-level textures in early layers to focused, clinically meaningful patterns in deeper layers. Our analysis reveals domain-specific controllability signatures aligned with diagnostic criteria, progressive spatial selectivity across the network hierarchy, and the substantial influence of scanning strategies on attention patterns. Beyond medical imaging, we articulate applications spanning computer vision, natural language processing, and cross-domain tasks. Our framework establishes controllability analysis as a unified, foundational interpretability paradigm for SSMs across all domains. Code and analysis tools will be made available upon publication",
      "categories": [
        "cs.CV",
        "cs.LG",
        "math.DS"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-16",
      "updated": "2025-11-16",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12694v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba",
            "SSM",
            "state space model"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Task-Aware Morphology Optimization of Planar Manipulators via Reinforcement Learning",
      "authors": [
        "Arvind Kumar Mishra",
        "Sohom Chakrabarty"
      ],
      "arxiv_id": "2511.12650v1",
      "summary": "In this work, Yoshikawa's manipulability index is used to investigate reinforcement learning (RL) as a framework for morphology optimization in planar robotic manipulators. A 2R manipulator tracking a circular end-effector path is first examined because this case has a known analytical optimum: equal link lengths and the second joint orthogonal to the first. This serves as a validation step to test whether RL can rediscover the optimum using reward feedback alone, without access to the manipulability expression or the Jacobian. Three RL algorithms (SAC, DDPG, and PPO) are compared with grid search and black-box optimizers, with morphology represented by a single action parameter phi that maps to the link lengths. All methods converge to the analytical solution, showing that numerical recovery of the optimum is possible without supplying analytical structure.\n  Most morphology design tasks have no closed-form solutions, and grid or heuristic search becomes expensive as dimensionality increases. RL is therefore explored as a scalable alternative. The formulation used for the circular path is extended to elliptical and rectangular paths by expanding the action space to the full morphology vector (L1, L2, theta2). In these non-analytical settings, RL continues to converge reliably, whereas grid and black-box methods require far larger evaluation budgets. These results indicate that RL is effective for both recovering known optima and solving morphology optimization problems without analytical solutions.",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-16",
      "updated": "2025-11-16",
      "comment": "10 pages, 11 figures, It is submitted as a journal option paper associated with the IFAC World Congress 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12650v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "PPO",
            "SAC"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "SocialNav-Map: Dynamic Mapping with Human Trajectory Prediction for Zero-Shot Social Navigation",
      "authors": [
        "Lingfeng Zhang",
        "Erjia Xiao",
        "Xiaoshuai Hao",
        "Haoxiang Fu",
        "Zeying Gong",
        "Long Chen",
        "Xiaojun Liang",
        "Renjing Xu",
        "Hangjun Ye",
        "Wenbo Ding"
      ],
      "arxiv_id": "2511.12232v2",
      "summary": "Social navigation in densely populated dynamic environments poses a significant challenge for autonomous mobile robots, requiring advanced strategies for safe interaction. Existing reinforcement learning (RL)-based methods require over 2000+ hours of extensive training and often struggle to generalize to unfamiliar environments without additional fine-tuning, limiting their practical application in real-world scenarios. To address these limitations, we propose SocialNav-Map, a novel zero-shot social navigation framework that combines dynamic human trajectory prediction with occupancy mapping, enabling safe and efficient navigation without the need for environment-specific training. Specifically, SocialNav-Map first transforms the task goal position into the constructed map coordinate system. Subsequently, it creates a dynamic occupancy map that incorporates predicted human movements as dynamic obstacles. The framework employs two complementary methods for human trajectory prediction: history prediction and orientation prediction. By integrating these predicted trajectories into the occupancy map, the robot can proactively avoid potential collisions with humans while efficiently navigating to its destination. Extensive experiments on the Social-HM3D and Social-MP3D datasets demonstrate that SocialNav-Map significantly outperforms state-of-the-art (SOTA) RL-based methods, which require 2,396 GPU hours of training. Notably, it reduces human collision rates by over 10% without necessitating any training in novel environments. By eliminating the need for environment-specific training, SocialNav-Map achieves superior navigation performance, paving the way for the deployment of social navigation systems in real-world environments characterized by diverse human behaviors. The code is available at: https://github.com/linglingxiansen/SocialNav-Map.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-15",
      "updated": "2025-11-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12232v2",
      "code_links": [
        {
          "url": "https://github.com/linglingxiansen/SocialNav-Map",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Deadlock-Free Hybrid RL-MAPF Framework for Zero-Shot Multi-Robot Navigation",
      "authors": [
        "Haoyi Wang",
        "Licheng Luo",
        "Yiannis Kantaros",
        "Bruno Sinopoli",
        "Mingyu Cai"
      ],
      "arxiv_id": "2511.22685v1",
      "summary": "Multi-robot navigation in cluttered environments presents fundamental challenges in balancing reactive collision avoidance with long-range goal achievement. When navigating through narrow passages\n  or confined spaces, deadlocks frequently emerge that prevent agents from reaching their destinations, particularly when Reinforcement Learning (RL) control policies encounter novel configurations out of learning distribution. Existing RL-based approaches suffer from limited generalization capability in unseen environments. We propose a hybrid framework that seamlessly integrates RL-based reactive navigation with on-demand Multi-Agent Path Finding (MAPF) to explicitly resolve topological deadlocks. Our approach integrates a safety layer that monitors agent progress to detect deadlocks and, when detected, triggers a coordination controller for affected agents. The framework constructs globally feasible trajectories via MAPF and regulates waypoint progression to reduce inter-agent conflicts during navigation.\n  Extensive evaluation on dense multi-agent benchmarks shows that our method boosts task completion from marginal to near-universal success, markedly reducing deadlocks and collisions. When integrated with hierarchical task planning, it enables coordinated navigation for heterogeneous robots, demonstrating that coupling reactive RL navigation with selective MAPF intervention yields a robust, zero-shot performance.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22685v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Multimodal Robust Prompt Distillation for 3D Point Cloud Models",
      "authors": [
        "Xiang Gu",
        "Liming Lu",
        "Xu Zheng",
        "Anan Du",
        "Yongbin Zhou",
        "Shuchao Pang"
      ],
      "arxiv_id": "2511.21574v1",
      "summary": "Adversarial attacks pose a significant threat to learning-based 3D point cloud models, critically undermining their reliability in security-sensitive applications. Existing defense methods often suffer from (1) high computational overhead and (2) poor generalization ability across diverse attack types. To bridge these gaps, we propose a novel yet efficient teacher-student framework, namely Multimodal Robust Prompt Distillation (MRPD) for distilling robust 3D point cloud model. It learns lightweight prompts by aligning student point cloud model's features with robust embeddings from three distinct teachers: a vision model processing depth projections, a high-performance 3D model, and a text encoder. To ensure a reliable knowledge transfer, this distillation is guided by a confidence-gated mechanism which dynamically balances the contribution of all input modalities. Notably, since the distillation is all during the training stage, there is no additional computational cost at inference. Extensive experiments demonstrate that MRPD substantially outperforms state-of-the-art defense methods against a wide range of white-box and black-box attacks, while even achieving better performance on clean data. Our work presents a new, practical paradigm for building robust 3D vision systems by efficiently harnessing multimodal knowledge.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21574v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "teacher-student"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Privacy-Preserving Federated Vision Transformer Learning Leveraging Lightweight Homomorphic Encryption in Medical AI",
      "authors": [
        "Al Amin",
        "Kamrul Hasan",
        "Liang Hong",
        "Sharif Ullah"
      ],
      "arxiv_id": "2511.20983v1",
      "summary": "Collaborative machine learning across healthcare institutions promises improved diagnostic accuracy by leveraging diverse datasets, yet privacy regulations such as HIPAA prohibit direct patient data sharing. While federated learning (FL) enables decentralized training without raw data exchange, recent studies show that model gradients in conventional FL remain vulnerable to reconstruction attacks, potentially exposing sensitive medical information. This paper presents a privacy-preserving federated learning framework combining Vision Transformers (ViT) with homomorphic encryption (HE) for secure multi-institutional histopathology classification. The approach leverages the ViT CLS token as a compact 768-dimensional feature representation for secure aggregation, encrypting these tokens using CKKS homomorphic encryption before transmission to the server. We demonstrate that encrypting CLS tokens achieves a 30-fold communication reduction compared to gradient encryption while maintaining strong privacy guarantees. Through evaluation on a three-client federated setup for lung cancer histopathology classification, we show that gradients are highly susceptible to model inversion attacks (PSNR: 52.26 dB, SSIM: 0.999, NMI: 0.741), enabling near-perfect image reconstruction. In contrast, the proposed CLS-protected HE approach prevents such attacks while enabling encrypted inference directly on ciphertexts, requiring only 326 KB of encrypted data transmission per aggregation round. The framework achieves 96.12 percent global classification accuracy in the unencrypted domain and 90.02 percent in the encrypted domain.",
      "categories": [
        "cs.CV",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "7 pages, 4 figures",
      "doi": "",
      "journal_ref": "IEEE ICNC2026",
      "pdf_url": "https://arxiv.org/pdf/2511.20983v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "[T]OMOMO"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "5_interaction_reaction"
      ]
    },
    {
      "title": "CostNav: A Navigation Benchmark for Cost-Aware Evaluation of Embodied Agents",
      "authors": [
        "Haebin Seong",
        "Sungmin Kim",
        "Minchan Kim",
        "Yongjun Cho",
        "Myunchul Joe",
        "Suhwan Choi",
        "Jaeyoon Jung",
        "Jiyong Youn",
        "Yoonshik Kim",
        "Samwoo Seong",
        "Yubeen Park",
        "Youngjae Yu",
        "Yunsung Lee"
      ],
      "arxiv_id": "2511.20216v1",
      "summary": "Existing navigation benchmarks focus on task success metrics while overlooking economic viability -- critical for commercial deployment of autonomous delivery robots. We introduce \\emph{CostNav}, a \\textbf{Micro-Navigation Economic Testbed} that evaluates embodied agents through comprehensive cost-revenue analysis aligned with real-world business operations. CostNav models the complete economic lifecycle including hardware, training, energy, maintenance costs, and delivery revenue with service-level agreements, using industry-derived parameters. \\textbf{To our knowledge, CostNav is the first work to quantitatively expose the gap between navigation research metrics and commercial viability}, revealing that optimizing for task success fundamentally differs from optimizing for economic deployment. Our cost model uses parameters derived from industry data sources (energy rates, delivery service pricing), and we project from a reduced-scale simulation to realistic deliveries. Under this projection, the baseline achieves 43.0\\% SLA compliance but is \\emph{not} commercially viable: yielding a loss of \\$30.009 per run with no finite break-even point, because operating costs are dominated by collision-induced maintenance, which accounts for 99.7\\% of per-run costs and highlights collision avoidance as a key optimization target. We demonstrate a learning-based on-device navigation baseline and establish a foundation for evaluating rule-based navigation, imitation learning, and cost-aware RL training. CostNav bridges the gap between navigation research and commercial deployment, enabling data-driven decisions about economic trade-offs across navigation paradigms.",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20216v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Arcadia: Toward a Full-Lifecycle Framework for Embodied Lifelong Learning",
      "authors": [
        "Minghe Gao",
        "Juncheng Li",
        "Yuze Lin",
        "Xuqi Liu",
        "Jiaming Ji",
        "Xiaoran Pan",
        "Zihan Xu",
        "Xian Li",
        "Mingjie Li",
        "Wei Ji",
        "Rong Wei",
        "Rui Tang",
        "Qizhou Wang",
        "Kai Shen",
        "Jun Xiao",
        "Qi Wu",
        "Siliang Tang",
        "Yueting Zhuang"
      ],
      "arxiv_id": "2512.00076v1",
      "summary": "We contend that embodied learning is fundamentally a lifecycle problem rather than a single-stage optimization. Systems that optimize only one link (data collection, simulation, learning, or deployment) rarely sustain improvement or generalize beyond narrow settings. We introduce Arcadia, a closed-loop framework that operationalizes embodied lifelong learning by tightly coupling four stages: (1) Self-evolving exploration and grounding for autonomous data acquisition in physical environments, (2) Generative scene reconstruction and augmentation for realistic and extensible scene creation, (3) a Shared embodied representation architecture that unifies navigation and manipulation within a single multimodal backbone, and (4) Sim-from-real evaluation and evolution that closes the feedback loop through simulation-based adaptation. This coupling is non-decomposable: removing any stage breaks the improvement loop and reverts to one-shot training. Arcadia delivers consistent gains on navigation and manipulation benchmarks and transfers robustly to physical robots, indicating that a tightly coupled lifecycle: continuous real-world data acquisition, generative simulation update, and shared-representation learning, supports lifelong improvement and end-to-end generalization. We release standardized interfaces enabling reproducible evaluation and cross-model comparison in reusable environments, positioning Arcadia as a scalable foundation for general-purpose embodied agents.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00076v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "representation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene reconstruction",
            "navigation"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "First-order Sobolev Reinforcement Learning",
      "authors": [
        "Fabian Schramm",
        "Nicolas Perrin-Gilbert",
        "Justin Carpentier"
      ],
      "arxiv_id": "2511.19165v1",
      "summary": "We propose a refinement of temporal-difference learning that enforces first-order Bellman consistency: the learned value function is trained to match not only the Bellman targets in value but also their derivatives with respect to states and actions. By differentiating the Bellman backup through differentiable dynamics, we obtain analytically consistent gradient targets. Incorporating these into the critic objective using a Sobolev-type loss encourages the critic to align with both the value and local geometry of the target function. This first-order TD matching principle can be seamlessly integrated into existing algorithms, such as Q-learning or actor-critic methods (e.g., DDPG, SAC), potentially leading to faster critic convergence and more stable policy gradients without altering their overall structure.",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "Workshop paper at Differentiable Systems and Scientific Machine Learning, EurIPS 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19165v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "SAC",
            "actor-critic"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Dreaming Falcon: Physics-Informed Model-Based Reinforcement Learning for Quadcopters",
      "authors": [
        "Eashan Vytla",
        "Bhavanishankar Kalavakolanu",
        "Andrew Perrault",
        "Matthew McCrink"
      ],
      "arxiv_id": "2511.18243v1",
      "summary": "Current control algorithms for aerial robots struggle with robustness in dynamic environments and adverse conditions. Model-based reinforcement learning (RL) has shown strong potential in handling these challenges while remaining sample-efficient. Additionally, Dreamer has demonstrated that online model-based RL can be achieved using a recurrent world model trained on replay buffer data. However, applying Dreamer to aerial systems has been quite challenging due to its sample inefficiency and poor generalization of dynamics models. Our work explores a physics-informed approach to world model learning and improves policy performance. The world model treats the quadcopter as a free-body system and predicts the net forces and moments acting on it, which are then passed through a 6-DOF Runge-Kutta integrator (RK4) to predict future state rollouts. In this paper, we compare this physics-informed method to a standard RNN-based world model. Although both models perform well on the training data, we observed that they fail to generalize to new trajectories, leading to rapid divergence in state rollouts, preventing policy convergence.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-23",
      "updated": "2025-11-23",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18243v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "world model",
            "dreamer"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Multi-speaker Attention Alignment for Multimodal Social Interaction",
      "authors": [
        "Liangyang Ouyang",
        "Yifei Huang",
        "Mingfang Zhang",
        "Caixin Kang",
        "Ryosuke Furuta",
        "Yoichi Sato"
      ],
      "arxiv_id": "2511.17952v1",
      "summary": "Understanding social interaction in video requires reasoning over a dynamic interplay of verbal and non-verbal cues: who is speaking, to whom, and with what gaze or gestures. While Multimodal Large Language Models (MLLMs) are natural candidates, simply adding visual inputs yields surprisingly inconsistent gains on social tasks. Our quantitative analysis of cross-modal attention inside state-of-the-art MLLMs reveals a core failure mode: in multi-speaker scenes, visual and textual tokens lack speaker-consistent alignment, exhibiting substantially weaker cross-modal attention than in object-centric images. To address this, we propose a multimodal multi-speaker attention alignment method that can be integrated into existing MLLMs. First, we introduce dynamic cross-modal head selection to identify attention heads most responsible for grounding. Then, an adaptive social-aware attention bias, computed from existing attention patterns and speaker locations, is injected into the attention mechanism. This bias reinforces alignment between a speaker's visual representation and their utterances without introducing trainable parameters or architectural changes. We integrate our method into three distinct MLLMs (LLaVA-NeXT-Video, Qwen2.5-VL, and InternVL3) and evaluate on three benchmarks (TVQA+, MMSI, OnlineMMSI). Across four social tasks, results demonstrate that our approach improves the ability of MLLMs and achieves state-of-the-art results. Attention visualizations confirm our method successfully focuses the model on speaker-relevant regions, enabling more robust multi-party social reasoning. Our implementation and model will be available at https://github.com/ut-vision/SocialInteraction.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-22",
      "updated": "2025-11-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17952v1",
      "code_links": [
        {
          "url": "https://github.com/ut-vision/SocialInteraction",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "[T]social interaction"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "5_interaction_reaction"
      ]
    },
    {
      "title": "S2AM3D: Scale-controllable Part Segmentation of 3D Point Cloud",
      "authors": [
        "Han Su",
        "Tianyu Huang",
        "Zichen Wan",
        "Xiaohe Wu",
        "Wangmeng Zuo"
      ],
      "arxiv_id": "2512.00995v1",
      "summary": "Part-level point cloud segmentation has recently attracted significant attention in 3D computer vision. Nevertheless, existing research is constrained by two major challenges: native 3D models lack generalization due to data scarcity, while introducing 2D pre-trained knowledge often leads to inconsistent segmentation results across different views. To address these challenges, we propose S2AM3D, which incorporates 2D segmentation priors with 3D consistent supervision. We design a point-consistent part encoder that aggregates multi-view 2D features through native 3D contrastive learning, producing globally consistent point features. A scale-aware prompt decoder is then proposed to enable real-time adjustment of segmentation granularity via continuous scale signals. Simultaneously, we introduce a large-scale, high-quality part-level point cloud dataset with more than 100k samples, providing ample supervision signals for model training. Extensive experiments demonstrate that S2AM3D achieves leading performance across multiple evaluation settings, exhibiting exceptional robustness and controllability when handling complex structures and parts with significant size variations.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00995v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "contrastive learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "SPIDER: Scalable Physics-Informed Dexterous Retargeting",
      "authors": [
        "Chaoyi Pan",
        "Changhao Wang",
        "Haozhi Qi",
        "Zixi Liu",
        "Homanga Bharadhwaj",
        "Akash Sharma",
        "Tingfan Wu",
        "Guanya Shi",
        "Jitendra Malik",
        "Francois Hogan"
      ],
      "arxiv_id": "2511.09484v1",
      "summary": "Learning dexterous and agile policy for humanoid and dexterous hand control requires large-scale demonstrations, but collecting robot-specific data is prohibitively expensive. In contrast, abundant human motion data is readily available from motion capture, videos, and virtual reality, which could help address the data scarcity problem. However, due to the embodiment gap and missing dynamic information like force and torque, these demonstrations cannot be directly executed on robots. To bridge this gap, we propose Scalable Physics-Informed DExterous Retargeting (SPIDER), a physics-based retargeting framework to transform and augment kinematic-only human demonstrations to dynamically feasible robot trajectories at scale. Our key insight is that human demonstrations should provide global task structure and objective, while large-scale physics-based sampling with curriculum-style virtual contact guidance should refine trajectories to ensure dynamical feasibility and correct contact sequences. SPIDER scales across diverse 9 humanoid/dexterous hand embodiments and 6 datasets, improving success rates by 18% compared to standard sampling, while being 10X faster than reinforcement learning (RL) baselines, and enabling the generation of a 2.4M frames dynamic-feasible robot dataset for policy learning. As a universal physics-based retargeting method, SPIDER can work with diverse quality data and generate diverse and high-quality data to enable efficient policy learning with methods like RL.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "Project website: https://jc-bao.github.io/spider-project/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09484v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid",
            "dexterous hand"
          ],
          "score": 4.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "policy learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 7.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning",
      "authors": [
        "Tharindu Fernando",
        "Clinton Fookes",
        "Sridha Sridharan"
      ],
      "arxiv_id": "2511.04949v1",
      "summary": "Rapid advances in generative AI have led to increasingly realistic deepfakes, posing growing challenges for law enforcement and public trust. Existing passive deepfake detectors struggle to keep pace, largely due to their dependence on specific forgery artifacts, which limits their ability to generalize to new deepfake types. Proactive deepfake detection using watermarks has emerged to address the challenge of identifying high-quality synthetic media. However, these methods often struggle to balance robustness against benign distortions with sensitivity to malicious tampering. This paper introduces a novel deep learning framework that harnesses high-dimensional latent space representations and the Multi-Agent Adversarial Reinforcement Learning (MAARL) paradigm to develop a robust and adaptive watermarking approach. Specifically, we develop a learnable watermark embedder that operates in the latent space, capturing high-level image semantics, while offering precise control over message encoding and extraction. The MAARL paradigm empowers the learnable watermarking agent to pursue an optimal balance between robustness and fragility by interacting with a dynamic curriculum of benign and malicious image manipulations simulated by an adversarial attacker agent. Comprehensive evaluations on the CelebA and CelebA-HQ benchmarks reveal that our method consistently outperforms state-of-the-art approaches, achieving improvements of over 4.5% on CelebA and more than 5.3% on CelebA-HQ under challenging manipulation scenarios.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-07",
      "updated": "2025-11-07",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04949v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations",
      "authors": [
        "Shichao Fan",
        "Kun Wu",
        "Zhengping Che",
        "Xinhua Wang",
        "Di Wu",
        "Fei Liao",
        "Ning Liu",
        "Yixue Zhang",
        "Zhen Zhao",
        "Zhiyuan Xu",
        "Meng Li",
        "Qingjie Liu",
        "Shanghang Zhang",
        "Min Wan",
        "Jian Tang"
      ],
      "arxiv_id": "2511.02776v1",
      "summary": "Recent progress in large-scale robotic datasets and vision-language models (VLMs) has advanced research on vision-language-action (VLA) models. However, existing VLA models still face two fundamental challenges: (i) producing precise low-level actions from high-dimensional observations, (ii) bridging domain gaps across heterogeneous data sources, including diverse robot embodiments and human demonstrations. Existing methods often encode latent variables from either visual dynamics or robotic actions to guide policy learning, but they fail to fully exploit the complementary multi-modal knowledge present in large-scale, heterogeneous datasets. In this work, we present X Robotic Model 1 (XR-1), a novel framework for versatile and scalable VLA learning across diverse robots, tasks, and environments. XR-1 introduces the \\emph{Unified Vision-Motion Codes (UVMC)}, a discrete latent representation learned via a dual-branch VQ-VAE that jointly encodes visual dynamics and robotic motion. UVMC addresses these challenges by (i) serving as an intermediate representation between the observations and actions, and (ii) aligning multimodal dynamic information from heterogeneous data sources to capture complementary knowledge. To effectively exploit UVMC, we propose a three-stage training paradigm: (i) self-supervised UVMC learning, (ii) UVMC-guided pretraining on large-scale cross-embodiment robotic datasets, and (iii) task-specific post-training. We validate XR-1 through extensive real-world experiments with more than 14,000 rollouts on six different robot embodiments, spanning over 120 diverse manipulation tasks. XR-1 consistently outperforms state-of-the-art baselines such as $π_{0.5}$, $π_0$, RDT, UniVLA, and GR00T-N1.5 while demonstrating strong generalization to novel objects, background variations, distractors, and illumination changes. Our project is at https://xr-1-vla.github.io/.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-04",
      "updated": "2025-11-04",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02776v1",
      "code_links": [
        {
          "url": "https://xr-1-vla.github.io/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "cross-embodiment"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "7_retargeting"
      ]
    },
    {
      "title": "MPCM-Net: Multi-scale network integrates partial attention convolution with Mamba for ground-based cloud image segmentation",
      "authors": [
        "Penghui Niu",
        "Jiashuai She",
        "Taotao Cai",
        "Yajuan Zhang",
        "Ping Zhang",
        "Junhua Gu",
        "Jianxin Li"
      ],
      "arxiv_id": "2511.11681v1",
      "summary": "Ground-based cloud image segmentation is a critical research domain for photovoltaic power forecasting. Current deep learning approaches primarily focus on encoder-decoder architectural refinements. However, existing methodologies exhibit several limitations:(1)they rely on dilated convolutions for multi-scale context extraction, lacking the partial feature effectiveness and interoperability of inter-channel;(2)attention-based feature enhancement implementations neglect accuracy-throughput balance; and (3)the decoder modifications fail to establish global interdependencies among hierarchical local features, limiting inference efficiency. To address these challenges, we propose MPCM-Net, a Multi-scale network that integrates Partial attention Convolutions with Mamba architectures to enhance segmentation accuracy and computational efficiency. Specifically, the encoder incorporates MPAC, which comprises:(1)a MPC block with ParCM and ParSM that enables global spatial interaction across multi-scale cloud formations, and (2)a MPA block combining ParAM and ParSM to extract discriminative features with reduced computational complexity. On the decoder side, a M2B is employed to mitigate contextual loss through a SSHD that maintains linear complexity while enabling deep feature aggregation across spatial and scale dimensions. As a key contribution to the community, we also introduce and release a dataset CSRC, which is a clear-label, fine-grained segmentation benchmark designed to overcome the critical limitations of existing public datasets. Extensive experiments on CSRC demonstrate the superior performance of MPCM-Net over state-of-the-art methods, achieving an optimal balance between segmentation accuracy and inference speed. The dataset and source code will be available at https://github.com/she1110/CSRC.",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11681v1",
      "code_links": [
        {
          "url": "https://github.com/she1110/CSRC",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "MPC"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Diffusion Policies with Value-Conditional Optimization for Offline Reinforcement Learning",
      "authors": [
        "Yunchang Ma",
        "Tenglong Liu",
        "Yixing Lan",
        "Xin Yin",
        "Changxin Zhang",
        "Xinglong Zhang",
        "Xin Xu"
      ],
      "arxiv_id": "2511.08922v1",
      "summary": "In offline reinforcement learning, value overestimation caused by out-of-distribution (OOD) actions significantly limits policy performance. Recently, diffusion models have been leveraged for their strong distribution-matching capabilities, enforcing conservatism through behavior policy constraints. However, existing methods often apply indiscriminate regularization to redundant actions in low-quality datasets, resulting in excessive conservatism and an imbalance between the expressiveness and efficiency of diffusion modeling. To address these issues, we propose DIffusion policies with Value-conditional Optimization (DIVO), a novel approach that leverages diffusion models to generate high-quality, broadly covered in-distribution state-action samples while facilitating efficient policy improvement. Specifically, DIVO introduces a binary-weighted mechanism that utilizes the advantage values of actions in the offline dataset to guide diffusion model training. This enables a more precise alignment with the dataset's distribution while selectively expanding the boundaries of high-advantage actions. During policy improvement, DIVO dynamically filters high-return-potential actions from the diffusion model, effectively guiding the learned policy toward better performance. This approach achieves a critical balance between conservatism and explorability in offline RL. We evaluate DIVO on the D4RL benchmark and compare it against state-of-the-art baselines. Empirical results demonstrate that DIVO achieves superior performance, delivering significant improvements in average returns across locomotion tasks and outperforming existing methods in the challenging AntMaze domain, where sparse rewards pose a major difficulty.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "IROS 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08922v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "locomotion"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Multistep Quasimetric Learning for Scalable Goal-conditioned Reinforcement Learning",
      "authors": [
        "Bill Chunyuan Zheng",
        "Vivek Myers",
        "Benjamin Eysenbach",
        "Sergey Levine"
      ],
      "arxiv_id": "2511.07730v2",
      "summary": "Learning how to reach goals in an environment is a longstanding challenge in AI, yet reasoning over long horizons remains a challenge for modern methods. The key question is how to estimate the temporal distance between pairs of observations. While temporal difference methods leverage local updates to provide optimality guarantees, they often perform worse than Monte Carlo methods that perform global updates (e.g., with multi-step returns), which lack such guarantees. We show how these approaches can be integrated into a practical GCRL method that fits a quasimetric distance using a multistep Monte-Carlo return. We show our method outperforms existing GCRL methods on long-horizon simulated tasks with up to 4000 steps, even with visual observations. We also demonstrate that our method can enable stitching in the real-world robotic manipulation domain (Bridge setup). Our approach is the first end-to-end GCRL method that enables multistep stitching in this real-world manipulation domain from an unlabeled offline dataset of visual observations.",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-11",
      "updated": "2025-11-14",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07730v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Improving Deepfake Detection with Reinforcement Learning-Based Adaptive Data Augmentation",
      "authors": [
        "Yuxuan Zhou",
        "Tao Yu",
        "Wen Huang",
        "Yuheng Zhang",
        "Tao Dai",
        "Shu-Tao Xia"
      ],
      "arxiv_id": "2511.07051v1",
      "summary": "The generalization capability of deepfake detectors is critical for real-world use. Data augmentation via synthetic fake face generation effectively enhances generalization, yet current SoTA methods rely on fixed strategies-raising a key question: Is a single static augmentation sufficient, or does the diversity of forgery features demand dynamic approaches? We argue existing methods overlook the evolving complexity of real-world forgeries (e.g., facial warping, expression manipulation), which fixed policies cannot fully simulate. To address this, we propose CRDA (Curriculum Reinforcement-Learning Data Augmentation), a novel framework guiding detectors to progressively master multi-domain forgery features from simple to complex. CRDA synthesizes augmented samples via a configurable pool of forgery operations and dynamically generates adversarial samples tailored to the detector's current learning state. Central to our approach is integrating reinforcement learning (RL) and causal inference. An RL agent dynamically selects augmentation actions based on detector performance to efficiently explore the vast augmentation space, adapting to increasingly challenging forgeries. Simultaneously, the agent introduces action space variations to generate heterogeneous forgery patterns, guided by causal inference to mitigate spurious correlations-suppressing task-irrelevant biases and focusing on causally invariant features. This integration ensures robust generalization by decoupling synthetic augmentation patterns from the model's learned representations. Extensive experiments show our method significantly improves detector generalizability, outperforming SOTA methods across multiple cross-domain datasets.",
      "categories": [
        "cs.CV",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07051v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Physically-Grounded Goal Imagination: Physics-Informed Variational Autoencoder for Self-Supervised Reinforcement Learning",
      "authors": [
        "Lan Thi Ha Nguyen",
        "Kien Ton Manh",
        "Anh Do Duc",
        "Nam Pham Hai"
      ],
      "arxiv_id": "2511.06745v1",
      "summary": "Self-supervised goal-conditioned reinforcement learning enables robots to autonomously acquire diverse skills without human supervision. However, a central challenge is the goal setting problem: robots must propose feasible and diverse goals that are achievable in their current environment. Existing methods like RIG (Visual Reinforcement Learning with Imagined Goals) use variational autoencoder (VAE) to generate goals in a learned latent space but have the limitation of producing physically implausible goals that hinder learning efficiency. We propose Physics-Informed RIG (PI-RIG), which integrates physical constraints directly into the VAE training process through a novel Enhanced Physics-Informed Variational Autoencoder (Enhanced p3-VAE), enabling the generation of physically consistent and achievable goals. Our key innovation is the explicit separation of the latent space into physics variables governing object dynamics and environmental factors capturing visual appearance, while enforcing physical consistency through differential equation constraints and conservation laws. This enables the generation of physically consistent and achievable goals that respect fundamental physical principles such as object permanence, collision constraints, and dynamic feasibility. Through extensive experiments, we demonstrate that this physics-informed goal generation significantly improves the quality of proposed goals, leading to more effective exploration and better skill acquisition in visual robotic manipulation tasks including reaching, pushing, and pick-and-place scenarios.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06745v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "MagBotSim: Physics-Based Simulation and Reinforcement Learning Environments for Magnetic Robotics",
      "authors": [
        "Lara Bergmann",
        "Cedric Grothues",
        "Klaus Neumann"
      ],
      "arxiv_id": "2511.16158v1",
      "summary": "Magnetic levitation is about to revolutionize in-machine material flow in industrial automation. Such systems are flexibly configurable and can include a large number of independently actuated shuttles (movers) that dynamically rebalance production capacity. Beyond their capabilities for dynamic transportation, these systems possess the inherent yet unexploited potential to perform manipulation. By merging the fields of transportation and manipulation into a coordinated swarm of magnetic robots (MagBots), we enable manufacturing systems to achieve significantly higher efficiency, adaptability, and compactness. To support the development of intelligent algorithms for magnetic levitation systems, we introduce MagBotSim (Magnetic Robotics Simulation): a physics-based simulation for magnetic levitation systems. By framing magnetic levitation systems as robot swarms and providing a dedicated simulation, this work lays the foundation for next generation manufacturing systems powered by Magnetic Robotics. MagBotSim's documentation, videos, experiments, and code are available at: https://ubi-coro.github.io/MagBotSim/",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16158v1",
      "code_links": [
        {
          "url": "https://ubi-coro.github.io/MagBotSim/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Bi-AQUA: Bilateral Control-Based Imitation Learning for Underwater Robot Arms via Lighting-Aware Action Chunking with Transformers",
      "authors": [
        "Takeru Tsunoori",
        "Masato Kobayashi",
        "Yuki Uranishi"
      ],
      "arxiv_id": "2511.16050v1",
      "summary": "Underwater robotic manipulation is fundamentally challenged by extreme lighting variations, color distortion, and reduced visibility. We introduce Bi-AQUA, the first underwater bilateral control-based imitation learning framework that integrates lighting-aware visual processing for underwater robot arms. Bi-AQUA employs a hierarchical three-level lighting adaptation mechanism: a Lighting Encoder that extracts lighting representations from RGB images without manual annotation and is implicitly supervised by the imitation objective, FiLM modulation of visual backbone features for adaptive, lighting-aware feature extraction, and an explicit lighting token added to the transformer encoder input for task-aware conditioning. Experiments on a real-world underwater pick-and-place task under diverse static and dynamic lighting conditions show that Bi-AQUA achieves robust performance and substantially outperforms a bilateral baseline without lighting modeling. Ablation studies further confirm that all three lighting-aware components are critical. This work bridges terrestrial bilateral control-based imitation learning and underwater manipulation, enabling force-sensitive autonomous operation in challenging marine environments. For additional material, please check: https://mertcookimg.github.io/bi-aqua",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16050v1",
      "code_links": [
        {
          "url": "https://mertcookimg.github.io/bi-aqua",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "LiSTAR: Ray-Centric World Models for 4D LiDAR Sequences in Autonomous Driving",
      "authors": [
        "Pei Liu",
        "Songtao Wang",
        "Lang Zhang",
        "Xingyue Peng",
        "Yuandong Lyu",
        "Jiaxin Deng",
        "Songxin Lu",
        "Weiliang Ma",
        "Xueyang Zhang",
        "Yifei Zhan",
        "XianPeng Lang",
        "Jun Ma"
      ],
      "arxiv_id": "2511.16049v1",
      "summary": "Synthesizing high-fidelity and controllable 4D LiDAR data is crucial for creating scalable simulation environments for autonomous driving. This task is inherently challenging due to the sensor's unique spherical geometry, the temporal sparsity of point clouds, and the complexity of dynamic scenes. To address these challenges, we present LiSTAR, a novel generative world model that operates directly on the sensor's native geometry. LiSTAR introduces a Hybrid-Cylindrical-Spherical (HCS) representation to preserve data fidelity by mitigating quantization artifacts common in Cartesian grids. To capture complex dynamics from sparse temporal data, it utilizes a Spatio-Temporal Attention with Ray-Centric Transformer (START) that explicitly models feature evolution along individual sensor rays for robust temporal coherence. Furthermore, for controllable synthesis, we propose a novel 4D point cloud-aligned voxel layout for conditioning and a corresponding discrete Masked Generative START (MaskSTART) framework, which learns a compact, tokenized representation of the scene, enabling efficient, high-resolution, and layout-guided compositional generation. Comprehensive experiments validate LiSTAR's state-of-the-art performance across 4D LiDAR reconstruction, prediction, and conditional generation, with substantial quantitative gains: reducing generation MMD by a massive 76%, improving reconstruction IoU by 32%, and lowering prediction L1 Med by 50%. This level of performance provides a powerful new foundation for creating realistic and controllable autonomous systems simulations. Project link: https://ocean-luna.github.io/LiSTAR.gitub.io.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16049v1",
      "code_links": [
        {
          "url": "https://ocean-luna.github.io/LiSTAR.gitub.io",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "MambaIO: Global-Coordinate Inertial Odometry for Pedestrians via Multi-Scale Frequency-Decoupled Modeling",
      "authors": [
        "Shanshan Zhang"
      ],
      "arxiv_id": "2511.15645v1",
      "summary": "Inertial Odometry (IO) enables real-time localization using only acceleration and angular velocity measurements from an Inertial Measurement Unit (IMU), making it a promising solution for localization in consumer-grade applications. Traditionally, IMU measurements in IO have been processed under two coordinate system paradigms: the body coordinate frame and the global coordinate frame, with the latter being widely adopted. However, recent studies in drone scenarios have demonstrated that the body frame can significantly improve localization accuracy, prompting a re-evaluation of the suitability of the global frame for pedestrian IO. To address this issue, this paper systematically evaluates the effectiveness of the global coordinate frame in pedestrian IO through theoretical analysis, qualitative inspection, and quantitative experiments. Building upon these findings, we further propose MambaIO, which decomposes IMU measurements into high-frequency and low-frequency components using a Laplacian pyramid. The low-frequency component is processed by a Mamba architecture to extract implicit contextual motion cues, while the high-frequency component is handled by a convolutional structure to capture fine-grained local motion details. Experiments on multiple public datasets show that MambaIO substantially reduces localization error and achieves state-of-the-art (SOTA) performance. To the best of our knowledge, this is the first application of the Mamba architecture to the inertial odometry task.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15645v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba"
          ],
          "score": 4.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "DoGCLR: Dominance-Game Contrastive Learning Network for Skeleton-Based Action Recognition",
      "authors": [
        "Yanshan Li",
        "Ke Ma",
        "Miaomiao Wei",
        "Linhui Dai"
      ],
      "arxiv_id": "2511.14179v2",
      "summary": "Existing self-supervised contrastive learning methods for skeleton-based action recognition often process all skeleton regions uniformly, and adopt a first-in-first-out (FIFO) queue to store negative samples, which leads to motion information loss and non-optimal negative sample selection. To address these challenges, this paper proposes Dominance-Game Contrastive Learning network for skeleton-based action Recognition (DoGCLR), a self-supervised framework based on game theory. DoGCLR models the construction of positive and negative samples as a dynamic Dominance Game, where both sample types interact to reach an equilibrium that balances semantic preservation and discriminative strength. Specifically, a spatio-temporal dual weight localization mechanism identifies key motion regions and guides region-wise augmentations to enhance motion diversity while maintaining semantics. In parallel, an entropy-driven dominance strategy manages the memory bank by retaining high entropy (hard) negatives and replacing low-entropy (weak) ones, ensuring consistent exposure to informative contrastive signals. Extensive experiments are conducted on NTU RGB+D and PKU-MMD datasets. On NTU RGB+D 60 X-Sub/X-View, DoGCLR achieves 81.1%/89.4% accuracy, and on NTU RGB+D 120 X-Sub/X-Set, DoGCLR achieves 71.2%/75.5% accuracy, surpassing state-of-the-art methods by 0.1%, 2.7%, 1.1%, and 2.3%, respectively. On PKU-MMD Part I/Part II, DoGCLR performs comparably to the state-of-the-art methods and achieves a 1.9% higher accuracy on Part II, highlighting its strong robustness on more challenging scenarios.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-19",
      "comment": "14 pages, 7 figures, journal",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14179v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]contrastive learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models",
      "authors": [
        "Yuhua Jiang",
        "Shuang Cheng",
        "Yan Ding",
        "Feifei Gao",
        "Biqing Qi"
      ],
      "arxiv_id": "2511.14148v1",
      "summary": "Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots. However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM). Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure. In this work, we propose asynchronous flow matching VLA (AsyncVLA), a novel framework that introduces temporal flexibility in asynchronous FM (AFM) and enables self-correction in action generation. AsyncVLA breaks from the vanilla SFM in VLA models by generating the action tokens in a non-uniform time schedule with action context awareness. Besides, our method introduces the confidence rater to extract confidence of the initially generated actions, enabling the model to selectively refine inaccurate action tokens before execution. Moreover, we propose a unified training procedure for SFM and AFM that endows a single model with both modes, improving KV-cache utilization. Extensive experiments on robotic manipulation benchmarks demonstrate that AsyncVLA is data-efficient and exhibits self-correction ability. AsyncVLA achieves state-of-the-art results across general embodied evaluations due to its asynchronous generation in AFM. Our code is available at https://github.com/YuhuaJiang2002/AsyncVLA.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14148v1",
      "code_links": [
        {
          "url": "https://github.com/YuhuaJiang2002/AsyncVLA",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]flow matching"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "DAP: A Discrete-token Autoregressive Planner for Autonomous Driving",
      "authors": [
        "Bowen Ye",
        "Bin Zhang",
        "Hang Zhao"
      ],
      "arxiv_id": "2511.13306v1",
      "summary": "Gaining sustainable performance improvement with scaling data and model budget remains a pivotal yet unresolved challenge in autonomous driving. While autoregressive models exhibited promising data-scaling efficiency in planning tasks, predicting ego trajectories alone suffers sparse supervision and weakly constrains how scene evolution should shape ego motion. Therefore, we introduce DAP, a discrete-token autoregressive planner that jointly forecasts BEV semantics and ego trajectories, thereby enforcing comprehensive representation learning and allowing predicted dynamics to directly condition ego motion. In addition, we incorporate a reinforcement-learning-based fine-tuning, which preserves supervised behavior cloning priors while injecting reward-guided improvements. Despite a compact 160M parameter budget, DAP achieves state-of-the-art performance on open-loop metrics and delivers competitive closed-loop results on the NAVSIM benchmark. Overall, the fully discrete-token autoregressive formulation operating on both rasterized BEV and ego actions provides a compact yet scalable planning paradigm for autonomous driving.",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13306v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "behavior cloning",
            "representation learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "ego-motion"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Towards High-Consistency Embodied World Model with Multi-View Trajectory Videos",
      "authors": [
        "Taiyi Su",
        "Jian Zhu",
        "Yaxuan Li",
        "Chong Ma",
        "Zitai Huang",
        "Hanli Wang",
        "Yi Xu"
      ],
      "arxiv_id": "2511.12882v2",
      "summary": "Embodied world models aim to predict and interact with the physical world through visual observations and actions. However, existing models struggle to accurately translate low-level actions (e.g., joint positions) into precise robotic movements in predicted frames, leading to inconsistencies with real-world physical interactions. To address these limitations, we propose MTV-World, an embodied world model that introduces Multi-view Trajectory-Video control for precise visuomotor prediction. Specifically, instead of directly using low-level actions for control, we employ trajectory videos obtained through camera intrinsic and extrinsic parameters and Cartesian-space transformation as control signals. However, projecting 3D raw actions onto 2D images inevitably causes a loss of spatial information, making a single view insufficient for accurate interaction modeling. To overcome this, we introduce a multi-view framework that compensates for spatial information loss and ensures high-consistency with physical world. MTV-World forecasts future frames based on multi-view trajectory videos as input and conditioning on an initial frame per view. Furthermore, to systematically evaluate both robotic motion precision and object interaction accuracy, we develop an auto-evaluation pipeline leveraging multimodal large models and referring video object segmentation models. To measure spatial consistency, we formulate it as an object location matching problem and adopt the Jaccard Index as the evaluation metric. Extensive experiments demonstrate that MTV-World achieves precise control execution and accurate physical interaction modeling in complex dual-arm scenarios.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-17",
      "updated": "2025-11-19",
      "comment": "15 pages, 23 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12882v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "dual-arm"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "SparseWorld-TC: Trajectory-Conditioned Sparse Occupancy World Model",
      "authors": [
        "Jiayuan Du",
        "Yiming Zhao",
        "Zhenglong Guo",
        "Yong Pan",
        "Wenbo Hou",
        "Zhihui Hao",
        "Kun Zhan",
        "Qijun Chen"
      ],
      "arxiv_id": "2511.22039v1",
      "summary": "This paper introduces a novel architecture for trajectory-conditioned forecasting of future 3D scene occupancy. In contrast to methods that rely on variational autoencoders (VAEs) to generate discrete occupancy tokens, which inherently limit representational capacity, our approach predicts multi-frame future occupancy in an end-to-end manner directly from raw image features. Inspired by the success of attention-based transformer architectures in foundational vision and language models such as GPT and VGGT, we employ a sparse occupancy representation that bypasses the intermediate bird's eye view (BEV) projection and its explicit geometric priors. This design allows the transformer to capture spatiotemporal dependencies more effectively. By avoiding both the finite-capacity constraint of discrete tokenization and the structural limitations of BEV representations, our method achieves state-of-the-art performance on the nuScenes benchmark for 1-3 second occupancy forecasting, outperforming existing approaches by a significant margin. Furthermore, it demonstrates robust scene dynamics understanding, consistently delivering high accuracy under arbitrary future trajectory conditioning.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22039v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "VGGT"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "BRIC: Bridging Kinematic Plans and Physical Control at Test Time",
      "authors": [
        "Dohun Lim",
        "Minji Kim",
        "Jaewoon Lim",
        "Sungchan Kim"
      ],
      "arxiv_id": "2511.20431v2",
      "summary": "We propose BRIC, a novel test-time adaptation (TTA) framework that enables long-term human motion generation by resolving execution discrepancies between diffusion-based kinematic motion planners and reinforcement learning-based physics controllers. While diffusion models can generate diverse and expressive motions conditioned on text and scene context, they often produce physically implausible outputs, leading to execution drift during simulation. To address this, BRIC dynamically adapts the physics controller to noisy motion plans at test time, while preserving pre-trained skills via a loss function that mitigates catastrophic forgetting. In addition, BRIC introduces a lightweight test-time guidance mechanism that steers the diffusion model in the signal space without updating its parameters. By combining both adaptation strategies, BRIC ensures consistent and physically plausible long-term executions across diverse environments in an effective and efficient manner. We validate the effectiveness of BRIC on a variety of long-term tasks, including motion composition, obstacle avoidance, and human-scene interaction, achieving state-of-the-art performance across all tasks.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-26",
      "comment": "Accepted to AAAI'26",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20431v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "motion generation"
          ],
          "score": 2.5
        },
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "human-scene interaction"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "2_algo_arch",
        "4_motion_diffusion",
        "5_interaction_reaction"
      ]
    },
    {
      "title": "ChessMamba: Structure-Aware Interleaving of State Spaces for Change Detection in Remote Sensing Images",
      "authors": [
        "Lei Ding",
        "Tong Liu",
        "Xuanguang Liu",
        "Xiangyun Liu",
        "Haitao Guo",
        "Jun Lu"
      ],
      "arxiv_id": "2511.19882v1",
      "summary": "Change detection (CD) in multitemporal remote sensing imagery presents significant challenges for fine-grained recognition, owing to heterogeneity and spatiotemporal misalignment. However, existing methodologies based on vision transformers or state-space models typically disrupt local structural consistency during temporal serialization, obscuring discriminative cues under misalignment and hindering reliable change localization. To address this, we introduce ChessMamba, a structure-aware framework leveraging interleaved state-space modeling for robust CD with multi-temporal inputs. ChessMamba integrates a SpatialMamba encoder with a lightweight cross-source interaction module, featuring two key innovations: (i) Chessboard interleaving with snake scanning order, which serializes multi-temporal features into a unified sequence within a single forward pass, thereby shortening interaction paths and enabling direct comparison for accurate change localization; and (ii) Structure-aware fusion via multi-dilated convolutions, selectively capturing center-and-corner neighborhood contexts within each mono-temporal. Comprehensive evaluations on three CD tasks, including binary CD, semantic CD and multimodal building damage assessment, demonstrate that ChessMamba effectively fuses heterogeneous features and achieves substantial accuracy improvements over state-of-the-art methods.The relevant code will be available at: github.com/DingLei14/ChessMamba.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19882v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba"
          ],
          "score": 4.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "UMCL: Unimodal-generated Multimodal Contrastive Learning for Cross-compression-rate Deepfake Detection",
      "authors": [
        "Ching-Yi Lai",
        "Chih-Yu Jian",
        "Pei-Cheng Chuang",
        "Chia-Ming Lee",
        "Chih-Chung Hsu",
        "Chiou-Ting Hsu",
        "Chia-Wen Lin"
      ],
      "arxiv_id": "2511.18983v1",
      "summary": "In deepfake detection, the varying degrees of compression employed by social media platforms pose significant challenges for model generalization and reliability. Although existing methods have progressed from single-modal to multimodal approaches, they face critical limitations: single-modal methods struggle with feature degradation under data compression in social media streaming, while multimodal approaches require expensive data collection and labeling and suffer from inconsistent modal quality or accessibility in real-world scenarios. To address these challenges, we propose a novel Unimodal-generated Multimodal Contrastive Learning (UMCL) framework for robust cross-compression-rate (CCR) deepfake detection. In the training stage, our approach transforms a single visual modality into three complementary features: compression-robust rPPG signals, temporal landmark dynamics, and semantic embeddings from pre-trained vision-language models. These features are explicitly aligned through an affinity-driven semantic alignment (ASA) strategy, which models inter-modal relationships through affinity matrices and optimizes their consistency through contrastive learning. Subsequently, our cross-quality similarity learning (CQSL) strategy enhances feature robustness across compression rates. Extensive experiments demonstrate that our method achieves superior performance across various compression rates and manipulation types, establishing a new benchmark for robust deepfake detection. Notably, our approach maintains high detection accuracy even when individual features degrade, while providing interpretable insights into feature relationships through explicit alignment.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "24-page manuscript accepted to IJCV",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18983v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]contrastive learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "GuideFlow: Constraint-Guided Flow Matching for Planning in End-to-End Autonomous Driving",
      "authors": [
        "Lin Liu",
        "Caiyan Jia",
        "Guanyi Yu",
        "Ziying Song",
        "JunQiao Li",
        "Feiyang Jia",
        "Peiliang Wu",
        "Xiaoshuai Hao",
        "Yandan Luo"
      ],
      "arxiv_id": "2511.18729v2",
      "summary": "Driving planning is a critical component of end-to-end (E2E) autonomous driving. However, prevailing Imitative E2E Planners often suffer from multimodal trajectory mode collapse, failing to produce diverse trajectory proposals. Meanwhile, Generative E2E Planners struggle to incorporate crucial safety and physical constraints directly into the generative process, necessitating an additional optimization stage to refine their outputs. In this paper, we propose \\textit{\\textbf{GuideFlow}}, a novel planning framework that leverages Constrained Flow Matching. Concretely, \\textit{\\textbf{GuideFlow}} explicitly models the flow matching process, which inherently mitigates mode collapse and allows for flexible guidance from various conditioning signals. Our core contribution lies in directly enforcing explicit constraints within the flow matching generation process, rather than relying on implicit constraint encoding. Crucially, \\textit{\\textbf{GuideFlow}} unifies the training of the flow matching with the Energy-Based Model (EBM) to enhance the model's autonomous optimization capability to robustly satisfy physical constraints. Secondly, \\textit{\\textbf{GuideFlow}} parameterizes driving aggressiveness as a control signal during generation, enabling precise manipulation of trajectory style. Extensive evaluations on major driving benchmarks (Bench2Drive, NuScenes, NavSim and ADV-NuScenes) validate the effectiveness of \\textit{\\textbf{GuideFlow}}. Notably, on the NavSim test hard split (Navhard), \\textit{\\textbf{GuideFlow}} achieved SOTA with an EPDMS score of 43.0. The code will be in https://github.com/liulin815/GuideFlow.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-12-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18729v2",
      "code_links": [
        {
          "url": "https://github.com/liulin815/GuideFlow",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]flow matching"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "CrossJEPA: Cross-Modal Joint-Embedding Predictive Architecture for Efficient 3D Representation Learning from 2D Images",
      "authors": [
        "Avishka Perera",
        "Kumal Hewagamage",
        "Saeedha Nazar",
        "Kavishka Abeywardana",
        "Hasitha Gallella",
        "Ranga Rodrigo",
        "Mohamed Afham"
      ],
      "arxiv_id": "2511.18424v1",
      "summary": "Image-to-point cross-modal learning has emerged to address the scarcity of large-scale 3D datasets in 3D representation learning. However, current methods that leverage 2D data often result in large, slow-to-train models, making them computationally expensive and difficult to deploy in resource-constrained environments. The architecture design of such models is therefore critical, determining their performance, memory footprint, and compute efficiency. The Joint-embedding Predictive Architecture (JEPA) has gained wide popularity in self-supervised learning for its simplicity and efficiency, but has been under-explored in cross-modal settings, partly due to the misconception that masking is intrinsic to JEPA. In this light, we propose CrossJEPA, a simple Cross-modal Joint Embedding Predictive Architecture that harnesses the knowledge of an image foundation model and trains a predictor to infer embeddings of specific rendered 2D views from corresponding 3D point clouds, thereby introducing a JEPA-style pretraining strategy beyond masking. By conditioning the predictor on cross-domain projection information, CrossJEPA purifies the supervision signal from semantics exclusive to the target domain. We further exploit the frozen teacher design with a one-time target embedding caching mechanism, yielding amortized efficiency. CrossJEPA achieves a new state-of-the-art in linear probing on the synthetic ModelNet40 (94.2%) and the real-world ScanObjectNN (88.3%) benchmarks, using only 14.1M pretraining parameters (8.5M in the point encoder), and about 6 pretraining hours on a standard single GPU. These results position CrossJEPA as a performant, memory-efficient, and fast-to-train framework for 3D representation learning via knowledge distillation. We analyze CrossJEPA intuitively, theoretically, and empirically, and extensively ablate our design choices. Code will be made available.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-23",
      "updated": "2025-11-23",
      "comment": "24 pages, 10 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18424v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]representation learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Real-World Reinforcement Learning of Active Perception Behaviors",
      "authors": [
        "Edward S. Hu",
        "Jie Wang",
        "Xingfang Yuan",
        "Fiona Luo",
        "Muyao Li",
        "Gaspard Lambrechts",
        "Oleh Rybkin",
        "Dinesh Jayaraman"
      ],
      "arxiv_id": "2512.01188v1",
      "summary": "A robot's instantaneous sensory observations do not always reveal task-relevant state information. Under such partial observability, optimal behavior typically involves explicitly acting to gain the missing information. Today's standard robot learning techniques struggle to produce such active perception behaviors. We propose a simple real-world robot learning recipe to efficiently train active perception policies. Our approach, asymmetric advantage weighted regression (AAWR), exploits access to \"privileged\" extra sensors at training time. The privileged sensors enable training high-quality privileged value functions that aid in estimating the advantage of the target policy. Bootstrapping from a small number of potentially suboptimal demonstrations and an easy-to-obtain coarse policy initialization, AAWR quickly acquires active perception behaviors and boosts task performance. In evaluations on 8 manipulation tasks on 3 robots spanning varying degrees of partial observability, AAWR synthesizes reliable active perception behaviors that outperform all prior approaches. When initialized with a \"generalist\" robot policy that struggles with active perception tasks, AAWR efficiently generates information-gathering behaviors that allow it to operate under severe partial observability for manipulation tasks. Website: https://penn-pal-lab.github.io/aawr/",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "comment": "NeurIPS 2025 camera ready",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01188v1",
      "code_links": [
        {
          "url": "https://penn-pal-lab.github.io/aawr/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Reinforcement Learning for Gliding Projectile Guidance and Control",
      "authors": [
        "Joel Cahn",
        "Antonin Thomas",
        "Philippe Pastor"
      ],
      "arxiv_id": "2512.01066v1",
      "summary": "This paper presents the development of a control law, which is intended to be implemented on an optical guided glider. This guiding law follows an innovative approach, the reinforcement learning. This control law is used to make navigation more flexible and autonomous in a dynamic environment. The final objective is to track a target detected with the camera and then guide the glider to this point with high precision. Already applied on quad-copter drones, we wish by this study to demonstrate the applicability of reinforcement learning for fixed-wing aircraft on all of its axis.",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "6 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01066v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Registration-Free Monitoring of Unstructured Point Cloud Data via Intrinsic Geometrical Properties",
      "authors": [
        "Mariafrancesca Patalano",
        "Giovanna Capizzi",
        "Kamran Paynabar"
      ],
      "arxiv_id": "2511.05623v1",
      "summary": "Modern sensing technologies have enabled the collection of unstructured point cloud data (PCD) of varying sizes, which are used to monitor the geometric accuracy of 3D objects. PCD are widely applied in advanced manufacturing processes, including additive, subtractive, and hybrid manufacturing. To ensure the consistency of analysis and avoid false alarms, preprocessing steps such as registration and mesh reconstruction are commonly applied prior to monitoring. However, these steps are error-prone, time-consuming and may introduce artifacts, potentially affecting monitoring outcomes. In this paper, we present a novel registration-free approach for monitoring PCD of complex shapes, eliminating the need for both registration and mesh reconstruction. Our proposal consists of two alternative feature learning methods and a common monitoring scheme. Feature learning methods leverage intrinsic geometric properties of the shape, captured via the Laplacian and geodesic distances. In the monitoring scheme, thresholding techniques are used to further select intrinsic features most indicative of potential out-of-control conditions. Numerical experiments and case studies highlight the effectiveness of the proposed approach in identifying different types of defects.",
      "categories": [
        "cs.CV",
        "cs.LG",
        "stat.ME",
        "stat.ML"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05623v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Self-Supervised Implicit Attention Priors for Point Cloud Reconstruction",
      "authors": [
        "Kyle Fogarty",
        "Chenyue Cai",
        "Jing Yang",
        "Zhilin Guo",
        "Cengiz Öztireli"
      ],
      "arxiv_id": "2511.04864v2",
      "summary": "Recovering high-quality surfaces from irregular point cloud is ill-posed unless strong geometric priors are available. We introduce an implicit self-prior approach that distills a shape-specific prior directly from the input point cloud itself and embeds it within an implicit neural representation. This is achieved by jointly training a small dictionary of learnable embeddings with an implicit distance field; at every query location, the field attends to the dictionary via cross-attention, enabling the network to capture and reuse repeating structures and long-range correlations inherent to the shape. Optimized solely with self-supervised point cloud reconstruction losses, our approach requires no external training data. To effectively integrate this learned prior while preserving input fidelity, the trained field is then sampled to extract densely distributed points and analytic normals via automatic differentiation. We integrate the resulting dense point cloud and corresponding normals into a robust implicit moving least squares (RIMLS) formulation. We show this hybrid strategy preserves fine geometric details in the input data, while leveraging the learned prior to regularize sparse regions. Experiments show that our method outperforms both classical and learning-based approaches in generating high-fidelity surfaces with superior detail preservation and robustness to common data degradations.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-06",
      "updated": "2025-11-12",
      "comment": "Accepted at 3DV 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04864v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Conformalized Non-uniform Sampling Strategies for Accelerated Sampling-based Motion Planning",
      "authors": [
        "Shubham Natraj",
        "Bruno Sinopoli",
        "Yiannis Kantaros"
      ],
      "arxiv_id": "2511.04835v1",
      "summary": "Sampling-based motion planners (SBMPs) are widely used to compute dynamically feasible robot paths. However, their reliance on uniform sampling often leads to poor efficiency and slow planning in complex environments. We introduce a novel non-uniform sampling strategy that integrates into existing SBMPs by biasing sampling toward `certified' regions. These regions are constructed by (i) generating an initial, possibly infeasible, path using any heuristic path predictor (e.g., A* or vision-language models) and (ii) applying conformal prediction to quantify the predictor's uncertainty. This process yields prediction sets around the initial-guess path that are guaranteed, with user-specified probability, to contain the optimal solution. To our knowledge, this is the first non-uniform sampling approach for SBMPs that provides such probabilistically correct guarantees on the sampling regions. Extensive evaluations demonstrate that our method consistently finds feasible paths faster and generalizes better to unseen environments than existing baselines.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04835v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]motion planning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "A Survey on Improving Human Robot Collaboration through Vision-and-Language Navigation",
      "authors": [
        "Nivedan Yakolli",
        "Avinash Gautam",
        "Abhijit Das",
        "Yuankai Qi",
        "Virendra Singh Shekhawat"
      ],
      "arxiv_id": "2512.00027v1",
      "summary": "Vision-and-Language Navigation (VLN) is a multi-modal, cooperative task requiring agents to interpret human instructions, navigate 3D environments, and communicate effectively under ambiguity. This paper presents a comprehensive review of recent VLN advancements in robotics and outlines promising directions to improve multi-robot coordination. Despite progress, current models struggle with bidirectional communication, ambiguity resolution, and collaborative decision-making in the multi-agent systems. We review approximately 200 relevant articles to provide an in-depth understanding of the current landscape. Through this survey, we aim to provide a thorough resource that inspires further research at the intersection of VLN and robotics. We advocate that the future VLN systems should support proactive clarification, real-time feedback, and contextual reasoning through advanced natural language understanding (NLU) techniques. Additionally, decentralized decision-making frameworks with dynamic role assignment are essential for scalable, efficient multi-robot collaboration. These innovations can significantly enhance human-robot interaction (HRI) and enable real-world deployment in domains such as healthcare, logistics, and disaster response.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00027v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Temporal Zoom Networks: Distance Regression and Continuous Depth for Efficient Action Localization",
      "authors": [
        "Ibne Farabi Shihab",
        "Sanjeda Akter",
        "Anuj Sharma"
      ],
      "arxiv_id": "2511.03943v3",
      "summary": "Temporal action localization requires both precise boundary detection and computational efficiency. Current methods apply uniform computation across all temporal positions, wasting resources on easy boundaries while struggling with ambiguous ones. We address this through two complementary innovations: Boundary Distance Regression (BDR), which replaces classification-based boundary detection with signed-distance regression achieving 3.3--16.7$\\times$ lower variance; and Adaptive Temporal Refinement (ATR), which allocates transformer depth continuously ($τ\\in[0,1]$) to concentrate computation near difficult boundaries. On THUMOS14, our method achieves 56.5\\% mAP@0.7 and 58.2\\% average mAP@[0.3:0.7] with 151G FLOPs, using 36\\% fewer FLOPs than ActionFormer++ (55.7\\% mAP@0.7 at 235G). Compared to uniform baselines, we achieve +2.9\\% mAP@0.7 (+1.8\\% avg mAP, 5.4\\% relative) with 24\\% fewer FLOPs and 29\\% lower latency, with particularly strong gains on short actions (+4.2\\%, 8.6\\% relative). Training requires 1.29$\\times$ baseline FLOPs, but this one-time cost is amortized over many inference runs; knowledge distillation further reduces this to 1.1$\\times$ while retaining 99.5\\% accuracy. Our contributions include: (i) a theoretically-grounded distance formulation with information-theoretic analysis showing optimal variance scaling; (ii) a continuous depth allocation mechanism avoiding discrete routing complexity; and (iii) consistent improvements across four datasets with gains correlating with boundary heterogeneity.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-06",
      "updated": "2025-11-13",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03943v3",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Motion Planning Under Temporal Logic Specifications In Semantically Unknown Environments",
      "authors": [
        "Azizollah Taheri",
        "Derya Aksaray"
      ],
      "arxiv_id": "2511.03652v1",
      "summary": "This paper addresses a motion planning problem to achieve spatio-temporal-logical tasks, expressed by syntactically co-safe linear temporal logic specifications (scLTL\\next), in uncertain environments. Here, the uncertainty is modeled as some probabilistic knowledge on the semantic labels of the environment. For example, the task is \"first go to region 1, then go to region 2\"; however, the exact locations of regions 1 and 2 are not known a priori, instead a probabilistic belief is available. We propose a novel automata-theoretic approach, where a special product automaton is constructed to capture the uncertainty related to semantic labels, and a reward function is designed for each edge of this product automaton. The proposed algorithm utilizes value iteration for online replanning. We show some theoretical results and present some simulations/experiments to demonstrate the efficacy of the proposed approach.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-05",
      "updated": "2025-11-05",
      "comment": "8 pages, 6 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03652v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]motion planning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding",
      "authors": [
        "Mauro Orazio Drago",
        "Luca Carlini",
        "Pelinsu Celebi Balyemez",
        "Dennis Pierantozzi",
        "Chiara Lena",
        "Cesare Hassan",
        "Danail Stoyanov",
        "Elena De Momi",
        "Sophia Bano",
        "Mobarak I. Hoque"
      ],
      "arxiv_id": "2511.03325v2",
      "summary": "Video Question Answering (VideoQA) in the surgical domain aims to enhance intraoperative understanding by enabling AI models to reason over temporally coherent events rather than isolated frames. Current approaches are limited to static image features, and available datasets often lack temporal annotations, ignoring the dynamics critical for accurate procedural interpretation. We propose SurgViVQA, a surgical VideoQA model that extends visual reasoning from static images to dynamic surgical scenes. It uses a Masked Video--Text Encoder to fuse video and question features, capturing temporal cues such as motion and tool--tissue interactions, which a fine-tuned large language model (LLM) then decodes into coherent answers. To evaluate its performance, we curated REAL-Colon-VQA, a colonoscopic video dataset that includes motion-related questions and diagnostic attributes, as well as out-of-template questions with rephrased or semantically altered formulations to assess model robustness. Experimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset shows that SurgViVQA outperforms existing image-based VQA benchmark models, particularly in keyword accuracy, improving over PitVQA by +11\\% on REAL-Colon-VQA and +9\\% on EndoVis18-VQA. A perturbation study on the questions further confirms improved generalizability and robustness to variations in question phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework for temporally-aware understanding in surgical VideoQA, enabling AI models to interpret dynamic procedural contexts more effectively. Code and dataset available at https://github.com/madratak/SurgViVQA.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-05",
      "updated": "2025-11-06",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03325v2",
      "code_links": [
        {
          "url": "https://github.com/madratak/SurgViVQA",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]scene understanding"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "IEC3D-AD: A 3D Dataset of Industrial Equipment Components for Unsupervised Point Cloud Anomaly Detection",
      "authors": [
        "Bingyang Guo",
        "Hongjie Li",
        "Ruiyun Yu",
        "Hanzhe Liang",
        "Jinbao Wang"
      ],
      "arxiv_id": "2511.03267v1",
      "summary": "3D anomaly detection (3D-AD) plays a critical role in industrial manufacturing, particularly in ensuring the reliability and safety of core equipment components. Although existing 3D datasets like Real3D-AD and MVTec 3D-AD offer broad application support, they fall short in capturing the complexities and subtle defects found in real industrial environments. This limitation hampers precise anomaly detection research, especially for industrial equipment components (IEC) such as bearings, rings, and bolts. To address this challenge, we have developed a point cloud anomaly detection dataset (IEC3D-AD) specific to real industrial scenarios. This dataset is directly collected from actual production lines, ensuring high fidelity and relevance. Compared to existing datasets, IEC3D-AD features significantly improved point cloud resolution and defect annotation granularity, facilitating more demanding anomaly detection tasks. Furthermore, inspired by generative 2D-AD methods, we introduce a novel 3D-AD paradigm (GMANet) on IEC3D-AD. This paradigm generates synthetic point cloud samples based on geometric morphological analysis, then reduces the margin and increases the overlap between normal and abnormal point-level features through spatial discrepancy optimization. Extensive experiments demonstrate the effectiveness of our method on both IEC3D-AD and other datasets.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-05",
      "updated": "2025-11-05",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03267v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Collaborative Assembly Policy Learning of a Sightless Robot",
      "authors": [
        "Zeqing Zhang",
        "Weifeng Lu",
        "Lei Yang",
        "Wei Jing",
        "Bowei Tang",
        "Jia Pan"
      ],
      "arxiv_id": "2511.03189v1",
      "summary": "This paper explores a physical human-robot collaboration (pHRC) task involving the joint insertion of a board into a frame by a sightless robot and a human operator. While admittance control is commonly used in pHRC tasks, it can be challenging to measure the force/torque applied by the human for accurate human intent estimation, limiting the robot's ability to assist in the collaborative task. Other methods that attempt to solve pHRC tasks using reinforcement learning (RL) are also unsuitable for the board-insertion task due to its safety constraints and sparse rewards. Therefore, we propose a novel RL approach that utilizes a human-designed admittance controller to facilitate more active robot behavior and reduce human effort. Through simulation and real-world experiments, we demonstrate that our approach outperforms admittance control in terms of success rate and task completion time. Additionally, we observed a significant reduction in measured force/torque when using our proposed approach compared to admittance control. The video of the experiments is available at https://youtu.be/va07Gw6YIog.",
      "categories": [
        "cs.RO",
        "cs.HC",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-05",
      "updated": "2025-11-05",
      "comment": "Accepted by IEEE ROBIO 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03189v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "[T]policy learning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Subsampled Randomized Fourier GaLore for Adapting Foundation Models in Depth-Driven Liver Landmark Segmentation",
      "authors": [
        "Yun-Chen Lin",
        "Jiayuan Huang",
        "Hanyuan Zhang",
        "Sergi Kavtaradze",
        "Matthew J. Clarkson",
        "Mobarak I. Hoque"
      ],
      "arxiv_id": "2511.03163v1",
      "summary": "Accurate detection and delineation of anatomical structures in medical imaging are critical for computer-assisted interventions, particularly in laparoscopic liver surgery where 2D video streams limit depth perception and complicate landmark localization. While recent works have leveraged monocular depth cues for enhanced landmark detection, challenges remain in fusing RGB and depth features and in efficiently adapting large-scale vision models to surgical domains. We propose a depth-guided liver landmark segmentation framework integrating semantic and geometric cues via vision foundation encoders. We employ Segment Anything Model V2 (SAM2) encoder to extract RGB features and Depth Anything V2 (DA2) encoder to extract depth-aware features. To efficiently adapt SAM2, we introduce SRFT-GaLore, a novel low-rank gradient projection method that replaces the computationally expensive SVD with a Subsampled Randomized Fourier Transform (SRFT). This enables efficient fine-tuning of high-dimensional attention layers without sacrificing representational power. A cross-attention fusion module further integrates RGB and depth cues. To assess cross-dataset generalization, we also construct a new Laparoscopic Liver Surgical Dataset (LLSD) as an external validation benchmark. On the public L3D dataset, our method achieves a 4.85% improvement in Dice Similarity Coefficient and a 11.78-point reduction in Average Symmetric Surface Distance compared to the D2GPLand. To further assess generalization capability, we evaluate our model on LLSD dataset. Our model maintains competitive performance and significantly outperforms SAM-based baselines, demonstrating strong cross-dataset robustness and adaptability to unseen surgical environments. These results demonstrate that our SRFT-GaLore-enhanced dual-encoder framework enables scalable and precise segmentation under real-time, depth-constrained surgical settings.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-05",
      "updated": "2025-11-05",
      "comment": "12 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03163v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "monocular depth",
            "localization",
            "Depth Anything"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "From Propagation to Prediction: Point-level Uncertainty Evaluation of MLS Point Clouds under Limited Ground Truth",
      "authors": [
        "Ziyang Xu",
        "Olaf Wysocki",
        "Christoph Holst"
      ],
      "arxiv_id": "2511.03053v1",
      "summary": "Evaluating uncertainty is critical for reliable use of Mobile Laser Scanning (MLS) point clouds in many high-precision applications such as Scan-to-BIM, deformation analysis, and 3D modeling. However, obtaining the ground truth (GT) for evaluation is often costly and infeasible in many real-world applications. To reduce this long-standing reliance on GT in uncertainty evaluation research, this study presents a learning-based framework for MLS point clouds that integrates optimal neighborhood estimation with geometric feature extraction. Experiments on a real-world dataset show that the proposed framework is feasible and the XGBoost model delivers fully comparable accuracy to Random Forest while achieving substantially higher efficiency (about 3 times faster), providing initial evidence that geometric features can be used to predict point-level uncertainty quantified by the C2C distance. In summary, this study shows that MLS point clouds' uncertainty is learnable, offering a novel learning-based viewpoint towards uncertainty evaluation research.",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-04",
      "updated": "2025-11-04",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03053v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "EvtSlowTV -- A Large and Diverse Dataset for Event-Based Depth Estimation",
      "authors": [
        "Sadiq Layi Macaulay",
        "Nimet Kaygusuz",
        "Simon Hadfield"
      ],
      "arxiv_id": "2511.02953v1",
      "summary": "Event cameras, with their high dynamic range (HDR) and low latency, offer a promising alternative for robust depth estimation in challenging environments. However, many event-based depth estimation approaches are constrained by small-scale annotated datasets, limiting their generalizability to real-world scenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event camera dataset curated from publicly available YouTube footage, which contains more than 13B events across various environmental conditions and motions, including seasonal hiking, flying, scenic driving, and underwater exploration. EvtSlowTV is an order of magnitude larger than existing event datasets, providing an unconstrained, naturalistic setting for event-based depth learning. This work shows the suitability of EvtSlowTV for a self-supervised learning framework to capitalise on the HDR potential of raw event streams. We further demonstrate that training with EvtSlowTV enhances the model's ability to generalise to complex scenes and motions. Our approach removes the need for frame-based annotations and preserves the asynchronous nature of event data.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-04",
      "updated": "2025-11-04",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02953v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]depth estimation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Non-Contact Manipulation of Induced Magnetic Dipoles",
      "authors": [
        "Seth Stewart",
        "Joseph Pawelski",
        "Steve Ward",
        "Andrew J. Petruska"
      ],
      "arxiv_id": "2511.02761v1",
      "summary": "Extending the field of magnetic manipulation to conductive, non-magnetic objects opens the door for a wide array of applications previously limited to hard or soft magnetic materials. Of particular interest is the recycling of space debris through the use of oscillating magnetic fields, which represent a cache of raw materials in an environment particularly suited to the low forces generated from inductive magnetic manipulation. Building upon previous work that demonstrated 3D open-loop position control by leveraging the opposing dipole moment created from induced eddy currents, this work demonstrates closed-loop position control of a semi-buoyant aluminum sphere in lab tests, and the efficacy of varying methods for force inversion is explored. The closed-loop methods represent a critical first step towards wider applications for 3-DOF position control of induced magnetic dipoles.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-04",
      "updated": "2025-11-04",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02761v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Object Detection as an Optional Basis: A Graph Matching Network for Cross-View UAV Localization",
      "authors": [
        "Tao Liu",
        "Kan Ren",
        "Qian Chen"
      ],
      "arxiv_id": "2511.02489v1",
      "summary": "With the rapid growth of the low-altitude economy, UAVs have become crucial for measurement and tracking in patrol systems. However, in GNSS-denied areas, satellite-based localization methods are prone to failure. This paper presents a cross-view UAV localization framework that performs map matching via object detection, aimed at effectively addressing cross-temporal, cross-view, heterogeneous aerial image matching. In typical pipelines, UAV visual localization is formulated as an image-retrieval problem: features are extracted to build a localization map, and the pose of a query image is estimated by matching it to a reference database with known poses. Because publicly available UAV localization datasets are limited, many approaches recast localization as a classification task and rely on scene labels in these datasets to ensure accuracy. Other methods seek to reduce cross-domain differences using polar-coordinate reprojection, perspective transformations, or generative adversarial networks; however, they can suffer from misalignment, content loss, and limited realism. In contrast, we leverage modern object detection to accurately extract salient instances from UAV and satellite images, and integrate a graph neural network to reason about inter-image and intra-image node relationships. Using a fine-grained, graph-based node-similarity metric, our method achieves strong retrieval and localization performance. Extensive experiments on public and real-world datasets show that our approach handles heterogeneous appearance differences effectively and generalizes well, making it applicable to scenarios with larger modality gaps, such as infrared-visible image matching. Our dataset will be publicly available at the following URL: https://github.com/liutao23/ODGNNLoc.git.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-04",
      "updated": "2025-11-04",
      "comment": "20 pages, Submitted to IEEE TIM",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02489v1",
      "code_links": [
        {
          "url": "https://github.com/liutao23/ODGNNLoc.git",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "A Novel Grouping-Based Hybrid Color Correction Algorithm for Color Point Clouds",
      "authors": [
        "Kuo-Liang Chung",
        "Ting-Chung Tang"
      ],
      "arxiv_id": "2511.02397v1",
      "summary": "Color consistency correction for color point clouds is a fundamental yet important task in 3D rendering and compression applications. In the past, most previous color correction methods aimed at correcting color for color images. The purpose of this paper is to propose a grouping-based hybrid color correction algorithm for color point clouds. Our algorithm begins by estimating the overlapping rate between the aligned source and target point clouds, and then adaptively partitions the target points into two groups, namely the close proximity group Gcl and the moderate proximity group Gmod, or three groups, namely Gcl, Gmod, and the distant proximity group Gdist, when the estimated overlapping rate is low or high, respectively. To correct color for target points in Gcl, a K-nearest neighbors based bilateral interpolation (KBI) method is proposed. To correct color for target points in Gmod, a joint KBI and the histogram equalization (JKHE) method is proposed. For target points in Gdist, a histogram equalization (HE) method is proposed for color correction. Finally, we discuss the grouping-effect free property and the ablation study in our algorithm. The desired color consistency correction benefit of our algorithm has been justified through 1086 testing color point cloud pairs against the state-of-the-art methods. The C++ source code of our algorithm can be accessed from the website: https://github.com/ivpml84079/Point-cloud-color-correction.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-04",
      "updated": "2025-11-04",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02397v1",
      "code_links": [
        {
          "url": "https://github.com/ivpml84079/Point-cloud-color-correction",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Cycle-Sync: Robust Global Camera Pose Estimation through Enhanced Cycle-Consistent Synchronization",
      "authors": [
        "Shaohan Li",
        "Yunpeng Shi",
        "Gilad Lerman"
      ],
      "arxiv_id": "2511.02329v1",
      "summary": "We introduce Cycle-Sync, a robust and global framework for estimating camera poses (both rotations and locations). Our core innovation is a location solver that adapts message-passing least squares (MPLS) -- originally developed for group synchronization -- to camera location estimation. We modify MPLS to emphasize cycle-consistent information, redefine cycle consistencies using estimated distances from previous iterations, and incorporate a Welsch-type robust loss. We establish the strongest known deterministic exact-recovery guarantee for camera location estimation, showing that cycle consistency alone -- without access to inter-camera distances -- suffices to achieve the lowest sample complexity currently known. To further enhance robustness, we introduce a plug-and-play outlier rejection module inspired by robust subspace recovery, and we fully integrate cycle consistency into MPLS for rotation synchronization. Our global approach avoids the need for bundle adjustment. Experiments on synthetic and real datasets show that Cycle-Sync consistently outperforms leading pose estimators, including full structure-from-motion pipelines with bundle adjustment.",
      "categories": [
        "cs.CV",
        "cs.RO",
        "math.NA",
        "stat.ME"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-04",
      "updated": "2025-11-04",
      "comment": "NeurIPS 2025 spotlight paper",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02329v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]pose estimation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "3D Point Cloud Object Detection on Edge Devices for Split Computing",
      "authors": [
        "Taisuke Noguchi",
        "Takuya Azumi"
      ],
      "arxiv_id": "2511.02293v1",
      "summary": "The field of autonomous driving technology is rapidly advancing, with deep learning being a key component. Particularly in the field of sensing, 3D point cloud data collected by LiDAR is utilized to run deep neural network models for 3D object detection. However, these state-of-the-art models are complex, leading to longer processing times and increased power consumption on edge devices. The objective of this study is to address these issues by leveraging Split Computing, a distributed machine learning inference method. Split Computing aims to lessen the computational burden on edge devices, thereby reducing processing time and power consumption. Furthermore, it minimizes the risk of data breaches by only transmitting intermediate data from the deep neural network model. Experimental results show that splitting after voxelization reduces the inference time by 70.8% and the edge device execution time by 90.0%. When splitting within the network, the inference time is reduced by up to 57.1%, and the edge device execution time is reduced by up to 69.5%.",
      "categories": [
        "cs.DC",
        "cs.CV"
      ],
      "primary_category": "cs.DC",
      "published": "2025-11-04",
      "updated": "2025-11-04",
      "comment": "6 pages. This version includes minor lstlisting configuration adjustments for successful compilation. No changes to content or layout. Originally published at ACM/IEEE RAGE 2024",
      "doi": "10.1109/RAGE62451.2024.00009",
      "journal_ref": "Proceedings of the 3rd Real-time And intelliGent Edge computing workshop (RAGE), 2024, pp. 1-6",
      "pdf_url": "https://arxiv.org/pdf/2511.02293v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Are Euler angles a useful rotation parameterisation for pose estimation with Normalizing Flows?",
      "authors": [
        "Giorgos Sfikas",
        "Konstantina Nikolaidou",
        "Foteini Papadopoulou",
        "George Retsinas",
        "Anastasios L. Kesidis"
      ],
      "arxiv_id": "2511.02277v1",
      "summary": "Object pose estimation is a task that is of central importance in 3D Computer Vision. Given a target image and a canonical pose, a single point estimate may very often be sufficient; however, a probabilistic pose output is related to a number of benefits when pose is not unambiguous due to sensor and projection constraints or inherent object symmetries. With this paper, we explore the usefulness of using the well-known Euler angles parameterisation as a basis for a Normalizing Flows model for pose estimation. Isomorphic to spatial rotation, 3D pose has been parameterized in a number of ways, either in or out of the context of parameter estimation. We explore the idea that Euler angles, despite their shortcomings, may lead to useful models in a number of aspects, compared to a model built on a more complex parameterisation.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-04",
      "updated": "2025-11-04",
      "comment": "BMVC 2025 workshop proceedings (Smart Cameras for Smarter Autonomous Vehicles & Robots)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02277v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]pose estimation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation",
      "authors": [
        "Youngjin Hong",
        "Houjian Yu",
        "Mingen Li",
        "Changhyun Choi"
      ],
      "arxiv_id": "2511.02239v1",
      "summary": "Learning generalizable policies for robotic manipulation increasingly relies on large-scale models that map language instructions to actions (L2A). However, this one-way paradigm often produces policies that execute tasks without deeper contextual understanding, limiting their ability to generalize or explain their behavior. We argue that the complementary skill of mapping actions back to language (A2L) is essential for developing more holistic grounding. An agent capable of both acting and explaining its actions can form richer internal representations and unlock new paradigms for self-supervised learning. We introduce LACY (Language-Action Cycle), a unified framework that learns such bidirectional mappings within a single vision-language model. LACY is jointly trained on three synergistic tasks: generating parameterized actions from language (L2A), explaining observed actions in language (A2L), and verifying semantic consistency between two language descriptions (L2C). This enables a self-improving cycle that autonomously generates and filters new training data through an active augmentation strategy targeting low-confidence cases, thereby improving the model without additional human labels. Experiments on pick-and-place tasks in both simulation and the real world show that LACY improves task success rates by 56.46% on average and yields more robust language-action grounding for robotic manipulation. Project page: https://vla2026.github.io/LACY/",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-04",
      "updated": "2025-11-04",
      "comment": "Preprint. Project page: https://vla2026.github.io/LACY/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02239v1",
      "code_links": [
        {
          "url": "https://vla2026.github.io/LACY/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Stein-based Optimization of Sampling Distributions in Model Predictive Path Integral Control",
      "authors": [
        "Jace Aldrich",
        "Odest Chadwicke Jenkins"
      ],
      "arxiv_id": "2511.02015v1",
      "summary": "This paper presents a novel method for Model Predictive Path Integral (MPPI) control that optimizes sample generation towards an optimal trajectory through Stein Variational Gradient Descent (SVGD). MPPI is traditionally reliant on randomly sampled trajectories, often by a Gaussian distribution. The result can lead to sample deprivation, under-representing the space of possible trajectories, and yield suboptimal results. Through introducing SVGD updates in between MPPI environment steps, we present Stein-Optimized Path-Integral Inference (SOPPI), an MPPI/SVGD algorithm that can dynamically update noise distributions at runtime to shape a more optimal representation without an excessive increase in computational requirements. We demonstrate the efficacy of our method systems ranging from a Cart-Pole to a two-dimensional bipedal walking task, indicating improved performance above standard MPPI across a range of hyper-parameters and demonstrate feasibility at lower particle counts. We discuss the applicability of this MPPI/SVGD method to higher degree-of-freedom systems, as well as its potential to new developments in state-of-the-art differentiable simulators.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "8 pages, 6 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02015v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "bipedal",
            "biped",
            "walking"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain",
      "authors": [
        "Kai Zhai",
        "Ziyan Huang",
        "Qiang Nie",
        "Xiang Li",
        "Bo Ouyang"
      ],
      "arxiv_id": "2511.01756v1",
      "summary": "2D-to-3D human pose lifting is a fundamental challenge for 3D human pose estimation in monocular video, where graph convolutional networks (GCNs) and attention mechanisms have proven to be inherently suitable for encoding the spatial-temporal correlations of skeletal joints. However, depth ambiguity and errors in 2D pose estimation lead to incoherence in the 3D trajectory. Previous studies have attempted to restrict jitters in the time domain, for instance, by constraining the differences between adjacent frames while neglecting the global spatial-temporal correlations of skeletal joint motion. To tackle this problem, we design HGFreNet, a novel GraphFormer architecture with hop-hybrid feature aggregation and 3D trajectory consistency in the frequency domain. Specifically, we propose a hop-hybrid graph attention (HGA) module and a Transformer encoder to model global joint spatial-temporal correlations. The HGA module groups all $k$-hop neighbors of a skeletal joint into a hybrid group to enlarge the receptive field and applies the attention mechanism to discover the latent correlations of these groups globally. We then exploit global temporal correlations by constraining trajectory consistency in the frequency domain. To provide 3D information for depth inference across frames and maintain coherence over time, a preliminary network is applied to estimate the 3D pose. Extensive experiments were conducted on two standard benchmark datasets: Human3.6M and MPI-INF-3DHP. The results demonstrate that the proposed HGFreNet outperforms state-of-the-art (SOTA) methods in terms of positional accuracy and temporal consistency.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01756v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]pose estimation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "MO-SeGMan: Rearrangement Planning Framework for Multi Objective Sequential and Guided Manipulation in Constrained Environments",
      "authors": [
        "Cankut Bora Tuncer",
        "Marc Toussaint",
        "Ozgur S. Oguz"
      ],
      "arxiv_id": "2511.01476v1",
      "summary": "In this work, we introduce MO-SeGMan, a Multi-Objective Sequential and Guided Manipulation planner for highly constrained rearrangement problems. MO-SeGMan generates object placement sequences that minimize both replanning per object and robot travel distance while preserving critical dependency structures with a lazy evaluation method. To address highly cluttered, non-monotone scenarios, we propose a Selective Guided Forward Search (SGFS) that efficiently relocates only critical obstacles and to feasible relocation points. Furthermore, we adopt a refinement method for adaptive subgoal selection to eliminate unnecessary pick-and-place actions, thereby improving overall solution quality. Extensive evaluations on nine benchmark rearrangement tasks demonstrate that MO-SeGMan generates feasible motion plans in all cases, consistently achieving faster solution times and superior solution quality compared to the baselines. These results highlight the robustness and scalability of the proposed framework for complex rearrangement planning problems.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "8 pages, 8 figures, website:https://sites.google.com/view/mo-segman/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01476v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "AERMANI-VLM: Structured Prompting and Reasoning for Aerial Manipulation with Vision Language Models",
      "authors": [
        "Sarthak Mishra",
        "Rishabh Dev Yadav",
        "Avirup Das",
        "Saksham Gupta",
        "Wei Pan",
        "Spandan Roy"
      ],
      "arxiv_id": "2511.01472v1",
      "summary": "The rapid progress of vision--language models (VLMs) has sparked growing interest in robotic control, where natural language can express the operation goals while visual feedback links perception to action. However, directly deploying VLM-driven policies on aerial manipulators remains unsafe and unreliable since the generated actions are often inconsistent, hallucination-prone, and dynamically infeasible for flight. In this work, we present AERMANI-VLM, the first framework to adapt pretrained VLMs for aerial manipulation by separating high-level reasoning from low-level control, without any task-specific fine-tuning. Our framework encodes natural language instructions, task context, and safety constraints into a structured prompt that guides the model to generate a step-by-step reasoning trace in natural language. This reasoning output is used to select from a predefined library of discrete, flight-safe skills, ensuring interpretable and temporally consistent execution. By decoupling symbolic reasoning from physical action, AERMANI-VLM mitigates hallucinated commands and prevents unsafe behavior, enabling robust task completion. We validate the framework in both simulation and hardware on diverse multi-step pick-and-place tasks, demonstrating strong generalization to previously unseen commands, objects, and environments.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01472v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Contact Map Transfer with Conditional Diffusion Model for Generalizable Dexterous Grasp Generation",
      "authors": [
        "Yiyao Ma",
        "Kai Chen",
        "Kexin Zheng",
        "Qi Dou"
      ],
      "arxiv_id": "2511.01276v1",
      "summary": "Dexterous grasp generation is a fundamental challenge in robotics, requiring both grasp stability and adaptability across diverse objects and tasks. Analytical methods ensure stable grasps but are inefficient and lack task adaptability, while generative approaches improve efficiency and task integration but generalize poorly to unseen objects and tasks due to data limitations. In this paper, we propose a transfer-based framework for dexterous grasp generation, leveraging a conditional diffusion model to transfer high-quality grasps from shape templates to novel objects within the same category. Specifically, we reformulate the grasp transfer problem as the generation of an object contact map, incorporating object shape similarity and task specifications into the diffusion process. To handle complex shape variations, we introduce a dual mapping mechanism, capturing intricate geometric relationship between shape templates and novel objects. Beyond the contact map, we derive two additional object-centric maps, the part map and direction map, to encode finer contact details for more stable grasps. We then develop a cascaded conditional diffusion model framework to jointly transfer these three maps, ensuring their intra-consistency. Finally, we introduce a robust grasp recovery mechanism, identifying reliable contact points and optimizing grasp configurations efficiently. Extensive experiments demonstrate the superiority of our proposed method. Our approach effectively balances grasp quality, generation efficiency, and generalization performance across various tasks. Project homepage: https://cmtdiffusion.github.io/",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01276v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]grasp"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "OmniVLA: Physically-Grounded Multimodal VLA with Unified Multi-Sensor Perception for Robotic Manipulation",
      "authors": [
        "Heyu Guo",
        "Shanmu Wang",
        "Ruichun Ma",
        "Shiqi Jiang",
        "Yasaman Ghasempour",
        "Omid Abari",
        "Baining Guo",
        "Lili Qiu"
      ],
      "arxiv_id": "2511.01210v2",
      "summary": "Vision-language-action (VLA) models have shown strong generalization for robotic action prediction through large-scale vision-language pretraining. However, most existing models rely solely on RGB cameras, limiting their perception and, consequently, manipulation capabilities. We present OmniVLA, an omni-modality VLA model that integrates novel sensing modalities for physically-grounded spatial intelligence beyond RGB perception. The core of our approach is the sensor-masked image, a unified representation that overlays spatially grounded and physically meaningful masks onto the RGB images, derived from sensors including an infrared camera, a mmWave radar, and a microphone array. This image-native unification keeps sensor input close to RGB statistics to facilitate training, provides a uniform interface across sensor hardware, and enables data-efficient learning with lightweight per-sensor projectors. Built on this, we present a multisensory vision-language-action model architecture and train the model based on an RGB-pretrained VLA backbone. We evaluate OmniVLA on challenging real-world tasks where sensor-modality perception guides the robotic manipulation. OmniVLA achieves an average task success rate of 84%, significantly outperforms both RGB-only and raw-sensor-input baseline models by 59% and 28% respectively, meanwhile showing higher learning efficiency and stronger generalization capability.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-03",
      "updated": "2025-11-06",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01210v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction",
      "authors": [
        "Narges Ghasemi",
        "Amir Ziashahabi",
        "Salman Avestimehr",
        "Cyrus Shahabi"
      ],
      "arxiv_id": "2511.01082v1",
      "summary": "Image geolocalization, the task of determining an image's geographic origin, poses significant challenges, largely due to visual similarities across disparate locations and the large search space. To address these issues, we propose a hierarchical sequence prediction approach inspired by how humans narrow down locations from broad regions to specific addresses. Analogously, our model predicts geographic tokens hierarchically, first identifying a general region and then sequentially refining predictions to increasingly precise locations. Rather than relying on explicit semantic partitions, our method uses S2 cells, a nested, multiresolution global grid, and sequentially predicts finer-level cells conditioned on visual inputs and previous predictions. This procedure mirrors autoregressive text generation in large language models. Much like in language modeling, final performance depends not only on training but also on inference-time strategy. We investigate multiple top-down traversal methods for autoregressive sampling, incorporating techniques from test-time compute scaling used in language models. Specifically, we integrate beam search and multi-sample inference while exploring various selection strategies to determine the final output. This enables the model to manage uncertainty by exploring multiple plausible paths through the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k datasets against two distinct sets of baselines: those that operate without a Multimodal Large Language Model (MLLM) and those that leverage one. In the MLLM-free setting, our model surpasses other comparable baselines on nearly all metrics, achieving state-of-the-art performance with accuracy gains of up to 13.9%. When augmented with an MLLM, our model outperforms all baselines, setting a new state-of-the-art across all metrics. The source code is available at https://github.com/NNargesNN/GeoToken.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-02",
      "updated": "2025-11-02",
      "comment": "Accepted to IEEE International Conference on Data Mining (ICDM) 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01082v1",
      "code_links": [
        {
          "url": "https://github.com/NNargesNN/GeoToken",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation",
      "authors": [
        "Xiangyu Shi",
        "Zerui Li",
        "Yanyuan Qiao",
        "Qi Wu"
      ],
      "arxiv_id": "2511.00933v1",
      "summary": "Recent advances in Vision-and-Language Navigation in Continuous Environments (VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve zero-shot navigation. However, existing methods often rely on panoramic observations and two-stage pipelines involving waypoint predictors, which introduce significant latency and limit real-world applicability. In this work, we propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that eliminates the need for panoramic views and waypoint predictors. Our approach uses only three frontal RGB-D images combined with natural language instructions, enabling MLLMs to directly predict actions. To enhance decision robustness, we introduce an Uncertainty-Aware Reasoning module that integrates (i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past Bidirectional Reasoning mechanism for globally coherent planning. Experiments on both simulated and real-robot environments demonstrate that our method significantly reduces per-step latency while achieving competitive or superior performance compared to panoramic-view baselines. These results demonstrate the practicality and effectiveness of Fast-SmartWay for real-world zero-shot embodied navigation.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-02",
      "updated": "2025-11-02",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00933v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks",
      "authors": [
        "Heng Zheng",
        "Yuling Shi",
        "Xiaodong Gu",
        "Haochen You",
        "Zijian Zhang",
        "Lubin Gan",
        "Hao Zhang",
        "Wenjun Huang",
        "Jin Huang"
      ],
      "arxiv_id": "2511.00908v1",
      "summary": "Visual geo-localization requires extensive geographic knowledge and sophisticated reasoning to determine image locations without GPS metadata. Traditional retrieval methods are constrained by database coverage and quality. Recent Large Vision-Language Models (LVLMs) enable direct location reasoning from image content, yet individual models struggle with diverse geographic regions and complex scenes. Existing multi-agent systems improve performance through model collaboration but treat all agent interactions uniformly. They lack mechanisms to handle conflicting predictions effectively. We propose \\textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph neural networks for visual geo-localization. Our approach models diverse debate relationships through typed edges, distinguishing supportive collaboration, competitive argumentation, and knowledge transfer. We introduce a dual-level debate mechanism combining node-level refinement and edge-level argumentation modeling. A cross-level topology refinement strategy enables co-evolution between graph structure and agent representations. Experiments on multiple benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art methods. Our framework transforms cognitive conflicts between agents into enhanced geo-localization accuracy through structured debate.",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-02",
      "updated": "2025-11-02",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00908v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "When Semantics Connect the Swarm: LLM-Driven Fuzzy Control for Cooperative Multi-Robot Underwater Coverage",
      "authors": [
        "Jingzehua Xu",
        "Weihang Zhang",
        "Yangyang Li",
        "Hongmiaoyi Zhang",
        "Guanwen Xie",
        "Jiwei Tang",
        "Shuai Zhang",
        "Yi Li"
      ],
      "arxiv_id": "2511.00783v2",
      "summary": "Underwater multi-robot cooperative coverage remains challenging due to partial observability, limited communication, environmental uncertainty, and the lack of access to global localization. To address these issues, this paper presents a semantics-guided fuzzy control framework that couples Large Language Models (LLMs) with interpretable control and lightweight coordination. Raw multimodal observations are compressed by the LLM into compact, human-interpretable semantic tokens that summarize obstacles, unexplored regions, and Objects Of Interest (OOIs) under uncertain perception. A fuzzy inference system with pre-defined membership functions then maps these tokens into smooth and stable steering and gait commands, enabling reliable navigation without relying on global positioning. Then, we further coordinate multiple robots by introducing semantic communication that shares intent and local context in linguistic form, enabling agreement on who explores where while avoiding redundant revisits. Extensive simulations in unknown reef-like environments show that, under limited sensing and communication, the proposed framework achieves robust OOI-oriented navigation and cooperative coverage with improved efficiency and adaptability, narrowing the gap between semantic cognition and distributed underwater control in GPS-denied, map-free conditions.",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-02",
      "updated": "2025-11-06",
      "comment": "This paper has been submitted to IEEE Transactions on Mobile Computing. Jingzehua Xu, Weihang Zhang, and Yangyang Li contributed equally to this work and are recognized as the co-first authors of the paper",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00783v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "gait"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization",
            "navigation"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Towards classification-based representation learning for place recognition on LiDAR scans",
      "authors": [
        "Maksim Konoplia",
        "Dmitrii Khizbullin"
      ],
      "arxiv_id": "2511.00738v2",
      "summary": "Place recognition is a crucial task in autonomous driving, allowing vehicles to determine their position using sensor data. While most existing methods rely on contrastive learning, we explore an alternative approach by framing place recognition as a multi-class classification problem. Our method assigns discrete location labels to LiDAR scans and trains an encoder-decoder model to classify each scan's position directly. We evaluate this approach on the NuScenes dataset and show that it achieves competitive performance compared to contrastive learning-based methods while offering advantages in training efficiency and stability.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-01",
      "updated": "2025-11-04",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00738v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]representation learning",
            "contrastive learning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized Guassian Splatting",
      "authors": [
        "Chun-Tin Wu",
        "Jun-Cheng Chen"
      ],
      "arxiv_id": "2511.00560v1",
      "summary": "Although 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel view synthesis, extending it to dynamic scenes still results in substantial memory overhead from replicating Gaussians across frames. To address this challenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines voxel-based representations with neural Gaussian splatting for efficient dynamic scene modeling. Instead of generating separate Gaussian sets per timestamp, our method employs a compact set of neural voxels with learned deformation fields to model temporal dynamics. The design greatly reduces memory consumption and accelerates training while preserving high image quality. We further introduce a novel view refinement stage that selectively improves challenging viewpoints through targeted optimization, maintaining global efficiency while enhancing rendering quality for difficult viewing angles. Experiments demonstrate that our method outperforms state-of-the-art approaches with significant memory reduction and faster training, enabling real-time rendering with superior visual fidelity.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-01",
      "updated": "2025-11-01",
      "comment": "10 pages, 7 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00560v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "gaussian splatting",
            "novel view synthesis"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations",
      "authors": [
        "Kiran Shahi",
        "Anup Bagale"
      ],
      "arxiv_id": "2511.00456v4",
      "summary": "Chest X-ray imaging is commonly used to diagnose pneumonia, but accurately localizing the pneumonia affected regions typically requires detailed pixel-level annotations, which are costly and time consuming to obtain. To address this limitation, this study proposes a weakly supervised deep learning framework for pneumonia classification and localization using Gradient-weighted Class Activation Mapping (Grad-CAM). Instead of relying on costly pixel-level annotations, the proposed method utilizes image-level labels to generate clinically meaningful heatmaps that highlight pneumonia affected regions. Furthermore, we evaluate seven pre-trained deep learning models including a Vision Transformer under identical training conditions, using focal loss and patient-wise splits to prevent data leakage. Experimental results suggest that all models achieved high accuracy (96-98%), with ResNet-18 and EfficientNet-B0 showing the best overall performance and MobileNet-V2 providing an efficient lightweight alternative. Grad-CAM heatmap visualizations in this study confirm that the proposed methods focus on clinically relevant lung regions, supporting the use of explainable AI for radiological diagnostics. Overall, this work highlights the potential of weakly supervised, explainable models that enhance transparency and clinical trust in AI-assisted pneumonia screening.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-01",
      "updated": "2025-12-16",
      "comment": "https://github.com/kiranshahi/pneumonia-analysis",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00456v4",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "WetExplorer: Automating Wetland Greenhouse-Gas Surveys with an Autonomous Mobile Robot",
      "authors": [
        "Jose Vasquez",
        "Xuping Zhang"
      ],
      "arxiv_id": "2511.10864v1",
      "summary": "Quantifying greenhouse-gases (GHG) in wetlands is critical for climate modeling and restoration assessment, yet manual sampling is labor-intensive, and time demanding. We present WetExplorer, an autonomous tracked robot that automates the full GHG-sampling workflow. The robot system integrates low-ground-pressure locomotion, centimeter-accurate lift placement, dual-RTK sensor fusion, obstacle avoidance planning, and deep-learning perception in a containerized ROS2 stack. Outdoor trials verified that the sensor-fusion stack maintains a mean localization error of 1.71 cm, the vision module estimates object pose with 7 mm translational and 3° rotational accuracy, while indoor trials demonstrated that the full motion-planning pipeline positions the sampling chamber within a global tolerance of 70 mm while avoiding obstacles, all without human intervention. By eliminating the manual bottleneck, WetExplorer enables high-frequency, multi-site GHG measurements and opens the door for dense, long-duration datasets in saturated wetland terrain.",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "comment": "To be published in 2025 IEEE International Conference on Robotics and Biomimetics",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10864v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "locomotion",
            "motion planning"
          ],
          "score": 4.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "GFT: Graph Feature Tuning for Efficient Point Cloud Analysis",
      "authors": [
        "Manish Dhakal",
        "Venkat R. Dasari",
        "Rajshekhar Sunderraman",
        "Yi Ding"
      ],
      "arxiv_id": "2511.10799v2",
      "summary": "Parameter-efficient fine-tuning (PEFT) significantly reduces computational and memory costs by updating only a small subset of the model's parameters, enabling faster adaptation to new tasks with minimal loss in performance. Previous studies have introduced PEFTs tailored for point cloud data, as general approaches are suboptimal. To further reduce the number of trainable parameters, we propose a point-cloud-specific PEFT, termed Graph Features Tuning (GFT), which learns a dynamic graph from initial tokenized inputs of the transformer using a lightweight graph convolution network and passes these graph features to deeper layers via skip connections and efficient cross-attention modules. Extensive experiments on object classification and segmentation tasks show that GFT operates in the same domain, rivalling existing methods, while reducing the trainable parameters. Code is available at https://github.com/manishdhakal/GFT.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-12-01",
      "comment": "Accepted to WACV 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10799v2",
      "code_links": [
        {
          "url": "https://github.com/manishdhakal/GFT",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation",
      "authors": [
        "Wei Li",
        "Renshan Zhang",
        "Rui Shao",
        "Zhijian Fang",
        "Kaiwen Zhou",
        "Zhuotao Tian",
        "Liqiang Nie"
      ],
      "arxiv_id": "2511.10518v1",
      "summary": "Vision-Language-Action (VLA) models have advanced in robotic manipulation, yet practical deployment remains hindered by two key limitations: 1) perceptual redundancy, where irrelevant visual inputs are processed inefficiently, and 2) superficial instruction-vision alignment, which hampers semantic grounding of actions. In this paper, we propose SemanticVLA, a novel VLA framework that performs Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation. Specifically: 1) To sparsify redundant perception while preserving semantic alignment, Semantic-guided Dual Visual Pruner (SD-Pruner) performs: Instruction-driven Pruner (ID-Pruner) extracts global action cues and local semantic anchors in SigLIP; Spatial-aggregation Pruner (SA-Pruner) compacts geometry-rich features into task-adaptive tokens in DINOv2. 2) To exploit sparsified features and integrate semantics with spatial geometry, Semantic-complementary Hierarchical Fuser (SH-Fuser) fuses dense patches and sparse tokens across SigLIP and DINOv2 for coherent representation. 3) To enhance the transformation from perception to action, Semantic-conditioned Action Coupler (SA-Coupler) replaces the conventional observation-to-DoF approach, yielding more efficient and interpretable behavior modeling for manipulation tasks. Extensive experiments on simulation and real-world tasks show that SemanticVLA sets a new SOTA in both performance and efficiency. SemanticVLA surpasses OpenVLA on LIBERO benchmark by 21.1% in success rate, while reducing training cost and inference latency by 3.0-fold and 2.7-fold.SemanticVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/SemanticVLA",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "Accepted to AAAI 2026 (Oral), Project Page: https://github.com/JiuTian-VL/SemanticVLA",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10518v1",
      "code_links": [
        {
          "url": "https://github.com/JiuTian-VL/SemanticVLA",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation",
      "authors": [
        "Xun Huang",
        "Shijia Zhao",
        "Yunxiang Wang",
        "Xin Lu",
        "Wanfa Zhang",
        "Rongsheng Qu",
        "Weixin Li",
        "Yunhong Wang",
        "Chenglu Wen"
      ],
      "arxiv_id": "2511.10376v2",
      "summary": "Embodied navigation is a fundamental capability for robotic agents operating. Real-world deployment requires open vocabulary generalization and low training overhead, motivating zero-shot methods rather than task-specific RL training. However, existing zero-shot methods that build explicit 3D scene graphs often compress rich visual observations into text-only relations, leading to high construction cost, irreversible loss of visual evidence, and constrained vocabularies. To address these limitations, we introduce the Multi-modal 3D Scene Graph (M3DSG), which preserves visual cues by replacing textual relation",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-14",
      "comment": "10 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10376v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Audio-VLA: Adding Contact Audio Perception to Vision-Language-Action Model for Robotic Manipulation",
      "authors": [
        "Xiangyi Wei",
        "Haotian Zhang",
        "Xinyi Cao",
        "Siyu Xie",
        "Weifeng Ge",
        "Yang Li",
        "Changbo Wang"
      ],
      "arxiv_id": "2511.09958v1",
      "summary": "The Vision-Language-Action models (VLA) have achieved significant advances in robotic manipulation recently. However, vision-only VLA models create fundamental limitations, particularly in perceiving interactive and manipulation dynamic processes. This paper proposes Audio-VLA, a multimodal manipulation policy that leverages contact audio to perceive contact events and dynamic process feedback. Audio-VLA overcomes the vision-only constraints of VLA models. Additionally, this paper introduces the Task Completion Rate (TCR) metric to systematically evaluate dynamic operational processes. Audio-VLA employs pre-trained DINOv2 and SigLIP as visual encoders, AudioCLIP as the audio encoder, and Llama2 as the large language model backbone. We apply LoRA fine-tuning to these pre-trained modules to achieve robust cross-modal understanding of both visual and acoustic inputs. A multimodal projection layer aligns features from different modalities into the same feature space. Moreover RLBench and LIBERO simulation environments are enhanced by adding collision-based audio generation to provide realistic sound feedback during object interactions. Since current robotic manipulation evaluations focus on final outcomes rather than providing systematic assessment of dynamic operational processes, the proposed TCR metric measures how well robots perceive dynamic processes during manipulation, creating a more comprehensive evaluation metric. Extensive experiments on LIBERO, RLBench, and two real-world tasks demonstrate Audio-VLA's superior performance over vision-only comparative methods, while the TCR metric effectively quantifies dynamic process perception capabilities.",
      "categories": [
        "cs.RO",
        "cs.SD"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09958v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "PuffyBot: An Untethered Shape Morphing Robot for Multi-environment Locomotion",
      "authors": [
        "Shashwat Singh",
        "Zilin Si",
        "Zeynep Temel"
      ],
      "arxiv_id": "2511.09885v1",
      "summary": "Amphibians adapt their morphologies and motions to accommodate movement in both terrestrial and aquatic environments. Inspired by these biological features, we present PuffyBot, an untethered shape morphing robot capable of changing its body morphology to navigate multiple environments. Our robot design leverages a scissor-lift mechanism driven by a linear actuator as its primary structure to achieve shape morphing. The transformation enables a volume change from 255.00 cm3 to 423.75 cm3, modulating the buoyant force to counteract a downward force of 3.237 N due to 330 g mass of the robot. A bell-crank linkage is integrated with the scissor-lift mechanism, which adjusts the servo-actuated limbs by 90 degrees, allowing a seamless transition between crawling and swimming modes. The robot is fully waterproof, using thermoplastic polyurethane (TPU) fabric to ensure functionality in aquatic environments. The robot can operate untethered for two hours with an onboard battery of 1000 mA h. Our experimental results demonstrate multi-environment locomotion, including crawling on the land, crawling on the underwater floor, swimming on the water surface, and bimodal buoyancy adjustment to submerge underwater or resurface. These findings show the potential of shape morphing to create versatile and energy efficient robotic platforms suitable for diverse environments.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "8 pages, 10 figures, IEEE RoboSoft 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09885v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]locomotion"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "RWKV-PCSSC: Exploring RWKV Model for Point Cloud Semantic Scene Completion",
      "authors": [
        "Wenzhe He",
        "Xiaojun Chen",
        "Wentang Chen",
        "Hongyu Wang",
        "Ying Liu",
        "Ruihui Li"
      ],
      "arxiv_id": "2511.09878v1",
      "summary": "Semantic Scene Completion (SSC) aims to generate a complete semantic scene from an incomplete input. Existing approaches often employ dense network architectures with a high parameter count, leading to increased model complexity and resource demands. To address these limitations, we propose RWKV-PCSSC, a lightweight point cloud semantic scene completion network inspired by the Receptance Weighted Key Value (RWKV) mechanism. Specifically, we introduce a RWKV Seed Generator (RWKV-SG) module that can aggregate features from a partial point cloud to produce a coarse point cloud with coarse features. Subsequently, the point-wise feature of the point cloud is progressively restored through multiple stages of the RWKV Point Deconvolution (RWKV-PD) modules. By leveraging a compact and efficient design, our method achieves a lightweight model representation. Experimental results demonstrate that RWKV-PCSSC reduces the parameter count by 4.18$\\times$ and improves memory efficiency by 1.37$\\times$ compared to state-of-the-art methods PointSSC. Furthermore, our network achieves state-of-the-art performance on established indoor (SSC-PC, NYUCAD-PC) and outdoor (PointSSC) scene dataset, as well as on our proposed datasets (NYUCAD-PC-V2, 3D-FRONT-PC).",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "13 pages, 8 figures, published to ACM MM",
      "doi": "10.1145/3746027.3754908",
      "journal_ref": "Proc. 33rd ACM Int. Conf. Multimedia (MM '25), Dublin, Ireland, 2025, pp. 161-170",
      "pdf_url": "https://arxiv.org/pdf/2511.09878v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "IPCD: Intrinsic Point-Cloud Decomposition",
      "authors": [
        "Shogo Sato",
        "Takuhiro Kaneko",
        "Shoichiro Takeda",
        "Tomoyasu Shimada",
        "Kazuhiko Murasaki",
        "Taiga Yoshida",
        "Ryuichi Tanida",
        "Akisato Kimura"
      ],
      "arxiv_id": "2511.09866v1",
      "summary": "Point clouds are widely used in various fields, including augmented reality (AR) and robotics, where relighting and texture editing are crucial for realistic visualization. Achieving these tasks requires accurately separating albedo from shade. However, performing this separation on point clouds presents two key challenges: (1) the non-grid structure of point clouds makes conventional image-based decomposition models ineffective, and (2) point-cloud models designed for other tasks do not explicitly consider global-light direction, resulting in inaccurate shade. In this paper, we introduce \\textbf{Intrinsic Point-Cloud Decomposition (IPCD)}, which extends image decomposition to the direct decomposition of colored point clouds into albedo and shade. To overcome challenge (1), we propose \\textbf{IPCD-Net} that extends image-based model with point-wise feature aggregation for non-grid data processing. For challenge (2), we introduce \\textbf{Projection-based Luminance Distribution (PLD)} with a hierarchical feature refinement, capturing global-light ques via multi-view projection. For comprehensive evaluation, we create a synthetic outdoor-scene dataset. Experimental results demonstrate that IPCD-Net reduces cast shadows in albedo and enhances color accuracy in shade. Furthermore, we showcase its applications in texture editing, relighting, and point-cloud registration under varying illumination. Finally, we verify the real-world applicability of IPCD-Net.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "Accepted in WACV2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09866v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "SpatialActor: Exploring Disentangled Spatial Representations for Robust Robotic Manipulation",
      "authors": [
        "Hao Shi",
        "Bin Xie",
        "Yingfei Liu",
        "Yang Yue",
        "Tiancai Wang",
        "Haoqiang Fan",
        "Xiangyu Zhang",
        "Gao Huang"
      ],
      "arxiv_id": "2511.09555v1",
      "summary": "Robotic manipulation requires precise spatial understanding to interact with objects in the real world. Point-based methods suffer from sparse sampling, leading to the loss of fine-grained semantics. Image-based methods typically feed RGB and depth into 2D backbones pre-trained on 3D auxiliary tasks, but their entangled semantics and geometry are sensitive to inherent depth noise in real-world that disrupts semantic understanding. Moreover, these methods focus on high-level geometry while overlooking low-level spatial cues essential for precise interaction. We propose SpatialActor, a disentangled framework for robust robotic manipulation that explicitly decouples semantics and geometry. The Semantic-guided Geometric Module adaptively fuses two complementary geometry from noisy depth and semantic-guided expert priors. Also, a Spatial Transformer leverages low-level spatial cues for accurate 2D-3D mapping and enables interaction among spatial features. We evaluate SpatialActor on multiple simulation and real-world scenarios across 50+ tasks. It achieves state-of-the-art performance with 87.4% on RLBench and improves by 13.9% to 19.4% under varying noisy conditions, showing strong robustness. Moreover, it significantly enhances few-shot generalization to new tasks and maintains robustness under various spatial perturbations. Project Page: https://shihao1895.github.io/SpatialActor",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "AAAI 2026 Oral | Project Page: https://shihao1895.github.io/SpatialActor",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09555v1",
      "code_links": [
        {
          "url": "https://shihao1895.github.io/SpatialActor",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation",
      "authors": [
        "Runhao Li",
        "Wenkai Guo",
        "Zhenyu Wu",
        "Changyuan Wang",
        "Haoyuan Deng",
        "Zhenyu Weng",
        "Yap-Peng Tan",
        "Ziwei Wang"
      ],
      "arxiv_id": "2511.09516v1",
      "summary": "Pre-trained Vision-Language-Action (VLA) models have achieved remarkable success in improving robustness and generalization for end-to-end robotic manipulation. However, these models struggle with long-horizon tasks due to their lack of memory and reliance solely on immediate sensory inputs. To address this limitation, we propose Memory-Augmented Prompting for Vision-Language-Action model (MAP-VLA), a novel framework that empowers pre-trained VLA models with demonstration-derived memory prompts to augment action generation for long-horizon robotic manipulation tasks. To achieve this, MAP-VLA first constructs a memory library from historical demonstrations, where each memory unit captures information about a specific stage of a task. These memory units are implemented as learnable soft prompts optimized through prompt tuning. Then, during real-time task execution, MAP-VLA retrieves relevant memory through trajectory similarity matching and dynamically integrates it into the VLA model for augmented action generation. Importantly, this prompt tuning and retrieval augmentation approach operates as a plug-and-play module for a frozen VLA model, offering a lightweight and flexible solution to improve task performance. Experimental results show that MAP-VLA delivers up to 7.0% absolute performance gains in the simulation benchmark and 25.0% on real robot evaluations for long-horizon tasks, surpassing the current state-of-the-art methods.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09516v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "EPSegFZ: Efficient Point Cloud Semantic Segmentation for Few- and Zero-Shot Scenarios with Language Guidance",
      "authors": [
        "Jiahui Wang",
        "Haiyue Zhu",
        "Haoren Guo",
        "Abdullah Al Mamun",
        "Cheng Xiang",
        "Tong Heng Lee"
      ],
      "arxiv_id": "2511.11700v1",
      "summary": "Recent approaches for few-shot 3D point cloud semantic segmentation typically require a two-stage learning process, i.e., a pre-training stage followed by a few-shot training stage. While effective, these methods face overreliance on pre-training, which hinders model flexibility and adaptability. Some models tried to avoid pre-training yet failed to capture ample information. In addition, current approaches focus on visual information in the support set and neglect or do not fully exploit other useful data, such as textual annotations. This inadequate utilization of support information impairs the performance of the model and restricts its zero-shot ability. To address these limitations, we present a novel pre-training-free network, named Efficient Point Cloud Semantic Segmentation for Few- and Zero-shot scenarios. Our EPSegFZ incorporates three key components. A Prototype-Enhanced Registers Attention (ProERA) module and a Dual Relative Positional Encoding (DRPE)-based cross-attention mechanism for improved feature extraction and accurate query-prototype correspondence construction without pre-training. A Language-Guided Prototype Embedding (LGPE) module that effectively leverages textual information from the support set to improve few-shot performance and enable zero-shot inference. Extensive experiments show that our method outperforms the state-of-the-art method by 5.68% and 3.82% on the S3DIS and ScanNet benchmarks, respectively.",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11700v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "PAN: A World Model for General, Interactable, and Long-Horizon World Simulation",
      "authors": [
        "PAN Team",
        "Jiannan Xiang",
        "Yi Gu",
        "Zihan Liu",
        "Zeyu Feng",
        "Qiyue Gao",
        "Yiyan Hu",
        "Benhao Huang",
        "Guangyi Liu",
        "Yichi Yang",
        "Kun Zhou",
        "Davit Abrahamyan",
        "Arif Ahmad",
        "Ganesh Bannur",
        "Junrong Chen",
        "Kimi Chen",
        "Mingkai Deng",
        "Ruobing Han",
        "Xinqi Huang",
        "Haoqiang Kang",
        "Zheqi Liu",
        "Enze Ma",
        "Hector Ren",
        "Yashowardhan Shinde",
        "Rohan Shingre",
        "Ramsundar Tanikella",
        "Kaiming Tao",
        "Dequan Yang",
        "Xinle Yu",
        "Cong Zeng",
        "Binglin Zhou",
        "Zhengzhong Liu",
        "Zhiting Hu",
        "Eric P. Xing"
      ],
      "arxiv_id": "2511.09057v3",
      "summary": "A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-12",
      "updated": "2025-11-15",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09057v3",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model",
            "latent dynamics"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning",
      "authors": [
        "Mobin Habibpour",
        "Fatemeh Afghah"
      ],
      "arxiv_id": "2511.08942v1",
      "summary": "While Vision-Language Models (VLMs) are set to transform robotic navigation, existing methods often underutilize their reasoning capabilities. To unlock the full potential of VLMs in robotics, we shift their role from passive observers to active strategists in the navigation process. Our framework outsources high-level planning to a VLM, which leverages its contextual understanding to guide a frontier-based exploration agent. This intelligent guidance is achieved through a trio of techniques: structured chain-of-thought prompting that elicits logical, step-by-step reasoning; dynamic inclusion of the agent's recent action history to prevent getting stuck in loops; and a novel capability that enables the VLM to interpret top-down obstacle maps alongside first-person views, thereby enhancing spatial awareness. When tested on challenging benchmarks like HM3D, Gibson, and MP3D, this method produces exceptionally direct and logical trajectories, marking a substantial improvement in navigation efficiency over existing approaches and charting a path toward more capable embodied agents.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08942v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Expand Your SCOPE: Semantic Cognition over Potential-Based Exploration for Embodied Visual Navigation",
      "authors": [
        "Ningnan Wang",
        "Weihuang Chen",
        "Liming Chen",
        "Haoxuan Ji",
        "Zhongyu Guo",
        "Xuchong Zhang",
        "Hongbin Sun"
      ],
      "arxiv_id": "2511.08935v1",
      "summary": "Embodied visual navigation remains a challenging task, as agents must explore unknown environments with limited knowledge. Existing zero-shot studies have shown that incorporating memory mechanisms to support goal-directed behavior can improve long-horizon planning performance. However, they overlook visual frontier boundaries, which fundamentally dictate future trajectories and observations, and fall short of inferring the relationship between partial visual observations and navigation goals. In this paper, we propose Semantic Cognition Over Potential-based Exploration (SCOPE), a zero-shot framework that explicitly leverages frontier information to drive potential-based exploration, enabling more informed and goal-relevant decisions. SCOPE estimates exploration potential with a Vision-Language Model and organizes it into a spatio-temporal potential graph, capturing boundary dynamics to support long-horizon planning. In addition, SCOPE incorporates a self-reconsideration mechanism that revisits and refines prior decisions, enhancing reliability and reducing overconfident errors. Experimental results on two diverse embodied navigation tasks show that SCOPE outperforms state-of-the-art baselines by 4.6\\% in accuracy. Further analysis demonstrates that its core components lead to improved calibration, stronger generalization, and higher decision quality.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08935v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "OG-PCL: Efficient Sparse Point Cloud Processing for Human Activity Recognition",
      "authors": [
        "Jiuqi Yan",
        "Chendong Xu",
        "Dongyu Liu"
      ],
      "arxiv_id": "2511.08910v1",
      "summary": "Human activity recognition (HAR) with millimeter-wave (mmWave) radar offers a privacy-preserving and robust alternative to camera- and wearable-based approaches. In this work, we propose the Occupancy-Gated Parallel-CNN Bi-LSTM (OG-PCL) network to process sparse 3D radar point clouds produced by mmWave sensing. Designed for lightweight deployment, the parameter size of the proposed OG-PCL is only 0.83M and achieves 91.75 accuracy on the RadHAR dataset, outperforming those existing baselines such as 2D CNN, PointNet, and 3D CNN methods. We validate the advantages of the tri-view parallel structure in preserving spatial information across three dimensions while maintaining efficiency through ablation studies. We further introduce the Occupancy-Gated Convolution (OGConv) block and demonstrate the necessity of its occupancy compensation mechanism for handling sparse point clouds. The proposed OG-PCL thus offers a compact yet accurate framework for real-time radar-based HAR on lightweight platforms.",
      "categories": [
        "eess.SP",
        "cs.CV"
      ],
      "primary_category": "eess.SP",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08910v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "DT-NVS: Diffusion Transformers for Novel View Synthesis",
      "authors": [
        "Wonbong Jang",
        "Jonathan Tremblay",
        "Lourdes Agapito"
      ],
      "arxiv_id": "2511.08823v1",
      "summary": "Generating novel views of a natural scene, e.g., every-day scenes both indoors and outdoors, from a single view is an under-explored problem, even though it is an organic extension to the object-centric novel view synthesis. Existing diffusion-based approaches focus rather on small camera movements in real scenes or only consider unnatural object-centric scenes, limiting their potential applications in real-world settings. In this paper we move away from these constrained regimes and propose a 3D diffusion model trained with image-only losses on a large-scale dataset of real-world, multi-category, unaligned, and casually acquired videos of everyday scenes. We propose DT-NVS, a 3D-aware diffusion model for generalized novel view synthesis that exploits a transformer-based architecture backbone. We make significant contributions to transformer and self-attention architectures to translate images to 3d representations, and novel camera conditioning strategies to allow training on real-world unaligned datasets. In addition, we introduce a novel training paradigm swapping the role of reference frame between the conditioning image and the sampled noisy input. We evaluate our approach on the 3D task of generalized novel view synthesis from a single input image and show improvements over state-of-the-art 3D aware diffusion models and deterministic approaches, while generating diverse outputs.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "14 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08823v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]novel view synthesis"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Adaptive graph Kolmogorov-Arnold network for 3D human pose estimation",
      "authors": [
        "Abu Taib Mohammed Shahjahan",
        "A. Ben Hamza"
      ],
      "arxiv_id": "2511.08809v1",
      "summary": "Graph convolutional network (GCN)-based methods have shown strong performance in 3D human pose estimation by leveraging the natural graph structure of the human skeleton. However, their local receptive field limits their ability to capture long-range dependencies essential for handling occlusions and depth ambiguities. They also exhibit spectral bias, which prioritizes low-frequency components while struggling to model high-frequency details. In this paper, we introduce PoseKAN, an adaptive graph Kolmogorov-Arnold Network (KAN), framework that extends KANs to graph-based learning for 2D-to-3D pose lifting from a single image. Unlike GCNs that use fixed activation functions, KANs employ learnable functions on graph edges, allowing data-driven, adaptive feature transformations. This enhances the model's adaptability and expressiveness, making it more expressive in learning complex pose variations. Our model employs multi-hop feature aggregation, ensuring the body joints can leverage information from both local and distant neighbors, leading to improved spatial awareness. It also incorporates residual PoseKAN blocks for deeper feature refinement, and a global response normalization for improved feature selectivity and contrast. Extensive experiments on benchmark datasets demonstrate the competitive performance of our model against state-of-the-art methods.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08809v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]pose estimation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Intuitive Programming, Adaptive Task Planning, and Dynamic Role Allocation in Human-Robot Collaboration",
      "authors": [
        "Marta Lagomarsino",
        "Elena Merlo",
        "Andrea Pupa",
        "Timo Birr",
        "Franziska Krebs",
        "Cristian Secchi",
        "Tamim Asfour",
        "Arash Ajoudani"
      ],
      "arxiv_id": "2511.08732v1",
      "summary": "Remarkable capabilities have been achieved by robotics and AI, mastering complex tasks and environments. Yet, humans often remain passive observers, fascinated but uncertain how to engage. Robots, in turn, cannot reach their full potential in human-populated environments without effectively modeling human states and intentions and adapting their behavior. To achieve a synergistic human-robot collaboration (HRC), a continuous information flow should be established: humans must intuitively communicate instructions, share expertise, and express needs. In parallel, robots must clearly convey their internal state and forthcoming actions to keep users informed, comfortable, and in control. This review identifies and connects key components enabling intuitive information exchange and skill transfer between humans and robots. We examine the full interaction pipeline: from the human-to-robot communication bridge translating multimodal inputs into robot-understandable representations, through adaptive planning and role allocation, to the control layer and feedback mechanisms to close the loop. Finally, we highlight trends and promising directions toward more adaptive, accessible HRC.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "Published in the Annual Review of Control, Robotics, and Autonomous Systems, Volume 9; copyright 2026 the author(s), CC BY 4.0, https://www.annualreviews.org",
      "doi": "10.1146/annurev-control-022624-013624",
      "journal_ref": "Annual Review of Control, Robotics, and Autonomous Systems (2026)",
      "pdf_url": "https://arxiv.org/pdf/2511.08732v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "human-to-robot",
            "human to robot"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "7_retargeting"
      ]
    },
    {
      "title": "Human Motion Intent Inferencing in Teleoperation Through a SINDy Paradigm",
      "authors": [
        "Michael Bowman",
        "Xiaoli Zhang"
      ],
      "arxiv_id": "2511.08377v1",
      "summary": "Intent inferencing in teleoperation has been instrumental in aligning operator goals and coordinating actions with robotic partners. However, current intent inference methods often ignore subtle motion that can be strong indicators for a sudden change in intent. Specifically, we aim to tackle 1) if we can detect sudden jumps in operator trajectories, 2) how we appropriately use these sudden jump motions to infer an operator's goal state, and 3) how to incorporate these discontinuous and continuous dynamics to infer operator motion. Our framework, called Psychic, models these small indicative motions through a jump-drift-diffusion stochastic differential equation to cover discontinuous and continuous dynamics. Kramers-Moyal (KM) coefficients allow us to detect jumps with a trajectory which we pair with a statistical outlier detection algorithm to nominate goal transitions. Through identifying jumps, we can perform early detection of existing goals and discover undefined goals in unstructured scenarios. Our framework then applies a Sparse Identification of Nonlinear Dynamics (SINDy) model using KM coefficients with the goal transitions as a control input to infer an operator's motion behavior in unstructured scenarios. We demonstrate Psychic can produce probabilistic reachability sets and compare our strategy to a negative log-likelihood model fit. We perform a retrospective study on 600 operator trajectories in a hands-free teleoperation task to evaluate the efficacy of our opensource package, Psychic, in both offline and online learning.",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "Open source software and video examples here: https://github.com/namwob44/Psychic",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08377v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]teleoperation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "X-IONet: Cross-Platform Inertial Odometry Network with Dual-Stage Attention",
      "authors": [
        "Dehan Shen",
        "Changhao Chen"
      ],
      "arxiv_id": "2511.08277v1",
      "summary": "Learning-based inertial odometry has achieved remarkable progress in pedestrian navigation. However, extending these methods to quadruped robots remains challenging due to their distinct and highly dynamic motion patterns. Models that perform well on pedestrian data often experience severe degradation when deployed on legged platforms. To tackle this challenge, we introduce X-IONet, a cross-platform inertial odometry framework that operates solely using a single Inertial Measurement Unit (IMU). X-IONet incorporates a rule-based expert selection module to classify motion platforms and route IMU sequences to platform-specific expert networks. The displacement prediction network features a dual-stage attention architecture that jointly models long-range temporal dependencies and inter-axis correlations, enabling accurate motion representation. It outputs both displacement and associated uncertainty, which are further fused through an Extended Kalman Filter (EKF) for robust state estimation. Extensive experiments on public pedestrian datasets and a self-collected quadruped robot dataset demonstrate that X-IONet achieves state-of-the-art performance, reducing Absolute Trajectory Error (ATE) by 14.3% and Relative Trajectory Error (RTE) by 11.4% on pedestrian data, and by 52.8% and 41.3% on quadruped robot data. These results highlight the effectiveness of X-IONet in advancing accurate and robust inertial navigation across both human and legged robot platforms.",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08277v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped",
            "legged robot"
          ],
          "score": 4.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Accurate and Efficient Surface Reconstruction from Point Clouds via Geometry-Aware Local Adaptation",
      "authors": [
        "Eito Ogawa",
        "Taiga Hayami",
        "Hiroshi Watanabe"
      ],
      "arxiv_id": "2511.08233v1",
      "summary": "Point cloud surface reconstruction has improved in accuracy with advances in deep learning, enabling applications such as infrastructure inspection. Recent approaches that reconstruct from small local regions rather than entire point clouds have attracted attention for their strong generalization capability. However, prior work typically places local regions uniformly and keeps their size fixed, limiting adaptability to variations in geometric complexity. In this study, we propose a method that improves reconstruction accuracy and efficiency by adaptively modulating the spacing and size of local regions based on the curvature of the input point cloud.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "4 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08233v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Effective Game-Theoretic Motion Planning via Nested Search",
      "authors": [
        "Avishav Engle",
        "Andrey Zhitnikov",
        "Oren Salzman",
        "Omer Ben-Porat",
        "Kiril Solovey"
      ],
      "arxiv_id": "2511.08001v1",
      "summary": "To facilitate effective, safe deployment in the real world, individual robots must reason about interactions with other agents, which often occur without explicit communication. Recent work has identified game theory, particularly the concept of Nash Equilibrium (NE), as a key enabler for behavior-aware decision-making. Yet, existing work falls short of fully unleashing the power of game-theoretic reasoning. Specifically, popular optimization-based methods require simplified robot dynamics and tend to get trapped in local minima due to convexification. Other works that rely on payoff matrices suffer from poor scalability due to the explicit enumeration of all possible trajectories. To bridge this gap, we introduce Game-Theoretic Nested Search (GTNS), a novel, scalable, and provably correct approach for computing NEs in general dynamical systems. GTNS efficiently searches the action space of all agents involved, while discarding trajectories that violate the NE constraint (no unilateral deviation) through an inner search over a lower-dimensional space. Our algorithm enables explicit selection among equilibria by utilizing a user-specified global objective, thereby capturing a rich set of realistic interactions. We demonstrate the approach on a variety of autonomous driving and racing scenarios where we achieve solutions in mere seconds on commodity hardware.",
      "categories": [
        "cs.RO",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08001v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]motion planning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "DANCE: Density-agnostic and Class-aware Network for Point Cloud Completion",
      "authors": [
        "Da-Yeong Kim",
        "Yeong-Jun Cho"
      ],
      "arxiv_id": "2511.07978v2",
      "summary": "Point cloud completion aims to recover missing geometric structures from incomplete 3D scans, which often suffer from occlusions or limited sensor viewpoints. Existing methods typically assume fixed input/output densities or rely on image-based representations, making them less suitable for real-world scenarios with variable sparsity and limited supervision. In this paper, we introduce Density-agnostic and Class-aware Network (DANCE), a novel framework that completes only the missing regions while preserving the observed geometry. DANCE generates candidate points via ray-based sampling from multiple viewpoints. A transformer decoder then refines their positions and predicts opacity scores, which determine the validity of each point for inclusion in the final surface. To incorporate semantic guidance, a lightweight classification head is trained directly on geometric features, enabling category-consistent completion without external image supervision. Extensive experiments on the PCN and MVP benchmarks show that DANCE outperforms state-of-the-art methods in accuracy and structural consistency, while remaining robust to varying input densities and noise levels.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-17",
      "comment": "7 pages, 11 figures, Accepted to AAAI 2026 (to appear)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07978v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Generating Sketches in a Hierarchical Auto-Regressive Process for Flexible Sketch Drawing Manipulation at Stroke-Level",
      "authors": [
        "Sicong Zang",
        "Shuhui Gao",
        "Zhijun Fang"
      ],
      "arxiv_id": "2511.07889v1",
      "summary": "Generating sketches with specific patterns as expected, i.e., manipulating sketches in a controllable way, is a popular task. Recent studies control sketch features at stroke-level by editing values of stroke embeddings as conditions. However, in order to provide generator a global view about what a sketch is going to be drawn, all these edited conditions should be collected and fed into generator simultaneously before generation starts, i.e., no further manipulation is allowed during sketch generating process. In order to realize sketch drawing manipulation more flexibly, we propose a hierarchical auto-regressive sketch generating process. Instead of generating an entire sketch at once, each stroke in a sketch is generated in a three-staged hierarchy: 1) predicting a stroke embedding to represent which stroke is going to be drawn, and 2) anchoring the predicted stroke on the canvas, and 3) translating the embedding to a sequence of drawing actions to form the full sketch. Moreover, the stroke prediction, anchoring and translation are proceeded auto-regressively, i.e., both the recently generated strokes and their positions are considered to predict the current one, guiding model to produce an appropriate stroke at a suitable position to benefit the full sketch generation. It is flexible to manipulate stroke-level sketch drawing at any time during generation by adjusting the exposed editable stroke embeddings.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "Accepted by AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07889v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "FlowFeat: Pixel-Dense Embedding of Motion Profiles",
      "authors": [
        "Nikita Araslanov",
        "Anna Sonnweber",
        "Daniel Cremers"
      ],
      "arxiv_id": "2511.07696v1",
      "summary": "Dense and versatile image representations underpin the success of virtually all computer vision applications. However, state-of-the-art networks, such as transformers, produce low-resolution feature grids, which are suboptimal for dense prediction tasks. To address this limitation, we present FlowFeat, a high-resolution and multi-task feature representation. The key ingredient behind FlowFeat is a novel distillation technique that embeds a distribution of plausible apparent motions, or motion profiles. By leveraging optical flow networks and diverse video data, we develop an effective self-supervised training framework that statistically approximates the apparent motion. With its remarkable level of spatial detail, FlowFeat encodes a compelling degree of geometric and semantic cues while exhibiting high temporal consistency. Empirically, FlowFeat significantly enhances the representational power of five state-of-the-art encoders and alternative upsampling strategies across three dense tasks: video object segmentation, monocular depth estimation and semantic segmentation. Training FlowFeat is computationally inexpensive and robust to inaccurate flow estimation, remaining highly effective even when using unsupervised flow networks. Our work takes a step forward towards reliable and versatile dense image representations.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "Project website: https://tum-vision.github.io/flowfeat",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07696v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation",
            "monocular depth",
            "optical flow"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "LiveNeRF: Efficient Face Replacement Through Neural Radiance Fields Integration",
      "authors": [
        "Tung Vu",
        "Hai Nguyen",
        "Cong Tran"
      ],
      "arxiv_id": "2511.07552v1",
      "summary": "Face replacement technology enables significant advancements in entertainment, education, and communication applications, including dubbing, virtual avatars, and cross-cultural content adaptation. Our LiveNeRF framework addresses critical limitations of existing methods by achieving real-time performance (33 FPS) with superior visual quality, enabling practical deployment in live streaming, video conferencing, and interactive media. The technology particularly benefits content creators, educators, and individuals with speech impairments through accessible avatar communication. While acknowledging potential misuse in unauthorized deepfake creation, we advocate for responsible deployment with user consent verification and integration with detection systems to ensure positive societal impact while minimizing risks.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07552v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]neural radiance"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Robotic versus Human Teleoperation for Remote Ultrasound",
      "authors": [
        "David Black",
        "Septimiu Salcudean"
      ],
      "arxiv_id": "2511.07275v1",
      "summary": "Diagnostic medical ultrasound is widely used, safe, and relatively low cost but requires a high degree of expertise to acquire and interpret the images. Personnel with this expertise are often not available outside of larger cities, leading to difficult, costly travel and long wait times for rural populations. To address this issue, tele-ultrasound techniques are being developed, including robotic teleoperation and recently human teleoperation, in which a novice user is remotely guided in a hand-over-hand manner through mixed reality to perform an ultrasound exam. These methods have not been compared, and their relative strengths are unknown. Human teleoperation may be more practical than robotics for small communities due to its lower cost and complexity, but this is only relevant if the performance is comparable. This paper therefore evaluates the differences between human and robotic teleoperation, examining practical aspects such as setup time and flexibility and experimentally comparing performance metrics such as completion time, position tracking, and force consistency. It is found that human teleoperation does not lead to statistically significant differences in completion time or position accuracy, with mean differences of 1.8% and 0.5%, respectively, and provides more consistent force application despite being substantially more practical and accessible.",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "Under review at IEEE TMRB. Extended version of a paper presented at the Hamlyn Symposium for Medical Robotics, 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07275v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]teleoperation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Multi-Agent Reinforcement Learning for Deadlock Handling among Autonomous Mobile Robots",
      "authors": [
        "Marcel Müller"
      ],
      "arxiv_id": "2511.07071v1",
      "summary": "This dissertation explores the application of multi-agent reinforcement learning (MARL) for handling deadlocks in intralogistics systems that rely on autonomous mobile robots (AMRs). AMRs enhance operational flexibility but also increase the risk of deadlocks, which degrade system throughput and reliability. Existing approaches often neglect deadlock handling in the planning phase and rely on rigid control rules that cannot adapt to dynamic operational conditions.\n  To address these shortcomings, this work develops a structured methodology for integrating MARL into logistics planning and operational control. It introduces reference models that explicitly consider deadlock-capable multi-agent pathfinding (MAPF) problems, enabling systematic evaluation of MARL strategies. Using grid-based environments and an external simulation software, the study compares traditional deadlock handling strategies with MARL-based solutions, focusing on PPO and IMPALA algorithms under different training and execution modes.\n  Findings reveal that MARL-based strategies, particularly when combined with centralized training and decentralized execution (CTDE), outperform rule-based methods in complex, congested environments. In simpler environments or those with ample spatial freedom, rule-based methods remain competitive due to their lower computational demands. These results highlight that MARL provides a flexible and scalable solution for deadlock handling in dynamic intralogistics scenarios, but requires careful tailoring to the operational context.",
      "categories": [
        "cs.MA",
        "cs.RO"
      ],
      "primary_category": "cs.MA",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "for associated repositories, see https://github.com/Nerozud/dl_reference_models and https://github.com/Nerozud/FTS_simpel",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07071v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "PPO"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "RaLD: Generating High-Resolution 3D Radar Point Clouds with Latent Diffusion",
      "authors": [
        "Ruijie Zhang",
        "Bixin Zeng",
        "Shengpeng Wang",
        "Fuhui Zhou",
        "Wei Wang"
      ],
      "arxiv_id": "2511.07067v1",
      "summary": "Millimeter-wave radar offers a promising sensing modality for autonomous systems thanks to its robustness in adverse conditions and low cost. However, its utility is significantly limited by the sparsity and low resolution of radar point clouds, which poses challenges for tasks requiring dense and accurate 3D perception. Despite that recent efforts have shown great potential by exploring generative approaches to address this issue, they often rely on dense voxel representations that are inefficient and struggle to preserve structural detail. To fill this gap, we make the key observation that latent diffusion models (LDMs), though successful in other modalities, have not been effectively leveraged for radar-based 3D generation due to a lack of compatible representations and conditioning strategies. We introduce RaLD, a framework that bridges this gap by integrating scene-level frustum-based LiDAR autoencoding, order-invariant latent representations, and direct radar spectrum conditioning. These insights lead to a more compact and expressive generation process. Experiments show that RaLD produces dense and accurate 3D point clouds from raw radar spectrums, offering a promising solution for robust perception in challenging environments.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07067v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "3D-ANC: Adaptive Neural Collapse for Robust 3D Point Cloud Recognition",
      "authors": [
        "Yuanmin Huang",
        "Wenxuan Li",
        "Mi Zhang",
        "Xiaohan Zhang",
        "Xiaoyu You",
        "Min Yang"
      ],
      "arxiv_id": "2511.07040v2",
      "summary": "Deep neural networks have recently achieved notable progress in 3D point cloud recognition, yet their vulnerability to adversarial perturbations poses critical security challenges in practical deployments. Conventional defense mechanisms struggle to address the evolving landscape of multifaceted attack patterns. Through systematic analysis of existing defenses, we identify that their unsatisfactory performance primarily originates from an entangled feature space, where adversarial attacks can be performed easily. To this end, we present 3D-ANC, a novel approach that capitalizes on the Neural Collapse (NC) mechanism to orchestrate discriminative feature learning. In particular, NC depicts where last-layer features and classifier weights jointly evolve into a simplex equiangular tight frame (ETF) arrangement, establishing maximally separable class prototypes. However, leveraging this advantage in 3D recognition confronts two substantial challenges: (1) prevalent class imbalance in point cloud datasets, and (2) complex geometric similarities between object categories. To tackle these obstacles, our solution combines an ETF-aligned classification module with an adaptive training framework consisting of representation-balanced learning (RBL) and dynamic feature direction loss (FDL). 3D-ANC seamlessly empowers existing models to develop disentangled feature spaces despite the complexity in 3D data distribution. Comprehensive evaluations state that 3D-ANC significantly improves the robustness of models with various structures on two datasets. For instance, DGCNN's classification accuracy is elevated from 27.2% to 80.9% on ModelNet40 -- a 53.7% absolute gain that surpasses leading baselines by 34.0%.",
      "categories": [
        "cs.CV",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-12-08",
      "comment": "AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07040v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Certified L2-Norm Robustness of 3D Point Cloud Recognition in the Frequency Domain",
      "authors": [
        "Liang Zhou",
        "Qiming Wang",
        "Tianze Chen"
      ],
      "arxiv_id": "2511.07029v1",
      "summary": "3D point cloud classification is a fundamental task in safety-critical applications such as autonomous driving, robotics, and augmented reality. However, recent studies reveal that point cloud classifiers are vulnerable to structured adversarial perturbations and geometric corruptions, posing risks to their deployment in safety-critical scenarios. Existing certified defenses limit point-wise perturbations but overlook subtle geometric distortions that preserve individual points yet alter the overall structure, potentially leading to misclassification. In this work, we propose FreqCert, a novel certification framework that departs from conventional spatial domain defenses by shifting robustness analysis to the frequency domain, enabling structured certification against global L2-bounded perturbations. FreqCert first transforms the input point cloud via the graph Fourier transform (GFT), then applies structured frequency-aware subsampling to generate multiple sub-point clouds. Each sub-cloud is independently classified by a standard model, and the final prediction is obtained through majority voting, where sub-clouds are constructed based on spectral similarity rather than spatial proximity, making the partitioning more stable under L2 perturbations and better aligned with the object's intrinsic structure. We derive a closed-form lower bound on the certified L2 robustness radius and prove its tightness under minimal and interpretable assumptions, establishing a theoretical foundation for frequency domain certification. Extensive experiments on the ModelNet40 and ScanObjectNN datasets demonstrate that FreqCert consistently achieves higher certified accuracy and empirical accuracy under strong perturbations. Our results suggest that spectral representations provide an effective pathway toward certifiable robustness in 3D point cloud recognition.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "Accepted by AAAI26",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07029v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "FoCLIP: A Feature-Space Misalignment Framework for CLIP-Based Image Manipulation and Detection",
      "authors": [
        "Yulin Chen",
        "Zeyuan Wang",
        "Tianyuan Yu",
        "Yingmei Wei",
        "Liang Bai"
      ],
      "arxiv_id": "2511.06947v1",
      "summary": "The well-aligned attribute of CLIP-based models enables its effective application like CLIPscore as a widely adopted image quality assessment metric. However, such a CLIP-based metric is vulnerable for its delicate multimodal alignment. In this work, we propose \\textbf{FoCLIP}, a feature-space misalignment framework for fooling CLIP-based image quality metric. Based on the stochastic gradient descent technique, FoCLIP integrates three key components to construct fooling examples: feature alignment as the core module to reduce image-text modality gaps, the score distribution balance module and pixel-guard regularization, which collectively optimize multimodal output equilibrium between CLIPscore performance and image quality. Such a design can be engineered to maximize the CLIPscore predictions across diverse input prompts, despite exhibiting either visual unrecognizability or semantic incongruence with the corresponding adversarial prompts from human perceptual perspectives. Experiments on ten artistic masterpiece prompts and ImageNet subsets demonstrate that optimized images can achieve significant improvement in CLIPscore while preserving high visual fidelity. In addition, we found that grayscale conversion induces significant feature degradation in fooling images, exhibiting noticeable CLIPscore reduction while preserving statistical consistency with original images. Inspired by this phenomenon, we propose a color channel sensitivity-driven tampering detection mechanism that achieves 91% accuracy on standard benchmarks. In conclusion, this work establishes a practical pathway for feature misalignment in CLIP-based multimodal systems and the corresponding defense method.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "15 page, 9 figures, published to PRCV",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06947v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "PanoNav: Mapless Zero-Shot Object Navigation with Panoramic Scene Parsing and Dynamic Memory",
      "authors": [
        "Qunchao Jin",
        "Yilin Wu",
        "Changhao Chen"
      ],
      "arxiv_id": "2511.06840v1",
      "summary": "Zero-shot object navigation (ZSON) in unseen environments remains a challenging problem for household robots, requiring strong perceptual understanding and decision-making capabilities. While recent methods leverage metric maps and Large Language Models (LLMs), they often depend on depth sensors or prebuilt maps, limiting the spatial reasoning ability of Multimodal Large Language Models (MLLMs). Mapless ZSON approaches have emerged to address this, but they typically make short-sighted decisions, leading to local deadlocks due to a lack of historical context. We propose PanoNav, a fully RGB-only, mapless ZSON framework that integrates a Panoramic Scene Parsing module to unlock the spatial parsing potential of MLLMs from panoramic RGB inputs, and a Memory-guided Decision-Making mechanism enhanced by a Dynamic Bounded Memory Queue to incorporate exploration history and avoid local deadlocks. Experiments on the public navigation benchmark show that PanoNav significantly outperforms representative baselines in both SR and SPL metrics.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "Accepted as a poster in AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06840v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Vision-Aided Online A* Path Planning for Efficient and Safe Navigation of Service Robots",
      "authors": [
        "Praveen Kumar",
        "Tushar Sandhan"
      ],
      "arxiv_id": "2511.06801v1",
      "summary": "The deployment of autonomous service robots in human-centric environments is hindered by a critical gap in perception and planning. Traditional navigation systems rely on expensive LiDARs that, while geometrically precise, are semantically unaware, they cannot distinguish a important document on an office floor from a harmless piece of litter, treating both as physically traversable. While advanced semantic segmentation exists, no prior work has successfully integrated this visual intelligence into a real-time path planner that is efficient enough for low-cost, embedded hardware. This paper presents a framework to bridge this gap, delivering context-aware navigation on an affordable robotic platform. Our approach centers on a novel, tight integration of a lightweight perception module with an online A* planner. The perception system employs a semantic segmentation model to identify user-defined visual constraints, enabling the robot to navigate based on contextual importance rather than physical size alone. This adaptability allows an operator to define what is critical for a given task, be it sensitive papers in an office or safety lines in a factory, thus resolving the ambiguity of what to avoid. This semantic perception is seamlessly fused with geometric data. The identified visual constraints are projected as non-geometric obstacles onto a global map that is continuously updated from sensor data, enabling robust navigation through both partially known and unknown environments. We validate our framework through extensive experiments in high-fidelity simulations and on a real-world robotic platform. The results demonstrate robust, real-time performance, proving that a cost-effective robot can safely navigate complex environments while respecting critical visual cues invisible to traditional planners.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "10 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06801v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "SlotVLA: Towards Modeling of Object-Relation Representations in Robotic Manipulation",
      "authors": [
        "Taisei Hanyu",
        "Nhat Chung",
        "Huy Le",
        "Toan Nguyen",
        "Yuki Ikebe",
        "Anthony Gunderman",
        "Duy Nguyen Ho Minh",
        "Khoa Vo",
        "Tung Kieu",
        "Kashu Yamazaki",
        "Chase Rainwater",
        "Anh Nguyen",
        "Ngan Le"
      ],
      "arxiv_id": "2511.06754v2",
      "summary": "Inspired by how humans reason over discrete objects and their relationships, we explore whether compact object-centric and object-relation representations can form a foundation for multitask robotic manipulation. Most existing robotic multitask models rely on dense embeddings that entangle both object and background cues, raising concerns about both efficiency and interpretability. In contrast, we study object-relation-centric representations as a pathway to more structured, efficient, and explainable visuomotor control. Our contributions are two-fold. First, we introduce LIBERO+, a fine-grained benchmark dataset designed to enable and evaluate object-relation reasoning in robotic manipulation. Unlike prior datasets, LIBERO+ provides object-centric annotations that enrich demonstrations with box- and mask-level labels as well as instance-level temporal tracking, supporting compact and interpretable visuomotor representations. Second, we propose SlotVLA, a slot-attention-based framework that captures both objects and their relations for action decoding. It uses a slot-based visual tokenizer to maintain consistent temporal object representations, a relation-centric decoder to produce task-relevant embeddings, and an LLM-driven module that translates these embeddings into executable actions. Experiments on LIBERO+ demonstrate that object-centric slot and object-relation slot representations drastically reduce the number of required visual tokens, while providing competitive generalization. Together, LIBERO+ and SlotVLA provide a compact, interpretable, and effective foundation for advancing object-relation-centric robotic manipulation.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-10",
      "updated": "2025-11-28",
      "comment": "under review",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06754v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "PointCubeNet: 3D Part-level Reasoning with 3x3x3 Point Cloud Blocks",
      "authors": [
        "Da-Yeong Kim",
        "Yeong-Jun Cho"
      ],
      "arxiv_id": "2511.06744v1",
      "summary": "In this paper, we propose PointCubeNet, a novel multi-modal 3D understanding framework that achieves part-level reasoning without requiring any part annotations. PointCubeNet comprises global and local branches. The proposed local branch, structured into 3x3x3 local blocks, enables part-level analysis of point cloud sub-regions with the corresponding local text labels. Leveraging the proposed pseudo-labeling method and local loss function, PointCubeNet is effectively trained in an unsupervised manner. The experimental results demonstrate that understanding 3D object parts enhances the understanding of the overall 3D object. In addition, this is the first attempt to perform unsupervised 3D part-level reasoning and achieves reliable and meaningful results.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06744v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "AvatarTex: High-Fidelity Facial Texture Reconstruction from Single-Image Stylized Avatars",
      "authors": [
        "Yuda Qiu",
        "Zitong Xiao",
        "Yiwei Zuo",
        "Zisheng Ye",
        "Weikai Chen",
        "Xiaoguang Han"
      ],
      "arxiv_id": "2511.06721v1",
      "summary": "We present AvatarTex, a high-fidelity facial texture reconstruction framework capable of generating both stylized and photorealistic textures from a single image. Existing methods struggle with stylized avatars due to the lack of diverse multi-style datasets and challenges in maintaining geometric consistency in non-standard textures. To address these limitations, AvatarTex introduces a novel three-stage diffusion-to-GAN pipeline. Our key insight is that while diffusion models excel at generating diversified textures, they lack explicit UV constraints, whereas GANs provide a well-structured latent space that ensures style and topology consistency. By integrating these strengths, AvatarTex achieves high-quality topology-aligned texture synthesis with both artistic and geometric coherence. Specifically, our three-stage pipeline first completes missing texture regions via diffusion-based inpainting, refines style and structure consistency using GAN-based latent optimization, and enhances fine details through diffusion-based repainting. To address the need for a stylized texture dataset, we introduce TexHub, a high-resolution collection of 20,000 multi-style UV textures with precise UV-aligned layouts. By leveraging TexHub and our structured diffusion-to-GAN pipeline, AvatarTex establishes a new state-of-the-art in multi-style facial texture reconstruction. TexHub will be released upon publication to facilitate future research in this field.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "3DV 2026 Accepted",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06721v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "latent optimization",
            "geometric consistency"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "7_retargeting"
      ]
    },
    {
      "title": "Real Garment Benchmark (RGBench): A Comprehensive Benchmark for Robotic Garment Manipulation featuring a High-Fidelity Scalable Simulator",
      "authors": [
        "Wenkang Hu",
        "Xincheng Tang",
        "Yanzhi E",
        "Yitong Li",
        "Zhengjie Shu",
        "Wei Li",
        "Huamin Wang",
        "Ruigang Yang"
      ],
      "arxiv_id": "2511.06434v2",
      "summary": "While there has been significant progress to use simulated data to learn robotic manipulation of rigid objects, applying its success to deformable objects has been hindered by the lack of both deformable object models and realistic non-rigid body simulators. In this paper, we present Real Garment Benchmark (RGBench), a comprehensive benchmark for robotic manipulation of garments. It features a diverse set of over 6000 garment mesh models, a new high-performance simulator, and a comprehensive protocol to evaluate garment simulation quality with carefully measured real garment dynamics. Our experiments demonstrate that our simulator outperforms currently available cloth simulators by a large margin, reducing simulation error by 20% while maintaining a speed of 3 times faster. We will publicly release RGBench to accelerate future research in robotic garment manipulation. Website: https://rgbench.github.io/",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-09",
      "updated": "2025-11-12",
      "comment": "2026 AAAI Accept",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06434v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "DiffusionUavLoc: Visually Prompted Diffusion for Cross-View UAV Localization",
      "authors": [
        "Tao Liu",
        "Kan Ren",
        "Qian Chen"
      ],
      "arxiv_id": "2511.06422v1",
      "summary": "With the rapid growth of the low-altitude economy, unmanned aerial vehicles (UAVs) have become key platforms for measurement and tracking in intelligent patrol systems. However, in GNSS-denied environments, localization schemes that rely solely on satellite signals are prone to failure. Cross-view image retrieval-based localization is a promising alternative, yet substantial geometric and appearance domain gaps exist between oblique UAV views and nadir satellite orthophotos. Moreover, conventional approaches often depend on complex network architectures, text prompts, or large amounts of annotation, which hinders generalization. To address these issues, we propose DiffusionUavLoc, a cross-view localization framework that is image-prompted, text-free, diffusion-centric, and employs a VAE for unified representation. We first use training-free geometric rendering to synthesize pseudo-satellite images from UAV imagery as structural prompts. We then design a text-free conditional diffusion model that fuses multimodal structural cues to learn features robust to viewpoint changes. At inference, descriptors are computed at a fixed time step t and compared using cosine similarity. On University-1652 and SUES-200, the method performs competitively for cross-view localization, especially for satellite-to-drone in University-1652.Our data and code will be published at the following URL: https://github.com/liutao23/DiffusionUavLoc.git.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-09",
      "updated": "2025-11-09",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06422v1",
      "code_links": [
        {
          "url": "https://github.com/liutao23/DiffusionUavLoc.git",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "CINEMAE: Leveraging Frozen Masked Autoencoders for Cross-Generator AI Image Detection",
      "authors": [
        "Minsuk Jang",
        "Hyeonseo Jeong",
        "Minseok Son",
        "Changick Kim"
      ],
      "arxiv_id": "2511.06325v1",
      "summary": "While context-based detectors have achieved strong generalization for AI-generated text by measuring distributional inconsistencies, image-based detectors still struggle with overfitting to generator-specific artifacts. We introduce CINEMAE, a novel paradigm for AIGC image detection that adapts the core principles of text detection methods to the visual domain. Our key insight is that Masked AutoEncoder (MAE), trained to reconstruct masked patches conditioned on visible context, naturally encodes semantic consistency expectations. We formalize this reconstruction process probabilistically, computing conditional Negative Log-Likelihood (NLL, p(masked | visible)) to quantify local semantic anomalies. By aggregating these patch-level statistics with global MAE features through learned fusion, CINEMAE achieves strong cross-generator generalization. Trained exclusively on Stable Diffusion v1.4, our method achieves over 95% accuracy on all eight unseen generators in the GenImage benchmark, substantially outperforming state-of-the-art detectors. This demonstrates that context-conditional reconstruction uncertainty provides a robust, transferable signal for AIGC detection.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-09",
      "updated": "2025-11-09",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06325v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]masked autoencoder",
            "MAE"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Robust Differentiable Collision Detection for General Objects",
      "authors": [
        "Jiayi Chen",
        "Wei Zhao",
        "Liangwang Ruan",
        "Baoquan Chen",
        "He Wang"
      ],
      "arxiv_id": "2511.06267v1",
      "summary": "Collision detection is a core component of robotics applications such as simulation, control, and planning. Traditional algorithms like GJK+EPA compute witness points (i.e., the closest or deepest-penetration pairs between two objects) but are inherently non-differentiable, preventing gradient flow and limiting gradient-based optimization in contact-rich tasks such as grasping and manipulation. Recent work introduced efficient first-order randomized smoothing to make witness points differentiable; however, their direction-based formulation is restricted to convex objects and lacks robustness for complex geometries. In this work, we propose a robust and efficient differentiable collision detection framework that supports both convex and concave objects across diverse scales and configurations. Our method introduces distance-based first-order randomized smoothing, adaptive sampling, and equivalent gradient transport for robust and informative gradient computation. Experiments on complex meshes from DexGraspNet and Objaverse show significant improvements over existing baselines. Finally, we demonstrate a direct application of our method for dexterous grasp synthesis to refine the grasp quality. The code is available at https://github.com/JYChen18/DiffCollision.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-09",
      "updated": "2025-11-09",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06267v1",
      "code_links": [
        {
          "url": "https://github.com/JYChen18/DiffCollision",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "grasping",
            "grasp"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Robust Nearest Neighbour Retrieval Using Targeted Manifold Manipulation",
      "authors": [
        "B. Ghosh",
        "H. Harikumar",
        "S. Rana"
      ],
      "arxiv_id": "2511.06261v2",
      "summary": "Nearest-neighbour retrieval is central to classification and explainable-AI pipelines, but current practice relies on hand-tuning feature layers and distance metrics. We propose Targeted Manifold Manipulation-Nearest Neighbour (TMM-NN), which reconceptualises retrieval by assessing how readily each sample can be nudged into a designated region of the feature manifold; neighbourhoods are defined by a sample's responsiveness to a targeted perturbation rather than absolute geometric distance. TMM-NN implements this through a lightweight, query-specific trigger patch. The patch is added to the query image, and the network is weakly ``backdoored'' so that any input with the patch is steered toward a dummy class. Images similar to the query need only a slight shift and are classified as the dummy class with high probability, while dissimilar ones are less affected. By ranking candidates by this confidence, TMM-NN retrieves the most semantically related neighbours. Robustness analysis and benchmark experiments confirm this trigger-based ranking outperforms traditional metrics under noise and across diverse tasks.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-09",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06261v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "StreamSTGS: Streaming Spatial and Temporal Gaussian Grids for Real-Time Free-Viewpoint Video",
      "authors": [
        "Zhihui Ke",
        "Yuyang Liu",
        "Xiaobo Zhou",
        "Tie Qiu"
      ],
      "arxiv_id": "2511.06046v1",
      "summary": "Streaming free-viewpoint video~(FVV) in real-time still faces significant challenges, particularly in training, rendering, and transmission efficiency. Harnessing superior performance of 3D Gaussian Splatting~(3DGS), recent 3DGS-based FVV methods have achieved notable breakthroughs in both training and rendering. However, the storage requirements of these methods can reach up to $10$MB per frame, making stream FVV in real-time impossible. To address this problem, we propose a novel FVV representation, dubbed StreamSTGS, designed for real-time streaming. StreamSTGS represents a dynamic scene using canonical 3D Gaussians, temporal features, and a deformation field. For high compression efficiency, we encode canonical Gaussian attributes as 2D images and temporal features as a video. This design not only enables real-time streaming, but also inherently supports adaptive bitrate control based on network condition without any extra training. Moreover, we propose a sliding window scheme to aggregate adjacent temporal features to learn local motions, and then introduce a transformer-guided auxiliary training module to learn global motions. On diverse FVV benchmarks, StreamSTGS demonstrates competitive performance on all metrics compared to state-of-the-art methods. Notably, StreamSTGS increases the PSNR by an average of $1$dB while reducing the average frame size to just $170$KB. The code is publicly available on https://github.com/kkkzh/StreamSTGS.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-08",
      "updated": "2025-11-08",
      "comment": "Accepted by AAAI 2026. Code will be released at https://www.github.com/kkkzh/StreamSTGS",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06046v1",
      "code_links": [
        {
          "url": "https://github.com/kkkzh/StreamSTGS",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "3DGS",
            "gaussian splatting"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "MoEGCL: Mixture of Ego-Graphs Contrastive Representation Learning for Multi-View Clustering",
      "authors": [
        "Jian Zhu",
        "Xin Zou",
        "Jun Sun",
        "Cheng Luo",
        "Lei Liu",
        "Lingfang Zeng",
        "Ning Zhang",
        "Bian Wu",
        "Chang Tang",
        "Lirong Dai"
      ],
      "arxiv_id": "2511.05876v3",
      "summary": "In recent years, the advancement of Graph Neural Networks (GNNs) has significantly propelled progress in Multi-View Clustering (MVC). However, existing methods face the problem of coarse-grained graph fusion. Specifically, current approaches typically generate a separate graph structure for each view and then perform weighted fusion of graph structures at the view level, which is a relatively rough strategy. To address this limitation, we present a novel Mixture of Ego-Graphs Contrastive Representation Learning (MoEGCL). It mainly consists of two modules. In particular, we propose an innovative Mixture of Ego-Graphs Fusion (MoEGF), which constructs ego graphs and utilizes a Mixture-of-Experts network to implement fine-grained fusion of ego graphs at the sample level, rather than the conventional view-level fusion. Additionally, we present the Ego Graph Contrastive Learning (EGCL) module to align the fused representation with the view-specific representation. The EGCL module enhances the representation similarity of samples from the same cluster, not merely from the same sample, further boosting fine-grained graph representation. Extensive experiments demonstrate that MoEGCL achieves state-of-the-art results in deep multi-view clustering tasks. The source code is publicly available at https://github.com/HackerHyper/MoEGCL.",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-08",
      "updated": "2025-11-29",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05876v3",
      "code_links": [
        {
          "url": "https://github.com/HackerHyper/MoEGCL",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]representation learning",
            "contrastive learning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Light-Field Dataset for Disparity Based Depth Estimation",
      "authors": [
        "Suresh Nehra",
        "Aupendu Kar",
        "Jayanta Mukhopadhyay",
        "Prabir Kumar Biswas"
      ],
      "arxiv_id": "2511.05866v1",
      "summary": "A Light Field (LF) camera consists of an additional two-dimensional array of micro-lenses placed between the main lens and sensor, compared to a conventional camera. The sensor pixels under each micro-lens receive light from a sub-aperture of the main lens. This enables the image sensor to capture both spatial information and the angular resolution of a scene point. This additional angular information is used to estimate the depth of a 3-D scene. The continuum of virtual viewpoints in light field data enables efficient depth estimation using Epipolar Line Images (EPIs) with robust occlusion handling. However, the trade-off between angular information and spatial information is very critical and depends on the focal position of the camera. To design, develop, implement, and test novel disparity-based light field depth estimation algorithms, the availability of suitable light field image datasets is essential. In this paper, a publicly available light field image dataset is introduced and thoroughly described. We have also demonstrated the effect of focal position on the disparity of a 3-D point as well as the shortcomings of the currently available light field dataset. The proposed dataset contains 285 light field images captured using a Lytro Illum LF camera and 13 synthetic LF images. The proposed dataset also comprises a synthetic dataset with similar disparity characteristics to those of a real light field camera. A real and synthetic stereo light field dataset is also created by using a mechanical gantry system and Blender. The dataset is available at https://github.com/aupendu/light-field-dataset.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-08",
      "updated": "2025-11-08",
      "comment": "This paper has been accepted to ACM ICVGIP 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05866v1",
      "code_links": [
        {
          "url": "https://github.com/aupendu/light-field-dataset",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]depth estimation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Point Cloud Segmentation of Integrated Circuits Package Substrates Surface Defects Using Causal Inference: Dataset Construction and Methodology",
      "authors": [
        "Bingyang Guo",
        "Qiang Zuo",
        "Ruiyun Yu"
      ],
      "arxiv_id": "2511.05853v1",
      "summary": "The effective segmentation of 3D data is crucial for a wide range of industrial applications, especially for detecting subtle defects in the field of integrated circuits (IC). Ceramic package substrates (CPS), as an important electronic material, are essential in IC packaging owing to their superior physical and chemical properties. However, the complex structure and minor defects of CPS, along with the absence of a publically available dataset, significantly hinder the development of CPS surface defect detection. In this study, we construct a high-quality point cloud dataset for 3D segmentation of surface defects in CPS, i.e., CPS3D-Seg, which has the best point resolution and precision compared to existing 3D industrial datasets. CPS3D-Seg consists of 1300 point cloud samples under 20 product categories, and each sample provides accurate point-level annotations. Meanwhile, we conduct a comprehensive benchmark based on SOTA point cloud segmentation algorithms to validate the effectiveness of CPS3D-Seg. Additionally, we propose a novel 3D segmentation method based on causal inference (CINet), which quantifies potential confounders in point clouds through Structural Refine (SR) and Quality Assessment (QA) Modules. Extensive experiments demonstrate that CINet significantly outperforms existing algorithms in both mIoU and accuracy.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-08",
      "updated": "2025-11-08",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05853v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "BOP-ASK: Object-Interaction Reasoning for Vision-Language Models",
      "authors": [
        "Vineet Bhat",
        "Sungsu Kim",
        "Valts Blukis",
        "Greg Heinrich",
        "Prashanth Krishnamurthy",
        "Ramesh Karri",
        "Stan Birchfield",
        "Farshad Khorrami",
        "Jonathan Tremblay"
      ],
      "arxiv_id": "2511.16857v2",
      "summary": "Vision Language Models (VLMs) have achieved impressive performance on spatial reasoning benchmarks, yet these evaluations mask critical weaknesses in understanding object interactions. Current benchmarks test high level relationships ('left of,' 'behind', etc.) but ignore fine-grained spatial understanding needed for real world applications: precise 3D localization, physical compatibility between objects, object affordances and multi step spatial planning. In this work, we present BOP-ASK, a novel large scale dataset for object interaction reasoning for both training and benchmarking. Our data generation pipeline leverages 6D object poses from the Benchmark for Object Pose Estimation (BOP) datasets from which we derive fine grained annotations such as grasp poses, referred object poses, path planning trajectories, relative spatial and depth relationships, and object-to-object relationships. BOP-ASK comprises over 150k images and 33M question answer pairs spanning six tasks (four novel), providing a rich resource for training and evaluating VLMs. We evaluate proprietary and open sourced VLMs, and conduct human evaluations on BOP-ASK-core, a contributed test benchmark. We also release BOP-ASK-lab, an out-of-distribution benchmark with images not sourced from BOP, enabling testing of generalization. Our experiments demonstrate that models trained on BOP-ASK outperform baselines and exhibit emergent capabilities such as precise object and grasp pose estimation, trajectory planning, and fine-grained object-centric spatial reasoning in cluttered environments. We will publicly release our datasets and dataset generation pipeline.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-12-04",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16857v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "grasp"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "pose estimation",
            "localization"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Vorion: A RISC-V GPU with Hardware-Accelerated 3D Gaussian Rendering and Training",
      "authors": [
        "Yipeng Wang",
        "Mengtian Yang",
        "Chieh-pu Lo",
        "Jaydeep P. Kulkarni"
      ],
      "arxiv_id": "2511.16831v1",
      "summary": "3D Gaussian Splatting (3DGS) has recently emerged as a foundational technique for real-time neural rendering, 3D scene generation, volumetric video (4D) capture. However, its rendering and training impose massive computation, making real-time rendering on edge devices and real-time 4D reconstruction on workstations currently infeasible. Given its fixed-function nature and similarity with traditional rasterization, 3DGS presents a strong case for dedicated hardware in the graphics pipeline of next-generation GPUs. This work, Vorion, presents the first GPGPU prototype with hardware-accelerated 3DGS rendering and training. Vorion features scalable architecture, minimal hardware change to traditional rasterizers, z-tiling to increase parallelism, and Gaussian/pixel-centric hybrid dataflow. We prototype the minimal system (8 SIMT cores, 2 Gaussian rasterizer) using TSMC 16nm FinFET technology, which achieves 19 FPS for rendering. The scaled design with 16 rasterizers achieves 38.6 iterations/s for training.",
      "categories": [
        "cs.AR",
        "cs.GR"
      ],
      "primary_category": "cs.AR",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16831v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "3DGS",
            "gaussian splatting"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Investigating Optical Flow Computation: From Local Methods to a Multiresolution Horn-Schunck Implementation with Bilinear Interpolation",
      "authors": [
        "Haytham Ziani"
      ],
      "arxiv_id": "2511.16535v1",
      "summary": "This paper presents an applied analysis of local and global methods, with a focus on the Horn-Schunck algorithm for optical flow computation. We explore the theoretical and practical aspects of local approaches, such as the Lucas-Kanade method, and global techniques such as Horn-Schunck. Additionally, we implement a multiresolution version of the Horn-Schunck algorithm, using bilinear interpolation and prolongation to improve accuracy and convergence. The study investigates the effectiveness of these combined strategies in estimating motion between frames, particularly under varying image conditions.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16535v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]optical flow"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "BoxingVI: A Multi-Modal Benchmark for Boxing Action Recognition and Localization",
      "authors": [
        "Rahul Kumar",
        "Vipul Baghel",
        "Sudhanshu Singh",
        "Bikash Kumar Badatya",
        "Shivam Yadav",
        "Babji Srinivasan",
        "Ravi Hegde"
      ],
      "arxiv_id": "2511.16524v1",
      "summary": "Accurate analysis of combat sports using computer vision has gained traction in recent years, yet the development of robust datasets remains a major bottleneck due to the dynamic, unstructured nature of actions and variations in recording environments. In this work, we present a comprehensive, well-annotated video dataset tailored for punch detection and classification in boxing. The dataset comprises 6,915 high-quality punch clips categorized into six distinct punch types, extracted from 20 publicly available YouTube sparring sessions and involving 18 different athletes. Each clip is manually segmented and labeled to ensure precise temporal boundaries and class consistency, capturing a wide range of motion styles, camera angles, and athlete physiques. This dataset is specifically curated to support research in real-time vision-based action recognition, especially in low-resource and unconstrained environments. By providing a rich benchmark with diverse punch examples, this contribution aims to accelerate progress in movement analysis, automated coaching, and performance assessment within boxing and related domains.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16524v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "LLaVA$^3$: Representing 3D Scenes like a Cubist Painter to Boost 3D Scene Understanding of VLMs",
      "authors": [
        "Doriand Petit",
        "Steve Bourgeois",
        "Vincent Gay-Bellile",
        "Florian Chabot",
        "Loïc Barthe"
      ],
      "arxiv_id": "2511.16454v1",
      "summary": "Developing a multi-modal language model capable of understanding 3D scenes remains challenging due to the limited availability of 3D training data, in contrast to the abundance of 2D datasets used for vision-language models (VLM). As an alternative, we introduce LLaVA$^3$ (pronounced LLaVA-Cube), a novel method that improves the 3D scene understanding capabilities of VLM using only multi-view 2D images and without any fine-tuning. Inspired by Cubist painters, who represented multiple viewpoints of a 3D object within a single picture, we propose to describe the 3D scene for the VLM through omnidirectional visual representations of each object. These representations are derived from an intermediate multi-view 3D reconstruction of the scene. Extensive experiments on 3D VQA and 3D language grounding show that our approach outperforms previous 2D-based VLM solutions.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "Accepted at AAAI'26",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16454v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]scene understanding"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "CylinderDepth: Cylindrical Spatial Attention for Multi-View Consistent Self-Supervised Surround Depth Estimation",
      "authors": [
        "Samer Abualhanud",
        "Christian Grannemann",
        "Max Mehltretter"
      ],
      "arxiv_id": "2511.16428v1",
      "summary": "Self-supervised surround-view depth estimation enables dense, low-cost 3D perception with a 360° field of view from multiple minimally overlapping images. Yet, most existing methods suffer from depth estimates that are inconsistent between overlapping images. Addressing this limitation, we propose a novel geometry-guided method for calibrated, time-synchronized multi-camera rigs that predicts dense, metric, and cross-view-consistent depth. Given the intrinsic and relative orientation parameters, a first depth map is predicted per image and the so-derived 3D points from all images are projected onto a shared unit cylinder, establishing neighborhood relations across different images. This produces a 2D position map for every image, where each pixel is assigned its projected position on the cylinder. Based on these position maps, we apply an explicit, non-learned spatial attention that aggregates features among pixels across images according to their distances on the cylinder, to predict a final depth map per image. Evaluated on the DDAD and nuScenes datasets, our approach improves the consistency of depth estimates across images and the overall depth compared to state-of-the-art methods.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16428v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]depth estimation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Real-Time 3D Object Detection with Inference-Aligned Learning",
      "authors": [
        "Chenyu Zhao",
        "Xianwei Zheng",
        "Zimin Xia",
        "Linwei Yue",
        "Nan Xue"
      ],
      "arxiv_id": "2511.16140v1",
      "summary": "Real-time 3D object detection from point clouds is essential for dynamic scene understanding in applications such as augmented reality, robotics and navigation. We introduce a novel Spatial-prioritized and Rank-aware 3D object detection (SR3D) framework for indoor point clouds, to bridge the gap between how detectors are trained and how they are evaluated. This gap stems from the lack of spatial reliability and ranking awareness during training, which conflicts with the ranking-based prediction selection used as inference. Such a training-inference gap hampers the model's ability to learn representations aligned with inference-time behavior. To address the limitation, SR3D consists of two components tailored to the spatial nature of point clouds during training: a novel spatial-prioritized optimal transport assignment that dynamically emphasizes well-located and spatially reliable samples, and a rank-aware adaptive self-distillation scheme that adaptively injects ranking perception via a self-distillation paradigm. Extensive experiments on ScanNet V2 and SUN RGB-D show that SR3D effectively bridges the training-inference gap and significantly outperforms prior methods in accuracy while maintaining real-time speed.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "Accepted by AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16140v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding",
            "point cloud",
            "navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Clustered Error Correction with Grouped 4D Gaussian Splatting",
      "authors": [
        "Taeho Kang",
        "Jaeyeon Park",
        "Kyungjin Lee",
        "Youngki Lee"
      ],
      "arxiv_id": "2511.16112v1",
      "summary": "Existing 4D Gaussian Splatting (4DGS) methods struggle to accurately reconstruct dynamic scenes, often failing to resolve ambiguous pixel correspondences and inadequate densification in dynamic regions. We address these issues by introducing a novel method composed of two key components: (1) Elliptical Error Clustering and Error Correcting Splat Addition that pinpoints dynamic areas to improve and initialize fitting splats, and (2) Grouped 4D Gaussian Splatting that improves consistency of mapping between splats and represented dynamic objects. Specifically, we classify rendering errors into missing-color and occlusion types, then apply targeted corrections via backprojection or foreground splitting guided by cross-view color consistency. Evaluations on Neural 3D Video and Technicolor datasets demonstrate that our approach significantly improves temporal consistency and achieves state-of-the-art perceptual rendering quality, improving 0.39dB of PSNR on the Technicolor Light Field dataset. Our visualization shows improved alignment between splats and dynamic objects, and the error correction method's capability to identify errors and properly initialize new splats. Our implementation details and source code are available at https://github.com/tho-kn/cem-4dgs.",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "16 pages, 8 figures, SIGGRAPH Asia Conference Papers 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16112v1",
      "code_links": [
        {
          "url": "https://github.com/tho-kn/cem-4dgs",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]gaussian splatting"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "RoMa v2: Harder Better Faster Denser Feature Matching",
      "authors": [
        "Johan Edstedt",
        "David Nordström",
        "Yushan Zhang",
        "Georg Bökman",
        "Jonathan Astermark",
        "Viktor Larsson",
        "Anders Heyden",
        "Fredrik Kahl",
        "Mårten Wadenbäck",
        "Michael Felsberg"
      ],
      "arxiv_id": "2511.15706v2",
      "summary": "Dense feature matching aims to estimate all correspondences between two images of a 3D scene and has recently been established as the gold-standard due to its high accuracy and robustness. However, existing dense matchers still fail or perform poorly for many hard real-world scenarios, and high-precision models are often slow, limiting their applicability. In this paper, we attack these weaknesses on a wide front through a series of systematic improvements that together yield a significantly better model. In particular, we construct a novel matching architecture and loss, which, combined with a curated diverse training distribution, enables our model to solve many complex matching tasks. We further make training faster through a decoupled two-stage matching-then-refinement pipeline, and at the same time, significantly reduce refinement memory usage through a custom CUDA kernel. Finally, we leverage the recent DINOv3 foundation model along with multiple other insights to make the model more robust and unbiased. In our extensive set of experiments we show that the resulting novel matcher sets a new state-of-the-art, being significantly more accurate than its predecessors. Code is available at https://github.com/Parskatt/romav2",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-19",
      "updated": "2025-11-20",
      "comment": "Added acknowledgements, and some minor fixes",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15706v2",
      "code_links": [
        {
          "url": "https://github.com/Parskatt/romav2",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "[T]feature matching"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "6_video_extraction"
      ]
    },
    {
      "title": "RRT*former: Environment-Aware Sampling-Based Motion Planning using Transformer",
      "authors": [
        "Mingyang Feng",
        "Shaoyuan Li",
        "Xiang Yin"
      ],
      "arxiv_id": "2511.15414v1",
      "summary": "We investigate the sampling-based optimal path planning problem for robotics in complex and dynamic environments. Most existing sampling-based algorithms neglect environmental information or the information from previous samples. Yet, these pieces of information are highly informative, as leveraging them can provide better heuristics when sampling the next state. In this paper, we propose a novel sampling-based planning algorithm, called \\emph{RRT*former}, which integrates the standard RRT* algorithm with a Transformer network in a novel way. Specifically, the Transformer is used to extract features from the environment and leverage information from previous samples to better guide the sampling process. Our extensive experiments demonstrate that, compared to existing sampling-based approaches such as RRT*, Neural RRT*, and their variants, our algorithm achieves considerable improvements in both the optimality of the path and sampling efficiency. The code for our implementation is available on https://github.com/fengmingyang666/RRTformer.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "Accepted to IROS 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15414v1",
      "code_links": [
        {
          "url": "https://github.com/fengmingyang666/RRTformer",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]motion planning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Platform-Agnostic Reinforcement Learning Framework for Safe Exploration of Cluttered Environments with Graph Attention",
      "authors": [
        "Gabriele Calzolari",
        "Vidya Sumathy",
        "Christoforos Kanellakis",
        "George Nikolakopoulos"
      ],
      "arxiv_id": "2511.15358v1",
      "summary": "Autonomous exploration of obstacle-rich spaces requires strategies that ensure efficiency while guaranteeing safety against collisions with obstacles. This paper investigates a novel platform-agnostic reinforcement learning framework that integrates a graph neural network-based policy for next-waypoint selection, with a safety filter ensuring safe mobility. Specifically, the neural network is trained using reinforcement learning through the Proximal Policy Optimization (PPO) algorithm to maximize exploration efficiency while minimizing safety filter interventions. Henceforth, when the policy proposes an infeasible action, the safety filter overrides it with the closest feasible alternative, ensuring consistent system behavior. In addition, this paper introduces a reward function shaped by a potential field that accounts for both the agent's proximity to unexplored regions and the expected information gain from reaching them. The proposed framework combines the adaptability of reinforcement learning-based exploration policies with the reliability provided by explicit safety mechanisms. This feature plays a key role in enabling the deployment of learning-based policies on robotic platforms operating in real-world environments. Extensive evaluations in both simulations and experiments performed in a lab environment demonstrate that the approach achieves efficient and safe exploration in cluttered spaces.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "8 pages, 6 figures, submitted to the 2026 IEEE International Conference on Robotics & Automation",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15358v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "PPO"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning",
      "authors": [
        "Yanchen Xu",
        "Ziheng Jiao",
        "Hongyuan Zhang",
        "Xuelong Li"
      ],
      "arxiv_id": "2511.15256v1",
      "summary": "The Group Relative Policy Optimization (GRPO), a reinforcement learning method used to fine-tune large language models (LLMs), has proved its effectiveness in practical applications such as DeepSeek-R1. It raises a question whether GRPO can be generalized to representation learning models. In this paper, we propose Group Relative Policy Optimization for Representation Model (GRPO-RM), and investigate the performance of GRPO-like policy in post-training representation models. Specifically, our method establishes a predefined output set to functionally replace token sequence sampling in LLMs, thereby generating an output group, which is essential for the probability-driven optimization of GRPO. In addition, a specialized reward function is designed to accommodate the properties of representation models. Extensive experiments are conducted on various real-world datasets to validate the effectiveness of our proposed method.",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15256v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "representation learning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Communication-Aware Asynchronous Distributed Trajectory Optimization for UAV Swarm",
      "authors": [
        "Yue Yu",
        "Xiaobo Zheng",
        "Shaoming He"
      ],
      "arxiv_id": "2511.14994v1",
      "summary": "Distributed optimization offers a promising paradigm for trajectory planning in Unmanned Aerial Vehicle (UAV) swarms, yet its deployment in communication-constrained environments remains challenging due to unreliable links and limited data exchange. This paper addresses this issue via a two-tier architecture explicitly designed for operation under communication constraints. We develop a Communication-Aware Asynchronous Distributed Trajectory Optimization (CA-ADTO) framework that integrates Parameterized Differential Dynamic Programming (PDDP) for local trajectory optimization of individual UAVs with an asynchronous Alternating Direction Method of Multipliers (async-ADMM) for swarm-level coordination. The proposed architecture enables fully distributed optimization while substantially reducing communication overhead, making it suitable for real-world scenarios in which reliable connectivity cannot be guaranteed. The method is particularly effective in handling nonlinear dynamics and spatio-temporal coupling under communication constraints.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14994v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]trajectory optimization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "X-WIN: Building Chest Radiograph World Model via Predictive Sensing",
      "authors": [
        "Zefan Yang",
        "Ge Wang",
        "James Hendler",
        "Mannudeep K. Kalra",
        "Pingkun Yan"
      ],
      "arxiv_id": "2511.14918v1",
      "summary": "Chest X-ray radiography (CXR) is an essential medical imaging technique for disease diagnosis. However, as 2D projectional images, CXRs are limited by structural superposition and hence fail to capture 3D anatomies. This limitation makes representation learning and disease diagnosis challenging. To address this challenge, we propose a novel CXR world model named X-WIN, which distills volumetric knowledge from chest computed tomography (CT) by learning to predict its 2D projections in latent space. The core idea is that a world model with internalized knowledge of 3D anatomical structure can predict CXRs under various transformations in 3D space. During projection prediction, we introduce an affinity-guided contrastive alignment loss that leverages mutual similarities to capture rich, correlated information across projections from the same volume. To improve model adaptability, we incorporate real CXRs into training through masked image modeling and employ a domain classifier to encourage statistically similar representations for real and simulated CXRs. Comprehensive experiments show that X-WIN outperforms existing foundation models on diverse downstream tasks using linear probing and few-shot fine-tuning. X-WIN also demonstrates the ability to render 2D projections for reconstructing a 3D CT volume.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14918v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model",
            "representation learning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards",
      "authors": [
        "Chia-Yu Hung",
        "Navonil Majumder",
        "Haoyuan Deng",
        "Liu Renhang",
        "Yankang Ang",
        "Amir Zadeh",
        "Chuan Li",
        "Dorien Herremans",
        "Ziwei Wang",
        "Soujanya Poria"
      ],
      "arxiv_id": "2511.14659v1",
      "summary": "Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "https://declare-lab.github.io/nora-1.5",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14659v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model",
            "flow matching"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "SLAM-AGS: Slide-Label Aware Multi-Task Pretraining Using Adaptive Gradient Surgery in Computational Cytology",
      "authors": [
        "Marco Acerbis",
        "Swarnadip Chatterjee",
        "Christophe Avenel",
        "Joakim Lindblad"
      ],
      "arxiv_id": "2511.14639v1",
      "summary": "Computational cytology faces two major challenges: i) instance-level labels are unreliable and prohibitively costly to obtain, ii) witness rates are extremely low. We propose SLAM-AGS, a Slide-Label-Aware Multitask pretraining framework that jointly optimizes (i) a weakly supervised similarity objective on slide-negative patches and (ii) a self-supervised contrastive objective on slide-positive patches, yielding stronger performance on downstream tasks. To stabilize learning, we apply Adaptive Gradient Surgery to tackle conflicting task gradients and prevent model collapse. We integrate the pretrained encoder into an attention-based Multiple Instance Learning aggregator for bag-level prediction and attention-guided retrieval of the most abnormal instances in a bag. On a publicly available bone-marrow cytology dataset, with simulated witness rates from 10% down to 0.5%, SLAM-AGS improves bag-level F1-Score and Top 400 positive cell retrieval over other pretraining methods, with the largest gains at low witness rates, showing that resolving gradient interference enables stable pretraining and better performance on downstream tasks. To facilitate reproducibility, we share our complete implementation and evaluation framework as open source: https://github.com/Ace95/SLAM-AGS.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "5 pages, 2 figures, Submitted to ISBI2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14639v1",
      "code_links": [
        {
          "url": "https://github.com/Ace95/SLAM-AGS",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]SLAM"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Rethinking the Encoding and Annotating of 3D Bounding Box: Corner-Aware 3D Object Detection from Point Clouds",
      "authors": [
        "Qinghao Meng",
        "Junbo Yin",
        "Jianbing Shen",
        "Yunde Jia"
      ],
      "arxiv_id": "2511.17619v1",
      "summary": "Center-aligned regression remains dominant in LiDAR-based 3D object detection, yet it suffers from fundamental instability: object centers often fall in sparse or empty regions of the bird's-eye-view (BEV) due to the front-surface-biased nature of LiDAR point clouds, leading to noisy and inaccurate bounding box predictions. To circumvent this limitation, we revisit bounding box representation and propose corner-aligned regression, which shifts the prediction target from unstable centers to geometrically informative corners that reside in dense, observable regions. Leveraging the inherent geometric constraints among corners and image 2D boxes, partial parameters of 3D bounding boxes can be recovered from corner annotations, enabling a weakly supervised paradigm without requiring complete 3D labels. We design a simple yet effective corner-aware detection head that can be plugged into existing detectors. Experiments on KITTI show our method improves performance by 3.5% AP over center-based baseline, and achieves 83% of fully supervised accuracy using only BEV corner clicks, demonstrating the effectiveness of our corner-aware regression strategy.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "8 pages, 5 figures, 2 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17619v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Parameter Aware Mamba Model for Multi-task Dense Prediction",
      "authors": [
        "Xinzhuo Yu",
        "Yunzhi Zhuge",
        "Sitong Gong",
        "Lu Zhang",
        "Pingping Zhang",
        "Huchuan Lu"
      ],
      "arxiv_id": "2511.14503v1",
      "summary": "Understanding the inter-relations and interactions between tasks is crucial for multi-task dense prediction. Existing methods predominantly utilize convolutional layers and attention mechanisms to explore task-level interactions. In this work, we introduce a novel decoder-based framework, Parameter Aware Mamba Model (PAMM), specifically designed for dense prediction in multi-task learning setting. Distinct from approaches that employ Transformers to model holistic task relationships, PAMM leverages the rich, scalable parameters of state space models to enhance task interconnectivity. It features dual state space parameter experts that integrate and set task-specific parameter priors, capturing the intrinsic properties of each task. This approach not only facilitates precise multi-task interactions but also allows for the global integration of task priors through the structured state space sequence model (S4). Furthermore, we employ the Multi-Directional Hilbert Scanning method to construct multi-angle feature sequences, thereby enhancing the sequence model's perceptual capabilities for 2D data. Extensive experiments on the NYUD-v2 and PASCAL-Context benchmarks demonstrate the effectiveness of our proposed method. Our code is available at https://github.com/CQC-gogopro/PAMM.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "Accepted to IEEE Transactions on Cybernetics",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14503v1",
      "code_links": [
        {
          "url": "https://github.com/CQC-gogopro/PAMM",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba",
            "state space model"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning",
      "authors": [
        "Xiuxiu Qi",
        "Yu Yang",
        "Jiannong Cao",
        "Luyao Bai",
        "Chongshan Fan",
        "Chengtai Cao",
        "Hongpeng Wang"
      ],
      "arxiv_id": "2511.14396v3",
      "summary": "Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-18",
      "updated": "2025-12-12",
      "comment": "Accepted at AAAI 2026, the Project website is available at https://qhemu.github.io/CCoL/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14396v3",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "bi-manual",
            "bimanual"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Gaussian Splatting-based Low-Rank Tensor Representation for Multi-Dimensional Image Recovery",
      "authors": [
        "Yiming Zeng",
        "Xi-Le Zhao",
        "Wei-Hao Wu",
        "Teng-Yu Ji",
        "Chao Wang"
      ],
      "arxiv_id": "2511.14270v2",
      "summary": "Tensor singular value decomposition (t-SVD) is a promising tool for multi-dimensional image representation, which decomposes a multi-dimensional image into a latent tensor and an accompanying transform matrix. However, two critical limitations of t-SVD methods persist: (1) the approximation of the latent tensor (e.g., tensor factorizations) is coarse and fails to accurately capture spatial local high-frequency information; (2) The transform matrix is composed of fixed basis atoms (e.g., complex exponential atoms in DFT and cosine atoms in DCT) and cannot precisely capture local high-frequency information along the mode-3 fibers. To address these two limitations, we propose a Gaussian Splatting-based Low-rank tensor Representation (GSLR) framework, which compactly and continuously represents multi-dimensional images. Specifically, we leverage tailored 2D Gaussian splatting and 1D Gaussian splatting to generate the latent tensor and transform matrix, respectively. The 2D and 1D Gaussian splatting are indispensable and complementary under this representation framework, which enjoys a powerful representation capability, especially for local high-frequency information. To evaluate the representation ability of the proposed GSLR, we develop an unsupervised GSLR-based multi-dimensional image recovery model. Extensive experiments on multi-dimensional image recovery demonstrate that GSLR consistently outperforms state-of-the-art methods, particularly in capturing local high-frequency information.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-19",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14270v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]gaussian splatting"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "V2VLoc: Robust GNSS-Free Collaborative Perception via LiDAR Localization",
      "authors": [
        "Wenkai Lin",
        "Qiming Xia",
        "Wen Li",
        "Xun Huang",
        "Chenglu Wen"
      ],
      "arxiv_id": "2511.14247v1",
      "summary": "Multi-agents rely on accurate poses to share and align observations, enabling a collaborative perception of the environment. However, traditional GNSS-based localization often fails in GNSS-denied environments, making consistent feature alignment difficult in collaboration. To tackle this challenge, we propose a robust GNSS-free collaborative perception framework based on LiDAR localization. Specifically, we propose a lightweight Pose Generator with Confidence (PGC) to estimate compact pose and confidence representations. To alleviate the effects of localization errors, we further develop the Pose-Aware Spatio-Temporal Alignment Transformer (PASTAT), which performs confidence-aware spatial alignment while capturing essential temporal context. Additionally, we present a new simulation dataset, V2VLoc, which can be adapted for both LiDAR localization and collaborative detection tasks. V2VLoc comprises three subsets: Town1Loc, Town4Loc, and V2VDet. Town1Loc and Town4Loc offer multi-traversal sequences for training in localization tasks, whereas V2VDet is specifically intended for the collaborative detection task. Extensive experiments conducted on the V2VLoc dataset demonstrate that our approach achieves state-of-the-art performance under GNSS-denied conditions. We further conduct extended experiments on the real-world V2V4Real dataset to validate the effectiveness and generalizability of PASTAT.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "AAAI2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14247v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "FlexiCup: Wireless Multimodal Suction Cup with Dual-Zone Vision-Tactile Sensing",
      "authors": [
        "Junhao Gong",
        "Shoujie Li",
        "Kit-Wa Sou",
        "Changqing Guo",
        "Hourong Huang",
        "Tong Wu",
        "Yifan Xie",
        "Chenxin Liang",
        "Chuqiao Lyu",
        "Xiaojun Liang",
        "Wenbo Ding"
      ],
      "arxiv_id": "2511.14139v1",
      "summary": "Conventional suction cups lack sensing capabilities for contact-aware manipulation in unstructured environments. This paper presents FlexiCup, a fully wireless multimodal suction cup that integrates dual-zone vision-tactile sensing. The central zone dynamically switches between vision and tactile modalities via illumination control for contact detection, while the peripheral zone provides continuous spatial awareness for approach planning. FlexiCup supports both vacuum and Bernoulli suction modes through modular mechanical configurations, achieving complete wireless autonomy with onboard computation and power. We validate hardware versatility through dual control paradigms. Modular perception-driven grasping across structured surfaces with varying obstacle densities demonstrates comparable performance between vacuum (90.0% mean success) and Bernoulli (86.7% mean success) modes. Diffusion-based end-to-end learning achieves 73.3% success on inclined transport and 66.7% on orange extraction tasks. Ablation studies confirm that multi-head attention coordinating dual-zone observations provides 13% improvements for contact-aware manipulation. Hardware designs and firmware are available at https://anonymous.4open.science/api/repo/FlexiCup-DA7D/file/index.html?v=8f531b44.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14139v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "grasping",
            "grasp"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "SMGeo: Cross-View Object Geo-Localization with Grid-Level Mixture-of-Experts",
      "authors": [
        "Fan Zhang",
        "Haoyuan Ren",
        "Fei Ma",
        "Qiang Yin",
        "Yongsheng Zhou"
      ],
      "arxiv_id": "2511.14093v1",
      "summary": "Cross-view object Geo-localization aims to precisely pinpoint the same object across large-scale satellite imagery based on drone images. Due to significant differences in viewpoint and scale, coupled with complex background interference, traditional multi-stage \"retrieval-matching\" pipelines are prone to cumulative errors. To address this, we present SMGeo, a promptable end-to-end transformer-based model for object Geo-localization. This model supports click prompting and can output object Geo-localization in real time when prompted to allow for interactive use. The model employs a fully transformer-based architecture, utilizing a Swin-Transformer for joint feature encoding of both drone and satellite imagery and an anchor-free transformer detection head for coordinate regression. In order to better capture both inter-modal and intra-view dependencies, we introduce a grid-level sparse Mixture-of-Experts (GMoE) into the cross-view encoder, allowing it to adaptively activate specialized experts according to the content, scale and source of each grid. We also employ an anchor-free detection head for coordinate regression, directly predicting object locations via heat-map supervision in the reference images. This approach avoids scale bias and matching complexity introduced by predefined anchor boxes. On the drone-to-satellite task, SMGeo achieves leading performance in accuracy at IoU=0.25 and mIoU metrics (e.g., 87.51%, 62.50%, and 61.45% in the test set, respectively), significantly outperforming representative methods such as DetGeo (61.97%, 57.66%, and 54.05%, respectively). Ablation studies demonstrate complementary gains from shared encoding, query-guided fusion, and grid-level sparse mixture-of-experts.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14093v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Error-Driven Scene Editing for 3D Grounding in Large Language Models",
      "authors": [
        "Yue Zhang",
        "Zun Wang",
        "Han Lin",
        "Jialu Li",
        "Jianing Yang",
        "Yonatan Bitton",
        "Idan Szpektor",
        "Mohit Bansal"
      ],
      "arxiv_id": "2511.14086v1",
      "summary": "Despite recent progress in 3D-LLMs, they remain limited in accurately grounding language to visual and spatial elements in 3D environments. This limitation stems in part from training data that focuses on language reasoning rather than spatial understanding due to scarce 3D resources, leaving inherent grounding biases unresolved. To address this, we propose 3D scene editing as a key mechanism to generate precise visual counterfactuals that mitigate these biases through fine-grained spatial manipulation, without requiring costly scene reconstruction or large-scale 3D data collection. Furthermore, to make these edits targeted and directly address the specific weaknesses of the model, we introduce DEER-3D, an error-driven framework following a structured \"Decompose, Diagnostic Evaluation, Edit, and Re-train\" workflow, rather than broadly or randomly augmenting data as in conventional approaches. Specifically, upon identifying a grounding failure of the 3D-LLM, our framework first diagnoses the exact predicate-level error (e.g., attribute or spatial relation). It then executes minimal, predicate-aligned 3D scene edits, such as recoloring or repositioning, to produce targeted counterfactual supervision for iterative model fine-tuning, significantly enhancing grounding accuracy. We evaluate our editing pipeline across multiple benchmarks for 3D grounding and scene understanding tasks, consistently demonstrating improvements across all evaluated datasets through iterative refinement. DEER-3D underscores the effectiveness of targeted, error-driven scene editing in bridging linguistic reasoning capabilities with spatial grounding in 3D LLMs.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "Code: https://github.com/zhangyuejoslin/Deer-3D",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14086v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene reconstruction",
            "scene understanding"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "BIM-Discrepancy-Driven Active Sensing for Risk-Aware UAV-UGV Navigation",
      "authors": [
        "Hesam Mojtahedi",
        "Reza Akhavian"
      ],
      "arxiv_id": "2511.14037v1",
      "summary": "This paper presents a BIM-discrepancy-driven active sensing framework for cooperative navigation between unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) in dynamic construction environments. Traditional navigation approaches rely on static Building Information Modeling (BIM) priors or limited onboard perception. In contrast, our framework continuously fuses real-time LiDAR data from aerial and ground robots with BIM priors to maintain an evolving 2D occupancy map. We quantify navigation safety through a unified corridor-risk metric integrating occupancy uncertainty, BIM-map discrepancy, and clearance. When risk exceeds safety thresholds, the UAV autonomously re-scans affected regions to reduce uncertainty and enable safe replanning. Validation in PX4-Gazebo simulation with Robotec GPU LiDAR demonstrates that risk-triggered re-scanning reduces mean corridor risk by 58% and map entropy by 43% compared to static BIM navigation, while maintaining clearance margins above 0.4 m. Compared to frontier-based exploration, our approach achieves similar uncertainty reduction in half the mission time. These results demonstrate that integrating BIM priors with risk-adaptive aerial sensing enables scalable, uncertainty-aware autonomy for construction robotics.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14037v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "RISE: Single Static Radar-based Indoor Scene Understanding",
      "authors": [
        "Kaichen Zhou",
        "Laura Dodds",
        "Sayed Saad Afzal",
        "Fadel Adib"
      ],
      "arxiv_id": "2511.14019v1",
      "summary": "Robust and privacy-preserving indoor scene understanding remains a fundamental open problem. While optical sensors such as RGB and LiDAR offer high spatial fidelity, they suffer from severe occlusions and introduce privacy risks in indoor environments. In contrast, millimeter-wave (mmWave) radar preserves privacy and penetrates obstacles, but its inherently low spatial resolution makes reliable geometric reasoning difficult.\n  We introduce RISE, the first benchmark and system for single-static-radar indoor scene understanding, jointly targeting layout reconstruction and object detection. RISE is built upon the key insight that multipath reflections, traditionally treated as noise, encode rich geometric cues. To exploit this, we propose a Bi-Angular Multipath Enhancement that explicitly models Angle-of-Arrival and Angle-of-Departure to recover secondary (ghost) reflections and reveal invisible structures. On top of these enhanced observations, a simulation-to-reality Hierarchical Diffusion framework transforms fragmented radar responses into complete layout reconstruction and object detection.\n  Our benchmark contains 50,000 frames collected across 100 real indoor trajectories, forming the first large-scale dataset dedicated to radar-based indoor scene understanding. Extensive experiments show that RISE reduces the Chamfer Distance by 60% (down to 16 cm) compared to the state of the art in layout reconstruction, and delivers the first mmWave-based object detection, achieving 58% IoU. These results establish RISE as a new foundation for geometry-aware and privacy-preserving indoor scene understanding using a single static radar.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14019v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]scene understanding"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Hessians in Birkhoff-Theoretic Trajectory Optimization",
      "authors": [
        "I. M. Ross"
      ],
      "arxiv_id": "2511.13963v1",
      "summary": "This paper derives various Hessians associated with Birkhoff-theoretic methods for trajectory optimization. According to a theorem proved in this paper, approximately 80% of the eigenvalues are contained in the narrow interval [-2, 4] for all Birkhoff-discretized optimal control problems. A preliminary analysis of computational complexity is also presented with further discussions on the grand challenge of solving a million point trajectory optimization problem.",
      "categories": [
        "math.OC",
        "cs.MS",
        "cs.RO",
        "math.NA"
      ],
      "primary_category": "math.OC",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "This paper appeared as an Engineering Note in the J. Guid. Control & Dynamics",
      "doi": "10.2514/1.G008778",
      "journal_ref": "Journal of Guidance Control and Dynamics, Vol. 48, No. 9, September 2025, 2105--2112",
      "pdf_url": "https://arxiv.org/pdf/2511.13963v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]trajectory optimization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Free-Form Scene Editor: Enabling Multi-Round Object Manipulation like in a 3D Engine",
      "authors": [
        "Xincheng Shuai",
        "Zhenyuan Qin",
        "Henghui Ding",
        "Dacheng Tao"
      ],
      "arxiv_id": "2511.13713v1",
      "summary": "Recent advances in text-to-image (T2I) diffusion models have significantly improved semantic image editing, yet most methods fall short in performing 3D-aware object manipulation. In this work, we present FFSE, a 3D-aware autoregressive framework designed to enable intuitive, physically-consistent object editing directly on real-world images. Unlike previous approaches that either operate in image space or require slow and error-prone 3D reconstruction, FFSE models editing as a sequence of learned 3D transformations, allowing users to perform arbitrary manipulations, such as translation, scaling, and rotation, while preserving realistic background effects (e.g., shadows, reflections) and maintaining global scene consistency across multiple editing rounds. To support learning of multi-round 3D-aware object manipulation, we introduce 3DObjectEditor, a hybrid dataset constructed from simulated editing sequences across diverse objects and scenes, enabling effective training under multi-round and dynamic conditions. Extensive experiments show that the proposed FFSE significantly outperforms existing methods in both single-round and multi-round 3D-aware editing scenarios.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "AAAI 2026, Project Page: https://henghuiding.com/FFSE/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13713v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "EL3DD: Extended Latent 3D Diffusion for Language Conditioned Multitask Manipulation",
      "authors": [
        "Jonas Bode",
        "Raphael Memmesheimer",
        "Sven Behnke"
      ],
      "arxiv_id": "2511.13312v1",
      "summary": "Acting in human environments is a crucial capability for general-purpose robots, necessitating a robust understanding of natural language and its application to physical tasks. This paper seeks to harness the capabilities of diffusion models within a visuomotor policy framework that merges visual and textual inputs to generate precise robotic trajectories. By employing reference demonstrations during training, the model learns to execute manipulation tasks specified through textual commands within the robot's immediate environment. The proposed research aims to extend an existing model by leveraging improved embeddings, and adapting techniques from diffusion models for image generation. We evaluate our methods on the CALVIN dataset, proving enhanced performance on various manipulation tasks and an increased long-horizon success rate when multiple tasks are executed in sequence. Our approach reinforces the usefulness of diffusion models and contributes towards general multitask manipulation.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "10 pages; 2 figures; 1 table. Prprint submitted to the European Robotics Forum 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13312v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Towards Metric-Aware Multi-Person Mesh Recovery by Jointly Optimizing Human Crowd in Camera Space",
      "authors": [
        "Kaiwen Wang",
        "Kaili Zheng",
        "Yiming Shi",
        "Chenyi Guo",
        "Ji Wu"
      ],
      "arxiv_id": "2511.13282v2",
      "summary": "Multi-person human mesh recovery from a single image is a challenging task, hindered by the scarcity of in-the-wild training data. Prevailing in-the-wild human mesh pseudo-ground-truth (pGT) generation pipelines are single-person-centric, where each human is processed individually without joint optimization. This oversight leads to a lack of scene-level consistency, producing individuals with conflicting depths and scales within the same image. To address this, we introduce Depth-conditioned Translation Optimization (DTO), a novel optimization-based method that jointly refines the camera-space translations of all individuals in a crowd. By leveraging anthropometric priors on human height and depth cues from a monocular depth estimator, DTO solves for a scene-consistent placement of all subjects within a principled Maximum a posteriori (MAP) framework. Applying DTO to the 4D-Humans dataset, we construct DTO-Humans, a new large-scale pGT dataset of 0.56M high-quality, scene-consistent multi-person images, featuring dense crowds with an average of 4.8 persons per image. Furthermore, we propose Metric-Aware HMR, an end-to-end network that directly estimates human mesh and camera parameters in metric scale. This is enabled by a camera branch and a relative metric loss that enforces plausible relative scales. Extensive experiments demonstrate that our method achieves state-of-the-art performance on relative depth reasoning and human mesh recovery. Code is available at: https://github.com/gouba2333/MA-HMR.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13282v2",
      "code_links": [
        {
          "url": "https://github.com/gouba2333/MA-HMR",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "monocular depth"
          ],
          "score": 2.0
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "human mesh recovery",
            "HMR"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam",
        "6_video_extraction"
      ]
    },
    {
      "title": "Force-Aware 3D Contact Modeling for Stable Grasp Generation",
      "authors": [
        "Zhuo Chen",
        "Zhongqun Zhang",
        "Yihua Cheng",
        "Ales Leonardis",
        "Hyung Jin Chang"
      ],
      "arxiv_id": "2511.13247v1",
      "summary": "Contact-based grasp generation plays a crucial role in various applications. Recent methods typically focus on the geometric structure of objects, producing grasps with diverse hand poses and plausible contact points. However, these approaches often overlook the physical attributes of the grasp, specifically the contact force, leading to reduced stability of the grasp. In this paper, we focus on stable grasp generation using explicit contact force predictions. First, we define a force-aware contact representation by transforming the normal force value into discrete levels and encoding it using a one-hot vector. Next, we introduce force-aware stability constraints. We define the stability problem as an acceleration minimization task and explicitly relate stability with contact geometry by formulating the underlying physical constraints. Finally, we present a pose optimizer that systematically integrates our contact representation and stability constraints to enable stable grasp generation. We show that these constraints can help identify key contact points for stability which provide effective initialization and guidance for optimization towards a stable grasp. Experiments are carried out on two public benchmarks, showing that our method brings about 20% improvement in stability metrics and adapts well to novel objects.",
      "categories": [
        "cs.GR"
      ],
      "primary_category": "cs.GR",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "AAAI'26 Camera Ready. Project page at https://chzh9311.github.io/force-aware-grasp-project/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13247v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]grasp"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer",
      "authors": [
        "Yonghui Yu",
        "Jiahang Cai",
        "Xun Wang",
        "Wenwu Yang"
      ],
      "arxiv_id": "2511.13208v2",
      "summary": "Existing multi-person video pose estimation methods typically adopt a two-stage pipeline: detecting individuals in each frame, followed by temporal modeling for single person pose estimation. This design relies on heuristic operations such as detection, RoI cropping, and non-maximum suppression (NMS), limiting both accuracy and efficiency. In this paper, we present a fully end-to-end framework for multi-person 2D pose estimation in videos, effectively eliminating heuristic operations. A key challenge is to associate individuals across frames under complex and overlapping temporal trajectories. To address this, we introduce a novel Pose-Aware Video transformEr Network (PAVE-Net), which features a spatial encoder to model intra-frame relations and a spatiotemporal pose decoder to capture global dependencies across frames. To achieve accurate temporal association, we propose a pose-aware attention mechanism that enables each pose query to selectively aggregate features corresponding to the same individual across consecutive frames. Additionally, we explicitly model spatiotemporal dependencies among pose keypoints to improve accuracy. Notably, our approach is the first end-to-end method for multi-frame 2D human pose estimation. Extensive experiments show that PAVE-Net substantially outperforms prior image-based end-to-end methods, achieving a 6.0 mAP improvement on PoseTrack2017, and delivers accuracy competitive with state-of-the-art two-stage video based approaches, while offering significant gains in efficiency. Project page: https://github.com/zgspose/PAVENet.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-12-02",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13208v2",
      "code_links": [
        {
          "url": "https://github.com/zgspose/PAVENet",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]pose estimation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "CapeNext: Rethinking and Refining Dynamic Support Information for Category-Agnostic Pose Estimation",
      "authors": [
        "Yu Zhu",
        "Dan Zeng",
        "Shuiwang Li",
        "Qijun Zhao",
        "Qiaomu Shen",
        "Bo Tang"
      ],
      "arxiv_id": "2511.13102v2",
      "summary": "Recent research in Category-Agnostic Pose Estimation (CAPE) has adopted fixed textual keypoint description as semantic prior for two-stage pose matching frameworks. While this paradigm enhances robustness and flexibility by disentangling the dependency of support images, our critical analysis reveals two inherent limitations of static joint embedding: (1) polysemy-induced cross-category ambiguity during the matching process(e.g., the concept \"leg\" exhibiting divergent visual manifestations across humans and furniture), and (2) insufficient discriminability for fine-grained intra-category variations (e.g., posture and fur discrepancies between a sleeping white cat and a standing black cat). To overcome these challenges, we propose a new framework that innovatively integrates hierarchical cross-modal interaction with dual-stream feature refinement, enhancing the joint embedding with both class-level and instance-specific cues from textual description and specific images. Experiments on the MP-100 dataset demonstrate that, regardless of the network backbone, CapeNext consistently outperforms state-of-the-art CAPE methods by a large margin.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-12-15",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13102v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]pose estimation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "RobustGait: Robustness Analysis for Appearance Based Gait Recognition",
      "authors": [
        "Reeshoon Sayera",
        "Akash Kumar",
        "Sirshapan Mitra",
        "Prudvi Kamtam",
        "Yogesh S Rawat"
      ],
      "arxiv_id": "2511.13065v1",
      "summary": "Appearance-based gait recognition have achieved strong performance on controlled datasets, yet systematic evaluation of its robustness to real-world corruptions and silhouette variability remains lacking. We present RobustGait, a framework for fine-grained robustness evaluation of appearance-based gait recognition systems. RobustGait evaluation spans four dimensions: the type of perturbation (digital, environmental, temporal, occlusion), the silhouette extraction method (segmentation and parsing networks), the architectural capacities of gait recognition models, and various deployment scenarios. The benchmark introduces 15 corruption types at 5 severity levels across CASIA-B, CCPG, and SUSTech1K, with in-the-wild validation on MEVID, and evaluates six state-of-the-art gait systems. We came across several exciting insights. First, applying noise at the RGB level better reflects real-world degradation, and reveal how distortions propagate through silhouette extraction to the downstream gait recognition systems. Second, gait accuracy is highly sensitive to silhouette extractor biases, revealing an overlooked source of benchmark bias. Third, robustness is dependent on both the type of perturbation and the architectural design. Finally, we explore robustness-enhancing strategies, showing that noise-aware training and knowledge distillation improve performance and move toward deployment-ready systems.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "IEEE WACV'26 Main Conference",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13065v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]gait"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "MGCA-Net: Multi-Grained Category-Aware Network for Open-Vocabulary Temporal Action Localization",
      "authors": [
        "Zhenying Fang",
        "Richang Hong"
      ],
      "arxiv_id": "2511.13039v1",
      "summary": "Open-Vocabulary Temporal Action Localization (OV-TAL) aims to recognize and localize instances of any desired action categories in videos without explicitly curating training data for all categories. Existing methods mostly recognize action categories at a single granularity, which degrades the recognition accuracy of both base and novel action categories. To address these issues, we propose a Multi-Grained Category-Aware Network (MGCA-Net) comprising a localizer, an action presence predictor, a conventional classifier, and a coarse-to-fine classifier. Specifically, the localizer localizes category-agnostic action proposals. For these action proposals, the action presence predictor estimates the probability that they belong to an action instance. At the same time, the conventional classifier predicts the probability of each action proposal over base action categories at the snippet granularity. Novel action categories are recognized by the coarse-to-fine classifier, which first identifies action presence at the video granularity. Finally, it assigns each action proposal to one category from the coarse categories at the proposal granularity. Through coarse-to-fine category awareness for novel actions and the conventional classifier's awareness of base actions, multi-grained category awareness is achieved, effectively enhancing localization performance. Comprehensive evaluations on the THUMOS'14 and ActivityNet-1.3 benchmarks demonstrate that our method achieves state-of-the-art performance. Furthermore, our MGCA-Net achieves state-of-the-art results under the Zero-Shot Temporal Action Localization setting.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "12 pages, 3 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13039v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Inertia-Informed Orientation Priors for Event-Based Optical Flow Estimation",
      "authors": [
        "Pritam P. Karmokar",
        "William J. Beksi"
      ],
      "arxiv_id": "2511.12961v1",
      "summary": "Event cameras, by virtue of their working principle, directly encode motion within a scene. Many learning-based and model-based methods exist that estimate event-based optical flow, however the temporally dense yet spatially sparse nature of events poses significant challenges. To address these issues, contrast maximization (CM) is a prominent model-based optimization methodology that estimates the motion trajectories of events within an event volume by optimally warping them. Since its introduction, the CM framework has undergone a series of refinements by the computer vision community. Nonetheless, it remains a highly non-convex optimization problem. In this paper, we introduce a novel biologically-inspired hybrid CM method for event-based optical flow estimation that couples visual and inertial motion cues. Concretely, we propose the use of orientation maps, derived from camera 3D velocities, as priors to guide the CM process. The orientation maps provide directional guidance and constrain the space of estimated motion trajectories. We show that this orientation-guided formulation leads to improved robustness and convergence in event-based optical flow estimation. The evaluation of our approach on the MVSEC, DSEC, and ECD datasets yields superior accuracy scores over the state of the art.",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "13 pages, 9 figures, and 3 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12961v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]optical flow"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation",
      "authors": [
        "Dexin Zuo",
        "Ang Li",
        "Wei Wang",
        "Wenxian Yu",
        "Danping Zou"
      ],
      "arxiv_id": "2511.12919v3",
      "summary": "Object 6D pose estimation, a crucial task for robotics and augmented reality applications, becomes particularly challenging when dealing with novel objects whose 3D models are not readily available. To reduce dependency on 3D models, recent studies have explored one-reference-based pose estimation, which requires only a single reference view instead of a complete 3D model. However, existing methods that rely on real-valued coordinate regression suffer from limited global consistency due to the local nature of convolutional architectures and face challenges in symmetric or occluded scenarios owing to a lack of uncertainty modeling. We present CoordAR, a novel autoregressive framework for one-reference 6D pose estimation of unseen objects. CoordAR formulates 3D-3D correspondences between the reference and query views as a map of discrete tokens, which is obtained in an autoregressive and probabilistic manner. To enable accurate correspondence regression, CoordAR introduces 1) a novel coordinate map tokenization that enables probabilistic prediction over discretized 3D space; 2) a modality-decoupled encoding strategy that separately encodes RGB appearance and coordinate cues; and 3) an autoregressive transformer decoder conditioned on both position-aligned query features and the partially generated token sequence. With these novel mechanisms, CoordAR significantly outperforms existing methods on multiple benchmarks and demonstrates strong robustness to symmetry, occlusion, and other challenges in real-world tests.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-12-15",
      "comment": "7 pages, accepted by AAAI 2026 (oral)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12919v3",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]pose estimation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Deep Imbalanced Multi-Target Regression: 3D Point Cloud Voxel Content Estimation in Simulated Forests",
      "authors": [
        "Amirhossein Hassanzadeh",
        "Bartosz Krawczyk",
        "Michael Saunders",
        "Rob Wible",
        "Keith Krause",
        "Dimah Dera",
        "Jan van Aardt"
      ],
      "arxiv_id": "2511.12740v1",
      "summary": "Voxelization is an effective approach to reduce the computational cost of processing Light Detection and Ranging (LiDAR) data, yet it results in a loss of fine-scale structural information. This study explores whether low-level voxel content information, specifically target occupancy percentage within a voxel, can be inferred from high-level voxelized LiDAR point cloud data collected from Digital Imaging and remote Sensing Image Generation (DIRSIG) software. In our study, the targets include bark, leaf, soil, and miscellaneous materials. We propose a multi-target regression approach in the context of imbalanced learning using Kernel Point Convolutions (KPConv). Our research leverages cost-sensitive learning to address class imbalance called density-based relevance (DBR). We employ weighted Mean Saquared Erorr (MSE), Focal Regression (FocalR), and regularization to improve the optimization of KPConv. This study performs a sensitivity analysis on the voxel size (0.25 - 2 meters) to evaluate the effect of various grid representations in capturing the nuances of the forest. This sensitivity analysis reveals that larger voxel sizes (e.g., 2 meters) result in lower errors due to reduced variability, while smaller voxel sizes (e.g., 0.25 or 0.5 meter) exhibit higher errors, particularly within the canopy, where variability is greatest. For bark and leaf targets, error values at smaller voxel size datasets (0.25 and 0.5 meter) were significantly higher than those in larger voxel size datasets (2 meters), highlighting the difficulty in accurately estimating within-canopy voxel content at fine resolutions. This suggests that the choice of voxel size is application-dependent. Our work fills the gap in deep imbalance learning models for multi-target regression and simulated datasets for 3D LiDAR point clouds of forests.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-16",
      "updated": "2025-11-16",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12740v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Appreciate the View: A Task-Aware Evaluation Framework for Novel View Synthesis",
      "authors": [
        "Saar Stern",
        "Ido Sobol",
        "Or Litany"
      ],
      "arxiv_id": "2511.12675v1",
      "summary": "The goal of Novel View Synthesis (NVS) is to generate realistic images of a given content from unseen viewpoints. But how can we trust that a generated image truly reflects the intended transformation? Evaluating its reliability remains a major challenge. While recent generative models, particularly diffusion-based approaches, have significantly improved NVS quality, existing evaluation metrics struggle to assess whether a generated image is both realistic and faithful to the source view and intended viewpoint transformation. Standard metrics, such as pixel-wise similarity and distribution-based measures, often mis-rank incorrect results as they fail to capture the nuanced relationship between the source image, viewpoint change, and generated output. We propose a task-aware evaluation framework that leverages features from a strong NVS foundation model, Zero123, combined with a lightweight tuning step to enhance discrimination. Using these features, we introduce two complementary evaluation metrics: a reference-based score, $D_{\\text{PRISM}}$, and a reference-free score, $\\text{MMD}_{\\text{PRISM}}$. Both reliably identify incorrect generations and rank models in agreement with human preference studies, addressing a fundamental gap in NVS evaluation. Our framework provides a principled and practical approach to assessing synthesis quality, paving the way for more reliable progress in novel view synthesis. To further support this goal, we apply our reference-free metric to six NVS methods across three benchmarks: Toys4K, Google Scanned Objects (GSO), and OmniObject3D, where $\\text{MMD}_{\\text{PRISM}}$ produces a clear and stable ranking, with lower scores consistently indicating stronger models.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-16",
      "updated": "2025-11-16",
      "comment": "3DV 2026. Project page: https://saarst.github.io/appreciate-the-view-website",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12675v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]novel view synthesis"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Toward Real-world Text Image Forgery Localization: Structured and Interpretable Data Synthesis",
      "authors": [
        "Zeqin Yu",
        "Haotao Xie",
        "Jian Zhang",
        "Jiangqun Ni",
        "Wenkan Su",
        "Jiwu Huang"
      ],
      "arxiv_id": "2511.12658v1",
      "summary": "Existing Text Image Forgery Localization (T-IFL) methods often suffer from poor generalization due to the limited scale of real-world datasets and the distribution gap caused by synthetic data that fails to capture the complexity of real-world tampering. To tackle this issue, we propose Fourier Series-based Tampering Synthesis (FSTS), a structured and interpretable framework for synthesizing tampered text images. FSTS first collects 16,750 real-world tampering instances from five representative tampering types, using a structured pipeline that records human-performed editing traces via multi-format logs (e.g., video, PSD, and editing logs). By analyzing these collected parameters and identifying recurring behavioral patterns at both individual and population levels, we formulate a hierarchical modeling framework. Specifically, each individual tampering parameter is represented as a compact combination of basis operation-parameter configurations, while the population-level distribution is constructed by aggregating these behaviors. Since this formulation draws inspiration from the Fourier series, it enables an interpretable approximation using basis functions and their learned weights. By sampling from this modeled distribution, FSTS synthesizes diverse and realistic training data that better reflect real-world forgery traces. Extensive experiments across four evaluation protocols demonstrate that models trained with FSTS data achieve significantly improved generalization on real-world datasets. Dataset is available at \\href{https://github.com/ZeqinYu/FSTS}{Project Page}.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-16",
      "updated": "2025-11-16",
      "comment": "NeurIPS 2025 D&B Track",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12658v1",
      "code_links": [
        {
          "url": "https://github.com/ZeqinYu/FSTS",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Evaluating Model-Agnostic Meta-Learning on MetaWorld ML10 Benchmark: Fast Adaptation in Robotic Manipulation Tasks",
      "authors": [
        "Sanjar Atamuradov"
      ],
      "arxiv_id": "2511.12383v1",
      "summary": "Meta-learning algorithms enable rapid adaptation to new tasks with minimal data, a critical capability for real-world robotic systems. This paper evaluates Model-Agnostic Meta-Learning (MAML) combined with Trust Region Policy Optimization (TRPO) on the MetaWorld ML10 benchmark, a challenging suite of ten diverse robotic manipulation tasks. We implement and analyze MAML-TRPO's ability to learn a universal initialization that facilitates few-shot adaptation across semantically different manipulation behaviors including pushing, picking, and drawer manipulation. Our experiments demonstrate that MAML achieves effective one-shot adaptation with clear performance improvements after a single gradient update, reaching final success rates of 21.0% on training tasks and 13.2% on held-out test tasks. However, we observe a generalization gap that emerges during meta-training, where performance on test tasks plateaus while training task performance continues to improve. Task-level analysis reveals high variance in adaptation effectiveness, with success rates ranging from 0% to 80% across different manipulation skills. These findings highlight both the promise and current limitations of gradient-based meta-learning for diverse robotic manipulation, and suggest directions for future work in task-aware adaptation and structured policy architectures.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-15",
      "updated": "2025-11-15",
      "comment": "7 pages, 5 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12383v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Changes in Real Time: Online Scene Change Detection with Multi-View Fusion",
      "authors": [
        "Chamuditha Jayanga Galappaththige",
        "Jason Lai",
        "Lloyd Windrim",
        "Donald Dansereau",
        "Niko Sünderhauf",
        "Dimity Miller"
      ],
      "arxiv_id": "2511.12370v2",
      "summary": "Online Scene Change Detection (SCD) is an extremely challenging problem that requires an agent to detect relevant changes on the fly while observing the scene from unconstrained viewpoints. Existing online SCD methods are significantly less accurate than offline approaches. We present the first online SCD approach that is pose-agnostic, label-free, and ensures multi-view consistency, while operating at over 10 FPS and achieving new state-of-the-art performance, surpassing even the best offline approaches. Our method introduces a new self-supervised fusion loss to infer scene changes from multiple cues and observations, PnP-based fast pose estimation against the reference scene, and a fast change-guided update strategy for the 3D Gaussian Splatting scene representation. Extensive experiments on complex real-world datasets demonstrate that our approach outperforms both online and offline baselines.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-15",
      "updated": "2025-12-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12370v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "gaussian splatting",
            "pose estimation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "EgoCogNav: Cognition-aware Human Egocentric Navigation",
      "authors": [
        "Zhiwen Qiu",
        "Ziang Liu",
        "Wenqian Niu",
        "Tapomayukh Bhattacharjee",
        "Saleh Kalantari"
      ],
      "arxiv_id": "2511.17581v1",
      "summary": "Modeling the cognitive and experiential factors of human navigation is central to deepening our understanding of human-environment interaction and to enabling safe social navigation and effective assistive wayfinding. Most existing methods focus on forecasting motions in fully observed scenes and often neglect human factors that capture how people feel and respond to space. To address this gap, We propose EgoCogNav, a multimodal egocentric navigation framework that predicts perceived path uncertainty as a latent state and jointly forecasts trajectories and head motion by fusing scene features with sensory cues. To facilitate research in the field, we introduce the Cognition-aware Egocentric Navigation (CEN) dataset consisting 6 hours of real-world egocentric recordings capturing diverse navigation behaviors in real-world scenarios. Experiments show that EgoCogNav learns the perceived uncertainty that highly correlates with human-like behaviors such as scanning, hesitation, and backtracking while generalizing to unseen environments.",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-15",
      "updated": "2025-11-15",
      "comment": "11 pages, 4 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17581v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Locomotion in CAVE: Enhancing Immersion through Full-Body Motion",
      "authors": [
        "Xiaohui Li",
        "Xiaolong Liu",
        "Zhongchen Shi",
        "Wei Chen",
        "Liang Xie",
        "Meng Gai",
        "Jun Cao",
        "Suxia Zhang",
        "Erwei Yin"
      ],
      "arxiv_id": "2511.12251v1",
      "summary": "Cave Automatic Virtual Environment (CAVE) is one of the virtual reality (VR) immersive devices currently used to present virtual environments. However, the locomotion methods in the CAVE are limited by unnatural interaction methods, severely hindering the user experience and immersion in the CAVE. We proposed a locomotion framework for CAVE environments aimed at enhancing the immersive locomotion experience through optimized human motion recognition technology. Firstly, we construct a four-sided display CAVE system, then through the dynamic method based on Perspective-n-Point to calibrate the camera, using the obtained camera intrinsics and extrinsic parameters, and an action recognition architecture to get the action category. At last, transform the action category to a graphical workstation that renders display effects on the screen. We designed a user study to validate the effectiveness of our method. Compared to the traditional methods, our method has significant improvements in realness and self-presence in the virtual environment, effectively reducing motion sickness.",
      "categories": [
        "cs.GR"
      ],
      "primary_category": "cs.GR",
      "published": "2025-11-15",
      "updated": "2025-11-15",
      "comment": "",
      "doi": "10.2139/ssrn.5626635",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12251v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]locomotion"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Innovative Design of Multi-functional Supernumerary Robotic Limbs with Ellipsoid Workspace Optimization",
      "authors": [
        "Jun Huo",
        "Jian Huang",
        "Jie Zuo",
        "Bo Yang",
        "Zhongzheng Fu",
        "Xi Li",
        "Samer Mohammed"
      ],
      "arxiv_id": "2511.12186v1",
      "summary": "Supernumerary robotic limbs (SRLs) offer substantial potential in both the rehabilitation of hemiplegic patients and the enhancement of functional capabilities for healthy individuals. Designing a general-purpose SRL device is inherently challenging, particularly when developing a unified theoretical framework that meets the diverse functional requirements of both upper and lower limbs. In this paper, we propose a multi-objective optimization (MOO) design theory that integrates grasping workspace similarity, walking workspace similarity, braced force for sit-to-stand (STS) movements, and overall mass and inertia. A geometric vector quantification method is developed using an ellipsoid to represent the workspace, aiming to reduce computational complexity and address quantification challenges. The ellipsoid envelope transforms workspace points into ellipsoid attributes, providing a parametric description of the workspace. Furthermore, the STS static braced force assesses the effectiveness of force transmission. The overall mass and inertia restricts excessive link length. To facilitate rapid and stable convergence of the model to high-dimensional irregular Pareto fronts, we introduce a multi-subpopulation correction firefly algorithm. This algorithm incorporates a strategy involving attractive and repulsive domains to effectively handle the MOO task. The optimized solution is utilized to redesign the prototype for experimentation to meet specified requirements. Six healthy participants and two hemiplegia patients participated in real experiments. Compared to the pre-optimization results, the average grasp success rate improved by 7.2%, while the muscle activity during walking and STS tasks decreased by an average of 12.7% and 25.1%, respectively. The proposed design theory offers an efficient option for the design of multi-functional SRL mechanisms.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-15",
      "updated": "2025-11-15",
      "comment": "",
      "doi": "10.1109/TRO.2025.3588763",
      "journal_ref": "IEEE Transactions on Robotics, vol. 41, pp. 4699-4718, 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.12186v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "walking",
            "grasping",
            "grasp"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Rethinking Multimodal Point Cloud Completion: A Completion-by-Correction Perspective",
      "authors": [
        "Wang Luo",
        "Di Wu",
        "Hengyuan Na",
        "Yinlin Zhu",
        "Miao Hu",
        "Guocong Quan"
      ],
      "arxiv_id": "2511.12170v2",
      "summary": "Point cloud completion aims to reconstruct complete 3D shapes from partial observations, which is a challenging problem due to severe occlusions and missing geometry. Despite recent advances in multimodal techniques that leverage complementary RGB images to compensate for missing geometry, most methods still follow a Completion-by-Inpainting paradigm, synthesizing missing structures from fused latent features. We empirically show that this paradigm often results in structural inconsistencies and topological artifacts due to limited geometric and semantic constraints. To address this, we rethink the task and propose a more robust paradigm, termed Completion-by-Correction, which begins with a topologically complete shape prior generated by a pretrained image-to-3D model and performs feature-space correction to align it with the partial observation. This paradigm shifts completion from unconstrained synthesis to guided refinement, enabling structurally consistent and observation-aligned reconstruction. Building upon this paradigm, we introduce PGNet, a multi-stage framework that conducts dual-feature encoding to ground the generative prior, synthesizes a coarse yet structurally aligned scaffold, and progressively refines geometric details via hierarchical correction. Experiments on the ShapeNetViPC dataset demonstrate the superiority of PGNet over state-of-the-art baselines in terms of average Chamfer Distance (-23.5%) and F-score (+7.1%).",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-15",
      "updated": "2025-12-01",
      "comment": "Accepted by AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12170v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Game-Theoretic Safe Multi-Agent Motion Planning with Reachability Analysis for Dynamic and Uncertain Environments (Extended Version)",
      "authors": [
        "Wenbin Mai",
        "Minghui Liwang",
        "Xinlei Yi",
        "Xiaoyu Xia",
        "Seyyedali Hosseinalipour",
        "Xianbin Wang"
      ],
      "arxiv_id": "2511.12160v1",
      "summary": "Ensuring safe, robust, and scalable motion planning for multi-agent systems in dynamic and uncertain environments is a persistent challenge, driven by complex inter-agent interactions, stochastic disturbances, and model uncertainties. To overcome these challenges, particularly the computational complexity of coupled decision-making and the need for proactive safety guarantees, we propose a Reachability-Enhanced Dynamic Potential Game (RE-DPG) framework, which integrates game-theoretic coordination into reachability analysis. This approach formulates multi-agent coordination as a dynamic potential game, where the Nash equilibrium (NE) defines optimal control strategies across agents. To enable scalability and decentralized execution, we develop a Neighborhood-Dominated iterative Best Response (ND-iBR) scheme, built upon an iterated $\\varepsilon$-BR (i$\\varepsilon$-BR) process that guarantees finite-step convergence to an $\\varepsilon$-NE. This allows agents to compute strategies based on local interactions while ensuring theoretical convergence guarantees. Furthermore, to ensure safety under uncertainty, we integrate a Multi-Agent Forward Reachable Set (MA-FRS) mechanism into the cost function, explicitly modeling uncertainty propagation and enforcing collision avoidance constraints. Through both simulations and real-world experiments in 2D and 3D environments, we validate the effectiveness of RE-DPG across diverse operational scenarios.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-15",
      "updated": "2025-11-15",
      "comment": "12 pages, 9 figures",
      "doi": "10.1109/TII.2025.3627632",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12160v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]motion planning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Point Cloud Quantization through Multimodal Prompting for 3D Understanding",
      "authors": [
        "Hongxuan Li",
        "Wencheng Zhu",
        "Huiying Xu",
        "Xinzhong Zhu",
        "Pengfei Zhu"
      ],
      "arxiv_id": "2511.12079v2",
      "summary": "Vector quantization has emerged as a powerful tool in large-scale multimodal models, unifying heterogeneous representations through discrete token encoding. However, its effectiveness hinges on robust codebook design. Current prototype-based approaches relying on trainable vectors or clustered centroids fall short in representativeness and interpretability, even as multimodal alignment demonstrates its promise in vision-language models. To address these limitations, we propose a simple multimodal prompting-driven quantization framework for point cloud analysis. Our methodology is built upon two core insights: 1) Text embeddings from pre-trained models inherently encode visual semantics through many-to-one contrastive alignment, naturally serving as robust prototype priors; and 2) Multimodal prompts enable adaptive refinement of these prototypes, effectively mitigating vision-language semantic gaps. The framework introduces a dual-constrained quantization space, enforced by compactness and separation regularization, which seamlessly integrates visual and prototype features, resulting in hybrid representations that jointly encode geometric and semantic information. Furthermore, we employ Gumbel-Softmax relaxation to achieve differentiable discretization while maintaining quantization sparsity. Extensive experiments on the ModelNet40 and ScanObjectNN datasets clearly demonstrate the superior effectiveness of the proposed method.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-15",
      "updated": "2025-11-19",
      "comment": "Accepted by AAAI 2026. 11 pages, 7 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12079v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "UniABG: Unified Adversarial View Bridging and Graph Correspondence for Unsupervised Cross-View Geo-Localization",
      "authors": [
        "Cuiqun Chen",
        "Qi Chen",
        "Bin Yang",
        "Xingyi Zhang"
      ],
      "arxiv_id": "2511.12054v1",
      "summary": "Cross-view geo-localization (CVGL) matches query images ($\\textit{e.g.}$, drone) to geographically corresponding opposite-view imagery ($\\textit{e.g.}$, satellite). While supervised methods achieve strong performance, their reliance on extensive pairwise annotations limits scalability. Unsupervised alternatives avoid annotation costs but suffer from noisy pseudo-labels due to intrinsic cross-view domain gaps. To address these limitations, we propose $\\textit{UniABG}$, a novel dual-stage unsupervised cross-view geo-localization framework integrating adversarial view bridging with graph-based correspondence calibration. Our approach first employs View-Aware Adversarial Bridging (VAAB) to model view-invariant features and enhance pseudo-label robustness. Subsequently, Heterogeneous Graph Filtering Calibration (HGFC) refines cross-view associations by constructing dual inter-view structure graphs, achieving reliable view correspondence. Extensive experiments demonstrate state-of-the-art unsupervised performance, showing that UniABG improves Satellite $\\rightarrow$ Drone AP by +10.63\\% on University-1652 and +16.73\\% on SUES-200, even surpassing supervised baselines. The source code is available at https://github.com/chenqi142/UniABG",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-15",
      "updated": "2025-11-15",
      "comment": "Accepted as Oral Presentation at AAAI 2026. 10 pages, 9 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12054v1",
      "code_links": [
        {
          "url": "https://github.com/chenqi142/UniABG",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "SRSplat: Feed-Forward Super-Resolution Gaussian Splatting from Sparse Multi-View Images",
      "authors": [
        "Xinyuan Hu",
        "Changyue Shi",
        "Chuxiao Yang",
        "Minghao Chen",
        "Jiajun Ding",
        "Tao Wei",
        "Chen Wei",
        "Zhou Yu",
        "Min Tan"
      ],
      "arxiv_id": "2511.12040v1",
      "summary": "Feed-forward 3D reconstruction from sparse, low-resolution (LR) images is a crucial capability for real-world applications, such as autonomous driving and embodied AI. However, existing methods often fail to recover fine texture details. This limitation stems from the inherent lack of high-frequency information in LR inputs. To address this, we propose \\textbf{SRSplat}, a feed-forward framework that reconstructs high-resolution 3D scenes from only a few LR views. Our main insight is to compensate for the deficiency of texture information by jointly leveraging external high-quality reference images and internal texture cues. We first construct a scene-specific reference gallery, generated for each scene using Multimodal Large Language Models (MLLMs) and diffusion models. To integrate this external information, we introduce the \\textit{Reference-Guided Feature Enhancement (RGFE)} module, which aligns and fuses features from the LR input images and their reference twin image. Subsequently, we train a decoder to predict the Gaussian primitives using the multi-view fused feature obtained from \\textit{RGFE}. To further refine predicted Gaussian primitives, we introduce \\textit{Texture-Aware Density Control (TADC)}, which adaptively adjusts Gaussian density based on the internal texture richness of the LR inputs. Extensive experiments demonstrate that our SRSplat outperforms existing methods on various datasets, including RealEstate10K, ACID, and DTU, and exhibits strong cross-dataset and cross-resolution generalization capabilities.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-15",
      "updated": "2025-11-15",
      "comment": "AAAI2026-Oral. Project Page: https://xinyuanhu66.github.io/SRSplat/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12040v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]gaussian splatting"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Switching control of underactuated multi-channel systems with input constraints for cooperative manipulation",
      "authors": [
        "Dongjae Lee",
        "Dimos V. Dimarogonas",
        "H. Jin Kim"
      ],
      "arxiv_id": "2511.22810v1",
      "summary": "This work presents an event-triggered switching control framework for a class of nonlinear underactuated multi-channel systems with input constraints. These systems are inspired by cooperative manipulation tasks involving underactuation, where multiple underactuated agents collaboratively push or pull an object to a target pose. Unlike existing approaches for multi-channel systems, our method addresses underactuation and the potential loss of controllability by additionally addressing channel assignment of agents. To simultaneously account for channel assignment, input constraints, and stabilization, we formulate the control problem as a Mixed Integer Linear Programming and derive sufficient conditions for its feasibility. To improve real-time computation efficiency, we introduce an event-triggered control scheme that maintains stability even between switching events through a quadratic programming-based stabilizing controller. We theoretically establish the semi-global exponential stability of the proposed method and the asymptotic stability of its extension to nonprehensile cooperative manipulation under noninstantaneous switching. The proposed framework is further validated through numerical simulations on 2D and 3D free-flyer systems and multi-robot nonprehensile pushing tasks.",
      "categories": [
        "eess.SY",
        "cs.RO"
      ],
      "primary_category": "eess.SY",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "14 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22810v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Distracted Robot: How Visual Clutter Undermine Robotic Manipulation",
      "authors": [
        "Amir Rasouli",
        "Montgomery Alban",
        "Sajjad Pakdamansavoji",
        "Zhiyuan Li",
        "Zhanguang Zhang",
        "Aaron Wu",
        "Xuan Zhao"
      ],
      "arxiv_id": "2511.22780v1",
      "summary": "In this work, we propose an evaluation protocol for examining the performance of robotic manipulation policies in cluttered scenes. Contrary to prior works, we approach evaluation from a psychophysical perspective, therefore we use a unified measure of clutter that accounts for environmental factors as well as the distractors quantity, characteristics, and arrangement. Using this measure, we systematically construct evaluation scenarios in both hyper-realistic simulation and real-world and conduct extensive experimentation on manipulation policies, in particular vision-language-action (VLA) models. Our experiments highlight the significant impact of scene clutter, lowering the performance of the policies, by as much as 34% and show that despite achieving similar average performance across the tasks, different VLA policies have unique vulnerabilities and a relatively low agreement on success scenarios. We further show that our clutter measure is an effective indicator of performance degradation and analyze the impact of distractors in terms of their quantity and occluding influence. At the end, we show that finetuning on enhanced data, although effective, does not equally remedy all negative impacts of clutter on performance.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "12 figures, 2 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22780v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Beyond Success: Refining Elegant Robot Manipulation from Mixed-Quality Data via Just-in-Time Intervention",
      "authors": [
        "Yanbo Mao",
        "Jianlong Fu",
        "Ruoxuan Zhang",
        "Hongxia Xie",
        "Meibao Yao"
      ],
      "arxiv_id": "2511.22555v1",
      "summary": "Vision-Language-Action (VLA) models have enabled notable progress in general-purpose robotic manipulation, yet their learned policies often exhibit variable execution quality. We attribute this variability to the mixed-quality nature of human demonstrations, where the implicit principles that govern how actions should be carried out are only partially satisfied. To address this challenge, we introduce the LIBERO-Elegant benchmark with explicit criteria for evaluating execution quality. Using these criteria, we develop a decoupled refinement framework that improves execution quality without modifying or retraining the base VLA policy. We formalize Elegant Execution as the satisfaction of Implicit Task Constraints (ITCs) and train an Elegance Critic via offline Calibrated Q-Learning to estimate the expected quality of candidate actions. At inference time, a Just-in-Time Intervention (JITI) mechanism monitors critic confidence and intervenes only at decision-critical moments, providing selective, on-demand refinement. Experiments on LIBERO-Elegant and real-world manipulation tasks show that the learned Elegance Critic substantially improves execution quality, even on unseen tasks. The proposed model enables robotic control that values not only whether tasks succeed, but also how they are performed.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22555v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Motion-to-Motion Latency Measurement Framework for Connected and Autonomous Vehicle Teleoperation",
      "authors": [
        "François Provost",
        "Faisal Hawlader",
        "Mehdi Testouri",
        "Raphaël Frank"
      ],
      "arxiv_id": "2511.22467v1",
      "summary": "Latency is a key performance factor for the teleoperation of Connected and Autonomous Vehicles (CAVs). It affects how quickly an operator can perceive changes in the driving environment and apply corrective actions. Most existing work focuses on Glass-to-Glass (G2G) latency, which captures delays only in the video pipeline. However, there is no standard method for measuring Motion-to-Motion (M2M) latency, defined as the delay between the physical steering movement of the remote operator and the corresponding steering motion in the vehicle. This paper presents an M2M latency measurement framework that uses Hall-effect sensors and two synchronized Raspberry Pi~5 devices. The system records interrupt-based timestamps on both sides to estimate M2M latency, independently of the underlying teleoperation architecture. Precision tests show an accuracy of 10--15~ms, while field results indicate that actuator delays dominate M2M latency, with median values above 750~ms.",
      "categories": [
        "cs.PF",
        "cs.RO"
      ],
      "primary_category": "cs.PF",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22467v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]teleoperation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "RoadSceneBench: A Lightweight Benchmark for Mid-Level Road Scene Understanding",
      "authors": [
        "Xiyan Liu",
        "Han Wang",
        "Yuhu Wang",
        "Junjie Cai",
        "Zhe Cao",
        "Jianzhong Yang",
        "Zhen Lu"
      ],
      "arxiv_id": "2511.22466v1",
      "summary": "Understanding mid-level road semantics, which capture the structural and contextual cues that link low-level perception to high-level planning, is essential for reliable autonomous driving and digital map construction. However, existing benchmarks primarily target perception tasks such as detection or segmentation, overlooking the reasoning capabilities required to infer road topology and dynamic scene structure. To address this gap, we present RoadSceneBench, a lightweight yet information-rich benchmark designed to evaluate and advance visual reasoning in complex road environments. Unlike large-scale perception datasets, RoadSceneBench emphasizes relational understanding and structural consistency, encouraging models to capture the underlying logic of real-world road scenes. Furthermore, to enhance reasoning reliability, we propose Hierarchical Relational Reward Propagation with Temporal Consistency (HRRP-T), a training framework for Vision-Language Models (VLMs) in which reward signals adaptively promote spatial coherence and semantic alignment throughout the reasoning process. This paradigm enables models to move beyond static recognition toward geometry-aware and temporally consistent reasoning. Extensive experiments demonstrate that our method achieves state-of-the-art performance across diverse road configurations. RoadSceneBench thus provides a compact yet powerful foundation for studying mid-level road semantics and fostering structure-aware autonomous perception. Our dataset is available at https://github.com/XiyanLiu/RoadSceneBench.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22466v1",
      "code_links": [
        {
          "url": "https://github.com/XiyanLiu/RoadSceneBench",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]scene understanding"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Improving Sparse IMU-based Motion Capture with Motion Label Smoothing",
      "authors": [
        "Zhaorui Meng",
        "Lu Yin",
        "Yangqing Hou",
        "Anjun Chen",
        "Shihui Guo",
        "Yipeng Qin"
      ],
      "arxiv_id": "2511.22288v1",
      "summary": "Sparse Inertial Measurement Units (IMUs) based human motion capture has gained significant momentum, driven by the adaptation of fundamental AI tools such as recurrent neural networks (RNNs) and transformers that are tailored for temporal and spatial modeling. Despite these achievements, current research predominantly focuses on pipeline and architectural designs, with comparatively little attention given to regularization methods, highlighting a critical gap in developing a comprehensive AI toolkit for this task. To bridge this gap, we propose motion label smoothing, a novel method that adapts the classic label smoothing strategy from classification to the sparse IMU-based motion capture task. Specifically, we first demonstrate that a naive adaptation of label smoothing, including simply blending a uniform vector or a ``uniform'' motion representation (e.g., dataset-average motion or a canonical T-pose), is suboptimal; and argue that a proper adaptation requires increasing the entropy of the smoothed labels. Second, we conduct a thorough analysis of human motion labels, identifying three critical properties: 1) Temporal Smoothness, 2) Joint Correlation, and 3) Low-Frequency Dominance, and show that conventional approaches to entropy enhancement (e.g., blending Gaussian noise) are ineffective as they disrupt these properties. Finally, we propose the blend of a novel skeleton-based Perlin noise for motion label smoothing, designed to raise label entropy while satisfying motion properties. Extensive experiments applying our motion label smoothing to three state-of-the-art methods across four real-world IMU datasets demonstrate its effectiveness and robust generalization (plug-and-play) capability.",
      "categories": [
        "cs.GR"
      ],
      "primary_category": "cs.GR",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "Accepted by AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22288v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "[T]IMU-based motion"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "6_video_extraction"
      ]
    },
    {
      "title": "MLATC: Fast Hierarchical Topological Mapping from 3D LiDAR Point Clouds Based on Adaptive Resonance Theory",
      "authors": [
        "Ryosuke Ofuchi",
        "Yuichiro Toda",
        "Naoki Masuyama",
        "Takayuki Matsuno"
      ],
      "arxiv_id": "2511.22238v1",
      "summary": "This paper addresses the problem of building global topological maps from 3D LiDAR point clouds for autonomous mobile robots operating in large-scale, dynamic, and unknown environments. Adaptive Resonance Theory-based Topological Clustering with Different Topologies (ATC-DT) builds global topological maps represented as graphs while mitigating catastrophic forgetting during sequential processing. However, its winner selection mechanism relies on an exhaustive nearest-neighbor search over all existing nodes, leading to scalability limitations as the map grows. To address this challenge, we propose a hierarchical extension called Multi-Layer ATC (MLATC). MLATC organizes nodes into a hierarchy, enabling the nearest-neighbor search to proceed from coarse to fine resolutions, thereby drastically reducing the number of distance evaluations per query. The number of layers is not fixed in advance. MLATC employs an adaptive layer addition mechanism that automatically deepens the hierarchy when lower layers become saturated, keeping the number of user-defined hyperparameters low. Simulation experiments on synthetic large-scale environments show that MLATC accelerates topological map building compared to the original ATC-DT and exhibits a sublinear, approximately logarithmic scaling of search time with respect to the number of nodes. Experiments on campus-scale real-world LiDAR datasets confirm that MLATC maintains a millisecond-level per-frame runtime and enables real-time global topological map building in large-scale environments, significantly outperforming the original ATC-DT in terms of computational efficiency.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22238v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "MRI-Based Brain Age Estimation with Supervised Contrastive Learning of Continuous Representation",
      "authors": [
        "Simon Joseph Clément Crête",
        "Marta Kersten-Oertel",
        "Yiming Xiao"
      ],
      "arxiv_id": "2511.22102v1",
      "summary": "MRI-based brain age estimation models aim to assess a subject's biological brain age based on information, such as neuroanatomical features. Various factors, including neurodegenerative diseases, can accelerate brain aging and measuring this phenomena could serve as a potential biomarker for clinical applications. While deep learning (DL)-based regression has recently attracted major attention, existing approaches often fail to capture the continuous nature of neuromorphological changes, potentially resulting in sub-optimal feature representation and results. To address this, we propose to use supervised contrastive learning with the recent Rank-N-Contrast (RNC) loss to estimate brain age based on widely used T1w structural MRI for the first time and leverage Grad-RAM to visually explain regression results. Experiments show that our proposed method achieves a mean absolute error (MAE) of 4.27 years and an $R^2$ of 0.93 with a limited dataset of training samples, significantly outperforming conventional deep regression with the same ResNet backbone while performing better or comparably with the state-of-the-art methods with significantly larger training data. Furthermore, Grad-RAM revealed more nuanced features related to age regression with the RNC loss than conventional deep regression. As an exploratory study, we employed the proposed method to estimate the gap between the biological and chronological brain ages in Alzheimer's Disease and Parkinson's disease patients, and revealed the correlation between the brain age gap and disease severity, demonstrating its potential as a biomarker in neurodegenerative disorders.",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22102v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "MAE",
            "[T]contrastive learning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "SwordRiding: A Unified Navigation Framework for Quadrotors in Unknown Complex Environments via Online Guiding Vector Fields",
      "authors": [
        "Xuchen Liu",
        "Ruocheng Li",
        "Bin Xin",
        "Weijia Yao",
        "Qigeng Duan",
        "Jinqiang Cui",
        "Ben M. Chen",
        "Jie Chen"
      ],
      "arxiv_id": "2511.22043v1",
      "summary": "Although quadrotor navigation has achieved high performance in trajectory planning and control, real-time adaptability in unknown complex environments remains a core challenge. This difficulty mainly arises because most existing planning frameworks operate in an open-loop manner, making it hard to cope with environmental uncertainties such as wind disturbances or external perturbations. This paper presents a unified real-time navigation framework for quadrotors in unknown complex environments, based on the online construction of guiding vector fields (GVFs) from discrete reference path points. In the framework, onboard perception modules build a Euclidean Signed Distance Field (ESDF) representation of the environment, which enables obstacle awareness and path distance evaluation. The system first generates discrete, collision-free path points using a global planner, and then parameterizes them via uniform B-splines to produce a smooth and physically feasible reference trajectory. An adaptive GVF is then synthesized from the ESDF and the optimized B-spline trajectory. Unlike conventional approaches, the method adopts a closed-loop navigation paradigm, which significantly enhances robustness under external disturbances. Compared with conventional GVF methods, the proposed approach directly accommodates discretized paths and maintains compatibility with standard planning algorithms. Extensive simulations and real-world experiments demonstrate improved robustness against external disturbances and superior real-time performance.",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "For an experimental demo, see https://www.youtube.com/watch?v=tKYCg266c4o. For the lemma proof, see https://github.com/SmartGroupSystems/GVF_close_loop_planning/blob/main/proofs.md",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22043v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "PathReasoning: A multimodal reasoning agent for query-based ROI navigation on whole-slide images",
      "authors": [
        "Kunpeng Zhang",
        "Hanwen Xu",
        "Sheng Wang"
      ],
      "arxiv_id": "2511.21902v1",
      "summary": "Deciphering tumor microenvironment from Whole Slide Images (WSIs) is intriguing as it is key to cancer diagnosis, prognosis and treatment response. While these gigapixel images on one hand offer a comprehensive portrait of cancer, on the other hand, the extremely large size, as much as more than 10 billion pixels, make it challenging and time-consuming to navigate to corresponding regions to support diverse clinical inspection. Inspired by pathologists who conducted navigation on WSIs with a combination of sampling, reasoning and self-reflection, we proposed \"PathReasoning\", a multi-modal reasoning agent that iteratively navigates across WSIs through multiple rounds of reasoning and refinements. Specifically, starting with randomly sampled candidate regions, PathReasoning reviews current selections with self-reflection, reasoning over the correspondence between visual observations and clinical questions, and concludes by proposing new regions to explore. Across rounds, PathReasoning builds a reasoning chain that gradually directs attention to diagnostically relevant areas. PathReasoning turns each whole slide into a sequence of question-guided views, allowing the model to efficiently find informative ROIs within a fixed number of steps, without the need for dense pixel-level annotations. PathReasoning can substantially outperform strong ROI-selection approaches by 6.7% and 3.1% of AUROC on subtyping and longitudinal analysis tasks. The high-quality ROIs further support accurate report generation on breast cancer, significantly outperforming the standard GPT-4o by 10% in accuracy. PathReasoning prioritizes question-specific regions and constructs interpretable reasoning chains, supporting efficient slide review, consistent diagnostic interpretations, comprehensive reporting, and evidence traceability in digital pathology.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21902v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Massively Parallel Imitation Learning of Mouse Forelimb Musculoskeletal Reaching Dynamics",
      "authors": [
        "Eric Leonardis",
        "Akira Nagamori",
        "Ayesha Thanawalla",
        "Yuanjia Yang",
        "Joshua Park",
        "Hutton Saunders",
        "Eiman Azim",
        "Talmo Pereira"
      ],
      "arxiv_id": "2511.21848v1",
      "summary": "The brain has evolved to effectively control the body, and in order to understand the relationship we need to model the sensorimotor transformations underlying embodied control. As part of a coordinated effort, we are developing a general-purpose platform for behavior-driven simulation modeling high fidelity behavioral dynamics, biomechanics, and neural circuit architectures underlying embodied control. We present a pipeline for taking kinematics data from the neuroscience lab and creating a pipeline for recapitulating those natural movements in a biomechanical model. We implement a imitation learning framework to perform a dexterous forelimb reaching task with a musculoskeletal model in a simulated physics environment. The mouse arm model is currently training at faster than 1 million training steps per second due to GPU acceleration with JAX and Mujoco-MJX. We present results that indicate that adding naturalistic constraints on energy and velocity lead to simulated musculoskeletal activity that better predict real EMG signals. This work provides evidence to suggest that energy and control constraints are critical to modeling musculoskeletal motor control.",
      "categories": [
        "cs.LG",
        "cs.RO",
        "q-bio.NC",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "Accepted at NeurIPS 2025 Workshop Data on the Brain & Mind: Concrete Applications of AI to Neuroscience and Cognitive Science. 12 pages, 4 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21848v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning",
            "MuJoCo"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Uncertainty Quantification for Visual Object Pose Estimation",
      "authors": [
        "Lorenzo Shaikewitz",
        "Charis Georgiou",
        "Luca Carlone"
      ],
      "arxiv_id": "2511.21666v1",
      "summary": "Quantifying the uncertainty of an object's pose estimate is essential for robust control and planning. Although pose estimation is a well-studied robotics problem, attaching statistically rigorous uncertainty is not well understood without strict distributional assumptions. We develop distribution-free pose uncertainty bounds about a given pose estimate in the monocular setting. Our pose uncertainty only requires high probability noise bounds on pixel detections of 2D semantic keypoints on a known object. This noise model induces an implicit, non-convex set of pose uncertainty constraints. Our key contribution is SLUE (S-Lemma Uncertainty Estimation), a convex program to reduce this set to a single ellipsoidal uncertainty bound that is guaranteed to contain the true object pose with high probability. SLUE solves a relaxation of the minimum volume bounding ellipsoid problem inspired by the celebrated S-lemma. It requires no initial guess of the bound's shape or size and is guaranteed to contain the true object pose with high probability. For tighter uncertainty bounds at the same confidence, we extend SLUE to a sum-of-squares relaxation hierarchy which is guaranteed to converge to the minimum volume ellipsoidal uncertainty bound for a given set of keypoint constraints. We show this pose uncertainty bound can easily be projected to independent translation and axis-angle orientation bounds. We evaluate SLUE on two pose estimation datasets and a real-world drone tracking scenario. Compared to prior work, SLUE generates substantially smaller translation bounds and competitive orientation bounds. We release code at https://github.com/MIT-SPARK/PoseUncertaintySets.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "18 pages, 9 figures. Code available: https://github.com/MIT-SPARK/PoseUncertaintySets",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21666v1",
      "code_links": [
        {
          "url": "https://github.com/MIT-SPARK/PoseUncertaintySets",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]pose estimation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "VacuumVLA: Boosting VLA Capabilities via a Unified Suction and Gripping Tool for Complex Robotic Manipulation",
      "authors": [
        "Hui Zhou",
        "Siyuan Huang",
        "Minxing Li",
        "Hao Zhang",
        "Lue Fan",
        "Shaoshuai Shi"
      ],
      "arxiv_id": "2511.21557v1",
      "summary": "Vision Language Action models have significantly advanced general purpose robotic manipulation by harnessing large scale pretrained vision and language representations. Among existing approaches, a majority of current VLA systems employ parallel two finger grippers as their default end effectors. However, such grippers face inherent limitations in handling certain real world tasks such as wiping glass surfaces or opening drawers without handles due to insufficient contact area or lack of adhesion. To overcome these challenges, we present a low cost, integrated hardware design that combines a mechanical two finger gripper with a vacuum suction unit, enabling dual mode manipulation within a single end effector. Our system supports flexible switching or synergistic use of both modalities, expanding the range of feasible tasks. We validate the efficiency and practicality of our design within two state of the art VLA frameworks: DexVLA and Pi0. Experimental results demonstrate that with the proposed hybrid end effector, robots can successfully perform multiple complex tasks that are infeasible for conventional two finger grippers alone. All hardware designs and controlling systems will be released.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "8 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21557v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Merge and Bound: Direct Manipulations on Weights for Class Incremental Learning",
      "authors": [
        "Taehoon Kim",
        "Donghwan Jang",
        "Bohyung Han"
      ],
      "arxiv_id": "2511.21490v1",
      "summary": "We present a novel training approach, named Merge-and-Bound (M&B) for Class Incremental Learning (CIL), which directly manipulates model weights in the parameter space for optimization. Our algorithm involves two types of weight merging: inter-task weight merging and intra-task weight merging. Inter-task weight merging unifies previous models by averaging the weights of models from all previous stages. On the other hand, intra-task weight merging facilitates the learning of current task by combining the model parameters within current stage. For reliable weight merging, we also propose a bounded update technique that aims to optimize the target model with minimal cumulative updates and preserve knowledge from previous tasks; this strategy reveals that it is possible to effectively obtain new models near old ones, reducing catastrophic forgetting. M&B is seamlessly integrated into existing CIL methods without modifying architecture components or revising learning objectives. We extensively evaluate our algorithm on standard CIL benchmarks and demonstrate superior performance compared to state-of-the-art methods.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21490v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "SurgMLLMBench: A Multimodal Large Language Model Benchmark Dataset for Surgical Scene Understanding",
      "authors": [
        "Tae-Min Choi",
        "Tae Kyeong Jeong",
        "Garam Kim",
        "Jaemin Lee",
        "Yeongyoon Koh",
        "In Cheul Choi",
        "Jae-Ho Chung",
        "Jong Woong Park",
        "Juyoun Park"
      ],
      "arxiv_id": "2511.21339v1",
      "summary": "Recent advances in multimodal large language models (LLMs) have highlighted their potential for medical and surgical applications. However, existing surgical datasets predominantly adopt a Visual Question Answering (VQA) format with heterogeneous taxonomies and lack support for pixel-level segmentation, limiting consistent evaluation and applicability. We present SurgMLLMBench, a unified multimodal benchmark explicitly designed for developing and evaluating interactive multimodal LLMs for surgical scene understanding, including the newly collected Micro-surgical Artificial Vascular anastomosIS (MAVIS) dataset. It integrates pixel-level instrument segmentation masks and structured VQA annotations across laparoscopic, robot-assisted, and micro-surgical domains under a unified taxonomy, enabling comprehensive evaluation beyond traditional VQA tasks and richer visual-conversational interactions. Extensive baseline experiments show that a single model trained on SurgMLLMBench achieves consistent performance across domains and generalizes effectively to unseen datasets. SurgMLLMBench will be publicly released as a robust resource to advance multimodal surgical AI research, supporting reproducible evaluation and development of interactive surgical reasoning models.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "10 pages, 5 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21339v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]scene understanding"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "PathMamba: A Hybrid Mamba-Transformer for Topologically Coherent Road Segmentation in Satellite Imagery",
      "authors": [
        "Jules Decaestecker",
        "Nicolas Vigne"
      ],
      "arxiv_id": "2511.21298v1",
      "summary": "Achieving both high accuracy and topological continuity in road segmentation from satellite imagery is a critical goal for applications ranging from urban planning to disaster response. State-of-the-art methods often rely on Vision Transformers, which excel at capturing global context, yet their quadratic complexity is a significant barrier to efficient deployment, particularly for on-board processing in resource-constrained platforms. In contrast, emerging State Space Models like Mamba offer linear-time efficiency and are inherently suited to modeling long, continuous structures. We posit that these architectures have complementary strengths. To this end, we introduce PathMamba, a novel hybrid architecture that integrates Mamba's sequential modeling with the Transformer's global reasoning. Our design strategically uses Mamba blocks to trace the continuous nature of road networks, preserving topological structure, while integrating Transformer blocks to refine features with global context. This approach yields topologically superior segmentation maps without the prohibitive scaling costs of pure attention-based models. Our experiments on the DeepGlobe Road Extraction and Massachusetts Roads datasets demonstrate that PathMamba sets a new state-of-the-art. Notably, it significantly improves topological continuity, as measured by the APLS metric, setting a new benchmark while remaining computationally competitive.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "11 pages, 5 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21298v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba",
            "state space model"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Transformer Driven Visual Servoing and Dual Arm Impedance Control for Fabric Texture Matching",
      "authors": [
        "Fuyuki Tokuda",
        "Akira Seino",
        "Akinari Kobayashi",
        "Kai Tang",
        "Kazuhiro Kosuge"
      ],
      "arxiv_id": "2511.21203v2",
      "summary": "In this paper, we propose a method to align and place a fabric piece on top of another using a dual-arm manipulator and a grayscale camera, so that their surface textures are accurately matched. We propose a novel control scheme that combines Transformer-driven visual servoing with dualarm impedance control. This approach enables the system to simultaneously control the pose of the fabric piece and place it onto the underlying one while applying tension to keep the fabric piece flat. Our transformer-based network incorporates pretrained backbones and a newly introduced Difference Extraction Attention Module (DEAM), which significantly enhances pose difference prediction accuracy. Trained entirely on synthetic images generated using rendering software, the network enables zero-shot deployment in real-world scenarios without requiring prior training on specific fabric textures. Real-world experiments demonstrate that the proposed system accurately aligns fabric pieces with different textures.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-26",
      "updated": "2025-12-11",
      "comment": "8 pages, 11 figures. Accepted to IEEE Robotics and Automation Letters (RA-L)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21203v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]dual-arm"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "BotaCLIP: Contrastive Learning for Botany-Aware Representation of Earth Observation Data",
      "authors": [
        "Selene Cerna",
        "Sara Si-Moussi",
        "Wilfried Thuiller",
        "Hadrien Hendrikx",
        "Vincent Miele"
      ],
      "arxiv_id": "2511.21194v1",
      "summary": "Foundation models have demonstrated a remarkable ability to learn rich, transferable representations across diverse modalities such as images, text, and audio. In modern machine learning pipelines, these representations often replace raw data as the primary input for downstream tasks. In this paper, we address the challenge of adapting a pre-trained foundation model to inject domain-specific knowledge, without retraining from scratch or incurring significant computational costs. To this end, we introduce BotaCLIP, a lightweight multimodal contrastive framework that adapts a pre-trained Earth Observation foundation model (DOFA) by aligning high-resolution aerial imagery with botanical relevés. Unlike generic embeddings, BotaCLIP internalizes ecological structure through contrastive learning with a regularization strategy that mitigates catastrophic forgetting. Once trained, the resulting embeddings serve as transferable representations for downstream predictors. Motivated by real-world applications in biodiversity modeling, we evaluated BotaCLIP representations in three ecological tasks: plant presence prediction, butterfly occurrence modeling, and soil trophic group abundance estimation. The results showed consistent improvements over those derived from DOFA and supervised baselines. More broadly, this work illustrates how domain-aware adaptation of foundation models can inject expert knowledge into data-scarce settings, enabling frugal representation learning.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21194v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "representation learning",
            "[T]contrastive learning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "MarketGen: A Scalable Simulation Platform with Auto-Generated Embodied Supermarket Environments",
      "authors": [
        "Xu Hu",
        "Yiyang Feng",
        "Junran Peng",
        "Jiawei He",
        "Liyi Chen",
        "Chuanchen Luo",
        "Xucheng Yin",
        "Qing Li",
        "Zhaoxiang Zhang"
      ],
      "arxiv_id": "2511.21161v1",
      "summary": "The development of embodied agents for complex commercial environments is hindered by a critical gap in existing robotics datasets and benchmarks, which primarily focus on household or tabletop settings with short-horizon tasks. To address this limitation, we introduce MarketGen, a scalable simulation platform with automatic scene generation for complex supermarket environments. MarketGen features a novel agent-based Procedural Content Generation (PCG) framework. It uniquely supports multi-modal inputs (text and reference images) and integrates real-world design principles to automatically generate complete, structured, and realistic supermarkets. We also provide an extensive and diverse 3D asset library with a total of 1100+ supermarket goods and parameterized facilities assets. Building on this generative foundation, we propose a novel benchmark for assessing supermarket agents, featuring two daily tasks in a supermarket: (1) Checkout Unloading: long-horizon tabletop tasks for cashier agents, and (2) In-Aisle Item Collection: complex mobile manipulation tasks for salesperson agents. We validate our platform and benchmark through extensive experiments, including the deployment of a modular agent system and successful sim-to-real transfer. MarketGen provides a comprehensive framework to accelerate research in embodied AI for complex commercial applications.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "Project Page: https://xuhu0529.github.io/MarketGen",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21161v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "mobile manipulation",
            "sim-to-real"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Safe and Stable Neural Network Dynamical Systems for Robot Motion Planning",
      "authors": [
        "Allen Emmanuel Binny",
        "Mahathi Anand",
        "Hugo T. M. Kussaba",
        "Lingyun Chen",
        "Shreenabh Agrawal",
        "Fares J. Abu-Dakka",
        "Abdalla Swikir"
      ],
      "arxiv_id": "2511.20593v1",
      "summary": "Learning safe and stable robot motions from demonstrations remains a challenge, especially in complex, nonlinear tasks involving dynamic, obstacle-rich environments. In this paper, we propose Safe and Stable Neural Network Dynamical Systems S$^2$-NNDS, a learning-from-demonstration framework that simultaneously learns expressive neural dynamical systems alongside neural Lyapunov stability and barrier safety certificates. Unlike traditional approaches with restrictive polynomial parameterizations, S$^2$-NNDS leverages neural networks to capture complex robot motions providing probabilistic guarantees through split conformal prediction in learned certificates. Experimental results on various 2D and 3D datasets -- including LASA handwriting and demonstrations recorded kinesthetically from the Franka Emika Panda robot -- validate S$^2$-NNDS effectiveness in learning robust, safe, and stable motions from potentially unsafe demonstrations.",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20593v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]motion planning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning",
      "authors": [
        "Guanjie Chen",
        "Shirui Huang",
        "Kai Liu",
        "Jianchen Zhu",
        "Xiaoye Qu",
        "Peng Chen",
        "Yu Cheng",
        "Yifu Sun"
      ],
      "arxiv_id": "2511.20549v1",
      "summary": "Diffusion Models have emerged as a leading class of generative models, yet their iterative sampling process remains computationally expensive. Timestep distillation is a promising technique to accelerate generation, but it often requires extensive training and leads to image quality degradation. Furthermore, fine-tuning these distilled models for specific objectives, such as aesthetic appeal or user preference, using Reinforcement Learning (RL) is notoriously unstable and easily falls into reward hacking. In this work, we introduce Flash-DMD, a novel framework that enables fast convergence with distillation and joint RL-based refinement. Specifically, we first propose an efficient timestep-aware distillation strategy that significantly reduces training cost with enhanced realism, outperforming DMD2 with only $2.1\\%$ its training cost. Second, we introduce a joint training scheme where the model is fine-tuned with an RL objective while the timestep distillation training continues simultaneously. We demonstrate that the stable, well-defined loss from the ongoing distillation acts as a powerful regularizer, effectively stabilizing the RL training process and preventing policy collapse. Extensive experiments on score-based and flow matching models show that our proposed Flash-DMD not only converges significantly faster but also achieves state-of-the-art generation quality in the few-step sampling regime, outperforming existing methods in visual quality, human preference, and text-image alignment metrics. Our work presents an effective paradigm for training efficient, high-fidelity, and stable generative models. Codes are coming soon.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20549v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "flow matching"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Thinking in 360°: Humanoid Visual Search in the Wild",
      "authors": [
        "Heyang Yu",
        "Yinan Han",
        "Xiangyu Zhang",
        "Baiqiao Yin",
        "Bowen Chang",
        "Xiangyu Han",
        "Xinhao Liu",
        "Jing Zhang",
        "Marco Pavone",
        "Chen Feng",
        "Saining Xie",
        "Yiming Li"
      ],
      "arxiv_id": "2511.20351v2",
      "summary": "Humans rely on the synergistic control of head (cephalomotor) and eye (oculomotor) to efficiently search for visual information in 360°. However, prior approaches to visual search are limited to a static image, neglecting the physical embodiment and its interaction with the 3D world. How can we develop embodied visual search agents as efficient as humans while bypassing the constraints imposed by real-world hardware? To this end, we propose humanoid visual search where a humanoid agent actively rotates its head to search for objects or paths in an immersive world represented by a 360° panoramic image. To study visual search in visually-crowded real-world scenarios, we build H* Bench, a new benchmark that moves beyond household scenes to challenging in-the-wild scenes that necessitate advanced visual-spatial reasoning capabilities, such as transportation hubs, large-scale retail spaces, urban streets, and public institutions. Our experiments first reveal that even top-tier proprietary models falter, achieving only ~30% success in object and path search. We then use post-training techniques to enhance the open-source Qwen2.5-VL, increasing its success rate by over threefold for both object search (14.83% to 47.38%) and path search (6.44% to 24.94%). Notably, the lower ceiling of path search reveals its inherent difficulty, which we attribute to the demand for sophisticated spatial commonsense. Our results not only show a promising path forward but also quantify the immense challenge that remains in building MLLM agents that can be seamlessly integrated into everyday human life.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-26",
      "comment": "Website: https://humanoid-vstar.github.io/ ; Code: https://github.com/humanoid-vstar/hstar",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20351v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "ArtiBench and ArtiBrain: Benchmarking Generalizable Vision-Language Articulated Object Manipulation",
      "authors": [
        "Yuhan Wu",
        "Tiantian Wei",
        "Shuo Wang",
        "ZhiChao Wang",
        "Yanyong Zhang",
        "Daniel Cremers",
        "Yan Xia"
      ],
      "arxiv_id": "2511.20330v2",
      "summary": "Interactive articulated manipulation requires long-horizon, multi-step interactions with appliances while maintaining physical consistency. Existing vision-language and diffusion-based policies struggle to generalize across parts, instances, and categories. We first introduce ArtiBench, a five-level benchmark covering kitchen, storage, office, and tool environments. ArtiBench enables structured evaluation from cross-part and cross-instance variation to long-horizon multi-object tasks, revealing the core generalization challenges of articulated object manipulation. Building on this benchmark, we propose ArtiBrain, a modular framework that unifies high-level reasoning with adaptive low-level control. ArtiBrain uses a VLM-based Task Reasoner (GPT-4.1) to decompose and validate subgoals, and employs a Hybrid Controller that combines geometry-aware keyframe execution with affordance-guided diffusion for precise and interpretable manipulation. An Affordance Memory Bank continually accumulates successful execution episodes and propagates part-level actionable affordances to unseen articulated parts and configurations. Extensive experiments on ArtiBench show that our ArtiBrain significantly outperforms state-of-the-art multimodal and diffusion-based methods in robustness and generalization. Code and dataset will be released upon acceptance.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-25",
      "updated": "2025-11-27",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20330v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "WPT: World-to-Policy Transfer via Online World Model Distillation",
      "authors": [
        "Guangfeng Jiang",
        "Yueru Luo",
        "Jun Liu",
        "Yi Huang",
        "Yiyao Zhu",
        "Zhan Qu",
        "Dave Zhenyu Chen",
        "Bingbing Liu",
        "Xu Yan"
      ],
      "arxiv_id": "2511.20095v1",
      "summary": "Recent years have witnessed remarkable progress in world models, which primarily aim to capture the spatio-temporal correlations between an agent's actions and the evolving environment. However, existing approaches often suffer from tight runtime coupling or depend on offline reward signals, resulting in substantial inference overhead or hindering end-to-end optimization. To overcome these limitations, we introduce WPT, a World-to-Policy Transfer training paradigm that enables online distillation under the guidance of an end-to-end world model. Specifically, we develop a trainable reward model that infuses world knowledge into a teacher policy by aligning candidate trajectories with the future dynamics predicted by the world model. Subsequently, we propose policy distillation and world reward distillation to transfer the teacher's reasoning ability into a lightweight student policy, enhancing planning performance while preserving real-time deployability. Extensive experiments on both open-loop and closed-loop benchmarks show that our WPT achieves state-of-the-art performance with a simple policy architecture: it attains a 0.11 collision rate (open-loop) and achieves a 79.23 driving score (closed-loop) surpassing both world-model-based and imitation-learning methods in accuracy and safety. Moreover, the student sustains up to 4.9x faster inference, while retaining most of the gains.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20095v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning",
            "[T]world model"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "FLaTEC: Frequency-Disentangled Latent Triplanes for Efficient Compression of LiDAR Point Clouds",
      "authors": [
        "Xiaoge Zhang",
        "Zijie Wu",
        "Mingtao Feng",
        "Zichen Geng",
        "Mehwish Nasim",
        "Saeed Anwar",
        "Ajmal Mian"
      ],
      "arxiv_id": "2511.20065v1",
      "summary": "Point cloud compression methods jointly optimize bitrates and reconstruction distortion. However, balancing compression ratio and reconstruction quality is difficult because low-frequency and high-frequency components contribute differently at the same resolution. To address this, we propose FLaTEC, a frequency-aware compression model that enables the compression of a full scan with high compression ratios. Our approach introduces a frequency-aware mechanism that decouples low-frequency structures and high-frequency textures, while hybridizing latent triplanes as a compact proxy for point cloud. Specifically, we convert voxelized embeddings into triplane representations to reduce sparsity, computational cost, and storage requirements. We then devise a frequency-disentangling technique that extracts compact low-frequency content while collecting high-frequency details across scales. The decoupled low-frequency and high-frequency components are stored in binary format. During decoding, full-spectrum signals are progressively recovered via a modulation block. Additionally, to compensate for the loss of 3D correlation, we introduce an efficient frequency-based attention mechanism that fosters local connectivity and outputs arbitrary resolution points. Our method achieves state-of-the-art rate-distortion performance and outperforms the standard codecs by 78\\% and 94\\% in BD-rate on both SemanticKITTI and Ford datasets.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20065v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]point cloud"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "MambaEye: A Size-Agnostic Visual Encoder with Causal Sequential Processing",
      "authors": [
        "Changho Choi",
        "Minho Kim",
        "Jinkyu Kim"
      ],
      "arxiv_id": "2511.19963v1",
      "summary": "Despite decades of progress, a truly input-size agnostic visual encoder-a fundamental characteristic of human vision-has remained elusive. We address this limitation by proposing \\textbf{MambaEye}, a novel, causal sequential encoder that leverages the low complexity and causal-process based pure Mamba2 backbone. Unlike previous Mamba-based vision encoders that often employ bidirectional processing, our strictly unidirectional approach preserves the inherent causality of State Space Models, enabling the model to generate a prediction at any point in its input sequence. A core innovation is our use of relative move embedding, which encodes the spatial shift between consecutive patches, providing a strong inductive bias for translation invariance and making the model inherently adaptable to arbitrary image resolutions and scanning patterns. To achieve this, we introduce a novel diffusion-inspired loss function that provides dense, step-wise supervision, training the model to build confidence as it gathers more visual evidence. We demonstrate that MambaEye exhibits robust performance across a wide range of image resolutions, especially at higher resolutions such as $1536^2$ on the ImageNet-1K classification task. This feat is achieved while maintaining linear time and memory complexity relative to the number of patches.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "Code will be released in github",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19963v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba",
            "state space model"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "ShapeForce: Low-Cost Soft Robotic Wrist for Contact-Rich Manipulation",
      "authors": [
        "Jinxuan Zhu",
        "Zihao Yan",
        "Yangyu Xiao",
        "Jingxiang Guo",
        "Chenrui Tie",
        "Xinyi Cao",
        "Yuhang Zheng",
        "Lin Shao"
      ],
      "arxiv_id": "2511.19955v2",
      "summary": "Contact feedback is essential for contact-rich robotic manipulation, as it allows the robot to detect subtle interaction changes and adjust its actions accordingly. Six-axis force-torque sensors are commonly used to obtain contact feedback, but their high cost and fragility have discouraged many researchers from adopting them in contact-rich tasks. To offer a more cost-efficient and easy-accessible source of contact feedback, we present ShapeForce, a low-cost, plug-and-play soft wrist that provides force-like signals for contact-rich robotic manipulation. Inspired by how humans rely on relative force changes in contact rather than precise force magnitudes, ShapeForce converts external force and torque into measurable deformations of its compliant core, which are then estimated via marker-based pose tracking and converted into force-like signals. Our design eliminates the need for calibration or specialized electronics to obtain exact values, and instead focuses on capturing force and torque changes sufficient for enabling contact-rich manipulation. Extensive experiments across diverse contact-rich tasks and manipulation policies demonstrate that ShapeForce delivers performance comparable to six-axis force-torque sensors at an extremely low cost.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-25",
      "updated": "2025-12-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19955v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Enhancing Geo-localization for Crowdsourced Flood Imagery via LLM-Guided Attention",
      "authors": [
        "Fengyi Xu",
        "Jun Ma",
        "Waishan Qiu",
        "Cui Guo",
        "Jack C. P. Cheng"
      ],
      "arxiv_id": "2512.11811v2",
      "summary": "Crowdsourced street-view imagery from social media provides real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing image geo-localization approaches, also known as Visual Place Recognition (VPR) models, exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geo-knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "published": "2025-11-25",
      "updated": "2025-12-16",
      "comment": "Updated author list to include additional contributor. Revised title and improved methodology section based on collaborative feedback",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.11811v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Learning Massively Multitask World Models for Continuous Control",
      "authors": [
        "Nicklas Hansen",
        "Hao Su",
        "Xiaolong Wang"
      ],
      "arxiv_id": "2511.19584v2",
      "summary": "General-purpose control demands agents that act across many tasks and embodiments, yet research on reinforcement learning (RL) for continuous control remains dominated by single-task or offline regimes, reinforcing a view that online RL does not scale. Inspired by the foundation model recipe (large-scale pretraining followed by light RL) we ask whether a single agent can be trained on hundreds of tasks with online interaction. To accelerate research in this direction, we introduce a new benchmark with 200 diverse tasks spanning many domains and embodiments, each with language instructions, demonstrations, and optionally image observations. We then present \\emph{Newt}, a language-conditioned multitask world model that is first pretrained on demonstrations to acquire task-aware representations and action priors, and then jointly optimized with online interaction across all tasks. Experiments show that Newt yields better multitask performance and data-efficiency than a set of strong baselines, exhibits strong open-loop control, and enables rapid adaptation to unseen tasks. We release our environments, demonstrations, code for training and evaluation, as well as 200+ checkpoints.",
      "categories": [
        "cs.LG",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-24",
      "updated": "2025-12-02",
      "comment": "Webpage: https://www.nicklashansen.com/NewtWM",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19584v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "[T]world model"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Leveraging LLMs for reward function design in reinforcement learning control tasks",
      "authors": [
        "Franklin Cardenoso",
        "Wouter Caarls"
      ],
      "arxiv_id": "2511.19355v1",
      "summary": "The challenge of designing effective reward functions in reinforcement learning (RL) represents a significant bottleneck, often requiring extensive human expertise and being time-consuming. Previous work and recent advancements in large language models (LLMs) have demonstrated their potential for automating the generation of reward functions. However, existing methodologies often require preliminary evaluation metrics, human-engineered feedback for the refinement process, or the use of environmental source code as context. To address these limitations, this paper introduces LEARN-Opt (LLM-based Evaluator and Analyzer for Reward functioN Optimization). This LLM-based, fully autonomous, and model-agnostic framework eliminates the need for preliminary metrics and environmental source code as context to generate, execute, and evaluate reward function candidates from textual descriptions of systems and task objectives. LEARN-Opt's main contribution lies in its ability to autonomously derive performance metrics directly from the system description and the task objective, enabling unsupervised evaluation and selection of reward functions. Our experiments indicate that LEARN-Opt achieves performance comparable to or better to that of state-of-the-art methods, such as EUREKA, while requiring less prior knowledge. We find that automated reward design is a high-variance problem, where the average-case candidate fails, requiring a multi-run approach to find the best candidates. Finally, we show that LEARN-Opt can unlock the potential of low-cost LLMs to find high-performing candidates that are comparable to, or even better than, those of larger models. This demonstrated performance affirms its potential to generate high-quality reward functions without requiring any preliminary human-defined metrics, thereby reducing engineering overhead and enhancing generalizability.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19355v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning",
            "reward design"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Rethinking Intermediate Representation for VLM-based Robot Manipulation",
      "authors": [
        "Weiliang Tang",
        "Jialin Gao",
        "Jia-Hui Pan",
        "Gang Wang",
        "Li Erran Li",
        "Yunhui Liu",
        "Mingyu Ding",
        "Pheng-Ann Heng",
        "Chi-Wing Fu"
      ],
      "arxiv_id": "2511.19315v1",
      "summary": "Vision-Language Model (VLM) is an important component to enable robust robot manipulation. Yet, using it to translate human instructions into an action-resolvable intermediate representation often needs a tradeoff between VLM-comprehensibility and generalizability. Inspired by context-free grammar, we design the Semantic Assembly representation named SEAM, by decomposing the intermediate representation into vocabulary and grammar. Doing so leads us to a concise vocabulary of semantically-rich operations and a VLM-friendly grammar for handling diverse unseen tasks. In addition, we design a new open-vocabulary segmentation paradigm with a retrieval-augmented few-shot learning strategy to localize fine-grained object parts for manipulation, effectively with the shortest inference time over all state-of-the-art parallel works. Also, we formulate new metrics for action-generalizability and VLM-comprehensibility, demonstrating the compelling performance of SEAM over mainstream representations on both aspects. Extensive real-world experiments further manifest its SOTA performance under varying settings and tasks.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19315v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Reconfigurable Auxetic Devices (RADs) for Robotic Surface Manipulation",
      "authors": [
        "Jacob Miske",
        "Ahyan Maya",
        "Ahnaf Inkiad",
        "Jeffrey Ian Lipton"
      ],
      "arxiv_id": "2512.00072v1",
      "summary": "Robotic surfaces traditionally use materials with a positive Poisson's ratio to push and pull on a manipulation interface. Auxetic materials with a negative Poisson's ratio may expand in multiple directions when stretched and enable conformable interfaces. Here we demonstrate reconfigurable auxetic lattices for robotic surface manipulation. Our approach enables shape control through reconfigurable locking or embedded servos that underactuate an auxetic lattice structure. Variable expansion of local lattice areas is enabled by backlash between unit cells. Demonstrations of variable surface conformity are presented with characterization metrics. Experimental results are validated against a simplified model of the system, which uses an activation function to model intercell coupling with backlash. Reconfigurable auxetic structures are shown to achieve manipulation via variable surface contraction and expansion. This structure maintains compliance with backlash in contrast with previous work on auxetics, opening new opportunities in adaptive robotic structures for surface manipulation tasks.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "13 pages, 9 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00072v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "MetroGS: Efficient and Stable Reconstruction of Geometrically Accurate High-Fidelity Large-Scale Scenes",
      "authors": [
        "Kehua Chen",
        "Tianlu Mao",
        "Zhuxin Ma",
        "Hao Jiang",
        "Zehao Li",
        "Zihan Liu",
        "Shuqi Gao",
        "Honglong Zhao",
        "Feng Dai",
        "Yucheng Zhang",
        "Zhaoqi Wang"
      ],
      "arxiv_id": "2511.19172v1",
      "summary": "Recently, 3D Gaussian Splatting and its derivatives have achieved significant breakthroughs in large-scale scene reconstruction. However, how to efficiently and stably achieve high-quality geometric fidelity remains a core challenge. To address this issue, we introduce MetroGS, a novel Gaussian Splatting framework for efficient and robust reconstruction in complex urban environments. Our method is built upon a distributed 2D Gaussian Splatting representation as the core foundation, serving as a unified backbone for subsequent modules. To handle potential sparse regions in complex scenes, we propose a structured dense enhancement scheme that utilizes SfM priors and a pointmap model to achieve a denser initialization, while incorporating a sparsity compensation mechanism to improve reconstruction completeness. Furthermore, we design a progressive hybrid geometric optimization strategy that organically integrates monocular and multi-view optimization to achieve efficient and accurate geometric refinement. Finally, to address the appearance inconsistency commonly observed in large-scale scenes, we introduce a depth-guided appearance modeling approach that learns spatial features with 3D consistency, facilitating effective decoupling between geometry and appearance and further enhancing reconstruction stability. Experiments on large-scale urban datasets demonstrate that MetroGS achieves superior geometric accuracy, rendering quality, offering a unified solution for high-fidelity large-scale scene reconstruction.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "Project page: https://m3phist0.github.io/MetroGS",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19172v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "gaussian splatting",
            "scene reconstruction"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Graph-based 3D Human Pose Estimation using WiFi Signals",
      "authors": [
        "Jichao Chen",
        "YangYang Qu",
        "Ruibo Tang",
        "Dirk Slock"
      ],
      "arxiv_id": "2511.19105v1",
      "summary": "WiFi-based human pose estimation (HPE) has attracted increasing attention due to its resilience to occlusion and privacy-preserving compared to camera-based methods. However, existing WiFi-based HPE approaches often employ regression networks that directly map WiFi channel state information (CSI) to 3D joint coordinates, ignoring the inherent topological relationships among human joints. In this paper, we present GraphPose-Fi, a graph-based framework that explicitly models skeletal topology for WiFi-based 3D HPE. Our framework comprises a CNN encoder shared across antennas for subcarrier-time feature extraction, a lightweight attention module that adaptively reweights features over time and across antennas, and a graph-based regression head that combines GCN layers with self-attention to capture local topology and global dependencies. Our proposed method significantly outperforms existing methods on the MM-Fi dataset in various settings. The source code is available at: https://github.com/Cirrick/GraphPose-Fi.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19105v1",
      "code_links": [
        {
          "url": "https://github.com/Cirrick/GraphPose-Fi",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]pose estimation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "LAA3D: A Benchmark of Detecting and Tracking Low-Altitude Aircraft in 3D Space",
      "authors": [
        "Hai Wu",
        "Shuai Tang",
        "Jiale Wang",
        "Longkun Zou",
        "Mingyue Guo",
        "Rongqin Liang",
        "Ke Chen",
        "Yaowei Wang"
      ],
      "arxiv_id": "2511.19057v1",
      "summary": "Perception of Low-Altitude Aircraft (LAA) in 3D space enables precise 3D object localization and behavior understanding. However, datasets tailored for 3D LAA perception remain scarce. To address this gap, we present LAA3D, a large-scale dataset designed to advance 3D detection and tracking of low-altitude aerial vehicles. LAA3D contains 15,000 real images and 600,000 synthetic frames, captured across diverse scenarios, including urban and suburban environments. It covers multiple aerial object categories, including electric Vertical Take-Off and Landing (eVTOL) aircraft, Micro Aerial Vehicles (MAVs), and Helicopters. Each instance is annotated with 3D bounding box, class label, and instance identity, supporting tasks such as 3D object detection, 3D multi-object tracking (MOT), and 6-DoF pose estimation. Besides, we establish the LAA3D Benchmark, integrating multiple tasks and methods with unified evaluation protocols for comparison. Furthermore, we propose MonoLAA, a monocular 3D detection baseline, achieving robust 3D localization from zoom cameras with varying focal lengths. Models pretrained on synthetic images transfer effectively to real-world data with fine-tuning, demonstrating strong sim-to-real generalization. Our LAA3D provides a comprehensive foundation for future research in low-altitude 3D object perception.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "25 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19057v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "sim-to-real"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "pose estimation",
            "localization"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "An Efficient Closed-Form Solution to Full Visual-Inertial State Initialization",
      "authors": [
        "Samuel Cerezo",
        "Seong Hun Lee",
        "Javier Civera"
      ],
      "arxiv_id": "2511.18910v2",
      "summary": "In this letter, we present a closed-form initialization method that recovers the full visual-inertial state without nonlinear optimization. Unlike previous approaches that rely on iterative solvers, our formulation yields analytical, easy-to-implement, and numerically stable solutions for reliable start-up. Our method builds on small-rotation and constant-velocity approximations, which keep the formulation compact while preserving the essential coupling between motion and inertial measurements. We further propose an observability-driven, two-stage initialization scheme that balances accuracy with initialization latency. Extensive experiments on the EuRoC dataset validate our assumptions: our method achieves 10-20% lower initialization error than optimization-based approaches, while using 4x shorter initialization windows and reducing computational cost by 5x.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-24",
      "updated": "2025-11-25",
      "comment": "8 pages, 2 figures, 10 tables. Submitted to RA-L",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18910v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]visual-inertial"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Robust Long-term Test-Time Adaptation for 3D Human Pose Estimation through Motion Discretization",
      "authors": [
        "Yilin Wen",
        "Kechuan Dong",
        "Yusuke Sugano"
      ],
      "arxiv_id": "2511.18851v1",
      "summary": "Online test-time adaptation addresses the train-test domain gap by adapting the model on unlabeled streaming test inputs before making the final prediction. However, online adaptation for 3D human pose estimation suffers from error accumulation when relying on self-supervision with imperfect predictions, leading to degraded performance over time. To mitigate this fundamental challenge, we propose a novel solution that highlights the use of motion discretization. Specifically, we employ unsupervised clustering in the latent motion representation space to derive a set of anchor motions, whose regularity aids in supervising the human pose estimator and enables efficient self-replay. Additionally, we introduce an effective and efficient soft-reset mechanism by reverting the pose estimator to its exponential moving average during continuous adaptation. We examine long-term online adaptation by continuously adapting to out-of-domain streaming test videos of the same individual, which allows for the capture of consistent personal shape and motion traits throughout the streaming observation. By mitigating error accumulation, our solution enables robust exploitation of these personal traits for enhanced accuracy. Experiments demonstrate that our solution outperforms previous online test-time adaptation methods and validate our design choices.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "Accepted by AAAI 2026, main track",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18851v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]pose estimation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "CNN-Based Camera Pose Estimation and Localisation of Scan Images for Aircraft Visual Inspection",
      "authors": [
        "Xueyan Oh",
        "Leonard Loh",
        "Shaohui Foong",
        "Zhong Bao Andy Koh",
        "Kow Leong Ng",
        "Poh Kang Tan",
        "Pei Lin Pearlin Toh",
        "U-Xuan Tan"
      ],
      "arxiv_id": "2511.18702v1",
      "summary": "General Visual Inspection is a manual inspection process regularly used to detect and localise obvious damage on the exterior of commercial aircraft. There has been increasing demand to perform this process at the boarding gate to minimise the downtime of the aircraft and automating this process is desired to reduce the reliance on human labour. Automating this typically requires estimating a camera's pose with respect to the aircraft for initialisation but most existing localisation methods require infrastructure, which is very challenging in uncontrolled outdoor environments and within the limited turnover time (approximately 2 hours) on an airport tarmac. Additionally, many airlines and airports do not allow contact with the aircraft's surface or using UAVs for inspection between flights, and restrict access to commercial aircraft. Hence, this paper proposes an on-site method that is infrastructure-free and easy to deploy for estimating a pan-tilt-zoom camera's pose and localising scan images. This method initialises using the same pan-tilt-zoom camera used for the inspection task by utilising a Deep Convolutional Neural Network fine-tuned on only synthetic images to predict its own pose. We apply domain randomisation to generate the dataset for fine-tuning the network and modify its loss function by leveraging aircraft geometry to improve accuracy. We also propose a workflow for initialisation, scan path planning, and precise localisation of images captured from a pan-tilt-zoom camera. We evaluate and demonstrate our approach through experiments with real aircraft, achieving root-mean-square camera pose estimation errors of less than 0.24 m and 2 degrees for all real scenes.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "12 pages, 12 figures",
      "doi": "10.1109/TITS.2024.3369653",
      "journal_ref": "X. Oh et al., \"CNN-Based Camera Pose Estimation and Localization of Scan Images for Aircraft Visual Inspection,\" in IEEE Transactions on Intelligent Transportation Systems, vol. 25, no. 8, pp. 8629-8640, Aug. 2024",
      "pdf_url": "https://arxiv.org/pdf/2511.18702v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]pose estimation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Stable Multi-Drone GNSS Tracking System for Marine Robots",
      "authors": [
        "Shuo Wen",
        "Edwin Meriaux",
        "Mariana Sosa Guzmán",
        "Zhizun Wang",
        "Junming Shi",
        "Gregory Dudek"
      ],
      "arxiv_id": "2511.18694v1",
      "summary": "Accurate localization is essential for marine robotics, yet Global Navigation Satellite System (GNSS) signals are unreliable or unavailable even at a very short distance below the water surface. Traditional alternatives, such as inertial navigation, Doppler Velocity Loggers (DVL), SLAM, and acoustic methods, suffer from error accumulation, high computational demands, or infrastructure dependence. In this work, we present a scalable multi-drone GNSS-based tracking system for surface and near-surface marine robots. Our approach combines efficient visual detection, lightweight multi-object tracking, GNSS-based triangulation, and a confidence-weighted Extended Kalman Filter (EKF) to provide stable GNSS estimation in real time. We further introduce a cross-drone tracking ID alignment algorithm that enforces global consistency across views, enabling robust multi-robot tracking with redundant aerial coverage. We validate our system in diversified complex settings to show the scalability and robustness of the proposed algorithm.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18694v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "SLAM",
            "localization",
            "navigation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Online Learning-Enhanced Lie Algebraic MPC for Robust Trajectory Tracking of Autonomous Surface Vehicles",
      "authors": [
        "Yinan Dong",
        "Ziyu Xu",
        "Tsimafei Lazouski",
        "Sangli Teng",
        "Maani Ghaffari"
      ],
      "arxiv_id": "2511.18683v1",
      "summary": "Autonomous surface vehicles (ASVs) are easily influenced by environmental disturbances such as wind and waves, making accurate trajectory tracking a persistent challenge in dynamic marine conditions. In this paper, we propose an efficient controller for trajectory tracking of marine vehicles under unknown disturbances by combining a convex error-state MPC on the Lie group with an online learning module to compensate for these disturbances in real time. This design enables adaptive and robust control while maintaining computational efficiency. Extensive evaluations in numerical simulations, the Virtual RobotX (VRX) simulator, and real-world field experiments demonstrate that our method achieves superior tracking accuracy under various disturbance scenarios compared with existing approaches.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18683v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]MPC"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Functional Localization Enforced Deep Anomaly Detection Using Fundus Images",
      "authors": [
        "Jan Benedikt Ruhland",
        "Thorsten Papenbrock",
        "Jan-Peter Sowa",
        "Ali Canbay",
        "Nicole Eter",
        "Bernd Freisleben",
        "Dominik Heider"
      ],
      "arxiv_id": "2511.18627v1",
      "summary": "Reliable detection of retinal diseases from fundus images is challenged by the variability in imaging quality, subtle early-stage manifestations, and domain shift across datasets. In this study, we systematically evaluated a Vision Transformer (ViT) classifier under multiple augmentation and enhancement strategies across several heterogeneous public datasets, as well as the AEyeDB dataset, a high-quality fundus dataset created in-house and made available for the research community. The ViT demonstrated consistently strong performance, with accuracies ranging from 0.789 to 0.843 across datasets and diseases. Diabetic retinopathy and age-related macular degeneration were detected reliably, whereas glaucoma remained the most frequently misclassified disease. Geometric and color augmentations provided the most stable improvements, while histogram equalization benefited datasets dominated by structural subtlety. Laplacian enhancement reduced performance across different settings.\n  On the Papila dataset, the ViT with geometric augmentation achieved an AUC of 0.91, outperforming previously reported convolutional ensemble baselines (AUC of 0.87), underscoring the advantages of transformer architectures and multi-dataset training. To complement the classifier, we developed a GANomaly-based anomaly detector, achieving an AUC of 0.76 while providing inherent reconstruction-based explainability and robust generalization to unseen data. Probabilistic calibration using GUESS enabled threshold-independent decision support for future clinical implementation.",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-23",
      "updated": "2025-11-23",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18627v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Object-centric Task Representation and Transfer using Diffused Orientation Fields",
      "authors": [
        "Cem Bilaloglu",
        "Tobias Löw",
        "Sylvain Calinon"
      ],
      "arxiv_id": "2511.18563v1",
      "summary": "Curved objects pose a fundamental challenge for skill transfer in robotics: unlike planar surfaces, they do not admit a global reference frame. As a result, task-relevant directions such as \"toward\" or \"along\" the surface vary with position and geometry, making object-centric tasks difficult to transfer across shapes. To address this, we introduce an approach using Diffused Orientation Fields (DOF), a smooth representation of local reference frames, for transfer learning of tasks across curved objects. By expressing manipulation tasks in these smoothly varying local frames, we reduce the problem of transferring tasks across curved objects to establishing sparse keypoint correspondences. DOF is computed online from raw point cloud data using diffusion processes governed by partial differential equations, conditioned on keypoints. We evaluate DOF under geometric, topological, and localization perturbations, and demonstrate successful transfer of tasks requiring continuous physical interaction such as inspection, slicing, and peeling across varied objects. We provide our open-source codes at our website https://github.com/idiap/diffused_fields_robotics",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-23",
      "updated": "2025-11-23",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18563v1",
      "code_links": [
        {
          "url": "https://github.com/idiap/diffused_fields_robotics",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud",
            "localization"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Gaze Beyond the Frame: Forecasting Egocentric 3D Visual Span",
      "authors": [
        "Heeseung Yun",
        "Joonil Na",
        "Jaeyeon Kim",
        "Calvin Murdock",
        "Gunhee Kim"
      ],
      "arxiv_id": "2511.18470v1",
      "summary": "People continuously perceive and interact with their surroundings based on underlying intentions that drive their exploration and behaviors. While research in egocentric user and scene understanding has focused primarily on motion and contact-based interaction, forecasting human visual perception itself remains less explored despite its fundamental role in guiding human actions and its implications for AR/VR and assistive technologies. We address the challenge of egocentric 3D visual span forecasting, predicting where a person's visual perception will focus next within their three-dimensional environment. To this end, we propose EgoSpanLift, a novel method that transforms egocentric visual span forecasting from 2D image planes to 3D scenes. EgoSpanLift converts SLAM-derived keypoints into gaze-compatible geometry and extracts volumetric visual span regions. We further combine EgoSpanLift with 3D U-Net and unidirectional transformers, enabling spatio-temporal fusion to efficiently predict future visual span in the 3D grid. In addition, we curate a comprehensive benchmark from raw egocentric multisensory data, creating a testbed with 364.6K samples for 3D visual span forecasting. Our approach outperforms competitive baselines for egocentric 2D gaze anticipation and 3D localization while achieving comparable results even when projected back onto 2D image planes without additional 2D-specific training.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-23",
      "updated": "2025-11-23",
      "comment": "NeurIPS 2025 Spotlight",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18470v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "SLAM",
            "scene understanding",
            "localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "4D-VGGT: A General Foundation Model with SpatioTemporal Awareness for Dynamic Scene Geometry Estimation",
      "authors": [
        "Haonan Wang",
        "Hanyu Zhou",
        "Haoyue Liu",
        "Luxin Yan"
      ],
      "arxiv_id": "2511.18416v1",
      "summary": "We investigate a challenging task of dynamic scene geometry estimation, which requires representing both spatial and temporal features. Typically, existing methods align the two features into a unified latent space to model scene geometry. However, this unified paradigm suffers from potential mismatched representation due to the heterogeneous nature between spatial and temporal features. In this work, we propose 4D-VGGT, a general foundation model with divide-and-conquer spatiotemporal representation for dynamic scene geometry. Our model is divided into three aspects: 1) Multi-setting input. We design an adaptive visual grid that supports input sequences with arbitrary numbers of views and time steps. 2) Multi-level representation. We propose a cross-view global fusion for spatial representation and a cross-time local fusion for temporal representation. 3) Multi-task prediction. We append multiple task-specific heads to spatiotemporal representations, enabling a comprehensive visual geometry estimation for dynamic scenes. Under this unified framework, these components enhance the feature discriminability and application universality of our model for dynamic scenes. In addition, we integrate multiple geometry datasets to train our model and conduct extensive experiments to verify the effectiveness of our method across various tasks on multiple dynamic scene geometry benchmarks.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-23",
      "updated": "2025-11-23",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18416v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]VGGT"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "RNN as Linear Transformer: A Closer Investigation into Representational Potentials of Visual Mamba Models",
      "authors": [
        "Timing Yang",
        "Guoyizhe Wei",
        "Alan Yuille",
        "Feng Wang"
      ],
      "arxiv_id": "2511.18380v1",
      "summary": "Mamba has recently garnered attention as an effective backbone for vision tasks. However, its underlying mechanism in visual domains remains poorly understood. In this work, we systematically investigate Mamba's representational properties and make three primary contributions. First, we theoretically analyze Mamba's relationship to Softmax and Linear Attention, confirming that it can be viewed as a low-rank approximation of Softmax Attention and thereby bridging the representational gap between Softmax and Linear forms. Second, we introduce a novel binary segmentation metric for activation map evaluation, extending qualitative assessments to a quantitative measure that demonstrates Mamba's capacity to model long-range dependencies. Third, by leveraging DINO for self-supervised pretraining, we obtain clearer activation maps than those produced by standard supervised approaches, highlighting Mamba's potential for interpretability. Notably, our model also achieves a 78.5 percent linear probing accuracy on ImageNet, underscoring its strong performance. We hope this work can provide valuable insights for future investigations of Mamba-based vision architectures.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-23",
      "updated": "2025-11-23",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18380v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba",
            "linear attention"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "ARIAL: An Agentic Framework for Document VQA with Precise Answer Localization",
      "authors": [
        "Ahmad Mohammadshirazi",
        "Pinaki Prasad Guha Neogi",
        "Dheeraj Kulshrestha",
        "Rajiv Ramnath"
      ],
      "arxiv_id": "2511.18192v2",
      "summary": "Document Visual Question Answering (VQA) requires models to not only extract accurate textual answers but also precisely localize them within document images, a capability critical for interpretability in high-stakes applications. However, existing systems achieve strong textual accuracy while producing unreliable spatial grounding, or sacrifice performance for interpretability. We present ARIAL (Agentic Reasoning for Interpretable Answer Localization), a modular framework that orchestrates specialized tools through an LLM-based planning agent to achieve both precise answer extraction and reliable spatial grounding. ARIAL decomposes Document VQA into structured subtasks: OCR-based text extraction with TrOCR, retrieval-augmented context selection using semantic search, answer generation via a fine-tuned Gemma 3-27B model, and explicit bounding-box localization through text-to-region alignment. This modular architecture produces transparent reasoning traces, enabling tool-level auditability and independent component optimization. We evaluate ARIAL on four benchmarks (DocVQA, FUNSD, CORD, and SROIE) using both textual accuracy (ANLS) and spatial precision (mAP at IoU 0.50 to 0.95). ARIAL achieves state-of-the-art results across all datasets: 88.7 ANLS and 50.1 mAP on DocVQA, 90.0 ANLS and 50.3 mAP on FUNSD, 85.5 ANLS and 60.2 mAP on CORD, and 93.1 ANLS on SROIE, surpassing the previous best method (DLaVA) by +2.8 ANLS and +3.9 mAP on DocVQA. Our work demonstrates how agentic orchestration of specialized tools can simultaneously improve performance and interpretability, providing a pathway toward trustworthy, explainable document AI systems.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-22",
      "updated": "2025-11-28",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18192v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]localization"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "SFHand: A Streaming Framework for Language-guided 3D Hand Forecasting and Embodied Manipulation",
      "authors": [
        "Ruicong Liu",
        "Yifei Huang",
        "Liangyang Ouyang",
        "Caixin Kang",
        "Yoichi Sato"
      ],
      "arxiv_id": "2511.18127v1",
      "summary": "Real-time 3D hand forecasting is a critical component for fluid human-computer interaction in applications like AR and assistive robotics. However, existing methods are ill-suited for these scenarios, as they typically require offline access to accumulated video sequences and cannot incorporate language guidance that conveys task intent. To overcome these limitations, we introduce SFHand, the first streaming framework for language-guided 3D hand forecasting. SFHand autoregressively predicts a comprehensive set of future 3D hand states, including hand type, 2D bounding box, 3D pose, and trajectory, from a continuous stream of video and language instructions. Our framework combines a streaming autoregressive architecture with an ROI-enhanced memory layer, capturing temporal context while focusing on salient hand-centric regions. To enable this research, we also introduce EgoHaFL, the first large-scale dataset featuring synchronized 3D hand poses and language instructions. We demonstrate that SFHand achieves new state-of-the-art results in 3D hand forecasting, outperforming prior work by a significant margin of up to 35.8%. Furthermore, we show the practical utility of our learned representations by transferring them to downstream embodied manipulation tasks, improving task success rates by up to 13.4% on multiple benchmarks. Dataset page: https://huggingface.co/datasets/ut-vision/EgoHaFL, project page: https://github.com/ut-vision/SFHand.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-22",
      "updated": "2025-11-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18127v1",
      "code_links": [
        {
          "url": "https://github.com/ut-vision/SFHand",
          "type": "github"
        },
        {
          "url": "https://huggingface.co/datasets/ut-vision",
          "type": "huggingface"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "M4-BLIP: Advancing Multi-Modal Media Manipulation Detection through Face-Enhanced Local Analysis",
      "authors": [
        "Hang Wu",
        "Ke Sun",
        "Jiayi Ji",
        "Xiaoshuai Sun",
        "Rongrong Ji"
      ],
      "arxiv_id": "2512.01214v1",
      "summary": "In the contemporary digital landscape, multi-modal media manipulation has emerged as a significant societal threat, impacting the reliability and integrity of information dissemination. Current detection methodologies in this domain often overlook the crucial aspect of localized information, despite the fact that manipulations frequently occur in specific areas, particularly in facial regions. In response to this critical observation, we propose the M4-BLIP framework. This innovative framework utilizes the BLIP-2 model, renowned for its ability to extract local features, as the cornerstone for feature extraction. Complementing this, we incorporate local facial information as prior knowledge. A specially designed alignment and fusion module within M4-BLIP meticulously integrates these local and global features, creating a harmonious blend that enhances detection accuracy. Furthermore, our approach seamlessly integrates with Large Language Models (LLM), significantly improving the interpretability of the detection outcomes. Extensive quantitative and visualization experiments validate the effectiveness of our framework against the state-of-the-art competitors.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "comment": "12 pages, 6 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01214v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "ReGlove: A Soft Pneumatic Glove for Activities of Daily Living Assistance via Wrist-Mounted Vision",
      "authors": [
        "Rosh Ho",
        "Jian Zhang"
      ],
      "arxiv_id": "2512.11824v1",
      "summary": "This paper presents ReGlove, a system that converts low-cost commercial pneumatic rehabilitation gloves into vision-guided assistive orthoses. Chronic upper-limb impairment affects millions worldwide, yet existing assistive technologies remain prohibitively expensive or rely on unreliable biological signals. Our platform integrates a wrist-mounted camera with an edge-computing inference engine (Raspberry Pi 5) to enable context-aware grasping without requiring reliable muscle signals. By adapting real-time YOLO-based computer vision models, the system achieves \\SI{96.73}{\\percent} grasp classification accuracy with sub-\\SI{40.00}{\\milli\\second} end-to-end latency. Physical validation using standardized benchmarks shows \\SI{82.71}{\\percent} success on YCB object manipulation and reliable performance across \\SI{27.00}{} Activities of Daily Living (ADL) tasks. With a total cost under \\$\\SI{250.00}{} and exclusively commercial components, ReGlove provides a technical foundation for accessible, vision-based upper-limb assistance that could benefit populations excluded from traditional EMG-controlled devices.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.11824v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "grasping",
            "grasp"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Stronger is not better: Better Augmentations in Contrastive Learning for Medical Image Segmentation",
      "authors": [
        "Azeez Idris",
        "Abdurahman Ali Mohammed",
        "Samuel Fanijo"
      ],
      "arxiv_id": "2512.05992v2",
      "summary": "Self-supervised contrastive learning is among the recent representation learning methods that have shown performance gains in several downstream tasks including semantic segmentation. This paper evaluates strong data augmentation, one of the most important components for self-supervised contrastive learning's improved performance. Strong data augmentation involves applying the composition of multiple augmentation techniques on images. Surprisingly, we find that the existing data augmentations do not always improve performance for semantic segmentation for medical images. We experiment with other augmentations that provide improved performance.",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "published": "2025-11-30",
      "updated": "2025-12-10",
      "comment": "NeurIPS Black in AI workshop - 2022",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.05992v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "representation learning",
            "[T]contrastive learning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "EmoDiffTalk:Emotion-aware Diffusion for Editable 3D Gaussian Talking Head",
      "authors": [
        "Chang Liu",
        "Tianjiao Jing",
        "Chengcheng Ma",
        "Xuanqi Zhou",
        "Zhengxuan Lian",
        "Qin Jin",
        "Hongliang Yuan",
        "Shi-Sheng Huang"
      ],
      "arxiv_id": "2512.05991v2",
      "summary": "Recent photo-realistic 3D talking head via 3D Gaussian Splatting still has significant shortcoming in emotional expression manipulation, especially for fine-grained and expansive dynamics emotional editing using multi-modal control. This paper introduces a new editable 3D Gaussian talking head, i.e. EmoDiffTalk. Our key idea is a novel Emotion-aware Gaussian Diffusion, which includes an action unit (AU) prompt Gaussian diffusion process for fine-grained facial animator, and moreover an accurate text-to-AU emotion controller to provide accurate and expansive dynamic emotional editing using text input. Experiments on public EmoTalk3D and RenderMe-360 datasets demonstrate superior emotional subtlety, lip-sync fidelity, and controllability of our EmoDiffTalk over previous works, establishing a principled pathway toward high-quality, diffusion-driven, multimodal editable 3D talking-head synthesis. To our best knowledge, our EmoDiffTalk is one of the first few 3D Gaussian Splatting talking-head generation framework, especially supporting continuous, multimodal emotional editing within the AU-based expression space.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-30",
      "updated": "2025-12-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.05991v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "gaussian splatting"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Image Generation as a Visual Planner for Robotic Manipulation",
      "authors": [
        "Ye Pang"
      ],
      "arxiv_id": "2512.00532v1",
      "summary": "Generating realistic robotic manipulation videos is an important step toward unifying perception, planning, and action in embodied agents. While existing video diffusion models require large domain-specific datasets and struggle to generalize, recent image generation models trained on language-image corpora exhibit strong compositionality, including the ability to synthesize temporally coherent grid images. This suggests a latent capacity for video-like generation even without explicit temporal modeling.\n  We explore whether such models can serve as visual planners for robots when lightly adapted using LoRA finetuning. We propose a two-part framework that includes: (1) text-conditioned generation, which uses a language instruction and the first frame, and (2) trajectory-conditioned generation, which uses a 2D trajectory overlay and the same initial frame. Experiments on the Jaco Play dataset, Bridge V2, and the RT1 dataset show that both modes produce smooth, coherent robot videos aligned with their respective conditions.\n  Our findings indicate that pretrained image generators encode transferable temporal priors and can function as video-like robotic planners under minimal supervision. Code is released at \\href{https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation}{https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation}.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-29",
      "updated": "2025-11-29",
      "comment": "11 pages 9 figures Under review at CVPR 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00532v1",
      "code_links": [
        {
          "url": "https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Sample-Efficient Expert Query Control in Active Imitation Learning via Conformal Prediction",
      "authors": [
        "Arad Firouzkouhi",
        "Omid Mirzaeedodangeh",
        "Lars Lindemann"
      ],
      "arxiv_id": "2512.00453v1",
      "summary": "Active imitation learning (AIL) combats covariate shift by querying an expert during training. However, expert action labeling often dominates the cost, especially in GPU-intensive simulators, human-in-the-loop settings, and robot fleets that revisit near-duplicate states. We present Conformalized Rejection Sampling for Active Imitation Learning (CRSAIL), a querying rule that requests an expert action only when the visited state is under-represented in the expert-labeled dataset. CRSAIL scores state novelty by the distance to the $K$-th nearest expert state and sets a single global threshold via conformal prediction. This threshold is the empirical $(1-α)$ quantile of on-policy calibration scores, providing a distribution-free calibration rule that links $α$ to the expected query rate and makes $α$ a task-agnostic tuning knob. This state-space querying strategy is robust to outliers and, unlike safety-gate-based AIL, can be run without real-time expert takeovers: we roll out full trajectories (episodes) with the learner and only afterward query the expert on a subset of visited states. Evaluated on MuJoCo robotics tasks, CRSAIL matches or exceeds expert-level reward while reducing total expert queries by up to 96% vs. DAgger and up to 65% vs. prior AIL methods, with empirical robustness to $α$ and $K$, easing deployment on novel systems with unknown dynamics.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-29",
      "updated": "2025-11-29",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00453v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]imitation learning",
            "MuJoCo"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Time-Series at the Edge: Tiny Separable CNNs for Wearable Gait Detection and Optimal Sensor Placement",
      "authors": [
        "Andrea Procopio",
        "Marco Esposito",
        "Sara Raggiunto",
        "Andrey Gizdov",
        "Alberto Belli",
        "Paola Pierleoni"
      ],
      "arxiv_id": "2512.00396v2",
      "summary": "We study on-device time-series analysis for gait detection in Parkinson's disease (PD) from short windows of triaxial acceleration, targeting resource-constrained wearables and edge nodes. We compare magnitude thresholding to three 1D CNNs for time-series analysis: a literature baseline (separable convolutions) and two ultra-light models - one purely separable and one with residual connections. Using the BioStampRC21 dataset, 2 s windows at 30 Hz, and subject-independent leave-one-subject-out (LOSO) validation on 16 PwPD with chest-worn IMUs, our residual separable model (Model 2, 533 params) attains PR-AUC = 94.5%, F1 = 91.2%, MCC = 89.4%, matching or surpassing the baseline (5,552 params; PR-AUC = 93.7%, F1 = 90.5%, MCC = 88.5%) with approximately 10x fewer parameters. The smallest model (Model 1, 305 params) reaches PR-AUC = 94.0%, F1 = 91.0%, MCC = 89.1%. Thresholding obtains high recall (89.0%) but low precision (76.5%), yielding many false positives and high inter-subject variance. Sensor-position analysis (train-on-all) shows chest and thighs are most reliable; forearms degrade precision/recall due to non-gait arm motion; naive fusion of all sites does not outperform the best single site. Both compact CNNs execute within tight memory/latency budgets on STM32-class MCUs (sub-10 ms on low-power boards), enabling on-sensor gating of transmission/storage. Overall, ultra-light separable CNNs provide a superior accuracy-efficiency-generalization trade-off to fixed thresholds for wearable PD gait detection and underscore the value of tailored time-series models for edge deployment.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-29",
      "updated": "2025-12-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00396v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]gait"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "THCRL: Trusted Hierarchical Contrastive Representation Learning for Multi-View Clustering",
      "authors": [
        "Jian Zhu"
      ],
      "arxiv_id": "2512.00368v2",
      "summary": "Multi-View Clustering (MVC) has garnered increasing attention in recent years. It is capable of partitioning data samples into distinct groups by learning a consensus representation. However, a significant challenge remains: the problem of untrustworthy fusion. This problem primarily arises from two key factors: 1) Existing methods often ignore the presence of inherent noise within individual views; 2) In traditional MVC methods using Contrastive Learning (CL), similarity computations typically rely on different views of the same instance, while neglecting the structural information from nearest neighbors within the same cluster. Consequently, this leads to the wrong direction for multi-view fusion. To address this problem, we present a novel Trusted Hierarchical Contrastive Representation Learning (THCRL). It consists of two key modules. Specifically, we propose the Deep Symmetry Hierarchical Fusion (DSHF) module, which leverages the UNet architecture integrated with multiple denoising mechanisms to achieve trustworthy fusion of multi-view data. Furthermore, we present the Average K-Nearest Neighbors Contrastive Learning (AKCL) module to align the fused representation with the view-specific representation. Unlike conventional strategies, AKCL enhances representation similarity among samples belonging to the same cluster, rather than merely focusing on the same sample across views, thereby reinforcing the confidence of the fused representation. Extensive experiments demonstrate that THCRL achieves the state-of-the-art performance in deep MVC tasks.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-29",
      "updated": "2025-12-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00368v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]representation learning",
            "contrastive learning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model",
      "authors": [
        "Zhe Li",
        "Xiang Bai",
        "Jieyu Zhang",
        "Zhuangzhe Wu",
        "Che Xu",
        "Ying Li",
        "Chengkai Hou",
        "Shanghang Zhang"
      ],
      "arxiv_id": "2511.00940v1",
      "summary": "Constructing accurate digital twins of articulated objects is essential for robotic simulation training and embodied AI world model building, yet historically requires painstaking manual modeling or multi-stage pipelines. In this work, we propose \\textbf{URDF-Anything}, an end-to-end automatic reconstruction framework based on a 3D multimodal large language model (MLLM). URDF-Anything utilizes an autoregressive prediction framework based on point-cloud and text multimodal input to jointly optimize geometric segmentation and kinematic parameter prediction. It implements a specialized $[SEG]$ token mechanism that interacts directly with point cloud features, enabling fine-grained part-level segmentation while maintaining consistency with the kinematic parameter predictions. Experiments on both simulated and real-world datasets demonstrate that our method significantly outperforms existing approaches regarding geometric segmentation (mIoU 17\\% improvement), kinematic parameter prediction (average error reduction of 29\\%), and physical executability (surpassing baselines by 50\\%). Notably, our method exhibits excellent generalization ability, performing well even on objects outside the training set. This work provides an efficient solution for constructing digital twins for robotic simulation, significantly enhancing the sim-to-real transfer capability.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-02",
      "updated": "2025-11-02",
      "comment": "Accepted to the 39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00940v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "sim-to-real"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "world model"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 5.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "A Study on Enhancing the Generalization Ability of Visuomotor Policies via Data Augmentation",
      "authors": [
        "Hanwen Wang"
      ],
      "arxiv_id": "2511.09932v1",
      "summary": "The generalization ability of visuomotor policy is crucial, as a good policy should be deployable across diverse scenarios. Some methods can collect large amounts of trajectory augmentation data to train more generalizable imitation learning policies, aimed at handling the random placement of objects on the scene's horizontal plane. However, the data generated by these methods still lack diversity, which limits the generalization ability of the trained policy. To address this, we investigate the performance of policies trained by existing methods across different scene layout factors via automate the data generation for those factors that significantly impact generalization. We have created a more extensively randomized dataset that can be efficiently and automatically generated with only a small amount of human demonstration. The dataset covers five types of manipulators and two types of grippers, incorporating extensive randomization factors such as camera pose, lighting conditions, tabletop texture, and table height across six manipulation tasks. We found that all of these factors influence the generalization ability of the policy. Applying any form of randomization enhances policy generalization, with diverse trajectories particularly effective in bridging visual gap. Notably, we investigated on low-cost manipulator the effect of the scene randomization proposed in this work on enhancing the generalization capability of visuomotor policies for zero-shot sim-to-real transfer.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09932v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "sim-to-real"
          ],
          "score": 4.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 5.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "EvoVLA: Self-Evolving Vision-Language-Action Model",
      "authors": [
        "Zeting Liu",
        "Zida Yang",
        "Zeyu Zhang",
        "Hao Tang"
      ],
      "arxiv_id": "2511.16166v1",
      "summary": "Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16166v1",
      "code_links": [
        {
          "url": "https://github.com/AIGeeksGroup/EvoVLA",
          "type": "github"
        },
        {
          "url": "https://aigeeksgroup.github.io/EvoVLA",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "sim-to-real"
          ],
          "score": 4.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "contrastive learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 5.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "GEN3D: Generating Domain-Free 3D Scenes from a Single Image",
      "authors": [
        "Yuxin Zhang",
        "Ziyu Lu",
        "Hongbo Duan",
        "Keyu Fan",
        "Pengting Luo",
        "Peiyu Zhuang",
        "Mengyu Yang",
        "Houde Liu"
      ],
      "arxiv_id": "2511.14291v1",
      "summary": "Despite recent advancements in neural 3D reconstruction, the dependence on dense multi-view captures restricts their broader applicability. Additionally, 3D scene generation is vital for advancing embodied AI and world models, which depend on diverse, high-quality scenes for learning and evaluation. In this work, we propose Gen3d, a novel method for generation of high-quality, wide-scope, and generic 3D scenes from a single image. After the initial point cloud is created by lifting the RGBD image, Gen3d maintains and expands its world model. The 3D scene is finalized through optimizing a Gaussian splatting representation. Extensive experiments on diverse datasets demonstrate the strong generalization capability and superior performance of our method in generating a world model and Synthesizing high-fidelity and consistent novel views.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "5 pages , 2 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14291v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "world model"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "gaussian splatting",
            "point cloud"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 5.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Discover, Learn, and Reinforce: Scaling Vision-Language-Action Pretraining with Diverse RL-Generated Trajectories",
      "authors": [
        "Rushuai Yang",
        "Zhiyuan Feng",
        "Tianxiang Zhang",
        "Kaixin Wang",
        "Chuheng Zhang",
        "Li Zhao",
        "Xiu Su",
        "Yi Chen",
        "Jiang Bian"
      ],
      "arxiv_id": "2511.19528v1",
      "summary": "Scaling vision-language-action (VLA) model pre-training requires large volumes of diverse, high-quality manipulation trajectories. Most current data is obtained via human teleoperation, which is expensive and difficult to scale. Reinforcement learning (RL) methods learn useful skills through autonomous exploration, making them a viable approach for generating data. However, standard RL training collapses to a narrow execution pattern, limiting its utility for large-scale pre-training. We propose Discover, Lea rn and Reinforce (DLR), an information-theoretic pattern discovery framework that generates multiple distinct, high-success behavioral patterns for VLA pretraining. Empirically, DLR generates a markedly more diverse trajectory corpus on LIBERO. Specifically, it learns multiple distinct, high-success strategies for the same task where standard RL discovers only one, and hence it covers substantially broader regions of the state-action space. When adapted to unseen downstream task suites, VLA models pretrained on our diverse RL data surpass counterparts trained on equal-sized standard RL datasets. Moreover, DLR exhibits positive data-scaling behavior that single-pattern RL lacks. These results position multi-pattern RL as a practical, scalable data engine for embodied foundation models.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19528v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "teleoperation"
          ],
          "score": 4.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 5.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Learning-based Cooperative Robotic Paper Wrapping: A Unified Control Policy with Residual Force Control",
      "authors": [
        "Rewida Ali",
        "Cristian C. Beltran-Hernandez",
        "Weiwei Wan",
        "Kensuke Harada"
      ],
      "arxiv_id": "2511.03181v1",
      "summary": "Human-robot cooperation is essential in environments such as warehouses and retail stores, where workers frequently handle deformable objects like paper, bags, and fabrics. Coordinating robotic actions with human assistance remains difficult due to the unpredictable dynamics of deformable materials and the need for adaptive force control. To explore this challenge, we focus on the task of gift wrapping, which exemplifies a long-horizon manipulation problem involving precise folding, controlled creasing, and secure fixation of paper. Success is achieved when the robot completes the sequence to produce a neatly wrapped package with clean folds and no tears.\n  We propose a learning-based framework that integrates a high-level task planner powered by a large language model (LLM) with a low-level hybrid imitation learning (IL) and reinforcement learning (RL) policy. At its core is a Sub-task Aware Robotic Transformer (START) that learns a unified policy from human demonstrations. The key novelty lies in capturing long-range temporal dependencies across the full wrapping sequence within a single model. Unlike vanilla Action Chunking with Transformer (ACT), typically applied to short tasks, our method introduces sub-task IDs that provide explicit temporal grounding. This enables robust performance across the entire wrapping process and supports flexible execution, as the policy learns sub-goals rather than merely replicating motion sequences.\n  Our framework achieves a 97% success rate on real-world wrapping tasks. We show that the unified transformer-based policy reduces the need for specialized models, allows controlled human supervision, and effectively bridges high-level intent with the fine-grained force control required for deformable object manipulation.",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-05",
      "updated": "2025-11-05",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03181v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "imitation learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 5.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "iFlyBot-VLA Technical Report",
      "authors": [
        "Yuan Zhang",
        "Chenyu Xue",
        "Wenjie Xu",
        "Chao Ji",
        "Jiajia wu",
        "Jia Pan"
      ],
      "arxiv_id": "2511.01914v1",
      "summary": "We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model trained under a novel framework. The main contributions are listed as follows: (1) a latent action model thoroughly trained on large-scale human and robotic manipulation videos; (2) a dual-level action representation framework that jointly supervises both the Vision-Language Model (VLM) and the action expert during training; (3) a mixed training strategy that combines robot trajectory data with general QA and spatial QA datasets, effectively enhancing the 3D perceptual and reasoning capabilities of the VLM backbone. Specifically, the VLM is trained to predict two complementary forms of actions: latent actions, derived from our latent action model pretrained on cross-embodiment manipulation data, which capture implicit high-level intentions; and structured discrete action tokens, obtained through frequency-domain transformations of continuous control signals, which encode explicit low-level dynamics. This dual supervision aligns the representation spaces of language, vision, and action, enabling the VLM to directly contribute to action generation. Experimental results on the LIBERO Franka benchmark demonstrate the superiority of our frame-work, while real-world evaluations further show that iFlyBot-VLA achieves competitive success rates across diverse and challenging manipulation tasks. Furthermore, we plan to open-source a portion of our self-constructed dataset to support future research in the community",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-01",
      "updated": "2025-11-01",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01914v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "cross-embodiment"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 5.0,
      "hit_pillars": [
        "1_robot_core",
        "7_retargeting"
      ]
    },
    {
      "title": "A Shared Control Framework for Mobile Robots with Planning-Level Intention Prediction",
      "authors": [
        "Jinyu Zhang",
        "Lijun Han",
        "Feng Jian",
        "Lingxi Zhang",
        "Hesheng Wang"
      ],
      "arxiv_id": "2511.08912v1",
      "summary": "In mobile robot shared control, effectively understanding human motion intention is critical for seamless human-robot collaboration. This paper presents a novel shared control framework featuring planning-level intention prediction. A path replanning algorithm is designed to adjust the robot's desired trajectory according to inferred human intentions. To represent future motion intentions, we introduce the concept of an intention domain, which serves as a constraint for path replanning. The intention-domain prediction and path replanning problems are jointly formulated as a Markov Decision Process and solved through deep reinforcement learning. In addition, a Voronoi-based human trajectory generation algorithm is developed, allowing the model to be trained entirely in simulation without human participation or demonstration data. Extensive simulations and real-world user studies demonstrate that the proposed method significantly reduces operator workload and enhances safety, without compromising task efficiency compared with existing assistive teleoperation approaches.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08912v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "teleoperation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "deep reinforcement learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 5.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Slow - Motion Video Synthesis for Basketball Using Frame Interpolation",
      "authors": [
        "Jiantang Huang"
      ],
      "arxiv_id": "2511.11644v1",
      "summary": "Basketball broadcast footage is traditionally captured at 30-60 fps, limiting viewers' ability to appreciate rapid plays such as dunks and crossovers. We present a real-time slow-motion synthesis system that produces high-quality basketball-specific interpolated frames by fine-tuning the recent Real-Time Intermediate Flow Estimation (RIFE) network on the SportsSloMo dataset. Our pipeline isolates the basketball subset of SportsSloMo, extracts training triplets, and fine-tunes RIFE with human-aware random cropping. We compare the resulting model against Super SloMo and the baseline RIFE model using Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM) on held-out clips. The fine-tuned RIFE attains a mean PSNR of 34.3 dB and SSIM of 0.949, outperforming Super SloMo by 2.1 dB and the baseline RIFE by 1.3 dB. A lightweight Gradio interface demonstrates end-to-end 4x slow-motion generation on a single RTX 4070 Ti Super at approximately 30 fps. These results indicate that task-specific adaptation is crucial for sports slow-motion, and that RIFE provides an attractive accuracy-speed trade-off for consumer applications.",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "3 pages, 4 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11644v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "motion synthesis",
            "motion generation"
          ],
          "score": 5.0
        }
      ],
      "relevance_score": 5.0,
      "hit_pillars": [
        "4_motion_diffusion"
      ]
    },
    {
      "title": "SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models",
      "authors": [
        "Senyu Fei",
        "Siyin Wang",
        "Li Ji",
        "Ao Li",
        "Shiduo Zhang",
        "Liming Liu",
        "Jinlong Hou",
        "Jingjing Gong",
        "Xianzhong Zhao",
        "Xipeng Qiu"
      ],
      "arxiv_id": "2511.15605v2",
      "summary": "Vision-Language-Action (VLA) models excel in robotic manipulation but are constrained by their heavy reliance on expert demonstrations, leading to demonstration bias and limiting performance. Reinforcement learning (RL) is a vital post-training strategy to overcome these limits, yet current VLA-RL methods, including group-based optimization approaches, are crippled by severe reward sparsity. Relying on binary success indicators wastes valuable information in failed trajectories, resulting in low training efficiency. To solve this, we propose Self-Referential Policy Optimization (SRPO), a novel VLA-RL framework. SRPO eliminates the need for external demonstrations or manual reward engineering by leveraging the model's own successful trajectories, generated within the current training batch, as a self-reference. This allows us to assign a progress-wise reward to failed attempts. A core innovation is the use of latent world representations to measure behavioral progress robustly. Instead of relying on raw pixels or requiring domain-specific fine-tuning, we utilize the compressed, transferable encodings from a world model's latent space. These representations naturally capture progress patterns across environments, enabling accurate, generalized trajectory comparison. Empirical evaluations on the LIBERO benchmark demonstrate SRPO's efficiency and effectiveness. Starting from a supervised baseline with 48.9% success, SRPO achieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision. Furthermore, SRPO shows substantial robustness, achieving a 167% performance improvement on the LIBERO-Plus benchmark.",
      "categories": [
        "cs.RO",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-19",
      "updated": "2025-11-30",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15605v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "world model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 5.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Theoretical Closed-loop Stability Bounds for Dynamical System Coupled with Diffusion Policies",
      "authors": [
        "Gabriel Lauzier",
        "Alexandre Girard",
        "François Ferland"
      ],
      "arxiv_id": "2511.15520v1",
      "summary": "Diffusion Policy has shown great performance in robotic manipulation tasks under stochastic perturbations, due to its ability to model multimodal action distributions. Nonetheless, its reliance on a computationally expensive reverse-time diffusion (denoising) process, for action inference, makes it challenging to use for real-time applications where quick decision-making is mandatory. This work studies the possibility of conducting the denoising process only partially before executing an action, allowing the plant to evolve according to its dynamics in parallel to the reverse-time diffusion dynamics ongoing on the computer. In a classical diffusion policy setting, the plant dynamics are usually slow and the two dynamical processes are uncoupled. Here, we investigate theoretical bounds on the stability of closed-loop systems using diffusion policies when the plant dynamics and the denoising dynamics are coupled. The contribution of this work gives a framework for faster imitation learning and a metric that yields if a controller will be stable based on the variance of the demonstrations.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "5 pages, 3 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15520v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning",
            "diffusion policy"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 5.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Decoupled Action Head: Confining Task Knowledge to Conditioning Layers",
      "authors": [
        "Jian Zhou",
        "Sihao Lin",
        "Shuai Fu",
        "Qi WU"
      ],
      "arxiv_id": "2511.12101v1",
      "summary": "Behavior Cloning (BC) is a data-driven supervised learning approach that has gained increasing attention with the success of scaling laws in language and vision domains. Among its implementations in robotic manipulation, Diffusion Policy (DP), with its two variants DP-CNN (DP-C) and DP-Transformer (DP-T), is one of the most effective and widely adopted models, demonstrating the advantages of predicting continuous action sequences. However, both DP and other BC methods remain constrained by the scarcity of paired training data, and the internal mechanisms underlying DP's effectiveness remain insufficiently understood, leading to limited generalization and a lack of principled design in model development. In this work, we propose a decoupled training recipe that leverages nearly cost-free kinematics-generated trajectories as observation-free data to pretrain a general action head (action generator). The pretrained action head is then frozen and adapted to novel tasks through feature modulation. Our experiments demonstrate the feasibility of this approach in both in-distribution and out-of-distribution scenarios. As an additional benefit, decoupling improves training efficiency; for instance, DP-C achieves up to a 41% speedup. Furthermore, the confinement of task-specific knowledge to the conditioning components under decoupling, combined with the near-identical performance of DP-C in both normal and decoupled training, indicates that the action generation backbone plays a limited role in robotic manipulation. Motivated by this observation, we introduce DP-MLP, which replaces the 244M-parameter U-Net backbone of DP-C with only 4M parameters of simple MLP blocks, achieving a 83.9% faster training speed under normal training and 89.1% under decoupling.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-15",
      "updated": "2025-11-15",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12101v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "behavior cloning",
            "diffusion policy"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 5.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "DocVAL: Validated Chain-of-Thought Distillation for Grounded Document VQA",
      "authors": [
        "Ahmad Mohammadshirazi",
        "Pinaki Prasad Guha Neogi",
        "Dheeraj Kulshrestha",
        "Rajiv Ramnath"
      ],
      "arxiv_id": "2511.22521v1",
      "summary": "Document visual question answering (DocVQA) requires models to jointly reason over textual content and spatial layout, yet current systems exhibit a sharp accuracy--efficiency trade-off: large teacher models achieve strong grounding but are too expensive for deployment, while compact students suffer substantial drops in localization performance. We propose DocVAL, a validated chain-of-thought distillation framework that transfers the spatial reasoning ability of a large teacher into a deployable student VLM through three key components: (1) teacher supervision with validation-time text detection to filter and denoise training signals, (2) a multi-module validator (VAL) that enforces answer correctness and geometric consistency while producing fine-grained, pixel-level error feedback, and (3) a two-stage student training scheme that first learns from validated CoT traces and then undergoes iterative refinement driven by VAL feedback. Our student (Gemma-3 12B) achieves 91.4\\% ANLS and 82.4\\% mAP on DocVQA as a pure VLM requiring no text detection or OCR at inference. Extensive ablations demonstrate that validated feedback contributes 6.3 mAP gain and iterative refinement accounts for 9.7 mAP improvement. We release 95k high-quality, validator-verified CoT traces to advance spatial reasoning research in document understanding.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22521v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "geometric consistency"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 5.0,
      "hit_pillars": [
        "3_perception_slam",
        "7_retargeting"
      ]
    },
    {
      "title": "MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts",
      "authors": [
        "Zilong Huang",
        "Jun He",
        "Xiaobin Huang",
        "Ziyi Xiong",
        "Yang Luo",
        "Junyan Ye",
        "Weijia Li",
        "Yiping Chen",
        "Ting Han"
      ],
      "arxiv_id": "2511.20415v2",
      "summary": "Generating realistic 3D cities is fundamental to world models, virtual reality, and game development, where an ideal urban scene must satisfy both stylistic diversity, fine-grained, and controllability. However, existing methods struggle to balance the creative flexibility offered by text-based generation with the object-level editability enabled by explicit structural representations. We introduce MajutsuCity, a natural language-driven and aesthetically adaptive framework for synthesizing structurally consistent and stylistically diverse 3D urban scenes. MajutsuCity represents a city as a composition of controllable layouts, assets, and materials, and operates through a four-stage pipeline. To extend controllability beyond initial generation, we further integrate MajutsuAgent, an interactive language-grounded editing agent} that supports five object-level operations. To support photorealistic and customizable scene synthesis, we also construct MajutsuDataset, a high-quality multimodal dataset} containing 2D semantic layouts and height maps, diverse 3D building assets, and curated PBR materials and skyboxes, each accompanied by detailed annotations. Meanwhile, we develop a practical set of evaluation metrics, covering key dimensions such as structural consistency, scene complexity, material fidelity, and lighting atmosphere. Extensive experiments demonstrate MajutsuCity reduces layout FID by 83.7% compared with CityDreamer and by 20.1% over CityCraft. Our method ranks first across all AQS and RDR scores, outperforming existing methods by a clear margin. These results confirm MajutsuCity as a new state-of-the-art in geometric fidelity, stylistic adaptability, and semantic controllability for 3D city generation. We expect our framework can inspire new avenues of research in 3D city generation. Our project page: https://longhz140516.github.io/MajutsuCity/.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-12-08",
      "comment": "13 pages, 6 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20415v2",
      "code_links": [
        {
          "url": "https://longhz140516.github.io/MajutsuCity/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "world model",
            "dreamer"
          ],
          "score": 3.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "height map"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 5.0,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "DRL-Guided Neural Batch Sampling for Semi-Supervised Pixel-Level Anomaly Detection",
      "authors": [
        "Amirhossein Khadivi Noghredeh",
        "Abdollah Safari",
        "Fatemeh Ziaeetabar",
        "Firoozeh Haghighi"
      ],
      "arxiv_id": "2511.20270v1",
      "summary": "Anomaly detection in industrial visual inspection is challenging due to the scarcity of defective samples. Most existing methods rely on unsupervised reconstruction using only normal data, often resulting in overfitting and poor detection of subtle defects. We propose a semi-supervised deep reinforcement learning framework that integrates a neural batch sampler, an autoencoder, and a predictor. The RL-based sampler adaptively selects informative patches by balancing exploration and exploitation through a composite reward. The autoencoder generates loss profiles highlighting abnormal regions, while the predictor performs segmentation in the loss-profile space. This interaction enables the system to effectively learn both normal and defective patterns with limited labeled data. Experiments on the MVTec AD dataset demonstrate that our method achieves higher accuracy and better localization of subtle anomalies than recent state-of-the-art approaches while maintaining low complexity, yielding an average improvement of 0.15 in F1_max and 0.06 in AUC, with a maximum gain of 0.37 in F1_max in the best case.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20270v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "deep reinforcement learning"
          ],
          "score": 3.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 5.0,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Bootstrap Dynamic-Aware 3D Visual Representation for Scalable Robot Learning",
      "authors": [
        "Qiwei Liang",
        "Boyang Cai",
        "Minghao Lai",
        "Sitong Zhuang",
        "Tao Lin",
        "Yan Qin",
        "Yixuan Ye",
        "Jiaming Liang",
        "Renjing Xu"
      ],
      "arxiv_id": "2512.00074v2",
      "summary": "Despite strong results on recognition and segmentation, current 3D visual pre-training methods often underperform on robotic manipulation. We attribute this gap to two factors: the lack of state-action-state dynamics modeling and the unnecessary redundancy of explicit geometric reconstruction. We introduce AFRO, a self-supervised framework that learns dynamics-aware 3D representations without action or reconstruction supervision. AFRO casts state prediction as a generative diffusion process and jointly models forward and inverse dynamics in a shared latent space to capture causal transition structure. To prevent feature leakage in action learning, we employ feature differencing and inverse-consistency supervision, improving the quality and stability of visual features. When combined with Diffusion Policy, AFRO substantially increases manipulation success rates across 16 simulated and 4 real-world tasks, outperforming existing pre-training approaches. The framework also scales favorably with data volume and task complexity. Qualitative visualizations indicate that AFRO learns semantically rich, discriminative features, offering an effective pre-training solution for 3D representation learning in robotics. Project page: https://kolakivy.github.io/AFRO/",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-25",
      "updated": "2025-12-04",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00074v2",
      "code_links": [
        {
          "url": "https://kolakivy.github.io/AFRO/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "diffusion policy",
            "representation learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 5.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding",
      "authors": [
        "Byeongjun Park",
        "Byung-Hoon Kim",
        "Hyungjin Chung",
        "Jong Chul Ye"
      ],
      "arxiv_id": "2511.19827v1",
      "summary": "We present ReDirector, a novel camera-controlled video retake generation method for dynamically captured variable-length videos. In particular, we rectify a common misuse of RoPE in previous works by aligning the spatiotemporal positions of the input video and the target retake. Moreover, we introduce Rotary Camera Encoding (RoCE), a camera-conditioned RoPE phase shift that captures and integrates multi-view relationships within and across the input and target videos. By integrating camera conditions into RoPE, our method generalizes to out-of-distribution camera trajectories and video lengths, yielding improved dynamic object localization and static background preservation. Extensive experiments further demonstrate significant improvements in camera controllability, geometric consistency, and video quality across various trajectories and lengths.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "Project page: https://byeongjun-park.github.io/ReDirector/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19827v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "geometric consistency"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 5.0,
      "hit_pillars": [
        "3_perception_slam",
        "7_retargeting"
      ]
    },
    {
      "title": "ReEXplore: Improving MLLMs for Embodied Exploration with Contextualized Retrospective Experience Replay",
      "authors": [
        "Gengyuan Zhang",
        "Mingcong Ding",
        "Jingpei Wu",
        "Ruotong Liao",
        "Volker Tresp"
      ],
      "arxiv_id": "2511.19033v1",
      "summary": "Embodied exploration is a target-driven process that requires embodied agents to possess fine-grained perception and knowledge-enhanced decision making. While recent attempts leverage MLLMs for exploration due to their strong perceptual and reasoning abilities, we find that MLLM-based embodied agents remain suboptimal in exploring new environments: (i) they rely on profound but stale pre-trained knowledge, (ii) training-based approaches such as imitation learning or reinforcement learning are expensive for long-horizon tasks with sparse outcome rewards, and (iii) frontier-based exploration yields a large, visually nuanced action space that is difficult for MLLMs to make reliable decisions. We address these challenges with ReEXplore, a training-free framework that performs retrospective experience replay to inject distilled, abstract experience at inference time, and hierarchical frontier selection to decompose frontier ranking into coarse-to-fine decisions. Our approach enables robust, traceable, and efficient exploration. Across multiple embodied exploration benchmarks, ReEXplore yields great improvements over strong MLLM baselines, up to 3x higher performance in both success rate and in navigation efficiency under open-source backbones.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "8 main pages plus 13 pages Appendix",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19033v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "imitation learning"
          ],
          "score": 3.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 5.0,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "SpeedAug: Policy Acceleration via Tempo-Enriched Policy and RL Fine-Tuning",
      "authors": [
        "Taewook Nam",
        "Sung Ju Hwang"
      ],
      "arxiv_id": "2512.00062v1",
      "summary": "Recent advances in robotic policy learning have enabled complex manipulation in real-world environments, yet the execution speed of these policies often lags behind hardware capabilities due to the cost of collecting faster demonstrations. Existing works on policy acceleration reinterpret action sequence for unseen execution speed, thereby encountering distributional shifts from the original demonstrations. Reinforcement learning is a promising approach that adapts policies for faster execution without additional demonstration, but its unguided exploration is sample inefficient. We propose SpeedAug, an RL-based policy acceleration framework that efficiently adapts pre-trained policies for faster task execution. SpeedAug constructs behavior prior that encompasses diverse tempos of task execution by pre-training a policy on speed-augmented demonstrations. Empirical results on robotic manipulation benchmarks show that RL fine-tuning initialized from this tempo-enriched policy significantly improves the sample efficiency of existing RL and policy acceleration methods while maintaining high success rate.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00062v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "policy learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 5.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "CADTrack: Learning Contextual Aggregation with Deformable Alignment for Robust RGBT Tracking",
      "authors": [
        "Hao Li",
        "Yuhao Wang",
        "Xiantao Hu",
        "Wenning Hao",
        "Pingping Zhang",
        "Dong Wang",
        "Huchuan Lu"
      ],
      "arxiv_id": "2511.17967v1",
      "summary": "RGB-Thermal (RGBT) tracking aims to exploit visible and thermal infrared modalities for robust all-weather object tracking. However, existing RGBT trackers struggle to resolve modality discrepancies, which poses great challenges for robust feature representation. This limitation hinders effective cross-modal information propagation and fusion, which significantly reduces the tracking accuracy. To address this limitation, we propose a novel Contextual Aggregation with Deformable Alignment framework called CADTrack for RGBT Tracking. To be specific, we first deploy the Mamba-based Feature Interaction (MFI) that establishes efficient feature interaction via state space models. This interaction module can operate with linear complexity, reducing computational cost and improving feature discrimination. Then, we propose the Contextual Aggregation Module (CAM) that dynamically activates backbone layers through sparse gating based on the Mixture-of-Experts (MoE). This module can encode complementary contextual information from cross-layer features. Finally, we propose the Deformable Alignment Module (DAM) to integrate deformable sampling and temporal propagation, mitigating spatial misalignment and localization drift. With the above components, our CADTrack achieves robust and accurate tracking in complex scenarios. Extensive experiments on five RGBT tracking benchmarks verify the effectiveness of our proposed method. The source code is released at https://github.com/IdolLab/CADTrack.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-22",
      "updated": "2025-11-22",
      "comment": "Accepted by AAAI2026. More modifications may be performed",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17967v1",
      "code_links": [
        {
          "url": "https://github.com/IdolLab/CADTrack",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "Mamba",
            "state space model"
          ],
          "score": 3.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 5.0,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "PanFlow: Decoupled Motion Control for Panoramic Video Generation",
      "authors": [
        "Cheng Zhang",
        "Hanwen Liang",
        "Donny Y. Chen",
        "Qianyi Wu",
        "Konstantinos N. Plataniotis",
        "Camilo Cruz Gambardella",
        "Jianfei Cai"
      ],
      "arxiv_id": "2512.00832v1",
      "summary": "Panoramic video generation has attracted growing attention due to its applications in virtual reality and immersive media. However, existing methods lack explicit motion control and struggle to generate scenes with large and complex motions. We propose PanFlow, a novel approach that exploits the spherical nature of panoramas to decouple the highly dynamic camera rotation from the input optical flow condition, enabling more precise control over large and dynamic motions. We further introduce a spherical noise warping strategy to promote loop consistency in motion across panorama boundaries. To support effective training, we curate a large-scale, motion-rich panoramic video dataset with frame-level pose and flow annotations. We also showcase the effectiveness of our method in various applications, including motion transfer and video editing. Extensive experiments demonstrate that PanFlow significantly outperforms prior methods in motion fidelity, visual quality, and temporal coherence. Our code, dataset, and models are available at https://github.com/chengzhag/PanFlow.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "Accepted by AAAI. Code: https://github.com/chengzhag/PanFlow",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00832v1",
      "code_links": [
        {
          "url": "https://github.com/chengzhag/PanFlow",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "optical flow"
          ],
          "score": 2.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "motion transfer"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 5.0,
      "hit_pillars": [
        "3_perception_slam",
        "7_retargeting"
      ]
    },
    {
      "title": "Transforming Monolithic Foundation Models into Embodied Multi-Agent Architectures for Human-Robot Collaboration",
      "authors": [
        "Nan Sun",
        "Bo Mao",
        "Yongchang Li",
        "Chenxu Wang",
        "Di Guo",
        "Huaping Liu"
      ],
      "arxiv_id": "2512.00797v1",
      "summary": "Foundation models have become central to unifying perception and planning in robotics, yet real-world deployment exposes a mismatch between their monolithic assumption that a single model can handle all cognitive functions and the distributed, dynamic nature of practical service workflows. Vision-language models offer strong semantic understanding but lack embodiment-aware action capabilities while relying on hand-crafted skills. Vision-Language-Action policies enable reactive manipulation but remain brittle across embodiments, weak in geometric grounding, and devoid of proactive collaboration mechanisms. These limitations indicate that scaling a single model alone cannot deliver reliable autonomy for service robots operating in human-populated settings. To address this gap, we present InteractGen, an LLM-powered multi-agent framework that decomposes robot intelligence into specialized agents for continuous perception, dependency-aware planning, decision and verification, failure reflection, and dynamic human delegation, treating foundation models as regulated components within a closed-loop collective. Deployed on a heterogeneous robot team and evaluated in a three-month open-use study, InteractGen improves task success, adaptability, and human-robot collaboration, providing evidence that multi-agent orchestration offers a more feasible path toward socially grounded service autonomy than further scaling standalone models.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "21 pages, 16 figures, 4 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00797v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "cross-embodiment"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 5.0,
      "hit_pillars": [
        "1_robot_core",
        "7_retargeting"
      ]
    },
    {
      "title": "DORAEMON: A Unified Library for Visual Object Modeling and Representation Learning at Scale",
      "authors": [
        "Ke Du",
        "Yimin Peng",
        "Chao Gao",
        "Fan Zhou",
        "Siqiao Xue"
      ],
      "arxiv_id": "2511.04394v1",
      "summary": "DORAEMON is an open-source PyTorch library that unifies visual object modeling and representation learning across diverse scales. A single YAML-driven workflow covers classification, retrieval and metric learning; more than 1000 pretrained backbones are exposed through a timm-compatible interface, together with modular losses, augmentations and distributed-training utilities. Reproducible recipes match or exceed reference results on ImageNet-1K, MS-Celeb-1M and Stanford online products, while one-command export to ONNX or HuggingFace bridges research and deployment. By consolidating datasets, models, and training techniques into one platform, DORAEMON offers a scalable foundation for rapid experimentation in visual recognition and representation learning, enabling efficient transfer of research advances to real-world applications. The repository is available at https://github.com/wuji3/DORAEMON.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "code: https://github.com/wuji3/DORAEMON",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04394v1",
      "code_links": [
        {
          "url": "https://github.com/wuji3/DORAEMON",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]representation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "M2S2L: Mamba-based Multi-Scale Spatial-temporal Learning for Video Anomaly Detection",
      "authors": [
        "Yang Liu",
        "Boan Chen",
        "Xiaoguang Zhu",
        "Jing Liu",
        "Peng Sun",
        "Wei Zhou"
      ],
      "arxiv_id": "2511.05564v1",
      "summary": "Video anomaly detection (VAD) is an essential task in the image processing community with prospects in video surveillance, which faces fundamental challenges in balancing detection accuracy with computational efficiency. As video content becomes increasingly complex with diverse behavioral patterns and contextual scenarios, traditional VAD approaches struggle to provide robust assessment for modern surveillance systems. Existing methods either lack comprehensive spatial-temporal modeling or require excessive computational resources for real-time applications. In this regard, we present a Mamba-based multi-scale spatial-temporal learning (M2S2L) framework in this paper. The proposed method employs hierarchical spatial encoders operating at multiple granularities and multi-temporal encoders capturing motion dynamics across different time scales. We also introduce a feature decomposition mechanism to enable task-specific optimization for appearance and motion reconstruction, facilitating more nuanced behavioral modeling and quality-aware anomaly assessment. Experiments on three benchmark datasets demonstrate that M2S2L framework achieves 98.5%, 92.1%, and 77.9% frame-level AUCs on UCSD Ped2, CUHK Avenue, and ShanghaiTech respectively, while maintaining efficiency with 20.1G FLOPs and 45 FPS inference speed, making it suitable for practical surveillance deployment.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-04",
      "updated": "2025-11-04",
      "comment": "IEEE VCIP 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05564v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Saliency-R1: Incentivizing Unified Saliency Reasoning Capability in MLLM with Confidence-Guided Reinforcement Learning",
      "authors": [
        "Long Li",
        "Shuichen Ji",
        "Ziyang Luo",
        "Zhihui Li",
        "Dingwen Zhang",
        "Junwei Han",
        "Nian Liu"
      ],
      "arxiv_id": "2511.00396v3",
      "summary": "Although multimodal large language models (MLLMs) excel in high-level vision-language reasoning, they lack inherent awareness of visual saliency, making it difficult to identify key visual elements. To bridge this gap, we propose Saliency-R1, the first unified MLLM framework that jointly tackles three representative and heterogeneous saliency tasks: Salient Object Detection (SOD), Salient Instance Segmentation (SIS), and Co-salient Object Detection (CoSOD), enhancing the model's capacity for saliency reasoning. We introduce a textual interface with structured tags (<rg>, <ins>) to encode region- and instance-level referring expressions, enabling a single referring segmenter to produce task-appropriate masks. To train the MLLM efficiently, we propose Confidence-Guided Policy Optimization (CGPO), a novel single-sample reinforcement learning algorithm. CGPO improves on GRPO by replacing group-normalized advantages with a per-sample signal based on reward-confidence discrepancy, thereby reducing computational waste, mitigating signal dilution, and lowering training overhead. Our model exceeds or matches the performance of robust open/closed-source MLLMs and specialized state-of-the-art methods across all three tasks, demonstrating the efficacy of our framework in saliency reasoning.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-01",
      "updated": "2025-11-26",
      "comment": "Main text (excluding references): 8 pages, 4 figures; Supplementary Materials (excluding references): 9 pages, 10 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00396v3",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning",
      "authors": [
        "Xuanle Zhao",
        "Deyang Jiang",
        "Zhixiong Zeng",
        "Lei Chen",
        "Haibo Qiu",
        "Jing Huang",
        "Yufeng Zhong",
        "Liming Zheng",
        "Yilin Cao",
        "Lin Ma"
      ],
      "arxiv_id": "2511.00391v2",
      "summary": "Multimodal code generation has garnered significant interest within the research community. Despite the notable success of recent vision-language models (VLMs) on specialized tasks like chart-to-code generation, their reliance on single-task training regimens fosters a narrow paradigm that hinders the development of generalized \\textbf{VI}sio\\textbf{N} \\textbf{C}ode \\textbf{I}ntelligence. In this work, we introduce \\textbf{VinciCoder}, a unified multimodal code generation model that addresses this limitation via a two-stage training framework. We begin by constructing a large-scale Supervised Finetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving direct code generation and visual-based code refinement. Subsequently, we introduce a Visual Reinforcement Learning (ViRL) strategy, which employs a coarse-to-fine reward mechanism to improve visual fidelity by calculating visual similarity across local and global image patches. Extensive experiments on diverse multimodal code generation benchmarks demonstrate that VinciCoder achieves state-of-the-art performance, surpassing recent open-source models. The ablation study further validates the effectiveness of our proposed coarse-to-fine ViRL strategy. The data, code and model is available at https://github.com/DocTron-hub/VinciCoder.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-01",
      "updated": "2025-11-27",
      "comment": "15 pages, 11 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00391v2",
      "code_links": [
        {
          "url": "https://github.com/DocTron-hub/VinciCoder",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Language-Guided Graph Representation Learning for Video Summarization",
      "authors": [
        "Wenrui Li",
        "Wei Han",
        "Hengyu Man",
        "Wangmeng Zuo",
        "Xiaopeng Fan",
        "Yonghong Tian"
      ],
      "arxiv_id": "2511.10953v1",
      "summary": "With the rapid growth of video content on social media, video summarization has become a crucial task in multimedia processing. However, existing methods face challenges in capturing global dependencies in video content and accommodating multimodal user customization. Moreover, temporal proximity between video frames does not always correspond to semantic proximity. To tackle these challenges, we propose a novel Language-guided Graph Representation Learning Network (LGRLN) for video summarization. Specifically, we introduce a video graph generator that converts video frames into a structured graph to preserve temporal order and contextual dependencies. By constructing forward, backward and undirected graphs, the video graph generator effectively preserves the sequentiality and contextual relationships of video content. We designed an intra-graph relational reasoning module with a dual-threshold graph convolution mechanism, which distinguishes semantically relevant frames from irrelevant ones between nodes. Additionally, our proposed language-guided cross-modal embedding module generates video summaries with specific textual descriptions. We model the summary generation output as a mixture of Bernoulli distribution and solve it with the EM algorithm. Experimental results show that our method outperforms existing approaches across multiple benchmarks. Moreover, we proposed LGRLN reduces inference time and model parameters by 87.8% and 91.7%, respectively. Our codes and pre-trained models are available at https://github.com/liwrui/LGRLN.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "comment": "Accepted by IEEE TPAMI",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10953v1",
      "code_links": [
        {
          "url": "https://github.com/liwrui/LGRLN",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]representation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals",
      "authors": [
        "Shruti Singh Baghel",
        "Yash Pratap Singh Rathore",
        "Sushovan Jena",
        "Anurag Pradhan",
        "Amit Shukla",
        "Arnav Bhavsar",
        "Pawan Goyal"
      ],
      "arxiv_id": "2511.10615v1",
      "summary": "Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "8 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10615v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "social interaction"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "3_perception_slam",
        "5_interaction_reaction"
      ]
    },
    {
      "title": "Multitask GLocal OBIA-Mamba for Sentinel-2 Landcover Mapping",
      "authors": [
        "Zack Dewis",
        "Yimin Zhu",
        "Zhengsen Xu",
        "Mabel Heffring",
        "Saeid Taleghanidoozdoozan",
        "Kaylee Xiao",
        "Motasem Alkayid",
        "Lincoln Linlin Xu"
      ],
      "arxiv_id": "2511.10604v1",
      "summary": "Although Sentinel-2 based land use and land cover (LULC) classification is critical for various environmental monitoring applications, it is a very difficult task due to some key data challenges (e.g., spatial heterogeneity, context information, signature ambiguity). This paper presents a novel Multitask Glocal OBIA-Mamba (MSOM) for enhanced Sentinel-2 classification with the following contributions. First, an object-based image analysis (OBIA) Mamba model (OBIA-Mamba) is designed to reduce redundant computation without compromising fine-grained details by using superpixels as Mamba tokens. Second, a global-local (GLocal) dual-branch convolutional neural network (CNN)-mamba architecture is designed to jointly model local spatial detail and global contextual information. Third, a multitask optimization framework is designed to employ dual loss functions to balance local precision with global consistency. The proposed approach is tested on Sentinel-2 imagery in Alberta, Canada, in comparison with several advanced classification approaches, and the results demonstrate that the proposed approach achieves higher classification accuracy and finer details that the other state-of-the-art methods.",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10604v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning",
      "authors": [
        "Yanbei Jiang",
        "Chao Lei",
        "Yihao Ding",
        "Krista Ehinger",
        "Jey Han Lau"
      ],
      "arxiv_id": "2511.10279v1",
      "summary": "Despite significant progress, Vision-Language Models (VLMs) still struggle with complex visual reasoning, where multi-step dependencies cause early errors to cascade through the reasoning chain. Existing post-training paradigms are limited: Supervised Fine-Tuning (SFT) relies on costly step-level annotations, while Reinforcement Learning with Verifiable Rewards (RLVR) methods like GRPO provide only sparse, outcome-level feedback, hindering stable optimization. We introduce PROPA (Process-level Reasoning Optimization with interleaved Policy Alignment), a novel framework that integrates Monte Carlo Tree Search (MCTS) with GRPO to generate dense, process-level rewards and optimize reasoning at each intermediate step without human annotations. To overcome the cold-start problem, PROPA interleaves GRPO updates with SFT, enabling the model to learn from both successful and failed reasoning trajectories. A Process Reward Model (PRM) is further trained to guide inference-time search, aligning the test-time search with the training signal. Across seven benchmarks and four VLM backbones, PROPA consistently outperforms both SFT- and RLVR-based baselines. It achieves up to 17.0% gains on in-domain tasks and 21.0% gains on out-of-domain tasks compared to existing state-of-the-art, establishing a strong reasoning and generalization capability for visual reasoning tasks. The code isavailable at: https://github.com/YanbeiJiang/PROPA.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10279v1",
      "code_links": [
        {
          "url": "https://github.com/YanbeiJiang/PROPA",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Learning by Neighbor-Aware Semantics, Deciding by Open-form Flows: Towards Robust Zero-Shot Skeleton Action Recognition",
      "authors": [
        "Yang Chen",
        "Miaoge Li",
        "Zhijie Rao",
        "Deze Zeng",
        "Song Guo",
        "Jingcai Guo"
      ],
      "arxiv_id": "2511.09388v1",
      "summary": "Recognizing unseen skeleton action categories remains highly challenging due to the absence of corresponding skeletal priors. Existing approaches generally follow an \"align-then-classify\" paradigm but face two fundamental issues, i.e., (i) fragile point-to-point alignment arising from imperfect semantics, and (ii) rigid classifiers restricted by static decision boundaries and coarse-grained anchors. To address these issues, we propose a novel method for zero-shot skeleton action recognition, termed $\\texttt{$\\textbf{Flora}$}$, which builds upon $\\textbf{F}$lexib$\\textbf{L}$e neighb$\\textbf{O}$r-aware semantic attunement and open-form dist$\\textbf{R}$ibution-aware flow cl$\\textbf{A}$ssifier. Specifically, we flexibly attune textual semantics by incorporating neighboring inter-class contextual cues to form direction-aware regional semantics, coupled with a cross-modal geometric consistency objective that ensures stable and robust point-to-region alignment. Furthermore, we employ noise-free flow matching to bridge the modality distribution gap between semantic and skeleton latent embeddings, while a condition-free contrastive regularization enhances discriminability, leading to a distribution-aware classifier with fine-grained decision boundaries achieved through token-level velocity predictions. Extensive experiments on three benchmark datasets validate the effectiveness of our method, showing particularly impressive performance even when trained with only 10\\% of the seen data. Code is available at https://github.com/cseeyangchen/Flora.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "Code is available at https://github.com/cseeyangchen/Flora",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09388v1",
      "code_links": [
        {
          "url": "https://github.com/cseeyangchen/Flora",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "flow matching"
          ],
          "score": 1.5
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "geometric consistency"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch",
        "7_retargeting"
      ]
    },
    {
      "title": "Enhancing Reinforcement Learning in 3D Environments through Semantic Segmentation: A Case Study in ViZDoom",
      "authors": [
        "Hugo Huang"
      ],
      "arxiv_id": "2511.11703v1",
      "summary": "Reinforcement learning (RL) in 3D environments with high-dimensional sensory input poses two major challenges: (1) the high memory consumption induced by memory buffers required to stabilise learning, and (2) the complexity of learning in partially observable Markov Decision Processes (POMDPs). This project addresses these challenges by proposing two novel input representations: SS-only and RGB+SS, both employing semantic segmentation on RGB colour images. Experiments were conducted in deathmatches of ViZDoom, utilizing perfect segmentation results for controlled evaluation. Our results showed that SS-only was able to reduce the memory consumption of memory buffers by at least 66.6%, and up to 98.6% when a vectorisable lossless compression technique with minimal overhead such as run-length encoding is applied. Meanwhile, RGB+SS significantly enhances RL agents' performance with the additional semantic information provided. Furthermore, we explored density-based heatmapping as a tool to visualise RL agents' movement patterns and evaluate their suitability for data collection. A brief comparison with a previous approach highlights how our method overcame common pitfalls in applying semantic segmentation in 3D environments like ViZDoom.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "Master's Thesis at the University of Edinburgh (2024)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11703v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "4KDehazeFlow: Ultra-High-Definition Image Dehazing via Flow Matching",
      "authors": [
        "Xingchi Chen",
        "Pu Wang",
        "Xuerui Li",
        "Chaopeng Li",
        "Juxiang Zhou",
        "Jianhou Gan",
        "Dianjie Lu",
        "Guijuan Zhang",
        "Wenqi Ren",
        "Zhuoran Zheng"
      ],
      "arxiv_id": "2511.09055v1",
      "summary": "Ultra-High-Definition (UHD) image dehazing faces challenges such as limited scene adaptability in prior-based methods and high computational complexity with color distortion in deep learning approaches. To address these issues, we propose 4KDehazeFlow, a novel method based on Flow Matching and the Haze-Aware vector field. This method models the dehazing process as a progressive optimization of continuous vector field flow, providing efficient data-driven adaptive nonlinear color transformation for high-quality dehazing. Specifically, our method has the following advantages: 1) 4KDehazeFlow is a general method compatible with various deep learning networks, without relying on any specific network architecture. 2) We propose a learnable 3D lookup table (LUT) that encodes haze transformation parameters into a compact 3D mapping matrix, enabling efficient inference through precomputed mappings. 3) We utilize a fourth-order Runge-Kutta (RK4) ordinary differential equation (ODE) solver to stably solve the dehazing flow field through an accurate step-by-step iterative method, effectively suppressing artifacts. Extensive experiments show that 4KDehazeFlow exceeds seven state-of-the-art methods. It delivers a 2dB PSNR increase and better performance in dense haze and color fidelity.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09055v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]flow matching"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "3D4D: An Interactive, Editable, 4D World Model via 3D Video Generation",
      "authors": [
        "Yunhong He",
        "Zhengqing Yuan",
        "Zhengzhong Tu",
        "Yanfang Ye",
        "Lichao Sun"
      ],
      "arxiv_id": "2511.08536v1",
      "summary": "We introduce 3D4D, an interactive 4D visualization framework that integrates WebGL with Supersplat rendering. It transforms static images and text into coherent 4D scenes through four core modules and employs a foveated rendering strategy for efficient, real-time multi-modal interaction. This framework enables adaptive, user-driven exploration of complex 4D environments. The project page and code are available at https://yunhonghe1021.github.io/NOVA/.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "Accepted by AAAI 2026 Demo Track",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08536v1",
      "code_links": [
        {
          "url": "https://yunhonghe1021.github.io/NOVA/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]world model"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Aligning by Misaligning: Boundary-aware Curriculum Learning for Multimodal Alignment",
      "authors": [
        "Hua Ye",
        "Hang Ding",
        "Siyuan Chen",
        "Yiyang Jiang",
        "Changyuan Zhang",
        "Xuan Zhang"
      ],
      "arxiv_id": "2511.08399v1",
      "summary": "Most multimodal models treat every negative pair alike, ignoring the ambiguous negatives that differ from the positive by only a small detail. We propose Boundary-Aware Curriculum with Local Attention (BACL), a lightweight add-on that turns these borderline cases into a curriculum signal. A Boundary-aware Negative Sampler gradually raises difficulty, while a Contrastive Local Attention loss highlights where the mismatch occurs. The two modules are fully differentiable and work with any off-the-shelf dual encoder. Theory predicts a fast O(1/n) error rate; practice shows up to +32% R@1 over CLIP and new SOTA on four large-scale benchmarks, all without extra labels.",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "24 pages, 6 figures, 5 tables. Submitted to NeurIPS 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08399v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]curriculum learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "DI3CL: Contrastive Learning With Dynamic Instances and Contour Consistency for SAR Land-Cover Classification Foundation Model",
      "authors": [
        "Zhongle Ren",
        "Hui Ding",
        "Kai Wang",
        "Biao Hou",
        "Xingyu Luo",
        "Weibin Li",
        "Licheng Jiao"
      ],
      "arxiv_id": "2511.07808v2",
      "summary": "Although significant advances have been achieved in SAR land-cover classification, recent methods remain predominantly focused on supervised learning, which relies heavily on extensive labeled datasets. This dependency not only limits scalability and generalization but also restricts adaptability to diverse application scenarios. In this paper, a general-purpose foundation model for SAR land-cover classification is developed, serving as a robust cornerstone to accelerate the development and deployment of various downstream models. Specifically, a Dynamic Instance and Contour Consistency Contrastive Learning (DI3CL) pre-training framework is presented, which incorporates a Dynamic Instance (DI) module and a Contour Consistency (CC) module. DI module enhances global contextual awareness by enforcing local consistency across different views of the same region. CC module leverages shallow feature maps to guide the model to focus on the geometric contours of SAR land-cover objects, thereby improving structural discrimination. Additionally, to enhance robustness and generalization during pre-training, a large-scale and diverse dataset named SARSense, comprising 460,532 SAR images, is constructed to enable the model to capture comprehensive and representative features. To evaluate the generalization capability of our foundation model, we conducted extensive experiments across a variety of SAR land-cover classification tasks, including SAR land-cover mapping, water body detection, and road extraction. The results consistently demonstrate that the proposed DI3CL outperforms existing methods. Our code and pre-trained weights are publicly available at: https://github.com/SARpre-train/DI3CL.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-12",
      "comment": "18 pages, 10 figures;Submitted to IEEE Transactions on Image Processing (TIP); In peer review",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07808v2",
      "code_links": [
        {
          "url": "https://github.com/SARpre-train/DI3CL",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]contrastive learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "MirrorMamba: Towards Scalable and Robust Mirror Detection in Videos",
      "authors": [
        "Rui Song",
        "Jiaying Lin",
        "Rynson W. H. Lau"
      ],
      "arxiv_id": "2511.06716v1",
      "summary": "Video mirror detection has received significant research attention, yet existing methods suffer from limited performance and robustness. These approaches often over-rely on single, unreliable dynamic features, and are typically built on CNNs with limited receptive fields or Transformers with quadratic computational complexity. To address these limitations, we propose a new effective and scalable video mirror detection method, called MirrorMamba. Our approach leverages multiple cues to adapt to diverse conditions, incorporating perceived depth, correspondence and optical. We also introduce an innovative Mamba-based Multidirection Correspondence Extractor, which benefits from the global receptive field and linear complexity of the emerging Mamba spatial state model to effectively capture correspondence properties. Additionally, we design a Mamba-based layer-wise boundary enforcement decoder to resolve the unclear boundary caused by the blurred depth map. Notably, this work marks the first successful application of the Mamba-based architecture in the field of mirror detection. Extensive experiments demonstrate that our method outperforms existing state-of-the-art approaches for video mirror detection on the benchmark datasets. Furthermore, on the most challenging and representative image-based mirror detection dataset, our approach achieves state-of-the-art performance, proving its robustness and generalizability.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06716v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Spatial-Frequency Enhanced Mamba for Multi-Modal Image Fusion",
      "authors": [
        "Hui Sun",
        "Long Lv",
        "Pingping Zhang",
        "Tongdan Tang",
        "Feng Tian",
        "Weibing Sun",
        "Huchuan Lu"
      ],
      "arxiv_id": "2511.06593v1",
      "summary": "Multi-Modal Image Fusion (MMIF) aims to integrate complementary image information from different modalities to produce informative images. Previous deep learning-based MMIF methods generally adopt Convolutional Neural Networks (CNNs) or Transformers for feature extraction. However, these methods deliver unsatisfactory performances due to the limited receptive field of CNNs and the high computational cost of Transformers. Recently, Mamba has demonstrated a powerful potential for modeling long-range dependencies with linear complexity, providing a promising solution to MMIF. Unfortunately, Mamba lacks full spatial and frequency perceptions, which are very important for MMIF. Moreover, employing Image Reconstruction (IR) as an auxiliary task has been proven beneficial for MMIF. However, a primary challenge is how to leverage IR efficiently and effectively. To address the above issues, we propose a novel framework named Spatial-Frequency Enhanced Mamba Fusion (SFMFusion) for MMIF. More specifically, we first propose a three-branch structure to couple MMIF and IR, which can retain complete contents from source images. Then, we propose the Spatial-Frequency Enhanced Mamba Block (SFMB), which can enhance Mamba in both spatial and frequency domains for comprehensive feature extraction. Finally, we propose the Dynamic Fusion Mamba Block (DFMB), which can be deployed across different branches for dynamic feature fusion. Extensive experiments show that our method achieves better results than most state-of-the-art methods on six MMIF datasets. The source code is available at https://github.com/SunHui1216/SFMFusion.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "This work is accepted by IEEE Transactions on Image Processing. More modifications may be performed",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06593v1",
      "code_links": [
        {
          "url": "https://github.com/SunHui1216/SFMFusion",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "VideoSSR: Video Self-Supervised Reinforcement Learning",
      "authors": [
        "Zefeng He",
        "Xiaoye Qu",
        "Yafu Li",
        "Siyuan Huang",
        "Daizong Liu",
        "Yu Cheng"
      ],
      "arxiv_id": "2511.06281v1",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has substantially advanced the video understanding capabilities of Multimodal Large Language Models (MLLMs). However, the rapid progress of MLLMs is outpacing the complexity of existing video datasets, while the manual annotation of new, high-quality data remains prohibitively expensive. This work investigates a pivotal question: Can the rich, intrinsic information within videos be harnessed to self-generate high-quality, verifiable training data? To investigate this, we introduce three self-supervised pretext tasks: Anomaly Grounding, Object Counting, and Temporal Jigsaw. We construct the Video Intrinsic Understanding Benchmark (VIUBench) to validate their difficulty, revealing that current state-of-the-art MLLMs struggle significantly on these tasks. Building upon these pretext tasks, we develop the VideoSSR-30K dataset and propose VideoSSR, a novel video self-supervised reinforcement learning framework for RLVR. Extensive experiments across 17 benchmarks, spanning four major video domains (General Video QA, Long Video QA, Temporal Grounding, and Complex Reasoning), demonstrate that VideoSSR consistently enhances model performance, yielding an average improvement of over 5\\%. These results establish VideoSSR as a potent foundational framework for developing more advanced video understanding in MLLMs. The code is available at https://github.com/lcqysl/VideoSSR.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-09",
      "updated": "2025-11-09",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06281v1",
      "code_links": [
        {
          "url": "https://github.com/lcqysl/VideoSSR",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Latent Refinement via Flow Matching for Training-free Linear Inverse Problem Solving",
      "authors": [
        "Hossein Askari",
        "Yadan Luo",
        "Hongfu Sun",
        "Fred Roosta"
      ],
      "arxiv_id": "2511.06138v1",
      "summary": "Recent advances in inverse problem solving have increasingly adopted flow priors over diffusion models due to their ability to construct straight probability paths from noise to data, thereby enhancing efficiency in both training and inference. However, current flow-based inverse solvers face two primary limitations: (i) they operate directly in pixel space, which demands heavy computational resources for training and restricts scalability to high-resolution images, and (ii) they employ guidance strategies with prior-agnostic posterior covariances, which can weaken alignment with the generative trajectory and degrade posterior coverage. In this paper, we propose LFlow (Latent Refinement via Flows), a training-free framework for solving linear inverse problems via pretrained latent flow priors. LFlow leverages the efficiency of flow matching to perform ODE sampling in latent space along an optimal path. This latent formulation further allows us to introduce a theoretically grounded posterior covariance, derived from the optimal vector field, enabling effective flow guidance. Experimental results demonstrate that our proposed method outperforms state-of-the-art latent diffusion solvers in reconstruction quality across most tasks. The code will be publicly available at https://github.com/hosseinaskari-cs/LFlow .",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-08",
      "updated": "2025-11-08",
      "comment": "37 pages, 16 figures,",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06138v1",
      "code_links": [
        {
          "url": "https://github.com/hosseinaskari-cs/LFlow",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]flow matching"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Parts-Mamba: Augmenting Joint Context with Part-Level Scanning for Occluded Human Skeleton",
      "authors": [
        "Tianyi Shen",
        "Huijuan Xu",
        "Nilesh Ahuja",
        "Omesh Tickoo",
        "Philip Shin",
        "Vijaykrishnan Narayanan"
      ],
      "arxiv_id": "2511.16860v1",
      "summary": "Skeleton action recognition involves recognizing human action from human skeletons. The use of graph convolutional networks (GCNs) has driven major advances in this recognition task. In real-world scenarios, the captured skeletons are not always perfect or complete because of occlusions of parts of the human body or poor communication quality, leading to missing parts in skeletons or videos with missing frames. In the presence of such non-idealities, existing GCN models perform poorly due to missing local context. To address this limitation, we propose Parts-Mamba, a hybrid GCN-Mamba model designed to enhance the ability to capture and maintain contextual information from distant joints. The proposed Parts-Mamba model effectively captures part-specific information through its parts-specific scanning feature and preserves non-neighboring joint context via a parts-body fusion module. Our proposed model is evaluated on the NTU RGB+D 60 and NTU RGB+D 120 datasets under different occlusion settings, achieving up to 12.9% improvement in accuracy.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-21",
      "updated": "2025-11-21",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16860v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding",
      "authors": [
        "Boshen Xu",
        "Zihan Xiao",
        "Jiaze Li",
        "Jianzhong Ju",
        "Zhenbo Luo",
        "Jian Luan",
        "Qin Jin"
      ],
      "arxiv_id": "2511.16595v2",
      "summary": "We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-26",
      "comment": "Project page: https://xuboshen.github.io/TimeViper; Code: https://github.com/xiaomi-research/timeviper",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16595v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution",
      "authors": [
        "Jaime Álvarez Urueña",
        "David Camacho",
        "Javier Huertas Tato"
      ],
      "arxiv_id": "2511.16541v2",
      "summary": "The rapid advancement of generative artificial intelligence has enabled the creation of synthetic images that are increasingly indistinguishable from authentic content, posing significant challenges for digital media integrity. This problem is compounded by the accelerated release cycle of novel generative models, which renders traditional detection approaches (reliant on periodic retraining) computationally infeasible and operationally impractical.\n  This work proposes a novel two-stage detection framework designed to address the generalization challenge inherent in synthetic image detection. The first stage employs a vision deep learning model trained via supervised contrastive learning to extract discriminative embeddings from input imagery. Critically, this model was trained on a strategically partitioned subset of available generators, with specific architectures withheld from training to rigorously ablate cross-generator generalization capabilities. The second stage utilizes a k-nearest neighbors (k-NN) classifier operating on the learned embedding space, trained in a few-shot learning paradigm incorporating limited samples from previously unseen test generators.\n  With merely 150 images per class in the few-shot learning regime, which are easily obtainable from current generation models, the proposed framework achieves an average detection accuracy of 91.3%, representing a 5.2 percentage point improvement over existing approaches . For the source attribution task, the proposed approach obtains improvements of of 14.70% and 4.27% in AUC and OSCR respectively on an open set classification context, marking a significant advancement toward robust, scalable forensic attribution systems capable of adapting to the evolving generative AI landscape without requiring exhaustive retraining protocols.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-21",
      "comment": "17 pages, 6 figures, 6 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16541v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]contrastive learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Saving Foundation Flow-Matching Priors for Inverse Problems",
      "authors": [
        "Yuxiang Wan",
        "Ryan Devera",
        "Wenjie Zhang",
        "Ju Sun"
      ],
      "arxiv_id": "2511.16520v1",
      "summary": "Foundation flow-matching (FM) models promise a universal prior for solving inverse problems (IPs), yet today they trail behind domain-specific or even untrained priors. How can we unlock their potential? We introduce FMPlug, a plug-in framework that redefines how foundation FMs are used in IPs. FMPlug combines an instance-guided, time-dependent warm-start strategy with a sharp Gaussianity regularization, adding problem-specific guidance while preserving the Gaussian structures. This leads to a significant performance boost across image restoration and scientific IPs. Our results point to a path for making foundation FM models practical, reusable priors for IP solving.",
      "categories": [
        "cs.LG",
        "cs.CV",
        "eess.IV",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16520v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]flow matching"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Safe and Optimal Variable Impedance Control via Certified Reinforcement Learning",
      "authors": [
        "Shreyas Kumar",
        "Ravi Prakash"
      ],
      "arxiv_id": "2511.16330v1",
      "summary": "Reinforcement learning (RL) offers a powerful approach for robots to learn complex, collaborative skills by combining Dynamic Movement Primitives (DMPs) for motion and Variable Impedance Control (VIC) for compliant interaction. However, this model-free paradigm often risks instability and unsafe exploration due to the time-varying nature of impedance gains. This work introduces Certified Gaussian Manifold Sampling (C-GMS), a novel trajectory-centric RL framework that learns combined DMP and VIC policies while guaranteeing Lyapunov stability and actuator feasibility by construction. Our approach reframes policy exploration as sampling from a mathematically defined manifold of stable gain schedules. This ensures every policy rollout is guaranteed to be stable and physically realizable, thereby eliminating the need for reward penalties or post-hoc validation. Furthermore, we provide a theoretical guarantee that our approach ensures bounded tracking error even in the presence of bounded model errors and deployment-time uncertainties. We demonstrate the effectiveness of C-GMS in simulation and verify its efficacy on a real robot, paving the way for reliable autonomous interaction in complex environments.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16330v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "VideoSeg-R1:Reasoning Video Object Segmentation via Reinforcement Learning",
      "authors": [
        "Zishan Xu",
        "Yifu Guo",
        "Yuquan Lu",
        "Fengyu Yang",
        "Junxin Li"
      ],
      "arxiv_id": "2511.16077v1",
      "summary": "Traditional video reasoning segmentation methods rely on supervised fine-tuning, which limits generalization to out-of-distribution scenarios and lacks explicit reasoning. To address this, we propose \\textbf{VideoSeg-R1}, the first framework to introduce reinforcement learning into video reasoning segmentation. It adopts a decoupled architecture that formulates the task as joint referring image segmentation and video mask propagation. It comprises three stages: (1) A hierarchical text-guided frame sampler to emulate human attention; (2) A reasoning model that produces spatial cues along with explicit reasoning chains; and (3) A segmentation-propagation stage using SAM2 and XMem. A task difficulty-aware mechanism adaptively controls reasoning length for better efficiency and accuracy. Extensive evaluations on multiple benchmarks demonstrate that VideoSeg-R1 achieves state-of-the-art performance in complex video reasoning and segmentation tasks. The code will be publicly available at https://github.com/euyis1019/VideoSeg-R1.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16077v1",
      "code_links": [
        {
          "url": "https://github.com/euyis1019/VideoSeg-R1",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Path Planning through Multi-Agent Reinforcement Learning in Dynamic Environments",
      "authors": [
        "Jonas De Maeyer",
        "Hossein Yarahmadi",
        "Moharram Challenger"
      ],
      "arxiv_id": "2511.15284v1",
      "summary": "Path planning in dynamic environments is a fundamental challenge in intelligent transportation and robotics, where obstacles and conditions change over time, introducing uncertainty and requiring continuous adaptation. While existing approaches often assume complete environmental unpredictability or rely on global planners, these assumptions limit scalability and practical deployment in real-world settings. In this paper, we propose a scalable, region-aware reinforcement learning (RL) framework for path planning in dynamic environments. Our method builds on the observation that environmental changes, although dynamic, are often localized within bounded regions. To exploit this, we introduce a hierarchical decomposition of the environment and deploy distributed RL agents that adapt to changes locally. We further propose a retraining mechanism based on sub-environment success rates to determine when policy updates are necessary. Two training paradigms are explored: single-agent Q-learning and multi-agent federated Q-learning, where local Q-tables are aggregated periodically to accelerate the learning process. Unlike prior work, we evaluate our methods in more realistic settings, where multiple simultaneous obstacle changes and increasing difficulty levels are present. Results show that the federated variants consistently outperform their single-agent counterparts and closely approach the performance of A* Oracle while maintaining shorter adaptation times and robust scalability. Although initial training remains time-consuming in large environments, our decentralized framework eliminates the need for a global planner and lays the groundwork for future improvements using deep RL and flexible environment decomposition.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15284v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Towards Unbiased Cross-Modal Representation Learning for Food Image-to-Recipe Retrieval",
      "authors": [
        "Qing Wang",
        "Chong-Wah Ngo",
        "Ee-Peng Lim"
      ],
      "arxiv_id": "2511.15201v1",
      "summary": "This paper addresses the challenges of learning representations for recipes and food images in the cross-modal retrieval problem. As the relationship between a recipe and its cooked dish is cause-and-effect, treating a recipe as a text source describing the visual appearance of a dish for learning representation, as the existing approaches, will create bias misleading image-and-recipe similarity judgment. Specifically, a food image may not equally capture every detail in a recipe, due to factors such as the cooking process, dish presentation, and image-capturing conditions. The current representation learning tends to capture dominant visual-text alignment while overlooking subtle variations that determine retrieval relevance. In this paper, we model such bias in cross-modal representation learning using causal theory. The causal view of this problem suggests ingredients as one of the confounder sources and a simple backdoor adjustment can alleviate the bias. By causal intervention, we reformulate the conventional model for food-to-recipe retrieval with an additional term to remove the potential bias in similarity judgment. Based on this theory-informed formulation, we empirically prove the oracle performance of retrieval on the Recipe1M dataset to be MedR=1 across the testing data sizes of 1K, 10K, and even 50K. We also propose a plug-and-play neural module, which is essentially a multi-label ingredient classifier for debiasing. New state-of-the-art search performances are reported on the Recipe1M dataset.",
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15201v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]representation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "BokehFlow: Depth-Free Controllable Bokeh Rendering via Flow Matching",
      "authors": [
        "Yachuan Huang",
        "Xianrui Luo",
        "Qiwen Wang",
        "Liao Shen",
        "Jiaqi Li",
        "Huiqiang Sun",
        "Zihao Huang",
        "Wei Jiang",
        "Zhiguo Cao"
      ],
      "arxiv_id": "2511.15066v1",
      "summary": "Bokeh rendering simulates the shallow depth-of-field effect in photography, enhancing visual aesthetics and guiding viewer attention to regions of interest. Although recent approaches perform well, rendering controllable bokeh without additional depth inputs remains a significant challenge. Existing classical and neural controllable methods rely on accurate depth maps, while generative approaches often struggle with limited controllability and efficiency. In this paper, we propose BokehFlow, a depth-free framework for controllable bokeh rendering based on flow matching. BokehFlow directly synthesizes photorealistic bokeh effects from all-in-focus images, eliminating the need for depth inputs. It employs a cross-attention mechanism to enable semantic control over both focus regions and blur intensity via text prompts. To support training and evaluation, we collect and synthesize four datasets. Extensive experiments demonstrate that BokehFlow achieves visually compelling bokeh effects and offers precise control, outperforming existing depth-dependent and generative methods in both rendering quality and efficiency.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15066v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]flow matching"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Z-Merge: Multi-Agent Reinforcement Learning for On-Ramp Merging with Zone-Specific V2X Traffic Information",
      "authors": [
        "Yassine Ibork",
        "Myounggyu Won",
        "Lokesh Das"
      ],
      "arxiv_id": "2511.14910v1",
      "summary": "Ramp merging is a critical and challenging task for autonomous vehicles (AVs), particularly in mixed traffic environments with human-driven vehicles (HVs). Existing approaches typically rely on either lane-changing or inter-vehicle gap creation strategies based solely on local or neighboring information, often leading to suboptimal performance in terms of safety and traffic efficiency. In this paper, we present a V2X (vehicle-to-everything communication)-assisted Multiagent Reinforcement Learning (MARL) framework for on-ramp merging that effectively coordinates the complex interplay between lane-changing and inter-vehicle gap adaptation strategies by utilizing zone-specific global information available from a roadside unit (RSU). The merging control problem is formulated as a Multiagent Partially Observable Markov Decision Process (MA-POMDP), where agents leverage both local and global observations through V2X communication. To support both discrete and continuous control decisions, we design a hybrid action space and adopt a parameterized deep Q-learning approach. Extensive simulations, integrating the SUMO traffic simulator and the MOSAIC V2X simulator, demonstrate that our framework significantly improves merging success rate, traffic efficiency, and road safety across diverse traffic scenarios.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14910v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "IMSE: Efficient U-Net-based Speech Enhancement using Inception Depthwise Convolution and Amplitude-Aware Linear Attention",
      "authors": [
        "Xinxin Tang",
        "Bin Qin",
        "Yufang Li"
      ],
      "arxiv_id": "2511.14515v2",
      "summary": "Achieving a balance between lightweight design and high performance remains a significant challenge for speech enhancement (SE) tasks on resource-constrained devices. Existing state-of-the-art methods, such as MUSE, have established a strong baseline with only 0.51M parameters by introducing a Multi-path Enhanced Taylor (MET) transformer and Deformable Embedding (DE). However, an in-depth analysis reveals that MUSE still suffers from efficiency bottlenecks: the MET module relies on a complex \"approximate-compensate\" mechanism to mitigate the limitations of Taylor-expansion-based attention, while the offset calculation for deformable embedding introduces additional computational burden. This paper proposes IMSE, a systematically optimized and ultra-lightweight network. We introduce two core innovations: 1) Replacing the MET module with Amplitude-Aware Linear Attention (MALA). MALA fundamentally rectifies the \"amplitude-ignoring\" problem in linear attention by explicitly preserving the norm information of query vectors in the attention calculation, achieving efficient global modeling without an auxiliary compensation branch. 2) Replacing the DE module with Inception Depthwise Convolution (IDConv). IDConv borrows the Inception concept, decomposing large-kernel operations into efficient parallel branches (square, horizontal, and vertical strips), thereby capturing spectrogram features with extremely low parameter redundancy. Extensive experiments on the VoiceBank+DEMAND dataset demonstrate that, compared to the MUSE baseline, IMSE significantly reduces the parameter count by 16.8\\% (from 0.513M to 0.427M) while achieving competitive performance comparable to the state-of-the-art on the PESQ metric (3.373). This study sets a new benchmark for the trade-off between model size and speech quality in ultra-lightweight speech enhancement.",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.SD",
      "published": "2025-11-18",
      "updated": "2025-12-01",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14515v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]linear attention"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Text-Driven Reasoning Video Editing via Reinforcement Learning on Digital Twin Representations",
      "authors": [
        "Yiqing Shen",
        "Chenjia Li",
        "Mathias Unberath"
      ],
      "arxiv_id": "2511.14100v1",
      "summary": "Text-driven video editing enables users to modify video content only using text queries. While existing methods can modify video content if explicit descriptions of editing targets with precise spatial locations and temporal boundaries are provided, these requirements become impractical when users attempt to conceptualize edits through implicit queries referencing semantic properties or object relationships. We introduce reasoning video editing, a task where video editing models must interpret implicit queries through multi-hop reasoning to infer editing targets before executing modifications, and a first model attempting to solve this complex task, RIVER (Reasoning-based Implicit Video Editor). RIVER decouples reasoning from generation through digital twin representations of video content that preserve spatial relationships, temporal trajectories, and semantic attributes. A large language model then processes this representation jointly with the implicit query, performing multi-hop reasoning to determine modifications, then outputs structured instructions that guide a diffusion-based editor to execute pixel-level changes. RIVER training uses reinforcement learning with rewards that evaluate reasoning accuracy and generation quality. Finally, we introduce RVEBenchmark, a benchmark of 100 videos with 519 implicit queries spanning three levels and categories of reasoning complexity specifically for reasoning video editing. RIVER demonstrates best performance on the proposed RVEBenchmark and also achieves state-of-the-art performance on two additional video editing benchmarks (VegGIE and FiVE), where it surpasses six baseline methods.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14100v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Reconstruction-Driven Multimodal Representation Learning for Automated Media Understanding",
      "authors": [
        "Yassir Benhammou",
        "Suman Kalyan",
        "Sujay Kumar"
      ],
      "arxiv_id": "2511.17596v1",
      "summary": "Broadcast and media organizations increasingly rely on artificial intelligence to automate the labor-intensive processes of content indexing, tagging, and metadata generation. However, existing AI systems typically operate on a single modality-such as video, audio, or text-limiting their understanding of complex, cross-modal relationships in broadcast material. In this work, we propose a Multimodal Autoencoder (MMAE) that learns unified representations across text, audio, and visual data, enabling end-to-end automation of metadata extraction and semantic clustering. The model is trained on the recently introduced LUMA dataset, a fully aligned benchmark of multimodal triplets representative of real-world media content. By minimizing joint reconstruction losses across modalities, the MMAE discovers modality-invariant semantic structures without relying on large paired or contrastive datasets. We demonstrate significant improvements in clustering and alignment metrics (Silhouette, ARI, NMI) compared to linear baselines, indicating that reconstruction-based multimodal embeddings can serve as a foundation for scalable metadata generation and cross-modal retrieval in broadcast archives. These results highlight the potential of reconstruction-driven multimodal learning to enhance automation, searchability, and content management efficiency in modern broadcast workflows.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "8 pages, 5 figures, 4 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17596v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]representation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Distribution Matching Distillation Meets Reinforcement Learning",
      "authors": [
        "Dengyang Jiang",
        "Dongyang Liu",
        "Zanyi Wang",
        "Qilong Wu",
        "Liuzhuozheng Li",
        "Hengzhuang Li",
        "Xin Jin",
        "David Liu",
        "Zhen Li",
        "Bo Zhang",
        "Mengmeng Wang",
        "Steven Hoi",
        "Peng Gao",
        "Harry Yang"
      ],
      "arxiv_id": "2511.13649v3",
      "summary": "Distribution Matching Distillation (DMD) distills a pre-trained multi-step diffusion model to a few-step one to improve inference efficiency. However, the performance of the latter is often capped by the former. To circumvent this dilemma, we propose DMDR, a novel framework that combines Reinforcement Learning (RL) techniques into the distillation process. We show that for the RL of the few-step generator, the DMD loss itself is a more effective regularization compared to the traditional ones. In turn, RL can help to guide the mode coverage process in DMD more effectively. These allow us to unlock the capacity of the few-step generator by conducting distillation and RL simultaneously. Meanwhile, we design the dynamic distribution guidance and dynamic renoise sampling training strategies to improve the initial distillation process. The experiments demonstrate that DMDR can achieve leading visual quality, prompt coherence among few-step methods, and even exhibit performance that exceeds the multi-step teacher.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-12-08",
      "comment": "The synergy of reinforcement learning and distribution matching distillation. See more: https://github.com/vvvvvjdy/dmdr",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13649v3",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks",
      "authors": [
        "Md. Iqbal Hossain",
        "Afia Sajeeda",
        "Neeresh Kumar Perla",
        "Ming Shao"
      ],
      "arxiv_id": "2511.13545v1",
      "summary": "The advent of multimodal deep learning models, such as CLIP, has unlocked new frontiers in a wide range of applications, from image-text understanding to classification tasks. However, these models are not safe for adversarial attacks, particularly backdoor attacks, which can subtly manipulate model behavior. Moreover, existing defense methods typically involve training from scratch or fine-tuning using a large dataset without pinpointing the specific labels that are affected. In this study, we introduce an innovative strategy to enhance the robustness of multimodal contrastive learning models against such attacks. In particular, given a poisoned CLIP model, our approach can identify the backdoor trigger and pinpoint the victim samples and labels in an efficient manner. To that end, an image segmentation ``oracle'' is introduced as the supervisor for the output of the poisoned CLIP. We develop two algorithms to rectify the poisoned model: (1) differentiating between CLIP and Oracle's knowledge to identify potential triggers; (2) pinpointing affected labels and victim samples, and curating a compact fine-tuning dataset. With this knowledge, we are allowed to rectify the poisoned CLIP model to negate backdoor effects. Extensive experiments on visual recognition benchmarks demonstrate our strategy is effective in CLIP-based backdoor defense.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13545v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]contrastive learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Hybrid-Domain Adaptative Representation Learning for Gaze Estimation",
      "authors": [
        "Qida Tan",
        "Hongyu Yang",
        "Wenchao Du"
      ],
      "arxiv_id": "2511.13222v1",
      "summary": "Appearance-based gaze estimation, aiming to predict accurate 3D gaze direction from a single facial image, has made promising progress in recent years. However, most methods suffer significant performance degradation in cross-domain evaluation due to interference from gaze-irrelevant factors, such as expressions, wearables, and image quality. To alleviate this problem, we present a novel Hybrid-domain Adaptative Representation Learning (shorted by HARL) framework that exploits multi-source hybrid datasets to learn robust gaze representation. More specifically, we propose to disentangle gaze-relevant representation from low-quality facial images by aligning features extracted from high-quality near-eye images in an unsupervised domain-adaptation manner, which hardly requires any computational or inference costs. Additionally, we analyze the effect of head-pose and design a simple yet efficient sparse graph fusion module to explore the geometric constraint between gaze direction and head-pose, leading to a dense and robust gaze representation. Extensive experiments on EyeDiap, MPIIFaceGaze, and Gaze360 datasets demonstrate that our approach achieves state-of-the-art accuracy of $\\textbf{5.02}^{\\circ}$ and $\\textbf{3.36}^{\\circ}$, and $\\textbf{9.26}^{\\circ}$ respectively, and present competitive performances through cross-dataset evaluation. The code is available at https://github.com/da60266/HARL.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "AAAI2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13222v1",
      "code_links": [
        {
          "url": "https://github.com/da60266/HARL",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]representation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration",
      "authors": [
        "Haodong Wang",
        "Tao Zhuo",
        "Xiuwei Zhang",
        "Hanlin Yin",
        "Wencong Wu",
        "Yanning Zhang"
      ],
      "arxiv_id": "2511.13168v1",
      "summary": "Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics. Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory. Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences. However, such gradient cues have not been effectively leveraged in deep learning frameworks for SAR-Optical image matching. To address this gap, we propose SOMA, a dense registration framework that integrates structural gradient priors into deep features and refines alignment through a hybrid matching strategy. Specifically, we introduce the Feature Gradient Enhancer (FGE), which embeds multi-scale, multi-directional gradient filters into the feature space using attention and reconstruction mechanisms to boost feature distinctiveness. Furthermore, we propose the Global-Local Affine-Flow Matcher (GLAM), which combines affine transformation and flow-based refinement within a coarse-to-fine architecture to ensure both structural consistency and local accuracy. Experimental results demonstrate that SOMA significantly improves registration precision, increasing the CMR@1px by 12.29% on the SEN1-2 dataset and 18.50% on the GFGE_SO dataset. In addition, SOMA exhibits strong robustness and generalizes well across diverse scenes and resolutions.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13168v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]flow matching"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object Detection with Curriculum Learning",
      "authors": [
        "Yoonjae Seo",
        "Ermal Elbasani",
        "Jaehong Lee"
      ],
      "arxiv_id": "2511.12976v1",
      "summary": "Most neural network quantization methods apply uniform bit precision across spatial regions, ignoring the heterogeneous structural and textural complexity of visual data. This paper introduces MCAQ-YOLO, a morphological complexity-aware quantization framework for object detection. The framework employs five morphological metrics - fractal dimension, texture entropy, gradient variance, edge density, and contour complexity - to characterize local visual morphology and guide spatially adaptive bit allocation. By correlating these metrics with quantization sensitivity, MCAQ-YOLO dynamically adjusts bit precision according to spatial complexity. In addition, a curriculum-based quantization-aware training scheme progressively increases quantization difficulty to stabilize optimization and accelerate convergence. Experimental results demonstrate a strong correlation between morphological complexity and quantization sensitivity and show that MCAQ-YOLO achieves superior detection accuracy and convergence efficiency compared with uniform quantization. On a safety equipment dataset, MCAQ-YOLO attains 85.6 percent mAP@0.5 with an average of 4.2 bits and a 7.6x compression ratio, yielding 3.5 percentage points higher mAP than uniform 4-bit quantization while introducing only 1.8 ms of additional runtime overhead per image. Cross-dataset validation on COCO and Pascal VOC further confirms consistent performance gains, indicating that morphology-driven spatial quantization can enhance efficiency and robustness for computationally constrained, safety-critical visual recognition tasks.",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "9 pages, 2 figures, 7 tables. Preprint",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12976v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]curriculum learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning",
      "authors": [
        "Junbo Zou",
        "Haotian Xia",
        "Zhen Ye",
        "Shengjie Zhang",
        "Christopher Lai",
        "Vicente Ordonez",
        "Weining Shen",
        "Hanjie Chen"
      ],
      "arxiv_id": "2511.12908v1",
      "summary": "Sports video understanding presents unique challenges, requiring models to perceive high-speed dynamics, comprehend complex rules, and reason over long temporal contexts. While Multimodal Large Language Models (MLLMs) have shown promise in genral domains, the current state of research in sports remains narrowly focused: existing approaches are either single-sport centric, limited to specific tasks, or rely on training-free paradigms that lack robust, learned reasoning process. To address this gap, we introduce DeepSport, the first end-to-end trained MLLM framework designed for multi-task, multi-sport video understanding. DeepSport shifts the paradigm from passive frame processing to active, iterative reasoning, empowering the model to ``think with videos'' by dynamically interrogating content via a specialized frame-extraction tool. To enable this, we propose a data distillation pipeline that synthesizes high-quality Chain-of-Thought (CoT) trajectories from 10 diverse data source, creating a unified resource of 78k training data. We then employ a two-stage training strategy, Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) with a novel gated tool-use reward, to optimize the model's reasoning process. Extensive experiments on the testing benchmark of 6.7k questions demonstrate that DeepSport achieves state-of-the-art performance, significantly outperforming baselines of both proprietary model and open-source models. Our work establishes a new foundation for domain-specific video reasoning to address the complexities of diverse sports.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12908v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "FusionFM: All-in-One Multi-Modal Image Fusion with Flow Matching",
      "authors": [
        "Huayi Zhu",
        "Xiu Shu",
        "Youqiang Xiong",
        "Qiao Liu",
        "Rui Chen",
        "Di Yuan",
        "Xiaojun Chang",
        "Zhenyu He"
      ],
      "arxiv_id": "2511.13794v1",
      "summary": "Current multi-modal image fusion methods typically rely on task-specific models, leading to high training costs and limited scalability. While generative methods provide a unified modeling perspective, they often suffer from slow inference due to the complex sampling trajectories from noise to image. To address this, we formulate image fusion as a direct probabilistic transport from source modalities to the fused image distribution, leveraging the flow matching paradigm to improve sampling efficiency and structural consistency. To mitigate the lack of high-quality fused images for supervision, we collect fusion results from multiple state-of-the-art models as priors, and employ a task-aware selection function to select the most reliable pseudo-labels for each task. We further introduce a Fusion Refiner module that employs a divide-and-conquer strategy to systematically identify, decompose, and enhance degraded components in selected pseudo-labels. For multi-task scenarios, we integrate elastic weight consolidation and experience replay mechanisms to preserve cross-task performance and enhance continual learning ability from both parameter stability and memory retention perspectives. Our approach achieves competitive performance across diverse fusion tasks, while significantly improving sampling efficiency and maintaining a lightweight model design. The code will be available at: https://github.com/Ist-Zhy/FusionFM.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13794v1",
      "code_links": [
        {
          "url": "https://github.com/Ist-Zhy/FusionFM",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]flow matching"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Hierarchical Frequency-Decomposition Graph Neural Networks for Road Network Representation Learning",
      "authors": [
        "Jingtian Ma",
        "Jingyuan Wang",
        "Leong Hou U"
      ],
      "arxiv_id": "2511.12507v1",
      "summary": "Road networks are critical infrastructures underpinning intelligent transportation systems and their related applications. Effective representation learning of road networks remains challenging due to the complex interplay between spatial structures and frequency characteristics in traffic patterns. Existing graph neural networks for modeling road networks predominantly fall into two paradigms: spatial-based methods that capture local topology but tend to over-smooth representations, and spectral-based methods that analyze global frequency components but often overlook localized variations. This spatial-spectral misalignment limits their modeling capacity for road networks exhibiting both coarse global trends and fine-grained local fluctuations. To bridge this gap, we propose HiFiNet, a novel hierarchical frequency-decomposition graph neural network that unifies spatial and spectral modeling. HiFiNet constructs a multi-level hierarchy of virtual nodes to enable localized frequency analysis, and employs a decomposition-updating-reconstruction framework with a topology-aware graph transformer to separately model and fuse low- and high-frequency signals. Theoretically justified and empirically validated on multiple real-world datasets across four downstream tasks, HiFiNet demonstrates superior performance and generalization ability in capturing effective road network representations.",
      "categories": [
        "cs.LG",
        "cs.GR"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-16",
      "updated": "2025-11-16",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12507v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]representation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding",
      "authors": [
        "Zhanheng Nie",
        "Chenghan Fu",
        "Daoze Zhang",
        "Junxian Wu",
        "Wanxian Guan",
        "Pengjie Wang",
        "Jian Xu",
        "Bo Zheng"
      ],
      "arxiv_id": "2511.12449v1",
      "summary": "The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-16",
      "updated": "2025-11-16",
      "comment": "11 pages, 7 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12449v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]representation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Learning Straight Flows: Variational Flow Matching for Efficient Generation",
      "authors": [
        "Chenrui Ma",
        "Xi Xiao",
        "Tianyang Wang",
        "Xiao Wang",
        "Yanning Shen"
      ],
      "arxiv_id": "2511.17583v1",
      "summary": "Flow Matching has limited ability in achieving one-step generation due to its reliance on learned curved trajectories. Previous studies have attempted to address this limitation by either modifying the coupling distribution to prevent interpolant intersections or introducing consistency and mean-velocity modeling to promote straight trajectory learning. However, these approaches often suffer from discrete approximation errors, training instability, and convergence difficulties. To tackle these issues, in the present work, we propose \\textbf{S}traight \\textbf{V}ariational \\textbf{F}low \\textbf{M}atching (\\textbf{S-VFM}), which integrates a variational latent code representing the ``generation overview'' into the Flow Matching framework. \\textbf{S-VFM} explicitly enforces trajectory straightness, ideally producing linear generation paths. The proposed method achieves competitive performance across three challenge benchmarks and demonstrates advantages in both training and inference efficiency compared with existing methods.",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-15",
      "updated": "2025-11-15",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17583v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]flow matching"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Constructing and Interpreting Digital Twin Representations for Visual Reasoning via Reinforcement Learning",
      "authors": [
        "Yiqing Shen",
        "Mathias Unberath"
      ],
      "arxiv_id": "2511.12365v1",
      "summary": "Visual reasoning may require models to interpret images and videos and respond to implicit text queries across diverse output formats, from pixel-level segmentation masks to natural language descriptions. Existing approaches rely on supervised fine-tuning with task-specific architectures. For example, reasoning segmentation, grounding, summarization, and visual question answering each demand distinct model designs and training, preventing unified solutions and limiting cross-task and cross-modality generalization. Hence, we propose DT-R1, a reinforcement learning framework that trains large language models to construct digital twin representations of complex multi-modal visual inputs and then reason over these high-level representations as a unified approach to visual reasoning. Specifically, we train DT-R1 using GRPO with a novel reward that validates both structural integrity and output accuracy. Evaluations in six visual reasoning benchmarks, covering two modalities and four task types, demonstrate that DT-R1 consistently achieves improvements over state-of-the-art task-specific models. DT-R1 opens a new direction where visual reasoning emerges from reinforcement learning with digital twin representations.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-15",
      "updated": "2025-11-15",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12365v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity",
      "authors": [
        "Zhichen Lai",
        "Hua Lu",
        "Huan Li",
        "Jialiang Li",
        "Christian S. Jensen"
      ],
      "arxiv_id": "2511.12061v1",
      "summary": "Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-15",
      "updated": "2025-11-15",
      "comment": "8 pages, 6 figures; accepted by AAAI 2026 as an Oral paper",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12061v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]contrastive learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Calibrated Multimodal Representation Learning with Missing Modalities",
      "authors": [
        "Xiaohao Liu",
        "Xiaobo Xia",
        "Jiaheng Wei",
        "Shuo Yang",
        "Xiu Su",
        "See-Kiong Ng",
        "Tat-Seng Chua"
      ],
      "arxiv_id": "2511.12034v1",
      "summary": "Multimodal representation learning harmonizes distinct modalities by aligning them into a unified latent space. Recent research generalizes traditional cross-modal alignment to produce enhanced multimodal synergy but requires all modalities to be present for a common instance, making it challenging to utilize prevalent datasets with missing modalities. We provide theoretical insights into this issue from an anchor shift perspective. Observed modalities are aligned with a local anchor that deviates from the optimal one when all modalities are present, resulting in an inevitable shift. To address this, we propose CalMRL for multimodal representation learning to calibrate incomplete alignments caused by missing modalities. Specifically, CalMRL leverages the priors and the inherent connections among modalities to model the imputation for the missing ones at the representation level. To resolve the optimization dilemma, we employ a bi-step learning method with the closed-form solution of the posterior distribution of shared latents. We validate its mitigation of anchor shift and convergence with theoretical guidance. By equipping the calibrated alignment with the existing advanced method, we offer new flexibility to absorb data with missing modalities, which is originally unattainable. Extensive experiments and comprehensive analyses demonstrate the superiority of CalMRL. Our code, model checkpoints, and evaluation raw data will be publicly available.",
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-15",
      "updated": "2025-11-15",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12034v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]representation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Predictive Safety Shield for Dyna-Q Reinforcement Learning",
      "authors": [
        "Jin Pin",
        "Krasowski Hanna",
        "Vanneaux Elena"
      ],
      "arxiv_id": "2511.21531v1",
      "summary": "Obtaining safety guarantees for reinforcement learning is a major challenge to achieve applicability for real-world tasks. Safety shields extend standard reinforcement learning and achieve hard safety guarantees. However, existing safety shields commonly use random sampling of safe actions or a fixed fallback controller, therefore disregarding future performance implications of different safe actions. In this work, we propose a predictive safety shield for model-based reinforcement learning agents in discrete space. Our safety shield updates the Q-function locally based on safe predictions, which originate from a safe simulation of the environment model. This shielding approach improves performance while maintaining hard safety guarantees. Our experiments on gridworld environments demonstrate that even short prediction horizons can be sufficient to identify the optimal path. We observe that our approach is robust to distribution shifts, e.g., between simulation and reality, without requiring additional training.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21531v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "CLRecogEye : Curriculum Learning towards exploiting convolution features for Dynamic Iris Recognition",
      "authors": [
        "Geetanjali Sharma",
        "Gaurav Jaswal",
        "Aditya Nigam",
        "Raghavendra Ramachandra"
      ],
      "arxiv_id": "2511.21097v1",
      "summary": "Iris authentication algorithms have achieved impressive recognition performance, making them highly promising for real-world applications such as border control, citizen identification, and both criminal investigations and commercial systems. However, their robustness is still challenged by variations in rotation, scale, specular reflections, and defocus blur. In addition, most existing approaches rely on straightforward point-to-point comparisons, typically using cosine or L2 distance, without effectively leveraging the spatio-spatial-temporal structure of iris patterns. To address these limitations, we propose a novel and generalized matching pipeline that learns rich spatio-spatial-temporal representations of iris features. Our approach first splits each iris image along one dimension, generating a sequence of sub-images that serve as input to a 3D-CNN, enabling the network to capture both spatial and spatio-spatial-temporal cues. To further enhance the modeling of spatio-spatial-temporal feature dynamics, we train the model in curriculum manner. This design allows the network to embed temporal dependencies directly into the feature space, improving discriminability in the deep metric domain. The framework is trained end-to-end with triplet and ArcFace loss in a curriculum manner, enforcing highly discriminative embeddings despite challenges like rotation, scale, reflections, and blur. This design yields a robust and generalizable solution for iris authentication.Github code: https://github.com/GeetanjaliGTZ/CLRecogEye",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "12 Pages, 3 figures, ISVC conference 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21097v1",
      "code_links": [
        {
          "url": "https://github.com/GeetanjaliGTZ/CLRecogEye",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]curriculum learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Hybrid Convolution and Frequency State Space Network for Image Compression",
      "authors": [
        "Haodong Pan",
        "Hao Wei",
        "Yusong Wang",
        "Nanning Zheng",
        "Caigui Jiang"
      ],
      "arxiv_id": "2511.20151v1",
      "summary": "Learned image compression (LIC) has recently benefited from Transformer based and state space model (SSM) based architectures. Convolutional neural networks (CNNs) effectively capture local high frequency details, whereas Transformers and SSMs provide strong long range modeling capabilities but may cause structural information loss or ignore frequency characteristics that are crucial for compression. In this work we propose HCFSSNet, a Hybrid Convolution and Frequency State Space Network for LIC. HCFSSNet uses CNNs to extract local high frequency structures and introduces a Vision Frequency State Space (VFSS) block that models long range low frequency information. The VFSS block combines an Omni directional Neighborhood State Space (VONSS) module, which scans features horizontally, vertically and diagonally, with an Adaptive Frequency Modulation Module (AFMM) that applies content adaptive weighting of discrete cosine transform frequency components for more efficient bit allocation. To further reduce redundancy in the entropy model, we integrate AFMM with a Swin Transformer to form a Frequency Swin Transformer Attention Module (FSTAM) for frequency aware side information modeling. Experiments on the Kodak, Tecnick and CLIC Professional Validation datasets show that HCFSSNet achieves competitive rate distortion performance compared with recent SSM based codecs such as MambaIC, while using significantly fewer parameters. On Kodak, Tecnick and CLIC, HCFSSNet reduces BD rate over the VTM anchor by 18.06, 24.56 and 22.44 percent, respectively, providing an efficient and interpretable hybrid architecture for future learned image compression systems.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "36 pages, 8 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20151v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "Mamba",
            "SSM",
            "state space model"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs",
      "authors": [
        "Meng Lu",
        "Ran Xu",
        "Yi Fang",
        "Wenxuan Zhang",
        "Yue Yu",
        "Gaurav Srivastava",
        "Yuchen Zhuang",
        "Mohamed Elhoseiny",
        "Charles Fleming",
        "Carl Yang",
        "Zhengzhong Tu",
        "Yang Xie",
        "Guanghua Xiao",
        "Hanrui Wang",
        "Di Jin",
        "Wenqi Shi",
        "Xuan Wang"
      ],
      "arxiv_id": "2511.19773v1",
      "summary": "While recent vision-language models (VLMs) demonstrate strong image understanding, their ability to \"think with images\", i.e., to reason through multi-step visual interactions, remains limited. We introduce VISTA-Gym, a scalable training environment for incentivizing tool-integrated visual reasoning capabilities in VLMs. VISTA-Gym unifies diverse real-world multimodal reasoning tasks (7 tasks from 13 datasets in total) with a standardized interface for visual tools (e.g., grounding, parsing), executable interaction loops, verifiable feedback signals, and efficient trajectory logging, enabling visual agentic reinforcement learning at scale. While recent VLMs exhibit strong text-only reasoning, both proprietary and open-source models still struggle with tool selection, invocation, and coordination. With VISTA-Gym, we train VISTA-R1 to interleave tool-use with agentic reasoning via multi-turn trajectory sampling and end-to-end reinforcement learning. Extensive experiments across 11 public reasoning-intensive VQA benchmarks show that VISTA-R1-8B outperforms state-of-the-art baselines with similar sizes by 9.51%-18.72%, demonstrating VISTA-Gym as an effective training ground to unlock the tool-integrated reasoning capabilities for VLMs.",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "17 pages, 9 figures, work in progress",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19773v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "VideoChat-M1: Collaborative Policy Planning for Video Understanding via Multi-Agent Reinforcement Learning",
      "authors": [
        "Boyu Chen",
        "Zikang Wang",
        "Zhengrong Yue",
        "Kainan Yan",
        "Chenyun Yu",
        "Yi Huang",
        "Zijun Liu",
        "Yafei Wen",
        "Xiaoxin Chen",
        "Yang Liu",
        "Peng Li",
        "Yali Wang"
      ],
      "arxiv_id": "2511.19524v1",
      "summary": "By leveraging tool-augmented Multimodal Large Language Models (MLLMs), multi-agent frameworks are driving progress in video understanding. However, most of them adopt static and non-learnable tool invocation mechanisms, which limit the discovery of diverse clues essential for robust perception and reasoning regarding temporally or spatially complex videos. To address this challenge, we propose a novel Multi-agent system for video understanding, namely VideoChat-M1. Instead of using a single or fixed policy, VideoChat-M1 adopts a distinct Collaborative Policy Planning (CPP) paradigm with multiple policy agents, which comprises three key processes. (1) Policy Generation: Each agent generates its unique tool invocation policy tailored to the user's query; (2) Policy Execution: Each agent sequentially invokes relevant tools to execute its policy and explore the video content; (3) Policy Communication: During the intermediate stages of policy execution, agents interact with one another to update their respective policies. Through this collaborative framework, all agents work in tandem, dynamically refining their preferred policies based on contextual insights from peers to effectively respond to the user's query. Moreover, we equip our CPP paradigm with a concise Multi-Agent Reinforcement Learning (MARL) method. Consequently, the team of policy agents can be jointly optimized to enhance VideoChat-M1's performance, guided by both the final answer reward and intermediate collaborative process feedback. Extensive experiments demonstrate that VideoChat-M1 achieves SOTA performance across eight benchmarks spanning four tasks. Notably, on LongVideoBench, our method outperforms the SOTA model Gemini 2.5 pro by 3.6% and GPT-4o by 15.6%.",
      "categories": [
        "cs.CV",
        "cs.MA"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "21 pages, 9 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19524v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Fewer Tokens, Greater Scaling: Self-Adaptive Visual Bases for Efficient and Expansive Representation Learning",
      "authors": [
        "Shawn Young",
        "Xingyu Zeng",
        "Lijian Xu"
      ],
      "arxiv_id": "2511.19515v1",
      "summary": "This paper investigates the fundamental relationship between model capacity and the minimal number of visual tokens required to preserve image semantics. Inspired by the Minimum Description Length principle, we reinterpret image tokens as vectors in a visual semantic space and define the intrinsic semantic complexity of an image as the smallest set of basis vectors needed to span this space. Building on this perspective, we propose Orthogonal Filtering, a lightweight module that adaptively clusters redundant tokens into a compact set of orthogonal bases. Through extensive experiments across a range of ViT models, we reveal a consistent token, model scaling law: larger models require significantly fewer tokens to span visual semantic space. Besides, we also contribute a visual long-context dataset.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19515v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]representation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "HiFi-MambaV2: Hierarchical Shared-Routed MoE for High-Fidelity MRI Reconstruction",
      "authors": [
        "Pengcheng Fang",
        "Hongli Chen",
        "Guangzhen Yao",
        "Jian Shi",
        "Fangfang Tang",
        "Xiaohao Cai",
        "Shanshan Shan",
        "Feng Liu"
      ],
      "arxiv_id": "2511.18534v1",
      "summary": "Reconstructing high-fidelity MR images from undersampled k-space data requires recovering high-frequency details while maintaining anatomical coherence. We present HiFi-MambaV2, a hierarchical shared-routed Mixture-of-Experts (MoE) Mamba architecture that couples frequency decomposition with content-adaptive computation. The model comprises two core components: (i) a separable frequency-consistent Laplacian pyramid (SF-Lap) that delivers alias-resistant, stable low- and high-frequency streams; and (ii) a hierarchical shared-routed MoE that performs per-pixel top-1 sparse dispatch to shared experts and local routers, enabling effective specialization with stable cross-depth behavior. A lightweight global context path is fused into an unrolled, data-consistency-regularized backbone to reinforce long-range reasoning and preserve anatomical coherence. Evaluated on fastMRI, CC359, ACDC, M4Raw, and Prostate158, HiFi-MambaV2 consistently outperforms CNN-, Transformer-, and prior Mamba-based baselines in PSNR, SSIM, and NMSE across single- and multi-coil settings and multiple acceleration factors, consistently surpassing consistent improvements in high-frequency detail and overall structural fidelity. These results demonstrate that HiFi-MambaV2 enables reliable and robust MRI reconstruction.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-23",
      "updated": "2025-11-23",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18534v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Plan-X: Instruct Video Generation via Semantic Planning",
      "authors": [
        "Lun Huang",
        "You Xie",
        "Hongyi Xu",
        "Tianpei Gu",
        "Chenxu Zhang",
        "Guoxian Song",
        "Zenan Li",
        "Xiaochen Zhao",
        "Linjie Luo",
        "Guillermo Sapiro"
      ],
      "arxiv_id": "2511.17986v1",
      "summary": "Diffusion Transformers have demonstrated remarkable capabilities in visual synthesis, yet they often struggle with high-level semantic reasoning and long-horizon planning. This limitation frequently leads to visual hallucinations and mis-alignments with user instructions, especially in scenarios involving complex scene understanding, human-object interactions, multi-stage actions, and in-context motion reasoning. To address these challenges, we propose Plan-X, a framework that explicitly enforces high-level semantic planning to instruct video generation process. At its core lies a Semantic Planner, a learnable multimodal language model that reasons over the user's intent from both text prompts and visual context, and autoregressively generates a sequence of text-grounded spatio-temporal semantic tokens. These semantic tokens, complementary to high-level text prompt guidance, serve as structured \"semantic sketches\" over time for the video diffusion model, which has its strength at synthesizing high-fidelity visual details. Plan-X effectively integrates the strength of language models in multimodal in-context reasoning and planning, together with the strength of diffusion models in photorealistic video synthesis. Extensive experiments demonstrate that our framework substantially reduces visual hallucinations and enables fine-grained, instruction-aligned video generation consistent with multimodal context.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-22",
      "updated": "2025-11-22",
      "comment": "The project page is at https://byteaigc.github.io/Plan-X",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17986v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding"
          ],
          "score": 2.0
        },
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "human-object interaction"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "3_perception_slam",
        "5_interaction_reaction"
      ]
    },
    {
      "title": "MambaTAD: When State-Space Models Meet Long-Range Temporal Action Detection",
      "authors": [
        "Hui Lu",
        "Yi Yu",
        "Shijian Lu",
        "Deepu Rajan",
        "Boon Poh Ng",
        "Alex C. Kot",
        "Xudong Jiang"
      ],
      "arxiv_id": "2511.17929v1",
      "summary": "Temporal Action Detection (TAD) aims to identify and localize actions by determining their starting and ending frames within untrimmed videos. Recent Structured State-Space Models such as Mamba have demonstrated potential in TAD due to their long-range modeling capability and linear computational complexity. On the other hand, structured state-space models often face two key challenges in TAD, namely, decay of temporal context due to recursive processing and self-element conflict during global visual context modeling, which become more severe while handling long-span action instances. Additionally, traditional methods for TAD struggle with detecting long-span action instances due to a lack of global awareness and inefficient detection heads. This paper presents MambaTAD, a new state-space TAD model that introduces long-range modeling and global feature detection capabilities for accurate temporal action detection. MambaTAD comprises two novel designs that complement each other with superior TAD performance. First, it introduces a Diagonal-Masked Bidirectional State-Space (DMBSS) module which effectively facilitates global feature fusion and temporal action detection. Second, it introduces a global feature fusion head that refines the detection progressively with multi-granularity features and global awareness. In addition, MambaTAD tackles TAD in an end-to-end one-stage manner using a new state-space temporal adapter(SSTA) which reduces network parameters and computation cost with linear complexity. Extensive experiments show that MambaTAD achieves superior TAD performance consistently across multiple public benchmarks.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-22",
      "updated": "2025-11-22",
      "comment": "",
      "doi": "",
      "journal_ref": "IEEE Transactions on Multimedia, 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.17929v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "PA-FAS: Towards Interpretable and Generalizable Multimodal Face Anti-Spoofing via Path-Augmented Reinforcement Learning",
      "authors": [
        "Yingjie Ma",
        "Xun Lin",
        "Yong Xu",
        "Weicheng Xie",
        "Zitong Yu"
      ],
      "arxiv_id": "2511.17927v1",
      "summary": "Face anti-spoofing (FAS) has recently advanced in multimodal fusion, cross-domain generalization, and interpretability. With large language models and reinforcement learning (RL), strategy-based training offers new opportunities to jointly model these aspects. However, multimodal reasoning is more complex than unimodal reasoning, requiring accurate feature representation and cross-modal verification while facing scarce, high-quality annotations, which makes direct application of RL sub-optimal. We identify two key limitations of supervised fine-tuning plus RL (SFT+RL) for multimodal FAS: (1) limited multimodal reasoning paths restrict the use of complementary modalities and shrink the exploration space after SFT, weakening the effect of RL; and (2) mismatched single-task supervision versus diverse reasoning paths causes reasoning confusion, where models may exploit shortcuts by mapping images directly to answers and ignoring the intended reasoning. To address this, we propose PA-FAS, which enhances reasoning paths by constructing high-quality extended reasoning sequences from limited annotations, enriching paths and relaxing exploration constraints. We further introduce an answer-shuffling mechanism during SFT to force comprehensive multimodal analysis instead of using superficial cues, thereby encouraging deeper reasoning and mitigating shortcut learning. PA-FAS significantly improves multimodal reasoning accuracy and cross-domain generalization, and better unifies multimodal fusion, generalization, and interpretability for trustworthy FAS.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-22",
      "updated": "2025-11-22",
      "comment": "Accepted by AAAI 2026 (Oral)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17927v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Accelerating Inference of Masked Image Generators via Reinforcement Learning",
      "authors": [
        "Pranav Subbaraman",
        "Shufan Li",
        "Siyan Zhao",
        "Aditya Grover"
      ],
      "arxiv_id": "2512.01094v1",
      "summary": "Masked Generative Models (MGM)s demonstrate strong capabilities in generating high-fidelity images. However, they need many sampling steps to create high-quality generations, resulting in slow inference speed. In this work, we propose Speed-RL, a novel paradigm for accelerating a pretrained MGMs to generate high-quality images in fewer steps. Unlike conventional distillation methods which formulate the acceleration problem as a distribution matching problem, where a few-step student model is trained to match the distribution generated by a many-step teacher model, we consider this problem as a reinforcement learning problem. Since the goal of acceleration is to generate high quality images in fewer steps, we can combine a quality reward with a speed reward and finetune the base model using reinforcement learning with the combined reward as the optimization target. Through extensive experiments, we show that the proposed method was able to accelerate the base model by a factor of 3x while maintaining comparable image quality.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "15 pages, 9 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01094v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "HanDyVQA: A Video QA Benchmark for Fine-Grained Hand-Object Interaction Dynamics",
      "authors": [
        "Masatoshi Tateno",
        "Gido Kato",
        "Hirokatsu Kataoka",
        "Yoichi Sato",
        "Takuma Yagi"
      ],
      "arxiv_id": "2512.00885v1",
      "summary": "Hand-object interaction (HOI) inherently involves dynamics where human manipulations produce distinct spatio-temporal effects on objects. However, existing semantic HOI benchmarks focused either on manipulation or on the resulting effects at a coarse level, lacking fine-grained spatio-temporal reasoning to capture the underlying dynamics in HOI. We introduce HanDyVQA, a fine-grained video question-answering benchmark that comprehensively covers both the manipulation and effect aspects of HOI. HanDyVQA comprises six complementary question types (Action, Process, Objects, Location, State Change, and Object Parts), totalling 11.1K multiple-choice QA pairs. Collected QA pairs recognizing manipulation styles, hand/object motions, and part-level state changes. HanDyVQA also includes 10.3K segmentation masks for Objects and Object Parts questions, enabling the evaluation of object/part-level reasoning in video object segmentation. We evaluated recent video foundation models on our benchmark and found that even the best-performing model, Gemini-2.5-Pro, reached only 73% average accuracy, which is far from human performance (97%). Further analysis shows the remaining challenges in spatial relationship, motion, and part-level geometric understanding. We also found that integrating explicit HOI-related cues into visual features improves performance, offering insights for developing future models with a deeper understanding of HOI dynamics.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "Project page: https://masatate.github.io/HanDyVQA-project-page/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00885v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "HOI"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "1_robot_core",
        "5_interaction_reaction"
      ]
    },
    {
      "title": "MambaScope: Coarse-to-Fine Scoping for Efficient Vision Mamba",
      "authors": [
        "Shanhui Liu",
        "Rui Xu",
        "Yunke Wang"
      ],
      "arxiv_id": "2512.00647v2",
      "summary": "Vision Mamba has emerged as a promising and efficient alternative to Vision Transformers, yet its efficiency remains fundamentally constrained by the number of input tokens. Existing token reduction approaches typically adopt token pruning or merging to reduce computation. However, they inherently lead to information loss as they discard or compress token representations. This problem is further exacerbated when the same fine-grained token processing is uniformly applied across all images regardless of visual complexity. We observe that not all inputs require fine-grained processing: simple images can be effectively handled at a coarse resolution, while only complex ones require refinement. Based on this insight, we propose MambaScope, an adaptive framework for efficient inference for Vision Mamba. MambaScope first performs coarse-grained inference by dividing the input image into large patches, significantly reducing token length and computation. When the model's prediction confidence is low, selected regions are re-processed at a finer resolution to recover essential visual details with minimal additional cost. This dynamic resolution assignment strategy allows MambaScope to allocate computation adaptively according to image complexity, achieving efficient processing without compromising accuracy. Experiments across various vision tasks demonstrate that MambaScope outperforms both the baseline Vision Mamba and state-of-the-art token reduction techniques in terms of accuracy and efficiency.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-29",
      "updated": "2025-12-03",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00647v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Hardware-Software Collaborative Computing of Photonic Spiking Reinforcement Learning for Robotic Continuous Control",
      "authors": [
        "Mengting Yu",
        "Shuiying Xiang",
        "Changjian Xie",
        "Yonghang Chen",
        "Haowen Zhao",
        "Xingxing Guo",
        "Yahui Zhang",
        "Yanan Han",
        "Yue Hao"
      ],
      "arxiv_id": "2512.00427v1",
      "summary": "Robotic continuous control tasks impose stringent demands on the energy efficiency and latency of computing architectures due to their high-dimensional state spaces and real-time interaction requirements. Conventional electronic computing platforms face computational bottlenecks, whereas the fusion of photonic computing and spiking reinforcement learning (RL) offers a promising alternative. Here, we propose a novel computing architecture based on photonic spiking RL, which integrates the Twin Delayed Deep Deterministic policy gradient (TD3) algorithm with spiking neural network (SNN). The proposed architecture employs an optical-electronic hybrid computing paradigm wherein a silicon photonic Mach-Zehnder interferometer (MZI) chip executes linear matrix computations, while nonlinear spiking activations are performed in the electronic domain. Experimental validation on the Pendulum-v1 and HalfCheetah-v2 benchmarks demonstrates the system capability for software-hardware co-inference, achieving a control policy reward of 5831 on HalfCheetah-v2, a 23.33% reduction in convergence steps, and an action deviation below 2.2%. Notably, this work represents the first application of a programmable MZI photonic computing chip to robotic continuous control tasks, attaining an energy efficiency of 1.39 TOPS/W and an ultralow computational latency of 120 ps. Such performance underscores the promise of photonic spiking RL for real-time decision-making in autonomous and industrial robotic systems.",
      "categories": [
        "cs.RO",
        "physics.optics"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-29",
      "updated": "2025-11-29",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00427v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "SMamDiff: Spatial Mamba for Stochastic Human Motion Prediction",
      "authors": [
        "Junqiao Fan",
        "Pengfei Liu",
        "Haocong Rao"
      ],
      "arxiv_id": "2512.00355v1",
      "summary": "With intelligent room-side sensing and service robots widely deployed, human motion prediction (HMP) is essential for safe, proactive assistance. However, many existing HMP methods either produce a single, deterministic forecast that ignores uncertainty or rely on probabilistic models that sacrifice kinematic plausibility. Diffusion models improve the accuracy-diversity trade-off but often depend on multi-stage pipelines that are costly for edge deployment. This work focuses on how to ensure spatial-temporal coherence within a single-stage diffusion model for HMP. We introduce SMamDiff, a Spatial Mamba-based Diffusion model with two novel designs: (i) a residual-DCT motion encoding that subtracts the last observed pose before a temporal DCT, reducing the first DC component ($f=0$) dominance and highlighting informative higher-frequency cues so the model learns how joints move rather than where they are; and (ii) a stickman-drawing spatial-mamba module that processes joints in an ordered, joint-by-joint manner, making later joints condition on earlier ones to induce long-range, cross-joint dependencies. On Human3.6M and HumanEva, these coherence mechanisms deliver state-of-the-art results among single-stage probabilistic HMP methods while using less latency and memory than multi-stage diffusion baselines.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-29",
      "updated": "2025-11-29",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00355v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]Mamba"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Human Mesh Modeling for Anny Body",
      "authors": [
        "Romain Brégier",
        "Guénolé Fiche",
        "Laura Bravo-Sánchez",
        "Thomas Lucas",
        "Matthieu Armando",
        "Philippe Weinzaepfel",
        "Grégory Rogez",
        "Fabien Baradel"
      ],
      "arxiv_id": "2511.03589v1",
      "summary": "Parametric body models are central to many human-centric tasks, yet existing models often rely on costly 3D scans and learned shape spaces that are proprietary and demographically narrow. We introduce Anny, a simple, fully differentiable, and scan-free human body model grounded in anthropometric knowledge from the MakeHuman community. Anny defines a continuous, interpretable shape space, where phenotype parameters (e.g. gender, age, height, weight) control blendshapes spanning a wide range of human forms -- across ages (from infants to elders), body types, and proportions. Calibrated using WHO population statistics, it provides realistic and demographically grounded human shape variation within a single unified model. Thanks to its openness and semantic control, Anny serves as a versatile foundation for 3D human modeling -- supporting millimeter-accurate scan fitting, controlled synthetic data generation, and Human Mesh Recovery (HMR). We further introduce Anny-One, a collection of 800k photorealistic humans generated with Anny, showing that despite its simplicity, HMR models trained with Anny can match the performance of those trained with scan-based body models, while remaining interpretable and broadly representative. The Anny body model and its code are released under the Apache 2.0 license, making Anny an accessible foundation for human-centric 3D modeling.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-05",
      "updated": "2025-11-05",
      "comment": "We release our model and code at https://github.com/naver/anny",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03589v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "human mesh recovery",
            "HMR"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "6_video_extraction"
      ]
    },
    {
      "title": "GUIDES: Guidance Using Instructor-Distilled Embeddings for Pre-trained Robot Policy Enhancement",
      "authors": [
        "Minquan Gao",
        "Xinyi Li",
        "Qing Yan",
        "Xiaojian Sun",
        "Xiaopan Zhang",
        "Chien-Ming Huang",
        "Jiachen Li"
      ],
      "arxiv_id": "2511.03400v2",
      "summary": "Pre-trained robot policies serve as the foundation of many validated robotic systems, which encapsulate extensive embodied knowledge. However, they often lack the semantic awareness characteristic of foundation models, and replacing them entirely is impractical in many situations due to high costs and the loss of accumulated knowledge. To address this gap, we introduce GUIDES, a lightweight framework that augments pre-trained policies with semantic guidance from foundation models without requiring architectural redesign. GUIDES employs a fine-tuned vision-language model (Instructor) to generate contextual instructions, which are encoded by an auxiliary module into guidance embeddings. These embeddings are injected into the policy's latent space, allowing the legacy model to adapt to this new semantic input through brief, targeted fine-tuning. For inference-time robustness, a large language model-based Reflector monitors the Instructor's confidence and, when confidence is low, initiates a reasoning loop that analyzes execution history, retrieves relevant examples, and augments the VLM's context to refine subsequent actions. Extensive validation in the RoboCasa simulation environment across diverse policy architectures shows consistent and substantial improvements in task success rates. Real-world deployment on a UR5 robot further demonstrates that GUIDES enhances motion precision for critical sub-tasks such as grasping. Overall, GUIDES offers a practical and resource-efficient pathway to upgrade, rather than replace, validated robot policies.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-05",
      "updated": "2025-11-14",
      "comment": "8 pages, 4 figures, Accepted by IEEE IROS 2025 Workshop WIR-M",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03400v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "grasping",
            "grasp"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "SENT Map -- Semantically Enhanced Topological Maps with Foundation Models",
      "authors": [
        "Raj Surya Rajendran Kathirvel",
        "Zach A Chavis",
        "Stephen J. Guy",
        "Karthik Desingh"
      ],
      "arxiv_id": "2511.03165v1",
      "summary": "We introduce SENT-Map, a semantically enhanced topological map for representing indoor environments, designed to support autonomous navigation and manipulation by leveraging advancements in foundational models (FMs). Through representing the environment in a JSON text format, we enable semantic information to be added and edited in a format that both humans and FMs understand, while grounding the robot to existing nodes during planning to avoid infeasible states during deployment. Our proposed framework employs a two stage approach, first mapping the environment alongside an operator with a Vision-FM, then using the SENT-Map representation alongside a natural-language query within an FM for planning. Our experimental results show that semantic-enhancement enables even small locally-deployable FMs to successfully plan over indoor environments.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-05",
      "updated": "2025-11-05",
      "comment": "Accepted at ICRA 2025 Workshop on Foundation Models and Neuro-Symbolic AI for Robotics",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03165v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Unsupervised Learning for Industrial Defect Detection: A Case Study on Shearographic Data",
      "authors": [
        "Jessica Plassmann",
        "Nicolas Schuler",
        "Georg von Freymann",
        "Michael Schuth"
      ],
      "arxiv_id": "2511.02541v2",
      "summary": "Shearography is a non-destructive testing method for detecting subsurface defects, offering high sensitivity and full-field inspection capabilities. However, its industrial adoption remains limited due to the need for expert interpretation. To reduce reliance on labeled data and manual evaluation, this study explores unsupervised learning methods for automated anomaly detection in shearographic images. Three architectures are evaluated: a fully connected autoencoder, a convolutional autoencoder, and a student-teacher feature matching model. All models are trained solely on defect-free data. A controlled dataset was developed using a custom specimen with reproducible defect patterns, enabling systematic acquisition of shearographic measurements under both ideal and realistic deformation conditions. Two training subsets were defined: one containing only undistorted, defect-free samples, and one additionally including globally deformed, yet defect-free, data. The latter simulates practical inspection conditions by incorporating deformation-induced fringe patterns that may obscure localized anomalies. The models are evaluated in terms of binary classification and, for the student-teacher model, spatial defect localization. Results show that the student-teacher approach achieves superior classification robustness and enables precise localization. Compared to the autoencoder-based models, it demonstrates improved separability of feature representations, as visualized through t-SNE embeddings. Additionally, a YOLOv8 model trained on labeled defect data serves as a reference to benchmark localization quality. This study underscores the potential of unsupervised deep learning for scalable, label-efficient shearographic inspection in industrial environments.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-04",
      "updated": "2025-12-11",
      "comment": "15 pages, 6 figures, 1 table; accepted for AI-2025 Forty-fifth SGAI International Conference on Artificial Intelligence CAMBRIDGE, ENGLAND 16-18 DECEMBER 2025",
      "doi": "10.1007/978-3-032-11442-6_22",
      "journal_ref": "Artificial Intelligence XLII. SGAI-AI 2025. Lecture Notes in Computer Science, vol 16302. Springer, Cham (2026), pp 316-329",
      "pdf_url": "https://arxiv.org/pdf/2511.02541v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "feature matching"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam",
        "6_video_extraction"
      ]
    },
    {
      "title": "LiteVoxel: Low-memory Intelligent Thresholding for Efficient Voxel Rasterization",
      "authors": [
        "Jee Won Lee",
        "Jongseong Brad Choi"
      ],
      "arxiv_id": "2511.02510v1",
      "summary": "Sparse-voxel rasterization is a fast, differentiable alternative for optimization-based scene reconstruction, but it tends to underfit low-frequency content, depends on brittle pruning heuristics, and can overgrow in ways that inflate VRAM. We introduce LiteVoxel, a self-tuning training pipeline that makes SV rasterization both steadier and lighter. Our loss is made low-frequency aware via an inverse-Sobel reweighting with a mid-training gamma-ramp, shifting gradient budget to flat regions only after geometry stabilize. Adaptation replaces fixed thresholds with a depth-quantile pruning logic on maximum blending weight, stabilized by EMA-hysteresis guards and refines structure through ray-footprint-based, priority-driven subdivision under an explicit growth budget. Ablations and full-system results across Mip-NeRF 360 (6scenes) and Tanks & Temples (3scenes) datasets show mitigation of errors in low-frequency regions and boundary instability while keeping PSNR/SSIM, training time, and FPS comparable to a strong SVRaster pipeline. Crucially, LiteVoxel reduces peak VRAM by ~40%-60% and preserves low-frequency detail that prior setups miss, enabling more predictable, memory-efficient training without sacrificing perceptual quality.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-04",
      "updated": "2025-11-04",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02510v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "NeRF",
            "scene reconstruction"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "SuckTac: Camera-based Tactile Sucker for Unstructured Surface Perception and Interaction",
      "authors": [
        "Ruiyong Yuan",
        "Jieji Ren",
        "Zhanxuan Peng",
        "Feifei Chen",
        "Guoying Gu"
      ],
      "arxiv_id": "2511.02294v1",
      "summary": "Suckers are significant for robots in picking, transferring, manipulation and locomotion on diverse surfaces. However, most of the existing suckers lack high-fidelity perceptual and tactile sensing, which impedes them from resolving the fine-grained geometric features and interaction status of the target surface. This limits their robust performance with irregular objects and in complex, unstructured environments. Inspired by the adaptive structure and high-performance sensory capabilities of cephalopod suckers, in this paper, we propose a novel, intelligent sucker, named SuckTac, that integrates a camera-based tactile sensor directly within its optimized structure to provide high-density perception and robust suction. Specifically, through joint structure design and optimization and based on a multi-material integrated casting technique, a camera and light source are embedded into the sucker, which enables in-situ, high-density perception of fine details like surface shape, texture and roughness. To further enhance robustness and adaptability, the sucker's mechanical design is also optimized by refining its profile, adding a compliant lip, and incorporating surface microstructure. Extensive experiments, including challenging tasks such as robotic cloth manipulation and soft mobile robot inspection, demonstrate the superior performance and broad applicability of the proposed system.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-04",
      "updated": "2025-11-04",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02294v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "locomotion",
            "manipulation"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Opto-Electronic Convolutional Neural Network Design Via Direct Kernel Optimization",
      "authors": [
        "Ali Almuallem",
        "Harshana Weligampola",
        "Abhiram Gnanasambandam",
        "Wei Xu",
        "Dilshan Godaliyadda",
        "Hamid R. Sheikh",
        "Stanley H. Chan",
        "Qi Guo"
      ],
      "arxiv_id": "2511.02065v1",
      "summary": "Opto-electronic neural networks integrate optical front-ends with electronic back-ends to enable fast and energy-efficient vision. However, conventional end-to-end optimization of both the optical and electronic modules is limited by costly simulations and large parameter spaces. We introduce a two-stage strategy for designing opto-electronic convolutional neural networks (CNNs): first, train a standard electronic CNN, then realize the optical front-end implemented as a metasurface array through direct kernel optimization of its first convolutional layer. This approach reduces computational and memory demands by hundreds of times and improves training stability compared to end-to-end optimization. On monocular depth estimation, the proposed two-stage design achieves twice the accuracy of end-to-end training under the same training time and resource constraints.",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02065v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation",
            "monocular depth"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "TRACE: Textual Reasoning for Affordance Coordinate Extraction",
      "authors": [
        "Sangyun Park",
        "Jin Kim",
        "Yuchen Cui",
        "Matthew S. Brown"
      ],
      "arxiv_id": "2511.01999v1",
      "summary": "Vision-Language Models (VLMs) struggle to translate high-level instructions into the precise spatial affordances required for robotic manipulation. While visual Chain-of-Thought (CoT) methods exist, they are often computationally intensive. In this work, we introduce TRACE (Textual Reasoning for Affordance Coordinate Extraction), a novel methodology that integrates a textual Chain of Reasoning (CoR) into the affordance prediction process. We use this methodology to create the TRACE dataset, a large-scale collection created via an autonomous pipeline that pairs instructions with explicit textual rationales. By fine-tuning a VLM on this data, our model learns to externalize its spatial reasoning before acting. Our experiments show that our TRACE-tuned model achieves state-of-the-art performance, reaching 48.1% accuracy on the primary Where2Place (W2P) benchmark (a 9.6% relative improvement) and 55.0% on the more challenging W2P(h) subset. Crucially, an ablation study demonstrates that performance scales directly with the amount of reasoning data used, confirming the CoR's effectiveness. Furthermore, analysis of the model's attention maps reveals an interpretable reasoning process where focus shifts dynamically across reasoning steps. This work shows that training VLMs to generate a textual CoR is an effective and robust strategy for enhancing the precision, reliability, and interpretability of VLM-based robot control. Our dataset and code are available at https://github.com/jink-ucla/TRACE",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "ICCV 2025. *Equal contribution. †Corresponding author",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01999v1",
      "code_links": [
        {
          "url": "https://github.com/jink-ucla/TRACE",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "affordance prediction"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning",
      "authors": [
        "Ming Li",
        "Jike Zhong",
        "Shitian Zhao",
        "Haoquan Zhang",
        "Shaoheng Lin",
        "Yuxiang Lai",
        "Chen Wei",
        "Konstantinos Psounis",
        "Kaipeng Zhang"
      ],
      "arxiv_id": "2511.01833v2",
      "summary": "The frontier of visual reasoning is shifting toward models like OpenAI o3, which can intelligently create and operate tools to transform images for problem-solving, also known as thinking-\\textit{with}-images in chain-of-thought. Yet existing benchmarks fail to fully capture this advanced capability. Even Visual Search, the most common benchmark for current thinking-\\textit{with}-images methods, tests only basic operations such as localization and cropping, offering little insight into more complex, dynamic, and tool-dependent reasoning. We introduce \\textbf{TIR-Bench}, a comprehensive benchmark for evaluating agentic thinking-with-images across 13 diverse tasks, each requiring novel tool use for image processing and manipulation in chain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from leading open-sourced and proprietary models to those with explicit tool-use augmentation. Results show that TIR-Bench is universally challenging, and strong performance requires genuine thinking-with-images capabilities. Finally, we present a pilot study comparing direct versus agentic fine-tuning.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-03",
      "updated": "2025-11-05",
      "comment": "Preprint",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01833v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model",
      "authors": [
        "Wenqi Liang",
        "Gan Sun",
        "Yao He",
        "Jiahua Dong",
        "Suyan Dai",
        "Ivan Laptev",
        "Salman Khan",
        "Yang Cong"
      ],
      "arxiv_id": "2511.01571v1",
      "summary": "Vision-Language-Action models (VLAs) are emerging as powerful tools for learning generalizable visuomotor control policies. However, current VLAs are mostly trained on large-scale image-text-action data and remain limited in two key ways: (i) they struggle with pixel-level scene understanding, and (ii) they rely heavily on textual prompts, which reduces their flexibility in real-world settings. To address these challenges, we introduce PixelVLA, the first VLA model designed to support both pixel-level reasoning and multimodal prompting with text and visual inputs. Our approach is built on a new visuomotor instruction tuning framework that integrates a multiscale pixel-aware encoder with a visual prompting encoder. To train PixelVLA effectively, we further propose a two-stage automated annotation pipeline that generates Pixel-160K, a large-scale dataset with pixel-level annotations derived from existing robot data. Experiments on three standard VLA benchmarks and two VLA model variants show that PixelVLA improves manipulation success rates by 10.1%-17.8% over OpenVLA, while requiring only 1.5% of its pretraining cost. These results demonstrate that PixelVLA can be integrated into existing VLAs to enable more accurate, efficient, and versatile robot control in complex environments. The dataset and code will be released as open source.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "17pages,7 figures, 5 tabels",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01571v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "CaRLi-V: Camera-RADAR-LiDAR Point-Wise 3D Velocity Estimation",
      "authors": [
        "Landson Guo",
        "Andres M. Diaz Aguilar",
        "William Talbot",
        "Turcan Tuna",
        "Marco Hutter",
        "Cesar Cadena"
      ],
      "arxiv_id": "2511.01383v1",
      "summary": "Accurate point-wise velocity estimation in 3D is crucial for robot interaction with non-rigid, dynamic agents, such as humans, enabling robust performance in path planning, collision avoidance, and object manipulation in dynamic environments. To this end, this paper proposes a novel RADAR, LiDAR, and camera fusion pipeline for point-wise 3D velocity estimation named CaRLi-V. This pipeline leverages raw RADAR measurements to create a novel RADAR representation, the velocity cube, which densely represents radial velocities within the RADAR's field-of-view. By combining the velocity cube for radial velocity extraction, optical flow for tangential velocity estimation, and LiDAR for point-wise range measurements through a closed-form solution, our approach can produce 3D velocity estimates for a dense array of points. Developed as an open-source ROS2 package, CaRLi-V has been field-tested against a custom dataset and proven to produce low velocity error metrics relative to ground truth, enabling point-wise velocity estimation for robotic applications.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01383v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "optical flow"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Web-Scale Collection of Video Data for 4D Animal Reconstruction",
      "authors": [
        "Brian Nlong Zhao",
        "Jiajun Wu",
        "Shangzhe Wu"
      ],
      "arxiv_id": "2511.01169v1",
      "summary": "Computer vision for animals holds great promise for wildlife research but often depends on large-scale data, while existing collection methods rely on controlled capture setups. Recent data-driven approaches show the potential of single-view, non-invasive analysis, yet current animal video datasets are limited--offering as few as 2.4K 15-frame clips and lacking key processing for animal-centric 3D/4D tasks. We introduce an automated pipeline that mines YouTube videos and processes them into object-centric clips, along with auxiliary annotations valuable for downstream tasks like pose estimation, tracking, and 3D/4D reconstruction. Using this pipeline, we amass 30K videos (2M frames)--an order of magnitude more than prior works. To demonstrate its utility, we focus on the 4D quadruped animal reconstruction task. To support this task, we present Animal-in-Motion (AiM), a benchmark of 230 manually filtered sequences with 11K frames showcasing clean, diverse animal motions. We evaluate state-of-the-art model-based and model-free methods on Animal-in-Motion, finding that 2D metrics favor the former despite unrealistic 3D shapes, while the latter yields more natural reconstructions but scores lower--revealing a gap in current evaluation. To address this, we enhance a recent model-free approach with sequence-level optimization, establishing the first 4D animal reconstruction baseline. Together, our pipeline, benchmark, and baseline aim to advance large-scale, markerless 4D animal reconstruction and related tasks from in-the-wild videos. Code and datasets are available at https://github.com/briannlongzhao/Animal-in-Motion.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "NeurIPS 2025 Datasets and Benchmarks",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01169v1",
      "code_links": [
        {
          "url": "https://github.com/briannlongzhao/Animal-in-Motion",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "pose estimation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Maestro: Orchestrating Robotics Modules with Vision-Language Models for Zero-Shot Generalist Robots",
      "authors": [
        "Junyao Shi",
        "Rujia Yang",
        "Kaitian Chao",
        "Selina Bingqing Wan",
        "Yifei Shao",
        "Jiahui Lei",
        "Jianing Qian",
        "Long Le",
        "Pratik Chaudhari",
        "Kostas Daniilidis",
        "Chuan Wen",
        "Dinesh Jayaraman"
      ],
      "arxiv_id": "2511.00917v2",
      "summary": "Today's best-explored routes towards generalist robots center on collecting ever larger \"observations-in actions-out\" robotics datasets to train large end-to-end models, copying a recipe that has worked for vision-language models (VLMs). We pursue a road less traveled: building generalist policies directly around VLMs by augmenting their general capabilities with specific robot capabilities encapsulated in a carefully curated set of perception, planning, and control modules. In Maestro, a VLM coding agent dynamically composes these modules into a programmatic policy for the current task and scenario. Maestro's architecture benefits from a streamlined closed-loop interface without many manually imposed structural constraints, and a comprehensive and diverse tool repertoire. As a result, it largely surpasses today's VLA models for zero-shot performance on challenging manipulation skills. Further, Maestro is easily extensible to incorporate new modules, easily editable to suit new embodiments such as a quadruped-mounted arm, and even easily adapts from minimal real-world experiences through local code edits.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-02",
      "updated": "2025-11-18",
      "comment": "Plan to resubmit after significant revisions",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00917v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped",
            "manipulation"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "RobIA: Robust Instance-aware Continual Test-time Adaptation for Deep Stereo",
      "authors": [
        "Jueun Ko",
        "Hyewon Park",
        "Hyesong Choi",
        "Dongbo Min"
      ],
      "arxiv_id": "2511.10107v1",
      "summary": "Stereo Depth Estimation in real-world environments poses significant challenges due to dynamic domain shifts, sparse or unreliable supervision, and the high cost of acquiring dense ground-truth labels. While recent Test-Time Adaptation (TTA) methods offer promising solutions, most rely on static target domain assumptions and input-invariant adaptation strategies, limiting their effectiveness under continual shifts. In this paper, we propose RobIA, a novel Robust, Instance-Aware framework for Continual Test-Time Adaptation (CTTA) in stereo depth estimation. RobIA integrates two key components: (1) Attend-and-Excite Mixture-of-Experts (AttEx-MoE), a parameter-efficient module that dynamically routes input to frozen experts via lightweight self-attention mechanism tailored to epipolar geometry, and (2) Robust AdaptBN Teacher, a PEFT-based teacher model that provides dense pseudo-supervision by complementing sparse handcrafted labels. This strategy enables input-specific flexibility, broad supervision coverage, improving generalization under domain shift. Extensive experiments demonstrate that RobIA achieves superior adaptation performance across dynamic target domains while maintaining computational efficiency.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "Accepted by Neural Information Processing Systems (NeurIPS) 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10107v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation",
            "stereo depth"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement",
      "authors": [
        "Lian He",
        "Meng Liu",
        "Qilang Ye",
        "Yu Zhou",
        "Xiang Deng",
        "Gangyi Ding"
      ],
      "arxiv_id": "2511.11702v1",
      "summary": "Understanding 3D scene-level affordances from natural language instructions is essential for enabling embodied agents to interact meaningfully in complex environments. However, this task remains challenging due to the need for semantic reasoning and spatial grounding. Existing methods mainly focus on object-level affordances or merely lift 2D predictions to 3D, neglecting rich geometric structure information in point clouds and incurring high computational costs. To address these limitations, we introduce Task-Aware 3D Scene-level Affordance segmentation (TASA), a novel geometry-optimized framework that jointly leverages 2D semantic cues and 3D geometric reasoning in a coarse-to-fine manner. To improve the affordance detection efficiency, TASA features a task-aware 2D affordance detection module to identify manipulable points from language and visual inputs, guiding the selection of task-relevant views. To fully exploit 3D geometric information, a 3D affordance refinement module is proposed to integrate 2D semantic priors with local 3D geometry, resulting in accurate and spatially coherent 3D affordance masks. Experiments on SceneFun3D demonstrate that TASA significantly outperforms the baselines in both accuracy and efficiency in scene-level affordance segmentation.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11702v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "affordance detection",
            "point cloud"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "RadHARSimulator V2: Video to Doppler Generator",
      "authors": [
        "Weicheng Gao"
      ],
      "arxiv_id": "2511.09022v1",
      "summary": "Radar-based human activity recognition (HAR) still lacks a comprehensive simulation method. Existing software is developed based on models or motion-captured data, resulting in limited flexibility. To address this issue, a simulator that directly generates Doppler spectra from recorded video footage (RadHARSimulator V2) is presented in this paper. Both computer vision and radar modules are included in the simulator. In computer vision module, the real-time model for object detection with global nearest neighbor is first used to detect and track human targets in the video. Then, the high-resolution network is used to estimate two-dimensional poses of the detected human targets. Next, the three-dimensional poses of the detected human targets are obtained by nearest matching method. Finally, smooth temporal three-dimensional pose estimation is achieved through Kalman filtering. In radar module, pose interpolation and smoothing are first achieved through the Savitzky-Golay method. Second, the delay model and the mirror method are used to simulate echoes in both free-space and through-the-wall scenarios. Then, range-time map is generated using pulse compression, moving target indication, and DnCNN. Next, Doppler-time map (DTM) is generated using short-time Fourier transform and DnCNN again. Finally, the ridge features on the DTM are extracted using the maximum local energy method. In addition, a hybrid parallel-serial neural network architecture is proposed for radar-based HAR. Numerical experiments are conducted and analyzed to demonstrate the effectiveness of the designed simulator and the proposed network model. The open-source code of this work can be found in: https://github.com/JoeyBGOfficial/RadHARSimulatorV2-Video-to-Doppler-Generator.",
      "categories": [
        "eess.SP",
        "cs.CV"
      ],
      "primary_category": "eess.SP",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "19 pages, 16 figures, 8 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09022v1",
      "code_links": [
        {
          "url": "https://github.com/JoeyBGOfficial/RadHARSimulatorV2-Video-to-Doppler-Generator",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "pose estimation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "PULSE"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam",
        "8_physics_animation"
      ]
    },
    {
      "title": "Intuitive control of supernumerary robotic limbs through a tactile-encoded neural interface",
      "authors": [
        "Tianyu Jia",
        "Xingchen Yang",
        "Ciaran McGeady",
        "Yifeng Li",
        "Jinzhi Lin",
        "Kit San Ho",
        "Feiyu Pan",
        "Linhong Ji",
        "Chong Li",
        "Dario Farina"
      ],
      "arxiv_id": "2511.08454v1",
      "summary": "Brain-computer interfaces (BCIs) promise to extend human movement capabilities by enabling direct neural control of supernumerary effectors, yet integrating augmented commands with multiple degrees of freedom without disrupting natural movement remains a key challenge. Here, we propose a tactile-encoded BCI that leverages sensory afferents through a novel tactile-evoked P300 paradigm, allowing intuitive and reliable decoding of supernumerary motor intentions even when superimposed with voluntary actions. The interface was evaluated in a multi-day experiment comprising of a single motor recognition task to validate baseline BCI performance and a dual task paradigm to assess the potential influence between the BCI and natural human movement. The brain interface achieved real-time and reliable decoding of four supernumerary degrees of freedom, with significant performance improvements after only three days of training. Importantly, after training, performance did not differ significantly between the single- and dual-BCI task conditions, and natural movement remained unimpaired during concurrent supernumerary control. Lastly, the interface was deployed in a movement augmentation task, demonstrating its ability to command two supernumerary robotic arms for functional assistance during bimanual tasks. These results establish a new neural interface paradigm for movement augmentation through stimulation of sensory afferents, expanding motor degrees of freedom without impairing natural movement.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08454v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "bi-manual",
            "bimanual"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Is It Truly Necessary to Process and Fit Minutes-Long Reference Videos for Personalized Talking Face Generation?",
      "authors": [
        "Rui-Qing Sun",
        "Ang Li",
        "Zhijing Wu",
        "Tian Lan",
        "Qianyu Lu",
        "Xingshan Yao",
        "Chen Xu",
        "Xian-Ling Mao"
      ],
      "arxiv_id": "2511.07940v1",
      "summary": "Talking Face Generation (TFG) aims to produce realistic and dynamic talking portraits, with broad applications in fields such as digital education, film and television production, e-commerce live streaming, and other related areas. Currently, TFG methods based on Neural Radiated Field (NeRF) or 3D Gaussian sputtering (3DGS) are received widespread attention. They learn and store personalized features from reference videos of each target individual to generate realistic speaking videos. To ensure models can capture sufficient 3D information and successfully learns the lip-audio mapping, previous studies usually require meticulous processing and fitting several minutes of reference video, which always takes hours. The computational burden of processing and fitting long reference videos severely limits the practical application value of these methods.However, is it really necessary to fit such minutes of reference video? Our exploratory case studies show that using some informative reference video segments of just a few seconds can achieve performance comparable to or even better than the full reference video. This indicates that video informative quality is much more important than its length. Inspired by this observation, we propose the ISExplore (short for Informative Segment Explore), a simple-yet-effective segment selection strategy that automatically identifies the informative 5-second reference video segment based on three key data quality dimensions: audio feature diversity, lip movement amplitude, and number of camera views. Extensive experiments demonstrate that our approach increases data processing and training speed by more than 5x for NeRF and 3DGS methods, while maintaining high-fidelity output. Project resources are available at xx.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07940v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3DGS",
            "NeRF"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Omni-View: Unlocking How Generation Facilitates Understanding in Unified 3D Model based on Multiview images",
      "authors": [
        "JiaKui Hu",
        "Shanshan Zhao",
        "Qing-Guo Chen",
        "Xuerui Qiu",
        "Jialun Liu",
        "Zhao Xu",
        "Weihua Luo",
        "Kaifu Zhang",
        "Yanye Lu"
      ],
      "arxiv_id": "2511.07222v1",
      "summary": "This paper presents Omni-View, which extends the unified multimodal understanding and generation to 3D scenes based on multiview images, exploring the principle that \"generation facilitates understanding\". Consisting of understanding model, texture module, and geometry module, Omni-View jointly models scene understanding, novel view synthesis, and geometry estimation, enabling synergistic interaction between 3D scene understanding and generation tasks. By design, it leverages the spatiotemporal modeling capabilities of its texture module responsible for appearance synthesis, alongside the explicit geometric constraints provided by its dedicated geometry module, thereby enriching the model's holistic understanding of 3D scenes. Trained with a two-stage strategy, Omni-View achieves a state-of-the-art score of 55.4 on the VSI-Bench benchmark, outperforming existing specialized 3D understanding models, while simultaneously delivering strong performance in both novel view synthesis and 3D scene generation.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "Under review",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07222v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "novel view synthesis",
            "scene understanding"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "LeCoT: revisiting network architecture for two-view correspondence pruning",
      "authors": [
        "Luanyuan Dai",
        "Xiaoyu Du",
        "Jinhui Tang"
      ],
      "arxiv_id": "2511.07078v1",
      "summary": "Two-view correspondence pruning aims to accurately remove incorrect correspondences (outliers) from initial ones and is widely applied to various computer vision tasks. Current popular strategies adopt multilayer perceptron (MLP) as the backbone, supplemented by additional modules to enhance the network ability to handle context information, which is a known limitation of MLPs. In contrast, we introduce a novel perspective for capturing correspondence context information without extra design modules. To this end, we design a two-view correspondence pruning network called LeCoT, which can naturally leverage global context information at different stages. Specifically, the core design of LeCoT is the Spatial-Channel Fusion Transformer block, a newly proposed component that efficiently utilizes both spatial and channel global context information among sparse correspondences. In addition, we integrate the proposed prediction block that utilizes correspondence features from intermediate stages to generate a probability set, which acts as guiding information for subsequent learning phases, allowing the network to more effectively capture robust global context information. Notably, this prediction block progressively refines the probability set, thereby mitigating the issue of information loss that is common in the traditional one. Extensive experiments prove that the proposed LeCoT outperforms state-of-the-art methods in correspondence pruning, relative pose estimation, homography estimation, visual localization, and $3$D~reconstruction tasks. The code is provided in https://github.com/Dailuanyuan2024/LeCoT-Revisiting-Network-Architecture-for-Two-View-Correspondence-Pruning.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "Just accepted at SCIENCE CHINA Information Sciences",
      "doi": "10.1007/s11432-024-4555-x",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07078v1",
      "code_links": [
        {
          "url": "https://github.com/Dailuanyuan2024/LeCoT-Revisiting-Network-Architecture-for-Two-View-Correspondence-Pruning",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "pose estimation",
            "localization"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Programmable Telescopic Soft Pneumatic Actuators for Deployable and Shape Morphing Soft Robots",
      "authors": [
        "Joel Kemp",
        "Andre Farinha",
        "David Howard",
        "Krishna Manaswi Digumarti",
        "Josh Pinskier"
      ],
      "arxiv_id": "2511.06673v1",
      "summary": "Soft Robotics presents a rich canvas for free-form and continuum devices capable of exerting forces in any direction and transforming between arbitrary configurations. However, there is no current way to tractably and directly exploit the design freedom due to the curse of dimensionality. Parameterisable sets of designs offer a pathway towards tractable, modular soft robotics that appropriately harness the behavioural freeform of soft structures to create rich embodied behaviours. In this work, we present a parametrised class of soft actuators, Programmable Telescopic Soft Pneumatic Actuators (PTSPAs). PTSPAs expand axially on inflation for deployable structures and manipulation in challenging confined spaces. We introduce a parametric geometry generator to customise actuator models from high-level inputs, and explore the new design space through semi-automated experimentation and systematic exploration of key parameters. Using it we characterise the actuators' extension/bending, expansion, and stiffness and reveal clear relationships between key design parameters and performance. Finally we demonstrate the application of the actuators in a deployable soft quadruped whose legs deploy to walk, enabling automatic adaptation to confined spaces. PTSPAs present new design paradigm for deployable and shape morphing structures and wherever large length changes are required.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "8 pages, 10 figures, Submitted to Robosoft 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06673v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped",
            "manipulation"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "BuildingWorld: A Structured 3D Building Dataset for Urban Foundation Models",
      "authors": [
        "Shangfeng Huang",
        "Ruisheng Wang",
        "Xin Wang"
      ],
      "arxiv_id": "2511.06337v1",
      "summary": "As digital twins become central to the transformation of modern cities, accurate and structured 3D building models emerge as a key enabler of high-fidelity, updatable urban representations. These models underpin diverse applications including energy modeling, urban planning, autonomous navigation, and real-time reasoning. Despite recent advances in 3D urban modeling, most learning-based models are trained on building datasets with limited architectural diversity, which significantly undermines their generalizability across heterogeneous urban environments. To address this limitation, we present BuildingWorld, a comprehensive and structured 3D building dataset designed to bridge the gap in stylistic diversity. It encompasses buildings from geographically and architecturally diverse regions -- including North America, Europe, Asia, Africa, and Oceania -- offering a globally representative dataset for urban-scale foundation modeling and analysis. Specifically, BuildingWorld provides about five million LOD2 building models collected from diverse sources, accompanied by real and simulated airborne LiDAR point clouds. This enables comprehensive research on 3D building reconstruction, detection and segmentation. Cyber City, a virtual city model, is introduced to enable the generation of unlimited training data with customized and structurally diverse point cloud distributions. Furthermore, we provide standardized evaluation metrics tailored for building reconstruction, aiming to facilitate the training, evaluation, and comparison of large-scale vision models and foundation models in structured 3D urban environments.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-09",
      "updated": "2025-11-09",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06337v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud",
            "navigation"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Exploring Category-level Articulated Object Pose Tracking on SE(3) Manifolds",
      "authors": [
        "Xianhui Meng",
        "Yukang Huo",
        "Li Zhang",
        "Liu Liu",
        "Haonan Jiang",
        "Yan Zhong",
        "Pingrui Zhang",
        "Cewu Lu",
        "Jun Liu"
      ],
      "arxiv_id": "2511.05996v1",
      "summary": "Articulated objects are prevalent in daily life and robotic manipulation tasks. However, compared to rigid objects, pose tracking for articulated objects remains an underexplored problem due to their inherent kinematic constraints. To address these challenges, this work proposes a novel point-pair-based pose tracking framework, termed \\textbf{PPF-Tracker}. The proposed framework first performs quasi-canonicalization of point clouds in the SE(3) Lie group space, and then models articulated objects using Point Pair Features (PPF) to predict pose voting parameters by leveraging the invariance properties of SE(3). Finally, semantic information of joint axes is incorporated to impose unified kinematic constraints across all parts of the articulated object. PPF-Tracker is systematically evaluated on both synthetic datasets and real-world scenarios, demonstrating strong generalization across diverse and challenging environments. Experimental results highlight the effectiveness and robustness of PPF-Tracker in multi-frame pose tracking of articulated objects. We believe this work can foster advances in robotics, embodied intelligence, and augmented reality. Codes are available at https://github.com/mengxh20/PPFTracker.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-08",
      "updated": "2025-11-08",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05996v1",
      "code_links": [
        {
          "url": "https://github.com/mengxh20/PPFTracker",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy",
      "authors": [
        "Yang Tian",
        "Yuyin Yang",
        "Yiman Xie",
        "Zetao Cai",
        "Xu Shi",
        "Ning Gao",
        "Hangxu Liu",
        "Xuekun Jiang",
        "Zherui Qiu",
        "Feng Yuan",
        "Yaping Li",
        "Ping Wang",
        "Junhao Cai",
        "Jia Zeng",
        "Hao Dong",
        "Jiangmiao Pang"
      ],
      "arxiv_id": "2511.16651v1",
      "summary": "Recent works explore how real and synthetic data contribute to Vision-Language-Action (VLA) models' generalization. While current VLA models have shown the strong effectiveness of large-scale real-robot pre-training, synthetic data has not previously demonstrated comparable capability at scale. This paper provides the first evidence that synthetic data alone can match the performance of the strongest $π$-dataset in pre-training a VLA model, revealing the substantial value of large-scale simulation. The resulting model also exhibits surprisingly zero-shot sim-to-real transfer on several challenging tasks. Our synthetic dataset, InternData-A1, contains over 630k trajectories and 7,433 hours across 4 embodiments, 18 skills, 70 tasks, and 227 scenes, covering rigid, articulated, deformable, and fluid-object manipulation. It is generated through a highly autonomous, fully decoupled, and compositional simulation pipeline that enables long-horizon skill composition, flexible task assembly, and heterogeneous embodiments with minimal manual tuning. Using the same architecture as $π_0$, we pre-train a model entirely on InternData-A1 and find that it matches the official $π_0$ across 49 simulation tasks, 5 real-world tasks, and 4 long-horizon dexterous tasks. We release the dataset and will open-source the generation pipeline to broaden access to large-scale robotic data and to lower the barrier to scalable data creation for embodied AI research.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16651v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "sim-to-real"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "End-to-End Motion Capture from Rigid Body Markers with Geodesic Loss",
      "authors": [
        "Hai Lan",
        "Zongyan Li",
        "Jianmin Hu",
        "Jialing Yang",
        "Houde Dai"
      ],
      "arxiv_id": "2511.16418v1",
      "summary": "Marker-based optical motion capture (MoCap), while long regarded as the gold standard for accuracy, faces practical challenges, such as time-consuming preparation and marker identification ambiguity, due to its reliance on dense marker configurations, which fundamentally limit its scalability. To address this, we introduce a novel fundamental unit for MoCap, the Rigid Body Marker (RBM), which provides unambiguous 6-DoF data and drastically simplifies setup. Leveraging this new data modality, we develop a deep-learning-based regression model that directly estimates SMPL parameters under a geodesic loss. This end-to-end approach matches the performance of optimization-based methods while requiring over an order of magnitude less computation. Trained on synthesized data from the AMASS dataset, our end-to-end model achieves state-of-the-art accuracy in body pose estimation. Real-world data captured using a Vicon optical tracking system further demonstrates the practical viability of our approach. Overall, the results show that combining sparse 6-DoF RBM with a manifold-aware geodesic loss yields a practical and high-fidelity solution for real-time MoCap in graphics, virtual reality, and biomechanics.",
      "categories": [
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "The source code is available in : https://github.com/wer010/GLRBM-Mocap",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16418v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "pose estimation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "SMPL"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam",
        "6_video_extraction"
      ]
    },
    {
      "title": "Homogeneous Proportional-Integral-Derivative Controller in Mobile Robotic Manipulators",
      "authors": [
        "Luis Luna",
        "Isaac Chairez",
        "Andrey Polyakov"
      ],
      "arxiv_id": "2511.16406v1",
      "summary": "Mobile robotic manipulators (MRMs), which integrate mobility and manipulation capabilities, present significant control challenges due to their nonlinear dynamics, underactuation, and coupling between the base and manipulator subsystems. This paper proposes a novel homogeneous Proportional-Integral-Derivative (hPID) control strategy tailored for MRMs to achieve robust and coordinated motion control. Unlike classical PID controllers, the hPID controller leverages the mathematical framework of homogeneous control theory to systematically enhance the stability and convergence properties of the closed-loop system, even in the presence of dynamic uncertainties and external disturbances involved into a system in a homogeneous way. A homogeneous PID structure is designed, ensuring improved convergence of tracking errors through a graded homogeneity approach that generalizes traditional PID gains to nonlinear, state-dependent functions. Stability analysis is conducted using Lyapunov-based methods, demonstrating that the hPID controller guarantees global asymptotic stability and finite-time convergence under mild assumptions. Experimental results on a representative MRM model validate the effectiveness of the hPID controller in achieving high-precision trajectory tracking for both the mobile base and manipulator arm, outperforming conventional linear PID controllers in terms of response time, steady-state error, and robustness to model uncertainties. This research contributes a scalable and analytically grounded control framework for enhancing the autonomy and reliability of next-generation mobile manipulation systems in structured and unstructured environments.",
      "categories": [
        "cs.RO",
        "nlin.AO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16406v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "mobile manipulation"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling",
      "authors": [
        "Minseok Seo",
        "Mark Hamilton",
        "Changick Kim"
      ],
      "arxiv_id": "2511.16301v2",
      "summary": "We present \\textbf{Upsample Anything}, a lightweight test-time optimization (TTO) framework that restores low-resolution features to high-resolution, pixel-wise outputs without any training. Although Vision Foundation Models demonstrate strong generalization across diverse downstream tasks, their representations are typically downsampled by 14x/16x (e.g., ViT), which limits their direct use in pixel-level applications. Existing feature upsampling approaches depend on dataset-specific retraining or heavy implicit optimization, restricting scalability and generalization. Upsample Anything addresses these issues through a simple per-image optimization that learns an anisotropic Gaussian kernel combining spatial and range cues, effectively bridging Gaussian Splatting and Joint Bilateral Upsampling. The learned kernel acts as a universal, edge-aware operator that transfers seamlessly across architectures and modalities, enabling precise high-resolution reconstruction of features, depth, or probability maps. It runs in only $\\approx0.419 \\text{s}$ per 224x224 image and achieves state-of-the-art performance on semantic segmentation, depth estimation, and both depth and probability map upsampling. \\textbf{Project page:} \\href{https://seominseok0429.github.io/Upsample-Anything/}{https://seominseok0429.github.io/Upsample-Anything/}",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-24",
      "comment": "15 pages, 12 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16301v2",
      "code_links": [
        {
          "url": "https://seominseok0429.github.io/Upsample-Anything/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation",
            "gaussian splatting"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Semantic Glitch: Agency and Artistry in an Autonomous Pixel Cloud",
      "authors": [
        "Qing Zhang",
        "Jing Huang",
        "Mingyang Xu",
        "Jun Rekimoto"
      ],
      "arxiv_id": "2511.16048v1",
      "summary": "While mainstream robotics pursues metric precision and flawless performance, this paper explores the creative potential of a deliberately \"lo-fi\" approach. We present the \"Semantic Glitch,\" a soft flying robotic art installation whose physical form, a 3D pixel style cloud, is a \"physical glitch\" derived from digital archaeology. We detail a novel autonomous pipeline that rejects conventional sensors like LiDAR and SLAM, relying solely on the qualitative, semantic understanding of a Multimodal Large Language Model to navigate. By authoring a bio-inspired personality for the robot through a natural language prompt, we create a \"narrative mind\" that complements the \"weak,\" historically, loaded body. Our analysis begins with a 13-minute autonomous flight log, and a follow-up study statistically validates the framework's robustness for authoring quantifiably distinct personas. The combined analysis reveals emergent behaviors, from landmark-based navigation to a compelling \"plan to execution\" gap, and a character whose unpredictable, plausible behavior stems from a lack of precise proprioception. This demonstrates a lo-fi framework for creating imperfect companions whose success is measured in character over efficiency.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "NeurIPS 2025 Creative AI Track, The Thirty-Ninth Annual Conference on Neural Information Processing Systems",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16048v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "SLAM",
            "navigation"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Physically Realistic Sequence-Level Adversarial Clothing for Robust Human-Detection Evasion",
      "authors": [
        "Dingkun Zhou",
        "Patrick P. K. Chan",
        "Hengxu Wu",
        "Shikang Zheng",
        "Ruiqi Huang",
        "Yuanjie Zhao"
      ],
      "arxiv_id": "2511.16020v2",
      "summary": "Deep neural networks used for human detection are highly vulnerable to adversarial manipulation, creating safety and privacy risks in real surveillance environments. Wearable attacks offer a realistic threat model, yet existing approaches usually optimize textures frame by frame and therefore fail to maintain concealment across long video sequences with motion, pose changes, and garment deformation. In this work, a sequence-level optimization framework is introduced to generate natural, printable adversarial textures for shirts, trousers, and hats that remain effective throughout entire walking videos in both digital and physical settings. Product images are first mapped to UV space and converted into a compact palette and control-point parameterization, with ICC locking to keep all colors printable. A physically based human-garment pipeline is then employed to simulate motion, multi-angle camera viewpoints, cloth dynamics, and illumination variation. An expectation-over-transformation objective with temporal weighting is used to optimize the control points so that detection confidence is minimized across whole sequences. Extensive experiments demonstrate strong and stable concealment, high robustness to viewpoint changes, and superior cross-model transferability. Physical garments produced with sublimation printing achieve reliable suppression under indoor and outdoor recordings, confirming real-world feasibility.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-12-13",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16020v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "walking",
            "manipulation"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Behavior Trees vs Executable Ontologies: a Comparative Analysis of Robot Control Paradigms",
      "authors": [
        "Alexander Boldachev"
      ],
      "arxiv_id": "2511.15274v1",
      "summary": "This paper compares two distinct approaches to modeling robotic behavior: imperative Behavior Trees (BTs) and declarative Executable Ontologies (EO), implemented through the boldsea framework. BTs structure behavior hierarchically using control-flow, whereas EO represents the domain as a temporal, event-based semantic graph driven by dataflow rules. We demonstrate that EO achieves comparable reactivity and modularity to BTs through a fundamentally different architecture: replacing polling-based tick execution with event-driven state propagation. We propose that EO offers an alternative framework, moving from procedural programming to semantic domain modeling, to address the semantic-process gap in traditional robotic control. EO supports runtime model modification, full temporal traceability, and a unified representation of data, logic, and interface - features that are difficult or sometimes impossible to achieve with BTs, although BTs excel in established, predictable scenarios. The comparison is grounded in a practical mobile manipulation task. This comparison highlights the respective operational strengths of each approach in dynamic, evolving robotic systems.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.FL"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "22 pages, 8 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15274v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "mobile manipulation"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Strong Duality and Dual Ascent Approach to Continuous-Time Chance-Constrained Stochastic Optimal Control",
      "authors": [
        "Apurva Patil",
        "Alfredo Duarte",
        "Fabrizio Bisetti",
        "Takashi Tanaka"
      ],
      "arxiv_id": "2511.19451v1",
      "summary": "The paper addresses a continuous-time continuous-space chance-constrained stochastic optimal control (SOC) problem where the probability of failure to satisfy given state constraints is explicitly bounded. We leverage the notion of exit time from continuous-time stochastic calculus to formulate a chance-constrained SOC problem. Without any conservative approximation, the chance constraint is transformed into an expectation of an indicator function which can be incorporated into the cost function by considering a dual formulation. We then express the dual function in terms of the solution to a Hamilton-Jacobi-Bellman partial differential equation parameterized by the dual variable. Under a certain assumption on the system dynamics and cost function, it is shown that a strong duality holds between the primal chance-constrained problem and its dual. The Path integral approach is utilized to numerically solve the dual problem via gradient ascent using open-loop samples of system trajectories. We present simulation studies on chance-constrained motion planning for spatial navigation of mobile robots and the solution of the path integral approach is compared with that of the finite difference method.",
      "categories": [
        "eess.SY",
        "cs.RO"
      ],
      "primary_category": "eess.SY",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2504.17154",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19451v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "motion planning"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "SceneEdited: A City-Scale Benchmark for 3D HD Map Updating via Image-Guided Change Detection",
      "authors": [
        "Chun-Jung Lin",
        "Tat-Jun Chin",
        "Sourav Garg",
        "Feras Dayoub"
      ],
      "arxiv_id": "2511.15153v1",
      "summary": "Accurate, up-to-date High-Definition (HD) maps are critical for urban planning, infrastructure monitoring, and autonomous navigation. However, these maps quickly become outdated as environments evolve, creating a need for robust methods that not only detect changes but also incorporate them into updated 3D representations. While change detection techniques have advanced significantly, there remains a clear gap between detecting changes and actually updating 3D maps, particularly when relying on 2D image-based change detection. To address this gap, we introduce SceneEdited, the first city-scale dataset explicitly designed to support research on HD map maintenance through 3D point cloud updating. SceneEdited contains over 800 up-to-date scenes covering 73 km of driving and approximate 3 $\\text{km}^2$ of urban area, with more than 23,000 synthesized object changes created both manually and automatically across 2000+ out-of-date versions, simulating realistic urban modifications such as missing roadside infrastructure, buildings, overpasses, and utility poles. Each scene includes calibrated RGB images, LiDAR scans, and detailed change masks for training and evaluation. We also provide baseline methods using a foundational image-based structure-from-motion pipeline for updating outdated scenes, as well as a comprehensive toolkit supporting scalability, trackability, and portability for future dataset expansion and unification of out-of-date object annotations. Both the dataset and the toolkit are publicly available at https://github.com/ChadLin9596/ScenePoint-ETK, establising a standardized benchmark for 3D map updating research.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "accepted by WACV 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15153v1",
      "code_links": [
        {
          "url": "https://github.com/ChadLin9596/ScenePoint-ETK",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud",
            "navigation"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Going Places: Place Recognition in Artificial and Natural Systems",
      "authors": [
        "Michael Milford",
        "Tobias Fischer"
      ],
      "arxiv_id": "2511.14341v1",
      "summary": "Place recognition, the ability to identify previously visited locations, is critical for both biological navigation and autonomous systems. This review synthesizes findings from robotic systems, animal studies, and human research to explore how different systems encode and recall place. We examine the computational and representational strategies employed across artificial systems, animals, and humans, highlighting convergent solutions such as topological mapping, cue integration, and memory management. Animal systems reveal evolved mechanisms for multimodal navigation and environmental adaptation, while human studies provide unique insights into semantic place concepts, cultural influences, and introspective capabilities. Artificial systems showcase scalable architectures and data-driven models. We propose a unifying set of concepts by which to consider and develop place recognition mechanisms and identify key challenges such as generalization, robustness, and environmental variability. This review aims to foster innovations in artificial localization by connecting future developments in artificial place recognition systems to insights from both animal navigation research and human spatial cognition studies.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "",
      "doi": "10.1146/annurev-control-032724-014418",
      "journal_ref": "Annual Review of Control, Robotics, and Autonomous Systems 2026, vol. 9",
      "pdf_url": "https://arxiv.org/pdf/2511.14341v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization",
            "navigation"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "3D Ground Truth Reconstruction from Multi-Camera Annotations Using UKF",
      "authors": [
        "Linh Van Ma",
        "Unse Fatima",
        "Tepy Sokun Chriv",
        "Haroon Imran",
        "Moongu Jeon"
      ],
      "arxiv_id": "2511.17609v1",
      "summary": "Accurate 3D ground truth estimation is critical for applications such as autonomous navigation, surveillance, and robotics. This paper introduces a novel method that uses an Unscented Kalman Filter (UKF) to fuse 2D bounding box or pose keypoint ground truth annotations from multiple calibrated cameras into accurate 3D ground truth. By leveraging human-annotated ground-truth 2D, our proposed method, a multi-camera single-object tracking algorithm, transforms 2D image coordinates into robust 3D world coordinates through homography-based projection and UKF-based fusion. Our proposed algorithm processes multi-view data to estimate object positions and shapes while effectively handling challenges such as occlusion. We evaluate our method on the CMC, Wildtrack, and Panoptic datasets, demonstrating high accuracy in 3D localization compared to the available 3D ground truth. Unlike existing approaches that provide only ground-plane information, our method also outputs the full 3D shape of each object. Additionally, the algorithm offers a scalable and fully automatic solution for multi-camera systems using only 2D image annotations.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "International Conference on Control, Automation and Information Sciences (ICCAIS) 2025, October 27 - 29, 2025 | Jeju, Korea",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17609v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization",
            "navigation"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting",
      "authors": [
        "Jiangnan Ye",
        "Jiedong Zhuang",
        "Lianrui Mu",
        "Wenjie Zheng",
        "Jiaqi Hu",
        "Xingze Zou",
        "Jing Wang",
        "Haoji Hu"
      ],
      "arxiv_id": "2511.13684v1",
      "summary": "We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "Submitting for Neurocomputing",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13684v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3DGS",
            "gaussian splatting"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "CUTE-Planner: Confidence-aware Uneven Terrain Exploration Planner",
      "authors": [
        "Miryeong Park",
        "Dongjin Cho",
        "Sanghyun Kim",
        "Younggun Cho"
      ],
      "arxiv_id": "2511.12984v1",
      "summary": "Planetary exploration robots must navigate uneven terrain while building reliable maps for space missions. However, most existing methods incorporate traversability constraints but may not handle high uncertainty in elevation estimates near complex features like craters, do not consider exploration strategies for uncertainty reduction, and typically fail to address how elevation uncertainty affects navigation safety and map quality. To address the problems, we propose a framework integrating safe path generation, adaptive confidence updates, and confidence-aware exploration strategies. Using Kalman-based elevation estimation, our approach generates terrain traversability and confidence scores, then incorporates them into Graph-Based exploration Planner (GBP) to prioritize exploration of traversable low-confidence regions. We evaluate our framework through simulated lunar experiments using a novel low-confidence region ratio metric, achieving 69% uncertainty reduction compared to baseline GBP. In terms of mission success rate, our method achieves 100% while baseline GBP achieves 0%, demonstrating improvements in exploration safety and map reliability.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "Accepted in International Conference on Space Robotics 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12984v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "traversability",
            "navigation"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Botany Meets Robotics in Alpine Scree Monitoring",
      "authors": [
        "Davide De Benedittis",
        "Giovanni Di Lorenzo",
        "Franco Angelini",
        "Barbara Valle",
        "Marina Serena Borgatti",
        "Paolo Remagnino",
        "Marco Caccianiga",
        "Manolo Garabini"
      ],
      "arxiv_id": "2511.12526v2",
      "summary": "According to the European Union's Habitat Directive, habitat monitoring plays a critical role in response to the escalating problems posed by biodiversity loss and environmental degradation. Scree habitats, hosting unique and often endangered species, face severe threats from climate change due to their high-altitude nature. Traditionally, their monitoring has required highly skilled scientists to conduct extensive fieldwork in remote, potentially hazardous locations, making the process resource-intensive and time-consuming. This paper presents a novel approach for scree habitat monitoring using a legged robot to assist botanists in data collection and species identification. Specifically, we deployed the ANYmal C robot in the Italian Alpine bio-region in two field campaigns spanning two years and leveraged deep learning to detect and classify key plant species of interest. Our results demonstrate that agile legged robots can navigate challenging terrains and increase the frequency and efficiency of scree monitoring. When paired with traditional phytosociological surveys performed by botanists, this robotics-assisted protocol not only streamlines field operations but also enhances data acquisition, storage, and usage. The outcomes of this research contribute to the evolving landscape of robotics in environmental science, paving the way for a more comprehensive and sustainable approach to habitat monitoring and preservation.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-16",
      "updated": "2025-12-09",
      "comment": "19 pages, 13 figures",
      "doi": "10.1109/TFR.2025.3632773",
      "journal_ref": "IEEE Transactions on Field Robotics, vol. 2, pp. 920-936, 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.12526v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "legged robot",
            "anymal"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Gaussians on Fire: High-Frequency Reconstruction of Flames",
      "authors": [
        "Jakob Nazarenus",
        "Dominik Michels",
        "Wojtek Palubicki",
        "Simin Kou",
        "Fang-Lue Zhang",
        "Soren Pirk",
        "Reinhard Koch"
      ],
      "arxiv_id": "2511.22459v1",
      "summary": "We propose a method to reconstruct dynamic fire in 3D from a limited set of camera views with a Gaussian-based spatiotemporal representation. Capturing and reconstructing fire and its dynamics is highly challenging due to its volatile nature, transparent quality, and multitude of high-frequency features. Despite these challenges, we aim to reconstruct fire from only three views, which consequently requires solving for under-constrained geometry. We solve this by separating the static background from the dynamic fire region by combining dense multi-view stereo images with monocular depth priors. The fire is initialized as a 3D flow field, obtained by fusing per-view dense optical flow projections. To capture the high frequency features of fire, each 3D Gaussian encodes a lifetime and linear velocity to match the dense optical flow. To ensure sub-frame temporal alignment across cameras we employ a custom hardware synchronization pattern -- allowing us to reconstruct fire with affordable commodity hardware. Our quantitative and qualitative validations across numerous reconstruction experiments demonstrate robust performance for diverse and challenging real fire scenarios.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "18 pages, 9 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22459v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "monocular depth",
            "optical flow"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Endo-G$^{2}$T: Geometry-Guided & Temporally Aware Time-Embedded 4DGS For Endoscopic Scenes",
      "authors": [
        "Yangle Liu",
        "Fengze Li",
        "Kan Liu",
        "Jieming Ma"
      ],
      "arxiv_id": "2511.21367v1",
      "summary": "Endoscopic (endo) video exhibits strong view-dependent effects such as specularities, wet reflections, and occlusions. Pure photometric supervision misaligns with geometry and triggers early geometric drift, where erroneous shapes are reinforced during densification and become hard to correct. We ask how to anchor geometry early for 4D Gaussian splatting (4DGS) while maintaining temporal consistency and efficiency in dynamic endoscopic scenes. Thus, we present Endo-G$^{2}$T, a geometry-guided and temporally aware training scheme for time-embedded 4DGS. First, geo-guided prior distillation converts confidence-gated monocular depth into supervision with scale-invariant depth and depth-gradient losses, using a warm-up-to-cap schedule to inject priors softly and avoid early overfitting. Second, a time-embedded Gaussian field represents dynamics in XYZT with a rotor-like rotation parameterization, yielding temporally coherent geometry with lightweight regularization that favors smooth motion and crisp opacity boundaries. Third, keyframe-constrained streaming improves efficiency and long-horizon stability through keyframe-focused optimization under a max-points budget, while non-keyframes advance with lightweight updates. Across EndoNeRF and StereoMIS-P1 datasets, Endo-G$^{2}$T achieves state-of-the-art results among monocular reconstruction baselines.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21367v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "monocular depth",
            "gaussian splatting"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Neural NMPC through Signed Distance Field Encoding for Collision Avoidance",
      "authors": [
        "Martin Jacquet",
        "Marvin Harms",
        "Kostas Alexis"
      ],
      "arxiv_id": "2511.21312v1",
      "summary": "This paper introduces a neural Nonlinear Model Predictive Control (NMPC) framework for mapless, collision-free navigation in unknown environments with Aerial Robots, using onboard range sensing. We leverage deep neural networks to encode a single range image, capturing all the available information about the environment, into a Signed Distance Function (SDF). The proposed neural architecture consists of two cascaded networks: a convolutional encoder that compresses the input image into a low-dimensional latent vector, and a Multi-Layer Perceptron that approximates the corresponding spatial SDF. This latter network parametrizes an explicit position constraint used for collision avoidance, which is embedded in a velocity-tracking NMPC that outputs thrust and attitude commands to the robot. First, a theoretical analysis of the contributed NMPC is conducted, verifying recursive feasibility and stability properties under fixed observations. Subsequently, we evaluate the open-loop performance of the learning-based components as well as the closed-loop performance of the controller in simulations and experiments. The simulation study includes an ablation study, comparisons with two state-of-the-art local navigation methods, and an assessment of the resilience to drifting odometry. The real-world experiments are conducted in forest environments, demonstrating that the neural NMPC effectively performs collision avoidance in cluttered settings against an adversarial reference velocity input and drifting position estimates.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "accepted for publication in IJRR",
      "doi": "10.1177/02783649251401223",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21312v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "model predictive control"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "When Robots Obey the Patch: Universal Transferable Patch Attacks on Vision-Language-Action Models",
      "authors": [
        "Hui Lu",
        "Yi Yu",
        "Yiming Yang",
        "Chenyu Yi",
        "Qixin Zhang",
        "Bingquan Shen",
        "Alex C. Kot",
        "Xudong Jiang"
      ],
      "arxiv_id": "2511.21192v2",
      "summary": "Vision-Language-Action (VLA) models are vulnerable to adversarial attacks, yet universal and transferable attacks remain underexplored, as most existing patches overfit to a single model and fail in black-box settings. To address this gap, we present a systematic study of universal, transferable adversarial patches against VLA-driven robots under unknown architectures, finetuned variants, and sim-to-real shifts. We introduce UPA-RFAS (Universal Patch Attack via Robust Feature, Attention, and Semantics), a unified framework that learns a single physical patch in a shared feature space while promoting cross-model transfer. UPA-RFAS combines (i) a feature-space objective with an $\\ell_1$ deviation prior and repulsive InfoNCE loss to induce transferable representation shifts, (ii) a robustness-augmented two-phase min-max procedure where an inner loop learns invisible sample-wise perturbations and an outer loop optimizes the universal patch against this hardened neighborhood, and (iii) two VLA-specific losses: Patch Attention Dominance to hijack text$\\to$vision attention and Patch Semantic Misalignment to induce image-text mismatch without labels. Experiments across diverse VLA models, manipulation suites, and physical executions show that UPA-RFAS consistently transfers across models, tasks, and viewpoints, exposing a practical patch-based attack surface and establishing a strong baseline for future defenses.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-30",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21192v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "sim-to-real"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Scenes as Tokens: Multi-Scale Normal Distributions Transform Tokenizer for General 3D Vision-Language Understanding",
      "authors": [
        "Yutao Tang",
        "Cheng Zhao",
        "Gaurav Mittal",
        "Rohith Kukkala",
        "Rama Chellappa",
        "Cheng Peng",
        "Mei Chen"
      ],
      "arxiv_id": "2511.21191v1",
      "summary": "Recent advances in 3D vision-language models (VLMs) highlight a strong potential for 3D scene understanding and reasoning. However, effectively tokenizing 3D scenes into holistic scene tokens, and leveraging these tokens across diverse 3D understanding tasks, remain highly challenging. We present NDTokenizer3D, a generalist 3D VLM that performs a wide range of 3D scene understanding tasks while naturally supporting human interactions, thereby bridging language-level reasoning with 3D spatial understanding. The core of our approach is a novel three-stage scene tokenization pipeline built upon a Multi-Scale Normal Distributions Transform (NDT) representation, paired with a Multi-Scale NDT Decoder (MSDec). Specifically, NDTokenizer3D first constructs a multi-scale NDT representation from raw high-resolution point clouds, preserving both global context and fine-grained geometric details. Next, the MSDec progressively fuses cross-scale NDT features, producing holistic scene tokens consumable by LLM endpoints. Beyond tokenization, MSDec is repurposed as a general interface for human-interactive prompting (points, boxes, masks) and segmentation-mask decoding, unifying diverse 3D scene understanding tasks within a single architecture. With this compact and unified design, NDTokenizer3D offers a fine-grained, general-purpose 3D VLM, achieving remarkable improvements in 3D Referring Segmentation, 3D Visual Question Answering, and 3D Dense Captioning.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21191v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding",
            "point cloud"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "FaithFusion: Harmonizing Reconstruction and Generation via Pixel-wise Information Gain",
      "authors": [
        "YuAn Wang",
        "Xiaofan Li",
        "Chi Huang",
        "Wenhao Zhang",
        "Hao Li",
        "Bosheng Wang",
        "Xun Sun",
        "Jun Wang"
      ],
      "arxiv_id": "2511.21113v1",
      "summary": "In controllable driving-scene reconstruction and 3D scene generation, maintaining geometric fidelity while synthesizing visually plausible appearance under large viewpoint shifts is crucial. However, effective fusion of geometry-based 3DGS and appearance-driven diffusion models faces inherent challenges, as the absence of pixel-wise, 3D-consistent editing criteria often leads to over-restoration and geometric drift. To address these issues, we introduce \\textbf{FaithFusion}, a 3DGS-diffusion fusion framework driven by pixel-wise Expected Information Gain (EIG). EIG acts as a unified policy for coherent spatio-temporal synthesis: it guides diffusion as a spatial prior to refine high-uncertainty regions, while its pixel-level weighting distills the edits back into 3DGS. The resulting plug-and-play system is free from extra prior conditions and structural modifications.Extensive experiments on the Waymo dataset demonstrate that our approach attains SOTA performance across NTA-IoU, NTL-IoU, and FID, maintaining an FID of 107.47 even at 6 meters lane shift. Our code is available at https://github.com/wangyuanbiubiubiu/FaithFusion.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "16 pages, 10 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21113v1",
      "code_links": [
        {
          "url": "https://github.com/wangyuanbiubiubiu/FaithFusion",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3DGS",
            "scene reconstruction"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "FlowerDance: MeanFlow for Efficient and Refined 3D Dance Generation",
      "authors": [
        "Kaixing Yang",
        "Xulong Tang",
        "Ziqiao Peng",
        "Xiangyue Zhang",
        "Puwei Wang",
        "Jun He",
        "Hongyan Liu"
      ],
      "arxiv_id": "2511.21029v1",
      "summary": "Music-to-dance generation aims to translate auditory signals into expressive human motion, with broad applications in virtual reality, choreography, and digital entertainment. Despite promising progress, the limited generation efficiency of existing methods leaves insufficient computational headroom for high-fidelity 3D rendering, thereby constraining the expressiveness of 3D characters during real-world applications. Thus, we propose FlowerDance, which not only generates refined motion with physical plausibility and artistic expressiveness, but also achieves significant generation efficiency on inference speed and memory utilization . Specifically, FlowerDance combines MeanFlow with Physical Consistency Constraints, which enables high-quality motion generation with only a few sampling steps. Moreover, FlowerDance leverages a simple but efficient model architecture with BiMamba-based backbone and Channel-Level Cross-Modal Fusion, which generates dance with efficient non-autoregressive manner. Meanwhile, FlowerDance supports motion editing, enabling users to interactively refine dance sequences. Extensive experiments on AIST++ and FineDance show that FlowerDance achieves state-of-the-art results in both motion quality and generation efficiency. Code will be released upon acceptance.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21029v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "Mamba"
          ],
          "score": 1.5
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "motion generation"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "2_algo_arch",
        "4_motion_diffusion"
      ]
    },
    {
      "title": "Metric, inertially aligned monocular state estimation via kinetodynamic priors",
      "authors": [
        "Jiaxin Liu",
        "Min Li",
        "Wanting Xu",
        "Liang Li",
        "Jiaqi Yang",
        "Laurent Kneip"
      ],
      "arxiv_id": "2511.20496v1",
      "summary": "Accurate state estimation for flexible robotic systems poses significant challenges, particular for platforms with dynamically deforming structures that invalidate rigid-body assumptions. This paper tackles this problem and allows to extend existing rigid-body pose estimation methods to non-rigid systems. Our approach hinges on two core assumptions: first, the elastic properties are captured by an injective deformation-force model, efficiently learned via a Multi-Layer Perceptron; second, we solve the platform's inherently smooth motion using continuous-time B-spline kinematic models. By continuously applying Newton's Second Law, our method establishes a physical link between visually-derived trajectory acceleration and predicted deformation-induced acceleration. We demonstrate that our approach not only enables robust and accurate pose estimation on non-rigid platforms, but that the properly modeled platform physics instigate inertial sensing properties. We demonstrate this feasibility on a simple spring-camera system, and show how it robustly resolves the typically ill-posed problem of metric scale and gravity recovery in monocular visual odometry.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20496v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "visual odometry",
            "pose estimation"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "VGGTFace: Topologically Consistent Facial Geometry Reconstruction in the Wild",
      "authors": [
        "Xin Ming",
        "Yuxuan Han",
        "Tianyu Huang",
        "Feng Xu"
      ],
      "arxiv_id": "2511.20366v2",
      "summary": "Reconstructing topologically consistent facial geometry is crucial for the digital avatar creation pipelines. Existing methods either require tedious manual efforts, lack generalization to in-the-wild data, or are constrained by the limited expressiveness of 3D Morphable Models. To address these limitations, we propose VGGTFace, an automatic approach that innovatively applies the 3D foundation model, i.e. VGGT, for topologically consistent facial geometry reconstruction from in-the-wild multi-view images captured by everyday users. Our key insight is that, by leveraging VGGT, our method naturally inherits strong generalization ability and expressive power from its large-scale training and point map representation. However, it is unclear how to reconstruct a topologically consistent mesh from VGGT, as the topology information is missing in its prediction. To this end, we augment VGGT with Pixel3DMM for injecting topology information via pixel-aligned UV values. In this manner, we convert the pixel-aligned point map of VGGT to a point cloud with topology. Tailored to this point cloud with known topology, we propose a novel Topology-Aware Bundle Adjustment strategy to fuse them, where we construct a Laplacian energy for the Bundle Adjustment objective. Our method achieves high-quality reconstruction in 10 seconds for 16 views on a single NVIDIA RTX 4090. Experiments demonstrate state-of-the-art results on benchmarks and impressive generalization to in-the-wild data. Code is available at https://github.com/grignarder/vggtface.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20366v2",
      "code_links": [
        {
          "url": "https://github.com/grignarder/vggtface",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud",
            "VGGT"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "AMB3R: Accurate Feed-forward Metric-scale 3D Reconstruction with Backend",
      "authors": [
        "Hengyi Wang",
        "Lourdes Agapito"
      ],
      "arxiv_id": "2511.20343v1",
      "summary": "We present AMB3R, a multi-view feed-forward model for dense 3D reconstruction on a metric-scale that addresses diverse 3D vision tasks. The key idea is to leverage a sparse, yet compact, volumetric scene representation as our backend, enabling geometric reasoning with spatial compactness. Although trained solely for multi-view reconstruction, we demonstrate that AMB3R can be seamlessly extended to uncalibrated visual odometry (online) or large-scale structure from motion without the need for task-specific fine-tuning or test-time optimization. Compared to prior pointmap-based models, our approach achieves state-of-the-art performance in camera pose, depth, and metric-scale estimation, 3D reconstruction, and even surpasses optimization-based SLAM and SfM methods with dense reconstruction priors on common benchmarks.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "Project page: https://hengyiwang.github.io/projects/amber",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20343v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "visual odometry",
            "SLAM"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Hibikino-Musashi@Home 2025 Team Description Paper",
      "authors": [
        "Ryohei Kobayashi",
        "Kosei Isomoto",
        "Kosei Yamao",
        "Soma Fumoto",
        "Koshun Arimura",
        "Naoki Yamaguchi",
        "Akinobu Mizutani",
        "Tomoya Shiba",
        "Kouki Kimizuka",
        "Yuta Ohno",
        "Ryo Terashima",
        "Hiromasa Yamaguchi",
        "Tomoaki Fujino",
        "Ryoga Maruno",
        "Wataru Yoshimura",
        "Kazuhito Mine",
        "Tang Phu Thien Nhan",
        "Yuga Yano",
        "Yuichiro Tanaka",
        "Takeshi Nishida",
        "Takashi Morie",
        "Hakaru Tamukoh"
      ],
      "arxiv_id": "2511.20180v1",
      "summary": "This paper provides an overview of the techniques employed by Hibikino-Musashi@Home, which intends to participate in the domestic standard platform league. The team developed a dataset generator for training a robot vision system and an open-source development environment running on a Human Support Robot simulator. The large-language-model-powered task planner selects appropriate primitive skills to perform the task requested by the user. Moreover, the team has focused on research involving brain-inspired memory models for adaptation to individual home environments. This approach aims to provide intuitive and personalized assistance. Additionally, the team contributed to the reusability of the navigation system developed by Pumas in RoboCup2024. The team aimed to design a home service robot to assist humans in their homes and continuously attend competitions to evaluate and improve the developed system.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20180v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "running"
          ],
          "score": 2.0
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam"
      ]
    },
    {
      "title": "Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation",
      "authors": [
        "Xiangkai Ma",
        "Lekai Xing",
        "Han Zhang",
        "Wenzhong Li",
        "Sanglu Lu"
      ],
      "arxiv_id": "2511.19859v1",
      "summary": "Vision-Language-Action (VLA) models built upon Chain-of-Thought (CoT) have achieved remarkable success in advancing general-purpose robotic agents, owing to its significant perceptual comprehension. Recently, since text-only CoT struggles to adequately capture scene details in complex spatial environments, a highly promising strategy involves leveraging visual priors to guide robotic action generation. Nevertheless, these strategies face two inherent challenges: (i) a modality gap between visual observations and low-level actions, and (ii) unstable training due to competing objectives between visual prediction and action generation. To address these challenges, we propose a Vision-Integrated Trajectory Alignment (VITA) framework that learns a shared discrete latent space for vision and action, enabling joint modeling of perception and motor control. VITA introduces a implicit visual CoT: autoregressively generated tokens is simultaneously decoded into future frames predictions and robot actions, thereby internalizing visual dynamics as an inductive bias for motion planning. Extensive experiments on simulated and real-world environments demonstrate state-of-the-art performance. VITA improves 14.5\\%, 9.6\\% and 12.1\\% over existing baselines on CALVIN, LIBERO and SimplerEnv. Furthermore, VITA attains an average success rate of 80.5\\% across six real-world tasks, demonstrating its potential as a generalist robotic manipulation model.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19859v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "motion planning"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "STAvatar: Soft Binding and Temporal Density Control for Monocular 3D Head Avatars Reconstruction",
      "authors": [
        "Jiankuo Zhao",
        "Xiangyu Zhu",
        "Zidu Wang",
        "Zhen Lei"
      ],
      "arxiv_id": "2511.19854v2",
      "summary": "Reconstructing high-fidelity and animatable 3D head avatars from monocular videos remains a challenging yet essential task. Existing methods based on 3D Gaussian Splatting typically bind Gaussians to mesh triangles and model deformations solely via Linear Blend Skinning, which results in rigid motion and limited expressiveness. Moreover, they lack specialized strategies to handle frequently occluded regions (e.g., mouth interiors, eyelids). To address these limitations, we propose STAvatar, which consists of two key components: (1) a UV-Adaptive Soft Binding framework that leverages both image-based and geometric priors to learn per-Gaussian feature offsets within the UV space. This UV representation supports dynamic resampling, ensuring full compatibility with Adaptive Density Control (ADC) and enhanced adaptability to shape and textural variations. (2) a Temporal ADC strategy, which first clusters structurally similar frames to facilitate more targeted computation of the densification criterion. It further introduces a novel fused perceptual error as clone criterion to jointly capture geometric and textural discrepancies, encouraging densification in regions requiring finer details. Extensive experiments on four benchmark datasets demonstrate that STAvatar achieves state-of-the-art reconstruction performance, especially in capturing fine-grained details and reconstructing frequently occluded regions. The code will be publicly available.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-27",
      "comment": "17 pages, 14 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19854v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "gaussian splatting"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Real-Time Object Tracking with On-Device Deep Learning for Adaptive Beamforming in Dynamic Acoustic Environments",
      "authors": [
        "Jorge Ortigoso-Narro",
        "Jose A. Belloch",
        "Adrian Amor-Martin",
        "Sandra Roger",
        "Maximo Cobos"
      ],
      "arxiv_id": "2511.19396v1",
      "summary": "Advances in object tracking and acoustic beamforming are driving new capabilities in surveillance, human-computer interaction, and robotics. This work presents an embedded system that integrates deep learning-based tracking with beamforming to achieve precise sound source localization and directional audio capture in dynamic environments. The approach combines single-camera depth estimation and stereo vision to enable accurate 3D localization of moving objects. A planar concentric circular microphone array constructed with MEMS microphones provides a compact, energy-efficient platform supporting 2D beam steering across azimuth and elevation. Real-time tracking outputs continuously adapt the array's focus, synchronizing the acoustic response with the target's position. By uniting learned spatial awareness with dynamic steering, the system maintains robust performance in the presence of multiple or moving sources. Experimental evaluation demonstrates significant gains in signal-to-interference ratio, making the design well-suited for teleconferencing, smart home devices, and assistive technologies.",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.SD",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19396v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation",
            "localization"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Proxy-Free Gaussian Splats Deformation with Splat-Based Surface Estimation",
      "authors": [
        "Jaeyeong Kim",
        "Seungwoo Yoo",
        "Minhyuk Sung"
      ],
      "arxiv_id": "2511.19542v1",
      "summary": "We introduce SpLap, a proxy-free deformation method for Gaussian splats (GS) based on a Laplacian operator computed from our novel surface-aware splat graph. Existing approaches to GS deformation typically rely on deformation proxies such as cages or meshes, but they suffer from dependency on proxy quality and additional computational overhead. An alternative is to directly apply Laplacian-based deformation techniques by treating splats as point clouds. However, this often fail to properly capture surface information due to lack of explicit structure. To address this, we propose a novel method that constructs a surface-aware splat graph, enabling the Laplacian operator derived from it to support more plausible deformations that preserve details and topology. Our key idea is to leverage the spatial arrangement encoded in splats, defining neighboring splats not merely by the distance between their centers, but by their intersections. Furthermore, we introduce a Gaussian kernel adaptation technique that preserves surface structure under deformation, thereby improving rendering quality after deformation. In our experiments, we demonstrate the superior performance of our method compared to both proxy-based and proxy-free baselines, evaluated on 50 challenging objects from the ShapeNet, Objaverse, and Sketchfab datasets, as well as the NeRF-Synthetic dataset. Code is available at https://github.com/kjae0/SpLap.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "17 pages, Accepted to 3DV 2026 (IEEE/CVF International Conference on 3D Vision)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19542v1",
      "code_links": [
        {
          "url": "https://github.com/kjae0/SpLap",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "NeRF",
            "point cloud"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention",
      "authors": [
        "Lei Xiao",
        "Jifeng Li",
        "Juntao Gao",
        "Feiyang Ye",
        "Yan Jin",
        "Jingjing Qian",
        "Jing Zhang",
        "Yong Wu",
        "Xiaoyuan Yu"
      ],
      "arxiv_id": "2511.18960v2",
      "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in embodied AI tasks. However, existing VLA models, often built upon Vision-Language Models (VLMs), typically process dense visual inputs independently at each timestep. This approach implicitly models the task as a Markov Decision Process (MDP). However, this history-agnostic design is suboptimal for effective visual token processing in dynamic sequential decision-making, as it fails to leverage the context of history. To address this limitation, we reformulate the problem from a Partially Observable Markov Decision Process (POMDP) perspective and propose a novel framework named AVA-VLA. Inspired by the POMDP that the action generation should be conditioned on the belief state. AVA-VLA introduces Active Visual Attention (AVA) to dynamically modulate visual processing. It achieves this by leveraging the recurrent state, which is a neural approximation of the agent's belief state derived from the previous decision step. Specifically, the AVA module uses the recurrent state to compute the soft weights to actively process task-relevant visual tokens based on its historical context. Comprehensive evaluations demonstrate that AVA-VLA achieves state-of-the-art performance across popular robotic benchmarks, including LIBERO and CALVIN. Furthermore, real-world deployments on a dual-arm robot platform validate the framework's practical applicability and robust sim-to-real transferability.",
      "categories": [
        "cs.LG",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-24",
      "updated": "2025-12-02",
      "comment": "18 pages, 10 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18960v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "dual-arm",
            "sim-to-real"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "The Determinant Ratio Matrix Approach to Solving 3D Matching and 2D Orthographic Projection Alignment Tasks",
      "authors": [
        "Andrew J. Hanson",
        "Sonya M. Hanson"
      ],
      "arxiv_id": "2511.19511v1",
      "summary": "Pose estimation is a general problem in computer vision with wide applications. The relative orientation of a 3D reference object can be determined from a 3D rotated version of that object, or from a projection of the rotated object to a 2D planar image. This projection can be a perspective projection (the PnP problem) or an orthographic projection (the OnP problem). We restrict our attention here to the OnP problem and the full 3D pose estimation task (the EnP problem). Here we solve the least squares systems for both the error-free EnP and OnP problems in terms of the determinant ratio matrix (DRaM) approach. The noisy-data case can be addressed with a straightforward rotation correction scheme. While the SVD and optimal quaternion eigensystem methods solve the noisy EnP 3D-3D alignment exactly, the noisy 3D-2D orthographic (OnP) task has no known comparable closed form, and can be solved by DRaM-class methods. We note that while previous similar work has been presented in the literature exploiting both the QR decomposition and the Moore-Penrose pseudoinverse transformations, here we place these methods in a larger context that has not previously been fully recognized in the absence of the corresponding DRaM solution. We term this class of solutions as the DRaM family, and conduct comparisons of the behavior of the families of solutions for the EnP and OnP rotation estimation problems. Overall, this work presents both a new solution to the 3D and 2D orthographic pose estimation problems and provides valuable insight into these classes of problems. With hindsight, we are able to show that our DRaM solutions to the exact EnP and OnP problems possess derivations that could have been discovered in the time of Gauss, and in fact generalize to all analogous N-dimensional Euclidean pose estimation problems.",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "12 pages of main text, 3 figures, 31 pages total (including references and 2 appendices, one with algorithm-defining source code)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19511v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "pose estimation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "3D pose estimation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam",
        "6_video_extraction"
      ]
    },
    {
      "title": "MimiCAT: Mimic with Correspondence-Aware Cascade-Transformer for Category-Free 3D Pose Transfer",
      "authors": [
        "Zenghao Chai",
        "Chen Tang",
        "Yongkang Wong",
        "Xulei Yang",
        "Mohan Kankanhalli"
      ],
      "arxiv_id": "2511.18370v1",
      "summary": "3D pose transfer aims to transfer the pose-style of a source mesh to a target character while preserving both the target's geometry and the source's pose characteristic. Existing methods are largely restricted to characters with similar structures and fail to generalize to category-free settings (e.g., transferring a humanoid's pose to a quadruped). The key challenge lies in the structural and transformation diversity inherent in distinct character types, which often leads to mismatched regions and poor transfer quality. To address these issues, we first construct a million-scale pose dataset across hundreds of distinct characters. We further propose MimiCAT, a cascade-transformer model designed for category-free 3D pose transfer. Instead of relying on strict one-to-one correspondence mappings, MimiCAT leverages semantic keypoint labels to learn a novel soft correspondence that enables flexible many-to-many matching across characters. The pose transfer is then formulated as a conditional generation process, in which the source transformations are first projected onto the target through soft correspondence matching and subsequently refined using shape-conditioned representations. Extensive qualitative and quantitative experiments demonstrate that MimiCAT transfers plausible poses across different characters, significantly outperforming prior methods that are limited to narrow category transfer (e.g., humanoid-to-humanoid).",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-23",
      "updated": "2025-11-23",
      "comment": "tech report",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18370v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped",
            "humanoid"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "RoboArmGS: High-Quality Robotic Arm Splatting via Bézier Curve Refinement",
      "authors": [
        "Hao Wang",
        "Xiaobao Wei",
        "Ying Li",
        "Qingpo Wuwu",
        "Dongli Wu",
        "Jiajun Cao",
        "Ming Lu",
        "Wenzhao Zheng",
        "Shanghang Zhang"
      ],
      "arxiv_id": "2511.17961v1",
      "summary": "Building high-quality digital assets of robotic arms is crucial yet challenging for the Real2Sim2Real pipeline. Current approaches naively bind static 3D Gaussians according to URDF links, forcing them to follow an URDF-rigged motion passively. However, real-world arm motion is noisy, and the idealized URDF-rigged motion cannot accurately model it, leading to severe rendering artifacts in 3D Gaussians. To address these challenges, we propose RoboArmGS, a novel hybrid representation that refines the URDF-rigged motion with learnable Bézier curves, enabling more accurate real-world motion modeling. To be more specific, we present a learnable Bézier Curve motion refiner that corrects per-joint residuals to address mismatches between real-world motion and URDF-rigged motion. RoboArmGS enables the learning of more accurate real-world motion while achieving a coherent binding of 3D Gaussians across arm parts. To support future research, we contribute a carefully collected dataset named RoboArm4D, which comprises several widely used robotic arms for evaluating the quality of building high-quality digital assets. We evaluate our approach on RoboArm4D, and RoboArmGS achieves state-of-the-art performance in real-world motion modeling and rendering quality. The code and dataset will be released.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-22",
      "updated": "2025-11-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17961v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "sim2real",
            "real2sim"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "VSRD++: Autolabeling for 3D Object Detection via Instance-Aware Volumetric Silhouette Rendering",
      "authors": [
        "Zihua Liu",
        "Hiroki Sakuma",
        "Masatoshi Okutomi"
      ],
      "arxiv_id": "2512.01178v1",
      "summary": "Monocular 3D object detection is a fundamental yet challenging task in 3D scene understanding. Existing approaches heavily depend on supervised learning with extensive 3D annotations, which are often acquired from LiDAR point clouds through labor-intensive labeling processes. To tackle this problem, we propose VSRD++, a novel weakly supervised framework for monocular 3D object detection that eliminates the reliance on 3D annotations and leverages neural-field-based volumetric rendering with weak 2D supervision. VSRD++ consists of a two-stage pipeline: multi-view 3D autolabeling and subsequent monocular 3D detector training. In the multi-view autolabeling stage, object surfaces are represented as signed distance fields (SDFs) and rendered as instance masks via the proposed instance-aware volumetric silhouette rendering. To optimize 3D bounding boxes, we decompose each instance's SDF into a cuboid SDF and a residual distance field (RDF) that captures deviations from the cuboid. To address the geometry inconsistency commonly observed in volume rendering methods applied to dynamic objects, we model the dynamic objects by including velocity into bounding box attributes as well as assigning confidence to each pseudo-label. Moreover, we also employ a 3D attribute initialization module to initialize the dynamic bounding box parameters. In the monocular 3D object detection phase, the optimized 3D bounding boxes serve as pseudo labels for training monocular 3D object detectors. Extensive experiments on the KITTI-360 dataset demonstrate that VSRD++ significantly outperforms existing weakly supervised approaches for monocular 3D object detection on both static and dynamic scenes. Code is available at https://github.com/Magicboomliu/VSRD_plus_plus",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "comment": "arXiv admin note: text overlap with arXiv:2404.00149",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01178v1",
      "code_links": [
        {
          "url": "https://github.com/Magicboomliu/VSRD_plus_plus",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding",
            "point cloud"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Integration of UWB Radar on Mobile Robots for Continuous Obstacle and Environment Mapping",
      "authors": [
        "Adelina Giurea",
        "Stijn Luchie",
        "Dieter Coppens",
        "Jeroen Hoebeke",
        "Eli De Poorter"
      ],
      "arxiv_id": "2512.01018v1",
      "summary": "This paper presents an infrastructure-free approach for obstacle detection and environmental mapping using ultra-wideband (UWB) radar mounted on a mobile robotic platform. Traditional sensing modalities such as visual cameras and Light Detection and Ranging (LiDAR) fail in environments with poor visibility due to darkness, smoke, or reflective surfaces. In these visioned-impaired conditions, UWB radar offers a promising alternative. To this end, this work explores the suitability of robot-mounted UWB radar for environmental mapping in dynamic, anchor-free scenarios. The study investigates how different materials (metal, concrete and plywood) and UWB radio channels (5 and 9) influence the Channel Impulse Response (CIR). Furthermore, a processing pipeline is proposed to achieve reliable mapping of detected obstacles, consisting of 3 steps: (i) target identification (based on CIR peak detection), (ii) filtering (based on peak properties, signal-to-noise score, and phase-difference of arrival), and (iii) clustering (based on distance estimation and angle-of-arrival estimation). The proposed approach successfully reduces noise and multipath effects, resulting in an obstacle detection precision of at least 82.36% and a recall of 89.46% on channel 9 even when detecting low-reflective materials such as plywood. This work offers a foundation for further development of UWB-based localisation and mapping (SLAM) systems that do not rely on visual features and, unlike conventional UWB localisation systems, do not require on fixed anchor nodes for triangulation.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "This paper has been submitted to IEEE Access Journal and is currently undergoing review",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01018v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "SLAM"
          ],
          "score": 2.0
        },
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "PULSE"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam",
        "8_physics_animation"
      ]
    },
    {
      "title": "Binary-Gaussian: Compact and Progressive Representation for 3D Gaussian Segmentation",
      "authors": [
        "An Yang",
        "Chenyu Liu",
        "Jun Du",
        "Jianqing Gao",
        "Jia Pan",
        "Jinshui Hu",
        "Baocai Yin",
        "Bing Yin",
        "Cong Liu"
      ],
      "arxiv_id": "2512.00944v1",
      "summary": "3D Gaussian Splatting (3D-GS) has emerged as an efficient 3D representation and a promising foundation for semantic tasks like segmentation. However, existing 3D-GS-based segmentation methods typically rely on high-dimensional category features, which introduce substantial memory overhead. Moreover, fine-grained segmentation remains challenging due to label space congestion and the lack of stable multi-granularity control mechanisms. To address these limitations, we propose a coarse-to-fine binary encoding scheme for per-Gaussian category representation, which compresses each feature into a single integer via the binary-to-decimal mapping, drastically reducing memory usage. We further design a progressive training strategy that decomposes panoptic segmentation into a series of independent sub-tasks, reducing inter-class conflicts and thereby enhancing fine-grained segmentation capability. Additionally, we fine-tune opacity during segmentation training to address the incompatibility between photometric rendering and semantic segmentation, which often leads to foreground-background confusion. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art segmentation performance while significantly reducing memory consumption and accelerating inference.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "",
      "doi": "",
      "journal_ref": "AAAI2026",
      "pdf_url": "https://arxiv.org/pdf/2512.00944v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3D gaussian splatting",
            "gaussian splatting"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "CircleFlow: Flow-Guided Camera Blur Estimation using a Circle Grid Target",
      "authors": [
        "Jiajian He",
        "Enjie Hu",
        "Shiqi Chen",
        "Tianchen Qiu",
        "Huajun Feng",
        "Zhihai Xu",
        "Yueting Chen"
      ],
      "arxiv_id": "2512.00796v1",
      "summary": "The point spread function (PSF) serves as a fundamental descriptor linking the real-world scene to the captured signal, manifesting as camera blur. Accurate PSF estimation is crucial for both optical characterization and computational vision, yet remains challenging due to the inherent ambiguity and the ill-posed nature of intensity-based deconvolution. We introduce CircleFlow, a high-fidelity PSF estimation framework that employs flow-guided edge localization for precise blur characterization. CircleFlow begins with a structured capture that encodes locally anisotropic and spatially varying PSFs by imaging a circle grid target, while leveraging the target's binary luminance prior to decouple image and kernel estimation. The latent sharp image is then reconstructed through subpixel alignment of an initialized binary structure guided by optical flow, whereas the PSF is modeled as an energy-constrained implicit neural representation. Both components are jointly optimized within a demosaicing-aware differentiable framework, ensuring physically consistent and robust PSF estimation enabled by accurate edge localization. Extensive experiments on simulated and real-world data demonstrate that CircleFlow achieves state-of-the-art accuracy and reliability, validating its effectiveness for practical PSF calibration.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00796v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "optical flow",
            "localization"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Sigma: The Key for Vision-Language-Action Models toward Telepathic Alignment",
      "authors": [
        "Libo Wang"
      ],
      "arxiv_id": "2512.00783v2",
      "summary": "To address the gap in humanoid robot cognitive systems regarding the lack of a time-updable mediating thought space between semantics and continuous control, this study constructs and trains a VLA model named \"Sigma\" that runs on a single RTX 4090. It uses the open-source pi05_base model as a foundation and preprocesses svla_so101_pickplace into a training dataset. The researcher independently designed an architecture for a vision-language-action model that combines deep semantic understanding and association to achieve telepathic communication. The training process involved repeated optimizations of data preprocessing, LoRA fine-tuning, and the inference-stage adapter. The experiment employed offline closed-loop replay, comparing Sigma with the untuned pure pi05_base model under data conditions. Results showed that Sigma exhibited a stable decrease in control MSE across vector, fragment, and entire trajectory timescales, while maintaining the telepathy norm and semantic-text alignment quality unchanged. It demonstrates that mind-responsive alignment control is quantified through an architecture that combines deep understanding of semantics and association without retraining the base model, which provides reproducible experience for semantic alignment and intention-driven behavior in humanoid robots.",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-30",
      "updated": "2025-12-02",
      "comment": "The Sigma model has been open-sourced on Hugging Face. Weights, dataset, some scripts, and logs are all available. The link is: https://huggingface.co/Veltraxor/Sigma",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00783v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "humanoid",
            "humanoid robot"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Odometry Without Correspondence from Inertially Constrained Ruled Surfaces",
      "authors": [
        "Chenqi Zhu",
        "Levi Burner",
        "Yiannis Aloimonos"
      ],
      "arxiv_id": "2512.00327v1",
      "summary": "Visual odometry techniques typically rely on feature extraction from a sequence of images and subsequent computation of optical flow. This point-to-point correspondence between two consecutive frames can be costly to compute and suffers from varying accuracy, which affects the odometry estimate's quality. Attempts have been made to bypass the difficulties originating from the correspondence problem by adopting line features and fusing other sensors (event camera, IMU) to improve performance, many of which still heavily rely on correspondence. If the camera observes a straight line as it moves, the image of the line sweeps a smooth surface in image-space time. It is a ruled surface and analyzing its shape gives information about odometry. Further, its estimation requires only differentially computed updates from point-to-line associations. Inspired by event cameras' propensity for edge detection, this research presents a novel algorithm to reconstruct 3D scenes and visual odometry from these ruled surfaces. By constraining the surfaces with the inertia measurements from an onboard IMU sensor, the dimensionality of the solution space is greatly reduced.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-29",
      "updated": "2025-11-29",
      "comment": "14 pages, 13 figures, 5 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00327v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "visual odometry",
            "optical flow"
          ],
          "score": 4.0
        }
      ],
      "relevance_score": 4.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Periodic Skill Discovery",
      "authors": [
        "Jonghae Park",
        "Daesol Cho",
        "Jusuk Lee",
        "Dongseok Shim",
        "Inkyu Jang",
        "H. Jin Kim"
      ],
      "arxiv_id": "2511.03187v3",
      "summary": "Unsupervised skill discovery in reinforcement learning (RL) aims to learn diverse behaviors without relying on external rewards. However, current methods often overlook the periodic nature of learned skills, focusing instead on increasing the mutual dependence between states and skills or maximizing the distance traveled in latent space. Considering that many robotic tasks - particularly those involving locomotion - require periodic behaviors across varying timescales, the ability to discover diverse periodic skills is essential. Motivated by this, we propose Periodic Skill Discovery (PSD), a framework that discovers periodic behaviors in an unsupervised manner. The key idea of PSD is to train an encoder that maps states to a circular latent space, thereby naturally encoding periodicity in the latent representation. By capturing temporal distance, PSD can effectively learn skills with diverse periods in complex robotic tasks, even with pixel-based observations. We further show that these learned skills achieve high performance on downstream tasks such as hurdling. Moreover, integrating PSD with an existing skill discovery method offers more diverse behaviors, thus broadening the agent's repertoire. Our code and demos are available at https://jonghaepark.github.io/psd/",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-05",
      "updated": "2025-11-27",
      "comment": "NeurIPS 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03187v3",
      "code_links": [
        {
          "url": "https://jonghaepark.github.io/psd/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "locomotion"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models",
      "authors": [
        "Xiaoyu Zhan",
        "Wenxuan Huang",
        "Hao Sun",
        "Xinyu Fu",
        "Changfeng Ma",
        "Shaosheng Cao",
        "Bohan Jia",
        "Shaohui Lin",
        "Zhenfei Yin",
        "Lei Bai",
        "Wanli Ouyang",
        "Yuanqi Li",
        "Jie Guo",
        "Yanwen Guo"
      ],
      "arxiv_id": "2511.01618v1",
      "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved 2D visual understanding, prompting interest in their application to complex 3D reasoning tasks. However, it remains unclear whether these models can effectively capture the detailed spatial information required for robust real-world performance, especially cross-view consistency, a key requirement for accurate 3D reasoning. Considering this issue, we introduce Viewpoint Learning, a task designed to evaluate and improve the spatial reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset, consisting of 100K object-centric image pairs with diverse viewpoints and corresponding question-answer pairs. Our approach employs a two-stage fine-tuning strategy: first, foundational knowledge is injected to the baseline MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in significant improvements across multiple tasks; second, generalization is enhanced through Reinforcement Learning using the Group Relative Policy Optimization (GRPO) algorithm on a broader set of questions. Additionally, we introduce a hybrid cold-start initialization method designed to simultaneously learn viewpoint representations and maintain coherent reasoning thinking. Experimental results show that our approach significantly activates the spatial reasoning ability of MLLM, improving performance on both in-domain and out-of-domain reasoning tasks. Our findings highlight the value of developing foundational spatial skills in MLLMs, supporting future progress in robotics, autonomous systems, and 3D scene understanding.",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01618v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning",
      "authors": [
        "Mahmut Selman Gokmen",
        "Cody Bumgardner"
      ],
      "arxiv_id": "2511.01610v1",
      "summary": "Vision Foundation Models (VFMs) have advanced representation learning through self-supervised methods. However, existing training pipelines are often inflexible, domain-specific, or computationally expensive, which limits their usability across different domains and resource settings. DINO-MX is a modular and extensible training framework that combines the core principles of DINO, DINOv2 and DINOv3 within a unified configuration-driven system. It supports a variety of transformer-based architectures and is fully compatible with the Hugging Face ecosystem. The framework includes multiple training strategies such as low-rank adaptation (LoRA), layer freezing, and knowledge distillation, along with support for distributed training through both Distributed Data Parallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to work with both natural and specialized data types, including single- and multi-channel images. Experimental results on diverse datasets show that DINO-MX achieves competitive performance while significantly reducing computational costs. Additionally, it offers interpretability tools and a label-guided data augmentation method that improves attention-based localization without the need for extra detection or segmentation heads. DINO-MX provides a reproducible and scalable foundation for developing, adapting, and benchmarking self-supervised vision models across a range of research and real-world applications.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01610v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "representation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models",
      "authors": [
        "Hongyin Zhang",
        "Shuo Zhang",
        "Junxi Jin",
        "Qixin Zeng",
        "Runze Li",
        "Donglin Wang"
      ],
      "arxiv_id": "2511.01331v2",
      "summary": "Vision-Language-Action (VLA) models have recently emerged as powerful general-purpose policies for robotic manipulation, benefiting from large-scale multi-modal pre-training. However, they often fail to generalize reliably in out-of-distribution deployments, where unavoidable disturbances such as observation noise, sensor errors, or actuation perturbations become prevalent. While recent Reinforcement Learning (RL)-based post-training provides a practical means to adapt pre-trained VLA models, existing methods mainly emphasize reward maximization and overlook robustness to environmental uncertainty. In this work, we introduce RobustVLA, a lightweight online RL post-training method designed to explicitly enhance the resilience of VLA models. Through a systematic robustness analysis, we identify two key regularizations: Jacobian regularization, which mitigates sensitivity to observation noise, and smoothness regularization, which stabilizes policies under action perturbations. Extensive experiments across diverse robotic environments demonstrate that RobustVLA significantly outperforms prior state-of-the-art methods in robustness and reliability. Our results highlight the importance of principled robustness-aware RL post-training as a key step toward improving the reliability and robustness of VLA models.",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-12-01",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01331v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects",
      "authors": [
        "Jiawei Wang",
        "Dingyou Wang",
        "Jiaming Hu",
        "Qixuan Zhang",
        "Jingyi Yu",
        "Lan Xu"
      ],
      "arxiv_id": "2511.01294v2",
      "summary": "A deep understanding of kinematic structures and movable components is essential for enabling robots to manipulate objects and model their own articulated forms. Such understanding is captured through articulated objects, which are essential for tasks such as physical simulation, motion planning, and policy learning. However, creating these models, particularly for objects with high degrees of freedom (DoF), remains a significant challenge. Existing methods typically rely on motion sequences or strong assumptions from hand-curated datasets, which hinders scalability. In this paper, we introduce Kinematify, an automated framework that synthesizes articulated objects directly from arbitrary RGB images or textual descriptions. Our method addresses two core challenges: (i) inferring kinematic topologies for high-DoF objects and (ii) estimating joint parameters from static geometry. To achieve this, we combine MCTS search for structural inference with geometry-driven optimization for joint reasoning, producing physically consistent and functionally valid descriptions. We evaluate Kinematify on diverse inputs from both synthetic and real-world environments, demonstrating improvements in registration and kinematic topology accuracy over prior work.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-11-04",
      "comment": "project page: https://sites.google.com/deemos.com/kinematify",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01294v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "motion planning"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "SLAP: Shortcut Learning for Abstract Planning",
      "authors": [
        "Y. Isabel Liu",
        "Bowen Li",
        "Benjamin Eysenbach",
        "Tom Silver"
      ],
      "arxiv_id": "2511.01107v1",
      "summary": "Long-horizon decision-making with sparse rewards and continuous states and actions remains a fundamental challenge in AI and robotics. Task and motion planning (TAMP) is a model-based framework that addresses this challenge by planning hierarchically with abstract actions (options). These options are manually defined, limiting the agent to behaviors that we as human engineers know how to program (pick, place, move). In this work, we propose Shortcut Learning for Abstract Planning (SLAP), a method that leverages existing TAMP options to automatically discover new ones. Our key idea is to use model-free reinforcement learning (RL) to learn shortcuts in the abstract planning graph induced by the existing options in TAMP. Without any additional assumptions or inputs, shortcut learning leads to shorter solutions than pure planning, and higher task success rates than flat and hierarchical RL. Qualitatively, SLAP discovers dynamic physical improvisations (e.g., slap, wiggle, wipe) that differ significantly from the manually-defined ones. In experiments in four simulated robotic environments, we show that SLAP solves and generalizes to a wide range of tasks, reducing overall plan lengths by over 50% and consistently outperforming planning and RL baselines.",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-02",
      "updated": "2025-11-02",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01107v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "motion planning"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "XFlowMP: Task-Conditioned Motion Fields for Generative Robot Planning with Schrodinger Bridges",
      "authors": [
        "Khang Nguyen",
        "Minh Nhat Vu"
      ],
      "arxiv_id": "2512.00022v1",
      "summary": "Generative robotic motion planning requires not only the synthesis of smooth and collision-free trajectories but also feasibility across diverse tasks and dynamic constraints. Prior planning methods, both traditional and generative, often struggle to incorporate high-level semantics with low-level constraints, especially the nexus between task configurations and motion controllability. In this work, we present XFlowMP, a task-conditioned generative motion planner that models robot trajectory evolution as entropic flows bridging stochastic noises and expert demonstrations via Schrodinger bridges given the inquiry task configuration. Specifically, our method leverages Schrodinger bridges as a conditional flow matching coupled with a score function to learn motion fields with high-order dynamics while encoding start-goal configurations, enabling the generation of collision-free and dynamically-feasible motions. Through evaluations, XFlowMP achieves up to 53.79% lower maximum mean discrepancy, 36.36% smoother motions, and 39.88% lower energy consumption while comparing to the next-best baseline on the RobotPointMass benchmark, and also reducing short-horizon planning time by 11.72%. On long-horizon motions in the LASA Handwriting dataset, our method maintains the trajectories with 1.26% lower maximum mean discrepancy, 3.96% smoother, and 31.97% lower energy. We further demonstrate the practicality of our method on the Kinova Gen3 manipulator, executing planning motions and confirming its robustness in real-world settings.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-02",
      "updated": "2025-11-02",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00022v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "motion planning"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "flow matching"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Learning a Thousand Tasks in a Day",
      "authors": [
        "Kamil Dreczkowski",
        "Pietro Vitiello",
        "Vitalis Vosylius",
        "Edward Johns"
      ],
      "arxiv_id": "2511.10110v1",
      "summary": "Humans are remarkably efficient at learning tasks from demonstrations, but today's imitation learning methods for robot manipulation often require hundreds or thousands of demonstrations per task. We investigate two fundamental priors for improving learning efficiency: decomposing manipulation trajectories into sequential alignment and interaction phases, and retrieval-based generalisation. Through 3,450 real-world rollouts, we systematically study this decomposition. We compare different design choices for the alignment and interaction phases, and examine generalisation and scaling trends relative to today's dominant paradigm of behavioural cloning with a single-phase monolithic policy. In the few-demonstrations-per-task regime (<10 demonstrations), decomposition achieves an order of magnitude improvement in data efficiency over single-phase learning, with retrieval consistently outperforming behavioural cloning for both alignment and interaction. Building on these insights, we develop Multi-Task Trajectory Transfer (MT3), an imitation learning method based on decomposition and retrieval. MT3 learns everyday manipulation tasks from as little as a single demonstration each, whilst also generalising to novel object instances. This efficiency enables us to teach a robot 1,000 distinct everyday tasks in under 24 hours of human demonstrator time. Through 2,200 additional real-world rollouts, we reveal MT3's capabilities and limitations across different task families. Videos of our experiments can be found on at https://www.robot-learning.uk/learning-1000-tasks.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "This is the author's version of the work. It is posted here by permission of the AAAS for personal use, not for redistribution. The definitive version was published in Science Robotics on 12 November 2025, DOI: https://www.science.org/doi/10.1126/scirobotics.adv7594. Link to project website: https://www.robot-learning.uk/learning-1000-tasks",
      "doi": "10.1126/scirobotics.adv7594",
      "journal_ref": "Sci. Robot.10, eadv7594 (2025)",
      "pdf_url": "https://arxiv.org/pdf/2511.10110v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Harnessing Bounded-Support Evolution Strategies for Policy Refinement",
      "authors": [
        "Ethan Hirschowitz",
        "Fabio Ramos"
      ],
      "arxiv_id": "2511.09923v2",
      "summary": "Improving competent robot policies with on-policy RL is often hampered by noisy, low-signal gradients. We revisit Evolution Strategies (ES) as a policy-gradient proxy and localize exploration with bounded, antithetic triangular perturbations, suitable for policy refinement. We propose Triangular-Distribution ES (TD-ES) which pairs bounded triangular noise with a centered-rank finite-difference estimator to deliver stable, parallelizable, gradient-free updates. In a two-stage pipeline - PPO pretraining followed by TD-ES refinement - this preserves early sample efficiency while enabling robust late-stage gains. Across a suite of robotic manipulation tasks, TD-ES raises success rates by 26.5% relative to PPO and greatly reduces variance, offering a simple, compute-light path to reliable refinement.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-13",
      "updated": "2025-11-14",
      "comment": "10 pages, 6 figures, to be published in Australasian Conference on Robotics and Automation (ACRA 2025)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09923v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "PPO"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "CoRL-MPPI: Enhancing MPPI With Learnable Behaviours For Efficient And Provably-Safe Multi-Robot Collision Avoidance",
      "authors": [
        "Stepan Dergachev",
        "Artem Pshenitsyn",
        "Aleksandr Panov",
        "Alexey Skrynnik",
        "Konstantin Yakovlev"
      ],
      "arxiv_id": "2511.09331v1",
      "summary": "Decentralized collision avoidance remains a core challenge for scalable multi-robot systems. One of the promising approaches to tackle this problem is Model Predictive Path Integral (MPPI) -- a framework that is naturally suited to handle any robot motion model and provides strong theoretical guarantees. Still, in practice MPPI-based controller may provide suboptimal trajectories as its performance relies heavily on uninformed random sampling. In this work, we introduce CoRL-MPPI, a novel fusion of Cooperative Reinforcement Learning and MPPI to address this limitation. We train an action policy (approximated as deep neural network) in simulation that learns local cooperative collision avoidance behaviors. This learned policy is then embedded into the MPPI framework to guide its sampling distribution, biasing it towards more intelligent and cooperative actions. Notably, CoRL-MPPI preserves all the theoretical guarantees of regular MPPI. We evaluate our approach in dense, dynamic simulation environments against state-of-the-art baselines, including ORCA, BVC, and a multi-agent MPPI implementation. Our results demonstrate that CoRL-MPPI significantly improves navigation efficiency (measured by success rate and makespan) and safety, enabling agile and robust multi-robot navigation.",
      "categories": [
        "cs.RO",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "The manuscript includes 9 pages, 4 figures, and 1 table",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09331v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Residual Rotation Correction using Tactile Equivariance",
      "authors": [
        "Yizhe Zhu",
        "Zhang Ye",
        "Boce Hu",
        "Haibo Zhao",
        "Yu Qi",
        "Dian Wang",
        "Robert Platt"
      ],
      "arxiv_id": "2511.07381v2",
      "summary": "Visuotactile policy learning augments vision-only policies with tactile input, facilitating contact-rich manipulation. However, the high cost of tactile data collection makes sample efficiency the key requirement for developing visuotactile policies. We present EquiTac, a framework that exploits the inherent SO(2) symmetry of in-hand object rotation to improve sample efficiency and generalization for visuotactile policy learning. EquiTac first reconstructs surface normals from raw RGB inputs of vision-based tactile sensors, so rotations of the normal vector field correspond to in-hand object rotations. An SO(2)-equivariant network then predicts a residual rotation action that augments a base visuomotor policy at test time, enabling real-time rotation correction without additional reorientation demonstrations. On a real robot, EquiTac accurately achieves robust zero-shot generalization to unseen in-hand orientations with very few training samples, where baselines fail even with more training data. To our knowledge, this is the first tactile learning method to explicitly encode tactile equivariance for policy learning, yielding a lightweight, symmetry-aware module that improves reliability in contact-rich tasks.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-10",
      "updated": "2025-11-11",
      "comment": "8 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07381v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "ConsistTalk: Intensity Controllable Temporally Consistent Talking Head Generation with Diffusion Noise Search",
      "authors": [
        "Zhenjie Liu",
        "Jianzhang Lu",
        "Renjie Lu",
        "Cong Liang",
        "Shangfei Wang"
      ],
      "arxiv_id": "2511.06833v1",
      "summary": "Recent advancements in video diffusion models have significantly enhanced audio-driven portrait animation. However, current methods still suffer from flickering, identity drift, and poor audio-visual synchronization. These issues primarily stem from entangled appearance-motion representations and unstable inference strategies. In this paper, we introduce \\textbf{ConsistTalk}, a novel intensity-controllable and temporally consistent talking head generation framework with diffusion noise search inference. First, we propose \\textbf{an optical flow-guided temporal module (OFT)} that decouples motion features from static appearance by leveraging facial optical flow, thereby reducing visual flicker and improving temporal consistency. Second, we present an \\textbf{Audio-to-Intensity (A2I) model} obtained through multimodal teacher-student knowledge distillation. By transforming audio and facial velocity features into a frame-wise intensity sequence, the A2I model enables joint modeling of audio and visual motion, resulting in more natural dynamics. This further enables fine-grained, frame-wise control of motion dynamics while maintaining tight audio-visual synchronization. Third, we introduce a \\textbf{diffusion noise initialization strategy (IC-Init)}. By enforcing explicit constraints on background coherence and motion continuity during inference-time noise search, we achieve better identity preservation and refine motion dynamics compared to the current autoregressive strategy. Extensive experiments demonstrate that ConsistTalk significantly outperforms prior methods in reducing flicker, preserving identity, and delivering temporally stable, high-fidelity talking head videos.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "AAAI26 poster",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06833v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "teacher-student"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "optical flow"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Flow-Aided Flight Through Dynamic Clutters From Point To Motion",
      "authors": [
        "Bowen Xu",
        "Zexuan Yan",
        "Minghao Lu",
        "Xiyu Fan",
        "Yi Luo",
        "Youshen Lin",
        "Zhiqiang Chen",
        "Yeke Chen",
        "Qiyuan Qiao",
        "Peng Lu"
      ],
      "arxiv_id": "2511.16372v2",
      "summary": "Challenges in traversing dynamic clutters lie mainly in the efficient perception of the environmental dynamics and the generation of evasive behaviors considering obstacle movement. Previous solutions have made progress in explicitly modeling the dynamic obstacle motion for avoidance, but this key dependency of decision-making is time-consuming and unreliable in highly dynamic scenarios with occlusions. On the contrary, without introducing object detection, tracking, and prediction, we empower the reinforcement learning (RL) with single LiDAR sensing to realize an autonomous flight system directly from point to motion. For exteroception, a depth sensing distance map achieving fixed-shape, low-resolution, and detail-safe is encoded from raw point clouds, and an environment change sensing point flow is adopted as motion features extracted from multi-frame observations. These two are integrated into a lightweight and easy-to-learn representation of complex dynamic environments. For action generation, the behavior of avoiding dynamic threats in advance is implicitly driven by the proposed change-aware sensing representation, where the policy optimization is indicated by the relative motion modulated distance field. With the deployment-friendly sensing simulation and dynamics model-free acceleration control, the proposed system shows a superior success rate and adaptability to alternatives, and the policy derived from the simulator can drive a real-world quadrotor with safe maneuvers.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-20",
      "updated": "2025-12-10",
      "comment": "Accepted to IEEE Robotics and Automation Letters (RA-L), November, 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16372v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "DynaMimicGen: A Data Generation Framework for Robot Learning of Dynamic Tasks",
      "authors": [
        "Vincenzo Pomponi",
        "Paolo Franceschi",
        "Stefano Baraldo",
        "Loris Roveda",
        "Oliver Avram",
        "Luca Maria Gambardella",
        "Anna Valente"
      ],
      "arxiv_id": "2511.16223v1",
      "summary": "Learning robust manipulation policies typically requires large and diverse datasets, the collection of which is time-consuming, labor-intensive, and often impractical for dynamic environments. In this work, we introduce DynaMimicGen (D-MG), a scalable dataset generation framework that enables policy training from minimal human supervision while uniquely supporting dynamic task settings. Given only a few human demonstrations, D-MG first segments the demonstrations into meaningful sub-tasks, then leverages Dynamic Movement Primitives (DMPs) to adapt and generalize the demonstrated behaviors to novel and dynamically changing environments. Improving prior methods that rely on static assumptions or simplistic trajectory interpolation, D-MG produces smooth, realistic, and task-consistent Cartesian trajectories that adapt in real time to changes in object poses, robot states, or scene geometry during task execution. Our method supports different scenarios - including scene layouts, object instances, and robot configurations - making it suitable for both static and highly dynamic manipulation tasks. We show that robot agents trained via imitation learning on D-MG-generated data achieve strong performance across long-horizon and contact-rich benchmarks, including tasks like cube stacking and placing mugs in drawers, even under unpredictable environment changes. By eliminating the need for extensive human demonstrations and enabling generalization in dynamic settings, D-MG offers a powerful and efficient alternative to manual data collection, paving the way toward scalable, autonomous robot learning.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16223v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Look, Zoom, Understand: The Robotic Eyeball for Embodied Perception",
      "authors": [
        "Jiashu Yang",
        "Yifan Han",
        "Yucheng Xie",
        "Ning Guo",
        "Wenzhao Lian"
      ],
      "arxiv_id": "2511.15279v1",
      "summary": "In embodied AI perception systems, visual perception should be active: the goal is not to passively process static images, but to actively acquire more informative data within pixel and spatial budget constraints. Existing vision models and fixed RGB-D camera systems fundamentally fail to reconcile wide-area coverage with fine-grained detail acquisition, severely limiting their efficacy in open-world robotic applications. To address this issue, we propose EyeVLA, a robotic eyeball for active visual perception that can take proactive actions based on instructions, enabling clear observation of fine-grained target objects and detailed information across a wide spatial extent. EyeVLA discretizes action behaviors into action tokens and integrates them with vision-language models (VLMs) that possess strong open-world understanding capabilities, enabling joint modeling of vision, language, and actions within a single autoregressive sequence. By using the 2D bounding box coordinates to guide the reasoning chain and applying reinforcement learning to refine the viewpoint selection policy, we transfer the open-world scene understanding capability of the VLM to a vision language action (VLA) policy using only minimal real-world data. Experiments show that our system efficiently performs instructed scenes in real-world environments and actively acquires more accurate visual information through instruction-driven actions of rotation and zoom, thereby achieving strong environmental perception capabilities. EyeVLA introduces a novel robotic vision system that leverages detailed and spatially rich, large-scale embodied data, and actively acquires highly informative visual observations for downstream embodied tasks.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15279v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Start Small, Think Big: Curriculum-based Relative Policy Optimization for Visual Grounding",
      "authors": [
        "Qingyang Yan",
        "Guangyao Chen",
        "Yixiong Zou"
      ],
      "arxiv_id": "2511.13924v1",
      "summary": "Chain-of-Thought (CoT) prompting has recently shown significant promise across various NLP and computer vision tasks by explicitly generating intermediate reasoning steps. However, we find that reinforcement learning (RL)-based fine-tuned CoT reasoning can paradoxically degrade performance in Visual Grounding tasks, particularly as CoT outputs become lengthy or complex. Additionally, our analysis reveals that increased dataset size does not always enhance performance due to varying data complexities. Motivated by these findings, we propose Curriculum-based Relative Policy Optimization (CuRPO), a novel training strategy that leverages CoT length and generalized Intersection over Union (gIoU) rewards as complexity indicators to progressively structure training data from simpler to more challenging examples. Extensive experiments on RefCOCO, RefCOCO+, RefCOCOg, and LISA datasets demonstrate the effectiveness of our approach. CuRPO consistently outperforms existing methods, including Visual-RFT, with notable improvements of up to +12.52 mAP on RefCOCO. Moreover, CuRPO exhibits exceptional efficiency and robustness, delivering strong localization performance even in few-shot learning scenarios, particularly benefiting tasks characterized by ambiguous and intricate textual descriptions.The code is released on https://github.com/qyoung-yan/CuRPO.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "AAAI 2026 (Oral)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13924v1",
      "code_links": [
        {
          "url": "https://github.com/qyoung-yan/CuRPO",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "FUSE: A Flow-based Mapping Between Shapes",
      "authors": [
        "Lorenzo Olearo",
        "Giulio Viganò",
        "Daniele Baieri",
        "Filippo Maggioli",
        "Simone Melzi"
      ],
      "arxiv_id": "2511.13431v1",
      "summary": "We introduce a novel neural representation for maps between 3D shapes based on flow-matching models, which is computationally efficient and supports cross-representation shape matching without large-scale training or data-driven procedures. 3D shapes are represented as the probability distribution induced by a continuous and invertible flow mapping from a fixed anchor distribution. Given a source and a target shape, the composition of the inverse flow (source to anchor) with the forward flow (anchor to target), we continuously map points between the two surfaces. By encoding the shapes with a pointwise task-tailored embedding, this construction provides an invertible and modality-agnostic representation of maps between shapes across point clouds, meshes, signed distance fields (SDFs), and volumetric data. The resulting representation consistently achieves high coverage and accuracy across diverse benchmarks and challenging settings in shape matching. Beyond shape matching, our framework shows promising results in other tasks, including UV mapping and registration of raw point cloud scans of human bodies.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "11 pages, 9 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13431v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "flow matching"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "CASL: Curvature-Augmented Self-supervised Learning for 3D Anomaly Detection",
      "authors": [
        "Yaohua Zha",
        "Xue Yuerong",
        "Chunlin Fan",
        "Yuansong Wang",
        "Tao Dai",
        "Ke Chen",
        "Shu-Tao Xia"
      ],
      "arxiv_id": "2511.12909v1",
      "summary": "Deep learning-based 3D anomaly detection methods have demonstrated significant potential in industrial manufacturing. However, many approaches are specifically designed for anomaly detection tasks, which limits their generalizability to other 3D understanding tasks. In contrast, self-supervised point cloud models aim for general-purpose representation learning, yet our investigation reveals that these classical models are suboptimal at anomaly detection under the unified fine-tuning paradigm. This motivates us to develop a more generalizable 3D model that can effectively detect anomalies without relying on task-specific designs. Interestingly, we find that using only the curvature of each point as its anomaly score already outperforms several classical self-supervised and dedicated anomaly detection models, highlighting the critical role of curvature in 3D anomaly detection. In this paper, we propose a Curvature-Augmented Self-supervised Learning (CASL) framework based on a reconstruction paradigm. Built upon the classical U-Net architecture, our approach introduces multi-scale curvature prompts to guide the decoder in predicting the spatial coordinates of each point. Without relying on any dedicated anomaly detection mechanisms, it achieves leading detection performance through straightforward anomaly classification fine-tuning. Moreover, the learned representations generalize well to standard 3D understanding tasks such as point cloud classification. The code is available at https://github.com/zyh16143998882/CASL.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "Accepted to AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12909v1",
      "code_links": [
        {
          "url": "https://github.com/zyh16143998882/CASL",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "representation learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Guiding the Inner Eye: A Framework for Hierarchical and Flexible Visual Grounded Reasoning",
      "authors": [
        "Zhaoyang Wei",
        "Wenchao Ding",
        "Yanchao Hao",
        "Xi Chen"
      ],
      "arxiv_id": "2511.22172v1",
      "summary": "Models capable of \"thinking with images\" by dynamically grounding their reasoning in visual evidence represent a major leap in multimodal AI. However, replicating and advancing this ability is non-trivial, with current methods often trapped between the instability of end-to-end reinforcement learning (RL) and the rigidity of supervised fine-tuning (SFT). This leads to models that either struggle to learn or lack the cognitive flexibility required for complex, real-world scenes. To navigate this dilemma, we introduce GRiP (Guided Reasoning and Perception), a novel two-stage training framework that cultivates robust and flexible visual grounded reasoning by explicitly guiding the model's perceptual focus and logical pathways. GRiP's core lies in its cognitive-enhanced RL stage, which features two key innovations: (1) a Salience-Weighted IoU Reward that incentivizes the model to prioritize the localization of mission-critical objects over trivial distractors, and (2) a Multi-Heuristic Reward that encourages cognitive flexibility by rewarding diverse yet logically valid reasoning pathways. Initialized from the Qwen2.5-VL-7B model, GRiP demonstrates significant performance gains across multiple challenging benchmarks. It achieves state-of-the-art results among open-source models on the highly challenging TreeBench and V* Bench, proving its effectiveness in complex visual reasoning. Our work demonstrates that moving beyond simplistic rewards and instead guiding models with cognitively-inspired signals for what to see and how to think is crucial for unlocking the next level of multimodal intelligence. The code will be made publicly available.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "9pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22172v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Seeing without Pixels: Perception from Camera Trajectories",
      "authors": [
        "Zihui Xue",
        "Kristen Grauman",
        "Dima Damen",
        "Andrew Zisserman",
        "Tengda Han"
      ],
      "arxiv_id": "2511.21681v1",
      "summary": "Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, \"how you move\" can indeed reveal \"what you are doing\" (egocentric) or \"observing\" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "Project website: https://sites.google.com/view/seeing-without-pixels",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21681v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "contrastive learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "pose estimation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "$\\mathcal{E}_0$: Enhancing Generalization and Fine-Grained Control in VLA Models via Continuized Discrete Diffusion",
      "authors": [
        "Zhihao Zhan",
        "Jiaying Zhou",
        "Likui Zhang",
        "Qinhan Lv",
        "Hao Liu",
        "Jusheng Zhang",
        "Weizheng Li",
        "Ziliang Chen",
        "Tianshui Chen",
        "Keze Wang",
        "Liang Lin",
        "Guangrun Wang"
      ],
      "arxiv_id": "2511.21542v1",
      "summary": "Vision-Language-Action (VLA) models offer a unified framework for robotic manipulation by integrating visual perception, language understanding, and control generation. Yet existing VLA models still struggle to generalize across diverse tasks, scenes, and camera viewpoints, and often produce coarse or unstable actions. We introduce E0, a continuized discrete diffusion framework that formulates action generation as iterative denoising over quantized action tokens. Compared with continuous diffusion policies, E0 offers two key advantages: (1) discrete action tokens align naturally with the symbolic structure of pretrained VLM/VLA backbones, enabling stronger semantic conditioning; and 2. discrete diffusion matches the true quantized nature of real-world robot control-whose hardware constraints (e.g., encoder resolution, control frequency, actuation latency) inherently discretize continuous signals-and therefore benefits from a Bayes-optimal denoiser that models the correct discrete action distribution, leading to stronger generalization. Compared with discrete autoregressive and mask-based discrete diffusion models, E0 supports a significantly larger and finer-grained action vocabulary and avoids the distributional mismatch introduced by masking-based corruptions-yielding more accurate fine-grained action control. We further introduce a spherical viewpoint perturbation augmentation method to improve robustness to camera shifts without additional data. Experiments on LIBERO, VLABench, and ManiSkill show that E0 achieves state-of-the-art performance across 14 diverse environments, outperforming strong baselines by 10.7% on average. Real-world evaluation on a Franka arm confirms that E0 delivers precise, robust, and transferable manipulation, establishing discrete diffusion as a promising direction for generalizable VLA policy learning.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21542v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "E-M3RF: An Equivariant Multimodal 3D Re-assembly Framework",
      "authors": [
        "Adeela Islam",
        "Stefano Fiorini",
        "Manuel Lecha",
        "Theodore Tsesmelis",
        "Stuart James",
        "Pietro Morerio",
        "Alessio Del Bue"
      ],
      "arxiv_id": "2511.21422v1",
      "summary": "3D reassembly is a fundamental geometric problem, and in recent years it has increasingly been challenged by deep learning methods rather than classical optimization. While learning approaches have shown promising results, most still rely primarily on geometric features to assemble a whole from its parts. As a result, methods struggle when geometry alone is insufficient or ambiguous, for example, for small, eroded, or symmetric fragments. Additionally, solutions do not impose physical constraints that explicitly prevent overlapping assemblies. To address these limitations, we introduce E-M3RF, an equivariant multimodal 3D reassembly framework that takes as input the point clouds, containing both point positions and colors of fractured fragments, and predicts the transformations required to reassemble them using SE(3) flow matching. Each fragment is represented by both geometric and color features: i) 3D point positions are encoded as rotationconsistent geometric features using a rotation-equivariant encoder, ii) the colors at each 3D point are encoded with a transformer. The two feature sets are then combined to form a multimodal representation. We experimented on four datasets: two synthetic datasets, Breaking Bad and Fantastic Breaks, and two real-world cultural heritage datasets, RePAIR and Presious, demonstrating that E-M3RF on the RePAIR dataset reduces rotation error by 23.1% and translation error by 13.2%, while Chamfer Distance decreases by 18.4% compared to competing methods.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21422v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "flow matching"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Dynamic Test-Time Compute Scaling in Control Policy: Difficulty-Aware Stochastic Interpolant Policy",
      "authors": [
        "Inkook Chun",
        "Seungjae Lee",
        "Michael S. Albergo",
        "Saining Xie",
        "Eric Vanden-Eijnden"
      ],
      "arxiv_id": "2511.20906v1",
      "summary": "Diffusion- and flow-based policies deliver state-of-the-art performance on long-horizon robotic manipulation and imitation learning tasks. However, these controllers employ a fixed inference budget at every control step, regardless of task complexity, leading to computational inefficiency for simple subtasks while potentially underperforming on challenging ones. To address these issues, we introduce Difficulty-Aware Stochastic Interpolant Policy (DA-SIP), a framework that enables robotic controllers to adaptively adjust their integration horizon in real time based on task difficulty. Our approach employs a difficulty classifier that analyzes observations to dynamically select the step budget, the optimal solver variant, and ODE/SDE integration at each control cycle. DA-SIP builds upon the stochastic interpolant formulation to provide a unified framework that unlocks diverse training and inference configurations for diffusion- and flow-based policies. Through comprehensive benchmarks across diverse manipulation tasks, DA-SIP achieves 2.6-4.4x reduction in total computation time while maintaining task success rates comparable to fixed maximum-computation baselines. By implementing adaptive computation within this framework, DA-SIP transforms generative robot controllers into efficient, task-aware systems that intelligently allocate inference resources where they provide the greatest benefit.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20906v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Boosting Reasoning in Large Multimodal Models via Activation Replay",
      "authors": [
        "Yun Xing",
        "Xiaobin Hu",
        "Qingdong He",
        "Jiangning Zhang",
        "Shuicheng Yan",
        "Shijian Lu",
        "Yu-Gang Jiang"
      ],
      "arxiv_id": "2511.19972v2",
      "summary": "Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach to incentivizing reasoning capability in Large Multimodal Models (LMMs), while the underlying mechanisms behind this post-training paradigm are poorly understood. We begin by exploring how input activations are affected by RLVR through the perspective of logit lens. Our systematic investigations across multiple post-trained LMMs suggest that RLVR shifts low-entropy activations unexpectedly, while high-entropy ones are less affected. We further demonstrate that such phenomena are associated with LMM reasoning by controlled experiments, suggesting a potentially beneficial role of modulating low-entropy activations. To this end, we propose Activation Replay, a novel simple yet effective training-free approach that boosts multimodal reasoning of post-trained LMMs without requiring expensive policy optimization. Our design involves manipulation of visual tokens at test time, replaying low-entropy activations from the input context of base LMMs to regulating the RLVR counterparts. Activation Replay triggers better reasoning across diverse scenarios, including mathematics, o3-like visual agents, and video reasoning. We further show that Activation Replay boosts Pass@K and mitigates narrower reasoning coverage of RLVR. Our design is compared against alternative choices, such as replaying high-entropy activations instead of low-entropy ones, or direct cross-model intervention instead of manipulating input tokens, demonstrating the superiority of our implementation. Codes will be made publicly available.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-27",
      "comment": "11 figures, 10 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19972v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Collaborate sim and real: Robot Bin Packing Learning in Real-world and Physical Engine",
      "authors": [
        "Lidi Zhang",
        "Han Wu",
        "Liyu Zhang",
        "Ruofeng Liu",
        "Haotian Wang",
        "Chao Li",
        "Desheng Zhang",
        "Yunhuai Liu",
        "Tian He"
      ],
      "arxiv_id": "2511.19932v1",
      "summary": "The 3D bin packing problem, with its diverse industrial applications, has garnered significant research attention in recent years. Existing approaches typically model it as a discrete and static process, while real-world applications involve continuous gravity-driven interactions. This idealized simplification leads to infeasible deployments (e.g., unstable packing) in practice. Simulations with physical engine offer an opportunity to emulate continuous gravity effects, enabling the training of reinforcement learning (RL) agents to address such limitations and improve packing stability. However, a simulation-to-reality gap persists due to dynamic variations in physical properties of real-world objects, such as various friction coefficients, elasticity, and non-uniform weight distributions. To bridge this gap, we propose a hybrid RL framework that collaborates with physical simulation with real-world data feedback. Firstly, domain randomization is applied during simulation to expose agents to a spectrum of physical parameters, enhancing their generalization capability. Secondly, the RL agent is fine-tuned with real-world deployment feedback, further reducing collapse rates. Extensive experiments demonstrate that our method achieves lower collapse rates in both simulated and real-world scenarios. Large-scale deployments in logistics systems validate the practical effectiveness, with a 35\\% reduction in packing collapse compared to baseline methods.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19932v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "domain randomization"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "MagicWorld: Interactive Geometry-driven Video World Exploration",
      "authors": [
        "Guangyuan Li",
        "Siming Zheng",
        "Shuolin Xu",
        "Jinwei Chen",
        "Bo Li",
        "Xiaobin Hu",
        "Lei Zhao",
        "Peng-Tao Jiang"
      ],
      "arxiv_id": "2511.18886v1",
      "summary": "Recent interactive video world model methods generate scene evolution conditioned on user instructions. Although they achieve impressive results, two key limitations remain. First, they fail to fully exploit the correspondence between instruction-driven scene motion and the underlying 3D geometry, which results in structural instability under viewpoint changes. Second, they easily forget historical information during multi-step interaction, resulting in error accumulation and progressive drift in scene semantics and structure. To address these issues, we propose MagicWorld, an interactive video world model that integrates 3D geometric priors and historical retrieval. MagicWorld starts from a single scene image, employs user actions to drive dynamic scene evolution, and autoregressively synthesizes continuous scenes. We introduce the Action-Guided 3D Geometry Module (AG3D), which constructs a point cloud from the first frame of each interaction and the corresponding action, providing explicit geometric constraints for viewpoint transitions and thereby improving structural consistency. We further propose History Cache Retrieval (HCR) mechanism, which retrieves relevant historical frames during generation and injects them as conditioning signals, helping the model utilize past scene information and mitigate error accumulation. Experimental results demonstrate that MagicWorld achieves notable improvements in scene stability and continuity across interaction iterations.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18886v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "world model"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "How to Train Your Latent Control Barrier Function: Smooth Safety Filtering Under Hard-to-Model Constraints",
      "authors": [
        "Kensuke Nakamura",
        "Arun L. Bishop",
        "Steven Man",
        "Aaron M. Johnson",
        "Zachary Manchester",
        "Andrea Bajcsy"
      ],
      "arxiv_id": "2511.18606v1",
      "summary": "Latent safety filters extend Hamilton-Jacobi (HJ) reachability to operate on latent state representations and dynamics learned directly from high-dimensional observations, enabling safe visuomotor control under hard-to-model constraints. However, existing methods implement \"least-restrictive\" filtering that discretely switch between nominal and safety policies, potentially undermining the task performance that makes modern visuomotor policies valuable. While reachability value functions can, in principle, be adapted to be control barrier functions (CBFs) for smooth optimization-based filtering, we theoretically and empirically show that current latent-space learning methods produce fundamentally incompatible value functions. We identify two sources of incompatibility: First, in HJ reachability, failures are encoded via a \"margin function\" in latent space, whose sign indicates whether or not a latent is in the constraint set. However, representing the margin function as a classifier yields saturated value functions that exhibit discontinuous jumps. We prove that the value function's Lipschitz constant scales linearly with the margin function's Lipschitz constant, revealing that smooth CBFs require smooth margins. Second, reinforcement learning (RL) approximations trained solely on safety policy data yield inaccurate value estimates for nominal policy actions, precisely where CBF filtering needs them. We propose the LatentCBF, which addresses both challenges through gradient penalties that lead to smooth margin functions without additional labeling, and a value-training procedure that mixes data from both nominal and safety policy distributions. Experiments on simulated benchmarks and hardware with a vision-based manipulation policy demonstrate that LatentCBF enables smooth safety filtering while doubling the task-completion rate over prior switching methods.",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-23",
      "updated": "2025-11-23",
      "comment": "3 figures, 10 tables, 22 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18606v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "MicCheck: Repurposing Off-the-Shelf Pin Microphones for Easy and Low-Cost Contact Sensing",
      "authors": [
        "Steven Oh",
        "Tai Inui",
        "Magdeline Kuan",
        "Jia-Yeu Lin"
      ],
      "arxiv_id": "2511.18299v1",
      "summary": "Robotic manipulation tasks are contact-rich, yet most imitation learning (IL) approaches rely primarily on vision, which struggles to capture stiffness, roughness, slip, and other fine interaction cues. Tactile signals can address this gap, but existing sensors often require expensive, delicate, or integration-heavy hardware. In this work, we introduce MicCheck, a plug-and-play acoustic sensing approach that repurposes an off-the-shelf Bluetooth pin microphone as a low-cost contact sensor. The microphone clips into a 3D-printed gripper insert and streams audio via a standard USB receiver, requiring no custom electronics or drivers. Despite its simplicity, the microphone provides signals informative enough for both perception and control. In material classification, it achieves 92.9% accuracy on a 10-class benchmark across four interaction types (tap, knock, slow press, drag). For manipulation, integrating pin microphone into an IL pipeline with open source hardware improves the success rate on picking and pouring task from 0.40 to 0.80 and enables reliable execution of contact-rich skills such as unplugging and sound-based sorting. Compared with high-resolution tactile sensors, pin microphones trade spatial detail for cost and ease of integration, offering a practical pathway for deploying acoustic contact sensing in low-cost robot setups.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-23",
      "updated": "2025-11-23",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18299v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "DiffSwap++: 3D Latent-Controlled Diffusion for Identity-Preserving Face Swapping",
      "authors": [
        "Weston Bondurant",
        "Arkaprava Sinha",
        "Hieu Le",
        "Srijan Das",
        "Stephanie Schuckers"
      ],
      "arxiv_id": "2511.05575v1",
      "summary": "Diffusion-based approaches have recently achieved strong results in face swapping, offering improved visual quality over traditional GAN-based methods. However, even state-of-the-art models often suffer from fine-grained artifacts and poor identity preservation, particularly under challenging poses and expressions. A key limitation of existing approaches is their failure to meaningfully leverage 3D facial structure, which is crucial for disentangling identity from pose and expression. In this work, we propose DiffSwap++, a novel diffusion-based face-swapping pipeline that incorporates 3D facial latent features during training. By guiding the generation process with 3D-aware representations, our method enhances geometric consistency and improves the disentanglement of facial identity from appearance attributes. We further design a diffusion architecture that conditions the denoising process on both identity embeddings and facial landmarks, enabling high-fidelity and identity-preserving face swaps. Extensive experiments on CelebA, FFHQ, and CelebV-Text demonstrate that DiffSwap++ outperforms prior methods in preserving source identity while maintaining target pose and expression. Additionally, we introduce a biometric-style evaluation and conduct a user study to further validate the realism and effectiveness of our approach. Code will be made publicly available at https://github.com/WestonBond/DiffSwapPP",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-04",
      "updated": "2025-11-04",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05575v1",
      "code_links": [
        {
          "url": "https://github.com/WestonBond/DiffSwapPP",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "geometric consistency"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "7_retargeting"
      ]
    },
    {
      "title": "Do Blind Spots Matter for Word-Referent Mapping? A Computational Study with Infant Egocentric Video",
      "authors": [
        "Zekai Shi",
        "Zhixi Cai",
        "Kalin Stefanov"
      ],
      "arxiv_id": "2511.11725v1",
      "summary": "Typically, children start to learn their first words between 6 and 9 months, linking spoken utterances to their visual referents. Without prior knowledge, a word encountered for the first time can be interpreted in countless ways; it might refer to any of the objects in the environment, their components, or attributes. Using longitudinal, egocentric, and ecologically valid data from the experience of one child, in this work, we propose a self-supervised and biologically plausible strategy to learn strong visual representations. Our masked autoencoder-based visual backbone incorporates knowledge about the blind spot in human eyes to define a novel masking strategy. This mask and reconstruct approach attempts to mimic the way the human brain fills the gaps in the eyes' field of view. This represents a significant shift from standard random masking strategies, which are difficult to justify from a biological perspective. The pretrained encoder is utilized in a contrastive learning-based video-text model capable of acquiring word-referent mappings. Extensive evaluation suggests that the proposed biologically plausible masking strategy is at least as effective as random masking for learning word-referent mappings from cross-situational and temporally extended episodes.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11725v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "masked autoencoder",
            "contrastive learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Baby Sophia: A Developmental Approach to Self-Exploration through Self-Touch and Hand Regard",
      "authors": [
        "Stelios Zarifis",
        "Ioannis Chalkiadakis",
        "Artemis Chardouveli",
        "Vasiliki Moutzouri",
        "Aggelos Sotirchos",
        "Katerina Papadimitriou",
        "Panagiotis Filntisis",
        "Niki Efthymiou",
        "Petros Maragos",
        "Katerina Pastra"
      ],
      "arxiv_id": "2511.09727v1",
      "summary": "Inspired by infant development, we propose a Reinforcement Learning (RL) framework for autonomous self-exploration in a robotic agent, Baby Sophia, using the BabyBench simulation environment. The agent learns self-touch and hand regard behaviors through intrinsic rewards that mimic an infant's curiosity-driven exploration of its own body. For self-touch, high-dimensional tactile inputs are transformed into compact, meaningful representations, enabling efficient learning. The agent then discovers new tactile contacts through intrinsic rewards and curriculum learning that encourage broad body coverage, balance, and generalization. For hand regard, visual features of the hands, such as skin-color and shape, are learned through motor babbling. Then, intrinsic rewards encourage the agent to perform novel hand motions, and follow its hands with its gaze. A curriculum learning setup from single-hand to dual-hand training allows the agent to reach complex visual-motor coordination. The results of this work demonstrate that purely curiosity-based signals, with no external supervision, can drive coordinated multimodal learning, imitating an infant's progression from random motor babbling to purposeful behaviors.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "5 pages, 3 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09727v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "curriculum learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Compression then Matching: An Efficient Pre-training Paradigm for Multimodal Embedding",
      "authors": [
        "Da Li",
        "Yuxiao Luo",
        "Keping Bi",
        "Jiafeng Guo",
        "Wei Yuan",
        "Biao Yang",
        "Yan Wang",
        "Fan Yang",
        "Tingting Gao",
        "Guorui Zhou"
      ],
      "arxiv_id": "2511.08480v1",
      "summary": "Vision-language models advance multimodal representation learning by acquiring transferable semantic embeddings, thereby substantially enhancing performance across a range of vision-language tasks, including cross-modal retrieval, clustering, and classification. An effective embedding is expected to comprehensively preserve the semantic content of the input while simultaneously emphasizing features that are discriminative for downstream tasks. Recent approaches demonstrate that VLMs can be adapted into competitive embedding models via large-scale contrastive learning, enabling the simultaneous optimization of two complementary objectives. We argue that the two aforementioned objectives can be decoupled: a comprehensive understanding of the input facilitates the embedding model in achieving superior performance in downstream tasks via contrastive learning. In this paper, we propose CoMa, a compressed pre-training phase, which serves as a warm-up stage for contrastive learning. Experiments demonstrate that with only a small amount of pre-training data, we can transform a VLM into a competitive embedding model. CoMa achieves new state-of-the-art results among VLMs of comparable size on the MMEB, realizing optimization in both efficiency and effectiveness.",
      "categories": [
        "cs.CV",
        "cs.IR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "Multimodal Embedding",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08480v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "representation learning",
            "contrastive learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "MRT: Learning Compact Representations with Mixed RWKV-Transformer for Extreme Image Compression",
      "authors": [
        "Han Liu",
        "Hengyu Man",
        "Xingtao Wang",
        "Wenrui Li",
        "Debin Zhao"
      ],
      "arxiv_id": "2511.06717v2",
      "summary": "Recent advances in extreme image compression have revealed that mapping pixel data into highly compact latent representations can significantly improve coding efficiency. However, most existing methods compress images into 2-D latent spaces via convolutional neural networks (CNNs) or Swin Transformers, which tend to retain substantial spatial redundancy, thereby limiting overall compression performance. In this paper, we propose a novel Mixed RWKV-Transformer (MRT) architecture that encodes images into more compact 1-D latent representations by synergistically integrating the complementary strengths of linear-attention-based RWKV and self-attention-based Transformer models. Specifically, MRT partitions each image into fixed-size windows, utilizing RWKV modules to capture global dependencies across windows and Transformer blocks to model local redundancies within each window. The hierarchical attention mechanism enables more efficient and compact representation learning in the 1-D domain. To further enhance compression efficiency, we introduce a dedicated RWKV Compression Model (RCM) tailored to the structure characteristics of the intermediate 1-D latent features in MRT. Extensive experiments on standard image compression benchmarks validate the effectiveness of our approach. The proposed MRT framework consistently achieves superior reconstruction quality at bitrates below 0.02 bits per pixel (bpp). Quantitative results based on the DISTS metric show that MRT significantly outperforms the state-of-the-art 2-D architecture GLC, achieving bitrate savings of 43.75%, 30.59% on the Kodak and CLIC2020 test datasets, respectively.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-14",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06717v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "linear attention",
            "representation learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "SPAN: Spatial-Projection Alignment for Monocular 3D Object Detection",
      "authors": [
        "Yifan Wang",
        "Yian Zhao",
        "Fanqi Pu",
        "Xiaochen Yang",
        "Yang Tang",
        "Xi Chen",
        "Wenming Yang"
      ],
      "arxiv_id": "2511.06702v1",
      "summary": "Existing monocular 3D detectors typically tame the pronounced nonlinear regression of 3D bounding box through decoupled prediction paradigm, which employs multiple branches to estimate geometric center, depth, dimensions, and rotation angle separately. Although this decoupling strategy simplifies the learning process, it inherently ignores the geometric collaborative constraints between different attributes, resulting in the lack of geometric consistency prior, thereby leading to suboptimal performance. To address this issue, we propose novel Spatial-Projection Alignment (SPAN) with two pivotal components: (i). Spatial Point Alignment enforces an explicit global spatial constraint between the predicted and ground-truth 3D bounding boxes, thereby rectifying spatial drift caused by decoupled attribute regression. (ii). 3D-2D Projection Alignment ensures that the projected 3D box is aligned tightly within its corresponding 2D detection bounding box on the image plane, mitigating projection misalignment overlooked in previous works. To ensure training stability, we further introduce a Hierarchical Task Learning strategy that progressively incorporates spatial-projection alignment as 3D attribute predictions refine, preventing early stage error propagation across attributes. Extensive experiments demonstrate that the proposed method can be easily integrated into any established monocular 3D detector and delivers significant performance improvements.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06702v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "geometric consistency"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "7_retargeting"
      ]
    },
    {
      "title": "On Accurate and Robust Estimation of 3D and 2D Circular Center: Method and Application to Camera-Lidar Calibration",
      "authors": [
        "Jiajun Jiang",
        "Xiao Hu",
        "Wancheng Liu",
        "Wei Jiang"
      ],
      "arxiv_id": "2511.06611v1",
      "summary": "Circular targets are widely used in LiDAR-camera extrinsic calibration due to their geometric consistency and ease of detection. However, achieving accurate 3D-2D circular center correspondence remains challenging. Existing methods often fail due to decoupled 3D fitting and erroneous 2D ellipse-center estimation. To address this, we propose a geometrically principled framework featuring two innovations: (i) a robust 3D circle center estimator based on conformal geometric algebra and RANSAC; and (ii) a chord-length variance minimization method to recover the true 2D projected center, resolving its dual-minima ambiguity via homography validation or a quasi-RANSAC fallback. Evaluated on synthetic and real-world datasets, our framework significantly outperforms state-of-the-art approaches. It reduces extrinsic estimation error and enables robust calibration across diverse sensors and target types, including natural circular objects. Our code will be publicly released for reproducibility.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06611v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "geometric consistency"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "7_retargeting"
      ]
    },
    {
      "title": "CoMA: Complementary Masking and Hierarchical Dynamic Multi-Window Self-Attention in a Unified Pre-training Framework",
      "authors": [
        "Jiaxuan Li",
        "Qing Xu",
        "Xiangjian He",
        "Ziyu Liu",
        "Chang Xing",
        "Zhen Chen",
        "Daokun Zhang",
        "Rong Qu",
        "Chang Wen Chen"
      ],
      "arxiv_id": "2511.05929v1",
      "summary": "Masked Autoencoders (MAE) achieve self-supervised learning of image representations by randomly removing a portion of visual tokens and reconstructing the original image as a pretext task, thereby significantly enhancing pretraining efficiency and yielding excellent adaptability across downstream tasks. However, MAE and other MAE-style paradigms that adopt random masking generally require more pre-training epochs to maintain adaptability. Meanwhile, ViT in MAE suffers from inefficient parameter use due to fixed spatial resolution across layers. To overcome these limitations, we propose the Complementary Masked Autoencoders (CoMA), which employ a complementary masking strategy to ensure uniform sampling across all pixels, thereby improving effective learning of all features and enhancing the model's adaptability. Furthermore, we introduce DyViT, a hierarchical vision transformer that employs a Dynamic Multi-Window Self-Attention (DM-MSA), significantly reducing the parameters and FLOPs while improving fine-grained feature learning. Pre-trained on ImageNet-1K with CoMA, DyViT matches the downstream performance of MAE using only 12% of the pre-training epochs, demonstrating more effective learning. It also attains a 10% reduction in pre-training time per epoch, further underscoring its superior pre-training efficiency.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-08",
      "updated": "2025-11-08",
      "comment": "9 pages, 5 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05929v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "masked autoencoder",
            "MAE"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Masked IRL: LLM-Guided Reward Disambiguation from Demonstrations and Language",
      "authors": [
        "Minyoung Hwang",
        "Alexandra Forsey-Smerek",
        "Nathaniel Dennler",
        "Andreea Bobu"
      ],
      "arxiv_id": "2511.14565v1",
      "summary": "Robots can adapt to user preferences by learning reward functions from demonstrations, but with limited data, reward models often overfit to spurious correlations and fail to generalize. This happens because demonstrations show robots how to do a task but not what matters for that task, causing the model to focus on irrelevant state details. Natural language can more directly specify what the robot should focus on, and, in principle, disambiguate between many reward functions consistent with the demonstrations. However, existing language-conditioned reward learning methods typically treat instructions as simple conditioning signals, without fully exploiting their potential to resolve ambiguity. Moreover, real instructions are often ambiguous themselves, so naive conditioning is unreliable. Our key insight is that these two input types carry complementary information: demonstrations show how to act, while language specifies what is important. We propose Masked Inverse Reinforcement Learning (Masked IRL), a framework that uses large language models (LLMs) to combine the strengths of both input types. Masked IRL infers state-relevance masks from language instructions and enforces invariance to irrelevant state components. When instructions are ambiguous, it uses LLM reasoning to clarify them in the context of the demonstrations. In simulation and on a real robot, Masked IRL outperforms prior language-conditioned IRL methods by up to 15% while using up to 4.7 times less data, demonstrating improved sample-efficiency, generalization, and robustness to ambiguous language. Project page: https://MIT-CLEAR-Lab.github.io/Masked-IRL and Code: https://github.com/MIT-CLEAR-Lab/Masked-IRL",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14565v1",
      "code_links": [
        {
          "url": "https://github.com/MIT-CLEAR-Lab/Masked-IRL",
          "type": "github"
        },
        {
          "url": "https://MIT-CLEAR-Lab.github.io/Masked-IRL",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "inverse reinforcement learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image",
      "authors": [
        "Ziang Cao",
        "Fangzhou Hong",
        "Zhaoxi Chen",
        "Liang Pan",
        "Ziwei Liu"
      ],
      "arxiv_id": "2511.13648v1",
      "summary": "3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "Project page: https://physx-anything.github.io/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13648v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "policy learning",
            "MuJoCo"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "KANGURA: Kolmogorov-Arnold Network-Based Geometry-Aware Learning with Unified Representation Attention for 3D Modeling of Complex Structures",
      "authors": [
        "Mohammad Reza Shafie",
        "Morteza Hajiabadi",
        "Hamed Khosravi",
        "Mobina Noori",
        "Imtiaz Ahmed"
      ],
      "arxiv_id": "2511.13798v1",
      "summary": "Microbial Fuel Cells (MFCs) offer a promising pathway for sustainable energy generation by converting organic matter into electricity through microbial processes. A key factor influencing MFC performance is the anode structure, where design and material properties play a crucial role. Existing predictive models struggle to capture the complex geometric dependencies necessary to optimize these structures. To solve this problem, we propose KANGURA: Kolmogorov-Arnold Network-Based Geometry-Aware Learning with Unified Representation Attention. KANGURA introduces a new approach to three-dimensional (3D) machine learning modeling. It formulates prediction as a function decomposition problem, where Kolmogorov-Arnold Network (KAN)- based representation learning reconstructs geometric relationships without a conventional multi- layer perceptron (MLP). To refine spatial understanding, geometry-disentangled representation learning separates structural variations into interpretable components, while unified attention mechanisms dynamically enhance critical geometric regions. Experimental results demonstrate that KANGURA outperforms over 15 state-of-the-art (SOTA) models on the ModelNet40 benchmark dataset, achieving 92.7% accuracy, and excels in a real-world MFC anode structure problem with 97% accuracy. This establishes KANGURA as a robust framework for 3D geometric modeling, unlocking new possibilities for optimizing complex structures in advanced manufacturing and quality-driven engineering applications.",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13798v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "predictive model",
            "representation learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Recurrent Autoregressive Diffusion: Global Memory Meets Local Attention",
      "authors": [
        "Taiye Chen",
        "Zihan Ding",
        "Anjian Li",
        "Christina Zhang",
        "Zeqi Xiao",
        "Yisen Wang",
        "Chi Jin"
      ],
      "arxiv_id": "2511.12940v1",
      "summary": "Recent advancements in video generation have demonstrated the potential of using video diffusion models as world models, with autoregressive generation of infinitely long videos through masked conditioning. However, such models, usually with local full attention, lack effective memory compression and retrieval for long-term generation beyond the window size, leading to issues of forgetting and spatiotemporal inconsistencies. To enhance the retention of historical information within a fixed memory budget, we introduce a recurrent neural network (RNN) into the diffusion transformer framework. Specifically, a diffusion model incorporating LSTM with attention achieves comparable performance to state-of-the-art RNN blocks, such as TTT and Mamba2. Moreover, existing diffusion-RNN approaches often suffer from performance degradation due to training-inference gap or the lack of overlap across windows. To address these limitations, we propose a novel Recurrent Autoregressive Diffusion (RAD) framework, which executes frame-wise autoregression for memory update and retrieval, consistently across training and inference time. Experiments on Memory Maze and Minecraft datasets demonstrate the superiority of RAD for long video generation, highlighting the efficiency of LSTM in sequence modeling.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12940v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "world model",
            "Mamba"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration",
      "authors": [
        "Mengyu Yang",
        "Yanming Yang",
        "Chenyi Xu",
        "Chenxi Song",
        "Yufan Zuo",
        "Tong Zhao",
        "Ruibo Li",
        "Chi Zhang"
      ],
      "arxiv_id": "2511.22533v1",
      "summary": "Diffusion models have achieved impressive generative quality across modalities like 2D images, videos, and 3D shapes, but their inference remains computationally expensive due to the iterative denoising process. While recent caching-based methods effectively reuse redundant computations to speed up 2D and video generation, directly applying these techniques to 3D diffusion models can severely disrupt geometric consistency. In 3D synthesis, even minor numerical errors in cached latent features accumulate, causing structural artifacts and topological inconsistencies. To overcome this limitation, we propose Fast3Dcache, a training-free geometry-aware caching framework that accelerates 3D diffusion inference while preserving geometric fidelity. Our method introduces a Predictive Caching Scheduler Constraint (PCSC) to dynamically determine cache quotas according to voxel stabilization patterns and a Spatiotemporal Stability Criterion (SSC) to select stable features for reuse based on velocity magnitude and acceleration criterion. Comprehensive experiments show that Fast3Dcache accelerates inference significantly, achieving up to a 27.12% speed-up and a 54.8% reduction in FLOPs, with minimal degradation in geometric quality as measured by Chamfer Distance (2.48%) and F-Score (1.95%).",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22533v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "geometric consistency"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "7_retargeting"
      ]
    },
    {
      "title": "DiffStyle360: Diffusion-Based 360° Head Stylization via Style Fusion Attention",
      "authors": [
        "Furkan Guzelant",
        "Arda Goktogan",
        "Tarık Kaya",
        "Aysegul Dundar"
      ],
      "arxiv_id": "2511.22411v1",
      "summary": "3D head stylization has emerged as a key technique for reimagining realistic human heads in various artistic forms, enabling expressive character design and creative visual experiences in digital media. Despite the progress in 3D-aware generation, existing 3D head stylization methods often rely on computationally expensive optimization or domain-specific fine-tuning to adapt to new styles. To address these limitations, we propose DiffStyle360, a diffusion-based framework capable of producing multi-view consistent, identity-preserving 3D head stylizations across diverse artistic domains given a single style reference image, without requiring per-style training. Building upon the 3D-aware DiffPortrait360 architecture, our approach introduces two key components: the Style Appearance Module, which disentangles style from content, and the Style Fusion Attention mechanism, which adaptively balances structure preservation and stylization fidelity in the latent space. Furthermore, we employ a 3D GAN-generated multi-view dataset for robust fine-tuning and introduce a temperaturebased key scaling strategy to control stylization intensity during inference. Extensive experiments on FFHQ and RenderMe360 demonstrate that DiffStyle360 achieves superior style quality, outperforming state-of-the-art GAN- and diffusion-based stylization methods across challenging style domains.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22411v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "structure preservation"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "7_retargeting"
      ]
    },
    {
      "title": "Pygmalion Effect in Vision: Image-to-Clay Translation for Reflective Geometry Reconstruction",
      "authors": [
        "Gayoung Lee",
        "Junho Kim",
        "Jin-Hwa Kim",
        "Junmo Kim"
      ],
      "arxiv_id": "2511.21098v1",
      "summary": "Understanding reflection remains a long-standing challenge in 3D reconstruction due to the entanglement of appearance and geometry under view-dependent reflections. In this work, we present the Pygmalion Effect in Vision, a novel framework that metaphorically \"sculpts\" reflective objects into clay-like forms through image-to-clay translation. Inspired by the myth of Pygmalion, our method learns to suppress specular cues while preserving intrinsic geometric consistency, enabling robust reconstruction from multi-view images containing complex reflections. Specifically, we introduce a dual-branch network in which a BRDF-based reflective branch is complemented by a clay-guided branch that stabilizes geometry and refines surface normals. The two branches are trained jointly using the synthesized clay-like images, which provide a neutral, reflection-free supervision signal that complements the reflective views. Experiments on both synthetic and real datasets demonstrate substantial improvement in normal accuracy and mesh completeness over existing reflection-handling methods. Beyond technical gains, our framework reveals that seeing by unshining, translating radiance into neutrality, can serve as a powerful inductive bias for reflective object geometry learning.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21098v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "geometric consistency"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "7_retargeting"
      ]
    },
    {
      "title": "One Patch is All You Need: Joint Surface Material Reconstruction and Classification from Minimal Visual Cues",
      "authors": [
        "Sindhuja Penchala",
        "Gavin Money",
        "Gabriel Marques",
        "Samuel Wood",
        "Jessica Kirschman",
        "Travis Atkison",
        "Shahram Rahimi",
        "Noorbakhsh Amiri Golilarz"
      ],
      "arxiv_id": "2511.20784v1",
      "summary": "Understanding material surfaces from sparse visual cues is critical for applications in robotics, simulation, and material perception. However, most existing methods rely on dense or full-scene observations, limiting their effectiveness in constrained or partial view environment. To address this challenge, we introduce SMARC, a unified model for Surface MAterial Reconstruction and Classification from minimal visual input. By giving only a single 10% contiguous patch of the image, SMARC recognizes and reconstructs the full RGB surface while simultaneously classifying the material category. Our architecture combines a Partial Convolutional U-Net with a classification head, enabling both spatial inpainting and semantic understanding under extreme observation sparsity. We compared SMARC against five models including convolutional autoencoders [17], Vision Transformer (ViT) [13], Masked Autoencoder (MAE) [5], Swin Transformer [9], and DETR [2] using Touch and Go dataset [16] of real-world surface textures. SMARC achieves state-of-the-art results with a PSNR of 17.55 dB and a material classification accuracy of 85.10%. Our findings highlight the advantages of partial convolution in spatial reasoning under missing data and establish a strong foundation for minimal-vision surface understanding.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "9 pages,3 figures, 5 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20784v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "masked autoencoder",
            "MAE"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Reinforcing Action Policies by Prophesying",
      "authors": [
        "Jiahui Zhang",
        "Ze Huang",
        "Chun Gu",
        "Zipei Ma",
        "Li Zhang"
      ],
      "arxiv_id": "2511.20633v1",
      "summary": "Vision-Language-Action (VLA) policies excel in aligning language, perception, and robot control. However, most VLAs are trained purely by imitation, which overfits to demonstrations, and is brittle under distribution shift. Reinforcement learning (RL) directly optimizes task reward and thus addresses this misalignment, but real-robot interaction is expensive and conventional simulators are hard to engineer and transfer. We address both data efficiency and optimization stability in VLA post-training via a learned world model and an RL procedure tailored to flow-based action heads. Specifically, we introduce Prophet, a unified action-to-video robot actuation pretrained across large-scale, heterogeneous robot data to learn reusable action-outcome dynamics. It is able to few-shot adapt to new robots, objects, and environments, yielding a rollout-ready simulator. Upon Prophet, we reinforce action policies with Flow-action-GRPO (FA-GRPO), which adapts Flow-GRPO to operate on VLA actions, and with FlowScale, a stepwise reweighting that rescales per-step gradients in the flow head. Together, Prophet, FA-GRPO, and FlowScale constitute ProphRL, a practical, data- and compute-efficient path to VLA post-training. Experiments show 5-17% success gains on public benchmarks and 24-30% gains on real robots across different VLA variants.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "https://LogosRoboticsGroup.github.io/ProphRL",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20633v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "world model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent Multimodal Learning",
      "authors": [
        "Bo Pang",
        "Chenxi Xu",
        "Jierui Ren",
        "Guoping Wang",
        "Sheng Li"
      ],
      "arxiv_id": "2511.20422v1",
      "summary": "Understanding the physical world requires perceptual models grounded in physical laws rather than mere statistical correlations. However, existing multimodal learning frameworks, focused on vision and language, lack physical consistency and overlook the intrinsic causal relationships among an object's geometry, material, vibration modes, and the sounds it produces. We introduce VibraVerse, a large-scale geometry-acoustics alignment dataset that explicitly bridges the causal chain from 3D geometry -> physical attributes -> modal parameters -> acoustic signals. Each 3D model has explicit physical properties (density, Young's modulus, Poisson's ratio) and volumetric geometry, from which modal eigenfrequencies and eigenvectors are computed for impact sound synthesis under controlled excitations. To establish this coherence, we introduce CLASP, a contrastive learning framework for cross-modal alignment that preserves the causal correspondence between an object's physical structure and its acoustic response. This framework enforces physically consistent alignment across modalities, ensuring that every sample is coherent, traceable to the governing equations, and embedded within a unified representation space spanning shape, image, and sound. Built upon VibraVerse, we define a suite of benchmark tasks for geometry-to-sound prediction, sound-guided shape reconstruction, and cross-modal representation learning. Extensive validations on these tasks demonstrate that models trained on VibraVerse exhibit superior accuracy, interpretability, and generalization across modalities. These results establish VibraVerse as a benchmark for physically consistent and causally interpretable multimodal learning, providing a foundation for sound-guided embodied perception and a deeper understanding of the physical world. The dataset will be open-sourced.",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.GR",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20422v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "representation learning",
            "contrastive learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Video Object Recognition in Mobile Edge Networks: Local Tracking or Edge Detection?",
      "authors": [
        "Kun Guo",
        "Yun Shen",
        "Xijun Wang",
        "Chaoqun You",
        "Yun Rui",
        "Tony Q. S. Quek"
      ],
      "arxiv_id": "2511.20716v1",
      "summary": "Fast and accurate video object recognition, which relies on frame-by-frame video analytics, remains a challenge for resource-constrained devices such as traffic cameras. Recent advances in mobile edge computing have made it possible to offload computation-intensive object detection to edge servers equipped with high-accuracy neural networks, while lightweight and fast object tracking algorithms run locally on devices. This hybrid approach offers a promising solution but introduces a new challenge: deciding when to perform edge detection versus local tracking. To address this, we formulate two long-term optimization problems for both single-device and multi-device scenarios, taking into account the temporal correlation of consecutive frames and the dynamic conditions of mobile edge networks. Based on the formulation, we propose the LTED-Ada in single-device setting, a deep reinforcement learning-based algorithm that adaptively selects between local tracking and edge detection, according to the frame rate as well as recognition accuracy and delay requirement. In multi-device setting, we further enhance LTED-Ada using federated learning to enable collaborative policy training across devices, thereby improving its generalization to unseen frame rates and performance requirements. Finally, we conduct extensive hardware-in-the-loop experiments using multiple Raspberry Pi 4B devices and a personal computer as the edge server, demonstrating the superiority of LTED-Ada.",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20716v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "deep reinforcement learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Unsupervised Multi-View Visual Anomaly Detection via Progressive Homography-Guided Alignment",
      "authors": [
        "Xintao Chen",
        "Xiaohao Xu",
        "Bozhong Zheng",
        "Yun Liu",
        "Yingna Wu"
      ],
      "arxiv_id": "2511.18766v1",
      "summary": "Unsupervised visual anomaly detection from multi-view images presents a significant challenge: distinguishing genuine defects from benign appearance variations caused by viewpoint changes. Existing methods, often designed for single-view inputs, treat multiple views as a disconnected set of images, leading to inconsistent feature representations and a high false-positive rate. To address this, we introduce ViewSense-AD (VSAD), a novel framework that learns viewpoint-invariant representations by explicitly modeling geometric consistency across views. At its core is our Multi-View Alignment Module (MVAM), which leverages homography to project and align corresponding feature regions between neighboring views. We integrate MVAM into a View-Align Latent Diffusion Model (VALDM), enabling progressive and multi-stage alignment during the denoising process. This allows the model to build a coherent and holistic understanding of the object's surface from coarse to fine scales. Furthermore, a lightweight Fusion Refiner Module (FRM) enhances the global consistency of the aligned features, suppressing noise and improving discriminative power. Anomaly detection is performed by comparing multi-level features from the diffusion model against a learned memory bank of normal prototypes. Extensive experiments on the challenging RealIAD and MANTA datasets demonstrate that VSAD sets a new state-of-the-art, significantly outperforming existing methods in pixel, view, and sample-level visual anomaly proving its robustness to large viewpoint shifts and complex textures.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18766v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "geometric consistency"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "7_retargeting"
      ]
    },
    {
      "title": "UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions",
      "authors": [
        "Guozhen Zhang",
        "Zixiang Zhou",
        "Teng Hu",
        "Ziqiao Peng",
        "Youliang Zhang",
        "Yi Chen",
        "Yuan Zhou",
        "Qinglin Lu",
        "Limin Wang"
      ],
      "arxiv_id": "2511.03334v1",
      "summary": "Due to the lack of effective cross-modal modeling, existing open-source audio-video generation methods often exhibit compromised lip synchronization and insufficient semantic consistency. To mitigate these drawbacks, we propose UniAVGen, a unified framework for joint audio and video generation. UniAVGen is anchored in a dual-branch joint synthesis architecture, incorporating two parallel Diffusion Transformers (DiTs) to build a cohesive cross-modal latent space. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which enables bidirectional, temporally aligned cross-attention, thus ensuring precise spatiotemporal synchronization and semantic consistency. Furthermore, this cross-modal interaction is augmented by a Face-Aware Modulation module, which dynamically prioritizes salient regions in the interaction process. To enhance generative fidelity during inference, we additionally introduce Modality-Aware Classifier-Free Guidance, a novel strategy that explicitly amplifies cross-modal correlation signals. Notably, UniAVGen's robust joint synthesis design enables seamless unification of pivotal audio-video tasks within a single model, such as joint audio-video generation and continuation, video-to-audio dubbing, and audio-driven video synthesis. Comprehensive experiments validate that, with far fewer training samples (1.3M vs. 30.1M), UniAVGen delivers overall advantages in audio-video synchronization, timbre consistency, and emotion consistency.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-05",
      "updated": "2025-11-05",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03334v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "classifier-free guidance"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 2.5,
      "hit_pillars": [
        "4_motion_diffusion"
      ]
    },
    {
      "title": "Dynamic Avatar-Scene Rendering from Human-centric Context",
      "authors": [
        "Wenqing Wang",
        "Haosen Yang",
        "Josef Kittler",
        "Xiatian Zhu"
      ],
      "arxiv_id": "2511.10539v1",
      "summary": "Reconstructing dynamic humans interacting with real-world environments from monocular videos is an important and challenging task. Despite considerable progress in 4D neural rendering, existing approaches either model dynamic scenes holistically or model scenes and backgrounds separately aim to introduce parametric human priors. However, these approaches either neglect distinct motion characteristics of various components in scene especially human, leading to incomplete reconstructions, or ignore the information exchange between the separately modeled components, resulting in spatial inconsistencies and visual artifacts at human-scene boundaries. To address this, we propose {\\bf Separate-then-Map} (StM) strategy that introduces a dedicated information mapping mechanism to bridge separately defined and optimized models. Our method employs a shared transformation function for each Gaussian attribute to unify separately modeled components, enhancing computational efficiency by avoiding exhaustive pairwise interactions while ensuring spatial and visual coherence between humans and their surroundings. Extensive experiments on monocular video datasets demonstrate that StM significantly outperforms existing state-of-the-art methods in both visual quality and rendering accuracy, particularly at challenging human-scene interaction boundaries.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "13 pages, 8 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10539v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "human-scene interaction"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 2.5,
      "hit_pillars": [
        "5_interaction_reaction"
      ]
    },
    {
      "title": "VISTA: A Vision and Intent-Aware Social Attention Framework for Multi-Agent Trajectory Prediction",
      "authors": [
        "Stephane Da Silva Martins",
        "Emanuel Aldea",
        "Sylvie Le Hégarat-Mascle"
      ],
      "arxiv_id": "2511.10203v1",
      "summary": "Multi-agent trajectory prediction is crucial for autonomous systems operating in dense, interactive environments. Existing methods often fail to jointly capture agents' long-term goals and their fine-grained social interactions, which leads to unrealistic multi-agent futures. We propose VISTA, a recursive goal-conditioned transformer for multi-agent trajectory forecasting. VISTA combines (i) a cross-attention fusion module that integrates long-horizon intent with past motion, (ii) a social-token attention mechanism for flexible interaction modeling across agents, and (iii) pairwise attention maps that make social influence patterns interpretable at inference time. Our model turns single-agent goal-conditioned prediction into a coherent multi-agent forecasting framework. Beyond standard displacement metrics, we evaluate trajectory collision rates as a measure of joint realism. On the high-density MADRAS benchmark and on SDD, VISTA achieves state-of-the-art accuracy and substantially fewer collisions. On MADRAS, it reduces the average collision rate of strong baselines from 2.14 to 0.03 percent, and on SDD it attains zero collisions while improving ADE, FDE, and minFDE. These results show that VISTA generates socially compliant, goal-aware, and interpretable trajectories, making it promising for safety-critical autonomous systems.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "Paper accepted at WACV 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10203v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "social interaction"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 2.5,
      "hit_pillars": [
        "5_interaction_reaction"
      ]
    },
    {
      "title": "Diffusion As Self-Distillation: End-to-End Latent Diffusion In One Model",
      "authors": [
        "Xiyuan Wang",
        "Muhan Zhang"
      ],
      "arxiv_id": "2511.14716v1",
      "summary": "Standard Latent Diffusion Models rely on a complex, three-part architecture consisting of a separate encoder, decoder, and diffusion network, which are trained in multiple stages. This modular design is computationally inefficient, leads to suboptimal performance, and prevents the unification of diffusion with the single-network architectures common in vision foundation models. Our goal is to unify these three components into a single, end-to-end trainable network. We first demonstrate that a naive joint training approach fails catastrophically due to ``latent collapse'', where the diffusion training objective interferes with the network's ability to learn a good latent representation. We identify the root causes of this instability by drawing a novel analogy between diffusion and self-distillation based unsupervised learning method. Based on this insight, we propose Diffusion as Self-Distillation (DSD), a new framework with key modifications to the training objective that stabilize the latent space. This approach enables, for the first time, the stable end-to-end training of a single network that simultaneously learns to encode, decode, and perform diffusion. DSD achieves outstanding performance on the ImageNet $256\\times 256$ conditional generation task: FID=13.44/6.38/4.25 with only 42M/118M/205M parameters and 50 training epochs on ImageNet, without using classifier-free-guidance.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "Tech Report. 10 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14716v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "classifier-free guidance"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 2.5,
      "hit_pillars": [
        "4_motion_diffusion"
      ]
    },
    {
      "title": "HiGFA: Hierarchical Guidance for Fine-grained Data Augmentation with Diffusion Models",
      "authors": [
        "Zhiguang Lu",
        "Qianqian Xu",
        "Peisong Wen",
        "Siran Dai",
        "Qingming Huang"
      ],
      "arxiv_id": "2511.12547v4",
      "summary": "Generative diffusion models show promise for data augmentation. However, applying them to fine-grained tasks presents a significant challenge: ensuring synthetic images accurately capture the subtle, category-defining features critical for high fidelity. Standard approaches, such as text-based Classifier-Free Guidance (CFG), often lack the required specificity, potentially generating misleading examples that degrade fine-grained classifier performance. To address this, we propose Hierarchically Guided Fine-grained Augmentation (HiGFA). HiGFA leverages the temporal dynamics of the diffusion sampling process. It employs strong text and transformed contour guidance with fixed strengths in the early-to-mid sampling stages to establish overall scene, style, and structure. In the final sampling stages, HiGFA activates a specialized fine-grained classifier guidance and dynamically modulates the strength of all guidance signals based on prediction confidence. This hierarchical, confidence-driven orchestration enables HiGFA to generate diverse yet faithful synthetic images by intelligently balancing global structure formation with precise detail refinement. Experiments on several FGVC datasets demonstrate the effectiveness of HiGFA.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-16",
      "updated": "2025-12-01",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12547v4",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "classifier-free guidance"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 2.5,
      "hit_pillars": [
        "4_motion_diffusion"
      ]
    },
    {
      "title": "AI killed the video star. Audio-driven diffusion model for expressive talking head generation",
      "authors": [
        "Baptiste Chopin",
        "Tashvik Dhamija",
        "Pranav Balaji",
        "Yaohui Wang",
        "Antitza Dantcheva"
      ],
      "arxiv_id": "2511.22488v1",
      "summary": "We propose Dimitra++, a novel framework for audio-driven talking head generation, streamlined to learn lip motion, facial expression, as well as head pose motion. Specifically, we propose a conditional Motion Diffusion Transformer (cMDT) to model facial motion sequences, employing a 3D representation. The cMDT is conditioned on two inputs: a reference facial image, which determines appearance, as well as an audio sequence, which drives the motion. Quantitative and qualitative experiments, as well as a user study on two widely employed datasets, i.e., VoxCeleb2 and CelebV-HQ, suggest that Dimitra++ is able to outperform existing approaches in generating realistic talking heads imparting lip motion, facial expression, and head pose.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "arXiv admin note: text overlap with arXiv:2502.17198",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22488v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "motion diffusion"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 2.5,
      "hit_pillars": [
        "4_motion_diffusion"
      ]
    },
    {
      "title": "Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy",
      "authors": [
        "Teng Hu",
        "Zhentao Yu",
        "Guozhen Zhang",
        "Zihan Su",
        "Zhengguang Zhou",
        "Youliang Zhang",
        "Yuan Zhou",
        "Qinglin Lu",
        "Ran Yi"
      ],
      "arxiv_id": "2511.21579v2",
      "summary": "The synthesis of synchronized audio-visual content is a key challenge in generative AI, with open-source models facing challenges in robust audio-video alignment. Our analysis reveals that this issue is rooted in three fundamental challenges of the joint diffusion process: (1) Correspondence Drift, where concurrently evolving noisy latents impede stable learning of alignment; (2) inefficient global attention mechanisms that fail to capture fine-grained temporal cues; and (3) the intra-modal bias of conventional Classifier-Free Guidance (CFG), which enhances conditionality but not cross-modal synchronization. To overcome these challenges, we introduce Harmony, a novel framework that mechanistically enforces audio-visual synchronization. We first propose a Cross-Task Synergy training paradigm to mitigate drift by leveraging strong supervisory signals from audio-driven video and video-driven audio generation tasks. Then, we design a Global-Local Decoupled Interaction Module for efficient and precise temporal-style alignment. Finally, we present a novel Synchronization-Enhanced CFG (SyncCFG) that explicitly isolates and amplifies the alignment signal during inference. Extensive experiments demonstrate that Harmony establishes a new state-of-the-art, significantly outperforming existing methods in both generation fidelity and, critically, in achieving fine-grained audio-visual synchronization.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-28",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21579v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "classifier-free guidance"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 2.5,
      "hit_pillars": [
        "4_motion_diffusion"
      ]
    },
    {
      "title": "From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings",
      "authors": [
        "Jiajie Zhang",
        "Sören Schwertfeger",
        "Alexander Kleiner"
      ],
      "arxiv_id": "2511.21428v1",
      "summary": "We present a novel unsupervised framework to unlock vast unlabeled human demonstration data from continuous industrial video streams for Vision-Language-Action (VLA) model pre-training. Our method first trains a lightweight motion tokenizer to encode motion dynamics, then employs an unsupervised action segmenter leveraging a novel \"Latent Action Energy\" metric to discover and segment semantically coherent action primitives. The pipeline outputs both segmented video clips and their corresponding latent action sequences, providing structured data directly suitable for VLA pre-training. Evaluations on public benchmarks and a proprietary electric motor assembly dataset demonstrate effective segmentation of key tasks performed by humans at workstations. Further clustering and quantitative assessment via a Vision-Language Model confirm the semantic coherence of the discovered action primitives. To our knowledge, this is the first fully automated end-to-end system for extracting and organizing VLA pre-training data from unstructured industrial videos, offering a scalable solution for embodied AI integration in manufacturing.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "10 pages, 5 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21428v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "motion token"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 2.5,
      "hit_pillars": [
        "4_motion_diffusion"
      ]
    },
    {
      "title": "MUSE: Manipulating Unified Framework for Synthesizing Emotions in Images via Test-Time Optimization",
      "authors": [
        "Yingjie Xia",
        "Xi Wang",
        "Jinglei Shi",
        "Vicky Kalogeiton",
        "Jian Yang"
      ],
      "arxiv_id": "2511.21051v1",
      "summary": "Images evoke emotions that profoundly influence perception, often prioritized over content. Current Image Emotional Synthesis (IES) approaches artificially separate generation and editing tasks, creating inefficiencies and limiting applications where these tasks naturally intertwine, such as therapeutic interventions or storytelling. In this work, we introduce MUSE, the first unified framework capable of both emotional generation and editing. By adopting a strategy conceptually aligned with Test-Time Scaling (TTS) that widely used in both LLM and diffusion model communities, it avoids the requirement for additional updating diffusion model and specialized emotional synthesis datasets. More specifically, MUSE addresses three key questions in emotional synthesis: (1) HOW to stably guide synthesis by leveraging an off-the-shelf emotion classifier with gradient-based optimization of emotional tokens; (2) WHEN to introduce emotional guidance by identifying the optimal timing using semantic similarity as a supervisory signal; and (3) WHICH emotion to guide synthesis through a multi-emotion loss that reduces interference from inherent and similar emotions. Experimental results show that MUSE performs favorably against all methods for both generation and editing, improving emotional accuracy and semantic diversity while maintaining an optimal balance between desired content, adherence to text prompts, and realistic emotional expression. It establishes a new paradigm for emotion synthesis.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21051v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "motion synthesis"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 2.5,
      "hit_pillars": [
        "4_motion_diffusion"
      ]
    },
    {
      "title": "SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis",
      "authors": [
        "Lingwei Dang",
        "Zonghan Li",
        "Juntong Li",
        "Hongwen Zhang",
        "Liang An",
        "Yebin Liu",
        "Qingyao Wu"
      ],
      "arxiv_id": "2511.19319v1",
      "summary": "Hand-Object Interaction (HOI) generation plays a critical role in advancing applications across animation and robotics. Current video-based methods are predominantly single-view, which impedes comprehensive 3D geometry perception and often results in geometric distortions or unrealistic motion patterns. While 3D HOI approaches can generate dynamically plausible motions, their dependence on high-quality 3D data captured in controlled laboratory settings severely limits their generalization to real-world scenarios. To overcome these limitations, we introduce SyncMV4D, the first model that jointly generates synchronized multi-view HOI videos and 4D motions by unifying visual prior, motion dynamics, and multi-view geometry. Our framework features two core innovations: (1) a Multi-view Joint Diffusion (MJD) model that co-generates HOI videos and intermediate motions, and (2) a Diffusion Points Aligner (DPA) that refines the coarse intermediate motion into globally aligned 4D metric point tracks. To tightly couple 2D appearance with 4D dynamics, we establish a closed-loop, mutually enhancing cycle. During the diffusion denoising process, the generated video conditions the refinement of the 4D motion, while the aligned 4D point tracks are reprojected to guide next-step joint generation. Experimentally, our method demonstrates superior performance to state-of-the-art alternatives in visual realism, motion plausibility, and multi-view consistency.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "Project Page: https://droliven.github.io/SyncMV4D",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19319v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "HOI"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 2.5,
      "hit_pillars": [
        "5_interaction_reaction"
      ]
    },
    {
      "title": "Peregrine: One-Shot Fine-Tuning for FHE Inference of General Deep CNNs",
      "authors": [
        "Huaming Ling",
        "Ying Wang",
        "Si Chen",
        "Junfeng Fan"
      ],
      "arxiv_id": "2511.18976v1",
      "summary": "We address two fundamental challenges in adapting general deep CNNs for FHE-based inference: approximating non-linear activations such as ReLU with low-degree polynomials while minimizing accuracy degradation, and overcoming the ciphertext capacity barrier that constrains high-resolution image processing on FHE inference. Our contributions are twofold: (1) a single-stage fine-tuning (SFT) strategy that directly converts pre-trained CNNs into FHE-friendly forms using low-degree polynomials, achieving competitive accuracy with minimal training overhead; and (2) a generalized interleaved packing (GIP) scheme that is compatible with feature maps of virtually arbitrary spatial resolutions, accompanied by a suite of carefully designed homomorphic operators that preserve the GIP-form encryption throughout computation. These advances enable efficient, end-to-end FHE inference across diverse CNN architectures. Experiments on CIFAR-10, ImageNet, and MS COCO demonstrate that the FHE-friendly CNNs obtained via our SFT strategy achieve accuracy comparable to baselines using ReLU or SiLU activations. Moreover, this work presents the first demonstration of FHE-based inference for YOLO architectures in object detection leveraging low-degree polynomial activations.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18976v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "OMOMO"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 2.5,
      "hit_pillars": [
        "5_interaction_reaction"
      ]
    },
    {
      "title": "Rethinking Garment Conditioning in Diffusion-based Virtual Try-On",
      "authors": [
        "Kihyun Na",
        "Jinyoung Choi",
        "Injung Kim"
      ],
      "arxiv_id": "2511.18775v1",
      "summary": "Virtual Try-On (VTON) is the task of synthesizing an image of a person wearing a target garment, conditioned on a person image and a garment image. While diffusion-based VTON models featuring a Dual UNet architecture demonstrate superior fidelity compared to single UNet models, they incur substantial computational and memory overhead due to their heavy structure. In this study, through visualization analysis and theoretical analysis, we derived three hypotheses regarding the learning of context features to condition the denoising process. Based on these hypotheses, we developed Re-CatVTON, an efficient single UNet model that achieves high performance. We further enhance the model by introducing a modified classifier-free guidance strategy tailored for VTON's spatial concatenation conditioning, and by directly injecting the ground-truth garment latent derived from the clean garment latent to prevent the accumulation of prediction error. The proposed Re-CatVTON significantly improves performance compared to its predecessor (CatVTON) and requires less computation and memory than the high-performance Dual UNet model, Leffa. Our results demonstrate improved FID, KID, and LPIPS scores, with only a marginal decrease in SSIM, establishing a new efficiency-performance trade-off for single UNet VTON models.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "15 pages (including references and supplementary material), 10 figures, 7 tables. Code and pretrained models will be released",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18775v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "classifier-free guidance"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 2.5,
      "hit_pillars": [
        "4_motion_diffusion"
      ]
    },
    {
      "title": "SocialFusion: Addressing Social Degradation in Pre-trained Vision-Language Models",
      "authors": [
        "Hamza Tahboub",
        "Weiyan Shi",
        "Gang Hua",
        "Huaizu Jiang"
      ],
      "arxiv_id": "2512.01148v1",
      "summary": "Understanding social interactions from visual cues is a fundamental challenge for a socially competent AI. While powerful pre-trained vision-language models (VLMs) have shown remarkable general capabilities, they surprisingly struggle to unify and learn multiple social perception tasks simultaneously, often exhibiting negative transfer. We identify that this negative transfer stems from a critical issue we term \"social degradation,\" whereby the general visual-linguistic pre-training process of VLMs impairs the visual encoder's ability to represent nuanced social information. We investigate this behavior further under two lenses: decodability through linear representation probing and compatibility through gradient conflict analysis, revealing that both play a role in the degradation, especially the former, which is significantly compromised in the VLM pre-training process. To address these issues, we propose SocialFusion, a unified framework that learns a minimal connection between a frozen visual encoder and a language model. Compared with existing VLMs, it exhibits positive transfer across all five social tasks, leveraging synergies between them to enhance overall performance and achieves comparable performance to task-specific state-of-the-art models on various benchmarks. Our findings suggest that current VLM pre-training strategies may be detrimental to acquiring general social competence and highlight the need for more socially-aware training paradigms.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "22 pages, 10 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01148v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "social interaction"
          ],
          "score": 2.5
        }
      ],
      "relevance_score": 2.5,
      "hit_pillars": [
        "5_interaction_reaction"
      ]
    },
    {
      "title": "EETnet: a CNN for Gaze Detection and Tracking for Smart-Eyewear",
      "authors": [
        "Andrea Aspesi",
        "Andrea Simpsi",
        "Aaron Tognoli",
        "Simone Mentasti",
        "Luca Merigo",
        "Matteo Matteucci"
      ],
      "arxiv_id": "2511.04779v1",
      "summary": "Event-based cameras are becoming a popular solution for efficient, low-power eye tracking. Due to the sparse and asynchronous nature of event data, they require less processing power and offer latencies in the microsecond range. However, many existing solutions are limited to validation on powerful GPUs, with no deployment on real embedded devices. In this paper, we present EETnet, a convolutional neural network designed for eye tracking using purely event-based data, capable of running on microcontrollers with limited resources. Additionally, we outline a methodology to train, evaluate, and quantize the network using a public dataset. Finally, we propose two versions of the architecture: a classification model that detects the pupil on a grid superimposed on the original image, and a regression model that operates at the pixel level.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "International Joint Conference on Neural Networks (IJCNN), 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04779v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "running"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "DMSORT: An efficient parallel maritime multi-object tracking architecture for unmanned vessel platforms",
      "authors": [
        "Shengyu Tang",
        "Zeyuan Lu",
        "Jiazhi Dong",
        "Changdong Yu",
        "Xiaoyu Wang",
        "Yaohui Lyu",
        "Weihao Xia"
      ],
      "arxiv_id": "2511.04128v2",
      "summary": "Accurate perception of the marine environment through robust multi-object tracking (MOT) is essential for ensuring safe vessel navigation and effective maritime surveillance. However, the complicated maritime environment often causes camera motion and subsequent visual degradation, posing significant challenges to MOT. To address this challenge, we propose an efficient Dual-branch Maritime SORT (DMSORT) method for maritime MOT. The core of the framework is a parallel tracker with affine compensation, which incorporates an object detection and re-identification (ReID) branch, along with a dedicated branch for dynamic camera motion estimation. Specifically, a Reversible Columnar Detection Network (RCDN) is integrated into the detection module to leverage multi-level visual features for robust object detection. Furthermore, a lightweight Transformer-based appearance extractor (Li-TAE) is designed to capture global contextual information and generate robust appearance features. Another branch decouples platform-induced and target-intrinsic motion by constructing a projective transformation, applying platform-motion compensation within the Kalman filter, and thereby stabilizing true object trajectories. Finally, a clustering-optimized feature fusion module effectively combines motion and appearance cues to ensure identity consistency under noise, occlusion, and drift. Extensive evaluations on the Singapore Maritime Dataset demonstrate that DMSORT achieves state-of-the-art performance. Notably, DMSORT attains the fastest runtime among existing ReID-based MOT frameworks while maintaining high identity consistency and robustness to jitter and occlusion. Code is available at: https://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-06",
      "updated": "2025-11-16",
      "comment": "This version clarifies several citation formatting inconsistencies caused by a technical issue in the reference management software used during manuscript preparation. All scientific data, experiments, and conclusions remain fully valid and unaffected. The clarification is provided to maintain transparency and consistency in the scholarly record",
      "doi": "10.1016/j.oceaneng.2025.123045",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04128v2",
      "code_links": [
        {
          "url": "https://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Faithful Contouring: Near-Lossless 3D Voxel Representation Free from Iso-surface",
      "authors": [
        "Yihao Luo",
        "Xianglong He",
        "Chuanyu Pan",
        "Yiwen Chen",
        "Jiaqi Wu",
        "Yangguang Li",
        "Wanli Ouyang",
        "Yuanming Hu",
        "Guang Yang",
        "ChoonHwai Yap"
      ],
      "arxiv_id": "2511.04029v3",
      "summary": "Accurate and efficient voxelized representations of 3D meshes are the foundation of 3D reconstruction and generation. However, existing representations based on iso-surface heavily rely on water-tightening or rendering optimization, which inevitably compromise geometric fidelity. We propose Faithful Contouring, a sparse voxelized representation that supports 2048+ resolutions for arbitrary meshes, requiring neither converting meshes to field functions nor extracting the isosurface during remeshing. It achieves near-lossless fidelity by preserving sharpness and internal structures, even for challenging cases with complex geometry and topology. The proposed method also shows flexibility for texturing, manipulation, and editing. Beyond representation, we design a dual-mode autoencoder for Faithful Contouring, enabling scalable and detail-preserving shape reconstruction. Extensive experiments show that Faithful Contouring surpasses existing methods in accuracy and efficiency for both representation and reconstruction. For direct representation, it achieves distance errors at the $10^{-5}$ level; for mesh reconstruction, it yields a 93\\% reduction in Chamfer Distance and a 35\\% improvement in F-score over strong baselines, confirming superior fidelity as a representation for 3D learning tasks.",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-06",
      "updated": "2025-11-12",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04029v3",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images",
      "authors": [
        "Sam Bahrami",
        "Dylan Campbell"
      ],
      "arxiv_id": "2511.03970v1",
      "summary": "Modern scene reconstruction methods are able to accurately recover 3D surfaces that are visible in one or more images. However, this leads to incomplete reconstructions, missing all occluded surfaces. While much progress has been made on reconstructing entire objects given partial observations using generative models, the structural elements of a scene, like the walls, floors and ceilings, have received less attention. We argue that these scene elements should be relatively easy to predict, since they are typically planar, repetitive and simple, and so less costly approaches may be suitable. In this work, we present a synthetic dataset -- Room Envelopes -- that facilitates progress on this task by providing a set of RGB images and two associated pointmaps for each image: one capturing the visible surface and one capturing the first surface once fittings and fixtures are removed, that is, the structural layout. As we show, this enables direct supervision for feed-forward monocular geometry estimators that predict both the first visible surface and the first layout surface. This confers an understanding of the scene's extent, as well as the shape and location of its objects.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03970v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene reconstruction"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization",
      "authors": [
        "Zhejia Cai",
        "Puhua Jiang",
        "Shiwei Mao",
        "Hongkun Cao",
        "Ruqi Huang"
      ],
      "arxiv_id": "2511.03950v1",
      "summary": "Reconstructing real-world objects from multi-view images is essential for applications in 3D editing, AR/VR, and digital content creation. Existing methods typically prioritize either geometric accuracy (Multi-View Stereo) or photorealistic rendering (Novel View Synthesis), often decoupling geometry and appearance optimization, which hinders downstream editing tasks. This paper advocates an unified treatment on geometry and appearance optimization for seamless Gaussian-mesh joint optimization. More specifically, we propose a novel framework that simultaneously optimizes mesh geometry (vertex positions and faces) and vertex colors via Gaussian-guided mesh differentiable rendering, leveraging photometric consistency from input images and geometric regularization from normal and depth maps. The obtained high-quality 3D reconstruction can be further exploit in down-stream editing tasks, such as relighting and shape deformation. The code will be publicly available upon acceptance.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "10 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03950v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "novel view synthesis"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Dynamic Shape Control of Soft Robots Enabled by Data-Driven Model Reduction",
      "authors": [
        "Iman Adibnazari",
        "Harsh Sharma",
        "Myungsun Park",
        "Jacobo Cervera-Torralba",
        "Boris Kramer",
        "Michael T. Tolley"
      ],
      "arxiv_id": "2511.03931v1",
      "summary": "Soft robots have shown immense promise in settings where they can leverage dynamic control of their entire bodies. However, effective dynamic shape control requires a controller that accounts for the robot's high-dimensional dynamics--a challenge exacerbated by a lack of general-purpose tools for modeling soft robots amenably for control. In this work, we conduct a comparative study of data-driven model reduction techniques for generating linear models amendable to dynamic shape control. We focus on three methods--the eigensystem realization algorithm, dynamic mode decomposition with control, and the Lagrangian operator inference (LOpInf) method. Using each class of model, we explored their efficacy in model predictive control policies for the dynamic shape control of a simulated eel-inspired soft robot in three experiments: 1) tracking simulated reference trajectories guaranteed to be feasible, 2) tracking reference trajectories generated from a biological model of eel kinematics, and 3) tracking reference trajectories generated by a reduced-scale physical analog. In all experiments, the LOpInf-based policies generated lower tracking errors than policies based on other models.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "comment": "20 Pages, 8 Figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03931v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "model predictive control"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Computed Tomography (CT)-derived Cardiovascular Flow Estimation Using Physics-Informed Neural Networks Improves with Sinogram-based Training: A Simulation Study",
      "authors": [
        "Jinyuxuan Guo",
        "Gurnoor Singh Khurana",
        "Alejandro Gonzalo Grande",
        "Juan C. del Alamo",
        "Francisco Contijoch"
      ],
      "arxiv_id": "2511.03876v1",
      "summary": "Background: Non-invasive imaging-based assessment of blood flow plays a critical role in evaluating heart function and structure. Computed Tomography (CT) is a widely-used imaging modality that can robustly evaluate cardiovascular anatomy and function, but direct methods to estimate blood flow velocity from movies of contrast evolution have not been developed.\n  Purpose: This study evaluates the impact of CT imaging on Physics-Informed Neural Networks (PINN)-based flow estimation and proposes an improved framework, SinoFlow, which uses sinogram data directly to estimate blood flow.\n  Methods: We generated pulsatile flow fields in an idealized 2D vessel bifurcation using computational fluid dynamics and simulated CT scans with varying gantry rotation speeds, tube currents, and pulse mode imaging settings. We compared the performance of PINN-based flow estimation using reconstructed images (ImageFlow) to SinoFlow.\n  Results: SinoFlow significantly improved flow estimation performance by avoiding propagating errors introduced by filtered backprojection. SinoFlow was robust across all tested gantry rotation speeds and consistently produced lower mean squared error and velocity errors than ImageFlow. Additionally, SinoFlow was compatible with pulsed-mode imaging and maintained higher accuracy with shorter pulse widths.\n  Conclusions: This study demonstrates the potential of SinoFlow for CT-based flow estimation, providing a more promising approach for non-invasive blood flow assessment. The findings aim to inform future applications of PINNs to CT images and provide a solution for image-based estimation, with reasonable acquisition parameters yielding accurate flow estimates.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG",
        "physics.med-ph"
      ],
      "primary_category": "eess.IV",
      "published": "2025-11-05",
      "updated": "2025-11-05",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03876v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "PULSE"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "8_physics_animation"
      ]
    },
    {
      "title": "Beyond Softmax: Dual-Branch Sigmoid Architecture for Accurate Class Activation Maps",
      "authors": [
        "Yoojin Oh",
        "Junhyug Noh"
      ],
      "arxiv_id": "2511.05590v1",
      "summary": "Class Activation Mapping (CAM) and its extensions have become indispensable tools for visualizing the evidence behind deep network predictions. However, by relying on a final softmax classifier, these methods suffer from two fundamental distortions: additive logit shifts that arbitrarily bias importance scores, and sign collapse that conflates excitatory and inhibitory features. We propose a simple, architecture-agnostic dual-branch sigmoid head that decouples localization from classification. Given any pretrained model, we clone its classification head into a parallel branch ending in per-class sigmoid outputs, freeze the original softmax head, and fine-tune only the sigmoid branch with class-balanced binary supervision. At inference, softmax retains recognition accuracy, while class evidence maps are generated from the sigmoid branch -- preserving both magnitude and sign of feature contributions. Our method integrates seamlessly with most CAM variants and incurs negligible overhead. Extensive evaluations on fine-grained tasks (CUB-200-2011, Stanford Cars) and WSOL benchmarks (ImageNet-1K, OpenImages30K) show improved explanation fidelity and consistent Top-1 Localization gains -- without any drop in classification accuracy. Code is available at https://github.com/finallyupper/beyond-softmax.",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-05",
      "updated": "2025-11-05",
      "comment": "Accepted at BMVC 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05590v1",
      "code_links": [
        {
          "url": "https://github.com/finallyupper/beyond-softmax",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Scheduling the Off-Diagonal Weingarten Loss of Neural SDFs for CAD Models",
      "authors": [
        "Haotian Yin",
        "Przemyslaw Musialski"
      ],
      "arxiv_id": "2511.03147v1",
      "summary": "Neural signed distance functions (SDFs) have become a powerful representation for geometric reconstruction from point clouds, yet they often require both gradient- and curvature-based regularization to suppress spurious warp and preserve structural fidelity. FlatCAD introduced the Off-Diagonal Weingarten (ODW) loss as an efficient second-order prior for CAD surfaces, approximating full-Hessian regularization at roughly half the computational cost. However, FlatCAD applies a fixed ODW weight throughout training, which is suboptimal: strong regularization stabilizes early optimization but suppresses detail recovery in later stages. We present scheduling strategies for the ODW loss that assign a high initial weight to stabilize optimization and progressively decay it to permit fine-scale refinement. We investigate constant, linear, quintic, and step interpolation schedules, as well as an increasing warm-up variant. Experiments on the ABC CAD dataset demonstrate that time-varying schedules consistently outperform fixed weights. Our method achieves up to a 35% improvement in Chamfer Distance over the FlatCAD baseline, establishing scheduling as a simple yet effective extension of curvature regularization for robust CAD reconstruction.",
      "categories": [
        "cs.GR",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.GR",
      "published": "2025-11-05",
      "updated": "2025-11-05",
      "comment": "Lecture Notes in Computer Science (LNCS), 20th International Symposium on Visual Computing 2025, 12 pages, 4 figures, preprint",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03147v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "3D Cal: An Open-Source Software Library for Calibrating Tactile Sensors",
      "authors": [
        "Rohan Kota",
        "Kaival Shah",
        "J. Edward Colgate",
        "Gregory Reardon"
      ],
      "arxiv_id": "2511.03078v2",
      "summary": "Tactile sensing plays a key role in enabling dexterous and reliable robotic manipulation, but realizing this capability requires substantial calibration to convert raw sensor readings into physically meaningful quantities. Despite its near-universal necessity, the calibration process remains ad hoc and labor-intensive. Here, we introduce 3D Cal, an open-source library that transforms a low-cost 3D printer into an automated probing device capable of generating large volumes of labeled training data for tactile sensor calibration. We demonstrate the utility of 3D Cal by calibrating two commercially available vision-based tactile sensors, DIGIT and GelSight Mini, to reconstruct high-quality depth maps using the collected data and a custom convolutional neural network. In addition, we perform a data ablation study to determine how much data is needed for accurate calibration, providing practical guidelines for researchers working with these specific sensors, and we benchmark the trained models on previously unseen objects to evaluate calibration accuracy and generalization performance. By automating tactile sensor calibration, 3D Cal can accelerate tactile sensing research, simplify sensor deployment, and promote the practical integration of tactile sensing in robotic platforms.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-04",
      "updated": "2025-11-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03078v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction & Editing",
      "authors": [
        "Antonio Oroz",
        "Matthias Nießner",
        "Tobias Kirschstein"
      ],
      "arxiv_id": "2511.02777v1",
      "summary": "We present PercHead, a method for single-image 3D head reconstruction and semantic 3D editing - two tasks that are inherently challenging due to severe view occlusions, weak perceptual supervision, and the ambiguity of editing in 3D space. We develop a unified base model for reconstructing view-consistent 3D heads from a single input image. The model employs a dual-branch encoder followed by a ViT-based decoder that lifts 2D features into 3D space through iterative cross-attention. Rendering is performed using Gaussian Splatting. At the heart of our approach is a novel perceptual supervision strategy based on DINOv2 and SAM2.1, which provides rich, generalized signals for both geometric and appearance fidelity. Our model achieves state-of-the-art performance in novel-view synthesis and, furthermore, exhibits exceptional robustness to extreme viewing angles compared to established baselines. Furthermore, this base model can be seamlessly extended for semantic 3D editing by swapping the encoder and finetuning the network. In this variant, we disentangle geometry and style through two distinct input modalities: a segmentation map to control geometry and either a text prompt or a reference image to specify appearance. We highlight the intuitive and powerful 3D editing capabilities of our model through a lightweight, interactive GUI, where users can effortlessly sculpt geometry by drawing segmentation maps and stylize appearance via natural language or image prompts.\n  Project Page: https://antoniooroz.github.io/PercHead Video: https://www.youtube.com/watch?v=4hFybgTk4kE",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-04",
      "updated": "2025-11-04",
      "comment": "Project Page: https://antoniooroz.github.io/PercHead/ Video: https://www.youtube.com/watch?v=4hFybgTk4kE",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02777v1",
      "code_links": [
        {
          "url": "https://antoniooroz.github.io/PercHead",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "gaussian splatting"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Many-vs-Many Missile Guidance via Virtual Targets",
      "authors": [
        "Marc Schneider",
        "Walter Fichter"
      ],
      "arxiv_id": "2511.02526v1",
      "summary": "This paper presents a novel approach to many-vs-many missile guidance using virtual targets (VTs) generated by a Normalizing Flows-based trajectory predictor. Rather than assigning n interceptors directly to m physical targets through conventional weapon target assignment algorithms, we propose a centralized strategy that constructs n VT trajectories representing probabilistic predictions of maneuvering target behavior. Each interceptor is guided toward its assigned VT using Zero-Effort-Miss guidance during midcourse flight, transitioning to Proportional Navigation guidance for terminal interception. This approach treats many-vs-many engagements as many-vs-distribution scenarios, exploiting numerical superiority (n > m) by distributing interceptors across diverse trajectory hypotheses rather than pursuing identical deterministic predictions. Monte Carlo simulations across various target-interceptor configurations (1-6 targets, 1-8 interceptors) demonstrate that the VT method matches or exceeds baseline straight-line prediction performance by 0-4.1% when n = m, with improvements increasing to 5.8-14.4% when n > m. The results confirm that probabilistic VTs enable effective exploitation of numerical superiority, significantly increasing interception probability in many-vs-many scenarios.",
      "categories": [
        "eess.SY",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "eess.SY",
      "published": "2025-11-04",
      "updated": "2025-11-04",
      "comment": "will be submitted to Journal of Guidance, Control, and Dynamics as Technical Note",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02526v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "OLATverse: A Large-scale Real-world Object Dataset with Precise Lighting Control",
      "authors": [
        "Xilong Zhou",
        "Jianchun Chen",
        "Pramod Rao",
        "Timo Teufel",
        "Linjie Lyu",
        "Tigran Minasian",
        "Oleksandr Sotnychenko",
        "Xiao-Xiao Long",
        "Marc Habermann",
        "Christian Theobalt"
      ],
      "arxiv_id": "2511.02483v3",
      "summary": "We introduce OLATverse, a large-scale dataset comprising around 9M images of 765 real-world objects, captured from multiple viewpoints under a diverse set of precisely controlled lighting conditions. While recent advances in object-centric inverse rendering, novel view synthesis and relighting have shown promising results, most techniques still heavily rely on the synthetic datasets for training and small-scale real-world datasets for benchmarking, which limits their realism and generalization. To address this gap, OLATverse offers two key advantages over existing datasets: large-scale coverage of real objects and high-fidelity appearance under precisely controlled illuminations. Specifically, OLATverse contains 765 common and uncommon real-world objects, spanning a wide range of material categories. Each object is captured using 35 DSLR cameras and 331 individually controlled light sources, enabling the simulation of diverse illumination conditions. In addition, for each object, we provide well-calibrated camera parameters, accurate object masks, photometric surface normals, and diffuse albedo as auxiliary resources. We also construct an extensive evaluation set, establishing the first comprehensive real-world object-centric benchmark for inverse rendering and normal estimation. We believe that OLATverse represents a pivotal step toward integrating the next generation of inverse rendering and relighting methods with real-world data. The full dataset, along with all post-processing workflows, will be publicly released at https://vcai.mpi-inf.mpg.de/projects/OLATverse/.",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-04",
      "updated": "2025-12-16",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02483v3",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "novel view synthesis"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Can Foundation Models Revolutionize Mobile AR Sparse Sensing?",
      "authors": [
        "Yiqin Zhao",
        "Tian Guo"
      ],
      "arxiv_id": "2511.02215v1",
      "summary": "Mobile sensing systems have long faced a fundamental trade-off between sensing quality and efficiency due to constraints in computation, power, and other limitations. Sparse sensing, which aims to acquire and process only a subset of sensor data, has been a key strategy for maintaining performance under such constraints. However, existing sparse sensing methods often suffer from reduced accuracy, as missing information across space and time introduces uncertainty into many sensing systems. In this work, we investigate whether foundation models can change the landscape of mobile sparse sensing. Using real-world mobile AR data, our evaluations demonstrate that foundation models offer significant improvements in geometry-aware image warping, a central technique for enabling accurate reuse of cross-frame information. Furthermore, our study demonstrates the scalability of foundation model-based sparse sensing and shows its leading performance in 3D scene reconstruction. Collectively, our study reveals critical aspects of the promises and the open challenges of integrating foundation models into mobile sparse sensing systems.",
      "categories": [
        "cs.CV",
        "cs.ET"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-04",
      "updated": "2025-11-04",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02215v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene reconstruction"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Hybrid Neural Network-Based Indoor Localisation System for Mobile Robots Using CSI Data in a Robotics Simulator",
      "authors": [
        "Javier Ballesteros-Jerez",
        "Jesus Martínez-Gómez",
        "Ismael García-Varea",
        "Luis Orozco-Barbosa",
        "Manuel Castillo-Cara"
      ],
      "arxiv_id": "2511.01797v1",
      "summary": "We present a hybrid neural network model for inferring the position of mobile robots using Channel State Information (CSI) data from a Massive MIMO system. By leveraging an existing CSI dataset, our approach integrates a Convolutional Neural Network (CNN) with a Multilayer Perceptron (MLP) to form a Hybrid Neural Network (HyNN) that estimates 2D robot positions. CSI readings are converted into synthetic images using the TINTO tool. The localisation solution is integrated with a robotics simulator, and the Robot Operating System (ROS), which facilitates its evaluation through heterogeneous test cases, and the adoption of state estimators like Kalman filters. Our contributions illustrate the potential of our HyNN model in achieving precise indoor localisation and navigation for mobile robots in complex environments. The study follows, and proposes, a generalisable procedure applicable beyond the specific use case studied, making it adaptable to different scenarios and datasets.",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "13 pages, 7 figures. Conference paper (ROBOVIS 2025)",
      "doi": "10.1007/978-3-032-00986-9_11",
      "journal_ref": "Robotics, Computer Vision and Intelligent Systems. ROBOVIS 2025. Communications in Computer and Information Science, vol 2629 (2025) 148-163",
      "pdf_url": "https://arxiv.org/pdf/2511.01797v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "EVLP:Learning Unified Embodied Vision-Language Planner with Reinforced Supervised Fine-Tuning",
      "authors": [
        "Xinyan Cai",
        "Shiguang Wu",
        "Dafeng Chi",
        "Yuzheng Zhuang",
        "Xingyue Quan",
        "Jianye Hao",
        "Qiang Guan"
      ],
      "arxiv_id": "2511.05553v1",
      "summary": "In complex embodied long-horizon manipulation tasks, effective task decomposition and execution require synergistic integration of textual logical reasoning and visual-spatial imagination to ensure efficient and accurate operation. Current methods fail to adopt a unified generation framework for multimodal planning, lead to inconsistent in multimodal planning. To address this challenge, we present \\textbf{EVLP (Embodied Vision-Language Planner)}, an innovative multimodal unified generation framework that jointly models linguistic reasoning and visual generation. Our approach achieves multimodal planning for long-horizon tasks through a novel training pipeline incorporating dynamic pretraining and reinforced alignment. Our core innovations consist of three key components: \\textbf{1) Unified Multimodal Generation Framework}: For understanding, We integrate semantic information with spatial features to provide comprehensive visual perception. For generation, we directly learn the joint distribution of discrete images for one-step visual synthesis, enabling coordinated language-visual modeling through learnable cross-modal attention mechanisms. \\textbf{2) Dynamic Perception Pretraining}: We propose a bidirectional dynamic alignment strategy employing inverse dynamics tasks and forward dynamics tasks, effectively strengthening multimodal correlations within a unified feature space. \\textbf{3) Reinforced Supervised Fine-Tuning}: While conducting instruction-based fine-tuning in the unified generation space, we construct a reinforce loss to align the spatial logic between textual actions and generated images, enabling the model to acquire spatio-awared multimodal planning capabilities.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05553v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Semantic BIM enrichment for firefighting assets: Fire-ART dataset and panoramic image-based 3D reconstruction",
      "authors": [
        "Ya Wen",
        "Yutong Qiao",
        "Chi Chiu Lam",
        "Ioannis Brilakis",
        "Sanghoon Lee",
        "Mun On Wong"
      ],
      "arxiv_id": "2511.01399v1",
      "summary": "Inventory management of firefighting assets is crucial for emergency preparedness, risk assessment, and on-site fire response. However, conventional methods are inefficient due to limited capabilities in automated asset recognition and reconstruction. To address the challenge, this research introduces the Fire-ART dataset and develops a panoramic image-based reconstruction approach for semantic enrichment of firefighting assets into BIM models. The Fire-ART dataset covers 15 fundamental assets, comprising 2,626 images and 6,627 instances, making it an extensive and publicly accessible dataset for asset recognition. In addition, the reconstruction approach integrates modified cube-map conversion and radius-based spherical camera projection to enhance recognition and localization accuracy. Through validations with two real-world case studies, the proposed approach achieves F1-scores of 73% and 88% and localization errors of 0.620 and 0.428 meters, respectively. The Fire-ART dataset and the reconstruction approach offer valuable resources and robust technical solutions to enhance the accurate digital management of fire safety equipment.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "",
      "doi": "10.1016/j.isprsjprs.2025.11.015",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01399v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Lateral Velocity Model for Vehicle Parking Applications",
      "authors": [
        "Luis Diener",
        "Jens Kalkkuhl",
        "Markus Enzweiler"
      ],
      "arxiv_id": "2511.01369v1",
      "summary": "Automated parking requires accurate localization for quick and precise maneuvering in tight spaces. While the longitudinal velocity can be measured using wheel encoders, the estimation of the lateral velocity remains a key challenge due to the absence of dedicated sensors in consumer-grade vehicles. Existing approaches often rely on simplified vehicle models, such as the zero-slip model, which assumes no lateral velocity at the rear axle. It is well established that this assumption does not hold during low-speed driving and researchers thus introduce additional heuristics to account for differences. In this work, we analyze real-world data from parking scenarios and identify a systematic deviation from the zero-slip assumption. We provide explanations for the observed effects and then propose a lateral velocity model that better captures the lateral dynamics of the vehicle during parking. The model improves estimation accuracy, while relying on only two parameters, making it well-suited for integration into consumer-grade applications.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "This manuscript has been submitted to Vehicle System Dynamics for possible publication",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01369v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Design and development of an electronics-free earthworm robot",
      "authors": [
        "Riddhi Das",
        "Joscha Teichmann",
        "Thomas Speck",
        "Falk J. Tauber"
      ],
      "arxiv_id": "2511.01347v1",
      "summary": "Soft robotic systems have gained widespread attention due to their inherent flexibility, adaptability, and safety, making them well-suited for varied applications. Among bioinspired designs, earthworm locomotion has been extensively studied for its efficient peristaltic motion, enabling movement in confined and unstructured environments. Existing earthworm-inspired robots primarily utilize pneumatic actuation due to its high force-to-weight ratio and ease of implementation. However, these systems often rely on bulky, power-intensive electronic control units, limiting their practicality. In this work, we present an electronics-free, earthworm-inspired pneumatic robot utilizing a modified Pneumatic Logic Gate (PLG) design. By integrating preconfigured PLG units with bellow actuators, we achieved a plug-and-play style modular system capable of peristaltic locomotion without external electronic components. The proposed design reduces system complexity while maintaining efficient actuation. We characterize the bellow actuators under different operating conditions and evaluate the robots locomotion performance. Our findings demonstrate that the modified PLG-based control system effectively generates peristaltic wave propagation, achieving autonomous motion with minimal deviation. This study serves as a proof of concept for the development of electronics-free, peristaltic soft robots. The proposed system has potential for applications in hazardous environments, where untethered, adaptable locomotion is critical. Future work will focus on further optimizing the robot design and exploring untethered operation using onboard compressed air sources.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "Conference Proceedings Paper Living Machines 2025",
      "doi": "10.1007/978-3-032-07448-5_15",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01347v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "locomotion"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model",
      "authors": [
        "Sampriti Soor",
        "Alik Pramanick",
        "Jothiprakash K",
        "Arijit Sur"
      ],
      "arxiv_id": "2511.01317v2",
      "summary": "The rapid growth of deep learning has brought about powerful models that can handle various tasks, like identifying images and understanding language. However, adversarial attacks, an unnoticed alteration, can deceive models, leading to inaccurate predictions. In this paper, a generative adversarial attack method is proposed that uses the CLIP model to create highly effective and visually imperceptible adversarial perturbations. The CLIP model's ability to align text and image representation helps incorporate natural language semantics with a guided loss to generate effective adversarial examples that look identical to the original inputs. This integration allows extensive scene manipulation, creating perturbations in multi-object environments specifically designed to deceive multilabel classifiers. Our approach integrates the concentrated perturbation strategy from Saliency-based Auto-Encoder (SSAE) with the dissimilar text embeddings similar to Generative Adversarial Multi-Object Scene Attacks (GAMA), resulting in perturbations that both deceive classification models and maintain high structural similarity to the original images. The model was tested on various tasks across diverse black-box victim models. The experimental results show that our method performs competitively, achieving comparable or superior results to existing techniques, while preserving greater visual fidelity.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-03",
      "updated": "2025-11-16",
      "comment": "18 pages, 3 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01317v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Design and Fabrication of Origami-Inspired Knitted Fabrics for Soft Robotics",
      "authors": [
        "Sehui Jeong",
        "Magaly C. Aviles",
        "Athena X. Naylor",
        "Cynthia Sung",
        "Allison M. Okamura"
      ],
      "arxiv_id": "2511.01272v1",
      "summary": "Soft robots employing compliant materials and deformable structures offer great potential for wearable devices that are comfortable and safe for human interaction. However, achieving both structural integrity and compliance for comfort remains a significant challenge. In this study, we present a novel fabrication and design method that combines the advantages of origami structures with the material programmability and wearability of knitted fabrics. We introduce a general design method that translates origami patterns into knit designs by programming both stitch and material patterns. The method creates folds in preferred directions while suppressing unintended buckling and bending by selectively incorporating heat fusible yarn to create rigid panels around compliant creases. We experimentally quantify folding moments and show that stitch patterning enhances folding directionality while the heat fusible yarn (1) keeps geometry consistent by reducing edge curl and (2) prevents out-of-plane deformations by stiffening panels. We demonstrate the framework through the successful reproduction of complex origami tessellations, including Miura-ori, Yoshimura, and Kresling patterns, and present a wearable knitted Kaleidocycle robot capable of locomotion. The combination of structural reconfigurability, material programmability, and potential for manufacturing scalability highlights knitted origami as a promising platform for next-generation wearable robotics.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01272v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "locomotion"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "An Adjoint Method for Differentiable Fluid Simulation on Flow Maps",
      "authors": [
        "Zhiqi Li",
        "Jinjin He",
        "Barnabás Börcsök",
        "Taiyuan Zhang",
        "Duowen Chen",
        "Tao Du",
        "Ming C. Lin",
        "Greg Turk",
        "Bo Zhu"
      ],
      "arxiv_id": "2511.01259v1",
      "summary": "This paper presents a novel adjoint solver for differentiable fluid simulation based on bidirectional flow maps. Our key observation is that the forward fluid solver and its corresponding backward, adjoint solver share the same flow map as the forward simulation. In the forward pass, this map transports fluid impulse variables from the initial frame to the current frame to simulate vortical dynamics. In the backward pass, the same map propagates adjoint variables from the current frame back to the initial frame to compute gradients. This shared long-range map allows the accuracy of gradient computation to benefit directly from improvements in flow map construction. Building on this insight, we introduce a novel adjoint solver that solves the adjoint equations directly on the flow map, enabling long-range and accurate differentiation of incompressible flows without differentiating intermediate numerical steps or storing intermediate variables, as required in conventional adjoint methods. To further improve efficiency, we propose a long-short time-sparse flow map representation for evolving adjoint variables. Our approach has low memory usage, requiring only 6.53GB of data at a resolution of $192^3$ while preserving high accuracy in tracking vorticity, enabling new differentiable simulation tasks that require precise identification, prediction, and control of vortex dynamics.",
      "categories": [
        "cs.GR",
        "physics.flu-dyn"
      ],
      "primary_category": "cs.GR",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "15 pages, 16 figures",
      "doi": "10.1145/3757377.3763903",
      "journal_ref": "ACM SIGGRAPH Asia Conference Proceedings (2025)",
      "pdf_url": "https://arxiv.org/pdf/2511.01259v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "PULSE"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "8_physics_animation"
      ]
    },
    {
      "title": "Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop",
      "authors": [
        "YoungJae Cheong",
        "Jhonghyun An"
      ],
      "arxiv_id": "2511.01250v2",
      "summary": "LiDAR semantic segmentation degrades in adverse weather because refraction, scattering, and point dropouts corrupt geometry. Prior work in weather simulation, mixing-based augmentation, domain randomization, and uncertainty or boundary regularization improves robustness but still overlooks structural vulnerabilities near boundaries, corners, and sparse regions. We present a Light Geometry-aware adapter. The module aligns azimuth and applies horizontal circular padding to preserve neighbor continuity across the 0~360 degree wrap-around boundary. A local-window K-Nearest Neighbors gathers nearby points and computes simple local statistics, which are compressed into compact geometry-aware cues. During training, these cues drive region-aware regularization that stabilizes predictions in structurally fragile areas. The adapter is plug and play, complements augmentation, and can be enabled only during training with negligible inference cost. We adopt a source-only cross-weather setup where models train on SemanticKITTI and are evaluated on SemanticSTF without target labels or fine-tuning. The adapter improves mIoU by 7.9 percentage points over the data-centric augmentation baseline and by 0.6 points over the class-centric regularization baseline. These results indicate that geometry-driven regularization is a key direction for all-weather LiDAR segmentation.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-03",
      "updated": "2025-11-06",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01250v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "domain randomization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Eyes on Target: Gaze-Aware Object Detection in Egocentric Video",
      "authors": [
        "Vishakha Lall",
        "Yisi Liu"
      ],
      "arxiv_id": "2511.01237v1",
      "summary": "Human gaze offers rich supervisory signals for understanding visual attention in complex visual environments. In this paper, we propose Eyes on Target, a novel depth-aware and gaze-guided object detection framework designed for egocentric videos. Our approach injects gaze-derived features into the attention mechanism of a Vision Transformer (ViT), effectively biasing spatial feature selection toward human-attended regions. Unlike traditional object detectors that treat all regions equally, our method emphasises viewer-prioritised areas to enhance object detection. We validate our method on an egocentric simulator dataset where human visual attention is critical for task assessment, illustrating its potential in evaluating human performance in simulation scenarios. We evaluate the effectiveness of our gaze-integrated model through extensive experiments and ablation studies, demonstrating consistent gains in detection accuracy over gaze-agnostic baselines on both the custom simulator dataset and public benchmarks, including Ego4D Ego-Motion and Ego-CH-Gaze datasets. To interpret model behaviour, we also introduce a gaze-aware attention head importance metric, revealing how gaze cues modulate transformer attention dynamics.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-03",
      "updated": "2025-11-03",
      "comment": "Accepted at RAAI 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01237v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "ego-motion"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis",
      "authors": [
        "Dongheng Lin",
        "Mengxue Qu",
        "Kunyang Han",
        "Jianbo Jiao",
        "Xiaojie Jin",
        "Yunchao Wei"
      ],
      "arxiv_id": "2511.00962v1",
      "summary": "Most video-anomaly research stops at frame-wise detection, offering little insight into why an event is abnormal, typically outputting only frame-wise anomaly scores without spatial or semantic context. Recent video anomaly localization and video anomaly understanding methods improve explainability but remain data-dependent and task-specific. We propose a unified reasoning framework that bridges the gap between temporal detection, spatial localization, and textual explanation. Our approach is built upon a chained test-time reasoning process that sequentially connects these tasks, enabling holistic zero-shot anomaly analysis without any additional training. Specifically, our approach leverages intra-task reasoning to refine temporal detections and inter-task chaining for spatial and semantic understanding, yielding improved interpretability and generalization in a fully zero-shot manner. Without any additional data or gradients, our method achieves state-of-the-art zero-shot performance across multiple video anomaly detection, localization, and explanation benchmarks. The results demonstrate that careful prompt design with task-wise chaining can unlock the reasoning power of foundation models, enabling practical, interpretable video anomaly analysis in a fully zero-shot manner. Project Page: https://rathgrith.github.io/Unified_Frame_VAA/.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-02",
      "updated": "2025-11-02",
      "comment": "NeurIPS 2025 poster",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00962v1",
      "code_links": [
        {
          "url": "https://rathgrith.github.io/Unified_Frame_VAA/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "pacSTL: PAC-Bounded Signal Temporal Logic from Data-Driven Reachability Analysis",
      "authors": [
        "Elizabeth Dietrich",
        "Hanna Krasowski",
        "Emir Cem Gezer",
        "Roger Skjetne",
        "Asgeir Johan Sørensen",
        "Murat Arcak"
      ],
      "arxiv_id": "2511.00934v1",
      "summary": "Real-world robotic systems must comply with safety requirements in the presence of uncertainty. To define and measure requirement adherence, Signal Temporal Logic (STL) offers a mathematically rigorous and expressive language. However, standard STL cannot account for uncertainty. We address this problem by presenting pacSTL, a framework that combines Probably Approximately Correct (PAC) bounded set predictions with an interval extension of STL through optimization problems on the atomic proposition level. pacSTL provides PAC-bounded robustness intervals on the specification level that can be utilized in monitoring. We demonstrate the effectiveness of this approach through maritime navigation and analyze the efficiency and scalability of pacSTL through simulation and real-world experimentation on model vessels.",
      "categories": [
        "cs.LO",
        "cs.RO"
      ],
      "primary_category": "cs.LO",
      "published": "2025-11-02",
      "updated": "2025-11-02",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00934v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction",
      "authors": [
        "Yu Liu",
        "Zhijie Liu",
        "Zedong Yang",
        "You-Fu Li",
        "He Kong"
      ],
      "arxiv_id": "2511.00858v1",
      "summary": "Predicting pedestrian crossing intentions is crucial for the navigation of mobile robots and intelligent vehicles. Although recent deep learning-based models have shown significant success in forecasting intentions, few consider incomplete observation under occlusion scenarios. To tackle this challenge, we propose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded motion patterns and leverages them to guide future intention prediction. During the denoising stage, we introduce an occlusion-aware diffusion transformer architecture to estimate noise features associated with occluded patterns, thereby enhancing the model's ability to capture contextual relationships in occluded semantic scenarios. Furthermore, an occlusion mask-guided reverse process is introduced to effectively utilize observation information, reducing the accumulation of prediction errors and enhancing the accuracy of reconstructed motion features. The performance of the proposed method under various occlusion scenarios is comprehensively evaluated and compared with existing methods on popular benchmarks, namely PIE and JAAD. Extensive experimental results demonstrate that the proposed method achieves more robust performance than existing methods in the literature.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-02",
      "updated": "2025-11-02",
      "comment": "This manuscript has been accepted to the IEEE Transactions on Intelligent Transportation Systems as a regular paper",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00858v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Benchmarking individual tree segmentation using multispectral airborne laser scanning data: the FGI-EMIT dataset",
      "authors": [
        "Lassi Ruoppa",
        "Tarmo Hietala",
        "Verneri Seppänen",
        "Josef Taher",
        "Teemu Hakala",
        "Xiaowei Yu",
        "Antero Kukko",
        "Harri Kaartinen",
        "Juha Hyyppä"
      ],
      "arxiv_id": "2511.00653v1",
      "summary": "Individual tree segmentation (ITS) from LiDAR point clouds is fundamental for applications such as forest inventory, carbon monitoring and biodiversity assessment. Traditionally, ITS has been achieved with unsupervised geometry-based algorithms, while more recent advances have shifted toward supervised deep learning (DL). In the past, progress in method development was hindered by the lack of large-scale benchmark datasets, and the availability of novel data formats, particularly multispectral (MS) LiDAR, remains limited to this day, despite evidence that MS reflectance can improve the accuracy of ITS. This study introduces FGI-EMIT, the first large-scale MS airborne laser scanning benchmark dataset for ITS. Captured at wavelengths 532, 905, and 1,550 nm, the dataset consists of 1,561 manually annotated trees, with a particular focus on small understory trees. Using FGI-EMIT, we comprehensively benchmarked four conventional unsupervised algorithms and four supervised DL approaches. Hyperparameters of unsupervised methods were optimized using a Bayesian approach, while DL models were trained from scratch. Among the unsupervised methods, Treeiso achieved the highest test set F1-score of 52.7%. The DL approaches performed significantly better overall, with the best model, ForestFormer3D, attaining an F1-score of 73.3%. The most significant difference was observed in understory trees, where ForestFormer3D exceeded Treeiso by 25.9 percentage points. An ablation study demonstrated that current DL-based approaches generally fail to leverage MS reflectance information when it is provided as additional input features, although single channel reflectance can improve accuracy marginally, especially for understory trees. A performance analysis across point densities further showed that DL methods consistently remain superior to unsupervised algorithms, even at densities as low as 10 points/m$^2$.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-01",
      "updated": "2025-11-01",
      "comment": "39 pages, 9 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00653v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models",
      "authors": [
        "Panwang Pan",
        "Chenguo Lin",
        "Jingjing Zhao",
        "Chenxin Li",
        "Yuchen Lin",
        "Haopeng Li",
        "Honglei Yan",
        "Kairun Wen",
        "Yunlong Lin",
        "Yixuan Yuan",
        "Yadong Mu"
      ],
      "arxiv_id": "2511.00503v1",
      "summary": "We introduce Diff4Splat, a feed-forward method that synthesizes controllable and explicit 4D scenes from a single image. Our approach unifies the generative priors of video diffusion models with geometry and motion constraints learned from large-scale 4D datasets. Given a single input image, a camera trajectory, and an optional text prompt, Diff4Splat directly predicts a deformable 3D Gaussian field that encodes appearance, geometry, and motion, all in a single forward pass, without test-time optimization or post-hoc refinement. At the core of our framework lies a video latent transformer, which augments video diffusion models to jointly capture spatio-temporal dependencies and predict time-varying 3D Gaussian primitives. Training is guided by objectives on appearance fidelity, geometric accuracy, and motion consistency, enabling Diff4Splat to synthesize high-quality 4D scenes in 30 seconds. We demonstrate the effectiveness of Diff4Splatacross video generation, novel view synthesis, and geometry extraction, where it matches or surpasses optimization-based methods for dynamic scene synthesis while being significantly more efficient.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-01",
      "updated": "2025-11-01",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00503v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "novel view synthesis"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Runge-Kutta Approximations for Direct Coning Compensation Applying Lie Theory",
      "authors": [
        "John A. Christian",
        "Michael R. Walker",
        "Wyatt Bridgman",
        "Michael J. Sparapany"
      ],
      "arxiv_id": "2511.00412v1",
      "summary": "The integration of gyroscope measurements is an essential task for most navigation systems. Modern vehicles typically use strapdown systems, such that gyro integration requires coning compensation to account for the sensor's rotation during the integration. Many coning compensation algorithms have been developed and a few are reviewed. This work introduces a new class of coning correction algorithm built directly from the classical Runge-Kutta integration routines. A simple case is shown to collapse to one of the most popular coning algorithms and a clear procedure for generating higher-order algorithms is presented.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-01",
      "updated": "2025-11-01",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00412v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "DEFT-LLM: Disentangled Expert Feature Tuning for Micro-Expression Recognition",
      "authors": [
        "Ren Zhang",
        "Huilai Li",
        "Chao qi",
        "Guoliang Xu",
        "Tianyu Zhou",
        "Wei wei",
        "Jianqin Yin"
      ],
      "arxiv_id": "2511.10948v1",
      "summary": "Micro expression recognition (MER) is crucial for inferring genuine emotion. Applying a multimodal large language model (MLLM) to this task enables spatio-temporal analysis of facial motion and provides interpretable descriptions. However, there are still two core challenges: (1) The entanglement of static appearance and dynamic motion cues prevents the model from focusing on subtle motion; (2) Textual labels in existing MER datasets do not fully correspond to underlying facial muscle movements, creating a semantic gap between text supervision and physical motion. To address these issues, we propose DEFT-LLM, which achieves motion semantic alignment by multi-expert disentanglement. We first introduce Uni-MER, a motion-driven instruction dataset designed to align text with local facial motion. Its construction leverages dual constraints from optical flow and Action Unit (AU) labels to ensure spatio-temporal consistency and reasonable correspondence to the movements. We then design an architecture with three experts to decouple facial dynamics into independent and interpretable representations (structure, dynamic textures, and motion-semantics). By integrating the instruction-aligned knowledge from Uni-MER into DEFT-LLM, our method injects effective physical priors for micro expressions while also leveraging the cross modal reasoning ability of large language models, thus enabling precise capture of subtle emotional cues. Experiments on multiple challenging MER benchmarks demonstrate state-of-the-art performance, as well as a particular advantage in interpretable modeling of local facial motion.",
      "categories": [
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10948v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "optical flow"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Dynamically Extensible and Retractable Robotic Leg Linkages for Multi-task Execution in Search and Rescue Scenarios",
      "authors": [
        "William Harris",
        "Lucas Yager",
        "Syler Sylvester",
        "Elizabeth Peiros",
        "Micheal C. Yip"
      ],
      "arxiv_id": "2511.10816v2",
      "summary": "Search and rescue (SAR) robots are required to quickly traverse terrain and perform high-force rescue tasks, necessitating both terrain adaptability and controlled high-force output. Few platforms exist today for SAR, and fewer still have the ability to cover both tasks of terrain adaptability and high-force output when performing extraction. While legged robots offer significant ability to traverse uneven terrain, they typically are unable to incorporate mechanisms that provide variable high-force outputs, unlike traditional wheel-based drive trains. This work introduces a novel concept for a dynamically extensible and retractable robot leg. Leveraging a dynamically extensible and retractable five-bar linkage design, it allows for mechanically switching between height-advantaged and force-advantaged configurations via a geometric transformation. A testbed evaluated leg performance across linkage geometries and operating modes, with empirical and analytical analyses conducted on stride length, force output, and stability. The results demonstrate that the morphing leg offers a promising path toward SAR robots that can both navigate terrain quickly and perform rescue tasks effectively.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-13",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10816v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "legged robot"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Querying Labeled Time Series Data with Scenario Programs",
      "authors": [
        "Edward Kim",
        "Devan Shanker",
        "Varun Bharadwaj",
        "Hongbeen Park",
        "Jinkyu Kim",
        "Hazem Torfah",
        "Daniel J Fremont",
        "Sanjit A Seshia"
      ],
      "arxiv_id": "2511.10627v1",
      "summary": "Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber physical systems (CPS). As a result, significant research efforts have been directed toward identifying failure scenarios within simulation environments. However, a critical question remains. Are the AV failure scenarios discovered in simulation reproducible on actual systems in the real world? The sim-to-real gap caused by differences between simulated and real sensor data means that failure scenarios identified in simulation might either be artifacts of synthetic sensor data or actual issues that also occur with real sensor data. To address this, an effective approach to validating simulated failure scenarios is to locate occurrences of these scenarios within real-world datasets and verify whether the failure persists on the datasets. To this end, we introduce a formal definition of how labeled time series sensor data can match an abstract scenario, represented as a scenario program using the Scenic probabilistic programming language. We present a querying algorithm that, given a scenario program and a labeled dataset, identifies the subset of data that matches the specified scenario. Our experiment shows that our algorithm is more accurate and orders of magnitude faster in querying scenarios than the state-of-the-art commercial vision large language models, and can scale with the duration of queried time series data.",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.FL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "",
      "doi": "10.1007/978-3-031-93706-4_12",
      "journal_ref": "NASA Formal Methods Conference 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.10627v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "sim-to-real"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "RoboBenchMart: Benchmarking Robots in Retail Environment",
      "authors": [
        "Konstantin Soshin",
        "Alexander Krapukhin",
        "Andrei Spiridonov",
        "Denis Shepelev",
        "Gregorii Bukhtuev",
        "Andrey Kuznetsov",
        "Vlad Shakhuro"
      ],
      "arxiv_id": "2511.10276v1",
      "summary": "Most existing robotic manipulation benchmarks focus on simplified tabletop scenarios, typically involving a stationary robotic arm interacting with various objects on a flat surface. To address this limitation, we introduce RoboBenchMart, a more challenging and realistic benchmark designed for dark store environments, where robots must perform complex manipulation tasks with diverse grocery items. This setting presents significant challenges, including dense object clutter and varied spatial configurations -- with items positioned at different heights, depths, and in close proximity. By targeting the retail domain, our benchmark addresses a setting with strong potential for near-term automation impact. We demonstrate that current state-of-the-art generalist models struggle to solve even common retail tasks. To support further research, we release the RoboBenchMart suite, which includes a procedural store layout generator, a trajectory generation pipeline, evaluation tools and fine-tuned baseline models.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10276v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "LiNeXt: Revisiting LiDAR Completion with Efficient Non-Diffusion Architectures",
      "authors": [
        "Wenzhe He",
        "Xiaojun Chen",
        "Ruiqi Wang",
        "Ruihui Li",
        "Huilong Pi",
        "Jiapeng Zhang",
        "Zhuo Tang",
        "Kenli Li"
      ],
      "arxiv_id": "2511.10209v2",
      "summary": "3D LiDAR scene completion from point clouds is a fundamental component of perception systems in autonomous vehicles. Previous methods have predominantly employed diffusion models for high-fidelity reconstruction. However, their multi-step iterative sampling incurs significant computational overhead, limiting its real-time applicability. To address this, we propose LiNeXt-a lightweight, non-diffusion network optimized for rapid and accurate point cloud completion. Specifically, LiNeXt first applies the Noise-to-Coarse (N2C) Module to denoise the input noisy point cloud in a single pass, thereby obviating the multi-step iterative sampling of diffusion-based methods. The Refine Module then takes the coarse point cloud and its intermediate features from the N2C Module to perform more precise refinement, further enhancing structural completeness. Furthermore, we observe that LiDAR point clouds exhibit a distance-dependent spatial distribution, being densely sampled at proximal ranges and sparsely sampled at distal ranges. Accordingly, we propose the Distance-aware Selected Repeat strategy to generate a more uniformly distributed noisy point cloud. On the SemanticKITTI dataset, LiNeXt achieves a 199.8x speedup in inference, reduces Chamfer Distance by 50.7%, and uses only 6.1% of the parameters compared with LiDiff. These results demonstrate the superior efficiency and effectiveness of LiNeXt for real-time scene completion.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-30",
      "comment": "18 pages, 13 figures, Accepted to AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10209v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Split-Layer: Enhancing Implicit Neural Representation by Maximizing the Dimensionality of Feature Space",
      "authors": [
        "Zhicheng Cai",
        "Hao Zhu",
        "Linsen Chen",
        "Qiu Shen",
        "Xun Cao"
      ],
      "arxiv_id": "2511.10142v1",
      "summary": "Implicit neural representation (INR) models signals as continuous functions using neural networks, offering efficient and differentiable optimization for inverse problems across diverse disciplines. However, the representational capacity of INR defined by the range of functions the neural network can characterize, is inherently limited by the low-dimensional feature space in conventional multilayer perceptron (MLP) architectures. While widening the MLP can linearly increase feature space dimensionality, it also leads to a quadratic growth in computational and memory costs. To address this limitation, we propose the split-layer, a novel reformulation of MLP construction. The split-layer divides each layer into multiple parallel branches and integrates their outputs via Hadamard product, effectively constructing a high-degree polynomial space. This approach significantly enhances INR's representational capacity by expanding the feature space dimensionality without incurring prohibitive computational overhead. Extensive experiments demonstrate that the split-layer substantially improves INR performance, surpassing existing methods across multiple tasks, including 2D image fitting, 2D CT reconstruction, 3D shape representation, and 5D novel view synthesis.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10142v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "novel view synthesis"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Toward bilipshiz geometric models",
      "authors": [
        "Yonatan Sverdlov",
        "Eitan Rosen",
        "Nadav Dym"
      ],
      "arxiv_id": "2511.11735v1",
      "summary": "Many neural networks for point clouds are, by design, invariant to the symmetries of this datatype: permutations and rigid motions. The purpose of this paper is to examine whether such networks preserve natural symmetry aware distances on the point cloud spaces, through the notion of bi-Lipschitz equivalence. This inquiry is motivated by recent work in the Equivariant learning literature which highlights the advantages of bi-Lipschitz models in other scenarios.\n  We consider two symmetry aware metrics on point clouds: (a) The Procrustes Matching (PM) metric and (b) Hard Gromov Wasserstien distances. We show that these two distances themselves are not bi-Lipschitz equivalent, and as a corollary deduce that popular invariant networks for point clouds are not bi-Lipschitz with respect to the PM metric. We then show how these networks can be modified so that they do obtain bi-Lipschitz guarantees. Finally, we provide initial experiments showing the advantage of the proposed bi-Lipschitz model over standard invariant models, for the tasks of finding correspondences between 3D point clouds.",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11735v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "LoG3D: Ultra-High-Resolution 3D Shape Modeling via Local-to-Global Partitioning",
      "authors": [
        "Xinran Yang",
        "Shuichang Lai",
        "Jiangjing Lyu",
        "Hongjie Li",
        "Bowen Pan",
        "Yuanqi Li",
        "Jie Guo",
        "Zhengkang Zhou",
        "Yanwen Guo"
      ],
      "arxiv_id": "2511.10040v2",
      "summary": "Generating high-fidelity 3D contents remains a fundamental challenge due to the complexity of representing arbitrary topologies-such as open surfaces and intricate internal structures-while preserving geometric details. Prevailing methods based on signed distance fields (SDFs) are hampered by costly watertight preprocessing and struggle with non-manifold geometries, while point-cloud representations often suffer from sampling artifacts and surface discontinuities. To overcome these limitations, we propose a novel 3D variational autoencoder (VAE) framework built upon unsigned distance fields (UDFs)-a more robust and computationally efficient representation that naturally handles complex and incomplete shapes. Our core innovation is a local-to-global (LoG) architecture that processes the UDF by partitioning it into uniform subvolumes, termed UBlocks. This architecture couples 3D convolutions for capturing local detail with sparse transformers for enforcing global coherence. A Pad-Average strategy further ensures smooth transitions at subvolume boundaries during reconstruction. This modular design enables seamless scaling to ultra-high resolutions up to $2048^3$-a regime previously unattainable for 3D VAEs. Experiments demonstrate state-of-the-art performance in both reconstruction accuracy and generative quality, yielding superior surface smoothness and geometric flexibility.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-18",
      "comment": "11 pages, 6 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10040v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models",
      "authors": [
        "Xinyi Wang",
        "Xun Yang",
        "Yanlong Xu",
        "Yuchen Wu",
        "Zhen Li",
        "Na Zhao"
      ],
      "arxiv_id": "2511.10017v1",
      "summary": "Effective human-agent collaboration in physical environments requires understanding not only what to act upon, but also where the actionable elements are and how to interact with them. Existing approaches often operate at the object level or disjointedly handle fine-grained affordance reasoning, lacking coherent, instruction-driven grounding and reasoning. In this work, we introduce a new task: Fine-grained 3D Embodied Reasoning, which requires an agent to predict, for each referenced affordance element in a 3D scene, a structured triplet comprising its spatial location, motion type, and motion axis, based on a task instruction. To solve this task, we propose AffordBot, a novel framework that integrates Multimodal Large Language Models (MLLMs) with a tailored chain-of-thought (CoT) reasoning paradigm. To bridge the gap between 3D input and 2D-compatible MLLMs, we render surround-view images of the scene and project 3D element candidates into these views, forming a rich visual representation aligned with the scene geometry. Our CoT pipeline begins with an active perception stage, prompting the MLLM to select the most informative viewpoint based on the instruction, before proceeding with step-by-step reasoning to localize affordance elements and infer plausible interaction motions. Evaluated on the SceneFun3D dataset, AffordBot achieves state-of-the-art performance, demonstrating strong generalization and physically grounded reasoning with only 3D point cloud input and MLLMs.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "NeurIPS 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10017v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "DBGroup: Dual-Branch Point Grouping for Weakly Supervised 3D Semantic Instance Segmentation",
      "authors": [
        "Xuexun Liu",
        "Xiaoxu Xu",
        "Qiudan Zhang",
        "Lin Ma",
        "Xu Wang"
      ],
      "arxiv_id": "2511.10003v2",
      "summary": "Weakly supervised 3D instance segmentation is essential for 3D scene understanding, especially as the growing scale of data and high annotation costs associated with fully supervised approaches. Existing methods primarily rely on two forms of weak supervision: one-thing-one-click annotations and bounding box annotations, both of which aim to reduce labeling efforts. However, these approaches still encounter limitations, including labor-intensive annotation processes, high complexity, and reliance on expert annotators. To address these challenges, we propose \\textbf{DBGroup}, a two-stage weakly supervised 3D instance segmentation framework that leverages scene-level annotations as a more efficient and scalable alternative. In the first stage, we introduce a Dual-Branch Point Grouping module to generate pseudo labels guided by semantic and mask cues extracted from multi-view images. To further improve label quality, we develop two refinement strategies: Granularity-Aware Instance Merging and Semantic Selection and Propagation. The second stage involves multi-round self-training on an end-to-end instance segmentation network using the refined pseudo-labels. Additionally, we introduce an Instance Mask Filter strategy to address inconsistencies within the pseudo labels. Extensive experiments demonstrate that DBGroup achieves competitive performance compared to sparse-point-level supervised 3D instance segmentation methods, while surpassing state-of-the-art scene-level supervised 3D semantic segmentation approaches. Code is available at https://github.com/liuxuexun/DBGroup.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10003v2",
      "code_links": [
        {
          "url": "https://github.com/liuxuexun/DBGroup",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "MosaicDoc: A Large-Scale Bilingual Benchmark for Visually Rich Document Understanding",
      "authors": [
        "Ketong Chen",
        "Yuhao Chen",
        "Yang Xue"
      ],
      "arxiv_id": "2511.09919v1",
      "summary": "Despite the rapid progress of Vision-Language Models (VLMs), their capabilities are inadequately assessed by existing benchmarks, which are predominantly English-centric, feature simplistic layouts, and support limited tasks. Consequently, they fail to evaluate model performance for Visually Rich Document Understanding (VRDU), a critical challenge involving complex layouts and dense text. To address this, we introduce DocWeaver, a novel multi-agent pipeline that leverages Large Language Models to automatically generate a new benchmark. The result is MosaicDoc, a large-scale, bilingual (Chinese and English) resource designed to push the boundaries of VRDU. Sourced from newspapers and magazines, MosaicDoc features diverse and complex layouts (including multi-column and non-Manhattan), rich stylistic variety from 196 publishers, and comprehensive multi-task annotations (OCR, VQA, reading order, and localization). With 72K images and over 600K QA pairs, MosaicDoc serves as a definitive benchmark for the field. Our extensive evaluation of state-of-the-art models on this benchmark reveals their current limitations in handling real-world document complexity and charts a clear path for future research.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09919v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "HCC-3D: Hierarchical Compensatory Compression for 98% 3D Token Reduction in Vision-Language Models",
      "authors": [
        "Liheng Zhang",
        "Jin Wang",
        "Hui Li",
        "Bingfeng Zhang",
        "Weifeng Liu"
      ],
      "arxiv_id": "2511.09883v1",
      "summary": "3D understanding has drawn significant attention recently, leveraging Vision-Language Models (VLMs) to enable multi-modal reasoning between point cloud and text data. Current 3D-VLMs directly embed the 3D point clouds into 3D tokens, following large 2D-VLMs with powerful reasoning capabilities. However, this framework has a great computational cost limiting its application, where we identify that the bottleneck lies in processing all 3D tokens in the Large Language Model (LLM) part. This raises the question: how can we reduce the computational overhead introduced by 3D tokens while preserving the integrity of their essential information? To address this question, we introduce Hierarchical Compensatory Compression (HCC-3D) to efficiently compress 3D tokens while maintaining critical detail retention. Specifically, we first propose a global structure compression (GSC), in which we design global queries to compress all 3D tokens into a few key tokens while keeping overall structural information. Then, to compensate for the information loss in GSC, we further propose an adaptive detail mining (ADM) module that selectively recompresses salient but under-attended features through complementary scoring. Extensive experiments demonstrate that HCC-3D not only achieves extreme compression ratios (approximately 98%) compared to previous 3D-VLMs, but also achieves new state-of-the-art performance, showing the great improvements on both efficiency and performance.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09883v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Provably Safe Stein Variational Clarity-Aware Informative Planning",
      "authors": [
        "Kaleb Ben Naveed",
        "Utkrisht Sahai",
        "Anouck Girard",
        "Dimitra Panagou"
      ],
      "arxiv_id": "2511.09836v1",
      "summary": "Autonomous robots are increasingly deployed for information-gathering tasks in environments that vary across space and time. Planning informative and safe trajectories in such settings is challenging because information decays when regions are not revisited. Most existing planners model information as static or uniformly decaying, ignoring environments where the decay rate varies spatially; those that model non-uniform decay often overlook how it evolves along the robot's motion, and almost all treat safety as a soft penalty. In this paper, we address these challenges. We model uncertainty in the environment using clarity, a normalized representation of differential entropy from our earlier work that captures how information improves through new measurements and decays over time when regions are not revisited. Building on this, we present Stein Variational Clarity-Aware Informative Planning, a framework that embeds clarity dynamics within trajectory optimization and enforces safety through a low-level filtering mechanism based on our earlier gatekeeper framework for safety verification. The planner performs Bayesian inference-based learning via Stein variational inference, refining a distribution over informative trajectories while filtering each nominal Stein informative trajectory to ensure safety. Hardware experiments and simulations across environments with varying decay rates and obstacles demonstrate consistent safety and reduced information deficits.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "comment": "Submitted to Learning for Dynamics & Control Conference 2026. Paper Website: (https://usahai18.github.io/stein_clarity/)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09836v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "trajectory optimization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers",
      "authors": [
        "Isaac Robinson",
        "Peter Robicheaux",
        "Matvei Popov",
        "Deva Ramanan",
        "Neehar Peri"
      ],
      "arxiv_id": "2511.09554v1",
      "summary": "Open-vocabulary detectors achieve impressive performance on COCO, but often fail to generalize to real-world datasets with out-of-distribution classes not typically found in their pre-training. Rather than simply fine-tuning a heavy-weight vision-language model (VLM) for new domains, we introduce RF-DETR, a light-weight specialist detection transformer that discovers accuracy-latency Pareto curves for any target dataset with weight-sharing neural architecture search (NAS). Our approach fine-tunes a pre-trained base network on a target dataset and evaluates thousands of network configurations with different accuracy-latency tradeoffs without re-training. Further, we revisit the \"tunable knobs\" for NAS to improve the transferability of DETRs to diverse target domains. Notably, RF-DETR significantly improves on prior state-of-the-art real-time methods on COCO and Roboflow100-VL. RF-DETR (nano) achieves 48.0 AP on COCO, beating D-FINE (nano) by 5.3 AP at similar latency, and RF-DETR (2x-large) outperforms GroundingDINO (tiny) by 1.2 AP on Roboflow100-VL while running 20x as fast. To the best of our knowledge, RF-DETR (2x-large) is the first real-time detector to surpass 60 AP on COCO. Our code is at https://github.com/roboflow/rf-detr",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "Project Page: https://rfdetr.roboflow.com/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09554v1",
      "code_links": [
        {
          "url": "https://github.com/roboflow/rf-detr",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "running"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Enriching Knowledge Distillation with Cross-Modal Teacher Fusion",
      "authors": [
        "Amir M. Mansourian",
        "Amir Mohammad Babaei",
        "Shohreh Kasaei"
      ],
      "arxiv_id": "2511.09286v1",
      "summary": "Multi-teacher knowledge distillation (KD), a more effective technique than traditional single-teacher methods, transfers knowledge from expert teachers to a compact student model using logit or feature matching. However, most existing approaches lack knowledge diversity, as they rely solely on unimodal visual information, overlooking the potential of cross-modal representations. In this work, we explore the use of CLIP's vision-language knowledge as a complementary source of supervision for KD, an area that remains largely underexplored. We propose a simple yet effective framework that fuses the logits and features of a conventional teacher with those from CLIP. By incorporating CLIP's multi-prompt textual guidance, the fused supervision captures both dataset-specific and semantically enriched visual cues. Beyond accuracy, analysis shows that the fused teacher yields more confident and reliable predictions, significantly increasing confident-correct cases while reducing confidently wrong ones. Moreover, fusion with CLIP refines the entire logit distribution, producing semantically meaningful probabilities for non-target classes, thereby improving inter-class consistency and distillation quality. Despite its simplicity, the proposed method, Enriching Knowledge Distillation (RichKD), consistently outperforms most existing baselines across multiple benchmarks and exhibits stronger robustness under distribution shifts and input corruptions.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "11 pages, 5 figures, 8 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09286v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "feature matching"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "6_video_extraction"
      ]
    },
    {
      "title": "HOTFLoc++: End-to-End Hierarchical LiDAR Place Recognition, Re-Ranking, and 6-DoF Metric Localisation in Forests",
      "authors": [
        "Ethan Griffiths",
        "Maryam Haghighat",
        "Simon Denman",
        "Clinton Fookes",
        "Milad Ramezani"
      ],
      "arxiv_id": "2511.09170v1",
      "summary": "This article presents HOTFLoc++, an end-to-end framework for LiDAR place recognition, re-ranking, and 6-DoF metric localisation in forests. Leveraging an octree-based transformer, our approach extracts hierarchical local descriptors at multiple granularities to increase robustness to clutter, self-similarity, and viewpoint changes in challenging scenarios, including ground-to-ground and ground-to-aerial in forest and urban environments. We propose a learnable multi-scale geometric verification module to reduce re-ranking failures in the presence of degraded single-scale correspondences. Our coarse-to-fine registration approach achieves comparable or lower localisation errors to baselines, with runtime improvements of two orders of magnitude over RANSAC for dense point clouds. Experimental results on public datasets show the superiority of our approach compared to state-of-the-art methods, achieving an average Recall@1 of 90.7% on CS-Wild-Places: an improvement of 29.6 percentage points over baselines, while maintaining high performance on single-source benchmarks with an average Recall@1 of 91.7% and 96.0% on Wild-Places and MulRan, respectively. Our method achieves under 2 m and 5 degrees error for 97.2% of 6-DoF registration attempts, with our multi-scale re-ranking module reducing localisation errors by ~2$\\times$ on average. The code will be available upon acceptance.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "9 pages, 2 figures. Submitted to RA-L",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09170v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "PIFF: A Physics-Informed Generative Flow Model for Real-Time Flood Depth Mapping",
      "authors": [
        "ChunLiang Wu",
        "Tsunhua Yang",
        "Hungying Chen"
      ],
      "arxiv_id": "2511.09130v1",
      "summary": "Flood mapping is crucial for assessing and mitigating flood impacts, yet traditional methods like numerical modeling and aerial photography face limitations in efficiency and reliability. To address these challenges, we propose PIFF, a physics-informed, flow-based generative neural network for near real-time flood depth estimation. Built on an image-to-image generative framework, it efficiently maps Digital Elevation Models (DEM) to flood depth predictions. The model is conditioned on a simplified inundation model (SPM) that embeds hydrodynamic priors into the training process. Additionally, a transformer-based rainfall encoder captures temporal dependencies in precipitation. Integrating physics-informed constraints with data-driven learning, PIFF captures the causal relationships between rainfall, topography, SPM, and flooding, replacing costly simulations with accurate, real-time flood maps. Using a 26 km study area in Tainan, Taiwan, with 182 rainfall scenarios ranging from 24 mm to 720 mm over 24 hours, our results demonstrate that PIFF offers an effective, data-driven alternative for flood prediction and response.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09130v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Neural B-frame Video Compression with Bi-directional Reference Harmonization",
      "authors": [
        "Yuxi Liu",
        "Dengchao Jin",
        "Shuai Huo",
        "Jiawen Gu",
        "Chao Zhou",
        "Huihui Bai",
        "Ming Lu",
        "Zhan Ma"
      ],
      "arxiv_id": "2511.08938v1",
      "summary": "Neural video compression (NVC) has made significant progress in recent years, while neural B-frame video compression (NBVC) remains underexplored compared to P-frame compression. NBVC can adopt bi-directional reference frames for better compression performance. However, NBVC's hierarchical coding may complicate continuous temporal prediction, especially at some hierarchical levels with a large frame span, which could cause the contribution of the two reference frames to be unbalanced. To optimize reference information utilization, we propose a novel NBVC method, termed Bi-directional Reference Harmonization Video Compression (BRHVC), with the proposed Bi-directional Motion Converge (BMC) and Bi-directional Contextual Fusion (BCF). BMC converges multiple optical flows in motion compression, leading to more accurate motion compensation on a larger scale. Then BCF explicitly models the weights of reference contexts under the guidance of motion compensation accuracy. With more efficient motions and contexts, BRHVC can effectively harmonize bi-directional references. Experimental results indicate that our BRHVC outperforms previous state-of-the-art NVC methods, even surpassing the traditional coding, VTM-RA (under random access configuration), on the HEVC datasets. The source code is released at https://github.com/kwai/NVC.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08938v1",
      "code_links": [
        {
          "url": "https://github.com/kwai/NVC",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "optical flow"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "XPRESS: X-Band Radar Place Recognition via Elliptical Scan Shaping",
      "authors": [
        "Hyesu Jang",
        "Wooseong Yang",
        "Ayoung Kim",
        "Dongje Lee",
        "Hanguen Kim"
      ],
      "arxiv_id": "2511.08863v1",
      "summary": "X-band radar serves as the primary sensor on maritime vessels, however, its application in autonomous navigation has been limited due to low sensor resolution and insufficient information content. To enable X-band radar-only autonomous navigation in maritime environments, this paper proposes a place recognition algorithm specifically tailored for X-band radar, incorporating an object density-based rule for efficient candidate selection and intentional degradation of radar detections to achieve robust retrieval performance. The proposed algorithm was evaluated on both public maritime radar datasets and our own collected dataset, and its performance was compared against state-of-the-art radar place recognition methods. An ablation study was conducted to assess the algorithm's performance sensitivity with respect to key parameters.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-12",
      "updated": "2025-11-12",
      "comment": "9 pages, 9 figures, Published in IEEE RA-L",
      "doi": "10.1109/LRA.2025.3627090",
      "journal_ref": "IEEE Robotics and Automation Letters, vol. 10, no. 12, pp. 13121-13128, Dec. 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.08863v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Enhancing Rotation-Invariant 3D Learning with Global Pose Awareness and Attention Mechanisms",
      "authors": [
        "Jiaxun Guo",
        "Manar Amayri",
        "Nizar Bouguila",
        "Xin Liu",
        "Wentao Fan"
      ],
      "arxiv_id": "2511.08833v1",
      "summary": "Recent advances in rotation-invariant (RI) learning for 3D point clouds typically replace raw coordinates with handcrafted RI features to ensure robustness under arbitrary rotations. However, these approaches often suffer from the loss of global pose information, making them incapable of distinguishing geometrically similar but spatially distinct structures. We identify that this limitation stems from the restricted receptive field in existing RI methods, leading to Wing-tip feature collapse, a failure to differentiate symmetric components (e.g., left and right airplane wings) due to indistinguishable local geometries. To overcome this challenge, we introduce the Shadow-informed Pose Feature (SiPF), which augments local RI descriptors with a globally consistent reference point (referred to as the 'shadow') derived from a learned shared rotation. This mechanism enables the model to preserve global pose awareness while maintaining rotation invariance. We further propose Rotation-invariant Attention Convolution (RIAttnConv), an attention-based operator that integrates SiPFs into the feature aggregation process, thereby enhancing the model's capacity to distinguish structurally similar components. Additionally, we design a task-adaptive shadow locating module based on the Bingham distribution over unit quaternions, which dynamically learns the optimal global rotation for constructing consistent shadows. Extensive experiments on 3D classification and part segmentation benchmarks demonstrate that our approach substantially outperforms existing RI methods, particularly in tasks requiring fine-grained spatial discrimination under arbitrary rotations.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "14 pages, 6 gigures,AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08833v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Information-Driven Fault Detection and Identification for Multi-Agent Spacecraft Systems: Collaborative On-Orbit Inspection Mission",
      "authors": [
        "Akshita Gupta",
        "Arna Bhardwaj",
        "Yashwanth Kumar Nakka",
        "Changrak Choi",
        "Amir Rahmani"
      ],
      "arxiv_id": "2511.08752v1",
      "summary": "This work presents a global-to-local, task-aware fault detection and identification (FDI) framework for multi-spacecraft systems conducting collaborative inspection missions in low Earth orbit. The inspection task is represented by a global information-driven cost functional that integrates the sensor model, spacecraft poses, and mission-level information-gain objectives. This formulation links guidance, control, and FDI by using the same cost function to drive both global task allocation and local sensing or motion decisions. Fault detection is achieved through comparisons between expected and observed task metrics, while higher-order cost-gradient measures enable the identification of faults among sensors, actuators, and state estimators. An adaptive thresholding mechanism captures the time-varying inspection geometry and dynamic mission conditions. Simulation results for representative multi-spacecraft inspection scenarios demonstrate the reliability of fault localization and classification under uncertainty, providing a unified, information-driven foundation for resilient autonomous inspection architectures.",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.MA",
        "cs.RO"
      ],
      "primary_category": "eess.SY",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "AIAA Book Chapter (accepted)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08752v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "ATOM-CBF: Adaptive Safe Perception-Based Control under Out-of-Distribution Measurements",
      "authors": [
        "Kai S. Yun",
        "Navid Azizan"
      ],
      "arxiv_id": "2511.08741v2",
      "summary": "Ensuring the safety of real-world systems is challenging, especially when they rely on learned perception modules to infer the system state from high-dimensional sensor data. These perception modules are vulnerable to epistemic uncertainty, often failing when encountering out-of-distribution (OoD) measurements not seen during training. To address this gap, we introduce ATOM-CBF (Adaptive-To-OoD-Measurement Control Barrier Function), a novel safe control framework that explicitly computes and adapts to the epistemic uncertainty from OoD measurements, without the need for ground-truth labels or information on distribution shifts. Our approach features two key components: (1) an OoD-aware adaptive perception error margin and (2) a safety filter that integrates this adaptive error margin, enabling the filter to adjust its conservatism in real-time. We provide empirical validation in simulations, demonstrating that ATOM-CBF maintains safety for an F1Tenth vehicle with LiDAR scans and a quadruped robot with RGB images.",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-11",
      "updated": "2025-11-13",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08741v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "quadruped"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Safe and Optimal Learning from Preferences via Weighted Temporal Logic with Applications in Robotics and Formula 1",
      "authors": [
        "Ruya Karagulle",
        "Cristian-Ioan Vasile",
        "Necmiye Ozay"
      ],
      "arxiv_id": "2511.08502v1",
      "summary": "Autonomous systems increasingly rely on human feedback to align their behavior, expressed as pairwise comparisons, rankings, or demonstrations. While existing methods can adapt behaviors, they often fail to guarantee safety in safety-critical domains. We propose a safety-guaranteed, optimal, and efficient approach to solve the learning problem from preferences, rankings, or demonstrations using Weighted Signal Temporal Logic (WSTL). WSTL learning problems, when implemented naively, lead to multi-linear constraints in the weights to be learned. By introducing structural pruning and log-transform procedures, we reduce the problem size and recast the problem as a Mixed-Integer Linear Program while preserving safety guarantees. Experiments on robotic navigation and real-world Formula 1 data demonstrate that the method effectively captures nuanced preferences and models complex task objectives.",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "8 pages, 2 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08502v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Retrospective motion correction in MRI using disentangled embeddings",
      "authors": [
        "Qi Wang",
        "Veronika Ecker",
        "Marcel Früh",
        "Sergios Gatidis",
        "Thomas Küstner"
      ],
      "arxiv_id": "2511.08365v1",
      "summary": "Physiological motion can affect the diagnostic quality of magnetic resonance imaging (MRI). While various retrospective motion correction methods exist, many struggle to generalize across different motion types and body regions. In particular, machine learning (ML)-based corrections are often tailored to specific applications and datasets. We hypothesize that motion artifacts, though diverse, share underlying patterns that can be disentangled and exploited. To address this, we propose a hierarchical vector-quantized (VQ) variational auto-encoder that learns a disentangled embedding of motion-to-clean image features. A codebook is deployed to capture finite collection of motion patterns at multiple resolutions, enabling coarse-to-fine correction. An auto-regressive model is trained to learn the prior distribution of motion-free images and is used at inference to guide the correction process. Unlike conventional approaches, our method does not require artifact-specific training and can generalize to unseen motion patterns. We demonstrate the approach on simulated whole-body motion artifacts and observe robust correction across varying motion severity. Our results suggest that the model effectively disentangled physical motion of the simulated motion-effective scans, therefore, improving the generalizability of the ML-based MRI motion correction. Our work of disentangling the motion features shed a light on its potential application across anatomical regions and motion types.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "5 pages, 3 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08365v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "whole-body motion"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation",
      "authors": [
        "Jae Joong Lee",
        "Bedrich Benes"
      ],
      "arxiv_id": "2511.08258v1",
      "summary": "Generating ground-level images from aerial views is a challenging task due to extreme viewpoint disparity, occlusions, and a limited field of view. We introduce Top2Ground, a novel diffusion-based method that directly generates photorealistic ground-view images from aerial input images without relying on intermediate representations such as depth maps or 3D voxels. Specifically, we condition the denoising process on a joint representation of VAE-encoded spatial features (derived from aerial RGB images and an estimated height map) and CLIP-based semantic embeddings. This design ensures the generation is both geometrically constrained by the scene's 3D structure and semantically consistent with its content. We evaluate Top2Ground on three diverse datasets: CVUSA, CVACT, and the Auto Arborist. Our approach shows 7.3% average improvement in SSIM across three benchmark datasets, showing Top2Ground can robustly handle both wide and narrow fields of view, highlighting its strong generalization capabilities.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08258v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "height map"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Pixel-level Quality Assessment for Oriented Object Detection",
      "authors": [
        "Yunhui Zhu",
        "Buliao Huang"
      ],
      "arxiv_id": "2511.08186v1",
      "summary": "Modern oriented object detectors typically predict a set of bounding boxes and select the top-ranked ones based on estimated localization quality. Achieving high detection performance requires that the estimated quality closely aligns with the actual localization accuracy. To this end, existing approaches predict the Intersection over Union (IoU) between the predicted and ground-truth (GT) boxes as a proxy for localization quality. However, box-level IoU prediction suffers from a structural coupling issue: since the predicted box is derived from the detector's internal estimation of the GT box, the predicted IoU--based on their similarity--can be overestimated for poorly localized boxes. To overcome this limitation, we propose a novel Pixel-level Quality Assessment (PQA) framework, which replaces box-level IoU prediction with the integration of pixel-level spatial consistency. PQA measures the alignment between each pixel's relative position to the predicted box and its corresponding position to the GT box. By operating at the pixel level, PQA avoids directly comparing the predicted box with the estimated GT box, thereby eliminating the inherent similarity bias in box-level IoU prediction. Furthermore, we introduce a new integration metric that aggregates pixel-level spatial consistency into a unified quality score, yielding a more accurate approximation of the actual localization quality. Extensive experiments on HRSC2016 and DOTA demonstrate that PQA can be seamlessly integrated into various oriented object detectors, consistently improving performance (e.g., +5.96% AP$_{50:95}$ on Rotated RetinaNet and +2.32% on STD).",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08186v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "WarpGAN: Warping-Guided 3D GAN Inversion with Style-Based Novel View Inpainting",
      "authors": [
        "Kaitao Huang",
        "Yan Yan",
        "Jing-Hao Xue",
        "Hanzi Wang"
      ],
      "arxiv_id": "2511.08178v1",
      "summary": "3D GAN inversion projects a single image into the latent space of a pre-trained 3D GAN to achieve single-shot novel view synthesis, which requires visible regions with high fidelity and occluded regions with realism and multi-view consistency. However, existing methods focus on the reconstruction of visible regions, while the generation of occluded regions relies only on the generative prior of 3D GAN. As a result, the generated occluded regions often exhibit poor quality due to the information loss caused by the low bit-rate latent code. To address this, we introduce the warping-and-inpainting strategy to incorporate image inpainting into 3D GAN inversion and propose a novel 3D GAN inversion method, WarpGAN. Specifically, we first employ a 3D GAN inversion encoder to project the single-view image into a latent code that serves as the input to 3D GAN. Then, we perform warping to a novel view using the depth map generated by 3D GAN. Finally, we develop a novel SVINet, which leverages the symmetry prior and multi-view image correspondence w.r.t. the same latent code to perform inpainting of occluded regions in the warped image. Quantitative and qualitative experiments demonstrate that our method consistently outperforms several state-of-the-art methods.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08178v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "novel view synthesis"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "VLMDiff: Leveraging Vision-Language Models for Multi-Class Anomaly Detection with Diffusion",
      "authors": [
        "Samet Hicsonmez",
        "Abd El Rahman Shabayek",
        "Djamila Aouada"
      ],
      "arxiv_id": "2511.08173v1",
      "summary": "Detecting visual anomalies in diverse, multi-class real-world images is a significant challenge. We introduce \\ours, a novel unsupervised multi-class visual anomaly detection framework. It integrates a Latent Diffusion Model (LDM) with a Vision-Language Model (VLM) for enhanced anomaly localization and detection. Specifically, a pre-trained VLM with a simple prompt extracts detailed image descriptions, serving as additional conditioning for LDM training. Current diffusion-based methods rely on synthetic noise generation, limiting their generalization and requiring per-class model training, which hinders scalability. \\ours, however, leverages VLMs to obtain normal captions without manual annotations or additional training. These descriptions condition the diffusion model, learning a robust normal image feature representation for multi-class anomaly detection. Our method achieves competitive performance, improving the pixel-level Per-Region-Overlap (PRO) metric by up to 25 points on the Real-IAD dataset and 8 points on the COCO-AD dataset, outperforming state-of-the-art diffusion-based approaches. Code is available at https://github.com/giddyyupp/VLMDiff.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "WACV 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08173v1",
      "code_links": [
        {
          "url": "https://github.com/giddyyupp/VLMDiff",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "I2E: Real-Time Image-to-Event Conversion for High-Performance Spiking Neural Networks",
      "authors": [
        "Ruichen Ma",
        "Liwei Meng",
        "Guanchao Qiao",
        "Ning Ning",
        "Yang Liu",
        "Shaogang Hu"
      ],
      "arxiv_id": "2511.08065v1",
      "summary": "Spiking neural networks (SNNs) promise highly energy-efficient computing, but their adoption is hindered by a critical scarcity of event-stream data. This work introduces I2E, an algorithmic framework that resolves this bottleneck by converting static images into high-fidelity event streams. By simulating microsaccadic eye movements with a highly parallelized convolution, I2E achieves a conversion speed over 300x faster than prior methods, uniquely enabling on-the-fly data augmentation for SNN training. The framework's effectiveness is demonstrated on large-scale benchmarks. An SNN trained on the generated I2E-ImageNet dataset achieves a state-of-the-art accuracy of 60.50%. Critically, this work establishes a powerful sim-to-real paradigm where pre-training on synthetic I2E data and fine-tuning on the real-world CIFAR10-DVS dataset yields an unprecedented accuracy of 92.5%. This result validates that synthetic event data can serve as a high-fidelity proxy for real sensor data, bridging a long-standing gap in neuromorphic engineering. By providing a scalable solution to the data problem, I2E offers a foundational toolkit for developing high-performance neuromorphic systems. The open-source algorithm and all generated datasets are provided to accelerate research in the field.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "AAAI-26 Oral",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08065v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "sim-to-real"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Generalized-Scale Object Counting with Gradual Query Aggregation",
      "authors": [
        "Jer Pelhan",
        "Alan Lukezic",
        "Matej Kristan"
      ],
      "arxiv_id": "2511.08048v1",
      "summary": "Few-shot detection-based counters estimate the number of instances in the image specified only by a few test-time exemplars. A common approach to localize objects across multiple sizes is to merge backbone features of different resolutions. Furthermore, to enable small object detection in densely populated regions, the input image is commonly upsampled and tiling is applied to cope with the increased computational and memory requirements. Because of these ad-hoc solutions, existing counters struggle with images containing diverse-sized objects and densely populated regions of small objects. We propose GECO2, an end-to-end few-shot counting and detection method that explicitly addresses the object scale issues. A new dense query representation gradually aggregates exemplar-specific feature information across scales that leads to high-resolution dense queries that enable detection of large as well as small objects. GECO2 surpasses state-of-the-art few-shot counters in counting as well as detection accuracy by 10% while running 3x times faster at smaller GPU memory footprint.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "Accepted to AAAI2026, code: https://github.com/jerpelhan/GECO2/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08048v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "running"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "USV Obstacles Detection and Tracking in Marine Environments",
      "authors": [
        "Yara AlaaEldin",
        "Enrico Simetti",
        "Francesca Odone"
      ],
      "arxiv_id": "2511.07950v1",
      "summary": "Developing a robust and effective obstacle detection and tracking system for Unmanned Surface Vehicle (USV) at marine environments is a challenging task. Research efforts have been made in this area during the past years by GRAAL lab at the university of Genova that resulted in a methodology for detecting and tracking obstacles on the image plane and, then, locating them in the 3D LiDAR point cloud. In this work, we continue on the developed system by, firstly, evaluating its performance on recently published marine datasets. Then, we integrate the different blocks of the system on ROS platform where we could test it in real-time on synchronized LiDAR and camera data collected in various marine conditions available in the MIT marine datasets. We present a thorough experimental analysis of the results obtained using two approaches; one that uses sensor fusion between the camera and LiDAR to detect and track the obstacles and the other uses only the LiDAR point cloud for the detection and tracking. In the end, we propose a hybrid approach that merges the advantages of both approaches to build an informative obstacles map of the surrounding environment to the USV.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07950v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Sparse3DPR: Training-Free 3D Hierarchical Scene Parsing and Task-Adaptive Subgraph Reasoning from Sparse RGB Views",
      "authors": [
        "Haida Feng",
        "Hao Wei",
        "Zewen Xu",
        "Haolin Wang",
        "Chade Li",
        "Yihong Wu"
      ],
      "arxiv_id": "2511.07813v1",
      "summary": "Recently, large language models (LLMs) have been explored widely for 3D scene understanding. Among them, training-free approaches are gaining attention for their flexibility and generalization over training-based methods. However, they typically struggle with accuracy and efficiency in practical deployment. To address the problems, we propose Sparse3DPR, a novel training-free framework for open-ended scene understanding, which leverages the reasoning capabilities of pre-trained LLMs and requires only sparse-view RGB inputs. Specifically, we introduce a hierarchical plane-enhanced scene graph that supports open vocabulary and adopts dominant planar structures as spatial anchors, which enables clearer reasoning chains and more reliable high-level inferences. Furthermore, we design a task-adaptive subgraph extraction method to filter query-irrelevant information dynamically, reducing contextual noise and improving 3D scene reasoning efficiency and accuracy. Experimental results demonstrate the superiority of Sparse3DPR, which achieves a 28.7% EM@1 improvement and a 78.2% speedup compared with ConceptGraphs on the Space3D-Bench. Moreover, Sparse3DPR obtains comparable performance to training-based methods on ScanQA, with additional real-world experiments confirming its robustness and generalization capability.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07813v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Navigating the Wild: Pareto-Optimal Visual Decision-Making in Image Space",
      "authors": [
        "Durgakant Pushp",
        "Weizhe Chen",
        "Zheng Chen",
        "Chaomin Luo",
        "Jason M. Gregory",
        "Lantao Liu"
      ],
      "arxiv_id": "2511.07750v1",
      "summary": "Navigating complex real-world environments requires semantic understanding and adaptive decision-making. Traditional reactive methods without maps often fail in cluttered settings, map-based approaches demand heavy mapping effort, and learning-based solutions rely on large datasets with limited generalization. To address these challenges, we present Pareto-Optimal Visual Navigation, a lightweight image-space framework that combines data-driven semantics, Pareto-optimal decision-making, and visual servoing for real-time navigation.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07750v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "RoboTAG: End-to-end Robot Configuration Estimation via Topological Alignment Graph",
      "authors": [
        "Yifan Liu",
        "Fangneng Zhan",
        "Wanhua Li",
        "Haowen Sun",
        "Katerina Fragkiadaki",
        "Hanspeter Pfister"
      ],
      "arxiv_id": "2511.07717v1",
      "summary": "Estimating robot pose from a monocular RGB image is a challenge in robotics and computer vision. Existing methods typically build networks on top of 2D visual backbones and depend heavily on labeled data for training, which is often scarce in real-world scenarios, causing a sim-to-real gap. Moreover, these approaches reduce the 3D-based problem to 2D domain, neglecting the 3D priors. To address these, we propose Robot Topological Alignment Graph (RoboTAG), which incorporates a 3D branch to inject 3D priors while enabling co-evolution of the 2D and 3D representations, alleviating the reliance on labels. Specifically, the RoboTAG consists of a 3D branch and a 2D branch, where nodes represent the states of the camera and robot system, and edges capture the dependencies between these variables or denote alignments between them. Closed loops are then defined in the graph, on which a consistency supervision across branches can be applied. This design allows us to utilize in-the-wild images as training data without annotations. Experimental results demonstrate that our method is effective across robot types, highlighting its potential to alleviate the data bottleneck in robotics.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-11",
      "updated": "2025-11-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07717v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "sim-to-real"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Cross Modal Fine-Grained Alignment via Granularity-Aware and Region-Uncertain Modeling",
      "authors": [
        "Jiale Liu",
        "Haoming Zhou",
        "Yishu Liu",
        "Bingzhi Chen",
        "Yuncheng Jiang"
      ],
      "arxiv_id": "2511.07710v3",
      "summary": "Fine-grained image-text alignment is a pivotal challenge in multimodal learning, underpinning key applications such as visual question answering, image captioning, and vision-language navigation. Unlike global alignment, fine-grained alignment requires precise correspondence between localized visual regions and textual tokens, often hindered by noisy attention mechanisms and oversimplified modeling of cross-modal relationships. In this work, we identify two fundamental limitations of existing approaches: the lack of robust intra-modal mechanisms to assess the significance of visual and textual tokens, leading to poor generalization in complex scenes; and the absence of fine-grained uncertainty modeling, which fails to capture the one-to-many and many-to-one nature of region-word correspondences. To address these issues, we propose a unified approach that incorporates significance-aware and granularity-aware modeling and region-level uncertainty modeling. Our method leverages modality-specific biases to identify salient features without relying on brittle cross-modal attention, and represents region features as a mixture of Gaussian distributions to capture fine-grained uncertainty. Extensive experiments on Flickr30K and MS-COCO demonstrate that our approach achieves state-of-the-art performance across various backbone architectures, significantly enhancing the robustness and interpretability of fine-grained image-text alignment.",
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-11",
      "updated": "2025-11-29",
      "comment": "10 pages, 6 figures, accepted by AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07710v3",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "CAVER: Curious Audiovisual Exploring Robot",
      "authors": [
        "Luca Macesanu",
        "Boueny Folefack",
        "Samik Singh",
        "Ruchira Ray",
        "Ben Abbatematteo",
        "Roberto Martín-Martín"
      ],
      "arxiv_id": "2511.07619v1",
      "summary": "Multimodal audiovisual perception can enable new avenues for robotic manipulation, from better material classification to the imitation of demonstrations for which only audio signals are available (e.g., playing a tune by ear). However, to unlock such multimodal potential, robots need to learn the correlations between an object's visual appearance and the sound it generates when they interact with it. Such an active sensorimotor experience requires new interaction capabilities, representations, and exploration methods to guide the robot in efficiently building increasingly rich audiovisual knowledge. In this work, we present CAVER, a novel robot that builds and utilizes rich audiovisual representations of objects. CAVER includes three novel contributions: 1) a novel 3D printed end-effector, attachable to parallel grippers, that excites objects' audio responses, 2) an audiovisual representation that combines local and global appearance information with sound features, and 3) an exploration algorithm that uses and builds the audiovisual representation in a curiosity-driven manner that prioritizes interacting with high uncertainty objects to obtain good coverage of surprising audio with fewer interactions. We demonstrate that CAVER builds rich representations in different scenarios more efficiently than several exploration baselines, and that the learned audiovisual representation leads to significant improvements in material classification and the imitation of audio-only human demonstrations. https://caver-bot.github.io/",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "9 pages, 6 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07619v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Preparation of Fractal-Inspired Computational Architectures for Advanced Large Language Model Analysis",
      "authors": [
        "Yash Mittal",
        "Dmitry Ignatov",
        "Radu Timofte"
      ],
      "arxiv_id": "2511.07329v1",
      "summary": "It introduces FractalNet, a fractal-inspired computational architectures for advanced large language model analysis that mainly challenges model diversity on a large scale in an efficient manner. The new set-up involves a template-driven generator, runner, and evaluation framework that, through systematic permutations of convolutional, normalization, activation, and dropout layers, can create more than 1,200 variants of neural networks. Fractal templates allow for structural recursion and multi-column pathways, thus, models become deeper and wider in a balanced way. Training utilizes PyTorch, Automatic Mixed Precision (AMP), and gradient checkpointing and is carried out on the CIFAR-10 dataset for five epochs. The outcomes show that fractal-based architectures are capable of strong performance and are computationally efficient. The paper positions fractal design as a feasible and resource-efficient method of automated architecture exploration.",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07329v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "AMP"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "8_physics_animation"
      ]
    },
    {
      "title": "4DSTR: Advancing Generative 4D Gaussians with Spatial-Temporal Rectification for High-Quality and Consistent 4D Generation",
      "authors": [
        "Mengmeng Liu",
        "Jiuming Liu",
        "Yunpeng Zhang",
        "Jiangtao Li",
        "Michael Ying Yang",
        "Francesco Nex",
        "Hao Cheng"
      ],
      "arxiv_id": "2511.07241v1",
      "summary": "Remarkable advances in recent 2D image and 3D shape generation have induced a significant focus on dynamic 4D content generation. However, previous 4D generation methods commonly struggle to maintain spatial-temporal consistency and adapt poorly to rapid temporal variations, due to the lack of effective spatial-temporal modeling. To address these problems, we propose a novel 4D generation network called 4DSTR, which modulates generative 4D Gaussian Splatting with spatial-temporal rectification. Specifically, temporal correlation across generated 4D sequences is designed to rectify deformable scales and rotations and guarantee temporal consistency. Furthermore, an adaptive spatial densification and pruning strategy is proposed to address significant temporal variations by dynamically adding or deleting Gaussian points with the awareness of their pre-frame movements. Extensive experiments demonstrate that our 4DSTR achieves state-of-the-art performance in video-to-4D generation, excelling in reconstruction quality, spatial-temporal consistency, and adaptation to rapid temporal movements.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "Accepted by AAAI 2026.The first two authors contributed equally",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07241v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "gaussian splatting"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Breaking the Stealth-Potency Trade-off in Clean-Image Backdoors with Generative Trigger Optimization",
      "authors": [
        "Binyan Xu",
        "Fan Yang",
        "Di Tang",
        "Xilin Dai",
        "Kehuan Zhang"
      ],
      "arxiv_id": "2511.07210v2",
      "summary": "Clean-image backdoor attacks, which use only label manipulation in training datasets to compromise deep neural networks, pose a significant threat to security-critical applications. A critical flaw in existing methods is that the poison rate required for a successful attack induces a proportional, and thus noticeable, drop in Clean Accuracy (CA), undermining their stealthiness. This paper presents a new paradigm for clean-image attacks that minimizes this accuracy degradation by optimizing the trigger itself. We introduce Generative Clean-Image Backdoors (GCB), a framework that uses a conditional InfoGAN to identify naturally occurring image features that can serve as potent and stealthy triggers. By ensuring these triggers are easily separable from benign task-related features, GCB enables a victim model to learn the backdoor from an extremely small set of poisoned examples, resulting in a CA drop of less than 1%. Our experiments demonstrate GCB's remarkable versatility, successfully adapting to six datasets, five architectures, and four tasks, including the first demonstration of clean-image backdoors in regression and segmentation. GCB also exhibits resilience against most of the existing backdoor defenses.",
      "categories": [
        "cs.CV",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-11",
      "comment": "19 pages, 22 figures, 15 tables. To appear in AAAI '26 (Oral). This paper extends the AAAI-2026 version by including the Appendix",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07210v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Geometric implicit neural representations for signed distance functions",
      "authors": [
        "Luiz Schirmer",
        "Tiago Novello",
        "Vinícius da Silva",
        "Guilherme Schardong",
        "Daniel Perazzo",
        "Hélio Lopes",
        "Nuno Gonçalves",
        "Luiz Velho"
      ],
      "arxiv_id": "2511.07206v1",
      "summary": "\\textit{Implicit neural representations} (INRs) have emerged as a promising framework for representing signals in low-dimensional spaces. This survey reviews the existing literature on the specialized INR problem of approximating \\textit{signed distance functions} (SDFs) for surface scenes, using either oriented point clouds or a set of posed images. We refer to neural SDFs that incorporate differential geometry tools, such as normals and curvatures, in their loss functions as \\textit{geometric} INRs. The key idea behind this 3D reconstruction approach is to include additional \\textit{regularization} terms in the loss function, ensuring that the INR satisfies certain global properties that the function should hold -- such as having unit gradient in the case of SDFs. We explore key methodological components, including the definition of INR, the construction of geometric loss functions, and sampling schemes from a differential geometry perspective. Our review highlights the significant advancements enabled by geometric INRs in surface reconstruction from oriented point clouds and posed images.",
      "categories": [
        "cs.CV",
        "cs.CG",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "",
      "doi": "10.1016/j.cag.2024.104085",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07206v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Achieving Effective Virtual Reality Interactions via Acoustic Gesture Recognition based on Large Language Models",
      "authors": [
        "Xijie Zhang",
        "Fengliang He",
        "Hong-Ning Dai"
      ],
      "arxiv_id": "2511.07085v1",
      "summary": "Natural and efficient interaction remains a critical challenge for virtual reality and augmented reality (VR/AR) systems. Vision-based gesture recognition suffers from high computational cost, sensitivity to lighting conditions, and privacy leakage concerns. Acoustic sensing provides an attractive alternative: by emitting inaudible high-frequency signals and capturing their reflections, channel impulse response (CIR) encodes how gestures perturb the acoustic field in a low-cost and user-transparent manner. However, existing CIR-based gesture recognition methods often rely on extensive training of models on large labeled datasets, making them unsuitable for few-shot VR scenarios. In this work, we propose the first framework that leverages large language models (LLMs) for CIR-based gesture recognition in VR/AR systems. Despite LLMs' strengths, it is non-trivial to achieve few-shot and zero-shot learning of CIR gestures due to their inconspicuous features. To tackle this challenge, we collect differential CIR rather than original CIR data. Moreover, we construct a real-world dataset collected from 10 participants performing 15 gestures across three categories (digits, letters, and shapes), with 10 repetitions each. We then conduct extensive experiments on this dataset using an LLM-adopted classifier. Results show that our LLM-based framework achieves accuracy comparable to classical machine learning baselines, while requiring no domain-specific retraining.",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.HC",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "5 pages, 4 figures, 1 table, under review at ICASSP 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07085v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "PULSE"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "8_physics_animation"
      ]
    },
    {
      "title": "Mono3DVG-EnSD: Enhanced Spatial-aware and Dimension-decoupled Text Encoding for Monocular 3D Visual Grounding",
      "authors": [
        "Yuzhen Li",
        "Min Liu",
        "Zhaoyang Li",
        "Yuan Bian",
        "Xueping Wang",
        "Erbo Zhai",
        "Yaonan Wang"
      ],
      "arxiv_id": "2511.06908v1",
      "summary": "Monocular 3D Visual Grounding (Mono3DVG) is an emerging task that locates 3D objects in RGB images using text descriptions with geometric cues. However, existing methods face two key limitations. Firstly, they often over-rely on high-certainty keywords that explicitly identify the target object while neglecting critical spatial descriptions. Secondly, generalized textual features contain both 2D and 3D descriptive information, thereby capturing an additional dimension of details compared to singular 2D or 3D visual features. This characteristic leads to cross-dimensional interference when refining visual features under text guidance. To overcome these challenges, we propose Mono3DVG-EnSD, a novel framework that integrates two key components: the CLIP-Guided Lexical Certainty Adapter (CLIP-LCA) and the Dimension-Decoupled Module (D2M). The CLIP-LCA dynamically masks high-certainty keywords while retaining low-certainty implicit spatial descriptions, thereby forcing the model to develop a deeper understanding of spatial relationships in captions for object localization. Meanwhile, the D2M decouples dimension-specific (2D/3D) textual features from generalized textual features to guide corresponding visual features at same dimension, which mitigates cross-dimensional interference by ensuring dimensionally-consistent cross-modal interactions. Through comprehensive comparisons and ablation studies on the Mono3DRefer dataset, our method achieves state-of-the-art (SOTA) performance across all metrics. Notably, it improves the challenging Far(Acc@0.5) scenario by a significant +13.54%.",
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "10 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06908v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Gaussian-Augmented Physics Simulation and System Identification with Complex Colliders",
      "authors": [
        "Federico Vasile",
        "Ri-Zhao Qiu",
        "Lorenzo Natale",
        "Xiaolong Wang"
      ],
      "arxiv_id": "2511.06846v1",
      "summary": "System identification involving the geometry, appearance, and physical properties from video observations is a challenging task with applications in robotics and graphics. Recent approaches have relied on fully differentiable Material Point Method (MPM) and rendering for simultaneous optimization of these properties. However, they are limited to simplified object-environment interactions with planar colliders and fail in more challenging scenarios where objects collide with non-planar surfaces. We propose AS-DiffMPM, a differentiable MPM framework that enables physical property estimation with arbitrarily shaped colliders. Our approach extends existing methods by incorporating a differentiable collision handling mechanism, allowing the target object to interact with complex rigid bodies while maintaining end-to-end optimization. We show AS-DiffMPM can be easily interfaced with various novel view synthesis methods as a framework for system identification from visual observations.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "Accepted to NeurIPS 2025. Project website: https://as-diffmpm.github.io/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06846v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "novel view synthesis"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Integrating Reweighted Least Squares with Plug-and-Play Diffusion Priors for Noisy Image Restoration",
      "authors": [
        "Ji Li",
        "Chao Wang"
      ],
      "arxiv_id": "2511.06823v1",
      "summary": "Existing plug-and-play image restoration methods typically employ off-the-shelf Gaussian denoisers as proximal operators within classical optimization frameworks based on variable splitting. Recently, denoisers induced by generative priors have been successfully integrated into regularized optimization methods for image restoration under Gaussian noise. However, their application to non-Gaussian noise--such as impulse noise--remains largely unexplored. In this paper, we propose a plug-and-play image restoration framework based on generative diffusion priors for robust removal of general noise types, including impulse noise. Within the maximum a posteriori (MAP) estimation framework, the data fidelity term is adapted to the specific noise model. Departing from the conventional least-squares loss used for Gaussian noise, we introduce a generalized Gaussian scale mixture-based loss, which approximates a wide range of noise distributions and leads to an $\\ell_q$-norm ($0<q\\leq2$) fidelity term. This optimization problem is addressed using an iteratively reweighted least squares (IRLS) approach, wherein the proximal step involving the generative prior is efficiently performed via a diffusion-based denoiser. Experimental results on benchmark datasets demonstrate that the proposed method effectively removes non-Gaussian impulse noise and achieves superior restoration performance.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "12 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06823v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "PULSE"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "8_physics_animation"
      ]
    },
    {
      "title": "UniADC: A Unified Framework for Anomaly Detection and Classification",
      "authors": [
        "Ximiao Zhang",
        "Min Xu",
        "Zheng Zhang",
        "Junlin Hu",
        "Xiuzhuang Zhou"
      ],
      "arxiv_id": "2511.06644v1",
      "summary": "In this paper, we introduce the task of unified anomaly detection and classification, which aims to simultaneously detect anomalous regions in images and identify their specific categories. Existing methods typically treat anomaly detection and classification as separate tasks, thereby neglecting their inherent correlation, limiting information sharing, and resulting in suboptimal performance. To address this, we propose UniADC, a unified anomaly detection and classification model that can effectively perform both tasks with only a few or even no anomaly images. Specifically, UniADC consists of two key components: a training-free controllable inpainting network and a multi-task discriminator. The inpainting network can synthesize anomaly images of specific categories by repainting normal regions guided by anomaly priors, and can also repaint few-shot anomaly samples to augment the available anomaly data. The multi-task discriminator is then trained on these synthesized samples, enabling precise anomaly detection and classification by aligning fine-grained image features with anomaly-category embeddings. We conduct extensive experiments on three anomaly detection and classification datasets, including MVTec-FS, MTD, and WFDD, and the results demonstrate that UniADC consistently outperforms existing methods in anomaly detection, localization, and classification. The code is available at https://github.com/cnulab/UniADC.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06644v1",
      "code_links": [
        {
          "url": "https://github.com/cnulab/UniADC",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "How Do VLAs Effectively Inherit from VLMs?",
      "authors": [
        "Chuheng Zhang",
        "Rushuai Yang",
        "Xiaoyu Chen",
        "Kaixin Wang",
        "Li Zhao",
        "Yi Chen",
        "Jiang Bian"
      ],
      "arxiv_id": "2511.06619v1",
      "summary": "Vision-language-action (VLA) models hold the promise to attain generalizable embodied control. To achieve this, a pervasive paradigm is to leverage the rich vision-semantic priors of large vision-language models (VLMs). However, the fundamental question persists: How do VLAs effectively inherit the prior knowledge from VLMs? To address this critical question, we introduce a diagnostic benchmark, GrinningFace, an emoji tabletop manipulation task where the robot arm is asked to place objects onto printed emojis corresponding to language instructions. This task design is particularly revealing -- knowledge associated with emojis is ubiquitous in Internet-scale datasets used for VLM pre-training, yet emojis themselves are largely absent from standard robotics datasets. Consequently, they provide a clean proxy: successful task completion indicates effective transfer of VLM priors to embodied control. We implement this diagnostic task in both simulated environment and a real robot, and compare various promising techniques for knowledge transfer. Specifically, we investigate the effects of parameter-efficient fine-tuning, VLM freezing, co-training, predicting discretized actions, and predicting latent actions. Through systematic evaluation, our work not only demonstrates the critical importance of preserving VLM priors for the generalization of VLA but also establishes guidelines for future research in developing truly generalizable embodied AI systems.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-10",
      "updated": "2025-11-10",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06619v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "GHOST: Solving the Traveling Salesman Problem on Graphs of Convex Sets",
      "authors": [
        "Jingtao Tang",
        "Hang Ma"
      ],
      "arxiv_id": "2511.06471v2",
      "summary": "We study GCS-TSP, a new variant of the Traveling Salesman Problem (TSP) defined over a Graph of Convex Sets (GCS) -- a powerful representation for trajectory planning that decomposes the configuration space into convex regions connected by a sparse graph. In this setting, edge costs are not fixed but depend on the specific trajectory selected through each convex region, making classical TSP methods inapplicable. We introduce GHOST, a hierarchical framework that optimally solves the GCS-TSP by combining combinatorial tour search with convex trajectory optimization. GHOST systematically explores tours on a complete graph induced by the GCS, using a novel abstract-path-unfolding algorithm to compute admissible lower bounds that guide best-first search at both the high level (over tours) and the low level (over feasible GCS paths realizing the tour). These bounds provide strong pruning power, enabling efficient search while avoiding unnecessary convex optimization calls. We prove that GHOST guarantees optimality and present a bounded-suboptimal variant for time-critical scenarios. Experiments show that GHOST is orders-of-magnitude faster than unified mixed-integer convex programming baselines for simple cases and uniquely handles complex trajectory planning problems involving high-order continuity constraints and an incomplete GCS.",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "published": "2025-11-09",
      "updated": "2025-11-13",
      "comment": "Accepted to AAAI-2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06471v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "trajectory optimization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "From Demonstrations to Safe Deployment: Path-Consistent Safety Filtering for Diffusion Policies",
      "authors": [
        "Ralf Römer",
        "Julian Balletshofer",
        "Jakob Thumm",
        "Marco Pavone",
        "Angela P. Schoellig",
        "Matthias Althoff"
      ],
      "arxiv_id": "2511.06385v1",
      "summary": "Diffusion policies (DPs) achieve state-of-the-art performance on complex manipulation tasks by learning from large-scale demonstration datasets, often spanning multiple embodiments and environments. However, they cannot guarantee safe behavior, so external safety mechanisms are needed. These, however, alter actions in ways unseen during training, causing unpredictable behavior and performance degradation. To address these problems, we propose path-consistent safety filtering (PACS) for DPs. Our approach performs path-consistent braking on a trajectory computed from the sequence of generated actions. In this way, we keep execution consistent with the policy's training distribution, maintaining the learned, task-completing behavior. To enable a real-time deployment and handle uncertainties, we verify safety using set-based reachability analysis. Our experimental evaluation in simulation and on three challenging real-world human-robot interaction tasks shows that PACS (a) provides formal safety guarantees in dynamic environments, (b) preserves task success rates, and (c) outperforms reactive safety approaches, such as control barrier functions, by up to 68% in terms of task success. Videos are available at our project website: https://tum-lsy.github.io/pacs/.",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-09",
      "updated": "2025-11-09",
      "comment": "Project page: https://tum-lsy.github.io/pacs/. 8 pages, 4 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06385v1",
      "code_links": [
        {
          "url": "https://tum-lsy.github.io/pacs/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Label-Efficient 3D Forest Mapping: Self-Supervised and Transfer Learning for Individual, Structural, and Species Analysis",
      "authors": [
        "Aldino Rizaldy",
        "Fabian Ewald Fassnacht",
        "Ahmed Jamal Afifi",
        "Hua Jiang",
        "Richard Gloaguen",
        "Pedram Ghamisi"
      ],
      "arxiv_id": "2511.06331v1",
      "summary": "Detailed structural and species information on individual tree level is increasingly important to support precision forestry, biodiversity conservation, and provide reference data for biomass and carbon mapping. Point clouds from airborne and ground-based laser scanning are currently the most suitable data source to rapidly derive such information at scale. Recent advancements in deep learning improved segmenting and classifying individual trees and identifying semantic tree components. However, deep learning models typically require large amounts of annotated training data which limits further improvement. Producing dense, high-quality annotations for 3D point clouds, especially in complex forests, is labor-intensive and challenging to scale. We explore strategies to reduce dependence on large annotated datasets using self-supervised and transfer learning architectures. Our objective is to improve performance across three tasks: instance segmentation, semantic segmentation, and tree classification using realistic and operational training sets. Our findings indicate that combining self-supervised learning with domain adaptation significantly enhances instance segmentation compared to training from scratch (AP50 +16.98%), self-supervised learning suffices for semantic segmentation (mIoU +1.79%), and hierarchical transfer learning enables accurate classification of unseen species (Jaccard +6.07%). To simplify use and encourage uptake, we integrated the tasks into a unified framework, streamlining the process from raw point clouds to tree delineation, structural analysis, and species classification. Pretrained models reduce energy consumption and carbon emissions by ~21%. This open-source contribution aims to accelerate operational extraction of individual tree information from laser scanning point clouds to support forestry, biodiversity, and carbon mapping.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-09",
      "updated": "2025-11-09",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06331v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "External Photoreflective Tactile Sensing Based on Surface Deformation Measurement",
      "authors": [
        "Seiichi Yamamoto",
        "Hiroki Ishizuka",
        "Takumi Kawasetsu",
        "Koh Hosoda",
        "Takayuki Kameoka",
        "Kango Yanagida",
        "Takato Horii",
        "Sei Ikeda",
        "Osamu Oshiro"
      ],
      "arxiv_id": "2511.06311v1",
      "summary": "We present a tactile sensing method enabled by the mechanical compliance of soft robots; an externally attachable photoreflective module reads surface deformation of silicone skin to estimate contact force without embedding tactile transducers. Locating the sensor off the contact interface reduces damage risk, preserves softness, and simplifies fabrication and maintenance. We first characterize the optical sensing element and the compliant skin, thendetermine the design of a prototype tactile sensor. Compression experiments validate the approach, exhibiting a monotonic force output relationship consistent with theory, low hysteresis, high repeatability over repeated cycles, and small response indentation speeds. We further demonstrate integration on a soft robotic gripper, where the module reliably detects grasp events. Compared with liquid filled or wireembedded tactile skins, the proposed modular add on architecture enhances durability, reduces wiring complexity, and supports straightforward deployment across diverse robot geometries. Because the sensing principle reads skin strain patterns, it also suggests extensions to other somatosensory cues such as joint angle or actuator state estimation from surface deformation. Overall, leveraging surface compliance with an external optical module provides a practical and robust route to equip soft robots with force perception while preserving structural flexibility and manufacturability, paving the way for robotic applications and safe human robot collaboration.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-09",
      "updated": "2025-11-09",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06311v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "grasp"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Adaptive 3D Reconstruction via Diffusion Priors and Forward Curvature-Matching Likelihood Updates",
      "authors": [
        "Seunghyeok Shin",
        "Dabin Kim",
        "Hongki Lim"
      ],
      "arxiv_id": "2511.06310v1",
      "summary": "Reconstructing high-quality point clouds from images remains challenging in computer vision. Existing generative-model-based approaches, particularly diffusion-model approaches that directly learn the posterior, may suffer from inflexibility -- they require conditioning signals during training, support only a fixed number of input views, and need complete retraining for different measurements. Recent diffusion-based methods have attempted to address this by combining prior models with likelihood updates, but they rely on heuristic fixed step sizes for the likelihood update that lead to slow convergence and suboptimal reconstruction quality. We advance this line of approach by integrating our novel Forward Curvature-Matching (FCM) update method with diffusion sampling. Our method dynamically determines optimal step sizes using only forward automatic differentiation and finite-difference curvature estimates, enabling precise optimization of the likelihood update. This formulation enables high-fidelity reconstruction from both single-view and multi-view inputs, and supports various input modalities through simple operator substitution -- all without retraining. Experiments on ShapeNet and CO3D datasets demonstrate that our method achieves superior reconstruction quality at matched or lower NFEs, yielding higher F-score and lower CD and EMD, validating its efficiency and adaptability for practical applications. Code is available at https://github.com/Seunghyeok0715/FCM",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-09",
      "updated": "2025-11-09",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06310v1",
      "code_links": [
        {
          "url": "https://github.com/Seunghyeok0715/FCM",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Temporal-Guided Visual Foundation Models for Event-Based Vision",
      "authors": [
        "Ruihao Xia",
        "Junhong Cai",
        "Luziwei Leng",
        "Liuyi Wang",
        "Chengju Liu",
        "Ran Cheng",
        "Yang Tang",
        "Pan Zhou"
      ],
      "arxiv_id": "2511.06238v1",
      "summary": "Event cameras offer unique advantages for vision tasks in challenging environments, yet processing asynchronous event streams remains an open challenge. While existing methods rely on specialized architectures or resource-intensive training, the potential of leveraging modern Visual Foundation Models (VFMs) pretrained on image data remains under-explored for event-based vision. To address this, we propose Temporal-Guided VFM (TGVFM), a novel framework that integrates VFMs with our temporal context fusion block seamlessly to bridge this gap. Our temporal block introduces three key components: (1) Long-Range Temporal Attention to model global temporal dependencies, (2) Dual Spatiotemporal Attention for multi-scale frame correlation, and (3) Deep Feature Guidance Mechanism to fuse semantic-temporal features. By retraining event-to-video models on real-world data and leveraging transformer-based VFMs, TGVFM preserves spatiotemporal dynamics while harnessing pretrained representations. Experiments demonstrate SoTA performance across semantic segmentation, depth estimation, and object detection, with improvements of 16%, 21%, and 16% over existing methods, respectively. Overall, this work unlocks the cross-modality potential of image-based VFMs for event-based vision with temporal reasoning. Code is available at https://github.com/XiaRho/TGVFM.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-09",
      "updated": "2025-11-09",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06238v1",
      "code_links": [
        {
          "url": "https://github.com/XiaRho/TGVFM",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "ExpReS-VLA: Specializing Vision-Language-Action Models Through Experience Replay and Retrieval",
      "authors": [
        "Shahram Najam Syed",
        "Yatharth Ahuja",
        "Arthur Jakobsson",
        "Jeff Ichnowski"
      ],
      "arxiv_id": "2511.06202v1",
      "summary": "Vision-Language-Action models such as OpenVLA show impressive zero-shot generalization across robotic manipulation tasks but often fail to adapt efficiently to new deployment environments. In many real-world applications, consistent high performance on a limited set of tasks is more important than broad generalization. We propose ExpReS-VLA, a method for specializing pre-trained VLA models through experience replay and retrieval while preventing catastrophic forgetting. ExpReS-VLA stores compact feature representations from the frozen vision backbone instead of raw image-action pairs, reducing memory usage by approximately 97 percent. During deployment, relevant past experiences are retrieved using cosine similarity and used to guide adaptation, while prioritized experience replay emphasizes successful trajectories. We also introduce Thresholded Hybrid Contrastive Loss, which enables learning from both successful and failed attempts. On the LIBERO simulation benchmark, ExpReS-VLA improves success rates from 82.6 to 93.1 percent on spatial reasoning tasks and from 61 to 72.3 percent on long-horizon tasks. On physical robot experiments with five manipulation tasks, it reaches 98 percent success on both seen and unseen settings, compared to 84.7 and 32 percent for naive fine-tuning. Adaptation takes 31 seconds using 12 demonstrations on a single RTX 5090 GPU, making the approach practical for real robot deployment.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-09",
      "updated": "2025-11-09",
      "comment": "10 pages, 5 figures, submitted to ICRA 2026. Equal contribution by first two authors",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06202v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "MiVID: Multi-Strategic Self-Supervision for Video Frame Interpolation using Diffusion Model",
      "authors": [
        "Priyansh Srivastava",
        "Romit Chatterjee",
        "Abir Sen",
        "Aradhana Behura",
        "Ratnakar Dash"
      ],
      "arxiv_id": "2511.06019v1",
      "summary": "Video Frame Interpolation (VFI) remains a cornerstone in video enhancement, enabling temporal upscaling for tasks like slow-motion rendering, frame rate conversion, and video restoration. While classical methods rely on optical flow and learning-based models assume access to dense ground-truth, both struggle with occlusions, domain shifts, and ambiguous motion. This article introduces MiVID, a lightweight, self-supervised, diffusion-based framework for video interpolation. Our model eliminates the need for explicit motion estimation by combining a 3D U-Net backbone with transformer-style temporal attention, trained under a hybrid masking regime that simulates occlusions and motion uncertainty. The use of cosine-based progressive masking and adaptive loss scheduling allows our network to learn robust spatiotemporal representations without any high-frame-rate supervision. Our framework is evaluated on UCF101-7 and DAVIS-7 datasets. MiVID is trained entirely on CPU using the datasets and 9-frame video segments, making it a low-resource yet highly effective pipeline. Despite these constraints, our model achieves optimal results at just 50 epochs, competitive with several supervised baselines.This work demonstrates the power of self-supervised diffusion priors for temporally coherent frame synthesis and provides a scalable path toward accessible and generalizable VFI systems.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-08",
      "updated": "2025-11-08",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.06019v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "optical flow"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Reperio-rPPG: Relational Temporal Graph Neural Networks for Periodicity Learning in Remote Physiological Measurement",
      "authors": [
        "Ba-Thinh Nguyen",
        "Thach-Ha Ngoc Pham",
        "Hoang-Long Duc Nguyen",
        "Thi-Duyen Ngo",
        "Thanh-Ha Le"
      ],
      "arxiv_id": "2511.05946v1",
      "summary": "Remote photoplethysmography (rPPG) is an emerging contactless physiological sensing technique that leverages subtle color variations in facial videos to estimate vital signs such as heart rate and respiratory rate. This non-invasive method has gained traction across diverse domains, including telemedicine, affective computing, driver fatigue detection, and health monitoring, owing to its scalability and convenience. Despite significant progress in remote physiological signal measurement, a crucial characteristic - the intrinsic periodicity - has often been underexplored or insufficiently modeled in previous approaches, limiting their ability to capture fine-grained temporal dynamics under real-world conditions. To bridge this gap, we propose Reperio-rPPG, a novel framework that strategically integrates Relational Convolutional Networks with a Graph Transformer to effectively capture the periodic structure inherent in physiological signals. Additionally, recognizing the limited diversity of existing rPPG datasets, we further introduce a tailored CutMix augmentation to enhance the model's generalizability. Extensive experiments conducted on three widely used benchmark datasets - PURE, UBFC-rPPG, and MMPD - demonstrate that Reperio-rPPG not only achieves state-of-the-art performance but also exhibits remarkable robustness under various motion (e.g., stationary, rotation, talking, walking) and illumination conditions (e.g., nature, low LED, high LED). The code is publicly available at https://github.com/deconasser/Reperio-rPPG.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-08",
      "updated": "2025-11-08",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05946v1",
      "code_links": [
        {
          "url": "https://github.com/deconasser/Reperio-rPPG",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "walking"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Disentangled Control of Multi-Agent Systems",
      "authors": [
        "Ruoyu Lin",
        "Gennaro Notomista",
        "Magnus Egerstedt"
      ],
      "arxiv_id": "2511.05900v2",
      "summary": "This paper develops a general framework for multi-agent control synthesis, which applies to a wide range of problems with convergence guarantees, regardless of the complexity of the underlying graph topology and the explicit time dependence of the objective function. The proposed framework systematically addresses a particularly challenging problem in multi-agent systems, i.e., decentralization of entangled dynamics among different agents, and it naturally supports multi-objective robotics and real-time implementations. To demonstrate its generality and effectiveness, the framework is implemented across three experiments, namely time-varying leader-follower formation control, decentralized coverage control for time-varying density functions without any approximations, which is a long-standing open problem, and safe formation navigation in dense environments.",
      "categories": [
        "eess.SY",
        "cs.RO"
      ],
      "primary_category": "eess.SY",
      "published": "2025-11-08",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05900v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Open-World 3D Scene Graph Generation for Retrieval-Augmented Reasoning",
      "authors": [
        "Fei Yu",
        "Quan Deng",
        "Shengeng Tang",
        "Yuehua Li",
        "Lechao Cheng"
      ],
      "arxiv_id": "2511.05894v1",
      "summary": "Understanding 3D scenes in open-world settings poses fundamental challenges for vision and robotics, particularly due to the limitations of closed-vocabulary supervision and static annotations. To address this, we propose a unified framework for Open-World 3D Scene Graph Generation with Retrieval-Augmented Reasoning, which enables generalizable and interactive 3D scene understanding. Our method integrates Vision-Language Models (VLMs) with retrieval-based reasoning to support multimodal exploration and language-guided interaction. The framework comprises two key components: (1) a dynamic scene graph generation module that detects objects and infers semantic relationships without fixed label sets, and (2) a retrieval-augmented reasoning pipeline that encodes scene graphs into a vector database to support text/image-conditioned queries. We evaluate our method on 3DSSG and Replica benchmarks across four tasks-scene question answering, visual grounding, instance retrieval, and task planning-demonstrating robust generalization and superior performance in diverse environments. Our results highlight the effectiveness of combining open-vocabulary perception with retrieval-based reasoning for scalable 3D scene understanding.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-08",
      "updated": "2025-11-08",
      "comment": "Accepted by AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05894v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Flow-Guided Implicit Neural Representation for Motion-Aware Dynamic MRI Reconstruction",
      "authors": [
        "Baoqing Li",
        "Yuanyuan Liu",
        "Congcong Liu",
        "Qingyong Zhu",
        "Jing Cheng",
        "Yihang Zhou",
        "Hao Chen",
        "Zhuo-Xu Cui",
        "Dong Liang"
      ],
      "arxiv_id": "2511.16948v1",
      "summary": "Dynamic magnetic resonance imaging (dMRI) captures temporally-resolved anatomy but is often challenged by limited sampling and motion-induced artifacts. Conventional motion-compensated reconstructions typically rely on pre-estimated optical flow, which is inaccurate under undersampling and degrades reconstruction quality. In this work, we propose a novel implicit neural representation (INR) framework that jointly models both the dynamic image sequence and its underlying motion field. Specifically, one INR is employed to parameterize the spatiotemporal image content, while another INR represents the optical flow. The two are coupled via the optical flow equation, which serves as a physics-inspired regularization, in addition to a data consistency loss that enforces agreement with k-space measurements. This joint optimization enables simultaneous recovery of temporally coherent images and motion fields without requiring prior flow estimation. Experiments on dynamic cardiac MRI datasets demonstrate that the proposed method outperforms state-of-the-art motion-compensated and deep learning approaches, achieving superior reconstruction quality, accurate motion estimation, and improved temporal fidelity. These results highlight the potential of implicit joint modeling with flow-regularized constraints for advancing dMRI reconstruction.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-21",
      "updated": "2025-11-21",
      "comment": "10 pages, 7 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16948v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "optical flow"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "DeltaDeno: Zero-Shot Anomaly Generation via Delta-Denoising Attribution",
      "authors": [
        "Chaoran Xu",
        "Chengkan Lv",
        "Qiyu Chen",
        "Yunkang Cao",
        "Feng Zhang",
        "Zhengtao Zhang"
      ],
      "arxiv_id": "2511.16920v1",
      "summary": "Anomaly generation is often framed as few-shot fine-tuning with anomalous samples, which contradicts the scarcity that motivates generation and tends to overfit category priors. We tackle the setting where no real anomaly samples or training are available. We propose Delta-Denoising (DeltaDeno), a training-free zero-shot anomaly generation method that localizes and edits defects by contrasting two diffusion branches driven by a minimal prompt pair under a shared schedule. By accumulating per-step denoising deltas into an image-specific localization map, we obtain a mask to guide the latent inpainting during later diffusion steps and preserve the surrounding context while generating realistic local defects. To improve stability and control, DeltaDeno performs token-level prompt refinement that aligns shared content and strengthens anomaly tokens, and applies a spatial attention bias restricted to anomaly tokens in the predicted region. Experiments on public datasets show that DeltaDeno achieves great generation, realism and consistent gains in downstream detection performance. Code will be made publicly available.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-21",
      "updated": "2025-11-21",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16920v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Single-Pixel Tactile Skin via Compressive Sampling",
      "authors": [
        "Ariel Slepyan",
        "Laura Xing",
        "Rudy Zhang",
        "Nitish Thakor"
      ],
      "arxiv_id": "2511.16898v1",
      "summary": "Development of large-area, high-speed electronic skins is a grand challenge for robotics, prosthetics, and human-machine interfaces, but is fundamentally limited by wiring complexity and data bottlenecks. Here, we introduce Single-Pixel Tactile Skin (SPTS), a paradigm that uses compressive sampling to reconstruct rich tactile information from an entire sensor array via a single output channel. This is achieved through a direct circuit-level implementation where each sensing element, equipped with a miniature microcontroller, contributes a dynamically weighted analog signal to a global sum, performing distributed compressed sensing in hardware. Our flexible, daisy-chainable design simplifies wiring to a few input lines and one output, and significantly reduces measurement requirements compared to raster scanning methods. We demonstrate the system's performance by achieving object classification at an effective 3500 FPS and by capturing transient dynamics, resolving an 8 ms projectile impact into 23 frames. A key feature is the support for adaptive reconstruction, where sensing fidelity scales with measurement time. This allows for rapid contact localization using as little as 7% of total data, followed by progressive refinement to a high-fidelity image - a capability critical for responsive robotic systems. This work offers an efficient pathway towards large-scale tactile intelligence for robotics and human-machine interfaces.",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-21",
      "updated": "2025-11-21",
      "comment": "24 pages, 6 main figures, 6 supplemental figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16898v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Glass Surface Detection: Leveraging Reflection Dynamics in Flash/No-flash Imagery",
      "authors": [
        "Tao Yan",
        "Hao Huang",
        "Yiwei Lu",
        "Zeyu Wang",
        "Ke Xu",
        "Yinghui Wang",
        "Xiaojun Chang",
        "Rynson W. H. Lau"
      ],
      "arxiv_id": "2511.16887v2",
      "summary": "Glass surfaces are ubiquitous in daily life, typically appearing colorless, transparent, and lacking distinctive features. These characteristics make glass surface detection a challenging computer vision task. Existing glass surface detection methods always rely on boundary cues (e.g., window and door frames) or reflection cues to locate glass surfaces, but they fail to fully exploit the intrinsic properties of the glass itself for accurate localization. We observed that in most real-world scenes, the illumination intensity in front of the glass surface differs from that behind it, which results in variations in the reflections visible on the glass surface. Specifically, when standing on the brighter side of the glass and applying a flash towards the darker side, existing reflections on the glass surface tend to disappear. Conversely, while standing on the darker side and applying a flash towards the brighter side, distinct reflections will appear on the glass surface. Based on this phenomenon, we propose NFGlassNet, a novel method for glass surface detection that leverages the reflection dynamics present in flash/no-flash imagery. Specifically, we propose a Reflection Contrast Mining Module (RCMM) for extracting reflections, and a Reflection Guided Attention Module (RGAM) for fusing features from reflection and glass surface for accurate glass surface detection. For learning our network, we also construct a dataset consisting of 3.3K no-flash and flash image pairs captured from various scenes with corresponding ground truth annotations. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods. Our code, model, and dataset will be available upon acceptance of the manuscript.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-21",
      "updated": "2025-12-09",
      "comment": "18 pages, 17 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16887v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Mesh RAG: Retrieval Augmentation for Autoregressive Mesh Generation",
      "authors": [
        "Xiatao Sun",
        "Chen Liang",
        "Qian Wang",
        "Daniel Rakita"
      ],
      "arxiv_id": "2511.16807v1",
      "summary": "3D meshes are a critical building block for applications ranging from industrial design and gaming to simulation and robotics. Traditionally, meshes are crafted manually by artists, a process that is time-intensive and difficult to scale. To automate and accelerate this asset creation, autoregressive models have emerged as a powerful paradigm for artistic mesh generation. However, current methods to enhance quality typically rely on larger models or longer sequences that result in longer generation time, and their inherent sequential nature imposes a severe quality-speed trade-off. This sequential dependency also significantly complicates incremental editing. To overcome these limitations, we propose Mesh RAG, a novel, training-free, plug-and-play framework for autoregressive mesh generation models. Inspired by RAG for language models, our approach augments the generation process by leveraging point cloud segmentation, spatial transformation, and point cloud registration to retrieve, generate, and integrate mesh components. This retrieval-based approach decouples generation from its strict sequential dependency, facilitating efficient and parallelizable inference. We demonstrate the wide applicability of Mesh RAG across various foundational autoregressive mesh generation models, showing it significantly enhances mesh quality, accelerates generation speed compared to sequential part prediction, and enables incremental editing, all without model retraining.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16807v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "SAM 3: Segment Anything with Concepts",
      "authors": [
        "Nicolas Carion",
        "Laura Gustafson",
        "Yuan-Ting Hu",
        "Shoubhik Debnath",
        "Ronghang Hu",
        "Didac Suris",
        "Chaitanya Ryali",
        "Kalyan Vasudev Alwala",
        "Haitham Khedr",
        "Andrew Huang",
        "Jie Lei",
        "Tengyu Ma",
        "Baishan Guo",
        "Arpit Kalla",
        "Markus Marks",
        "Joseph Greer",
        "Meng Wang",
        "Peize Sun",
        "Roman Rädle",
        "Triantafyllos Afouras",
        "Effrosyni Mavroudi",
        "Katherine Xu",
        "Tsung-Han Wu",
        "Yu Zhou",
        "Liliane Momeni",
        "Rishi Hazra",
        "Shuangrui Ding",
        "Sagar Vaze",
        "Francois Porcher",
        "Feng Li",
        "Siyuan Li",
        "Aishwarya Kamath",
        "Ho Kei Cheng",
        "Piotr Dollár",
        "Nikhila Ravi",
        "Kate Saenko",
        "Pengchuan Zhang",
        "Christoph Feichtenhofer"
      ],
      "arxiv_id": "2511.16719v1",
      "summary": "We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., \"yellow school bus\"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16719v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Late-decoupled 3D Hierarchical Semantic Segmentation with Semantic Prototype Discrimination based Bi-branch Supervision",
      "authors": [
        "Shuyu Cao",
        "Chongshou Li",
        "Jie Xu",
        "Tianrui Li",
        "Na Zhao"
      ],
      "arxiv_id": "2511.16650v1",
      "summary": "3D hierarchical semantic segmentation (3DHS) is crucial for embodied intelligence applications that demand a multi-grained and multi-hierarchy understanding of 3D scenes. Despite the progress, previous 3DHS methods have overlooked following two challenges: I) multi-label learning with a parameter-sharing model can lead to multi-hierarchy conflicts in cross-hierarchy optimization, and II) the class imbalance issue is inevitable across multiple hierarchies of 3D scenes, which makes the model performance become dominated by major classes. To address these issues, we propose a novel framework with a primary 3DHS branch and an auxiliary discrimination branch. Specifically, to alleviate the multi-hierarchy conflicts, we propose a late-decoupled 3DHS framework which employs multiple decoders with the coarse-to-fine hierarchical guidance and consistency. The late-decoupled architecture can mitigate the underfitting and overfitting conflicts among multiple hierarchies and can also constrain the class imbalance problem in each individual hierarchy. Moreover, we introduce a 3DHS-oriented semantic prototype based bi-branch supervision mechanism, which additionally learns class-wise discriminative point cloud features and performs mutual supervision between the auxiliary and 3DHS branches, to enhance the class-imbalance segmentation. Extensive experiments on multiple datasets and backbones demonstrate that our approach achieves state-of-the-art 3DHS performance, and its core components can also be used as a plug-and-play enhancement to improve previous methods.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16650v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Multi-Agent Coordination in Autonomous Vehicle Routing: A Simulation-Based Study of Communication, Memory, and Routing Loops",
      "authors": [
        "KM Khalid Saifullah",
        "Daniel Palmer"
      ],
      "arxiv_id": "2511.17656v1",
      "summary": "Multi-agent coordination is critical for next-generation autonomous vehicle (AV) systems, yet naive implementations of communication-based rerouting can lead to catastrophic performance degradation. This study investigates a fundamental problem in decentralized multi-agent navigation: routing loops, where vehicles without persistent obstacle memory become trapped in cycles of inefficient path recalculation. Through systematic simulation experiments involving 72 unique configurations across varying vehicle densities (15, 35, 55 vehicles) and obstacle frequencies (6, 20 obstacles), we demonstrate that memory-less reactive rerouting increases average travel time by up to 682% compared to baseline conditions. To address this, we introduce Object Memory Management (OMM), a lightweight mechanism enabling agents to retain and share knowledge of previously encountered obstacles. OMM operates by maintaining a distributed blacklist of blocked nodes, which each agent consults during Dijkstra-based path recalculation, effectively preventing redundant routing attempts. Our results show that OMM-enabled coordination reduces average travel time by 75.7% and wait time by 88% compared to memory-less systems, while requiring only 1.67 route recalculations per vehicle versus 9.83 in memory-less scenarios. This work provides empirical evidence that persistent, shared memory is not merely beneficial but essential for robust multi-agent coordination in dynamic environments. The findings have implications beyond autonomous vehicles, informing the design of decentralized systems in robotics, network routing, and distributed AI. We provide a comprehensive experimental analysis, including detailed scenario breakdowns, scalability assessments, and visual documentation of the routing loop phenomenon, demonstrating OMM's critical role in preventing detrimental feedback cycles in cooperative multi-agent systems.",
      "categories": [
        "cs.MA",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.MA",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17656v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "YOWO: You Only Walk Once to Jointly Map An Indoor Scene and Register Ceiling-mounted Cameras",
      "authors": [
        "Fan Yang",
        "Sosuke Yamao",
        "Ikuo Kusajima",
        "Atsunori Moteki",
        "Shoichi Masui",
        "Shan Jiang"
      ],
      "arxiv_id": "2511.16521v1",
      "summary": "Using ceiling-mounted cameras (CMCs) for indoor visual capturing opens up a wide range of applications. However, registering CMCs to the target scene layout presents a challenging task. While manual registration with specialized tools is inefficient and costly, automatic registration with visual localization may yield poor results when visual ambiguity exists. To alleviate these issues, we propose a novel solution for jointly mapping an indoor scene and registering CMCs to the scene layout. Our approach involves equipping a mobile agent with a head-mounted RGB-D camera to traverse the entire scene once and synchronize CMCs to capture this mobile agent. The egocentric videos generate world-coordinate agent trajectories and the scene layout, while the videos of CMCs provide pseudo-scale agent trajectories and CMC relative poses. By correlating all the trajectories with their corresponding timestamps, the CMC relative poses can be aligned to the world-coordinate scene layout. Based on this initialization, a factor graph is customized to enable the joint optimization of ego-camera poses, scene layout, and CMC poses. We also develop a new dataset, setting the first benchmark for collaborative scene mapping and CMC registration (https://sites.google.com/view/yowo/home). Experimental results indicate that our method not only effectively accomplishes two tasks within a unified framework, but also jointly enhances their performance. We thus provide a reliable tool to facilitate downstream position-aware applications.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16521v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference",
      "authors": [
        "Ziyan Liu",
        "Yeqiu Chen",
        "Hongyi Cai",
        "Tao Lin",
        "Shuo Yang",
        "Zheng Liu",
        "Bo Zhao"
      ],
      "arxiv_id": "2511.16449v2",
      "summary": "Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA's intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-21",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16449v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "NaTex: Seamless Texture Generation as Latent Color Diffusion",
      "authors": [
        "Zeqiang Lai",
        "Yunfei Zhao",
        "Zibo Zhao",
        "Xin Yang",
        "Xin Huang",
        "Jingwei Huang",
        "Xiangyu Yue",
        "Chunchao Guo"
      ],
      "arxiv_id": "2511.16317v1",
      "summary": "We present NaTex, a native texture generation framework that predicts texture color directly in 3D space. In contrast to previous approaches that rely on baking 2D multi-view images synthesized by geometry-conditioned Multi-View Diffusion models (MVDs), NaTex avoids several inherent limitations of the MVD pipeline. These include difficulties in handling occluded regions that require inpainting, achieving precise mesh-texture alignment along boundaries, and maintaining cross-view consistency and coherence in both content and color intensity. NaTex features a novel paradigm that addresses the aforementioned issues by viewing texture as a dense color point cloud. Driven by this idea, we propose latent color diffusion, which comprises a geometry-awared color point cloud VAE and a multi-control diffusion transformer (DiT), entirely trained from scratch using 3D data, for texture reconstruction and generation. To enable precise alignment, we introduce native geometry control that conditions the DiT on direct 3D spatial information via positional embeddings and geometry latents. We co-design the VAE-DiT architecture, where the geometry latents are extracted via a dedicated geometry branch tightly coupled with the color VAE, providing fine-grained surface guidance that maintains strong correspondence with the texture. With these designs, NaTex demonstrates strong performance, significantly outperforming previous methods in texture coherence and alignment. Moreover, NaTex also exhibits strong generalization capabilities, either training-free or with simple tuning, for various downstream applications, e.g., material generation, texture refinement, and part segmentation and texturing.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "Technical Report",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16317v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "How Robot Dogs See the Unseeable",
      "authors": [
        "Oliver Bimber",
        "Karl Dietrich von Ellenrieder",
        "Michael Haller",
        "Rakesh John Amala Arokia Nathan",
        "Gianni Lunardi",
        "Marco Camurri",
        "Mohamed Youssef",
        "Santos Miguel Orozco Soto",
        "Jeremy E. Niven"
      ],
      "arxiv_id": "2511.16262v1",
      "summary": "Peering, a side-to-side motion used by animals to estimate distance through motion parallax, offers a powerful bio-inspired strategy to overcome a fundamental limitation in robotic vision: partial occlusion. Conventional robot cameras, with their small apertures and large depth of field, render both foreground obstacles and background objects in sharp focus, causing occluders to obscure critical scene information. This work establishes a formal connection between animal peering and synthetic aperture (SA) sensing from optical imaging. By having a robot execute a peering motion, its camera describes a wide synthetic aperture. Computational integration of the captured images synthesizes an image with an extremely shallow depth of field, effectively blurring out occluding elements while bringing the background into sharp focus. This efficient, wavelength-independent technique enables real-time, high-resolution perception across various spectral bands. We demonstrate that this approach not only restores basic scene understanding but also empowers advanced visual reasoning in large multimodal models, which fail with conventionally occluded imagery. Unlike feature-dependent multi-view 3D vision methods or active sensors like LiDAR, SA sensing via peering is robust to occlusion, computationally efficient, and immediately deployable on any mobile robot. This research bridges animal behavior and robotics, suggesting that peering motions for synthetic aperture sensing are a key to advanced scene understanding in complex, cluttered environments.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16262v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models",
      "authors": [
        "Yuping Yan",
        "Yuhan Xie",
        "Yixin Zhang",
        "Lingjuan Lyu",
        "Handing Wang",
        "Yaochu Jin"
      ],
      "arxiv_id": "2511.16203v3",
      "summary": "Vision-Language-Action models (VLAs) have recently demonstrated remarkable progress in embodied environments, enabling robots to perceive, reason, and act through unified multimodal understanding. Despite their impressive capabilities, the adversarial robustness of these systems remains largely unexplored, especially under realistic multimodal and black-box conditions. Existing studies mainly focus on single-modality perturbations and overlook the cross-modal misalignment that fundamentally affects embodied reasoning and decision-making. In this paper, we introduce VLA-Fool, a comprehensive study of multimodal adversarial robustness in embodied VLA models under both white-box and black-box settings. VLA-Fool unifies three levels of multimodal adversarial attacks: (1) textual perturbations through gradient-based and prompt-based manipulations, (2) visual perturbations via patch and noise distortions, and (3) cross-modal misalignment attacks that intentionally disrupt the semantic correspondence between perception and instruction. We further incorporate a VLA-aware semantic space into linguistic prompts, developing the first automatically crafted and semantically guided prompting framework. Experiments on the LIBERO benchmark using a fine-tuned OpenVLA model reveal that even minor multimodal perturbations can cause significant behavioral deviations, demonstrating the fragility of embodied multimodal alignment.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-12-11",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16203v3",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "FOOTPASS: A Multi-Modal Multi-Agent Tactical Context Dataset for Play-by-Play Action Spotting in Soccer Broadcast Videos",
      "authors": [
        "Jeremie Ochin",
        "Raphael Chekroun",
        "Bogdan Stanciulescu",
        "Sotiris Manitsaris"
      ],
      "arxiv_id": "2511.16183v1",
      "summary": "Soccer video understanding has motivated the creation of datasets for tasks such as temporal action localization, spatiotemporal action detection (STAD), or multiobject tracking (MOT). The annotation of structured sequences of events (who does what, when, and where) used for soccer analytics requires a holistic approach that integrates both STAD and MOT. However, current action recognition methods remain insufficient for constructing reliable play-by-play data and are typically used to assist rather than fully automate annotation. Parallel research has advanced tactical modeling, trajectory forecasting, and performance analysis, all grounded in game-state and play-by-play data. This motivates leveraging tactical knowledge as a prior to support computer-vision-based predictions, enabling more automated and reliable extraction of play-by-play data. We introduce Footovision Play-by-Play Action Spotting in Soccer Dataset (FOOTPASS), the first benchmark for play-by-play action spotting over entire soccer matches in a multi-modal, multi-agent tactical context. It enables the development of methods for player-centric action spotting that exploit both outputs from computer-vision tasks (e.g., tracking, identification) and prior knowledge of soccer, including its tactical regularities over long time horizons, to generate reliable play-by-play data streams. These streams form an essential input for data-driven sports analytics.",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16183v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight",
      "authors": [
        "Yi Yang",
        "Xueqi Li",
        "Yiyang Chen",
        "Jin Song",
        "Yihan Wang",
        "Zipeng Xiao",
        "Jiadi Su",
        "You Qiaoben",
        "Pengfei Liu",
        "Zhijie Deng"
      ],
      "arxiv_id": "2511.16175v1",
      "summary": "Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms $π_{0.5}$, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16175v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "PairHuman: A High-Fidelity Photographic Dataset for Customized Dual-Person Generation",
      "authors": [
        "Ting Pan",
        "Ye Wang",
        "Peiguang Jing",
        "Rui Ma",
        "Zili Yi",
        "Yu Liu"
      ],
      "arxiv_id": "2511.16712v2",
      "summary": "Personalized dual-person portrait customization has considerable potential applications, such as preserving emotional memories and facilitating wedding photography planning. However, the absence of a benchmark dataset hinders the pursuit of high-quality customization in dual-person portrait generation. In this paper, we propose the PairHuman dataset, which is the first large-scale benchmark dataset specifically designed for generating dual-person portraits that meet high photographic standards. The PairHuman dataset contains more than 100K images that capture a variety of scenes, attire, and dual-person interactions, along with rich metadata, including detailed image descriptions, person localization, human keypoints, and attribute tags. We also introduce DHumanDiff, which is a baseline specifically crafted for dual-person portrait generation that features enhanced facial consistency and simultaneously balances in personalized person generation and semantic-driven scene creation. Finally, the experimental results demonstrate that our dataset and method produce highly customized portraits with superior visual quality that are tailored to human preferences. Our dataset is publicly available at https://github.com/annaoooo/PairHuman.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-24",
      "comment": "46 pages, 31 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16712v2",
      "code_links": [
        {
          "url": "https://github.com/annaoooo/PairHuman",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "PushingBots: Collaborative Pushing via Neural Accelerated Combinatorial Hybrid Optimization",
      "authors": [
        "Zili Tang",
        "Ying Zhang",
        "Meng Guo"
      ],
      "arxiv_id": "2511.15995v1",
      "summary": "Many robots are not equipped with a manipulator and many objects are not suitable for prehensile manipulation (such as large boxes and cylinders). In these cases, pushing is a simple yet effective non-prehensile skill for robots to interact with and further change the environment. Existing work often assumes a set of predefined pushing modes and fixed-shape objects. This work tackles the general problem of controlling a robotic fleet to push collaboratively numerous arbitrary objects to respective destinations, within complex environments of cluttered and movable obstacles. It incorporates several characteristic challenges for multi-robot systems such as online task coordination under large uncertainties of cost and duration, and for contact-rich tasks such as hybrid switching among different contact modes, and under-actuation due to constrained contact forces. The proposed method is based on combinatorial hybrid optimization over dynamic task assignments and hybrid execution via sequences of pushing modes and associated forces. It consists of three main components: (I) the decomposition, ordering and rolling assignment of pushing subtasks to robot subgroups; (II) the keyframe guided hybrid search to optimize the sequence of parameterized pushing modes for each subtask; (III) the hybrid control to execute these modes and transit among them. Last but not least, a diffusion-based accelerator is adopted to predict the keyframes and pushing modes that should be prioritized during hybrid search; and further improve planning efficiency. The framework is complete under mild assumptions. Its efficiency and effectiveness under different numbers of robots and general-shaped objects are validated extensively in simulations and hardware experiments, as well as generalizations to heterogeneous robots, planar assembly and 6D pushing.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-20",
      "updated": "2025-11-20",
      "comment": "20 pages, 24 figures. Accepted to IEEE Transactions on Robotics (T-RO), 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15995v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "The Role of Consequential and Functional Sound in Human-Robot Interaction: Toward Audio Augmented Reality Interfaces",
      "authors": [
        "Aliyah Smith",
        "Monroe Kennedy"
      ],
      "arxiv_id": "2511.15956v2",
      "summary": "As robots become increasingly integrated into everyday environments, understanding how they communicate with humans is critical. Sound offers a powerful channel for interaction, encompassing both operational noises and intentionally designed auditory cues. In this study, we examined the effects of consequential and functional sounds on human perception and behavior, including a novel exploration of spatial sound through localization and handover tasks. Results show that consequential sounds of the Kinova Gen3 manipulator did not negatively affect perceptions, spatial localization is highly accurate for lateral cues but declines for frontal cues, and spatial sounds can simultaneously convey task-relevant information while promoting warmth and reducing discomfort. These findings highlight the potential of functional and transformative auditory design to enhance human-robot collaboration and inform future sound-based interaction strategies.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-20",
      "updated": "2025-11-26",
      "comment": "9 pages, 6 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15956v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click",
      "authors": [
        "Raphael Ruschel",
        "Hardikkumar Prajapati",
        "Awsafur Rahman",
        "B. S. Manjunath"
      ],
      "arxiv_id": "2511.15948v2",
      "summary": "State-of-the-art Video Scene Graph Generation (VSGG) systems provide structured visual understanding but operate as closed, feed-forward pipelines with no ability to incorporate human guidance. In contrast, promptable segmentation models such as SAM2 enable precise user interaction but lack semantic or relational reasoning. We introduce Click2Graph, the first interactive framework for Panoptic Video Scene Graph Generation (PVSG) that unifies visual prompting with spatial, temporal, and semantic understanding. From a single user cue, such as a click or bounding box, Click2Graph segments and tracks the subject across time, autonomously discovers interacting objects, and predicts <subject, object, predicate> triplets to form a temporally consistent scene graph. Our framework introduces two key components: a Dynamic Interaction Discovery Module that generates subject-conditioned object prompts, and a Semantic Classification Head that performs joint entity and predicate reasoning. Experiments on the OpenPVSG benchmark demonstrate that Click2Graph establishes a strong foundation for user-guided PVSG, showing how human prompting can be combined with panoptic grounding and relational inference to enable controllable and interpretable video scene understanding.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-20",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15948v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "SPHaptics: A Real-Time Bidirectional Haptic Interaction Framework for Coupled Rigid-Soft Body and Lagrangian Fluid Simulation in Virtual Environments",
      "authors": [
        "William Baumgartner",
        "Gizem Kayar-Ceylan"
      ],
      "arxiv_id": "2511.15908v1",
      "summary": "Haptic feedback enhances immersion in virtual environments by allowing users to physically interact with simulated objects. Supporting accurate force responses in multiphysics systems is challenging because physically based simulation of fluid, rigid, and deformable materials is computationally demanding, especially when interaction must occur in real time. We present a unified framework for real-time, bidirectional haptic interaction with rigid bodies, deformable objects, and Lagrangian fluids in virtual reality (VR). Our approach integrates Smoothed Particle Hydrodynamics (SPH) with two-way force coupling and feedback smoothing to maintain stability and produce physically meaningful tactile responses. This enables users to manipulate objects immersed in fluid and feel reaction forces consistent with fluid-structure behavior. We demonstrate the capabilities of our framework through interactive VR scenarios involving fluid stirring, soft tissue manipulation, and rigid-body interaction. The proposed system advances haptic-enabled multiphysics simulation by unifying fluid, soft-body, and rigid-body dynamics into a single platform suitable for immersive educational applications.",
      "categories": [
        "cs.GR"
      ],
      "primary_category": "cs.GR",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "9 pages, 9 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15908v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Optimus-Q: Utilizing Federated Learning in Adaptive Robots for Intelligent Nuclear Power Plant Operations through Quantum Cryptography",
      "authors": [
        "Sai Puppala",
        "Ismail Hossain",
        "Jahangir Alam",
        "Sajedul Talukder"
      ],
      "arxiv_id": "2511.15614v1",
      "summary": "The integration of advanced robotics in nuclear power plants (NPPs) presents a transformative opportunity to enhance safety, efficiency, and environmental monitoring in high-stakes environments. Our paper introduces the Optimus-Q robot, a sophisticated system designed to autonomously monitor air quality and detect contamination while leveraging adaptive learning techniques and secure quantum communication. Equipped with advanced infrared sensors, the Optimus-Q robot continuously streams real-time environmental data to predict hazardous gas emissions, including carbon dioxide (CO$_2$), carbon monoxide (CO), and methane (CH$_4$). Utilizing a federated learning approach, the robot collaborates with other systems across various NPPs to improve its predictive capabilities without compromising data privacy. Additionally, the implementation of Quantum Key Distribution (QKD) ensures secure data transmission, safeguarding sensitive operational information. Our methodology combines systematic navigation patterns with machine learning algorithms to facilitate efficient coverage of designated areas, thereby optimizing contamination monitoring processes. Through simulations and real-world experiments, we demonstrate the effectiveness of the Optimus-Q robot in enhancing operational safety and responsiveness in nuclear facilities. This research underscores the potential of integrating robotics, machine learning, and quantum technologies to revolutionize monitoring systems in hazardous environments.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15614v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Computer-Use Agents as Judges for Generative User Interface",
      "authors": [
        "Kevin Qinghong Lin",
        "Siyuan Hu",
        "Linjie Li",
        "Zhengyuan Yang",
        "Lijuan Wang",
        "Philip Torr",
        "Mike Zheng Shou"
      ],
      "arxiv_id": "2511.15567v1",
      "summary": "Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI.",
      "categories": [
        "cs.CV",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "Project: https://showlab.github.io/AUI Github: https://github.com/showlab/AUI",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15567v1",
      "code_links": [
        {
          "url": "https://github.com/showlab/AUI",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Discovering Optimal Natural Gaits of Dissipative Systems via Virtual Energy Injection",
      "authors": [
        "Korbinian Griesbauer",
        "Davide Calzolari",
        "Maximilian Raff",
        "C. David Remy",
        "Alin Albu-Schäffer"
      ],
      "arxiv_id": "2511.15513v1",
      "summary": "Legged robots offer several advantages when navigating unstructured environments, but they often fall short of the efficiency achieved by wheeled robots. One promising strategy to improve their energy economy is to leverage their natural (unactuated) dynamics using elastic elements. This work explores that concept by designing energy-optimal control inputs through a unified, multi-stage framework. It starts with a novel energy injection technique to identify passive motion patterns by harnessing the system's natural dynamics. This enables the discovery of passive solutions even in systems with energy dissipation caused by factors such as friction or plastic collisions. Building on these passive solutions, we then employ a continuation approach to derive energy-optimal control inputs for the fully actuated, dissipative robotic system. The method is tested on simulated models to demonstrate its applicability in both single- and multi-legged robotic systems. This analysis provides valuable insights into the design and operation of elastic legged robots, offering pathways to improve their efficiency and adaptability by exploiting the natural system dynamics.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "Preprint Version, IEEE Robotics and Automation Letters (RA-L), accepted November 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15513v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "legged robot"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "One algebra for all : Geometric Algebra methods for neurosymbolic XR scene authoring, animation and neural rendering",
      "authors": [
        "Manos Kamarianakis",
        "Antonis Protopsaltis",
        "George Papagiannakis"
      ],
      "arxiv_id": "2511.15398v1",
      "summary": "This position paper delves into the transformative role of Geometric Algebra (GA) in advancing specific areas of Computer Graphics (CG) and Extended Reality (XR), particularly in character animation, rendering, rigging, neural rendering, and generative AI-driven scene editing. Common CG algorithms require handling rotations, translations, and dilations (uniform scalings) in operations such as object rendering, rigged model animation, soft-body deformation, and XR simulations. Traditional representation forms - such as matrices, quaternions, and vectors - often introduce limitations in precision and performance. Recent breakthroughs in the use of GA suggest it can significantly enhance these processes by encapsulating geometric forms and transformations into uniform algebraic expressions, which maintain critical geometric properties throughout multi-step transformations. Furthermore, we explore how GA can serve as a unifying mathematical substrate for neurosymbolic XR scene authoring, bridging learned neural representations and explicit geometric reasoning. This paper outlines how GA-based approaches can improve the fidelity of rigged character animations, enhance soft-body simulations, streamline real-time rendering, and optimize neural and generative AI scene editing. GA offers a coherent and efficient framework for these processes, resulting in superior visual outcomes and computational efficiency, particularly in XR environments.",
      "categories": [
        "cs.GR"
      ],
      "primary_category": "cs.GR",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "10 pages, 9 Figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15398v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "character animation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "8_physics_animation"
      ]
    },
    {
      "title": "ShelfOcc: Native 3D Supervision beyond LiDAR for Vision-Based Occupancy Estimation",
      "authors": [
        "Simon Boeder",
        "Fabian Gigengack",
        "Simon Roesler",
        "Holger Caesar",
        "Benjamin Risse"
      ],
      "arxiv_id": "2511.15396v1",
      "summary": "Recent progress in self- and weakly supervised occupancy estimation has largely relied on 2D projection or rendering-based supervision, which suffers from geometric inconsistencies and severe depth bleeding. We thus introduce ShelfOcc, a vision-only method that overcomes these limitations without relying on LiDAR. ShelfOcc brings supervision into native 3D space by generating metrically consistent semantic voxel labels from video, enabling true 3D supervision without any additional sensors or manual 3D annotations. While recent vision-based 3D geometry foundation models provide a promising source of prior knowledge, they do not work out of the box as a prediction due to sparse or noisy and inconsistent geometry, especially in dynamic driving scenes. Our method introduces a dedicated framework that mitigates these issues by filtering and accumulating static geometry consistently across frames, handling dynamic content and propagating semantic information into a stable voxel representation. This data-centric shift in supervision for weakly/shelf-supervised occupancy estimation allows the use of essentially any SOTA occupancy model architecture without relying on LiDAR data. We argue that such high-quality supervision is essential for robust occupancy learning and constitutes an important complementary avenue to architectural innovation. On the Occ3D-nuScenes benchmark, ShelfOcc substantially outperforms all previous weakly/shelf-supervised methods (up to a 34% relative improvement), establishing a new data-driven direction for LiDAR-free 3D scene understanding.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15396v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration",
      "authors": [
        "Yifu Guo",
        "Zishan Xu",
        "Zhiyuan Yao",
        "Yuquan Lu",
        "Jiaye Lin",
        "Sen Hu",
        "Zhenheng Tang",
        "Huacan Wang",
        "Ronghao Chen"
      ],
      "arxiv_id": "2511.15351v2",
      "summary": "Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "published": "2025-11-19",
      "updated": "2025-12-12",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15351v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Adaptive thresholding pattern for fingerprint forgery detection",
      "authors": [
        "Zahra Farzadpour",
        "Masoumeh Azghani"
      ],
      "arxiv_id": "2511.15322v1",
      "summary": "Fingerprint liveness detection systems have been affected by spoofing, which is a severe threat for fingerprint-based biometric systems. Therefore, it is crucial to develop some techniques to distinguish the fake fingerprints from the real ones. The software based techniques can detect the fingerprint forgery automatically. Also, the scheme shall be resistant against various distortions such as noise contamination, pixel missing and block missing, so that the forgers cannot deceive the detector by adding some distortions to the faked fingerprint. In this paper, we propose a fingerprint forgery detection algorithm based on a suggested adaptive thresholding pattern. The anisotropic diffusion of the input image is passed through three levels of the wavelet transform. The coefficients of different layers are adaptively thresholded and concatenated to produce the feature vector which is classified using the SVM classifier. Another contribution of the paper is to investigate the effect of various distortions such as pixel missing, block missing, and noise contamination. Our suggested approach includes a novel method that exhibits improved resistance against a range of distortions caused by environmental phenomena or manipulations by malicious users. In quantitative comparisons, our proposed method outperforms its counterparts by approximately 8% and 5% in accuracy for missing pixel scenarios of 90% and block missing scenarios of size 70x70 , respectively. This highlights the novelty approach in addressing such challenges.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "25 pages, 10 figures, Journal paper",
      "doi": "10.1007/s11042-024-18649-3",
      "journal_ref": "Farzadpour, Z., & Azghani, M. (2024). Adaptive thresholding pattern for fingerprint forgery detection. Multimedia Tools and Applications, 83(34), 81665-81683",
      "pdf_url": "https://arxiv.org/pdf/2511.15322v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Adapt-As-You-Walk Through the Clouds: Training-Free Online Test-Time Adaptation of 3D Vision-Language Foundation Models",
      "authors": [
        "Mehran Tamjidi",
        "Hamidreza Dastmalchi",
        "Mohammadreza Alimoradijazi",
        "Ali Cheraghian",
        "Aijun An",
        "Morteza Saberi"
      ],
      "arxiv_id": "2511.15311v2",
      "summary": "3D Vision-Language Foundation Models (VLFMs) have shown strong generalization and zero-shot recognition capabilities in open-world point cloud processing tasks. However, these models often underperform in practical scenarios where data are noisy, incomplete, or drawn from a different distribution than the training data. To address this, we propose Uni-Adapter, a novel training-free online test-time adaptation (TTA) strategy for 3D VLFMs based on dynamic prototype learning. We define a 3D cache to store class-specific cluster centers as prototypes, which are continuously updated to capture intra-class variability in heterogeneous data distributions. These dynamic prototypes serve as anchors for cache-based logit computation via similarity scoring. Simultaneously, a graph-based label smoothing module captures inter-prototype similarities to enforce label consistency among similar prototypes. Finally, we unify predictions from the original 3D VLFM and the refined 3D cache using entropy-weighted aggregation for reliable adaptation. Without retraining, Uni-Adapter effectively mitigates distribution shifts, achieving state-of-the-art performance on diverse 3D benchmarks over different 3D VLFMs, improving ModelNet-40C by 10.55%, ScanObjectNN-C by 8.26%, and ShapeNet-C by 4.49% over the source 3D VLFMs. Project page: https://mehran-tam.github.io/Uni-Adapter",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-19",
      "updated": "2025-11-20",
      "comment": "Accepted by AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15311v2",
      "code_links": [
        {
          "url": "https://mehran-tam.github.io/Uni-Adapter",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer",
      "authors": [
        "Wenhan Yu",
        "Wang Chen",
        "Guanqiang Qi",
        "Weikang Li",
        "Yang Li",
        "Lei Sha",
        "Deguo Xia",
        "Jizhou Huang"
      ],
      "arxiv_id": "2511.15090v1",
      "summary": "Document Visual Question Answering (DocVQA) is a fundamental task for multimodal document understanding and a key testbed for vision language reasoning. However, most existing DocVQA datasets are limited to the page level and lack fine grained spatial grounding, constraining the interpretability and reasoning capability of Vision Language Models (VLMs). To address this gap, we introduce BBox DocVQA a large scale, bounding box grounded dataset designed to enhance spatial reasoning and evidence localization in visual documents. We further present an automated construction pipeline, Segment Judge and Generate, which integrates a segment model for region segmentation, a VLM for semantic judgment, and another advanced VLM for question answer generation, followed by human verification for quality assurance. The resulting dataset contains 3.6 K diverse documents and 32 K QA pairs, encompassing single and multi region as well as single and multi page scenarios. Each QA instance is grounded on explicit bounding boxes, enabling fine grained evaluation of spatial semantic alignment. Benchmarking multiple state of the art VLMs (e.g., GPT 5, Qwen2.5 VL, and InternVL) on BBox DocVQA reveals persistent challenges in spatial grounding and reasoning accuracy. Furthermore, fine tuning on BBox DocVQA substantially improves both bounding box localization and answer generation, validating its effectiveness for enhancing the reasoning ability of VLMs. Our dataset and code will be publicly released to advance research on interpretable and spatially grounded vision language reasoning.",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.DB",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "comment": "22 pages, 4 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15090v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "CPSL: Representing Volumetric Video via Content-Promoted Scene Layers",
      "authors": [
        "Kaiyuan Hu",
        "Yili Jin",
        "Junhua Liu",
        "Xize Duan",
        "Hong Kang",
        "Xue Liu"
      ],
      "arxiv_id": "2511.14927v1",
      "summary": "Volumetric video enables immersive and interactive visual experiences by supporting free viewpoint exploration and realistic motion parallax. However, existing volumetric representations from explicit point clouds to implicit neural fields, remain costly in capture, computation, and rendering, which limits their scalability for on-demand video and reduces their feasibility for real-time communication.\n  To bridge this gap, we propose Content-Promoted Scene Layers (CPSL), a compact 2.5D video representation that brings the perceptual benefits of volumetric video to conventional 2D content. Guided by per-frame depth and content saliency, CPSL decomposes each frame into a small set of geometry-consistent layers equipped with soft alpha bands and an edge-depth cache that jointly preserve occlusion ordering and boundary continuity. These lightweight, 2D-encodable assets enable parallax-corrected novel-view synthesis via depth-weighted warping and front-to-back alpha compositing, bypassing expensive 3D reconstruction. Temporally, CPSL maintains inter-frame coherence using motion-guided propagation and per-layer encoding, supporting real-time playback with standard video codecs. Across multiple benchmarks, CPSL achieves superior perceptual quality and boundary fidelity compared with layer-based and neural-field baselines while reducing storage and rendering cost by several folds. Our approach offer a practical path from 2D video to scalable 2.5D immersive media.",
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14927v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Robust Verification of Controllers under State Uncertainty via Hamilton-Jacobi Reachability Analysis",
      "authors": [
        "Albert Lin",
        "Alessandro Pinto",
        "Somil Bansal"
      ],
      "arxiv_id": "2511.14755v1",
      "summary": "As perception-based controllers for autonomous systems become increasingly popular in the real world, it is important that we can formally verify their safety and performance despite perceptual uncertainty. Unfortunately, the verification of such systems remains challenging, largely due to the complexity of the controllers, which are often nonlinear, nonconvex, learning-based, and/or black-box. Prior works propose verification algorithms that are based on approximate reachability methods, but they often restrict the class of controllers and systems that can be handled or result in overly conservative analyses. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for general nonlinear systems that can compute optimal reachable sets under worst-case system uncertainties; however, its application to perception-based systems is currently underexplored. In this work, we propose RoVer-CoRe, a framework for the Robust Verification of Controllers via HJ Reachability. To the best of our knowledge, RoVer-CoRe is the first HJ reachability-based framework for the verification of perception-based systems under perceptual uncertainty. Our key insight is to concatenate the system controller, observation function, and the state estimation modules to obtain an equivalent closed-loop system that is readily compatible with existing reachability frameworks. Within RoVer-CoRe, we propose novel methods for formal safety verification and robust controller design. We demonstrate the efficacy of the framework in case studies involving aircraft taxiing and NN-based rover navigation. Code is available at the link in the footnote.",
      "categories": [
        "cs.RO",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "Submitted to the 8th Annual Learning for Dynamics & Control Conference",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14755v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers",
      "authors": [
        "Yutian Chen",
        "Yuheng Qiu",
        "Ruogu Li",
        "Ali Agha",
        "Shayegan Omidshafiei",
        "Jay Patrikar",
        "Sebastian Scherer"
      ],
      "arxiv_id": "2511.14751v1",
      "summary": "We propose Confidence-Guided Token Merging (Co-Me), an acceleration mechanism for visual geometric transformers without retraining or finetuning the base model. Co-Me distilled a light-weight confidence predictor to rank tokens by uncertainty and selectively merge low-confidence ones, effectively reducing computation while maintaining spatial coverage. Compared to similarity-based merging or pruning, the confidence signal in Co-Me reliably indicates regions emphasized by the transformer, enabling substantial acceleration without degrading performance. Co-Me applies seamlessly to various multi-view and streaming visual geometric transformers, achieving speedups that scale with sequence length. When applied to VGGT and MapAnything, Co-Me achieves up to $11.3\\times$ and $7.2\\times$ speedup, making visual geometric transformers practical for real-time 3D perception and reconstruction.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14751v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "VGGT"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Improving segmentation of retinal arteries and veins using cardiac signal in doppler holograms",
      "authors": [
        "Marius Dubosc",
        "Yann Fischer",
        "Zacharie Auray",
        "Nicolas Boutry",
        "Edwin Carlinet",
        "Michael Atlan",
        "Thierry Geraud"
      ],
      "arxiv_id": "2511.14654v1",
      "summary": "Doppler holography is an emerging retinal imaging technique that captures the dynamic behavior of blood flow with high temporal resolution, enabling quantitative assessment of retinal hemodynamics. This requires accurate segmentation of retinal arteries and veins, but traditional segmentation methods focus solely on spatial information and overlook the temporal richness of holographic data. In this work, we propose a simple yet effective approach for artery-vein segmentation in temporal Doppler holograms using standard segmentation architectures. By incorporating features derived from a dedicated pulse analysis pipeline, our method allows conventional U-Nets to exploit temporal dynamics and achieve performance comparable to more complex attention- or iteration-based models. These findings demonstrate that time-resolved preprocessing can unlock the full potential of deep learning for Doppler holography, opening new perspectives for quantitative exploration of retinal hemodynamics. The dataset is publicly available at https://huggingface.co/datasets/DigitalHolography/",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "5 pages, 3 figures, 1 table. Submitted to ISBI2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14654v1",
      "code_links": [
        {
          "url": "https://huggingface.co/datasets/DigitalHolography",
          "type": "huggingface"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "PULSE"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "8_physics_animation"
      ]
    },
    {
      "title": "A Quantitative Method for Shoulder Presentation Evaluation in Biometric Identity Documents",
      "authors": [
        "Alfonso Pedro Ridao"
      ],
      "arxiv_id": "2511.14376v1",
      "summary": "International standards for biometric identity documents mandate strict compliance with pose requirements, including the square presentation of a subject's shoulders. However, the literature on automated quality assessment offers few quantitative methods for evaluating this specific attribute. This paper proposes a Shoulder Presentation Evaluation (SPE) algorithm to address this gap. The method quantifies shoulder yaw and roll using only the 3D coordinates of two shoulder landmarks provided by common pose estimation frameworks. The algorithm was evaluated on a dataset of 121 portrait images. The resulting SPE scores demonstrated a strong Pearson correlation (r approx. 0.80) with human-assigned labels. An analysis of the metric's filtering performance, using an adapted Error-versus-Discard methodology, confirmed its utility in identifying non-compliant samples. The proposed algorithm is a viable lightweight tool for automated compliance checking in enrolment systems.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "13 pages, 4 figures, conference or journal submission. Course project from DTU Compute, Technical University of Denmark",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14376v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "pose estimation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model",
      "authors": [
        "Rishi Gupta",
        "Mukilan Karuppasamy",
        "Shyam Marjit",
        "Aditay Tripathi",
        "Anirban Chakraborty"
      ],
      "arxiv_id": "2511.14368v1",
      "summary": "While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited. Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually. We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions. To address this, we present two key contributions: (1) a new, large-scale dataset of image-sketch-instruction triplets designed to facilitate both pretraining and instruction tuning, and (2) O3SLM, an LVLM trained on this dataset. Comprehensive evaluations on multiple sketch-based tasks: (a) object localization, (b) counting, (c) image retrieval i.e., (SBIR and fine-grained SBIR), and (d) visual question answering (VQA); while incorporating the three existing sketch datasets, namely QuickDraw!, Sketchy, and Tu Berlin, along with our generated SketchVCL dataset, show that O3SLM achieves state-of-the-art performance, substantially outperforming existing LVLMs in sketch comprehension and reasoning.",
      "categories": [
        "cs.CV",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "Accepted to AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14368v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "NeuralSSD: A Neural Solver for Signed Distance Surface Reconstruction",
      "authors": [
        "Zi-Chen Xi",
        "Jiahui Huang",
        "Hao-Xiang Chen",
        "Francis Williams",
        "Qun-Ce Xu",
        "Tai-Jiang Mu",
        "Shi-Min Hu"
      ],
      "arxiv_id": "2511.14283v1",
      "summary": "We proposed a generalized method, NeuralSSD, for reconstructing a 3D implicit surface from the widely-available point cloud data. NeuralSSD is a solver-based on the neural Galerkin method, aimed at reconstructing higher-quality and accurate surfaces from input point clouds. Implicit method is preferred due to its ability to accurately represent shapes and its robustness in handling topological changes. However, existing parameterizations of implicit fields lack explicit mechanisms to ensure a tight fit between the surface and input data. To address this, we propose a novel energy equation that balances the reliability of point cloud information. Additionally, we introduce a new convolutional network that learns three-dimensional information to achieve superior optimization results. This approach ensures that the reconstructed surface closely adheres to the raw input points and infers valuable inductive biases from point clouds, resulting in a highly accurate and stable surface reconstruction. NeuralSSD is evaluated on a variety of challenging datasets, including the ShapeNet and Matterport datasets, and achieves state-of-the-art results in terms of both surface reconstruction accuracy and generalizability.",
      "categories": [
        "cs.CV",
        "cs.GR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "Under review",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14283v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution",
      "authors": [
        "N Dinesh Reddy",
        "Dylan Snyder",
        "Lona Kiragu",
        "Mirajul Mohin",
        "Shahrear Bin Amin",
        "Sudeep Pillai"
      ],
      "arxiv_id": "2511.14210v2",
      "summary": "We introduce Orion, a visual agent that integrates vision-based reasoning with tool-augmented execution to achieve powerful, precise, multi-step visual intelligence across images, video, and documents. Unlike traditional vision-language models that generate descriptive outputs, Orion orchestrates a suite of specialized computer vision tools, including object detection, keypoint localization, panoptic segmentation, Optical Character Recognition (OCR), and geometric analysis, to execute complex multi-step visual workflows. The system achieves competitive performance across MMMU, MMBench, DocVQA, and MMLongBench while extending monolithic VLM capabilities to production-grade visual intelligence. Through its agentic, tool-augmented approach, Orion enables autonomous visual reasoning that bridges neural perception with symbolic execution, marking the transition from passive visual understanding to active, tool-driven visual intelligence.\n  Try Orion for free at: https://chat.vlm.run\n  Learn more at: https://www.vlm.run/orion",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-19",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14210v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Towards Deploying VLA without Fine-Tuning: Plug-and-Play Inference-Time VLA Policy Steering via Embodied Evolutionary Diffusion",
      "authors": [
        "Zhuo Li",
        "Junjia Liu",
        "Zhipeng Dong",
        "Tao Teng",
        "Quentin Rouxel",
        "Darwin Caldwell",
        "Fei Chen"
      ],
      "arxiv_id": "2511.14178v1",
      "summary": "Vision-Language-Action (VLA) models have demonstrated significant potential in real-world robotic manipulation. However, pre-trained VLA policies still suffer from substantial performance degradation during downstream deployment. Although fine-tuning can mitigate this issue, its reliance on costly demonstration collection and intensive computation makes it impractical in real-world settings. In this work, we introduce VLA-Pilot, a plug-and-play inference-time policy steering method for zero-shot deployment of pre-trained VLA without any additional fine-tuning or data collection. We evaluate VLA-Pilot on six real-world downstream manipulation tasks across two distinct robotic embodiments, encompassing both in-distribution and out-of-distribution scenarios. Experimental results demonstrate that VLA-Pilot substantially boosts the success rates of off-the-shelf pre-trained VLA policies, enabling robust zero-shot generalization to diverse tasks and embodiments. Experimental videos and code are available at: https://rip4kobe.github.io/vla-pilot/.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "9 pages, 8 figures, submitted to IEEE RA-L",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14178v1",
      "code_links": [
        {
          "url": "https://rip4kobe.github.io/vla-pilot/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Wave-Former: Through-Occlusion 3D Reconstruction via Wireless Shape Completion",
      "authors": [
        "Laura Dodds",
        "Maisy Lam",
        "Waleed Akbar",
        "Yibo Cheng",
        "Fadel Adib"
      ],
      "arxiv_id": "2511.14152v2",
      "summary": "We present Wave-Former, a novel method capable of high-accuracy 3D shape reconstruction for completely occluded, diverse, everyday objects. This capability can open new applications spanning robotics, augmented reality, and logistics. Our approach leverages millimeter-wave (mmWave) wireless signals, which can penetrate common occlusions and reflect off hidden objects. In contrast to past mmWave reconstruction methods, which suffer from limited coverage and high noise, Wave-Former introduces a physics-aware shape completion model capable of inferring full 3D geometry. At the heart of Wave-Former's design is a novel three-stage pipeline which bridges raw wireless signals with recent advancements in vision-based shape completion by incorporating physical properties of mmWave signals. The pipeline proposes candidate geometric surfaces, employs a transformer-based shape completion model designed specifically for mmWave signals, and finally performs entropy-guided surface selection. This enables Wave-Former to be trained using entirely synthetic point-clouds, while demonstrating impressive generalization to real-world data. In head-to-head comparisons with state-of-the-art baselines, Wave-Former raises recall from 54% to 72% while maintaining a high precision of 85%.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-19",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14152v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning Framework with Vision-Language Models",
      "authors": [
        "Hao Zhen",
        "Yunxiang Yang",
        "Jidong J. Yang"
      ],
      "arxiv_id": "2511.14120v1",
      "summary": "Pedestrian-vehicle incidents remain a critical urban safety challenge, with pedestrians accounting for over 20% of global traffic fatalities. Although existing video-based systems can detect when incidents occur, they provide little insight into how these events unfold across the distinct cognitive phases of pedestrian behavior. Recent vision-language models (VLMs) have shown strong potential for video understanding, but they remain limited in that they typically process videos in isolation, without explicit temporal structuring or multi-view integration. This paper introduces Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning (MP-PVIR), a unified framework that systematically processes multi-view video streams into structured diagnostic reports through four stages: (1) event-triggered multi-view video acquisition, (2) pedestrian behavior phase segmentation, (3) phase-specific multi-view reasoning, and (4) hierarchical synthesis and diagnostic reasoning. The framework operationalizes behavioral theory by automatically segmenting incidents into cognitive phases, performing synchronized multi-view analysis within each phase, and synthesizing results into causal chains with targeted prevention strategies. Particularly, two specialized VLMs underpin the MP-PVIR pipeline: TG-VLM for behavioral phase segmentation (mIoU = 0.4881) and PhaVR-VLM for phase-aware multi-view analysis, achieving a captioning score of 33.063 and up to 64.70% accuracy on question answering. Finally, a designated large language model is used to generate comprehensive reports detailing scene understanding, behavior interpretation, causal reasoning, and prevention recommendations. Evaluation on the Woven Traffic Safety dataset shows that MP-PVIR effectively translates multi-view video data into actionable insights, advancing AI-driven traffic safety analytics for vehicle-infrastructure cooperative systems.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "comment": "23 pages, 4 figures, 3 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14120v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Part-X-MLLM: Part-aware 3D Multimodal Large Language Model",
      "authors": [
        "Chunshi Wang",
        "Junliang Ye",
        "Yunhan Yang",
        "Yang Li",
        "Zizhuo Lin",
        "Jun Zhu",
        "Zhuo Chen",
        "Yawei Luo",
        "Chunchao Guo"
      ],
      "arxiv_id": "2511.13647v1",
      "summary": "We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13647v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Trust in Vision-Language Models: Insights from a Participatory User Workshop",
      "authors": [
        "Agnese Chiatti",
        "Lara Piccolo",
        "Sara Bernardini",
        "Matteo Matteucci",
        "Viola Schiaffonati"
      ],
      "arxiv_id": "2511.13458v1",
      "summary": "With the growing deployment of Vision-Language Models (VLMs), pre-trained on large image-text and video-text datasets, it is critical to equip users with the tools to discern when to trust these systems. However, examining how user trust in VLMs builds and evolves remains an open problem. This problem is exacerbated by the increasing reliance on AI models as judges for experimental validation, to bypass the cost and implications of running participatory design studies directly with users. Following a user-centred approach, this paper presents preliminary results from a workshop with prospective VLM users. Insights from this pilot workshop inform future studies aimed at contextualising trust metrics and strategies for participants' engagement to fit the case of user-VLM interaction.",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.HC",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "Proceedings of the The European Workshop on Trustworthy AI (Trust-AI) at ECAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.13458v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "running"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Computer Vision based group activity detection and action spotting",
      "authors": [
        "Narthana Sivalingam",
        "Santhirarajah Sivasthigan",
        "Thamayanthi Mahendranathan",
        "G. M. R. I. Godaliyadda",
        "M. P. B. Ekanayake",
        "H. M. V. R. Herath"
      ],
      "arxiv_id": "2511.13315v1",
      "summary": "Group activity detection in multi-person scenes is challenging due to complex human interactions, occlusions, and variations in appearance over time. This work presents a computer vision based framework for group activity recognition and action spotting using a combination of deep learning models and graph based relational reasoning. The system first applies Mask R-CNN to obtain accurate actor localization through bounding boxes and instance masks. Multiple backbone networks, including Inception V3, MobileNet, and VGG16, are used to extract feature maps, and RoIAlign is applied to preserve spatial alignment when generating actor specific features. The mask information is then fused with the feature maps to obtain refined masked feature representations for each actor. To model interactions between individuals, we construct Actor Relation Graphs that encode appearance similarity and positional relations using methods such as normalized cross correlation, sum of absolute differences, and dot product. Graph Convolutional Networks operate on these graphs to reason about relationships and predict both individual actions and group level activities. Experiments on the Collective Activity dataset demonstrate that the combination of mask based feature refinement, robust similarity search, and graph neural network reasoning leads to improved recognition performance across both crowded and non crowded scenarios. This approach highlights the potential of integrating segmentation, feature extraction, and relational graph reasoning for complex video understanding tasks.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13315v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack",
      "authors": [
        "Chenyang Li",
        "Wenbing Tang",
        "Yihao Huang",
        "Sinong Simon Zhan",
        "Ming Hu",
        "Xiaojun Jia",
        "Yang Liu"
      ],
      "arxiv_id": "2511.13132v1",
      "summary": "Vision-and-Language Navigation (VLN) agents have made remarkable progress, but their robustness remains insufficiently studied. Existing adversarial evaluations often rely on perturbations that manifest as unusual textures rarely encountered in everyday indoor environments. Errors under such contrived conditions have limited practical relevance, as real-world agents are unlikely to encounter such artificial patterns. In this work, we focus on indoor lighting, an intrinsic yet largely overlooked scene attribute that strongly influences navigation. We propose Indoor Lighting-based Adversarial Attack (ILA), a black-box framework that manipulates global illumination to disrupt VLN agents. Motivated by typical household lighting usage, we design two attack modes: Static Indoor Lighting-based Attack (SILA), where the lighting intensity remains constant throughout an episode, and Dynamic Indoor Lighting-based Attack (DILA), where lights are switched on or off at critical moments to induce abrupt illumination changes. We evaluate ILA on two state-of-the-art VLN models across three navigation tasks. Results show that ILA significantly increases failure rates while reducing trajectory efficiency, revealing previously unrecognized vulnerabilities of VLN agents to realistic indoor lighting variations.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13132v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "A Lightweight 3D Anomaly Detection Method with Rotationally Invariant Features",
      "authors": [
        "Hanzhe Liang",
        "Jie Zhou",
        "Can Gao",
        "Bingyang Guo",
        "Jinbao Wang",
        "Linlin Shen"
      ],
      "arxiv_id": "2511.13115v2",
      "summary": "3D anomaly detection (AD) is a crucial task in computer vision, aiming to identify anomalous points or regions from point cloud data. However, existing methods may encounter challenges when handling point clouds with changes in orientation and position because the resulting features may vary significantly. To address this problem, we propose a novel Rotationally Invariant Features (RIF) framework for 3D AD. Firstly, to remove the adverse effect of variations on point cloud data, we develop a Point Coordinate Mapping (PCM) technique, which maps each point into a rotationally invariant space to maintain consistency of representation. Then, to learn robust and discriminative features, we design a lightweight Convolutional Transform Feature Network (CTF-Net) to extract rotationally invariant features for the memory bank. To improve the ability of the feature extractor, we introduce the idea of transfer learning to pre-train the feature extractor with 3D data augmentation. Experimental results show that the proposed method achieves the advanced performance on the Anomaly-ShapeNet dataset, with an average P-AUROC improvement of 17.7\\%, and also gains the best performance on the Real3D-AD dataset, with an average P-AUROC improvement of 1.6\\%. The strong generalization ability of RIF has been verified by combining it with traditional feature extraction methods on anomaly detection tasks, demonstrating great potential for industrial applications.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-12-14",
      "comment": "Preprint. Accept by Pattern Recognition",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13115v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Orientation-Free Neural Network-Based Bias Estimation for Low-Cost Stationary Accelerometers",
      "authors": [
        "Michal Levin",
        "Itzik Klein"
      ],
      "arxiv_id": "2511.13071v1",
      "summary": "Low-cost micro-electromechanical accelerometers are widely used in navigation, robotics, and consumer devices for motion sensing and position estimation. However, their performance is often degraded by bias errors. To eliminate deterministic bias terms a calibration procedure is applied under stationary conditions. It requires accelerom- eter leveling or complex orientation-dependent calibration procedures. To overcome those requirements, in this paper we present a model-free learning-based calibration method that estimates accelerometer bias under stationary conditions, without requiring knowledge of the sensor orientation and without the need to rotate the sensors. The proposed approach provides a fast, practical, and scalable solution suitable for rapid field deployment. Experimental validation on a 13.39-hour dataset collected from six accelerometers shows that the proposed method consistently achieves error levels more than 52% lower than traditional techniques. On a broader scale, this work contributes to the advancement of accurate calibration methods in orientation-free scenarios. As a consequence, it improves the reliability of low-cost inertial sensors in diverse scientific and industrial applications and eliminates the need for leveled calibration.",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "22 pages, 10 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13071v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Unidirectional-Road-Network-Based Global Path Planning for Cleaning Robots in Semi-Structured Environments",
      "authors": [
        "Yong Li",
        "Hui Cheng"
      ],
      "arxiv_id": "2511.13048v1",
      "summary": "Practical global path planning is critical for commercializing cleaning robots working in semi-structured environments. In the literature, global path planning methods for free space usually focus on path length and neglect the traffic rule constraints of the environments, which leads to high-frequency re-planning and increases collision risks. In contrast, those for structured environments are developed mainly by strictly complying with the road network representing the traffic rule constraints, which may result in an overlong path that hinders the overall navigation efficiency. This article proposes a general and systematic approach to improve global path planning performance in semi-structured environments. A unidirectional road network is built to represent the traffic constraints in semi-structured environments and a hybrid strategy is proposed to achieve a guaranteed planning result.Cutting across the road at the starting and the goal points are allowed to achieve a shorter path. Especially, a two-layer potential map is proposed to achieve a guaranteed performance when the starting and the goal points are in complex intersections. Comparative experiments are carried out to validate the effectiveness of the proposed method. Quantitative experimental results show that, compared with the state-of-art, the proposed method guarantees a much better balance between path length and the consistency with the road network.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "2023 IEEE International Conference on Robotics and Automation (ICRA)",
      "doi": "10.1109/ICRA48891.2023.10161557",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13048v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "DiffPixelFormer: Differential Pixel-Aware Transformer for RGB-D Indoor Scene Segmentation",
      "authors": [
        "Yan Gong",
        "Jianli Lu",
        "Yongsheng Gao",
        "Jie Zhao",
        "Xiaojuan Zhang",
        "Susanto Rahardja"
      ],
      "arxiv_id": "2511.13047v1",
      "summary": "Indoor semantic segmentation is fundamental to computer vision and robotics, supporting applications such as autonomous navigation, augmented reality, and smart environments. Although RGB-D fusion leverages complementary appearance and geometric cues, existing methods often depend on computationally intensive cross-attention mechanisms and insufficiently model intra- and inter-modal feature relationships, resulting in imprecise feature alignment and limited discriminative representation. To address these challenges, we propose DiffPixelFormer, a differential pixel-aware Transformer for RGB-D indoor scene segmentation that simultaneously enhances intra-modal representations and models inter-modal interactions. At its core, the Intra-Inter Modal Interaction Block (IIMIB) captures intra-modal long-range dependencies via self-attention and models inter-modal interactions with the Differential-Shared Inter-Modal (DSIM) module to disentangle modality-specific and shared cues, enabling fine-grained, pixel-level cross-modal alignment. Furthermore, a dynamic fusion strategy balances modality contributions and fully exploits RGB-D information according to scene characteristics. Extensive experiments on the SUN RGB-D and NYUDv2 benchmarks demonstrate that DiffPixelFormer-L achieves mIoU scores of 54.28% and 59.95%, outperforming DFormer-L by 1.78% and 2.75%, respectively. Code is available at https://github.com/gongyan1/DiffPixelFormer.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "11 pages, 5 figures, 5 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13047v1",
      "code_links": [
        {
          "url": "https://github.com/gongyan1/DiffPixelFormer",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "APP: A* Post-Processing Algorithm for Robots with Bidirectional Shortcut and Path Perturbation",
      "authors": [
        "Yong Li",
        "Hui Cheng"
      ],
      "arxiv_id": "2511.13042v1",
      "summary": "Paths generated by A* and other graph-search-based planners are widely used in the robotic field. Due to the restricted node-expansion directions, the resulting paths are usually not the shortest. Besides, unnecessary heading changes, or zig-zag patterns, exist even when no obstacle is nearby, which is inconsistent with the human intuition that the path segments should be straight in wide-open space due to the absence of obstacles. This article puts forward a general and systematic post-processing algorithm for A* and other graph-search-based planners. The A* post-processing algorithm, called APP, is developed based on the costmap, which is widely used in commercial service robots. First, a bidirectional vertices reduction algorithm is proposed to tackle the asymm- etry of the path and the environments. During the forward and backward vertices reduction, a thorough shortcut strategy is put forward to improve the path-shortening performance and avoid unnecessary heading changes. Second, an iterative path perturbation algorithm is adopted to locally reduce the number of unnecessary heading changes and improve the path smooth- ness. Comparative experiments are then carried out to validate the superiority of the proposed method. Quantitative performance indexes show that APP outperforms the existing methods in planning time, path length as well as the number of unnecessary heading changes. Finally, field navigation experiments are carried out to verify the practicability of APP.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "10.1109/LRA.2023.3320432",
      "journal_ref": "IEEE Robotics and Automation Letters ( Volume: 8, Issue: 11, November 2023)",
      "pdf_url": "https://arxiv.org/pdf/2511.13042v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "ArtiWorld: LLM-Driven Articulation of 3D Objects in Scenes",
      "authors": [
        "Yixuan Yang",
        "Luyang Xie",
        "Zhen Luo",
        "Zixiang Zhao",
        "Tongsheng Ding",
        "Mingqi Gao",
        "Feng Zheng"
      ],
      "arxiv_id": "2511.12977v2",
      "summary": "Building interactive simulators and scalable robot-learning environments requires a large number of articulated assets. However, most existing 3D assets in simulation are rigid, and manually converting them into articulated objects is extremely labor- and cost-intensive. This raises a natural question: can we automatically identify articulable objects in a scene and convert them into articulated assets directly? In this paper, we present ArtiWorld, a scene-aware pipeline that localizes candidate articulable objects from textual scene descriptions and reconstructs executable URDF models that preserve the original geometry. At the core of this pipeline is Arti4URDF, which leverages 3D point cloud, prior knowledge of a large language model (LLM), and a URDF-oriented prompt design to rapidly convert rigid objects into interactive URDF-based articulated objects while maintaining their 3D shape. We evaluate ArtiWorld at three levels: 3D simulated objects, full 3D simulated scenes, and real-world scan scenes. Across all three settings, our method consistently outperforms existing approaches and achieves state-of-the-art performance, while preserving object geometry and correctly capturing object interactivity to produce usable URDF-based articulated models. This provides a practical path toward building interactive, robot-ready simulation environments directly from existing 3D assets. Code and data will be released.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-18",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12977v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Generative Photographic Control for Scene-Consistent Video Cinematic Editing",
      "authors": [
        "Huiqiang Sun",
        "Liao Shen",
        "Zhan Peng",
        "Kun Wang",
        "Size Wu",
        "Yuhang Zang",
        "Tianqi Liu",
        "Zihao Huang",
        "Xingyu Zeng",
        "Zhiguo Cao",
        "Wei Li",
        "Chen Change Loy"
      ],
      "arxiv_id": "2511.12921v1",
      "summary": "Cinematic storytelling is profoundly shaped by the artful manipulation of photographic elements such as depth of field and exposure. These effects are crucial in conveying mood and creating aesthetic appeal. However, controlling these effects in generative video models remains highly challenging, as most existing methods are restricted to camera motion control. In this paper, we propose CineCtrl, the first video cinematic editing framework that provides fine control over professional camera parameters (e.g., bokeh, shutter speed). We introduce a decoupled cross-attention mechanism to disentangle camera motion from photographic inputs, allowing fine-grained, independent control without compromising scene consistency. To overcome the shortage of training data, we develop a comprehensive data generation strategy that leverages simulated photographic effects with a dedicated real-world collection pipeline, enabling the construction of a large-scale dataset for robust model training. Extensive experiments demonstrate that our model generates high-fidelity videos with precisely controlled, user-specified photographic camera effects.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12921v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "TOPP-DWR: Time-Optimal Path Parameterization of Differential-Driven Wheeled Robots Considering Piecewise-Constant Angular Velocity Constraints",
      "authors": [
        "Yong Li",
        "Yujun Huang",
        "Yi Chen",
        "Hui Cheng"
      ],
      "arxiv_id": "2511.12910v1",
      "summary": "Differential-driven wheeled robots (DWR) represent the quintessential type of mobile robots and find extensive appli- cations across the robotic field. Most high-performance control approaches for DWR explicitly utilize the linear and angular velocities of the trajectory as control references. However, existing research on time-optimal path parameterization (TOPP) for mobile robots usually neglects the angular velocity and joint vel- ocity constraints, which can result in degraded control perfor- mance in practical applications. In this article, a systematic and practical TOPP algorithm named TOPP-DWR is proposed for DWR and other mobile robots. First, the non-uniform B-spline is adopted to represent the initial trajectory in the task space. Second, the piecewise-constant angular velocity, as well as joint velocity, linear velocity, and linear acceleration constraints, are incorporated into the TOPP problem. During the construction of the optimization problem, the aforementioned constraints are uniformly represented as linear velocity constraints. To boost the numerical computational efficiency, we introduce a slack variable to reformulate the problem into second-order-cone programming (SOCP). Subsequently, comparative experiments are conducted to validate the superiority of the proposed method. Quantitative performance indexes show that TOPP-DWR achieves TOPP while adhering to all constraints. Finally, field autonomous navigation experiments are carried out to validate the practicability of TOPP-DWR in real-world applications.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "comment": "",
      "doi": "",
      "journal_ref": "2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2511.12910v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views",
      "authors": [
        "Junyi Ma",
        "Wentao Bao",
        "Jingyi Xu",
        "Guanzhong Sun",
        "Yu Zheng",
        "Erhang Zhang",
        "Xieyuanli Chen",
        "Hesheng Wang"
      ],
      "arxiv_id": "2511.12878v3",
      "summary": "Forecasting how human hands move in egocentric views is critical for applications like augmented reality and human-robot policy transfer. Recently, several hand trajectory prediction (HTP) methods have been developed to generate future possible hand waypoints, which still suffer from insufficient prediction targets, inherent modality gaps, entangled hand-head motion, and limited validation in downstream tasks. To address these limitations, we present a universal hand motion forecasting framework considering multi-modal input, multi-dimensional and multi-target prediction patterns, and multi-task affordances for downstream applications. We harmonize multiple modalities by vision-language fusion, global context incorporation, and task-aware text embedding injection, to forecast hand waypoints in both 2D and 3D spaces. A novel dual-branch diffusion is proposed to concurrently predict human head and hand movements, capturing their motion synergy in egocentric vision. By introducing target indicators, the prediction model can forecast the specific joint waypoints of the wrist or the fingers, besides the widely studied hand center points. In addition, we enable Uni-Hand to additionally predict hand-object interaction states (contact/separation) to facilitate downstream tasks better. As the first work to incorporate downstream task evaluation in the literature, we build novel benchmarks to assess the real-world applicability of hand motion forecasting algorithms. The experimental results on multiple publicly available datasets and our newly proposed benchmarks demonstrate that Uni-Hand achieves the state-of-the-art performance in multi-dimensional and multi-target hand motion forecasting. Extensive validation in multiple downstream tasks also presents its impressive human-robot policy transfer to enable robotic manipulation, and effective feature enhancement for action anticipation/recognition.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-17",
      "updated": "2025-12-05",
      "comment": "Extended journal version of MMTwin (IROS'25). Code and data: https://github.com/IRMVLab/UniHand",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12878v3",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "BridgeEQA: Virtual Embodied Agents for Real Bridge Inspections",
      "authors": [
        "Subin Varghese",
        "Joshua Gao",
        "Asad Ur Rahman",
        "Vedhus Hoskere"
      ],
      "arxiv_id": "2511.12676v1",
      "summary": "Deploying embodied agents that can answer questions about their surroundings in realistic real-world settings remains difficult, partly due to the scarcity of benchmarks that faithfully capture practical operating conditions. We propose infrastructure inspection as a compelling domain for open-vocabulary Embodied Question Answering (EQA): it naturally demands multi-scale reasoning, long-range spatial understanding, and complex semantic relationships, while offering unique evaluation advantages via standardized National Bridge Inventory (NBI) condition ratings (0-9), professional inspection reports, and egocentric imagery.\n  We introduce BridgeEQA, a benchmark of 2,200 open-vocabulary question-answer pairs (in the style of OpenEQA) grounded in professional inspection reports across 200 real-world bridge scenes with 47.93 images on average per scene. Questions require synthesizing visual evidence across multiple images and aligning responses with NBI condition ratings. We further propose a new EQA metric Image Citation Relevance to evaluate the ability of a model to cite relevant images.\n  Evaluations of state-of-the-art vision-language models reveal substantial performance gaps under episodic memory EQA settings. To address this, we propose Embodied Memory Visual Reasoning (EMVR), which formulates inspection as sequential navigation over an image-based scene graph: images are nodes, and an agent takes actions to traverse views, compare evidence, and reason within a Markov decision process. EMVR shows strong performance over the baselines. We publicly release both the dataset and code.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-16",
      "updated": "2025-11-16",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12676v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "C3Net: Context-Contrast Network for Camouflaged Object Detection",
      "authors": [
        "Baber Jan",
        "Aiman H. El-Maleh",
        "Abdul Jabbar Siddiqui",
        "Abdul Bais",
        "Saeed Anwar"
      ],
      "arxiv_id": "2511.12627v1",
      "summary": "Camouflaged object detection identifies objects that blend seamlessly with their surroundings through similar colors, textures, and patterns. This task challenges both traditional segmentation methods and modern foundation models, which fail dramatically on camouflaged objects. We identify six fundamental challenges in COD: Intrinsic Similarity, Edge Disruption, Extreme Scale Variation, Environmental Complexities, Contextual Dependencies, and Salient-Camouflaged Object Disambiguation. These challenges frequently co-occur and compound the difficulty of detection, requiring comprehensive architectural solutions. We propose C3Net, which addresses all challenges through a specialized dual-pathway decoder architecture. The Edge Refinement Pathway employs gradient-initialized Edge Enhancement Modules to recover precise boundaries from early features. The Contextual Localization Pathway utilizes our novel Image-based Context Guidance mechanism to achieve intrinsic saliency suppression without external models. An Attentive Fusion Module synergistically combines the two pathways via spatial gating. C3Net achieves state-of-the-art performance with S-measures of 0.898 on COD10K, 0.904 on CAMO, and 0.913 on NC4K, while maintaining efficient processing. C3Net demonstrates that complex, multifaceted detection challenges require architectural innovation, with specialized components working synergistically to achieve comprehensive coverage beyond isolated improvements. Code, model weights, and results are available at https://github.com/Baber-Jan/C3Net.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-16",
      "updated": "2025-11-16",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12627v1",
      "code_links": [
        {
          "url": "https://github.com/Baber-Jan/C3Net",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Visible Structure Retrieval for Lightweight Image-Based Relocalisation",
      "authors": [
        "Fereidoon Zangeneh",
        "Leonard Bruns",
        "Amit Dekel",
        "Alessandro Pieropan",
        "Patric Jensfelt"
      ],
      "arxiv_id": "2511.12503v1",
      "summary": "Accurate camera pose estimation from an image observation in a previously mapped environment is commonly done through structure-based methods: by finding correspondences between 2D keypoints on the image and 3D structure points in the map. In order to make this correspondence search tractable in large scenes, existing pipelines either rely on search heuristics, or perform image retrieval to reduce the search space by comparing the current image to a database of past observations. However, these approaches result in elaborate pipelines or storage requirements that grow with the number of past observations. In this work, we propose a new paradigm for making structure-based relocalisation tractable. Instead of relying on image retrieval or search heuristics, we learn a direct mapping from image observations to the visible scene structure in a compact neural network. Given a query image, a forward pass through our novel visible structure retrieval network allows obtaining the subset of 3D structure points in the map that the image views, thus reducing the search space of 2D-3D correspondences. We show that our proposed method enables performing localisation with an accuracy comparable to the state of the art, while requiring lower computational and storage footprint.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-16",
      "updated": "2025-11-16",
      "comment": "Accepted at BMVC 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12503v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "pose estimation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Multilaminate piezoelectric PVDF actuators to enhance performance of soft micro robots",
      "authors": [
        "Nicholas Gunter",
        "Heiko Kabutz",
        "Kaushik Jayaram"
      ],
      "arxiv_id": "2511.12380v1",
      "summary": "Multilayer piezoelectric polyvinylidene fluoride (PVDF) actuators are a promising approach to enhance performance of soft microrobotic systems. In this work, we develop and characterize multilayer PVDF actuators with parallel voltage distribution across each layer, bridging a unique design space between brittle high-force PZT stacks and compliant but lower-bandwidth soft polymer actuators. We show the effects of layer thickness and number of layers in actuator performance and their agreement with a first principles model. By varying these parameters, we demonstrate actuators capable of >3 mm of free deflection, >20 mN of blocked force, and >=500 Hz, while operating at voltages as low as 150 volts. To illustrate their potential for robotic integration, we integrate our actuators into a planar, translating microrobot that leverages resonance to achieve locomotion with robustness to large perturbations.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-15",
      "updated": "2025-11-15",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12380v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "locomotion"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "AttackVLA: Benchmarking Adversarial and Backdoor Attacks on Vision-Language-Action Models",
      "authors": [
        "Jiayu Li",
        "Yunhan Zhao",
        "Xiang Zheng",
        "Zonghuan Xu",
        "Yige Li",
        "Xingjun Ma",
        "Yu-Gang Jiang"
      ],
      "arxiv_id": "2511.12149v1",
      "summary": "Vision-Language-Action (VLA) models enable robots to interpret natural-language instructions and perform diverse tasks, yet their integration of perception, language, and control introduces new safety vulnerabilities. Despite growing interest in attacking such models, the effectiveness of existing techniques remains unclear due to the absence of a unified evaluation framework. One major issue is that differences in action tokenizers across VLA architectures hinder reproducibility and fair comparison. More importantly, most existing attacks have not been validated in real-world scenarios. To address these challenges, we propose AttackVLA, a unified framework that aligns with the VLA development lifecycle, covering data construction, model training, and inference. Within this framework, we implement a broad suite of attacks, including all existing attacks targeting VLAs and multiple adapted attacks originally developed for vision-language models, and evaluate them in both simulation and real-world settings. Our analysis of existing attacks reveals a critical gap: current methods tend to induce untargeted failures or static action states, leaving targeted attacks that drive VLAs to perform precise long-horizon action sequences largely unexplored. To fill this gap, we introduce BackdoorVLA, a targeted backdoor attack that compels a VLA to execute an attacker-specified long-horizon action sequence whenever a trigger is present. We evaluate BackdoorVLA in both simulated benchmarks and real-world robotic settings, achieving an average targeted success rate of 58.4% and reaching 100% on selected tasks. Our work provides a standardized framework for evaluating VLA vulnerabilities and demonstrates the potential for precise adversarial manipulation, motivating further research on securing VLA-based embodied systems.",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CR",
      "published": "2025-11-15",
      "updated": "2025-11-15",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12149v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Towards Obstacle-Avoiding Control of Planar Snake Robots Exploring Neuro-Evolution of Augmenting Topologies",
      "authors": [
        "Advik Sinha",
        "Akshay Arjun",
        "Abhijit Das",
        "Joyjit Mukherjee"
      ],
      "arxiv_id": "2511.12148v1",
      "summary": "This work aims to develop a resource-efficient solution for obstacle-avoiding tracking control of a planar snake robot in a densely cluttered environment with obstacles. Particularly, Neuro-Evolution of Augmenting Topologies (NEAT) has been employed to generate dynamic gait parameters for the serpenoid gait function, which is implemented on the joint angles of the snake robot, thus controlling the robot on a desired dynamic path. NEAT is a single neural-network based evolutionary algorithm that is known to work extremely well when the input layer is of significantly higher dimension and the output layer is of a smaller size. For the planar snake robot, the input layer consists of the joint angles, link positions, head link position as well as obstacle positions in the vicinity. However, the output layer consists of only the frequency and offset angle of the serpenoid gait that control the speed and heading of the robot, respectively. Obstacle data from a LiDAR and the robot data from various sensors, along with the location of the end goal and time, are employed to parametrize a reward function that is maximized over iterations by selective propagation of superior neural networks. The implementation and experimental results showcase that the proposed approach is computationally efficient, especially for large environments with many obstacles. The proposed framework has been verified through a physics engine simulation study on PyBullet. The approach shows superior results to existing state-of-the-art methodologies and comparable results to the very recent CBRL approach with significantly lower computational overhead. The video of the simulation can be found here: https://sites.google.com/view/neatsnakerobot",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-15",
      "updated": "2025-11-15",
      "comment": "9 pages, 6 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12148v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "gait"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "GLOW: Global Illumination-Aware Inverse Rendering of Indoor Scenes Captured with Dynamic Co-Located Light & Camera",
      "authors": [
        "Jiaye Wu",
        "Saeed Hadadan",
        "Geng Lin",
        "Peihan Tu",
        "Matthias Zwicker",
        "David Jacobs",
        "Roni Sengupta"
      ],
      "arxiv_id": "2511.22857v1",
      "summary": "Inverse rendering of indoor scenes remains challenging due to the ambiguity between reflectance and lighting, exacerbated by inter-reflections among multiple objects. While natural illumination-based methods struggle to resolve this ambiguity, co-located light-camera setups offer better disentanglement as lighting can be easily calibrated via Structure-from-Motion. However, such setups introduce additional complexities like strong inter-reflections, dynamic shadows, near-field lighting, and moving specular highlights, which existing approaches fail to handle. We present GLOW, a Global Illumination-aware Inverse Rendering framework designed to address these challenges. GLOW integrates a neural implicit surface representation with a neural radiance cache to approximate global illumination, jointly optimizing geometry and reflectance through carefully designed regularization and initialization. We then introduce a dynamic radiance cache that adapts to sharp lighting discontinuities from near-field motion, and a surface-angle-weighted radiometric loss to suppress specular artifacts common in flashlight captures. Experiments show that GLOW substantially outperforms prior methods in material reflectance estimation under both natural and co-located illumination.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-28",
      "updated": "2025-11-28",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22857v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "neural radiance"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Safe Autonomous Lane Changing: Planning with Dynamic Risk Fields and Time-Varying Convex Space Generation",
      "authors": [
        "Zhen Tian",
        "Zhihao Lin"
      ],
      "arxiv_id": "2511.22829v1",
      "summary": "This paper presents a novel trajectory planning pipeline for complex driving scenarios like autonomous lane changing, by integrating risk-aware planning with guaranteed collision avoidance into a unified optimization framework. We first construct a dynamic risk fields (DRF) that captures both the static and dynamic collision risks from surrounding vehicles. Then, we develop a rigorous strategy for generating time-varying convex feasible spaces that ensure kinematic feasibility and safety requirements. The trajectory planning problem is formulated as a finite-horizon optimal control problem and solved using a constrained iterative Linear Quadratic Regulator (iLQR) algorithm that jointly optimizes trajectory smoothness, control effort, and risk exposure while maintaining strict feasibility. Extensive simulations demonstrate that our method outperforms traditional approaches in terms of safety and efficiency, achieving collision-free trajectories with shorter lane-changing distances (28.59 m) and times (2.84 s) while maintaining smooth and comfortable acceleration patterns. In dense roundabout environments the planner further demonstrates robust adaptability, producing larger safety margins, lower jerk, and superior curvature smoothness compared with APF, MPC, and RRT based baselines. These results confirm that the integrated DRF with convex feasible space and constrained iLQR solver provides a balanced solution for safe, efficient, and comfortable trajectory generation in dynamic and interactive traffic scenarios.",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-28",
      "updated": "2025-11-28",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22829v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "MPC"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Emergent Extreme-View Geometry in 3D Foundation Models",
      "authors": [
        "Yiwen Zhang",
        "Joseph Tung",
        "Ruojin Cai",
        "David Fouhey",
        "Hadar Averbuch-Elor"
      ],
      "arxiv_id": "2511.22686v2",
      "summary": "3D foundation models (3DFMs) have recently transformed 3D vision, enabling joint prediction of depths, poses, and point maps directly from images. Yet their ability to reason under extreme, non-overlapping views remains largely unexplored. In this work, we study their internal representations and find that 3DFMs exhibit an emergent understanding of extreme-view geometry, despite never being trained for such conditions. To further enhance these capabilities, we introduce a lightweight alignment scheme that refines their internal 3D representation by tuning only a small subset of backbone bias terms, leaving all decoder heads frozen. This targeted adaptation substantially improves relative pose estimation under extreme viewpoints without degrading per-image depth or point quality. Additionally, we contribute MegaUnScene, a new benchmark of Internet scenes unseen by existing 3DFMs, with dedicated test splits for both relative pose estimation and dense 3D reconstruction. All code and data will be released.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-27",
      "updated": "2025-12-01",
      "comment": "Project page is at https://ext-3dfms.github.io/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22686v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "pose estimation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "GazeTrack: High-Precision Eye Tracking Based on Regularization and Spatial Computing",
      "authors": [
        "Xiaoyin Yang"
      ],
      "arxiv_id": "2511.22607v1",
      "summary": "Eye tracking has become increasingly important in virtual and augmented reality applications; however, the current gaze accuracy falls short of meeting the requirements for spatial computing. We designed a gaze collection framework and utilized high-precision equipment to gather the first precise benchmark dataset, GazeTrack, encompassing diverse ethnicities, ages, and visual acuity conditions for pupil localization and gaze tracking. We propose a novel shape error regularization method to constrain pupil ellipse fitting and train on open-source datasets, enhancing semantic segmentation and pupil position prediction accuracy. Additionally, we invent a novel coordinate transformation method similar to paper unfolding to accurately predict gaze vectors on the GazeTrack dataset. Finally, we built a gaze vector generation model that achieves reduced gaze angle error with lower computational complexity compared to other methods.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "10 pages, 7 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22607v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization",
      "authors": [
        "Yifan Du",
        "Kun Zhou",
        "Yingqian Min",
        "Yue Ling",
        "Wayne Xin Zhao",
        "Youbin Wu"
      ],
      "arxiv_id": "2511.22586v1",
      "summary": "We study how different Chain-of-Thought (CoT) designs affect the acquisition of the generalizable visual reasoning ability in vision-language models (VLMs). While CoT data, especially long or visual CoT such as \"think with image\", has been widely used to supervise intermediate reasoning, it remains unclear why specific CoT designs help and which ones truly support generalizable reasoning. To systematically evaluate this, we focus on a controlled maze-solving benchmark where reasoning rules are fully visual, difficulty can be tuned by grid size, and all the intermediate steps can be automatically generated. Using Qwen2.5-VL-7B under a standard SFT-then-RL pipeline, we compare three representative CoT formats: Language CoT, Grounding CoT (with spatial coordinate trajectories), and Visual CoT (with image manipulations). Our experiments reveal that visual and longer CoT mainly accelerate convergence but do not lift the final performance ceiling; concise CoT containing only essential grounding steps outperforms longer traces; and, strikingly, CoT retaining only the minimal grounding results generalizes best across different maze sizes. We further validate these insights on other vision-centric tasks. These findings highlight a \"short is long\" effect and provide practical guidance for constructing more generalizable SFT datasets for visual reasoning.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22586v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Text Condition Embedded Regression Network for Automated Dental Abutment Design",
      "authors": [
        "Mianjie Zheng",
        "Xinquan Yang",
        "Xuguang Li",
        "Xiaoling Luo",
        "Xuefen Liu",
        "Kun Tang",
        "He Meng",
        "Linlin Shen"
      ],
      "arxiv_id": "2511.22578v1",
      "summary": "The abutment is an important part of artificial dental implants, whose design process is time-consuming and labor-intensive. Long-term use of inappropriate dental implant abutments may result in implant complications, including peri-implantitis. Using artificial intelligence to assist dental implant abutment design can quickly improve the efficiency of abutment design and enhance abutment adaptability. In this paper, we propose a text condition embedded abutment design framework (TCEAD), the novel automated abutment design solution available in literature. The proposed study extends the self-supervised learning framework of the mesh mask autoencoder (MeshMAE) by introducing a text-guided localization (TGL) module to facilitate abutment area localization. As the parameter determination of the abutment is heavily dependent on local fine-grained features (the width and height of the implant and the distance to the opposing tooth), we pre-train the encoder using oral scan data to improve the model's feature extraction ability. Moreover, considering that the abutment area is only a small part of the oral scan data, we designed a TGL module, which introduces the description of the abutment area through the text encoder of Contrastive Language-Image Pre-training (CLIP), enabling the network to quickly locate the abutment area. We validated the performance of TCEAD on a large abutment design dataset. Extensive experiments demonstrate that TCEAD achieves an Intersection over Union (IoU) improvement of 0.8%-12.85% over other mainstream methods, underscoring its potential in automated dental abutment design.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22578v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "MoLT: Mixture of Layer-Wise Tokens for Efficient Audio-Visual Learning",
      "authors": [
        "Kyeongha Rho",
        "Hyeongkeun Lee",
        "Jae Won Cho",
        "Joon Son Chung"
      ],
      "arxiv_id": "2512.00115v1",
      "summary": "In this paper, we propose Mixture of Layer-Wise Tokens (MoLT), a parameter- and memory-efficient adaptation framework for audio-visual learning. The key idea of MoLT is to replace conventional, computationally heavy sequential adaptation at every transformer layer with a parallel, lightweight scheme that extracts and fuses layer-wise tokens only from the late layers. We adopt two types of adapters to distill modality-specific information and cross-modal interaction into compact latent tokens in a layer-wise manner. A token fusion module then dynamically fuses these layer-wise tokens by taking into account their relative significance. To prevent the redundancy of latent tokens, we apply an orthogonality regularization between latent tokens during training. Through the systematic analysis of the position of adaptation in the pre-trained transformers, we extract latent tokens only from the late layers of the transformers. This strategic adaptation approach avoids error propagation from the volatile early-layer features, thereby maximizing the adaptation performance while maintaining parameter and memory efficiency. Through extensive experiments, we demonstrate that MoLT outperforms existing methods on diverse audio-visual benchmarks, including Audio-Visual Question Answering, Audio-Visual Segmentation, and Audio-Visual Event Localization.",
      "categories": [
        "cs.SD",
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.SD",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "10 pages, 5 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00115v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Fin3R: Fine-tuning Feed-forward 3D Reconstruction Models via Monocular Knowledge Distillation",
      "authors": [
        "Weining Ren",
        "Hongjun Wang",
        "Xiao Tan",
        "Kai Han"
      ],
      "arxiv_id": "2511.22429v1",
      "summary": "We present Fin3R, a simple, effective, and general fine-tuning method for feed-forward 3D reconstruction models. The family of feed-forward reconstruction model regresses pointmap of all input images to a reference frame coordinate system, along with other auxiliary outputs, in a single forward pass. However, we find that current models struggle with fine geometry and robustness due to (\\textit{i}) the scarcity of high-fidelity depth and pose supervision and (\\textit{ii}) the inherent geometric misalignment from multi-view pointmap regression. Fin3R jointly tackles two issues with an extra lightweight fine-tuning step. We freeze the decoder, which handles view matching, and fine-tune only the image encoder-the component dedicated to feature extraction. The encoder is enriched with fine geometric details distilled from a strong monocular teacher model on large, unlabeled datasets, using a custom, lightweight LoRA adapter. We validate our method on a wide range of models, including DUSt3R, MASt3R, CUT3R, and VGGT. The fine-tuned models consistently deliver sharper boundaries, recover complex structures, and achieve higher geometric accuracy in both single- and multi-view settings, while adding only the tiny LoRA weights, which leave test-time memory and latency virtually unchanged. Project page: \\href{http://visual-ai.github.io/fin3r}{https://visual-ai.github.io/fin3r}",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "NeurIPS 2025",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22429v1",
      "code_links": [
        {
          "url": "http://visual-ai.github.io/fin3r",
          "type": "project_page"
        },
        {
          "url": "https://visual-ai.github.io/fin3r",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "VGGT"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Prompt-based Consistent Video Colorization",
      "authors": [
        "Silvia Dani",
        "Tiberio Uricchio",
        "Lorenzo Seidenari"
      ],
      "arxiv_id": "2511.22330v1",
      "summary": "Existing video colorization methods struggle with temporal flickering or demand extensive manual input. We propose a novel approach automating high-fidelity video colorization using rich semantic guidance derived from language and segmentation. We employ a language-conditioned diffusion model to colorize grayscale frames. Guidance is provided via automatically generated object masks and textual prompts; our primary automatic method uses a generic prompt, achieving state-of-the-art results without specific color input. Temporal stability is achieved by warping color information from previous frames using optical flow (RAFT); a correction step detects and fixes inconsistencies introduced by warping. Evaluations on standard benchmarks (DAVIS30, VIDEVO20) show our method achieves state-of-the-art performance in colorization accuracy (PSNR) and visual realism (Colorfulness, CDC), demonstrating the efficacy of automated prompt-based guidance for consistent video colorization.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22330v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "optical flow"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Content Adaptive Encoding For Interactive Game Streaming",
      "authors": [
        "Shakarim Soltanayev",
        "Odysseas Zisimopoulos",
        "Mohammad Ashraful Anam",
        "Man Cheung Kung",
        "Angeliki Katsenou",
        "Yiannis Andreopoulos"
      ],
      "arxiv_id": "2511.22327v1",
      "summary": "Video-on-demand streaming has benefitted from \\textit{content-adaptive encoding} (CAE), i.e., adaptation of resolution and/or quantization parameters for each scene based on convex hull optimization. However, CAE is very challenging to develop and deploy for interactive game streaming (IGS). Commercial IGS services impose ultra-low latency encoding with no lookahead or buffering, and have extremely tight compute constraints for any CAE algorithm execution. We propose the first CAE approach for resolution adaptation in IGS based on compact encoding metadata from past frames. Specifically, we train a convolutional neural network (CNN) to infer the best resolution from the options available for the upcoming scene based on a running window of aggregated coding block statistics from the current scene. By deploying the trained CNN within a practical IGS setup based on HEVC encoding, our proposal: (i) improves over the default fixed-resolution ladder of HEVC by 2.3 Bjøntegaard Delta-VMAF points; (ii) infers using 1ms of a single CPU core per scene, thereby having no latency overhead.",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "5 pages",
      "doi": "",
      "journal_ref": "Picture Coding Symposium 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.22327v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "running"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "BrepGPT: Autoregressive B-rep Generation with Voronoi Half-Patch",
      "authors": [
        "Pu Li",
        "Wenhao Zhang",
        "Weize Quan",
        "Biao Zhang",
        "Peter Wonka",
        "Dong-Ming Yan"
      ],
      "arxiv_id": "2511.22171v1",
      "summary": "Boundary representation (B-rep) is the de facto standard for CAD model representation in modern industrial design. The intricate coupling between geometric and topological elements in B-rep structures has forced existing generative methods to rely on cascaded multi-stage networks, resulting in error accumulation and computational inefficiency. We present BrepGPT, a single-stage autoregressive framework for B-rep generation. Our key innovation lies in the Voronoi Half-Patch (VHP) representation, which decomposes B-reps into unified local units by assigning geometry to nearest half-edges and sampling their next pointers. Unlike hierarchical representations that require multiple distinct encodings for different structural levels, our VHP representation facilitates unifying geometric attributes and topological relations in a single, coherent format. We further leverage dual VQ-VAEs to encode both vertex topology and Voronoi Half-Patches into vertex-based tokens, achieving a more compact sequential encoding. A decoder-only Transformer is then trained to autoregressively predict these tokens, which are subsequently mapped to vertex-based features and decoded into complete B-rep models. Experiments demonstrate that BrepGPT achieves state-of-the-art performance in unconditional B-rep generation. The framework also exhibits versatility in various applications, including conditional generation from category labels, point clouds, text descriptions, and images, as well as B-rep autocompletion and interpolation.",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22171v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action",
      "authors": [
        "Zhen Fang",
        "Zhuoyang Liu",
        "Jiaming Liu",
        "Hao Chen",
        "Yu Zeng",
        "Shiting Huang",
        "Zehui Chen",
        "Lin Chen",
        "Shanghang Zhang",
        "Feng Zhao"
      ],
      "arxiv_id": "2511.22134v1",
      "summary": "To build a generalizable Vision-Language-Action (VLA) model with strong reasoning ability, a common strategy is to first train a specialist VLA on robot demonstrations to acquire reliable manipulation skills, and then incorporate mixed annotated robot data together with multimodal data to restore broader reasoning capabilities. However, we observe that the resulting reasoning VLA often suffers from degraded action performance compared to the specialist model before fine-tuning, a phenomenon we refer to as action degeneration. To address this issue, we propose DualVLA, which enhances action performance through carefully designed post-training while still preserving reasoning capability. We first introduce a dual-layer data pruning method that removes redundant embodied reasoning, preventing it from adversely influencing action learning. To further strengthen action generation, we design a dual-teacher adaptive distillation strategy that assigns different supervision signals to different data domains while maintaining reasoning ability. To fill the evaluation gap for generalist VLAs, we also propose VLA Score, which decouples VLA capability into reasoning, intention, action, and alignment dimensions for a more fine-grained assessment. Experiments show that DualVLA achieves an average success rate of 61.0 in SimplerEnv and an average score of 65.4 across eight competitive multimodal benchmarks, demonstrating a stronger balance between precise action execution and multimodal understanding. Project Website: https://costaliya.github.io/DualVLA/.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22134v1",
      "code_links": [
        {
          "url": "https://costaliya.github.io/DualVLA/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "SoftNash: Entropy-Regularized Nash Games for Non-Fighting Virtual Fixtures",
      "authors": [
        "Tai Inui",
        "Jee-Hwan Ryu"
      ],
      "arxiv_id": "2511.22087v1",
      "summary": "Virtual fixtures (VFs) improve precision in teleoperation but often ``fight'' the user, inflating mental workload and eroding the sense of agency. We propose Soft-Nash Virtual Fixtures, a game-theoretic shared-control policy that softens the classic two-player linear-quadratic (LQ) Nash solution by inflating the fixture's effort weight with a single, interpretable scalar parameter $τ$. This yields a continuous dial on controller assertiveness: $τ=0$ recovers a hard, performance-focused Nash / virtual fixture controller, while larger $τ$ reduce gains and pushback, yet preserve the equilibrium structure and continuity of closed-loop stability. We derive Soft-Nash from both a KL-regularized trust-region and a maximum-entropy viewpoint, obtaining a closed-form robot best response that shrinks authority and aligns the fixture with the operator's input as $τ$ grows. We implement Soft-Nash on a 6-DoF haptic device in 3D tracking task ($n=12$). Moderate softness ($τ\\approx 1-3$, especially $τ=2$) maintains tracking error statistically indistinguishable from a tuned classic VF while sharply reducing controller-user conflict, lowering NASA-TLX workload, and increasing Sense of Agency (SoAS). A composite BalancedScore that combines normalized accuracy and non-fighting behavior peaks near $τ=2-3$. These results show that a one-parameter Soft-Nash policy can preserve accuracy while improving comfort and perceived agency, providing a practical and interpretable pathway to personalized shared control in haptics and teleoperation.",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22087v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "teleoperation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Constant-Volume Deformation Manufacturing for Material-Efficient Shaping",
      "authors": [
        "Lei Li",
        "Jiale Gong",
        "Ziyang Li",
        "Hong Wang"
      ],
      "arxiv_id": "2511.22042v1",
      "summary": "Additive and subtractive manufacturing enable complex geometries but rely on discrete stacking or local removal, limiting continuous and controllable deformation and causing volume loss and shape deviations. We present a volumepreserving digital-mold paradigm that integrates real-time volume-consistency modeling with geometry-informed deformation prediction and an error-compensation strategy to achieve highly predictable shaping of plastic materials. By analyzing deformation patterns and error trends from post-formed point clouds, our method corrects elastic rebound and accumulation errors, maintaining volume consistency and surface continuity. Experiments on five representative geometries demonstrate that the system reproduces target shapes with high fidelity while achieving over 98% material utilization. This approach establishes a digitally driven, reproducible pathway for sustainable, zero-waste shaping of user-defined designs, bridging digital modeling, real-time sensing, and adaptive forming, and advancing next-generation sustainable and customizable manufacturing.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-27",
      "updated": "2025-11-27",
      "comment": "46 pages, 27 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.22042v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "PAT3D: Physics-Augmented Text-to-3D Scene Generation",
      "authors": [
        "Guying Lin",
        "Kemeng Huang",
        "Michael Liu",
        "Ruihan Gao",
        "Hanke Chen",
        "Lyuhao Chen",
        "Beijia Lu",
        "Taku Komura",
        "Yuan Liu",
        "Jun-Yan Zhu",
        "Minchen Li"
      ],
      "arxiv_id": "2511.21978v1",
      "summary": "We introduce PAT3D, the first physics-augmented text-to-3D scene generation framework that integrates vision-language models with physics-based simulation to produce physically plausible, simulation-ready, and intersection-free 3D scenes. Given a text prompt, PAT3D generates 3D objects, infers their spatial relations, and organizes them into a hierarchical scene tree, which is then converted into initial conditions for simulation. A differentiable rigid-body simulator ensures realistic object interactions under gravity, driving the scene toward static equilibrium without interpenetrations. To further enhance scene quality, we introduce a simulation-in-the-loop optimization procedure that guarantees physical stability and non-intersection, while improving semantic consistency with the input prompt. Experiments demonstrate that PAT3D substantially outperforms prior approaches in physical plausibility, semantic consistency, and visual quality. Beyond high-quality generation, PAT3D uniquely enables simulation-ready 3D scenes for downstream tasks such as scene editing and robotic manipulation. Code and data will be released upon acceptance.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "19 pages, 12 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21978v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "AmodalGen3D: Generative Amodal 3D Object Reconstruction from Sparse Unposed Views",
      "authors": [
        "Junwei Zhou",
        "Yu-Wing Tai"
      ],
      "arxiv_id": "2511.21945v1",
      "summary": "Reconstructing 3D objects from a few unposed and partially occluded views is a common yet challenging problem in real-world scenarios, where many object surfaces are never directly observed. Traditional multi-view or inpainting-based approaches struggle under such conditions, often yielding incomplete or geometrically inconsistent reconstructions. We introduce AmodalGen3D, a generative framework for amodal 3D object reconstruction that infers complete, occlusion-free geometry and appearance from arbitrary sparse inputs. The model integrates 2D amodal completion priors with multi-view stereo geometry conditioning, supported by a View-Wise Cross Attention mechanism for sparse-view feature fusion and a Stereo-Conditioned Cross Attention module for unobserved structure inference. By jointly modeling visible and hidden regions, AmodalGen3D faithfully reconstructs 3D objects that are consistent with sparse-view constraints while plausibly hallucinating unseen parts. Experiments on both synthetic and real-world datasets demonstrate that AmodalGen3D achieves superior fidelity and completeness under occlusion-heavy sparse-view settings, addressing a pressing need for object-level 3D scene reconstruction in robotics, AR/VR, and embodied AI applications.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "18 pages, 14 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21945v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene reconstruction"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training",
      "authors": [
        "Haotian Xue",
        "Qi Chen",
        "Zhonghao Wang",
        "Xun Huang",
        "Eli Shechtman",
        "Jinrong Xie",
        "Yongxin Chen"
      ],
      "arxiv_id": "2511.21592v1",
      "summary": "Video diffusion models achieve strong frame-level fidelity but still struggle with motion coherence, dynamics and realism, often producing jitter, ghosting, or implausible dynamics. A key limitation is that the standard denoising MSE objective provides no direct supervision on temporal consistency, allowing models to achieve low loss while still generating poor motion. We propose MoGAN, a motion-centric post-training framework that improves motion realism without reward models or human preference data. Built atop a 3-step distilled video diffusion model, we train a DiT-based optical-flow discriminator to differentiate real from generated motion, combined with a distribution-matching regularizer to preserve visual fidelity. With experiments on Wan2.1-T2V-1.3B, MoGAN substantially improves motion quality across benchmarks. On VBench, MoGAN boosts motion score by +7.3% over the 50-step teacher and +13.3% over the 3-step DMD model. On VideoJAM-Bench, MoGAN improves motion score by +7.4% over the teacher and +8.8% over DMD, while maintaining comparable or even better aesthetic and image-quality scores. A human study further confirms that MoGAN is preferred for motion quality (52% vs. 38% for the teacher; 56% vs. 29% for DMD). Overall, MoGAN delivers significantly more realistic motion without sacrificing visual fidelity or efficiency, offering a practical path toward fast, high-quality video generation. Project webpage is: https://xavihart.github.io/mogan.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21592v1",
      "code_links": [
        {
          "url": "https://xavihart.github.io/mogan",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "optical flow"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Resolution Where It Counts: Hash-based GPU-Accelerated 3D Reconstruction via Variance-Adaptive Voxel Grids",
      "authors": [
        "Lorenzo De Rebotti",
        "Emanuele Giacomini",
        "Giorgio Grisetti",
        "Luca Di Giammarino"
      ],
      "arxiv_id": "2511.21459v1",
      "summary": "Efficient and scalable 3D surface reconstruction from range data remains a core challenge in computer graphics and vision, particularly in real-time and resource-constrained scenarios. Traditional volumetric methods based on fixed-resolution voxel grids or hierarchical structures like octrees often suffer from memory inefficiency, computational overhead, and a lack of GPU support. We propose a novel variance-adaptive, multi-resolution voxel grid that dynamically adjusts voxel size based on the local variance of signed distance field (SDF) observations. Unlike prior multi-resolution approaches that rely on recursive octree structures, our method leverages a flat spatial hash table to store all voxel blocks, supporting constant-time access and full GPU parallelism. This design enables high memory efficiency and real-time scalability. We further demonstrate how our representation supports GPU-accelerated rendering through a parallel quad-tree structure for Gaussian Splatting, enabling effective control over splat density. Our open-source CUDA/C++ implementation achieves up to 13x speedup and 4x lower memory usage compared to fixed-resolution baselines, while maintaining on par results in terms of reconstruction accuracy, offering a practical and extensible solution for high-performance 3D reconstruction.",
      "categories": [
        "cs.GR"
      ],
      "primary_category": "cs.GR",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "Accepted for publication in ACM Transaction on Graphics. Project site: https://rvp-group.github.io/mrhash/",
      "doi": "10.1145/3777909",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21459v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "gaussian splatting"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Dual Preintegration for Relative State Estimation",
      "authors": [
        "Ruican Xia",
        "Hailong Pei"
      ],
      "arxiv_id": "2511.21189v1",
      "summary": "Relative State Estimation perform mutually localization between two mobile agents undergoing six-degree-of-freedom motion. Based on the principle of circular motion, the estimation accuracy is sensitive to nonlinear rotations of the reference platform, particularly under large inter-platform distances. This phenomenon is even obvious for linearized kinematics, because cumulative linearization errors significantly degrade precision. In virtual reality (VR) applications, this manifests as substantial positional errors in 6-DoF controller tracking during rapid rotations of the head-mounted display. The linearization errors introduce drift in the estimate and render the estimator inconsistent. In the field of odometry, IMU preintegration is proposed as a kinematic observation to enable efficient relinearization, thus mitigate linearized error. Building on this theory, we propose dual preintegration, a novel observation integrating IMU preintegration from both platforms. This method serves as kinematic constraints for consecutive relative state and supports efficient relinearization. We also perform observability analysis of the state and analytically formulate the accordingly null space. Algorithm evaluation encompasses both simulations and real-world experiments. Multiple nonlinear rotations on the reference platform are simulated to compare the precision of the proposed method with that of other state-of-the-art (SOTA) algorithms. The field test compares the proposed method and SOTA algorithms in the application of VR controller tracking from the perspectives of bias observability, nonlinear rotation, and background texture. The results demonstrate that the proposed method is more precise and robust than the SOTA algorithms.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-26",
      "updated": "2025-11-26",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21189v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Estimating Fog Parameters from a Sequence of Stereo Images",
      "authors": [
        "Yining Ding",
        "João F. C. Mota",
        "Andrew M. Wallace",
        "Sen Wang"
      ],
      "arxiv_id": "2511.20865v1",
      "summary": "We propose a method which, given a sequence of stereo foggy images, estimates the parameters of a fog model and updates them dynamically. In contrast with previous approaches, which estimate the parameters sequentially and thus are prone to error propagation, our algorithm estimates all the parameters simultaneously by solving a novel optimisation problem. By assuming that fog is only locally homogeneous, our method effectively handles real-world fog, which is often globally inhomogeneous. The proposed algorithm can be easily used as an add-on module in existing visual Simultaneous Localisation and Mapping (SLAM) or odometry systems in the presence of fog. In order to assess our method, we also created a new dataset, the Stereo Driving In Real Fog (SDIRF), consisting of high-quality, consecutive stereo frames of real, foggy road scenes under a variety of visibility conditions, totalling over 40 minutes and 34k frames. As a first-of-its-kind, SDIRF contains the camera's photometric parameters calibrated in a lab environment, which is a prerequisite for correctly applying the atmospheric scattering model to foggy images. The dataset also includes the counterpart clear data of the same routes recorded in overcast weather, which is useful for companion work in image defogging and depth reconstruction. We conducted extensive experiments using both synthetic foggy data and real foggy sequences from SDIRF to demonstrate the superiority of the proposed algorithm over prior methods. Our method not only produces the most accurate estimates on synthetic data, but also adapts better to real fog. We make our code and SDIRF publicly available\\footnote{https://github.com/SenseRoboticsLab/estimating-fog-parameters} to the community with the aim of advancing the research on visual perception in fog.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "10.1109/TPAMI.2025.3626275",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20865v1",
      "code_links": [
        {
          "url": "https://github.com/SenseRoboticsLab/estimating-fog-parameters",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "SLAM"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Evaluating the Performance of Deep Learning Models in Whole-body Dynamic 3D Posture Prediction During Load-reaching Activities",
      "authors": [
        "Seyede Niloofar Hosseini",
        "Ali Mojibi",
        "Mahdi Mohseni",
        "Navid Arjmand",
        "Alireza Taheri"
      ],
      "arxiv_id": "2511.20615v1",
      "summary": "This study aimed to explore the application of deep neural networks for whole-body human posture prediction during dynamic load-reaching activities. Two time-series models were trained using bidirectional long short-term memory (BLSTM) and transformer architectures. The dataset consisted of 3D full-body plug-in gait dynamic coordinates from 20 normal-weight healthy male individuals each performing 204 load-reaching tasks from different load positions while adapting various lifting and handling techniques. The model inputs consisted of the 3D position of the hand-load position, lifting (stoop, full-squat and semi-squat) and handling (one- and two-handed) techniques, body weight and height, and the 3D coordinate data of the body posture from the first 25% of the task duration. These inputs were used by the models to predict body coordinates during the remaining 75% of the task period. Moreover, a novel method was proposed to improve the accuracy of the previous and present posture prediction networks by enforcing constant body segment lengths through the optimization of a new cost function. The results indicated that the new cost function decreased the prediction error of the models by approximately 8% and 21% for the arm and leg models, respectively. We indicated that utilizing the transformer architecture, with a root-mean-square-error of 47.0 mm, exhibited ~58% more accurate long-term performance than the BLSTM-based model. This study merits the use of neural networks that capture time series dependencies in 3D motion frames, providing a unique approach for understanding and predict motion dynamics during manual material handling activities.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "10 pages, 6 figures, 7 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20615v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "gait"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Mistake Attribution: Fine-Grained Mistake Understanding in Egocentric Videos",
      "authors": [
        "Yayuan Li",
        "Aadit Jain",
        "Filippos Bellos",
        "Jason J. Corso"
      ],
      "arxiv_id": "2511.20525v1",
      "summary": "We introduce Mistake Attribution (MATT), a task for fine-grained understanding of human mistakes in egocentric video. Unlike prior mistake understanding work, which lacks fine-grained output, MATT concretely attributes mistakes to the input instruction text or the attempt video. MATT determines what part of the instruction is violated (semantic role), when the deviation becomes irreversible (the Point-of-No-Return, PNR), and where the mistake appears in the PNR frame. We develop MisEngine, a data engine that automatically constructs attribution-rich mistake samples from existing datasets and inherits their annotations. Applied to large egocentric corpora, MisEngine yields EPIC-KITCHENS-M and Ego4D-M, two datasets that are up to two orders of magnitude larger than prior mistake datasets. We then present MisFormer, a unified attention-based model for mistake attribution across semantic (what), temporal (when), and spatial (where) dimensions, trained using MisEngine supervision. Experiments on our new datasets and prior benchmarks show that MisFormer outperforms strong video-language, temporal localization, hand-object interaction, and mistake-detection baselines.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "11 pages, 4 figures, 6 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20525v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Power-Efficient Autonomous Mobile Robots",
      "authors": [
        "Liangkai Liu",
        "Weisong Shi",
        "Kang G. Shin"
      ],
      "arxiv_id": "2511.20467v1",
      "summary": "This paper presents pNav, a novel power-management system that significantly enhances the power/energy-efficiency of Autonomous Mobile Robots (AMRs) by jointly optimizing their physical/mechanical and cyber subsystems. By profiling AMRs' power consumption, we identify three challenges in achieving CPS (cyber-physical system) power-efficiency that involve both cyber (C) and physical (P) subsystems: (1) variabilities of system power consumption breakdown, (2) environment-aware navigation locality, and (3) coordination of C and P subsystems. pNav takes a multi-faceted approach to achieve power-efficiency of AMRs. First, it integrates millisecond-level power consumption prediction for both C and P subsystems. Second, it includes novel real-time modeling and monitoring of spatial and temporal navigation localities for AMRs. Third, it supports dynamic coordination of AMR software (navigation, detection) and hardware (motors, DVFS driver) configurations. pNav is prototyped using the Robot Operating System (ROS) Navigation Stack, 2D LiDAR, and camera. Our in-depth evaluation with a real robot and Gazebo environments demonstrates a >96% accuracy in predicting power consumption and a 38.1% reduction in power consumption without compromising navigation accuracy and safety.",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "13 pages, 16 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20467v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Improved adaptive wind driven optimization algorithm for real-time path planning",
      "authors": [
        "Shiqian Liu",
        "Azlan Mohd Zain",
        "Le-le Mao"
      ],
      "arxiv_id": "2511.20394v1",
      "summary": "Recently, path planning has achieved remarkable progress in enhancing global search capability and convergence accuracy through heuristic and learning-inspired optimization frameworks. However, real-time adaptability in dynamic environments remains a critical challenge for autonomous navigation, particularly when robots must generate collision-free, smooth, and efficient trajectories under complex constraints. By analyzing the difficulties in dynamic path planning, the Wind Driven Optimization (WDO) algorithm emerges as a promising framework owing to its physically interpretable search dynamics. Motivated by these observations, this work revisits the WDO principle and proposes a variant formulation, Multi-hierarchical adaptive wind driven optimization(MAWDO), that improves adaptability and robustness in time-varying environments. To mitigate instability and premature convergence, a hierarchical-guidance mechanism divides the population into multiple groups guided by individual, regional, and global leaders to balance exploration and exploitation. Extensive evaluations on sixteen benchmark functions show that MAWDO achieves superior optimization accuracy, convergence stability, and adaptability over state-of-the art metaheuristics. In dynamic path planning, MAWDO shortens the path length to 469.28 pixels, improving over Multi-strategy ensemble wind driven optimization(MEWDO), Adaptive wind driven optimization(AWDO) and WDO by 3.51\\%, 11.63\\% and 14.93\\%, and achieves the smallest optimality gap (1.01) with smoothness 0.71 versus 13.50 and 15.67 for AWDO and WDO, leading to smoother, shorter, and collision-free trajectories that confirm its effectiveness for real-time path planning in complex environments.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "23 pages, 4 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20394v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "How Robot Kinematics Influence Human Performance in Virtual Robot-to-Human Handover Tasks",
      "authors": [
        "Róisín Keenan",
        "Joost C. Dessing"
      ],
      "arxiv_id": "2511.20299v1",
      "summary": "Recent advancements in robotics have increased the possibilities for integrating robotic systems into human-involved workplaces, highlighting the need to examine and optimize human-robot coordination in collaborative settings. This study explores human-robot interactions during handover tasks using Virtual Reality (VR) to investigate differences in human motor performance across various task dynamics and robot kinematics. A VR-based robot handover simulation afforded safe and controlled assessments of human-robot interactions. In separate experiments, four potential influences on human performance were examined (1) control over task initiation and robot movement synchrony (temporal and spatiotemporal); (2) partner appearance (human versus robotic); (3) robot velocity profiles (minimum jerk, constant velocity, constant acceleration, and biphasic); and (4) the timing of rotational object motion. Findings across experiments emphasize humans benefit from robots providing early and salient visual information about task-relevant object motion, and advantages of human-like smooth robot trajectories. To varying degrees, these manipulations improved predictive accuracy and synchronization during interaction. This suggests that human-robot interactions should be designed to allow humans to leverage their natural capabilities for detecting biological motion, which conversely may reduce the need for costly robotic computations or added cognitive adaptation on the human side.",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20299v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Dynamic-ICP: Doppler-Aware Iterative Closest Point Registration for Dynamic Scenes",
      "authors": [
        "Dong Wang",
        "Daniel Casado Herraez",
        "Stefan May",
        "Andreas Nüchter"
      ],
      "arxiv_id": "2511.20292v1",
      "summary": "Reliable odometry in highly dynamic environments remains challenging when it relies on ICP-based registration: ICP assumes near-static scenes and degrades in repetitive or low-texture geometry. We introduce Dynamic-ICP, a Doppler-aware registration framework. The method (i) estimates ego motion from per-point Doppler velocity via robust regression and builds a velocity filter, (ii) clusters dynamic objects and reconstructs object-wise translational velocities from ego-compensated radial measurements, (iii) predicts dynamic points with a constant-velocity model, and (iv) aligns scans using a compact objective that combines point-to-plane geometry residual with a translation-invariant, rotation-only Doppler residual. The approach requires no external sensors or sensor-vehicle calibration and operates directly on FMCW LiDAR range and Doppler velocities. We evaluate Dynamic-ICP on three datasets-HeRCULES, HeLiPR, AevaScenes-focusing on highly dynamic scenes. Dynamic-ICP consistently improves rotational stability and translation accuracy over the state-of-the-art methods. Our approach is also simple to integrate into existing pipelines, runs in real time, and provides a lightweight solution for robust registration in dynamic environments. To encourage further research, the code is available at: https://github.com/JMUWRobotics/Dynamic-ICP.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "8 pages, 5 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20292v1",
      "code_links": [
        {
          "url": "https://github.com/JMUWRobotics/Dynamic-ICP",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "ego-motion"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Zoo3D: Zero-Shot 3D Object Detection at Scene Level",
      "authors": [
        "Andrey Lemeshko",
        "Bulat Gabdullin",
        "Nikita Drozdov",
        "Anton Konushin",
        "Danila Rukhovich",
        "Maksim Kolodiazhnyi"
      ],
      "arxiv_id": "2511.20253v1",
      "summary": "3D object detection is fundamental for spatial understanding. Real-world environments demand models capable of recognizing diverse, previously unseen objects, which remains a major limitation of closed-set methods. Existing open-vocabulary 3D detectors relax annotation requirements but still depend on training scenes, either as point clouds or images. We take this a step further by introducing Zoo3D, the first training-free 3D object detection framework. Our method constructs 3D bounding boxes via graph clustering of 2D instance masks, then assigns semantic labels using a novel open-vocabulary module with best-view selection and view-consensus mask generation. Zoo3D operates in two modes: the zero-shot Zoo3D$_0$, which requires no training at all, and the self-supervised Zoo3D$_1$, which refines 3D box prediction by training a class-agnostic detector on Zoo3D$_0$-generated pseudo labels. Furthermore, we extend Zoo3D beyond point clouds to work directly with posed and even unposed images. Across ScanNet200 and ARKitScenes benchmarks, both Zoo3D$_0$ and Zoo3D$_1$ achieve state-of-the-art results in open-vocabulary 3D object detection. Remarkably, our zero-shot Zoo3D$_0$ outperforms all existing self-supervised methods, hence demonstrating the power and adaptability of training-free, off-the-shelf approaches for real-world 3D understanding. Code is available at https://github.com/col14m/zoo3d .",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20253v1",
      "code_links": [
        {
          "url": "https://github.com/col14m/zoo3d",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "V-Attack: Targeting Disentangled Value Features for Controllable Adversarial Attacks on LVLMs",
      "authors": [
        "Sen Nie",
        "Jie Zhang",
        "Jianxin Yan",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "arxiv_id": "2511.20223v1",
      "summary": "Adversarial attacks have evolved from simply disrupting predictions on conventional task-specific models to the more complex goal of manipulating image semantics on Large Vision-Language Models (LVLMs). However, existing methods struggle with controllability and fail to precisely manipulate the semantics of specific concepts in the image. We attribute this limitation to semantic entanglement in the patch-token representations on which adversarial attacks typically operate: global context aggregated by self-attention in the vision encoder dominates individual patch features, making them unreliable handles for precise local semantic manipulation. Our systematic investigation reveals a key insight: value features (V) computed within the transformer attention block serve as much more precise handles for manipulation. We show that V suppresses global-context channels, allowing it to retain high-entropy, disentangled local semantic information. Building on this discovery, we propose V-Attack, a novel method designed for precise local semantic attacks. V-Attack targets the value features and introduces two core components: (1) a Self-Value Enhancement module to refine V's intrinsic semantic richness, and (2) a Text-Guided Value Manipulation module that leverages text prompts to locate source concept and optimize it toward a target concept. By bypassing the entangled patch features, V-Attack achieves highly effective semantic control. Extensive experiments across diverse LVLMs, including LLaVA, InternVL, DeepseekVL and GPT-4o, show that V-Attack improves the attack success rate by an average of 36% over state-of-the-art methods, exposing critical vulnerabilities in modern visual-language understanding. Our code and data are available https://github.com/Summu77/V-Attack.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "21 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20223v1",
      "code_links": [
        {
          "url": "https://github.com/Summu77/V-Attack",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "SKEL-CF: Coarse-to-Fine Biomechanical Skeleton and Surface Mesh Recovery",
      "authors": [
        "Da Li",
        "Jiping Jin",
        "Xuanlong Yu",
        "Wei Liu",
        "Xiaodong Cun",
        "Kai Chen",
        "Rui Fan",
        "Jiangang Kong",
        "Xi Shen"
      ],
      "arxiv_id": "2511.20157v3",
      "summary": "Parametric 3D human models such as SMPL have driven significant advances in human pose and shape estimation, yet their simplified kinematics limit biomechanical realism. The recently proposed SKEL model addresses this limitation by re-rigging SMPL with an anatomically accurate skeleton. However, estimating SKEL parameters directly remains challenging due to limited training data, perspective ambiguities, and the inherent complexity of human articulation. We introduce SKEL-CF, a coarse-to-fine framework for SKEL parameter estimation. SKEL-CF employs a transformer-based encoder-decoder architecture, where the encoder predicts coarse camera and SKEL parameters, and the decoder progressively refines them in successive layers. To ensure anatomically consistent supervision, we convert the existing SMPL-based dataset 4DHuman into a SKEL-aligned version, 4DHuman-SKEL, providing high-quality training data for SKEL estimation. In addition, to mitigate depth and scale ambiguities, we explicitly incorporate camera modeling into the SKEL-CF pipeline and demonstrate its importance across diverse viewpoints. Extensive experiments validate the effectiveness of the proposed design. On the challenging MOYO dataset, SKEL-CF achieves 85.0 MPJPE / 51.4 PA-MPJPE, significantly outperforming the previous SKEL-based state-of-the-art HSMR (104.5 / 79.6). These results establish SKEL-CF as a scalable and anatomically faithful framework for human motion analysis, bridging the gap between computer vision and biomechanics. Our implementation is available on the project page: https://pokerman8.github.io/SKEL-CF/.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-27",
      "comment": "Project page: https://pokerman8.github.io/SKEL-CF/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20157v3",
      "code_links": [
        {
          "url": "https://pokerman8.github.io/SKEL-CF/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "SMPL"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "6_video_extraction"
      ]
    },
    {
      "title": "Explainable Visual Anomaly Detection via Concept Bottleneck Models",
      "authors": [
        "Arianna Stropeni",
        "Valentina Zaccaria",
        "Francesco Borsatti",
        "Davide Dalle Pezze",
        "Manuel Barusco",
        "Gian Antonio Susto"
      ],
      "arxiv_id": "2511.20088v1",
      "summary": "In recent years, Visual Anomaly Detection (VAD) has gained significant attention due to its ability to identify anomalous images using only normal images during training. Many VAD models work without supervision but are still able to provide visual explanations by highlighting the anomalous regions within an image. However, although these visual explanations can be helpful, they lack a direct and semantically meaningful interpretation for users. To address this limitation, we propose extending Concept Bottleneck Models (CBMs) to the VAD setting. By learning meaningful concepts, the network can provide human-interpretable descriptions of anomalies, offering a novel and more insightful way to explain them. Our contributions are threefold: (i) we develop a Concept Dataset to support research on CBMs for VAD; (ii) we improve the CBM architecture to generate both concept-based and visual explanations, bridging semantic and localization interpretability; and (iii) we introduce a pipeline for synthesizing artificial anomalies, preserving the VAD paradigm of minimizing dependence on rare anomalous samples. Our approach, Concept-Aware Visual Anomaly Detection (CONVAD), achieves performance comparable to classic VAD methods while providing richer, concept-driven explanations that enhance interpretability and trust in VAD systems.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20088v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Tell Model Where to Look: Mitigating Hallucinations in MLLMs by Vision-Guided Attention",
      "authors": [
        "Jianfei Zhao",
        "Feng Zhang",
        "Xin Sun",
        "Chong Feng",
        "Zhixing Tan"
      ],
      "arxiv_id": "2511.20032v1",
      "summary": "Visual attention serves as the primary mechanism through which MLLMs interpret visual information; however, its limited localization capability often leads to hallucinations. We observe that although MLLMs can accurately extract visual semantics from visual tokens, they fail to fully leverage this advantage during subsequent inference. To address this limitation, we propose Vision-Guided Attention (VGA), a training-free method that first constructs precise visual grounding by exploiting the semantic content of visual tokens, and then uses this grounding to guide the model's focus toward relevant visual regions. In image captioning, VGA further refines this guidance dynamically during generation by suppressing regions that have already been described. In VGA, each token undergoes only a single forward pass, introducing a negligible latency overhead of just 4.36\\%. In addition, VGA is fully compatible with efficient attention implementations such as FlashAttention. Extensive experiments across diverse MLLMs and multiple hallucination benchmarks demonstrate that VGA achieves state-of-the-art dehallucination performance. Further analysis confirms that explicit visual guidance plays a crucial role in enhancing the visual understanding capabilities of MLLMs.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "Under Review",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20032v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Foundry: Distilling 3D Foundation Models for the Edge",
      "authors": [
        "Guillaume Letellier",
        "Siddharth Srivastava",
        "Frédéric Jurie",
        "Gaurav Sharma"
      ],
      "arxiv_id": "2511.20721v1",
      "summary": "Foundation models pre-trained with self-supervised learning (SSL) on large-scale datasets have become powerful general-purpose feature extractors. However, their immense size and computational cost make them prohibitive for deployment on edge devices such as robots and AR/VR headsets. Existing compression techniques like standard knowledge distillation create efficient 'specialist' models but sacrifice the crucial, downstream-agnostic generality that makes foundation models so valuable.  In this paper, we introduce Foundation Model Distillation (FMD), a new paradigm for compressing large SSL models into compact, efficient, and faithful proxies that retain their general-purpose representational power. We present Foundry, the first implementation of FMD for 3D point clouds. Our approach, Foundry, trains a student to learn a compressed set of SuperTokens that reconstruct the teacher's token-level representations, capturing a compact basis of its latent space. A single distilled model maintains strong transferability across diverse downstream tasks-classification, part segmentation, and few-shot scenarios-approaching full foundation-model performance while using significantly fewer tokens and FLOPs, making such models more practical for deployment on resourceconstrained hardware.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20721v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Multi-Context Fusion Transformer for Pedestrian Crossing Intention Prediction in Urban Environments",
      "authors": [
        "Yuanzhe Li",
        "Hang Zhong",
        "Steffen Müller"
      ],
      "arxiv_id": "2511.20011v1",
      "summary": "Pedestrian crossing intention prediction is essential for autonomous vehicles to improve pedestrian safety and reduce traffic accidents. However, accurate pedestrian intention prediction in urban environments remains challenging due to the multitude of factors affecting pedestrian behavior. In this paper, we propose a multi-context fusion Transformer (MFT) that leverages diverse numerical contextual attributes across four key dimensions, encompassing pedestrian behavior context, environmental context, pedestrian localization context and vehicle motion context, to enable accurate pedestrian intention prediction. MFT employs a progressive fusion strategy, where mutual intra-context attention enables reciprocal interactions within each context, thereby facilitating feature sequence fusion and yielding a context token as a context-specific representation. This is followed by mutual cross-context attention, which integrates features across contexts with a global CLS token serving as a compact multi-context representation. Finally, guided intra-context attention refines context tokens within each context through directed interactions, while guided cross-context attention strengthens the global CLS token to promote multi-context fusion via guided information propagation, yielding deeper and more efficient integration. Experimental results validate the superiority of MFT over state-of-the-art methods, achieving accuracy rates of 73%, 93%, and 90% on the JAADbeh, JAADall, and PIE datasets, respectively. Extensive ablation studies are further conducted to investigate the effectiveness of the network architecture and contribution of different input context. Our code is open-source: https://github.com/ZhongHang0307/Multi-Context-Fusion-Transformer.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20011v1",
      "code_links": [
        {
          "url": "https://github.com/ZhongHang0307/Multi-Context-Fusion-Transformer",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Prune-Then-Plan: Step-Level Calibration for Stable Frontier Exploration in Embodied Question Answering",
      "authors": [
        "Noah Frahm",
        "Prakrut Patel",
        "Yue Zhang",
        "Shoubin Yu",
        "Mohit Bansal",
        "Roni Sengupta"
      ],
      "arxiv_id": "2511.19768v1",
      "summary": "Large vision-language models (VLMs) have improved embodied question answering (EQA) agents by providing strong semantic priors for open-vocabulary reasoning. However, when used directly for step-level exploration, VLMs often exhibit frontier oscillations, unstable back-and-forth movements caused by overconfidence and miscalibration, leading to inefficient navigation and degraded answer quality. We propose Prune-Then-Plan, a simple and effective framework that stabilizes exploration through step-level calibration. Instead of trusting raw VLM scores, our method prunes implausible frontier choices using a Holm-Bonferroni inspired pruning procedure and then delegates final decisions to a coverage-based planner. This separation converts overconfident predictions into conservative, interpretable actions by relying on human-level judgments to calibrate the step-level behavior of VLMs. Integrated into the 3D-Mem EQA framework, our approach achieves relative improvements of up to 49% and 33% in visually grounded SPL and LLM-Match metrics respectively over baselines. Overall, our method achieves better scene coverage under equal exploration budgets on both OpenEQA and EXPRESS-Bench datasets.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "webpage: https://noahfrahm.github.io/Prune-Then-Plan-project-page/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19768v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "A Storage-Efficient Feature for 3D Concrete Defect Segmentation to Replace Normal Vector",
      "authors": [
        "Linxin Hua",
        "Jianghua Deng",
        "Ye Lu"
      ],
      "arxiv_id": "2511.19760v1",
      "summary": "Point cloud reconstruction of damage offers an effective solution to image-based methods vulnerable to background noise, yet its application is constrained by the high volume of 3D data. This study proposes a new feature, relative angle, computed as the angle between the normal vector of a point and the average normal vector of its parent point cloud. This single-dimensional feature provides directionality information equivalent to normal vectors for concrete surface defect characteristics. Through entropy-based feature evaluation, this study demonstrates the ability of relative angle to filter out redundant information in undamaged sections while retaining effective information in damaged sections. By training and testing with PointNet++, models based on the relative angles achieved similar performance to that of models based on normal vectors while delivering 27.6% storage reduction and 83% input channel compression. This novel feature has the potential to enable larger-batch execution on resource-constrained hardware without the necessity of architectural modifications to models.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "25 pages, 7 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19760v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "IndEgo: A Dataset of Industrial Scenarios and Collaborative Work for Egocentric Assistants",
      "authors": [
        "Vivek Chavan",
        "Yasmina Imgrund",
        "Tung Dao",
        "Sanwantri Bai",
        "Bosong Wang",
        "Ze Lu",
        "Oliver Heimann",
        "Jörg Krüger"
      ],
      "arxiv_id": "2511.19684v1",
      "summary": "We introduce IndEgo, a multimodal egocentric and exocentric dataset addressing common industrial tasks, including assembly/disassembly, logistics and organisation, inspection and repair, woodworking, and others. The dataset contains 3,460 egocentric recordings (approximately 197 hours), along with 1,092 exocentric recordings (approximately 97 hours). A key focus of the dataset is collaborative work, where two workers jointly perform cognitively and physically intensive tasks. The egocentric recordings include rich multimodal data and added context via eye gaze, narration, sound, motion, and others. We provide detailed annotations (actions, summaries, mistake annotations, narrations), metadata, processed outputs (eye gaze, hand pose, semi-dense point cloud), and benchmarks on procedural and non-procedural task understanding, Mistake Detection, and reasoning-based Question Answering. Baseline evaluations for Mistake Detection, Question Answering and collaborative task understanding show that the dataset presents a challenge for the state-of-the-art multimodal models. Our dataset is available at: https://huggingface.co/datasets/FraunhoferIPK/IndEgo",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "Accepted to NeurIPS 2025 D&B Track. Project Page: https://indego-dataset.github.io/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19684v1",
      "code_links": [
        {
          "url": "https://huggingface.co/datasets/FraunhoferIPK",
          "type": "huggingface"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection",
      "authors": [
        "Qiang Wang",
        "Xinyuan Gao",
        "SongLin Dong",
        "Jizhou Han",
        "Jiangyang Li",
        "Yuhang He",
        "Yihong Gong"
      ],
      "arxiv_id": "2511.19436v1",
      "summary": "We present VDC-Agent, a self-evolving framework for Video Detailed Captioning that requires neither human annotations nor larger teacher models. The agent forms a closed loop of caption generation, principle-guided scoring (score and textual suggestions), and prompt refinement. When caption quality regresses, a self-reflection path leverages the previous chain-of-thought to amend the update. Running this process on unlabeled videos produces trajectories of (caption, score) pairs. We convert the trajectories into preference tuples and filter out samples with JSON parsing errors, resulting in VDC-Agent-19K, which contains 18,886 automatically constructed pairs. We then fine-tune the base MLLM on this dataset using an easy-to-hard curriculum direct preference optimization. Built on Qwen2.5-VL-7B-Instruct, our VDC-Agent-7B attains state-of-the-art performance on the VDC benchmark with 49.08% average accuracy and 2.50 score, surpassing specialized video captioners and improving over the base model by +5.13% accuracy and +0.27 score at similar inference cost.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19436v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "running"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Mixture of Horizons in Action Chunking",
      "authors": [
        "Dong Jing",
        "Gang Wang",
        "Jiaqi Liu",
        "Weiliang Tang",
        "Zelong Sun",
        "Yunchao Yao",
        "Zhenyu Wei",
        "Yunhui Liu",
        "Zhiwu Lu",
        "Mingyu Ding"
      ],
      "arxiv_id": "2511.19433v1",
      "summary": "Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the $\\textbf{action chunk length}$ used during training, termed $\\textbf{horizon}$. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a $\\textbf{mixture of horizons (MoH)}$ strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5$\\times$ higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies $π_0$, $π_{0.5}$, and one-step regression policy $π_{\\text{reg}}$ demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, $π_{0.5}$ with MoH reaches a new state-of-the-art with 99$\\%$ average success rate on LIBERO after only $30k$ training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "15 pages, 14 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19433v1",
      "code_links": [
        {
          "url": "https://github.com/Timsty1/MixtureOfHorizons",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "MonoMSK: Monocular 3D Musculoskeletal Dynamics Estimation",
      "authors": [
        "Farnoosh Koleini",
        "Hongfei Xue",
        "Ahmed Helmy",
        "Pu Wang"
      ],
      "arxiv_id": "2511.19326v1",
      "summary": "Reconstructing biomechanically realistic 3D human motion - recovering both kinematics (motion) and kinetics (forces) - is a critical challenge. While marker-based systems are lab-bound and slow, popular monocular methods use oversimplified, anatomically inaccurate models (e.g., SMPL) and ignore physics, fundamentally limiting their biomechanical fidelity. In this work, we introduce MonoMSK, a hybrid framework that bridges data-driven learning and physics-based simulation for biomechanically realistic 3D human motion estimation from monocular video. MonoMSK jointly recovers both kinematics (motions) and kinetics (forces and torques) through an anatomically accurate musculoskeletal model. By integrating transformer-based inverse dynamics with differentiable forward kinematics and dynamics layers governed by ODE-based simulation, MonoMSK establishes a physics-regulated inverse-forward loop that enforces biomechanical causality and physical plausibility. A novel forward-inverse consistency loss further aligns motion reconstruction with the underlying kinetic reasoning. Experiments on BML-MoVi, BEDLAM, and OpenCap show that MonoMSK significantly outperforms state-of-the-art methods in kinematic accuracy, while for the first time enabling precise monocular kinetics estimation.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19326v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "SMPL"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "6_video_extraction"
      ]
    },
    {
      "title": "AvatarBrush: Monocular Reconstruction of Gaussian Avatars with Intuitive Local Editing",
      "authors": [
        "Mengtian Li",
        "Shengxiang Yao",
        "Yichen Pan",
        "Haiyao Xiao",
        "Zhongmei Li",
        "Zhifeng Xie",
        "Keyu Chen"
      ],
      "arxiv_id": "2511.19189v1",
      "summary": "The efficient reconstruction of high-quality and intuitively editable human avatars presents a pressing challenge in the field of computer vision. Recent advancements, such as 3DGS, have demonstrated impressive reconstruction efficiency and rapid rendering speeds. However, intuitive local editing of these representations remains a significant challenge. In this work, we propose AvatarBrush, a framework that reconstructs fully animatable and locally editable avatars using only a monocular video input. We propose a three-layer model to represent the avatar and, inspired by mesh morphing techniques, design a framework to generate the Gaussian model from local information of the parametric body model. Compared to previous methods that require scanned meshes or multi-view captures as input, our approach reduces costs and enhances editing capabilities such as body shape adjustment, local texture modification, and geometry transfer. Our experimental results demonstrate superior quality across two datasets and emphasize the enhanced, user-friendly, and localized editing capabilities of our method.",
      "categories": [
        "cs.GR"
      ],
      "primary_category": "cs.GR",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19189v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3DGS"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation",
      "authors": [
        "Moazzam Umer Gondal",
        "Hamad Ul Qudous",
        "Daniya Siddiqui",
        "Asma Ahmad Farhan"
      ],
      "arxiv_id": "2511.19149v1",
      "summary": "This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting. The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization. The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index. These attributes, together with retrieved style examples, create a factual evidence pack that is used to guide an LLM to generate human-like captions and contextually rich hashtags. A fine-tuned BLIP model is used as a supervised baseline model for comparison. Experimental results show that the YOLO detector is able to obtain a mean Average Precision (mAP@0.5) of 0.71 for nine categories of garments. The RAG-LLM pipeline generates expressive attribute-aligned captions and achieves mean attribute coverage of 0.80 with full coverage at the 50% threshold in hashtag generation, whereas BLIP gives higher lexical overlap and lower generalization. The retrieval-augmented approach exhibits better factual grounding, less hallucination, and great potential for scalable deployment in various clothing domains. These results demonstrate the use of retrieval-augmented generation as an effective and interpretable paradigm for automated and visually grounded fashion content generation.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "Submitted to Expert Systems with Applications",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19149v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection",
      "authors": [
        "Hai Ci",
        "Ziheng Peng",
        "Pei Yang",
        "Yingxin Xuan",
        "Mike Zheng Shou"
      ],
      "arxiv_id": "2511.19111v2",
      "summary": "Diffusion-based editing enables realistic modification of local image regions, making AI-generated content harder to detect. Existing AIGC detection benchmarks focus on classifying entire images, overlooking the localization of diffusion-based edits. We introduce DiffSeg30k, a publicly available dataset of 30k diffusion-edited images with pixel-level annotations, designed to support fine-grained detection. DiffSeg30k features: 1) In-the-wild images--we collect images or image prompts from COCO to reflect real-world content diversity; 2) Diverse diffusion models--local edits using eight SOTA diffusion models; 3) Multi-turn editing--each image undergoes up to three sequential edits to mimic real-world sequential editing; and 4) Realistic editing scenarios--a vision-language model (VLM)-based pipeline automatically identifies meaningful regions and generates context-aware prompts covering additions, removals, and attribute changes. DiffSeg30k shifts AIGC detection from binary classification to semantic segmentation, enabling simultaneous localization of edits and identification of the editing models. We benchmark three baseline segmentation approaches, revealing significant challenges in semantic segmentation tasks, particularly concerning robustness to image distortions. Experiments also reveal that segmentation models, despite being trained for pixel-level localization, emerge as highly reliable whole-image classifiers of diffusion edits, outperforming established forgery classifiers while showing great potential in cross-generator generalization. We believe DiffSeg30k will advance research in fine-grained localization of AI-generated content by demonstrating the promise and limitations of segmentation-based methods. DiffSeg30k is released at: https://huggingface.co/datasets/Chaos2629/Diffseg30k",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-26",
      "comment": "16 pages, 10 figures; typos corrected, references added",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19111v2",
      "code_links": [
        {
          "url": "https://huggingface.co/datasets/Chaos2629",
          "type": "huggingface"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Analysis of Deep-Learning Methods in an ISO/TS 15066-Compliant Human-Robot Safety Framework",
      "authors": [
        "David Bricher",
        "Andreas Mueller"
      ],
      "arxiv_id": "2511.19094v1",
      "summary": "Over the last years collaborative robots have gained great success in manufacturing applications where human and robot work together in close proximity. However, current ISO/TS-15066-compliant implementations often limit the efficiency of collaborative tasks due to conservative speed restrictions. For this reason, this paper introduces a deep-learning-based human-robot-safety framework (HRSF) that aims at a dynamical adaptation of robot velocities depending on the separation distance between human and robot while respecting maximum biomechanical force and pressure limits. The applicability of the framework was investigated for four different deep learning approaches that can be used for human body extraction: human body recognition, human body segmentation, human pose estimation, and human body part segmentation. Unlike conventional industrial safety systems, the proposed HRSF differentiates individual human body parts from other objects, enabling optimized robot process execution. Experiments demonstrated a quantitative reduction in cycle time of up to 15% compared to conventional safety technology.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "MDPI Sensors, published 22 November 2025",
      "doi": "10.3390/s25237136",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19094v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "pose estimation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Granular Computing-driven SAM: From Coarse-to-Fine Guidance for Prompt-Free Segmentation",
      "authors": [
        "Qiyang Yu",
        "Yu Fang",
        "Tianrui Li",
        "Xuemei Cao",
        "Yan Chen",
        "Jianghao Li",
        "Fan Min",
        "Yi Zhang"
      ],
      "arxiv_id": "2511.19062v1",
      "summary": "Prompt-free image segmentation aims to generate accurate masks without manual guidance. Typical pre-trained models, notably Segmentation Anything Model (SAM), generate prompts directly at a single granularity level. However, this approach has two limitations: (1) Localizability, lacking mechanisms for autonomous region localization; (2) Scalability, limited fine-grained modeling at high resolution. To address these challenges, we introduce Granular Computing-driven SAM (Grc-SAM), a coarse-to-fine framework motivated by Granular Computing (GrC). First, the coarse stage adaptively extracts high-response regions from features to achieve precise foreground localization and reduce reliance on external prompts. Second, the fine stage applies finer patch partitioning with sparse local swin-style attention to enhance detail modeling and enable high-resolution segmentation. Third, refined masks are encoded as latent prompt embeddings for the SAM decoder, replacing handcrafted prompts with an automated reasoning process. By integrating multi-granularity attention, Grc-SAM bridges granular computing with vision transformers. Extensive experimental results demonstrate Grc-SAM outperforms baseline methods in both accuracy and scalability. It offers a unique granular computational perspective for prompt-free segmentation.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "19 pages, 7 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19062v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Perceptual Taxonomy: Evaluating and Guiding Hierarchical Scene Reasoning in Vision-Language Models",
      "authors": [
        "Jonathan Lee",
        "Xingrui Wang",
        "Jiawei Peng",
        "Luoxin Ye",
        "Zehan Zheng",
        "Tiezheng Zhang",
        "Tao Wang",
        "Wufei Ma",
        "Siyi Chen",
        "Yu-Cheng Chou",
        "Prakhar Kaushik",
        "Alan Yuille"
      ],
      "arxiv_id": "2511.19526v1",
      "summary": "We propose Perceptual Taxonomy, a structured process of scene understanding that first recognizes objects and their spatial configurations, then infers task-relevant properties such as material, affordance, function, and physical attributes to support goal-directed reasoning. While this form of reasoning is fundamental to human cognition, current vision-language benchmarks lack comprehensive evaluation of this ability and instead focus on surface-level recognition or image-text alignment.\n  To address this gap, we introduce Perceptual Taxonomy, a benchmark for physically grounded visual reasoning. We annotate 3173 objects with four property families covering 84 fine-grained attributes. Using these annotations, we construct a multiple-choice question benchmark with 5802 images across both synthetic and real domains. The benchmark contains 28033 template-based questions spanning four types (object description, spatial reasoning, property matching, and taxonomy reasoning), along with 50 expert-crafted questions designed to evaluate models across the full spectrum of perceptual taxonomy reasoning.\n  Experimental results show that leading vision-language models perform well on recognition tasks but degrade by 10 to 20 percent on property-driven questions, especially those requiring multi-step reasoning over structured attributes. These findings highlight a persistent gap in structured visual understanding and the limitations of current models that rely heavily on pattern matching. We also show that providing in-context reasoning examples from simulated scenes improves performance on real-world and expert-curated questions, demonstrating the effectiveness of perceptual-taxonomy-guided prompting.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19526v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "TPG-INR: Target Prior-Guided Implicit 3D CT Reconstruction for Enhanced Sparse-view Imaging",
      "authors": [
        "Qinglei Cao",
        "Ziyao Tang",
        "Xiaoqin Tang"
      ],
      "arxiv_id": "2511.18806v1",
      "summary": "X-ray imaging, based on penetration, enables detailed visualization of internal structures. Building on this capability, existing implicit 3D reconstruction methods have adapted the NeRF model and its variants for internal CT reconstruction. However, these approaches often neglect the significance of objects' anatomical priors for implicit learning, limiting both reconstruction precision and learning efficiency, particularly in ultra-sparse view scenarios. To address these challenges, we propose a novel 3D CT reconstruction framework that employs a 'target prior' derived from the object's projection data to enhance implicit learning. Our approach integrates positional and structural encoding to facilitate voxel-wise implicit reconstruction, utilizing the target prior to guide voxel sampling and enrich structural encoding. This dual strategy significantly boosts both learning efficiency and reconstruction quality. Additionally, we introduce a CUDA-based algorithm for rapid estimation of high-quality 3D target priors from sparse-view projections. Experiments utilizing projection data from a complex abdominal dataset demonstrate that the proposed model substantially enhances learning efficiency, outperforming the current leading model, NAF, by a factor of ten. In terms of reconstruction quality, it also exceeds the most accurate model, NeRP, achieving PSNR improvements of 3.57 dB, 5.42 dB, and 5.70 dB with 10, 20, and 30 projections, respectively. The code is available at https://github.com/qlcao171/TPG-INR.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "Please consider this version as the latest camera-ready version",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18806v1",
      "code_links": [
        {
          "url": "https://github.com/qlcao171/TPG-INR",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "NeRF"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "PartDiffuser: Part-wise 3D Mesh Generation via Discrete Diffusion",
      "authors": [
        "Yichen Yang",
        "Hong Li",
        "Haodong Zhu",
        "Linin Yang",
        "Guojun Lei",
        "Sheng Xu",
        "Baochang Zhang"
      ],
      "arxiv_id": "2511.18801v1",
      "summary": "Existing autoregressive (AR) methods for generating artist-designed meshes struggle to balance global structural consistency with high-fidelity local details, and are susceptible to error accumulation. To address this, we propose PartDiffuser, a novel semi-autoregressive diffusion framework for point-cloud-to-mesh generation. The method first performs semantic segmentation on the mesh and then operates in a \"part-wise\" manner: it employs autoregression between parts to ensure global topology, while utilizing a parallel discrete diffusion process within each semantic part to precisely reconstruct high-frequency geometric features. PartDiffuser is based on the DiT architecture and introduces a part-aware cross-attention mechanism, using point clouds as hierarchical geometric conditioning to dynamically control the generation process, thereby effectively decoupling the global and local generation tasks. Experiments demonstrate that this method significantly outperforms state-of-the-art (SOTA) models in generating 3D meshes with rich detail, exhibiting exceptional detail representation suitable for real-world applications.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18801v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Understanding Task Transfer in Vision-Language Models",
      "authors": [
        "Bhuvan Sachdeva",
        "Karan Uppal",
        "Abhinav Java",
        "Vineeth N. Balasubramanian"
      ],
      "arxiv_id": "2511.18787v1",
      "summary": "Vision-Language Models (VLMs) perform well on multimodal benchmarks but lag behind humans and specialized models on visual perception tasks like depth estimation or object counting. Finetuning on one task can unpredictably affect performance on others, making task-specific finetuning challenging. In this paper, we address this challenge through a systematic study of task transferability. We examine how finetuning a VLM on one perception task affects its zero-shot performance on others. To quantify these effects, we introduce Perfection Gap Factor (PGF), a metric that captures both the breadth and magnitude of transfer. Using three open-weight VLMs evaluated across 13 perception tasks, we construct a task-transfer graph that reveals previously unobserved relationships among perception tasks. Our analysis uncovers patterns of positive and negative transfer, identifies groups of tasks that mutually influence each other, organizes tasks into personas based on their transfer behavior and demonstrates how PGF can guide data selection for more efficient training. These findings highlight both opportunities for positive transfer and risks of negative interference, offering actionable guidance for advancing VLMs.",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18787v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Inverse Rendering for High-Genus Surface Meshes from Multi-View Images",
      "authors": [
        "Xiang Gao",
        "Xinmu Wang",
        "Xiaolong Wu",
        "Jiazhi Li",
        "Jingyu Shi",
        "Yu Guo",
        "Yuanpeng Liu",
        "Xiyun Song",
        "Heather Yu",
        "Zongfang Lin",
        "Xianfeng David Gu"
      ],
      "arxiv_id": "2511.18680v1",
      "summary": "We present a topology-informed inverse rendering approach for reconstructing high-genus surface meshes from multi-view images. Compared to 3D representations like voxels and point clouds, mesh-based representations are preferred as they enable the application of differential geometry theory and are optimized for modern graphics pipelines. However, existing inverse rendering methods often fail catastrophically on high-genus surfaces, leading to the loss of key topological features, and tend to oversmooth low-genus surfaces, resulting in the loss of surface details. This failure stems from their overreliance on Adam-based optimizers, which can lead to vanishing and exploding gradients. To overcome these challenges, we introduce an adaptive V-cycle remeshing scheme in conjunction with a re-parametrized Adam optimizer to enhance topological and geometric awareness. By periodically coarsening and refining the deforming mesh, our method informs mesh vertices of their current topology and geometry before optimization, mitigating gradient issues while preserving essential topological features. Additionally, we enforce topological consistency by constructing topological primitives with genus numbers that match those of ground truth using Gauss-Bonnet theorem. Experimental results demonstrate that our inverse rendering approach outperforms the current state-of-the-art method, achieving significant improvements in Chamfer Distance and Volume IoU, particularly for high-genus surfaces, while also enhancing surface details for low-genus surfaces.",
      "categories": [
        "cs.GR",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "comment": "3DV2026 Accepted (Poster)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18680v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "An Analysis of Constraint-Based Multi-Agent Pathfinding Algorithms",
      "authors": [
        "Hannah Lee",
        "James D. Motes",
        "Marco Morales",
        "Nancy M. Amato"
      ],
      "arxiv_id": "2511.18604v1",
      "summary": "This study informs the design of future multi-agent pathfinding (MAPF) and multi-robot motion planning (MRMP) algorithms by guiding choices based on constraint classification for constraint-based search algorithms. We categorize constraints as conservative or aggressive and provide insights into their search behavior, focusing specifically on vanilla Conflict-Based Search (CBS) and Conflict-Based Search with Priorities (CBSw/P). Under a hybrid grid-roadmap representation with varying resolution, we observe that aggressive (priority constraint) formulations tend to solve more instances as agent count or resolution increases, whereas conservative (motion constraint) formulations yield stronger solution quality when both succeed. Findings are synthesized in a decision flowchart, aiding users in selecting suitable constraints. Recommendations extend to Multi-Robot Motion Planning (MRMP), emphasizing the importance of considering topological features alongside problem, solution, and representation features. A comprehensive exploration of the study, including raw data and map performance, is available in our public GitHub Repository: https://GitHub.com/hannahjmlee/constraint-mapf-analysis",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-23",
      "updated": "2025-11-23",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18604v1",
      "code_links": [
        {
          "url": "https://GitHub.com/hannahjmlee/constraint-mapf-analysis",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "motion planning"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Explicit Bounds on the Hausdorff Distance for Truncated mRPI Sets via Norm-Dependent Contraction Rates",
      "authors": [
        "Jiaxun Sun"
      ],
      "arxiv_id": "2511.18374v1",
      "summary": "This paper establishes the first explicit and closed-form upper bound on the Hausdorff distance between the truncated minimal robust positively invariant (mRPI) set and its infinite-horizon limit. While existing mRPI approximations guarantee asymptotic convergence through geometric or norm-based arguments, none provides a computable expression that quantifies the truncation error for a given horizon. We show that the error satisfies \\( d_H(\\mathcal{E}_N,\\mathcal{E}_\\infty) \\le r_W\\,γ^{N+1}/(1-γ), \\) where $γ<1$ is the induced-norm contraction factor and $r_W$ depends only on the disturbance set. The bound is fully analytic, requires no iterative set computations, and directly characterizes the decay rate of the truncated Minkowski series. We further demonstrate that the choice of vector norm serves as a design parameter that accelerates convergence, enabling substantially tighter horizon selection for robust invariant-set computations and tube-based MPC. Numerical experiments validate the sharpness, scalability, and practical relevance of the proposed bound.",
      "categories": [
        "cs.RO",
        "eess.SY",
        "math.DS"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-23",
      "updated": "2025-11-23",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18374v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "MPC"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "SwiftVGGT: A Scalable Visual Geometry Grounded Transformer for Large-Scale Scenes",
      "authors": [
        "Jungho Lee",
        "Minhyeok Lee",
        "Sunghun Yang",
        "Minseok Kang",
        "Sangyoun Lee"
      ],
      "arxiv_id": "2511.18290v1",
      "summary": "3D reconstruction in large-scale scenes is a fundamental task in 3D perception, but the inherent trade-off between accuracy and computational efficiency remains a significant challenge. Existing methods either prioritize speed and produce low-quality results, or achieve high-quality reconstruction at the cost of slow inference times. In this paper, we propose SwiftVGGT, a training-free method that significantly reduce inference time while preserving high-quality dense 3D reconstruction. To maintain global consistency in large-scale scenes, SwiftVGGT performs loop closure without relying on the external Visual Place Recognition (VPR) model. This removes redundant computation and enables accurate reconstruction over kilometer-scale environments. Furthermore, we propose a simple yet effective point sampling method to align neighboring chunks using a single Sim(3)-based Singular Value Decomposition (SVD) step. This eliminates the need for the Iteratively Reweighted Least Squares (IRLS) optimization commonly used in prior work, leading to substantial speed-ups. We evaluate SwiftVGGT on multiple datasets and show that it achieves state-of-the-art reconstruction quality while requiring only 33% of the inference time of recent VGGT-based large-scale reconstruction approaches.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-23",
      "updated": "2025-11-23",
      "comment": "Project Page: https://Jho-Yonsei.github.io/SwiftVGGT/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18290v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "VGGT"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "UniFlow: Towards Zero-Shot LiDAR Scene Flow for Autonomous Vehicles via Cross-Domain Generalization",
      "authors": [
        "Siyi Li",
        "Qingwen Zhang",
        "Ishan Khatri",
        "Kyle Vedder",
        "Deva Ramanan",
        "Neehar Peri"
      ],
      "arxiv_id": "2511.18254v1",
      "summary": "LiDAR scene flow is the task of estimating per-point 3D motion between consecutive point clouds. Recent methods achieve centimeter-level accuracy on popular autonomous vehicle (AV) datasets, but are typically only trained and evaluated on a single sensor. In this paper, we aim to learn general motion priors that transfer to diverse and unseen LiDAR sensors. However, prior work in LiDAR semantic segmentation and 3D object detection demonstrate that naively training on multiple datasets yields worse performance than single dataset models. Interestingly, we find that this conventional wisdom does not hold for motion estimation, and that state-of-the-art scene flow methods greatly benefit from cross-dataset training. We posit that low-level tasks such as motion estimation may be less sensitive to sensor configuration; indeed, our analysis shows that models trained on fast-moving objects (e.g., from highway datasets) perform well on fast-moving objects, even across different datasets. Informed by our analysis, we propose UniFlow, a family of feedforward models that unifies and trains on multiple large-scale LiDAR scene flow datasets with diverse sensor placements and point cloud densities. Our frustratingly simple solution establishes a new state-of-the-art on Waymo and nuScenes, improving over prior work by 5.1% and 35.2% respectively. Moreover, UniFlow achieves state-of-the-art accuracy on unseen datasets like TruckScenes, outperforming prior TruckScenes-specific models by 30.1%.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-23",
      "updated": "2025-11-23",
      "comment": "Project Page: https://lisiyi777.github.io/UniFlow/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18254v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "InfiniBench: Infinite Benchmarking for Visual Spatial Reasoning with Customizable Scene Complexity",
      "authors": [
        "Haoming Wang",
        "Qiyao Xue",
        "Wei Gao"
      ],
      "arxiv_id": "2511.18200v2",
      "summary": "Modern vision-language models (VLMs) are expected to have abilities of spatial reasoning with diverse scene complexities, but evaluating such abilities is difficult due to the lack of benchmarks that are not only diverse and scalable but also fully customizable. Existing benchmarks offer limited customizability over the scene complexity and are incapable of isolating and analyzing specific VLM failure modes under distinct spatial conditions. To address this gap, instead of individually presenting benchmarks for different scene complexities, in this paper we present InfiniBench, a fully automated, customizable and user-friendly benchmark generator that can synthesize a theoretically infinite variety of 3D scenes with parameterized control on scene complexity. InfiniBench uniquely translates scene descriptions in natural language into photo-realistic videos with complex and physically plausible 3D layouts. This is achieved through three key innovations: 1) a LLM-based agentic framework that iteratively refines procedural scene constraints from scene descriptions; 2) a flexible cluster-based layout optimizer that generates dense and cluttered scenes previously intractable for procedural methods; and 3) a task-aware camera trajectory optimization method that renders scenes into videos with full object coverage as VLM input. Experiments demonstrate that InfiniBench outperforms state-of-the-art procedural and LLM-based 3D generation methods in prompt fidelity and physical plausibility, especially in high-complexity scenarios. We further showcased the usefulness of InfiniBench, by generating benchmarks for representative spatial reasoning tasks including measurement, perspective-taking and spatiotemporal tracking.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-22",
      "updated": "2025-12-05",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18200v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "trajectory optimization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Muskie: Multi-view Masked Image Modeling for 3D Vision Pre-training",
      "authors": [
        "Wenyu Li",
        "Sidun Liu",
        "Peng Qiao",
        "Yong Dou",
        "Tongrui Hu"
      ],
      "arxiv_id": "2511.18115v1",
      "summary": "We present Muskie, a native multi-view vision backbone designed for 3D vision tasks. Unlike existing models, which are frame-wise and exhibit limited multi-view consistency, Muskie is designed to process multiple views simultaneously and introduce multi-view consistency in pre-training stage. Muskie is trained to reconstruct heavily masked content in one view by finding and utilizing geometric correspondences from other views. Through this pretext task and our proposed aggressive masking strategy, the model implicitly to learn view-invariant features and develop strong geometric understanding without any 3D supervision. Compared with state-of-the-art frame-wise backbones such as DINO, Muskie achieves higher multi-view correspondence accuracy. Furthermore, we demonstrate that using Muskie as a backbone consistently enhances performance on downstream 3D tasks, including camera pose estimation and pointmap reconstruction. Codes are publicly available at https://leo-frank.github.io/Muskie/",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-22",
      "updated": "2025-11-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18115v1",
      "code_links": [
        {
          "url": "https://leo-frank.github.io/Muskie/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "pose estimation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "AdaPerceiver: Transformers with Adaptive Width, Depth, and Tokens",
      "authors": [
        "Purvish Jajal",
        "Nick John Eliopoulos",
        "Benjamin Shiue-Hal Chou",
        "George K. Thiruvathukal",
        "Yung-Hsiang Lu",
        "James C. Davis"
      ],
      "arxiv_id": "2511.18105v1",
      "summary": "Modern transformer architectures achieve remarkable performance across tasks and domains but remain rigid in how they allocate computation at inference time. Real-world deployment often requires models to adapt to diverse hardware and latency constraints, yet most approaches to dynamic computation focus on a single axis -- such as reducing the number of tokens. We present a novel capability: AdaPerceiver, the first transformer architecture with unified adaptivity across depth, width, and tokens within a single model. We propose an architecture that supports adaptivity along these axes. We couple this with an efficient joint training regime that ensures the model maintains performance across its various configurations. We evaluate AdaPerceiver on image classification, semantic segmentation, and depth estimation tasks. On image classification, AdaPerceiver expands the accuracy-throughput Pareto front. It achieves 85.4% accuracy while yielding 36% higher throughput than FlexiViT-L. On dense prediction, AdaPerceiver matches ViT-H/14 while having $\\sim$26x fewer encoder FLOPs (floating-point operations) on semantic segmentation and depth estimation. Finally, we show how AdaPerceiver equipped with a policy can maintain ImageNet1K accuracy ($\\pm0.1$ percentage points) while reducing FLOPs by $24-33$%.",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-22",
      "updated": "2025-11-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18105v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Spotlight: Identifying and Localizing Video Generation Errors Using VLMs",
      "authors": [
        "Aditya Chinchure",
        "Sahithya Ravi",
        "Pushkar Shukla",
        "Vered Shwartz",
        "Leonid Sigal"
      ],
      "arxiv_id": "2511.18102v1",
      "summary": "Current text-to-video models (T2V) can generate high-quality, temporally coherent, and visually realistic videos. Nonetheless, errors still often occur, and are more nuanced and local compared to the previous generation of T2V models. While current evaluation paradigms assess video models across diverse dimensions, they typically evaluate videos holistically without identifying when specific errors occur or describing their nature. We address this gap by introducing Spotlight, a novel task aimed at localizing and explaining video-generation errors. We generate 600 videos using 200 diverse textual prompts and three state-of-the-art video generators (Veo 3, Seedance, and LTX-2), and annotate over 1600 fine-grained errors across six types, including motion, physics, and prompt adherence. We observe that adherence and physics errors are predominant and persist across longer segments, whereas appearance-disappearance and body pose errors manifest in shorter segments. We then evaluate current VLMs on Spotlight and find that VLMs lag significantly behind humans in error identification and localization in videos. We propose inference-time strategies to probe the limits of current VLMs on our task, improving performance by nearly 2x. Our task paves a way forward to building fine-grained evaluation tools and more sophisticated reward models for video generators.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-22",
      "updated": "2025-11-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18102v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Continually Evolving Skill Knowledge in Vision Language Action Model",
      "authors": [
        "Yuxuan Wu",
        "Guangming Wang",
        "Zhiheng Yang",
        "Maoqing Yao",
        "Brian Sheil",
        "Hesheng Wang"
      ],
      "arxiv_id": "2511.18085v2",
      "summary": "Developing general robot intelligence in open environments requires continual skill learning. Recent Vision-Language-Action (VLA) models leverage massive pretraining data to support diverse manipulation tasks, but they still depend heavily on task-specific fine-tuning, revealing a lack of continual learning capability. Existing continual learning methods are also resource-intensive to scale to VLA models. We propose Stellar VLA, a knowledge-driven continual learning framework with two variants: T-Stellar, modeling task-centric knowledge space, and TS-Stellar, capturing hierarchical task-skill structure. Stellar VLA enables self-supervised knowledge evolution through joint learning of task latent representation and the knowledge space, reducing annotation needs. Knowledge-guided expert routing provide task specialization without extra network parameters, lowering training overhead. Experiments on the LIBERO benchmark and real-world tasks show over 50 percentage average improvement in final success rates relative to baselines. TS-Stellar further excels in complex action inference, and in-depth analyses verify effective knowledge retention and discovery. Our code will be released soon.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-22",
      "updated": "2025-11-25",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18085v2",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "ActDistill: General Action-Guided Self-Derived Distillation for Efficient Vision-Language-Action Models",
      "authors": [
        "Wencheng Ye",
        "Tianshi Wang",
        "Lei Zhu",
        "Fengling Li",
        "Guoli Yang"
      ],
      "arxiv_id": "2511.18082v1",
      "summary": "Recent Vision-Language-Action (VLA) models have shown impressive flexibility and generalization, yet their deployment in robotic manipulation remains limited by heavy computational overhead and inference latency. In this work, we present ActDistill, a general action-guided self-derived distillation framework that transfers the action prediction capability of any existing VLA model to a lightweight counterpart. Unlike previous efficiency strategies that primarily emphasize vision-language correlations, ActDistill leverages action priors to guide knowledge transfer and model compression, achieving action-oriented efficiency for VLA models. Specifically, we employ a well-trained VLA model as the teacher and introduce a graph-structured encapsulation strategy to explicitly model the hierarchical evolution of action prediction. The student model, derived from the graph-encapsulated teacher, is further equipped with a dynamic router that adaptively selects computation paths based on action prediction demands, guided by hierarchical graph-informed supervision to ensure smooth and efficient evolution. During inference, graph-related auxiliary components are removed, allowing the student to execute only dynamically routed layers and predict high-precision actions with minimal computation and latency. Experiments on embodied benchmarks demonstrate that ActDistill achieves comparable or superior performance to full-scale VLA models while reducing computation by over 50% with up to 1.67 times speedup, thereby establishing a general paradigm toward efficient embodied intelligence.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-22",
      "updated": "2025-11-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18082v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "VK-Det: Visual Knowledge Guided Prototype Learning for Open-Vocabulary Aerial Object Detection",
      "authors": [
        "Jianhang Yao",
        "Yongbin Zheng",
        "Siqi Lu",
        "Wanying Xu",
        "Peng Sun"
      ],
      "arxiv_id": "2511.18075v1",
      "summary": "To identify objects beyond predefined categories, open-vocabulary aerial object detection (OVAD) leverages the zero-shot capabilities of visual-language models (VLMs) to generalize from base to novel categories. Existing approaches typically utilize self-learning mechanisms with weak text supervision to generate region-level pseudo-labels to align detectors with VLMs semantic spaces. However, text dependence induces semantic bias, restricting open-vocabulary expansion to text-specified concepts. We propose $\\textbf{VK-Det}$, a $\\textbf{V}$isual $\\textbf{K}$nowledge-guided open-vocabulary object $\\textbf{Det}$ection framework $\\textit{without}$ extra supervision. First, we discover and leverage vision encoder's inherent informative region perception to attain fine-grained localization and adaptive distillation. Second, we introduce a novel prototype-aware pseudo-labeling strategy. It models inter-class decision boundaries through feature clustering and maps detection regions to latent categories via prototype matching. This enhances attention to novel objects while compensating for missing supervision. Extensive experiments show state-of-the-art performance, achieving 30.1 $\\mathrm{mAP}^{N}$ on DIOR and 23.3 $\\mathrm{mAP}^{N}$ on DOTA, outperforming even extra supervised methods.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-22",
      "updated": "2025-11-22",
      "comment": "15 pages, 8 figures, accepted by AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18075v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance",
      "authors": [
        "Beining Xu",
        "Siting Zhu",
        "Zhao Jin",
        "Junxian Li",
        "Hesheng Wang"
      ],
      "arxiv_id": "2512.01223v1",
      "summary": "3D Visual Grounding (3DVG) focuses on locating objects in 3D scenes based on natural language descriptions, serving as a fundamental task for embodied AI and robotics. Recent advances in Multi-modal Large Language Models (MLLMs) have motivated research into extending them to 3DVG. However, MLLMs primarily process 2D visual inputs and struggle with understanding 3D spatial structure of scenes solely from these limited perspectives. Existing methods mainly utilize viewpoint-dependent rendering of reconstructed point clouds to provide explicit structural guidance for MLLMs in 3DVG tasks, leading to inefficiency and limited spatial reasoning. To address this issue, we propose S$^2$-MLLM, an efficient framework that enhances spatial reasoning in MLLMs through implicit spatial reasoning. We introduce a spatial guidance strategy that leverages the structure awareness of feed-forward 3D reconstruction. By acquiring 3D structural understanding during training, our model can implicitly reason about 3D scenes without relying on inefficient point cloud reconstruction. Moreover, we propose a structure-enhanced module (SE), which first employs intra-view and inter-view attention mechanisms to capture dependencies within views and correspondences across views. The module further integrates multi-level position encoding to associate visual representations with spatial positions and viewpoint information, enabling more accurate structural understanding. Extensive experiments demonstrate that S$^2$-MLLM unifies superior performance, generalization, and efficiency, achieving significant performance over existing methods across the ScanRefer, Nr3D, and Sr3D datasets. Code will be available upon acceptance.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "comment": "18 pages, 9 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01223v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "OmniFD: A Unified Model for Versatile Face Forgery Detection",
      "authors": [
        "Haotian Liu",
        "Haoyu Chen",
        "Chenhui Pan",
        "You Hu",
        "Guoying Zhao",
        "Xiaobai Li"
      ],
      "arxiv_id": "2512.01128v1",
      "summary": "Face forgery detection encompasses multiple critical tasks, including identifying forged images and videos and localizing manipulated regions and temporal segments. Current approaches typically employ task-specific models with independent architectures, leading to computational redundancy and ignoring potential correlations across related tasks. We introduce OmniFD, a unified framework that jointly addresses four core face forgery detection tasks within a single model, i.e., image and video classification, spatial localization, and temporal localization. Our architecture consists of three principal components: (1) a shared Swin Transformer encoder that extracts unified 4D spatiotemporal representations from both images and video inputs, (2) a cross-task interaction module with learnable queries that dynamically captures inter-task dependencies through attention-based reasoning, and (3) lightweight decoding heads that transform refined representations into corresponding predictions for all FFD tasks. Extensive experiments demonstrate OmniFD's advantage over task-specific models. Its unified design leverages multi-task learning to capture generalized representations across tasks, especially enabling fine-grained knowledge transfer that facilitates other tasks. For example, video classification accuracy improves by 4.63% when image data are incorporated. Furthermore, by unifying images, videos and the four tasks within one framework, OmniFD achieves superior performance across diverse benchmarks with high efficiency and scalability, e.g., reducing 63% model parameters and 50% training time. It establishes a practical and generalizable solution for comprehensive face forgery detection in real-world applications. The source code is made available at https://github.com/haotianll/OmniFD.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01128v1",
      "code_links": [
        {
          "url": "https://github.com/haotianll/OmniFD",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "localization"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Learning Eigenstructures of Unstructured Data Manifolds",
      "authors": [
        "Roy Velich",
        "Arkadi Piven",
        "David Bensaïd",
        "Daniel Cremers",
        "Thomas Dagès",
        "Ron Kimmel"
      ],
      "arxiv_id": "2512.01103v1",
      "summary": "We introduce a novel framework that directly learns a spectral basis for shape and manifold analysis from unstructured data, eliminating the need for traditional operator selection, discretization, and eigensolvers. Grounded in optimal-approximation theory, we train a network to decompose an implicit approximation operator by minimizing the reconstruction error in the learned basis over a chosen distribution of probe functions. For suitable distributions, they can be seen as an approximation of the Laplacian operator and its eigendecomposition, which are fundamental in geometry processing. Furthermore, our method recovers in a unified manner not only the spectral basis, but also the implicit metric's sampling density and the eigenvalues of the underlying operator. Notably, our unsupervised method makes no assumption on the data manifold, such as meshing or manifold dimensionality, allowing it to scale to arbitrary datasets of any dimension. On point clouds lying on surfaces in 3D and high-dimensional image manifolds, our approach yields meaningful spectral bases, that can resemble those of the Laplacian, without explicit construction of an operator. By replacing the traditional operator selection, construction, and eigendecomposition with a learning-based approach, our framework offers a principled, data-driven alternative to conventional pipelines. This opens new possibilities in geometry processing for unstructured data, particularly in high-dimensional spaces.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.01103v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "point cloud"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "SAGAS: Semantic-Aware Graph-Assisted Stitching for Offline Temporal Logic Planning",
      "authors": [
        "Ruijia Liu",
        "Ancheng Hou",
        "Shaoyuan Li",
        "Xiang Yin"
      ],
      "arxiv_id": "2512.00775v1",
      "summary": "Linear Temporal Logic (LTL) provides a rigorous framework for complex robotic tasks, yet existing methods often rely on accurate dynamics models or expensive online interactions. In this work, we address LTL-constrained control in a challenging offline, model-free setting, utilizing only fixed, task-agnostic datasets of fragmented trajectories. We propose SAGAS, a novel framework combining graph-assisted trajectory stitching with automata-guided planning. First, we construct a latent reachability graph from a learned temporal-distance representation. To bridge the semantic gap, we augment this graph with certified anchor nodes and probabilistic soft labels. We then translate the specification into a Büchi automaton and search the implicit product space to derive a cost-minimal prefix-suffix plan. Finally, a subgoal-conditioned low-level policy is deployed to execute these latent waypoints. Experiments on OGBench locomotion domains demonstrate that SAGAS successfully synthesizes efficient trajectories for diverse LTL tasks, effectively bridging the gap between fragmented offline data and complex logical constraints.",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00775v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "locomotion"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "Charts Are Not Images: On the Challenges of Scientific Chart Editing",
      "authors": [
        "Shawn Li",
        "Ryan Rossi",
        "Sungchul Kim",
        "Sunav Choudhary",
        "Franck Dernoncourt",
        "Puneet Mathur",
        "Zhengzhong Tu",
        "Yue Zhao"
      ],
      "arxiv_id": "2512.00752v1",
      "summary": "Generative models, such as diffusion and autoregressive approaches, have demonstrated impressive capabilities in editing natural images. However, applying these tools to scientific charts rests on a flawed assumption: a chart is not merely an arrangement of pixels but a visual representation of structured data governed by a graphical grammar. Consequently, chart editing is not a pixel-manipulation task but a structured transformation problem. To address this fundamental mismatch, we introduce \\textit{FigEdit}, a large-scale benchmark for scientific figure editing comprising over 30,000 samples. Grounded in real-world data, our benchmark is distinguished by its diversity, covering 10 distinct chart types and a rich vocabulary of complex editing instructions. The benchmark is organized into five distinct and progressively challenging tasks: single edits, multi edits, conversational edits, visual-guidance-based edits, and style transfer. Our evaluation of a range of state-of-the-art models on this benchmark reveals their poor performance on scientific figures, as they consistently fail to handle the underlying structured transformations required for valid edits. Furthermore, our analysis indicates that traditional evaluation metrics (e.g., SSIM, PSNR) have limitations in capturing the semantic correctness of chart edits. Our benchmark demonstrates the profound limitations of pixel-level manipulation and provides a robust foundation for developing and evaluating future structure-aware models. By releasing \\textit{FigEdit} (https://github.com/adobe-research/figure-editing), we aim to enable systematic progress in structure-aware figure editing, provide a common ground for fair comparison, and encourage future research on models that understand both the visual and semantic layers of scientific charts.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00752v1",
      "code_links": [
        {
          "url": "https://github.com/adobe-research/figure-editing",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "REM: Evaluating LLM Embodied Spatial Reasoning through Multi-Frame Trajectories",
      "authors": [
        "Jacob Thompson",
        "Emiliano Garcia-Lopez",
        "Yonatan Bisk"
      ],
      "arxiv_id": "2512.00736v1",
      "summary": "Humans build viewpoint-independent cognitive maps through navigation, enabling intuitive reasoning about object permanence and spatial relations. We argue that multimodal large language models (MLLMs), despite extensive video training, lack this fundamental spatial reasoning capability, a critical limitation for embodied applications. To demonstrate these limitations and drive research, we introduce REM (Reasoning over Embodied Multi-Frame Trajectories), a benchmark using controllable 3D environments for long-horizon embodied spatial reasoning. REM systematically evaluates key aspects like object permanence/distinction, spatial relationships, and numerical tracking across dynamic embodied viewpoints. Our evaluation shows that the best-performing current models exhibit promising overall performance, but become increasingly unreliable at even moderate complexity levels easily handled by humans. These findings highlight challenges MLLMs face in developing robust spatial representations from sequential visual input. Consequently, REM provides targeted metrics and diagnostics to foster improved spatial understanding in future models.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "published": "2025-11-30",
      "updated": "2025-11-30",
      "comment": "",
      "doi": "",
      "journal_ref": "Proceedings of the Conference on Language Modeling (COLM 2025)",
      "pdf_url": "https://arxiv.org/pdf/2512.00736v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "navigation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "SatireDecoder: Visual Cascaded Decoupling for Enhancing Satirical Image Comprehension",
      "authors": [
        "Yue Jiang",
        "Haiwei Xue",
        "Minghao Han",
        "Mingcheng Li",
        "Xiaolu Hou",
        "Dingkang Yang",
        "Lihua Zhang",
        "Xu Zheng"
      ],
      "arxiv_id": "2512.00582v1",
      "summary": "Satire, a form of artistic expression combining humor with implicit critique, holds significant social value by illuminating societal issues. Despite its cultural and societal significance, satire comprehension, particularly in purely visual forms, remains a challenging task for current vision-language models. This task requires not only detecting satire but also deciphering its nuanced meaning and identifying the implicated entities. Existing models often fail to effectively integrate local entity relationships with global context, leading to misinterpretation, comprehension biases, and hallucinations. To address these limitations, we propose SatireDecoder, a training-free framework designed to enhance satirical image comprehension. Our approach proposes a multi-agent system performing visual cascaded decoupling to decompose images into fine-grained local and global semantic representations. In addition, we introduce a chain-of-thought reasoning strategy guided by uncertainty analysis, which breaks down the complex satire comprehension process into sequential subtasks with minimized uncertainty. Our method significantly improves interpretive accuracy while reducing hallucinations. Experimental results validate that SatireDecoder outperforms existing baselines in comprehending visual satire, offering a promising direction for vision-language reasoning in nuanced, high-level semantic tasks.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-29",
      "updated": "2025-11-29",
      "comment": "Accepted by AAAI 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00582v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "HuMoR"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "6_video_extraction"
      ]
    },
    {
      "title": "Describe Anything Anywhere At Any Moment",
      "authors": [
        "Nicolas Gorlo",
        "Lukas Schmid",
        "Luca Carlone"
      ],
      "arxiv_id": "2512.00565v1",
      "summary": "Computer vision and robotics applications ranging from augmented reality to robot autonomy in large-scale environments require spatio-temporal memory frameworks that capture both geometric structure for accurate language-grounding as well as semantic detail. Existing methods face a tradeoff, where producing rich open-vocabulary descriptions comes at the expense of real-time performance when these descriptions have to be grounded in 3D. To address these challenges, we propose Describe Anything, Anywhere, at Any Moment (DAAAM), a novel spatio-temporal memory framework for large-scale and real-time 4D scene understanding. DAAAM introduces a novel optimization-based frontend to infer detailed semantic descriptions from localized captioning models, such as the Describe Anything Model (DAM), leveraging batch processing to speed up inference by an order of magnitude for online processing. It leverages such semantic understanding to build a hierarchical 4D scene graph (SG), which acts as an effective globally spatially and temporally consistent memory representation. DAAAM constructs 4D SGs with detailed, geometrically grounded descriptions while maintaining real-time performance. We show that DAAAM's 4D SG interfaces well with a tool-calling agent for inference and reasoning.\n  We thoroughly evaluate DAAAM in the complex task of spatio-temporal question answering on the NaVQA benchmark and show its generalization capabilities for sequential task grounding on the SG3D benchmark. We further curate an extended OC-NaVQA benchmark for large-scale and long-time evaluations. DAAAM achieves state-of-the-art results in both tasks, improving OC-NaVQA question accuracy by 53.6%, position errors by 21.9%, temporal errors by 21.6%, and SG3D task grounding accuracy by 27.8% over the most competitive baselines, respectively. We release our data and code open-source.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-29",
      "updated": "2025-11-29",
      "comment": "14 pages, 5 figures, 6 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00565v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene understanding"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "CC-FMO: Camera-Conditioned Zero-Shot Single Image to 3D Scene Generation with Foundation Model Orchestration",
      "authors": [
        "Boshi Tang",
        "Henry Zheng",
        "Rui Huang",
        "Gao Huang"
      ],
      "arxiv_id": "2512.00493v1",
      "summary": "High-quality 3D scene generation from a single image is crucial for AR/VR and embodied AI applications. Early approaches struggle to generalize due to reliance on specialized models trained on curated small datasets. While recent advancements in large-scale 3D foundation models have significantly enhanced instance-level generation, coherent scene generation remains a challenge, where performance is limited by inaccurate per-object pose estimations and spatial inconsistency. To this end, this paper introduces CC-FMO, a zero-shot, camera-conditioned pipeline for single-image to 3D scene generation that jointly conforms to the object layout in input image and preserves instance fidelity. CC-FMO employs a hybrid instance generator that combines semantics-aware vector-set representation with detail-rich structured latent representation, yielding object geometries that are both semantically plausible and high-quality. Furthermore, CC-FMO enables the application of foundational pose estimation models in the scene generation task via a simple yet effective camera-conditioned scale-solving algorithm, to enforce scene-level coherence. Extensive experiments demonstrate that CC-FMO consistently generates high-fidelity camera-aligned compositional scenes, outperforming all state-of-the-art methods.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-29",
      "updated": "2025-11-29",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00493v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "pose estimation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards",
      "authors": [
        "Minh-Quan Le",
        "Yuanzhi Zhu",
        "Vicky Kalogeiton",
        "Dimitris Samaras"
      ],
      "arxiv_id": "2512.00425v1",
      "summary": "Recent video diffusion models can synthesize visually compelling clips, yet often violate basic physical laws-objects float, accelerations drift, and collisions behave inconsistently-revealing a persistent gap between visual realism and physical realism. We propose $\\texttt{NewtonRewards}$, the first physics-grounded post-training framework for video generation based on $\\textit{verifiable rewards}$. Instead of relying on human or VLM feedback, $\\texttt{NewtonRewards}$ extracts $\\textit{measurable proxies}$ from generated videos using frozen utility models: optical flow serves as a proxy for velocity, while high-level appearance features serve as a proxy for mass. These proxies enable explicit enforcement of Newtonian structure through two complementary rewards: a Newtonian kinematic constraint enforcing constant-acceleration dynamics, and a mass conservation reward preventing trivial, degenerate solutions. We evaluate $\\texttt{NewtonRewards}$ on five Newtonian Motion Primitives (free fall, horizontal/parabolic throw, and ramp sliding down/up) using our newly constructed large-scale benchmark, $\\texttt{NewtonBench-60K}$. Across all primitives in visual and physics metrics, $\\texttt{NewtonRewards}$ consistently improves physical plausibility, motion smoothness, and temporal coherence over prior post-training methods. It further maintains strong performance under out-of-distribution shifts in height, speed, and friction. Our results show that physics-grounded verifiable rewards offer a scalable path toward physics-aware video generation.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-11-29",
      "updated": "2025-11-29",
      "comment": "Project page: https://cvlab-stonybrook.github.io/NewtonRewards",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00425v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知 (Perception & SLAM)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "optical flow"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    }
  ],
  "query_info": {
    "tags": [
      "cs.RO",
      "cs.CV",
      "cs.GR"
    ],
    "date_ranges": [
      [
        "2025-11-01T00:00:00-04:00",
        "2025-11-07T00:00:00-05:00",
        "2025-11-07"
      ],
      [
        "2025-11-08T00:00:00-05:00",
        "2025-11-14T00:00:00-05:00",
        "2025-11-14"
      ],
      [
        "2025-11-15T00:00:00-05:00",
        "2025-11-21T00:00:00-05:00",
        "2025-11-21"
      ],
      [
        "2025-11-22T00:00:00-05:00",
        "2025-11-28T00:00:00-05:00",
        "2025-11-28"
      ],
      [
        "2025-11-29T00:00:00-05:00",
        "2025-11-30T23:59:59-05:00",
        "2025-11-30"
      ]
    ],
    "filtered": true
  }
}