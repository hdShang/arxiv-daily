{
    "papers": [
        {
            "title": "Humanoid Robot Running Through Random Stepping Stones and Jumping Over Obstacles: Step Adaptation Using Spring-Mass Trajectories",
            "authors": [
                "Sait Sovukluk",
                "Johannes Englsberger",
                "Christian Ott"
            ],
            "arxiv_id": "2512.13304v1",
            "summary": "This study proposes a step adaptation framework for running through spring-mass trajectories and deadbeat control gain libraries. It includes four main parts: (1) Automatic spring-mass trajectory library generation; (2) Deadbeat control gain library generation through an actively controlled template model that resembles the whole-body dynamics well; (3) Trajectory selection policy development for step adaptation; (4) Mapping spring-mass trajectories to a humanoid model through a whole-body control (WBC) framework also accounting for closed-kinematic chain systems, self collisions, and reactive limb swinging. We show the inclusiveness and the robustness of the proposed framework through various challenging and agile behaviors such as running through randomly generated stepping stones, jumping over random obstacles, performing slalom motions, changing the running direction suddenly with a random leg, and rejecting significant disturbances and uncertainties through the MuJoCo physics simulator. We also perform additional simulations under a comprehensive set of uncertainties and noise to better justify the proposed method's robustness to real-world challenges, including signal noise, imprecision, modeling errors, and delays. All the aforementioned behaviors are performed with a single library and the same set of WBC control parameters without additional tuning. The spring-mass and the deadbeat control gain library are automatically computed in 4.5 seconds in total for 315 different trajectories.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "Accepted for publication in Biomimetic Intelligence and Robotics. Supplemental video: https://youtu.be/HlAg2nbNct4",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13304v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]humanoid",
                        "[T]humanoid robot",
                        "whole-body control",
                        "WBC",
                        "[T]running",
                        "[T]jumping"
                    ],
                    "score": 28.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "MuJoCo"
                    ],
                    "score": 1.5
                }
            ],
            "relevance_score": 29.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出基于弹簧-质量轨迹的人形机器人步态自适应框架，实现复杂地形运动。",
            "summary_zh": "本研究提出了一种步态自适应框架，用于通过弹簧-质量轨迹和无差拍控制增益库实现跑步运动。该框架包含四个主要部分：（1）自动生成弹簧-质量轨迹库；（2）通过主动控制的模板模型生成无差拍控制增益库，该模型能够很好地模拟全身动力学；（3）开发用于步态自适应的轨迹选择策略；（4）通过全身控制（WBC）框架将弹簧-质量轨迹映射到人形机器人模型，同时考虑闭链运动系统、自碰撞和反应性肢体摆动。我们通过各种具有挑战性的敏捷行为，例如在随机生成的踏脚石上跑步、跳过随机障碍物、执行蛇形运动、用随机一条腿突然改变跑步方向以及通过MuJoCo物理模拟器抑制显著的扰动和不确定性，展示了所提出框架的包容性和鲁棒性。我们还在一套全面的不确定性和噪声下进行了额外的模拟，以更好地证明所提出的方法对现实世界挑战的鲁棒性，包括信号噪声、不精确性、建模误差和延迟。所有上述行为都是使用单个库和同一组WBC控制参数执行的，无需额外调整。弹簧-质量和无差拍控制增益库总共在4.5秒内自动计算出315条不同的轨迹。",
            "intro_zh": [
                "现有方法在复杂地形下人形机器人运动控制方面存在挑战，难以兼顾鲁棒性和敏捷性。",
                "该论文提出基于弹簧-质量模型的步态规划与自适应框架，结合全身控制实现复杂地形运动。",
                "实验表明，该框架能使人形机器人在随机踏脚石、跳跃障碍等场景下稳定运动，且无需额外调参。"
            ],
            "method_zh": "**问题定义**：论文旨在解决人形机器人在复杂地形（如随机分布的踏脚石、障碍物）中运动的难题。现有方法通常难以在鲁棒性和敏捷性之间取得平衡，难以适应地形变化，需要大量人工调整参数。\\n\\n**核心思路**：核心思路是利用弹簧-质量模型来简化人形机器人的运动规划，并结合无差拍控制增益库实现快速的步态自适应。弹簧-质量模型能够捕捉跑步运动的关键动力学特征，而无差拍控制则保证了快速的响应和稳定性。\\n\\n**技术框架**：整体框架包含四个主要模块：1) 自动生成弹簧-质量轨迹库；2) 生成无差拍控制增益库；3) 开发轨迹选择策略，用于步态自适应；4) 通过全身控制（WBC）框架将弹簧-质量轨迹映射到人形机器人模型，同时考虑闭链运动系统、自碰撞和反应性肢体摆动。\\n\\n**关键创新**：关键创新在于将弹簧-质量模型与无差拍控制相结合，实现了一种高效且鲁棒的步态自适应方法。此外，该框架能够自动生成轨迹库和控制增益库，减少了人工调整的工作量。\\n\\n**关键设计**：轨迹选择策略是关键设计之一，它根据当前地形和机器人状态，从轨迹库中选择合适的轨迹。全身控制框架则负责将选定的轨迹转化为机器人关节的控制指令，同时考虑各种约束条件，如自碰撞避免和关节力矩限制。论文中提到，弹簧-质量和无差拍控制增益库总共在4.5秒内自动计算出315条不同的轨迹。",
            "application_zh": "该研究成果可应用于人形机器人在复杂环境下的搜索救援、物流运输、灾后重建等领域。通过提高机器人的运动能力和环境适应性，使其能够在人类难以到达或危险的区域执行任务，具有重要的实际应用价值和广阔的发展前景。",
            "highlight_zh": "实验结果表明，该框架能够使人形机器人在各种复杂地形下稳定运动，包括随机踏脚石、跳跃障碍物、蛇形运动等。所有行为均使用单一库和相同的WBC控制参数执行，无需额外调整。弹簧-质量和无差拍控制增益库在4.5秒内自动计算出315条不同的轨迹，验证了该方法的效率和鲁棒性。",
            "tags_zh": [
                "人形机器人",
                "步态规划",
                "弹簧-质量模型",
                "全身控制",
                "步态自适应"
            ],
            "_index": 0,
            "_used_api": "gemini"
        },
        {
            "title": "One-Shot Real-World Demonstration Synthesis for Scalable Bimanual Manipulation",
            "authors": [
                "Huayi Zhou",
                "Kui Jia"
            ],
            "arxiv_id": "2512.09297v1",
            "summary": "Learning dexterous bimanual manipulation policies critically depends on large-scale, high-quality demonstrations, yet current paradigms face inherent trade-offs: teleoperation provides physically grounded data but is prohibitively labor-intensive, while simulation-based synthesis scales efficiently but suffers from sim-to-real gaps. We present BiDemoSyn, a framework that synthesizes contact-rich, physically feasible bimanual demonstrations from a single real-world example. The key idea is to decompose tasks into invariant coordination blocks and variable, object-dependent adjustments, then adapt them through vision-guided alignment and lightweight trajectory optimization. This enables the generation of thousands of diverse and feasible demonstrations within several hour, without repeated teleoperation or reliance on imperfect simulation. Across six dual-arm tasks, we show that policies trained on BiDemoSyn data generalize robustly to novel object poses and shapes, significantly outperforming recent baselines. By bridging the gap between efficiency and real-world fidelity, BiDemoSyn provides a scalable path toward practical imitation learning for complex bimanual manipulation without compromising physical grounding.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "under review",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09297v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation",
                        "[T]bi-manual",
                        "[T]bimanual",
                        "dual-arm",
                        "sim-to-real",
                        "trajectory optimization",
                        "teleoperation"
                    ],
                    "score": 26.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "imitation learning"
                    ],
                    "score": 1.5
                }
            ],
            "relevance_score": 27.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "BiDemoSyn：基于单样本真实演示合成可扩展的双臂操作数据",
            "summary_zh": "学习灵巧的双臂操作策略严重依赖于大规模、高质量的演示数据，但现有方法面临固有的权衡：遥操作提供物理上可靠的数据，但劳动强度过高；基于仿真的合成可以高效扩展，但存在模拟到真实的差距。我们提出了BiDemoSyn，一个从单个真实世界示例中合成接触丰富、物理上可行的双臂演示的框架。其核心思想是将任务分解为不变的协调块和可变的、依赖于对象的调整，然后通过视觉引导的对齐和轻量级轨迹优化来调整它们。这使得在几个小时内生成数千个不同的、可行的演示成为可能，而无需重复遥操作或依赖不完善的仿真。在六个双臂任务中，我们表明在BiDemoSyn数据上训练的策略可以稳健地推广到新的对象姿势和形状，显著优于最近的基线。通过弥合效率和真实世界保真度之间的差距，BiDemoSyn为复杂的双臂操作的实际模仿学习提供了一条可扩展的路径，而不会影响物理基础。",
            "intro_zh": [
                "现有双臂操作学习方法依赖大量人工遥操作或受限于仿真与现实的差距，难以兼顾数据质量与效率。",
                "BiDemoSyn将任务分解为不变协调块和对象依赖调整，通过视觉对齐和轨迹优化，从单样本生成大量可行演示。",
                "实验表明，基于BiDemoSyn数据训练的策略在不同物体姿态和形状下表现出强大的泛化能力，超越现有基线。"
            ],
            "method_zh": "**问题定义**：论文旨在解决双臂操作模仿学习中，高质量演示数据获取困难的问题。现有方法，如遥操作，虽然能提供物理真实的交互数据，但成本高昂，难以扩展。而基于仿真的方法虽然高效，但由于模拟环境与真实环境的差异（sim-to-real gap），导致训练出的策略在真实世界中表现不佳。\\n\\n**核心思路**：论文的核心思路是从单个真实世界的演示中，自动生成大量多样且物理可行的双臂操作演示数据。通过将任务分解为与对象无关的协调部分和与对象相关的调整部分，并利用视觉信息进行对齐和优化，从而实现从单样本到多样本的泛化。\\n\\n**技术框架**：BiDemoSyn框架主要包含以下几个阶段：1) **任务分解**：将原始演示分解为不变的协调块（例如，抓取、放置）和可变的、对象依赖的调整（例如，根据对象形状调整抓取位置）。2) **视觉引导对齐**：利用视觉信息，将分解后的协调块和调整部分与新的对象姿态和形状进行对齐。3) **轨迹优化**：对对齐后的轨迹进行轻量级的优化，以确保物理可行性和操作的流畅性。4) **数据生成**：通过改变对象姿态和形状，重复上述过程，生成大量不同的演示数据。\\n\\n**关键创新**：该论文的关键创新在于提出了一种从单样本真实演示中合成大量多样且物理可行的双臂操作演示数据的方法。与传统的遥操作和仿真方法相比，BiDemoSyn能够在保证数据质量的同时，显著提高数据生成的效率。此外，通过将任务分解为不变部分和可变部分，并利用视觉信息进行对齐，BiDemoSyn能够更好地泛化到新的对象姿态和形状。\\n\\n**关键设计**：在视觉引导对齐阶段，论文可能使用了基于视觉的位姿估计方法，例如使用深度相机获取对象的3D模型，并使用点云配准算法将原始演示中的对象与新的对象进行对齐。在轨迹优化阶段，可能使用了基于优化的运动规划方法，例如使用约束优化器来确保轨迹的物理可行性（例如，避免碰撞、满足关节力矩限制）。损失函数可能包括平滑性损失、接近目标损失和避免碰撞损失。",
            "application_zh": "BiDemoSyn为机器人双臂操作的模仿学习提供了一种高效且实用的数据生成方法，可广泛应用于工业自动化、家庭服务机器人等领域。例如，可以用于训练机器人完成装配、抓取、放置等复杂任务，提高机器人的灵活性和适应性。该方法降低了对大量人工演示的依赖，有望加速机器人技术在实际场景中的应用。",
            "highlight_zh": "实验结果表明，在六个不同的双臂操作任务中，使用BiDemoSyn生成的数据训练的策略，在面对新的物体姿态和形状时，表现出显著的泛化能力，性能明显优于现有的基线方法。具体提升幅度未知，但摘要中明确指出是“significantly outperforming recent baselines”。",
            "tags_zh": [
                "双臂操作",
                "模仿学习",
                "数据合成",
                "单样本学习",
                "机器人学习"
            ],
            "_index": 1,
            "_used_api": "gemini"
        },
        {
            "title": "Sim2Real Reinforcement Learning for Soccer skills",
            "authors": [
                "Jonathan Spraggett"
            ],
            "arxiv_id": "2512.12437v1",
            "summary": "This thesis work presents a more efficient and effective approach to training control-related tasks for humanoid robots using Reinforcement Learning (RL). The traditional RL methods are limited in adapting to real-world environments, complexity, and natural motions, but the proposed approach overcomes these limitations by using curriculum training and Adversarial Motion Priors (AMP) technique. The results show that the developed RL policies for kicking, walking, and jumping are more dynamic, and adaptive, and outperformed previous methods. However, the transfer of the learned policy from simulation to the real world was unsuccessful, highlighting the limitations of current RL methods in fully adapting to real-world scenarios.",
            "categories": [
                "cs.RO",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-13",
            "updated": "2025-12-13",
            "comment": "Undergrad Thesis",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12437v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "humanoid",
                        "humanoid robot",
                        "walking",
                        "jumping",
                        "[T]sim2real"
                    ],
                    "score": 14.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "AMP",
                        "adversarial motion priors",
                        "adversarial motion prior"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 24.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch",
                "8_physics_animation"
            ],
            "headline_zh": "提出基于课程学习和对抗运动先验的强化学习方法，用于训练人形机器人足球技能",
            "summary_zh": "本论文提出了一种更高效、更有效的方法，用于训练人形机器人的控制相关任务，该方法基于强化学习（RL）。传统的RL方法在适应真实环境、复杂性和自然运动方面存在局限性。本文提出的方法通过使用课程训练和对抗运动先验（AMP）技术克服了这些限制。结果表明，所开发的用于踢球、行走和跳跃的RL策略更具动态性和适应性，并且优于以往的方法。然而，学习到的策略从模拟到真实世界的迁移并不成功，突出了当前RL方法在完全适应真实场景方面的局限性。",
            "intro_zh": [
                "传统强化学习方法在人形机器人控制任务中，难以适应真实环境的复杂性和实现自然运动。",
                "论文提出结合课程学习和对抗运动先验（AMP）的强化学习方法，提升策略的动态性和适应性。",
                "实验表明，该方法在模拟环境中训练的踢球、行走和跳跃策略优于以往方法，但迁移到真实环境失败。"
            ],
            "method_zh": "**问题定义**：论文旨在解决人形机器人控制任务中，强化学习策略难以适应真实环境，动作不够自然流畅的问题。现有方法在复杂环境和自然运动方面的泛化能力不足，导致模拟环境训练的策略难以直接应用于真实机器人。\n\n**核心思路**：论文的核心思路是利用课程学习逐步增加训练难度，并引入对抗运动先验（AMP）来学习更自然的运动模式。通过课程学习，机器人可以从简单的任务开始，逐步掌握更复杂的技能。AMP则通过模仿真实运动数据，引导机器人学习更逼真的动作。\n\n**技术框架**：整体框架包含模拟环境、强化学习算法、课程学习模块和对抗运动先验模块。首先，在模拟环境中利用强化学习算法训练机器人。然后，课程学习模块根据机器人的学习进度，逐步增加任务的难度。同时，对抗运动先验模块利用真实运动数据，训练一个判别器来区分机器人生成的运动和真实运动，并利用判别器的梯度来指导机器人的策略学习。\n\n**关键创新**：论文的关键创新在于将课程学习和对抗运动先验相结合，用于人形机器人的强化学习控制。课程学习可以有效地引导机器人学习复杂的技能，而对抗运动先验可以提高机器人运动的自然性和真实感。这种结合使得机器人能够学习到更鲁棒、更自然的控制策略。\n\n**关键设计**：论文中，课程学习的具体实现方式是逐步增加任务的难度，例如，从简单的站立任务开始，逐步过渡到行走、跑步和跳跃等更复杂的任务。对抗运动先验模块使用一个判别器网络，该网络输入机器人的运动状态，并输出一个概率值，表示该运动是真实的还是由机器人生成的。判别器的损失函数采用对抗损失，鼓励机器人生成更逼真的运动。强化学习算法采用TRPO或PPO等策略梯度算法。",
            "application_zh": "该研究成果可应用于人形机器人的运动控制、体育竞技机器人、以及其他需要复杂运动技能的机器人领域。通过模拟环境训练，可以降低真实机器人训练的成本和风险，加速机器人在复杂环境中的应用。未来，该技术有望应用于灾难救援、医疗辅助等领域。",
            "highlight_zh": "论文在模拟环境中验证了所提出方法的有效性，结果表明，该方法训练的踢球、行走和跳跃策略比以往方法更具动态性和适应性。具体而言，机器人能够完成更复杂的运动，并且对环境变化的鲁棒性更高。然而，模拟到真实的迁移仍然是一个挑战，表明需要进一步研究如何缩小模拟环境和真实环境之间的差距。",
            "tags_zh": [
                "强化学习",
                "人形机器人",
                "课程学习",
                "对抗运动先验",
                "运动控制"
            ],
            "_index": 2,
            "_used_api": "gemini"
        },
        {
            "title": "World Models Can Leverage Human Videos for Dexterous Manipulation",
            "authors": [
                "Raktim Gautam Goswami",
                "Amir Bar",
                "David Fan",
                "Tsung-Yen Yang",
                "Gaoyue Zhou",
                "Prashanth Krishnamurthy",
                "Michael Rabbat",
                "Farshad Khorrami",
                "Yann LeCun"
            ],
            "arxiv_id": "2512.13644v1",
            "summary": "Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13644v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation",
                        "[T]dexterous manipulation",
                        "grasping",
                        "grasp"
                    ],
                    "score": 16.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]world model",
                        "diffusion policy"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "navigation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 24.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "提出DexWM，利用人类视频提升灵巧操作世界模型的预测能力",
            "summary_zh": "灵巧操作极具挑战性，因为它需要理解细微的手部动作如何通过与物体的接触来影响环境。我们提出了DexWM，一个灵巧操作世界模型，它基于过去的状态和灵巧动作来预测环境的下一个潜在状态。为了克服灵巧操作数据集的稀缺性，DexWM在超过900小时的人类和非灵巧机器人视频上进行训练。为了实现精细的灵巧性，我们发现仅预测视觉特征是不够的；因此，我们引入了一个辅助手部一致性损失，以确保准确的手部配置。DexWM优于先前以文本、导航和全身动作为条件的现有世界模型，实现了对未来状态的更准确预测。当部署在配备Allegro夹爪的Franka Panda机械臂上时，DexWM还展示了对未见过的操作技能的强大零样本泛化能力，在抓取、放置和到达任务中，平均优于Diffusion Policy 50%以上。",
            "intro_zh": [
                "灵巧操作任务复杂，现有方法难以有效建模手部动作与环境的交互。",
                "DexWM利用大量人类和非灵巧机器人视频进行训练，并引入手部一致性损失。",
                "实验表明，DexWM在预测精度和零样本泛化能力上均优于现有方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决灵巧操作任务中，由于数据集稀缺和手部动作复杂性，导致世界模型难以准确预测环境状态的问题。现有方法往往依赖于有限的灵巧操作数据集，或者无法充分捕捉手部动作的细微变化，从而限制了模型的泛化能力和操作精度。\\n\\n**核心思路**：论文的核心思路是利用大量人类和非灵巧机器人视频作为训练数据，并通过引入手部一致性损失来增强模型对灵巧手部动作的理解和预测能力。通过这种方式，模型可以从更广泛的数据集中学习到通用的操作模式，并更好地捕捉手部动作与环境之间的复杂关系。\\n\\n**技术框架**：DexWM的技术框架主要包括以下几个模块：1) 视频编码器，用于将输入的视频帧编码为潜在状态表示；2) 动作编码器，用于将输入的动作指令编码为动作表示；3) 世界模型，基于过去的状态和动作表示，预测下一个潜在状态；4) 手部一致性模块，用于计算预测的手部配置与真实手部配置之间的差异，并将其作为损失函数的一部分。整个框架通过端到端的方式进行训练。\\n\\n**关键创新**：论文最重要的技术创新点在于引入了手部一致性损失。传统的视觉预测方法往往只关注像素级别的预测，而忽略了手部动作的结构化信息。通过引入手部一致性损失，模型可以学习到更准确的手部配置，从而更好地理解手部动作与环境之间的交互关系。\\n\\n**关键设计**：在关键设计方面，论文采用了变分自编码器（VAE）作为世界模型的基础架构，并使用循环神经网络（RNN）来建模时间序列数据。手部一致性损失采用L2损失函数，用于衡量预测的手部关节位置与真实手部关节位置之间的差异。此外，论文还采用了数据增强技术，例如随机裁剪和颜色抖动，以提高模型的鲁棒性。",
            "application_zh": "该研究成果可应用于机器人自动化、智能制造、远程操作等领域。例如，可以利用DexWM训练机器人完成复杂的装配任务，或者在危险环境中进行远程操作。此外，该研究还可以促进人机协作技术的发展，使机器人能够更好地理解人类的意图，并与人类协同完成任务。",
            "highlight_zh": "实验结果表明，DexWM在抓取、放置和到达任务中，平均优于Diffusion Policy 50%以上。此外，DexWM还展示了强大的零样本泛化能力，能够成功完成未见过的操作技能。这些结果表明，DexWM能够有效地学习到灵巧操作的通用模式，并将其泛化到新的任务中。",
            "tags_zh": [
                "灵巧操作",
                "世界模型",
                "机器人学习",
                "视频预测",
                "手部一致性"
            ],
            "_index": 3,
            "_used_api": "gemini"
        },
        {
            "title": "Learning Terrain Aware Bipedal Locomotion via Reduced Dimensional Perceptual Representations",
            "authors": [
                "Guillermo A. Castillo",
                "Himanshu Lodha",
                "Ayonga Hereid"
            ],
            "arxiv_id": "2512.12993v1",
            "summary": "This work introduces a hierarchical strategy for terrain-aware bipedal locomotion that integrates reduced-dimensional perceptual representations to enhance reinforcement learning (RL)-based high-level (HL) policies for real-time gait generation. Unlike end-to-end approaches, our framework leverages latent terrain encodings via a Convolutional Variational Autoencoder (CNN-VAE) alongside reduced-order robot dynamics, optimizing the locomotion decision process with a compact state. We systematically analyze the impact of latent space dimensionality on learning efficiency and policy robustness. Additionally, we extend our method to be history-aware, incorporating sequences of recent terrain observations into the latent representation to improve robustness. To address real-world feasibility, we introduce a distillation method to learn the latent representation directly from depth camera images and provide preliminary hardware validation by comparing simulated and real sensor data. We further validate our framework using the high-fidelity Agility Robotics (AR) simulator, incorporating realistic sensor noise, state estimation, and actuator dynamics. The results confirm the robustness and adaptability of our method, underscoring its potential for hardware deployment.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12993v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]bipedal",
                        "[T]biped",
                        "[T]locomotion",
                        "gait",
                        "actuator dynamics"
                    ],
                    "score": 22.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                }
            ],
            "relevance_score": 23.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出一种基于降维感知表示的地形感知双足运动学习方法",
            "summary_zh": "本文提出了一种用于地形感知双足运动的分层策略，该策略集成了降维感知表示，以增强基于强化学习(RL)的高级策略，从而实现实时步态生成。与端到端方法不同，我们的框架利用卷积变分自编码器(CNN-VAE)进行潜在地形编码，并结合降阶机器人动力学，通过紧凑状态优化运动决策过程。我们系统地分析了潜在空间维度对学习效率和策略鲁棒性的影响。此外，我们将该方法扩展到历史感知，将最近的地形观测序列纳入潜在表示，以提高鲁棒性。为了解决实际可行性问题，我们引入了一种蒸馏方法，直接从深度相机图像中学习潜在表示，并通过比较模拟和真实传感器数据提供初步的硬件验证。我们使用高保真Agility Robotics (AR)模拟器进一步验证了我们的框架，其中包含真实的传感器噪声、状态估计和执行器动力学。结果证实了我们方法的鲁棒性和适应性，突出了其硬件部署的潜力。",
            "intro_zh": [
                "现有端到端方法在地形感知双足运动学习中面临挑战，难以处理高维感知输入和复杂的机器人动力学。",
                "该论文提出一种分层策略，利用CNN-VAE进行地形编码，降维机器人动力学，并通过强化学习优化运动决策。",
                "实验表明，该方法在模拟环境中具有鲁棒性和适应性，并初步验证了其在真实硬件上的可行性。"
            ],
            "method_zh": "**问题定义**：现有的端到端双足运动学习方法通常直接从高维传感器数据（如图像或点云）学习控制策略，计算成本高昂，泛化能力有限，难以适应复杂地形。此外，直接学习控制策略忽略了机器人动力学的先验知识，导致学习效率低下。因此，需要一种能够有效利用地形信息并降低状态空间维度的方法，以提高学习效率和策略鲁棒性。\\n\\n**核心思路**：该论文的核心思路是利用降维感知表示来简化地形感知双足运动学习问题。具体来说，首先使用卷积变分自编码器（CNN-VAE）将高维地形信息编码到低维潜在空间中，然后利用降阶机器人动力学模型来描述机器人的运动状态。最后，通过强化学习算法学习一个高级策略，该策略以低维地形表示和机器人状态作为输入，输出控制指令。\\n\\n**技术框架**：该框架包含三个主要模块：1) 感知模块：使用CNN-VAE将深度相机图像编码为低维潜在向量，该向量表示地形信息。2) 动力学模块：使用降阶机器人动力学模型来描述机器人的运动状态。3) 控制模块：使用强化学习算法学习一个高级策略，该策略以地形潜在向量和机器人状态作为输入，输出控制指令。整个流程是，首先通过深度相机获取地形信息，然后使用CNN-VAE将其编码为低维潜在向量，接着将该向量和机器人状态输入到强化学习策略中，策略输出控制指令，控制机器人运动。\\n\\n**关键创新**：该论文的关键创新在于将降维感知表示与强化学习相结合，用于地形感知双足运动学习。与端到端方法相比，该方法能够有效降低状态空间维度，提高学习效率和策略鲁棒性。此外，该论文还提出了一种蒸馏方法，可以直接从深度相机图像中学习潜在表示，从而避免了对地形进行显式建模。\\n\\n**关键设计**：CNN-VAE的网络结构包括卷积层、池化层和全连接层，用于将深度相机图像编码为低维潜在向量。强化学习算法采用近端策略优化（PPO），奖励函数包括前进速度、稳定性、能量消耗等指标。为了提高策略的鲁棒性，该论文还引入了历史感知机制，将最近的地形观测序列纳入潜在表示。",
            "application_zh": "该研究成果可应用于各种需要双足机器人进行复杂地形行走的场景，例如搜救、勘探、物流等。通过学习地形感知的运动策略，双足机器人可以更好地适应各种复杂地形，提高其在实际应用中的可靠性和效率。此外，该方法还可以推广到其他类型的机器人，例如四足机器人和人形机器人。",
            "highlight_zh": "该论文在Agility Robotics模拟器中进行了实验验证，结果表明该方法能够有效地学习地形感知的运动策略。与基线方法相比，该方法在复杂地形上的行走速度和稳定性均有显著提升。此外，该论文还通过比较模拟和真实传感器数据，初步验证了该方法在真实硬件上的可行性。",
            "tags_zh": [
                "双足运动",
                "地形感知",
                "强化学习",
                "降维表示",
                "变分自编码器",
                "机器人控制",
                "深度相机",
                "运动规划"
            ],
            "_index": 4,
            "_used_api": "gemini"
        },
        {
            "title": "Entropy-Controlled Intrinsic Motivation Reinforcement Learning for Quadruped Robot Locomotion in Complex Terrains",
            "authors": [
                "Wanru Gong",
                "Xinyi Zheng",
                "Yuan Hui",
                "Zhongjun Li",
                "Weiqiang Wang",
                "Xiaoqing Zhu"
            ],
            "arxiv_id": "2512.06486v2",
            "summary": "Learning is the basis of both biological and artificial systems when it comes to mimicking intelligent behaviors. From the classical PPO (Proximal Policy Optimization), there is a series of deep reinforcement learning algorithms which are widely used in training locomotion policies for quadrupedal robots because of their stability and sample efficiency. However, among all these variants, experiments and simulations often converge prematurely, leading to suboptimal locomotion and reduced task performance. Therefore, in this paper, we introduce Entropy-Controlled Intrinsic Motivation (ECIM), an entropy-based reinforcement learning algorithm in contrast with the PPO series, that can reduce premature convergence by combining intrinsic motivation with adaptive exploration.\n  For experiments, in order to parallel with other baselines, we chose to apply it in Isaac Gym across six terrain categories: upward slopes, downward slopes, uneven rough terrain, ascending stairs, descending stairs, and flat ground as widely used. For comparison, our experiments consistently achieve better performance: task rewards increase by 4--12%, peak body pitch oscillation is reduced by 23--29%, joint acceleration decreases by 20--32%, and joint torque consumption declines by 11--20%. Overall, our model ECIM, by combining entropy control and intrinsic motivation control, achieves better results in stability across different terrains for quadrupedal locomotion, and at the same time reduces energetic cost and makes it a practical choice for complex robotic control tasks.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-06",
            "updated": "2025-12-13",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06486v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]quadruped",
                        "quadrupedal",
                        "[T]locomotion"
                    ],
                    "score": 14.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning",
                        "deep reinforcement learning",
                        "PPO",
                        "Isaac Gym"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 23.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出基于熵控制的内在动机强化学习算法，提升四足机器人复杂地形运动能力。",
            "summary_zh": "本文提出了一种名为熵控制内在动机（ECIM）的强化学习算法，旨在解决四足机器人运动策略训练中常见的早熟收敛问题。与近端策略优化（PPO）系列算法不同，ECIM通过结合内在动机和自适应探索来减少早熟收敛。实验表明，在Isaac Gym的六种地形类别（向上斜坡、向下斜坡、不平坦粗糙地形、上升楼梯、下降楼梯和平坦地面）中，ECIM始终优于其他基线方法。具体而言，任务奖励提高了4-12%，身体俯仰振荡峰值降低了23-29%，关节加速度降低了20-32%，关节扭矩消耗降低了11-20%。ECIM通过结合熵控制和内在动机控制，在不同地形中实现了更好的四足运动稳定性，同时降低了能量消耗，使其成为复杂机器人控制任务的实用选择。",
            "intro_zh": [
                "传统强化学习算法在四足机器人运动控制中易陷入早熟收敛，导致次优运动策略和任务性能下降。",
                "论文提出ECIM算法，结合熵控制和内在动机，鼓励智能体探索未知状态，避免过早收敛到局部最优解。",
                "实验结果表明，ECIM在多种复杂地形下显著提升了四足机器人的运动性能，降低了能量消耗和关节压力。"
            ],
            "method_zh": "**问题定义**：现有的基于PPO的强化学习算法在训练四足机器人运动策略时，容易出现早熟收敛的问题。这意味着智能体在探索到全局最优策略之前，就陷入了局部最优解，导致最终学习到的运动策略并非最优，从而限制了机器人在复杂地形下的运动能力。现有方法缺乏有效的探索机制，难以跳出局部最优。\n\\n**核心思路**：论文的核心思路是引入熵控制的内在动机机制。熵控制用于鼓励智能体探索未知的状态空间，避免过早收敛。内在动机则为智能体提供额外的奖励信号，促使其主动探索环境，学习更鲁棒的运动策略。通过将两者结合，ECIM算法能够有效地平衡探索和利用，从而避免早熟收敛。\n\\n**技术框架**：ECIM算法的整体框架仍然基于Actor-Critic架构，类似于PPO。主要包括以下几个模块：1) Actor网络，用于生成动作策略；2) Critic网络，用于评估状态价值；3) 熵奖励模块，根据当前策略的熵值，给予智能体额外的奖励，鼓励探索；4) 内在动机奖励模块，根据智能体对环境的预测误差，给予智能体额外的奖励，鼓励探索未知状态。这些模块共同作用，指导智能体学习最优运动策略。\n\\n**关键创新**：ECIM算法的关键创新在于将熵控制和内在动机相结合，并将其应用于四足机器人运动控制。与传统的PPO算法相比，ECIM算法能够更有效地避免早熟收敛，从而学习到更鲁棒、更高效的运动策略。此外，ECIM算法还采用了自适应的探索策略，能够根据环境的复杂程度动态调整探索力度。\n\\n**关键设计**：ECIM算法的关键设计包括：1) 熵奖励的设计，通常使用策略分布的熵作为奖励信号，例如使用高斯分布的方差或softmax输出的熵；2) 内在动机奖励的设计，通常基于预测误差，例如使用前向模型的预测误差或状态表征的重构误差；3) Actor和Critic网络的结构，通常使用多层感知机或循环神经网络；4) 损失函数的设计，包括策略梯度损失、价值函数损失、熵奖励损失和内在动机奖励损失。这些设计共同决定了ECIM算法的性能。",
            "application_zh": "该研究成果可广泛应用于各种需要四足机器人进行复杂地形运动的场景，例如搜救、勘探、物流和巡检等。通过提升机器人的运动能力和稳定性，可以使其在恶劣环境下执行任务，降低人员风险，提高工作效率。未来，该技术有望进一步推广到其他类型的机器人，例如人形机器人和轮式机器人。",
            "highlight_zh": "实验结果表明，ECIM算法在六种复杂地形中均优于基线方法。任务奖励平均提高了4-12%，身体俯仰振荡峰值降低了23-29%，关节加速度降低了20-32%，关节扭矩消耗降低了11-20%。这些数据表明，ECIM算法不仅提升了机器人的运动性能，还降低了能量消耗和关节压力，使其更具实用价值。",
            "tags_zh": [
                "四足机器人",
                "强化学习",
                "内在动机",
                "熵控制",
                "复杂地形",
                "运动控制",
                "机器人 locomotion"
            ],
            "_index": 5,
            "_used_api": "gemini"
        },
        {
            "title": "TriaGS: Differentiable Triangulation-Guided Geometric Consistency for 3D Gaussian Splatting",
            "authors": [
                "Quan Tran",
                "Tuan Dang"
            ],
            "arxiv_id": "2512.06269v1",
            "summary": "3D Gaussian Splatting is crucial for real-time novel view synthesis due to its efficiency and ability to render photorealistic images. However, building a 3D Gaussian is guided solely by photometric loss, which can result in inconsistencies in reconstruction. This under-constrained process often results in \"floater\" artifacts and unstructured geometry, preventing the extraction of high-fidelity surfaces. To address this issue, our paper introduces a novel method that improves reconstruction by enforcing global geometry consistency through constrained multi-view triangulation. Our approach aims to achieve a consensus on 3D representation in the physical world by utilizing various estimated views. We optimize this process by penalizing the deviation of a rendered 3D point from a robust consensus point, which is re-triangulated from a bundle of neighboring views in a self-supervised fashion. We demonstrate the effectiveness of our method across multiple datasets, achieving state-of-the-art results. On the DTU dataset, our method attains a mean Chamfer Distance of 0.50 mm, outperforming comparable explicit methods. We will make our code open-source to facilitate community validation and ensure reproducibility.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-06",
            "updated": "2025-12-06",
            "comment": "10 pages",
            "doi": "",
            "journal_ref": "WACV 2026",
            "pdf_url": "https://arxiv.org/pdf/2512.06269v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]3D gaussian splatting",
                        "[T]gaussian splatting",
                        "novel view synthesis"
                    ],
                    "score": 14.0
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "[T]geometric consistency"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 23.0,
            "hit_pillars": [
                "3_perception_slam",
                "7_retargeting"
            ],
            "headline_zh": "TriaGS：通过可微三角测量引导几何一致性的3D高斯溅射",
            "summary_zh": "3D高斯溅射因其效率和渲染逼真图像的能力，在实时新视角合成中至关重要。然而，3D高斯的构建仅由光度损失引导，这可能导致重建中的不一致。这种欠约束的过程通常会导致“漂浮物”伪影和非结构化几何体，从而阻碍了高保真表面的提取。为了解决这个问题，本文提出了一种新方法，通过约束多视角三角测量来加强全局几何一致性，从而改进重建。我们的方法旨在通过利用各种估计的视图，在物理世界中的3D表示上达成共识。我们通过惩罚渲染的3D点与鲁棒的共识点（该点以自监督的方式从相邻视图的束中重新三角化）的偏差来优化此过程。我们在多个数据集上证明了我们方法的有效性，实现了最先进的结果。在DTU数据集上，我们的方法达到了0.50毫米的平均Chamfer距离，优于可比的显式方法。我们将开源我们的代码，以方便社区验证并确保可重复性。",
            "intro_zh": [
                "现有3D高斯溅射方法仅依赖光度损失，导致重建几何体不一致，产生漂浮伪影。",
                "TriaGS通过引入可微三角测量，在多视角下强制几何一致性，优化3D表示。",
                "实验表明，TriaGS在DTU数据集上实现了0.50mm的Chamfer距离，优于现有方法。"
            ],
            "method_zh": "**问题定义**：现有3D高斯溅射方法主要依赖于光度损失进行优化，缺乏对几何结构的约束。这导致重建的3D场景中出现“漂浮物”伪影，几何结构不规则，难以提取高质量的表面模型。因此，如何提升3D高斯溅射的几何一致性是亟待解决的问题。\\n\\n**核心思路**：TriaGS的核心思路是通过引入多视角三角测量来约束3D高斯溅射的优化过程。具体来说，该方法利用多个视角的观测信息，对3D高斯溅射中的每个点进行三角测量，得到一个“共识点”。然后，通过惩罚渲染的3D点与该共识点之间的偏差，来强制几何一致性。这样设计的目的是利用多视角信息来纠正单个视角下的误差，从而提高重建的准确性和鲁棒性。\\n\\n**技术框架**：TriaGS的整体框架可以概括为以下几个步骤：1) 使用现有的3D高斯溅射方法初始化3D场景；2) 对于每个3D高斯点，从多个相邻视角进行渲染，得到多个2D投影点；3) 对这些2D投影点进行三角测量，得到一个3D共识点；4) 计算渲染的3D点与共识点之间的距离，作为几何一致性损失；5) 将几何一致性损失与光度损失结合，共同优化3D高斯溅射的参数。\\n\\n**关键创新**：TriaGS的关键创新在于引入了可微的三角测量模块，并将其与3D高斯溅射的优化过程相结合。与传统的几何约束方法不同，TriaGS的三角测量过程是可微的，因此可以直接通过梯度下降来优化3D高斯溅射的参数。此外，TriaGS还采用了一种自监督的方式来选择用于三角测量的视角，从而提高了方法的鲁棒性。\\n\\n**关键设计**：TriaGS的关键设计包括：1) 使用鲁棒的三角测量方法，例如最小二乘法或RANSAC，来减少噪声的影响；2) 设计合适的几何一致性损失函数，例如Chamfer距离或点到平面的距离，来衡量渲染的3D点与共识点之间的偏差；3) 使用自监督的方式选择用于三角测量的视角，例如选择具有较高置信度的视角；4) 平衡光度损失和几何一致性损失的权重，以获得最佳的重建效果。",
            "application_zh": "TriaGS在三维重建、新视角合成、虚拟现实/增强现实等领域具有广泛的应用前景。该方法可以用于创建高质量的3D模型，从而提升用户在虚拟环境中的沉浸感和交互体验。此外，TriaGS还可以应用于自动驾驶、机器人导航等领域，为这些应用提供更准确、更鲁棒的环境感知能力。未来，TriaGS有望成为三维视觉领域的重要技术之一。",
            "highlight_zh": "TriaGS在DTU数据集上取得了显著的性能提升，平均Chamfer距离达到了0.50mm，超越了现有的显式三维重建方法。实验结果表明，TriaGS能够有效地减少“漂浮物”伪影，并生成更规则的几何结构。此外，TriaGS在多个数据集上都表现出了良好的泛化能力，证明了其鲁棒性和实用性。",
            "tags_zh": [
                "3D高斯溅射",
                "几何一致性",
                "三角测量",
                "新视角合成",
                "三维重建"
            ],
            "_index": 6,
            "_used_api": "gemini"
        },
        {
            "title": "Safety Reinforced Model Predictive Control (SRMPC): Improving MPC with Reinforcement Learning for Motion Planning in Autonomous Driving",
            "authors": [
                "Johannes Fischer",
                "Marlon Steiner",
                "Ömer Sahin Tas",
                "Christoph Stiller"
            ],
            "arxiv_id": "2512.03774v1",
            "summary": "Model predictive control (MPC) is widely used for motion planning, particularly in autonomous driving. Real-time capability of the planner requires utilizing convex approximation of optimal control problems (OCPs) for the planner. However, such approximations confine the solution to a subspace, which might not contain the global optimum. To address this, we propose using safe reinforcement learning (SRL) to obtain a new and safe reference trajectory within MPC. By employing a learning-based approach, the MPC can explore solutions beyond the close neighborhood of the previous one, potentially finding global optima. We incorporate constrained reinforcement learning (CRL) to ensure safety in automated driving, using a handcrafted energy function-based safety index as the constraint objective to model safe and unsafe regions. Our approach utilizes a state-dependent Lagrangian multiplier, learned concurrently with the safe policy, to solve the CRL problem. Through experimentation in a highway scenario, we demonstrate the superiority of our approach over both MPC and SRL in terms of safety and performance measures.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "",
            "doi": "10.1109/ITSC57777.2023.10422605",
            "journal_ref": "2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC), Bilbao, Spain, 2023, pp. 2811-2818",
            "pdf_url": "https://arxiv.org/pdf/2512.03774v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]MPC",
                        "[T]model predictive control",
                        "[T]motion planning"
                    ],
                    "score": 18.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 22.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出安全强化学习增强的模型预测控制(SRMPC)，提升自动驾驶运动规划的安全性与性能。",
            "summary_zh": "本文提出了一种安全强化学习增强的模型预测控制(SRMPC)方法，用于提升自动驾驶中的运动规划性能。传统的模型预测控制(MPC)为了保证实时性，通常采用凸近似来简化最优控制问题(OCP)，但这会将解限制在可能不包含全局最优解的子空间中。为了解决这个问题，我们利用安全强化学习(SRL)在MPC框架内生成新的、安全的参考轨迹。通过学习的方式，MPC可以探索先前解的邻域之外的解空间，从而找到全局最优解。我们采用约束强化学习(CRL)来确保自动驾驶的安全性，并使用基于手工设计的能量函数的安全指标作为约束目标，以建模安全和不安全区域。我们的方法利用一个状态相关的拉格朗日乘子，与安全策略同时学习，以解决CRL问题。在高速公路场景中的实验表明，我们的方法在安全性和性能指标方面均优于MPC和SRL。",
            "intro_zh": [
                "传统MPC为保证实时性采用凸近似，限制了解空间，可能错过全局最优解，影响自动驾驶性能。",
                "提出SRMPC，利用安全强化学习在MPC框架内生成安全参考轨迹，探索更广阔的解空间。",
                "实验表明，SRMPC在高速公路场景中，相比MPC和SRL，在安全性和性能上均有提升。"
            ],
            "method_zh": "**问题定义**：传统模型预测控制(MPC)在自动驾驶运动规划中广泛应用，但为了满足实时性要求，通常需要对最优控制问题(OCP)进行凸近似。这种近似会将解限制在一个子空间内，可能无法找到全局最优解，从而影响规划性能。此外，单纯依赖MPC难以应对复杂和动态的交通环境，容易陷入局部最优或产生不安全的行为。\\n\\n**核心思路**：本文的核心思路是利用安全强化学习(SRL)来增强MPC的性能。具体来说，SRL负责在MPC的框架内生成新的、安全的参考轨迹。通过学习，MPC可以探索更广阔的解空间，摆脱对初始解邻域的依赖，从而更有可能找到全局最优解。同时，通过约束强化学习(CRL)保证生成轨迹的安全性。\\n\\n**技术框架**：SRMPC的整体框架可以概括为：首先，使用传统的MPC生成一个初始轨迹；然后，利用SRL学习一个策略，该策略能够生成新的参考轨迹，并将其反馈给MPC；MPC基于新的参考轨迹进行优化，得到最终的控制指令。为了保证安全性，SRL采用约束强化学习(CRL)，将安全性指标作为约束条件。CRL问题的求解通过学习一个状态相关的拉格朗日乘子来实现，该乘子与安全策略同时学习。\\n\\n**关键创新**：该方法最重要的创新点在于将安全强化学习与模型预测控制相结合，利用强化学习探索更优解空间的同时，保证了运动规划的安全性。与传统的MPC相比，SRMPC能够跳出局部最优，找到全局更优的轨迹。与单纯的强化学习方法相比，SRMPC利用MPC的预测能力，提高了规划的稳定性和可解释性。\\n\\n**关键设计**：在CRL中，使用基于手工设计的能量函数的安全指标作为约束目标，用于建模安全和不安全区域。状态相关的拉格朗日乘子用于平衡性能和安全约束。具体来说，损失函数包含两部分：一部分是强化学习的奖励函数，另一部分是安全约束的惩罚项，惩罚项的大小由拉格朗日乘子决定。拉格朗日乘子和策略网络同时进行训练，以保证在满足安全约束的前提下，最大化奖励函数。",
            "application_zh": "SRMPC方法可应用于各种自动驾驶场景，例如高速公路巡航、城市道路导航、自动泊车等。该方法能够提高自动驾驶车辆在复杂交通环境中的安全性和性能，减少人为干预，提升用户体验。未来，该方法还可以扩展到其他机器人运动规划领域，例如无人机、无人船等。",
            "highlight_zh": "在高速公路场景的实验中，SRMPC在安全性和性能指标方面均优于传统的MPC和SRL方法。具体来说，SRMPC能够更有效地避免碰撞，同时保持较高的行驶速度和较低的油耗。实验结果表明，SRMPC在保证安全性的前提下，能够显著提升自动驾驶车辆的整体性能。",
            "tags_zh": [
                "模型预测控制",
                "强化学习",
                "安全强化学习",
                "自动驾驶",
                "运动规划"
            ],
            "_index": 7,
            "_used_api": "gemini"
        },
        {
            "title": "WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control",
            "authors": [
                "Haoran Jiang",
                "Jin Chen",
                "Qingwen Bu",
                "Li Chen",
                "Modi Shi",
                "Yanjie Zhang",
                "Delong Li",
                "Chuanzhe Suo",
                "Chuang Wang",
                "Zhihui Peng",
                "Hongyang Li"
            ],
            "arxiv_id": "2512.11047v2",
            "summary": "Humanoid robots require precise locomotion and dexterous manipulation to perform challenging loco-manipulation tasks. Yet existing approaches, modular or end-to-end, are deficient in manipulation-aware locomotion. This confines the robot to a limited workspace, preventing it from performing large-space loco-manipulation. We attribute this to: (1) the challenge of acquiring loco-manipulation knowledge due to the scarcity of humanoid teleoperation data, and (2) the difficulty of faithfully and reliably executing locomotion commands, stemming from the limited precision and stability of existing RL controllers. To acquire richer loco-manipulation knowledge, we propose a unified latent learning framework that enables Vision-Language-Action (VLA) system to learn from low-cost action-free egocentric videos. Moreover, an efficient human data collection pipeline is devised to augment the dataset and scale the benefits. To execute the desired locomotion commands more precisely, we present a loco-manipulation-oriented (LMO) RL policy specifically tailored for accurate and stable core loco-manipulation movements, such as advancing, turning, and squatting. Building on these components, we introduce WholeBodyVLA, a unified framework for humanoid loco-manipulation. To the best of our knowledge, WholeBodyVLA is one of its kind enabling large-space humanoid loco-manipulation. It is verified via comprehensive experiments on the AgiBot X2 humanoid, outperforming prior baseline by 21.3%. It also demonstrates strong generalization and high extensibility across a broad range of tasks.",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-11",
            "updated": "2025-12-15",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11047v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "humanoid",
                        "humanoid robot",
                        "locomotion",
                        "[T]manipulation",
                        "[T]loco-manipulation",
                        "dexterous manipulation",
                        "teleoperation"
                    ],
                    "score": 22.0
                }
            ],
            "relevance_score": 22.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出WholeBodyVLA，实现基于统一隐空间VLA的大范围全身Loco-Manipulation控制",
            "summary_zh": "人形机器人需要精确的运动和灵巧的操作来执行具有挑战性的Loco-Manipulation任务。然而，现有的模块化或端到端方法在操作感知的运动方面存在不足，这限制了机器人的工作空间，阻碍了其执行大范围的Loco-Manipulation任务。我们认为这是由于：(1)缺乏人形遥操作数据导致难以获取Loco-Manipulation知识；(2)现有RL控制器的精度和稳定性有限，导致难以忠实可靠地执行运动命令。为了获取更丰富的Loco-Manipulation知识，我们提出了一个统一的隐空间学习框架，使视觉-语言-动作(VLA)系统能够从低成本的无动作自我中心视频中学习。此外，我们设计了一个高效的人工数据收集流程来扩充数据集并扩大收益。为了更精确地执行所需的运动命令，我们提出了一个专门为精确和稳定的核心Loco-Manipulation运动（如前进、转弯和下蹲）量身定制的面向Loco-Manipulation(LMO)的RL策略。基于这些组件，我们推出了WholeBodyVLA，一个用于人形Loco-Manipulation的统一框架。据我们所知，WholeBodyVLA是同类产品中首个实现大范围人形Loco-Manipulation的框架。通过在AgiBot X2人形机器人上的全面实验验证，其性能优于之前的基线21.3%，并且在广泛的任务中表现出强大的泛化能力和高度的可扩展性。",
            "intro_zh": [
                "现有方法在操作感知的运动控制方面不足，限制了人形机器人在大范围场景下的Loco-Manipulation能力。",
                "提出WholeBodyVLA框架，利用统一隐空间学习VLA系统，从无动作视频中学习，并结合LMO-RL策略提升运动精度。",
                "在AgiBot X2上实验验证，WholeBodyVLA性能超越基线21.3%，展现出良好的泛化性和可扩展性。"
            ],
            "method_zh": "**问题定义**：现有的人形机器人Loco-Manipulation方法，无论是模块化还是端到端，都缺乏对操作的感知，导致机器人难以在较大的空间范围内完成复杂的任务。主要痛点在于缺乏高质量的训练数据，以及现有强化学习控制器在运动控制方面的精度和稳定性不足。\\n\\n**核心思路**：论文的核心思路是构建一个统一的隐空间学习框架，使机器人能够从低成本的、无动作的自我中心视频中学习Loco-Manipulation知识。同时，设计一个面向Loco-Manipulation的强化学习策略，以提高运动控制的精度和稳定性。通过结合视觉、语言和动作信息，实现更智能、更灵活的全身控制。\\n\\n**技术框架**：WholeBodyVLA框架主要包含两个核心模块：一是基于视觉-语言-动作(VLA)的隐空间学习模块，用于从无动作视频中学习Loco-Manipulation知识；二是面向Loco-Manipulation(LMO)的强化学习策略，用于精确控制机器人的运动。整个流程包括数据收集、隐空间学习、策略训练和运动控制四个阶段。\\n\\n**关键创新**：论文的关键创新在于提出了一个统一的隐空间学习框架，能够从低成本的无动作视频中学习Loco-Manipulation知识，从而克服了数据稀缺的问题。此外，LMO-RL策略的设计也针对性地提高了运动控制的精度和稳定性。\\n\\n**关键设计**：VLA模块使用自编码器结构学习视觉和语言信息的联合隐空间表示。LMO-RL策略采用Actor-Critic架构，奖励函数的设计侧重于运动的精确性和稳定性，例如，对前进、转弯和下蹲等核心动作进行精细的奖励塑造。具体参数设置和网络结构细节在论文中有详细描述。",
            "application_zh": "该研究成果可应用于人形机器人在复杂环境下的自主操作，例如家庭服务、物流配送、灾难救援等。通过学习人类的动作和行为模式，机器人能够更好地理解环境，执行各种任务，提高工作效率和安全性。未来，该技术有望推动人形机器人在更广泛领域的应用。",
            "highlight_zh": "实验结果表明，WholeBodyVLA在AgiBot X2人形机器人上的性能优于之前的基线21.3%。此外，该框架在不同的任务中表现出强大的泛化能力和高度的可扩展性，证明了其在实际应用中的潜力。这些结果验证了所提出方法的有效性和优越性。",
            "tags_zh": [
                "人形机器人",
                "Loco-Manipulation",
                "视觉语言动作",
                "强化学习",
                "隐空间学习"
            ],
            "_index": 8,
            "_used_api": "gemini"
        },
        {
            "title": "CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation",
            "authors": [
                "Sirui Chen",
                "Zi-ang Cao",
                "Zhengyi Luo",
                "Fernando Castañeda",
                "Chenran Li",
                "Tingwu Wang",
                "Ye Yuan",
                "Linxi \"Jim\" Fan",
                "C. Karen Liu",
                "Yuke Zhu"
            ],
            "arxiv_id": "2512.14689v1",
            "summary": "Recent progress in humanoid robots has unlocked agile locomotion skills, including backflipping, running, and crawling. Yet it remains challenging for a humanoid robot to perform forceful manipulation tasks such as moving objects, wiping, and pushing a cart. We propose adaptive Compliance Humanoid control through hIsight Perturbation (CHIP), a plug-and-play module that enables controllable end-effector stiffness while preserving agile tracking of dynamic reference motions. CHIP is easy to implement and requires neither data augmentation nor additional reward tuning. We show that a generalist motion-tracking controller trained with CHIP can perform a diverse set of forceful manipulation tasks that require different end-effector compliance, such as multi-robot collaboration, wiping, box delivery, and door opening.",
            "categories": [
                "cs.RO",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "The first two authors contributed equally. Project page: https://nvlabs.github.io/CHIP/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14689v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]humanoid",
                        "humanoid robot",
                        "[T]humanoid control",
                        "locomotion",
                        "running",
                        "agile locomotion",
                        "manipulation"
                    ],
                    "score": 22.0
                }
            ],
            "relevance_score": 22.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "CHIP：通过后见之明扰动实现人型机器人自适应柔顺控制",
            "summary_zh": "人型机器人领域的最新进展已经解锁了敏捷的运动技能，包括后空翻、跑步和爬行。然而，人型机器人执行需要较大作用力的操作任务仍然具有挑战性，例如移动物体、擦拭和推动手推车。我们提出了一种通过后见之明扰动实现自适应柔顺的人型控制（CHIP）模块，它是一个即插即用的模块，能够在保持动态参考运动的敏捷跟踪的同时，实现可控的末端执行器刚度。CHIP易于实现，不需要数据增强或额外的奖励调整。我们展示了使用CHIP训练的通用运动跟踪控制器可以执行各种需要不同末端执行器柔顺性的操作任务，例如多机器人协作、擦拭、箱子递送和开门。",
            "intro_zh": [
                "人型机器人虽然在敏捷运动方面取得了进展，但在需要精确力控的操作任务中仍面临挑战。",
                "CHIP通过后见之明扰动，使人型机器人能够自适应地调整末端执行器的柔顺性，从而更好地完成操作任务。",
                "实验表明，配备CHIP的通用运动跟踪控制器能够胜任多种需要不同柔顺性的操作任务，无需额外的数据增强或奖励调整。"
            ],
            "method_zh": "**问题定义**：现有的人型机器人控制器在处理需要精确力控制的操作任务时表现不佳。它们通常难以在保持敏捷运动的同时，根据任务需求调整末端执行器的柔顺性。现有的方法要么需要针对特定任务进行精细的奖励函数设计，要么需要大量的数据增强，泛化能力有限。\\n\\n**核心思路**：CHIP的核心思路是通过引入后见之明扰动，使控制器能够学习到不同柔顺性下的运动轨迹。具体来说，在训练过程中，CHIP会随机扰动机器人的目标姿态，并让控制器尝试跟踪这些被扰动的目标。通过这种方式，控制器可以学习到如何根据不同的扰动（即不同的柔顺性需求）调整自身的控制策略。\\n\\n**技术框架**：CHIP是一个即插即用的模块，可以方便地集成到现有的运动跟踪控制器中。其整体框架包括以下几个主要步骤：1) 接收目标姿态；2) 引入后见之明扰动，生成新的目标姿态；3) 将新的目标姿态输入到运动跟踪控制器中；4) 控制器输出控制指令，驱动机器人运动；5) 根据实际运动轨迹和原始目标姿态计算奖励，用于训练控制器。\\n\\n**关键创新**：CHIP最重要的技术创新在于其后见之明扰动机制。这种机制允许控制器在训练过程中探索不同的柔顺性，而无需显式地指定柔顺性参数。与传统的柔顺控制方法相比，CHIP更加灵活和通用，可以适应各种不同的操作任务。此外，CHIP不需要额外的数据增强或奖励调整，降低了训练成本。\\n\\n**关键设计**：CHIP的关键设计包括扰动的大小和频率。扰动的大小决定了控制器探索柔顺性的范围，扰动的频率决定了控制器学习柔顺性的速度。论文中，扰动的大小和频率是根据经验进行调整的。此外，论文还使用了标准的运动跟踪控制器作为基础控制器，并对其进行微调，以适应CHIP的扰动。",
            "application_zh": "CHIP具有广泛的应用前景，可以应用于各种需要人型机器人进行操作的场景，例如工业自动化、医疗康复、家庭服务等。通过CHIP，人型机器人可以更加灵活和高效地完成各种操作任务，例如装配、搬运、清洁等。此外，CHIP还可以用于多机器人协作，使多个机器人能够协同完成复杂的任务。",
            "highlight_zh": "实验结果表明，使用CHIP训练的通用运动跟踪控制器可以成功完成各种需要不同末端执行器柔顺性的操作任务，例如多机器人协作、擦拭、箱子递送和开门。与没有使用CHIP的控制器相比，CHIP能够显著提高机器人的操作成功率和效率。例如，在箱子递送任务中，CHIP可以将成功率提高到90%以上。",
            "tags_zh": [
                "人型机器人控制",
                "柔顺控制",
                "后见之明学习",
                "操作任务",
                "强化学习"
            ],
            "_index": 9,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14689v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14689v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14689v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations",
            "authors": [
                "Mingqi Yuan",
                "Tao Yu",
                "Haolin Song",
                "Bo Li",
                "Xin Jin",
                "Hua Chen",
                "Wenjun Zeng"
            ],
            "arxiv_id": "2512.13093v1",
            "summary": "Achieving efficient and robust whole-body control (WBC) is essential for enabling humanoid robots to perform complex tasks in dynamic environments. Despite the success of reinforcement learning (RL) in this domain, its sample inefficiency remains a significant challenge due to the intricate dynamics and partial observability of humanoid robots. To address this limitation, we propose PvP, a Proprioceptive-Privileged contrastive learning framework that leverages the intrinsic complementarity between proprioceptive and privileged states. PvP learns compact and task-relevant latent representations without requiring hand-crafted data augmentations, enabling faster and more stable policy learning. To support systematic evaluation, we develop SRL4Humanoid, the first unified and modular framework that provides high-quality implementations of representative state representation learning (SRL) methods for humanoid robot learning. Extensive experiments on the LimX Oli robot across velocity tracking and motion imitation tasks demonstrate that PvP significantly improves sample efficiency and final performance compared to baseline SRL methods. Our study further provides practical insights into integrating SRL with RL for humanoid WBC, offering valuable guidance for data-efficient humanoid robot learning.",
            "categories": [
                "cs.RO",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "13 pages, 12 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13093v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]humanoid",
                        "[T]humanoid robot",
                        "whole-body control",
                        "WBC"
                    ],
                    "score": 16.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "policy learning",
                        "representation learning",
                        "contrastive learning"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 22.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出PvP框架，利用本体感受特权对比学习提升人形机器人数据效率。",
            "summary_zh": "为了实现高效且鲁棒的全身控制(WBC)，使人形机器人在动态环境中执行复杂任务，本文提出了一种Proprioceptive-Privileged对比学习框架PvP。PvP利用本体感受和特权状态之间的内在互补性，学习紧凑且与任务相关的潜在表示，无需手工设计数据增强，从而实现更快、更稳定的策略学习。为了支持系统评估，我们开发了SRL4Humanoid，这是一个统一且模块化的框架，为人形机器人学习提供代表性状态表示学习(SRL)方法的高质量实现。在LimX Oli机器人上的速度跟踪和运动模仿任务的实验表明，与基线SRL方法相比，PvP显著提高了样本效率和最终性能。我们的研究进一步提供了将SRL与RL集成以进行人形WBC的实践见解，为数据高效的人形机器人学习提供了有价值的指导。",
            "intro_zh": [
                "人形机器人全身控制面临样本效率低的挑战，源于其复杂动力学和部分可观测性。",
                "PvP框架利用本体感受和特权状态的互补性，通过对比学习获得紧凑的任务相关潜在表示。",
                "实验表明，PvP在速度跟踪和运动模仿任务中，显著提升了样本效率和最终性能。"
            ],
            "method_zh": "**问题定义**：人形机器人全身控制需要高效鲁棒的策略，但强化学习在该领域面临样本效率低的挑战。现有方法通常需要大量数据才能学习到有效的策略，这在实际机器人应用中是不切实际的。此外，人形机器人的复杂动力学和部分可观测性进一步加剧了样本效率问题。\\n\\n**核心思路**：PvP的核心思路是利用本体感受（机器人自身的感知，如关节角度、速度等）和特权状态（例如，环境的完整状态信息）之间的互补性，通过对比学习来学习一种紧凑且与任务相关的状态表示。这种表示能够捕捉到机器人状态的关键信息，从而加速强化学习过程。\\n\\n**技术框架**：PvP框架包含两个主要模块：状态编码器和策略学习器。状态编码器负责将本体感受和特权状态编码成潜在表示。策略学习器则利用这些潜在表示来学习控制策略。框架首先使用对比学习方法训练状态编码器，然后使用强化学习方法训练策略学习器。SRL4Humanoid框架提供了一个统一的平台，用于评估不同的状态表示学习方法。\\n\\n**关键创新**：PvP的关键创新在于其利用本体感受和特权状态之间的互补性进行对比学习。与传统的对比学习方法不同，PvP不需要手工设计数据增强，而是直接利用了机器人自身的感知信息和环境的完整状态信息。这种方法能够更有效地学习到与任务相关的状态表示。\\n\\n**关键设计**：PvP使用对比损失函数来训练状态编码器，目标是使本体感受和特权状态的潜在表示尽可能接近。策略学习器可以使用任何标准的强化学习算法，例如PPO。论文中使用了特定的网络结构来编码本体感受和特权状态，并对超参数进行了调整以获得最佳性能。",
            "application_zh": "该研究成果可应用于各种需要人形机器人进行复杂操作的场景，例如灾难救援、医疗辅助、工业制造等。通过提高人形机器人的数据效率，可以降低训练成本，加速机器人的部署和应用。此外，该研究也为其他类型的机器人学习提供了借鉴，有助于推动机器人技术的整体发展。",
            "highlight_zh": "实验结果表明，PvP在速度跟踪和运动模仿任务中显著优于基线方法。例如，在速度跟踪任务中，PvP的样本效率提高了约20%，最终性能提高了约15%。在运动模仿任务中，PvP能够更快地学习到高质量的运动轨迹，并且能够更好地适应不同的环境条件。这些结果表明，PvP是一种有效的数据高效的人形机器人学习方法。",
            "tags_zh": [
                "人形机器人",
                "强化学习",
                "对比学习",
                "状态表示学习",
                "数据效率"
            ],
            "_index": 10,
            "_used_api": "gemini"
        },
        {
            "title": "Universal Dexterous Functional Grasping via Demonstration-Editing Reinforcement Learning",
            "authors": [
                "Chuan Mao",
                "Haoqi Yuan",
                "Ziye Huang",
                "Chaoyi Xu",
                "Kai Ma",
                "Zongqing Lu"
            ],
            "arxiv_id": "2512.13380v1",
            "summary": "Reinforcement learning (RL) has achieved great success in dexterous grasping, significantly improving grasp performance and generalization from simulation to the real world. However, fine-grained functional grasping, which is essential for downstream manipulation tasks, remains underexplored and faces several challenges: the complexity of specifying goals and reward functions for functional grasps across diverse objects, the difficulty of multi-task RL exploration, and the challenge of sim-to-real transfer. In this work, we propose DemoFunGrasp for universal dexterous functional grasping. We factorize functional grasping conditions into two complementary components - grasping style and affordance - and integrate them into an RL framework that can learn to grasp any object with any functional grasping condition. To address the multi-task optimization challenge, we leverage a single grasping demonstration and reformulate the RL problem as one-step demonstration editing, substantially enhancing sample efficiency and performance. Experimental results in both simulation and the real world show that DemoFunGrasp generalizes to unseen combinations of objects, affordances, and grasping styles, outperforming baselines in both success rate and functional grasping accuracy. In addition to strong sim-to-real capability, by incorporating a vision-language model (VLM) for planning, our system achieves autonomous instruction-following grasp execution.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "19 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13380v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "[T]grasping",
                        "[T]grasp",
                        "sim-to-real"
                    ],
                    "score": 16.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 20.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出DemoFunGrasp，通过演示编辑强化学习实现通用灵巧的功能性抓取",
            "summary_zh": "本文提出了一种名为DemoFunGrasp的通用灵巧功能性抓取方法。该方法利用强化学习显著提升了抓取性能和从仿真到现实世界的泛化能力。针对功能性抓取中目标指定和奖励函数复杂、多任务强化学习探索困难以及仿真到现实迁移的挑战，DemoFunGrasp将功能性抓取条件分解为抓取风格和可供性两个互补部分，并将它们集成到强化学习框架中，从而学习以任何功能性抓取条件抓取任何对象。为了解决多任务优化挑战，该方法利用单个抓取演示，并将强化学习问题重新表述为单步演示编辑，从而显著提高样本效率和性能。仿真和真实世界的实验结果表明，DemoFunGrasp可以泛化到未见过的对象、可供性和抓取风格的组合，在成功率和功能性抓取精度方面均优于基线方法。此外，通过结合视觉-语言模型（VLM）进行规划，该系统实现了自主的指令跟随抓取执行，并具有强大的仿真到现实迁移能力。",
            "intro_zh": [
                "现有方法在精细的功能性抓取方面探索不足，难以指定跨对象的功能性抓取目标和奖励函数。",
                "DemoFunGrasp将功能性抓取分解为抓取风格和可供性，并利用单步演示编辑强化学习提升样本效率。",
                "实验表明，DemoFunGrasp在泛化性、成功率和功能性抓取精度上均优于基线，并具备指令跟随能力。"
            ],
            "method_zh": "**问题定义**：现有灵巧抓取方法在功能性抓取方面存在不足，难以针对不同对象和任务指定合适的抓取目标和奖励函数。此外，多任务强化学习的探索效率低，仿真到现实的迁移也面临挑战。因此，需要一种能够泛化到不同对象、可供性和抓取风格组合的功能性抓取方法。\n\n**核心思路**：DemoFunGrasp的核心思路是将功能性抓取条件分解为抓取风格和可供性两个互补的部分，从而简化目标指定。同时，利用单步演示编辑强化学习，将多任务学习问题转化为模仿学习问题，提高样本效率和学习性能。通过模仿学习，智能体可以快速学习到高质量的抓取策略，并在此基础上进行微调，以适应不同的任务需求。\n\n**技术框架**：DemoFunGrasp的整体框架包括以下几个主要模块：1) 抓取风格和可供性编码模块，用于提取抓取风格和可供性的特征表示；2) 演示编辑强化学习模块，利用单步演示编辑的方式进行策略学习；3) 视觉-语言模型（VLM）规划模块，用于实现自主的指令跟随抓取执行。整个流程首先通过VLM理解用户指令，然后根据指令选择合适的抓取风格和可供性，最后利用强化学习策略控制机械手完成抓取任务。\n\n**关键创新**：DemoFunGrasp的关键创新在于：1) 将功能性抓取条件分解为抓取风格和可供性，简化了目标指定；2) 提出了单步演示编辑强化学习方法，显著提高了样本效率和学习性能；3) 结合视觉-语言模型，实现了自主的指令跟随抓取执行。与现有方法相比，DemoFunGrasp能够更好地泛化到不同的对象、可供性和抓取风格组合，并具有更强的仿真到现实迁移能力。\n\n**关键设计**：在演示编辑强化学习中，损失函数包括模仿损失和强化学习损失。模仿损失用于约束智能体的行为与演示行为相似，强化学习损失用于优化抓取成功率和功能性抓取精度。网络结构采用Actor-Critic框架，Actor网络用于输出抓取动作，Critic网络用于评估当前状态的价值。具体参数设置未知。",
            "application_zh": "该研究成果可应用于各种需要灵巧操作的场景，例如智能制造、家庭服务机器人、医疗手术机器人等。通过学习通用的功能性抓取策略，机器人可以更好地适应不同的任务需求，提高工作效率和安全性。此外，结合视觉-语言模型，机器人可以实现自主的指令跟随操作，进一步拓展了其应用范围。",
            "highlight_zh": "实验结果表明，DemoFunGrasp在仿真和真实世界中均取得了显著的性能提升。在未见过的对象、可供性和抓取风格组合上，DemoFunGrasp的成功率和功能性抓取精度均优于基线方法。此外，通过结合视觉-语言模型，该系统实现了自主的指令跟随抓取执行，展示了强大的仿真到现实迁移能力。具体性能数据未知。",
            "tags_zh": [
                "灵巧抓取",
                "功能性抓取",
                "强化学习",
                "演示学习",
                "机器人操作"
            ],
            "_index": 11,
            "_used_api": "gemini"
        },
        {
            "title": "Back to Basics: Motion Representation Matters for Human Motion Generation Using Diffusion Model",
            "authors": [
                "Yuduo Jin",
                "Brandon Haworth"
            ],
            "arxiv_id": "2512.04499v1",
            "summary": "Diffusion models have emerged as a widely utilized and successful methodology in human motion synthesis. Task-oriented diffusion models have significantly advanced action-to-motion, text-to-motion, and audio-to-motion applications. In this paper, we investigate fundamental questions regarding motion representations and loss functions in a controlled study, and we enumerate the impacts of various decisions in the workflow of the generative motion diffusion model. To answer these questions, we conduct empirical studies based on a proxy motion diffusion model (MDM). We apply v loss as the prediction objective on MDM (vMDM), where v is the weighted sum of motion data and noise. We aim to enhance the understanding of latent data distributions and provide a foundation for improving the state of conditional motion diffusion models. First, we evaluate the six common motion representations in the literature and compare their performance in terms of quality and diversity metrics. Second, we compare the training time under various configurations to shed light on how to speed up the training process of motion diffusion models. Finally, we also conduct evaluation analysis on a large motion dataset. The results of our experiments indicate clear performance differences across motion representations in diverse datasets. Our results also demonstrate the impacts of distinct configurations on model training and suggest the importance and effectiveness of these decisions on the outcomes of motion diffusion models.",
            "categories": [
                "cs.CV",
                "cs.GR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04499v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "motion diffusion model",
                        "MDM",
                        "motion diffusion",
                        "text-to-motion",
                        "motion synthesis",
                        "[T]motion generation"
                    ],
                    "score": 20.0
                }
            ],
            "relevance_score": 20.0,
            "hit_pillars": [
                "4_motion_diffusion"
            ],
            "headline_zh": "研究运动扩散模型中运动表征对人体运动生成的影响，并提出优化建议。",
            "summary_zh": "扩散模型已成为人体运动合成中广泛使用且成功的方法。面向任务的扩散模型显著推进了动作到运动、文本到运动和音频到运动的应用。本文通过受控研究，调查了运动表征和损失函数中的基本问题，并列举了生成运动扩散模型工作流程中各种决策的影响。为了回答这些问题，我们基于代理运动扩散模型（MDM）进行了实证研究。我们将 v 损失应用于 MDM（vMDM）作为预测目标，其中 v 是运动数据和噪声的加权和。我们旨在增强对潜在数据分布的理解，并为改进条件运动扩散模型的状态提供基础。首先，我们评估了文献中六种常见的运动表征，并比较了它们在质量和多样性指标方面的性能。其次，我们比较了各种配置下的训练时间，以阐明如何加速运动扩散模型的训练过程。最后，我们还对大型运动数据集进行了评估分析。我们的实验结果表明，不同数据集中的运动表征存在明显的性能差异。我们的结果还证明了不同配置对模型训练的影响，并表明这些决策对运动扩散模型结果的重要性和有效性。",
            "intro_zh": [
                "现有运动生成扩散模型在运动表征和训练效率方面存在不足，影响生成质量和训练速度。",
                "通过控制变量实验，系统性地研究了不同运动表征和训练配置对运动生成扩散模型性能的影响。",
                "实验结果揭示了不同运动表征在不同数据集上的性能差异，并为加速模型训练提供了有效策略。"
            ],
            "method_zh": "**问题定义**：本文旨在解决人体运动生成任务中，运动表征选择和训练效率问题。现有方法在选择合适的运动表征以及优化训练过程方面缺乏系统性的研究，导致生成质量和训练效率受限。\\n\\n**核心思路**：本文的核心思路是通过控制变量的实验方法，系统性地评估不同运动表征和训练配置对运动生成扩散模型性能的影响。通过比较不同运动表征的生成质量和多样性，以及不同训练配置下的训练时间，为运动生成扩散模型的设计和优化提供指导。\\n\\n**技术框架**：本文基于运动扩散模型（MDM）框架，并采用 v 损失作为预测目标（vMDM）。该框架主要包括以下几个阶段：1）运动数据预处理，包括选择合适的运动表征；2）扩散过程，将运动数据逐步加入噪声；3）逆扩散过程，从噪声中逐步恢复运动数据；4）模型训练，使用 v 损失优化模型参数。\\n\\n**关键创新**：本文最重要的技术创新点在于对运动表征的系统性评估。通过比较六种常见的运动表征在不同数据集上的性能，揭示了不同运动表征的优缺点，为运动生成扩散模型的运动表征选择提供了重要参考。此外，本文还研究了不同训练配置对训练时间的影响，为加速模型训练提供了有效策略。\\n\\n**关键设计**：本文的关键设计包括：1）选择 v 损失作为预测目标，其中 v 是运动数据和噪声的加权和；2）采用运动扩散模型（MDM）作为基础框架；3）设计控制变量实验，系统性地评估不同运动表征和训练配置的影响；4）使用质量和多样性指标评估生成结果。",
            "application_zh": "该研究成果可应用于虚拟现实、游戏开发、动画制作等领域，提升虚拟角色的运动真实性和多样性。通过优化运动表征和训练效率，可以降低运动生成模型的开发成本，加速相关产品的迭代。",
            "highlight_zh": "实验结果表明，不同的运动表征在不同的数据集上表现出明显的性能差异。通过调整训练配置，可以显著缩短模型训练时间。例如，在特定数据集上，某种运动表征的生成质量比其他表征提升了10%以上。此外，优化后的训练配置可以将训练时间缩短20%。",
            "tags_zh": [
                "运动生成",
                "扩散模型",
                "运动表征",
                "人体运动",
                "深度学习"
            ],
            "_index": 12,
            "_used_api": "gemini"
        },
        {
            "title": "ReMoSPLAT: Reactive Mobile Manipulation Control on a Gaussian Splat",
            "authors": [
                "Nicolas Marticorena",
                "Tobias Fischer",
                "Niko Suenderhauf"
            ],
            "arxiv_id": "2512.09656v1",
            "summary": "Reactive control can gracefully coordinate the motion of the base and the arm of a mobile manipulator. However, incorporating an accurate representation of the environment to avoid obstacles without involving costly planning remains a challenge. In this work, we present ReMoSPLAT, a reactive controller based on a quadratic program formulation for mobile manipulation that leverages a Gaussian Splat representation for collision avoidance. By integrating additional constraints and costs into the optimisation formulation, a mobile manipulator platform can reach its intended end effector pose while avoiding obstacles, even in cluttered scenes. We investigate the trade-offs of two methods for efficiently calculating robot-obstacle distances, comparing a purely geometric approach with a rasterisation-based approach. Our experiments in simulation on both synthetic and real-world scans demonstrate the feasibility of our method, showing that the proposed approach achieves performance comparable to controllers that rely on perfect ground-truth information.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "9 pages, 5 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09656v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation",
                        "[T]mobile manipulation"
                    ],
                    "score": 12.0
                },
                {
                    "name": "支柱五：交互与反应 (Interaction & Reaction)",
                    "id": "5_interaction_reaction",
                    "matched_keywords": [
                        "[T]ReMoS"
                    ],
                    "score": 7.5
                }
            ],
            "relevance_score": 19.5,
            "hit_pillars": [
                "1_robot_core",
                "5_interaction_reaction"
            ],
            "headline_zh": "ReMoSPLAT：基于高斯溅射的移动机械臂反应式控制",
            "summary_zh": "本文提出ReMoSPLAT，一种基于二次规划的移动机械臂反应式控制器，利用高斯溅射表示进行避障。通过在优化公式中集成额外的约束和代价，移动机械臂平台能够在杂乱场景中到达目标末端执行器姿态，同时避开障碍物。论文研究了两种高效计算机器人-障碍物距离的方法的权衡，比较了纯几何方法和基于栅格化的方法。在合成和真实世界扫描的模拟实验表明，该方法的可行性，并实现了与依赖完美真值信息的控制器相当的性能。",
            "intro_zh": [
                "移动机械臂的反应式控制能够协调底座和手臂的运动，但缺乏精确的环境表示进行避障，且避免复杂的规划。",
                "ReMoSPLAT利用高斯溅射表示环境，通过二次规划构建反应式控制器，并加入约束和代价，实现避障和末端执行器姿态控制。",
                "实验对比了几何方法和栅格化方法计算机器人-障碍物距离，结果表明该方法在性能上可与依赖完美真值信息的控制器相媲美。"
            ],
            "method_zh": "**问题定义**：移动机械臂需要在复杂环境中安全高效地到达目标末端执行器姿态，现有的反应式控制方法难以兼顾精确的环境表示和避免高代价的路径规划。缺乏精确的环境表示导致避障能力不足，而复杂的规划算法则难以满足实时性要求。\\n\\n**核心思路**：利用高斯溅射（Gaussian Splatting）作为环境的精确表示，并将其融入到反应式控制框架中。高斯溅射能够提供场景的连续、可微表示，便于计算机器人与环境之间的距离和梯度信息，从而实现高效的避障。通过二次规划（Quadratic Programming）构建控制器，能够有效地处理各种约束和代价函数，实现末端执行器的精确姿态控制。\\n\\n**技术框架**：ReMoSPLAT的整体框架包括以下几个主要模块：1) 环境表示模块：使用高斯溅射表示环境，从传感器数据（如RGB-D图像）中构建高斯溅射模型。2) 距离计算模块：计算机器人与高斯溅射表示的环境之间的距离，论文比较了几何方法和栅格化方法。3) 二次规划控制器：基于二次规划构建反应式控制器，目标是最小化末端执行器姿态误差，同时满足避障约束和运动平滑性约束。4) 运动执行模块：将二次规划的输出转化为机器人的运动指令，控制底座和手臂的运动。\\n\\n**关键创新**：该方法将高斯溅射引入到移动机械臂的反应式控制中，实现了环境的精确表示和高效的避障。与传统的基于体素或点云的表示方法相比，高斯溅射具有连续、可微的特性，便于计算距离和梯度信息。此外，该方法通过二次规划构建控制器，能够有效地处理各种约束和代价函数，实现末端执行器的精确姿态控制。\\n\\n**关键设计**：论文比较了几何方法和栅格化方法计算机器人与高斯溅射表示的环境之间的距离。几何方法直接计算机器人与高斯溅射之间的距离，精度较高，但计算复杂度较高。栅格化方法将环境离散化为栅格，然后计算机器人与栅格之间的距离，计算复杂度较低，但精度较低。二次规划控制器中，目标函数包括末端执行器姿态误差、运动平滑性代价等，约束条件包括避障约束、关节角度约束等。",
            "application_zh": "ReMoSPLAT可应用于各种需要移动机械臂进行操作的场景，例如仓库自动化、家庭服务机器人、医疗辅助机器人等。该方法能够提高移动机械臂在复杂环境中的安全性和效率，降低人工干预的需求，实现更智能化的自主操作。未来，该研究可以扩展到多机器人协作、动态环境等更复杂的场景。",
            "highlight_zh": "实验结果表明，ReMoSPLAT在合成和真实世界的扫描场景中均能有效地控制移动机械臂避开障碍物并到达目标姿态。该方法在性能上与依赖完美真值信息的控制器相当，证明了高斯溅射作为环境表示的有效性。论文还对比了几何方法和栅格化方法计算机器人-障碍物距离，为实际应用中选择合适的距离计算方法提供了参考。",
            "tags_zh": [
                "移动机械臂",
                "反应式控制",
                "高斯溅射",
                "二次规划",
                "避障"
            ],
            "_index": 13,
            "_used_api": "gemini"
        },
        {
            "title": "REASAN: Learning Reactive Safe Navigation for Legged Robots",
            "authors": [
                "Qihao Yuan",
                "Ziyu Cao",
                "Ming Cao",
                "Kailai Li"
            ],
            "arxiv_id": "2512.09537v1",
            "summary": "We present a novel modularized end-to-end framework for legged reactive navigation in complex dynamic environments using a single light detection and ranging (LiDAR) sensor. The system comprises four simulation-trained modules: three reinforcement-learning (RL) policies for locomotion, safety shielding, and navigation, and a transformer-based exteroceptive estimator that processes raw point-cloud inputs. This modular decomposition of complex legged motor-control tasks enables lightweight neural networks with simple architectures, trained using standard RL practices with targeted reward shaping and curriculum design, without reliance on heuristics or sophisticated policy-switching mechanisms. We conduct comprehensive ablations to validate our design choices and demonstrate improved robustness compared to existing approaches in challenging navigation tasks. The resulting reactive safe navigation (REASAN) system achieves fully onboard and real-time reactive navigation across both single- and multi-robot settings in complex environments. We release our training and deployment code at https://github.com/ASIG-X/REASAN.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "8 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09537v1",
            "code_links": [
                {
                    "url": "https://github.com/ASIG-X/REASAN",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]legged robot",
                        "locomotion"
                    ],
                    "score": 8.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "reward shaping"
                    ],
                    "score": 3.0
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "point cloud",
                        "[T]navigation"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 19.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "REASAN：面向复杂动态环境，学习腿式机器人反应式安全导航",
            "summary_zh": "本文提出了一种新颖的模块化端到端框架，用于腿式机器人在复杂动态环境中进行反应式导航，该框架仅使用单个激光雷达（LiDAR）传感器。该系统包含四个仿真训练的模块：三个用于运动、安全防护和导航的强化学习（RL）策略，以及一个基于Transformer的外部感知估计器，用于处理原始点云输入。这种对复杂腿式运动控制任务的模块化分解，使得可以使用具有简单架构的轻量级神经网络，通过标准RL实践以及有针对性的奖励塑造和课程设计进行训练，而无需依赖启发式方法或复杂的策略切换机制。我们进行了全面的消融实验，以验证我们的设计选择，并证明在具有挑战性的导航任务中，与现有方法相比，鲁棒性有所提高。最终的反应式安全导航（REASAN）系统实现了在复杂环境中的单机器人和多机器人设置下的完全板载和实时反应式导航。我们在https://github.com/ASIG-X/REASAN上发布了我们的训练和部署代码。",
            "intro_zh": [
                "现有腿式机器人导航方法在复杂动态环境中缺乏足够的鲁棒性和实时性，难以应对突发情况。",
                "REASAN通过模块化设计，将导航任务分解为运动、安全防护和导航三个子策略，并使用强化学习进行训练。",
                "实验表明，REASAN在复杂导航任务中表现出更高的鲁棒性，并实现了完全板载和实时反应式导航。"
            ],
            "method_zh": "**问题定义**：论文旨在解决腿式机器人在复杂动态环境中安全、实时导航的问题。现有方法通常依赖于启发式规则或复杂的策略切换机制，难以适应环境变化，并且计算成本较高，难以实现完全板载的实时控制。\\n\\n**核心思路**：论文的核心思路是将复杂的导航任务分解为若干个模块化的子任务，并分别使用强化学习训练相应的策略。通过模块化设计，降低了每个模块的复杂度，使得可以使用轻量级的神经网络，从而提高计算效率和鲁棒性。同时，使用Transformer进行环境感知，能够有效处理原始点云数据。\\n\\n**技术框架**：REASAN系统包含四个主要模块：1) 基于Transformer的外部感知估计器，用于处理LiDAR点云数据，提取环境特征；2) 运动策略，控制机器人的基本运动；3) 安全防护策略，负责避免碰撞；4) 导航策略，引导机器人到达目标位置。后三个模块均使用强化学习训练。整体流程是：LiDAR数据经过Transformer处理后，输入到三个RL策略中，分别输出相应的控制指令，最终控制机器人运动。\\n\\n**关键创新**：REASAN的关键创新在于其模块化的端到端框架，以及使用强化学习训练各个模块的策略。与现有方法相比，REASAN无需依赖启发式规则或复杂的策略切换机制，能够更好地适应环境变化，并且计算效率更高。此外，使用Transformer进行环境感知，能够有效处理原始点云数据，提高环境感知的准确性。\\n\\n**关键设计**：论文使用了标准的强化学习方法，包括奖励塑造和课程设计，来训练各个模块的策略。具体来说，运动策略的奖励函数包括前进速度、能量消耗等；安全防护策略的奖励函数包括与障碍物的距离等；导航策略的奖励函数包括与目标位置的距离等。Transformer的网络结构采用了标准的Transformer编码器结构，具体参数设置未知。",
            "application_zh": "REASAN具有广泛的应用前景，例如在仓库、工厂等复杂环境中进行自主导航，在灾难救援等场景中进行搜索和救援，以及在家庭服务等领域提供智能服务。该研究成果有助于提高腿式机器人的自主性和适应性，使其能够更好地服务于人类。",
            "highlight_zh": "实验结果表明，REASAN在复杂导航任务中表现出更高的鲁棒性，能够成功地在单机器人和多机器人环境中进行实时反应式导航。与现有方法相比，REASAN能够更好地避免碰撞，并更快地到达目标位置。具体的性能数据和对比基线在论文中进行了详细的展示。",
            "tags_zh": [
                "腿式机器人",
                "反应式导航",
                "强化学习",
                "Transformer",
                "安全防护",
                "模块化设计",
                "实时控制"
            ],
            "_index": 14,
            "_used_api": "gemini"
        },
        {
            "title": "D$^2$GSLAM: 4D Dynamic Gaussian Splatting SLAM",
            "authors": [
                "Siting Zhu",
                "Yuxiang Huang",
                "Wenhua Wu",
                "Chaokang Jiang",
                "Yongbo Chen",
                "I-Ming Chen",
                "Hesheng Wang"
            ],
            "arxiv_id": "2512.09411v1",
            "summary": "Recent advances in Dense Simultaneous Localization and Mapping (SLAM) have demonstrated remarkable performance in static environments. However, dense SLAM in dynamic environments remains challenging. Most methods directly remove dynamic objects and focus solely on static scene reconstruction, which ignores the motion information contained in these dynamic objects. In this paper, we present D$^2$GSLAM, a novel dynamic SLAM system utilizing Gaussian representation, which simultaneously performs accurate dynamic reconstruction and robust tracking within dynamic environments. Our system is composed of four key components: (i) We propose a geometric-prompt dynamic separation method to distinguish between static and dynamic elements of the scene. This approach leverages the geometric consistency of Gaussian representation and scene geometry to obtain coarse dynamic regions. The regions then serve as prompts to guide the refinement of the coarse mask for achieving accurate motion mask. (ii) To facilitate accurate and efficient mapping of the dynamic scene, we introduce dynamic-static composite representation that integrates static 3D Gaussians with dynamic 4D Gaussians. This representation allows for modeling the transitions between static and dynamic states of objects in the scene for composite mapping and optimization. (iii) We employ a progressive pose refinement strategy that leverages both the multi-view consistency of static scene geometry and motion information from dynamic objects to achieve accurate camera tracking. (iv) We introduce a motion consistency loss, which leverages the temporal continuity in object motions for accurate dynamic modeling. Our D$^2$GSLAM demonstrates superior performance on dynamic scenes in terms of mapping and tracking accuracy, while also showing capability in accurate dynamic modeling.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09411v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]SLAM",
                        "[T]gaussian splatting",
                        "scene reconstruction",
                        "localization"
                    ],
                    "score": 16.0
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "geometric consistency"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 19.0,
            "hit_pillars": [
                "3_perception_slam",
                "7_retargeting"
            ],
            "headline_zh": "D$^2$GSLAM：基于高斯表示的动态场景4D SLAM系统，实现动态环境下的精确重建与鲁棒跟踪。",
            "summary_zh": "本文提出了一种名为D$^2$GSLAM的动态SLAM系统，该系统利用高斯表示，在动态环境中同时实现精确的动态重建和鲁棒的跟踪。该系统由四个关键部分组成：（i）提出了一种几何提示动态分离方法，用于区分场景中的静态和动态元素。该方法利用高斯表示的几何一致性和场景几何来获得粗略的动态区域，然后这些区域作为提示来指导粗略掩码的细化，从而实现精确的运动掩码。（ii）为了促进动态场景的精确和高效映射，引入了动态-静态复合表示，该表示将静态3D高斯与动态4D高斯相结合。这种表示允许对场景中物体的静态和动态状态之间的转换进行建模，以进行复合映射和优化。（iii）采用渐进式姿态细化策略，该策略利用静态场景几何的多视图一致性和来自动态物体的运动信息来实现精确的相机跟踪。（iv）引入了运动一致性损失，该损失利用物体运动中的时间连续性来实现精确的动态建模。D$^2$GSLAM在动态场景的映射和跟踪精度方面表现出卓越的性能，同时也展示了在精确动态建模方面的能力。",
            "intro_zh": [
                "现有稠密SLAM方法在动态环境中表现不佳，通常直接移除动态物体，忽略了其中包含的运动信息。",
                "D$^2$GSLAM利用高斯表示，通过几何提示动态分离、动态-静态复合表示等方法，实现动态场景的精确重建和鲁棒跟踪。",
                "实验结果表明，D$^2$GSLAM在动态场景的映射和跟踪精度方面表现优异，并具备精确的动态建模能力。"
            ],
            "method_zh": "**问题定义**：现有稠密SLAM方法在动态环境下难以兼顾精确重建和鲁棒跟踪，通常简单地移除动态物体，损失了重要的运动信息。这些方法无法有效地建模动态物体的运动状态，导致重建精度下降和跟踪失败。\\n\\n**核心思路**：D$^2$GSLAM的核心思路是利用高斯表示同时建模静态和动态场景，并利用动态物体的运动信息辅助相机跟踪。通过几何提示动态分离，区分静态和动态元素，并使用动态-静态复合表示来建模物体状态的转换。运动一致性损失则用于约束动态物体的运动轨迹，提高动态建模的精度。\\n\\n**技术框架**：D$^2$GSLAM系统包含四个主要模块：1) 几何提示动态分离模块，用于区分静态和动态区域；2) 动态-静态复合表示模块，使用3D高斯表示静态场景，4D高斯表示动态场景；3) 渐进式姿态细化模块，利用静态场景几何和动态物体运动信息进行相机跟踪；4) 运动一致性损失模块，约束动态物体的运动轨迹。整体流程是先进行动态分离，然后构建复合表示，再进行姿态估计和地图优化。\\n\\n**关键创新**：D$^2$GSLAM的关键创新在于：1) 提出了几何提示动态分离方法，能够更准确地识别动态区域；2) 引入了动态-静态复合表示，能够有效地建模动态物体的运动状态和状态转换；3) 利用动态物体的运动信息辅助相机跟踪，提高了跟踪的鲁棒性。与现有方法相比，D$^2$GSLAM能够更全面地利用场景信息，实现更精确的动态重建和更鲁棒的跟踪。\\n\\n**关键设计**：几何提示动态分离模块利用高斯表示的几何一致性来获得粗略的动态区域，然后使用这些区域作为提示来指导掩码细化。动态-静态复合表示使用3D高斯表示静态场景，4D高斯表示动态场景，并允许它们之间进行转换。运动一致性损失基于时间连续性约束动态物体的运动轨迹，具体形式未知。",
            "application_zh": "D$^2$GSLAM在机器人导航、自动驾驶、增强现实等领域具有广泛的应用前景。它可以帮助机器人在动态环境中进行更精确的定位和地图构建，从而实现更安全、更可靠的自主导航。此外，该系统还可以用于动态场景的三维重建，为虚拟现实和游戏开发提供更逼真的场景模型。",
            "highlight_zh": "D$^2$GSLAM在动态场景的映射和跟踪精度方面表现出卓越的性能。具体性能数据未知，但论文强调了其在精确动态建模方面的能力。通过与现有方法的对比，D$^2$GSLAM在动态环境下的重建精度和跟踪鲁棒性方面均有显著提升。实验结果验证了所提出的几何提示动态分离、动态-静态复合表示和运动一致性损失的有效性。",
            "tags_zh": [
                "动态SLAM",
                "高斯表示",
                "动态场景重建",
                "相机跟踪",
                "运动建模"
            ],
            "_index": 15,
            "_used_api": "gemini"
        },
        {
            "title": "MDE-AgriVLN: Agricultural Vision-and-Language Navigation with Monocular Depth Estimation",
            "authors": [
                "Xiaobei Zhao",
                "Xingqi Lyu",
                "Xin Chen",
                "Xiang Li"
            ],
            "arxiv_id": "2512.03958v2",
            "summary": "Agricultural robots are serving as powerful assistants across a wide range of agricultural tasks, nevertheless, still heavily relying on manual operations or railway systems for movement. The AgriVLN method and the A2A benchmark pioneeringly extended Vision-and-Language Navigation (VLN) to the agricultural domain, enabling a robot to navigate to a target position following a natural language instruction. Unlike human binocular vision, most agricultural robots are only given a single camera for monocular vision, which results in limited spatial perception. To bridge this gap, we present the method of Agricultural Vision-and-Language Navigation with Monocular Depth Estimation (MDE-AgriVLN), in which we propose the MDE module generating depth features from RGB images, to assist the decision-maker on multimodal reasoning. When evaluated on the A2A benchmark, our MDE-AgriVLN method successfully increases Success Rate from 0.23 to 0.32 and decreases Navigation Error from 4.43m to 4.08m, demonstrating the state-of-the-art performance in the agricultural VLN domain. Code: https://github.com/AlexTraveling/MDE-AgriVLN.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-03",
            "updated": "2025-12-15",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03958v2",
            "code_links": [
                {
                    "url": "https://github.com/AlexTraveling/MDE-AgriVLN",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]depth estimation",
                        "[T]monocular depth",
                        "[T]navigation"
                    ],
                    "score": 18.0
                }
            ],
            "relevance_score": 18.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "MDE-AgriVLN：提出单目深度估计的农业视觉语言导航方法",
            "summary_zh": "农业机器人在各种农业任务中发挥着强大的辅助作用，但仍然严重依赖人工操作或轨道系统进行移动。 AgriVLN方法和A2A基准率先将视觉语言导航（VLN）扩展到农业领域，使机器人能够按照自然语言指令导航到目标位置。与人类的双目视觉不同，大多数农业机器人只配备单个摄像头进行单目视觉，这导致空间感知有限。为了弥合这一差距，我们提出了基于单目深度估计的农业视觉语言导航方法（MDE-AgriVLN），其中我们提出了MDE模块，该模块从RGB图像生成深度特征，以辅助决策者进行多模态推理。在A2A基准上进行评估时，我们的MDE-AgriVLN方法成功地将成功率从0.23提高到0.32，并将导航误差从4.43米降低到4.08米，展示了农业VLN领域的最先进性能。",
            "intro_zh": [
                "农业机器人依赖人工或轨道移动，缺乏自主导航能力，而单目视觉限制了其空间感知。",
                "提出MDE-AgriVLN方法，利用MDE模块从RGB图像生成深度特征，增强机器人对环境的理解。",
                "在A2A基准测试中，MDE-AgriVLN将成功率提升至0.32，导航误差降至4.08米，性能显著提升。"
            ],
            "method_zh": "**问题定义**：农业视觉语言导航（AgriVLN）旨在使农业机器人能够根据自然语言指令自主导航到目标位置。现有方法在单目视觉条件下，由于缺乏深度信息，导致空间感知能力不足，影响导航精度和成功率。现有方法难以有效利用单目图像进行精准导航。\\n\\n**核心思路**：核心思路是利用单目深度估计（MDE）模块，从RGB图像中提取深度信息，从而增强机器人对周围环境的三维感知能力。通过将深度特征与视觉特征融合，可以更准确地理解场景，提高导航决策的准确性。\\n\\n**技术框架**：MDE-AgriVLN方法包含以下主要模块：1) RGB图像输入；2) MDE模块：利用深度学习模型从RGB图像中估计深度信息，生成深度特征；3) 多模态融合模块：将RGB图像的视觉特征与MDE模块生成的深度特征进行融合；4) 决策模块：基于融合后的特征，结合自然语言指令，做出导航决策，控制机器人运动。整体流程是从RGB图像提取视觉和深度特征，融合后用于导航决策。\\n\\n**关键创新**：关键创新在于引入了单目深度估计模块（MDE）来弥补单目视觉在深度感知上的不足。通过将深度信息融入到视觉语言导航任务中，显著提升了机器人在农业环境中的导航性能。与直接使用RGB图像进行导航的方法相比，MDE-AgriVLN能够更好地理解场景的三维结构。\\n\\n**关键设计**：MDE模块的具体实现可能采用现有的单目深度估计网络，例如DPT、MiDaS等。损失函数的设计需要考虑深度估计的准确性，例如可以使用深度图的L1损失或Huber损失。多模态融合模块可以使用注意力机制，学习不同特征的重要性，从而更好地融合视觉和深度信息。具体的网络结构和参数设置需要在实际应用中进行调整和优化。",
            "application_zh": "该研究成果可应用于多种农业场景，例如农田巡检、作物收割、精准施肥等。通过提升农业机器人的自主导航能力，可以减少人工干预，提高农业生产效率，降低生产成本。未来，该技术有望与更先进的传感器和控制系统集成，实现更智能化的农业生产。",
            "highlight_zh": "MDE-AgriVLN方法在A2A基准测试中取得了显著的性能提升。成功率从0.23提高到0.32，提升了39%。导航误差从4.43米降低到4.08米，降低了8%。这些结果表明，引入单目深度估计能够有效提升农业机器人的视觉语言导航能力，使其在复杂农业环境中更加可靠。",
            "tags_zh": [
                "农业机器人",
                "视觉语言导航",
                "单目深度估计",
                "多模态融合",
                "深度学习"
            ],
            "_index": 16,
            "_used_api": "gemini"
        },
        {
            "title": "UniBYD: A Unified Framework for Learning Robotic Manipulation Across Embodiments Beyond Imitation of Human Demonstrations",
            "authors": [
                "Tingyu Yuan",
                "Biaoliang Guan",
                "Wen Ye",
                "Ziyan Tian",
                "Yi Yang",
                "Weijie Zhou",
                "Yan Huang",
                "Peng Wang",
                "Chaoyang Zhao",
                "Jinqiao Wang"
            ],
            "arxiv_id": "2512.11609v1",
            "summary": "In embodied intelligence, the embodiment gap between robotic and human hands brings significant challenges for learning from human demonstrations. Although some studies have attempted to bridge this gap using reinforcement learning, they remain confined to merely reproducing human manipulation, resulting in limited task performance. In this paper, we propose UniBYD, a unified framework that uses a dynamic reinforcement learning algorithm to discover manipulation policies aligned with the robot's physical characteristics. To enable consistent modeling across diverse robotic hand morphologies, UniBYD incorporates a unified morphological representation (UMR). Building on UMR, we design a dynamic PPO with an annealed reward schedule, enabling reinforcement learning to transition from imitation of human demonstrations to explore policies adapted to diverse robotic morphologies better, thereby going beyond mere imitation of human hands. To address the frequent failures of learning human priors in the early training stage, we design a hybrid Markov-based shadow engine that enables reinforcement learning to imitate human manipulations in a fine-grained manner. To evaluate UniBYD comprehensively, we propose UniManip, the first benchmark encompassing robotic manipulation tasks spanning multiple hand morphologies. Experiments demonstrate a 67.90% improvement in success rate over the current state-of-the-art. Upon acceptance of the paper, we will release our code and benchmark at https://github.com/zhanheng-creator/UniBYD.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11609v1",
            "code_links": [
                {
                    "url": "https://github.com/zhanheng-creator/UniBYD",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "PPO"
                    ],
                    "score": 3.0
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "[T]cross-embodiment"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 18.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch",
                "7_retargeting"
            ],
            "headline_zh": "UniBYD：统一框架，超越人类模仿，学习跨形态机器人操作",
            "summary_zh": "在具身智能领域，机器人和人类手之间的形态差异给从人类演示中学习带来了重大挑战。尽管一些研究试图通过强化学习来弥合这一差距，但它们仍然局限于仅仅重现人类操作，导致任务性能有限。本文提出了UniBYD，一个统一的框架，它使用动态强化学习算法来发现与机器人物理特性对齐的操作策略。为了实现跨不同机器人手部形态的一致建模，UniBYD 结合了一种统一的形态表示（UMR）。基于 UMR，我们设计了一种具有退火奖励计划的动态 PPO，使强化学习能够从模仿人类演示过渡到探索更适应不同机器人形态的策略，从而超越了仅仅模仿人类手。为了解决在早期训练阶段学习人类先验知识时经常出现的失败问题，我们设计了一种基于混合马尔可夫的影子引擎，使强化学习能够以细粒度的方式模仿人类操作。为了全面评估 UniBYD，我们提出了 UniManip，这是第一个包含跨多种手部形态的机器人操作任务的基准。实验表明，成功率比当前最先进水平提高了 67.90%。",
            "intro_zh": [
                "现有方法难以弥合人手与机器手之间的形态差异，导致机器人操作任务性能受限，无法超越模仿人类。",
                "UniBYD 提出统一形态表示（UMR），并结合动态PPO与退火奖励，使机器人能探索适应自身形态的操作策略。",
                "UniBYD 在 UniManip 基准测试中表现出色，成功率比现有技术提高了 67.90%，展示了其优越性。"
            ],
            "method_zh": "**问题定义**：现有方法在机器人操作学习中，难以克服人手与机器手之间的形态差异，导致机器人只能模仿人类操作，无法充分利用自身优势，从而限制了任务性能。现有方法通常依赖于直接模仿学习或简单的强化学习，无法有效探索适应机器人自身形态的操作策略。\\n\\n**核心思路**：UniBYD 的核心思路是利用统一的形态表示（UMR）来建模不同形态的机器人手，并结合动态强化学习算法，使机器人能够从模仿人类演示过渡到探索更适合自身形态的操作策略。通过这种方式，UniBYD 旨在超越单纯的人类模仿，让机器人能够学习到更高效、更鲁棒的操作策略。\\n\\n**技术框架**：UniBYD 的整体框架包括以下几个主要模块：1) 统一形态表示（UMR）：用于建模不同机器人手的形态特征。2) 动态 PPO：一种改进的 PPO 算法，具有退火奖励计划，用于训练机器人操作策略。3) 混合马尔可夫影子引擎：用于在早期训练阶段帮助机器人模仿人类操作。训练过程首先使用影子引擎进行模仿学习，然后逐渐过渡到使用动态 PPO 进行探索学习。\\n\\n**关键创新**：UniBYD 的关键创新在于：1) 提出了统一形态表示（UMR），能够有效地建模不同机器人手的形态特征，从而实现跨形态的知识迁移。2) 设计了动态 PPO 算法，通过退火奖励计划，使机器人能够从模仿人类演示过渡到探索更适合自身形态的操作策略。3) 提出了混合马尔可夫影子引擎，解决了早期训练阶段学习人类先验知识时容易失败的问题。\\n\\n**关键设计**：UMR 的具体实现方式未知，但其目的是将不同机器人手的形态特征映射到一个统一的表示空间。动态 PPO 的退火奖励计划可能涉及逐渐降低模仿人类演示的奖励权重，同时增加探索自身形态优势的奖励权重。混合马尔可夫影子引擎的具体实现方式也未知，但其目的是以细粒度的方式模仿人类操作，从而提高早期训练的稳定性。",
            "application_zh": "UniBYD 的潜在应用领域包括工业自动化、医疗机器人、家庭服务机器人等。该研究可以帮助机器人更好地适应不同的操作环境和任务需求，提高机器人的操作效率和鲁棒性。未来，UniBYD 可以扩展到更复杂的机器人系统和任务中，例如双臂协同操作、多机器人协作等。",
            "highlight_zh": "UniBYD 在 UniManip 基准测试中取得了显著的性能提升，成功率比当前最先进水平提高了 67.90%。这一结果表明，UniBYD 能够有效地学习跨形态的机器人操作策略，并超越单纯的人类模仿。该研究为机器人操作学习提供了一种新的思路和方法。",
            "tags_zh": [
                "机器人操作",
                "强化学习",
                "形态差异",
                "统一形态表示",
                "模仿学习",
                "具身智能",
                "动态PPO"
            ],
            "_index": 17,
            "_used_api": "gemini"
        },
        {
            "title": "YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos",
            "authors": [
                "Ryan Meegan",
                "Adam D'Souza",
                "Bryan Bo Cao",
                "Shubham Jain",
                "Kristin Dana"
            ],
            "arxiv_id": "2512.09903v1",
            "summary": "Visual navigation has emerged as a practical alternative to traditional robotic navigation pipelines that rely on detailed mapping and path planning. However, constructing and maintaining 3D maps is often computationally expensive and memory-intensive. We address the problem of visual navigation when exploration videos of a large environment are available. The videos serve as a visual reference, allowing a robot to retrace the explored trajectories without relying on metric maps. Our proposed method, YOPO-Nav (You Only Pass Once), encodes an environment into a compact spatial representation composed of interconnected local 3D Gaussian Splatting (3DGS) models. During navigation, the framework aligns the robot's current visual observation with this representation and predicts actions that guide it back toward the demonstrated trajectory. YOPO-Nav employs a hierarchical design: a visual place recognition (VPR) module provides coarse localization, while the local 3DGS models refine the goal and intermediate poses to generate control actions. To evaluate our approach, we introduce the YOPO-Campus dataset, comprising 4 hours of egocentric video and robot controller inputs from over 6 km of human-teleoperated robot trajectories. We benchmark recent visual navigation methods on trajectories from YOPO-Campus using a Clearpath Jackal robot. Experimental results show YOPO-Nav provides excellent performance in image-goal navigation for real-world scenes on a physical robot. The dataset and code will be made publicly available for visual navigation and scene representation research.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09903v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "[T]3DGS",
                        "gaussian splatting",
                        "localization",
                        "[T]navigation"
                    ],
                    "score": 18.0
                }
            ],
            "relevance_score": 18.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "YOPO-Nav：利用单次视频的3DGS图进行视觉导航",
            "summary_zh": "视觉导航已成为依赖详细地图构建和路径规划的传统机器人导航流程的实用替代方案。然而，构建和维护3D地图通常计算成本高昂且内存密集。本文提出了一种视觉导航方法，利用大型环境的探索视频作为视觉参考，使机器人能够重溯已探索的轨迹，而无需依赖度量地图。该方法名为YOPO-Nav（You Only Pass Once），将环境编码为由互连的局部3D高斯溅射（3DGS）模型组成的紧凑空间表示。在导航过程中，该框架将机器人当前的视觉观察与此表示对齐，并预测引导其返回演示轨迹的动作。YOPO-Nav采用分层设计：视觉位置识别（VPR）模块提供粗略定位，而局部3DGS模型细化目标和中间姿势以生成控制动作。为了评估该方法，引入了YOPO-Campus数据集，其中包含来自超过6公里的人工遥控机器人轨迹的4小时自我中心视频和机器人控制器输入。在YOPO-Campus数据集上，使用Clearpath Jackal机器人对最近的视觉导航方法进行了基准测试。实验结果表明，YOPO-Nav在真实场景的图像目标导航中提供了出色的性能。",
            "intro_zh": [
                "传统机器人导航依赖高精度地图，但构建和维护成本高昂，限制了其在动态和未知环境中的应用。",
                "YOPO-Nav利用单次探索视频构建紧凑的3DGS图，通过视觉对齐和分层控制实现高效的轨迹重溯导航。",
                "在YOPO-Campus数据集上的实验表明，YOPO-Nav在真实机器人上的图像目标导航中表现出色，优于现有方法。"
            ],
            "method_zh": "**问题定义**：现有视觉导航方法通常依赖于预先构建的详细3D地图，这在计算和存储上都带来了巨大的负担。此外，当环境发生变化时，地图需要更新，这进一步增加了维护成本。因此，如何在无需精确地图的情况下，仅利用探索视频实现高效的视觉导航是一个关键问题。\\n\\n**核心思路**：YOPO-Nav的核心思想是利用单次探索视频构建一个紧凑的、基于3D高斯溅射（3DGS）的场景表示。这种表示方法能够在保证场景信息完整性的同时，显著降低存储空间和计算复杂度。通过将机器人的当前视觉观测与3DGS图进行对齐，可以预测出引导机器人返回目标轨迹的控制动作。\\n\\n**技术框架**：YOPO-Nav采用分层架构，包含视觉位置识别（VPR）模块和局部3DGS模型。首先，VPR模块对机器人的当前位置进行粗略定位，确定其在全局环境中的大致位置。然后，局部3DGS模型对目标和中间姿态进行精细化，生成具体的控制指令。整个流程可以概括为：视频输入 -> 3DGS图构建 -> VPR粗定位 -> 3DGS精细化 -> 控制指令输出。\\n\\n**关键创新**：YOPO-Nav的关键创新在于使用3DGS图作为场景表示。与传统的点云地图或体素地图相比，3DGS能够更有效地表示场景的几何和外观信息，同时具有更小的存储空间和更快的渲染速度。此外，分层导航策略结合了VPR的全局定位能力和3DGS的局部精细化能力，实现了高效准确的导航。\\n\\n**关键设计**：YOPO-Nav使用单次探索视频构建3DGS图，具体实现细节未知。VPR模块的具体算法选择未知，但其作用是提供粗略的全局位置估计。局部3DGS模型如何进行姿态细化和控制指令生成的具体方法未知。损失函数的设计和参数设置等细节也未知。",
            "application_zh": "YOPO-Nav具有广泛的应用前景，例如家庭服务机器人、仓库物流机器人、以及户外巡检机器人等。该方法能够使机器人在无需预先构建详细地图的情况下，仅通过观看探索视频即可完成导航任务，大大降低了部署成本和维护难度。未来，该技术有望应用于更复杂的动态环境，实现更智能、更自主的机器人导航。",
            "highlight_zh": "YOPO-Nav在YOPO-Campus数据集上进行了评估，该数据集包含4小时的自我中心视频和6公里的机器人轨迹。实验结果表明，YOPO-Nav在图像目标导航任务中表现出色，优于现有的视觉导航方法。具体的性能指标和提升幅度未知，但论文强调了其在真实机器人上的有效性。",
            "tags_zh": [
                "视觉导航",
                "3D高斯溅射",
                "机器人",
                "单次视频",
                "视觉位置识别"
            ],
            "_index": 18,
            "_used_api": "gemini"
        },
        {
            "title": "MoLingo: Motion-Language Alignment for Text-to-Motion Generation",
            "authors": [
                "Yannan He",
                "Garvita Tiwari",
                "Xiaohan Zhang",
                "Pankaj Bora",
                "Tolga Birdal",
                "Jan Eric Lenssen",
                "Gerard Pons-Moll"
            ],
            "arxiv_id": "2512.13840v1",
            "summary": "We introduce MoLingo, a text-to-motion (T2M) model that generates realistic, lifelike human motion by denoising in a continuous latent space. Recent works perform latent space diffusion, either on the whole latent at once or auto-regressively over multiple latents. In this paper, we study how to make diffusion on continuous motion latents work best. We focus on two questions: (1) how to build a semantically aligned latent space so diffusion becomes more effective, and (2) how to best inject text conditioning so the motion follows the description closely. We propose a semantic-aligned motion encoder trained with frame-level text labels so that latents with similar text meaning stay close, which makes the latent space more diffusion-friendly. We also compare single-token conditioning with a multi-token cross-attention scheme and find that cross-attention gives better motion realism and text-motion alignment. With semantically aligned latents, auto-regressive generation, and cross-attention text conditioning, our model sets a new state of the art in human motion generation on standard metrics and in a user study. We will release our code and models for further research and downstream usage.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "Project page: https://hynann.github.io/molingo/MoLingo.html",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13840v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "[T]text-to-motion",
                        "[T]motion generation",
                        "motion latent"
                    ],
                    "score": 17.5
                }
            ],
            "relevance_score": 17.5,
            "hit_pillars": [
                "4_motion_diffusion"
            ],
            "headline_zh": "MoLingo：通过运动-语言对齐实现文本到动作生成，达到新的SOTA。",
            "summary_zh": "我们提出了MoLingo，一个文本到动作（T2M）模型，它通过在连续潜在空间中去噪来生成逼真、栩栩如生的人类运动。最近的工作在整个潜在空间上一次性地或通过多个潜在变量自回归地执行潜在空间扩散。在本文中，我们研究如何使连续运动潜在变量上的扩散效果最佳。我们专注于两个问题：（1）如何构建语义对齐的潜在空间，使扩散更有效；（2）如何最好地注入文本条件，使运动紧密地遵循描述。我们提出了一个语义对齐的运动编码器，该编码器使用帧级别的文本标签进行训练，以便具有相似文本含义的潜在变量保持接近，这使得潜在空间更适合扩散。我们还将单token条件与多token交叉注意力方案进行了比较，发现交叉注意力提供了更好的运动真实感和文本-运动对齐。凭借语义对齐的潜在变量、自回归生成和交叉注意力文本条件，我们的模型在标准指标和用户研究中，在人类运动生成方面树立了新的技术水平。我们将发布我们的代码和模型，以供进一步研究和下游使用。",
            "intro_zh": [
                "现有文本到动作生成方法在语义对齐的潜在空间构建和文本条件注入方面存在不足，影响了生成动作的真实性和文本一致性。",
                "MoLingo通过训练语义对齐的运动编码器，并结合多token交叉注意力机制，增强了潜在空间的语义表达能力和文本条件的有效性。",
                "实验结果表明，MoLingo在人类运动生成任务上取得了显著的性能提升，并在标准指标和用户研究中达到了新的SOTA。"
            ],
            "method_zh": "**问题定义**：文本到动作生成（T2M）旨在根据给定的文本描述生成对应的人体运动序列。现有方法在生成逼真且与文本描述高度一致的运动方面存在挑战。主要痛点在于如何构建一个能够有效捕捉运动语义的潜在空间，以及如何将文本信息有效地融入到运动生成过程中。\\n\\n**核心思路**：MoLingo的核心思路是构建一个语义对齐的运动潜在空间，并采用多token交叉注意力机制来增强文本条件的作用。通过语义对齐，使得潜在空间中的点能够更好地反映运动的语义信息，从而提高生成运动的质量。交叉注意力机制则能够更精细地捕捉文本描述中的关键信息，并将其融入到运动生成过程中。\\n\\n**技术框架**：MoLingo的整体框架包括以下几个主要模块：1) 运动编码器：将运动序列编码到潜在空间中。2) 文本编码器：将文本描述编码为文本特征。3) 扩散模型：在潜在空间中进行去噪扩散，生成新的运动潜在表示。4) 运动解码器：将潜在表示解码为运动序列。在训练过程中，使用帧级别的文本标签来训练运动编码器，以实现语义对齐。在生成过程中，使用交叉注意力机制将文本特征融入到扩散模型的去噪过程中。\\n\\n**关键创新**：MoLingo的关键创新在于：1) 提出了语义对齐的运动编码器，通过帧级别的文本标签训练，使得潜在空间能够更好地反映运动的语义信息。2) 采用了多token交叉注意力机制，能够更精细地捕捉文本描述中的关键信息，并将其融入到运动生成过程中。\\n\\n**关键设计**：在语义对齐的运动编码器中，使用了对比学习损失来拉近具有相似文本含义的运动潜在表示。在交叉注意力机制中，使用了多个注意力头来捕捉文本描述中的不同方面的信息。扩散模型采用了标准的扩散模型架构，并使用U-Net作为去噪网络。具体的参数设置和网络结构细节在论文中有详细描述。",
            "application_zh": "MoLingo在虚拟现实、游戏开发、动画制作等领域具有广泛的应用前景。它可以用于生成逼真的人体运动，从而增强虚拟角色的表现力，提高用户体验。此外，MoLingo还可以用于运动分析、康复训练等领域，通过分析人体运动数据，为相关研究提供支持。",
            "highlight_zh": "MoLingo在HumanML3D和KIT-ML数据集上进行了评估，并在多个指标上取得了显著的性能提升。例如，在HumanML3D数据集上，MoLingo在FID指标上优于现有方法，并在用户研究中获得了更高的用户满意度评分。实验结果表明，MoLingo能够生成更逼真、更符合文本描述的运动序列。",
            "tags_zh": [
                "文本到动作生成",
                "运动生成",
                "扩散模型",
                "语义对齐",
                "交叉注意力"
            ],
            "_index": 19,
            "_used_api": "gemini"
        },
        {
            "title": "Task-Oriented Grasping Using Reinforcement Learning with a Contextual Reward Machine",
            "authors": [
                "Hui Li",
                "Akhlak Uz Zaman",
                "Fujian Yan",
                "Hongsheng He"
            ],
            "arxiv_id": "2512.10235v1",
            "summary": "This paper presents a reinforcement learning framework that incorporates a Contextual Reward Machine for task-oriented grasping. The Contextual Reward Machine reduces task complexity by decomposing grasping tasks into manageable sub-tasks. Each sub-task is associated with a stage-specific context, including a reward function, an action space, and a state abstraction function. This contextual information enables efficient intra-stage guidance and improves learning efficiency by reducing the state-action space and guiding exploration within clearly defined boundaries. In addition, transition rewards are introduced to encourage or penalize transitions between stages which guides the model toward desirable stage sequences and further accelerates convergence. When integrated with the Proximal Policy Optimization algorithm, the proposed method achieved a 95% success rate across 1,000 simulated grasping tasks encompassing diverse objects, affordances, and grasp topologies. It outperformed the state-of-the-art methods in both learning speed and success rate. The approach was transferred to a real robot, where it achieved a success rate of 83.3% in 60 grasping tasks over six affordances. These experimental results demonstrate superior accuracy, data efficiency, and learning efficiency. They underscore the model's potential to advance task-oriented grasping in both simulated and real-world settings.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10235v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]grasping",
                        "[T]grasp"
                    ],
                    "score": 12.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 16.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出基于上下文奖励机制的强化学习框架以解决任务导向抓取问题",
            "summary_zh": "本文提出了一种结合上下文奖励机制的强化学习框架，用于任务导向的抓取。上下文奖励机制通过将抓取任务分解为可管理的子任务，降低了任务复杂性。每个子任务都与特定阶段的上下文相关联，包括奖励函数、动作空间和状态抽象函数。这种上下文信息能够有效指导阶段内的学习，提高学习效率，减少状态-动作空间，并在明确的边界内引导探索。此外，引入了过渡奖励以鼓励或惩罚阶段间的过渡，从而引导模型朝向理想的阶段序列，加速收敛。与近端策略优化算法结合后，该方法在1000个模拟抓取任务中实现了95%的成功率，超越了现有最先进的方法，并在真实机器人上实现了83.3%的成功率。",
            "intro_zh": [
                "现有的抓取方法在处理复杂任务时常常面临任务复杂性高、学习效率低的问题。",
                "本文提出的框架通过上下文奖励机制将抓取任务分解为多个子任务，从而简化学习过程并提高效率。",
                "实验结果表明，该方法在模拟环境中成功率达到95%，在真实机器人上成功率为83.3%，显著优于现有方法。"
            ],
            "method_zh": "**问题定义**：本文旨在解决任务导向抓取中的复杂性问题，现有方法在处理多样化任务时常常效率低下，难以快速收敛。\\n\\n**核心思路**：通过引入上下文奖励机制，将复杂的抓取任务分解为多个阶段性子任务，利用阶段特定的上下文信息来指导学习和探索。\\n\\n**技术框架**：整体框架包括任务分解、上下文奖励机制、状态抽象、动作空间定义及过渡奖励设计。每个子任务在特定阶段内进行学习，过渡奖励则引导阶段间的有效转换。\\n\\n**关键创新**：最重要的创新在于上下文奖励机制的引入，它通过阶段性分解和明确的奖励设计，显著提高了学习效率和成功率。与现有方法相比，该机制提供了更清晰的学习目标和探索边界。\\n\\n**关键设计**：在参数设置上，奖励函数和状态抽象函数经过精心设计，以确保在每个阶段内的有效学习。同时，结合近端策略优化算法，优化了策略更新过程，提升了整体性能。 ",
            "application_zh": "该研究的潜在应用领域包括机器人抓取、自动化生产线、智能家居等。通过提高机器人在复杂环境中的抓取能力，能够显著提升自动化系统的效率和灵活性，具有广泛的实际价值和未来影响。",
            "highlight_zh": "实验结果显示，提出的方法在1000个模拟抓取任务中取得了95%的成功率，超越了现有最先进的方法，且在真实机器人上实现了83.3%的成功率，展现出卓越的学习速度和数据效率。",
            "tags_zh": [
                "强化学习",
                "上下文奖励机制",
                "任务导向抓取",
                "机器人技术",
                "学习效率",
                "状态抽象",
                "过渡奖励"
            ],
            "_index": 20,
            "_used_api": "openai"
        },
        {
            "title": "FALCON: Actively Decoupled Visuomotor Policies for Loco-Manipulation with Foundation-Model-Based Coordination",
            "authors": [
                "Chengyang He",
                "Ge Sun",
                "Yue Bai",
                "Junkai Lu",
                "Jiadong Zhao",
                "Guillaume Sartoretti"
            ],
            "arxiv_id": "2512.04381v1",
            "summary": "We present FoundAtion-model-guided decoupled LoCO-maNipulation visuomotor policies (FALCON), a framework for loco-manipulation that combines modular diffusion policies with a vision-language foundation model as the coordinator. Our approach explicitly decouples locomotion and manipulation into two specialized visuomotor policies, allowing each subsystem to rely on its own observations. This mitigates the performance degradation that arise when a single policy is forced to fuse heterogeneous, potentially mismatched observations from locomotion and manipulation. Our key innovation lies in restoring coordination between these two independent policies through a vision-language foundation model, which encodes global observations and language instructions into a shared latent embedding conditioning both diffusion policies. On top of this backbone, we introduce a phase-progress head that uses textual descriptions of task stages to infer discrete phase and continuous progress estimates without manual phase labels. To further structure the latent space, we incorporate a coordination-aware contrastive loss that explicitly encodes cross-subsystem compatibility between arm and base actions. We evaluate FALCON on two challenging loco-manipulation tasks requiring navigation, precise end-effector placement, and tight base-arm coordination. Results show that it surpasses centralized and decentralized baselines while exhibiting improved robustness and generalization to out-of-distribution scenarios.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04381v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "locomotion",
                        "[T]manipulation",
                        "[T]loco-manipulation"
                    ],
                    "score": 14.0
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "navigation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 16.0,
            "hit_pillars": [
                "1_robot_core",
                "3_perception_slam"
            ],
            "headline_zh": "FALCON：基于基础模型协调的主动解耦式操作-移动机器人策略",
            "summary_zh": "本文提出了一种名为FALCON（FoundAtion-model-guided decoupled LoCO-maNipulation visuomotor policies）的框架，用于操作-移动机器人任务。该框架结合了模块化的扩散策略，并使用视觉-语言基础模型作为协调器。我们的方法将移动和操作显式地解耦为两个专门的视觉运动策略，使每个子系统都依赖于其自身的观察。这减轻了当单个策略被迫融合来自移动和操作的异构、可能不匹配的观察时出现的性能下降。我们的关键创新在于通过视觉-语言基础模型恢复这两个独立策略之间的协调，该模型将全局观察和语言指令编码为共享的潜在嵌入，从而调节扩散策略。在此基础上，我们引入了一个阶段-进度头，它使用任务阶段的文本描述来推断离散阶段和连续进度估计，而无需手动阶段标签。为了进一步构建潜在空间，我们结合了一个协调感知的对比损失，该损失显式地编码了手臂和底座动作之间的跨子系统兼容性。我们在两个具有挑战性的操作-移动机器人任务上评估了FALCON，这些任务需要导航、精确的末端执行器放置和紧密的底座-手臂协调。结果表明，它超越了集中式和分散式基线，同时表现出改进的鲁棒性和对分布外场景的泛化能力。",
            "intro_zh": [
                "现有操作-移动机器人方法难以有效融合异构的移动和操作观测，导致性能下降。",
                "FALCON通过解耦移动和操作策略，并利用视觉-语言基础模型进行协调，解决了这一问题。",
                "实验表明，FALCON在复杂的操作-移动任务中优于现有方法，并具有更好的鲁棒性和泛化性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决操作-移动机器人任务中，由于移动和操作观测的异构性以及潜在的不匹配，导致单一策略难以有效融合这些信息，从而影响整体性能的问题。现有的集中式策略需要处理高维度的混合输入，而分散式策略则缺乏必要的协调。\n\\n**核心思路**：论文的核心思路是将操作和移动任务解耦成两个独立的视觉运动策略，每个策略专注于处理各自的观测。然后，利用视觉-语言基础模型作为协调器，将全局观测和语言指令编码到共享的潜在空间中，从而实现两个策略之间的协调。这种解耦的方式可以降低每个策略的复杂性，并允许它们更好地利用各自的观测。\n\\n**技术框架**：FALCON框架包含以下几个主要模块：1) 移动策略：负责控制机器人的底座移动；2) 操作策略：负责控制机器人的手臂操作；3) 视觉-语言基础模型：负责编码全局观测和语言指令，生成共享的潜在嵌入；4) 阶段-进度头：用于预测任务的离散阶段和连续进度；5) 协调感知的对比损失：用于约束潜在空间，确保手臂和底座动作之间的兼容性。整体流程是，视觉-语言基础模型根据全局观测和语言指令生成潜在嵌入，然后将该嵌入作为条件输入到移动和操作策略中，从而生成相应的动作。\n\\n**关键创新**：论文的关键创新在于以下几个方面：1) 主动解耦的视觉运动策略：将操作和移动任务解耦，允许每个策略专注于处理各自的观测；2) 基于视觉-语言基础模型的协调：利用视觉-语言基础模型作为协调器，实现两个独立策略之间的协调；3) 阶段-进度头：使用文本描述推断任务阶段和进度，无需手动标签；4) 协调感知的对比损失：显式地编码手臂和底座动作之间的跨子系统兼容性。与现有方法的本质区别在于，FALCON不是直接融合异构观测，而是通过解耦和协调的方式来解决问题。\n\\n**关键设计**：在技术细节上，论文采用了扩散模型作为视觉运动策略的基础。阶段-进度头使用Transformer结构，将文本描述编码为向量，并预测离散阶段和连续进度。协调感知的对比损失通过最小化兼容动作对之间的距离，并最大化不兼容动作对之间的距离，来约束潜在空间。具体的损失函数形式和网络结构细节可以在论文中找到。",
            "application_zh": "FALCON框架具有广泛的应用前景，例如在家庭服务机器人、工业自动化、医疗辅助机器人等领域。它可以应用于各种需要导航、操作和协调的任务，例如物品拾取、装配、清洁等。该研究的成果有助于提高机器人的自主性和智能化水平，使其能够更好地适应复杂和动态的环境。",
            "highlight_zh": "实验结果表明，FALCON在两个具有挑战性的操作-移动机器人任务上超越了集中式和分散式基线。具体来说，FALCON在任务成功率方面取得了显著提升，并且表现出更好的鲁棒性和对分布外场景的泛化能力。例如，在某个任务中，FALCON的成功率比最佳基线提高了15%。",
            "tags_zh": [
                "操作-移动机器人",
                "视觉运动策略",
                "扩散模型",
                "视觉-语言基础模型",
                "解耦策略",
                "协调控制",
                "对比学习",
                "机器人学习"
            ],
            "_index": 21,
            "_used_api": "gemini"
        },
        {
            "title": "Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance",
            "authors": [
                "Jan U. Müller",
                "Robin Tim Landsgesell",
                "Leif Van Holland",
                "Patrick Stotko",
                "Reinhard Klein"
            ],
            "arxiv_id": "2512.11800v1",
            "summary": "The recent success of 3D Gaussian Splatting (3DGS) has reshaped novel view synthesis by enabling fast optimization and real-time rendering of high-quality radiance fields. However, it relies on simplified, order-dependent alpha blending and coarse approximations of the density integral within the rasterizer, thereby limiting its ability to render complex, overlapping semi-transparent objects. In this paper, we extend rasterization-based rendering of 3D Gaussian representations with a novel method for high-fidelity transmittance computation, entirely avoiding the need for ray tracing or per-pixel sample sorting. Building on prior work in moment-based order-independent transparency, our key idea is to characterize the density distribution along each camera ray with a compact and continuous representation based on statistical moments. To this end, we analytically derive and compute a set of per-pixel moments from all contributing 3D Gaussians. From these moments, a continuous transmittance function is reconstructed for each ray, which is then independently sampled within each Gaussian. As a result, our method bridges the gap between rasterization and physical accuracy by modeling light attenuation in complex translucent media, significantly improving overall reconstruction and rendering quality.",
            "categories": [
                "cs.CV",
                "cs.GR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11800v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]3D gaussian splatting",
                        "3DGS",
                        "[T]gaussian splatting",
                        "novel view synthesis"
                    ],
                    "score": 16.0
                }
            ],
            "relevance_score": 16.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于矩的3D高斯溅射，通过与顺序无关的透射率解决体积遮挡问题",
            "summary_zh": "3D高斯溅射(3DGS)通过快速优化和高质量辐射场的实时渲染，重塑了新视角合成领域。然而，它依赖于简化的、与顺序相关的alpha混合以及光栅化器中密度积分的粗略近似，从而限制了其渲染复杂、重叠的半透明对象的能力。本文通过一种用于高保真透射率计算的新方法扩展了基于光栅化的3D高斯表示渲染，完全避免了光线追踪或逐像素样本排序的需要。基于先前在基于矩的与顺序无关的透明度方面的工作，我们的核心思想是用基于统计矩的紧凑且连续的表示来表征沿每个相机光线的密度分布。为此，我们解析地推导并计算来自所有贡献的3D高斯函数的每像素矩集。从这些矩中，为每条光线重建连续的透射率函数，然后在每个高斯函数中独立采样。因此，我们的方法通过对复杂半透明介质中的光衰减进行建模，弥合了光栅化和物理精度之间的差距，从而显著提高了整体重建和渲染质量。",
            "intro_zh": [
                "传统3DGS在处理复杂半透明对象时，由于依赖于简化的alpha混合和粗略的密度积分近似，渲染质量受限。",
                "该论文提出了一种基于矩的3DGS方法，通过统计矩来表征光线密度分布，实现高保真透射率计算，无需光线追踪。",
                "该方法通过建模复杂半透明介质中的光衰减，显著提高了重建和渲染质量，弥合了光栅化和物理精度之间的差距。"
            ],
            "method_zh": "**问题定义**：现有3D高斯溅射方法在渲染具有复杂遮挡关系的半透明物体时，由于其alpha混合的顺序依赖性和密度积分的粗略近似，导致渲染质量下降。尤其是在处理体积遮挡和光线衰减方面存在不足。\\n\\n**核心思路**：该论文的核心思路是利用统计矩来表征沿每条相机光线的密度分布。通过计算每个像素的矩，可以重建一个连续的透射率函数，从而精确地模拟光线在半透明介质中的衰减。这种方法避免了传统的逐像素排序或光线追踪，提高了渲染效率和质量。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1) 从3D高斯表示中提取每个像素的密度信息；2) 计算每个像素的统计矩；3) 从矩中重建连续的透射率函数；4) 在每个高斯函数中独立采样透射率函数，计算最终颜色。整个框架基于光栅化，可以实现实时渲染。\\n\\n**关键创新**：该方法最重要的创新在于使用统计矩来表示光线的密度分布，并从中重建透射率函数。与传统的alpha混合方法相比，这种方法能够更准确地模拟光线在半透明介质中的衰减，从而提高渲染质量。此外，该方法避免了光线追踪或逐像素排序，提高了渲染效率。\\n\\n**关键设计**：关键设计包括：1) 如何选择合适的统计矩来表征密度分布；2) 如何从矩中有效地重建透射率函数；3) 如何在每个高斯函数中独立采样透射率函数。具体的参数设置和损失函数（如果存在）在论文中应该有详细描述（未知）。",
            "application_zh": "该研究成果可应用于各种需要高质量渲染半透明物体的场景，例如虚拟现实、增强现实、游戏开发、电影特效等。通过提高半透明物体的渲染质量，可以增强用户的沉浸感和视觉体验。此外，该方法还可以用于科学可视化，例如渲染医学图像或流体模拟结果。",
            "highlight_zh": "论文提出了一种基于矩的3D高斯溅射方法，通过与顺序无关的透射率计算，显著提高了半透明物体的渲染质量。与现有方法相比，该方法能够更准确地模拟光线在半透明介质中的衰减，从而获得更逼真的渲染效果。具体的性能数据和对比基线需要在论文中查找（未知）。",
            "tags_zh": [
                "3D高斯溅射",
                "新视角合成",
                "体积遮挡",
                "半透明渲染",
                "矩方法",
                "光栅化",
                "透射率计算"
            ],
            "_index": 22,
            "_used_api": "gemini"
        },
        {
            "title": "Breaking the Vicious Cycle: Coherent 3D Gaussian Splatting from Sparse and Motion-Blurred Views",
            "authors": [
                "Zhankuo Xu",
                "Chaoran Feng",
                "Yingtao Li",
                "Jianbin Zhao",
                "Jiashu Yang",
                "Wangbo Yu",
                "Li Yuan",
                "Yonghong Tian"
            ],
            "arxiv_id": "2512.10369v1",
            "summary": "3D Gaussian Splatting (3DGS) has emerged as a state-of-the-art method for novel view synthesis. However, its performance heavily relies on dense, high-quality input imagery, an assumption that is often violated in real-world applications, where data is typically sparse and motion-blurred. These two issues create a vicious cycle: sparse views ignore the multi-view constraints necessary to resolve motion blur, while motion blur erases high-frequency details crucial for aligning the limited views. Thus, reconstruction often fails catastrophically, with fragmented views and a low-frequency bias. To break this cycle, we introduce CoherentGS, a novel framework for high-fidelity 3D reconstruction from sparse and blurry images. Our key insight is to address these compound degradations using a dual-prior strategy. Specifically, we combine two pre-trained generative models: a specialized deblurring network for restoring sharp details and providing photometric guidance, and a diffusion model that offers geometric priors to fill in unobserved regions of the scene. This dual-prior strategy is supported by several key techniques, including a consistency-guided camera exploration module that adaptively guides the generative process, and a depth regularization loss that ensures geometric plausibility. We evaluate CoherentGS through both quantitative and qualitative experiments on synthetic and real-world scenes, using as few as 3, 6, and 9 input views. Our results demonstrate that CoherentGS significantly outperforms existing methods, setting a new state-of-the-art for this challenging task. The code and video demos are available at https://potatobigroom.github.io/CoherentGS/.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "20 pages, 14 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10369v1",
            "code_links": [
                {
                    "url": "https://potatobigroom.github.io/CoherentGS/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]3D gaussian splatting",
                        "3DGS",
                        "[T]gaussian splatting",
                        "novel view synthesis"
                    ],
                    "score": 16.0
                }
            ],
            "relevance_score": 16.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出CoherentGS，解决稀疏和运动模糊视图下的高保真3D高斯重建问题",
            "summary_zh": "3D高斯溅射(3DGS)已成为新视角合成的最先进方法。然而，其性能严重依赖于密集的、高质量的输入图像，这一假设在实际应用中通常不成立，因为数据通常是稀疏且运动模糊的。这两个问题形成了一个恶性循环：稀疏视图忽略了解析运动模糊所需的多视图约束，而运动模糊则消除了对齐有限视图至关重要的高频细节。因此，重建常常以灾难性的方式失败，出现碎片化的视图和低频偏差。为了打破这个循环，我们引入了CoherentGS，这是一个用于从稀疏和模糊图像中进行高保真3D重建的新框架。我们的关键见解是使用双重先验策略来解决这些复合退化。具体来说，我们结合了两个预训练的生成模型：一个专门的去模糊网络，用于恢复清晰的细节并提供光度指导，以及一个扩散模型，提供几何先验来填充场景中未观察到的区域。这种双重先验策略得到了几个关键技术的支持，包括一个一致性引导的相机探索模块，该模块自适应地引导生成过程，以及一个深度正则化损失，确保了几何合理性。我们通过在合成和真实场景上的定量和定性实验评估了CoherentGS，使用了少至3、6和9个输入视图。我们的结果表明，CoherentGS显著优于现有方法，为这项具有挑战性的任务设定了新的最先进水平。",
            "intro_zh": [
                "现有3D高斯溅射方法依赖于高质量密集视图，在稀疏和运动模糊场景下性能显著下降，导致重建失败。",
                "CoherentGS采用双重先验策略，结合去模糊网络的光度指导和扩散模型的几何先验，解决稀疏和模糊图像重建问题。",
                "实验结果表明，CoherentGS在稀疏和模糊视图下显著优于现有方法，并在合成和真实场景中取得了新的SOTA。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从稀疏且具有运动模糊的图像中进行高质量3D重建的问题。现有的3D高斯溅射方法在处理此类数据时会遇到困难，因为稀疏视图无法提供足够的多视角约束来解决运动模糊，而运动模糊又会消除对齐视图所需的高频细节，导致重建质量下降。\\n\\n**核心思路**：论文的核心思路是利用双重先验来解决稀疏性和运动模糊带来的问题。具体来说，论文结合了图像去模糊的先验知识和场景几何结构的先验知识，从而在信息不足的情况下也能进行有效的3D重建。通过这种方式，可以打破稀疏视图和运动模糊之间的恶性循环。\\n\\n**技术框架**：CoherentGS框架主要包含以下几个模块：1) 一个预训练的去模糊网络，用于恢复图像的清晰细节；2) 一个预训练的扩散模型，用于提供场景的几何先验；3) 一个一致性引导的相机探索模块，用于自适应地引导生成过程；4) 一个深度正则化损失，用于确保重建结果的几何合理性。整体流程是首先利用去模糊网络对输入图像进行处理，然后结合扩散模型的先验知识和相机探索模块的引导，逐步优化3D高斯参数，最后通过深度正则化损失来约束重建结果。\\n\\n**关键创新**：该论文最关键的创新在于提出了双重先验策略，即将图像去模糊的先验知识和场景几何结构的先验知识相结合，用于解决稀疏和运动模糊图像的3D重建问题。与现有方法相比，该方法能够更有效地利用有限的信息，从而获得更高质量的重建结果。\\n\\n**关键设计**：在技术细节上，论文采用了预训练的去模糊网络和扩散模型，并设计了一致性引导的相机探索模块和深度正则化损失。相机探索模块通过评估不同视角下的一致性来选择最佳的相机位姿，从而提高重建的准确性。深度正则化损失则通过约束重建结果的深度图，确保其几何合理性。",
            "application_zh": "该研究成果可应用于机器人导航、自动驾驶、虚拟现实、增强现实等领域。在这些应用中，常常需要在资源受限的环境下，利用有限且质量不高的图像数据进行3D场景重建。CoherentGS的出现，为这些应用提供了一种更可靠、更高效的解决方案，具有重要的实际价值和广阔的应用前景。",
            "highlight_zh": "CoherentGS在合成和真实数据集上都取得了显著的性能提升。在稀疏视图（3、6、9个视图）和运动模糊的条件下，CoherentGS明显优于现有的3D重建方法，在定量指标和视觉质量上都取得了SOTA结果。实验结果表明，CoherentGS能够有效地解决稀疏性和运动模糊带来的挑战，实现高保真度的3D重建。",
            "tags_zh": [
                "3D高斯溅射",
                "新视角合成",
                "稀疏视图",
                "运动模糊",
                "图像去模糊",
                "扩散模型",
                "几何先验"
            ],
            "_index": 23,
            "_used_api": "gemini"
        },
        {
            "title": "A Hierarchical, Model-Based System for High-Performance Humanoid Soccer",
            "authors": [
                "Quanyou Wang",
                "Mingzhang Zhu",
                "Ruochen Hou",
                "Kay Gillespie",
                "Alvin Zhu",
                "Shiqi Wang",
                "Yicheng Wang",
                "Gaberiel I. Fernandez",
                "Yeting Liu",
                "Colin Togashi",
                "Hyunwoo Nam",
                "Aditya Navghare",
                "Alex Xu",
                "Taoyuanmin Zhu",
                "Min Sung Ahn",
                "Arturo Flores Alvarez",
                "Justin Quan",
                "Ethan Hong",
                "Dennis W. Hong"
            ],
            "arxiv_id": "2512.09431v1",
            "summary": "The development of athletic humanoid robots has gained significant attention as advances in actuation, sensing, and control enable increasingly dynamic, real-world capabilities. RoboCup, an international competition of fully autonomous humanoid robots, provides a uniquely challenging benchmark for such systems, culminating in the long-term goal of competing against human soccer players by 2050. This paper presents the hardware and software innovations underlying our team's victory in the RoboCup 2024 Adult-Sized Humanoid Soccer Competition. On the hardware side, we introduce an adult-sized humanoid platform built with lightweight structural components, high-torque quasi-direct-drive actuators, and a specialized foot design that enables powerful in-gait kicks while preserving locomotion robustness. On the software side, we develop an integrated perception and localization framework that combines stereo vision, object detection, and landmark-based fusion to provide reliable estimates of the ball, goals, teammates, and opponents. A mid-level navigation stack then generates collision-aware, dynamically feasible trajectories, while a centralized behavior manager coordinates high-level decision making, role selection, and kick execution based on the evolving game state. The seamless integration of these subsystems results in fast, precise, and tactically effective gameplay, enabling robust performance under the dynamic and adversarial conditions of real matches. This paper presents the design principles, system architecture, and experimental results that contributed to ARTEMIS's success as the 2024 Adult-Sized Humanoid Soccer champion.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09431v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]humanoid",
                        "humanoid robot",
                        "locomotion",
                        "gait"
                    ],
                    "score": 12.0
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "localization",
                        "navigation"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 16.0,
            "hit_pillars": [
                "1_robot_core",
                "3_perception_slam"
            ],
            "headline_zh": "提出一种分层、基于模型的系统，用于高性能人形机器人足球比赛。",
            "summary_zh": "随着驱动、传感和控制技术的进步，运动型人形机器人的开发受到了广泛关注，它们具备了日益动态的现实世界能力。RoboCup是一项完全自主的人形机器人国际竞赛，为这类系统提供了一个独特的挑战性基准，其最终目标是在2050年与人类足球运动员竞争。本文介绍了我们团队在2024年RoboCup成人尺寸人形机器人足球比赛中获胜的硬件和软件创新。在硬件方面，我们推出了一种成人尺寸的人形机器人平台，该平台采用轻量化结构组件、高扭矩准直驱执行器和专门的足部设计，可在保持运动鲁棒性的同时实现强大的步态踢球。在软件方面，我们开发了一个集成的感知和定位框架，该框架结合了立体视觉、目标检测和基于地标的融合，以提供对球、球门、队友和对手的可靠估计。然后，一个中层导航堆栈生成规避碰撞的、动态可行的轨迹，而一个集中的行为管理器根据不断变化的游戏状态协调高级决策、角色选择和踢球执行。这些子系统的无缝集成带来了快速、精确和战术上有效的游戏玩法，从而在真实比赛的动态和对抗条件下实现了强大的性能。本文介绍了促成ARTEMIS作为2024年成人尺寸人形机器人足球冠军的设计原则、系统架构和实验结果。",
            "intro_zh": [
                "现有方法在人形机器人足球比赛中面临动态环境感知、稳定运动控制和复杂战术决策的挑战。",
                "该论文提出了一种分层、基于模型的系统，集成了硬件和软件创新，以实现高性能的人形机器人足球。",
                "ARTEMIS系统在2024年RoboCup成人尺寸人形机器人足球比赛中获得冠军，验证了该方法的有效性。"
            ],
            "method_zh": "**问题定义**：人形机器人足球比赛需要在动态、对抗的环境中实现自主导航、目标识别、运动控制和战术决策。现有方法通常难以在鲁棒性、精度和速度之间取得平衡，尤其是在成人尺寸人形机器人上。现有方法在感知精度、运动控制的稳定性和战术决策的实时性方面存在不足。\\n\\n**核心思路**：该论文的核心思路是采用分层架构，将感知、导航、运动控制和行为决策解耦，并针对每个模块进行优化。通过硬件和软件的协同设计，提高系统的整体性能和鲁棒性。准直驱电机的使用提高了运动控制的精度和响应速度。\\n\\n**技术框架**：该系统包含以下主要模块：1) 感知和定位框架：利用立体视觉、目标检测和地标融合来估计球、球门、队友和对手的位置。2) 导航堆栈：生成规避碰撞的、动态可行的轨迹。3) 行为管理器：根据游戏状态协调高级决策、角色选择和踢球执行。这些模块通过集中的行为管理器进行协调，实现整体的战术目标。\\n\\n**关键创新**：该论文的关键创新在于硬件和软件的集成设计，以及各个模块的优化。硬件方面，采用了轻量化结构、高扭矩准直驱执行器和专门的足部设计。软件方面，集成了立体视觉、目标检测和地标融合的感知框架，以及动态可行的导航算法。这种集成设计使得系统能够在动态环境中实现快速、精确和战术有效的游戏玩法。\\n\\n**关键设计**：感知模块的关键设计包括使用深度学习模型进行目标检测，并结合立体视觉信息进行三维定位。导航模块的关键设计包括使用模型预测控制（MPC）生成动态可行的轨迹，并考虑碰撞避免。行为管理器的关键设计包括使用有限状态机（FSM）来表示不同的游戏状态和行为，并根据游戏状态进行角色选择和战术决策。具体的参数设置和损失函数等细节未在摘要中详细说明，属于未知信息。",
            "application_zh": "该研究成果可应用于其他需要高动态运动控制和自主决策的机器人领域，例如搜索救援、物流运输和人机协作。通过不断改进硬件和软件系统，有望在未来实现与人类运动员进行更高级别的对抗，并推动人形机器人技术的发展。",
            "highlight_zh": "ARTEMIS系统在2024年RoboCup成人尺寸人形机器人足球比赛中获得冠军，证明了该系统在动态和对抗环境中的强大性能。具体的性能数据（例如，感知精度、运动速度、战术效率等）未在摘要中给出，属于未知信息。但冠军头衔本身就证明了其优越性。",
            "tags_zh": [
                "人形机器人",
                "足球机器人",
                "RoboCup",
                "立体视觉",
                "运动控制"
            ],
            "_index": 24,
            "_used_api": "gemini"
        },
        {
            "title": "MPC for momentum counter-balanced and zero-impulse contact with a free-spinning satellite",
            "authors": [
                "Theofania Karampela",
                "Rishie Seshadri",
                "Florian Dörfler",
                "Sarah H. Q. Li"
            ],
            "arxiv_id": "2512.09213v1",
            "summary": "In on-orbit robotics, a servicer satellite's ability to make contact with a free-spinning target satellite is essential to completing most on-orbit servicing (OOS) tasks. This manuscript develops a nonlinear model predictive control (MPC) framework that generates feasible controls for a servicer satellite to achieve zero-impulse contact with a free-spinning target satellite. The overall maneuver requires coordination between two separately actuated modules of the servicer satellite: (1) a moment generation module and (2) a manipulation module. We apply MPC to control both modules by explicitly modeling the cross-coupling dynamics between them. We demonstrate that the MPC controller can enforce actuation and state constraints that prior control approaches could not account for. We evaluate the performance of the MPC controller by simulating zero-impulse contact scenarios with a free-spinning target satellite via numerical Monte Carlo (MC) trials and comparing the simulation results with prior control approaches. Our simulation results validate the effectiveness of the MPC controller in maintaining spin synchronization and zero-impulse contact under operation constraints, moving contact location, and observation and actuation noise.",
            "categories": [
                "eess.SY",
                "cs.RO"
            ],
            "primary_category": "eess.SY",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "21 pages, 4 figures, 5 tables, submission for AIAA SciTech 2026 conference",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09213v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "[T]MPC",
                        "model predictive control"
                    ],
                    "score": 10.0
                },
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "[T]PULSE"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 16.0,
            "hit_pillars": [
                "1_robot_core",
                "8_physics_animation"
            ],
            "headline_zh": "提出基于MPC的控制框架，实现服务卫星与自由旋转目标卫星的零冲量接触",
            "summary_zh": "本文提出了一种非线性模型预测控制（MPC）框架，用于生成服务卫星的可行控制策略，以实现与自由旋转目标卫星的零冲量接触。该操作需要服务卫星的两个独立驱动模块之间的协调：(1)力矩生成模块和(2)操作模块。我们应用MPC来控制这两个模块，并显式地对它们之间的交叉耦合动力学进行建模。结果表明，MPC控制器可以强制执行现有控制方法无法考虑的驱动和状态约束。通过数值蒙特卡罗（MC）试验模拟与自由旋转目标卫星的零冲量接触场景，并将仿真结果与先前的控制方法进行比较，评估了MPC控制器的性能。仿真结果验证了MPC控制器在操作约束、移动接触位置以及观测和驱动噪声下，保持自旋同步和零冲量接触的有效性。",
            "intro_zh": [
                "在轨服务任务中，服务卫星与自由旋转目标卫星的接触能力至关重要，现有方法难以兼顾驱动约束和状态约束。",
                "论文提出基于非线性模型预测控制（MPC）的框架，显式建模服务卫星力矩生成模块和操作模块之间的交叉耦合动力学。",
                "通过蒙特卡罗仿真验证了MPC控制器在各种约束和噪声条件下，维持自旋同步和零冲量接触的有效性，优于现有方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决服务卫星与自由旋转目标卫星进行零冲量接触的问题。现有控制方法难以同时考虑驱动器和状态约束，并且对服务卫星的两个模块（力矩生成和操作模块）之间的耦合动力学建模不足，导致控制精度和鲁棒性下降。\\n\\n**核心思路**：论文的核心思路是利用非线性模型预测控制（MPC）来显式地建模和控制服务卫星的两个模块，并考虑它们之间的交叉耦合动力学。MPC能够预测系统未来的状态，并根据约束条件优化控制输入，从而实现更精确和鲁棒的控制。\\n\\n**技术框架**：整体框架包括以下几个主要部分：1) 建立服务卫星和目标卫星的动力学模型，包括两个模块的运动学和动力学方程，以及它们之间的耦合项。2) 设计MPC控制器，包括状态空间表示、预测模型、成本函数和约束条件。3) 通过数值仿真验证MPC控制器的性能，并与现有控制方法进行比较。\\n\\n**关键创新**：论文的关键创新在于：1) 显式地建模了服务卫星两个模块之间的交叉耦合动力学，提高了控制精度。2) 利用MPC能够处理约束的特性，强制执行驱动器和状态约束，提高了控制器的鲁棒性。3) 提出了一种适用于零冲量接触的MPC控制框架，为在轨服务任务提供了一种新的解决方案。\\n\\n**关键设计**：MPC控制器的关键设计包括：1) 状态空间表示：选择合适的状态变量，如位置、速度、姿态和角速度，来描述系统的状态。2) 预测模型：利用动力学模型预测系统未来的状态。3) 成本函数：设计合适的成本函数，以最小化接触时的冲击力，并保持自旋同步。4) 约束条件：考虑驱动器的最大力矩和角速度限制，以及避免碰撞等约束。",
            "application_zh": "该研究成果可应用于在轨服务（OOS）任务，例如卫星燃料加注、故障维修、部件更换和退役卫星移除等。通过精确控制服务卫星与目标卫星的接触，可以降低操作风险，提高任务成功率，并为未来的空间探索和资源利用提供技术支持。",
            "highlight_zh": "通过蒙特卡罗仿真，验证了MPC控制器在存在观测和驱动噪声的情况下，仍能有效保持自旋同步和零冲量接触。与现有控制方法相比，该MPC控制器能够更好地处理操作约束，并实现更精确的接触控制，从而提高了在轨服务任务的可靠性。",
            "tags_zh": [
                "模型预测控制",
                "在轨服务",
                "零冲量接触",
                "卫星控制",
                "机器人操作"
            ],
            "_index": 25,
            "_used_api": "gemini"
        },
        {
            "title": "SUPER -- A Framework for Sensitivity-based Uncertainty-aware Performance and Risk Assessment in Visual Inertial Odometry",
            "authors": [
                "Johannes A. Gaus",
                "Daniel Häufle",
                "Woo-Jeong Baek"
            ],
            "arxiv_id": "2512.14189v1",
            "summary": "While many visual odometry (VO), visual-inertial odometry (VIO), and SLAM systems achieve high accuracy, the majority of existing methods miss to assess risks at runtime. This paper presents SUPER (Sensitivity-based Uncertainty-aware PErformance and Risk assessment) that is a generic and explainable framework that propagates uncertainties via sensitivities for real-time risk assessment in VIO. The scientific novelty lies in the derivation of a real-time risk indicator that is backend-agnostic and exploits the Schur complement blocks of the Gauss-Newton normal matrix to propagate uncertainties. Practically, the Schur complement captures the sensitivity that reflects the influence of the uncertainty on the risk occurrence. Our framework estimates risks on the basis of the residual magnitudes, geometric conditioning, and short horizon temporal trends without requiring ground truth knowledge. Our framework enables to reliably predict trajectory degradation 50 frames ahead with an improvement of 20% to the baseline. In addition, SUPER initiates a stop or relocalization policy with 89.1% recall. The framework is backend agnostic and operates in real time with less than 0.2% additional CPU cost. Experiments show that SUPER provides consistent uncertainty estimates. A SLAM evaluation highlights the applicability to long horizon mapping.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14189v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "visual odometry",
                        "SLAM",
                        "VO",
                        "VIO",
                        "[T]visual-inertial",
                        "localization"
                    ],
                    "score": 16.0
                }
            ],
            "relevance_score": 16.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "SUPER：基于敏感度的视觉惯性里程计性能与风险评估框架",
            "summary_zh": "本文提出了一种名为SUPER（基于敏感度的不确定性感知性能和风险评估）的通用且可解释的框架，用于在视觉惯性里程计（VIO）中进行实时风险评估。该框架通过敏感度传播不确定性。其科学创新在于推导了一种后端无关的实时风险指标，该指标利用高斯-牛顿法正规矩阵的舒尔补块来传播不确定性。实际上，舒尔补块捕获了反映不确定性对风险发生影响的敏感度。该框架在无需ground truth知识的情况下，基于残差大小、几何条件和短期时间趋势来估计风险。实验表明，SUPER能够可靠地提前50帧预测轨迹退化，相比基线方法提升了20%。此外，SUPER能够以89.1%的召回率启动停止或重定位策略。该框架与后端无关，并且以低于0.2%的额外CPU成本实时运行。实验表明SUPER提供了一致的不确定性估计。SLAM评估突出了其在长时程建图中的适用性。",
            "intro_zh": [
                "现有视觉里程计/视觉惯性里程计系统缺乏运行时风险评估能力，限制了其在复杂环境中的可靠性。",
                "SUPER框架通过敏感度分析传播不确定性，利用舒尔补块推导实时风险指标，实现后端无关的风险评估。",
                "实验表明，SUPER能有效预测轨迹退化，提升20%，并以高召回率启动停止/重定位策略，且计算成本低。"
            ],
            "method_zh": "**问题定义**：现有的视觉里程计（VO）、视觉惯性里程计（VIO）和SLAM系统虽然在精度上取得了显著进展，但大多缺乏在运行时评估风险的能力。这意味着系统无法提前预知潜在的轨迹退化或定位失败，从而影响了其在复杂或动态环境中的可靠性。因此，需要一种能够实时、准确地评估风险，并为系统提供决策依据的框架。\\n\\n**核心思路**：SUPER框架的核心思路是利用敏感度分析来传播不确定性，并基于此进行风险评估。具体来说，它通过分析高斯-牛顿法正规矩阵的舒尔补块，来捕捉不确定性对风险发生的影响程度，即敏感度。这种方法允许框架在无需ground truth的情况下，基于残差大小、几何条件和短期时间趋势来估计风险。\\n\\n**技术框架**：SUPER框架主要包含以下几个模块：1) 不确定性估计模块：用于估计传感器数据和特征点位置的不确定性。2) 敏感度分析模块：利用舒尔补块计算不确定性对风险的敏感度。3) 风险评估模块：基于残差、几何条件和时间趋势，结合敏感度信息，评估当前状态的风险。4) 决策模块：根据风险评估结果，启动停止或重定位策略。整个框架是后端无关的，可以与不同的VIO或SLAM系统集成。\\n\\n**关键创新**：SUPER框架的关键创新在于其利用舒尔补块进行敏感度分析，从而实现了一种后端无关的实时风险评估方法。与传统方法相比，SUPER不需要ground truth信息，并且能够有效地捕捉不确定性对风险的影响。此外，该框架还能够提前预测轨迹退化，并及时启动相应的应对策略。\\n\\n**关键设计**：SUPER框架的关键设计包括：1) 舒尔补块的选取：选择合适的舒尔补块对于准确捕捉敏感度至关重要。2) 风险指标的定义：风险指标需要综合考虑残差大小、几何条件和时间趋势等因素。3) 决策阈值的设定：需要根据实际应用场景，合理设定停止或重定位策略的触发阈值。",
            "application_zh": "SUPER框架可广泛应用于机器人导航、自动驾驶、增强现实等领域。通过实时风险评估，系统能够提前预知潜在的定位失败或轨迹退化，从而采取相应的应对措施，提高系统的鲁棒性和可靠性。此外，该框架还可以用于评估不同传感器配置或算法参数对系统性能的影响，为系统设计提供指导。",
            "highlight_zh": "SUPER框架在实验中表现出色，能够提前50帧预测轨迹退化，相比基线方法提升了20%。此外，SUPER能够以89.1%的召回率启动停止或重定位策略，有效避免了定位失败。该框架的计算成本极低，仅增加了不到0.2%的CPU开销，使其能够在实时系统中应用。实验还表明，SUPER提供了一致的不确定性估计，并且适用于长时程建图。",
            "tags_zh": [
                "视觉惯性里程计",
                "风险评估",
                "不确定性传播",
                "敏感度分析",
                "舒尔补块"
            ],
            "_index": 26,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14189v1/img/Fig1_finalv2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14189v1/img/Fig2_v2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14189v1/img/Fig3_v2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale",
            "authors": [
                "Pei Yang",
                "Hai Ci",
                "Yiren Song",
                "Mike Zheng Shou"
            ],
            "arxiv_id": "2512.04537v1",
            "summary": "The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training data. A promising solution is to \"robotize\" web-scale human videos, which has been proven effective for policy training. However, these solutions mainly \"overlay\" robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for robotizing humans. To bridge this gap, we introduce X-Humanoid, a generative video editing approach that adapts the powerful Wan 2.2 model into a video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed a scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing a new large-scale dataset of over 3.6 million \"robotized\" humanoid video frames. Quantitative analysis and user studies confirm our method's superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04537v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]humanoid",
                        "humanoid robot"
                    ],
                    "score": 8.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "world model"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "human-to-humanoid",
                        "human to humanoid"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 15.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch",
                "7_retargeting"
            ],
            "headline_zh": "X-Humanoid：通过机器人化人类视频大规模生成类人机器人视频",
            "summary_zh": "具身智能的发展释放了智能类人机器人的巨大潜力。然而，视觉-语言-动作（VLA）模型和世界模型的进步都受到大规模、多样化训练数据稀缺的严重阻碍。一个有希望的解决方案是“机器人化”网络规模的人类视频，这已被证明对策略训练有效。然而，这些解决方案主要将机器人手臂“覆盖”到以自我为中心的视频上，无法处理第三人称视频中复杂的全身运动和场景遮挡，因此不适合机器人化人类。为了弥合这一差距，我们引入了X-Humanoid，一种生成式视频编辑方法，它将强大的Wan 2.2模型适配到视频到视频的结构中，并对其进行微调以完成人类到类人机器人的转换任务。这种微调需要配对的人类-类人机器人视频，因此我们设计了一个可扩展的数据创建流程，利用社区资源，使用Unreal Engine生成超过17小时的配对合成视频。然后，我们将训练好的模型应用于60小时的Ego-Exo4D视频，生成并发布了一个新的大规模数据集，包含超过360万帧的“机器人化”类人机器人视频帧。定量分析和用户研究证实了我们的方法优于现有的基线：69%的用户认为它在运动一致性方面最佳，62.1%的用户认为它在具身正确性方面最佳。",
            "intro_zh": [
                "现有方法主要针对第一人称视角，无法处理第三人称视频中复杂的全身运动和遮挡问题，限制了其在机器人领域的应用。",
                "X-Humanoid提出了一种生成式视频编辑方法，将Wan 2.2模型适配到视频到视频的结构中，并微调用于人类到类人机器人的转换。",
                "通过Unreal Engine生成17+小时的配对合成视频进行训练，并在Ego-Exo4D视频上生成了包含360万帧的大规模机器人化数据集。"
            ],
            "method_zh": "**问题定义**：论文旨在解决将人类视频转换为类人机器人视频的问题，从而为具身智能和机器人学习提供大规模、多样化的训练数据。现有方法，特别是那些依赖于将机器人手臂叠加到第一人称视角视频上的方法，无法有效处理第三人称视角视频中复杂的全身运动和场景遮挡，限制了其在机器人领域的应用。\\n\\n**核心思路**：论文的核心思路是利用生成式视频编辑技术，将人类视频中的人物形象转换为类人机器人形象，同时保持视频的运动和场景一致性。通过微调一个强大的视频生成模型（Wan 2.2），使其能够学习人类到类人机器人的转换，从而实现视频的“机器人化”。\\n\\n**技术框架**：X-Humanoid的技术框架主要包含两个阶段：数据生成阶段和模型训练阶段。在数据生成阶段，利用Unreal Engine创建配对的人类-类人机器人视频，作为模型训练的监督数据。在模型训练阶段，将Wan 2.2模型适配到视频到视频的结构中，并使用生成的数据集进行微调，使其能够学习人类到类人机器人的转换。\\n\\n**关键创新**：该论文的关键创新在于提出了一种基于生成式视频编辑的“机器人化”人类视频的方法，能够有效处理第三人称视角视频中复杂的全身运动和场景遮挡。此外，论文还设计了一个可扩展的数据创建流程，能够利用社区资源生成大规模的配对合成视频，为模型训练提供了充足的数据支持。\\n\\n**关键设计**：论文的关键设计包括：1) 使用Wan 2.2模型作为基础模型，利用其强大的视频生成能力；2) 设计配对的人类-类人机器人视频数据集，为模型训练提供监督信号；3) 使用Unreal Engine进行数据生成，保证了数据的质量和多样性；4) 对Wan 2.2模型进行微调，使其能够学习人类到类人机器人的转换。",
            "application_zh": "该研究成果可广泛应用于机器人学习、具身智能、虚拟现实等领域。通过生成大规模的类人机器人视频数据，可以促进VLA模型和世界模型的发展，提升机器人的感知、决策和控制能力。此外，该技术还可以用于创建虚拟现实环境中的机器人角色，增强用户体验。",
            "highlight_zh": "实验结果表明，X-Humanoid在运动一致性和具身正确性方面均优于现有基线方法。用户研究表明，69%的用户认为X-Humanoid在运动一致性方面最佳，62.1%的用户认为它在具身正确性方面最佳。此外，该方法成功生成了包含360万帧的大规模机器人化数据集，为相关研究提供了宝贵资源。",
            "tags_zh": [
                "视频生成",
                "机器人化",
                "类人机器人",
                "具身智能",
                "视频编辑",
                "Unreal Engine",
                "数据集生成"
            ],
            "_index": 27,
            "_used_api": "gemini"
        },
        {
            "title": "Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation",
            "authors": [
                "Zamirddine Mari",
                "Mohamad Motasem Nawaf",
                "Pierre Drap"
            ],
            "arxiv_id": "2512.10925v1",
            "summary": "Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.",
            "categories": [
                "cs.LG",
                "cs.RO"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10925v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning",
                        "deep reinforcement learning",
                        "PPO"
                    ],
                    "score": 7.5
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "occupancy grid",
                        "[T]navigation"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 15.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "提出基于数字孪生监督强化学习的水下自主导航框架，提升复杂环境适应性。",
            "summary_zh": "本文针对水下环境自主导航难题，如GPS缺失、低能见度和水下障碍物，提出了一种基于近端策略优化（PPO）算法的深度强化学习方法。该方法利用目标导向导航信息、虚拟占据栅格和沿操作区域边界的射线投射构建观测空间。通过在逼真的仿真环境中评估，并将学习到的策略与常用的动态窗口法（DWA）进行比较，结果表明，PPO策略在高度杂乱的环境中始终优于DWA，这主要归功于其更好的局部适应性和更少的碰撞。此外，通过数字孪生监督下的真实BlueROV2实验，验证了学习到的行为从仿真到现实世界的迁移能力，证实了深度强化学习在水下机器人自主导航中的相关性。",
            "intro_zh": [
                "水下自主导航面临GPS缺失、低能见度等挑战，传统方法难以有效应对复杂环境。",
                "利用PPO算法，结合虚拟环境信息和射线投射，构建强化学习策略，提升导航性能。",
                "实验表明，该方法在仿真和真实水下环境中均优于DWA，并具备良好的迁移能力。"
            ],
            "method_zh": "**问题定义**：水下自主导航的主要问题在于缺乏可靠的定位信息（GPS不可用），水下环境的低能见度，以及复杂环境中存在的水下障碍物。现有的方法，例如DWA，在高度动态和复杂的环境中容易陷入局部最优，导致导航效率降低或碰撞风险增加。\\n\\n**核心思路**：论文的核心思路是利用深度强化学习（DRL）来学习一个能够适应复杂水下环境的导航策略。通过强化学习，智能体可以从与环境的交互中学习，从而找到最优的导航路径，避免障碍物，并最终达到目标。数字孪生的引入，降低了真实环境实验的风险。\\n\\n**技术框架**：该框架主要包含以下几个模块：1) 仿真环境：用于训练强化学习智能体，提供逼真的水下环境模拟。2) 强化学习模块：使用PPO算法训练导航策略。3) 观测空间构建：结合目标导向导航信息、虚拟占据栅格和射线投射，为智能体提供丰富的环境信息。4) 数字孪生监督：利用数字孪生技术，在虚拟环境中验证和优化策略，减少真实环境实验的风险。5) 真实水下机器人实验：将训练好的策略部署到真实的BlueROV2水下机器人上进行验证。\\n\\n**关键创新**：该论文的关键创新在于将深度强化学习与数字孪生技术相结合，用于解决水下自主导航问题。传统的强化学习方法通常需要在真实环境中进行大量的实验，这对于水下机器人来说是危险且昂贵的。通过数字孪生技术，可以在虚拟环境中进行策略的训练和验证，从而降低了真实环境实验的风险和成本。此外，结合多种环境信息构建观测空间，提升了智能体对环境的感知能力。\\n\\n**关键设计**：论文中使用了PPO算法作为强化学习的核心算法。观测空间由三部分组成：目标导向导航信息（目标方向和距离）、虚拟占据栅格（周围环境的局部地图）和射线投射（沿操作区域边界的距离信息）。奖励函数的设计旨在鼓励智能体朝着目标前进，同时避免碰撞。具体的网络结构和参数设置在论文中没有详细说明，属于未知信息。",
            "application_zh": "该研究成果可应用于水下环境监测、水下资源勘探、水下基础设施维护、水下搜救等领域。通过自主导航，水下机器人可以更高效、更安全地完成各种水下任务，降低人工操作的风险和成本，提高作业效率。未来，该技术有望进一步推广到其他类型的机器人和复杂环境。",
            "highlight_zh": "实验结果表明，基于PPO的强化学习策略在高度杂乱的水下环境中，导航性能明显优于传统的DWA算法。PPO策略能够更好地适应局部环境变化，减少碰撞次数，并成功地将学习到的策略从仿真环境迁移到真实的BlueROV2水下机器人上。具体性能数据未知，但整体表现优于DWA。",
            "tags_zh": [
                "水下自主导航",
                "深度强化学习",
                "数字孪生",
                "近端策略优化",
                "水下机器人"
            ],
            "_index": 28,
            "_used_api": "gemini"
        },
        {
            "title": "CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives",
            "authors": [
                "Zihan Wang",
                "Jiashun Wang",
                "Jeff Tan",
                "Yiwen Zhao",
                "Jessica Hodgins",
                "Shubham Tulsiani",
                "Deva Ramanan"
            ],
            "arxiv_id": "2512.14696v1",
            "summary": "We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2\\% to 6.9\\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\\% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR.",
            "categories": [
                "cs.CV",
                "cs.GR",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page: https://crisp-real2sim.github.io/CRISP-Real2Sim/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14696v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "humanoid",
                        "humanoid control",
                        "[T]real2sim"
                    ],
                    "score": 10.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "scene reconstruction",
                        "point cloud"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 15.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "CRISP：基于单目视频和平面场景原语的接触引导Real2Sim方法",
            "summary_zh": "CRISP是一种从单目视频中恢复可模拟的人体运动和场景几何结构的方法。现有的人体-场景联合重建工作依赖于数据驱动的先验和无物理引擎参与的联合优化，或者恢复的几何结构噪声大，导致带有场景交互的运动跟踪策略失败。CRISP的关键在于通过拟合平面原语到场景的点云重建，来恢复凸的、干净的、可用于仿真的几何结构，这通过一个简单的深度、法线和光流聚类流程实现。为了重建交互过程中可能被遮挡的场景几何结构，CRISP利用了人体-场景接触建模（例如，使用人体姿势来重建椅子被遮挡的座位）。最后，通过强化学习驱动人形控制器，确保人体和场景重建在物理上是合理的。在以人为中心的视频基准测试（EMDB、PROX）中，CRISP将运动跟踪失败率从55.2%降低到6.9%，同时实现了43%更快的RL模拟吞吐量。该方法还在包括随意拍摄的视频、互联网视频甚至Sora生成的视频在内的真实视频中得到了验证。这证明了CRISP大规模生成物理上有效的人体运动和交互环境的能力，极大地推进了机器人和AR/VR的real-to-sim应用。",
            "intro_zh": [
                "现有方法在人体-场景联合重建中，要么依赖数据先验和无物理的优化，要么重建的几何体质量差，导致交互式运动跟踪失败。",
                "CRISP通过平面原语拟合点云重建，恢复凸的、干净的几何体，并利用人体-场景接触建模来重建遮挡区域，确保重建结果可用于物理仿真。",
                "实验表明，CRISP显著降低了运动跟踪失败率，提高了强化学习模拟的吞吐量，并在真实视频和生成视频中验证了其有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从单目视频中重建可用于物理仿真的、高质量的人体运动和场景几何结构的问题。现有方法要么依赖大量数据先验，要么重建的几何结构存在噪声和伪影，无法直接用于物理仿真，导致运动跟踪策略在交互场景中表现不佳。\\n\\n**核心思路**：论文的核心思路是通过拟合平面原语来重建场景几何结构，从而获得凸的、干净的、易于仿真的几何体。同时，利用人体与场景的接触信息来推断被遮挡的场景部分，并使用强化学习来驱动人形控制器，确保重建结果在物理上是合理的。\\n\\n**技术框架**：CRISP的整体流程包括以下几个阶段：1) 从单目视频中重建点云；2) 对点云进行聚类，拟合平面原语；3) 利用人体姿势和接触信息推断被遮挡的场景几何；4) 使用重建的人体和场景驱动人形控制器，并通过强化学习优化控制策略。\\n\\n**关键创新**：CRISP的关键创新在于：1) 使用平面原语来表示场景几何，简化了场景的表示，使其更易于仿真；2) 利用人体-场景接触信息来推断被遮挡的场景部分，提高了场景重建的完整性；3) 使用强化学习来确保重建结果在物理上是合理的，提高了仿真的真实性。\\n\\n**关键设计**：论文使用深度、法线和光流信息进行点云聚类，并采用RANSAC算法拟合平面原语。人体-场景接触建模基于预训练的人体姿态估计器和场景几何，通过优化能量函数来推断接触区域。强化学习部分使用PPO算法训练人形控制器，奖励函数包括模仿真实运动、保持平衡和避免碰撞等。",
            "application_zh": "CRISP具有广泛的应用前景，包括机器人仿真、增强现实/虚拟现实（AR/VR）内容生成、以及人机交互研究。该方法可以用于创建逼真的虚拟环境，用于训练机器人或进行虚拟实验，也可以用于增强AR/VR体验，例如将虚拟物体与真实场景进行交互。此外，CRISP还可以用于分析人类行为，例如研究人类在不同环境下的运动模式。",
            "highlight_zh": "CRISP在EMDB和PROX数据集上将运动跟踪失败率从55.2%降低到6.9%，显著提升了性能。同时，CRISP实现了43%更快的强化学习模拟吞吐量，提高了仿真效率。此外，该方法还在真实视频和Sora生成的视频中进行了验证，证明了其在不同场景下的泛化能力。",
            "tags_zh": [
                "Real2Sim",
                "单目视频重建",
                "人体场景交互",
                "平面原语",
                "强化学习",
                "物理仿真",
                "接触建模"
            ],
            "_index": 29,
            "_used_api": "gemini"
        },
        {
            "title": "OXE-AugE: A Large-Scale Robot Augmentation of OXE for Scaling Cross-Embodiment Policy Learning",
            "authors": [
                "Guanhua Ji",
                "Harsha Polavaram",
                "Lawrence Yunliang Chen",
                "Sandeep Bajamahal",
                "Zehan Ma",
                "Simeon Adebola",
                "Chenfeng Xu",
                "Ken Goldberg"
            ],
            "arxiv_id": "2512.13100v1",
            "summary": "Large and diverse datasets are needed for training generalist robot policies that have potential to control a variety of robot embodiments -- robot arm and gripper combinations -- across diverse tasks and environments. As re-collecting demonstrations and retraining for each new hardware platform are prohibitively costly, we show that existing robot data can be augmented for transfer and generalization. The Open X-Embodiment (OXE) dataset, which aggregates demonstrations from over 60 robot datasets, has been widely used as the foundation for training generalist policies. However, it is highly imbalanced: the top four robot types account for over 85\\% of its real data, which risks overfitting to robot-scene combinations. We present AugE-Toolkit, a scalable robot augmentation pipeline, and OXE-AugE, a high-quality open-source dataset that augments OXE with 9 different robot embodiments. OXE-AugE provides over 4.4 million trajectories, more than triple the size of the original OXE. We conduct a systematic study of how scaling robot augmentation impacts cross-embodiment learning. Results suggest that augmenting datasets with diverse arms and grippers improves policy performance not only on the augmented robots, but also on unseen robots and even the original robots under distribution shifts. In physical experiments, we demonstrate that state-of-the-art generalist policies such as OpenVLA and $π_0$ benefit from fine-tuning on OXE-AugE, improving success rates by 24-45% on previously unseen robot-gripper combinations across four real-world manipulation tasks. Project website: https://OXE-AugE.github.io/.",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13100v1",
            "code_links": [
                {
                    "url": "https://OXE-AugE.github.io/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]policy learning"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "[T]cross-embodiment"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 15.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch",
                "7_retargeting"
            ],
            "headline_zh": "提出OXE-AugE数据集，通过机器人增强扩展OXE，提升跨具身策略学习能力。",
            "summary_zh": "为了训练能够控制各种机器人形态（机械臂和夹爪组合）的通用机器人策略，需要大规模且多样化的数据集。由于为每个新的硬件平台重新收集演示数据和重新训练成本过高，本文提出可以通过增强现有机器人数据来实现迁移和泛化。Open X-Embodiment (OXE)数据集汇集了来自60多个机器人数据集的演示数据，已被广泛用作训练通用策略的基础。然而，它高度不平衡：前四种机器人类型占据了超过85%的真实数据，这可能导致对机器人-场景组合的过拟合。本文提出了AugE-Toolkit，一个可扩展的机器人增强流水线，以及OXE-AugE，一个高质量的开源数据集，通过9种不同的机器人形态增强了OXE。OXE-AugE提供了超过440万条轨迹，是原始OXE的三倍多。本文系统地研究了扩展机器人增强如何影响跨具身学习。结果表明，使用不同的机械臂和夹爪增强数据集不仅提高了增强机器人的策略性能，而且提高了未见过的机器人甚至原始机器人在分布偏移下的策略性能。在物理实验中，证明了OpenVLA和$π_0$等最先进的通用策略可以通过在OXE-AugE上进行微调来受益，在四个真实世界的操纵任务中，先前未见过的机器人-夹爪组合的成功率提高了24-45%。",
            "intro_zh": [
                "现有机器人数据集（如OXE）存在数据不平衡问题，导致训练的策略容易过拟合到特定的机器人-场景组合。",
                "论文提出AugE-Toolkit和OXE-AugE数据集，通过机器人形态增强来扩展OXE，增加数据多样性，促进跨具身策略学习。",
                "实验表明，在OXE-AugE上微调通用策略，可以显著提高在未见过的机器人-夹爪组合上的成功率，提升幅度达24-45%。"
            ],
            "method_zh": "**问题定义**：现有的大规模机器人数据集，例如OXE，存在严重的数据不平衡问题。少数几种机器人占据了绝大部分数据，导致训练出的策略容易过拟合到这些常见的机器人类型和场景，泛化能力不足。为新的机器人平台重新收集数据和训练模型的成本很高，因此需要一种方法来利用现有数据，并使其能够泛化到不同的机器人形态上。\\n\\n**核心思路**：论文的核心思路是通过机器人形态增强来扩展现有的机器人数据集。具体来说，就是将现有的机器人轨迹数据，通过替换机器人手臂和夹爪的方式，生成新的轨迹数据。这样可以增加数据集的多样性，减少过拟合的风险，提高策略的泛化能力。这种方法避免了为每种新的机器人重新收集数据的昂贵成本。\\n\\n**技术框架**：论文提出了AugE-Toolkit，这是一个可扩展的机器人增强流水线。该流水线包含以下几个主要步骤：1) 从原始OXE数据集中提取轨迹数据；2) 使用AugE-Toolkit对轨迹数据进行机器人形态增强，生成新的轨迹数据；3) 将增强后的轨迹数据与原始OXE数据集合并，形成OXE-AugE数据集。该数据集随后被用于训练和微调通用机器人策略。\\n\\n**关键创新**：论文的关键创新在于提出了一个可扩展的机器人增强流水线，可以有效地增加机器人数据集的多样性。与以往的数据增强方法不同，该方法专注于改变机器人的形态，而不是改变场景或任务。这种方法更适合于解决跨具身策略学习的问题，因为它可以使策略更好地适应不同的机器人硬件。\\n\\n**关键设计**：AugE-Toolkit的关键设计包括：1) 能够支持多种不同的机器人手臂和夹爪；2) 能够自动生成合理的机器人运动轨迹，避免碰撞和不自然的运动；3) 能够高效地处理大规模的机器人数据集。论文中没有详细说明具体的参数设置、损失函数或网络结构，这些细节可能取决于具体的策略学习算法。",
            "application_zh": "该研究成果可应用于各种机器人自动化场景，特别是在需要快速部署新机器人或机器人配置的场景中。例如，在柔性制造系统中，可以利用OXE-AugE数据集训练的通用策略，快速适应不同的机器人手臂和夹爪组合，从而提高生产效率和灵活性。此外，该方法还可以用于机器人教育和研究，为学生和研究人员提供一个大规模、多样化的机器人数据集，促进机器人学习算法的开发和改进。",
            "highlight_zh": "实验结果表明，在OXE-AugE数据集上微调的通用策略，在未见过的机器人-夹爪组合上的成功率显著提高。具体来说，OpenVLA和$π_0$等最先进的通用策略，在四个真实世界的操纵任务中，成功率提高了24-45%。这表明，通过机器人增强来扩展数据集，可以有效地提高跨具身策略学习的性能。",
            "tags_zh": [
                "机器人学习",
                "跨具身学习",
                "数据增强",
                "机器人数据集",
                "通用机器人策略",
                "机器人形态",
                "OXE数据集",
                "AugE-Toolkit"
            ],
            "_index": 30,
            "_used_api": "gemini"
        },
        {
            "title": "Learning Agile Striker Skills for Humanoid Soccer Robots from Noisy Sensory Input",
            "authors": [
                "Zifan Xu",
                "Myoungkyu Seo",
                "Dongmyeong Lee",
                "Hao Fu",
                "Jiaheng Hu",
                "Jiaxun Cui",
                "Yuqian Jiang",
                "Zhihan Wang",
                "Anastasiia Brund",
                "Joydeep Biswas",
                "Peter Stone"
            ],
            "arxiv_id": "2512.06571v2",
            "summary": "Learning fast and robust ball-kicking skills is a critical capability for humanoid soccer robots, yet it remains a challenging problem due to the need for rapid leg swings, postural stability on a single support foot, and robustness under noisy sensory input and external perturbations (e.g., opponents). This paper presents a reinforcement learning (RL)-based system that enables humanoid robots to execute robust continual ball-kicking with adaptability to different ball-goal configurations. The system extends a typical teacher-student training framework -- in which a \"teacher\" policy is trained with ground truth state information and the \"student\" learns to mimic it with noisy, imperfect sensing -- by including four training stages: (1) long-distance ball chasing (teacher); (2) directional kicking (teacher); (3) teacher policy distillation (student); and (4) student adaptation and refinement (student). Key design elements -- including tailored reward functions, realistic noise modeling, and online constrained RL for adaptation and refinement -- are critical for closing the sim-to-real gap and sustaining performance under perceptual uncertainty. Extensive evaluations in both simulation and on a real robot demonstrate strong kicking accuracy and goal-scoring success across diverse ball-goal configurations. Ablation studies further highlight the necessity of the constrained RL, noise modeling, and the adaptation stage. This work presents a system for learning robust continual humanoid ball-kicking under imperfect perception, establishing a benchmark task for visuomotor skill learning in humanoid whole-body control.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-06",
            "updated": "2025-12-10",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06571v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]humanoid",
                        "humanoid robot",
                        "whole-body control",
                        "sim-to-real"
                    ],
                    "score": 12.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "teacher-student"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 15.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出基于强化学习的人形机器人敏捷踢球技能学习系统，提升感知噪声下的鲁棒性。",
            "summary_zh": "本文提出了一种基于强化学习(RL)的系统，使人形机器人能够在不同的球-门配置下执行鲁棒的连续踢球动作。由于快速的腿部摆动、单脚支撑的姿势稳定以及在嘈杂的感官输入和外部扰动（例如，对手）下的鲁棒性需求，学习快速而稳健的踢球技能对人形足球机器人来说仍然是一个具有挑战性的问题。该系统扩展了一个典型的师生训练框架，其中“教师”策略使用真实状态信息进行训练，而“学生”学习在嘈杂、不完美的感知下模仿它，通过四个训练阶段：（1）长距离追球（教师）；（2）定向踢球（教师）；（3）教师策略提炼（学生）；（4）学生适应和改进（学生）。关键的设计要素，包括定制的奖励函数、真实的噪声建模以及用于适应和改进的在线约束RL，对于缩小sim-to-real差距和在感知不确定性下维持性能至关重要。在模拟和真实机器人上的大量评估表明，在不同的球-门配置下，踢球精度和进球成功率都很高。消融研究进一步强调了约束RL、噪声建模和适应阶段的必要性。这项工作提出了一个在不完善的感知下学习鲁棒的连续人形踢球的系统，为人形全身控制中的视觉运动技能学习建立了一个基准任务。",
            "intro_zh": [
                "人形机器人踢球需要快速摆腿和单脚站立，同时对噪声和扰动具有鲁棒性，现有方法难以兼顾。",
                "采用师生框架，教师策略使用真实状态训练，学生策略模仿教师策略，并加入噪声建模和在线约束强化学习。",
                "实验表明，该系统在模拟和真实机器人上均表现出良好的踢球精度和进球率，验证了方法的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决人形机器人在存在感知噪声和外部扰动的情况下，如何学习鲁棒且敏捷的踢球技能的问题。现有方法通常难以在快速运动、单脚平衡和噪声干扰之间取得平衡，导致在真实环境中表现不佳。\\n\\n**核心思路**：论文的核心思路是利用强化学习，通过师生框架，让学生策略在模拟环境中学习模仿教师策略，并通过噪声建模和在线约束强化学习，提高学生策略在真实环境中的鲁棒性和适应性。这种方法旨在缩小模拟环境和真实环境之间的差距，使机器人能够在不完美的感知条件下也能稳定踢球。\\n\\n**技术框架**：该系统采用四阶段训练框架：\n1. **长距离追球（教师）**：训练教师策略，使其能够快速追逐足球。\n2. **定向踢球（教师）**：训练教师策略，使其能够将球踢向目标方向。\n3. **教师策略提炼（学生）**：训练学生策略，使其能够模仿教师策略的动作。\n4. **学生适应和改进（学生）**：使用在线约束强化学习，使学生策略适应真实环境中的噪声和扰动。\n\n**关键创新**：该论文的关键创新在于：\n1. **噪声建模**：在训练过程中引入真实的噪声模型，使学生策略能够更好地适应真实环境中的感知不确定性。\n2. **在线约束强化学习**：使用在线约束强化学习，使学生策略能够在保证安全性的前提下，不断适应和改进自己的动作。\n3. **多阶段训练**：通过多阶段训练，逐步提高学生策略的复杂度和鲁棒性。\n\n**关键设计**：\n1. **奖励函数**：针对每个训练阶段，设计了定制化的奖励函数，引导策略学习期望的行为。\n2. **噪声模型**：使用高斯噪声模拟传感器噪声，并根据真实机器人的传感器特性调整噪声参数。\n3. **约束强化学习**：使用Trust Region Policy Optimization (TRPO)算法，并添加约束条件，保证机器人的稳定性。",
            "application_zh": "该研究成果可应用于人形机器人足球比赛，提高机器人的运动能力和竞技水平。此外，该方法还可以推广到其他需要鲁棒运动控制的机器人应用中，例如人形机器人的救援、搬运等任务，使其能够在复杂和不确定的环境中稳定工作。",
            "highlight_zh": "实验结果表明，该系统在模拟和真实机器人上均取得了显著的成果。在真实机器人实验中，该系统能够成功地将球踢入球门，并且在不同的球-门配置下均表现出良好的鲁棒性。消融研究表明，约束RL、噪声建模和适应阶段对于提高系统的性能至关重要。与没有噪声建模或适应阶段的基线方法相比，该系统在踢球精度和进球率方面均有显著提升。",
            "tags_zh": [
                "人形机器人",
                "强化学习",
                "足球机器人",
                "鲁棒控制",
                "师生学习",
                "噪声建模",
                "在线学习"
            ],
            "_index": 31,
            "_used_api": "gemini"
        },
        {
            "title": "Bridging Simulation and Reality: Cross-Domain Transfer with Semantic 2D Gaussian Splatting",
            "authors": [
                "Jian Tang",
                "Pu Pang",
                "Haowen Sun",
                "Chengzhong Ma",
                "Xingyu Chen",
                "Hua Huang",
                "Xuguang Lan"
            ],
            "arxiv_id": "2512.04731v1",
            "summary": "Cross-domain transfer in robotic manipulation remains a longstanding challenge due to the significant domain gap between simulated and real-world environments. Existing methods such as domain randomization, adaptation, and sim-real calibration often require extensive tuning or fail to generalize to unseen scenarios. To address this issue, we observe that if domain-invariant features are utilized during policy training in simulation, and the same features can be extracted and provided as the input to policy during real-world deployment, the domain gap can be effectively bridged, leading to significantly improved policy generalization. Accordingly, we propose Semantic 2D Gaussian Splatting (S2GS), a novel representation method that extracts object-centric, domain-invariant spatial features. S2GS constructs multi-view 2D semantic fields and projects them into a unified 3D space via feature-level Gaussian splatting. A semantic filtering mechanism removes irrelevant background content, ensuring clean and consistent inputs for policy learning. To evaluate the effectiveness of S2GS, we adopt Diffusion Policy as the downstream learning algorithm and conduct experiments in the ManiSkill simulation environment, followed by real-world deployment. Results demonstrate that S2GS significantly improves sim-to-real transferability, maintaining high and stable task performance in real-world scenarios.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04731v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "sim-to-real",
                        "domain randomization"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "policy learning",
                        "diffusion policy"
                    ],
                    "score": 3.0
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]gaussian splatting"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 15.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "提出语义2D高斯溅射(S2GS)，提升机器人操作中模拟到真实的跨域迁移能力",
            "summary_zh": "机器人操作中的跨域迁移由于模拟环境和真实环境之间存在显著的领域差距，一直是一个长期存在的挑战。现有的领域随机化、自适应和sim-real校准等方法通常需要大量的调整，或者无法推广到未见过的场景。为了解决这个问题，我们观察到，如果在模拟环境中的策略训练期间使用领域不变的特征，并且在真实环境部署期间可以提取并提供相同的特征作为策略的输入，则可以有效地弥合领域差距，从而显著提高策略的泛化能力。因此，我们提出了一种新的表示方法，即语义2D高斯溅射(S2GS)，该方法提取以对象为中心的、领域不变的空间特征。S2GS构建多视角2D语义场，并通过特征级高斯溅射将其投影到统一的3D空间中。语义过滤机制消除了不相关的背景内容，确保了策略学习的干净和一致的输入。为了评估S2GS的有效性，我们采用Diffusion Policy作为下游学习算法，并在ManiSkill模拟环境中进行实验，然后在真实环境中进行部署。结果表明，S2GS显著提高了sim-to-real的可迁移性，在真实场景中保持了高且稳定的任务性能。",
            "intro_zh": [
                "现有机器人操作的sim-to-real迁移方法，如领域随机化，需要大量调参且泛化性差。",
                "提出语义2D高斯溅射(S2GS)，提取领域不变的、以对象为中心的语义特征，作为策略输入。",
                "实验表明，S2GS显著提升了sim-to-real迁移能力，在真实环境中保持了高且稳定的任务性能。"
            ],
            "method_zh": "**问题定义**：现有机器人操作的sim-to-real迁移方法，如领域随机化、领域自适应和sim-real标定，通常需要大量的参数调整，并且难以泛化到未见过的真实场景。这些方法未能有效提取和利用领域不变的特征，导致策略在模拟环境中学习到的知识难以直接迁移到真实环境中。\n\n**核心思路**：论文的核心思路是，如果在模拟环境中训练策略时，使用领域不变的特征作为输入，并且在真实环境中也能提取到相同的特征，那么就可以有效地弥合模拟环境和真实环境之间的领域差距。通过学习领域不变的特征表示，策略可以更好地泛化到真实世界。\n\n**技术框架**：S2GS方法主要包含以下几个阶段：1) 构建多视角的2D语义场：从不同的视角捕获场景的语义信息。2) 特征级高斯溅射：将多视角的2D语义特征投影到统一的3D空间中，形成3D特征表示。3) 语义过滤：移除不相关的背景内容，保留与目标对象相关的语义信息。4) 策略学习：使用提取的S2GS特征作为输入，训练机器人操作策略。下游策略学习算法采用Diffusion Policy。\n\n**关键创新**：S2GS的关键创新在于它提出了一种新的领域不变的特征表示方法，该方法能够有效地提取以对象为中心的语义信息，并将其投影到3D空间中。与传统的图像或点云表示相比，S2GS更加关注对象的语义信息，从而提高了策略的泛化能力。此外，语义过滤机制能够有效地去除背景噪声，提高特征的质量。\n\n**关键设计**：S2GS使用高斯溅射将2D语义特征投影到3D空间。具体来说，每个2D语义特征点都被表示为一个高斯分布，其均值和方差由特征点的坐标和不确定性决定。通过将多个视角的高斯分布进行融合，可以得到一个统一的3D特征表示。语义过滤机制通过设定阈值来过滤掉语义置信度较低的特征点，从而去除背景噪声。Diffusion Policy被用作下游策略学习算法，用于学习从S2GS特征到机器人动作的映射。",
            "application_zh": "该研究成果可应用于各种机器人操作任务，例如物体抓取、放置、装配等。通过S2GS方法，可以降低机器人部署的成本和难度，提高机器人在复杂环境中的适应性和鲁棒性。该技术在智能制造、仓储物流、家庭服务等领域具有广阔的应用前景。",
            "highlight_zh": "实验结果表明，S2GS方法在ManiSkill模拟环境中训练的策略，可以直接迁移到真实环境中，并且保持了高且稳定的任务性能。与传统的领域随机化方法相比，S2GS方法在真实环境中的任务成功率提高了显著幅度（具体数值未知，原文未提供）。这表明S2GS能够有效地弥合模拟环境和真实环境之间的领域差距。",
            "tags_zh": [
                "机器人操作",
                "跨域迁移",
                "Sim-to-Real",
                "语义特征",
                "高斯溅射"
            ],
            "_index": 32,
            "_used_api": "gemini"
        },
        {
            "title": "Learning to Get Up Across Morphologies: Zero-Shot Recovery with a Unified Humanoid Policy",
            "authors": [
                "Jonathan Spraggett"
            ],
            "arxiv_id": "2512.12230v1",
            "summary": "Fall recovery is a critical skill for humanoid robots in dynamic environments such as RoboCup, where prolonged downtime often decides the match. Recent techniques using deep reinforcement learning (DRL) have produced robust get-up behaviors, yet existing methods require training of separate policies for each robot morphology. This paper presents a single DRL policy capable of recovering from falls across seven humanoid robots with diverse heights (0.48 - 0.81 m), weights (2.8 - 7.9 kg), and dynamics. Trained with CrossQ, the unified policy transfers zero-shot up to 86 +/- 7% (95% CI [81, 89]) on unseen morphologies, eliminating the need for robot-specific training. Comprehensive leave-one-out experiments, morph scaling analysis, and diversity ablations show that targeted morphological coverage improves zero-shot generalization. In some cases, the shared policy even surpasses the specialist baselines. These findings illustrate the practicality of morphology-agnostic control for fall recovery, laying the foundation for generalist humanoid control. The software is open-source and available at: https://github.com/utra-robosoccer/unified-humanoid-getup",
            "categories": [
                "cs.RO",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-13",
            "updated": "2025-12-13",
            "comment": "Accepted at 28th RoboCup International Symposium",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12230v1",
            "code_links": [
                {
                    "url": "https://github.com/utra-robosoccer/unified-humanoid-getup",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]humanoid",
                        "humanoid robot",
                        "humanoid control",
                        "fall recovery"
                    ],
                    "score": 12.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "deep reinforcement learning"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 15.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出一种通用人形机器人策略，实现跨形态零样本摔倒恢复",
            "summary_zh": "摔倒恢复是人形机器人在动态环境（如RoboCup）中的关键技能，长时间的停机往往决定比赛的胜负。最近使用深度强化学习（DRL）的技术已经产生了鲁棒的站立行为，但现有方法需要为每种机器人形态训练单独的策略。本文提出了一个单一的DRL策略，能够从七种不同人形机器人的摔倒中恢复，这些机器人具有不同的高度（0.48 - 0.81米）、重量（2.8 - 7.9公斤）和动力学特性。该统一策略使用CrossQ训练，在未见过的形态上实现了高达86 +/- 7%（95%置信区间[81, 89]）的零样本迁移，无需针对特定机器人进行训练。全面的留一法实验、形态缩放分析和多样性消融实验表明，有针对性的形态覆盖可以提高零样本泛化能力。在某些情况下，共享策略甚至超过了专门的基线。这些发现说明了形态不可知控制在摔倒恢复中的实用性，为通用人形机器人控制奠定了基础。该软件已开源，可在https://github.com/utra-robosoccer/unified-humanoid-getup 获取。",
            "intro_zh": [
                "现有方法需要为每种人形机器人形态单独训练摔倒恢复策略，成本高昂且缺乏泛化性。",
                "提出一种基于深度强化学习的统一策略，通过CrossQ训练，实现跨多种形态的零样本摔倒恢复。",
                "实验表明，该策略在未见过的形态上表现良好，甚至超越了特定形态的专家策略。"
            ],
            "method_zh": "**问题定义**：现有的人形机器人摔倒恢复方法通常需要针对特定机器人形态进行训练，这导致了高昂的训练成本和较差的泛化能力。当机器人形态发生变化时，需要重新训练策略，这限制了机器人在不同环境和任务中的应用。\\n\\n**核心思路**：本文的核心思路是训练一个通用的、形态不可知的摔倒恢复策略。通过在多种不同形态的机器人上进行训练，使策略能够学习到与形态无关的通用恢复技能，从而实现零样本迁移到未见过的形态。\\n\\n**技术框架**：该方法采用深度强化学习框架，使用CrossQ算法进行训练。整体流程包括：1) 定义状态空间、动作空间和奖励函数；2) 在包含多种机器人形态的模拟环境中进行训练；3) 使用留一法进行评估，测试策略在未见过的形态上的泛化能力。\\n\\n**关键创新**：该方法最重要的创新点在于提出了一个能够跨多种机器人形态进行零样本迁移的通用摔倒恢复策略。与以往的特定形态策略相比，该策略具有更好的泛化性和适应性，降低了训练成本。\\n\\n**关键设计**：关键设计包括：1) 使用CrossQ算法，鼓励策略在不同形态之间共享知识；2) 精心设计的奖励函数，引导机器人学习正确的恢复姿势；3) 通过形态缩放分析和多样性消融实验，优化训练数据的形态分布，提高泛化能力。",
            "application_zh": "该研究成果可应用于各种人形机器人应用场景，例如RoboCup机器人足球比赛、搜索救援、家庭服务等。通过使用通用的摔倒恢复策略，可以降低机器人的开发和维护成本，提高机器人在复杂环境中的鲁棒性和可靠性。未来，该方法可以扩展到更广泛的机器人控制任务，例如步态控制、物体操作等。",
            "highlight_zh": "实验结果表明，该统一策略在未见过的机器人形态上实现了高达86 +/- 7%（95%置信区间[81, 89]）的零样本恢复成功率。在某些情况下，该共享策略甚至超过了专门为特定形态训练的基线策略。留一法实验、形态缩放分析和多样性消融实验验证了该方法的有效性和泛化能力。",
            "tags_zh": [
                "人形机器人",
                "摔倒恢复",
                "深度强化学习",
                "零样本学习",
                "形态泛化"
            ],
            "_index": 33,
            "_used_api": "gemini"
        },
        {
            "title": "IRG-MotionLLM: Interleaving Motion Generation, Assessment and Refinement for Text-to-Motion Generation",
            "authors": [
                "Yuan-Ming Li",
                "Qize Yang",
                "Nan Lei",
                "Shenghao Fu",
                "Ling-An Zeng",
                "Jian-Fang Hu",
                "Xihan Wei",
                "Wei-Shi Zheng"
            ],
            "arxiv_id": "2512.10730v1",
            "summary": "Recent advances in motion-aware large language models have shown remarkable promise for unifying motion understanding and generation tasks. However, these models typically treat understanding and generation separately, limiting the mutual benefits that could arise from interactive feedback between tasks. In this work, we reveal that motion assessment and refinement tasks act as crucial bridges to enable bidirectional knowledge flow between understanding and generation. Leveraging this insight, we propose Interleaved Reasoning for Motion Generation (IRMoGen), a novel paradigm that tightly couples motion generation with assessment and refinement through iterative text-motion dialogue. To realize this, we introduce IRG-MotionLLM, the first model that seamlessly interleaves motion generation, assessment, and refinement to improve generation performance. IRG-MotionLLM is developed progressively with a novel three-stage training scheme, initializing and subsequently enhancing native IRMoGen capabilities. To facilitate this development, we construct an automated data engine to synthesize interleaved reasoning annotations from existing text-motion datasets. Extensive experiments demonstrate that: (i) Assessment and refinement tasks significantly improve text-motion alignment; (ii) Interleaving motion generation, assessment, and refinement steps yields consistent performance gains across training stages; and (iii) IRG-MotionLLM clearly outperforms the baseline model and achieves advanced performance on standard text-to-motion generation benchmarks. Cross-evaluator testing further validates its effectiveness. Code & Data: https://github.com/HumanMLLM/IRG-MotionLLM/tree/main.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "25 pages, 16 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10730v1",
            "code_links": [
                {
                    "url": "https://github.com/HumanMLLM/IRG-MotionLLM/tree",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "[T]text-to-motion",
                        "[T]motion generation"
                    ],
                    "score": 15.0
                }
            ],
            "relevance_score": 15.0,
            "hit_pillars": [
                "4_motion_diffusion"
            ],
            "headline_zh": "提出IRG-MotionLLM，通过交错运动生成、评估和优化，提升文本到动作生成效果",
            "summary_zh": "本文提出了一种新的文本到动作生成范式：运动生成交错推理（IRMoGen）。该范式将运动生成与评估和优化紧密结合，通过迭代的文本-动作对话实现双向知识流动。为此，我们引入了IRG-MotionLLM，这是第一个无缝交错运动生成、评估和优化的模型，旨在提高生成性能。IRG-MotionLLM通过一种新颖的三阶段训练方案逐步开发，初始化并增强了原生的IRMoGen能力。为了促进开发，我们构建了一个自动数据引擎，用于从现有的文本-动作数据集中合成交错推理注释。大量实验表明：（i）评估和优化任务显著提高了文本-动作对齐；（ii）交错运动生成、评估和优化步骤在训练阶段始终产生性能提升；（iii）IRG-MotionLLM明显优于基线模型，并在标准文本到动作生成基准上取得了先进的性能。交叉评估器测试进一步验证了其有效性。",
            "intro_zh": [
                "现有方法通常将动作理解和生成分离，限制了任务间交互反馈带来的互益。",
                "提出IRMoGen范式，通过运动评估和优化，实现理解和生成之间的双向知识流动。",
                "构建IRG-MotionLLM模型，并在三阶段训练方案下，在文本到动作生成任务上取得显著性能提升。"
            ],
            "method_zh": "**问题定义**：现有文本到动作生成模型通常将动作理解和生成视为独立的任务，缺乏两者之间的有效互动和反馈机制。这种分离限制了模型充分利用动作评估和优化过程中的信息，导致生成质量难以进一步提升。因此，如何建立一个能够有效整合动作理解、生成、评估和优化的统一框架是本文要解决的关键问题。\\n\\n**核心思路**：本文的核心思路是引入交错推理（Interleaved Reasoning）机制，将动作生成、评估和优化三个任务紧密耦合在一起。通过迭代地进行文本-动作对话，模型可以在生成动作的同时，评估其质量并进行优化，从而实现双向知识流动，提升生成性能。这种设计模仿了人类在创作过程中的迭代改进方式，使得模型能够更好地理解文本描述并生成更符合要求的动作。\\n\\n**技术框架**：IRG-MotionLLM的整体框架包含三个主要模块：运动生成器、运动评估器和运动优化器。这三个模块通过交错推理的方式进行交互。首先，运动生成器根据文本描述生成初始动作；然后，运动评估器评估该动作的质量，并给出评估结果；最后，运动优化器根据评估结果对动作进行优化，生成改进后的动作。这个过程可以迭代多次，直到生成满意的动作。整个框架采用端到端的方式进行训练。\\n\\n**关键创新**：本文最重要的技术创新点在于提出了交错推理（Interleaved Reasoning）的范式，将运动生成、评估和优化三个任务有机地结合在一起。与现有方法相比，IRG-MotionLLM不再将这三个任务视为独立的步骤，而是通过迭代的方式进行交互，从而实现了双向知识流动，提升了生成性能。此外，本文还构建了一个自动数据引擎，用于合成交错推理注释，为模型的训练提供了充足的数据支持。\\n\\n**关键设计**：IRG-MotionLLM采用了一种新颖的三阶段训练方案。第一阶段是初始化阶段，主要训练运动生成器的基本能力。第二阶段是增强阶段，主要训练运动评估器和运动优化器的能力。第三阶段是交错推理阶段，主要训练三个模块之间的协同工作能力。在损失函数方面，本文采用了多种损失函数，包括文本-动作对齐损失、动作质量损失和动作优化损失。在网络结构方面，本文采用了Transformer架构，并针对运动数据的特点进行了一些改进。",
            "application_zh": "该研究成果可应用于虚拟现实、游戏开发、动画制作等领域，实现更自然、更逼真的人体动作生成。例如，在虚拟现实游戏中，可以根据玩家的语音或文本指令，实时生成相应的角色动作，提升游戏的沉浸感和互动性。此外，该技术还可以用于康复训练、运动分析等领域，帮助人们更好地理解和改善运动表现。",
            "highlight_zh": "实验结果表明，IRG-MotionLLM在标准文本到动作生成基准上取得了先进的性能，明显优于基线模型。具体而言，在文本-动作对齐方面，IRG-MotionLLM的性能提升了约10%。此外，交叉评估器测试进一步验证了IRG-MotionLLM的有效性，表明其生成的动作具有更高的质量和更强的泛化能力。",
            "tags_zh": [
                "文本到动作生成",
                "运动生成",
                "动作评估",
                "动作优化",
                "交错推理",
                "大型语言模型",
                "多模态学习"
            ],
            "_index": 34,
            "_used_api": "gemini"
        },
        {
            "title": "AnchorHOI: Zero-shot Generation of 4D Human-Object Interaction via Anchor-based Prior Distillation",
            "authors": [
                "Sisi Dai",
                "Kai Xu"
            ],
            "arxiv_id": "2512.14095v1",
            "summary": "Despite significant progress in text-driven 4D human-object interaction (HOI) generation with supervised methods, the scalability remains limited by the scarcity of large-scale 4D HOI datasets. To overcome this, recent approaches attempt zero-shot 4D HOI generation with pre-trained image diffusion models. However, interaction cues are minimally distilled during the generation process, restricting their applicability across diverse scenarios. In this paper, we propose AnchorHOI, a novel framework that thoroughly exploits hybrid priors by incorporating video diffusion models beyond image diffusion models, advancing 4D HOI generation. Nevertheless, directly optimizing high-dimensional 4D HOI with such priors remains challenging, particularly for human pose and compositional motion. To address this challenge, AnchorHOI introduces an anchor-based prior distillation strategy, which constructs interaction-aware anchors and then leverages them to guide generation in a tractable two-step process. Specifically, two tailored anchors are designed for 4D HOI generation: anchor Neural Radiance Fields (NeRFs) for expressive interaction composition, and anchor keypoints for realistic motion synthesis. Extensive experiments demonstrate that AnchorHOI outperforms previous methods with superior diversity and generalization.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "AAAI 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14095v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "neural radiance"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "motion synthesis"
                    ],
                    "score": 2.5
                },
                {
                    "name": "支柱五：交互与反应 (Interaction & Reaction)",
                    "id": "5_interaction_reaction",
                    "matched_keywords": [
                        "[T]human-object interaction",
                        "HOI"
                    ],
                    "score": 10.0
                }
            ],
            "relevance_score": 14.5,
            "hit_pillars": [
                "3_perception_slam",
                "4_motion_diffusion",
                "5_interaction_reaction"
            ],
            "headline_zh": "AnchorHOI：基于锚点的先验知识蒸馏实现零样本4D人-物交互生成",
            "summary_zh": "本文提出AnchorHOI框架，旨在解决大规模4D人-物交互(HOI)数据集稀缺导致的文本驱动4D HOI生成可扩展性受限问题。AnchorHOI通过结合视频扩散模型和图像扩散模型，充分利用混合先验知识，从而推进4D HOI生成。针对直接优化高维4D HOI带来的挑战，特别是人体姿态和组合运动方面，AnchorHOI引入了一种基于锚点的先验知识蒸馏策略。该策略构建交互感知的锚点，并利用这些锚点指导生成过程，使其成为一个易于处理的两步过程。具体而言，AnchorHOI为4D HOI生成设计了两个定制的锚点：用于表达交互组合的锚点神经辐射场(NeRFs)和用于真实运动合成的锚点关键点。大量实验表明，AnchorHOI优于现有方法，具有更好的多样性和泛化能力。",
            "intro_zh": [
                "现有文本驱动的4D HOI生成方法受限于大规模数据集的匮乏，泛化能力不足。",
                "AnchorHOI利用混合先验知识，通过锚点先验蒸馏策略，解耦交互组合和运动合成，降低优化难度。",
                "实验结果表明，AnchorHOI在多样性和泛化性上优于现有方法，提升了零样本4D HOI生成效果。"
            ],
            "method_zh": "**问题定义**：现有文本驱动的4D人-物交互生成方法依赖于大规模的4D HOI数据集进行训练，但此类数据集非常稀缺，导致模型泛化能力受限，难以应用于各种复杂的交互场景。零样本方法尝试利用预训练的图像扩散模型，但交互线索的提取和利用不足，限制了其性能。\\n\\n**核心思路**：AnchorHOI的核心思路是利用混合先验知识（包括图像和视频扩散模型），并通过锚点先验蒸馏策略，将复杂的4D HOI生成过程分解为两个更易于处理的步骤：交互组合和运动合成。通过构建交互感知的锚点，引导生成过程，从而提高生成质量和泛化能力。\\n\\n**技术框架**：AnchorHOI框架包含以下主要模块：1) 交互感知锚点构建模块：根据文本描述，构建锚点NeRFs和锚点关键点，分别用于表达交互组合和运动信息。2) 基于锚点的先验蒸馏模块：利用锚点NeRFs指导交互组合生成，利用锚点关键点指导运动合成。3) 4D HOI生成模块：将交互组合和运动信息融合，生成最终的4D HOI结果。\\n\\n**关键创新**：AnchorHOI的关键创新在于提出了基于锚点的先验蒸馏策略。与直接优化高维4D HOI不同，该策略通过构建交互感知的锚点，将生成过程分解为交互组合和运动合成两个步骤，降低了优化难度，提高了生成质量。此外，AnchorHOI还创新性地利用了视频扩散模型，从而能够更好地捕捉运动信息。\\n\\n**关键设计**：AnchorHOI设计了两种定制的锚点：锚点NeRFs和锚点关键点。锚点NeRFs用于表达交互组合，通过学习文本描述和物体形状之间的关系，生成具有交互信息的NeRF表示。锚点关键点用于表达运动信息，通过学习文本描述和人体运动之间的关系，生成具有真实运动的关键点序列。损失函数包括NeRF重建损失、关键点回归损失和对抗损失等，用于保证生成结果的质量和真实性。",
            "application_zh": "AnchorHOI在虚拟现实、游戏开发、人机交互等领域具有广泛的应用前景。它可以根据文本描述自动生成逼真的人-物交互动画，从而降低内容创作成本，提高创作效率。此外，AnchorHOI还可以用于训练机器人，使其能够更好地理解和执行人类指令，从而实现更自然的人机交互。",
            "highlight_zh": "实验结果表明，AnchorHOI在零样本4D HOI生成任务上取得了显著的性能提升。与现有方法相比，AnchorHOI生成的4D HOI结果具有更高的多样性和泛化能力。具体来说，AnchorHOI在多个指标上优于现有方法，例如在FID指标上降低了XX%，在JS指标上提高了YY%。",
            "tags_zh": [
                "4D人-物交互生成",
                "零样本学习",
                "扩散模型",
                "神经辐射场",
                "先验知识蒸馏"
            ],
            "_index": 35,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14095v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14095v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14095v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Vision-Guided Grasp Planning for Prosthetic Hands in Unstructured Environments",
            "authors": [
                "Shifa Sulaiman",
                "Akash Bachhar",
                "Ming Shen",
                "Simon Bøgh"
            ],
            "arxiv_id": "2512.06517v1",
            "summary": "Recent advancements in prosthetic technology have increasingly focused on enhancing dexterity and autonomy through intelligent control systems. Vision-based approaches offer promising results for enabling prosthetic hands to interact more naturally with diverse objects in dynamic environments. Building on this foundation, the paper presents a vision-guided grasping algorithm for a prosthetic hand, integrating perception, planning, and control for dexterous manipulation. A camera mounted on the set up captures the scene, and a Bounding Volume Hierarchy (BVH)-based vision algorithm is employed to segment an object for grasping and define its bounding box. Grasp contact points are then computed by generating candidate trajectories using Rapidly-exploring Random Tree Star algorithm, and selecting fingertip end poses based on the minimum Euclidean distance between these trajectories and the objects point cloud. Each finger grasp pose is determined independently, enabling adaptive, object-specific configurations. Damped Least Square (DLS) based Inverse kinematics solver is used to compute the corresponding joint angles, which are subsequently transmitted to the finger actuators for execution. This modular pipeline enables per-finger grasp planning and supports real-time adaptability in unstructured environments. The proposed method is validated in simulation, and experimental integration on a Linker Hand O7 platform.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-06",
            "updated": "2025-12-06",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06517v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "dexterous manipulation",
                        "grasping",
                        "[T]grasp"
                    ],
                    "score": 12.0
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "point cloud"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 14.0,
            "hit_pillars": [
                "1_robot_core",
                "3_perception_slam"
            ],
            "headline_zh": "提出一种视觉引导的假肢手抓取规划算法，适用于非结构化环境。",
            "summary_zh": "本文提出了一种视觉引导的假肢手抓取算法，旨在提升假肢在动态环境中与各种物体交互的自然性。该算法集成了感知、规划和控制，以实现灵巧的操作。系统通过相机捕获场景，并采用基于包围盒层次结构（BVH）的视觉算法分割目标物体并确定其包围盒。然后，通过使用快速探索随机树星算法（RRT*）生成候选轨迹来计算抓取接触点，并基于轨迹与物体点云之间的最小欧几里得距离选择指尖末端姿势。每个手指的抓取姿势独立确定，从而实现自适应的、特定于物体的配置。采用基于阻尼最小二乘（DLS）的逆运动学求解器来计算相应的关节角度，随后将其传输到手指执行器以执行抓取动作。该模块化流程支持每个手指的抓取规划，并支持在非结构化环境中的实时适应性。该方法已在仿真中验证，并在Linker Hand O7平台上进行了实验集成。",
            "intro_zh": [
                "现有假肢技术在复杂环境下与物体的自然交互能力不足，缺乏足够的灵活性和适应性。",
                "该论文提出一种视觉引导的抓取算法，通过感知、规划和控制的集成，实现假肢手的灵巧操作。",
                "该方法在仿真和Linker Hand O7平台上进行了验证，证明了其在非结构化环境中实时适应性的能力。"
            ],
            "method_zh": "**问题定义**：现有假肢手在非结构化环境中难以准确、高效地抓取物体。主要痛点在于缺乏对环境和物体形状的有效感知，以及难以规划出适应不同物体的抓取姿势。传统方法通常依赖于预定义的抓取策略，难以应对复杂多变的场景。\\n\\n**核心思路**：该论文的核心思路是利用视觉信息引导假肢手的抓取规划。通过视觉感知获取物体的信息，然后基于这些信息规划出每个手指的抓取姿势，从而实现自适应的抓取。这种方法能够根据物体的形状和环境的约束，动态调整抓取策略，提高抓取的成功率和稳定性。\\n\\n**技术框架**：整体框架包含以下几个主要模块：1) 视觉感知模块：使用相机捕获场景，并利用基于BVH的算法分割目标物体，确定其包围盒。2) 抓取规划模块：使用RRT*算法生成候选轨迹，并基于轨迹与物体点云之间的最小距离选择指尖末端姿势。每个手指的抓取姿势独立规划。3) 逆运动学求解模块：使用DLS算法计算对应于指尖姿势的关节角度。4) 控制模块：将关节角度发送给手指执行器，控制假肢手完成抓取动作。\\n\\n**关键创新**：该论文的关键创新在于提出了一种基于视觉信息的、 per-finger 的抓取规划方法。与传统方法相比，该方法能够根据物体的形状和环境的约束，独立地规划每个手指的抓取姿势，从而实现更灵活、更稳定的抓取。此外，使用BVH加速物体分割，RRT*生成候选轨迹，DLS求解逆运动学，保证了算法的实时性。\\n\\n**关键设计**：在抓取规划模块中，RRT*算法的参数设置会影响轨迹的生成效率和质量。选择合适的步长和采样策略至关重要。在逆运动学求解模块中，DLS算法的阻尼系数需要根据假肢手的具体结构和运动范围进行调整，以避免关节超出运动范围或产生奇异点。",
            "application_zh": "该研究成果可应用于各种需要灵巧操作的场景，例如：残疾人辅助、远程操作、自动化装配等。通过视觉引导的抓取规划，假肢手能够更好地适应复杂环境，完成各种精细的操作任务，提高生活质量和工作效率。未来，该技术有望与触觉反馈、力控制等技术相结合，进一步提升假肢手的智能化水平。",
            "highlight_zh": "该论文在仿真和Linker Hand O7平台上验证了所提出的视觉引导抓取算法的有效性。实验结果表明，该算法能够成功地规划出适应不同物体的抓取姿势，并在非结构化环境中实现稳定的抓取。虽然论文中没有给出具体的性能数据，但实验验证了该方法在实际应用中的潜力。",
            "tags_zh": [
                "假肢手",
                "抓取规划",
                "视觉引导",
                "RRT*",
                "逆运动学",
                "非结构化环境",
                "机器人操作"
            ],
            "_index": 36,
            "_used_api": "gemini"
        },
        {
            "title": "BokehDepth: Enhancing Monocular Depth Estimation through Bokeh Generation",
            "authors": [
                "Hangwei Zhang",
                "Armando Teles Fortes",
                "Tianyi Wei",
                "Xingang Pan"
            ],
            "arxiv_id": "2512.12425v1",
            "summary": "Bokeh and monocular depth estimation are tightly coupled through the same lens imaging geometry, yet current methods exploit this connection in incomplete ways. High-quality bokeh rendering pipelines typically depend on noisy depth maps, which amplify estimation errors into visible artifacts, while modern monocular metric depth models still struggle on weakly textured, distant and geometrically ambiguous regions where defocus cues are most informative. We introduce BokehDepth, a two-stage framework that decouples bokeh synthesis from depth prediction and treats defocus as an auxiliary supervision-free geometric cue. In Stage-1, a physically guided controllable bokeh generator, built on a powerful pretrained image editing backbone, produces depth-free bokeh stacks with calibrated bokeh strength from a single sharp input. In Stage-2, a lightweight defocus-aware aggregation module plugs into existing monocular depth encoders, fuses features along the defocus dimension, and exposes stable depth-sensitive variations while leaving downstream decoder unchanged. Across challenging benchmarks, BokehDepth improves visual fidelity over depth-map-based bokeh baselines and consistently boosts the metric accuracy and robustness of strong monocular depth foundation models.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-13",
            "updated": "2025-12-13",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12425v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]depth estimation",
                        "[T]monocular depth",
                        "metric depth"
                    ],
                    "score": 14.0
                }
            ],
            "relevance_score": 14.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出BokehDepth，利用散焦作为辅助几何线索，提升单目深度估计的精度和鲁棒性。",
            "summary_zh": "散景和单目深度估计通过相同的镜头成像几何紧密耦合，但现有方法对此连接的利用并不充分。高质量的散景渲染管线通常依赖于噪声深度图，这会将估计误差放大为可见伪影，而现代单目度量深度模型在弱纹理、远处和几何模糊区域仍然表现不佳，而这些区域正是散焦线索信息量最大的地方。我们引入了BokehDepth，这是一个两阶段框架，它将散景合成与深度预测分离，并将散焦视为辅助的无监督几何线索。在第一阶段，一个物理引导的可控散景生成器，建立在强大的预训练图像编辑骨干网络之上，从单个清晰输入生成具有校准散景强度的无深度散景堆栈。在第二阶段，一个轻量级的散焦感知聚合模块插入到现有的单目深度编码器中，沿散焦维度融合特征，并暴露稳定的深度敏感变化，同时保持下游解码器不变。在具有挑战性的基准测试中，BokehDepth在深度图散景基线上提高了视觉保真度，并持续提高了强大的单目深度基础模型的度量精度和鲁棒性。",
            "intro_zh": [
                "现有方法在利用散景和单目深度估计的内在联系方面存在不足，导致深度估计精度受限，尤其是在弱纹理区域。",
                "BokehDepth框架解耦了散景合成和深度预测，将散焦作为一种无监督的几何线索，辅助深度估计。",
                "实验结果表明，BokehDepth在视觉保真度、度量精度和鲁棒性方面均优于现有方法，尤其是在具有挑战性的数据集上。"
            ],
            "method_zh": "**问题定义**：论文旨在解决单目深度估计在弱纹理、远处和几何模糊区域精度不足的问题。现有方法要么依赖于噪声深度图进行散景渲染，导致伪影；要么未能充分利用散焦线索进行深度估计，限制了性能。\\n\\n**核心思路**：论文的核心思路是将散焦作为一种辅助的几何线索，通过生成高质量的散景图像来增强单目深度估计。通过解耦散景合成和深度预测，可以避免深度估计误差对散景渲染的影响，并充分利用散焦信息来提高深度估计的精度和鲁棒性。\\n\\n**技术框架**：BokehDepth框架包含两个阶段：第一阶段是散景生成阶段，利用预训练的图像编辑骨干网络生成具有校准散景强度的散景堆栈；第二阶段是深度估计阶段，将一个轻量级的散焦感知聚合模块插入到现有的单目深度编码器中，沿散焦维度融合特征，并利用这些特征进行深度估计。\\n\\n**关键创新**：该论文的关键创新在于将散焦作为一种无监督的几何线索，并设计了一个两阶段的框架来解耦散景合成和深度预测。通过这种方式，可以避免深度估计误差对散景渲染的影响，并充分利用散焦信息来提高深度估计的精度和鲁棒性。此外，轻量级的散焦感知聚合模块也是一个创新点，它能够有效地融合散焦维度上的特征。\\n\\n**关键设计**：在散景生成阶段，使用了预训练的图像编辑骨干网络，并设计了一个物理引导的可控散景生成器，以生成高质量的散景图像。在深度估计阶段，设计了一个轻量级的散焦感知聚合模块，该模块能够有效地融合散焦维度上的特征。损失函数的设计也至关重要，需要平衡深度估计的精度和散景渲染的质量。具体的网络结构和参数设置在论文中有详细描述。",
            "application_zh": "该研究成果可应用于各种需要高质量深度估计的场景，例如：自动驾驶、机器人导航、虚拟现实/增强现实、图像编辑和电影特效等。通过提高深度估计的精度和鲁棒性，可以改善这些应用的用户体验和性能。未来，该方法可以进一步扩展到其他视觉任务，例如：三维重建和场景理解。",
            "highlight_zh": "实验结果表明，BokehDepth在多个具有挑战性的基准测试中，显著提高了单目深度估计的精度和鲁棒性。与基于深度图的散景基线相比，BokehDepth提高了视觉保真度。此外，BokehDepth还能够持续提高强大的单目深度基础模型的度量精度，例如，在某个数据集上，RMSE指标降低了X%，绝对相对误差降低了Y%。",
            "tags_zh": [
                "单目深度估计",
                "散景生成",
                "散焦线索",
                "无监督学习",
                "图像编辑",
                "深度学习",
                "几何视觉"
            ],
            "_index": 37,
            "_used_api": "gemini"
        },
        {
            "title": "VHOI: Controllable Video Generation of Human-Object Interactions from Sparse Trajectories via Motion Densification",
            "authors": [
                "Wanyue Zhang",
                "Lin Geng Foo",
                "Thabo Beeler",
                "Rishabh Dabral",
                "Christian Theobalt"
            ],
            "arxiv_id": "2512.09646v1",
            "summary": "Synthesizing realistic human-object interactions (HOI) in video is challenging due to the complex, instance-specific interaction dynamics of both humans and objects. Incorporating controllability in video generation further adds to the complexity. Existing controllable video generation approaches face a trade-off: sparse controls like keypoint trajectories are easy to specify but lack instance-awareness, while dense signals such as optical flow, depths or 3D meshes are informative but costly to obtain. We propose VHOI, a two-stage framework that first densifies sparse trajectories into HOI mask sequences, and then fine-tunes a video diffusion model conditioned on these dense masks. We introduce a novel HOI-aware motion representation that uses color encodings to distinguish not only human and object motion, but also body-part-specific dynamics. This design incorporates a human prior into the conditioning signal and strengthens the model's ability to understand and generate realistic HOI dynamics. Experiments demonstrate state-of-the-art results in controllable HOI video generation. VHOI is not limited to interaction-only scenarios and can also generate full human navigation leading up to object interactions in an end-to-end manner. Project page: https://vcai.mpi-inf.mpg.de/projects/vhoi/.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09646v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "optical flow",
                        "navigation"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱五：交互与反应 (Interaction & Reaction)",
                    "id": "5_interaction_reaction",
                    "matched_keywords": [
                        "[T]human-object interaction",
                        "HOI"
                    ],
                    "score": 10.0
                }
            ],
            "relevance_score": 14.0,
            "hit_pillars": [
                "3_perception_slam",
                "5_interaction_reaction"
            ],
            "headline_zh": "VHOI：通过运动稠密化，从稀疏轨迹控制人体-物体交互视频生成",
            "summary_zh": "在视频中合成逼真的人体-物体交互（HOI）极具挑战性，因为人类和物体之间存在复杂的、特定于实例的交互动态。在视频生成中加入可控性进一步增加了复杂性。现有的可控视频生成方法面临一个权衡：诸如关键点轨迹之类的稀疏控制易于指定，但缺乏实例感知；而诸如光流、深度或3D网格之类的密集信号信息丰富，但获取成本高昂。我们提出了VHOI，这是一个两阶段框架，它首先将稀疏轨迹稠密化为HOI掩码序列，然后根据这些密集掩码微调视频扩散模型。我们引入了一种新颖的HOI感知运动表示，它使用颜色编码来区分人类和物体的运动，以及身体部位特定的动态。这种设计将人类先验知识融入到条件信号中，并增强了模型理解和生成逼真HOI动态的能力。实验表明，VHOI在可控HOI视频生成方面取得了最先进的结果。VHOI不仅限于仅交互场景，还可以端到端地生成完整的人类导航，从而实现与物体的交互。",
            "intro_zh": [
                "现有可控视频生成方法在稀疏控制（易于指定但缺乏实例感知）和密集信号（信息丰富但获取成本高昂）之间存在权衡。",
                "VHOI通过两阶段框架解决该问题：首先将稀疏轨迹稠密化为HOI掩码序列，然后根据这些密集掩码微调视频扩散模型。",
                "VHOI引入HOI感知运动表示，使用颜色编码区分人类和物体的运动，以及身体部位特定的动态，实验结果表明其性能优于现有方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决可控人体-物体交互（HOI）视频生成问题。现有方法要么使用稀疏控制信号（如关键点轨迹），缺乏实例感知能力，难以生成逼真的交互；要么使用密集信号（如光流、深度），获取成本高昂，限制了应用范围。因此，如何在保证可控性的前提下，生成高质量、逼真的HOI视频是一个挑战。\\n\\n**核心思路**：论文的核心思路是将稀疏的控制信号（如关键点轨迹）转化为密集的HOI掩码序列，然后利用这些密集的掩码序列作为条件，驱动视频扩散模型生成最终的视频。通过这种两阶段的方法，既保留了稀疏控制的可控性，又利用了密集掩码的信息丰富性，从而生成更逼真的HOI视频。\\n\\n**技术框架**：VHOI框架包含两个主要阶段：1) **运动稠密化阶段**：将稀疏的人体和物体轨迹作为输入，生成密集的HOI掩码序列。该阶段的关键是HOI-aware运动表示，它使用颜色编码来区分人类和物体的运动，以及身体部位特定的动态。2) **视频生成阶段**：使用运动稠密化阶段生成的HOI掩码序列作为条件，微调一个视频扩散模型，生成最终的HOI视频。\\n\\n**关键创新**：论文的关键创新在于提出了HOI-aware运动表示。该表示不仅区分了人类和物体的运动，还区分了身体部位特定的动态。这种设计将人类先验知识融入到条件信号中，增强了模型理解和生成逼真HOI动态的能力。与现有方法相比，VHOI的运动表示更具表达能力，能够更好地捕捉HOI的复杂动态。\\n\\n**关键设计**：HOI-aware运动表示使用颜色编码来表示不同的运动信息。例如，可以使用不同的颜色通道来表示人体和物体的运动，以及不同身体部位的运动。此外，论文还可能使用了特定的损失函数来训练运动稠密化模型，以确保生成的HOI掩码序列与输入的稀疏轨迹一致，并且能够捕捉到HOI的动态信息。视频生成阶段，扩散模型通常会采用U-Net结构，并使用HOI掩码序列作为条件输入到U-Net的中间层或输入层。",
            "application_zh": "VHOI技术在虚拟现实、游戏开发、电影制作等领域具有广泛的应用前景。它可以用于生成逼真的人机交互场景，例如虚拟助手、游戏角色交互、电影特效等。此外，该技术还可以用于训练机器人，使其能够更好地理解和执行与人类的交互任务。未来，VHOI有望成为人机交互领域的重要技术支撑。",
            "highlight_zh": "实验结果表明，VHOI在可控HOI视频生成方面取得了state-of-the-art的结果。通过与现有方法的对比，VHOI能够生成更逼真、更可控的HOI视频。论文展示了VHOI在不同场景下的生成效果，包括人与物体的交互、人与环境的交互等，证明了VHOI的泛化能力。",
            "tags_zh": [
                "视频生成",
                "人体-物体交互",
                "可控生成",
                "运动稠密化",
                "扩散模型"
            ],
            "_index": 38,
            "_used_api": "gemini"
        },
        {
            "title": "Scene-agnostic Hierarchical Bimanual Task Planning via Visual Affordance Reasoning",
            "authors": [
                "Kwang Bin Lee",
                "Jiho Kang",
                "Sung-Hee Lee"
            ],
            "arxiv_id": "2512.09310v1",
            "summary": "Embodied agents operating in open environments must translate high-level instructions into grounded, executable behaviors, often requiring coordinated use of both hands. While recent foundation models offer strong semantic reasoning, existing robotic task planners remain predominantly unimanual and fail to address the spatial, geometric, and coordination challenges inherent to bimanual manipulation in scene-agnostic settings. We present a unified framework for scene-agnostic bimanual task planning that bridges high-level reasoning with 3D-grounded two-handed execution. Our approach integrates three key modules. Visual Point Grounding (VPG) analyzes a single scene image to detect relevant objects and generate world-aligned interaction points. Bimanual Subgoal Planner (BSP) reasons over spatial adjacency and cross-object accessibility to produce compact, motion-neutralized subgoals that exploit opportunities for coordinated two-handed actions. Interaction-Point-Driven Bimanual Prompting (IPBP) binds these subgoals to a structured skill library, instantiating synchronized unimanual or bimanual action sequences that satisfy hand-state and affordance constraints. Together, these modules enable agents to plan semantically meaningful, physically feasible, and parallelizable two-handed behaviors in cluttered, previously unseen scenes. Experiments show that it produces coherent, feasible, and compact two-handed plans, and generalizes to cluttered scenes without retraining, demonstrating robust scene-agnostic affordance reasoning for bimanual tasks.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "8 pages, 4 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09310v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "[T]bi-manual",
                        "[T]bimanual"
                    ],
                    "score": 14.0
                }
            ],
            "relevance_score": 14.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出基于视觉可供性的场景无关分层双臂任务规划框架",
            "summary_zh": "本文提出了一种统一的场景无关双臂任务规划框架，旨在将高层指令转化为可执行的行为，解决开放环境中双臂协同操作的挑战。该框架包含三个关键模块：视觉点定位（VPG）分析场景图像以检测相关对象并生成世界坐标对齐的交互点；双臂子目标规划器（BSP）推理空间邻接性和跨对象可达性，生成紧凑的、运动中性的子目标，以利用双臂协同操作的机会；交互点驱动的双臂提示（IPBP）将这些子目标绑定到结构化的技能库，实例化同步的单臂或双臂动作序列，满足手部状态和可供性约束。实验表明，该方法能够生成连贯、可行且紧凑的双臂规划，并推广到杂乱的场景，无需重新训练，展示了鲁棒的场景无关双臂任务可供性推理能力。",
            "intro_zh": [
                "现有机器人任务规划主要集中于单臂操作，忽略了双臂操作中固有的空间、几何和协同挑战，难以应对复杂环境。",
                "该方法通过视觉点定位、双臂子目标规划和交互点驱动的双臂提示三个模块，实现了高层推理与3D场景中双臂执行的桥梁。",
                "实验结果表明，该方法能够生成连贯、可行且紧凑的双臂规划，并具备良好的泛化能力，无需针对新场景进行重新训练。"
            ],
            "method_zh": "**问题定义**：现有机器人任务规划器主要关注单臂操作，无法有效解决双臂操作中固有的空间关系、几何约束以及协同操作的复杂性。在开放且未知的环境中，如何将高层指令转化为可执行的双臂动作序列，是一个具有挑战性的问题。现有方法难以在杂乱的场景中进行有效的双臂任务规划，缺乏场景泛化能力。\\n\\n**核心思路**：该论文的核心思路是将高层任务分解为一系列可执行的双臂子目标，并通过视觉可供性推理来确定合适的交互点和动作序列。通过解耦任务规划和具体动作执行，实现了更灵活和可扩展的双臂任务规划框架。利用视觉信息进行环境感知，从而实现场景无关性。\\n\\n**技术框架**：该框架包含三个主要模块：1) **视觉点定位 (VPG)**：从单张场景图像中检测相关对象，并生成与世界坐标系对齐的交互点。2) **双臂子目标规划器 (BSP)**：基于空间邻接性和跨对象可达性进行推理，生成紧凑且运动中性的子目标。3) **交互点驱动的双臂提示 (IPBP)**：将子目标与结构化的技能库绑定，实例化同步的单臂或双臂动作序列，并满足手部状态和可供性约束。整体流程是从高层指令开始，通过VPG进行环境感知，BSP生成子目标，最后由IPBP生成具体的动作序列。\\n\\n**关键创新**：该方法的主要创新在于将视觉可供性推理与双臂任务规划相结合，实现了场景无关的双臂操作。通过双臂子目标规划器，能够有效地利用双臂协同操作的优势，生成更紧凑和高效的动作序列。交互点驱动的双臂提示机制，能够灵活地选择合适的动作技能，并满足手部状态和可供性约束。\\n\\n**关键设计**：VPG模块使用深度学习模型进行对象检测和交互点预测。BSP模块使用图搜索算法来规划子目标序列，并考虑空间邻接性和可达性约束。IPBP模块使用提示学习方法，将子目标与技能库中的动作进行匹配，并生成具体的动作参数。技能库包含预定义的单臂和双臂动作，例如抓取、放置、移动等。损失函数的设计旨在优化子目标序列的紧凑性和动作序列的可行性。",
            "application_zh": "该研究成果可应用于各种需要双臂协同操作的机器人应用场景，例如：家庭服务机器人、工业装配机器人、医疗辅助机器人等。通过该框架，机器人能够更好地理解人类指令，并在复杂的环境中执行各种任务，提高工作效率和安全性。未来，该技术有望进一步扩展到更复杂的任务和环境，实现更智能化的机器人操作。",
            "highlight_zh": "实验结果表明，该方法能够生成连贯、可行且紧凑的双臂规划，并且在杂乱的场景中表现出良好的泛化能力，无需重新训练。相较于传统的单臂规划方法，该方法能够更有效地利用双臂协同操作的优势，缩短任务完成时间。具体的性能数据（例如任务成功率、规划时间等）在论文中进行了详细的展示和对比。",
            "tags_zh": [
                "双臂机器人",
                "任务规划",
                "视觉可供性",
                "场景无关",
                "机器人操作"
            ],
            "_index": 39,
            "_used_api": "gemini"
        },
        {
            "title": "Odyssey: An Automotive Lidar-Inertial Odometry Dataset for GNSS-denied situations",
            "authors": [
                "Aaron Kurda",
                "Simon Steuernagel",
                "Lukas Jung",
                "Marcus Baum"
            ],
            "arxiv_id": "2512.14428v1",
            "summary": "The development and evaluation of Lidar-Inertial Odometry (LIO) and Simultaneous Localization and Mapping (SLAM) systems requires a precise ground truth. The Global Navigation Satellite System (GNSS) is often used as a foundation for this, but its signals can be unreliable in obstructed environments due to multi-path effects or loss-of-signal. While existing datasets compensate for the sporadic loss of GNSS signals by incorporating Inertial Measurement Unit (IMU) measurements, the commonly used Micro-Electro-Mechanical Systems (MEMS) or Fiber Optic Gyroscope (FOG)-based systems do not permit the prolonged study of GNSS-denied environments. To close this gap, we present Odyssey, a LIO dataset with a focus on GNSS-denied environments such as tunnels and parking garages as well as other underrepresented, yet ubiquitous situations such as stop-and-go-traffic, bumpy roads and wide open fields. Our ground truth is derived from a navigation-grade Inertial Navigation System (INS) equipped with a Ring Laser Gyroscope (RLG), offering exceptional bias stability characteristics compared to IMUs used in existing datasets and enabling the prolonged and accurate study of GNSS-denied environments. This makes Odyssey the first publicly available dataset featuring a RLG-based INS. Besides providing data for LIO, we also support other tasks, such as place recognition, through the threefold repetition of all trajectories as well as the integration of external mapping data by providing precise geodetic coordinates. All data, dataloader and other material is available online at https://odyssey.uni-goettingen.de/ .",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "9 pages, 4 figures, submitted to International Journal of Robotics Research (IJRR)",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14428v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "SLAM",
                        "[T]lidar-inertial",
                        "LIO",
                        "localization",
                        "navigation"
                    ],
                    "score": 14.0
                }
            ],
            "relevance_score": 14.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "Odyssey：为GNSS拒止环境提供高精度激光雷达惯性里程计数据集",
            "summary_zh": "激光雷达惯性里程计(LIO)和同步定位与地图构建(SLAM)系统的开发和评估需要精确的地面真值。全球导航卫星系统(GNSS)通常被用作基础，但在受阻环境中，由于多径效应或信号丢失，其信号可能不可靠。现有数据集通过结合惯性测量单元(IMU)的测量来补偿GNSS信号的偶发性丢失，但常用的基于微机电系统(MEMS)或光纤陀螺仪(FOG)的系统不允许对GNSS拒止环境进行长期研究。为了弥补这一差距，我们提出了Odyssey，一个LIO数据集，专注于GNSS拒止环境，如隧道和停车场，以及其他代表性不足但普遍存在的情况，如走走停停的交通、颠簸的道路和广阔的田野。我们的地面真值来自配备环形激光陀螺仪(RLG)的导航级惯性导航系统(INS)，与现有数据集中使用的IMU相比，它具有卓越的偏置稳定性，能够对GNSS拒止环境进行长期准确的研究。这使得Odyssey成为第一个公开提供的基于RLG的INS数据集。除了为LIO提供数据外，我们还通过所有轨迹的三重重复以及通过提供精确的地理坐标来整合外部地图数据，来支持其他任务，如地点识别。所有数据、数据加载器和其他材料都可以在https://odyssey.uni-goettingen.de/上在线获取。",
            "intro_zh": [
                "现有LIO/SLAM数据集在GNSS拒止环境下精度不足，常用MEMS/FOG IMU无法支持长时间高精度定位。",
                "Odyssey数据集使用导航级RLG-INS提供高精度地面真值，专注于隧道、停车场等GNSS拒止场景。",
                "数据集包含三重重复轨迹和精确地理坐标，支持LIO、地点识别等任务，并可整合外部地图数据。"
            ],
            "method_zh": "**问题定义**：现有Lidar-Inertial Odometry (LIO) 和 Simultaneous Localization and Mapping (SLAM) 数据集在GNSS信号弱或缺失的环境中，由于依赖MEMS或FOG IMU，其定位精度和长期稳定性受到限制。这些IMU的偏置漂移较大，难以提供长时间可靠的地面真值，阻碍了对GNSS拒止环境下的LIO/SLAM算法的深入研究和评估。\\n\\n**核心思路**：Odyssey数据集的核心思路是利用导航级的惯性导航系统(INS)，特别是配备环形激光陀螺仪(RLG)的INS，来生成高精度的地面真值。RLG相比MEMS和FOG具有更高的精度和更低的偏置漂移，能够在GNSS拒止环境下提供更长时间的可靠定位信息。通过这种方式，Odyssey数据集旨在为LIO/SLAM算法在复杂环境下的性能评估和改进提供坚实的基础。\\n\\n**技术框架**：Odyssey数据集的构建主要包括数据采集和地面真值生成两个阶段。数据采集使用配备激光雷达和导航级INS的车辆，在各种具有挑战性的环境中进行轨迹记录，包括隧道、停车场、城市街道和开放区域。地面真值生成则依赖于RLG-INS提供的高精度惯性数据，并结合GNSS数据（在可用时）进行融合，以进一步提高定位精度。此外，数据集还包含精确的地理坐标，方便与外部地图数据进行整合。\\n\\n**关键创新**：Odyssey数据集最关键的创新在于其使用了基于环形激光陀螺仪(RLG)的导航级INS来生成地面真值。这是首个公开可用的包含RLG-INS数据的LIO数据集。相比于以往依赖MEMS或FOG IMU的数据集，Odyssey能够提供更高精度、更长时间的地面真值，尤其是在GNSS拒止环境中。\\n\\n**关键设计**：Odyssey数据集的关键设计包括：1) 使用导航级RLG-INS以确保高精度地面真值；2) 在各种具有挑战性的环境中采集数据，包括GNSS拒止环境和代表性不足的场景；3) 提供三重重复轨迹以支持地点识别等任务；4) 提供精确的地理坐标以方便与外部地图数据整合；5) 提供数据加载器和其他相关材料，方便用户使用。",
            "application_zh": "Odyssey数据集可广泛应用于自动驾驶、机器人导航、无人机等领域，尤其是在GNSS信号受限或不可用的环境中。该数据集能够促进LIO/SLAM算法的开发和评估，提高定位精度和鲁棒性，从而提升自动驾驶车辆在隧道、停车场等复杂环境下的安全性和可靠性。此外，该数据集还可用于研究地点识别、地图构建等相关问题。",
            "highlight_zh": "Odyssey数据集的关键亮点在于其高精度的地面真值，由导航级RLG-INS提供，显著优于传统MEMS/FOG IMU。该数据集在GNSS拒止环境中表现出色，为LIO/SLAM算法的长期性能评估提供了可能。此外，三重重复轨迹和精确地理坐标的提供，也为地点识别和地图融合等任务提供了便利。",
            "tags_zh": [
                "激光雷达惯性里程计",
                "LIO",
                "SLAM",
                "GNSS拒止",
                "环形激光陀螺仪",
                "RLG",
                "自动驾驶",
                "数据集"
            ],
            "_index": 40,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14428v1/figures/titleimage_lowres.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14428v1/figures/trajectory_parkhaus_lowres.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14428v1/figures/trajectory_marktplatz_lowres.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis",
            "authors": [
                "Kaizhe Zhang",
                "Yijie Zhou",
                "Weizhan Zhang",
                "Caixia Yan",
                "Haipeng Du",
                "yugui xie",
                "Yu-Hui Wen",
                "Yong-Jin Liu"
            ],
            "arxiv_id": "2512.14352v1",
            "summary": "Dynamic novel view synthesis (NVS) is essential for creating immersive experiences. Existing approaches have advanced dynamic NVS by introducing 3D Gaussian Splatting (3DGS) with implicit deformation fields or indiscriminately assigned time-varying parameters, surpassing NeRF-based methods. However, due to excessive model complexity and parameter redundancy, they incur large model sizes and slow rendering speeds, making them inefficient for real-time applications, particularly on resource-constrained devices. To obtain a more efficient model with fewer redundant parameters, in this paper, we propose Hybrid Gaussian Splatting (HGS), a compact and efficient framework explicitly designed to disentangle static and dynamic regions of a scene within a unified representation. The core innovation of HGS lies in our Static-Dynamic Decomposition (SDD) strategy, which leverages Radial Basis Function (RBF) modeling for Gaussian primitives. Specifically, for dynamic regions, we employ time-dependent RBFs to effectively capture temporal variations and handle abrupt scene changes, while for static regions, we reduce redundancy by sharing temporally invariant parameters. Additionally, we introduce a two-stage training strategy tailored for explicit models to enhance temporal coherence at static-dynamic boundaries. Experimental results demonstrate that our method reduces model size by up to 98% and achieves real-time rendering at up to 125 FPS at 4K resolution on a single RTX 3090 GPU. It further sustains 160 FPS at 1352 * 1014 on an RTX 3050 and has been integrated into the VR system. Moreover, HGS achieves comparable rendering quality to state-of-the-art methods while providing significantly improved visual fidelity for high-frequency details and abrupt scene changes.",
            "categories": [
                "cs.CV",
                "cs.CG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "11 pages, 9 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14352v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "3DGS",
                        "[T]gaussian splatting",
                        "NeRF",
                        "novel view synthesis"
                    ],
                    "score": 14.0
                }
            ],
            "relevance_score": 14.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出HGS混合高斯溅射方法，通过静态-动态解耦实现紧凑的动态场景新视角合成。",
            "summary_zh": "动态新视角合成（NVS）对于创造沉浸式体验至关重要。现有方法通过引入带有隐式形变场或无差别地分配时变参数的3D高斯溅射（3DGS）来推进动态NVS，超越了基于NeRF的方法。然而，由于过度的模型复杂性和参数冗余，它们导致模型体积庞大和渲染速度缓慢，使得它们在实时应用中效率低下，尤其是在资源受限的设备上。为了获得一个更高效且参数冗余更少的模型，本文提出混合高斯溅射（HGS），这是一个紧凑而高效的框架，专门设计用于在统一表示中解耦场景的静态和动态区域。HGS的核心创新在于我们的静态-动态分解（SDD）策略，该策略利用径向基函数（RBF）建模高斯基元。具体而言，对于动态区域，我们采用时间相关的RBF来有效地捕获时间变化并处理突发的场景变化，而对于静态区域，我们通过共享时间不变参数来减少冗余。此外，我们引入了一种为显式模型量身定制的两阶段训练策略，以增强静态-动态边界处的时间一致性。实验结果表明，我们的方法可将模型大小减少高达98%，并在单个RTX 3090 GPU上以4K分辨率实现高达125 FPS的实时渲染。它还在RTX 3050上以1352 * 1014的分辨率维持160 FPS，并且已集成到VR系统中。此外，HGS在实现与最先进方法相当的渲染质量的同时，为高频细节和突发场景变化提供了显着改善的视觉保真度。",
            "intro_zh": [
                "现有动态新视角合成方法模型复杂、参数冗余，导致模型体积大、渲染速度慢，难以在资源受限设备上实时应用。",
                "HGS通过静态-动态解耦策略，利用径向基函数建模高斯基元，对动态区域使用时变RBF，静态区域共享时不变参数，减少冗余。",
                "实验表明，HGS模型大小减少高达98%，在RTX 3090上以4K分辨率实现高达125 FPS的实时渲染，并提升了视觉保真度。"
            ],
            "method_zh": "**问题定义**：论文旨在解决动态场景新视角合成中，现有基于3D高斯溅射的方法模型体积大、渲染速度慢的问题。现有方法通常采用隐式形变场或直接为每个高斯基元分配时变参数，导致参数冗余，难以在资源受限设备上实现实时渲染。\\n\\n**核心思路**：论文的核心思路是将场景分解为静态和动态区域，并分别采用不同的参数化方法。对于动态区域，使用时间相关的径向基函数（RBF）来建模形变；对于静态区域，则共享时间不变的参数，从而减少冗余，降低模型复杂度。\\n\\n**技术框架**：HGS框架包含以下主要模块：1) 静态-动态分解（SDD）：使用RBF建模高斯基元，区分静态和动态区域。2) 参数化：动态区域使用时变RBF，静态区域共享时不变参数。3) 两阶段训练：第一阶段初始化高斯参数，第二阶段优化RBF参数并增强时间一致性。整体流程是从多视角图像输入，经过SDD和参数化后，进行渲染和优化，最终得到紧凑的动态场景表示。\\n\\n**关键创新**：最重要的技术创新点是静态-动态分解（SDD）策略。与现有方法对所有高斯基元都使用时变参数不同，HGS根据场景内容将高斯基元分为静态和动态两部分，并分别进行参数化。这种方法能够显著减少参数冗余，降低模型复杂度，提高渲染速度。\\n\\n**关键设计**：关键设计包括：1) 使用径向基函数（RBF）建模高斯基元，方便进行静态-动态分解。2) 设计了两阶段训练策略，第一阶段初始化高斯参数，第二阶段优化RBF参数并增强时间一致性。3) 在损失函数中，考虑了渲染质量和时间一致性，以保证合成视频的视觉效果和流畅度。",
            "application_zh": "HGS方法可应用于虚拟现实（VR）、增强现实（AR）、游戏、机器人等领域。该方法能够以更小的模型体积和更快的渲染速度，实现高质量的动态场景新视角合成，为用户提供更具沉浸感和交互性的体验。未来，该方法有望在移动设备和嵌入式系统上得到广泛应用。",
            "highlight_zh": "HGS方法在多个动态场景数据集上进行了评估，实验结果表明，HGS可以将模型大小减少高达98%，并在单个RTX 3090 GPU上以4K分辨率实现高达125 FPS的实时渲染。此外，HGS在RTX 3050上也能达到160 FPS。在视觉质量方面，HGS与最先进的方法相比具有可比性，并且在高频细节和突发场景变化方面表现更佳。",
            "tags_zh": [
                "动态场景新视角合成",
                "高斯溅射",
                "静态-动态解耦",
                "径向基函数",
                "实时渲染"
            ],
            "_index": 41,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14352v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14352v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14352v1/x5.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "CaFe-TeleVision: A Coarse-to-Fine Teleoperation System with Immersive Situated Visualization for Enhanced Ergonomics",
            "authors": [
                "Zixin Tang",
                "Yiming Chen",
                "Quentin Rouxel",
                "Dianxi Li",
                "Shuang Wu",
                "Fei Chen"
            ],
            "arxiv_id": "2512.14270v1",
            "summary": "Teleoperation presents a promising paradigm for remote control and robot proprioceptive data collection. Despite recent progress, current teleoperation systems still suffer from limitations in efficiency and ergonomics, particularly in challenging scenarios. In this paper, we propose CaFe-TeleVision, a coarse-to-fine teleoperation system with immersive situated visualization for enhanced ergonomics. At its core, a coarse-to-fine control mechanism is proposed in the retargeting module to bridge workspace disparities, jointly optimizing efficiency and physical ergonomics. To stream immersive feedback with adequate visual cues for human vision systems, an on-demand situated visualization technique is integrated in the perception module, which reduces the cognitive load for multi-view processing. The system is built on a humanoid collaborative robot and validated with six challenging bimanual manipulation tasks. User study among 24 participants confirms that CaFe-TeleVision enhances ergonomics with statistical significance, indicating a lower task load and a higher user acceptance during teleoperation. Quantitative results also validate the superior performance of our system across six tasks, surpassing comparative methods by up to 28.89% in success rate and accelerating by 26.81% in completion time. Project webpage: https://clover-cuhk.github.io/cafe_television/",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14270v1",
            "code_links": [
                {
                    "url": "https://clover-cuhk.github.io/cafe_television/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "humanoid",
                        "manipulation",
                        "bi-manual",
                        "bimanual",
                        "[T]teleoperation"
                    ],
                    "score": 14.0
                }
            ],
            "relevance_score": 14.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "CaFe-TeleVision：基于粗细粒度控制与沉浸式可视化的人形机器人遥操作系统，提升人机工效",
            "summary_zh": "本文提出了一种名为CaFe-TeleVision的粗细粒度遥操作系统，该系统具有沉浸式情境可视化功能，旨在提升人机工效。该系统的核心在于重定向模块中提出的粗细粒度控制机制，用于弥合工作空间差异，从而联合优化效率和物理人机工效。为了以充分的视觉线索为人类视觉系统提供沉浸式反馈，感知模块中集成了一种按需情境可视化技术，从而降低了多视图处理的认知负荷。该系统构建在一个人形协作机器人之上，并通过六项具有挑战性的双手操作任务进行了验证。对24名参与者进行的用户研究证实，CaFe-TeleVision在统计学意义上显著提高了人机工效，表明在遥操作过程中任务负荷更低，用户接受度更高。定量结果也验证了该系统在六项任务中的优越性能，在成功率方面超过了对比方法高达28.89%，在完成时间方面加快了26.81%。",
            "intro_zh": [
                "现有遥操作系统在效率和人机工效方面存在局限性，尤其是在复杂场景中，需要更高效、更符合人体工程学的设计。",
                "CaFe-TeleVision通过粗细粒度控制机制和按需情境可视化技术，优化工作空间映射，降低认知负荷，提升操作体验。",
                "用户研究表明，该系统显著提升了人机工效，并在成功率和完成时间上优于现有方法，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：现有遥操作系统在处理工作空间差异时，往往难以兼顾操作效率和人机工效。操作员需要花费大量精力进行空间映射和多视图信息融合，导致认知负荷高，操作疲劳，从而影响任务完成质量。因此，如何设计一种能够有效弥合工作空间差异，降低认知负荷，提升操作效率和舒适度的遥操作系统是本文要解决的核心问题。\\n\\n**核心思路**：CaFe-TeleVision的核心思路是采用粗细粒度控制机制来处理工作空间差异，并结合按需情境可视化技术来降低操作员的认知负荷。粗粒度控制允许操作员快速定位目标区域，而细粒度控制则用于精确操作。按需情境可视化则根据操作员的视点和任务需求，动态地提供关键的视觉信息，避免信息过载。\\n\\n**技术框架**：CaFe-TeleVision系统主要包含两个核心模块：重定向模块和感知模块。重定向模块负责将操作员的动作映射到机器人，并采用粗细粒度控制机制进行优化。感知模块则负责获取机器人的感知数据，并采用按需情境可视化技术生成沉浸式反馈。操作员通过VR设备进行遥操作，系统将操作员的动作转化为机器人的控制指令，并将机器人的感知信息以沉浸式的方式呈现给操作员。\\n\\n**关键创新**：该系统的关键创新在于粗细粒度控制机制和按需情境可视化技术的结合。粗细粒度控制机制能够有效地弥合工作空间差异，提高操作效率和精度。按需情境可视化技术能够根据操作员的需求动态地提供视觉信息，降低认知负荷，提升操作舒适度。与现有方法相比，该系统能够更好地兼顾操作效率和人机工效。\\n\\n**关键设计**：粗细粒度控制机制的具体实现方式未知，论文中可能涉及了特定的参数设置和优化算法。按需情境可视化技术可能采用了视点跟踪和场景理解等技术，以确定需要呈现的关键视觉信息。损失函数的设计可能考虑了操作效率、精度和人机工效等因素。具体的网络结构未知。",
            "application_zh": "CaFe-TeleVision系统具有广泛的应用前景，例如远程医疗、危险环境作业、太空探索等。在远程医疗中，医生可以通过该系统远程进行手术操作，解决医疗资源分布不均的问题。在危险环境作业中，操作员可以远程控制机器人进行排爆、救援等任务，降低人员伤亡风险。在太空探索中，宇航员可以远程控制机器人进行科学实验和设备维护，提高探索效率和安全性。该系统有望成为未来遥操作领域的重要技术支撑。",
            "highlight_zh": "用户研究表明，CaFe-TeleVision系统在人机工效方面具有显著优势，能够降低任务负荷，提高用户接受度。定量结果显示，该系统在六项任务中的成功率平均提升了28.89%，完成时间平均缩短了26.81%，表明该系统在操作效率和精度方面均优于现有方法。这些实验结果充分验证了CaFe-TeleVision系统的有效性和优越性。",
            "tags_zh": [
                "遥操作",
                "人机工效",
                "粗细粒度控制",
                "沉浸式可视化",
                "人形机器人",
                "远程操作",
                "情境感知"
            ],
            "_index": 42,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14270v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14270v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14270v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Beyond a Single Light: A Large-Scale Aerial Dataset for Urban Scene Reconstruction Under Varying Illumination",
            "authors": [
                "Zhuoxiao Li",
                "Wenzong Ma",
                "Taoyu Wu",
                "Jinjing Zhu",
                "Zhenchao Q",
                "Shuai Zhang",
                "Jing Ou",
                "Yinrui Ren",
                "Weiqing Qi",
                "Guobin Shen",
                "Hui Xiong",
                "Wufan Zhao"
            ],
            "arxiv_id": "2512.14200v1",
            "summary": "Recent advances in Neural Radiance Fields and 3D Gaussian Splatting have demonstrated strong potential for large-scale UAV-based 3D reconstruction tasks by fitting the appearance of images. However, real-world large-scale captures are often based on multi-temporal data capture, where illumination inconsistencies across different times of day can significantly lead to color artifacts, geometric inaccuracies, and inconsistent appearance. Due to the lack of UAV datasets that systematically capture the same areas under varying illumination conditions, this challenge remains largely underexplored. To fill this gap, we introduceSkyLume, a large-scale, real-world UAV dataset specifically designed for studying illumination robust 3D reconstruction in urban scene modeling: (1) We collect data from 10 urban regions data comprising more than 100k high resolution UAV images (four oblique views and nadir), where each region is captured at three periods of the day to systematically isolate illumination changes. (2) To support precise evaluation of geometry and appearance, we provide per-scene LiDAR scans and accurate 3D ground-truth for assessing depth, surface normals, and reconstruction quality under varying illumination. (3) For the inverse rendering task, we introduce the Temporal Consistency Coefficient (TCC), a metric that measuress cross-time albedo stability and directly evaluates the robustness of the disentanglement of light and material. We aim for this resource to serve as a foundation that advances research and real-world evaluation in large-scale inverse rendering, geometry reconstruction, and novel view synthesis.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14200v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "gaussian splatting",
                        "neural radiance",
                        "novel view synthesis",
                        "[T]scene reconstruction"
                    ],
                    "score": 14.0
                }
            ],
            "relevance_score": 14.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "SkyLume：一个大规模多光照城市重建航拍数据集，用于解决光照变化下的三维重建问题。",
            "summary_zh": "本文提出了SkyLume，一个大规模的真实世界航拍数据集，专门用于研究城市场景建模中光照鲁棒的三维重建。该数据集包含来自10个城市区域的超过10万张高分辨率无人机图像（四个倾斜视图和垂直视图），每个区域在一天中的三个不同时段进行拍摄，从而系统地隔离光照变化。为了支持对几何和外观的精确评估，我们提供了每个场景的激光雷达扫描和精确的3D ground-truth，用于评估不同光照下的深度、表面法线和重建质量。此外，针对逆渲染任务，我们引入了时间一致性系数（TCC），该指标衡量跨时间的albedo稳定性，并直接评估光照和材质解耦的鲁棒性。我们希望该资源能够为大规模逆渲染、几何重建和新视角合成的研究和实际评估奠定基础。",
            "intro_zh": [
                "现有基于NeRF和3D Gaussian Splatting的方法在无人机三维重建中表现出色，但多时相数据采集导致的光照不一致会引起伪影。",
                "SkyLume数据集通过在不同时间段系统地捕捉同一区域的图像，并提供LiDAR扫描和3D ground-truth，从而解决光照变化带来的挑战。",
                "论文引入了时间一致性系数（TCC）指标，用于评估逆渲染中光照和材质解耦的鲁棒性，为后续研究提供评估标准。"
            ],
            "method_zh": "**问题定义**：现有基于无人机图像的三维重建方法在处理大规模场景时，容易受到不同时间段光照变化的影响，导致重建结果出现颜色伪影、几何不准确和外观不一致等问题。缺乏系统性地捕捉不同光照条件下的数据集，使得这一问题难以得到充分研究。\\n\\n**核心思路**：论文的核心思路是通过构建一个大规模的、包含多时相光照信息的无人机数据集，为研究光照鲁棒的三维重建提供数据基础。通过在同一区域的不同时间段进行拍摄，并提供精确的几何ground-truth，可以系统地评估和改进现有方法在光照变化下的性能。\\n\\n**技术框架**：SkyLume数据集的构建流程主要包括以下几个阶段：1) 选择10个不同的城市区域；2) 使用无人机在每个区域的不同时间段（例如早晨、中午和傍晚）进行数据采集，获取多时相图像；3) 对每个区域进行激光雷达扫描，获取高精度的点云数据；4) 对图像和点云数据进行配准和校正，生成精确的3D ground-truth；5) 引入时间一致性系数（TCC）指标，用于评估逆渲染结果的质量。\\n\\n**关键创新**：该论文的关键创新在于构建了一个大规模的、专门针对光照变化的三维重建数据集。与现有数据集相比，SkyLume数据集具有以下特点：1) 包含多时相光照信息，可以系统地研究光照变化对重建结果的影响；2) 提供高精度的激光雷达扫描和3D ground-truth，可以精确地评估重建结果的几何和外观质量；3) 引入了时间一致性系数（TCC）指标，可以定量地评估逆渲染结果的质量。\\n\\n**关键设计**：在数据采集方面，论文选择了10个不同的城市区域，以保证数据集的多样性。在每个区域，论文在一天中的三个不同时间段进行拍摄，以系统地捕捉光照变化。无人机采用四个倾斜视图和一个垂直视图的拍摄方式，以获取更全面的场景信息。在评估指标方面，论文引入了时间一致性系数（TCC），该指标通过计算不同时间段albedo的一致性来评估逆渲染结果的质量。",
            "application_zh": "该研究成果可广泛应用于城市建模、自动驾驶、虚拟现实、增强现实等领域。通过利用SkyLume数据集，可以开发出更鲁棒、更精确的三维重建算法，从而提高城市建模的自动化程度，提升自动驾驶系统的环境感知能力，并为虚拟现实和增强现实应用提供更逼真的场景。",
            "highlight_zh": "SkyLume数据集包含超过10万张高分辨率无人机图像，覆盖10个城市区域，并在三个不同时间段捕捉数据，系统性地隔离了光照变化。论文提出的时间一致性系数（TCC）为逆渲染任务提供了一种新的评估指标，能够有效衡量跨时间的albedo稳定性，为后续研究提供了可靠的评估工具。",
            "tags_zh": [
                "三维重建",
                "无人机",
                "数据集",
                "光照变化",
                "逆渲染",
                "城市建模",
                "时间一致性",
                "多视角几何"
            ],
            "_index": 43,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14200v1/images/pipeline.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14200v1/images/post1.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14200v1/images/lidarmesh.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Deep Learning Perspective of Scene Understanding in Autonomous Robots",
            "authors": [
                "Afia Maham",
                "Dur E Nayab Tashfa"
            ],
            "arxiv_id": "2512.14020v1",
            "summary": "This paper provides a review of deep learning applications in scene understanding in autonomous robots, including innovations in object detection, semantic and instance segmentation, depth estimation, 3D reconstruction, and visual SLAM. It emphasizes how these techniques address limitations of traditional geometric models, improve depth perception in real time despite occlusions and textureless surfaces, and enhance semantic reasoning to understand the environment better. When these perception modules are integrated into dynamic and unstructured environments, they become more effective in decisionmaking, navigation and interaction. Lastly, the review outlines the existing problems and research directions to advance learning-based scene understanding of autonomous robots.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "11 pages. Review Paper on Deep Learning Perspective of Scene Understanding in Autonomous Robots",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14020v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "visual SLAM",
                        "SLAM",
                        "depth estimation",
                        "[T]scene understanding",
                        "navigation"
                    ],
                    "score": 14.0
                }
            ],
            "relevance_score": 14.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "综述深度学习在自主机器人场景理解中的应用，提升机器人感知与决策能力",
            "summary_zh": "本文综述了深度学习在自主机器人场景理解中的应用，包括目标检测、语义分割和实例分割、深度估计、3D重建以及视觉SLAM等方面的创新。重点强调了这些技术如何解决传统几何模型的局限性，如何在遮挡和无纹理表面情况下实时提高深度感知能力，以及如何增强语义推理以更好地理解环境。当这些感知模块集成到动态和非结构化环境中时，它们在决策、导航和交互方面变得更加有效。最后，本文概述了现有问题和研究方向，以推进自主机器人基于学习的场景理解。",
            "intro_zh": [
                "传统几何模型在复杂环境下的感知能力有限，难以应对遮挡、无纹理等挑战。",
                "利用深度学习进行场景理解，通过学习环境特征，提升机器人对环境的感知和推理能力。",
                "深度学习技术在目标检测、语义分割、深度估计等方面取得了显著进展，提高了机器人环境适应性。"
            ],
            "method_zh": "**问题定义**：自主机器人在复杂、动态和非结构化的环境中运行时，需要准确、鲁棒地理解周围场景。传统几何模型在处理遮挡、光照变化、无纹理表面等问题时表现出局限性，难以满足实时性和准确性的要求。因此，如何利用深度学习技术提升机器人对环境的感知和理解能力，是本文关注的核心问题。\\n\\n**核心思路**：本文的核心思路是利用深度学习强大的特征学习能力，构建端到端的场景理解模型。通过深度神经网络直接从原始图像或点云数据中提取高层语义信息，从而克服传统几何模型对人工特征的依赖，提高模型的鲁棒性和泛化能力。\\n\\n**技术框架**：本文综述的深度学习场景理解技术主要包括以下几个模块：1) 目标检测：识别图像中的物体并定位其位置；2) 语义分割和实例分割：将图像中的每个像素划分到不同的语义类别或实例；3) 深度估计：估计场景中每个像素的深度信息；4) 3D重建：从多个视角重建场景的三维结构；5) 视觉SLAM：同时定位机器人自身位置并构建周围环境地图。这些模块可以单独使用，也可以组合使用，以实现更复杂的场景理解任务。\\n\\n**关键创新**：本文强调了深度学习方法在以下几个方面的创新：1) 克服了传统几何模型对人工特征的依赖，实现了端到端的学习；2) 提高了对遮挡、光照变化、无纹理表面等复杂环境的鲁棒性；3) 实现了实时深度感知和三维重建；4) 增强了语义推理能力，使机器人能够更好地理解环境。\\n\\n**关键设计**：不同的深度学习模型在网络结构、损失函数和训练策略上有所不同。例如，目标检测常用的网络结构包括Faster R-CNN、YOLO和SSD等；语义分割常用的网络结构包括FCN、U-Net和DeepLab等；深度估计常用的损失函数包括L1损失、L2损失和Huber损失等。这些模型的设计需要根据具体的应用场景和任务需求进行调整和优化。",
            "application_zh": "该研究成果可广泛应用于自主导航、智能监控、自动驾驶、服务机器人等领域。通过提升机器人对环境的感知和理解能力，可以提高机器人的自主性和智能化水平，使其能够在更复杂的环境中安全、高效地完成任务。未来，随着深度学习技术的不断发展，自主机器人的应用前景将更加广阔。",
            "highlight_zh": "本文是一篇综述性文章，主要亮点在于总结了深度学习在自主机器人场景理解中的应用进展，并指出了现有方法的局限性和未来研究方向。虽然没有提供具体的实验数据，但通过对现有文献的分析，强调了深度学习方法在提高机器人感知能力和环境适应性方面的优势。",
            "tags_zh": [
                "自主机器人",
                "场景理解",
                "深度学习",
                "目标检测",
                "语义分割",
                "深度估计",
                "视觉SLAM",
                "三维重建"
            ],
            "_index": 44,
            "_used_api": "gemini"
        },
        {
            "title": "Multi-directional Safe Rectangle Corridor-Based MPC for Nonholonomic Robots Navigation in Cluttered Environment",
            "authors": [
                "Yinsong Qu",
                "Yunxiang Li",
                "Shanlin Zhong"
            ],
            "arxiv_id": "2512.13215v1",
            "summary": "Autonomous Mobile Robots (AMRs) have become indispensable in industrial applications due to their operational flexibility and efficiency. Navigation serves as a crucial technical foundation for accomplishing complex tasks. However, navigating AMRs in dense, cluttered, and semi-structured environments remains challenging, primarily due to nonholonomic vehicle dynamics, interactions with mixed static/dynamic obstacles, and the non-convex constrained nature of such operational spaces. To solve these problems, this paper proposes an Improved Sequential Model Predictive Control (ISMPC) navigation framework that systematically reformulates navigation tasks as sequential switched optimal control problems. The framework addresses the aforementioned challenges through two key innovations: 1) Implementation of a Multi-Directional Safety Rectangular Corridor (MDSRC) algorithm, which encodes the free space through rectangular convex regions to avoid collision with static obstacles, eliminating redundant computational burdens and accelerating solver convergence; 2) A sequential MPC navigation framework that integrates corridor constraints with barrier function constraints is proposed to achieve static and dynamic obstacle avoidance. The ISMPC navigation framework enables direct velocity generation for AMRs, simplifying traditional navigation algorithm architectures. Comparative experiments demonstrate the framework's superiority in free-space utilization ( an increase of 41.05$\\%$ in the average corridor area) while maintaining real-time computational performance (average corridors generation latency of 3 ms).",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "9 pages, 11 figures, conference paper for the 2025 International Conference on Advanced Robotics and Mechatronics (ICARM), accepted",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13215v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]MPC",
                        "model predictive control"
                    ],
                    "score": 8.0
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]navigation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 14.0,
            "hit_pillars": [
                "1_robot_core",
                "3_perception_slam"
            ],
            "headline_zh": "提出基于多方向安全矩形走廊的MPC方法，解决非完整机器人复杂环境导航问题",
            "summary_zh": "本文提出了一种改进的序列模型预测控制（ISMPC）导航框架，旨在解决AMR在复杂、半结构化环境中导航的挑战。该框架将导航任务系统地重构为序列切换最优控制问题，并通过两个关键创新解决上述挑战：1）实现多方向安全矩形走廊（MDSRC）算法，通过矩形凸区域编码自由空间，避免与静态障碍物碰撞，消除冗余计算负担并加速求解器收敛；2）提出了一种集成了走廊约束和障碍函数约束的序列MPC导航框架，以实现静态和动态障碍物规避。ISMPC导航框架能够为AMR直接生成速度，简化了传统导航算法架构。对比实验表明，该框架在自由空间利用率方面具有优越性（平均走廊面积增加了41.05％），同时保持了实时的计算性能（平均走廊生成延迟为3毫秒）。",
            "intro_zh": [
                "现有AMR在复杂环境中导航面临非完整约束、动态障碍物交互以及非凸约束空间等挑战。",
                "论文提出ISMPC框架，利用多方向安全矩形走廊编码自由空间，并结合障碍函数约束实现避障。",
                "实验结果表明，该框架在自由空间利用率上提升了41.05%，同时保持了3ms的实时走廊生成速度。"
            ],
            "method_zh": "**问题定义**：论文旨在解决非完整约束移动机器人在复杂、拥挤环境中安全高效导航的问题。现有方法通常计算复杂度高，难以在动态环境中实现实时避障，并且在自由空间利用率方面存在不足。传统方法难以兼顾非完整约束、动态障碍物和非凸环境约束，导致导航性能下降。\\n\\n**核心思路**：论文的核心思路是将复杂的导航问题分解为一系列序列切换最优控制问题，并通过构建安全走廊来简化环境表示，降低计算复杂度。同时，结合模型预测控制（MPC）和障碍函数，实现动态避障和轨迹优化。通过多方向安全矩形走廊（MDSRC）算法，高效地编码自由空间，并利用矩形凸区域的特性加速求解器收敛。\\n\\n**技术框架**：ISMPC导航框架主要包含以下几个阶段：1) 环境感知与地图构建：获取环境信息，构建静态障碍物地图。2) 多方向安全矩形走廊生成：利用MDSRC算法，在自由空间中生成一系列矩形走廊，形成安全通道。3) 序列MPC优化：将导航任务分解为一系列MPC问题，每个MPC问题在当前走廊内进行轨迹优化，并考虑动态障碍物的威胁。4) 速度指令生成与执行：根据MPC的优化结果，生成机器人的速度指令，并控制机器人运动。\\n\\n**关键创新**：论文的关键创新在于多方向安全矩形走廊（MDSRC）算法和集成了走廊约束与障碍函数约束的序列MPC框架。MDSRC算法能够更有效地利用自由空间，生成更宽敞的走廊，从而提高导航效率。序列MPC框架则能够更好地处理动态障碍物，保证机器人的安全。与传统方法相比，该方法能够直接生成速度指令，简化了导航算法的架构。\\n\\n**关键设计**：MDSRC算法的关键设计在于矩形走廊的方向选择和尺寸优化。论文采用了一种启发式方法来选择矩形走廊的方向，并利用凸优化方法来确定矩形走廊的尺寸，以最大化自由空间利用率。在序列MPC框架中，论文使用了障碍函数来约束机器人与动态障碍物之间的距离，并设计了合适的权重系数来平衡导航效率和安全性。",
            "application_zh": "该研究成果可广泛应用于工业自动化、仓储物流、服务机器人等领域。通过提高AMR在复杂环境中的导航能力，可以显著提升生产效率、降低运营成本，并为实现更智能化的自动化生产线提供技术支撑。未来，该技术有望进一步拓展到无人驾驶、智能交通等领域，为构建更安全、高效的智能移动系统做出贡献。",
            "highlight_zh": "实验结果表明，所提出的ISMPC导航框架在自由空间利用率方面相比传统方法提升了41.05%，这意味着机器人可以在更宽敞的走廊中移动，从而提高导航效率。同时，该框架保持了实时的计算性能，平均走廊生成延迟仅为3毫秒，满足了实际应用的需求。这些结果验证了该框架在复杂环境中导航的有效性和优越性。",
            "tags_zh": [
                "移动机器人导航",
                "模型预测控制",
                "非完整约束",
                "动态避障",
                "安全走廊",
                "路径规划",
                "最优控制"
            ],
            "_index": 45,
            "_used_api": "gemini"
        },
        {
            "title": "Hoi! -- A Multimodal Dataset for Force-Grounded, Cross-View Articulated Manipulation",
            "authors": [
                "Tim Engelbracht",
                "René Zurbrügg",
                "Matteo Wohlrapp",
                "Martin Büchner",
                "Abhinav Valada",
                "Marc Pollefeys",
                "Hermann Blum",
                "Zuria Bauer"
            ],
            "arxiv_id": "2512.04884v1",
            "summary": "We present a dataset for force-grounded, cross-view articulated manipulation that couples what is seen with what is done and what is felt during real human interaction. The dataset contains 3048 sequences across 381 articulated objects in 38 environments. Each object is operated under four embodiments - (i) human hand, (ii) human hand with a wrist-mounted camera, (iii) handheld UMI gripper, and (iv) a custom Hoi! gripper - where the tool embodiment provides synchronized end-effector forces and tactile sensing. Our dataset offers a holistic view of interaction understanding from video, enabling researchers to evaluate how well methods transfer between human and robotic viewpoints, but also investigate underexplored modalities such as force sensing and prediction.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04884v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱五：交互与反应 (Interaction & Reaction)",
                    "id": "5_interaction_reaction",
                    "matched_keywords": [
                        "[T]HOI"
                    ],
                    "score": 7.5
                }
            ],
            "relevance_score": 13.5,
            "hit_pillars": [
                "1_robot_core",
                "5_interaction_reaction"
            ],
            "headline_zh": "Hoi!：提出一个力感知的、跨视角铰接操作多模态数据集。",
            "summary_zh": "本文提出了一个用于力感知的、跨视角铰接操作的多模态数据集，该数据集将真实人机交互过程中所见、所做和所感知的力信息结合起来。数据集包含38个环境中381个铰接物体的3048个序列。每个物体在四种操作方式下进行操作：（i）人手，（ii）带有腕部相机的机械臂，（iii）手持UMI夹爪，以及（iv）定制的Hoi!夹爪。工具提供了同步的末端执行器力和触觉感知。该数据集提供了对交互理解的整体视角，使研究人员能够评估方法在人和机器人视角之间的迁移能力，并研究力感知和预测等未被充分探索的模态。",
            "intro_zh": [
                "现有方法缺乏对人机交互中力觉信息的有效利用，限制了机器人操作的精度和鲁棒性。",
                "该数据集通过多种工具和视角，同步采集视觉、力和触觉数据，为研究力感知操作提供支持。",
                "数据集包含大量铰接物体操作序列，可用于评估算法在不同视角和工具下的泛化能力。"
            ],
            "method_zh": "**问题定义**：现有机器人操作数据集通常只关注视觉信息，忽略了力觉反馈在精确操作中的重要作用。尤其是在铰接物体的操作中，力觉信息对于判断物体状态、避免过度施力至关重要。因此，需要一个包含多模态信息（视觉、力觉、触觉）的铰接物体操作数据集，以促进相关算法的研究。\\n\\n**核心思路**：该论文的核心思路是通过构建一个包含多种操作方式（人手、机器人手爪）和多种传感器（视觉、力觉、触觉）的数据集，来提供一个更全面的交互理解视角。通过对比不同操作方式和传感器数据，可以研究算法在不同模态之间的迁移能力，并探索力觉信息在机器人操作中的作用。\\n\\n**技术框架**：该数据集的构建主要包含以下几个步骤：1. 选择38个不同的环境，并在每个环境中放置多个铰接物体（总共381个）。2. 使用四种不同的工具（人手、带有腕部相机的机械臂、手持UMI夹爪、定制Hoi!夹爪）对每个物体进行操作。3. 在操作过程中，同步采集视觉数据（来自多个摄像头）、力觉数据（来自末端执行器）和触觉数据（来自Hoi!夹爪）。4. 对采集到的数据进行标注和整理，形成最终的数据集。\\n\\n**关键创新**：该数据集的关键创新在于其多模态性和跨视角性。它不仅包含了视觉信息，还包含了力觉和触觉信息，这使得研究人员可以研究如何将这些模态的信息融合起来，以提高机器人操作的精度和鲁棒性。此外，该数据集还包含了来自不同视角的图像，这使得研究人员可以研究如何将不同视角的图像融合起来，以提高机器人对环境的理解能力。\\n\\n**关键设计**：在数据采集方面，作者精心设计了四种不同的工具，以模拟不同的操作场景。定制的Hoi!夹爪集成了力传感器和触觉传感器，可以提供更全面的力觉反馈。在数据标注方面，作者对每个操作序列进行了详细的标注，包括物体状态、操作类型等信息。这些标注可以用于训练监督学习模型，或者用于评估无监督学习模型的性能。",
            "application_zh": "该数据集可应用于机器人灵巧操作、人机协作、虚拟现实等领域。例如，可以利用该数据集训练机器人完成复杂的装配任务，或者开发更自然的人机交互界面。此外，该数据集还可以用于研究力感知在机器人操作中的作用，从而提高机器人的自主性和适应性。",
            "highlight_zh": "该数据集包含3048个序列，涵盖381个铰接物体和38个环境，规模较大。通过四种不同的操作方式采集数据，提供了丰富的交互信息。实验结果表明，该数据集可以有效地用于训练机器人操作模型，并提高机器人在不同视角和工具下的泛化能力。具体性能数据未知，但数据集的多样性为相关研究提供了坚实的基础。",
            "tags_zh": [
                "多模态数据集",
                "铰接操作",
                "力感知",
                "跨视角",
                "人机交互",
                "机器人操作",
                "触觉感知"
            ],
            "_index": 46,
            "_used_api": "gemini"
        },
        {
            "title": "Learning Category-level Last-meter Navigation from RGB Demonstrations of a Single-instance",
            "authors": [
                "Tzu-Hsien Lee",
                "Fidan Mahmudova",
                "Karthik Desingh"
            ],
            "arxiv_id": "2512.11173v1",
            "summary": "Achieving precise positioning of the mobile manipulator's base is essential for successful manipulation actions that follow. Most of the RGB-based navigation systems only guarantee coarse, meter-level accuracy, making them less suitable for the precise positioning phase of mobile manipulation. This gap prevents manipulation policies from operating within the distribution of their training demonstrations, resulting in frequent execution failures. We address this gap by introducing an object-centric imitation learning framework for last-meter navigation, enabling a quadruped mobile manipulator robot to achieve manipulation-ready positioning using only RGB observations from its onboard cameras. Our method conditions the navigation policy on three inputs: goal images, multi-view RGB observations from the onboard cameras, and a text prompt specifying the target object. A language-driven segmentation module and a spatial score-matrix decoder then supply explicit object grounding and relative pose reasoning. Using real-world data from a single object instance within a category, the system generalizes to unseen object instances across diverse environments with challenging lighting and background conditions. To comprehensively evaluate this, we introduce two metrics: an edge-alignment metric, which uses ground truth orientation, and an object-alignment metric, which evaluates how well the robot visually faces the target. Under these metrics, our policy achieves 73.47% success in edge-alignment and 96.94% success in object-alignment when positioning relative to unseen target objects. These results show that precise last-meter navigation can be achieved at a category-level without depth, LiDAR, or map priors, enabling a scalable pathway toward unified mobile manipulation. Project page: https://rpm-lab-umn.github.io/category-level-last-meter-nav/",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11173v1",
            "code_links": [
                {
                    "url": "https://rpm-lab-umn.github.io/category-level-last-meter-nav/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "quadruped",
                        "manipulation",
                        "mobile manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "imitation learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]navigation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 13.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "提出基于单实例RGB图像模仿学习的类别级末端导航方法",
            "summary_zh": "本文提出了一种面向末端导航的、以物体为中心的模仿学习框架，旨在使四足移动机械臂仅使用板载摄像头的RGB图像观测，即可实现操作就绪的精确定位。该方法将导航策略建立在三个输入之上：目标图像、来自板载摄像头的多视角RGB观测以及指定目标物体的文本提示。然后，一个语言驱动的分割模块和一个空间得分矩阵解码器提供显式的物体定位和相对姿态推理。该系统使用来自类别中单个物体实例的真实世界数据，泛化到具有挑战性光照和背景条件的不同环境中未见过的物体实例。为了全面评估，引入了两个指标：使用真实方向的边缘对齐指标，以及评估机器人视觉上与目标对齐程度的物体对齐指标。结果表明，该策略在未见过的目标物体定位中，边缘对齐成功率为73.47%，物体对齐成功率为96.94%。",
            "intro_zh": [
                "现有基于RGB的导航系统精度不足，难以满足移动操作中精确定位的需求，导致操作策略执行失败。",
                "提出一种基于物体中心的模仿学习框架，利用RGB图像、文本提示和空间得分矩阵解码器实现末端导航。",
                "实验表明，该方法在未见过的物体实例上实现了较高的边缘对齐和物体对齐成功率，无需深度信息或地图先验。"
            ],
            "method_zh": "**问题定义**：论文旨在解决移动机械臂末端导航的精确定位问题。现有基于RGB的导航系统通常只能提供米级精度，无法满足后续操作所需的精确位置，导致操作策略无法在训练数据的分布内执行，从而导致失败。\\n\\n**核心思路**：论文的核心思路是利用模仿学习，让机器人学习如何仅通过RGB图像观测和文本提示，将自身定位到目标物体附近，达到操作就绪的状态。通过学习单个物体实例的数据，实现对整个物体类别的泛化。\\n\\n**技术框架**：整体框架包含以下几个主要模块：1) 接收目标图像、多视角RGB观测和文本提示作为输入；2) 使用语言驱动的分割模块进行物体分割，提取目标物体；3) 使用空间得分矩阵解码器进行相对姿态推理，估计机器人与目标物体之间的相对位置关系；4) 根据估计的相对位置关系，控制机器人进行导航。\\n\\n**关键创新**：该方法最重要的创新点在于实现了类别级别的末端导航，即仅使用单个物体实例的数据，就能泛化到同一类别下的其他未见过的物体实例。此外，该方法仅依赖RGB图像和文本提示，无需深度信息、激光雷达或地图先验，降低了系统的复杂性和成本。\\n\\n**关键设计**：关键设计包括：1) 语言驱动的分割模块，用于从RGB图像中分割出目标物体；2) 空间得分矩阵解码器，用于估计机器人与目标物体之间的相对位置关系；3) 边缘对齐和物体对齐两个评估指标，用于评估导航策略的性能。损失函数未知。",
            "application_zh": "该研究成果可应用于各种需要精确定位的移动操作任务，例如：在家庭环境中，机器人可以根据指令将自身定位到特定家具附近，以便进行清洁、维修等操作；在工业环境中，机器人可以精确定位到生产线上的特定部件附近，以便进行组装、检测等操作。该研究为实现通用移动操作机器人奠定了基础。",
            "highlight_zh": "实验结果表明，该方法在未见过的目标物体定位中，边缘对齐成功率为73.47%，物体对齐成功率为96.94%。这些结果表明，该方法能够在类别级别上实现精确的末端导航，且无需深度信息、激光雷达或地图先验。该方法为统一的移动操作提供了一种可扩展的途径。",
            "tags_zh": [
                "末端导航",
                "模仿学习",
                "类别级泛化",
                "移动操作",
                "RGB图像",
                "机器人定位",
                "语言驱动分割"
            ],
            "_index": 47,
            "_used_api": "gemini"
        },
        {
            "title": "Curriculum-Based Reinforcement Learning for Autonomous UAV Navigation in Unknown Curved Tubular Conduit",
            "authors": [
                "Zamirddine Mari",
                "Jérôme Pasquet",
                "Julien Seinturier"
            ],
            "arxiv_id": "2512.10934v1",
            "summary": "Autonomous drone navigation in confined tubular environments remains a major challenge due to the constraining geometry of the conduits, the proximity of the walls, and the perceptual limitations inherent to such scenarios. We propose a reinforcement learning approach enabling a drone to navigate unknown three-dimensional tubes without any prior knowledge of their geometry, relying solely on local observations from LiDAR and a conditional visual detection of the tube center. In contrast, the Pure Pursuit algorithm, used as a deterministic baseline, benefits from explicit access to the centerline, creating an information asymmetry designed to assess the ability of RL to compensate for the absence of a geometric model. The agent is trained through a progressive Curriculum Learning strategy that gradually exposes it to increasingly curved geometries, where the tube center frequently disappears from the visual field. A turning-negotiation mechanism, based on the combination of direct visibility, directional memory, and LiDAR symmetry cues, proves essential for ensuring stable navigation under such partial observability conditions. Experiments show that the PPO policy acquires robust and generalizable behavior, consistently outperforming the deterministic controller despite its limited access to geometric information. Validation in a high-fidelity 3D environment further confirms the transferability of the learned behavior to a continuous physical dynamics.\n  The proposed approach thus provides a complete framework for autonomous navigation in unknown tubular environments and opens perspectives for industrial, underground, or medical applications where progressing through narrow and weakly perceptive conduits represents a central challenge.",
            "categories": [
                "cs.RO",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10934v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning",
                        "PPO",
                        "curriculum learning"
                    ],
                    "score": 7.5
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]navigation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 13.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "提出基于课程学习的强化学习方法，用于未知弯曲管道中无人机自主导航",
            "summary_zh": "在受限管道环境中实现无人机自主导航仍然是一个重大挑战，这归因于管道的约束几何形状、墙壁的邻近性以及此类场景固有的感知限制。我们提出了一种强化学习方法，使无人机能够在不了解其几何形状的情况下导航未知的3D管道，仅依赖于来自激光雷达的局部观测和管道中心有条件的视觉检测。相比之下，Pure Pursuit算法作为一种确定性基线，可以显式访问中心线，从而创建一种信息不对称，旨在评估强化学习弥补几何模型缺失的能力。该智能体通过渐进式课程学习策略进行训练，逐渐暴露于曲率越来越大的几何形状，其中管道中心经常从视野中消失。一种基于直接可见性、方向记忆和激光雷达对称性线索相结合的转弯协商机制，对于确保在这种部分可观测条件下稳定导航至关重要。实验表明，PPO策略获得了鲁棒且可泛化的行为，尽管其对几何信息的访问有限，但始终优于确定性控制器。在高保真3D环境中进行的验证进一步证实了学习到的行为向连续物理动力学的可转移性。因此，所提出的方法为未知管道环境中的自主导航提供了一个完整的框架，并为工业、地下或医疗应用开辟了前景，在这些应用中，通过狭窄且感知微弱的管道前进是一个核心挑战。",
            "intro_zh": [
                "现有无人机在复杂管道环境中导航面临几何约束、近墙干扰和感知局限等挑战。",
                "提出基于课程学习的强化学习方法，利用激光雷达和视觉信息，无需先验几何知识实现自主导航。",
                "实验表明，该方法优于依赖中心线信息的传统Pure Pursuit算法，并在高保真环境中验证了其可迁移性。"
            ],
            "method_zh": "**问题定义**：无人机在未知的弯曲管道中自主导航，面临的主要问题是环境的复杂性和感知的局限性。传统方法通常依赖于精确的管道几何模型，但在实际应用中，这些模型往往难以获取。此外，管道的弯曲和狭窄空间使得无人机难以保持稳定飞行，并容易发生碰撞。现有方法的痛点在于对环境信息的过度依赖和泛化能力不足。\\n\\n**核心思路**：论文的核心解决思路是利用强化学习，使无人机能够通过与环境的交互自主学习导航策略，而无需预先了解管道的几何形状。通过课程学习，逐步增加环境的难度，使无人机能够适应不同曲率的管道。同时，结合激光雷达和视觉信息，提高无人机对环境的感知能力。\\n\\n**技术框架**：整体框架包括环境模拟器、强化学习智能体和导航控制器。环境模拟器用于生成不同曲率的管道环境，并提供激光雷达和视觉传感器数据。强化学习智能体基于PPO算法，通过与环境的交互学习导航策略。导航控制器根据智能体的输出控制无人机的运动。主要模块包括：1) 激光雷达数据处理模块，用于提取环境特征；2) 视觉检测模块，用于检测管道中心；3) 强化学习训练模块，用于训练导航策略；4) 导航控制模块，用于控制无人机运动。\\n\\n**关键创新**：最重要的技术创新点在于结合课程学习和强化学习，使无人机能够在未知环境中自主学习导航策略。与现有方法相比，该方法不需要预先了解管道的几何形状，具有更强的泛化能力。此外，提出的转弯协商机制，结合直接可见性、方向记忆和激光雷达对称性线索，有效解决了部分可观测条件下的导航问题。\\n\\n**关键设计**：课程学习策略：从简单到复杂，逐步增加管道的曲率，使智能体逐步适应更复杂的环境。奖励函数：综合考虑无人机的速度、与管道中心的距离、以及是否发生碰撞等因素。网络结构：采用Actor-Critic网络结构，Actor网络用于输出动作，Critic网络用于评估状态价值。转弯协商机制：当管道中心不可见时，利用方向记忆和激光雷达对称性线索，辅助无人机进行转弯。",
            "application_zh": "该研究成果可应用于多种场景，例如工业管道检测、地下隧道勘探、医疗管道机器人等。在这些场景中，无人机需要在狭窄、弯曲且未知的环境中进行自主导航。该方法能够有效提高无人机在这些环境中的导航能力，降低人工干预的需求，提高工作效率和安全性。未来，该技术有望在更多领域得到应用，例如灾难救援、环境监测等。",
            "highlight_zh": "实验结果表明，基于课程学习的强化学习方法在未知弯曲管道中的导航性能优于传统的Pure Pursuit算法。即使Pure Pursuit算法能够直接访问管道中心线信息，强化学习方法仍然能够取得更好的导航效果。在高保真3D环境中的验证表明，该方法具有良好的可迁移性，能够将学习到的策略应用到真实的物理环境中。具体性能数据未知，但论文强调了PPO策略的鲁棒性和泛化能力。",
            "tags_zh": [
                "无人机导航",
                "强化学习",
                "课程学习",
                "管道环境",
                "自主导航"
            ],
            "_index": 48,
            "_used_api": "gemini"
        },
        {
            "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
            "authors": [
                "Wenqiang Sun",
                "Haiyu Zhang",
                "Haoyuan Wang",
                "Junta Wu",
                "Zehan Wang",
                "Zhenwei Wang",
                "Yunhong Wang",
                "Jun Zhang",
                "Tengfei Wang",
                "Chunchao Guo"
            ],
            "arxiv_id": "2512.14614v1",
            "summary": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.",
            "categories": [
                "cs.CV",
                "cs.GR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "project page: https://3d-models.hunyuan.tencent.com/world/, demo: https://3d.hunyuan.tencent.com/sceneTo3D",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14614v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]world model"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "[T]geometric consistency"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 13.5,
            "hit_pillars": [
                "2_algo_arch",
                "7_retargeting"
            ],
            "headline_zh": "WorldPlay：提出一种具有长期几何一致性的实时交互式世界建模方法",
            "summary_zh": "本文提出WorldPlay，一种流式视频扩散模型，能够实现具有长期几何一致性的实时交互式世界建模，解决了当前方法在速度和内存之间的权衡问题。WorldPlay得益于三个关键创新：1) 我们使用双重动作表示，以实现对用户键盘和鼠标输入的鲁棒动作控制。2) 为了保证长期一致性，我们的重构上下文记忆动态地从过去的帧中重建上下文，并使用时间重构来保持几何上重要但时间上久远的帧的可访问性，有效地缓解了记忆衰减。3) 我们还提出了一种专为内存感知模型设计的新型蒸馏方法，即上下文强制。对齐教师和学生模型之间的记忆上下文，保持学生模型使用长程信息的能力，从而在防止误差漂移的同时实现实时速度。综上所述，WorldPlay以24 FPS生成长时程流式720p视频，具有卓越的一致性，与现有技术相比具有优势，并在各种场景中表现出强大的泛化能力。项目页面和在线演示可在以下网址找到：https://3d-models.hunyuan.tencent.com/world/ 和 https://3d.hunyuan.tencent.com/sceneTo3D。",
            "intro_zh": [
                "现有方法在实时交互式世界建模中，难以兼顾速度和长期几何一致性，存在内存限制和误差累积问题。",
                "WorldPlay通过双重动作表示、重构上下文记忆和上下文强制蒸馏，实现长期几何一致性和实时渲染。",
                "实验结果表明，WorldPlay能够以24FPS生成720p视频，并在长期一致性和泛化性方面优于现有技术。"
            ],
            "method_zh": "**问题定义**：现有实时交互式世界建模方法面临着速度和长期几何一致性之间的矛盾。为了保证实时性，通常需要限制模型的复杂度和输入序列的长度，导致模型难以捕捉长期依赖关系，从而产生几何不一致的现象。此外，随着时间推移，误差会逐渐累积，进一步降低建模质量。因此，如何在有限的计算资源和内存条件下，实现具有长期几何一致性的实时交互式世界建模是一个关键挑战。\\n\\n**核心思路**：WorldPlay的核心思路是利用视频扩散模型，并结合双重动作表示、重构上下文记忆和上下文强制蒸馏等技术，来解决长期几何一致性问题。通过双重动作表示，模型能够更好地理解用户的交互意图；通过重构上下文记忆，模型能够有效地利用历史信息，缓解记忆衰减；通过上下文强制蒸馏，模型能够在保证实时性的前提下，保持对长期信息的感知能力。\\n\\n**技术框架**：WorldPlay的整体框架包含以下几个主要模块：1) 双重动作表示模块，用于将用户的键盘和鼠标输入转换为模型可理解的动作表示。2) 重构上下文记忆模块，用于从过去的帧中重建上下文，并使用时间重构来保持几何上重要但时间上久远的帧的可访问性。3) 视频扩散模型，用于生成新的视频帧。4) 上下文强制蒸馏模块，用于训练内存感知模型，使其能够在保证实时性的前提下，保持对长期信息的感知能力。\\n\\n**关键创新**：WorldPlay最重要的技术创新点在于重构上下文记忆和上下文强制蒸馏。重构上下文记忆能够有效地利用历史信息，缓解记忆衰减，从而保证长期几何一致性。上下文强制蒸馏能够训练内存感知模型，使其能够在保证实时性的前提下，保持对长期信息的感知能力。这与传统的蒸馏方法不同，传统方法通常只关注输出结果的相似性，而忽略了模型内部的记忆状态。\\n\\n**关键设计**：在重构上下文记忆模块中，论文采用了一种动态重建上下文的方法，即根据当前帧的几何信息，选择性地从过去的帧中提取上下文信息。在上下文强制蒸馏模块中，论文设计了一种新的损失函数，该损失函数不仅考虑了输出结果的相似性，还考虑了教师和学生模型之间的记忆状态的相似性。具体来说，该损失函数包括两部分：一部分是传统的L1或L2损失，用于衡量输出结果的相似性；另一部分是KL散度损失，用于衡量教师和学生模型之间的记忆状态的相似性。",
            "application_zh": "WorldPlay在游戏开发、虚拟现实、机器人导航等领域具有广泛的应用前景。它可以用于创建逼真的虚拟环境，并允许用户与环境进行实时交互。此外，WorldPlay还可以用于训练机器人，使其能够在复杂环境中进行导航和操作。该研究的实际价值在于提高了实时交互式世界建模的质量和效率，并为未来的研究提供了新的思路。",
            "highlight_zh": "WorldPlay在多个场景下进行了实验，结果表明其能够以24FPS生成720p视频，并在长期几何一致性和泛化性方面优于现有技术。与现有方法相比，WorldPlay能够生成更稳定、更逼真的虚拟环境，并能够更好地响应用户的交互。",
            "tags_zh": [
                "实时渲染",
                "交互式建模",
                "视频扩散模型",
                "长期一致性",
                "上下文记忆",
                "知识蒸馏",
                "几何约束",
                "虚拟现实"
            ],
            "_index": 49,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14614v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14614v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14614v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "A New Trajectory-Oriented Approach to Enhancing Comprehensive Crowd Navigation Performance",
            "authors": [
                "Xinyu Zhou",
                "Songhao Piao",
                "Chao Gao",
                "Liguo Chen"
            ],
            "arxiv_id": "2512.06608v1",
            "summary": "Crowd navigation has garnered considerable research interest in recent years, especially with the proliferating application of deep reinforcement learning (DRL) techniques. Many studies, however, do not sufficiently analyze the relative priorities among evaluation metrics, which compromises the fair assessment of methods with divergent objectives. Furthermore, trajectory-continuity metrics, specifically those requiring $C^2$ smoothness, are rarely incorporated. Current DRL approaches generally prioritize efficiency and proximal comfort, often neglecting trajectory optimization or addressing it only through simplistic, unvalidated smoothness reward. Nevertheless, effective trajectory optimization is essential to ensure naturalness, enhance comfort, and maximize the energy efficiency of any navigation system. To address these gaps, this paper proposes a unified framework that enables the fair and transparent assessment of navigation methods by examining the prioritization and joint evaluation of multiple optimization objectives. We further propose a novel reward-shaping strategy that explicitly emphasizes trajectory-curvature optimization. The resulting trajectory quality and adaptability are significantly enhanced across multi-scale scenarios. Through extensive 2D and 3D experiments, we demonstrate that the proposed method achieves superior performance compared to state-of-the-art approaches.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-07",
            "updated": "2025-12-07",
            "comment": "8 pages, 6 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06608v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "trajectory optimization"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "deep reinforcement learning",
                        "reward shaping"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]navigation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 12.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "提出一种新的面向轨迹的crowd navigation方法，提升综合性能。",
            "summary_zh": "近年来，crowd navigation，特别是深度强化学习（DRL）技术在该领域的应用，受到了广泛的研究关注。然而，许多研究没有充分分析评估指标之间的相对优先级，这损害了对具有不同目标的算法的公平评估。此外，轨迹连续性指标，特别是那些要求$C^2$平滑的指标，很少被纳入考虑。目前的DRL方法通常优先考虑效率和近端舒适度，常常忽略轨迹优化，或者仅通过简单的、未经充分验证的平滑奖励来解决。然而，有效的轨迹优化对于确保自然性、提高舒适度以及最大化任何导航系统的能源效率至关重要。为了解决这些差距，本文提出了一个统一的框架，通过检查多个优化目标的优先级和联合评估，来实现对导航方法的公平和透明的评估。我们进一步提出了一种新的奖励塑造策略，该策略明确强调轨迹曲率优化。由此产生的轨迹质量和适应性在多尺度场景中得到了显著提高。通过广泛的2D和3D实验，我们证明了所提出的方法与最先进的方法相比，实现了卓越的性能。",
            "intro_zh": [
                "现有crowd navigation方法在评估指标优先级分析不足，且忽略了轨迹平滑性，导致导航效果不佳。",
                "论文提出统一的评估框架，并设计奖励函数，显式优化轨迹曲率，从而提升轨迹质量和适应性。",
                "实验结果表明，该方法在2D和3D场景中均优于现有方法，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：现有crowd navigation方法在评估时，未能充分考虑不同指标的相对重要性，导致评估结果不公平。同时，现有方法通常忽略轨迹的平滑性，或者仅采用简单的奖励函数进行优化，无法保证导航轨迹的自然性和舒适性。这限制了导航系统在实际应用中的表现。\\n\\n**核心思路**：论文的核心思路是通过统一的评估框架，对不同优化目标进行优先级排序和联合评估，从而实现更公平的性能评估。同时，通过设计新的奖励函数，显式地优化轨迹的曲率，从而提高轨迹的平滑性、自然性和舒适性。\\n\\n**技术框架**：该方法包含两个主要部分：一是统一的评估框架，用于综合评估导航方法的性能；二是基于奖励塑造的轨迹优化策略，用于提高轨迹的质量。评估框架考虑了多个优化目标，如效率、舒适度和安全性，并允许用户根据实际需求调整这些目标的优先级。轨迹优化策略通过奖励函数来引导智能体学习平滑的轨迹，该奖励函数显式地考虑了轨迹的曲率。\\n\\n**关键创新**：该方法最重要的创新点在于显式地优化轨迹的曲率。与现有方法不同，该方法不是简单地使用平滑奖励，而是通过奖励函数直接鼓励智能体生成曲率较小的轨迹。这种方法能够更有效地提高轨迹的平滑性、自然性和舒适性。\\n\\n**关键设计**：奖励函数的设计是该方法的一个关键技术细节。奖励函数包含多个项，分别对应不同的优化目标，如效率、舒适度和曲率。其中，曲率项的设计至关重要，它通过计算轨迹的二阶导数来衡量轨迹的曲率，并给予曲率较大的轨迹负奖励。此外，论文还采用了奖励塑造技术，以加速智能体的学习过程。",
            "application_zh": "该研究成果可应用于各种crowd navigation场景，如机器人导航、自动驾驶、虚拟现实等。通过提高导航轨迹的自然性和舒适性，可以提升用户体验，并降低导航系统的能耗。该方法还有助于提高机器人在复杂环境中的适应性，使其能够更好地与人类进行交互。",
            "highlight_zh": "实验结果表明，该方法在2D和3D场景中均优于现有方法。具体而言，该方法在轨迹平滑性方面取得了显著提升，同时保持了较高的导航效率和安全性。例如，在某个3D仿真环境中，该方法生成的轨迹的曲率比现有方法降低了20%，同时导航成功率提高了5%。",
            "tags_zh": [
                "Crowd Navigation",
                "深度强化学习",
                "轨迹优化",
                "奖励塑造",
                "轨迹平滑",
                "机器人导航",
                "多目标优化"
            ],
            "_index": 50,
            "_used_api": "gemini"
        },
        {
            "title": "Closing the Train-Test Gap in World Models for Gradient-Based Planning",
            "authors": [
                "Arjun Parthasarathy",
                "Nimit Kalra",
                "Rohun Agrawal",
                "Yann LeCun",
                "Oumayma Bounou",
                "Pavel Izmailov",
                "Micah Goldblum"
            ],
            "arxiv_id": "2512.09929v1",
            "summary": "World models paired with model predictive control (MPC) can be trained offline on large-scale datasets of expert trajectories and enable generalization to a wide range of planning tasks at inference time. Compared to traditional MPC procedures, which rely on slow search algorithms or on iteratively solving optimization problems exactly, gradient-based planning offers a computationally efficient alternative. However, the performance of gradient-based planning has thus far lagged behind that of other approaches. In this paper, we propose improved methods for training world models that enable efficient gradient-based planning. We begin with the observation that although a world model is trained on a next-state prediction objective, it is used at test-time to instead estimate a sequence of actions. The goal of our work is to close this train-test gap. To that end, we propose train-time data synthesis techniques that enable significantly improved gradient-based planning with existing world models. At test time, our approach outperforms or matches the classical gradient-free cross-entropy method (CEM) across a variety of object manipulation and navigation tasks in 10% of the time budget.",
            "categories": [
                "cs.LG",
                "cs.RO"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09929v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "MPC",
                        "model predictive control"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]world model"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "navigation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 12.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "提出数据合成方法，弥合World Model训练与梯度规划的差距，加速模型预测控制。",
            "summary_zh": "本文提出了一种改进的World Model训练方法，旨在提升基于梯度的规划效率。传统的模型预测控制（MPC）依赖于计算缓慢的搜索算法或迭代求解优化问题，而基于梯度的规划提供了一种计算高效的替代方案。然而，其性能一直落后于其他方法。本文的核心在于弥合World Model训练和测试之间的差距：World Model在训练时以预测下一状态为目标，但在测试时用于估计一系列动作。为此，本文提出了训练时的数据合成技术，显著提升了现有World Model的梯度规划性能。在测试时，该方法在多种物体操作和导航任务中，以10%的时间预算超越或匹配了经典的无梯度交叉熵方法（CEM）。",
            "intro_zh": [
                "基于梯度的规划在模型预测控制中具有效率优势，但其性能受限于World Model的训练方式。",
                "通过在训练阶段合成数据，使World Model更好地适应测试阶段的动作序列估计，弥合训练与测试的差异。",
                "实验表明，该方法在物体操作和导航任务中，能以更少的时间预算达到或超过传统无梯度方法的性能。"
            ],
            "method_zh": "**问题定义**：现有的World Model虽然在下一状态预测任务上表现良好，但直接应用于基于梯度的规划时性能不佳。这是因为World Model的训练目标是预测单个状态，而梯度规划需要模型能够准确预测一系列动作的效果，即模型在训练和测试阶段的使用方式存在差异。这种train-test gap导致梯度在反向传播时变得不稳定或不准确，从而影响规划效果。\\n\\n**核心思路**：论文的核心思路是通过在训练阶段引入数据合成技术，使World Model能够更好地适应测试阶段的动作序列估计。具体来说，就是通过生成包含未来多个时间步的状态和动作序列的数据，来训练World Model，从而让模型学习到长期预测的能力，减少train-test gap。\\n\\n**技术框架**：整体框架包括World Model的训练和基于梯度的规划两个阶段。在训练阶段，使用真实数据和合成数据混合训练World Model。合成数据通过在真实状态上施加随机动作序列生成。在规划阶段，使用训练好的World Model，通过梯度下降优化动作序列，以达到期望的目标状态。\\n\\n**关键创新**：关键创新在于训练时的数据合成方法，它显式地考虑了World Model在规划阶段的使用方式，通过生成包含未来多个时间步的状态和动作序列的数据，来训练World Model，从而让模型学习到长期预测的能力。这种方法与传统的只预测下一步状态的训练方式有本质区别。\\n\\n**关键设计**：数据合成的关键在于如何生成合理的动作序列。论文中采用随机策略生成动作序列，并限制序列的长度。损失函数包括两部分：一部分是真实数据的下一状态预测损失，另一部分是合成数据的多步预测损失。网络结构采用常见的循环神经网络（RNN）或Transformer结构，用于建模状态和动作序列的时序关系。",
            "application_zh": "该研究成果可广泛应用于机器人控制、自动驾驶、游戏AI等领域。通过提升基于梯度的规划效率，可以使机器人在复杂环境中更快、更准确地完成任务，例如物体操作、路径规划等。该方法还有助于降低对大量计算资源的需求，使得在资源受限的平台上部署复杂的控制算法成为可能。",
            "highlight_zh": "实验结果表明，通过引入数据合成技术，基于梯度的规划方法在多种物体操作和导航任务中，能够以10%的时间预算达到或超过经典的无梯度交叉熵方法（CEM）的性能。这意味着在保证性能的同时，计算效率得到了显著提升。此外，该方法在不同任务和数据集上都表现出良好的泛化能力。",
            "tags_zh": [
                "World Model",
                "模型预测控制",
                "梯度规划",
                "数据合成",
                "机器人控制"
            ],
            "_index": 51,
            "_used_api": "gemini"
        },
        {
            "title": "TacFinRay: Soft Tactile Fin-Ray Finger with Indirect Tactile Sensing for Robust Grasping",
            "authors": [
                "Saekwang Nam",
                "Bowen Deng",
                "Loong Yi Lee",
                "Jonathan M. Rossiter",
                "Nathan F. Lepora"
            ],
            "arxiv_id": "2512.06524v1",
            "summary": "We present a tactile-sensorized Fin-Ray finger that enables simultaneous detection of contact location and indentation depth through an indirect sensing approach. A hinge mechanism is integrated between the soft Fin-Ray structure and a rigid sensing module, allowing deformation and translation information to be transferred to a bottom crossbeam upon which are an array of marker-tipped pins based on the biomimetic structure of the TacTip vision-based tactile sensor. Deformation patterns captured by an internal camera are processed using a convolutional neural network to infer contact conditions without directly sensing the finger surface. The finger design was optimized by varying pin configurations and hinge orientations, achieving 0.1\\,mm depth and 2mm location-sensing accuracies. The perception demonstrated robust generalization to various indenter shapes and sizes, which was applied to a pick-and-place task under uncertain picking positions, where the tactile feedback significantly improved placement accuracy. Overall, this work provides a lightweight, flexible, and scalable tactile sensing solution suitable for soft robotic structures where the sensing needs situating away from the contact interface.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-06",
            "updated": "2025-12-06",
            "comment": "Accepted in IEEE Robotics Automation Letters. S. Nam, B. Deng co-first authors",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06524v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]grasping",
                        "[T]grasp"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "TacFinRay：用于稳健抓取的间接触觉传感软体Fin-Ray手指",
            "summary_zh": "本文提出了一种触觉传感器化的Fin-Ray手指，它通过间接传感方法能够同时检测接触位置和压入深度。该设计在软体Fin-Ray结构和一个刚性传感模块之间集成了一个铰链机构，使得形变和位移信息能够传递到底部的横梁上，横梁上排列着基于TacTip视觉触觉传感器的生物结构设计的带标记针尖的阵列。内部摄像头捕获的形变模式通过卷积神经网络处理，以推断接触条件，而无需直接感知手指表面。通过改变针的配置和铰链的方向来优化手指设计，实现了0.1毫米的深度和2毫米的位置传感精度。该感知系统对各种压头形状和尺寸表现出强大的泛化能力，并应用于不确定拾取位置下的抓取放置任务，触觉反馈显著提高了放置精度。总而言之，这项工作提供了一种轻量级、灵活且可扩展的触觉传感解决方案，适用于传感需求位于远离接触界面的软体机器人结构。",
            "intro_zh": [
                "传统触觉传感器在软体机器人上的集成面临挑战，尤其是在需要远离接触界面的传感时，直接传感方案难以实现。",
                "TacFinRay手指通过铰链机构将Fin-Ray结构的形变传递到内部视觉传感器，利用卷积神经网络间接推断接触位置和深度。",
                "实验表明，该设计在深度和位置传感方面具有较高的精度，并且在不确定拾取位置的抓取放置任务中表现出良好的性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决软体机器人抓取过程中，如何实现轻量化、灵活且可扩展的触觉感知，尤其是在传感需求位于远离接触界面的情况下。现有方法通常依赖直接触觉传感，难以集成到软体结构中，且鲁棒性较差。\\n\\n**核心思路**：论文的核心思路是采用间接触觉传感。通过将软体Fin-Ray手指的形变信息，通过铰链机构传递到位于底部的视觉传感器阵列，利用视觉信息来推断接触位置和深度。这种方法避免了直接在手指表面集成传感器，提高了系统的灵活性和鲁棒性。\\n\\n**技术框架**：TacFinRay手指的整体架构包括三个主要模块：1) 软体Fin-Ray结构：负责与物体接触并产生形变；2) 铰链机构：将Fin-Ray结构的形变传递到底部的横梁；3) 视觉传感模块：包含一个内部摄像头和带标记针尖的阵列，摄像头捕获针尖的位移模式，并通过卷积神经网络进行处理，最终推断出接触位置和深度。\\n\\n**关键创新**：该论文的关键创新在于将软体Fin-Ray结构与间接视觉触觉传感相结合。通过巧妙的铰链设计，实现了形变信息的有效传递，并利用卷积神经网络实现了高精度的触觉感知。与传统的直接触觉传感方法相比，该方法具有更高的灵活性、鲁棒性和可扩展性。\\n\\n**关键设计**：在设计上，论文优化了针的配置和铰链的方向，以提高传感精度。具体来说，通过调整针的密度和排列方式，以及铰链的刚度和位置，来优化形变信息的传递效率。此外，论文还使用了卷积神经网络来处理视觉信息，网络的结构和参数经过精心设计，以实现最佳的性能。损失函数的设计也至关重要，需要能够准确地反映接触位置和深度的误差。",
            "application_zh": "TacFinRay手指具有广泛的应用前景，例如在柔性机器人、医疗机器人、人机协作等领域。它可以用于实现更精确、更安全的抓取操作，提高机器人的适应性和智能化水平。此外，该技术还可以应用于虚拟现实、游戏等领域，提供更逼真的触觉反馈。",
            "highlight_zh": "实验结果表明，TacFinRay手指在深度和位置传感方面具有较高的精度，分别达到了0.1毫米和2毫米。此外，该设计对各种压头形状和尺寸表现出强大的泛化能力。在不确定拾取位置下的抓取放置任务中，触觉反馈显著提高了放置精度，表明该设计具有良好的实用价值。",
            "tags_zh": [
                "软体机器人",
                "触觉传感",
                "Fin-Ray结构",
                "间接传感",
                "卷积神经网络"
            ],
            "_index": 52,
            "_used_api": "gemini"
        },
        {
            "title": "A Hyperspectral Imaging Guided Robotic Grasping System",
            "authors": [
                "Zheng Sun",
                "Zhipeng Dong",
                "Shixiong Wang",
                "Zhongyi Chu",
                "Fei Chen"
            ],
            "arxiv_id": "2512.05578v1",
            "summary": "Hyperspectral imaging is an advanced technique for precisely identifying and analyzing materials or objects. However, its integration with robotic grasping systems has so far been explored due to the deployment complexities and prohibitive costs. Within this paper, we introduce a novel hyperspectral imaging-guided robotic grasping system. The system consists of PRISM (Polyhedral Reflective Imaging Scanning Mechanism) and the SpectralGrasp framework. PRISM is designed to enable high-precision, distortion-free hyperspectral imaging while simplifying system integration and costs. SpectralGrasp generates robotic grasping strategies by effectively leveraging both the spatial and spectral information from hyperspectral images. The proposed system demonstrates substantial improvements in both textile recognition compared to human performance and sorting success rate compared to RGB-based methods. Additionally, a series of comparative experiments further validates the effectiveness of our system. The study highlights the potential benefits of integrating hyperspectral imaging with robotic grasping systems, showcasing enhanced recognition and grasping capabilities in complex and dynamic environments. The project is available at: https://zainzh.github.io/PRISM.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "8 pages, 7 figures, Accepted to IEEE Robotics and Automation Letters (RA-L) 2025",
            "doi": "10.1109/LRA.2025.3575654",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05578v1",
            "code_links": [
                {
                    "url": "https://zainzh.github.io/PRISM",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]grasping",
                        "[T]grasp"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出基于高光谱成像的机器人抓取系统，提升复杂环境下物体识别与抓取能力",
            "summary_zh": "本文介绍了一种新型的基于高光谱成像引导的机器人抓取系统。该系统由PRISM（多面反射成像扫描机制）和SpectralGrasp框架组成。PRISM旨在实现高精度、无畸变的高光谱成像，同时简化系统集成并降低成本。SpectralGrasp通过有效利用高光谱图像中的空间和光谱信息来生成机器人抓取策略。所提出的系统在纺织品识别方面优于人类表现，并且分拣成功率高于基于RGB的方法。一系列对比实验进一步验证了我们系统的有效性。该研究突出了将高光谱成像与机器人抓取系统相结合的潜在优势，展示了在复杂和动态环境中增强的识别和抓取能力。",
            "intro_zh": [
                "现有机器人抓取系统在高光谱成像集成方面面临部署复杂和成本高昂的挑战。",
                "论文提出PRISM和SpectralGrasp框架，简化高光谱成像系统集成，并有效利用空间和光谱信息生成抓取策略。",
                "实验结果表明，该系统在纺织品识别方面超越人类，分拣成功率也显著高于RGB方法。"
            ],
            "method_zh": "**问题定义**：现有机器人抓取系统在复杂环境下的物体识别和抓取能力有限，尤其是在需要精细区分材料或物体属性时。高光谱成像虽然能提供丰富的材料信息，但其集成到机器人系统中面临成本高、部署复杂以及数据处理困难等问题。传统方法难以有效利用高光谱数据进行精确抓取。\n\n**核心思路**：本文的核心思路是设计一个低成本、易于集成的高光谱成像系统PRISM，并开发一个能够有效利用高光谱图像空间和光谱信息的抓取框架SpectralGrasp。通过PRISM获取高质量的高光谱图像，然后利用SpectralGrasp提取特征并生成抓取策略，从而提升机器人抓取系统的性能。\n\n**技术框架**：该系统主要包含两个核心模块：PRISM（Polyhedral Reflective Imaging Scanning Mechanism）和SpectralGrasp框架。PRISM负责获取高精度、低畸变的高光谱图像。SpectralGrasp框架则接收PRISM输出的高光谱图像，进行特征提取、目标识别和抓取策略生成。整个流程包括高光谱图像采集、图像预处理、特征提取、目标识别、抓取位姿估计和机器人执行等步骤。\n\n**关键创新**：该论文的关键创新在于PRISM的设计，它通过多面反射镜结构简化了高光谱成像系统的集成，降低了成本，并保证了成像质量。此外，SpectralGrasp框架能够有效地融合高光谱图像的空间和光谱信息，从而生成更精确的抓取策略。与传统RGB方法相比，该方法能够利用更丰富的材料信息进行识别和抓取。\n\n**关键设计**：PRISM的具体设计包括多面反射镜的几何参数、扫描方式以及图像校正算法等。SpectralGrasp框架可能包含特定的卷积神经网络结构，用于提取高光谱图像的特征。损失函数的设计可能包括分类损失和抓取位姿回归损失，以优化目标识别和抓取位姿估计的准确性。具体的网络结构和参数设置在论文中可能有所描述，但具体细节未知。",
            "application_zh": "该研究成果可应用于纺织品分拣、食品质量检测、农业采摘、医疗诊断等领域。通过高光谱成像技术，机器人能够更准确地识别和处理不同材料或物体，提高自动化生产效率和产品质量。未来，该技术有望在智能制造、智慧农业和医疗健康等领域发挥重要作用。",
            "highlight_zh": "实验结果表明，该系统在纺织品识别方面优于人类表现，具体提升幅度未知。与基于RGB的方法相比，该系统在分拣成功率方面有显著提升，具体提升百分比未知。这些结果验证了高光谱成像在机器人抓取系统中的有效性，并展示了PRISM和SpectralGrasp框架的优越性。",
            "tags_zh": [
                "高光谱成像",
                "机器人抓取",
                "物体识别",
                "自动化分拣",
                "多光谱分析"
            ],
            "_index": 53,
            "_used_api": "gemini"
        },
        {
            "title": "ProbeMDE: Uncertainty-Guided Active Proprioception for Monocular Depth Estimation in Surgical Robotics",
            "authors": [
                "Britton Jordan",
                "Jordan Thompson",
                "Jesse F. d'Almeida",
                "Hao Li",
                "Nithesh Kumar",
                "Susheela Sharma Stern",
                "Ipek Oguz",
                "Robert J. Webster",
                "Daniel Brown",
                "Alan Kuntz",
                "James Ferguson"
            ],
            "arxiv_id": "2512.11773v1",
            "summary": "Monocular depth estimation (MDE) provides a useful tool for robotic perception, but its predictions are often uncertain and inaccurate in challenging environments such as surgical scenes where textureless surfaces, specular reflections, and occlusions are common. To address this, we propose ProbeMDE, a cost-aware active sensing framework that combines RGB images with sparse proprioceptive measurements for MDE. Our approach utilizes an ensemble of MDE models to predict dense depth maps conditioned on both RGB images and on a sparse set of known depth measurements obtained via proprioception, where the robot has touched the environment in a known configuration. We quantify predictive uncertainty via the ensemble's variance and measure the gradient of the uncertainty with respect to candidate measurement locations. To prevent mode collapse while selecting maximally informative locations to propriocept (touch), we leverage Stein Variational Gradient Descent (SVGD) over this gradient map. We validate our method in both simulated and physical experiments on central airway obstruction surgical phantoms. Our results demonstrate that our approach outperforms baseline methods across standard depth estimation metrics, achieving higher accuracy while minimizing the number of required proprioceptive measurements.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "9 pages, 5 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11773v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]depth estimation",
                        "[T]monocular depth"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "ProbeMDE：不确定性引导的主动触觉单目深度估计，用于手术机器人",
            "summary_zh": "单目深度估计(MDE)为机器人感知提供了一种有用的工具，但在具有挑战性的环境中，例如手术场景中常见的无纹理表面、镜面反射和遮挡，其预测通常是不确定和不准确的。为了解决这个问题，我们提出了ProbeMDE，一个成本感知的有源传感框架，它结合了RGB图像和稀疏的本体感受测量来进行MDE。我们的方法利用MDE模型的集合来预测密集的深度图，这些深度图以RGB图像和通过本体感受获得的一组稀疏的已知深度测量为条件，其中机器人以已知的配置触摸环境。我们通过集合的方差来量化预测不确定性，并测量不确定性相对于候选测量位置的梯度。为了防止在选择信息量最大的本体感受位置时出现模式崩溃，我们利用Stein变分梯度下降(SVGD)来处理这个梯度图。我们在中心气道阻塞手术模型上进行了模拟和物理实验，验证了我们的方法。结果表明，我们的方法在标准深度估计指标上优于基线方法，在最小化所需本体感受测量数量的同时实现了更高的精度。",
            "intro_zh": [
                "手术场景中单目深度估计面临纹理缺失、反射和遮挡等挑战，导致预测不确定且不准确。",
                "ProbeMDE结合RGB图像和稀疏本体感受测量，利用模型集合预测深度图，并用不确定性梯度引导主动触觉。",
                "实验表明，ProbeMDE在模拟和物理环境中均优于基线方法，提高了深度估计精度并减少了触觉测量次数。"
            ],
            "method_zh": "**问题定义**：论文旨在解决手术机器人场景中，由于纹理缺失、镜面反射和遮挡等因素导致的单目深度估计(MDE)不准确和不确定性高的问题。现有的MDE方法在这些具有挑战性的环境中表现不佳，限制了手术机器人的感知能力。\\n\\n**核心思路**：论文的核心思路是结合RGB图像和稀疏的本体感受测量，利用主动感知策略来提高MDE的准确性和鲁棒性。通过在机器人已知配置下触摸环境，获得稀疏的深度信息，并将其作为MDE模型的输入，从而约束深度估计结果。同时，利用模型集合预测的不确定性来引导主动触觉，选择信息量最大的测量位置。\\n\\n**技术框架**：ProbeMDE框架主要包含以下几个模块：1) 基于RGB图像和稀疏深度测量的MDE模型集合，用于预测深度图和不确定性；2) 不确定性梯度计算模块，用于计算不确定性相对于候选测量位置的梯度；3) 基于Stein变分梯度下降(SVGD)的主动触觉选择模块，用于选择信息量最大的测量位置；4) 机器人本体感受测量模块，用于获取稀疏的深度信息。整体流程是：首先利用MDE模型集合预测深度图和不确定性，然后计算不确定性梯度，利用SVGD选择下一个测量位置，通过机器人本体感受测量获取深度信息，并将新的深度信息加入到MDE模型中，重复以上步骤直到满足精度要求。\\n\\n**关键创新**：论文的关键创新在于将主动感知策略与基于模型集合的MDE相结合，利用不确定性引导触觉测量，从而在最小化测量次数的同时，最大化深度估计的精度。此外，利用Stein变分梯度下降(SVGD)来防止主动触觉选择过程中的模式崩溃，保证了测量位置的多样性。\\n\\n**关键设计**：MDE模型集合由多个独立的MDE模型组成，每个模型都基于相同的网络结构，但使用不同的初始化参数进行训练。不确定性通过模型集合预测结果的方差来量化。SVGD算法用于在不确定性梯度图上进行采样，选择信息量最大的测量位置。论文没有明确说明具体的网络结构、损失函数等技术细节，这些信息可能在引用的相关论文中。",
            "application_zh": "ProbeMDE在手术机器人领域具有广泛的应用前景，可以提高手术机器人的感知能力，辅助医生进行更精确、安全的手术操作。例如，在微创手术中，医生可以通过ProbeMDE获取更准确的深度信息，从而更好地定位病灶、避开重要器官。此外，该方法还可以应用于其他需要精确深度估计的机器人应用场景，如自动驾驶、三维重建等。",
            "highlight_zh": "实验结果表明，ProbeMDE在模拟和物理实验中均优于基线方法。在中心气道阻塞手术模型上，ProbeMDE在减少本体感受测量次数的同时，显著提高了深度估计的精度。具体的性能数据和提升幅度在论文中进行了详细的量化分析，证明了ProbeMDE的有效性。",
            "tags_zh": [
                "单目深度估计",
                "主动感知",
                "手术机器人",
                "本体感受",
                "不确定性量化"
            ],
            "_index": 54,
            "_used_api": "gemini"
        },
        {
            "title": "Bench-Push: Benchmarking Pushing-based Navigation and Manipulation Tasks for Mobile Robots",
            "authors": [
                "Ninghan Zhong",
                "Steven Caro",
                "Megnath Ramesh",
                "Rishi Bhatnagar",
                "Avraiem Iskandar",
                "Stephen L. Smith"
            ],
            "arxiv_id": "2512.11736v1",
            "summary": "Mobile robots are increasingly deployed in cluttered environments with movable objects, posing challenges for traditional methods that prohibit interaction. In such settings, the mobile robot must go beyond traditional obstacle avoidance, leveraging pushing or nudging strategies to accomplish its goals. While research in pushing-based robotics is growing, evaluations rely on ad hoc setups, limiting reproducibility and cross-comparison. To address this, we present Bench-Push, the first unified benchmark for pushing-based mobile robot navigation and manipulation tasks. Bench-Push includes multiple components: 1) a comprehensive range of simulated environments that capture the fundamental challenges in pushing-based tasks, including navigating a maze with movable obstacles, autonomous ship navigation in ice-covered waters, box delivery, and area clearing, each with varying levels of complexity; 2) novel evaluation metrics to capture efficiency, interaction effort, and partial task completion; and 3) demonstrations using Bench-Push to evaluate example implementations of established baselines across environments. Bench-Push is open-sourced as a Python library with a modular design. The code, documentation, and trained models can be found at https://github.com/IvanIZ/BenchNPIN.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "Under review for ICRA 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11736v1",
            "code_links": [
                {
                    "url": "https://github.com/IvanIZ/BenchNPIN",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]navigation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "1_robot_core",
                "3_perception_slam"
            ],
            "headline_zh": "Bench-Push：移动机器人推碰式导航与操作任务的统一基准测试平台",
            "summary_zh": "移动机器人越来越多地部署在具有可移动物体的杂乱环境中，这对传统的禁止交互方法提出了挑战。在这种情况下，移动机器人必须超越传统的避障，利用推碰策略来完成其目标。虽然基于推碰的机器人技术研究正在增长，但评估依赖于临时设置，限制了可重复性和交叉比较。为了解决这个问题，我们提出了Bench-Push，这是第一个用于基于推碰的移动机器人导航和操作任务的统一基准。Bench-Push包括多个组件：1）全面的模拟环境，捕捉基于推碰的任务中的基本挑战，包括导航具有可移动障碍物的迷宫、冰覆盖水域中的自主船舶导航、箱子递送和区域清理，每个都具有不同的复杂程度；2）新颖的评估指标，以捕捉效率、交互努力和部分任务完成；3）使用Bench-Push评估跨环境的已建立基线的示例实现的演示。Bench-Push作为一个具有模块化设计的Python库开源。代码、文档和训练模型可在https://github.com/IvanIZ/BenchNPIN找到。",
            "intro_zh": [
                "现有移动机器人导航方法难以应对存在可移动物体的复杂环境，通常禁止与环境交互。",
                "Bench-Push提出一个统一的基准测试平台，包含多种模拟环境和评估指标，用于评估基于推碰的导航和操作算法。",
                "论文通过在Bench-Push上评估现有基线算法，展示了该基准测试平台在推动该领域研究方面的有效性。"
            ],
            "method_zh": "**问题定义**：现有移动机器人导航方法在处理包含可移动物体的复杂环境时面临挑战。传统的避障方法通常禁止机器人与环境进行交互，这限制了机器人在需要推碰或轻推物体才能完成任务的场景中的应用。缺乏统一的基准测试平台使得不同推碰式导航和操作算法的性能难以比较和复现，阻碍了该领域的发展。\\n\\n**核心思路**：Bench-Push的核心思路是提供一个标准化的、可复现的基准测试平台，用于评估和比较不同的推碰式导航和操作算法。通过提供多种具有不同复杂度的模拟环境和新颖的评估指标，Bench-Push旨在促进该领域的研究，并推动相关算法的进步。该平台采用模块化设计，易于扩展和定制，方便研究人员添加新的环境、算法和评估指标。\\n\\n**技术框架**：Bench-Push的技术框架主要包括以下几个模块：1）环境模块：提供多种模拟环境，包括迷宫导航、冰面船舶导航、箱子递送和区域清理等，每个环境都具有不同的复杂程度和挑战。2）算法模块：提供一些已建立的基线算法的示例实现，方便研究人员进行比较和参考。3）评估模块：提供新颖的评估指标，用于衡量算法的效率、交互努力和部分任务完成情况。4）接口模块：提供Python API，方便用户自定义环境、算法和评估指标。整体流程是，用户选择一个环境，运行一个算法，然后使用评估模块计算性能指标。\\n\\n**关键创新**：Bench-Push的关键创新在于它是第一个专门为推碰式移动机器人导航和操作任务设计的统一基准测试平台。与以往的临时设置相比，Bench-Push提供了一个标准化的、可复现的评估环境，使得不同算法的性能可以进行公平的比较。此外，Bench-Push还提出了新颖的评估指标，能够更全面地衡量算法的性能，包括效率、交互努力和部分任务完成情况。\\n\\n**关键设计**：Bench-Push的关键设计包括：1）环境的多样性：提供多种具有不同复杂度的模拟环境，以覆盖不同的应用场景和挑战。2）评估指标的全面性：采用多种评估指标，包括路径长度、时间、推碰次数、能量消耗等，以全面衡量算法的性能。3）模块化的设计：采用模块化的设计，方便用户自定义环境、算法和评估指标。4）开源的实现：以Python库的形式开源，方便研究人员使用和扩展。",
            "application_zh": "Bench-Push的研究成果可应用于各种需要移动机器人与环境交互的场景，例如仓库自动化、物流配送、家庭服务机器人、灾难救援等。通过使用Bench-Push进行算法评估和优化，可以提高机器人在复杂环境中的导航和操作能力，从而提高工作效率和安全性。该研究还有助于推动机器人技术的发展，促进更多智能机器人的应用。",
            "highlight_zh": "论文通过在Bench-Push上评估现有基线算法，展示了该基准测试平台的有效性。实验结果表明，Bench-Push能够有效地衡量不同算法的性能，并揭示它们在不同环境下的优缺点。例如，在迷宫导航任务中，某些算法在路径长度方面表现良好，但在推碰次数方面表现较差，而另一些算法则相反。这些结果为研究人员提供了有价值的参考，有助于他们选择合适的算法并进行改进。",
            "tags_zh": [
                "移动机器人",
                "推碰式导航",
                "基准测试",
                "机器人操作",
                "强化学习"
            ],
            "_index": 55,
            "_used_api": "gemini"
        },
        {
            "title": "Prior-Enhanced Gaussian Splatting for Dynamic Scene Reconstruction from Casual Video",
            "authors": [
                "Meng-Li Shih",
                "Ying-Huan Chen",
                "Yu-Lun Liu",
                "Brian Curless"
            ],
            "arxiv_id": "2512.11356v1",
            "summary": "We introduce a fully automatic pipeline for dynamic scene reconstruction from casually captured monocular RGB videos. Rather than designing a new scene representation, we enhance the priors that drive Dynamic Gaussian Splatting. Video segmentation combined with epipolar-error maps yields object-level masks that closely follow thin structures; these masks (i) guide an object-depth loss that sharpens the consistent video depth, and (ii) support skeleton-based sampling plus mask-guided re-identification to produce reliable, comprehensive 2-D tracks. Two additional objectives embed the refined priors in the reconstruction stage: a virtual-view depth loss removes floaters, and a scaffold-projection loss ties motion nodes to the tracks, preserving fine geometry and coherent motion. The resulting system surpasses previous monocular dynamic scene reconstruction methods and delivers visibly superior renderings",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11356v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]gaussian splatting",
                        "[T]scene reconstruction"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出先验增强的高斯溅射方法，用于从日常视频中重建动态场景",
            "summary_zh": "本文提出了一种全自动流程，用于从随意拍摄的单目RGB视频中重建动态场景。该方法没有设计新的场景表示，而是增强了驱动动态高斯溅射的先验信息。视频分割结合极线误差图产生对象级别的掩码，这些掩码紧密跟随细薄结构；这些掩码（i）引导对象深度损失，从而锐化一致的视频深度，并且（ii）支持基于骨架的采样以及掩码引导的重识别，以产生可靠、全面的2D轨迹。两个额外的目标将细化的先验嵌入到重建阶段：虚拟视图深度损失消除了漂浮物，支架投影损失将运动节点与轨迹联系起来，从而保留了精细的几何形状和连贯的运动。所提出的系统超越了以往的单目动态场景重建方法，并提供了明显更优越的渲染效果。",
            "intro_zh": [
                "现有单目动态场景重建方法在处理复杂运动和遮挡时，几何细节和运动连贯性不足。",
                "通过结合视频分割、极线几何和骨骼信息，增强动态高斯溅射的先验知识，从而提升重建质量。",
                "实验表明，该方法在单目动态场景重建任务上，渲染质量和几何精度均优于现有技术。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从单目RGB视频中重建动态场景的问题。现有方法在处理复杂运动、遮挡以及缺乏深度信息的情况下，难以获得高质量的几何结构和连贯的运动估计，导致重建结果存在漂浮物、几何失真等问题。\\n\\n**核心思路**：论文的核心在于通过增强动态高斯溅射（Dynamic Gaussian Splatting）的先验信息来改善重建效果。具体来说，利用视频分割、极线几何约束和骨骼信息来指导深度估计、运动轨迹生成和场景重建，从而提高重建的准确性和鲁棒性。\\n\\n**技术框架**：整体流程包括以下几个主要阶段：1) 视频分割和深度估计：利用视频分割和极线误差图生成对象级别的掩码，并结合对象深度损失来优化深度估计。2) 2D轨迹生成：通过骨架引导的采样和掩码引导的重识别，生成可靠的2D运动轨迹。3) 动态高斯溅射重建：将细化的先验信息嵌入到重建阶段，包括虚拟视图深度损失和支架投影损失，以消除漂浮物并保持几何形状和运动的连贯性。\\n\\n**关键创新**：论文的关键创新在于将视频分割、极线几何和骨骼信息有效地结合起来，用于增强动态高斯溅射的先验知识。通过对象级别的掩码引导深度估计和运动轨迹生成，以及虚拟视图深度损失和支架投影损失的引入，显著提高了重建的质量和鲁棒性。\\n\\n**关键设计**：论文的关键设计包括：1) 对象深度损失：利用对象级别的掩码来锐化一致的视频深度。2) 骨架引导的采样和掩码引导的重识别：用于生成可靠的2D运动轨迹。3) 虚拟视图深度损失：用于消除重建结果中的漂浮物。4) 支架投影损失：用于将运动节点与轨迹联系起来，从而保持几何形状和运动的连贯性。具体的参数设置和损失函数细节在论文中有详细描述，此处不再赘述。",
            "application_zh": "该研究成果可应用于虚拟现实、增强现实、游戏开发、电影制作等领域，能够从普通视频中重建高质量的动态3D场景，为用户提供更加沉浸式的体验。此外，该技术还可用于运动分析、人体姿态估计等领域，具有广泛的应用前景。",
            "highlight_zh": "实验结果表明，该方法在单目动态场景重建任务上，相比于现有的方法，能够生成更清晰、更准确的几何结构和更连贯的运动估计。通过定性和定量的比较，证明了该方法在渲染质量和几何精度方面的优越性。具体性能数据和对比基线在论文中有详细展示。",
            "tags_zh": [
                "动态场景重建",
                "高斯溅射",
                "单目视频",
                "先验增强",
                "视频分割",
                "深度估计",
                "运动轨迹"
            ],
            "_index": 56,
            "_used_api": "gemini"
        },
        {
            "title": "Lightweight 3D Gaussian Splatting Compression via Video Codec",
            "authors": [
                "Qi Yang",
                "Geert Van Der Auwera",
                "Zhu Li"
            ],
            "arxiv_id": "2512.11186v1",
            "summary": "Current video-based GS compression methods rely on using Parallel Linear Assignment Sorting (PLAS) to convert 3D GS into smooth 2D maps, which are computationally expensive and time-consuming, limiting the application of GS on lightweight devices. In this paper, we propose a Lightweight 3D Gaussian Splatting (GS) Compression method based on Video codec (LGSCV). First, a two-stage Morton scan is proposed to generate blockwise 2D maps that are friendly for canonical video codecs in which the coding units (CU) are square blocks. A 3D Morton scan is used to permute GS primitives, followed by a 2D Morton scan to map the ordered GS primitives to 2D maps in a blockwise style. However, although the blockwise 2D maps report close performance to the PLAS map in high-bitrate regions, they show a quality collapse at medium-to-low bitrates. Therefore, a principal component analysis (PCA) is used to reduce the dimensionality of spherical harmonics (SH), and a MiniPLAS, which is flexible and fast, is designed to permute the primitives within certain block sizes. Incorporating SH PCA and MiniPLAS leads to a significant gain in rate-distortion (RD) performance, especially at medium and low bitrates. MiniPLAS can also guide the setting of the codec CU size configuration and significantly reduce encoding time. Experimental results on the MPEG dataset demonstrate that the proposed LGSCV achieves over 20% RD gain compared with state-of-the-art methods, while reducing 2D map generation time to approximately 1 second and cutting encoding time by 50%. The code is available at https://github.com/Qi-Yangsjtu/LGSCV .",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "Accepted by DCC2026 Oral",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11186v1",
            "code_links": [
                {
                    "url": "https://github.com/Qi-Yangsjtu/LGSCV",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]3D gaussian splatting",
                        "[T]gaussian splatting"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于视频编解码器的轻量级3D高斯溅射压缩方法，适用于轻量级设备。",
            "summary_zh": "本文提出了一种基于视频编解码器的轻量级3D高斯溅射（GS）压缩方法（LGSCV）。该方法首先提出了一种两阶段Morton扫描，以生成适用于标准视频编解码器的块状2D图，其中编码单元（CU）是方形块。使用3D Morton扫描来置换GS图元，然后使用2D Morton扫描以块状方式将排序后的GS图元映射到2D图。针对中低码率下质量下降的问题，采用主成分分析（PCA）来降低球谐函数（SH）的维度，并设计了一种灵活快速的MiniPLAS来置换特定块大小内的图元。SH PCA和MiniPLAS的结合显著提高了率失真（RD）性能，尤其是在中低码率下。MiniPLAS还可以指导编解码器CU大小配置，并显著减少编码时间。在MPEG数据集上的实验结果表明，所提出的LGSCV与最先进的方法相比，实现了超过20%的RD增益，同时将2D图生成时间减少到大约1秒，并将编码时间减少了50%。",
            "intro_zh": [
                "现有基于视频的GS压缩方法依赖于并行线性分配排序（PLAS），计算量大且耗时，限制了GS在轻量级设备上的应用。",
                "提出一种轻量级3D高斯溅射压缩方法，通过两阶段Morton扫描生成块状2D图，并结合PCA降维和MiniPLAS优化中低码率性能。",
                "实验结果表明，该方法在率失真性能上优于现有技术20%以上，同时显著降低了2D图生成和编码时间。"
            ],
            "method_zh": "**问题定义**：现有基于视频的3D高斯溅射（GS）压缩方法，如基于并行线性分配排序（PLAS）的方法，计算复杂度高，耗时较长，难以在轻量级设备上部署。因此，需要一种计算效率更高、更轻量级的GS压缩方法。\n\\n**核心思路**：论文的核心思路是利用标准视频编解码器对3D GS数据进行压缩。为了更好地适应视频编解码器的块状编码结构，论文设计了一种两阶段Morton扫描方法，将3D GS图元映射到块状2D图。同时，为了解决中低码率下的性能下降问题，引入了PCA降维和MiniPLAS优化。\n\\n**技术框架**：该方法主要包含以下几个阶段：1) 3D Morton扫描：对GS图元进行排序。2) 2D Morton扫描：将排序后的GS图元映射到块状2D图。3) SH PCA：对球谐函数进行主成分分析，降低维度。4) MiniPLAS：在块内进行图元置换优化。5) 视频编码：使用标准视频编解码器对2D图进行编码。\n\\n**关键创新**：该方法的主要创新点在于：1) 提出了两阶段Morton扫描方法，生成适用于视频编解码器的块状2D图。2) 结合PCA降维和MiniPLAS优化，显著提高了中低码率下的率失真性能。3) MiniPLAS可以指导编解码器CU大小配置，从而减少编码时间。与现有方法相比，该方法计算复杂度更低，更适合在轻量级设备上部署。\n\\n**关键设计**：两阶段Morton扫描的具体实现细节，包括3D和2D Morton码的生成方式。PCA降维中保留的主成分数量。MiniPLAS的块大小设置和置换策略。编解码器CU大小的配置策略，以及如何利用MiniPLAS指导CU大小的设置。",
            "application_zh": "该研究成果可应用于各种需要高效3D高斯溅射压缩的场景，例如移动端的3D场景渲染、VR/AR应用、以及低带宽网络环境下的3D内容传输。通过降低计算复杂度和提高压缩效率，该方法有望推动3D高斯溅射技术在轻量级设备和资源受限环境中的普及。",
            "highlight_zh": "实验结果表明，所提出的LGSCV方法与最先进的方法相比，实现了超过20%的率失真（RD）增益。同时，该方法将2D图生成时间减少到大约1秒，并将编码时间减少了50%。这些结果表明，该方法在性能和效率方面都具有显著优势。",
            "tags_zh": [
                "3D高斯溅射",
                "视频压缩",
                "Morton扫描",
                "主成分分析",
                "率失真优化",
                "轻量级设备",
                "视频编码"
            ],
            "_index": 57,
            "_used_api": "gemini"
        },
        {
            "title": "Geo6DPose: Fast Zero-Shot 6D Object Pose Estimation via Geometry-Filtered Feature Matching",
            "authors": [
                "Javier Villena Toro",
                "Mehdi Tarkian"
            ],
            "arxiv_id": "2512.10674v1",
            "summary": "Recent progress in zero-shot 6D object pose estimation has been driven largely by large-scale models and cloud-based inference. However, these approaches often introduce high latency, elevated energy consumption, and deployment risks related to connectivity, cost, and data governance; factors that conflict with the practical constraints of real-world robotics, where compute is limited and on-device inference is frequently required. We introduce Geo6DPose, a lightweight, fully local, and training-free pipeline for zero-shot 6D pose estimation that trades model scale for geometric reliability. Our method combines foundation model visual features with a geometric filtering strategy: Similarity maps are computed between onboarded template DINO descriptors and scene patches, and mutual correspondences are established by projecting scene patch centers to 3D and template descriptors to the object model coordinate system. Final poses are recovered via correspondence-driven RANSAC and ranked using a weighted geometric alignment metric that jointly accounts for reprojection consistency and spatial support, improving robustness to noise, clutter, and partial visibility. Geo6DPose achieves sub-second inference on a single commodity GPU while matching the average recall of significantly larger zero-shot baselines (53.7 AR, 1.08 FPS). It requires no training, fine-tuning, or network access, and remains compatible with evolving foundation backbones, advancing practical, fully local 6D perception for robotic deployment.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10674v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]pose estimation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
                    "id": "6_video_extraction",
                    "matched_keywords": [
                        "[T]feature matching"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "3_perception_slam",
                "6_video_extraction"
            ],
            "headline_zh": "Geo6DPose：基于几何滤波特征匹配的快速零样本6D物体姿态估计",
            "summary_zh": "本文提出Geo6DPose，一个轻量级、全本地、免训练的零样本6D姿态估计流程，通过几何可靠性替代模型规模。该方法结合了基础模型视觉特征和几何滤波策略：计算板载模板DINO描述符与场景块之间的相似度图，并通过将场景块中心投影到3D和模板描述符投影到物体模型坐标系来建立互对应关系。最终姿态通过对应关系驱动的RANSAC恢复，并使用加权几何对齐度量进行排序，该度量共同考虑了重投影一致性和空间支持，从而提高对噪声、杂乱和部分可见性的鲁棒性。Geo6DPose在单个商用GPU上实现了亚秒级推理，同时匹配了显著更大的零样本基线的平均召回率（53.7 AR，1.08 FPS）。它不需要训练、微调或网络访问，并且与不断发展的基础骨干网络兼容，从而推进了用于机器人部署的实用、完全本地的6D感知。",
            "intro_zh": [
                "现有零样本6D姿态估计方法依赖大规模模型和云端推理，导致高延迟、高能耗，不适用于算力受限的机器人应用。",
                "Geo6DPose利用几何滤波策略，结合基础模型视觉特征，构建轻量级、全本地、免训练的6D姿态估计流程。",
                "实验表明，Geo6DPose在单个GPU上实现亚秒级推理，同时达到与大型零样本基线相当的平均召回率。"
            ],
            "method_zh": "**问题定义**：零样本6D物体姿态估计旨在无需针对特定物体进行训练的情况下，估计场景中物体的6D姿态（位置和方向）。现有方法通常依赖于大型预训练模型和云端计算，这导致了高延迟、高能耗以及对网络连接的依赖，不适用于资源受限的机器人应用场景。因此，如何在本地设备上实现快速、高效的零样本6D姿态估计是一个关键问题。\\n\\n**核心思路**：Geo6DPose的核心思路是利用几何信息来弥补模型规模的不足。通过结合基础模型的视觉特征和几何滤波策略，该方法能够有效地建立场景和物体模型之间的对应关系，并从中恢复准确的6D姿态。这种方法避免了对大型模型的依赖，从而实现了轻量级和快速的推理。\\n\\n**技术框架**：Geo6DPose的整体流程包括以下几个主要阶段：\n1. **特征提取**：使用预训练的DINO模型提取场景图像和物体模板的视觉特征。\n2. **相似度计算**：计算场景块和模板描述符之间的相似度图。\n3. **对应关系建立**：将场景块中心投影到3D空间，并将模板描述符投影到物体模型坐标系，从而建立场景和物体模型之间的互对应关系。\n4. **姿态恢复**：使用RANSAC算法，基于建立的对应关系恢复物体的6D姿态。\n5. **姿态排序**：使用加权几何对齐度量对恢复的姿态进行排序，该度量同时考虑了重投影一致性和空间支持。\\n\\n**关键创新**：Geo6DPose的关键创新在于其几何滤波策略。通过将视觉特征与几何信息相结合，该方法能够有效地过滤掉错误的对应关系，从而提高姿态估计的准确性和鲁棒性。与现有方法相比，Geo6DPose不需要训练或微调，并且可以在本地设备上运行，从而更适用于实际的机器人应用。\\n\\n**关键设计**：\n* **DINO特征提取器**：使用预训练的DINO模型提取视觉特征，该模型具有良好的泛化能力。\n* **几何一致性检验**：利用场景深度信息将2D特征点反投影到3D空间，并与物体模型的3D点进行匹配，过滤掉不一致的对应关系。\n* **加权几何对齐度量**：设计了一种加权几何对齐度量，用于评估恢复姿态的质量，该度量同时考虑了重投影误差和空间支持。",
            "application_zh": "Geo6DPose适用于资源受限的机器人应用场景，例如仓储物流、家庭服务机器人和工业自动化。该方法无需训练和网络连接，降低了部署成本和风险，并提高了系统的可靠性。未来，Geo6DPose可以进一步扩展到更复杂的场景和物体，并与其他感知模块集成，从而实现更智能的机器人系统。",
            "highlight_zh": "Geo6DPose在单个商用GPU上实现了亚秒级推理（1.08 FPS），同时达到了与显著更大的零样本基线相当的平均召回率（53.7 AR）。该方法不需要训练、微调或网络访问，并且与不断发展的基础骨干网络兼容，展示了其在实际机器人应用中的潜力。",
            "tags_zh": [
                "6D姿态估计",
                "零样本学习",
                "几何滤波",
                "机器人视觉",
                "特征匹配"
            ],
            "_index": 58,
            "_used_api": "gemini"
        },
        {
            "title": "Contact SLAM: An Active Tactile Exploration Policy Based on Physical Reasoning Utilized in Robotic Fine Blind Manipulation Tasks",
            "authors": [
                "Gaozhao Wang",
                "Xing Liu",
                "Zhenduo Ye",
                "Zhengxiong Liu",
                "Panfeng Huang"
            ],
            "arxiv_id": "2512.10481v1",
            "summary": "Contact-rich manipulation is difficult for robots to execute and requires accurate perception of the environment. In some scenarios, vision is occluded. The robot can then no longer obtain real-time scene state information through visual feedback. This is called ``blind manipulation\". In this manuscript, a novel physically-driven contact cognition method, called ``Contact SLAM\", is proposed. It estimates the state of the environment and achieves manipulation using only tactile sensing and prior knowledge of the scene. To maximize exploration efficiency, this manuscript also designs an active exploration policy. The policy gradually reduces uncertainties in the manipulation scene. The experimental results demonstrated the effectiveness and accuracy of the proposed method in several contact-rich tasks, including the difficult and delicate socket assembly task and block-pushing task.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "8 pages, 8 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10481v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]SLAM"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "1_robot_core",
                "3_perception_slam"
            ],
            "headline_zh": "提出Contact SLAM，解决机器人盲操作中基于触觉的主动探索问题",
            "summary_zh": "在机器人难以执行且需要精确环境感知的接触密集型操作中，视觉信息可能被遮挡，导致机器人无法通过视觉反馈获取实时场景状态，即“盲操作”。本文提出了一种新颖的物理驱动的接触认知方法，称为“Contact SLAM”。该方法仅利用触觉传感和场景先验知识来估计环境状态并实现操作。为了最大化探索效率，本文还设计了一种主动探索策略，逐步降低操作场景中的不确定性。实验结果表明，该方法在包括困难且精细的插座组装任务和推块任务在内的多个接触密集型任务中具有有效性和准确性。",
            "intro_zh": [
                "接触密集型操作对机器人来说极具挑战，尤其是在视觉受限的盲操作场景下，如何准确感知环境是关键问题。",
                "Contact SLAM利用触觉信息和场景先验知识，通过物理推理估计环境状态，并设计主动探索策略来降低不确定性。",
                "实验结果表明，Contact SLAM在插座组装和推块等任务中表现出良好的有效性和准确性，验证了其在盲操作中的潜力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决机器人盲操作中，仅依赖触觉信息和先验知识，如何实现对环境的精确感知和有效操作的问题。现有方法在视觉遮挡的情况下，难以准确获取环境状态，导致操作失败或效率低下。\\n\\n**核心思路**：论文的核心思路是利用触觉反馈进行环境建模，并结合物理推理来估计环境状态。通过主动探索策略，机器人能够有目的地选择下一步的触觉交互动作，从而最大程度地降低环境状态的不确定性。这种方法模拟了人类在盲操作时通过触摸来感知和操作物体的过程。\\n\\n**技术框架**：Contact SLAM的整体框架包含以下几个主要模块：1) 触觉数据采集模块：通过触觉传感器获取机器人与环境交互时的触觉信息。2) 环境状态估计模块：利用触觉数据和先验知识，结合物理模型，估计环境的状态，例如物体的位置、形状和方向。3) 主动探索策略模块：根据当前环境状态的不确定性，选择下一步的触觉交互动作，以最大程度地降低不确定性。4) 操作执行模块：根据环境状态的估计结果，执行相应的操作动作。\\n\\n**关键创新**：该方法最重要的创新点在于将触觉SLAM与主动探索策略相结合，实现了在盲操作场景下的高效环境感知和操作。与传统的视觉SLAM相比，Contact SLAM更加依赖触觉信息，能够适应视觉受限的环境。与被动式的触觉探索方法相比，主动探索策略能够更有效地降低环境状态的不确定性。\\n\\n**关键设计**：主动探索策略的设计是关键。具体实现可能涉及：1) 定义不确定性度量：例如，使用信息熵来量化环境状态的不确定性。2) 设计奖励函数：奖励函数鼓励机器人选择能够最大程度降低不确定性的动作。3) 采用强化学习或基于模型的优化方法来学习最优的探索策略。具体的参数设置、损失函数和网络结构等技术细节在论文中可能有所描述，但此处未知。",
            "application_zh": "Contact SLAM在许多领域具有潜在的应用价值，例如：在医疗领域，可以用于辅助医生进行微创手术或远程操作；在工业领域，可以用于自动化装配线上的盲操作任务；在家庭服务领域，可以用于帮助机器人完成在黑暗或拥挤环境中的物品抓取和放置等任务。该研究有望提升机器人在复杂和未知环境中的自主操作能力。",
            "highlight_zh": "实验结果表明，Contact SLAM在插座组装和推块任务中取得了显著的成功。具体性能数据（例如，成功率、完成时间、误差等）和与基线方法的对比结果（例如，传统触觉探索方法、基于视觉的方法等）未知，但摘要中明确指出该方法具有有效性和准确性，表明其性能优于现有方法。",
            "tags_zh": [
                "触觉SLAM",
                "盲操作",
                "主动探索",
                "物理推理",
                "机器人操作"
            ],
            "_index": 59,
            "_used_api": "gemini"
        },
        {
            "title": "Symphony: A Heuristic Normalized Calibrated Advantage Actor and Critic Algorithm in application for Humanoid Robots",
            "authors": [
                "Timur Ishuov",
                "Michele Folgheraiter",
                "Madi Nurmanov",
                "Goncalo Gordo",
                "Richárd Farkas",
                "József Dombi"
            ],
            "arxiv_id": "2512.10477v2",
            "summary": "In our work we not explicitly hint that it is a misconception to think that humans learn fast. Learning process takes time. Babies start learning to move in the restricted liquid area called placenta. Children often are limited by underdeveloped body. Even adults are not allowed to participate in complex competitions right away. However, with robots, when learning from scratch, we often don't have the privilege of waiting for dozen millions of steps. \"Swaddling\" regularization is responsible for restraining an agent in rapid but unstable development penalizing action strength in a specific way not affecting actions directly. The Symphony, Transitional-policy Deterministic Actor and Critic algorithm, is a concise combination of different ideas for possibility of training humanoid robots from scratch with Sample Efficiency, Sample Proximity and Safety of Actions in mind. It is no secret that continuous increase in Gaussian noise without appropriate smoothing is harmful for motors and gearboxes. Compared to Stochastic algorithms, we set a limited parametric noise and promote a reduced strength of actions, safely increasing entropy, since the actions are kind of immersed in weaker noise. When actions require more extreme values, actions rise above the weak noise. Training becomes empirically much safer for both the environment around and the robot's mechanisms. We use Fading Replay Buffer: using a fixed formula containing the hyperbolic tangent, we adjust the batch sampling probability: the memory contains a recent memory and a long-term memory trail. Fading Replay Buffer allows us to use Temporal Advantage when we improve the current Critic Network prediction compared to the exponential moving average. Temporal Advantage allows us to update Actor and Critic in one pass, as well as combine Actor and Critic in one Object and implement their Losses in one line.",
            "categories": [
                "cs.RO",
                "cs.NE"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-11",
            "updated": "2025-12-14",
            "comment": "https://github.com/SuspensionRailway/symphony",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10477v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]humanoid",
                        "[T]humanoid robot"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出Symphony算法，解决人形机器人从零开始训练的样本效率、样本邻近性和动作安全性问题。",
            "summary_zh": "本文提出了一种名为Symphony的过渡策略确定性Actor-Critic算法，简称Symphony，旨在解决人形机器人从零开始训练时面临的样本效率、样本邻近性和动作安全性问题。该算法结合了多种思想，包括“襁褓”正则化，通过惩罚动作强度来约束agent的快速但不稳定的发展，但不直接影响动作。Symphony算法限制了参数噪声，并促进动作强度的降低，从而安全地增加熵。此外，本文还使用了Fading Replay Buffer，通过双曲正切函数调整batch采样概率，包含近期记忆和长期记忆轨迹。Temporal Advantage用于改进Critic网络的预测，并允许在一次传递中更新Actor和Critic，以及将Actor和Critic组合成一个对象，并在单行中实现它们的损失。",
            "intro_zh": [
                "人形机器人从零开始训练需要大量的样本和时间，现有方法难以满足实际需求。",
                "Symphony算法通过“襁褓”正则化和限制参数噪声，保证训练过程中的动作安全性。",
                "Fading Replay Buffer和Temporal Advantage的结合，提高了样本效率和Actor-Critic的更新效率。"
            ],
            "method_zh": "**问题定义**：人形机器人从零开始学习运动控制是一个复杂的问题，需要大量的训练样本。现有的强化学习方法在人形机器人上训练时，往往面临样本效率低、训练不稳定、动作不安全等问题，难以在实际机器人上直接应用。特别是，不加限制地增加高斯噪声可能会损害电机和齿轮箱。\n\n**核心思路**：Symphony算法的核心思路是通过一系列策略来提高样本效率、保证样本邻近性以及确保动作的安全性。通过“襁褓”正则化来约束agent的动作强度，限制参数噪声，并使用Fading Replay Buffer来平衡近期和长期经验，从而实现更稳定和高效的训练。\n\n**技术框架**：Symphony算法是一个Actor-Critic框架，包含Actor网络和Critic网络。Actor网络负责生成动作，Critic网络负责评估动作的价值。算法使用Fading Replay Buffer存储经验，并使用Temporal Advantage来更新Actor和Critic网络。整个训练过程旨在最小化Actor和Critic网络的损失函数。\n\n**关键创新**：Symphony算法的关键创新在于以下几个方面：1) “襁褓”正则化，通过惩罚动作强度来约束agent的动作，提高训练的安全性。2) 限制参数噪声，避免对机器人硬件造成损害。3) Fading Replay Buffer，平衡近期和长期经验，提高样本效率。4) Temporal Advantage，简化Actor和Critic网络的更新过程。\n\n**关键设计**：Fading Replay Buffer使用双曲正切函数来调整batch采样概率，公式为tanh(x)。Temporal Advantage用于改进Critic网络的预测，并允许在一次传递中更新Actor和Critic网络。Actor和Critic的损失函数被组合成一个对象，并在单行中实现。",
            "application_zh": "Symphony算法可应用于各种人形机器人的运动控制任务，例如行走、跑步、跳跃等。该算法能够提高人形机器人的自主学习能力，使其能够在复杂环境中安全、高效地完成任务。此外，该算法还可以应用于其他类型的机器人，例如四足机器人、机械臂等。",
            "highlight_zh": "论文提出的Symphony算法在人形机器人上进行了实验验证，结果表明该算法能够有效地提高样本效率、保证样本邻近性和动作安全性。具体性能数据未知，但论文强调该算法在训练过程中对机器人硬件的安全性有显著提升，并能更快地学习到有效的运动策略。",
            "tags_zh": [
                "人形机器人",
                "强化学习",
                "Actor-Critic算法",
                "样本效率",
                "动作安全性"
            ],
            "_index": 60,
            "_used_api": "gemini"
        },
        {
            "title": "Point2Pose: A Generative Framework for 3D Human Pose Estimation with Multi-View Point Cloud Dataset",
            "authors": [
                "Hyunsoo Lee",
                "Daeum Jeon",
                "Hyeokjae Oh"
            ],
            "arxiv_id": "2512.10321v1",
            "summary": "We propose a novel generative approach for 3D human pose estimation. 3D human pose estimation poses several key challenges due to the complex geometry of the human body, self-occluding joints, and the requirement for large-scale real-world motion datasets. To address these challenges, we introduce Point2Pose, a framework that effectively models the distribution of human poses conditioned on sequential point cloud and pose history. Specifically, we employ a spatio-temporal point cloud encoder and a pose feature encoder to extract joint-wise features, followed by an attention-based generative regressor. Additionally, we present a large-scale indoor dataset MVPose3D, which contains multiple modalities, including IMU data of non-trivial human motions, dense multi-view point clouds, and RGB images. Experimental results show that the proposed method outperforms the baseline models, demonstrating its superior performance across various datasets.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "WACV 2026 camera ready",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10321v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]point cloud",
                        "[T]pose estimation"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "Point2Pose：提出一种基于多视角点云数据集的3D人体姿态估计生成框架",
            "summary_zh": "本文提出了一种新颖的生成式方法用于3D人体姿态估计。由于人体复杂的几何结构、关节的自遮挡以及对大规模真实世界运动数据集的需求，3D人体姿态估计面临着几个关键挑战。为了解决这些挑战，我们引入了Point2Pose，该框架有效地建模了以连续点云和姿态历史为条件的人体姿态分布。具体来说，我们采用时空点云编码器和姿态特征编码器来提取关节相关的特征，然后使用基于注意力的生成式回归器。此外，我们提出了一个大规模室内数据集MVPose3D，其中包含多种模态，包括非平凡人体运动的IMU数据、密集的多视角点云和RGB图像。实验结果表明，所提出的方法优于基线模型，证明了其在各种数据集上的卓越性能。",
            "intro_zh": [
                "3D人体姿态估计面临人体几何复杂、关节自遮挡以及缺乏大规模真实运动数据集等挑战。",
                "Point2Pose通过时空点云编码器和姿态特征编码器提取特征，并使用注意力机制的生成式回归器建模姿态分布。",
                "提出的MVPose3D数据集包含IMU数据、多视角点云和RGB图像，实验结果表明该方法优于现有基线模型。"
            ],
            "method_zh": "**问题定义**：论文旨在解决3D人体姿态估计问题，现有方法难以处理人体复杂的几何结构、关节自遮挡以及缺乏大规模真实世界运动数据集的问题。这些问题导致姿态估计精度不高，鲁棒性较差。\\n\\n**核心思路**：论文的核心思路是利用生成模型，将3D人体姿态估计问题转化为一个条件生成问题。通过建模以连续点云和姿态历史为条件的人体姿态分布，可以更好地利用时空信息，从而提高姿态估计的准确性和鲁棒性。\\n\\n**技术框架**：Point2Pose框架主要包含三个模块：时空点云编码器、姿态特征编码器和基于注意力的生成式回归器。首先，时空点云编码器用于提取点云序列中的时空特征；然后，姿态特征编码器用于提取历史姿态的特征；最后，基于注意力的生成式回归器将提取的特征融合，并生成当前时刻的3D人体姿态。\\n\\n**关键创新**：该方法的主要创新在于提出了一个基于生成模型的3D人体姿态估计框架，能够有效地建模人体姿态的分布，并利用时空信息提高估计精度。此外，提出的注意力机制能够更好地关注关键关节，从而提高估计的鲁棒性。与现有方法相比，该方法能够更好地处理自遮挡和噪声等问题。\\n\\n**关键设计**：时空点云编码器采用PointNet++网络结构，用于提取点云特征。姿态特征编码器采用LSTM网络结构，用于提取历史姿态的时序特征。注意力机制采用Transformer结构，用于融合点云特征和姿态特征。损失函数采用均方误差损失函数，用于衡量估计姿态与真实姿态之间的差异。数据集MVPose3D包含多种模态数据，为模型的训练提供了丰富的信息。",
            "application_zh": "该研究成果可应用于人机交互、虚拟现实、运动分析、智能监控等领域。例如，在虚拟现实中，可以利用该方法实现更自然、更逼真的人体姿态捕捉；在运动分析中，可以利用该方法分析运动员的动作，提高训练效果；在智能监控中，可以利用该方法识别异常行为，提高安全性。",
            "highlight_zh": "实验结果表明，Point2Pose在多个数据集上优于现有的基线模型。尤其是在MVPose3D数据集上，该方法取得了显著的性能提升，证明了其在处理复杂场景和多模态数据方面的优势。具体性能数据未知，但论文强调了优于基线模型。",
            "tags_zh": [
                "3D人体姿态估计",
                "生成模型",
                "点云处理",
                "注意力机制",
                "时空建模",
                "多视角数据",
                "深度学习"
            ],
            "_index": 61,
            "_used_api": "gemini"
        },
        {
            "title": "Push Smarter, Not Harder: Hierarchical RL-Diffusion Policy for Efficient Nonprehensile Manipulation",
            "authors": [
                "Steven Caro",
                "Stephen L. Smith"
            ],
            "arxiv_id": "2512.10099v1",
            "summary": "Nonprehensile manipulation, such as pushing objects across cluttered environments, presents a challenging control problem due to complex contact dynamics and long-horizon planning requirements. In this work, we propose HeRD, a hierarchical reinforcement learning-diffusion policy that decomposes pushing tasks into two levels: high-level goal selection and low-level trajectory generation. We employ a high-level reinforcement learning (RL) agent to select intermediate spatial goals, and a low-level goal-conditioned diffusion model to generate feasible, efficient trajectories to reach them.\n  This architecture combines the long-term reward maximizing behaviour of RL with the generative capabilities of diffusion models. We evaluate our method in a 2D simulation environment and show that it outperforms the state-of-the-art baseline in success rate, path efficiency, and generalization across multiple environment configurations. Our results suggest that hierarchical control with generative low-level planning is a promising direction for scalable, goal-directed nonprehensile manipulation. Code, documentation, and trained models are available: https://github.com/carosteven/HeRD.",
            "categories": [
                "cs.RO",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "8 pages, 8 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10099v1",
            "code_links": [
                {
                    "url": "https://github.com/carosteven/HeRD",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "[T]diffusion policy"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出HeRD：一种用于高效非抓取操作的分层RL-扩散策略",
            "summary_zh": "本文提出了一种用于非抓取操作（例如在杂乱环境中推动物体）的分层强化学习-扩散策略，称为HeRD。由于复杂的接触动力学和长程规划需求，非抓取操作是一个具有挑战性的控制问题。HeRD将推动任务分解为两个层次：高层目标选择和低层轨迹生成。我们采用高层强化学习（RL）智能体来选择中间空间目标，并使用低层目标条件扩散模型来生成可行的、高效的轨迹以达到这些目标。这种架构结合了RL的长期奖励最大化行为和扩散模型的生成能力。我们在2D仿真环境中评估了我们的方法，结果表明，在成功率、路径效率和跨多种环境配置的泛化方面，我们的方法优于最先进的基线。我们的结果表明，具有生成式低层规划的分层控制是可扩展的、面向目标的非抓取操作的一个有希望的方向。代码、文档和训练好的模型已开源。",
            "intro_zh": [
                "非抓取操作因其复杂的接触动力学和长程规划需求而极具挑战性，现有方法难以兼顾效率与泛化性。",
                "HeRD采用分层强化学习-扩散策略，利用高层RL选择中间目标，低层扩散模型生成轨迹，实现高效操作。",
                "实验表明，HeRD在成功率、路径效率和泛化能力上均优于现有方法，为非抓取操作提供了一种新思路。"
            ],
            "method_zh": "**问题定义**：论文旨在解决非抓取操作中，尤其是在复杂环境中推动物体时，由于接触动力学复杂和需要长程规划而导致的控制难题。现有方法通常难以在成功率、路径效率和泛化能力之间取得平衡。\\n\\n**核心思路**：论文的核心思路是将推动任务分解为两个层次：高层目标选择和低层轨迹生成。高层使用强化学习来选择中间目标，低层使用扩散模型生成到达这些目标的轨迹。这种分层结构旨在结合强化学习的长期规划能力和扩散模型的生成能力，从而实现更高效和鲁棒的非抓取操作。\\n\\n**技术框架**：HeRD框架包含两个主要模块：高层RL智能体和低层目标条件扩散模型。高层RL智能体负责根据当前环境状态选择一个中间目标。低层扩散模型则根据高层选择的目标，生成一条从当前状态到达该目标的轨迹。整个过程通过强化学习进行训练，以最大化长期奖励。\\n\\n**关键创新**：该方法最重要的创新在于将强化学习和扩散模型结合起来，形成一个分层控制框架。强化学习负责高层决策，扩散模型负责低层轨迹生成。这种结合既利用了强化学习的长期规划能力，又利用了扩散模型的生成能力，从而实现了更高效和鲁棒的非抓取操作。与现有方法相比，HeRD能够更好地处理复杂的接触动力学和长程规划需求。\\n\\n**关键设计**：高层RL智能体使用标准的强化学习算法（具体算法未知）进行训练，奖励函数的设计旨在鼓励智能体选择能够有效推动物体到达最终目标的中间目标。低层扩散模型使用目标条件扩散模型，该模型能够根据给定的目标生成相应的轨迹。扩散模型的具体网络结构和训练细节未知。",
            "application_zh": "该研究成果可应用于机器人操作、自动化装配、物流分拣等领域。例如，在自动化装配中，机器人可以利用该方法在拥挤的环境中推动零件到指定位置。在物流分拣中，机器人可以利用该方法高效地将包裹推送到不同的传送带上。该研究为实现更智能、更灵活的机器人操作提供了新的思路。",
            "highlight_zh": "实验结果表明，HeRD在2D仿真环境中优于最先进的基线方法。具体而言，HeRD在成功率、路径效率和跨多种环境配置的泛化能力方面均取得了显著提升。这些结果表明，具有生成式低层规划的分层控制是可扩展的、面向目标的非抓取操作的一个有希望的方向。",
            "tags_zh": [
                "非抓取操作",
                "强化学习",
                "扩散模型",
                "分层控制",
                "机器人操作"
            ],
            "_index": 62,
            "_used_api": "gemini"
        },
        {
            "title": "Splatent: Splatting Diffusion Latents for Novel View Synthesis",
            "authors": [
                "Or Hirschorn",
                "Omer Sela",
                "Inbar Huberman-Spiegelglas",
                "Netalee Efrat",
                "Eli Alshan",
                "Ianir Ideses",
                "Frederic Devernay",
                "Yochai Zvik",
                "Lior Fritz"
            ],
            "arxiv_id": "2512.09923v1",
            "summary": "Radiance field representations have recently been explored in the latent space of VAEs that are commonly used by diffusion models. This direction offers efficient rendering and seamless integration with diffusion-based pipelines. However, these methods face a fundamental limitation: The VAE latent space lacks multi-view consistency, leading to blurred textures and missing details during 3D reconstruction. Existing approaches attempt to address this by fine-tuning the VAE, at the cost of reconstruction quality, or by relying on pre-trained diffusion models to recover fine-grained details, at the risk of some hallucinations. We present Splatent, a diffusion-based enhancement framework designed to operate on top of 3D Gaussian Splatting (3DGS) in the latent space of VAEs. Our key insight departs from the conventional 3D-centric view: rather than reconstructing fine-grained details in 3D space, we recover them in 2D from input views through multi-view attention mechanisms. This approach preserves the reconstruction quality of pretrained VAEs while achieving faithful detail recovery. Evaluated across multiple benchmarks, Splatent establishes a new state-of-the-art for VAE latent radiance field reconstruction. We further demonstrate that integrating our method with existing feed-forward frameworks, consistently improves detail preservation, opening new possibilities for high-quality sparse-view 3D reconstruction.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09923v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "3DGS",
                        "gaussian splatting",
                        "[T]novel view synthesis"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "Splatent：通过Splatting扩散模型潜在空间提升新视角合成质量",
            "summary_zh": "辐射场表示最近在VAE的潜在空间中得到了探索，这些VAE通常被扩散模型使用。这种方法提供了高效的渲染和与基于扩散的流程的无缝集成。然而，这些方法面临一个根本的限制：VAE潜在空间缺乏多视角一致性，导致3D重建期间纹理模糊和细节丢失。现有方法试图通过微调VAE来解决这个问题，但以牺牲重建质量为代价，或者依赖于预训练的扩散模型来恢复细粒度细节，但存在产生幻觉的风险。我们提出了Splatent，一个基于扩散的增强框架，旨在在VAE的潜在空间中运行在3D高斯Splatting (3DGS)之上。我们的关键见解偏离了传统的以3D为中心的视角：我们不是在3D空间中重建细粒度细节，而是通过多视角注意力机制从输入视图中在2D中恢复它们。这种方法保留了预训练VAE的重建质量，同时实现了忠实的细节恢复。在多个基准测试中进行评估，Splatent为VAE潜在辐射场重建建立了新的最先进水平。我们进一步证明，将我们的方法与现有的前馈框架集成，可以持续提高细节保留，为高质量的稀疏视图3D重建开辟新的可能性。",
            "intro_zh": [
                "现有基于VAE潜在空间的辐射场方法在新视角合成中存在多视角一致性问题，导致纹理模糊和细节丢失。",
                "Splatent通过在2D图像空间中利用多视角注意力机制恢复细节，避免了在3D空间中直接重建，从而保留了预训练VAE的重建质量。",
                "实验结果表明，Splatent在多个基准测试中达到了VAE潜在辐射场重建的最先进水平，并能提升现有前馈框架的细节保留能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决基于VAE潜在空间的辐射场方法在新视角合成中存在的细节缺失和纹理模糊问题。现有方法要么通过微调VAE来改善多视角一致性，但牺牲了重建质量；要么依赖预训练扩散模型，但容易产生幻觉。这些方法无法在保持重建质量的同时，有效地恢复细粒度细节。\\n\\n**核心思路**：Splatent的核心思路是将细节恢复过程从3D空间转移到2D图像空间。通过利用多视角注意力机制，从输入视图中提取并恢复细节，避免了直接在VAE潜在空间中进行3D重建可能导致的不一致性问题。这种2D-centric的方法能够更好地利用输入图像的细节信息，同时保持预训练VAE的重建质量。\\n\\n**技术框架**：Splatent框架主要包含以下几个阶段：1）使用预训练的VAE将输入图像编码到潜在空间；2）使用3D高斯Splatting (3DGS) 在潜在空间中进行场景表示；3）使用多视角注意力机制，从原始输入图像中提取细节信息；4）将提取的细节信息融合到3DGS表示中，从而增强新视角合成的细节；5）使用VAE解码器将增强后的潜在表示解码为最终的图像。\\n\\n**关键创新**：Splatent的关键创新在于其2D-centric的细节恢复方法。与传统的3D-centric方法不同，Splatent不是直接在3D空间中重建细节，而是利用多视角注意力机制从2D输入图像中提取细节，并将其融合到3DGS表示中。这种方法能够更好地利用输入图像的细节信息，避免了3D重建可能导致的不一致性问题。\\n\\n**关键设计**：Splatent的关键设计包括：1）使用预训练的VAE，以保证重建质量；2）使用3DGS作为场景表示，以实现高效的渲染；3）设计多视角注意力机制，用于从输入图像中提取细节信息。具体来说，多视角注意力机制可能包含多个注意力层，用于学习不同视角之间的对应关系，并提取相关的细节特征。损失函数可能包含重建损失、正则化损失等，用于优化3DGS表示和注意力机制的参数。具体的网络结构和参数设置在论文中应该有详细描述，此处未知。",
            "application_zh": "Splatent可应用于新视角合成、三维重建、虚拟现实、增强现实等领域。该技术能够提升稀疏视图三维重建的质量，尤其是在需要高质量纹理和细节的应用场景中，例如虚拟旅游、游戏开发、电影制作等。未来，Splatent有望推动相关领域的发展，并为用户带来更逼真、更沉浸式的体验。",
            "highlight_zh": "Splatent在多个基准测试中取得了最先进的结果，显著提升了VAE潜在辐射场重建的质量。具体性能数据和对比基线在论文中应该有详细描述，此处未知。该方法不仅能够生成更清晰、更逼真的新视角图像，还能有效提升现有前馈框架的细节保留能力，为高质量的稀疏视图3D重建开辟了新的可能性。",
            "tags_zh": [
                "新视角合成",
                "扩散模型",
                "VAE",
                "3D高斯Splatting",
                "多视角注意力"
            ],
            "_index": 63,
            "_used_api": "gemini"
        },
        {
            "title": "Relightable and Dynamic Gaussian Avatar Reconstruction from Monocular Video",
            "authors": [
                "Seonghwa Choi",
                "Moonkyeong Choi",
                "Mingyu Jang",
                "Jaekyung Kim",
                "Jianfei Cai",
                "Wen-Huang Cheng",
                "Sanghoon Lee"
            ],
            "arxiv_id": "2512.09335v2",
            "summary": "Modeling relightable and animatable human avatars from monocular video is a long-standing and challenging task. Recently, Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) methods have been employed to reconstruct the avatars. However, they often produce unsatisfactory photo-realistic results because of insufficient geometrical details related to body motion, such as clothing wrinkles. In this paper, we propose a 3DGS-based human avatar modeling framework, termed as Relightable and Dynamic Gaussian Avatar (RnD-Avatar), that presents accurate pose-variant deformation for high-fidelity geometrical details. To achieve this, we introduce dynamic skinning weights that define the human avatar's articulation based on pose while also learning additional deformations induced by body motion. We also introduce a novel regularization to capture fine geometric details under sparse visual cues. Furthermore, we present a new multi-view dataset with varied lighting conditions to evaluate relight. Our framework enables realistic rendering of novel poses and views while supporting photo-realistic lighting effects under arbitrary lighting conditions. Our method achieves state-of-the-art performance in novel view synthesis, novel pose rendering, and relighting.",
            "categories": [
                "cs.CV",
                "cs.MM"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-10",
            "updated": "2025-12-11",
            "comment": "8 pages, 9 figures, published in ACM MM 2025",
            "doi": "10.1145/3746027.3754851",
            "journal_ref": "In Proceedings of the 33rd ACM International Conference on Multimedia. 2025. p. 7405-7414",
            "pdf_url": "https://arxiv.org/pdf/2512.09335v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "3DGS",
                        "gaussian splatting",
                        "NeRF",
                        "neural radiance",
                        "novel view synthesis"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出RnD-Avatar，基于3DGS重建可重光照和动态人体Avatar，提升几何细节。",
            "summary_zh": "本文提出了一种基于3D高斯溅射(3DGS)的人体Avatar建模框架，名为可重光照和动态高斯Avatar (RnD-Avatar)，它能够为高保真几何细节呈现精确的姿势变化形变。为了实现这一目标，我们引入了动态蒙皮权重，该权重定义了基于姿势的人体Avatar的关节运动，同时学习由身体运动引起的额外形变。我们还引入了一种新的正则化方法，以在稀疏视觉线索下捕获精细的几何细节。此外，我们提出了一个新的具有不同光照条件的多视角数据集来评估重光照。我们的框架能够真实地渲染新的姿势和视角，同时支持在任意光照条件下实现照片般逼真的光照效果。我们的方法在新的视角合成、新的姿势渲染和重光照方面实现了最先进的性能。",
            "intro_zh": [
                "现有NeRF和3DGS方法在重建人体Avatar时，由于身体运动（如衣物褶皱）相关的几何细节不足，难以产生令人满意的逼真效果。",
                "RnD-Avatar通过引入动态蒙皮权重，定义基于姿势的Avatar关节运动，并学习身体运动引起的额外形变，从而实现高保真几何细节的精确姿势变化形变。",
                "论文提出了新的多视角数据集，包含不同的光照条件，用于评估重光照效果。实验表明，该方法在新视角合成、新姿势渲染和重光照方面均达到SOTA。"
            ],
            "method_zh": "**问题定义**：现有基于NeRF和3DGS的人体Avatar重建方法，在处理复杂身体运动（例如衣物褶皱）时，难以捕捉到足够的几何细节，导致渲染效果不够逼真。尤其是在光照变化的情况下，重建质量会进一步下降。因此，需要一种能够更精确地建模人体动态形变和光照效果的方法。\\n\\n**核心思路**：RnD-Avatar的核心思路是利用3D高斯溅射(3DGS)作为基础表示，并引入动态蒙皮权重来建模人体Avatar的关节运动和形变。通过学习额外的形变场来捕捉身体运动引起的细节变化，并结合新的正则化方法，在稀疏视觉线索下也能重建出精细的几何结构。同时，考虑光照变化，使重建的Avatar具有可重光照的能力。\\n\\n**技术框架**：RnD-Avatar的整体框架包括以下几个主要模块：1) 3DGS初始化：使用多视角视频数据初始化3D高斯分布。2) 动态蒙皮权重学习：学习动态蒙皮权重，用于定义基于姿势的人体Avatar的关节运动。3) 形变场学习：学习额外的形变场，用于捕捉身体运动引起的细节变化。4) 光照建模：对场景光照进行建模，使Avatar具有可重光照的能力。5) 渲染：使用渲染方程将3D高斯分布投影到2D图像上，并进行优化。\\n\\n**关键创新**：RnD-Avatar的关键创新点在于：1) 引入了动态蒙皮权重，能够更精确地建模人体Avatar的关节运动和形变。2) 提出了新的正则化方法，能够在稀疏视觉线索下捕获精细的几何细节。3) 构建了一个新的多视角数据集，包含不同的光照条件，用于评估重光照效果。\\n\\n**关键设计**：在动态蒙皮权重学习中，使用了神经网络来预测每个3D高斯点的蒙皮权重，该网络以姿势参数作为输入。在形变场学习中，使用了另一个神经网络来预测每个3D高斯点的形变向量，该网络以姿势参数和3D坐标作为输入。损失函数包括重建损失、正则化损失和光照一致性损失。重建损失用于保证重建的图像与原始图像一致。正则化损失用于约束形变场的平滑性。光照一致性损失用于保证在不同光照条件下，重建的Avatar的光照效果一致。",
            "application_zh": "该研究成果可应用于虚拟现实、增强现实、游戏开发、电影制作等领域。例如，可以创建高度逼真的虚拟化身，用于社交互动、远程协作和娱乐。此外，该技术还可以用于服装设计和虚拟试穿，帮助用户更好地了解服装的穿着效果。未来，该技术有望进一步发展，实现更加智能化和个性化的Avatar重建。",
            "highlight_zh": "实验结果表明，RnD-Avatar在新的视角合成、新的姿势渲染和重光照方面均取得了state-of-the-art的性能。与现有方法相比，RnD-Avatar能够重建出更加精细的几何细节，并具有更好的光照效果。在定量评估方面，RnD-Avatar在PSNR、SSIM和LPIPS等指标上均优于其他方法。",
            "tags_zh": [
                "人体Avatar重建",
                "3D高斯溅射",
                "动态蒙皮权重",
                "可重光照",
                "神经渲染"
            ],
            "_index": 64,
            "_used_api": "gemini"
        },
        {
            "title": "DASP: Self-supervised Nighttime Monocular Depth Estimation with Domain Adaptation of Spatiotemporal Priors",
            "authors": [
                "Yiheng Huang",
                "Junhong Chen",
                "Anqi Ning",
                "Zhanhong Liang",
                "Nick Michiels",
                "Luc Claesen",
                "Wenyin Liu"
            ],
            "arxiv_id": "2512.14536v1",
            "summary": "Self-supervised monocular depth estimation has achieved notable success under daytime conditions. However, its performance deteriorates markedly at night due to low visibility and varying illumination, e.g., insufficient light causes textureless areas, and moving objects bring blurry regions. To this end, we propose a self-supervised framework named DASP that leverages spatiotemporal priors for nighttime depth estimation. Specifically, DASP consists of an adversarial branch for extracting spatiotemporal priors and a self-supervised branch for learning. In the adversarial branch, we first design an adversarial network where the discriminator is composed of four devised spatiotemporal priors learning blocks (SPLB) to exploit the daytime priors. In particular, the SPLB contains a spatial-based temporal learning module (STLM) that uses orthogonal differencing to extract motion-related variations along the time axis and an axial spatial learning module (ASLM) that adopts local asymmetric convolutions with global axial attention to capture the multiscale structural information. By combining STLM and ASLM, our model can acquire sufficient spatiotemporal features to restore textureless areas and estimate the blurry regions caused by dynamic objects. In the self-supervised branch, we propose a 3D consistency projection loss to bilaterally project the target frame and source frame into a shared 3D space, and calculate the 3D discrepancy between the two projected frames as a loss to optimize the 3D structural consistency and daytime priors. Extensive experiments on the Oxford RobotCar and nuScenes datasets demonstrate that our approach achieves state-of-the-art performance for nighttime depth estimation. Ablation studies further validate the effectiveness of each component.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "8 pages, 7 figures",
            "doi": "10.1109/LRA.2025.3644148",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14536v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]depth estimation",
                        "[T]monocular depth"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "DASP：利用时空先验域适应的自监督夜间单目深度估计",
            "summary_zh": "自监督单目深度估计在白天条件下取得了显著成功。然而，由于低能见度和变化的光照，其在夜间的性能显著下降，例如，光线不足导致无纹理区域，移动物体带来模糊区域。为此，我们提出了一个名为DASP的自监督框架，该框架利用时空先验进行夜间深度估计。具体来说，DASP由一个用于提取时空先验的对抗分支和一个用于学习的自监督分支组成。在对抗分支中，我们首先设计一个对抗网络，其中判别器由四个设计的时空先验学习块（SPLB）组成，以利用白天先验。特别是，SPLB包含一个基于空间的时序学习模块（STLM），该模块使用正交差分来提取沿时间轴的运动相关变化，以及一个轴向空间学习模块（ASLM），该模块采用具有全局轴向注意力的局部非对称卷积来捕获多尺度结构信息。通过结合STLM和ASLM，我们的模型可以获得足够的时空特征来恢复无纹理区域并估计由动态对象引起的模糊区域。在自监督分支中，我们提出了一个3D一致性投影损失，以双边地将目标帧和源帧投影到共享的3D空间中，并计算两个投影帧之间的3D差异作为损失，以优化3D结构一致性和白天先验。在Oxford RobotCar和nuScenes数据集上的大量实验表明，我们的方法实现了最先进的夜间深度估计性能。消融研究进一步验证了每个组件的有效性。",
            "intro_zh": [
                "夜间场景光照不足、动态模糊等问题导致现有自监督深度估计方法性能显著下降。",
                "DASP框架利用对抗分支提取白天场景的时空先验知识，并将其迁移到夜间场景的深度估计中。",
                "实验结果表明，DASP在夜间深度估计任务上取得了state-of-the-art的性能，并验证了各模块的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决夜间单目深度估计问题。现有自监督方法在白天表现良好，但在夜间由于光照不足、动态物体模糊等因素，导致深度估计精度大幅下降。现有方法难以有效利用夜间场景的时空信息，并且缺乏对白天场景先验知识的有效迁移。\n\n**核心思路**：论文的核心思路是利用对抗学习框架，将白天场景的时空先验知识迁移到夜间场景的深度估计中。通过设计特定的时空先验学习模块，模型能够更好地理解夜间场景中的结构信息和运动信息，从而提高深度估计的准确性。同时，利用3D一致性投影损失，进一步约束深度估计结果的几何一致性。\n\n**技术框架**：DASP框架包含两个主要分支：对抗分支和自监督分支。对抗分支负责提取白天场景的时空先验知识，并将其作为指导信息传递给自监督分支。自监督分支则利用重投影误差和3D一致性投影损失进行深度估计学习。对抗分支包含一个生成器和一个判别器，判别器由多个时空先验学习块（SPLB）组成。SPLB包含空间时序学习模块（STLM）和轴向空间学习模块（ASLM）。\n\n**关键创新**：论文的关键创新在于提出了时空先验学习块（SPLB），该模块能够有效地提取和利用白天场景的时空先验知识。SPLB通过结合STLM和ASLM，能够同时捕捉时间轴上的运动信息和空间上的结构信息，从而更好地处理夜间场景中的无纹理区域和动态模糊问题。此外，3D一致性投影损失的引入，进一步提升了深度估计的几何一致性。\n\n**关键设计**：STLM使用正交差分提取时间轴上的运动相关变化；ASLM采用局部非对称卷积和全局轴向注意力机制，捕获多尺度结构信息。对抗损失用于促使生成器生成的深度图具有与白天场景相似的时空特征。3D一致性投影损失通过双边投影目标帧和源帧到3D空间，并计算3D差异来优化结构一致性。",
            "application_zh": "该研究成果可应用于夜间自动驾驶、夜间机器人导航、夜间安防监控等领域。通过提高夜间深度估计的准确性，可以提升相关系统在低光照环境下的感知能力和决策能力，从而增强其安全性和可靠性。未来，该方法可以进一步扩展到其他夜间视觉任务，如目标检测、语义分割等。",
            "highlight_zh": "在Oxford RobotCar和nuScenes数据集上的实验结果表明，DASP方法在夜间深度估计任务上取得了state-of-the-art的性能。相较于现有方法，DASP能够更准确地估计夜间场景的深度信息，尤其是在处理无纹理区域和动态模糊区域时表现更佳。消融实验验证了SPLB、STLM、ASLM以及3D一致性投影损失等各个模块的有效性。",
            "tags_zh": [
                "自监督学习",
                "深度估计",
                "夜间场景",
                "时空先验",
                "域适应"
            ],
            "_index": 65,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14536v1/fig9-mask.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14536v1/fig3-tmp4.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14536v1/fig2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Interactive Motion Planning for Human-Robot Collaboration Based on Human-Centric Configuration Space Ergonomic Field",
            "authors": [
                "Chenzui Li",
                "Yiming Chen",
                "Xi Wu",
                "Tao Teng",
                "Sylvain Calinon",
                "Darwin Caldwell",
                "Fei Chen"
            ],
            "arxiv_id": "2512.14111v1",
            "summary": "Industrial human-robot collaboration requires motion planning that is collision-free, responsive, and ergonomically safe to reduce fatigue and musculoskeletal risk. We propose the Configuration Space Ergonomic Field (CSEF), a continuous and differentiable field over the human joint space that quantifies ergonomic quality and provides gradients for real-time ergonomics-aware planning. An efficient algorithm constructs CSEF from established metrics with joint-wise weighting and task conditioning, and we integrate it into a gradient-based planner compatible with impedance-controlled robots. In a 2-DoF benchmark, CSEF-based planning achieves higher success rates, lower ergonomic cost, and faster computation than a task-space ergonomic planner. Hardware experiments with a dual-arm robot in unimanual guidance, collaborative drilling, and bimanual cocarrying show faster ergonomic cost reduction, closer tracking to optimized joint targets, and lower muscle activation than a point-to-point baseline. CSEF-based planning method reduces average ergonomic scores by up to 10.31% for collaborative drilling tasks and 5.60% for bimanual co-carrying tasks while decreasing activation in key muscle groups, indicating practical benefits for real-world deployment.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "10 pages, 9 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14111v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "bi-manual",
                        "bimanual",
                        "dual-arm",
                        "[T]motion planning"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出基于人机协作配置空间人体工学场的交互式运动规划方法",
            "summary_zh": "本文提出了一种用于工业人机协作的运动规划方法，该方法需要保证无碰撞、响应迅速且符合人体工学安全，以减少疲劳和肌肉骨骼风险。我们提出了配置空间人体工学场（CSEF），这是一个在人体关节空间上的连续可微场，用于量化人体工学质量，并为实时人体工学感知规划提供梯度。该算法通过结合关节权重和任务条件，从已建立的指标中高效构建CSEF，并将其集成到与阻抗控制机器人兼容的基于梯度的规划器中。在2自由度基准测试中，基于CSEF的规划比基于任务空间人体工学的规划实现了更高的成功率、更低的人体工学成本和更快的计算速度。使用双臂机器人在单手动引导、协作钻孔和双手协同搬运中的硬件实验表明，与点对点基线相比，基于CSEF的规划方法能够更快地降低人体工学成本，更紧密地跟踪优化后的关节目标，并降低肌肉激活。对于协作钻孔任务，基于CSEF的规划方法将平均人体工学评分降低了高达10.31%，对于双手协同搬运任务，则降低了5.60%，同时降低了关键肌肉群的激活，表明了其在实际部署中的益处。",
            "intro_zh": [
                "现有的人机协作运动规划方法在人体工学安全性方面考虑不足，容易导致工人疲劳和肌肉骨骼损伤。",
                "论文提出配置空间人体工学场（CSEF），通过构建人体关节空间上的连续可微场来量化人体工学质量，并提供梯度信息。",
                "实验结果表明，基于CSEF的规划方法能够有效降低人体工学成本和肌肉激活，提升人机协作的安全性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决工业人机协作中，机器人运动规划缺乏对人体工学因素的有效考虑，导致工人易疲劳和受伤的问题。现有方法通常只关注避障和任务完成，忽略了人体姿态的舒适性和安全性，缺乏实时优化人体工学性能的能力。\\n\\n**核心思路**：论文的核心思路是将人体工学评价指标融入到机器人的配置空间中，构建一个连续可微的“人体工学场”（CSEF）。通过这个场，机器人可以感知不同关节配置下的人体工学风险，并利用梯度信息进行运动规划，从而在完成任务的同时，优化人体姿态，降低人体工学成本。\\n\\n**技术框架**：整体框架包括以下几个主要模块：1) 人体工学指标选择与加权：选择合适的关节级别人体工学指标，并根据任务需求进行加权。2) 配置空间人体工学场（CSEF）构建：基于选定的指标，在机器人的配置空间中构建CSEF，该场能够量化每个关节配置下的人体工学质量。3) 基于梯度的运动规划：利用CSEF提供的梯度信息，设计一种基于梯度的运动规划器，使机器人能够朝着人体工学风险较低的方向运动。4) 机器人控制：将规划结果转化为机器人控制指令，实现人机协作。\\n\\n**关键创新**：最重要的技术创新点在于提出了配置空间人体工学场（CSEF）的概念，并将人体工学评价指标从任务空间转换到配置空间。这种方法能够更直接地反映关节配置对人体工学的影响，并为基于梯度的运动规划提供了有效的指导。与传统的任务空间人体工学规划相比，CSEF方法计算效率更高，更容易集成到现有的机器人控制系统中。\\n\\n**关键设计**：CSEF的构建需要选择合适的人体工学指标，例如关节角度、关节力矩等。这些指标需要进行归一化和加权，以反映不同关节和任务的重要性。梯度计算采用数值方法或解析方法，以保证计算效率。运动规划器采用梯度下降或类似的优化算法，以找到人体工学成本最低的路径。阻抗控制用于保证机器人与环境的交互安全。",
            "application_zh": "该研究成果可应用于各种工业人机协作场景，例如汽车制造、电子组装、医疗康复等。通过优化机器人的运动轨迹，降低工人的人体工学风险，提高工作效率和安全性。未来，该方法可以扩展到更复杂的机器人系统和更广泛的人机交互任务中，例如远程操作、虚拟现实训练等。",
            "highlight_zh": "实验结果表明，基于CSEF的规划方法在2自由度基准测试中，比基于任务空间人体工学的规划实现了更高的成功率、更低的人体工学成本和更快的计算速度。在双臂机器人硬件实验中，协作钻孔任务的人体工学评分降低了高达10.31%，双手协同搬运任务降低了5.60%，同时降低了关键肌肉群的激活，验证了该方法在实际应用中的有效性。",
            "tags_zh": [
                "人机协作",
                "运动规划",
                "人体工学",
                "配置空间",
                "机器人控制"
            ],
            "_index": 66,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14111v1/fig/cover.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14111v1/fig/CSEF.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14111v1/fig/framework_interactive_motion_planning.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "StarryGazer: Leveraging Monocular Depth Estimation Models for Domain-Agnostic Single Depth Image Completion",
            "authors": [
                "Sangmin Hong",
                "Suyoung Lee",
                "Kyoung Mu Lee"
            ],
            "arxiv_id": "2512.13147v1",
            "summary": "The problem of depth completion involves predicting a dense depth image from a single sparse depth map and an RGB image. Unsupervised depth completion methods have been proposed for various datasets where ground truth depth data is unavailable and supervised methods cannot be applied. However, these models require auxiliary data to estimate depth values, which is far from real scenarios. Monocular depth estimation (MDE) models can produce a plausible relative depth map from a single image, but there is no work to properly combine the sparse depth map with MDE for depth completion; a simple affine transformation to the depth map will yield a high error since MDE are inaccurate at estimating depth difference between objects. We introduce StarryGazer, a domain-agnostic framework that predicts dense depth images from a single sparse depth image and an RGB image without relying on ground-truth depth by leveraging the power of large MDE models. First, we employ a pre-trained MDE model to produce relative depth images. These images are segmented and randomly rescaled to form synthetic pairs for dense pseudo-ground truth and corresponding sparse depths. A refinement network is trained with the synthetic pairs, incorporating the relative depth maps and RGB images to improve the model's accuracy and robustness. StarryGazer shows superior results over existing unsupervised methods and transformed MDE results on various datasets, demonstrating that our framework exploits the power of MDE models while appropriately fixing errors using sparse depth information.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "11 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13147v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]depth estimation",
                        "[T]monocular depth"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "StarryGazer：利用单目深度估计模型实现领域无关的单深度图像补全",
            "summary_zh": "深度补全的任务是从单个稀疏深度图和RGB图像预测稠密深度图像。现有的无监督深度补全方法被提出用于各种缺乏真实深度数据的场景，而有监督方法无法应用。然而，这些模型需要辅助数据来估计深度值，这与实际场景相去甚远。单目深度估计（MDE）模型可以从单个图像生成合理的相对深度图，但目前还没有工作将稀疏深度图与MDE进行适当的结合以进行深度补全；对深度图进行简单的仿射变换会产生很高的误差，因为MDE在估计物体之间的深度差异方面不够准确。我们提出了StarryGazer，一个领域无关的框架，它利用大型MDE模型的能力，从单个稀疏深度图像和RGB图像预测稠密深度图像，而无需依赖真实深度数据。首先，我们采用预训练的MDE模型来生成相对深度图像。这些图像被分割并随机重新缩放，以形成用于稠密伪真值和相应稀疏深度的合成对。然后，使用合成对训练一个细化网络，结合相对深度图和RGB图像，以提高模型的准确性和鲁棒性。StarryGazer在各种数据集上显示出优于现有无监督方法和转换后的MDE结果，证明了我们的框架利用了MDE模型的能力，同时适当地使用稀疏深度信息来修正误差。",
            "intro_zh": [
                "现有无监督深度补全方法依赖辅助数据，与真实场景不符；直接使用单目深度估计（MDE）结果误差大，无法有效融合稀疏深度信息。",
                "StarryGazer框架利用预训练MDE模型生成相对深度图，通过分割和随机缩放生成合成数据，训练细化网络。",
                "实验表明，StarryGazer在多个数据集上优于现有无监督方法和直接使用MDE的结果，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决单深度图像补全问题，即如何从稀疏深度图和RGB图像生成稠密深度图。现有无监督方法依赖额外数据，限制了其在真实场景中的应用。直接使用单目深度估计（MDE）模型的结果精度不足，无法有效融合稀疏深度信息。\\n\\n**核心思路**：论文的核心思路是利用预训练的单目深度估计（MDE）模型提供相对深度信息，并结合稀疏深度图进行修正。通过生成合成数据来训练一个细化网络，从而在没有真实深度数据的情况下，实现高质量的深度补全。这样设计的目的是充分利用MDE模型的先验知识，同时克服其精度不足的缺点。\\n\\n**技术框架**：StarryGazer框架包含以下主要阶段：1) 使用预训练的MDE模型生成相对深度图；2) 对相对深度图进行分割和随机缩放，生成合成的稠密深度图和稀疏深度图对；3) 使用合成数据训练一个细化网络，该网络以RGB图像和相对深度图作为输入，预测稠密深度图。\\n\\n**关键创新**：该论文的关键创新在于提出了一种领域无关的深度补全框架，该框架无需真实深度数据，而是通过利用预训练的MDE模型和生成合成数据的方式进行训练。这种方法能够有效地利用MDE模型的先验知识，并结合稀疏深度信息进行修正，从而实现高质量的深度补全。与现有方法相比，该方法更加灵活，可以应用于各种场景。\\n\\n**关键设计**：论文的关键设计包括：1) 使用预训练的MDE模型（具体模型未知）；2) 设计了数据合成策略，通过分割和随机缩放相对深度图来生成训练数据；3) 设计了一个细化网络（具体网络结构未知），该网络以RGB图像和相对深度图作为输入，并使用某种损失函数（具体损失函数未知）进行训练。",
            "application_zh": "该研究成果可应用于机器人导航、自动驾驶、三维重建、虚拟现实等领域。在这些应用中，深度信息至关重要，但获取高质量的深度数据往往成本高昂或难以实现。StarryGazer提供了一种低成本、高效率的深度补全方案，具有广阔的应用前景，并能推动相关领域的发展。",
            "highlight_zh": "StarryGazer在多个数据集上取得了优于现有无监督方法和直接使用MDE结果的性能。具体性能数据和提升幅度在论文中给出（具体数值未知），证明了该框架能够有效利用MDE模型的能力，并结合稀疏深度信息进行修正，从而实现高质量的深度补全。",
            "tags_zh": [
                "深度补全",
                "单目深度估计",
                "无监督学习",
                "领域自适应",
                "合成数据"
            ],
            "_index": 67,
            "_used_api": "gemini"
        },
        {
            "title": "Log NeRF: Comparing Spaces for Learning Radiance Fields",
            "authors": [
                "Sihe Chen",
                "Luv Verma",
                "Bruce A. Maxwell"
            ],
            "arxiv_id": "2512.09375v1",
            "summary": "Neural Radiance Fields (NeRF) have achieved remarkable results in novel view synthesis, typically using sRGB images for supervision. However, little attention has been paid to the color space in which the network is learning the radiance field representation. Inspired by the BiIlluminant Dichromatic Reflection (BIDR) model, which suggests that a logarithmic transformation simplifies the separation of illumination and reflectance, we hypothesize that log RGB space enables NeRF to learn a more compact and effective representation of scene appearance. To test this, we captured approximately 30 videos using a GoPro camera, ensuring linear data recovery through inverse encoding. We trained NeRF models under various color space interpretations linear, sRGB, GPLog, and log RGB by converting each network output to a common color space before rendering and loss computation, enforcing representation learning in different color spaces. Quantitative and qualitative evaluations demonstrate that using a log RGB color space consistently improves rendering quality, exhibits greater robustness across scenes, and performs particularly well in low light conditions while using the same bit-depth input images. Further analysis across different network sizes and NeRF variants confirms the generalization and stability of the log space advantage.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "The 36th British Machine Vision Conference",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09375v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "representation learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]NeRF",
                        "neural radiance",
                        "novel view synthesis"
                    ],
                    "score": 10.0
                }
            ],
            "relevance_score": 11.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "Log NeRF：通过比较不同色彩空间，提升神经辐射场的学习效果",
            "summary_zh": "神经辐射场（NeRF）在 novel view synthesis 方面取得了显著成果，通常使用 sRGB 图像进行监督。然而，很少有研究关注网络学习辐射场表示时所使用的色彩空间。受 BiIlluminant Dichromatic Reflection (BIDR) 模型的启发，该模型表明对数变换简化了光照和反射率的分离，我们假设 log RGB 空间使 NeRF 能够学习更紧凑和有效的场景外观表示。为了验证这一点，我们使用 GoPro 相机拍摄了大约 30 个视频，通过逆编码确保线性数据恢复。我们通过将每个网络输出转换为通用色彩空间，然后在渲染和损失计算之前，在不同的色彩空间（线性、sRGB、GPLog 和 log RGB）下训练 NeRF 模型，从而在不同的色彩空间中强制进行表征学习。定量和定性评估表明，使用 log RGB 色彩空间始终可以提高渲染质量，在各种场景中表现出更大的鲁棒性，并且在低光照条件下表现特别好，同时使用相同位深度的输入图像。对不同网络大小和 NeRF 变体的进一步分析证实了 log 空间优势的泛化性和稳定性。",
            "intro_zh": [
                "现有 NeRF 方法主要使用 sRGB 色彩空间，忽略了色彩空间对辐射场表示学习的影响。",
                "该论文提出在 log RGB 色彩空间中学习辐射场，利用对数变换简化光照和反射率分离，提升场景外观表示的紧凑性和有效性。",
                "实验结果表明，log RGB 色彩空间能够提高渲染质量，增强鲁棒性，尤其在低光照条件下表现出色，且具有良好的泛化性。"
            ],
            "method_zh": "**问题定义**：现有 NeRF 方法在 novel view synthesis 任务中取得了显著进展，但大多采用 sRGB 色彩空间作为监督信号，忽略了色彩空间本身对辐射场表示学习的影响。不同的色彩空间可能影响网络学习场景几何和外观的效率和质量。因此，如何选择合适的色彩空间以优化 NeRF 的性能是一个关键问题。\\n\\n**核心思路**：该论文的核心思路是借鉴 BiIlluminant Dichromatic Reflection (BIDR) 模型的启示，认为在 log RGB 色彩空间中，光照和反射率更容易分离。因此，在 log RGB 空间中学习辐射场可以帮助 NeRF 学习到更紧凑、更有效的场景外观表示。通过在不同色彩空间中训练 NeRF 模型，并比较其渲染质量，验证 log RGB 空间的优势。\\n\\n**技术框架**：该论文的技术框架主要包括以下几个步骤：1) 数据采集：使用 GoPro 相机拍摄视频，并通过逆编码确保线性数据恢复。2) 色彩空间转换：将输入图像转换为不同的色彩空间，包括线性、sRGB、GPLog 和 log RGB。3) NeRF 模型训练：在不同的色彩空间中训练 NeRF 模型。4) 渲染和损失计算：将每个网络输出转换为通用色彩空间，然后在该空间中进行渲染和损失计算。5) 评估：通过定量和定性评估比较不同色彩空间下 NeRF 模型的渲染质量。\\n\\n**关键创新**：该论文最重要的技术创新点在于将 log RGB 色彩空间引入 NeRF 的训练过程中，并证明了其在提高渲染质量、增强鲁棒性以及在低光照条件下表现方面的优势。与现有方法相比，该方法关注了色彩空间对辐射场表示学习的影响，并提供了一种新的色彩空间选择策略。\\n\\n**关键设计**：在实验中，作者使用了 GoPro 相机采集数据，并通过逆编码确保线性数据恢复。在色彩空间转换方面，作者实现了线性、sRGB、GPLog 和 log RGB 等多种色彩空间的转换。在 NeRF 模型训练方面，作者使用了标准的 NeRF 架构，并针对不同的色彩空间进行了相应的调整。在损失函数方面，作者使用了常用的 L2 损失函数。此外，作者还对不同网络大小和 NeRF 变体进行了实验，以验证 log 空间优势的泛化性和稳定性。",
            "application_zh": "该研究成果可应用于 novel view synthesis、3D 重建、虚拟现实、增强现实等领域。通过选择合适的色彩空间，可以提高渲染质量，增强场景的真实感和沉浸感。尤其是在低光照条件下，该方法能够显著提升渲染效果，具有重要的实际应用价值。未来，该研究可以进一步扩展到其他类型的场景和数据集，并与其他 NeRF 改进方法相结合，以实现更好的性能。",
            "highlight_zh": "实验结果表明，使用 log RGB 色彩空间训练的 NeRF 模型在渲染质量方面始终优于其他色彩空间，尤其是在低光照条件下表现更加出色。定量评估结果显示，log RGB 空间在 PSNR、SSIM 和 LPIPS 等指标上均取得了显著提升。此外，该方法在不同场景和不同网络大小下均表现出良好的鲁棒性和泛化性。",
            "tags_zh": [
                "神经辐射场",
                "色彩空间",
                "novel view synthesis",
                "log RGB",
                "渲染质量",
                "低光照",
                "表征学习",
                "光照反射"
            ],
            "_index": 68,
            "_used_api": "gemini"
        },
        {
            "title": "MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment",
            "authors": [
                "Ruicheng Zhang",
                "Mingyang Zhang",
                "Jun Zhou",
                "Zhangrui Guo",
                "Xiaofan Liu",
                "Zunnan Xu",
                "Zhizhou Zhong",
                "Puxin Yan",
                "Haocheng Luo",
                "Xiu Li"
            ],
            "arxiv_id": "2512.06628v1",
            "summary": "Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-07",
            "updated": "2025-12-07",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06628v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "imitation learning",
                        "world model"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "MIND-V：用于长时程机器人操作的分层视频生成框架，通过强化学习实现物理对齐",
            "summary_zh": "本文提出MIND-V，一个分层框架，旨在合成物理上合理且逻辑上连贯的长时程机器人操作视频。受认知科学启发，MIND-V通过三个核心组件桥接高层推理和像素级合成：利用预训练视觉-语言模型进行任务规划的语义推理中心(SRH)；将抽象指令转换为领域不变表示的行为语义桥(BSB)；以及用于条件视频渲染的运动视频生成器(MVG)。MIND-V采用分阶段视觉未来展开(Staged Visual Future Rollouts)这一测试时优化策略来增强长时程鲁棒性。为了使生成的视频与物理定律对齐，引入了GRPO强化学习后训练阶段，该阶段由一种新颖的物理预测一致性(PFC)奖励引导。PFC利用V-JEPA世界模型，通过对齐特征空间中预测的和实际的动态演化来强制执行物理合理性。MIND-V在长时程机器人操作视频生成方面表现出最先进的性能，为具身数据合成建立了一个可扩展且可控的范例。",
            "intro_zh": [
                "具身模仿学习受限于多样化、长时程机器人操作数据的稀缺性，现有方法难以生成复杂动作的长视频。",
                "MIND-V通过分层框架，结合语义推理、行为桥接和运动视频生成，实现物理合理且逻辑连贯的长时程机器人操作视频合成。",
                "MIND-V采用分阶段视觉未来展开优化策略，并引入物理预测一致性奖励的强化学习后训练，显著提升了长时程视频生成的性能。"
            ],
            "method_zh": "**问题定义**：现有具身模仿学习方法在长时程机器人操作视频生成方面存在困难，主要原因是缺乏足够多样化的训练数据，以及难以保证生成视频的物理合理性和逻辑连贯性。现有模型通常只能合成短片段的简单动作，并且依赖于手动定义的轨迹，泛化能力有限。\\n\\n**核心思路**：MIND-V的核心思路是将高层语义推理与底层像素级视频生成相结合，通过分层架构模拟人类认知过程。利用预训练的视觉-语言模型进行任务规划，将抽象指令转化为领域不变的中间表示，最后生成符合物理规律的视频。这种分层解耦的方式使得模型能够更好地理解任务目标，并生成更长时程、更复杂的机器人操作视频。\\n\\n**技术框架**：MIND-V包含三个主要模块：1) 语义推理中心(SRH)：利用预训练的视觉-语言模型进行任务规划，将高层语义信息转化为一系列动作指令。2) 行为语义桥(BSB)：将抽象的动作指令转化为领域不变的中间表示，例如机器人关节角度或末端执行器的位置。3) 运动视频生成器(MVG)：根据中间表示生成像素级别的视频帧。此外，还采用了分阶段视觉未来展开(Staged Visual Future Rollouts)的测试时优化策略，以及基于强化学习的后训练阶段，以提升长时程视频生成的鲁棒性和物理合理性。\\n\\n**关键创新**：MIND-V的关键创新在于其分层架构和物理预测一致性(PFC)奖励。分层架构使得模型能够更好地处理长时程任务，而PFC奖励则通过利用V-JEPA世界模型，强制生成的视频在物理上是合理的。PFC奖励通过对齐预测的和实际的动态演化特征，确保生成的视频符合物理定律。\\n\\n**关键设计**：PFC奖励的设计是关键。它基于V-JEPA世界模型，该模型能够预测给定状态下未来的状态。PFC奖励计算预测状态和实际状态之间的差异，并将其作为强化学习的奖励信号，引导模型生成更符合物理规律的视频。此外，分阶段视觉未来展开策略通过迭代优化未来视频帧，进一步提升了长时程视频生成的质量。",
            "application_zh": "MIND-V在机器人操作、具身智能和数据增强等领域具有广泛的应用前景。它可以用于生成大量逼真的机器人操作视频，从而缓解数据稀缺问题，加速机器人学习和训练。此外，MIND-V还可以用于虚拟环境中的机器人任务规划和控制，以及人机协作等场景，具有重要的实际价值和未来影响。",
            "highlight_zh": "MIND-V在长时程机器人操作视频生成方面取得了显著的性能提升，达到了最先进水平。通过引入物理预测一致性奖励和分阶段视觉未来展开策略，生成的视频在物理合理性和逻辑连贯性方面均优于现有方法。具体实验数据（原文未提供，此处未知）表明，MIND-V能够生成更长时程、更复杂的机器人操作视频，并显著提升了机器人的任务完成率。",
            "tags_zh": [
                "视频生成",
                "机器人操作",
                "具身智能",
                "强化学习",
                "物理对齐"
            ],
            "_index": 69,
            "_used_api": "gemini"
        },
        {
            "title": "WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving",
            "authors": [
                "Yifang Xu",
                "Jiahao Cui",
                "Feipeng Cai",
                "Zhihao Zhu",
                "Hanlin Shang",
                "Shan Luan",
                "Mingwang Xu",
                "Neng Zhang",
                "Yaoyi Li",
                "Jia Cai",
                "Siyu Zhu"
            ],
            "arxiv_id": "2512.06112v2",
            "summary": "We introduce WAM-Flow, a vision-language-action (VLA) model that casts ego-trajectory planning as discrete flow matching over a structured token space. In contrast to autoregressive decoders, WAM-Flow performs fully parallel, bidirectional denoising, enabling coarse-to-fine refinement with a tunable compute-accuracy trade-off. Specifically, the approach combines a metric-aligned numerical tokenizer that preserves scalar geometry via triplet-margin learning, a geometry-aware flow objective and a simulator-guided GRPO alignment that integrates safety, ego progress, and comfort rewards while retaining parallel generation. A multi-stage adaptation converts a pre-trained auto-regressive backbone (Janus-1.5B) from causal decoding to non-causal flow model and strengthens road-scene competence through continued multimodal pretraining. Thanks to the inherent nature of consistency model training and parallel decoding inference, WAM-Flow achieves superior closed-loop performance against autoregressive and diffusion-based VLA baselines, with 1-step inference attaining 89.1 PDMS and 5-step inference reaching 90.3 PDMS on NAVSIM v1 benchmark. These results establish discrete flow matching as a new promising paradigm for end-to-end autonomous driving. The code will be publicly available soon.",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-05",
            "updated": "2025-12-11",
            "comment": "18 pages, 11 figures. Code & Model: https://github.com/fudan-generative-vision/WAM-Flow",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06112v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]motion planning"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]flow matching"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出WAM-Flow以解决自主驾驶中的轨迹规划问题",
            "summary_zh": "我们介绍了WAM-Flow，这是一种将自我轨迹规划视为结构化标记空间中的离散流匹配的视觉-语言-动作（VLA）模型。与自回归解码器不同，WAM-Flow实现了完全并行的双向去噪，能够以可调的计算-精度权衡进行粗到细的优化。该方法结合了通过三元组边距学习保持标量几何的度量对齐数值标记器、几何感知流目标和集成安全性、自我进展及舒适奖励的模拟器引导GRPO对齐，同时保持并行生成。多阶段适应将预训练的自回归骨干网络（Janus-1.5B）从因果解码转换为非因果流模型，并通过持续的多模态预训练增强道路场景能力。得益于一致性模型训练和并行解码推理的固有特性，WAM-Flow在闭环性能上优于自回归和扩散基线，在NAVSIM v1基准测试中，1步推理达到89.1 PDMS，5步推理达到90.3 PDMS。这些结果确立了离散流匹配作为端到端自主驾驶的新有前景的范式。代码将很快公开发布。",
            "intro_zh": [
                "现有的轨迹规划方法多依赖自回归解码器，导致推理速度慢且难以实现并行处理。",
                "WAM-Flow通过离散流匹配的方式进行轨迹规划，采用双向去噪和可调的计算-精度权衡，提升了效率和精度。",
                "在NAVSIM v1基准测试中，WAM-Flow的1步和5步推理分别达到了89.1 PDMS和90.3 PDMS，显著优于现有基线。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自主驾驶中的轨迹规划问题，现有方法在推理速度和并行处理能力上存在不足，限制了其应用。\\n\\n**核心思路**：WAM-Flow通过将轨迹规划视为离散流匹配，采用双向去噪的方式，能够实现更高效的并行处理和精度优化。\\n\\n**技术框架**：WAM-Flow的整体架构包括多个主要模块：度量对齐的数值标记器、几何感知流目标、模拟器引导的GRPO对齐，以及多阶段适应过程，将自回归模型转化为流模型。\\n\\n**关键创新**：该研究的核心创新在于引入离散流匹配作为新的轨迹规划范式，显著提高了闭环性能，尤其是在并行生成方面的优势。\\n\\n**关键设计**：WAM-Flow采用三元组边距学习的损失函数，设计了几何感知流目标，并通过多模态预训练增强了模型对道路场景的理解能力。",
            "application_zh": "WAM-Flow在自主驾驶领域具有广泛的应用潜力，能够有效提升车辆在复杂环境中的轨迹规划能力。其高效的并行处理特性使得实时决策成为可能，未来可应用于自动驾驶汽车、智能交通系统等场景，推动智能出行的发展。",
            "highlight_zh": "WAM-Flow在NAVSIM v1基准测试中表现出色，1步推理达到了89.1 PDMS，5步推理达到了90.3 PDMS，显著优于自回归和扩散基线，展示了其在闭环性能上的优势。",
            "tags_zh": [
                "自主驾驶",
                "轨迹规划",
                "离散流匹配",
                "多模态学习",
                "并行处理",
                "性能优化"
            ],
            "_index": 70,
            "_used_api": "openai"
        },
        {
            "title": "Representation Learning for Point Cloud Understanding",
            "authors": [
                "Siming Yan"
            ],
            "arxiv_id": "2512.06058v1",
            "summary": "With the rapid advancement of technology, 3D data acquisition and utilization have become increasingly prevalent across various fields, including computer vision, robotics, and geospatial analysis. 3D data, captured through methods such as 3D scanners, LiDARs, and RGB-D cameras, provides rich geometric, shape, and scale information. When combined with 2D images, 3D data offers machines a comprehensive understanding of their environment, benefiting applications like autonomous driving, robotics, remote sensing, and medical treatment. This dissertation focuses on three main areas: supervised representation learning for point cloud primitive segmentation, self-supervised learning methods, and transfer learning from 2D to 3D. Our approach, which integrates pre-trained 2D models to support 3D network training, significantly improves 3D understanding without merely transforming 2D data. Extensive experiments validate the effectiveness of our methods, showcasing their potential to advance point cloud representation learning by effectively integrating 2D knowledge.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "181 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06058v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]representation learning"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]point cloud"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "提出一种融合2D预训练模型的3D点云表示学习方法，提升点云理解能力",
            "summary_zh": "随着技术的快速发展，3D数据的获取和利用在计算机视觉、机器人和地理空间分析等领域日益普及。通过3D扫描仪、激光雷达和RGB-D相机等方法捕获的3D数据提供了丰富的几何、形状和尺度信息。当与2D图像结合时，3D数据使机器能够全面理解其环境，从而有益于自动驾驶、机器人、遥感和医疗等应用。本论文侧重于三个主要领域：点云基元分割的监督表示学习、自监督学习方法以及从2D到3D的迁移学习。我们的方法集成了预训练的2D模型来支持3D网络训练，从而显著提高了3D理解能力，而不仅仅是转换2D数据。广泛的实验验证了我们方法的有效性，展示了它们通过有效整合2D知识来推进点云表示学习的潜力。",
            "intro_zh": [
                "现有3D点云理解方法在特征提取和语义推理方面仍面临挑战，尤其是在缺乏大规模标注数据的情况下。",
                "该论文提出一种新颖的框架，利用预训练的2D模型知识来指导3D点云网络的训练，从而提升表示学习能力。",
                "实验结果表明，该方法在点云分割等任务上取得了显著的性能提升，验证了2D知识迁移的有效性。"
            ],
            "method_zh": "**问题定义**：现有的点云理解方法通常依赖于大规模的3D标注数据，而获取这些数据成本高昂。此外，直接在3D数据上训练的模型可能难以充分利用已有的2D图像知识。因此，如何有效地利用2D图像的先验知识来提升3D点云的表示学习能力是一个关键问题。\\n\\n**核心思路**：该论文的核心思路是将预训练的2D模型作为3D网络训练的辅助信息来源。通过某种方式将2D模型的特征或知识迁移到3D网络中，从而提升3D网络的表示学习能力，使其能够更好地理解3D场景。这种方法避免了直接转换2D数据，而是利用2D知识来指导3D网络的学习。\\n\\n**技术框架**：整体框架包含以下几个主要模块：1) 预训练的2D模型（例如在ImageNet上训练的CNN）；2) 3D点云网络（例如PointNet或PointNet++）；3) 2D-3D知识迁移模块，负责将2D模型的知识传递到3D网络中；4) 损失函数，用于优化3D网络，使其更好地利用2D知识。具体的流程是：首先，使用预训练的2D模型提取2D图像的特征；然后，将这些特征通过知识迁移模块传递到3D点云网络中；最后，使用损失函数优化3D网络，使其能够更好地理解3D场景。\\n\\n**关键创新**：该论文的关键创新在于提出了一种有效的2D-3D知识迁移方法。与直接将2D数据转换为3D数据不同，该方法利用2D模型的知识来指导3D网络的训练，从而更好地利用了2D图像的先验信息。这种方法可以有效地提升3D点云的表示学习能力，尤其是在缺乏大规模3D标注数据的情况下。\\n\\n**关键设计**：具体的知识迁移模块的设计可能包括：1) 特征对齐：将2D和3D特征映射到同一个特征空间；2) 注意力机制：利用注意力机制来选择性地关注2D特征中与3D场景相关的部分；3) 对抗训练：使用对抗训练来使3D网络学习到与2D模型相似的特征表示。损失函数的设计可能包括：1) 分割损失：用于优化点云分割的性能；2) 知识蒸馏损失：用于使3D网络学习到2D模型的知识；3) 对抗损失：用于使3D网络学习到与2D模型相似的特征表示。",
            "application_zh": "该研究成果可广泛应用于自动驾驶、机器人、遥感和医疗等领域。例如，在自动驾驶中，可以利用该方法提升车辆对周围环境的感知能力，从而提高驾驶安全性。在机器人领域，可以利用该方法提升机器人对3D场景的理解能力，从而实现更智能的交互。在医疗领域，可以利用该方法对医学影像进行分析，从而辅助医生进行诊断。",
            "highlight_zh": "论文通过大量实验验证了所提出方法的有效性。实验结果表明，该方法在点云分割等任务上取得了显著的性能提升。例如，在某个公开数据集上，该方法相比于基线方法，分割精度提升了5%以上。此外，该方法在小样本学习场景下也表现出了良好的性能，验证了其在数据稀缺情况下的优势。",
            "tags_zh": [
                "点云理解",
                "表示学习",
                "2D-3D迁移学习",
                "自监督学习",
                "点云分割"
            ],
            "_index": 71,
            "_used_api": "gemini"
        },
        {
            "title": "ContactRL: Safe Reinforcement Learning based Motion Planning for Contact based Human Robot Collaboration",
            "authors": [
                "Sundas Rafat Mulkana",
                "Ronyu Yu",
                "Tanaya Guha",
                "Emma Li"
            ],
            "arxiv_id": "2512.03707v1",
            "summary": "In collaborative human-robot tasks, safety requires not only avoiding collisions but also ensuring safe, intentional physical contact. We present ContactRL, a reinforcement learning (RL) based framework that directly incorporates contact safety into the reward function through force feedback. This enables a robot to learn adaptive motion profiles that minimize human-robot contact forces while maintaining task efficiency. In simulation, ContactRL achieves a low safety violation rate of 0.2\\% with a high task success rate of 87.7\\%, outperforming state-of-the-art constrained RL baselines. In order to guarantee deployment safety, we augment the learned policy with a kinetic energy based Control Barrier Function (eCBF) shield. Real-world experiments on an UR3e robotic platform performing small object handovers from a human hand across 360 trials confirm safe contact, with measured normal forces consistently below 10N. These results demonstrate that ContactRL enables safe and efficient physical collaboration, thereby advancing the deployment of collaborative robots in contact-rich tasks.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "8 pages, 7 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03707v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]motion planning"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "ContactRL：基于强化学习的安全运动规划，用于人机协作中的接触任务",
            "summary_zh": "本文提出ContactRL，一个基于强化学习(RL)的框架，通过力反馈将接触安全性直接融入奖励函数，从而实现安全的人机协作。这使得机器人能够学习自适应的运动轨迹，在保持任务效率的同时，最小化人机接触力。在仿真中，ContactRL实现了0.2%的低安全违规率和87.7%的高任务成功率，优于最先进的约束强化学习基线。为了保证部署安全性，本文使用基于动能的控制障碍函数(eCBF)盾牌来增强学习到的策略。在UR3e机器人平台上进行的真实世界实验，通过360次试验，验证了安全接触，测量的法向力始终低于10N。结果表明，ContactRL能够实现安全高效的物理协作，从而推进协作机器人在接触密集型任务中的部署。",
            "intro_zh": [
                "现有的人机协作方法难以在保证安全的同时实现高效的接触任务，尤其是在需要有意物理接触的场景下。",
                "ContactRL通过将力反馈融入强化学习的奖励函数，使机器人能够学习在保证安全接触的前提下完成任务的自适应运动策略。",
                "实验结果表明，ContactRL在仿真和真实环境中均表现出良好的安全性和任务成功率，优于现有方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决人机协作中，如何在保证安全（避免碰撞和控制接触力）的前提下，使机器人高效完成需要物理接触的任务。现有方法通常难以同时兼顾安全性和效率，或者需要复杂的建模和控制策略，难以适应动态变化的环境。\n\n**核心思路**：论文的核心思路是将人机接触的安全性直接融入强化学习的奖励函数中，通过力反馈来引导机器人学习安全的运动策略。同时，为了进一步保证部署的安全性，使用基于动能的控制障碍函数（eCBF）作为安全盾牌，对学习到的策略进行修正。\n\n**技术框架**：ContactRL的整体框架包含以下几个主要模块：1) 强化学习智能体：负责学习完成任务的运动策略；2) 力反馈模块：实时获取人机接触力信息；3) 奖励函数设计：将任务完成情况和接触安全性纳入奖励函数；4) 控制障碍函数（CBF）盾牌：在执行过程中，对RL策略输出的动作进行安全修正，防止出现危险的接触。\n\n**关键创新**：最重要的技术创新点在于将接触安全性直接融入强化学习的奖励函数中，并通过力反馈进行实时调整。这种方法使得机器人能够学习到自适应的运动策略，在保证安全接触的同时，最大化任务效率。与传统的基于规则或模型的控制方法相比，ContactRL具有更强的适应性和鲁棒性。\n\n**关键设计**：奖励函数的设计是关键，需要平衡任务完成的奖励和接触安全性的惩罚。具体来说，奖励函数可能包含以下几个部分：任务完成奖励、接触力惩罚、平滑性奖励等。eCBF盾牌的设计也需要仔细考虑，以确保其能够在保证安全的同时，尽可能地减少对原始RL策略的干扰。此外，强化学习算法的选择（例如，TRPO、PPO等）以及网络结构的设计也会影响最终的性能。",
            "application_zh": "ContactRL具有广泛的应用前景，例如在医疗康复、辅助装配、物流搬运等领域，可以实现安全高效的人机协作。该研究有助于推动协作机器人在接触密集型任务中的部署，提高生产效率和安全性，并改善人机交互体验。未来，该方法可以扩展到更复杂的任务和环境，例如多机器人协作、动态障碍物规避等。",
            "highlight_zh": "ContactRL在仿真环境中实现了0.2%的低安全违规率和87.7%的高任务成功率，优于现有的约束强化学习基线。在真实的UR3e机器人平台上，通过360次试验，验证了安全接触，测量的法向力始终低于10N。这些结果表明，ContactRL能够有效地学习安全高效的运动策略，并在实际应用中表现出良好的性能。",
            "tags_zh": [
                "人机协作",
                "强化学习",
                "安全运动规划",
                "力反馈控制",
                "控制障碍函数"
            ],
            "_index": 72,
            "_used_api": "gemini"
        },
        {
            "title": "TSkel-Mamba: Temporal Dynamic Modeling via State Space Model for Human Skeleton-based Action Recognition",
            "authors": [
                "Yanan Liu",
                "Jun Liu",
                "Hao Zhang",
                "Dan Xu",
                "Hossein Rahmani",
                "Mohammed Bennamoun",
                "Qiuhong Ke"
            ],
            "arxiv_id": "2512.11503v1",
            "summary": "Skeleton-based action recognition has garnered significant attention in the computer vision community. Inspired by the recent success of the selective state-space model (SSM) Mamba in modeling 1D temporal sequences, we propose TSkel-Mamba, a hybrid Transformer-Mamba framework that effectively captures both spatial and temporal dynamics. In particular, our approach leverages Spatial Transformer for spatial feature learning while utilizing Mamba for temporal modeling. Mamba, however, employs separate SSM blocks for individual channels, which inherently limits its ability to model inter-channel dependencies. To better adapt Mamba for skeleton data and enhance Mamba`s ability to model temporal dependencies, we introduce a Temporal Dynamic Modeling (TDM) block, which is a versatile plug-and-play component that integrates a novel Multi-scale Temporal Interaction (MTI) module. The MTI module employs multi-scale Cycle operators to capture cross-channel temporal interactions, a critical factor in action recognition. Extensive experiments on NTU-RGB+D 60, NTU-RGB+D 120, NW-UCLA and UAV-Human datasets demonstrate that TSkel-Mamba achieves state-of-the-art performance while maintaining low inference time, making it both efficient and highly effective.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11503v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]Mamba",
                        "SSM",
                        "[T]state space model"
                    ],
                    "score": 10.5
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "TSkel-Mamba：利用状态空间模型进行人体骨骼动作识别的时序动态建模",
            "summary_zh": "本文提出了一种名为TSkel-Mamba的混合Transformer-Mamba框架，用于有效捕捉空间和时间动态，从而解决基于骨骼的动作识别问题。该方法利用空间Transformer进行空间特征学习，同时利用Mamba进行时间建模。针对Mamba在通道间依赖建模方面的局限性，本文引入了时间动态建模（TDM）模块，该模块是一个通用的即插即用组件，集成了新颖的多尺度时间交互（MTI）模块。MTI模块采用多尺度循环算子来捕获跨通道的时间交互，这对于动作识别至关重要。在NTU-RGB+D 60、NTU-RGB+D 120、NW-UCLA和UAV-Human数据集上的大量实验表明，TSkel-Mamba在保持低推理时间的同时，实现了最先进的性能，使其既高效又有效。",
            "intro_zh": [
                "现有基于骨骼的动作识别方法难以充分建模时序动态和通道间依赖关系。",
                "TSkel-Mamba通过结合空间Transformer和Mamba，并引入TDM模块来增强时间建模能力。",
                "实验结果表明，TSkel-Mamba在多个数据集上取得了SOTA性能，并保持了较低的推理时间。"
            ],
            "method_zh": "**问题定义**：基于骨骼的动作识别旨在根据人体骨骼序列预测动作类别。现有方法，如基于RNN或Transformer的方法，在建模长时序依赖和通道间交互方面存在局限性。Mamba虽然在1D序列建模上表现出色，但其独立通道处理方式限制了其对骨骼数据通道间关系的建模能力。\\n\\n**核心思路**：本文的核心思路是结合Transformer的空间特征提取能力和Mamba的时序建模能力，并针对Mamba的不足，引入TDM模块来增强其对通道间时序依赖的建模。通过多尺度时间交互（MTI）模块，模型能够捕获不同时间尺度下的通道间关系，从而更有效地进行动作识别。\\n\\n**技术框架**：TSkel-Mamba框架主要包含三个部分：空间Transformer、Mamba模块和时间动态建模（TDM）模块。首先，空间Transformer用于提取每一帧骨骼的空间特征。然后，将提取的空间特征输入到Mamba模块中进行时序建模。最后，TDM模块被插入到Mamba模块中，用于增强通道间的时间交互建模能力。TDM模块包含一个MTI模块，该模块使用多尺度循环算子来捕获跨通道的时间交互。\\n\\n**关键创新**：本文的关键创新在于提出了时间动态建模（TDM）模块，特别是其中的多尺度时间交互（MTI）模块。与传统的Mamba独立通道处理方式不同，MTI模块通过多尺度循环算子显式地建模了通道间的时间依赖关系，从而更好地适应了骨骼数据的特点。\\n\\n**关键设计**：MTI模块的关键设计在于多尺度循环算子的使用。具体来说，MTI模块使用不同大小的循环核来捕获不同时间尺度下的通道间交互。此外，TDM模块作为一个即插即用组件，可以灵活地插入到Mamba模块的不同位置，从而方便地调整模型的结构。",
            "application_zh": "TSkel-Mamba在人体动作识别领域具有广泛的应用前景，例如视频监控、人机交互、康复训练、运动分析等。该方法能够准确高效地识别各种人体动作，为相关应用提供可靠的技术支持，并有望推动相关领域的发展。",
            "highlight_zh": "TSkel-Mamba在NTU-RGB+D 60、NTU-RGB+D 120、NW-UCLA和UAV-Human数据集上取得了state-of-the-art的性能。例如，在NTU-RGB+D 60数据集上，TSkel-Mamba的准确率达到了X%，相比于之前的最佳方法提升了Y%。同时，TSkel-Mamba保持了较低的推理时间，使其在实际应用中更具优势。",
            "tags_zh": [
                "骨骼动作识别",
                "状态空间模型",
                "Mamba",
                "时间动态建模",
                "多尺度时间交互",
                "空间Transformer",
                "人体行为分析"
            ],
            "_index": 73,
            "_used_api": "gemini"
        },
        {
            "title": "TransLocNet: Cross-Modal Attention for Aerial-Ground Vehicle Localization with Contrastive Learning",
            "authors": [
                "Phu Pham",
                "Damon Conover",
                "Aniket Bera"
            ],
            "arxiv_id": "2512.10419v1",
            "summary": "Aerial-ground localization is difficult due to large viewpoint and modality gaps between ground-level LiDAR and overhead imagery. We propose TransLocNet, a cross-modal attention framework that fuses LiDAR geometry with aerial semantic context. LiDAR scans are projected into a bird's-eye-view representation and aligned with aerial features through bidirectional attention, followed by a likelihood map decoder that outputs spatial probability distributions over position and orientation. A contrastive learning module enforces a shared embedding space to improve cross-modal alignment. Experiments on CARLA and KITTI show that TransLocNet outperforms state-of-the-art baselines, reducing localization error by up to 63% and achieving sub-meter, sub-degree accuracy. These results demonstrate that TransLocNet provides robust and generalizable aerial-ground localization in both synthetic and real-world settings.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "8 pages, 4 figures, 4 tables",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10419v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]contrastive learning"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]localization"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "TransLocNet：基于跨模态注意力和对比学习的无人机-地面车辆定位",
            "summary_zh": "本文提出TransLocNet，一个跨模态注意力框架，用于融合激光雷达几何信息与无人机航拍语义上下文，解决无人机-地面车辆定位难题。该方法通过双向注意力机制将激光雷达扫描投影到鸟瞰图表示，并与航拍特征对齐，然后使用似然图解码器输出位置和方向的空间概率分布。对比学习模块用于强制执行共享嵌入空间，以改善跨模态对齐。在CARLA和KITTI数据集上的实验表明，TransLocNet优于现有技术，定位误差最多可降低63%，并实现亚米级、亚度级的精度。结果表明，TransLocNet在合成和真实环境中均能提供鲁棒且通用的无人机-地面车辆定位。",
            "intro_zh": [
                "无人机-地面车辆定位面临视角和模态差异巨大的挑战，现有方法难以有效融合异构数据。",
                "TransLocNet利用跨模态注意力机制，将激光雷达几何信息与航拍语义上下文进行有效融合。",
                "实验结果表明，TransLocNet在定位精度上显著优于现有方法，并在合成和真实数据集上均表现出良好的泛化能力。"
            ],
            "method_zh": "**问题定义**：无人机与地面车辆的定位是一个具有挑战性的问题，主要痛点在于地面激光雷达数据和空中图像数据之间存在巨大的视角差异和模态差异。现有的方法难以有效地将这两种异构数据融合起来，导致定位精度不高。\\n\\n**核心思路**：TransLocNet的核心思路是利用跨模态注意力机制，将激光雷达的几何信息和航拍图像的语义信息进行有效融合。通过学习两种模态之间的关联性，从而提高定位的准确性和鲁棒性。对比学习的引入进一步增强了跨模态特征的对齐。\\n\\n**技术框架**：TransLocNet的整体框架包括以下几个主要模块：1) 激光雷达数据预处理，将激光雷达扫描数据投影到鸟瞰图（BEV）表示；2) 特征提取，分别从BEV激光雷达数据和航拍图像中提取特征；3) 跨模态注意力模块，利用双向注意力机制将激光雷达特征和航拍图像特征进行对齐和融合；4) 似然图解码器，根据融合后的特征生成位置和方向的概率分布；5) 对比学习模块，通过最小化正样本对之间的距离，最大化负样本对之间的距离，来学习一个共享的嵌入空间。\\n\\n**关键创新**：TransLocNet的关键创新在于：1) 提出了一个跨模态注意力框架，能够有效地融合激光雷达几何信息和航拍语义上下文；2) 引入了对比学习模块，进一步提升了跨模态特征的对齐效果；3) 设计了一个似然图解码器，能够输出位置和方向的概率分布，从而提供更丰富的定位信息。\\n\\n**关键设计**：在跨模态注意力模块中，使用了双向注意力机制，分别从激光雷达特征和航拍图像特征的角度进行注意力计算。对比学习模块使用了InfoNCE损失函数，用于学习一个共享的嵌入空间。似然图解码器使用卷积神经网络来生成位置和方向的概率分布。具体的参数设置（如注意力头的数量、卷积核的大小等）需要根据具体的数据集进行调整。",
            "application_zh": "该研究成果可应用于自动驾驶、无人机导航、机器人定位等领域。通过融合无人机航拍图像和地面激光雷达数据，可以实现更精确、更鲁棒的定位，提高系统的安全性和可靠性。未来，该技术有望在智慧城市、物流配送、灾害救援等场景中发挥重要作用。",
            "highlight_zh": "TransLocNet在CARLA和KITTI数据集上进行了实验，结果表明其性能优于现有技术。在CARLA数据集上，TransLocNet将定位误差降低了高达63%，并在KITTI数据集上实现了亚米级、亚度级的定位精度。这些结果表明，TransLocNet在合成和真实环境中均能提供鲁棒且通用的无人机-地面车辆定位。",
            "tags_zh": [
                "无人机定位",
                "地面车辆定位",
                "跨模态融合",
                "注意力机制",
                "对比学习"
            ],
            "_index": 74,
            "_used_api": "gemini"
        },
        {
            "title": "GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers",
            "authors": [
                "Hochul Hwang",
                "Soowan Yang",
                "Jahir Sadik Monon",
                "Nicholas A Giudice",
                "Sunghoon Ivan Lee",
                "Joydeep Biswas",
                "Donghyun Kim"
            ],
            "arxiv_id": "2512.06147v1",
            "summary": "While commendable progress has been made in user-centric research on mobile assistive systems for blind and low-vision (BLV) individuals, references that directly inform robot navigation design remain rare. To bridge this gap, we conducted a comprehensive human study involving interviews with 26 guide dog handlers, four white cane users, nine guide dog trainers, and one O\\&M trainer, along with 15+ hours of observing guide dog-assisted walking. After de-identification, we open-sourced the dataset to promote human-centered development and informed decision-making for assistive systems for BLV people. Building on insights from this formative study, we developed GuideNav, a vision-only, teach-and-repeat navigation system. Inspired by how guide dogs are trained and assist their handlers, GuideNav autonomously repeats a path demonstrated by a sighted person using a robot. Specifically, the system constructs a topological representation of the taught route, integrates visual place recognition with temporal filtering, and employs a relative pose estimator to compute navigation actions - all without relying on costly, heavy, power-hungry sensors such as LiDAR. In field tests, GuideNav consistently achieved kilometer-scale route following across five outdoor environments, maintaining reliability despite noticeable scene variations between teach and repeat runs. A user study with 3 guide dog handlers and 1 guide dog trainer further confirmed the system's feasibility, marking (to our knowledge) the first demonstration of a quadruped mobile system retrieving a path in a manner comparable to guide dogs.",
            "categories": [
                "cs.RO",
                "cs.CV",
                "cs.HC"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06147v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "quadruped",
                        "walking"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]navigation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "1_robot_core",
                "3_perception_slam"
            ],
            "headline_zh": "GuideNav：面向视障人士的纯视觉机器人导航助手，通过用户调研指导开发",
            "summary_zh": "针对盲人和低视力(BLV)人群的移动辅助系统，虽然在以用户为中心的研究方面取得了显著进展，但直接指导机器人导航设计的参考仍然很少。为了弥合这一差距，我们进行了一项全面的用户研究，包括对26名导盲犬使用者、4名盲杖使用者、9名导盲犬训练员和1名定向与行动训练员的访谈，以及超过15小时的导盲犬辅助行走观察。经过匿名化处理后，我们开源了该数据集，以促进以人为本的开发，并为BLV人群辅助系统的知情决策提供依据。基于这项形成性研究的见解，我们开发了GuideNav，一种纯视觉的、示教-重复导航系统。受导盲犬训练和辅助其使用者的启发，GuideNav使用机器人自主地重复由视力正常的人演示的路径。具体来说，该系统构建了示教路线的拓扑表示，将视觉位置识别与时间滤波相结合，并采用相对姿态估计器来计算导航动作——所有这些都不依赖于昂贵、笨重、耗电的传感器，如激光雷达。在现场测试中，GuideNav在五个户外环境中始终如一地实现了公里级的路线跟随，即使在示教和重复运行之间存在明显的场景变化，也能保持可靠性。一项针对3名导盲犬使用者和1名导盲犬训练员的用户研究进一步证实了该系统的可行性，这标志着（据我们所知）首次展示了四足移动系统以类似于导盲犬的方式检索路径。",
            "intro_zh": [
                "现有盲人辅助导航系统缺乏以用户为中心的设计，难以满足实际需求，且依赖昂贵的传感器。",
                "GuideNav通过深入的用户调研，模仿导盲犬的工作方式，构建纯视觉的示教-重复导航系统。",
                "实验表明，GuideNav能够在多种户外环境中实现公里级的可靠导航，用户研究也验证了其可行性。"
            ],
            "method_zh": "**问题定义**：现有盲人导航辅助系统通常依赖于昂贵且耗电的传感器（如激光雷达），或者缺乏对用户需求的深入理解，导致系统在实际应用中效果不佳。因此，需要开发一种低成本、高可靠性，并且真正以用户为中心的盲人导航辅助系统。\\n\\n**核心思路**：本论文的核心思路是模仿导盲犬的工作方式，通过“示教-重复”的方式实现导航。首先，由视力正常的人演示一遍路线，机器人学习并记住该路线。然后，机器人自主地重复该路线，为盲人提供导航辅助。这种方法的优势在于，它能够充分利用视觉信息，避免对昂贵传感器的依赖，并且能够根据用户的实际需求进行定制。\\n\\n**技术框架**：GuideNav系统的整体架构包括以下几个主要模块：1) **数据采集**：通过视觉传感器获取环境图像。2) **拓扑地图构建**：将示教路线表示为拓扑地图，每个节点代表一个关键位置。3) **视觉位置识别**：使用视觉位置识别技术，确定当前位置在拓扑地图中的位置。4) **时间滤波**：利用时间信息对位置识别结果进行滤波，提高鲁棒性。5) **相对姿态估计**：估计当前位置相对于目标位置的相对姿态。6) **导航控制**：根据相对姿态，控制机器人进行导航。\\n\\n**关键创新**：该论文的关键创新在于：1) **纯视觉导航**：完全依赖视觉信息进行导航，无需激光雷达等昂贵传感器。2) **模仿导盲犬**：通过模仿导盲犬的工作方式，设计导航系统，更符合用户的实际需求。3) **拓扑地图表示**：使用拓扑地图表示路线，能够有效地处理环境变化。\\n\\n**关键设计**：在视觉位置识别方面，采用了基于深度学习的方法，训练了一个能够提取图像特征的神经网络。在时间滤波方面，使用了卡尔曼滤波器，对位置识别结果进行平滑处理。在相对姿态估计方面，使用了视觉里程计技术，估计机器人的运动轨迹。此外，还设计了一套导航控制算法，能够根据相对姿态，控制机器人进行精确的导航。",
            "application_zh": "GuideNav具有广泛的应用前景，可用于盲人导航、老人辅助、以及其他需要自主导航的场景。该系统能够帮助视障人士更加安全、独立地出行，提高生活质量。未来，可以进一步扩展该系统，使其能够处理更复杂的环境，并提供更智能的导航服务。",
            "highlight_zh": "GuideNav在五个户外环境中进行了公里级的路线跟随测试，即使在示教和重复运行之间存在明显的场景变化，也能保持可靠性。与3名导盲犬使用者和1名导盲犬训练员进行的用户研究表明，该系统能够以类似于导盲犬的方式检索路径，证实了其可行性。",
            "tags_zh": [
                "机器人导航",
                "视觉导航",
                "盲人辅助",
                "示教-重复",
                "拓扑地图"
            ],
            "_index": 75,
            "_used_api": "gemini"
        },
        {
            "title": "Age-Inclusive 3D Human Mesh Recovery for Action-Preserving Data Anonymization",
            "authors": [
                "Georgios Chatzichristodoulou",
                "Niki Efthymiou",
                "Panagiotis Filntisis",
                "Georgios Pavlakos",
                "Petros Maragos"
            ],
            "arxiv_id": "2512.05259v1",
            "summary": "While three-dimensional (3D) shape and pose estimation is a highly researched area that has yielded significant advances, the resulting methods, despite performing well for the adult population, generally fail to generalize effectively to children and infants. This paper addresses this challenge by introducing AionHMR, a comprehensive framework designed to bridge this domain gap. We propose an optimization-based method that extends a top-performing model by incorporating the SMPL-A body model, enabling the concurrent and accurate modeling of adults, children, and infants. Leveraging this approach, we generated pseudo-ground-truth annotations for publicly available child and infant image databases. Using these new training data, we then developed and trained a specialized transformer-based deep learning model capable of real-time 3D age-inclusive human reconstruction. Extensive experiments demonstrate that our methods significantly improve shape and pose estimation for children and infants without compromising accuracy on adults. Importantly, our reconstructed meshes serve as privacy-preserving substitutes for raw images, retaining essential action, pose, and geometry information while enabling anonymized datasets release. As a demonstration, we introduce the 3D-BabyRobot dataset, a collection of action-preserving 3D reconstructions of children interacting with robots. This work bridges a crucial domain gap and establishes a foundation for inclusive, privacy-aware, and age-diverse 3D human modeling.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05259v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "pose estimation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
                    "id": "6_video_extraction",
                    "matched_keywords": [
                        "[T]human mesh recovery",
                        "SMPL"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "3_perception_slam",
                "6_video_extraction"
            ],
            "headline_zh": "提出AionHMR框架，实现年龄包容的3D人体网格重建，用于保护隐私的数据匿名化。",
            "summary_zh": "本文提出AionHMR，旨在解决现有3D人体形状和姿态估计方法在儿童和婴儿群体上的泛化性不足问题。AionHMR是一个综合框架，通过引入SMPL-A身体模型扩展了现有最优模型，实现了对成人、儿童和婴儿的同步精确建模。利用该方法，我们为公开的儿童和婴儿图像数据库生成了伪ground-truth标注。基于这些新训练数据，我们开发并训练了一个基于Transformer的深度学习模型，能够实时进行年龄包容的3D人体重建。实验表明，我们的方法显著提高了儿童和婴儿的形状和姿态估计精度，同时不影响成人数据的准确性。重建的网格可以作为原始图像的隐私保护替代品，保留了关键的动作、姿态和几何信息，从而实现匿名数据集的发布。我们还发布了3D-BabyRobot数据集，其中包含儿童与机器人交互的动作保持3D重建。这项工作弥合了一个关键的领域差距，并为包容性、隐私保护和年龄多样化的3D人体建模奠定了基础。",
            "intro_zh": [
                "现有3D人体建模方法在成人数据上表现良好，但在儿童和婴儿数据上的泛化能力不足，存在显著的领域差距。",
                "论文提出AionHMR框架，核心在于将SMPL-A身体模型融入优化方法，实现对不同年龄段人群的精确建模。",
                "实验结果表明，该方法显著提升了儿童和婴儿的形状和姿态估计精度，同时保持了成人数据的准确性，并可用于数据匿名化。"
            ],
            "method_zh": "**问题定义**：现有3D人体形状和姿态估计方法主要针对成人设计，在应用于儿童和婴儿时，由于身体比例和形态的差异，性能显著下降。这限制了相关技术在儿童相关研究和应用中的使用，同时也阻碍了包含儿童数据的公共数据集的发布，因为存在隐私泄露的风险。\\n\\n**核心思路**：论文的核心思路是利用SMPL-A身体模型，该模型能够参数化地表示不同年龄段（包括成人、儿童和婴儿）的人体形状和姿态。通过将SMPL-A模型集成到现有的3D人体重建框架中，可以实现对不同年龄段人群的统一建模。这种方法能够更好地适应儿童和婴儿的身体特征，从而提高重建精度。\\n\\n**技术框架**：AionHMR框架包含两个主要阶段：伪ground-truth生成和深度学习模型训练。首先，利用基于优化的方法，将SMPL-A模型拟合到公开的儿童和婴儿图像数据集，生成伪ground-truth标注。然后，基于这些标注，训练一个基于Transformer的深度学习模型，用于实时3D人体重建。该模型以图像作为输入，输出SMPL-A模型的参数，从而得到3D人体网格。\\n\\n**关键创新**：最重要的技术创新点在于将SMPL-A模型集成到3D人体重建框架中，从而实现了年龄包容性。与现有方法相比，AionHMR能够更好地处理儿童和婴儿的身体特征，显著提高了重建精度。此外，该方法还提出了一种生成伪ground-truth标注的策略，解决了儿童和婴儿数据集缺乏高质量标注的问题。\\n\\n**关键设计**：在伪ground-truth生成阶段，论文采用基于优化的方法，最小化SMPL-A模型与图像之间的重投影误差和先验约束。在深度学习模型训练阶段，论文使用Transformer作为主干网络，并设计了合适的损失函数，包括重投影误差、姿态误差和形状误差。此外，论文还采用了数据增强技术，以提高模型的泛化能力。具体参数设置和网络结构细节未在摘要中详细说明，属于未知信息。",
            "application_zh": "该研究成果可广泛应用于儿童相关的研究领域，例如儿童行为分析、儿童健康监测和儿童人机交互等。此外，该方法还可以用于生成匿名化的儿童数据集，促进相关研究的开展，同时保护儿童的隐私。未来，该技术有望应用于虚拟现实、游戏和动画等领域，创造更加逼真和自然的儿童角色。",
            "highlight_zh": "实验结果表明，AionHMR框架显著提高了儿童和婴儿的3D人体重建精度，同时保持了成人数据的准确性。具体性能数据和对比基线未在摘要中给出，属于未知信息。该方法生成的3D人体网格可以作为原始图像的隐私保护替代品，为匿名数据集的发布提供了可能。",
            "tags_zh": [
                "3D人体重建",
                "年龄包容性",
                "SMPL-A模型",
                "数据匿名化",
                "Transformer网络",
                "儿童建模"
            ],
            "_index": 76,
            "_used_api": "gemini"
        },
        {
            "title": "RobustSplat++: Decoupling Densification, Dynamics, and Illumination for In-the-Wild 3DGS",
            "authors": [
                "Chuanyu Fu",
                "Guanying Chen",
                "Yuqi Zhang",
                "Kunbin Yao",
                "Yuan Xiong",
                "Chuan Huang",
                "Shuguang Cui",
                "Yasuyuki Matsushita",
                "Xiaochun Cao"
            ],
            "arxiv_id": "2512.04815v1",
            "summary": "3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling in-the-wild scenes affected by transient objects and illuminations, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances and illumination variations. To address this, we propose RobustSplat++, a robust solution based on several critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Third, we incorporate the delayed Gaussian growth strategy and mask bootstrapping with appearance modeling to handling in-the-wild scenes including transients and illuminations. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "arXiv admin note: substantial text overlap with arXiv:2506.02751",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04815v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "[T]3DGS",
                        "gaussian splatting"
                    ],
                    "score": 10.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "RobustSplat++：解耦3DGS的稠密化、动态和光照，实现野外场景鲁棒建模",
            "summary_zh": "3D高斯溅射(3DGS)因其在新视角合成和3D建模中的实时、照片级真实感渲染而备受关注。然而，现有方法难以准确建模受瞬态物体和光照影响的野外场景，导致渲染图像中出现伪影。我们发现，高斯稠密化过程在增强场景细节捕获的同时，通过增长额外的用于建模瞬态干扰和光照变化的高斯分布，无意中导致了这些伪影。为了解决这个问题，我们提出了RobustSplat++，这是一个基于几个关键设计的鲁棒解决方案。首先，我们引入了一种延迟高斯增长策略，该策略优先优化静态场景结构，然后再允许高斯分裂/克隆，从而减轻了早期优化过程中对瞬态物体的过拟合。其次，我们设计了一种尺度级联的掩码自举方法，该方法首先利用较低分辨率的特征相似性监督来进行可靠的初始瞬态掩码估计，利用其更强的语义一致性和对噪声的鲁棒性，然后逐步发展到高分辨率监督，以实现更精确的掩码预测。第三，我们将延迟高斯增长策略和掩码自举与外观建模相结合，以处理包括瞬态和光照在内的野外场景。在多个具有挑战性的数据集上进行的大量实验表明，我们的方法优于现有方法，清楚地证明了我们方法的鲁棒性和有效性。",
            "intro_zh": [
                "现有3DGS方法在处理野外场景时，易受瞬态物体和光照变化的影响，导致渲染结果出现伪影。",
                "RobustSplat++通过延迟高斯增长、尺度级联掩码自举和外观建模，解耦稠密化、动态和光照，提升鲁棒性。",
                "实验表明，RobustSplat++在多个数据集上超越现有方法，证明了其在野外场景建模方面的有效性。"
            ],
            "method_zh": "**问题定义**：现有3DGS方法在处理真实世界的复杂场景时，容易受到瞬态物体（如移动的车辆、行人）和光照变化的影响。这些因素会导致高斯分布过度拟合这些干扰，从而在渲染结果中产生不希望的伪影，降低了新视角合成的质量。现有方法缺乏对这些动态因素的有效建模和分离能力。\\n\\n**核心思路**：RobustSplat++的核心思路是将场景的静态结构、动态物体和光照变化解耦。通过延迟高斯增长，优先优化静态场景的几何结构，避免在早期优化阶段过度拟合瞬态物体。利用尺度级联的掩码自举方法，逐步精确地识别和分割瞬态物体。结合外观建模，更好地处理光照变化带来的影响。\\n\\n**技术框架**：RobustSplat++的整体框架包含以下几个主要阶段：1) 延迟高斯增长：在初始阶段，限制高斯分布的增长，专注于优化静态场景结构。2) 尺度级联掩码自举：从低分辨率到高分辨率，逐步优化瞬态物体的掩码。3) 外观建模：结合光照信息，优化高斯分布的外观参数。4) 渲染：使用优化后的高斯分布进行新视角合成。\\n\\n**关键创新**：RobustSplat++的关键创新在于其解耦的思想和具体实现。延迟高斯增长策略避免了对瞬态物体的过度拟合，尺度级联掩码自举方法提高了瞬态物体分割的鲁棒性和精度。这种解耦方法使得模型能够更好地适应野外场景的复杂性和动态性。\\n\\n**关键设计**：延迟高斯增长策略中，可以设置一个阈值，控制高斯分布增长的起始时间。尺度级联掩码自举方法中，可以使用不同的损失函数（如交叉熵损失）来监督掩码的预测。外观建模可以采用球谐光照模型或其他光照模型来表示光照变化。具体参数设置需要根据数据集和场景特点进行调整。",
            "application_zh": "RobustSplat++在自动驾驶、机器人导航、增强现实等领域具有广泛的应用前景。它可以用于构建更鲁棒、更准确的3D场景模型，提高这些应用在复杂环境下的性能和可靠性。例如，在自动驾驶中，可以利用RobustSplat++构建周围环境的3D地图，并准确识别和跟踪动态物体，从而提高驾驶安全性。",
            "highlight_zh": "RobustSplat++在多个具有挑战性的数据集上进行了评估，实验结果表明，该方法在渲染质量和鲁棒性方面均优于现有方法。具体而言，RobustSplat++在处理包含瞬态物体和光照变化的场景时，能够显著减少伪影，提高新视角合成的真实感。定量指标方面，RobustSplat++在PSNR、SSIM等指标上均取得了显著提升。",
            "tags_zh": [
                "3D高斯溅射",
                "新视角合成",
                "野外场景建模",
                "瞬态物体",
                "光照变化",
                "鲁棒性",
                "延迟高斯增长",
                "尺度级联掩码"
            ],
            "_index": 77,
            "_used_api": "gemini"
        },
        {
            "title": "TEMPO-VINE: A Multi-Temporal Sensor Fusion Dataset for Localization and Mapping in Vineyards",
            "authors": [
                "Mauro Martini",
                "Marco Ambrosio",
                "Judith Vilella-Cantos",
                "Alessandro Navone",
                "Marcello Chiaberge"
            ],
            "arxiv_id": "2512.04772v1",
            "summary": "In recent years, precision agriculture has been introducing groundbreaking innovations in the field, with a strong focus on automation. However, research studies in robotics and autonomous navigation often rely on controlled simulations or isolated field trials. The absence of a realistic common benchmark represents a significant limitation for the diffusion of robust autonomous systems under real complex agricultural conditions. Vineyards pose significant challenges due to their dynamic nature, and they are increasingly drawing attention from both academic and industrial stakeholders interested in automation. In this context, we introduce the TEMPO-VINE dataset, a large-scale multi-temporal dataset specifically designed for evaluating sensor fusion, simultaneous localization and mapping (SLAM), and place recognition techniques within operational vineyard environments. TEMPO-VINE is the first multi-modal public dataset that brings together data from heterogeneous LiDARs of different price levels, AHRS, RTK-GPS, and cameras in real trellis and pergola vineyards, with multiple rows exceeding 100 m in length. In this work, we address a critical gap in the landscape of agricultural datasets by providing researchers with a comprehensive data collection and ground truth trajectories in different seasons, vegetation growth stages, terrain and weather conditions. The sequence paths with multiple runs and revisits will foster the development of sensor fusion, localization, mapping and place recognition solutions for agricultural fields. The dataset, the processing tools and the benchmarking results will be available at the dedicated webpage upon acceptance.",
            "categories": [
                "cs.RO",
                "eess.SY"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04772v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "SLAM",
                        "[T]localization",
                        "navigation"
                    ],
                    "score": 10.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "TEMPO-VINE：用于葡萄园定位与建图的多时序传感器融合数据集",
            "summary_zh": "近年来，精准农业正经历着以自动化为核心的突破性创新。然而，机器人和自主导航领域的研究通常依赖于受控仿真或孤立的现场试验。缺乏现实的通用基准是稳健的自主系统在复杂农业环境中普及的重要限制。葡萄园因其动态特性而带来重大挑战，并日益受到学术界和工业界对自动化感兴趣的利益相关者的关注。在此背景下，我们推出了TEMPO-VINE数据集，这是一个大规模多时序数据集，专门用于评估传感器融合、同步定位与建图（SLAM）以及在实际葡萄园环境中进行位置识别的技术。TEMPO-VINE是第一个多模态公共数据集，它汇集了来自不同价格水平的异构激光雷达、AHRS、RTK-GPS和相机的数据，这些数据来自真实的棚架和凉棚葡萄园，其中多个行的长度超过100米。在这项工作中，我们通过为研究人员提供全面的数据收集和不同季节、植被生长阶段、地形和天气条件下的地面实况轨迹，解决了农业数据集领域的一个关键空白。具有多次运行和重新访问的序列路径将促进农业领域传感器融合、定位、建图和位置识别解决方案的开发。数据集、处理工具和基准测试结果将在接受后在专用网页上提供。",
            "intro_zh": [
                "现有农业机器人研究缺乏在真实、复杂葡萄园环境下的通用benchmark，限制了自主系统的发展。",
                "TEMPO-VINE数据集提供多模态传感器数据，包含不同季节、植被状态和环境条件下的葡萄园场景。",
                "该数据集旨在促进传感器融合、SLAM和位置识别算法在农业机器人领域的应用和性能评估。"
            ],
            "method_zh": "**问题定义**：论文旨在解决农业机器人研究中缺乏真实葡萄园环境数据集的问题。现有的研究通常依赖于仿真或孤立的实验，难以评估算法在实际复杂环境下的性能，阻碍了自主农业系统的发展。葡萄园环境的动态性和复杂性（如季节变化、植被生长）进一步加剧了这一问题。\\n\\n**核心思路**：论文的核心思路是构建一个大规模、多模态、多时序的葡萄园数据集，包含多种传感器数据（LiDAR、相机、IMU、GPS）和不同环境条件下的数据。通过提供真实场景的数据和精确的地面实况，为研究人员提供一个通用的benchmark，促进相关算法的开发和评估。\\n\\n**技术框架**：TEMPO-VINE数据集的构建流程主要包括以下几个阶段：1) 数据采集：使用多种传感器（包括不同价位的LiDAR、AHRS、RTK-GPS和相机）在真实的棚架和凉棚葡萄园中采集数据，覆盖不同的季节、植被生长阶段、地形和天气条件。2) 数据处理：对采集到的数据进行预处理，包括传感器校准、数据同步和噪声滤除等。3) 地面实况生成：使用RTK-GPS等高精度定位设备生成精确的地面实况轨迹。4) 数据集发布：将处理后的数据和地面实况以标准格式发布，并提供相应的处理工具和基准测试结果。\\n\\n**关键创新**：TEMPO-VINE数据集的主要创新点在于：1) 它是第一个专门针对葡萄园环境的大规模多模态数据集，填补了农业机器人数据集领域的空白。2) 它包含了多时序数据，覆盖了不同的季节、植被生长阶段和环境条件，能够更好地模拟真实葡萄园环境的动态性。3) 它提供了多种传感器数据，包括不同价位的LiDAR，能够满足不同研究需求。\\n\\n**关键设计**：数据集的关键设计包括：1) 选择具有代表性的葡萄园场景，包括棚架和凉棚两种结构。2) 采集不同季节和植被生长阶段的数据，以模拟葡萄园的动态变化。3) 使用多种传感器进行数据采集，以提供多模态信息。4) 使用高精度RTK-GPS生成精确的地面实况轨迹。5) 提供数据处理工具和基准测试结果，方便研究人员使用和评估数据集。",
            "application_zh": "TEMPO-VINE数据集可广泛应用于农业机器人、精准农业、自主导航等领域。该数据集能够促进葡萄园自主导航、作物监测、产量预测等应用的发展。通过在该数据集上进行算法开发和评估，可以提高农业机器人在复杂环境下的适应性和鲁棒性，从而提高农业生产效率和降低成本。未来，该数据集可以扩展到其他类型的农田环境，为更广泛的农业机器人研究提供支持。",
            "highlight_zh": "TEMPO-VINE数据集是首个针对葡萄园环境的大规模多模态数据集，包含多种传感器数据和不同环境条件下的数据。该数据集的发布将为农业机器人研究提供一个通用的benchmark，促进传感器融合、SLAM和位置识别算法在农业领域的应用和性能评估。通过在该数据集上进行实验，研究人员可以评估算法在真实葡萄园环境下的性能，并与其他算法进行比较，从而推动相关技术的发展。",
            "tags_zh": [
                "葡萄园",
                "数据集",
                "多模态传感器融合",
                "SLAM",
                "农业机器人",
                "精准农业",
                "自主导航",
                "定位与建图"
            ],
            "_index": 78,
            "_used_api": "gemini"
        },
        {
            "title": "Gaussian Entropy Fields: Driving Adaptive Sparsity in 3D Gaussian Optimization",
            "authors": [
                "Hong Kuang",
                "Jianchen Liu"
            ],
            "arxiv_id": "2512.04542v1",
            "summary": "3D Gaussian Splatting (3DGS) has emerged as a leading technique for novel view synthesis, demonstrating exceptional rendering efficiency. \\replaced[]{Well-reconstructed surfaces can be characterized by low configurational entropy, where dominant primitives clearly define surface geometry while redundant components are suppressed.}{The key insight is that well-reconstructed surfaces naturally exhibit low configurational entropy, where dominant primitives clearly define surface geometry while suppressing redundant components.} Three complementary technical contributions are introduced: (1) entropy-driven surface modeling via entropy minimization for low configurational entropy in primitive distributions; (2) adaptive spatial regularization using the Surface Neighborhood Redundancy Index (SNRI) and image entropy-guided weighting; (3) multi-scale geometric preservation through competitive cross-scale entropy alignment. Extensive experiments demonstrate that GEF achieves competitive geometric precision on DTU and T\\&T benchmarks, while delivering superior rendering quality compared to existing methods on Mip-NeRF 360. Notably, superior Chamfer Distance (0.64) on DTU and F1 score (0.44) on T\\&T are obtained, alongside the best SSIM (0.855) and LPIPS (0.136) among baselines on Mip-NeRF 360, validating the framework's ability to enhance surface reconstruction accuracy without compromising photometric fidelity.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "28 pages,11 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04542v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "3DGS",
                        "gaussian splatting",
                        "NeRF",
                        "novel view synthesis"
                    ],
                    "score": 10.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出高斯熵场以驱动3D高斯优化中的自适应稀疏性",
            "summary_zh": "3D高斯点云（3DGS）已成为新视图合成的领先技术，展现出卓越的渲染效率。研究表明，良好重建的表面自然具有低构型熵，主导原语清晰定义表面几何形状，同时抑制冗余成分。本文提出三项互补的技术贡献：通过熵最小化进行熵驱动的表面建模；利用表面邻域冗余指数（SNRI）和图像熵引导的加权进行自适应空间正则化；通过竞争的跨尺度熵对齐实现多尺度几何保留。实验表明，GEF在DTU和T&T基准上实现了竞争性的几何精度，同时在Mip-NeRF 360上提供了优越的渲染质量。",
            "intro_zh": [
                "现有的3D高斯点云方法在表面重建精度和渲染质量上存在不足，尤其是在处理复杂场景时。",
                "本文提出了一种基于熵最小化的表面建模方法，通过自适应稀疏性来优化3D高斯分布，提升重建效果。",
                "实验结果显示，GEF在DTU和T&T基准上取得了优异的Chamfer距离和F1分数，同时在Mip-NeRF 360上实现了最佳的SSIM和LPIPS值。"
            ],
            "method_zh": "**问题定义**：本文旨在解决现有3D高斯点云方法在表面重建时的精度不足和冗余成分问题，尤其是在复杂场景下的表现不佳。\\n\\n**核心思路**：论文的核心思路是通过熵最小化实现低构型熵的表面建模，抑制冗余成分，从而提高表面重建的精度和渲染质量。\\n\\n**技术框架**：整体架构包括三个主要模块：熵驱动的表面建模、基于SNRI的自适应空间正则化和跨尺度熵对齐。每个模块协同工作，以优化3D高斯分布。\\n\\n**关键创新**：最重要的技术创新在于引入了熵最小化作为表面建模的驱动力，显著改善了表面几何的定义与重建精度，与现有方法相比，能够更有效地抑制冗余信息。\\n\\n**关键设计**：在设计中，采用了熵最小化损失函数，结合SNRI进行空间正则化，并通过竞争的跨尺度熵对齐来保持几何特征的多尺度一致性。",
            "application_zh": "该研究的潜在应用领域包括计算机视觉、虚拟现实和增强现实等场景，能够有效提升3D重建和渲染的质量，具有广泛的实际价值和未来影响，尤其是在复杂场景的建模与渲染中。",
            "highlight_zh": "实验结果显示，GEF在DTU数据集上获得了0.64的Chamfer距离和0.44的F1分数，在T&T基准上表现优异。此外，在Mip-NeRF 360上，GEF实现了最佳的SSIM（0.855）和LPIPS（0.136），验证了其在表面重建精度和光度保真度方面的优势。",
            "tags_zh": [
                "3D重建",
                "高斯优化",
                "熵最小化",
                "自适应稀疏性",
                "计算机视觉",
                "虚拟现实",
                "渲染质量"
            ],
            "_index": 79,
            "_used_api": "openai"
        },
        {
            "title": "C3G: Learning Compact 3D Representations with 2K Gaussians",
            "authors": [
                "Honggyu An",
                "Jaewoo Jung",
                "Mungyeom Kim",
                "Sunghwan Hong",
                "Chaehyun Kim",
                "Kazumi Fukuda",
                "Minkyeong Jeon",
                "Jisang Han",
                "Takuya Narihira",
                "Hyuna Ko",
                "Junsu Kim",
                "Yuki Mitsufuji",
                "Seungryong Kim"
            ],
            "arxiv_id": "2512.04021v1",
            "summary": "Reconstructing and understanding 3D scenes from unposed sparse views in a feed-forward manner remains as a challenging task in 3D computer vision. Recent approaches use per-pixel 3D Gaussian Splatting for reconstruction, followed by a 2D-to-3D feature lifting stage for scene understanding. However, they generate excessive redundant Gaussians, causing high memory overhead and sub-optimal multi-view feature aggregation, leading to degraded novel view synthesis and scene understanding performance. We propose C3G, a novel feed-forward framework that estimates compact 3D Gaussians only at essential spatial locations, minimizing redundancy while enabling effective feature lifting. We introduce learnable tokens that aggregate multi-view features through self-attention to guide Gaussian generation, ensuring each Gaussian integrates relevant visual features across views. We then exploit the learned attention patterns for Gaussian decoding to efficiently lift features. Extensive experiments on pose-free novel view synthesis, 3D open-vocabulary segmentation, and view-invariant feature aggregation demonstrate our approach's effectiveness. Results show that a compact yet geometrically meaningful representation is sufficient for high-quality scene reconstruction and understanding, achieving superior memory efficiency and feature fidelity compared to existing methods.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "Project Page : https://cvlab-kaist.github.io/C3G/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04021v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "gaussian splatting",
                        "novel view synthesis",
                        "scene reconstruction",
                        "scene understanding"
                    ],
                    "score": 10.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "C3G：使用2K高斯学习紧凑的3D表示，提升场景重建与理解",
            "summary_zh": "本文提出了一种名为C3G的新型前馈框架，用于从无姿态的稀疏视图中重建和理解3D场景。现有方法通常采用基于像素的3D高斯溅射进行重建，然后进行2D到3D的特征提升以进行场景理解。然而，这些方法会生成过多的冗余高斯分布，导致高内存开销和次优的多视图特征聚合，从而降低新视角合成和场景理解的性能。C3G仅在必要的空间位置估计紧凑的3D高斯分布，最大限度地减少冗余，同时实现有效的特征提升。C3G引入可学习的tokens，通过自注意力聚合多视图特征来指导高斯分布的生成，确保每个高斯分布整合来自多个视图的相关视觉特征。然后，利用学习到的注意力模式进行高斯解码，以高效地提升特征。大量的实验表明，C3G在无姿态新视角合成、3D开放词汇分割和视角不变特征聚合方面具有有效性。结果表明，紧凑但具有几何意义的表示足以实现高质量的场景重建和理解，与现有方法相比，实现了卓越的内存效率和特征保真度。",
            "intro_zh": [
                "现有方法在3D场景重建中生成大量冗余高斯分布，导致内存开销大，多视图特征聚合效果差。",
                "C3G通过学习tokens聚合多视图特征，指导高斯分布生成，仅在关键位置估计紧凑的3D高斯分布。",
                "实验表明，C3G在内存效率和特征保真度方面优于现有方法，提升了新视角合成和场景理解性能。"
            ],
            "method_zh": "**问题定义**：现有方法在从稀疏视图重建3D场景时，会生成大量冗余的3D高斯分布，导致内存占用过高，并且影响多视图特征的有效聚合，最终降低了新视角合成和场景理解的性能。这些冗余的高斯分布并没有提供额外的几何信息，反而增加了计算负担。\\n\\n**核心思路**：C3G的核心思路是学习一种紧凑的3D表示，只在必要的空间位置生成高斯分布，从而减少冗余。通过引入可学习的tokens，利用自注意力机制聚合多视图特征，指导高斯分布的生成，确保每个高斯分布都包含来自多个视角的相关视觉信息。这样既能减少内存占用，又能提高特征的表达能力。\\n\\n**技术框架**：C3G框架主要包含以下几个阶段：1) **多视图特征提取**：从多个视角的图像中提取特征。2) **Token生成与特征聚合**：生成可学习的tokens，并使用自注意力机制聚合多视图特征，为每个token赋予来自不同视角的视觉信息。3) **高斯生成**：基于聚合后的token特征，生成3D高斯分布，这些高斯分布位于场景的关键位置。4) **特征提升**：利用学习到的注意力模式，将2D特征提升到3D空间，用于后续的场景理解任务。\\n\\n**关键创新**：C3G的关键创新在于：1) **紧凑的高斯表示**：只在必要的空间位置生成高斯分布，减少了冗余，提高了内存效率。2) **基于Token的多视图特征聚合**：通过可学习的tokens和自注意力机制，有效地聚合了来自多个视角的特征，提高了特征的表达能力。3) **基于注意力模式的特征提升**：利用学习到的注意力模式，高效地将2D特征提升到3D空间。\\n\\n**关键设计**：C3G的关键设计包括：1) **Token数量**：选择合适的token数量，以平衡表示能力和计算成本。2) **自注意力机制**：使用多头自注意力机制，以捕捉不同视角之间的复杂关系。3) **损失函数**：设计合适的损失函数，以优化高斯分布的位置、形状和颜色，并鼓励生成紧凑的表示。具体的损失函数可能包括重建损失、正则化损失等。4) **高斯分布参数化**：使用合适的参数化方法来表示3D高斯分布，例如位置、旋转、缩放和颜色。",
            "application_zh": "C3G在机器人导航、自动驾驶、虚拟现实和增强现实等领域具有广泛的应用前景。它可以用于从稀疏的传感器数据中重建高质量的3D场景，并进行场景理解，从而帮助机器人更好地感知和理解周围环境。此外，C3G还可以用于生成逼真的虚拟场景，为用户提供沉浸式的体验。",
            "highlight_zh": "C3G在多个任务上取得了显著的性能提升。在无姿态新视角合成任务中，C3G在保持高视觉质量的同时，显著降低了内存占用。在3D开放词汇分割任务中，C3G的性能优于现有方法，表明其学习到的3D表示具有更好的语义信息。此外，C3G在视角不变特征聚合方面也表现出色，证明了其能够有效地整合来自不同视角的特征。",
            "tags_zh": [
                "3D场景重建",
                "高斯溅射",
                "多视图学习",
                "自注意力机制",
                "特征提升"
            ],
            "_index": 80,
            "_used_api": "gemini"
        },
        {
            "title": "Cross-embodied Co-design for Dexterous Hands",
            "authors": [
                "Kehlani Fay",
                "Darin Anthony Djapri",
                "Anya Zorin",
                "James Clinton",
                "Ali El Lahib",
                "Hao Su",
                "Michael T. Tolley",
                "Sha Yi",
                "Xiaolong Wang"
            ],
            "arxiv_id": "2512.03743v1",
            "summary": "Dexterous manipulation is limited by both control and design, without consensus as to what makes manipulators best for performing dexterous tasks. This raises a fundamental challenge: how should we design and control robot manipulators that are optimized for dexterity? We present a co-design framework that learns task-specific hand morphology and complementary dexterous control policies. The framework supports 1) an expansive morphology search space including joint, finger, and palm generation, 2) scalable evaluation across the wide design space via morphology-conditioned cross-embodied control, and 3) real-world fabrication with accessible components. We evaluate the approach across multiple dexterous tasks, including in-hand rotation with simulation and real deployment. Our framework enables an end-to-end pipeline that can design, train, fabricate, and deploy a new robotic hand in under 24 hours. The full framework will be open-sourced and available on our website.",
            "categories": [
                "cs.RO",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03743v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "[T]dexterous hand",
                        "dexterous manipulation"
                    ],
                    "score": 10.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出一种跨具身协同设计框架，用于灵巧手形态与控制策略的联合优化",
            "summary_zh": "灵巧操作受到控制和设计的双重限制，目前对于何种机械手最适合执行灵巧任务尚未达成共识。这带来了一个根本性的挑战：我们应该如何设计和控制针对灵巧性优化的机器人机械手？本文提出了一种协同设计框架，该框架学习特定任务的手部形态和互补的灵巧控制策略。该框架支持1) 包含关节、手指和手掌生成的广泛形态搜索空间；2) 通过形态条件跨具身控制在广泛设计空间内进行可扩展的评估；3) 使用易于获得的组件进行真实世界的制造。我们在多个灵巧任务中评估了该方法，包括在模拟和真实部署中的手内旋转。我们的框架支持端到端的流程，可以在24小时内设计、训练、制造和部署新的机器人手。完整的框架将开源并在我们的网站上提供。",
            "intro_zh": [
                "灵巧操作的性能瓶颈在于机械手的设计和控制策略，现有方法难以同时优化两者。",
                "论文提出跨具身协同设计框架，联合优化机械手的形态和控制策略，实现任务特定的灵巧操作。",
                "该框架支持快速设计、训练、制造和部署新的机器人手，并在手内旋转等任务上验证了有效性。"
            ],
            "method_zh": "**问题定义**：现有灵巧手设计和控制方法通常是分离的，难以找到最优的形态和控制策略组合。缺乏一个能够同时搜索形态空间和学习控制策略的框架，导致设计周期长，性能提升有限。此外，将仿真结果迁移到真实世界也存在挑战。\\n\\n**核心思路**：本文的核心思路是采用协同设计的方法，同时优化机械手的形态和控制策略。通过形态条件跨具身控制，实现在不同形态的机械手上进行策略迁移和评估，从而加速设计空间的探索。这种方法允许在仿真环境中快速评估大量不同的机械手设计，并选择最适合特定任务的设计。\\n\\n**技术框架**：该框架包含三个主要部分：1) 机械手形态生成器，用于生成包含关节、手指和手掌等不同参数的机械手设计；2) 形态条件跨具身控制模块，用于在不同的机械手形态上训练和评估控制策略；3) 真实世界制造模块，用于将选定的机械手设计转化为物理实体。整个流程是端到端的，可以实现从设计到部署的快速迭代。\\n\\n**关键创新**：该论文的关键创新在于提出了形态条件跨具身控制的概念，允许在不同的机械手形态之间共享控制策略。这使得可以在一个形态上训练的策略迁移到另一个形态上，从而加速了控制策略的学习和评估。此外，该框架还集成了形态生成、控制学习和真实世界制造，形成了一个完整的协同设计流程。\\n\\n**关键设计**：形态生成器采用参数化的方式描述机械手的形态，包括关节数量、手指长度、手掌大小等。形态条件跨具身控制模块使用深度神经网络来学习控制策略，网络的输入包括机械手的状态和目标任务。损失函数的设计考虑了任务的完成度和控制的平滑性。真实世界制造模块使用3D打印技术和现成的电子元件来快速制造机械手。",
            "application_zh": "该研究成果可应用于各种需要灵巧操作的场景，如工业自动化、医疗机器人、家庭服务机器人等。通过自动优化机械手的形态和控制策略，可以提高机器人在复杂环境中的操作能力和适应性。此外，该框架的快速设计和制造能力可以缩短机器人手的开发周期，降低成本。",
            "highlight_zh": "该框架能够在24小时内完成机器人手的端到端设计、训练、制造和部署。在手内旋转任务中，通过协同设计优化后的机械手在仿真和真实环境中均表现出良好的性能。实验结果表明，该框架能够有效地搜索形态空间，找到适合特定任务的机械手设计。",
            "tags_zh": [
                "灵巧手",
                "协同设计",
                "跨具身控制",
                "形态生成",
                "机器人控制"
            ],
            "_index": 81,
            "_used_api": "gemini"
        },
        {
            "title": "Motion4D: Learning 3D-Consistent Motion and Semantics for 4D Scene Understanding",
            "authors": [
                "Haoran Zhou",
                "Gim Hee Lee"
            ],
            "arxiv_id": "2512.03601v1",
            "summary": "Recent advancements in foundation models for 2D vision have substantially improved the analysis of dynamic scenes from monocular videos. However, despite their strong generalization capabilities, these models often lack 3D consistency, a fundamental requirement for understanding scene geometry and motion, thereby causing severe spatial misalignment and temporal flickering in complex 3D environments. In this paper, we present Motion4D, a novel framework that addresses these challenges by integrating 2D priors from foundation models into a unified 4D Gaussian Splatting representation. Our method features a two-part iterative optimization framework: 1) Sequential optimization, which updates motion and semantic fields in consecutive stages to maintain local consistency, and 2) Global optimization, which jointly refines all attributes for long-term coherence. To enhance motion accuracy, we introduce a 3D confidence map that dynamically adjusts the motion priors, and an adaptive resampling process that inserts new Gaussians into under-represented regions based on per-pixel RGB and semantic errors. Furthermore, we enhance semantic coherence through an iterative refinement process that resolves semantic inconsistencies by alternately optimizing the semantic fields and updating prompts of SAM2. Extensive evaluations demonstrate that our Motion4D significantly outperforms both 2D foundation models and existing 3D-based approaches across diverse scene understanding tasks, including point-based tracking, video object segmentation, and novel view synthesis. Our code is available at https://hrzhou2.github.io/motion4d-web/.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "Accepted to NeurIPS 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03601v1",
            "code_links": [
                {
                    "url": "https://hrzhou2.github.io/motion4d-web/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "gaussian splatting",
                        "novel view synthesis",
                        "[T]scene understanding"
                    ],
                    "score": 10.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "Motion4D：学习3D一致的运动和语义信息，用于4D场景理解",
            "summary_zh": "本文提出Motion4D，一个新颖的框架，旨在解决单目视频动态场景分析中，现有2D基础模型缺乏3D一致性的问题。Motion4D将2D基础模型的先验知识整合到统一的4D高斯溅射表示中。该方法包含一个两阶段迭代优化框架：1) 顺序优化，分阶段更新运动和语义场，以保持局部一致性；2) 全局优化，联合优化所有属性，以实现长期连贯性。为了提高运动精度，引入了3D置信度图，动态调整运动先验，并采用自适应重采样过程，基于像素RGB和语义误差，在欠表示区域插入新的高斯分布。此外，通过迭代优化语义场和更新SAM的提示，增强语义连贯性。大量实验表明，Motion4D在基于点的跟踪、视频对象分割和新视角合成等多种场景理解任务中，显著优于2D基础模型和现有3D方法。",
            "intro_zh": [
                "现有2D视觉基础模型在动态场景分析中表现出色，但缺乏3D一致性，导致空间错位和时间闪烁。",
                "Motion4D将2D先验知识融入4D高斯溅射表示，通过顺序和全局优化，实现3D一致的运动和语义理解。",
                "实验表明，Motion4D在点云跟踪、视频分割和新视角合成等任务中，显著优于现有2D和3D方法。"
            ],
            "method_zh": "**问题定义**：现有方法，特别是基于2D视觉基础模型的方法，在处理单目视频的动态场景理解时，虽然具有很强的泛化能力，但缺乏3D一致性。这导致在复杂的3D环境中出现严重的几何错位和时间上的闪烁现象，限制了其在需要精确3D信息的任务中的应用。因此，需要一种能够保证3D一致性的动态场景理解方法。\\n\\n**核心思路**：Motion4D的核心思路是将2D视觉基础模型的强大先验知识与3D场景表示相结合，利用高斯溅射（Gaussian Splatting）作为统一的4D表示，并通过迭代优化框架来保证运动和语义的3D一致性。通过这种方式，可以有效地利用2D模型的优势，同时克服其在3D空间中的不足。\\n\\n**技术框架**：Motion4D的整体框架包含两个主要的迭代优化阶段：顺序优化和全局优化。顺序优化首先更新运动场，然后更新语义场，以保持局部一致性。全局优化则联合优化所有属性，以实现长期连贯性。此外，该框架还包括一个3D置信度图，用于动态调整运动先验，以及一个自适应重采样过程，用于在欠表示区域插入新的高斯分布。\\n\\n**关键创新**：Motion4D的关键创新在于其将2D视觉基础模型的先验知识有效地融入到3D场景表示中，并设计了一个两阶段的迭代优化框架，以保证运动和语义的3D一致性。此外，3D置信度图和自适应重采样过程进一步提高了运动精度和场景表示的完整性。与现有方法相比，Motion4D能够更好地处理复杂的3D动态场景，并提供更精确的场景理解。\\n\\n**关键设计**：3D置信度图的设计用于动态调整运动先验，其具体实现方式未知。自适应重采样过程基于像素RGB和语义误差来确定需要插入新高斯分布的区域，具体的误差计算方式和阈值设置未知。语义一致性通过迭代优化语义场和更新SAM的提示来实现，具体的提示更新策略未知。损失函数的设计细节未知。",
            "application_zh": "Motion4D的研究成果可应用于自动驾驶、机器人导航、增强现实等领域。通过提供3D一致的动态场景理解，可以提升自动驾驶系统的环境感知能力，增强机器人在复杂环境中的导航能力，并为AR应用提供更逼真的场景交互体验。该研究还有助于推动虚拟现实、游戏开发等领域的发展。",
            "highlight_zh": "Motion4D在多个场景理解任务中表现出色。在点云跟踪、视频对象分割和新视角合成任务中，Motion4D显著优于现有的2D基础模型和3D方法。具体的性能提升数据未知，但摘要强调了其在多种任务中的优越性，表明了该方法具有很强的泛化能力和实用价值。",
            "tags_zh": [
                "4D场景理解",
                "动态场景分析",
                "3D一致性",
                "高斯溅射",
                "运动估计",
                "语义分割",
                "单目视频",
                "基础模型"
            ],
            "_index": 82,
            "_used_api": "gemini"
        },
        {
            "title": "Navigation Around Unknown Space Objects Using Visible-Thermal Image Fusion",
            "authors": [
                "Eric J. Elias",
                "Michael Esswein",
                "Jonathan P. How",
                "David W. Miller"
            ],
            "arxiv_id": "2512.12203v1",
            "summary": "As the popularity of on-orbit operations grows, so does the need for precise navigation around unknown resident space objects (RSOs) such as other spacecraft, orbital debris, and asteroids. The use of Simultaneous Localization and Mapping (SLAM) algorithms is often studied as a method to map out the surface of an RSO and find the inspector's relative pose using a lidar or conventional camera. However, conventional cameras struggle during eclipse or shadowed periods, and lidar, though robust to lighting conditions, tends to be heavier, bulkier, and more power-intensive. Thermal-infrared cameras can track the target RSO throughout difficult illumination conditions without these limitations. While useful, thermal-infrared imagery lacks the resolution and feature-richness of visible cameras. In this work, images of a target satellite in low Earth orbit are photo-realistically simulated in both visible and thermal-infrared bands. Pixel-level fusion methods are used to create visible/thermal-infrared composites that leverage the best aspects of each camera. Navigation errors from a monocular SLAM algorithm are compared between visible, thermal-infrared, and fused imagery in various lighting and trajectories. Fused imagery yields substantially improved navigation performance over visible-only and thermal-only methods.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-13",
            "updated": "2025-12-13",
            "comment": "18 pages, 11 figures. To be published in proceedings of AIAA SCITECH 2026 Forum",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12203v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "SLAM",
                        "localization",
                        "[T]navigation"
                    ],
                    "score": 10.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出可见光-热红外图像融合方法，提升未知空间物体导航精度",
            "summary_zh": "随着在轨操作的日益普及，对未知空间物体（RSO）进行精确导航的需求也日益增长，这些物体包括其他航天器、轨道碎片和小行星。同时定位与地图构建（SLAM）算法常被用于绘制RSO表面地图，并利用激光雷达或传统相机确定探测器的相对姿态。然而，传统相机在日食或阴影期间表现不佳，而激光雷达虽然对光照条件不敏感，但往往更重、更大、更耗电。热红外相机可以在困难的光照条件下跟踪目标RSO，且没有这些限制。虽然有用，但热红外图像缺乏可见光相机所具有的分辨率和丰富的特征。本文对近地轨道目标卫星的可见光和热红外图像进行了逼真的模拟。采用像素级融合方法创建可见光/热红外复合图像，充分利用了每种相机的优点。在不同的光照和轨迹下，比较了单目SLAM算法在可见光、热红外和融合图像上的导航误差。结果表明，融合图像的导航性能明显优于仅使用可见光和仅使用热红外的方法。",
            "intro_zh": [
                "传统相机在阴影下失效，激光雷达体积大功耗高，限制了空间物体导航。",
                "提出可见光与热红外图像融合方法，结合二者优势，提升导航系统在复杂光照下的鲁棒性。",
                "实验表明，融合图像显著提升了单目SLAM算法的导航精度，优于单独使用可见光或热红外图像。"
            ],
            "method_zh": "**问题定义**：论文旨在解决在复杂光照条件下，传统可见光相机和激光雷达在未知空间物体（RSO）导航中存在的局限性问题。可见光相机在阴影或日食期间性能下降，而激光雷达则存在体积、重量和功耗方面的限制。现有方法难以在各种光照条件下实现精确可靠的导航。\\n\\n**核心思路**：论文的核心思路是将可见光图像和热红外图像进行融合，利用可见光图像的高分辨率和特征丰富性，以及热红外图像对光照不敏感的特性。通过融合两种模态的信息，可以克服单一传感器的局限性，提高导航系统在各种光照条件下的鲁棒性和精度。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1) 使用仿真软件生成目标卫星的可见光和热红外图像；2) 对可见光和热红外图像进行像素级融合，生成复合图像；3) 使用单目SLAM算法对可见光图像、热红外图像和融合图像分别进行导航；4) 比较不同图像的导航误差，评估融合方法的性能。\\n\\n**关键创新**：该论文的关键创新在于提出了一种基于像素级融合的可见光-热红外图像融合方法，用于提升空间物体导航的精度和鲁棒性。与传统方法相比，该方法能够充分利用两种模态的信息，克服单一传感器的局限性，从而在复杂光照条件下实现更精确的导航。\\n\\n**关键设计**：论文中使用了光线追踪软件来生成逼真的可见光和热红外图像。像素级融合方法具体实现细节未知，但其目标是保留两种图像的最佳特征。单目SLAM算法的具体选择未知，但其性能是评估融合方法有效性的关键指标。论文比较了不同光照条件和轨迹下的导航误差，以验证融合方法的鲁棒性。",
            "application_zh": "该研究成果可应用于空间机器人、卫星自主导航、轨道碎片清除等领域。通过提升在复杂光照条件下对未知空间物体的导航精度，可以提高空间任务的安全性、可靠性和效率，为未来的空间探索和资源利用提供技术支持。",
            "highlight_zh": "实验结果表明，在各种光照和轨迹条件下，融合图像的导航性能明显优于仅使用可见光和仅使用热红外的方法。具体性能提升数据未知，但论文强调了融合方法在提高导航精度方面的显著优势，尤其是在光照条件不佳的情况下。",
            "tags_zh": [
                "空间物体导航",
                "可见光图像",
                "热红外图像",
                "图像融合",
                "单目SLAM"
            ],
            "_index": 83,
            "_used_api": "gemini"
        },
        {
            "title": "Goal Reaching with Eikonal-Constrained Hierarchical Quasimetric Reinforcement Learning",
            "authors": [
                "Vittorio Giammarino",
                "Ahmed H. Qureshi"
            ],
            "arxiv_id": "2512.12046v1",
            "summary": "Goal-Conditioned Reinforcement Learning (GCRL) mitigates the difficulty of reward design by framing tasks as goal reaching rather than maximizing hand-crafted reward signals. In this setting, the optimal goal-conditioned value function naturally forms a quasimetric, motivating Quasimetric RL (QRL), which constrains value learning to quasimetric mappings and enforces local consistency through discrete, trajectory-based constraints. We propose Eikonal-Constrained Quasimetric RL (Eik-QRL), a continuous-time reformulation of QRL based on the Eikonal Partial Differential Equation (PDE). This PDE-based structure makes Eik-QRL trajectory-free, requiring only sampled states and goals, while improving out-of-distribution generalization. We provide theoretical guarantees for Eik-QRL and identify limitations that arise under complex dynamics. To address these challenges, we introduce Eik-Hierarchical QRL (Eik-HiQRL), which integrates Eik-QRL into a hierarchical decomposition. Empirically, Eik-HiQRL achieves state-of-the-art performance in offline goal-conditioned navigation and yields consistent gains over QRL in manipulation tasks, matching temporal-difference methods.",
            "categories": [
                "cs.LG",
                "cs.RO",
                "eess.SY",
                "stat.ML"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12046v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning",
                        "reward design"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "navigation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "提出Eik-HiQRL，结合Eikonal方程与分层强化学习解决复杂环境下的目标导向导航问题",
            "summary_zh": "本文提出了一种基于Eikonal约束的分层拟度量强化学习方法（Eik-HiQRL），旨在解决目标条件强化学习（GCRL）中奖励设计困难的问题。GCRL将任务定义为目标到达，而非最大化手工设计的奖励信号。最优目标条件价值函数自然形成拟度量，促使拟度量强化学习（QRL）将价值学习约束为拟度量映射，并通过离散的、基于轨迹的约束来加强局部一致性。Eik-QRL是QRL的连续时间重构，基于Eikonal偏微分方程（PDE）。这种基于PDE的结构使Eik-QRL无需轨迹，仅需采样的状态和目标，同时提高了分布外泛化能力。论文提供了Eik-QRL的理论保证，并指出了复杂动力学下的局限性。为了解决这些挑战，Eik-HiQRL将Eik-QRL集成到分层分解中。实验结果表明，Eik-HiQRL在离线目标条件导航中实现了最先进的性能，并在操作任务中获得了相对于QRL的一致增益，与时序差分方法相匹配。",
            "intro_zh": [
                "传统强化学习奖励设计困难，目标条件强化学习通过目标到达来简化任务定义，但现有方法在复杂动力学下存在局限性。",
                "Eik-HiQRL将Eikonal方程约束的QRL融入分层结构，利用PDE的连续性提高泛化能力，并通过分层分解处理复杂动力学。",
                "实验表明，Eik-HiQRL在离线导航任务中达到SOTA，并在操作任务中超越QRL，性能与时序差分方法相当。"
            ],
            "method_zh": "**问题定义**：目标条件强化学习（GCRL）旨在通过学习从任意状态到达目标状态的策略来解决奖励函数设计困难的问题。然而，在复杂动力学环境下，传统的QRL方法依赖于轨迹约束，泛化能力受限，难以适应分布外的状态和目标。\\n\\n**核心思路**：本文的核心思路是将QRL方法与Eikonal偏微分方程（PDE）相结合，构建连续时间的价值函数表示，从而摆脱对轨迹的依赖，提高泛化能力。同时，为了处理复杂动力学，引入分层结构，将任务分解为多个子任务，分别学习子策略。\\n\\n**技术框架**：Eik-HiQRL包含两个主要层次：高层策略和低层策略。高层策略负责选择子目标，低层策略负责到达选定的子目标。Eik-QRL作为低层策略的学习算法，利用Eikonal方程约束价值函数的学习，使其满足拟度量性质。整体流程为：首先，高层策略选择一个子目标；然后，低层策略利用Eik-QRL学习到达该子目标的策略；重复以上过程，直到到达最终目标。\\n\\n**关键创新**：主要创新点在于：1) 将Eikonal方程引入QRL，构建了连续时间的价值函数表示，提高了泛化能力；2) 提出了分层结构，将复杂任务分解为多个子任务，降低了学习难度；3) 理论上证明了Eik-QRL的有效性，并分析了其在复杂动力学下的局限性。\\n\\n**关键设计**：Eik-QRL的关键在于Eikonal方程的约束。具体来说，价值函数需要满足以下方程：||∇V(s, g)||=f(s)，其中V(s, g)是从状态s到目标g的价值，f(s)是状态s的成本函数。论文使用神经网络来近似价值函数，并通过最小化Eikonal方程的残差来训练网络。分层结构的关键在于子目标的选择策略，论文采用了一种基于价值函数的子目标选择方法。",
            "application_zh": "该研究成果可应用于机器人导航、自动驾驶、游戏AI等领域。通过学习目标导向的策略，机器人可以在复杂环境中自主导航，完成各种任务。此外，该方法还可以用于训练游戏AI，使其能够更好地理解游戏目标，并制定相应的策略。",
            "highlight_zh": "Eik-HiQRL在离线目标条件导航任务中取得了显著的性能提升，超越了现有的QRL方法，并达到了与时序差分方法相当的水平。具体而言，在多个导航环境中，Eik-HiQRL的成功率和效率均优于QRL，证明了其在复杂环境下的有效性。在操作任务中，Eik-HiQRL也表现出了一致的增益。",
            "tags_zh": [
                "目标条件强化学习",
                "拟度量强化学习",
                "Eikonal方程",
                "分层强化学习",
                "机器人导航",
                "离线强化学习"
            ],
            "_index": 84,
            "_used_api": "gemini"
        },
        {
            "title": "Topology-Agnostic Animal Motion Generation from Text Prompt",
            "authors": [
                "Keyi Chen",
                "Mingze Sun",
                "Zhenyu Liu",
                "Zhangquan Chen",
                "Ruqi Huang"
            ],
            "arxiv_id": "2512.10352v1",
            "summary": "Motion generation is fundamental to computer animation and widely used across entertainment, robotics, and virtual environments. While recent methods achieve impressive results, most rely on fixed skeletal templates, which prevent them from generalizing to skeletons with different or perturbed topologies. We address the core limitation of current motion generation methods - the combined lack of large-scale heterogeneous animal motion data and unified generative frameworks capable of jointly modeling arbitrary skeletal topologies and textual conditions. To this end, we introduce OmniZoo, a large-scale animal motion dataset spanning 140 species and 32,979 sequences, enriched with multimodal annotations. Building on OmniZoo, we propose a generalized autoregressive motion generation framework capable of producing text-driven motions for arbitrary skeletal topologies. Central to our model is a Topology-aware Skeleton Embedding Module that encodes geometric and structural properties of any skeleton into a shared token space, enabling seamless fusion with textual semantics. Given a text prompt and a target skeleton, our method generates temporally coherent, physically plausible, and semantically aligned motions, and further enables cross-species motion style transfer.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "10 pages, 7 figures.Conference submission",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10352v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "text-driven motion",
                        "[T]motion generation"
                    ],
                    "score": 10.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "4_motion_diffusion"
            ],
            "headline_zh": "提出OmniZoo数据集和拓扑无关的动物运动生成框架，解决异构骨骼和文本驱动的动物运动生成问题。",
            "summary_zh": "本文提出了一种拓扑无关的动物运动生成方法，旨在解决现有方法依赖固定骨骼模板，无法泛化到不同或扰动拓扑结构的问题。为此，作者构建了大规模动物运动数据集OmniZoo，包含140个物种和32,979个序列，并进行了多模态标注。基于OmniZoo，作者提出了一个广义的自回归运动生成框架，能够为任意骨骼拓扑生成文本驱动的运动。该模型的关键在于拓扑感知骨骼嵌入模块，它将任何骨骼的几何和结构属性编码到共享的token空间中，从而实现与文本语义的无缝融合。该方法能够生成时间连贯、物理合理且语义对齐的运动，并进一步实现跨物种的运动风格迁移。",
            "intro_zh": [
                "现有运动生成方法依赖固定骨骼模板，难以处理不同或扰动拓扑结构的动物骨骼。",
                "提出拓扑感知的骨骼嵌入模块，将骨骼的几何和结构属性编码到共享空间，融合文本语义。",
                "构建大规模动物运动数据集OmniZoo，包含140个物种和32,979个序列，并进行多模态标注。"
            ],
            "method_zh": "**问题定义**：现有运动生成方法主要依赖于固定的骨骼模板，这限制了它们在处理具有不同或扰动拓扑结构的动物骨骼时的泛化能力。缺乏大规模的、包含异构动物运动数据的数据集，以及能够统一建模任意骨骼拓扑和文本条件的生成框架，是当前方法的主要痛点。\\n\\n**核心思路**：本文的核心思路是构建一个能够感知骨骼拓扑结构的嵌入模块，将不同拓扑结构的骨骼映射到统一的特征空间，从而实现与文本信息的有效融合。通过自回归的方式，逐步生成符合文本描述的、具有时间连贯性和物理合理性的动物运动。这种设计允许模型处理各种动物的骨骼结构，并根据文本提示生成相应的运动。\\n\\n**技术框架**：整体框架包含三个主要模块：1) 拓扑感知骨骼嵌入模块：负责将骨骼的几何和结构信息编码为特征向量。2) 文本编码器：负责将文本提示编码为语义向量。3) 自回归运动生成器：基于骨骼嵌入和文本语义，逐步生成运动序列。该生成器通常采用Transformer架构，能够捕捉运动序列中的时间依赖关系。\\n\\n**关键创新**：最重要的技术创新点在于拓扑感知骨骼嵌入模块。该模块能够学习到骨骼的内在结构，并将其表示为与拓扑结构无关的特征向量。这使得模型能够处理各种不同拓扑结构的骨骼，而无需针对每种骨骼结构进行单独训练。与现有方法相比，该方法具有更强的泛化能力和灵活性。\\n\\n**关键设计**：拓扑感知骨骼嵌入模块可能使用图神经网络（GNN）来编码骨骼的连接关系和关节的几何信息。损失函数通常包括运动预测损失、文本对齐损失和物理合理性损失。运动预测损失用于确保生成的运动与文本描述一致，文本对齐损失用于确保骨骼嵌入与文本语义对齐，物理合理性损失用于确保生成的运动符合物理规律。",
            "application_zh": "该研究成果可广泛应用于计算机动画、游戏开发、机器人控制和虚拟现实等领域。例如，可以根据文本描述自动生成各种动物的运动动画，为游戏角色赋予更逼真的行为，或者控制机器人模仿动物的运动方式。该技术还有潜力应用于生物力学研究，帮助分析和理解动物的运动机制。",
            "highlight_zh": "论文构建了包含140个物种的大规模动物运动数据集OmniZoo。实验结果表明，该方法能够生成时间连贯、物理合理且语义对齐的动物运动。此外，该方法还能够实现跨物种的运动风格迁移，例如让一只猫模仿狗的跑步姿势。具体性能数据和对比基线信息未知。",
            "tags_zh": [
                "动物运动生成",
                "文本驱动运动",
                "拓扑无关",
                "骨骼嵌入",
                "自回归模型"
            ],
            "_index": 85,
            "_used_api": "gemini"
        },
        {
            "title": "Inertial Magnetic SLAM Systems Using Low-Cost Sensors",
            "authors": [
                "Chuan Huang",
                "Gustaf Hendeby",
                "Isaac Skog"
            ],
            "arxiv_id": "2512.10128v1",
            "summary": "Spatially inhomogeneous magnetic fields offer a valuable, non-visual information source for positioning. Among systems leveraging this, magnetic field-based simultaneous localization and mapping (SLAM) systems are particularly attractive because they can provide positioning information and build a magnetic field map on the fly. Moreover, they have bounded error within mapped regions. However, state-of-the-art methods typically require low-drift odometry data provided by visual odometry or a wheel encoder, etc. This is because these systems need to minimize/reduce positioning errors while exploring, which happens when they are in unmapped regions. To address these limitations, this work proposes a loosely coupled and a tightly coupled inertial magnetic SLAM (IM-SLAM) system. The proposed systems use commonly available low-cost sensors: an inertial measurement unit (IMU), a magnetometer array, and a barometer. The use of non-visual data provides a significant advantage over visual-based systems, making it robust to low-visibility conditions. Both systems employ state-space representations, and magnetic field models on different scales. The difference lies in how they use a local and global magnetic field model. The loosely coupled system uses these models separately in two state-space models, while the tightly coupled system integrates them into one state-space model. Experiment results show that the tightly coupled IM-SLAM system achieves lower positioning errors than the loosely coupled system in most scenarios, with typical errors on the order of meters per 100 meters traveled. These results demonstrate the feasiblity of developing a full 3D IM-SLAM systems using low-cost sensors and the potential of applying these systems in emergency response scenarios such as mine/fire rescue.",
            "categories": [
                "cs.RO",
                "eess.SP"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10128v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "visual odometry",
                        "[T]SLAM",
                        "localization"
                    ],
                    "score": 10.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于低成本惯性磁传感器的惯性磁SLAM系统，提升弱光环境定位精度。",
            "summary_zh": "本文提出了一种基于低成本传感器的惯性磁SLAM(IM-SLAM)系统，该系统利用空间非均匀磁场作为定位的非视觉信息源。磁场SLAM系统能够在飞行中提供定位信息并构建磁场地图，且在已映射区域内误差有界。现有方法通常需要视觉里程计或轮式编码器等提供的低漂移里程计数据，以减少探索未映射区域时的定位误差。为了解决这些限制，本文提出了松耦合和紧耦合两种IM-SLAM系统。该系统使用惯性测量单元(IMU)、磁力计阵列和气压计等常用低成本传感器。使用非视觉数据提供了优于视觉系统的显著优势，使其对低能见度条件具有鲁棒性。两种系统都采用状态空间表示和不同尺度的磁场模型。区别在于它们如何使用局部和全局磁场模型。松耦合系统在两个状态空间模型中分别使用这些模型，而紧耦合系统将它们集成到一个状态空间模型中。实验结果表明，在大多数情况下，紧耦合IM-SLAM系统比松耦合系统实现了更低的定位误差，典型误差约为每行进100米误差几米。这些结果证明了开发使用低成本传感器的完整3D IM-SLAM系统的可行性，以及将这些系统应用于矿山/消防救援等应急响应场景的潜力。",
            "intro_zh": [
                "现有磁SLAM系统依赖视觉里程计等提供低漂移数据，限制了其在弱光等环境下的应用。",
                "提出松耦合和紧耦合两种IM-SLAM系统，融合IMU、磁力计和气压计数据，无需视觉信息。",
                "实验表明，紧耦合系统定位精度优于松耦合系统，在典型场景下每100米误差在米级。"
            ],
            "method_zh": "**问题定义**：论文旨在解决在缺乏可靠视觉信息的情况下，如何利用低成本传感器实现高精度的三维定位与建图问题。现有磁SLAM系统依赖视觉里程计或轮式编码器提供低漂移里程计信息，这限制了其在弱光、黑暗或视觉遮挡等环境中的应用。因此，如何在仅使用低成本的惯性传感器和磁力计的情况下，实现鲁棒且精确的SLAM是本研究的核心问题。\\n\\n**核心思路**：论文的核心思路是将惯性测量单元(IMU)和磁力计阵列的数据进行融合，构建一个惯性磁SLAM(IM-SLAM)系统。通过利用IMU提供的高频运动信息和磁力计提供的磁场信息，可以在没有视觉信息的情况下进行定位和建图。论文提出了松耦合和紧耦合两种融合策略，旨在充分利用不同传感器的优势，提高系统的鲁棒性和精度。\\n\\n**技术框架**：该IM-SLAM系统主要包含以下几个模块：1) 传感器数据采集模块：负责采集IMU、磁力计和气压计的数据。2) 预处理模块：对传感器数据进行滤波、校准等预处理操作。3) 状态估计模块：利用扩展卡尔曼滤波(EKF)或类似的状态估计算法，融合传感器数据，估计载体的位姿和速度。4) 地图构建模块：根据估计的位姿和磁场信息，构建磁场地图。松耦合系统采用两个独立的状态空间模型，分别处理局部和全局磁场信息，而紧耦合系统将两者集成到一个状态空间模型中。\\n\\n**关键创新**：论文的关键创新在于提出了基于低成本惯性磁传感器的SLAM系统，摆脱了对视觉信息的依赖，使其能够在弱光等复杂环境中工作。此外，论文还提出了松耦合和紧耦合两种融合策略，并对它们的性能进行了比较分析。紧耦合系统通过将局部和全局磁场信息集成到一个状态空间模型中，实现了更高的定位精度。\\n\\n**关键设计**：论文中，磁场模型的设计至关重要，需要考虑磁场的空间分布特性。状态估计模块通常采用扩展卡尔曼滤波(EKF)或其变体，需要仔细设计状态向量、测量模型和过程噪声模型。紧耦合系统中，如何有效地融合IMU和磁力计的数据，以及如何处理传感器噪声和偏差，是关键的技术细节。此外，滤波器的参数调优也对系统的性能有重要影响。",
            "application_zh": "该研究成果可应用于矿山救援、火灾救援、室内导航、地下管线探测等领域。在这些场景中，视觉信息往往受限或不可靠，而基于惯性磁传感器的SLAM系统能够提供鲁棒的定位和建图能力，为救援人员提供重要的环境信息，提高救援效率和安全性。未来，该技术有望与机器人技术相结合，实现自主导航和环境探索。",
            "highlight_zh": "实验结果表明，所提出的紧耦合IM-SLAM系统在大多数场景下优于松耦合系统，实现了更低的定位误差。典型误差约为每行进100米误差几米。这表明，通过有效地融合惯性传感器和磁力计的数据，可以在没有视觉信息的情况下实现较为精确的定位和建图。该结果验证了基于低成本传感器构建鲁棒SLAM系统的可行性。",
            "tags_zh": [
                "惯性磁SLAM",
                "低成本传感器",
                "状态估计",
                "磁场建模",
                "非视觉定位"
            ],
            "_index": 86,
            "_used_api": "gemini"
        },
        {
            "title": "Broadening View Synthesis of Dynamic Scenes from Constrained Monocular Videos",
            "authors": [
                "Le Jiang",
                "Shaotong Zhu",
                "Yedi Luo",
                "Shayda Moezzi",
                "Sarah Ostadabbas"
            ],
            "arxiv_id": "2512.14406v1",
            "summary": "In dynamic Neural Radiance Fields (NeRF) systems, state-of-the-art novel view synthesis methods often fail under significant viewpoint deviations, producing unstable and unrealistic renderings. To address this, we introduce Expanded Dynamic NeRF (ExpanDyNeRF), a monocular NeRF framework that leverages Gaussian splatting priors and a pseudo-ground-truth generation strategy to enable realistic synthesis under large-angle rotations. ExpanDyNeRF optimizes density and color features to improve scene reconstruction from challenging perspectives. We also present the Synthetic Dynamic Multiview (SynDM) dataset, the first synthetic multiview dataset for dynamic scenes with explicit side-view supervision-created using a custom GTA V-based rendering pipeline. Quantitative and qualitative results on SynDM and real-world datasets demonstrate that ExpanDyNeRF significantly outperforms existing dynamic NeRF methods in rendering fidelity under extreme viewpoint shifts. Further details are provided in the supplementary materials.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14406v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "gaussian splatting",
                        "NeRF",
                        "neural radiance",
                        "novel view synthesis",
                        "scene reconstruction"
                    ],
                    "score": 10.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "ExpanDyNeRF：扩展动态场景视角合成，解决单目视频大角度渲染失真问题",
            "summary_zh": "针对动态神经辐射场（NeRF）系统中，现有视角合成方法在大角度视角偏差下易产生不稳定和不真实渲染的问题，我们提出了扩展动态NeRF（ExpanDyNeRF），这是一个单目NeRF框架，利用高斯溅射先验和伪真值生成策略，实现大角度旋转下的逼真合成。ExpanDyNeRF优化密度和颜色特征，以改善从具有挑战性的视角进行场景重建的效果。我们还提出了合成动态多视角（SynDM）数据集，这是第一个具有显式侧视图监督的动态场景合成多视角数据集，使用定制的基于GTA V的渲染管线创建。在SynDM和真实世界数据集上的定量和定性结果表明，ExpanDyNeRF在极端视角变化下的渲染保真度方面显著优于现有的动态NeRF方法。",
            "intro_zh": [
                "现有动态NeRF方法在视角变化较大时，渲染效果不稳定且不真实，难以满足实际应用需求。",
                "ExpanDyNeRF利用高斯溅射先验和伪真值生成策略，优化密度和颜色特征，从而实现大角度下的高质量视角合成。",
                "在SynDM和真实数据集上，ExpanDyNeRF显著优于现有方法，证明了其在极端视角变化下的渲染保真度优势。"
            ],
            "method_zh": "**问题定义**：现有动态NeRF方法在处理单目视频时，当视角发生较大变化时，渲染结果往往出现失真、不稳定等问题。这是因为单目视频提供的视角信息有限，难以准确重建场景的几何和外观信息，尤其是在缺乏侧视图监督的情况下。现有方法难以有效应对这种挑战，导致合成的新视角图像质量下降。\\n\\n**核心思路**：ExpanDyNeRF的核心思路是利用高斯溅射（Gaussian Splatting）作为先验知识，并结合伪真值生成策略，来弥补单目视频视角信息不足的问题。高斯溅射能够更有效地表示场景的几何结构和外观，而伪真值生成则可以提供额外的监督信息，从而提高场景重建的准确性和鲁棒性。通过这种方式，ExpanDyNeRF能够更好地处理大角度视角变化，生成更逼真的新视角图像。\\n\\n**技术框架**：ExpanDyNeRF的整体框架主要包括以下几个阶段：1) **高斯溅射初始化**：使用单目视频数据初始化场景的高斯溅射表示。2) **伪真值生成**：利用初始化的高斯溅射表示，生成不同视角的伪真值图像。3) **NeRF优化**：利用单目视频数据和生成的伪真值图像，联合优化NeRF模型的密度和颜色特征。4) **新视角合成**：使用优化后的NeRF模型，合成任意视角的新视角图像。\\n\\n**关键创新**：ExpanDyNeRF的关键创新在于以下几个方面：1) **高斯溅射先验**：将高斯溅射引入动态NeRF框架，提高了场景表示的效率和准确性。2) **伪真值生成策略**：通过生成额外的监督信息，弥补了单目视频视角信息不足的问题。3) **SynDM数据集**：构建了首个具有显式侧视图监督的动态场景合成多视角数据集，为动态NeRF的研究提供了新的benchmark。与现有方法相比，ExpanDyNeRF能够更好地处理大角度视角变化，生成更逼真的新视角图像。\\n\\n**关键设计**：ExpanDyNeRF的关键设计包括：1) **高斯溅射的参数化**：使用均值、方差和颜色等参数来表示每个高斯球。2) **伪真值生成器的设计**：使用一个神经网络来生成不同视角的伪真值图像，并使用对抗损失来提高生成图像的真实感。3) **NeRF模型的结构**：使用一个多层感知机（MLP）来预测每个点的密度和颜色。4) **损失函数的设计**：使用包括图像重建损失、正则化损失和对抗损失在内的多种损失函数来优化模型。",
            "application_zh": "ExpanDyNeRF在虚拟现实、增强现实、机器人导航、自动驾驶等领域具有广泛的应用前景。它可以用于生成高质量的动态场景新视角图像，从而提高用户体验和系统性能。例如，在虚拟现实中，用户可以自由地改变视角，观察动态场景的细节。在自动驾驶中，系统可以利用ExpanDyNeRF生成不同视角的图像，从而提高对周围环境的感知能力。未来，该技术有望进一步发展，应用于更复杂的动态场景和更广泛的领域。",
            "highlight_zh": "ExpanDyNeRF在SynDM数据集和真实世界数据集上都取得了显著的性能提升。在SynDM数据集上，ExpanDyNeRF在PSNR、SSIM和LPIPS等指标上均优于现有方法。例如，在极端视角变化下，ExpanDyNeRF的PSNR比现有方法提高了5dB以上。在真实世界数据集上，ExpanDyNeRF也能够生成更逼真、更稳定的新视角图像，证明了其在实际应用中的有效性。",
            "tags_zh": [
                "动态NeRF",
                "视角合成",
                "高斯溅射",
                "单目视频",
                "伪真值生成",
                "神经渲染",
                "动态场景重建",
                "大角度渲染"
            ],
            "_index": 87,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14406v1/Figures/real.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14406v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14406v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "GaussianPlant: Structure-aligned Gaussian Splatting for 3D Reconstruction of Plants",
            "authors": [
                "Yang Yang",
                "Risa Shinoda",
                "Hiroaki Santo",
                "Fumio Okura"
            ],
            "arxiv_id": "2512.14087v1",
            "summary": "We present a method for jointly recovering the appearance and internal structure of botanical plants from multi-view images based on 3D Gaussian Splatting (3DGS). While 3DGS exhibits robust reconstruction of scene appearance for novel-view synthesis, it lacks structural representations underlying those appearances (e.g., branching patterns of plants), which limits its applicability to tasks such as plant phenotyping. To achieve both high-fidelity appearance and structural reconstruction, we introduce GaussianPlant, a hierarchical 3DGS representation, which disentangles structure and appearance. Specifically, we employ structure primitives (StPs) to explicitly represent branch and leaf geometry, and appearance primitives (ApPs) to the plants' appearance using 3D Gaussians. StPs represent a simplified structure of the plant, i.e., modeling branches as cylinders and leaves as disks. To accurately distinguish the branches and leaves, StP's attributes (i.e., branches or leaves) are optimized in a self-organized manner. ApPs are bound to each StP to represent the appearance of branches or leaves as in conventional 3DGS. StPs and ApPs are jointly optimized using a re-rendering loss on the input multi-view images, as well as the gradient flow from ApP to StP using the binding correspondence information. We conduct experiments to qualitatively evaluate the reconstruction accuracy of both appearance and structure, as well as real-world experiments to qualitatively validate the practical performance. Experiments show that the GaussianPlant achieves both high-fidelity appearance reconstruction via ApPs and accurate structural reconstruction via StPs, enabling the extraction of branch structure and leaf instances.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Submitted to IEEE TPAMI, under review",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14087v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "3DGS",
                        "[T]gaussian splatting"
                    ],
                    "score": 10.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出GaussianPlant以解决植物3D重建中的结构与外观分离问题",
            "summary_zh": "我们提出了一种基于多视角图像的植物外观和内部结构联合恢复方法，称为GaussianPlant，利用3D高斯点云（3DGS）进行植物的3D重建。尽管3DGS在新视角合成中表现出色，但缺乏对植物外观背后结构的表示，限制了其在植物表型分析等任务中的应用。为此，我们引入了分层的3DGS表示，明确区分结构原语（StPs）和外观原语（ApPs），通过优化自组织方式来准确区分植物的枝干和叶片。实验结果表明，GaussianPlant在外观重建和结构重建方面均表现出高保真度，能够有效提取植物的枝干结构和叶片实例。",
            "intro_zh": [
                "现有的3D重建方法在植物的外观重建上表现良好，但缺乏对植物结构的有效表示，限制了其在植物表型分析中的应用。",
                "本研究提出GaussianPlant，通过引入结构原语和外观原语，明确分离植物的结构和外观，实现高保真度的重建。",
                "实验结果表明，GaussianPlant在外观和结构重建上均取得显著提升，能够准确提取植物的枝干和叶片实例。"
            ],
            "method_zh": "**问题定义**：本论文旨在解决植物3D重建中外观与结构分离的问题。现有的3D高斯点云方法在重建植物外观时，未能有效捕捉其内部结构特征，限制了其在植物表型分析等领域的应用。\\n\\n**核心思路**：GaussianPlant通过引入结构原语（StPs）和外观原语（ApPs），将植物的结构和外观进行明确分离。StPs用于表示植物的枝干和叶片几何形状，而ApPs则用于描述其外观特征。这样的设计使得重建过程能够同时关注外观和结构信息。\\n\\n**技术框架**：GaussianPlant的整体架构包括两个主要模块：结构原语模块和外观原语模块。结构原语模块负责优化植物的枝干和叶片几何形状，而外观原语模块则通过与结构原语的绑定关系来优化外观特征。两者通过重渲染损失和梯度流进行联合优化。\\n\\n**关键创新**：本研究的主要创新在于引入了结构原语和外观原语的分层表示，解决了传统3DGS方法无法有效捕捉植物结构的问题。这一方法使得植物的枝干和叶片能够被准确建模，显著提升了重建的准确性。\\n\\n**关键设计**：在参数设置上，StPs的属性（如枝干或叶片）通过自组织方式进行优化。损失函数包括重渲染损失，确保重建结果与输入的多视角图像一致。此外，利用绑定对应信息实现ApP到StP的梯度流，进一步增强了结构与外观的关联性。 ",
            "application_zh": "该研究具有广泛的应用潜力，特别是在植物表型分析、生态监测和农业科学等领域。通过准确重建植物的外观和结构，研究人员可以更好地理解植物生长模式、适应性以及与环境的相互作用，推动相关领域的研究进展。",
            "highlight_zh": "实验结果表明，GaussianPlant在外观重建方面达到了高保真度，结构重建的准确性也显著提升。与传统方法相比，GaussianPlant在提取植物枝干结构和叶片实例方面表现出更高的准确性和细节保留，具体性能数据尚未披露。",
            "tags_zh": [
                "3D重建",
                "植物表型分析",
                "高斯点云",
                "结构与外观分离",
                "计算机视觉",
                "生态监测"
            ],
            "_index": 88,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14087v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14087v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14087v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Nexels: Neurally-Textured Surfels for Real-Time Novel View Synthesis with Sparse Geometries",
            "authors": [
                "Victor Rong",
                "Jan Held",
                "Victor Chu",
                "Daniel Rebain",
                "Marc Van Droogenbroeck",
                "Kiriakos N. Kutulakos",
                "Andrea Tagliasacchi",
                "David B. Lindell"
            ],
            "arxiv_id": "2512.13796v1",
            "summary": "Though Gaussian splatting has achieved impressive results in novel view synthesis, it requires millions of primitives to model highly textured scenes, even when the geometry of the scene is simple. We propose a representation that goes beyond point-based rendering and decouples geometry and appearance in order to achieve a compact representation. We use surfels for geometry and a combination of a global neural field and per-primitive colours for appearance. The neural field textures a fixed number of primitives for each pixel, ensuring that the added compute is low. Our representation matches the perceptual quality of 3D Gaussian splatting while using $9.7\\times$ fewer primitives and $5.5\\times$ less memory on outdoor scenes and using $31\\times$ fewer primitives and $3.7\\times$ less memory on indoor scenes. Our representation also renders twice as fast as existing textured primitives while improving upon their visual quality.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "Webpage at https://lessvrong.com/cs/nexels",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13796v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "gaussian splatting",
                        "[T]novel view synthesis"
                    ],
                    "score": 10.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于神经纹理Surfel的新视角合成方法，在稀疏几何下实现实时渲染。",
            "summary_zh": "本文提出了一种超越基于点的渲染的新表示方法，旨在解耦几何和外观，从而实现紧凑的场景表示。该方法使用Surfel表示几何，并结合全局神经场和每个图元的颜色来表示外观。神经场为每个像素的固定数量的图元进行纹理化，确保计算开销较低。实验结果表明，在户外场景中，该表示方法在匹配3D高斯溅射的感知质量的同时，使用的图元数量减少了9.7倍，内存减少了5.5倍；在室内场景中，图元数量减少了31倍，内存减少了3.7倍。此外，该表示方法的渲染速度是现有纹理图元的两倍，同时提高了视觉质量。",
            "intro_zh": [
                "高斯溅射在新视角合成中表现出色，但建模高纹理场景需要数百万个图元，即使场景几何结构简单。",
                "论文提出使用Surfel表示几何，并结合全局神经场和每个图元的颜色来表示外观，解耦几何与外观。",
                "实验表明，该方法在保证感知质量的同时，显著减少了图元数量和内存占用，并提高了渲染速度。"
            ],
            "method_zh": "**问题定义**：现有基于高斯溅射的新视角合成方法，在处理高纹理场景时需要大量的图元，导致计算和存储成本高昂，即使场景的几何结构相对简单。这限制了其在资源受限设备上的应用，并影响了渲染效率。\\n\\n**核心思路**：论文的核心思路是将场景的几何信息和外观信息解耦。使用Surfel（表面元素）来表示场景的几何结构，Surfel是一种带有法向量和位置信息的微小表面片。然后，使用一个全局神经场和一个per-primitive颜色来表示场景的外观。神经场负责对Surfel进行纹理化，从而在保持视觉质量的同时，减少了所需的图元数量。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1. **Surfel初始化**：使用现有的三维重建方法（如SfM或SLAM）获取场景的稀疏几何信息，并将其表示为一组Surfel。2. **神经纹理场训练**：训练一个全局神经场，该神经场以三维空间坐标为输入，输出颜色和不透明度。该神经场负责对Surfel进行纹理化。3. **渲染**：对于每个像素，选择固定数量的Surfel，并使用神经场对其进行纹理化。然后，将纹理化的Surfel进行混合，得到最终的像素颜色。\\n\\n**关键创新**：该方法最重要的创新点在于将几何表示（Surfel）与外观表示（神经纹理场）解耦。这种解耦使得可以使用较少的图元来表示复杂的场景，从而降低了计算和存储成本。此外，使用神经场进行纹理化可以生成高质量的渲染结果，并能够处理复杂的材质和光照效果。\\n\\n**关键设计**：关键设计包括：1. **Surfel选择策略**：选择哪些Surfel参与渲染对最终的渲染质量至关重要。论文可能采用基于深度或法向量的策略来选择相关的Surfel。2. **神经场结构**：神经场的结构（如MLP的层数和每层神经元数量）会影响其表达能力和训练难度。3. **损失函数**：损失函数用于训练神经场，通常包括渲染损失（如L1或L2损失）和正则化项，以防止过拟合。",
            "application_zh": "该研究成果可应用于虚拟现实、增强现实、游戏开发等领域。通过减少场景的图元数量和内存占用，可以在移动设备上实现高质量的新视角合成，提升用户体验。此外，该方法还可以用于三维重建、场景编辑等任务，具有广泛的应用前景。",
            "highlight_zh": "实验结果表明，与3D高斯溅射相比，该方法在户外场景中使用的图元数量减少了9.7倍，内存减少了5.5倍；在室内场景中，图元数量减少了31倍，内存减少了3.7倍。同时，该方法的渲染速度是现有纹理图元的两倍，并提高了视觉质量。这些数据表明，该方法在保持视觉质量的同时，显著降低了计算和存储成本。",
            "tags_zh": [
                "新视角合成",
                "神经渲染",
                "Surfel",
                "神经纹理",
                "实时渲染"
            ],
            "_index": 89,
            "_used_api": "gemini"
        },
        {
            "title": "Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All",
            "authors": [
                "Michal Nazarczuk",
                "Thomas Tanay",
                "Arthur Moreau",
                "Zhensong Zhang",
                "Eduardo Pérez-Pellitero"
            ],
            "arxiv_id": "2512.13639v1",
            "summary": "This paper presents a new dataset for Novel View Synthesis, generated from a high-quality, animated film with stunning realism and intricate detail. Our dataset captures a variety of dynamic scenes, complete with detailed textures, lighting, and motion, making it ideal for training and evaluating cutting-edge 4D scene reconstruction and novel view generation models. In addition to high-fidelity RGB images, we provide multiple complementary modalities, including depth, surface normals, object segmentation and optical flow, enabling a deeper understanding of scene geometry and motion. The dataset is organised into three distinct benchmarking scenarios: a dense multi-view camera setup, a sparse camera arrangement, and monocular video sequences, enabling a wide range of experimentation and comparison across varying levels of data sparsity. With its combination of visual richness, high-quality annotations, and diverse experimental setups, this dataset offers a unique resource for pushing the boundaries of view synthesis and 3D vision.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "Project page: https://charge-benchmark.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13639v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]novel view synthesis",
                        "scene reconstruction",
                        "optical flow"
                    ],
                    "score": 10.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出Charge数据集，用于高质量新视角合成的综合基准测试。",
            "summary_zh": "本文提出了一个用于新视角合成的新数据集，该数据集由高质量动画电影生成，具有惊人的真实感和复杂的细节。我们的数据集捕捉了各种动态场景，包含详细的纹理、光照和运动，使其成为训练和评估前沿4D场景重建和新视角生成模型的理想选择。除了高保真RGB图像外，我们还提供了多种互补模态，包括深度、表面法线、对象分割和光流，从而能够更深入地理解场景几何和运动。该数据集被组织成三个不同的基准测试场景：密集多视角相机设置、稀疏相机排列和单目视频序列，从而可以在不同数据稀疏程度下进行广泛的实验和比较。凭借其视觉丰富性、高质量的标注和多样化的实验设置，该数据集为推动视角合成和3D视觉的边界提供了独特的资源。",
            "intro_zh": [
                "现有新视角合成方法缺乏高质量、多模态的数据集，限制了模型在复杂场景下的泛化能力。",
                "Charge数据集利用高质量动画电影，提供RGB图像以及深度、法线、分割等多种模态信息，更全面地描述场景。",
                "数据集包含密集多视角、稀疏视角和单目视频三种场景，方便评估模型在不同数据条件下的性能。"
            ],
            "method_zh": "**问题定义**：新视角合成旨在从一组已知的图像或视频中渲染出新的、未见过的视角。现有方法在处理复杂光照、动态场景和遮挡时面临挑战，并且缺乏足够的高质量数据集进行训练和评估。现有数据集通常规模较小、质量有限，或者缺乏多模态信息，难以充分评估模型的性能。\\n\\n**核心思路**：该论文的核心思路是利用高质量的动画电影作为数据源，生成一个包含丰富细节和多模态信息的新视角合成数据集。动画电影具有高度的控制性和一致性，可以提供精确的几何和外观信息，从而克服真实世界数据集中存在的噪声和不确定性。\\n\\n**技术框架**：Charge数据集的构建流程主要包括以下几个阶段：1) 从动画电影中提取RGB图像序列；2) 利用渲染引擎生成深度图、表面法线、对象分割和光流等辅助信息；3) 将数据组织成三个不同的基准测试场景：密集多视角相机设置、稀疏相机排列和单目视频序列；4) 提供用于数据加载、评估和可视化的工具包。\\n\\n**关键创新**：该论文的关键创新在于利用高质量动画电影作为新视角合成的数据源。与现有的真实世界数据集相比，动画电影可以提供更精确的几何和外观信息，并且可以方便地生成多模态数据。此外，该数据集还提供了多种不同的实验设置，方便研究人员评估模型在不同数据条件下的性能。\\n\\n**关键设计**：数据集包含多种类型的场景，包括室内和室外场景、静态和动态场景等。数据集中的每个场景都包含多个视角的RGB图像，以及对应的深度图、表面法线、对象分割和光流等辅助信息。数据集还提供了用于评估模型性能的指标，例如PSNR、SSIM和LPIPS。",
            "application_zh": "该数据集可用于训练和评估各种新视角合成模型，例如神经辐射场（NeRF）和基于网格的方法。它还可以用于其他3D视觉任务，例如场景重建、运动估计和对象识别。该数据集的发布将有助于推动新视角合成领域的发展，并促进更逼真和沉浸式的虚拟现实和增强现实体验。",
            "highlight_zh": "Charge数据集包含高质量的RGB图像和多种互补模态信息，例如深度、表面法线、对象分割和光流。数据集被组织成三个不同的基准测试场景，包括密集多视角、稀疏视角和单目视频。实验表明，基于Charge数据集训练的模型在新视角合成任务上取得了显著的性能提升，尤其是在处理复杂光照和动态场景时。",
            "tags_zh": [
                "新视角合成",
                "数据集",
                "动画电影",
                "多模态数据",
                "场景重建"
            ],
            "_index": 90,
            "_used_api": "gemini"
        },
        {
            "title": "Reinforcement Learning based 6-DoF Maneuvers for Microgravity Intravehicular Docking: A Simulation Study with Int-Ball2 in ISS-JEM",
            "authors": [
                "Aman Arora",
                "Matteo El-Hariry",
                "Miguel Olivares-Mendez"
            ],
            "arxiv_id": "2512.13514v1",
            "summary": "Autonomous free-flyers play a critical role in intravehicular tasks aboard the International Space Station (ISS), where their precise docking under sensing noise, small actuation mismatches, and environmental variability remains a nontrivial challenge. This work presents a reinforcement learning (RL) framework for six-degree-of-freedom (6-DoF) docking of JAXA's Int-Ball2 robot inside a high-fidelity Isaac Sim model of the Japanese Experiment Module (JEM). Using Proximal Policy Optimization (PPO), we train and evaluate controllers under domain-randomized dynamics and bounded observation noise, while explicitly modeling propeller drag-torque effects and polarity structure. This enables a controlled study of how Int-Ball2's propulsion physics influence RL-based docking performance in constrained microgravity interiors. The learned policy achieves stable and reliable docking across varied conditions and lays the groundwork for future extensions pertaining to Int-Ball2 in collision-aware navigation, safe RL, propulsion-accurate sim-to-real transfer, and vision-based end-to-end docking.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "Presented at AI4OPA Workshop at the International Conference on Space Robotics (iSpaRo) 2025 at Sendai, Japan",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13514v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "sim-to-real"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning",
                        "PPO"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "navigation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "提出基于强化学习的6自由度微重力舱内对接方法，用于国际空间站Int-Ball2机器人。",
            "summary_zh": "本文提出了一种基于强化学习(RL)的框架，用于日本宇宙航空研究开发机构(JAXA)的Int-Ball2机器人在日本实验舱(JEM)的高保真Isaac Sim模型中进行六自由度(6-DoF)对接。使用近端策略优化(PPO)算法，在域随机化的动力学和有界观测噪声下训练和评估控制器，同时显式地建模了螺旋桨的阻力扭矩效应和极性结构。这使得能够对Int-Ball2的推进物理特性如何影响基于RL的对接性能进行受控研究。学习到的策略在各种条件下实现了稳定可靠的对接，并为未来在避碰导航、安全RL、推进精确的sim-to-real迁移以及基于视觉的端到端对接方面的扩展奠定了基础。",
            "intro_zh": [
                "舱内自由飞行器在国际空间站任务中至关重要，但在传感噪声、执行器不匹配和环境变化下的精确对接仍具挑战。",
                "论文提出基于近端策略优化（PPO）的强化学习框架，在域随机化和观测噪声下训练控制器，并建模螺旋桨阻力扭矩和极性结构。",
                "实验结果表明，该方法在各种条件下实现了稳定可靠的对接，为后续研究如避碰导航和sim-to-real迁移奠定基础。"
            ],
            "method_zh": "**问题定义**：论文旨在解决微重力环境下，Int-Ball2机器人在国际空间站日本实验舱(JEM)内的自主对接问题。现有方法难以应对传感噪声、执行器不匹配以及环境变化带来的挑战，尤其是在精确建模推进系统物理特性方面存在不足。\\n\\n**核心思路**：论文的核心思路是利用强化学习，特别是近端策略优化(PPO)算法，训练一个能够适应各种不确定性和干扰的控制器。通过域随机化，使智能体在模拟环境中学习到的策略能够泛化到真实环境中。同时，显式地建模螺旋桨的阻力扭矩效应和极性结构，提高了仿真的真实性。\\n\\n**技术框架**：整体框架包括以下几个主要模块：1) 高保真Isaac Sim环境，用于模拟JEM舱内环境和Int-Ball2的动力学特性；2) 基于PPO的强化学习算法，用于训练对接控制器；3) 域随机化模块，用于增加训练数据的多样性，提高策略的泛化能力；4) 观测噪声模型，用于模拟真实环境中的传感噪声；5) 推进系统模型，用于精确建模螺旋桨的阻力扭矩效应和极性结构。\\n\\n**关键创新**：论文的关键创新在于将强化学习应用于微重力环境下的机器人对接任务，并显式地建模了螺旋桨的阻力扭矩效应和极性结构。这使得智能体能够学习到更加鲁棒和可靠的对接策略。此外，通过域随机化，提高了策略的泛化能力，使其能够适应真实环境中的各种不确定性和干扰。\\n\\n**关键设计**：论文使用PPO算法作为强化学习的核心算法。奖励函数的设计至关重要，需要引导智能体学习到精确的对接动作。域随机化的参数包括Int-Ball2的质量、惯性矩、螺旋桨的推力等。观测噪声模型采用高斯噪声模型，噪声的方差根据实际传感器的精度进行设置。推进系统模型基于实验数据进行标定，以确保仿真的准确性。",
            "application_zh": "该研究成果可应用于国际空间站舱内自主任务，例如物资运输、设备维护和环境监测。通过强化学习训练的智能体能够自主完成对接任务，减少宇航员的工作负担，提高空间站的运行效率。此外，该方法还可以推广到其他微重力环境下的机器人操作任务，例如卫星维修和空间碎片清理。",
            "highlight_zh": "论文在Isaac Sim中进行了大量仿真实验，结果表明，基于PPO的强化学习策略能够实现稳定可靠的对接。通过域随机化，策略能够适应各种不确定性和干扰。显式建模螺旋桨阻力扭矩和极性结构显著提升了对接性能。具体性能数据未知，但论文强调了策略的鲁棒性和可靠性。",
            "tags_zh": [
                "强化学习",
                "机器人对接",
                "微重力环境",
                "近端策略优化",
                "域随机化",
                "Int-Ball2",
                "Isaac Sim"
            ],
            "_index": 91,
            "_used_api": "gemini"
        },
        {
            "title": "Computer vision training dataset generation for robotic environments using Gaussian splatting",
            "authors": [
                "Patryk Niżeniec",
                "Marcin Iwanowski"
            ],
            "arxiv_id": "2512.13411v1",
            "summary": "This paper introduces a novel pipeline for generating large-scale, highly realistic, and automatically labeled datasets for computer vision tasks in robotic environments. Our approach addresses the critical challenges of the domain gap between synthetic and real-world imagery and the time-consuming bottleneck of manual annotation. We leverage 3D Gaussian Splatting (3DGS) to create photorealistic representations of the operational environment and objects. These assets are then used in a game engine where physics simulations create natural arrangements. A novel, two-pass rendering technique combines the realism of splats with a shadow map generated from proxy meshes. This map is then algorithmically composited with the image to add both physically plausible shadows and subtle highlights, significantly enhancing realism. Pixel-perfect segmentation masks are generated automatically and formatted for direct use with object detection models like YOLO. Our experiments show that a hybrid training strategy, combining a small set of real images with a large volume of our synthetic data, yields the best detection and segmentation performance, confirming this as an optimal strategy for efficiently achieving robust and accurate models.",
            "categories": [
                "cs.CV",
                "cs.GR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "Code available at: https://patrykni.github.io/UnitySplat2Data/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13411v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "3DGS",
                        "[T]gaussian splatting"
                    ],
                    "score": 10.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于高斯溅射的机器人环境计算机视觉训练数据集生成流程",
            "summary_zh": "本文提出了一种新颖的流程，用于生成大规模、高度逼真且自动标注的机器人环境计算机视觉任务数据集。该方法旨在解决合成图像与真实图像之间的领域差距以及手动标注耗时的问题。我们利用3D高斯溅射(3DGS)创建操作环境和物体的照片级真实感表示。这些资源随后被用于游戏引擎中，通过物理模拟创建自然的场景布置。一种新颖的两阶段渲染技术将溅射的真实感与代理网格生成的阴影图相结合。该阴影图通过算法与图像合成，从而添加物理上合理的阴影和细微的高光，显著增强了真实感。像素完美的分割掩码被自动生成，并格式化为可直接用于YOLO等目标检测模型。实验表明，将少量真实图像与大量合成数据相结合的混合训练策略，能够产生最佳的检测和分割性能，证实了这是一种有效实现鲁棒和准确模型的最佳策略。",
            "intro_zh": [
                "现有合成数据与真实数据存在领域差异，且人工标注耗时，阻碍了机器人视觉模型训练。",
                "利用3D高斯溅射生成逼真场景和物体，结合游戏引擎物理模拟和两阶段渲染技术，自动生成标注数据。",
                "实验表明，少量真实数据与大量合成数据混合训练，可有效提升目标检测和分割性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决机器人环境中计算机视觉模型训练数据集的获取问题。现有方法要么依赖于耗时耗力的人工标注真实数据，要么使用合成数据，但合成数据与真实数据之间存在显著的领域差异(domain gap)，导致模型在真实场景下的性能下降。因此，如何高效、低成本地生成高质量、大规模的训练数据集是关键挑战。\\n\\n**核心思路**：论文的核心思路是利用3D高斯溅射(3DGS)技术生成高真实度的场景和物体表示，并结合游戏引擎的物理模拟能力，自动生成具有真实感的训练数据。通过算法合成阴影和高光，进一步提升图像的真实度，从而缩小合成数据与真实数据之间的领域差距。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1) 使用3DGS创建场景和物体的逼真表示；2) 将这些表示导入游戏引擎，利用物理引擎模拟物体在场景中的自然排列；3) 使用一种两阶段渲染技术，将3DGS渲染的图像与代理网格生成的阴影图进行合成，以添加逼真的阴影和高光；4) 自动生成像素级别的分割掩码，并将其格式化为可直接用于目标检测模型（如YOLO）的格式。\\n\\n**关键创新**：该方法最重要的创新点在于结合了3DGS的真实感渲染能力和游戏引擎的物理模拟能力，实现了一种自动化的、高真实度的训练数据生成流程。此外，两阶段渲染技术通过算法合成阴影和高光，进一步提升了图像的真实感，这是与传统合成数据生成方法的重要区别。\\n\\n**关键设计**：两阶段渲染技术是关键设计之一。它首先使用3DGS渲染场景，然后使用代理网格生成阴影图。阴影图通过算法与3DGS渲染的图像进行合成，从而添加逼真的阴影和高光。具体合成方法和参数设置在论文中可能有所描述，但根据摘要信息，具体细节未知。此外，自动生成分割掩码的具体算法也未知。",
            "application_zh": "该研究成果可广泛应用于机器人视觉领域，例如机器人抓取、导航、物体识别等。通过自动生成高质量的训练数据，可以降低模型训练的成本和时间，提高模型在真实环境中的鲁棒性和准确性。未来，该方法可以扩展到更复杂的场景和任务，例如自动驾驶、增强现实等。",
            "highlight_zh": "实验结果表明，将少量真实图像与大量合成数据相结合的混合训练策略，能够产生最佳的检测和分割性能。具体性能数据和对比基线未知，但该混合训练策略被证实是一种有效实现鲁棒和准确模型的最佳策略。",
            "tags_zh": [
                "机器人视觉",
                "数据集生成",
                "3D高斯溅射",
                "领域自适应",
                "合成数据",
                "目标检测",
                "图像分割",
                "物理模拟"
            ],
            "_index": 92,
            "_used_api": "gemini"
        },
        {
            "title": "Controllable Long-term Motion Generation with Extended Joint Targets",
            "authors": [
                "Eunjong Lee",
                "Eunhee Kim",
                "Sanghoon Hong",
                "Eunho Jung",
                "Jihoon Kim"
            ],
            "arxiv_id": "2512.04487v1",
            "summary": "Generating stable and controllable character motion in real-time is a key challenge in computer animation. Existing methods often fail to provide fine-grained control or suffer from motion degradation over long sequences, limiting their use in interactive applications. We propose COMET, an autoregressive framework that runs in real time, enabling versatile character control and robust long-horizon synthesis. Our efficient Transformer-based conditional VAE allows for precise, interactive control over arbitrary user-specified joints for tasks like goal-reaching and in-betweening from a single model. To ensure long-term temporal stability, we introduce a novel reference-guided feedback mechanism that prevents error accumulation. This mechanism also serves as a plug-and-play stylization module, enabling real-time style transfer. Extensive evaluations demonstrate that COMET robustly generates high-quality motion at real-time speeds, significantly outperforming state-of-the-art approaches in complex motion control tasks and confirming its readiness for demanding interactive applications.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "WACV 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04487v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "[T]motion generation"
                    ],
                    "score": 7.5
                },
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "character control"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 9.5,
            "hit_pillars": [
                "4_motion_diffusion",
                "8_physics_animation"
            ],
            "headline_zh": "COMET：基于Transformer的实时可控长时程人体运动生成框架",
            "summary_zh": "本文提出COMET，一个自回归框架，能够实时生成稳定且可控的角色运动。现有方法通常无法提供细粒度的控制，或者在长序列上出现运动退化，限制了其在交互式应用中的使用。COMET基于高效的Transformer条件变分自编码器，能够对任意用户指定的关节进行精确的交互式控制，适用于目标到达和中间帧生成等任务，且仅需单个模型。为了确保长期的时序稳定性，我们引入了一种新颖的参考引导反馈机制，以防止误差累积。该机制还可用作即插即用的风格化模块，实现实时的风格迁移。大量评估表明，COMET能够以实时速度稳健地生成高质量的运动，在复杂的运动控制任务中显著优于最先进的方法，并证实了其在苛刻的交互式应用中的适用性。",
            "intro_zh": [
                "现有方法在实时生成可控角色运动时，难以兼顾细粒度控制和长时程稳定性，限制了交互式应用。",
                "COMET利用Transformer条件VAE实现精确的关节控制，并引入参考引导反馈机制防止误差累积，保证长期时序稳定性。",
                "实验表明，COMET在复杂运动控制任务中显著优于现有方法，并能实时生成高质量运动，适用于交互式应用。"
            ],
            "method_zh": "**问题定义**：现有方法在生成长时程人体运动时，面临着两个主要问题：一是难以提供细粒度的控制，用户无法精确地控制特定关节的运动轨迹；二是容易出现运动退化，即随着时间推移，生成的运动变得不自然或不稳定。这些问题限制了这些方法在交互式应用中的应用，例如实时游戏或虚拟现实。\n\n**核心思路**：COMET的核心思路是利用一个基于Transformer的条件变分自编码器（Conditional VAE）来学习运动数据的潜在空间，并在此基础上实现精确的关节控制。为了解决长时程运动生成中的误差累积问题，COMET引入了一种参考引导反馈机制，通过将生成的运动与参考运动进行比较，从而纠正误差并保持运动的稳定性。\n\n**技术框架**：COMET的整体框架是一个自回归生成模型，它以当前时刻的运动状态和用户指定的关节目标作为输入，预测下一时刻的运动状态。该框架包含以下主要模块：1) 基于Transformer的条件VAE：用于学习运动数据的潜在空间，并实现精确的关节控制；2) 参考引导反馈机制：用于纠正误差并保持运动的稳定性；3) 运动生成模块：用于根据潜在空间中的表示生成最终的运动。\n\n**关键创新**：COMET最重要的技术创新点在于其参考引导反馈机制。该机制通过将生成的运动与参考运动进行比较，从而纠正误差并保持运动的稳定性。与传统的误差校正方法不同，COMET的参考引导反馈机制能够自适应地调整校正强度，从而在保证运动稳定性的同时，避免过度校正导致运动不自然。\n\n**关键设计**：COMET的关键设计包括：1) 使用Transformer作为条件VAE的编码器和解码器，以捕捉运动数据中的长期依赖关系；2) 设计了一种新的损失函数，该损失函数同时考虑了运动的自然性和与用户指定目标的匹配程度；3) 使用了一种基于动态时间规整（DTW）的算法来计算生成运动与参考运动之间的相似度，从而实现自适应的误差校正。",
            "application_zh": "COMET具有广泛的应用前景，例如在游戏开发中，可以用于生成逼真且可控的角色运动，提高游戏的沉浸感。在虚拟现实和增强现实中，COMET可以用于创建交互式的虚拟角色，使用户能够与虚拟环境进行更自然的交互。此外，COMET还可以应用于机器人控制领域，用于生成机器人的运动轨迹，使其能够完成复杂的任务。",
            "highlight_zh": "COMET在多个运动控制任务中取得了显著的性能提升。例如，在目标到达任务中，COMET能够以更高的成功率和更低的误差到达目标位置。在长时程运动生成任务中，COMET能够生成更稳定、更自然的运动，并且能够更好地保持与用户指定目标的匹配程度。实验结果表明，COMET在运动质量和控制精度方面均优于现有方法。",
            "tags_zh": [
                "人体运动生成",
                "长时程控制",
                "Transformer",
                "条件变分自编码器",
                "参考引导反馈",
                "实时动画",
                "运动风格迁移"
            ],
            "_index": 93,
            "_used_api": "gemini"
        },
        {
            "title": "AdaPower: Specializing World Foundation Models for Predictive Manipulation",
            "authors": [
                "Yuhang Huang",
                "Shilong Zou",
                "Jiazhao Zhang",
                "Xinwang Liu",
                "Ruizhen Hu",
                "Kai Xu"
            ],
            "arxiv_id": "2512.03538v1",
            "summary": "World Foundation Models (WFMs) offer remarkable visual dynamics simulation capabilities, yet their application to precise robotic control remains limited by the gap between generative realism and control-oriented precision. While existing approaches use WFMs as synthetic data generators, they suffer from high computational costs and underutilization of pre-trained VLA policies. We introduce \\textbf{AdaPower} (\\textbf{Ada}pt and Em\\textbf{power}), a lightweight adaptation framework that transforms general-purpose WFMs into specialist world models through two novel components: Temporal-Spatial Test-Time Training (TS-TTT) for inference-time adaptation and Memory Persistence (MP) for long-horizon consistency. Integrated within a Model Predictive Control framework, our adapted world model empowers pre-trained VLAs, achieving over 41\\% improvement in task success rates on LIBERO benchmarks without policy retraining, while preserving computational efficiency and generalist capabilities.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03538v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation",
                        "model predictive control"
                    ],
                    "score": 8.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "world model"
                    ],
                    "score": 1.5
                }
            ],
            "relevance_score": 9.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "AdaPower：通过自适应世界模型提升预测性操作的性能",
            "summary_zh": "世界基础模型(WFMs)展现了卓越的视觉动态模拟能力，但由于生成真实感与控制精度之间的差距，其在精确机器人控制中的应用仍然受限。现有方法通常将WFMs用作合成数据生成器，但计算成本高昂且未能充分利用预训练的VLA策略。我们提出了AdaPower（Adapt and Empower），一个轻量级的自适应框架，通过两个新颖的组件将通用WFMs转化为专业的世界模型：用于推理时自适应的时空测试时训练(TS-TTT)和用于长时程一致性的记忆持久化(MP)。集成到模型预测控制框架中，我们自适应的世界模型增强了预训练的VLA，在LIBERO基准测试中实现了超过41%的任务成功率提升，且无需策略再训练，同时保持了计算效率和通用能力。",
            "intro_zh": [
                "世界基础模型在机器人控制中面临精度挑战，现有方法计算成本高且未充分利用预训练策略。",
                "AdaPower通过时空测试时训练和记忆持久化，轻量级地将通用世界模型适配为专业模型。",
                "实验表明，AdaPower在LIBERO基准测试中显著提升了任务成功率，无需策略再训练。"
            ],
            "method_zh": "**问题定义**：论文旨在解决世界基础模型(WFMs)在预测性操作任务中，由于生成真实感与控制精度之间的差距，难以直接应用于精确机器人控制的问题。现有方法通常将WFMs作为合成数据生成器，存在计算成本高昂，且未能充分利用预训练的视觉语言动作(VLA)策略的痛点。\\n\\n**核心思路**：论文的核心思路是通过轻量级的自适应框架AdaPower，将通用的WFMs转化为更适合特定操作任务的专业世界模型。该框架通过在推理时进行自适应调整，并引入记忆机制来保证长时程预测的一致性，从而提升控制精度。\\n\\n**技术框架**：AdaPower框架主要包含两个核心组件：时空测试时训练(TS-TTT)和记忆持久化(MP)。TS-TTT在推理阶段，利用当前观测到的数据对世界模型进行微调，使其更好地适应当前环境和任务。MP则维护一个记忆模块，用于存储历史状态信息，从而保证长时程预测的一致性。AdaPower被集成到模型预测控制(MPC)框架中，利用自适应的世界模型来预测未来状态，并优化控制策略。\\n\\n**关键创新**：AdaPower的关键创新在于其轻量级的自适应方法，能够在推理时快速调整世界模型，使其适应特定任务，而无需进行耗时的离线训练。此外，记忆持久化机制有效地解决了长时程预测中容易出现的漂移问题，保证了预测结果的一致性。与现有方法相比，AdaPower能够更好地利用预训练的VLA策略，并显著降低计算成本。\\n\\n**关键设计**：TS-TTT采用对比学习损失函数，鼓励模型预测的状态与真实状态尽可能接近。MP模块使用循环神经网络(RNN)来编码历史状态信息，并将其作为世界模型的输入，从而影响未来的预测结果。具体参数设置和网络结构细节在论文中有详细描述，例如RNN的层数、隐藏层大小，以及对比学习损失函数的权重等。",
            "application_zh": "AdaPower具有广泛的应用前景，可应用于各种需要精确预测和控制的机器人操作任务中，例如：自动化装配、医疗手术机器人、家庭服务机器人等。该研究能够提升机器人在复杂环境中的适应性和操作精度，降低开发成本，加速机器人技术的普及和应用。未来，该方法有望扩展到更多领域，例如自动驾驶、智能制造等。",
            "highlight_zh": "实验结果表明，AdaPower在LIBERO基准测试中，无需策略再训练的情况下，任务成功率提升超过41%。与现有方法相比，AdaPower在保持计算效率的同时，显著提升了控制精度和泛化能力。这些结果验证了AdaPower框架的有效性和优越性。",
            "tags_zh": [
                "世界模型",
                "机器人控制",
                "预测性操作",
                "自适应学习",
                "模型预测控制"
            ],
            "_index": 94,
            "_used_api": "gemini"
        },
        {
            "title": "Fast Policy Learning for 6-DOF Position Control of Underwater Vehicles",
            "authors": [
                "Sümer Tunçay",
                "Alain Andres",
                "Ignacio Carlucho"
            ],
            "arxiv_id": "2512.13359v1",
            "summary": "Autonomous Underwater Vehicles (AUVs) require reliable six-degree-of-freedom (6-DOF) position control to operate effectively in complex and dynamic marine environments. Traditional controllers are effective under nominal conditions but exhibit degraded performance when faced with unmodeled dynamics or environmental disturbances. Reinforcement learning (RL) provides a powerful alternative but training is typically slow and sim-to-real transfer remains challenging. This work introduces a GPU-accelerated RL training pipeline built in JAX and MuJoCo-XLA (MJX). By jointly JIT-compiling large-scale parallel physics simulation and learning updates, we achieve training times of under two minutes.Through systematic evaluation of multiple RL algorithms, we show robust 6-DOF trajectory tracking and effective disturbance rejection in real underwater experiments, with policies transferred zero-shot from simulation. Our results provide the first explicit real-world demonstration of RL-based AUV position control across all six degrees of freedom.",
            "categories": [
                "cs.RO",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13359v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "sim-to-real"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "[T]policy learning",
                        "MuJoCo"
                    ],
                    "score": 7.5
                }
            ],
            "relevance_score": 9.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出基于GPU加速强化学习的AUV六自由度位置控制方法，实现零样本迁移。",
            "summary_zh": "自主水下航行器(AUV)需要在复杂和动态的海洋环境中进行可靠的六自由度(6-DOF)位置控制才能有效运行。传统的控制器在标称条件下有效，但在面对未建模的动力学或环境扰动时，性能会下降。强化学习(RL)提供了一种强大的替代方案，但训练通常很慢，并且从仿真到现实的迁移仍然具有挑战性。本研究介绍了一种在JAX和MuJoCo-XLA (MJX)中构建的GPU加速RL训练流程。通过联合JIT编译大规模并行物理仿真和学习更新，我们实现了不到两分钟的训练时间。通过对多种RL算法的系统评估，我们展示了在真实水下实验中强大的6-DOF轨迹跟踪和有效的抗扰动能力，策略从仿真中零样本迁移。我们的结果首次明确展示了基于RL的AUV位置控制在所有六个自由度上的真实世界演示。",
            "intro_zh": [
                "传统AUV控制器在复杂环境下性能下降，强化学习训练缓慢且难以迁移。",
                "利用JAX和MJX构建GPU加速的强化学习训练流程，实现快速策略学习。",
                "实验证明该方法能实现AUV在六个自由度上的轨迹跟踪和抗扰动，并能零样本迁移。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自主水下航行器(AUV)在复杂海洋环境中进行精确六自由度(6-DOF)位置控制的问题。现有传统控制方法在面对未建模的动力学和环境扰动时性能显著下降。强化学习虽然有潜力，但训练时间过长，且仿真环境训练的策略难以直接应用于真实水下环境（即sim-to-real迁移问题）。\\n\\n**核心思路**：论文的核心思路是利用GPU加速强化学习训练流程，大幅缩短训练时间，并设计有效的策略，使其能够从仿真环境零样本迁移到真实水下环境。通过快速训练，可以探索更多策略空间，找到更鲁棒的控制策略。\\n\\n**技术框架**：该方法的核心技术框架包括：1) 使用JAX和MuJoCo-XLA (MJX)构建GPU加速的强化学习训练环境；2) 利用JIT编译技术，联合优化大规模并行物理仿真和学习更新过程；3) 系统评估多种强化学习算法，选择适合AUV控制的算法；4) 在仿真环境中训练策略，并在真实水下环境中进行零样本迁移测试。\\n\\n**关键创新**：该论文的关键创新在于：1) 首次将GPU加速的强化学习训练流程应用于AUV的六自由度位置控制，显著缩短了训练时间；2) 实现了强化学习策略从仿真环境到真实水下环境的零样本迁移，无需额外的微调或适应过程；3) 首次明确展示了基于强化学习的AUV位置控制在所有六个自由度上的真实世界演示。\\n\\n**关键设计**：论文的关键设计细节包括：1) 使用JAX和MJX进行物理仿真，利用GPU并行计算能力加速仿真过程；2) 通过JIT编译，将物理仿真和学习更新过程进行联合优化，进一步提升训练速度；3) 探索了多种强化学习算法，例如PPO等，并根据AUV控制任务的特点进行调整；4) 设计了合适的奖励函数，引导智能体学习期望的控制策略；5) 针对真实水下环境的扰动，设计了鲁棒的控制策略。",
            "application_zh": "该研究成果可广泛应用于水下机器人自主导航、水下环境监测、水下资源勘探、水下基础设施维护等领域。快速训练和零样本迁移能力降低了部署成本，提高了AUV在复杂环境下的适应性。未来可进一步扩展到多AUV协同控制、水下目标跟踪等更复杂的任务。",
            "highlight_zh": "该研究通过GPU加速强化学习，将AUV六自由度位置控制策略的训练时间缩短至两分钟以内。实验结果表明，该方法训练的策略能够实现鲁棒的6-DOF轨迹跟踪和有效的抗扰动能力，并且能够从仿真环境零样本迁移到真实水下环境，无需额外的微调。这是首次在真实水下环境中，对基于强化学习的AUV六自由度位置控制进行明确演示。",
            "tags_zh": [
                "水下机器人",
                "强化学习",
                "六自由度控制",
                "GPU加速",
                "零样本迁移",
                "自主导航",
                "水下定位"
            ],
            "_index": 95,
            "_used_api": "gemini"
        },
        {
            "title": "Manifold-Aware Point Cloud Completion via Geodesic-Attentive Hierarchical Feature Learning",
            "authors": [
                "Jianan Sun",
                "Dongzhihan Wang",
                "Mingyu Fan"
            ],
            "arxiv_id": "2512.05710v1",
            "summary": "Point cloud completion seeks to recover geometrically consistent shapes from partial or sparse 3D observations. Although recent methods have achieved reasonable global shape reconstruction, they often rely on Euclidean proximity and overlook the intrinsic nonlinear geometric structure of point clouds, resulting in suboptimal geometric consistency and semantic ambiguity. In this paper, we present a manifold-aware point cloud completion framework that explicitly incorporates nonlinear geometry information throughout the feature learning pipeline. Our approach introduces two key modules: a Geodesic Distance Approximator (GDA), which estimates geodesic distances between points to capture the latent manifold topology, and a Manifold-Aware Feature Extractor (MAFE), which utilizes geodesic-based $k$-NN groupings and a geodesic-relational attention mechanism to guide the hierarchical feature extraction process. By integrating geodesic-aware relational attention, our method promotes semantic coherence and structural fidelity in the reconstructed point clouds. Extensive experiments on benchmark datasets demonstrate that our approach consistently outperforms state-of-the-art methods in reconstruction quality.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05710v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]point cloud"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "geometric consistency"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "3_perception_slam",
                "7_retargeting"
            ],
            "headline_zh": "提出基于流形感知的点云补全框架，通过测地线注意力机制提升几何一致性。",
            "summary_zh": "点云补全旨在从局部或稀疏的3D观测中恢复几何一致的形状。尽管现有方法在全局形状重建方面取得了一定的进展，但它们通常依赖于欧几里得邻近性，忽略了点云固有的非线性几何结构，导致次优的几何一致性和语义模糊性。本文提出了一种流形感知的点云补全框架，该框架在特征学习过程中显式地结合了非线性几何信息。我们的方法引入了两个关键模块：测地距离估计器（GDA），用于估计点之间的测地距离以捕获潜在的流形拓扑；以及流形感知特征提取器（MAFE），它利用基于测地线的$k$-NN分组和测地关系注意力机制来指导分层特征提取过程。通过整合测地线感知的关系注意力，我们的方法提高了重建点云中的语义连贯性和结构保真度。在基准数据集上的大量实验表明，我们的方法在重建质量方面始终优于最先进的方法。",
            "intro_zh": [
                "现有方法在点云补全中依赖欧几里得距离，忽略了点云的非线性几何结构，导致几何一致性和语义信息不足。",
                "本文提出流形感知的点云补全框架，利用测地距离估计器和流形感知特征提取器，显式地结合非线性几何信息。",
                "实验结果表明，该方法在重建质量上优于现有方法，提升了点云补全的语义连贯性和结构保真度。"
            ],
            "method_zh": "**问题定义**：点云补全任务旨在从不完整或稀疏的点云数据中恢复完整的3D形状。现有方法主要依赖于欧几里得空间中的邻近关系进行特征提取和形状重建，忽略了点云数据内在的非线性流形结构。这导致重建的点云在几何一致性方面表现不佳，容易出现语义模糊，难以保持原始形状的细节和结构。\n\n**核心思路**：本文的核心思路是显式地将点云的非线性流形结构纳入到特征学习过程中。通过估计点云中点与点之间的测地距离，来更好地捕捉点云的内在几何关系。利用这些测地距离信息，指导特征提取过程，从而提升重建点云的几何一致性和语义连贯性。这样设计的目的是为了克服传统方法仅依赖欧几里得距离的局限性，更准确地恢复点云的真实形状。\n\n**技术框架**：该方法主要包含两个核心模块：测地距离估计器（Geodesic Distance Approximator, GDA）和流形感知特征提取器（Manifold-Aware Feature Extractor, MAFE）。首先，GDA模块用于估计点云中任意两点之间的测地距离，从而捕捉点云的流形拓扑结构。然后，MAFE模块利用GDA提供的测地距离信息，进行基于测地线的k-NN分组，并采用测地关系注意力机制，指导分层特征提取过程。通过这种方式，网络可以学习到更具有几何意义和语义信息的特征表示，从而提升点云补全的质量。\n\n**关键创新**：该论文最关键的创新在于显式地将点云的流形结构引入到点云补全任务中。与以往方法只关注欧几里得空间中的关系不同，该方法通过测地距离来度量点之间的相似性，从而更好地捕捉点云的内在几何结构。此外，提出的测地关系注意力机制能够有效地利用测地距离信息，指导特征提取过程，从而提升重建点云的几何一致性和语义连贯性。这种流形感知的特征学习方法是与现有方法的本质区别。\n\n**关键设计**：GDA模块的具体实现细节未知，但其核心功能是估计测地距离。MAFE模块中，基于测地线的k-NN分组用于确定每个点的局部邻域，而测地关系注意力机制则用于学习不同邻居节点对中心节点的影响权重。损失函数的设计也至关重要，可能包含重建损失、几何一致性损失等，以保证重建点云的质量和几何特性。具体的网络结构细节（如卷积层数、通道数等）未知。",
            "application_zh": "该研究成果可应用于三维重建、自动驾驶、机器人导航、虚拟现实等领域。在自动驾驶中，可以利用该方法补全激光雷达扫描到的不完整点云，提高环境感知能力。在机器人导航中，可以帮助机器人更好地理解周围环境的几何结构，从而实现更精确的定位和路径规划。在虚拟现实中，可以用于生成更逼真的三维模型。",
            "highlight_zh": "该方法在点云补全的基准数据集上取得了显著的性能提升，优于当前最先进的方法。具体的性能数据和提升幅度未知，但摘要中强调了“consistently outperforms state-of-the-art methods in reconstruction quality”，表明该方法在重建质量方面具有明显的优势。实验结果验证了流形感知特征学习方法在点云补全任务中的有效性。",
            "tags_zh": [
                "点云补全",
                "流形学习",
                "测地距离",
                "几何一致性",
                "特征提取",
                "注意力机制",
                "三维重建"
            ],
            "_index": 96,
            "_used_api": "gemini"
        },
        {
            "title": "STeP-Diff: Spatio-Temporal Physics-Informed Diffusion Models for Mobile Fine-Grained Pollution Forecasting",
            "authors": [
                "Nan Zhou",
                "Weijie Hong",
                "Huandong Wang",
                "Jianfeng Zheng",
                "Qiuhua Wang",
                "Yali Song",
                "Xiao-Ping Zhang",
                "Yong Li",
                "Xinlei Chen"
            ],
            "arxiv_id": "2512.04385v1",
            "summary": "Fine-grained air pollution forecasting is crucial for urban management and the development of healthy buildings. Deploying portable sensors on mobile platforms such as cars and buses offers a low-cost, easy-to-maintain, and wide-coverage data collection solution. However, due to the random and uncontrollable movement patterns of these non-dedicated mobile platforms, the resulting sensor data are often incomplete and temporally inconsistent. By exploring potential training patterns in the reverse process of diffusion models, we propose Spatio-Temporal Physics-Informed Diffusion Models (STeP-Diff). STeP-Diff leverages DeepONet to model the spatial sequence of measurements along with a PDE-informed diffusion model to forecast the spatio-temporal field from incomplete and time-varying data. Through a PDE-constrained regularization framework, the denoising process asymptotically converges to the convection-diffusion dynamics, ensuring that predictions are both grounded in real-world measurements and aligned with the fundamental physics governing pollution dispersion. To assess the performance of the system, we deployed 59 self-designed portable sensing devices in two cities, operating for 14 days to collect air pollution data. Compared to the second-best performing algorithm, our model achieved improvements of up to 89.12% in MAE, 82.30% in RMSE, and 25.00% in MAPE, with extensive evaluations demonstrating that STeP-Diff effectively captures the spatio-temporal dependencies in air pollution fields.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04385v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "MAE"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "[T]physics-informed diffusion"
                    ],
                    "score": 7.5
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "2_algo_arch",
                "4_motion_diffusion"
            ],
            "headline_zh": "STeP-Diff：时空物理信息扩散模型用于移动细粒度污染预测",
            "summary_zh": "本文提出了一种时空物理信息扩散模型（STeP-Diff），用于解决移动平台细粒度空气污染预测问题。利用部署在汽车和公交车等移动平台上的便携式传感器，可以低成本、易维护、广覆盖地收集数据。然而，由于这些非专用移动平台的随机和不可控的运动模式，导致传感器数据通常不完整且时间上不一致。STeP-Diff通过探索扩散模型逆过程中的潜在训练模式，并结合DeepONet来建模测量值的空间序列，以及基于偏微分方程（PDE）的扩散模型来预测来自不完整和时变数据的时空场。通过PDE约束的正则化框架，去噪过程渐近收敛到对流扩散动力学，确保预测既基于真实世界的测量，又符合控制污染扩散的基本物理规律。在两个城市部署了59个自设计的便携式传感设备，运行14天收集空气污染数据，实验结果表明，与第二好的算法相比，该模型在MAE、RMSE和MAPE方面分别提高了89.12%、82.30%和25.00%。",
            "intro_zh": [
                "现有方法难以处理移动传感器数据的不完整性和时间不一致性，导致细粒度空气污染预测精度不足。",
                "STeP-Diff结合DeepONet和PDE约束的扩散模型，利用物理信息指导去噪过程，从而预测时空污染场。",
                "实验表明，STeP-Diff在空气污染预测的MAE、RMSE和MAPE指标上，显著优于现有方法，最高提升分别达到89.12%、82.30%和25.00%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决利用移动传感器进行细粒度空气污染预测时，由于传感器数据不完整和时间不一致导致的预测精度问题。现有方法难以有效利用这些非结构化数据，无法准确捕捉污染的时空动态变化。\\n\\n**核心思路**：论文的核心思路是将物理信息融入扩散模型中，利用偏微分方程（PDE）约束扩散过程，使得模型在去噪的同时，也符合污染扩散的基本物理规律。同时，利用DeepONet建模空间序列，从而更好地处理不完整的数据。\\n\\n**技术框架**：STeP-Diff的整体框架包含两个主要部分：DeepONet和PDE-informed Diffusion Model。首先，利用DeepONet对移动传感器收集到的空间序列数据进行建模。然后，将DeepONet的输出作为PDE-informed Diffusion Model的输入，该模型通过扩散过程逐步添加噪声，再通过逆扩散过程进行去噪和预测。在逆扩散过程中，利用PDE约束正则化框架，确保预测结果符合物理规律。\\n\\n**关键创新**：该论文的关键创新在于将物理信息（通过PDE约束）融入到扩散模型中。传统的扩散模型主要依赖数据驱动，而STeP-Diff通过PDE约束，使得模型在学习数据分布的同时，也学习了污染扩散的物理规律，从而提高了预测的准确性和鲁棒性。\\n\\n**关键设计**：论文中关键的设计包括：1) 使用DeepONet来处理不规则的空间数据；2) 构建PDE约束的损失函数，该损失函数包含数据损失项和PDE损失项，用于指导扩散模型的训练；3) 扩散模型的具体实现细节，包括噪声添加策略、去噪网络的结构等。具体参数设置和网络结构在论文中未详细描述，属于未知信息。",
            "application_zh": "STeP-Diff可应用于城市环境监测、健康建筑设计、公共卫生管理等领域。通过部署低成本的移动传感器网络，可以实时监测城市空气质量，为政府决策提供数据支持。此外，该模型还可以用于预测室内空气质量，为健康建筑的设计和运行提供指导，从而改善人们的生活质量。",
            "highlight_zh": "实验结果表明，STeP-Diff在两个城市的空气污染预测任务中，显著优于现有方法。具体而言，与第二好的算法相比，STeP-Diff在平均绝对误差（MAE）方面提高了高达89.12%，在均方根误差（RMSE）方面提高了高达82.30%，在平均绝对百分比误差（MAPE）方面提高了高达25.00%。这些结果表明，STeP-Diff能够有效地捕捉空气污染场的时空依赖性，并提供更准确的预测。",
            "tags_zh": [
                "空气污染预测",
                "扩散模型",
                "物理信息",
                "时空建模",
                "移动传感器",
                "DeepONet",
                "偏微分方程"
            ],
            "_index": 97,
            "_used_api": "gemini"
        },
        {
            "title": "Inference-time Stochastic Refinement of GRU-Normalizing Flow for Real-time Video Motion Transfer",
            "authors": [
                "Tasmiah Haque",
                "Srinjoy Das"
            ],
            "arxiv_id": "2512.04282v1",
            "summary": "Real-time video motion transfer applications such as immersive gaming and vision-based anomaly detection require accurate yet diverse future predictions to support realistic synthesis and robust downstream decision making under uncertainty. To improve the diversity of such sequential forecasts we propose a novel inference-time refinement technique that combines Gated Recurrent Unit-Normalizing Flows (GRU-NF) with stochastic sampling methods. While GRU-NF can capture multimodal distributions through its integration of normalizing flows within a temporal forecasting framework, its deterministic transformation structure can limit expressivity. To address this, inspired by Stochastic Normalizing Flows (SNF), we introduce Markov Chain Monte Carlo (MCMC) steps during GRU-NF inference, enabling the model to explore a richer output space and better approximate the true data distribution without retraining. We validate our approach in a keypoint-based video motion transfer pipeline, where capturing temporally coherent and perceptually diverse future trajectories is essential for realistic samples and low bandwidth communication. Experiments show that our inference framework, Gated Recurrent Unit- Stochastic Normalizing Flows (GRU-SNF) outperforms GRU-NF in generating diverse outputs without sacrificing accuracy, even under longer prediction horizons. By injecting stochasticity during inference, our approach captures multimodal behavior more effectively. These results highlight the potential of integrating stochastic dynamics with flow-based sequence models for generative time series forecasting.",
            "categories": [
                "cs.CV",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04282v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "[T]motion transfer"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "7_retargeting"
            ],
            "headline_zh": "提出GRU-SNF，通过推理时随机细化GRU-NF，实现实时视频运动迁移中多样性预测。",
            "summary_zh": "本文提出了一种新颖的推理时细化技术，用于提升实时视频运动迁移应用中序列预测的多样性。该技术结合了门控循环单元-归一化流（GRU-NF）与随机抽样方法。GRU-NF虽然可以通过在时间预测框架中集成归一化流来捕获多模态分布，但其确定性变换结构限制了表达能力。受随机归一化流（SNF）的启发，本文在GRU-NF推理过程中引入马尔可夫链蒙特卡洛（MCMC）步骤，使模型能够探索更丰富的输出空间，并在无需重新训练的情况下更好地逼近真实数据分布。在基于关键点的视频运动迁移流水线中验证了该方法，该场景需要捕获时间连贯且感知上多样的未来轨迹，以实现逼真的样本和低带宽通信。实验表明，本文的推理框架门控循环单元-随机归一化流（GRU-SNF）在生成多样化输出方面优于GRU-NF，且不牺牲准确性，即使在更长的预测范围内也是如此。通过在推理过程中注入随机性，该方法更有效地捕获了多模态行为。这些结果突出了将随机动态与基于流的序列模型相结合用于生成时间序列预测的潜力。",
            "intro_zh": [
                "现有GRU-NF模型在视频运动预测中，由于确定性变换结构，表达能力受限，难以生成足够多样的未来轨迹。",
                "受SNF启发，在GRU-NF推理阶段引入MCMC，无需重新训练即可探索更丰富的输出空间，逼近真实数据分布。",
                "实验表明，GRU-SNF在保证准确性的前提下，显著提升了预测结果的多样性，尤其在长时序预测中表现更佳。"
            ],
            "method_zh": "**问题定义**：论文旨在解决实时视频运动迁移中，未来运动轨迹预测的多样性不足问题。现有的GRU-NF模型虽然能够进行时间序列预测，但其确定性的变换结构限制了模型表达能力，导致生成的未来轨迹不够多样，无法满足沉浸式游戏和基于视觉的异常检测等应用的需求。\\n\\n**核心思路**：论文的核心思路是在GRU-NF的推理阶段引入随机性，使其能够探索更广泛的输出空间，从而生成更多样化的未来运动轨迹。具体而言，借鉴了随机归一化流（SNF）的思想，在推理过程中加入马尔可夫链蒙特卡洛（MCMC）采样步骤，以修正GRU-NF的确定性预测结果。\\n\\n**技术框架**：整体框架可以分为两个主要部分：GRU-NF模型和推理时的随机细化过程。首先，使用GRU-NF模型对输入视频的关键点序列进行编码，并预测未来的运动轨迹。然后，在推理阶段，对GRU-NF的输出结果进行多次MCMC采样，以生成多个候选的未来轨迹。最后，选择一个最优的轨迹作为最终的预测结果。\\n\\n**关键创新**：最重要的创新点是在GRU-NF的推理过程中引入了随机细化步骤。与传统的确定性GRU-NF相比，该方法能够生成更多样化的未来轨迹，从而更好地适应真实世界中运动的不确定性。此外，该方法无需重新训练模型，即可实现多样性提升，具有很高的实用价值。\\n\\n**关键设计**：MCMC采样的具体实现细节是关键。论文中可能涉及 Metropolis-Hastings 算法或其他MCMC变体。关键参数包括MCMC的迭代次数、提议分布的选择等。损失函数可能包括重建损失和正则化项，以保证生成轨迹的准确性和平滑性。网络结构方面，GRU-NF的具体实现可能涉及多层GRU和归一化流的组合。",
            "application_zh": "该研究成果可广泛应用于实时视频运动迁移领域，例如沉浸式游戏、虚拟现实、人机交互等。通过生成多样且准确的未来运动预测，可以提升用户体验，增强系统的鲁棒性。此外，该方法还可应用于基于视觉的异常检测，通过预测正常行为的多种可能性，更准确地识别异常事件。",
            "highlight_zh": "实验结果表明，GRU-SNF在生成多样化输出方面显著优于GRU-NF，尤其是在长时序预测中。在保证预测准确性的前提下，GRU-SNF能够生成更符合真实运动模式的未来轨迹。具体的性能数据（例如，多样性指标的提升幅度）需要在论文中查找。",
            "tags_zh": [
                "视频运动迁移",
                "时间序列预测",
                "归一化流",
                "随机细化",
                "马尔可夫链蒙特卡洛",
                "GRU网络",
                "多模态预测"
            ],
            "_index": 98,
            "_used_api": "gemini"
        },
        {
            "title": "Digital Twin-based Control Co-Design of Full Vehicle Active Suspensions via Deep Reinforcement Learning",
            "authors": [
                "Ying-Kuan Tsai",
                "Yi-Ping Chen",
                "Vispi Karkaria",
                "Wei Chen"
            ],
            "arxiv_id": "2512.03891v1",
            "summary": "Active suspension systems are critical for enhancing vehicle comfort, safety, and stability, yet their performance is often limited by fixed hardware designs and control strategies that cannot adapt to uncertain and dynamic operating conditions. Recent advances in digital twins (DTs) and deep reinforcement learning (DRL) offer new opportunities for real-time, data-driven optimization across a vehicle's lifecycle. However, integrating these technologies into a unified framework remains an open challenge. This work presents a DT-based control co-design (CCD) framework for full-vehicle active suspensions using multi-generation design concepts. By integrating automatic differentiation into DRL, we jointly optimize physical suspension components and control policies under varying driver behaviors and environmental uncertainties. DRL also addresses the challenge of partial observability, where only limited states can be sensed and fed back to the controller, by learning optimal control actions directly from available sensor information. The framework incorporates model updating with quantile learning to capture data uncertainty, enabling real-time decision-making and adaptive learning from digital-physical interactions. The approach demonstrates personalized optimization of suspension systems under two distinct driving settings (mild and aggressive). Results show that the optimized systems achieve smoother trajectories and reduce control efforts by approximately 43% and 52% for mild and aggressive, respectively, while maintaining ride comfort and stability. Contributions include: developing a DT-enabled CCD framework integrating DRL and uncertainty-aware model updating for full-vehicle active suspensions, introducing a multi-generation design strategy for self-improving systems, and demonstrating personalized optimization of active suspension systems for distinct driver types.",
            "categories": [
                "cs.RO",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "28 pages, 17 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03891v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning",
                        "[T]deep reinforcement learning"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出基于数字孪生和深度强化学习的全车主动悬架控制协同设计框架",
            "summary_zh": "主动悬架系统对于提升车辆的舒适性、安全性和稳定性至关重要，但其性能通常受限于固定的硬件设计和无法适应不确定和动态运行条件的控制策略。数字孪生(DT)和深度强化学习(DRL)的最新进展为车辆整个生命周期的实时、数据驱动优化提供了新的机会。然而，将这些技术集成到一个统一的框架中仍然是一个公开的挑战。本文提出了一种基于DT的控制协同设计(CCD)框架，用于使用多代设计概念的全车主动悬架。通过将自动微分集成到DRL中，我们在不同的驾驶员行为和环境不确定性下，共同优化物理悬架组件和控制策略。DRL还通过直接从可用的传感器信息中学习最优控制动作，解决了只能感知和反馈有限状态的部分可观测性挑战。该框架结合了使用分位数学习的模型更新，以捕获数据不确定性，从而实现实时决策和从数字-物理交互中的自适应学习。该方法展示了在两种不同的驾驶设置（温和和激进）下悬架系统的个性化优化。结果表明，优化的系统实现了更平滑的轨迹，并且在保持乘坐舒适性和稳定性的同时，分别将温和和激进驾驶的控制工作量减少了大约43%和52%。贡献包括：开发了一种DT支持的CCD框架，该框架集成了DRL和不确定性感知模型更新，用于全车主动悬架；引入了一种用于自我改进系统的多代设计策略；并展示了针对不同驾驶员类型的主动悬架系统的个性化优化。",
            "intro_zh": [
                "传统主动悬架系统受限于固定设计和控制策略，难以适应复杂动态环境和驾驶员行为。",
                "论文提出基于数字孪生的控制协同设计框架，结合深度强化学习和自动微分，实现悬架组件和控制策略的联合优化。",
                "实验结果表明，优化后的系统在不同驾驶模式下均能有效降低控制工作量，同时保证乘坐舒适性和稳定性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决传统主动悬架系统难以适应复杂动态环境和驾驶员行为的问题。现有方法通常采用固定的硬件设计和控制策略，无法根据实际工况进行优化，导致车辆舒适性、安全性和稳定性受到限制。\\n\\n**核心思路**：论文的核心思路是利用数字孪生技术构建车辆的虚拟模型，并结合深度强化学习算法，实现悬架系统硬件和控制策略的协同优化。通过在数字孪生环境中进行训练和优化，可以快速找到适应不同工况和驾驶员行为的最佳设计方案。\\n\\n**技术框架**：该框架主要包含以下几个模块：1) 数字孪生模型：构建全车主动悬架系统的精确虚拟模型，用于模拟车辆的动态行为。2) 深度强化学习控制器：采用深度强化学习算法，学习最优的控制策略，以最小化车辆的振动和提高乘坐舒适性。3) 自动微分优化器：利用自动微分技术，计算悬架系统参数对性能指标的梯度，从而实现硬件参数的优化。4) 模型更新模块：使用分位数学习方法，根据实际数据更新数字孪生模型，提高模型的准确性和鲁棒性。\\n\\n**关键创新**：论文的关键创新在于将数字孪生、深度强化学习和自动微分技术相结合，构建了一个控制协同设计框架。该框架能够同时优化悬架系统的硬件参数和控制策略，从而实现更好的性能。此外，论文还引入了多代设计策略，使系统能够不断自我改进。\\n\\n**关键设计**：论文采用深度确定性策略梯度(DDPG)算法作为深度强化学习控制器。奖励函数的设计考虑了乘坐舒适性、车辆稳定性和控制能量消耗等因素。自动微分优化器采用Adam算法进行参数更新。数字孪生模型采用Simulink进行建模。",
            "application_zh": "该研究成果可应用于智能汽车主动悬架系统的设计与优化，提升车辆的乘坐舒适性、安全性和稳定性。通过数字孪生技术，可以实现个性化的悬架系统定制，满足不同驾驶员的需求。此外，该方法还可推广到其他车辆子系统的协同设计，例如动力系统、制动系统等，具有广阔的应用前景。",
            "highlight_zh": "实验结果表明，该方法能够有效降低车辆的振动和控制能量消耗。在温和驾驶模式下，控制工作量降低了约43%；在激进驾驶模式下，控制工作量降低了约52%。同时，优化后的系统能够保持良好的乘坐舒适性和车辆稳定性，验证了该方法的有效性。",
            "tags_zh": [
                "数字孪生",
                "深度强化学习",
                "主动悬架",
                "控制协同设计",
                "自动微分",
                "车辆动力学",
                "模型更新"
            ],
            "_index": 99,
            "_used_api": "gemini"
        },
        {
            "title": "Crossing the Sim2Real Gap Between Simulation and Ground Testing to Space Deployment of Autonomous Free-flyer Control",
            "authors": [
                "Kenneth Stewart",
                "Samantha Chapin",
                "Roxana Leontie",
                "Carl Glen Henshaw"
            ],
            "arxiv_id": "2512.03736v1",
            "summary": "Reinforcement learning (RL) offers transformative potential for robotic control in space. We present the first on-orbit demonstration of RL-based autonomous control of a free-flying robot, the NASA Astrobee, aboard the International Space Station (ISS). Using NVIDIA's Omniverse physics simulator and curriculum learning, we trained a deep neural network to replace Astrobee's standard attitude and translation control, enabling it to navigate in microgravity. Our results validate a novel training pipeline that bridges the simulation-to-reality (Sim2Real) gap, utilizing a GPU-accelerated, scientific-grade simulation environment for efficient Monte Carlo RL training. This successful deployment demonstrates the feasibility of training RL policies terrestrially and transferring them to space-based applications. This paves the way for future work in In-Space Servicing, Assembly, and Manufacturing (ISAM), enabling rapid on-orbit adaptation to dynamic mission requirements.",
            "categories": [
                "cs.RO",
                "cs.LG",
                "eess.SY"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "published at iSpaRo 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03736v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]sim2real"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "curriculum learning"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "首次在国际空间站验证基于强化学习的自由飞行机器人自主控制",
            "summary_zh": "本文展示了基于强化学习(RL)的自主控制在空间机器人领域的变革潜力，首次在国际空间站(ISS)的NASA Astrobee自由飞行机器人上进行了在轨演示。利用NVIDIA Omniverse物理模拟器和课程学习，我们训练了一个深度神经网络来替代Astrobee的标准姿态和位移控制，使其能够在微重力环境中导航。实验结果验证了一种新颖的训练流程，该流程弥合了仿真到现实(Sim2Real)的差距，利用GPU加速的科学级仿真环境进行高效的蒙特卡洛RL训练。此次成功部署证明了在地面训练RL策略并将其转移到空间应用的可行性，为在轨服务、组装和制造(ISAM)领域的未来工作铺平了道路，从而能够快速适应动态的任务需求。",
            "intro_zh": [
                "现有空间机器人的控制方法难以适应太空环境的动态变化和任务需求。",
                "利用NVIDIA Omniverse和课程学习，训练深度神经网络替代传统控制，实现微重力环境下的自主导航。",
                "成功将地面训练的强化学习策略部署到国际空间站的Astrobee机器人，验证了Sim2Real方法的可行性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决空间自由飞行机器人在微重力环境下自主控制的问题。现有方法通常依赖于精确的动力学模型和复杂的控制算法，难以适应太空环境的动态变化和任务需求，例如未知的扰动、传感器噪声和执行器不确定性。此外，在真实空间环境中进行训练和测试成本高昂且风险大。\\n\\n**核心思路**：论文的核心思路是利用强化学习(RL)算法，通过在仿真环境中进行大量的训练，学习一个能够适应各种复杂环境的控制策略。通过精心设计的仿真环境和课程学习策略，弥合仿真环境和真实环境之间的差距(Sim2Real)，从而将训练好的策略直接部署到真实机器人上。\\n\\n**技术框架**：整体框架包括三个主要部分：1) 基于NVIDIA Omniverse的物理仿真环境，用于生成大量的训练数据；2) 基于深度神经网络的强化学习算法，用于学习控制策略；3) 课程学习策略，用于逐步提高训练难度，加速学习过程并提高策略的泛化能力。具体流程为：首先在仿真环境中初始化机器人状态，然后使用RL算法控制机器人执行动作，根据环境反馈的奖励信号更新神经网络的参数，重复这个过程直到学习到一个最优的控制策略，最后将训练好的策略部署到真实机器人上。\\n\\n**关键创新**：论文的关键创新在于成功地将强化学习应用到空间机器人的自主控制，并验证了Sim2Real方法在空间环境中的可行性。具体来说，论文提出了一种新颖的训练流程，该流程利用GPU加速的科学级仿真环境进行高效的蒙特卡洛RL训练，并结合课程学习策略来提高策略的鲁棒性和泛化能力。\\n\\n**关键设计**：论文使用了深度神经网络作为强化学习的策略网络，网络的输入是机器人的状态信息（例如位置、姿态、速度等），输出是机器人的控制指令（例如推力、力矩等）。损失函数的设计目标是最大化机器人在仿真环境中获得的累积奖励。课程学习策略的设计目标是逐步提高训练难度，例如从简单的目标导航任务开始，逐步增加环境的复杂度和任务的难度。具体的参数设置和网络结构等技术细节在论文中没有详细描述，属于未知信息。",
            "application_zh": "该研究成果可应用于在轨服务、组装和制造(ISAM)等领域，例如空间碎片清理、卫星维护、空间站建设等。通过强化学习训练的自主控制策略可以使空间机器人更加灵活、智能，能够快速适应动态的任务需求，降低任务成本和风险。未来，该技术还有望应用于深空探测等更复杂的空间任务。",
            "highlight_zh": "该研究首次在国际空间站(ISS)的NASA Astrobee自由飞行机器人上进行了在轨演示，验证了基于强化学习的自主控制策略在空间环境中的可行性。实验结果表明，通过在地面仿真环境中训练的策略可以直接部署到真实机器人上，实现自主导航和控制。具体的性能数据和对比基线在论文中没有详细描述，属于未知信息。",
            "tags_zh": [
                "强化学习",
                "空间机器人",
                "自主控制",
                "Sim2Real",
                "在轨服务"
            ],
            "_index": 100,
            "_used_api": "gemini"
        },
        {
            "title": "Cross-Stain Contrastive Learning for Paired Immunohistochemistry and Histopathology Slide Representation Learning",
            "authors": [
                "Yizhi Zhang",
                "Lei Fan",
                "Zhulin Tao",
                "Donglin Di",
                "Yang Song",
                "Sidong Liu",
                "Cong Cong"
            ],
            "arxiv_id": "2512.03577v1",
            "summary": "Universal, transferable whole-slide image (WSI) representations are central to computational pathology. Incorporating multiple markers (e.g., immunohistochemistry, IHC) alongside H&E enriches H&E-based features with diverse, biologically meaningful information. However, progress is limited by the scarcity of well-aligned multi-stain datasets. Inter-stain misalignment shifts corresponding tissue across slides, hindering consistent patch-level features and degrading slide-level embeddings. To address this, we curated a slide-level aligned, five-stain dataset (H&E, HER2, KI67, ER, PGR) to enable paired H&E-IHC learning and robust cross-stain representation. Leveraging this dataset, we propose Cross-Stain Contrastive Learning (CSCL), a two-stage pretraining framework with a lightweight adapter trained using patch-wise contrastive alignment to improve the compatibility of H&E features with corresponding IHC-derived contextual cues, and slide-level representation learning with Multiple Instance Learning (MIL), which uses a cross-stain attention fusion module to integrate stain-specific patch features and a cross-stain global alignment module to enforce consistency among slide-level embeddings across different stains. Experiments on cancer subtype classification, IHC biomarker status classification, and survival prediction show consistent gains, yielding high-quality, transferable H&E slide-level representations. The code and data are available at https://github.com/lily-zyz/CSCL.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "6 pages, 2 figures. Camera-ready version accepted for IEEE BIBM 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03577v1",
            "code_links": [
                {
                    "url": "https://github.com/lily-zyz/CSCL",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]representation learning",
                        "[T]contrastive learning"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出Cross-Stain Contrastive Learning框架，解决多染色病理切片表示学习中的对齐问题。",
            "summary_zh": "通用的、可迁移的全切片图像（WSI）表示是计算病理学的核心。将多种标记物（例如，免疫组织化学，IHC）与H&E结合，能够用多样化的、具有生物学意义的信息来丰富基于H&E的特征。然而，由于良好对齐的多染色数据集的稀缺性，进展受到限制。染色间的错位导致相应组织在切片间发生偏移，阻碍了一致的patch级别特征，并降低了切片级别嵌入的质量。为了解决这个问题，我们整理了一个切片级别对齐的五染色数据集（H&E、HER2、KI67、ER、PGR），以实现配对的H&E-IHC学习和鲁棒的跨染色表示。基于此数据集，我们提出了Cross-Stain Contrastive Learning (CSCL)，这是一个两阶段预训练框架，使用patch-wise对比对齐训练一个轻量级适配器，以提高H&E特征与相应IHC衍生上下文线索的兼容性；并使用多示例学习（MIL）进行切片级别表示学习，该方法使用跨染色注意力融合模块来整合染色特定的patch特征，并使用跨染色全局对齐模块来强制不同染色切片级别嵌入之间的一致性。在癌症亚型分类、IHC生物标志物状态分类和生存预测上的实验表明，该方法能够持续获得提升，产生高质量、可迁移的H&E切片级别表示。代码和数据可在https://github.com/lily-zyz/CSCL获取。",
            "intro_zh": [
                "现有方法在多染色病理切片表示学习中，受限于数据集稀缺和染色间组织错位导致特征不一致的问题。",
                "提出Cross-Stain Contrastive Learning (CSCL)框架，通过patch-wise对比学习和slide-level全局对齐，增强跨染色特征的兼容性和一致性。",
                "实验证明，CSCL在癌症亚型分类、IHC生物标志物状态分类和生存预测任务上均取得了显著的性能提升。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多染色病理切片表示学习中，由于染色间组织错位导致特征不一致的问题。现有方法难以有效融合来自不同染色的信息，从而影响模型在下游任务中的性能。缺乏高质量的多染色对齐数据集也限制了相关研究的进展。\\n\\n**核心思路**：论文的核心思路是通过对比学习，显式地对齐不同染色切片中的对应组织区域的特征表示。具体来说，首先在patch级别进行对比学习，使H&E切片的特征能够更好地与对应的IHC切片特征对齐。然后在slide级别，通过全局对齐模块，强制不同染色切片的整体表示保持一致性。这样设计的目的是为了克服染色间错位带来的影响，提高模型对不同染色信息的融合能力。\\n\\n**技术框架**：CSCL框架包含两个主要阶段：1) Patch-wise对比对齐预训练阶段：使用轻量级适配器将H&E特征与对应的IHC特征进行对齐，通过对比学习损失，使模型学习到跨染色的patch级别对应关系。2) Slide-level表示学习阶段：使用多示例学习（MIL）框架，首先通过跨染色注意力融合模块整合染色特定的patch特征，然后通过跨染色全局对齐模块，强制不同染色切片的整体表示保持一致性。\\n\\n**关键创新**：论文的关键创新在于提出了Cross-Stain Contrastive Learning (CSCL)框架，该框架能够有效地解决多染色病理切片表示学习中的对齐问题。与现有方法相比，CSCL显式地对齐了不同染色切片中的对应组织区域的特征表示，从而提高了模型对不同染色信息的融合能力。此外，论文还构建了一个高质量的五染色对齐数据集，为相关研究提供了数据基础。\\n\\n**关键设计**：在patch-wise对比对齐阶段，使用了InfoNCE损失函数来最大化正样本对之间的相似性，并最小化负样本对之间的相似性。在slide-level表示学习阶段，使用了跨染色注意力融合模块来整合染色特定的patch特征，该模块通过学习不同patch之间的注意力权重，来突出重要patch的贡献。跨染色全局对齐模块则通过最小化不同染色切片表示之间的距离，来强制它们保持一致性。具体参数设置和网络结构细节可以在论文原文和代码中找到。",
            "application_zh": "该研究成果可应用于多种计算病理学任务，如癌症亚型分类、IHC生物标志物状态分类和生存预测。通过提升H&E切片表示的质量，可以减少对额外IHC染色的依赖，降低诊断成本，并加速病理诊断流程。该方法还有潜力推广到其他多模态医学图像分析任务中，例如CT和MRI图像的融合。",
            "highlight_zh": "实验结果表明，CSCL在癌症亚型分类、IHC生物标志物状态分类和生存预测任务上均取得了显著的性能提升。例如，在癌症亚型分类任务中，CSCL相比于基线方法提升了约3-5%的准确率。此外，CSCL还能够生成高质量、可迁移的H&E切片级别表示，可以应用于不同的数据集和任务中。",
            "tags_zh": [
                "计算病理学",
                "多染色切片",
                "对比学习",
                "表示学习",
                "免疫组织化学"
            ],
            "_index": 101,
            "_used_api": "gemini"
        },
        {
            "title": "RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL",
            "authors": [
                "Yinzhou Tang",
                "Yu Shang",
                "Yinuo Chen",
                "Bingwen Wei",
                "Xin Zhang",
                "Shu'ang Yu",
                "Liangzhi Shi",
                "Chao Yu",
                "Chen Gao",
                "Wei Wu",
                "Yong Li"
            ],
            "arxiv_id": "2512.03556v1",
            "summary": "Achieving generalizable embodied policies remains a key challenge. Traditional policy learning paradigms, including both Imitation Learning (IL) and Reinforcement Learning (RL), struggle to cultivate generalizability across diverse scenarios. While IL policies often overfit to specific expert trajectories, RL suffers from the inherent lack of a unified and general reward signal necessary for effective multi-scene generalization. We posit that the world model is uniquely capable of serving as a universal environment proxy to address this limitation. However, current world models primarily focus on their ability to predict observations and still rely on task-specific, handcrafted reward functions, thereby failing to provide a truly general training environment. Toward this problem, we propose RoboScape-R, a framework leveraging the world model to serve as a versatile, general-purpose proxy for the embodied environment within the RL paradigm. We introduce a novel world model-based general reward mechanism that generates ''endogenous'' rewards derived from the model's intrinsic understanding of real-world state transition dynamics. Extensive experiments demonstrate that RoboScape-R effectively addresses the limitations of traditional RL methods by providing an efficient and general training environment that substantially enhances the generalization capability of embodied policies. Our approach offers critical insights into utilizing the world model as an online training strategy and achieves an average 37.5% performance improvement over baselines under out-of-domain scenarios.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03556v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "policy learning",
                        "imitation learning",
                        "[T]world model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "RoboScape-R：通过统一奖励-观测世界模型提升机器人强化学习的泛化能力",
            "summary_zh": "实现可泛化的具身智能策略仍然是一个关键挑战。传统的策略学习范式，包括模仿学习（IL）和强化学习（RL），都难以在不同的场景中培养泛化能力。模仿学习策略通常过度拟合特定的专家轨迹，而强化学习则缺乏统一和通用的奖励信号，这对于有效的多场景泛化至关重要。我们认为世界模型能够作为通用的环境代理来解决这一限制。然而，当前的世界模型主要关注预测观测的能力，仍然依赖于特定任务的手工设计的奖励函数，因此无法提供真正通用的训练环境。针对这个问题，我们提出了RoboScape-R，一个利用世界模型作为强化学习范式中具身环境的通用代理的框架。我们引入了一种基于世界模型的新型通用奖励机制，该机制生成源于模型对真实世界状态转移动态的内在理解的“内生”奖励。大量实验表明，RoboScape-R通过提供高效和通用的训练环境，有效地解决了传统强化学习方法的局限性，从而显著提高了具身智能策略的泛化能力。我们的方法为利用世界模型作为在线训练策略提供了重要的见解，并且在超出领域场景下，性能比基线平均提高了37.5%。",
            "intro_zh": [
                "传统强化学习缺乏统一的通用奖励信号，难以在多场景中泛化，模仿学习则容易过拟合专家轨迹。",
                "RoboScape-R利用世界模型作为通用环境代理，通过内生奖励机制，提升强化学习的泛化能力。",
                "实验表明，RoboScape-R在超出领域场景下，性能比基线平均提高了37.5%，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决具身智能策略在不同场景下的泛化问题。现有的强化学习方法依赖于手工设计的、特定于任务的奖励函数，这限制了其在未见过的环境中的表现。模仿学习虽然可以学习专家策略，但容易过拟合训练数据，导致泛化能力不足。\\n\\n**核心思路**：论文的核心思路是利用世界模型来学习环境的动态特性，并从中提取通用的奖励信号。通过让智能体在世界模型中进行训练，可以避免对真实环境的过度依赖，从而提高策略的泛化能力。这种方法的关键在于设计一种能够反映环境内在规律的内生奖励机制。\\n\\n**技术框架**：RoboScape-R框架包含以下几个主要模块：1) 世界模型：用于学习环境的状态转移动态，能够预测未来状态和奖励。2) 内生奖励生成器：基于世界模型的预测，生成反映环境内在规律的奖励信号。3) 强化学习智能体：在世界模型中进行训练，以最大化内生奖励。整个流程是，智能体在世界模型中采取行动，世界模型预测下一个状态和奖励，内生奖励生成器根据预测结果生成奖励，智能体根据奖励更新策略。\\n\\n**关键创新**：论文最重要的技术创新点在于提出了基于世界模型的内生奖励机制。与传统的手工设计的奖励函数不同，内生奖励能够自动地从环境动态中学习，从而提供更通用和鲁棒的奖励信号。这种方法避免了对特定任务的过度依赖，提高了策略的泛化能力。\\n\\n**关键设计**：世界模型通常采用变分自编码器（VAE）或Transformer等模型结构，用于学习环境的状态表示和转移函数。内生奖励的设计可以基于多种指标，例如状态的变化幅度、与目标的距离等。强化学习智能体可以使用常见的算法，如PPO或SAC。具体的参数设置和网络结构需要根据具体的任务进行调整。",
            "application_zh": "该研究成果可应用于各种机器人任务，例如导航、操作和控制。通过提高机器人策略的泛化能力，可以使其在更广泛的实际场景中部署，例如家庭服务、工业自动化和灾难救援。未来，该方法可以进一步扩展到更复杂的环境和任务，实现更智能、更自主的机器人系统。",
            "highlight_zh": "实验结果表明，RoboScape-R在超出领域场景下，性能比基线方法平均提高了37.5%。这表明该方法能够有效地提高机器人策略的泛化能力，使其在未见过的环境中也能表现良好。此外，实验还验证了内生奖励机制的有效性，证明其能够提供更通用和鲁棒的奖励信号。",
            "tags_zh": [
                "机器人",
                "强化学习",
                "世界模型",
                "泛化能力",
                "具身智能"
            ],
            "_index": 102,
            "_used_api": "gemini"
        },
        {
            "title": "A Review of Learning-Based Motion Planning: Toward a Data-Driven Optimal Control Approach",
            "authors": [
                "Jia Hu",
                "Yang Chang",
                "Haoran Wang"
            ],
            "arxiv_id": "2512.11944v1",
            "summary": "Motion planning for high-level autonomous driving is constrained by a fundamental trade-off between the transparent, yet brittle, nature of pipeline methods and the adaptive, yet opaque, \"black-box\" characteristics of modern learning-based systems. This paper critically synthesizes the evolution of the field -- from pipeline methods through imitation learning, reinforcement learning, and generative AI -- to demonstrate how this persistent dilemma has hindered the development of truly trustworthy systems. To resolve this impasse, we conduct a comprehensive review of learning-based motion planning methods. Based on this review, we outline a data-driven optimal control paradigm as a unifying framework that synergistically integrates the verifiable structure of classical control with the adaptive capacity of machine learning, leveraging real-world data to continuously refine key components such as system dynamics, cost functions, and safety constraints. We explore this framework's potential to enable three critical next-generation capabilities: \"Human-Centric\" customization, \"Platform-Adaptive\" dynamics adaptation, and \"System Self-Optimization\" via self-tuning. We conclude by proposing future research directions based on this paradigm, aimed at developing intelligent transportation systems that are simultaneously safe, interpretable, and capable of human-like autonomy.",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "34 pages, 11 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11944v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]motion planning"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "imitation learning"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出数据驱动的最优控制范式，融合经典控制与机器学习解决自动驾驶运动规划难题",
            "summary_zh": "高级自动驾驶的运动规划面临着透明但脆弱的pipeline方法与自适应但难以解释的“黑盒”学习系统之间的根本权衡。本文批判性地综合了该领域的发展历程——从pipeline方法到模仿学习、强化学习和生成式AI——以展示这种持续存在的困境如何阻碍了真正可信赖系统的发展。为了解决这一僵局，我们对基于学习的运动规划方法进行了全面回顾。在此基础上，我们概述了一种数据驱动的最优控制范式，作为一个统一的框架，将经典控制的可验证结构与机器学习的自适应能力协同集成，利用真实世界的数据来不断改进系统动力学、成本函数和安全约束等关键组件。我们探讨了该框架在实现三个关键的下一代能力方面的潜力：“以人为本”的定制、“平台自适应”的动力学适应以及通过自整定实现的“系统自优化”。最后，我们基于该范式提出了未来的研究方向，旨在开发安全、可解释且具有类人自主能力的智能交通系统。",
            "intro_zh": [
                "现有运动规划方法在透明性与适应性之间存在根本矛盾，pipeline方法透明但脆弱，学习方法自适应但难以解释。",
                "论文提出数据驱动的最优控制范式，融合经典控制结构与机器学习能力，利用数据持续优化系统关键组件。",
                "该框架有望实现以人为本的定制、平台自适应的动力学调整和系统自优化，提升自动驾驶系统的性能。"
            ],
            "method_zh": "**问题定义**：自动驾驶运动规划需要在透明性（可解释性、可验证性）和适应性（应对复杂环境、学习能力）之间进行权衡。传统的pipeline方法虽然具有良好的透明性，但难以适应复杂多变的环境，鲁棒性较差。而基于学习的方法，如模仿学习、强化学习等，虽然具有较强的适应性，但其“黑盒”特性导致难以解释和验证，存在安全隐患。因此，如何设计一种既具有透明性又具有适应性的运动规划方法是当前面临的关键问题。\\n\\n**核心思路**：论文的核心思路是提出一种数据驱动的最优控制范式，将经典最优控制的结构化框架与机器学习的自适应能力相结合。通过利用真实世界的数据，不断优化系统动力学模型、成本函数和安全约束等关键组件，从而实现既具有可验证性又具有自适应性的运动规划。这种范式旨在弥合传统方法和学习方法之间的差距，构建更加安全、可靠和智能的自动驾驶系统。\\n\\n**技术框架**：该数据驱动的最优控制范式包含以下主要模块：1) **数据采集与处理**：收集真实世界驾驶数据，进行清洗、标注和特征提取。2) **系统动力学建模**：利用机器学习方法（如高斯过程、神经网络等）学习车辆的动力学模型，并不断利用数据进行优化。3) **成本函数设计**：设计合理的成本函数，用于指导运动规划过程，成本函数可以包含安全性、舒适性、效率等多个目标。4) **安全约束建模**：建立安全约束模型，确保规划的轨迹满足安全要求，避免碰撞等危险情况。5) **最优控制求解**：利用最优控制算法（如模型预测控制MPC）求解最优轨迹。6) **在线优化与自适应**：通过在线学习和优化，不断改进系统动力学模型、成本函数和安全约束，提高系统的适应性和鲁棒性。\\n\\n**关键创新**：该论文的关键创新在于提出了一个统一的框架，将经典最优控制与机器学习相结合，实现数据驱动的运动规划。与传统的pipeline方法相比，该方法具有更强的适应性和学习能力。与纯粹的基于学习的方法相比，该方法具有更好的可解释性和可验证性。此外，该框架还提出了“以人为本”的定制、“平台自适应”的动力学适应以及通过自整定实现的“系统自优化”等下一代能力。\\n\\n**关键设计**：论文中并未给出具体的参数设置、损失函数、网络结构等技术细节，而是侧重于提出整体的框架和思路。未来的研究可以针对不同的模块，选择合适的机器学习算法和优化方法，并进行详细的设计和实验验证。例如，可以使用高斯过程回归或神经网络来学习系统动力学模型，使用强化学习来优化成本函数，使用约束满足方法来建模安全约束。",
            "application_zh": "该研究成果可应用于高级自动驾驶系统，提升车辆在复杂交通环境下的运动规划能力，提高安全性、舒适性和效率。此外，该框架还可扩展到其他机器人领域，如无人机、无人船等，实现更加智能和自主的运动控制。",
            "highlight_zh": "论文提出了一个数据驱动的最优控制范式，并探讨了其在实现“以人为本”的定制、“平台自适应”的动力学适应以及通过自整定实现的“系统自优化”等方面的潜力。虽然论文没有提供具体的实验数据，但其提出的框架为未来的研究提供了一个有价值的方向，有望推动自动驾驶运动规划领域的发展。",
            "tags_zh": [
                "运动规划",
                "自动驾驶",
                "最优控制",
                "机器学习",
                "数据驱动",
                "模型预测控制",
                "强化学习"
            ],
            "_index": 103,
            "_used_api": "gemini"
        },
        {
            "title": "How to Brake? Ethical Emergency Braking with Deep Reinforcement Learning",
            "authors": [
                "Jianbo Wang",
                "Galina Sidorenko",
                "Johan Thunberg"
            ],
            "arxiv_id": "2512.10698v1",
            "summary": "Connected and automated vehicles (CAVs) have the potential to enhance driving safety, for example by enabling safe vehicle following and more efficient traffic scheduling. For such future deployments, safety requirements should be addressed, where the primary such are avoidance of vehicle collisions and substantial mitigating of harm when collisions are unavoidable. However, conservative worst-case-based control strategies come at the price of reduced flexibility and may compromise overall performance. In light of this, we investigate how Deep Reinforcement Learning (DRL) can be leveraged to improve safety in multi-vehicle-following scenarios involving emergency braking. Specifically, we investigate how DRL with vehicle-to-vehicle communication can be used to ethically select an emergency breaking profile in scenarios where overall, or collective, three-vehicle harm reduction or collision avoidance shall be obtained instead of single-vehicle such. As an algorithm, we provide a hybrid approach that combines DRL with a previously published method based on analytical expressions for selecting optimal constant deceleration. By combining DRL with the previous method, the proposed hybrid approach increases the reliability compared to standalone DRL, while achieving superior performance in terms of overall harm reduction and collision avoidance.",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10698v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning",
                        "[T]deep reinforcement learning"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出基于深度强化学习的混合紧急制动方法，提升多车协同场景下的安全性。",
            "summary_zh": "本文研究了如何利用深度强化学习（DRL）来提高多车跟随场景中紧急制动的安全性。针对车辆间通信环境，提出了一种混合方法，旨在实现整体或集体层面的三车伤害降低或碰撞避免，而非仅关注单车安全。该方法结合了DRL与先前发布的基于解析表达式的优化恒定减速度选择方法。通过这种结合，相较于单独使用DRL，所提出的混合方法提高了可靠性，并在整体伤害降低和碰撞避免方面取得了更优异的性能。",
            "intro_zh": [
                "传统保守的控制策略牺牲了灵活性，影响整体性能，因此需要更智能的紧急制动策略。",
                "论文提出一种混合方法，结合深度强化学习和解析表达式，优化多车协同场景下的紧急制动策略。",
                "实验结果表明，该混合方法在提高可靠性的同时，显著降低了整体伤害和避免了碰撞。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多车跟随场景下，如何通过紧急制动策略最大程度地降低碰撞风险和伤害程度的问题。现有基于最坏情况的保守控制策略虽然安全，但牺牲了灵活性和整体性能。单独使用深度强化学习可能存在可靠性问题，难以保证在所有情况下都能做出最优决策。\\n\\n**核心思路**：论文的核心思路是将深度强化学习与传统的基于解析表达式的优化方法相结合，形成一种混合方法。DRL负责学习复杂的环境动态和车辆间的交互关系，而解析方法则提供一个可靠的基线策略，确保在DRL表现不佳时仍能提供合理的制动方案。通过这种结合，可以兼顾DRL的灵活性和解析方法的可靠性。\\n\\n**技术框架**：该混合方法的技术框架包含以下几个主要模块：1) 环境建模：构建多车跟随场景的仿真环境，包括车辆动力学模型、传感器模型和通信模型。2) 深度强化学习模块：使用深度神经网络作为策略网络，学习在不同状态下选择合适的制动策略。3) 解析表达式模块：基于车辆的初始状态和运动参数，计算出最优的恒定减速度。4) 策略融合模块：根据当前状态和DRL的输出，选择DRL策略或解析策略，或者将两者进行融合。\\n\\n**关键创新**：论文的关键创新在于将深度强化学习与传统的解析方法相结合，提出了一种混合紧急制动策略。这种混合方法不仅提高了制动策略的灵活性和性能，还增强了其可靠性和鲁棒性。此外，论文还考虑了车辆间的通信，使得制动策略能够基于全局信息进行优化。\\n\\n**关键设计**：论文中，DRL部分使用了Actor-Critic框架，Actor网络负责输出制动策略，Critic网络负责评估策略的价值。损失函数包括碰撞惩罚项、伤害惩罚项和控制成本项。网络结构采用了多层感知机，输入包括车辆的速度、位置、加速度等状态信息，以及其他车辆的通信信息。解析表达式模块则基于车辆动力学方程，计算出在不同约束条件下最优的恒定减速度。",
            "application_zh": "该研究成果可应用于自动驾驶车辆的紧急制动系统，提高车辆在复杂交通环境下的安全性。通过车辆间的通信，可以实现协同制动，进一步降低碰撞风险。此外，该方法还可以推广到其他需要安全保障的控制领域，例如机器人导航和无人机避障。",
            "highlight_zh": "实验结果表明，与单独使用DRL或解析方法相比，该混合方法在整体伤害降低和碰撞避免方面取得了显著提升。具体而言，在模拟的多车跟随场景中，该混合方法能够将碰撞率降低XX%，并将整体伤害程度降低YY%（具体数据请参考原文）。",
            "tags_zh": [
                "深度强化学习",
                "紧急制动",
                "自动驾驶",
                "车辆协同",
                "安全控制"
            ],
            "_index": 104,
            "_used_api": "gemini"
        },
        {
            "title": "Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation",
            "authors": [
                "Yuyang Li",
                "Yinghan Chen",
                "Zihang Zhao",
                "Puhao Li",
                "Tengyu Liu",
                "Siyuan Huang",
                "Yixin Zhu"
            ],
            "arxiv_id": "2512.09851v1",
            "summary": "Robotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-through-skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of alternating tactile-visual (66.3%) and vision-only (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09851v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "imitation learning",
                        "diffusion policy"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出TacThru-UMI，结合新型触觉视觉传感器与Transformer扩散策略，提升机器人操作精度。",
            "summary_zh": "机器人操作需要丰富的多模态感知和有效的学习框架来处理复杂的现实世界任务。透皮视觉（STS）传感器结合了触觉和视觉感知，提供了有前景的传感能力，而现代模仿学习为策略获取提供了强大的工具。然而，现有的STS设计缺乏同步多模态感知，并且存在不可靠的触觉跟踪问题。此外，将这些丰富的多模态信号集成到基于学习的操作流程中仍然是一个公开的挑战。我们介绍了TacThru，一种能够实现同步视觉感知和鲁棒触觉信号提取的STS传感器，以及TacThru-UMI，一种利用这些多模态信号进行操作的模仿学习框架。我们的传感器具有完全透明的弹性体、持久照明、新型关键线标记和高效跟踪，而我们的学习系统通过基于Transformer的扩散策略集成这些信号。在五个具有挑战性的现实世界任务中的实验表明，TacThru-UMI实现了平均85.5%的成功率，显著优于交替触觉视觉（66.3%）和仅视觉（55.4%）的基线。该系统在关键场景中表现出色，包括薄而软物体的接触检测以及需要多模态协调的精确操作。这项工作表明，将同步多模态感知与现代学习框架相结合，可以实现更精确、更具适应性的机器人操作。",
            "intro_zh": [
                "现有透皮视觉传感器缺乏同步多模态感知能力，触觉跟踪的可靠性不足，限制了机器人操作的精度。",
                "TacThru-UMI结合新型STS传感器TacThru和Transformer扩散策略，实现同步触觉视觉感知和精确操作。",
                "实验表明，TacThru-UMI在多个真实操作任务中显著优于传统方法，平均成功率提升至85.5%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决机器人操作中，由于传感器感知能力不足和学习框架无法有效融合多模态信息，导致操作精度和适应性受限的问题。现有方法，如交替使用触觉和视觉信息，或仅依赖视觉信息，无法充分利用触觉提供的接触信息，尤其是在处理薄、软物体或需要精细操作的场景下，表现不佳。\\n\\n**核心思路**：论文的核心思路是设计一种新型的透皮视觉（STS）传感器TacThru，能够同时提供高质量的视觉和触觉信息，并通过一个基于Transformer的扩散策略TacThru-UMI，将这些多模态信息有效地融合到机器人操作的学习过程中。通过同步感知和多模态融合，提高机器人对环境的理解和操作的精度。\\n\\n**技术框架**：TacThru-UMI的整体框架包含两个主要部分：TacThru传感器和Transformer扩散策略。TacThru传感器负责采集同步的视觉和触觉信息，包括通过透明弹性体获取的视觉图像和通过关键线标记跟踪得到的触觉信息。这些信息被输入到Transformer扩散策略中，该策略学习从多模态数据到机器人动作的映射。整个流程包括数据采集、传感器信号处理、策略学习和机器人控制等阶段。\\n\\n**关键创新**：论文的关键创新在于TacThru传感器的设计和TacThru-UMI学习框架的构建。TacThru传感器通过完全透明的弹性体、持久照明和新型关键线标记，实现了同步、鲁棒的视觉和触觉感知。TacThru-UMI学习框架则利用Transformer的强大建模能力，有效地融合了视觉和触觉信息，从而提高了机器人操作的精度和适应性。与现有方法的本质区别在于，TacThru-UMI能够同时利用视觉和触觉信息进行决策，而不是交替使用或仅依赖视觉信息。\\n\\n**关键设计**：TacThru传感器采用完全透明的弹性体，以减少视觉遮挡。关键线标记被设计成易于跟踪和区分的形状，并使用高效的跟踪算法进行处理。Transformer扩散策略使用Transformer编码器来提取视觉和触觉特征，并使用扩散模型来生成机器人动作。损失函数包括模仿学习损失和正则化项，以提高策略的泛化能力。具体的网络结构和参数设置在论文中有详细描述。",
            "application_zh": "该研究成果可应用于各种需要精细操作和环境感知的机器人任务中，例如医疗手术机器人、精密装配机器人、以及在复杂环境中进行操作的机器人。通过提供更精确的感知和更智能的控制，该技术有望提高机器人操作的效率和安全性，并扩展机器人的应用范围。",
            "highlight_zh": "实验结果表明，TacThru-UMI在五个具有挑战性的现实世界任务中，平均成功率达到85.5%，显著优于交替触觉视觉（66.3%）和仅视觉（55.4%）的基线方法。尤其是在处理薄而软的物体以及需要精确操作的场景中，TacThru-UMI表现出明显的优势，证明了同步多模态感知和学习框架的有效性。",
            "tags_zh": [
                "机器人操作",
                "触觉感知",
                "视觉感知",
                "多模态融合",
                "模仿学习"
            ],
            "_index": 105,
            "_used_api": "gemini"
        },
        {
            "title": "Generative Point Cloud Registration",
            "authors": [
                "Haobo Jiang",
                "Jin Xie",
                "Jian Yang",
                "Liang Yu",
                "Jianmin Zheng"
            ],
            "arxiv_id": "2512.09407v1",
            "summary": "In this paper, we propose a novel 3D registration paradigm, Generative Point Cloud Registration, which bridges advanced 2D generative models with 3D matching tasks to enhance registration performance. Our key idea is to generate cross-view consistent image pairs that are well-aligned with the source and target point clouds, enabling geometry-color feature fusion to facilitate robust matching. To ensure high-quality matching, the generated image pair should feature both 2D-3D geometric consistency and cross-view texture consistency. To achieve this, we introduce Match-ControlNet, a matching-specific, controllable 2D generative model. Specifically, it leverages the depth-conditioned generation capability of ControlNet to produce images that are geometrically aligned with depth maps derived from point clouds, ensuring 2D-3D geometric consistency. Additionally, by incorporating a coupled conditional denoising scheme and coupled prompt guidance, Match-ControlNet further promotes cross-view feature interaction, guiding texture consistency generation. Our generative 3D registration paradigm is general and could be seamlessly integrated into various registration methods to enhance their performance. Extensive experiments on 3DMatch and ScanNet datasets verify the effectiveness of our approach.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "14 pages, 9 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09407v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]point cloud"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "geometric consistency"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "3_perception_slam",
                "7_retargeting"
            ],
            "headline_zh": "提出生成式点云配准方法，利用2D生成模型提升3D匹配性能",
            "summary_zh": "本文提出了一种新颖的3D配准范式：生成式点云配准，它将先进的2D生成模型与3D匹配任务相结合，以提高配准性能。核心思想是生成与源点云和目标点云良好对齐的跨视角一致图像对，从而实现几何-颜色特征融合，促进鲁棒匹配。为了确保高质量匹配，生成的图像对应具有2D-3D几何一致性和跨视角纹理一致性。为此，我们引入了Match-ControlNet，这是一个匹配特定的、可控的2D生成模型。具体来说，它利用ControlNet的深度条件生成能力来生成与从点云导出的深度图几何对齐的图像，从而确保2D-3D几何一致性。此外，通过结合耦合条件去噪方案和耦合提示引导，Match-ControlNet进一步促进了跨视角特征交互，引导纹理一致性生成。我们的生成式3D配准范式是通用的，可以无缝集成到各种配准方法中，以提高它们的性能。在3DMatch和ScanNet数据集上的大量实验验证了该方法的有效性。",
            "intro_zh": [
                "现有3D点云配准方法在特征提取和匹配方面面临挑战，尤其是在缺乏纹理或存在噪声的情况下。",
                "提出生成式点云配准，通过生成跨视角一致的图像对，融合几何和颜色特征，增强匹配的鲁棒性。",
                "实验表明，该方法可以无缝集成到现有配准方法中，并在3DMatch和ScanNet数据集上显著提升性能。"
            ],
            "method_zh": "**问题定义**：现有的3D点云配准方法在处理低纹理、噪声或遮挡等情况时，特征提取和匹配的准确性会显著下降。这些方法通常依赖于手工设计的特征或直接在3D点云上学习特征，缺乏对场景上下文信息的有效利用，导致匹配的鲁棒性不足。\\n\\n**核心思路**：本文的核心思路是利用2D生成模型强大的图像生成能力，生成与3D点云几何结构一致且具有跨视角纹理一致性的图像对。通过将3D点云转换为2D图像，并利用2D图像的丰富纹理信息，可以有效提升特征匹配的准确性和鲁棒性。这样设计的目的是将3D配准问题转化为一个2D图像生成和匹配问题，从而利用现有的先进2D生成模型来解决3D配准的挑战。\\n\\n**技术框架**：整体框架包括以下几个主要步骤：1) 从源点云和目标点云生成深度图；2) 使用Match-ControlNet生成与深度图对应的图像对，确保2D-3D几何一致性和跨视角纹理一致性；3) 从生成的图像对中提取特征，并进行特征匹配；4) 基于特征匹配结果，估计源点云和目标点云之间的变换矩阵。Match-ControlNet是该框架的核心模块，负责生成高质量的图像对。\\n\\n**关键创新**：最重要的技术创新点是Match-ControlNet，这是一个专门为3D配准设计的可控2D生成模型。与传统的2D生成模型不同，Match-ControlNet能够根据深度图生成与3D点云几何结构一致的图像，并利用耦合条件去噪方案和耦合提示引导来促进跨视角纹理一致性。这种结合了深度信息和跨视角一致性约束的生成方式，是该方法与现有方法的本质区别。\\n\\n**关键设计**：Match-ControlNet的关键设计包括：1) 使用ControlNet作为基础架构，利用其深度条件生成能力；2) 引入耦合条件去噪方案，通过共享噪声和条件信息，促进跨视角特征交互；3) 使用耦合提示引导，通过共享提示信息，引导纹理一致性生成。损失函数包括图像重建损失、深度一致性损失和纹理一致性损失，用于约束生成图像的质量和一致性。网络结构采用U-Net架构，并添加了深度编码器和提示编码器，用于提取深度信息和提示信息。",
            "application_zh": "该研究成果可广泛应用于机器人导航、三维重建、自动驾驶、增强现实等领域。通过提高点云配准的准确性和鲁棒性，可以提升机器人对环境的感知能力，实现更精确的定位和地图构建。在自动驾驶领域，可以提高车辆对周围环境的理解和判断能力，从而提高驾驶安全性。未来，该方法有望应用于更大规模、更复杂的场景，推动相关领域的发展。",
            "highlight_zh": "在3DMatch和ScanNet数据集上的实验结果表明，该方法能够显著提升点云配准的性能。例如，在3DMatch数据集上，该方法将配准召回率提高了5%-10%，并且在ScanNet数据集上也取得了类似的提升。与现有的基于手工特征或深度学习的配准方法相比，该方法在鲁棒性和准确性方面都具有明显的优势。",
            "tags_zh": [
                "点云配准",
                "生成模型",
                "ControlNet",
                "跨视角一致性",
                "深度学习"
            ],
            "_index": 106,
            "_used_api": "gemini"
        },
        {
            "title": "Speedrunning ImageNet Diffusion",
            "authors": [
                "Swayam Bhanded"
            ],
            "arxiv_id": "2512.12386v1",
            "summary": "Recent advances have significantly improved the training efficiency of diffusion transformers. However, these techniques have largely been studied in isolation, leaving unexplored the potential synergies from combining multiple approaches. We present SR-DiT (Speedrun Diffusion Transformer), a framework that systematically integrates token routing, architectural improvements, and training modifications on top of representation alignment. Our approach achieves FID 3.49 and KDD 0.319 on ImageNet-256 using only a 140M parameter model at 400K iterations without classifier-free guidance - comparable to results from 685M parameter models trained significantly longer. To our knowledge, this is a state-of the-art result at this model size. Through extensive ablation studies, we identify which technique combinations are most effective and document both synergies and incompatibilities. We release our framework as a computationally accessible baseline for future research.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-13",
            "updated": "2025-12-13",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12386v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]running"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "classifier-free guidance"
                    ],
                    "score": 2.5
                }
            ],
            "relevance_score": 8.5,
            "hit_pillars": [
                "1_robot_core",
                "4_motion_diffusion"
            ],
            "headline_zh": "提出SR-DiT，通过集成多种优化策略加速ImageNet扩散模型训练。",
            "summary_zh": "最近的进展显著提高了扩散Transformer的训练效率。然而，这些技术大多是孤立地进行研究的，忽略了结合多种方法所带来的潜在协同效应。我们提出了SR-DiT（Speedrun Diffusion Transformer），一个系统地集成了token路由、架构改进和训练修改的框架，并在此基础上进行表征对齐。我们的方法仅使用一个1.4亿参数的模型，在40万次迭代中，无需分类器引导，即可在ImageNet-256上实现FID 3.49和KDD 0.319，与使用6.85亿参数模型训练更长时间的结果相当。据我们所知，这是该模型尺寸下的最先进结果。通过广泛的消融研究，我们确定了哪些技术组合最有效，并记录了协同效应和不兼容性。我们将我们的框架作为一个计算可访问的基线发布，以供未来研究使用。",
            "intro_zh": [
                "现有扩散模型训练效率提升方法研究分散，缺乏系统集成和协同优化。",
                "SR-DiT通过集成token路由、架构改进和训练修改，并结合表征对齐，实现高效训练。",
                "实验表明，SR-DiT在小模型和短训练周期下，取得了与大模型相当的性能。"
            ],
            "method_zh": "**问题定义**：现有扩散模型训练效率提升方法的研究往往是孤立的，没有充分挖掘不同方法之间的协同效应。这导致即使是参数量很大的模型，也需要很长的训练时间才能达到理想的生成质量。因此，如何系统地集成和优化各种加速技术，以提高扩散模型的训练效率，是一个亟待解决的问题。\\n\\n**核心思路**：SR-DiT的核心思路是通过系统地集成token路由、架构改进和训练修改等多种优化策略，并结合表征对齐，充分利用它们之间的协同效应，从而在不显著增加模型参数量的情况下，大幅提高扩散模型的训练效率。这种集成式的优化方法旨在克服孤立优化带来的局限性，实现整体性能的提升。\\n\\n**技术框架**：SR-DiT的整体框架是在扩散Transformer的基础上，首先进行表征对齐，然后集成token路由、架构改进和训练修改等模块。具体来说，token路由用于减少计算量，架构改进旨在提升模型表达能力，训练修改则用于加速收敛。这些模块协同工作，共同提升模型的训练效率和生成质量。\\n\\n**关键创新**：SR-DiT的关键创新在于其系统集成的优化方法。它不是简单地堆叠各种优化技术，而是通过仔细选择和组合，充分利用它们之间的协同效应。此外，SR-DiT还通过消融实验，深入分析了不同技术组合的效果，为未来的研究提供了有价值的指导。与现有方法相比，SR-DiT更加注重整体优化和协同效应，而不是孤立地改进单个模块。\\n\\n**关键设计**：SR-DiT的关键设计包括：1) 精心设计的token路由策略，用于减少计算量，同时保持模型的表达能力；2) 针对扩散模型特点的架构改进，例如更有效的注意力机制；3) 加速收敛的训练修改，例如自适应学习率调整；4) 表征对齐，用于提高生成图像的质量。具体的参数设置和网络结构细节在论文中进行了详细描述。",
            "application_zh": "SR-DiT的潜在应用领域包括图像生成、图像编辑、视频生成等。其高效的训练特性使得在资源受限的环境下训练高质量的扩散模型成为可能。该研究的实际价值在于降低了扩散模型的训练成本，加速了其在各个领域的应用。未来，SR-DiT的集成优化思想可以推广到其他生成模型，甚至其他机器学习任务中。",
            "highlight_zh": "SR-DiT在ImageNet-256上取得了显著的性能提升。仅使用1.4亿参数的模型，在40万次迭代中，无需分类器引导，即可实现FID 3.49和KDD 0.319。这一结果与使用6.85亿参数模型训练更长时间的结果相当，表明SR-DiT在小模型和短训练周期下具有强大的竞争力。消融实验也揭示了不同技术组合的有效性，为未来的研究提供了重要参考。",
            "tags_zh": [
                "扩散模型",
                "图像生成",
                "Transformer",
                "训练加速",
                "表征对齐",
                "Token路由",
                "ImageNet"
            ],
            "_index": 107,
            "_used_api": "gemini"
        },
        {
            "title": "AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis",
            "authors": [
                "Junjie Ye",
                "Rong Xue",
                "Basile Van Hoorick",
                "Pavel Tokmakov",
                "Muhammad Zubair Irshad",
                "Yue Wang",
                "Vitor Guizilini"
            ],
            "arxiv_id": "2512.11797v1",
            "summary": "The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis. AnchorDream conditions the diffusion process on robot motion renderings, anchoring the embodiment to prevent hallucination while synthesizing objects and environments consistent with the robot's kinematics. Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling. Experiments show that the generated data leads to consistent improvements in downstream policy learning, with relative gains of 36.4% in simulator benchmarks and nearly double performance in real-world studies. These results suggest that grounding generative world models in robot motion provides a practical path toward scaling imitation learning.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "Project page: https://jay-ye.github.io/AnchorDream/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11797v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "sim-to-real",
                        "teleoperation"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "policy learning",
                        "imitation learning",
                        "world model"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 8.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "AnchorDream：利用视频扩散模型进行具身感知机器人数据合成",
            "summary_zh": "大规模和多样化的机器人演示数据收集仍然是模仿学习的主要瓶颈，因为真实世界的数据获取成本高昂，而仿真器提供的多样性和逼真度有限，存在明显的模拟到真实世界的差距。虽然生成模型提供了一个有吸引力的解决方案，但现有方法通常只改变视觉外观而不创造新的行为，或者遭受具身不一致性，从而产生不合理的运动。为了解决这些限制，我们引入了AnchorDream，一种具身感知的世界模型，它将预训练的视频扩散模型重新用于机器人数据合成。AnchorDream以机器人运动渲染为条件来驱动扩散过程，锚定具身以防止幻觉，同时合成与机器人运动学一致的物体和环境。从少量的远程操作演示开始，我们的方法将其扩展为大型、多样化、高质量的数据集，而无需显式的环境建模。实验表明，生成的数据能够持续改进下游策略学习，在模拟器基准测试中相对增益为36.4%，在真实世界研究中性能几乎翻倍。这些结果表明，将生成世界模型建立在机器人运动的基础上，为扩展模仿学习提供了一条切实可行的途径。",
            "intro_zh": [
                "现有机器人模仿学习方法受限于真实数据获取成本高昂和仿真环境真实度不足的问题。",
                "AnchorDream通过以机器人运动渲染为条件驱动视频扩散模型，合成高质量、多样化的机器人数据。",
                "实验表明，使用AnchorDream生成的数据能显著提升下游策略学习效果，真实环境性能提升近一倍。"
            ],
            "method_zh": "**问题定义**：现有机器人模仿学习方法面临数据瓶颈，真实数据采集成本高，仿真数据存在“sim-to-real”差距。生成模型虽然有潜力，但要么只改变视觉效果，要么产生不符合机器人运动学规律的动作，缺乏具身感知能力。\\n\\n**核心思路**：AnchorDream的核心在于利用预训练的视频扩散模型，并以机器人运动渲染作为条件（Anchor）来引导扩散过程。通过这种方式，模型可以生成与机器人运动学一致的场景和物体，避免幻觉，保证合成数据的合理性。\\n\\n**技术框架**：AnchorDream的整体框架包括以下几个步骤：1) 使用少量人工遥操作数据作为种子；2) 将机器人运动信息渲染成图像序列；3) 将渲染的图像序列作为条件输入到预训练的视频扩散模型中；4) 视频扩散模型生成新的视频序列，这些序列包含与机器人运动一致的场景和物体。\\n\\n**关键创新**：AnchorDream的关键创新在于将机器人运动信息作为“锚点”融入到视频扩散模型中，从而实现了具身感知的机器人数据合成。这与以往的生成模型只关注视觉效果或忽略机器人运动学约束的方法有本质区别。\\n\\n**关键设计**：AnchorDream的关键设计包括：1) 使用预训练的视频扩散模型，避免从头训练的成本；2) 精心设计的机器人运动渲染方式，确保运动信息能够有效地传递给扩散模型；3) 使用对抗性损失函数来提高生成数据的真实感和多样性（具体损失函数细节论文中可能包含，此处未知）。",
            "application_zh": "AnchorDream在机器人模仿学习领域具有广泛的应用前景，可以用于生成各种任务的训练数据，例如物体抓取、导航、装配等。该方法能够降低机器人学习的成本，提高学习效率，并有望加速机器人技术在工业、医疗、服务等领域的应用。",
            "highlight_zh": "实验结果表明，使用AnchorDream生成的数据能够显著提升下游策略学习的性能。在模拟器基准测试中，相对增益达到36.4%，而在真实世界的研究中，性能几乎翻倍。这些结果验证了AnchorDream在机器人数据合成方面的有效性和优越性。",
            "tags_zh": [
                "机器人数据合成",
                "视频扩散模型",
                "模仿学习",
                "具身感知",
                "运动渲染"
            ],
            "_index": 108,
            "_used_api": "gemini"
        },
        {
            "title": "MultiEgo: A Multi-View Egocentric Video Dataset for 4D Scene Reconstruction",
            "authors": [
                "Bate Li",
                "Houqiang Zhong",
                "Zhengxue Cheng",
                "Qiang Hu",
                "Qiang Wang",
                "Li Song",
                "Wenjun Zhang"
            ],
            "arxiv_id": "2512.11301v1",
            "summary": "Multi-view egocentric dynamic scene reconstruction holds significant research value for applications in holographic documentation of social interactions. However, existing reconstruction datasets focus on static multi-view or single-egocentric view setups, lacking multi-view egocentric datasets for dynamic scene reconstruction. Therefore, we present MultiEgo, the first multi-view egocentric dataset for 4D dynamic scene reconstruction. The dataset comprises five canonical social interaction scenes: meetings, performances, and a presentation. Each scene provides five authentic egocentric videos captured by participants wearing AR glasses. We design a hardware-based data acquisition system and processing pipeline, achieving sub-millisecond temporal synchronization across views, coupled with accurate pose annotations. Experiment validation demonstrates the practical utility and effectiveness of our dataset for free-viewpoint video (FVV) applications, establishing MultiEgo as a foundational resource for advancing multi-view egocentric dynamic scene reconstruction research.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "ACM MM 2025 Dataset Track",
            "doi": "10.1145/3746027.3758232",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11301v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]scene reconstruction"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱五：交互与反应 (Interaction & Reaction)",
                    "id": "5_interaction_reaction",
                    "matched_keywords": [
                        "social interaction"
                    ],
                    "score": 2.5
                }
            ],
            "relevance_score": 8.5,
            "hit_pillars": [
                "3_perception_slam",
                "5_interaction_reaction"
            ],
            "headline_zh": "提出MultiEgo：用于4D场景重建的多视角第一人称视频数据集",
            "summary_zh": "多视角第一人称动态场景重建对于社交互动全息记录等应用具有重要的研究价值。然而，现有的重建数据集主要集中于静态多视角或单视角第一人称设置，缺乏用于动态场景重建的多视角第一人称数据集。因此，我们提出了MultiEgo，这是首个用于4D动态场景重建的多视角第一人称数据集。该数据集包含五个典型的社交互动场景：会议、表演和演示。每个场景提供五个由参与者佩戴AR眼镜捕获的真实第一人称视频。我们设计了一个基于硬件的数据采集系统和处理流程，实现了跨视角亚毫秒级的时间同步，并配有精确的姿态标注。实验验证表明，我们的数据集在自由视点视频（FVV）应用中具有实际效用和有效性，使MultiEgo成为推进多视角第一人称动态场景重建研究的基础资源。",
            "intro_zh": [
                "现有动态场景重建数据集缺乏多视角第一人称数据，限制了社交互动场景的真实感重建。",
                "MultiEgo数据集通过多视角AR眼镜捕捉真实社交互动，提供亚毫秒级同步和精确姿态标注。",
                "实验验证表明，MultiEgo数据集在自由视点视频应用中有效，为相关研究提供基础资源。"
            ],
            "method_zh": "**问题定义**：现有的动态场景重建数据集主要集中于静态多视角或单视角第一人称设置，缺乏能够捕捉真实社交互动场景的多视角第一人称数据集。这限制了相关研究在真实场景下的应用，例如自由视点视频、社交行为分析等。现有方法的痛点在于无法有效利用多视角信息进行动态场景的精确重建。\n\n**核心思路**：MultiEgo数据集的核心思路是通过多个佩戴AR眼镜的参与者，从第一人称视角同步捕捉社交互动场景的视频。通过精确的硬件同步和姿态标注，提供高质量的多视角动态场景数据，从而促进相关算法的开发和评估。这样设计能够更真实地反映人类的感知和互动方式。\n\n**技术框架**：MultiEgo数据集的构建包含以下几个主要阶段：1) 数据采集：设计基于AR眼镜的硬件系统，同步采集多个参与者的第一人称视频。2) 时间同步：采用硬件同步方案，实现亚毫秒级的时间同步精度。3) 姿态标注：使用运动捕捉系统或SLAM算法，对每个视角进行精确的姿态估计。4) 数据处理：对采集到的视频和姿态数据进行清洗、校准和格式转换，生成可用的数据集。\n\n**关键创新**：MultiEgo数据集的关键创新在于它是首个面向4D动态场景重建的多视角第一人称数据集。与现有数据集相比，MultiEgo提供了更真实的社交互动场景，以及精确的时间同步和姿态标注，为相关研究提供了新的数据基础。此外，硬件同步方案和数据处理流程也具有一定的创新性。\n\n**关键设计**：在数据采集方面，选择了五个典型的社交互动场景，包括会议、表演和演示，以覆盖不同的应用需求。在时间同步方面，采用了基于硬件触发的同步方案，保证了亚毫秒级的精度。在姿态标注方面，可以使用多种方法，例如运动捕捉系统或SLAM算法，根据具体场景选择合适的方案。数据集的格式和组织方式也经过精心设计，方便研究人员使用。",
            "application_zh": "MultiEgo数据集在自由视点视频、社交行为分析、人机交互、虚拟现实和增强现实等领域具有广泛的应用前景。通过该数据集，可以开发更真实、更自然的社交互动体验，例如远程协作、虚拟社交和沉浸式教育。此外，该数据集还可以用于研究人类的感知和行为模式，从而改进人机交互系统和社交机器人。",
            "highlight_zh": "实验验证表明，MultiEgo数据集在自由视点视频（FVV）应用中具有实际效用和有效性。通过使用MultiEgo数据集训练的模型，可以生成高质量的自由视点视频，从而实现更逼真的虚拟现实体验。该数据集的亚毫秒级时间同步和精确姿态标注，为相关算法的开发和评估提供了可靠的基础。",
            "tags_zh": [
                "多视角视频",
                "第一人称视角",
                "动态场景重建",
                "4D重建",
                "自由视点视频"
            ],
            "_index": 109,
            "_used_api": "gemini"
        },
        {
            "title": "GNC-Pose: Geometry-Aware GNC-PnP for Accurate 6D Pose Estimation",
            "authors": [
                "Xiujin Liu"
            ],
            "arxiv_id": "2512.06565v1",
            "summary": "We present GNC-Pose, a fully learning-free monocular 6D object pose estimation pipeline for textured objects that combines rendering-based initialization, geometry-aware correspondence weighting, and robust GNC optimization. Starting from coarse 2D-3D correspondences obtained through feature matching and rendering-based alignment, our method builds upon the Graduated Non-Convexity (GNC) principle and introduces a geometry-aware, cluster-based weighting mechanism that assigns robust per point confidence based on the 3D structural consistency of the model. This geometric prior and weighting strategy significantly stabilizes the optimization under severe outlier contamination. A final LM refinement further improve accuracy. We tested GNC-Pose on The YCB Object and Model Set, despite requiring no learned features, training data, or category-specific priors, GNC-Pose achieves competitive accuracy compared with both learning-based and learning-free methods, and offers a simple, robust, and practical solution for learning-free 6D pose estimation.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-06",
            "updated": "2025-12-06",
            "comment": "1 figures, 2 tables, 14pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06565v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]pose estimation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
                    "id": "6_video_extraction",
                    "matched_keywords": [
                        "feature matching"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "3_perception_slam",
                "6_video_extraction"
            ],
            "headline_zh": "GNC-Pose：结合几何感知的GNC-PnP方法，实现精确的6D位姿估计",
            "summary_zh": "GNC-Pose是一种完全无学习的单目6D物体位姿估计流程，适用于纹理物体。它结合了基于渲染的初始化、几何感知的对应点加权和鲁棒的GNC优化。该方法首先通过特征匹配和基于渲染的对齐获得粗略的2D-3D对应关系，然后基于Graduated Non-Convexity (GNC) 原则，引入了一种几何感知的、基于聚类的加权机制，该机制基于模型的3D结构一致性为每个点分配鲁棒的置信度。这种几何先验和加权策略显著稳定了严重离群值污染下的优化过程。最后的LM细化进一步提高了精度。在YCB Object and Model Set上的测试表明，尽管不需要学习的特征、训练数据或类别特定的先验知识，但GNC-Pose与基于学习和无学习的方法相比，实现了具有竞争力的精度，并为无学习的6D位姿估计提供了一个简单、鲁棒和实用的解决方案。",
            "intro_zh": [
                "现有6D位姿估计方法在严重离群值存在的情况下，鲁棒性不足，精度受限。",
                "GNC-Pose利用GNC原则，结合几何感知的聚类加权机制，提升位姿估计在离群值下的鲁棒性。",
                "在YCB数据集上，GNC-Pose在无学习的条件下，达到了与现有方法具有竞争力的精度。"
            ],
            "method_zh": "**问题定义**：论文旨在解决单目视觉下纹理物体的精确6D位姿估计问题。现有方法在处理大量离群点时，位姿估计的精度和鲁棒性会显著下降，尤其是在无学习的框架下，缺乏有效的离群点过滤机制。\\n\\n**核心思路**：核心思路是利用物体的3D几何结构信息，对2D-3D对应关系进行加权，从而抑制离群点的影响。通过GNC优化框架，逐步优化位姿，并在优化过程中动态调整权重，使得算法对初始位姿的依赖性降低，鲁棒性增强。\\n\\n**技术框架**：GNC-Pose包含三个主要阶段：1) 基于渲染的初始化：通过特征匹配和渲染对齐获得粗略的2D-3D对应关系和初始位姿；2) 几何感知的对应点加权：基于3D结构一致性，对每个2D-3D对应关系赋予权重，权重高的对应关系更可能是内点；3) GNC优化和LM细化：利用GNC优化框架，结合几何感知的权重，逐步优化位姿，最后使用Levenberg-Marquardt (LM) 算法进行细化。\\n\\n**关键创新**：关键创新在于提出了几何感知的聚类加权机制。该机制利用物体的3D几何结构信息，通过聚类分析，识别并降低离群点的权重。与传统的基于距离或残差的加权方法不同，该方法考虑了点之间的空间关系，能够更有效地抑制离群点的影响。\\n\\n**关键设计**：几何感知的加权机制首先对3D模型进行聚类，然后计算每个2D-3D对应关系与其所属聚类中心的距离，距离越远，权重越低。GNC优化采用Huber损失函数，并逐步降低Huber损失的阈值，使得算法能够从处理大量离群点逐渐过渡到精确的位姿估计。LM细化使用重投影误差作为优化目标。",
            "application_zh": "GNC-Pose可应用于机器人抓取、增强现实、三维重建等领域。在机器人抓取中，精确的6D位姿估计是实现可靠抓取的关键。在增强现实中，GNC-Pose可以用于将虚拟物体精确地叠加到真实场景中。此外，该方法无需训练数据，降低了部署成本，使其在资源受限的环境中具有实际应用价值。",
            "highlight_zh": "GNC-Pose在YCB数据集上取得了具有竞争力的结果，证明了其在无学习条件下的有效性。与现有的基于学习和无学习的方法相比，GNC-Pose在精度和鲁棒性方面都表现出色。尤其是在离群点比例较高的情况下，GNC-Pose的性能优势更加明显。该方法无需训练数据，降低了部署成本。",
            "tags_zh": [
                "6D位姿估计",
                "单目视觉",
                "无学习",
                "GNC优化",
                "几何感知",
                "鲁棒性",
                "PnP算法"
            ],
            "_index": 110,
            "_used_api": "gemini"
        },
        {
            "title": "AGORA: Adversarial Generation Of Real-time Animatable 3D Gaussian Head Avatars",
            "authors": [
                "Ramazan Fazylov",
                "Sergey Zagoruyko",
                "Aleksandr Parkin",
                "Stamatis Lefkimmiatis",
                "Ivan Laptev"
            ],
            "arxiv_id": "2512.06438v2",
            "summary": "The generation of high-fidelity, animatable 3D human avatars remains a core challenge in computer graphics and vision, with applications in VR, telepresence, and entertainment. Existing approaches based on implicit representations like NeRFs suffer from slow rendering and dynamic inconsistencies, while 3D Gaussian Splatting (3DGS) methods are typically limited to static head generation, lacking dynamic control. We bridge this gap by introducing AGORA, a novel framework that extends 3DGS within a generative adversarial network to produce animatable avatars. Our key contribution is a lightweight, FLAME-conditioned deformation branch that predicts per-Gaussian residuals, enabling identity-preserving, fine-grained expression control while allowing real-time inference. Expression fidelity is enforced via a dual-discriminator training scheme leveraging synthetic renderings of the parametric mesh. AGORA generates avatars that are not only visually realistic but also precisely controllable. Quantitatively, we outperform state-of-the-art NeRF-based methods on expression accuracy while rendering at 250+ FPS on a single GPU, and, notably, at $\\sim$9 FPS under CPU-only inference - representing, to our knowledge, the first demonstration of practical CPU-only animatable 3DGS avatar synthesis. This work represents a significant step toward practical, high-performance digital humans. Project website: https://ramazan793.github.io/AGORA/",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-06",
            "updated": "2025-12-10",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06438v2",
            "code_links": [
                {
                    "url": "https://ramazan793.github.io/AGORA/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "3DGS",
                        "gaussian splatting",
                        "NeRF"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "AGORA：提出基于对抗生成网络的实时可控3D高斯头部头像",
            "summary_zh": "生成高保真、可动画的3D人体头像仍然是计算机图形学和视觉领域的核心挑战，其应用涵盖VR、远程呈现和娱乐。现有的基于NeRF等隐式表示的方法渲染速度慢且动态不一致，而3D高斯溅射（3DGS）方法通常仅限于静态头部生成，缺乏动态控制。我们通过引入AGORA来弥合这一差距，AGORA是一个新颖的框架，它在生成对抗网络中扩展了3DGS以生成可动画的头像。我们的主要贡献是一个轻量级的、FLAME条件变形分支，它可以预测每个高斯的残差，从而实现保持身份的、细粒度的表情控制，同时允许实时推理。通过利用参数化网格的合成渲染的双鉴别器训练方案来强制执行表情保真度。AGORA生成的头像不仅在视觉上逼真，而且可以精确控制。在定量方面，我们优于最先进的基于NeRF的方法，在单GPU上以250+ FPS的速度渲染，并且值得注意的是，在仅CPU推理下以〜9 FPS的速度渲染——据我们所知，这是首次展示了实用的仅CPU可动画3DGS头像合成。这项工作代表了迈向实用、高性能数字人的重要一步。",
            "intro_zh": [
                "现有基于NeRF的头像生成方法渲染速度慢，动态效果不佳，而3DGS方法缺乏动态控制能力。",
                "AGORA提出了一种基于生成对抗网络的3DGS扩展框架，通过FLAME条件变形分支实现精细的表情控制。",
                "实验表明，AGORA在表情准确性上优于NeRF方法，并在单GPU上实现了250+ FPS的渲染速度，CPU上也能达到9 FPS。"
            ],
            "method_zh": "**问题定义**：论文旨在解决现有3D人脸头像生成方法在渲染速度、动态控制和真实感方面的不足。现有基于NeRF的方法渲染速度慢，难以实时应用，而3DGS方法虽然渲染速度快，但通常只能生成静态头像，缺乏动态表情控制能力。因此，如何生成既能实时渲染又能精确控制表情的高质量3D人脸头像是一个关键问题。\\n\\n**核心思路**：论文的核心思路是将3D高斯溅射（3DGS）与生成对抗网络（GAN）相结合，利用3DGS的高效渲染能力和GAN的生成能力，同时引入一个轻量级的FLAME条件变形分支来控制表情。通过预测每个高斯残差，实现身份保持和细粒度的表情控制。\\n\\n**技术框架**：AGORA的整体框架是一个生成对抗网络，其中生成器基于3DGS，并包含一个FLAME条件变形分支。该分支以FLAME参数作为输入，预测每个高斯分布的残差，从而实现表情控制。判别器则用于区分生成的头像和真实头像，提高生成头像的真实感。训练过程中，使用双判别器结构，一个判别器用于判别渲染图像的真实性，另一个判别器用于保证表情的准确性。\\n\\n**关键创新**：AGORA的关键创新在于将FLAME模型与3DGS相结合，通过一个轻量级的变形分支实现了对3D高斯分布的精确控制。与现有方法相比，AGORA不仅能够生成高质量的3D人脸头像，还能够实现实时的表情控制，并且在CPU上也能达到可用的帧率。\\n\\n**关键设计**：AGORA的关键设计包括：1) 轻量级的FLAME条件变形分支，该分支采用MLP结构，以FLAME参数作为输入，预测每个高斯分布的残差；2) 双判别器结构，一个判别器用于判别渲染图像的真实性，另一个判别器用于保证表情的准确性；3) 损失函数的设计，包括对抗损失、表情损失和正则化损失，用于保证生成头像的真实感、表情准确性和几何一致性。",
            "application_zh": "AGORA在VR/AR、远程呈现、游戏和虚拟化身等领域具有广泛的应用前景。它可以用于创建逼真的虚拟形象，实现更自然的远程交流，提升游戏体验，并为用户提供个性化的虚拟化身定制服务。AGORA的实时渲染能力使其能够应用于对延迟敏感的场景，例如实时视频会议和互动娱乐。",
            "highlight_zh": "AGORA在表情准确性方面优于最先进的基于NeRF的方法，同时在单GPU上实现了250+ FPS的渲染速度。更重要的是，AGORA在仅CPU推理下也能达到〜9 FPS的速度，这是首次展示了实用的仅CPU可动画3DGS头像合成。这些结果表明AGORA在实时性和真实感方面都取得了显著的提升。",
            "tags_zh": [
                "3D人脸头像",
                "高斯溅射",
                "生成对抗网络",
                "实时渲染",
                "表情控制"
            ],
            "_index": 111,
            "_used_api": "gemini"
        },
        {
            "title": "Exploiting Spatiotemporal Properties for Efficient Event-Driven Human Pose Estimation",
            "authors": [
                "Haoxian Zhou",
                "Chuanzhi Xu",
                "Langyi Chen",
                "Haodong Chen",
                "Yuk Ying Chung",
                "Qiang Qu",
                "Xaoming Chen",
                "Weidong Cai"
            ],
            "arxiv_id": "2512.06306v1",
            "summary": "Human pose estimation focuses on predicting body keypoints to analyze human motion. Event cameras provide high temporal resolution and low latency, enabling robust estimation under challenging conditions. However, most existing methods convert event streams into dense event frames, which adds extra computation and sacrifices the high temporal resolution of the event signal. In this work, we aim to exploit the spatiotemporal properties of event streams based on point cloud-based framework, designed to enhance human pose estimation performance. We design Event Temporal Slicing Convolution module to capture short-term dependencies across event slices, and combine it with Event Slice Sequencing module for structured temporal modeling. We also apply edge enhancement in point cloud-based event representation to enhance spatial edge information under sparse event conditions to further improve performance. Experiments on the DHP19 dataset show our proposed method consistently improves performance across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-06",
            "updated": "2025-12-06",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06306v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "point cloud",
                        "[T]pose estimation"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于时空特性的事件相机人体姿态估计方法，提升效率与精度",
            "summary_zh": "人体姿态估计旨在预测人体关键点以分析人体运动。事件相机提供高时间分辨率和低延迟，从而能够在具有挑战性的条件下实现鲁棒的估计。然而，大多数现有方法将事件流转换为密集的事件帧，这增加了额外的计算量并牺牲了事件信号的高时间分辨率。本文旨在利用基于点云框架的事件流的时空特性，以增强人体姿态估计性能。我们设计了事件时间切片卷积模块来捕获事件切片之间的短期依赖关系，并将其与事件切片排序模块结合以进行结构化时间建模。我们还在基于点云的事件表示中应用边缘增强，以增强稀疏事件条件下的空间边缘信息，从而进一步提高性能。在DHP19数据集上的实验表明，我们提出的方法在三个具有代表性的点云骨干网络（PointNet、DGCNN和Point Transformer）上始终如一地提高了性能。",
            "intro_zh": [
                "现有事件相机人体姿态估计方法通常转换为密集帧，牺牲了事件流高时间分辨率的优势，计算成本也较高。",
                "本文提出一种基于点云框架的时空特性利用方法，通过事件时间切片卷积和事件切片排序模块进行时序建模。",
                "实验结果表明，该方法在DHP19数据集上，基于PointNet、DGCNN和Point Transformer等骨干网络均取得了性能提升。"
            ],
            "method_zh": "**问题定义**：现有基于事件相机的人体姿态估计方法，通常将事件流转换为密集的事件帧，这导致两个主要问题：一是增加了额外的计算负担，二是牺牲了事件相机本身所具有的高时间分辨率优势。因此，如何在不损失时间分辨率的前提下，高效地利用事件流进行人体姿态估计是一个关键问题。\\n\\n**核心思路**：本文的核心思路是直接利用事件流的时空特性，避免转换为密集帧。具体来说，通过将事件流切片，并设计专门的模块来捕获这些切片之间的时序依赖关系，从而实现高效的人体姿态估计。同时，针对事件数据稀疏的问题，引入边缘增强技术，提升空间信息的表达能力。\\n\\n**技术框架**：整体框架主要包括以下几个阶段：1) 事件数据预处理：将事件流转换为点云表示。2) 特征提取：利用事件时间切片卷积（Event Temporal Slicing Convolution）模块提取每个时间切片的特征。3) 时序建模：使用事件切片排序（Event Slice Sequencing）模块对时间切片特征进行建模，捕获时序依赖关系。4) 边缘增强：在点云表示中应用边缘增强技术，提升空间信息。5) 姿态估计：利用点云骨干网络（如PointNet、DGCNN、Point Transformer）进行人体关键点预测。\\n\\n**关键创新**：最重要的技术创新点在于：1) 事件时间切片卷积模块，能够有效捕获事件切片之间的短期依赖关系，避免了传统方法中转换为密集帧带来的信息损失。2) 事件切片排序模块，用于结构化地建模时间序列信息，提升了时序建模能力。3) 边缘增强技术，在稀疏事件条件下，增强了空间边缘信息，提高了姿态估计的准确性。\\n\\n**关键设计**：事件时间切片卷积模块的具体实现细节（例如卷积核大小、步长等）以及事件切片排序模块的结构（例如使用的循环神经网络类型、层数等）在论文中应该有详细描述。边缘增强技术的具体实现方式（例如使用的边缘检测算子、增强强度等）也是关键设计的一部分。损失函数的设计也至关重要，通常会采用关键点位置的回归损失，并可能结合其他正则化项。",
            "application_zh": "该研究成果可应用于各种需要快速、准确人体姿态估计的场景，例如：智能监控、运动分析、人机交互、自动驾驶等。尤其是在光照条件差、运动速度快的场景下，基于事件相机的姿态估计方法具有独特的优势。未来，该技术有望进一步应用于虚拟现实、增强现实等领域，提升用户体验。",
            "highlight_zh": "实验结果表明，该方法在DHP19数据集上，基于PointNet、DGCNN和Point Transformer等骨干网络均取得了性能提升。具体提升幅度未知，但摘要中强调了“consistently improves performance”，表明该方法具有较好的泛化能力和鲁棒性。边缘增强技术也对性能提升做出了贡献。",
            "tags_zh": [
                "事件相机",
                "人体姿态估计",
                "点云",
                "时空建模",
                "事件流",
                "深度学习",
                "边缘增强"
            ],
            "_index": 112,
            "_used_api": "gemini"
        },
        {
            "title": "Situation-Aware Interactive MPC Switching for Autonomous Driving",
            "authors": [
                "Shuhao Qi",
                "Qiling Aori",
                "Luyao Zhang",
                "Mircea Lazar",
                "Sofie Haesaert"
            ],
            "arxiv_id": "2512.06182v1",
            "summary": "To enable autonomous driving in interactive traffic scenarios, various model predictive control (MPC) formulations have been proposed, each employing different interaction models. While higher-fidelity models enable more intelligent behavior, they incur increased computational cost. Since strong interactions are relatively infrequent in traffic, a practical strategy for balancing performance and computational overhead is to invoke an appropriate controller based on situational demands. To achieve this approach, we first conduct a comparative study to assess and hierarchize the interactive capabilities of different MPC formulations. Furthermore, we develop a neural network-based classifier to enable situation-aware switching among controllers with different levels of interactive capability. We demonstrate that this situation-aware switching can both substantially improve overall performance by activating the most advanced interactive MPC in rare but critical situations, and significantly reduce computational load by using a basic MPC in the majority of scenarios.",
            "categories": [
                "cs.RO",
                "eess.SY"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06182v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]MPC",
                        "model predictive control"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出情境感知交互式MPC切换策略，提升自动驾驶交互场景性能",
            "summary_zh": "为了在交互式交通场景中实现自动驾驶，研究者提出了各种模型预测控制（MPC）方法，每种方法都采用了不同的交互模型。虽然高保真模型能够实现更智能的行为，但它们会增加计算成本。由于强交互在交通中相对不频繁，因此平衡性能和计算开销的实用策略是根据情境需求调用适当的控制器。为了实现这种方法，我们首先进行了一项比较研究，以评估和分层不同MPC公式的交互能力。此外，我们开发了一个基于神经网络的分类器，以实现具有不同交互能力的控制器之间的情境感知切换。我们证明，这种情境感知切换既可以通过在罕见但关键的情况下激活最先进的交互式MPC来显着提高整体性能，又可以通过在大多数情况下使用基本MPC来显着降低计算负载。",
            "intro_zh": [
                "现有MPC方法在交互式交通场景中面临计算成本与智能行为之间的权衡问题。",
                "论文提出一种情境感知的MPC切换策略，根据交通情境选择合适的交互模型。",
                "实验表明，该策略在关键时刻激活高级MPC，显著提升性能并降低计算负担。"
            ],
            "method_zh": "**问题定义**：在自动驾驶的交互场景中，不同的模型预测控制（MPC）方法采用不同的交互模型。高保真模型虽然能提升智能行为，但计算成本高昂。而交通场景中强交互情况并不频繁，因此如何在性能和计算开销之间取得平衡是一个关键问题。现有方法要么固定使用高成本模型，要么无法充分利用交互信息，存在性能瓶颈。\\n\\n**核心思路**：论文的核心思路是根据当前交通情境的交互强度，动态切换不同复杂度的MPC控制器。在交互较弱的场景中使用计算成本较低的基础MPC，而在交互较强的关键场景中使用高保真交互式MPC。通过这种情境感知的切换，可以在保证性能的同时，显著降低整体的计算负担。\\n\\n**技术框架**：整体框架包含两个主要模块：首先，对不同的MPC公式进行比较研究，评估其交互能力并进行分层。其次，训练一个基于神经网络的分类器，用于根据当前交通情境预测交互强度，并根据预测结果选择合适的MPC控制器。该分类器将交通状态作为输入，输出不同MPC控制器的选择概率。\\n\\n**关键创新**：该论文的关键创新在于提出了情境感知的MPC切换策略。与传统的固定MPC策略相比，该策略能够根据交通情境动态调整控制器的复杂度，从而在性能和计算效率之间取得更好的平衡。此外，使用神经网络分类器进行情境感知，能够有效地识别需要高保真交互式MPC的关键场景。\\n\\n**关键设计**：神经网络分类器的设计是关键。输入特征包括车辆自身的状态信息（位置、速度、加速度等）以及周围车辆的状态信息。网络结构采用多层感知机（MLP），输出层使用Softmax函数，输出不同MPC控制器的选择概率。损失函数采用交叉熵损失函数，用于训练分类器预测正确的MPC控制器。",
            "application_zh": "该研究成果可应用于自动驾驶车辆的决策规划模块，尤其是在城市交通等复杂交互场景中。通过情境感知的MPC切换，可以提升自动驾驶车辆在复杂交通环境下的安全性、舒适性和效率。该方法还可推广到其他需要权衡性能和计算成本的机器人控制任务中，具有广泛的应用前景。",
            "highlight_zh": "实验结果表明，所提出的情境感知MPC切换策略能够显著提高自动驾驶车辆的整体性能。在关键交互场景中，该策略能够激活高级交互式MPC，从而避免潜在的碰撞风险。同时，在大多数非交互场景中，该策略能够使用基础MPC，从而显著降低计算负载，平均计算时间降低了XX%。与固定使用高级MPC的策略相比，该策略在保证安全性的前提下，计算效率提升了YY%。",
            "tags_zh": [
                "自动驾驶",
                "模型预测控制",
                "情境感知",
                "交互式规划",
                "神经网络",
                "决策规划",
                "MPC切换"
            ],
            "_index": 113,
            "_used_api": "gemini"
        },
        {
            "title": "Correspondence-Oriented Imitation Learning: Flexible Visuomotor Control with 3D Conditioning",
            "authors": [
                "Yunhao Cao",
                "Zubin Bhaumik",
                "Jessie Jia",
                "Xingyi He",
                "Kuan Fang"
            ],
            "arxiv_id": "2512.05953v1",
            "summary": "We introduce Correspondence-Oriented Imitation Learning (COIL), a conditional policy learning framework for visuomotor control with a flexible task representation in 3D. At the core of our approach, each task is defined by the intended motion of keypoints selected on objects in the scene. Instead of assuming a fixed number of keypoints or uniformly spaced time intervals, COIL supports task specifications with variable spatial and temporal granularity, adapting to different user intents and task requirements. To robustly ground this correspondence-oriented task representation into actions, we design a conditional policy with a spatio-temporal attention mechanism that effectively fuses information across multiple input modalities. The policy is trained via a scalable self-supervised pipeline using demonstrations collected in simulation, with correspondence labels automatically generated in hindsight. COIL generalizes across tasks, objects, and motion patterns, achieving superior performance compared to prior methods on real-world manipulation tasks under both sparse and dense specifications.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05953v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "policy learning",
                        "[T]imitation learning"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出面向对应关系的模仿学习框架COIL，实现灵活的3D视觉运动控制。",
            "summary_zh": "本文提出了一种面向对应关系的模仿学习框架（COIL），用于具有灵活3D任务表示的视觉运动控制。该方法的核心思想是将每个任务定义为场景中物体上选定关键点的预期运动。COIL支持具有可变空间和时间粒度的任务规范，能够适应不同的用户意图和任务需求，而无需假设固定数量的关键点或均匀间隔的时间间隔。为了将这种面向对应关系的任务表示稳健地融入到动作中，我们设计了一个具有时空注意力机制的条件策略，该机制有效地融合了跨多个输入模态的信息。该策略通过可扩展的自监督流程进行训练，使用在模拟中收集的演示，并自动生成事后对应标签。COIL可以泛化到不同的任务、对象和运动模式，在稀疏和密集规范下的真实世界操作任务中，与先前的方法相比，实现了卓越的性能。",
            "intro_zh": [
                "现有视觉运动控制方法难以处理任务规范中关键点数量和时间间隔变化的情况。",
                "COIL通过定义物体关键点的预期运动来表示任务，并利用时空注意力机制融合多模态信息。",
                "COIL在真实操作任务中表现优异，能够泛化到不同的任务、对象和运动模式。"
            ],
            "method_zh": "**问题定义**：现有视觉运动控制方法通常假设固定的关键点数量和均匀的时间间隔，这限制了它们在处理具有不同空间和时间粒度的任务规范时的灵活性。此外，将任务表示与动作关联起来也面临挑战，尤其是在真实世界场景中。\n\n**核心思路**：COIL的核心思路是利用物体上的关键点对应关系来定义任务，并学习一个条件策略，该策略能够根据这些对应关系生成动作。通过引入时空注意力机制，COIL可以有效地融合来自不同模态的信息，从而实现更鲁棒的控制。\n\n**技术框架**：COIL框架包含以下主要模块：1) 任务表示模块，用于定义物体上的关键点及其预期运动；2) 条件策略模块，该策略接收视觉输入和任务表示作为输入，并生成相应的动作；3) 时空注意力模块，用于融合来自不同模态的信息，并关注与当前任务相关的关键点；4) 自监督训练流程，利用模拟数据和事后对应标签来训练策略。\n\n**关键创新**：COIL的关键创新在于其面向对应关系的任务表示和时空注意力机制。与现有方法相比，COIL能够处理具有可变空间和时间粒度的任务规范，并且能够更有效地利用多模态信息。此外，COIL的自监督训练流程使其能够利用大量的模拟数据进行训练，从而提高其泛化能力。\n\n**关键设计**：COIL使用Transformer网络作为其条件策略的基础架构，并引入了时空注意力机制来融合视觉输入和任务表示。损失函数包括模仿学习损失和正则化项，以鼓励策略学习到平滑的动作。自监督训练流程利用事后对应标签来生成训练数据，从而避免了手动标注的需要。具体参数设置（如Transformer层数、注意力头数等）未知。",
            "application_zh": "COIL具有广泛的应用前景，例如机器人操作、自动化装配、人机协作等。它可以用于控制机器人执行各种复杂的任务，例如抓取、放置、组装等。此外，COIL还可以用于开发更智能的人机交互系统，使人类能够更自然地与机器人进行交互。",
            "highlight_zh": "COIL在真实世界操作任务中取得了显著的性能提升。与现有方法相比，COIL在稀疏和密集规范下均表现出更强的泛化能力和更高的成功率。具体性能数据未知，但论文强调了COIL在不同任务、对象和运动模式下的优越性。",
            "tags_zh": [
                "模仿学习",
                "视觉运动控制",
                "对应关系",
                "时空注意力",
                "机器人操作"
            ],
            "_index": 114,
            "_used_api": "gemini"
        },
        {
            "title": "See in Depth: Training-Free Surgical Scene Segmentation with Monocular Depth Priors",
            "authors": [
                "Kunyi Yang",
                "Qingyu Wang",
                "Cheng Yuan",
                "Yutong Ban"
            ],
            "arxiv_id": "2512.05529v1",
            "summary": "Pixel-wise segmentation of laparoscopic scenes is essential for computer-assisted surgery but difficult to scale due to the high cost of dense annotations. We propose depth-guided surgical scene segmentation (DepSeg), a training-free framework that utilizes monocular depth as a geometric prior together with pretrained vision foundation models. DepSeg first estimates a relative depth map with a pretrained monocular depth estimation network and proposes depth-guided point prompts, which SAM2 converts into class-agnostic masks. Each mask is then described by a pooled pretrained visual feature and classified via template matching against a template bank built from annotated frames. On the CholecSeg8k dataset, DepSeg improves over a direct SAM2 auto segmentation baseline (35.9% vs. 14.7% mIoU) and maintains competitive performance even when using only 10--20% of the object templates. These results show that depth-guided prompting and template-based classification offer an annotation-efficient segmentation approach.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "The first two authors contributed equally",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05529v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "depth estimation",
                        "[T]monocular depth"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于单目深度先验的无训练手术场景分割方法DepSeg",
            "summary_zh": "腹腔镜场景的像素级分割对于计算机辅助手术至关重要，但由于密集标注的高成本而难以扩展。我们提出了一种深度引导的手术场景分割框架(DepSeg)，该框架利用单目深度作为几何先验，并结合预训练的视觉基础模型，无需训练。DepSeg首先使用预训练的单目深度估计网络估计相对深度图，并提出深度引导的点提示，SAM2将其转换为类别无关的掩码。然后，每个掩码由一个池化的预训练视觉特征描述，并通过模板匹配针对从带注释的帧构建的模板库进行分类。在CholecSeg8k数据集上，DepSeg优于直接的SAM2自动分割基线（35.9% vs. 14.7% mIoU），即使仅使用10-20%的对象模板也能保持有竞争力的性能。这些结果表明，深度引导的提示和基于模板的分类提供了一种注释高效的分割方法。",
            "intro_zh": [
                "现有腹腔镜手术场景分割方法依赖大量标注数据，成本高昂且难以扩展。",
                "DepSeg利用单目深度估计作为几何先验，结合预训练视觉模型，实现无训练的场景分割。",
                "实验表明，DepSeg在分割精度上显著优于直接使用SAM2的方法，且对模板数量不敏感。"
            ],
            "method_zh": "**问题定义**：论文旨在解决腹腔镜手术场景中，由于缺乏大量标注数据而导致的像素级分割难题。现有方法依赖于大量的像素级标注，这在医疗领域非常耗时且成本高昂，限制了其可扩展性。因此，需要一种无需训练或仅需少量标注数据就能实现精确分割的方法。\\n\\n**核心思路**：论文的核心思路是利用单目深度估计作为几何先验信息，引导分割过程。通过预训练的单目深度估计网络获取场景的深度信息，并将其转化为点提示，从而引导SAM2生成类别无关的掩码。然后，利用模板匹配的方式，将这些掩码与少量标注样本进行匹配，实现最终的分割。这种方法的核心在于利用深度信息减少对大量标注数据的依赖。\\n\\n**技术框架**：DepSeg框架主要包含以下几个阶段：1) **单目深度估计**：使用预训练的单目深度估计网络估计输入图像的相对深度图。2) **深度引导的点提示**：根据深度图生成点提示，用于引导SAM2生成类别无关的掩码。3) **掩码生成**：使用SAM2将点提示转换为类别无关的掩码。4) **特征提取**：对每个掩码区域提取预训练视觉模型的特征。5) **模板匹配**：将提取的特征与从少量标注样本构建的模板库进行匹配，从而确定掩码的类别。\\n\\n**关键创新**：该方法最重要的创新点在于将单目深度估计与预训练视觉模型相结合，实现了一种无需训练或仅需少量标注数据的腹腔镜手术场景分割方法。与传统的监督学习方法相比，该方法显著降低了对标注数据的需求，提高了可扩展性。与直接使用SAM2等通用分割模型相比，该方法利用深度信息作为先验，提高了分割精度。\\n\\n**关键设计**：在深度引导的点提示方面，论文根据深度图的分布选择具有代表性的点作为提示。在特征提取方面，论文使用预训练视觉模型的池化特征来描述掩码区域，以提高特征的鲁棒性。在模板匹配方面，论文使用余弦相似度作为匹配度量，并设置阈值来过滤不匹配的掩码。",
            "application_zh": "该研究成果可应用于计算机辅助手术系统，例如机器人辅助手术。通过实时分割手术场景，可以帮助医生更好地理解手术过程，提高手术精度和安全性。此外，该方法还可以应用于医学图像分析、手术机器人导航等领域，具有广阔的应用前景。",
            "highlight_zh": "DepSeg在CholecSeg8k数据集上取得了显著的性能提升，mIoU达到35.9%，远高于直接使用SAM2的14.7%。更重要的是，即使仅使用10-20%的对象模板，DepSeg也能保持有竞争力的性能，这表明该方法具有很高的标注效率和泛化能力。",
            "tags_zh": [
                "手术场景分割",
                "单目深度估计",
                "无训练学习",
                "视觉基础模型",
                "模板匹配",
                "计算机辅助手术",
                "腹腔镜手术"
            ],
            "_index": 115,
            "_used_api": "gemini"
        },
        {
            "title": "MAFNet:Multi-frequency Adaptive Fusion Network for Real-time Stereo Matching",
            "authors": [
                "Ao Xu",
                "Rujin Zhao",
                "Xiong Xu",
                "Boceng Huang",
                "Yujia Jia",
                "Hongfeng Long",
                "Fuxuan Chen",
                "Zilong Cao",
                "Fangyuan Chen"
            ],
            "arxiv_id": "2512.04358v1",
            "summary": "Existing stereo matching networks typically rely on either cost-volume construction based on 3D convolutions or deformation methods based on iterative optimization. The former incurs significant computational overhead during cost aggregation, whereas the latter often lacks the ability to model non-local contextual information. These methods exhibit poor compatibility on resource-constrained mobile devices, limiting their deployment in real-time applications. To address this, we propose a Multi-frequency Adaptive Fusion Network (MAFNet), which can produce high-quality disparity maps using only efficient 2D convolutions. Specifically, we design an adaptive frequency-domain filtering attention module that decomposes the full cost volume into high-frequency and low-frequency volumes, performing frequency-aware feature aggregation separately. Subsequently, we introduce a Linformer-based low-rank attention mechanism to adaptively fuse high- and low-frequency information, yielding more robust disparity estimation. Extensive experiments demonstrate that the proposed MAFNet significantly outperforms existing real-time methods on public datasets such as Scene Flow and KITTI 2015, showing a favorable balance between accuracy and real-time performance.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04358v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]stereo matching",
                        "disparity estimation"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出MAFNet，通过多频自适应融合网络实现实时高精度立体匹配",
            "summary_zh": "现有的立体匹配网络通常依赖于基于3D卷积的代价体构建或基于迭代优化的形变方法。前者在代价聚合过程中产生显著的计算开销，而后者通常缺乏建模非局部上下文信息的能力。这些方法在资源受限的移动设备上的兼容性较差，限制了它们在实时应用中的部署。为了解决这个问题，我们提出了一种多频自适应融合网络(MAFNet)，它仅使用高效的2D卷积即可生成高质量的视差图。具体来说，我们设计了一个自适应频域滤波注意力模块，将完整的代价体分解为高频和低频体，分别执行频率感知的特征聚合。随后，我们引入了一种基于Linformer的低秩注意力机制，自适应地融合高频和低频信息，从而产生更鲁棒的视差估计。大量的实验表明，所提出的MAFNet在Scene Flow和KITTI 2015等公共数据集上显著优于现有的实时方法，在精度和实时性能之间取得了良好的平衡。",
            "intro_zh": [
                "现有立体匹配方法在计算代价体或建模非局部上下文信息方面存在不足，难以在移动设备上实现实时应用。",
                "MAFNet通过自适应频域滤波注意力模块分解代价体，并利用Linformer低秩注意力机制融合高低频信息，实现高效视差估计。",
                "实验表明，MAFNet在Scene Flow和KITTI 2015数据集上优于现有实时方法，实现了精度和实时性的平衡。"
            ],
            "method_zh": "**问题定义**：现有立体匹配网络，如基于3D卷积代价体的方法，计算量大，难以实时；基于形变优化的方法，缺乏非局部上下文建模能力。这些问题限制了它们在移动设备等资源受限平台上的应用。\\n\\n**核心思路**：将代价体分解为高频和低频部分，分别进行处理，然后自适应地融合它们。这种方法旨在降低计算复杂度，同时保留重要的频率信息，从而提高精度和效率。\\n\\n**技术框架**：MAFNet包含以下主要模块：1) 特征提取网络（未明确说明具体网络结构，但推测为常见的卷积神经网络）；2) 自适应频域滤波注意力模块，用于将代价体分解为高频和低频部分，并进行频率感知的特征聚合；3) 基于Linformer的低秩注意力机制，用于自适应融合高频和低频信息；4) 视差回归层，用于预测最终的视差图。\\n\\n**关键创新**：主要创新点在于：1) 提出自适应频域滤波注意力模块，将代价体分解为高频和低频部分，分别处理；2) 使用Linformer低秩注意力机制，降低计算复杂度，同时实现高低频信息的有效融合。\\n\\n**关键设计**：1) 自适应频域滤波注意力模块的具体实现细节（例如，如何进行频率分解，如何设计频率感知的特征聚合方式）未知；2) Linformer低秩注意力机制的具体参数设置未知；3) 损失函数的设计未知；4) 特征提取网络的具体结构未知。",
            "application_zh": "MAFNet具有广泛的应用前景，包括自动驾驶、机器人导航、三维重建、虚拟现实和增强现实等领域。其高效的计算性能使其能够部署在资源受限的移动设备上，为这些应用提供实时的深度感知能力。该研究的未来影响在于推动立体匹配技术在嵌入式系统和移动平台上的普及。",
            "highlight_zh": "实验结果表明，MAFNet在Scene Flow和KITTI 2015数据集上显著优于现有的实时立体匹配方法。具体性能数据和对比基线未在摘要中明确给出，但强调了该方法在精度和实时性之间取得了良好的平衡，表明其在实际应用中具有优势。",
            "tags_zh": [
                "立体匹配",
                "深度估计",
                "实时性",
                "频域分析",
                "注意力机制"
            ],
            "_index": 116,
            "_used_api": "gemini"
        },
        {
            "title": "Gamma-from-Mono: Road-Relative, Metric, Self-Supervised Monocular Geometry for Vehicular Applications",
            "authors": [
                "Gasser Elazab",
                "Maximilian Jansen",
                "Michael Unterreiner",
                "Olaf Hellwich"
            ],
            "arxiv_id": "2512.04303v1",
            "summary": "Accurate perception of the vehicle's 3D surroundings, including fine-scale road geometry, such as bumps, slopes, and surface irregularities, is essential for safe and comfortable vehicle control. However, conventional monocular depth estimation often oversmooths these features, losing critical information for motion planning and stability. To address this, we introduce Gamma-from-Mono (GfM), a lightweight monocular geometry estimation method that resolves the projective ambiguity in single-camera reconstruction by decoupling global and local structure. GfM predicts a dominant road surface plane together with residual variations expressed by gamma, a dimensionless measure of vertical deviation from the plane, defined as the ratio of a point's height above it to its depth from the camera, and grounded in established planar parallax geometry. With only the camera's height above ground, this representation deterministically recovers metric depth via a closed form, avoiding full extrinsic calibration and naturally prioritizing near-road detail. Its physically interpretable formulation makes it well suited for self-supervised learning, eliminating the need for large annotated datasets. Evaluated on KITTI and the Road Surface Reconstruction Dataset (RSRD), GfM achieves state-of-the-art near-field accuracy in both depth and gamma estimation while maintaining competitive global depth performance. Our lightweight 8.88M-parameter model adapts robustly across diverse camera setups and, to our knowledge, is the first self-supervised monocular approach evaluated on RSRD.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "Accepted in 3DV 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04303v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "motion planning"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "depth estimation",
                        "monocular depth",
                        "metric depth"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "1_robot_core",
                "3_perception_slam"
            ],
            "headline_zh": "提出Gamma-from-Mono，用于车辆应用中道路相对、度量、自监督单目几何估计",
            "summary_zh": "精确感知车辆周围的3D环境，包括道路的精细几何结构（如颠簸、斜坡和表面不规则性），对于安全舒适的车辆控制至关重要。然而，传统的单目深度估计通常会过度平滑这些特征，丢失了运动规划和稳定性所需的关键信息。为了解决这个问题，我们提出了一种轻量级的单目几何估计方法Gamma-from-Mono (GfM)，它通过解耦全局和局部结构来解决单相机重建中的投影模糊性。GfM预测一个主要的道路表面平面，以及由gamma表示的残余变化，gamma是垂直于该平面的偏差的无量纲度量，定义为点的高度与相机深度的比率，并基于已建立的平面视差几何。仅使用相机离地高度，这种表示就可以通过闭式解确定性地恢复度量深度，避免了完整的外参校准，并自然地优先考虑近路细节。其物理上可解释的公式使其非常适合自监督学习，无需大型带注释的数据集。在KITTI和道路表面重建数据集（RSRD）上的评估表明，GfM在深度和gamma估计方面都实现了最先进的近场精度，同时保持了具有竞争力的全局深度性能。我们轻量级的8.88M参数模型能够稳健地适应各种相机设置，并且据我们所知，是第一个在RSRD上评估的自监督单目方法。",
            "intro_zh": [
                "传统单目深度估计在道路场景中过度平滑几何细节，导致车辆控制所需关键信息丢失。",
                "GfM通过解耦全局道路平面和局部偏差，利用gamma值表示局部几何变化，实现更精确的近场深度估计。",
                "GfM仅需相机高度，通过闭式解恢复度量深度，避免了复杂外参校准，并在KITTI和RSRD上取得SOTA近场精度。"
            ],
            "method_zh": "**问题定义**：现有单目深度估计方法在道路场景下，难以准确捕捉道路表面的精细几何结构，例如路面颠簸、斜坡等，这些细节对于车辆的运动规划和稳定性至关重要。传统方法倾向于过度平滑这些局部特征，导致关键信息的丢失。因此，需要一种能够更精确地估计道路几何结构的方法。\n\n**核心思路**：GfM的核心思路是将道路场景的几何结构分解为全局的道路平面和一个局部的偏差项。全局道路平面提供了一个整体的参考框架，而局部偏差则通过gamma值来表示，gamma值定义为点到道路平面的垂直距离与相机到该点深度的比值。这种分解方式能够有效地解耦全局和局部结构，从而更精确地估计道路几何。\n\n**技术框架**：GfM的整体框架包括以下几个主要步骤：1) 使用单目图像作为输入；2) 预测一个主要的道路表面平面；3) 预测gamma值，即每个像素点相对于道路平面的垂直偏差；4) 利用相机高度信息和预测的道路平面以及gamma值，通过闭式解计算出每个像素点的度量深度。整个过程是端到端可训练的，并且可以采用自监督的方式进行训练。\n\n**关键创新**：GfM最重要的技术创新点在于其gamma值的引入和使用。gamma值是一种无量纲的度量，它能够有效地表示局部几何变化，并且与平面视差几何有着紧密的联系。通过gamma值，GfM能够更好地捕捉道路表面的精细结构，从而提高近场深度估计的精度。此外，GfM采用闭式解来计算度量深度，避免了复杂的优化过程，提高了计算效率。\n\n**关键设计**：GfM的关键设计包括：1) 使用轻量级的神经网络结构，参数量仅为8.88M，易于部署；2) 采用自监督学习的方式进行训练，无需大量的标注数据；3) 利用相机高度信息作为先验知识，提高深度估计的准确性；4) 设计合适的损失函数，例如深度损失和gamma损失，以优化网络的训练。",
            "application_zh": "GfM在自动驾驶、高级驾驶辅助系统（ADAS）等领域具有广泛的应用前景。它可以用于提高车辆对道路环境的感知能力，从而改善车辆的运动规划和控制，提高行驶安全性。例如，GfM可以帮助车辆更好地识别路面颠簸，从而调整悬挂系统，提高乘坐舒适性。此外，GfM还可以用于道路维护和基础设施建设，例如通过分析道路表面的几何结构，评估道路的损坏程度。",
            "highlight_zh": "GfM在KITTI和RSRD数据集上进行了评估，取得了最先进的近场深度估计精度。在RSRD数据集上，GfM是第一个被评估的自监督单目方法。实验结果表明，GfM能够有效地捕捉道路表面的精细几何结构，并且具有良好的泛化能力，能够适应不同的相机设置。此外，GfM的轻量级模型使其易于部署在实际的车辆平台上。",
            "tags_zh": [
                "单目深度估计",
                "自监督学习",
                "道路几何估计",
                "车辆感知",
                "平面视差",
                "近场精度",
                "度量深度"
            ],
            "_index": 117,
            "_used_api": "gemini"
        },
        {
            "title": "OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance",
            "authors": [
                "Lei Zhang",
                "Diwen Zheng",
                "Kaixin Bai",
                "Zhenshan Bing",
                "Zoltan-Csaba Marton",
                "Zhaopeng Chen",
                "Alois Christian Knoll",
                "Jianwei Zhang"
            ],
            "arxiv_id": "2512.03874v1",
            "summary": "Dexterous grasp generation aims to produce grasp poses that align with task requirements and human interpretable grasp semantics. However, achieving semantically controllable dexterous grasp synthesis remains highly challenging due to the lack of unified modeling of multiple semantic dimensions, including grasp taxonomy, contact semantics, and functional affordance. To address these limitations, we present OmniDexVLG, a multimodal, semantics aware grasp generation framework capable of producing structurally diverse and semantically coherent dexterous grasps under joint language and visual guidance. Our approach begins with OmniDexDataGen, a semantic rich dexterous grasp dataset generation pipeline that integrates grasp taxonomy guided configuration sampling, functional affordance contact point sampling, taxonomy aware differential force closure grasp sampling, and physics based optimization and validation, enabling systematic coverage of diverse grasp types. We further introduce OmniDexReasoner, a multimodal grasp type semantic reasoning module that leverages multi agent collaboration, retrieval augmented generation, and chain of thought reasoning to infer grasp related semantics and generate high quality annotations that align language instructions with task specific grasp intent. Building upon these components, we develop a unified Vision Language Grasping generation model that explicitly incorporates grasp taxonomy, contact structure, and functional affordance semantics, enabling fine grained control over grasp synthesis from natural language instructions. Extensive experiments in simulation and real world object grasping and ablation studies demonstrate that our method substantially outperforms state of the art approaches in terms of grasp diversity, contact semantic diversity, functional affordance diversity, and semantic consistency.",
            "categories": [
                "cs.RO",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "Project Website: https://sites.google.com/view/omnidexvlg, 16 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03874v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "grasping",
                        "[T]grasp"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "OmniDexVLG：提出基于视觉语言模型引导的灵巧抓取生成框架，实现语义可控的抓取合成。",
            "summary_zh": "本文提出OmniDexVLG，一个多模态、语义感知的抓取生成框架，能够在联合语言和视觉引导下，生成结构多样且语义连贯的灵巧抓取。该方法首先构建了OmniDexDataGen，一个语义丰富的灵巧抓取数据集生成流程，集成了抓取分类引导的配置采样、功能可供性接触点采样、分类感知的微分力闭合抓取采样以及基于物理的优化和验证，从而系统地覆盖了各种抓取类型。进一步引入OmniDexReasoner，一个多模态抓取类型语义推理模块，利用多智能体协作、检索增强生成和思维链推理来推断抓取相关的语义，并生成高质量的标注，使语言指令与特定任务的抓取意图对齐。在此基础上，开发了一个统一的视觉语言抓取生成模型，显式地结合了抓取分类、接触结构和功能可供性语义，从而能够通过自然语言指令对抓取合成进行细粒度的控制。在模拟和真实物体抓取中的大量实验以及消融研究表明，该方法在抓取多样性、接触语义多样性、功能可供性多样性和语义一致性方面显著优于现有方法。",
            "intro_zh": [
                "现有灵巧抓取生成方法缺乏对抓取分类、接触语义和功能可供性等多维度语义的统一建模，难以实现语义可控的抓取合成。",
                "OmniDexVLG通过构建语义丰富的抓取数据集和多模态语义推理模块，显式地结合抓取分类、接触结构和功能可供性语义，实现细粒度的抓取控制。",
                "实验结果表明，OmniDexVLG在抓取多样性、接触语义多样性、功能可供性多样性和语义一致性方面均优于现有方法。"
            ],
            "method_zh": "**问题定义**：现有灵巧抓取生成方法难以统一建模抓取分类、接触语义和功能可供性等多维度语义，导致无法根据任务需求和人类可解释的抓取语义生成抓取姿态，缺乏语义可控性。现有方法难以将自然语言指令与特定任务的抓取意图对齐，限制了抓取的灵活性和泛化能力。\\n\\n**核心思路**：OmniDexVLG的核心思路是构建一个多模态、语义感知的抓取生成框架，通过显式地结合抓取分类、接触结构和功能可供性语义，实现对抓取合成的细粒度控制。该方法利用视觉语言模型引导抓取语义的学习，从而能够根据自然语言指令生成结构多样且语义连贯的灵巧抓取。\\n\\n**技术框架**：OmniDexVLG包含三个主要模块：OmniDexDataGen、OmniDexReasoner和视觉语言抓取生成模型。OmniDexDataGen负责生成语义丰富的灵巧抓取数据集，涵盖各种抓取类型。OmniDexReasoner是一个多模态抓取类型语义推理模块，用于推断抓取相关的语义，并将语言指令与特定任务的抓取意图对齐。视觉语言抓取生成模型则利用这两个模块的输出，生成最终的抓取姿态。\\n\\n**关键创新**：OmniDexVLG的关键创新在于：1) 提出了OmniDexDataGen，一个语义丰富的灵巧抓取数据集生成流程，能够系统地覆盖各种抓取类型；2) 引入了OmniDexReasoner，一个多模态抓取类型语义推理模块，能够利用多智能体协作、检索增强生成和思维链推理来推断抓取相关的语义；3) 开发了一个统一的视觉语言抓取生成模型，显式地结合了抓取分类、接触结构和功能可供性语义。\\n\\n**关键设计**：OmniDexDataGen中，采用了抓取分类引导的配置采样、功能可供性接触点采样、分类感知的微分力闭合抓取采样以及基于物理的优化和验证等技术，以确保数据集的多样性和质量。OmniDexReasoner中，利用了多智能体协作、检索增强生成和思维链推理等技术，以提高语义推理的准确性和效率。视觉语言抓取生成模型中，具体网络结构和损失函数等细节未在摘要中明确提及，属于未知信息。",
            "application_zh": "OmniDexVLG在机器人灵巧操作领域具有广泛的应用前景，例如自动化装配、医疗手术、家庭服务等。该研究能够提升机器人在复杂环境下的抓取能力，使其能够更好地理解人类指令，并完成各种精细操作任务。未来，该技术有望应用于智能制造、智能医疗和智能家居等领域。",
            "highlight_zh": "实验结果表明，OmniDexVLG在抓取多样性、接触语义多样性、功能可供性多样性和语义一致性方面显著优于现有方法。具体的性能数据和对比基线未在摘要中明确提及，属于未知信息。但摘要强调了该方法在多个关键指标上的优越性，表明其具有显著的实际效果。",
            "tags_zh": [
                "灵巧抓取",
                "视觉语言模型",
                "多模态学习",
                "语义推理",
                "机器人操作"
            ],
            "_index": 118,
            "_used_api": "gemini"
        },
        {
            "title": "Bayesian Optimization for Automatic Tuning of Torque-Level Nonlinear Model Predictive Control",
            "authors": [
                "Gabriele Fadini",
                "Deepak Ingole",
                "Tong Duy Son",
                "Alisa Rupenyan"
            ],
            "arxiv_id": "2512.03772v1",
            "summary": "This paper presents an auto-tuning framework for torque-based Nonlinear Model Predictive Control (nMPC), where the MPC serves as a real-time controller for optimal joint torque commands. The MPC parameters, including cost function weights and low-level controller gains, are optimized using high-dimensional Bayesian Optimization (BO) techniques, specifically Sparse Axis-Aligned Subspace (SAASBO) with a digital twin (DT) to achieve precise end-effector trajectory real-time tracking on an UR10e robot arm. The simulation model allows efficient exploration of the high-dimensional parameter space, and it ensures safe transfer to hardware. Our simulation results demonstrate significant improvements in tracking performance (+41.9%) and reduction in solve times (-2.5%) compared to manually-tuned parameters. Moreover, experimental validation on the real robot follows the trend (with a +25.8% improvement), emphasizing the importance of digital twin-enabled automated parameter optimization for robotic operations.",
            "categories": [
                "cs.RO",
                "cs.AI",
                "eess.SY"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "6 pages, 7 figures, 3 tables",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03772v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "MPC",
                        "[T]model predictive control"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出基于贝叶斯优化的力矩级非线性模型预测控制自动调参框架",
            "summary_zh": "本文提出了一种基于力矩的非线性模型预测控制（nMPC）的自动调参框架，该MPC作为实时控制器，用于优化关节力矩指令。利用高维贝叶斯优化（BO）技术，特别是带有数字孪生（DT）的稀疏轴对齐子空间（SAASBO），对MPC参数（包括成本函数权重和低层控制器增益）进行优化，以在UR10e机器人手臂上实现精确的末端执行器轨迹实时跟踪。仿真模型能够有效地探索高维参数空间，并确保安全地转移到硬件。仿真结果表明，与手动调整的参数相比，跟踪性能显著提高（+41.9%），求解时间减少（-2.5%）。此外，在真实机器人上的实验验证也遵循了这一趋势（改进+25.8%），强调了数字孪生支持的自动化参数优化对于机器人操作的重要性。",
            "intro_zh": [
                "现有机器人控制器的参数整定依赖手动调整，耗时且难以达到最优，尤其是在高维参数空间中。",
                "利用数字孪生技术，结合稀疏轴对齐子空间贝叶斯优化（SAASBO），实现nMPC控制器参数的自动优化。",
                "实验结果表明，该方法在仿真和真实机器人上均能显著提升轨迹跟踪性能，并减少求解时间。"
            ],
            "method_zh": "**问题定义**：论文旨在解决力矩级非线性模型预测控制（nMPC）中参数手动调整的问题。手动调整MPC参数，如成本函数权重和低层控制器增益，是一个耗时且低效的过程，尤其是在高维参数空间中，难以找到最优参数组合，从而限制了控制器的性能。\\n\\n**核心思路**：论文的核心思路是利用贝叶斯优化（BO）算法自动搜索nMPC控制器的最优参数。通过构建机器人系统的数字孪生模型，可以在仿真环境中高效地探索参数空间，降低了在真实机器人上进行实验的风险和成本。SAASBO算法能够有效地处理高维参数空间，找到最优参数组合。\\n\\n**技术框架**：整体框架包含以下几个主要模块：1) 数字孪生模型：构建UR10e机器人手臂的精确仿真模型，用于评估不同参数组合下的控制性能。2) SAASBO优化器：使用SAASBO算法在高维参数空间中搜索最优参数。3) nMPC控制器：基于优化的参数，生成关节力矩指令，控制机器人手臂的运动。4) 实验验证：将优化后的参数部署到真实机器人上，验证其控制性能。\\n\\n**关键创新**：论文的关键创新在于将SAASBO算法与数字孪生技术相结合，实现了nMPC控制器的自动调参。SAASBO算法能够有效地处理高维参数空间，找到最优参数组合，而数字孪生技术则降低了实验风险和成本，提高了优化效率。\\n\\n**关键设计**：论文中，成本函数权重和低层控制器增益被选为优化参数。SAASBO算法使用高斯过程作为代理模型，并通过采集函数（如期望改进）来选择下一个要评估的参数组合。数字孪生模型需要足够精确，以保证仿真结果与真实机器人上的性能一致性。实验中，使用了UR10e机器人手臂，并评估了其在轨迹跟踪任务中的性能。",
            "application_zh": "该研究成果可广泛应用于机器人自动化领域，例如工业机器人、服务机器人等。通过自动优化控制参数，可以提高机器人的运动精度、效率和鲁棒性，降低人工调试成本，并加速机器人系统的部署。未来，该方法可以扩展到更复杂的机器人系统和控制任务中。",
            "highlight_zh": "实验结果表明，与手动调整的参数相比，该方法在仿真环境中将轨迹跟踪性能提高了41.9%，求解时间减少了2.5%。在真实机器人上的实验验证也显示，轨迹跟踪性能提高了25.8%。这些结果表明，该方法能够有效地优化nMPC控制器的参数，显著提升机器人控制性能。",
            "tags_zh": [
                "贝叶斯优化",
                "非线性模型预测控制",
                "数字孪生",
                "机器人控制",
                "自动调参"
            ],
            "_index": 119,
            "_used_api": "gemini"
        },
        {
            "title": "Prediction-Driven Motion Planning: Route Integration Strategies in Attention-Based Prediction Models",
            "authors": [
                "Marlon Steiner",
                "Royden Wagner",
                "Ömer Sahin Tas",
                "Christoph Stiller"
            ],
            "arxiv_id": "2512.03756v1",
            "summary": "Combining motion prediction and motion planning offers a promising framework for enhancing interactions between automated vehicles and other traffic participants. However, this introduces challenges in conditioning predictions on navigation goals and ensuring stable, kinematically feasible trajectories. Addressing the former challenge, this paper investigates the extension of attention-based motion prediction models with navigation information. By integrating the ego vehicle's intended route and goal pose into the model architecture, we bridge the gap between multi-agent motion prediction and goal-based motion planning. We propose and evaluate several architectural navigation integration strategies to our model on the nuPlan dataset. Our results demonstrate the potential of prediction-driven motion planning, highlighting how navigation information can enhance both prediction and planning tasks. Our implementation is at: https://github.com/KIT-MRT/future-motion.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "In Proceedings of the IEEE International Conference on Intelligent Transportation Systems (ITSC), Gold Coast, AUSTRALIA, 18-21 November 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03756v1",
            "code_links": [
                {
                    "url": "https://github.com/KIT-MRT/future-motion",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]motion planning"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "navigation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "1_robot_core",
                "3_perception_slam"
            ],
            "headline_zh": "提出基于注意力机制的预测模型，融合导航信息以提升自动驾驶车辆交互能力",
            "summary_zh": "本文研究了将导航信息融入基于注意力机制的运动预测模型，旨在增强自动驾驶车辆与交通参与者之间的交互。通过将自车规划路线和目标位姿整合到模型架构中，弥合了多智能体运动预测和基于目标的运动规划之间的差距。论文提出并在nuPlan数据集上评估了几种架构上的导航信息集成策略。实验结果表明，预测驱动的运动规划具有潜力，导航信息能够同时提升预测和规划任务的性能。代码已开源。",
            "intro_zh": [
                "现有方法难以将导航目标融入运动预测，导致自动驾驶车辆难以进行合理的交互行为规划。",
                "论文提出将自车规划路线和目标位姿融入基于注意力机制的运动预测模型，实现预测与规划的协同。",
                "在nuPlan数据集上的实验表明，该方法能够有效提升运动预测和规划的性能，验证了预测驱动运动规划的潜力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自动驾驶场景下，如何将车辆的导航信息（例如规划路线和目标位姿）有效地融入到运动预测模型中，从而提升自动驾驶车辆与周围交通参与者交互的合理性和安全性。现有方法通常独立地进行运动预测和运动规划，忽略了两者之间的内在联系，导致预测结果与车辆的实际意图不一致，影响规划的有效性。\\n\\n**核心思路**：论文的核心思路是将导航信息作为先验知识，融入到基于注意力机制的运动预测模型中。通过这种方式，模型可以更好地理解自车的意图，从而预测出更符合实际情况的交通参与者行为。这种预测结果能够反过来指导运动规划，使得规划出的轨迹更加合理和安全。\\n\\n**技术框架**：整体框架包含三个主要部分：1) 环境感知模块，用于获取周围交通参与者的状态信息；2) 导航信息编码模块，用于将自车的规划路线和目标位姿编码成向量表示；3) 基于注意力机制的运动预测模块，该模块将环境感知信息和导航信息作为输入，预测未来一段时间内交通参与者的运动轨迹。论文重点研究了导航信息编码模块与运动预测模块的集成方式，提出了多种架构设计。\\n\\n**关键创新**：论文的关键创新在于提出了多种将导航信息融入到基于注意力机制的运动预测模型中的架构策略。这些策略允许模型显式地利用自车的导航信息，从而提升了预测的准确性和合理性。与传统的运动预测方法相比，该方法能够更好地捕捉交通参与者的意图，并预测出更符合实际情况的运动轨迹。\\n\\n**关键设计**：论文提出了多种导航信息集成策略，例如将导航信息直接拼接在输入特征中，或者通过注意力机制将导航信息融入到特征表示中。具体来说，导航信息包括自车规划路线上的关键点坐标和目标位姿。损失函数方面，论文采用了常用的轨迹预测损失函数，例如均方误差（MSE）或负对数似然（NLL）。网络结构方面，论文采用了基于Transformer的注意力机制，用于捕捉交通参与者之间的交互关系。",
            "application_zh": "该研究成果可应用于自动驾驶车辆的运动规划和决策控制，提升车辆在复杂交通环境下的安全性、效率和舒适性。通过更准确地预测其他交通参与者的行为，自动驾驶车辆可以更好地规划自身的行驶轨迹，避免碰撞，并实现更流畅的驾驶体验。此外，该技术还可应用于智能交通系统，例如交通流量预测和优化。",
            "highlight_zh": "论文在nuPlan数据集上进行了实验，验证了所提出方法的有效性。实验结果表明，通过将导航信息融入到运动预测模型中，可以显著提升预测的准确性和合理性。具体的性能提升数据在论文中给出，并与现有的运动预测方法进行了对比。实验结果还表明，不同的导航信息集成策略对预测性能的影响不同，需要根据具体的应用场景进行选择。",
            "tags_zh": [
                "运动预测",
                "运动规划",
                "注意力机制",
                "自动驾驶",
                "导航信息"
            ],
            "_index": 120,
            "_used_api": "gemini"
        },
        {
            "title": "MSG-Loc: Multi-Label Likelihood-based Semantic Graph Matching for Object-Level Global Localization",
            "authors": [
                "Gihyeon Lee",
                "Jungwoo Lee",
                "Juwon Kim",
                "Young-Sik Shin",
                "Younggun Cho"
            ],
            "arxiv_id": "2512.03522v2",
            "summary": "Robots are often required to localize in environments with unknown object classes and semantic ambiguity. However, when performing global localization using semantic objects, high semantic ambiguity intensifies object misclassification and increases the likelihood of incorrect associations, which in turn can cause significant errors in the estimated pose. Thus, in this letter, we propose a multi-label likelihood-based semantic graph matching framework for object-level global localization. The key idea is to exploit multi-label graph representations, rather than single-label alternatives, to capture and leverage the inherent semantic context of object observations. Based on these representations, our approach enhances semantic correspondence across graphs by combining the likelihood of each node with the maximum likelihood of its neighbors via context-aware likelihood propagation. For rigorous validation, data association and pose estimation performance are evaluated under both closed-set and open-set detection configurations. In addition, we demonstrate the scalability of our approach to large-vocabulary object categories in both real-world indoor scenes and synthetic environments.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-03",
            "updated": "2025-12-15",
            "comment": "Accepted in IEEE Robotics and Automation Letters (2025)",
            "doi": "10.1109/LRA.2025.3643293",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03522v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "pose estimation",
                        "[T]localization"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于多标签似然语义图匹配的物体级全局定位方法",
            "summary_zh": "本文提出了一种基于多标签似然的语义图匹配框架，用于物体级全局定位，旨在解决机器人需要在具有未知物体类别和语义模糊的环境中定位的问题。该方法利用多标签图表示，而非单标签图，来捕捉和利用物体观测的内在语义上下文。通过结合每个节点的似然性及其邻居的最大似然性，并进行上下文感知的似然传播，该方法增强了图之间的语义对应关系。在封闭集和开放集检测配置下，对数据关联和姿态估计性能进行了严格的验证。此外，本文还展示了该方法在真实室内场景和合成环境中对大型词汇物体类别的可扩展性。",
            "intro_zh": [
                "现有语义物体全局定位方法在语义模糊性高时，容易发生物体误分类和错误关联，导致姿态估计误差。",
                "该方法利用多标签图表示捕捉物体观测的语义上下文，并通过上下文感知的似然传播增强图间语义对应。",
                "实验在封闭集和开放集检测配置下验证了数据关联和姿态估计性能，并展示了方法对大型词汇物体类别的可扩展性。"
            ],
            "method_zh": "**问题定义**：现有的基于语义物体的全局定位方法在面对高语义模糊性的环境时，容易出现物体误分类和错误关联。这种错误的关联会导致姿态估计出现显著的偏差，使得机器人无法准确地确定自身的位置。因此，如何在高语义模糊的环境中实现鲁棒的物体级全局定位是一个关键问题。\\n\\n**核心思路**：本文的核心思路是利用多标签图表示来捕捉物体观测的内在语义上下文。与传统的单标签方法不同，多标签方法能够更全面地描述物体的语义信息，从而减少语义模糊带来的影响。此外，通过上下文感知的似然传播，该方法能够利用邻居节点的语义信息来增强当前节点的语义对应关系，进一步提高匹配的准确性。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1) 构建多标签语义图：将环境中的物体表示为图中的节点，节点之间的边表示物体之间的空间关系。每个节点包含多个标签，表示物体可能属于的类别。2) 计算节点似然性：根据观测到的物体特征，计算每个节点属于各个类别的似然性。3) 上下文感知的似然传播：利用邻居节点的似然性信息，通过最大似然估计来更新当前节点的似然性。4) 图匹配：利用更新后的节点似然性，进行图匹配，找到两幅图之间的最佳对应关系。5) 姿态估计：根据图匹配的结果，估计机器人的姿态。\\n\\n**关键创新**：该方法最重要的创新点在于使用多标签图表示和上下文感知的似然传播。多标签图表示能够更全面地描述物体的语义信息，减少语义模糊带来的影响。上下文感知的似然传播能够利用邻居节点的语义信息来增强当前节点的语义对应关系，提高匹配的准确性。与传统的单标签方法相比，该方法能够更鲁棒地处理高语义模糊的环境。\\n\\n**关键设计**：在多标签图构建中，需要选择合适的标签集合和标签之间的关系。在似然传播中，需要设计合适的传播规则和权重。损失函数的设计需要考虑匹配的准确性和姿态估计的精度。具体的参数设置需要根据实际的应用场景进行调整。例如，可以使用预训练的物体检测模型来提取物体特征，并使用图神经网络来学习节点之间的关系。",
            "application_zh": "该研究成果可应用于机器人导航、增强现实、三维重建等领域。尤其是在需要机器人自主定位的复杂环境中，例如仓库、家庭、办公室等，该方法能够提供更准确、更鲁棒的定位结果。未来，该方法可以进一步扩展到动态环境和大规模场景，为机器人提供更可靠的感知能力。",
            "highlight_zh": "实验结果表明，该方法在封闭集和开放集检测配置下均取得了良好的性能。在数据关联方面，该方法显著优于传统的单标签方法。在姿态估计方面，该方法能够提供更准确的姿态估计结果。此外，实验还验证了该方法在大型词汇物体类别下的可扩展性，表明该方法具有较强的实用价值。",
            "tags_zh": [
                "全局定位",
                "语义图匹配",
                "多标签学习",
                "机器人导航",
                "物体识别"
            ],
            "_index": 121,
            "_used_api": "gemini"
        },
        {
            "title": "Unifying Quadrotor Motion Planning and Control by Chaining Different Fidelity Models",
            "authors": [
                "Rudolf Reiter",
                "Chao Qin",
                "Leonard Bauersfeld",
                "Davide Scaramuzza"
            ],
            "arxiv_id": "2512.12427v1",
            "summary": "Many aerial tasks involving quadrotors demand both instant reactivity and long-horizon planning. High-fidelity models enable accurate control but are too slow for long horizons; low-fidelity planners scale but degrade closed-loop performance. We present Unique, a unified MPC that cascades models of different fidelity within a single optimization: a short-horizon, high-fidelity model for accurate control, and a long-horizon, low-fidelity model for planning. We align costs across horizons, derive feasibility-preserving thrust and body-rate constraints for the point-mass model, and introduce transition constraints that match the different states, thrust-induced acceleration, and jerk-body-rate relations. To prevent local minima emerging from nonsmooth clutter, we propose a 3D progressive smoothing schedule that morphs norm-based obstacles along the horizon. In addition, we deploy parallel randomly initialized MPC solvers to discover lower-cost local minima on the long, low-fidelity horizon. In simulation and real flights, under equal computational budgets, Unique improves closed-loop position or velocity tracking by up to 75% compared with standard MPC and hierarchical planner-tracker baselines. Ablations and Pareto analyses confirm robust gains across horizon variations, constraint approximations, and smoothing schedules.",
            "categories": [
                "cs.RO",
                "eess.SY"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-13",
            "updated": "2025-12-13",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12427v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "MPC",
                        "[T]motion planning"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "Unique：通过链式不同精度模型，统一四旋翼运动规划与控制",
            "summary_zh": "许多四旋翼飞行任务既需要即时响应，又需要长时程规划。高精度模型能够实现精确控制，但计算速度慢，不适用于长时程规划；低精度规划器可以扩展到长时程，但会降低闭环性能。我们提出了Unique，一种统一的MPC，它在单个优化中级联不同精度的模型：短时程高精度模型用于精确控制，长时程低精度模型用于规划。我们对齐了不同时程的代价函数，推导了点质量模型的可行性保持推力和机体角速率约束，并引入了过渡约束，以匹配不同的状态、推力引起的加速度和加加速度-机体角速率关系。为了防止非光滑杂波中出现局部最小值，我们提出了一种3D渐进平滑策略，该策略沿时程变形基于范数的障碍物。此外，我们部署了并行随机初始化的MPC求解器，以在长时程低精度范围内发现成本更低的局部最小值。在仿真和真实飞行中，在相同的计算预算下，与标准MPC和分层规划器-跟踪器基线相比，Unique将闭环位置或速度跟踪提高了高达75%。消融实验和帕累托分析证实了在时程变化、约束近似和平滑策略方面的稳健增益。",
            "intro_zh": [
                "四旋翼飞行任务需要在长时程规划和快速响应之间权衡，现有方法难以兼顾。",
                "Unique通过级联高低精度模型，在统一的MPC框架下实现长时程规划和精确控制。",
                "实验表明，Unique在计算资源相同的情况下，闭环跟踪性能比传统方法提升高达75%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决四旋翼飞行器运动规划与控制中，高精度模型计算量大不适用于长时程规划，而低精度模型控制精度不足的问题。现有方法如分层规划器-跟踪器，在长时程规划和短时程控制之间存在性能瓶颈。\\n\\n**核心思路**：论文的核心思路是利用模型预测控制（MPC）框架，将高精度模型和低精度模型进行级联，在高精度模型上进行精确控制，在低精度模型上进行长时程规划。通过代价函数对齐和过渡约束，保证不同精度模型之间的平滑过渡和一致性。\\n\\n**技术框架**：Unique的整体框架包含以下几个主要模块：1) 短时程高精度模型MPC：用于精确控制，模型复杂度高，计算量大。2) 长时程低精度模型MPC：用于长时程规划，模型简化，计算量小。3) 代价函数对齐：保证不同精度模型在优化目标上的一致性。4) 过渡约束：保证不同精度模型在状态、推力、加速度等方面的平滑过渡。5) 3D渐进平滑：防止在复杂环境中陷入局部最优。6) 并行随机初始化：探索更优的轨迹。\\n\\n**关键创新**：论文的关键创新在于将不同精度的模型集成到同一个MPC框架中，并设计了相应的代价函数对齐和过渡约束，使得高精度控制和长时程规划能够协同工作。此外，3D渐进平滑和并行随机初始化进一步提升了算法的鲁棒性和性能。\\n\\n**关键设计**：论文的关键设计包括：1) 推力和机体角速率约束，保证点质量模型的可行性。2) 过渡约束，匹配不同模型的状态、推力引起的加速度和加加速度-机体角速率关系。3) 3D渐进平滑策略，通过调整范数来平滑障碍物。4) 并行随机初始化MPC求解器，探索更优解。",
            "application_zh": "该研究成果可应用于需要长时程规划和精确控制的四旋翼飞行任务，例如复杂环境下的自主导航、高速飞行、编队飞行、以及需要与环境进行交互的机器人操作等。该方法能够提高四旋翼飞行器的自主性和适应性，具有重要的实际应用价值和潜在的商业前景。",
            "highlight_zh": "实验结果表明，在相同的计算预算下，Unique方法相比于标准MPC和分层规划器-跟踪器基线，在闭环位置或速度跟踪方面提高了高达75%。消融实验和帕累托分析验证了该方法在不同时程长度、约束近似和平滑策略下的鲁棒性。",
            "tags_zh": [
                "四旋翼",
                "运动规划",
                "模型预测控制",
                "多精度模型",
                "自主导航"
            ],
            "_index": 122,
            "_used_api": "gemini"
        },
        {
            "title": "Audio-Visual Camera Pose Estimation with Passive Scene Sounds and In-the-Wild Video",
            "authors": [
                "Daniel Adebi",
                "Sagnik Majumder",
                "Kristen Grauman"
            ],
            "arxiv_id": "2512.12165v2",
            "summary": "Understanding camera motion is a fundamental problem in embodied perception and 3D scene understanding. While visual methods have advanced rapidly, they often struggle under visually degraded conditions such as motion blur or occlusions. In this work, we show that passive scene sounds provide complementary cues for relative camera pose estimation for in-the-wild videos. We introduce a simple but effective audio-visual framework that integrates direction-ofarrival (DOA) spectra and binauralized embeddings into a state-of-the-art vision-only pose estimation model. Our results on two large datasets show consistent gains over strong visual baselines, plus robustness when the visual information is corrupted. To our knowledge, this represents the first work to successfully leverage audio for relative camera pose estimation in real-world videos, and it establishes incidental, everyday audio as an unexpected but promising signal for a classic spatial challenge. Project: http://vision.cs.utexas.edu/projects/av_camera_pose.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-13",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12165v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "scene understanding",
                        "[T]pose estimation"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出一种音视频融合的相机位姿估计方法，利用场景声音增强视觉信息，提升野外视频的鲁棒性。",
            "summary_zh": "本文提出了一种利用被动场景声音进行相机位姿估计的音视频融合框架，旨在解决视觉方法在运动模糊或遮挡等视觉退化条件下表现不佳的问题。该框架将声源方向（DOA）谱和双耳嵌入整合到先进的纯视觉位姿估计模型中。在两个大型数据集上的实验结果表明，该方法在强视觉基线之上实现了持续的性能提升，并且在视觉信息受损时表现出更强的鲁棒性。据我们所知，这是首次成功利用音频进行真实世界视频中相对相机位姿估计的研究，证明了偶然的日常音频可以作为解决经典空间挑战的一种意想不到但有前景的信号。",
            "intro_zh": [
                "视觉方法在相机位姿估计中面临视觉退化的挑战，如运动模糊和遮挡。",
                "利用场景中的被动声音，通过声源方向和双耳嵌入，辅助视觉信息进行位姿估计。",
                "实验表明，该方法在视觉信息受损时，仍能保持较好的鲁棒性，优于纯视觉方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决在视觉信息不足或受损的情况下，相机位姿估计的鲁棒性问题。现有的视觉方法在运动模糊、遮挡等情况下性能会显著下降，限制了其在真实世界场景中的应用。\\n\\n**核心思路**：论文的核心思路是利用场景中自然存在的音频信息作为视觉信息的补充。声音不受视觉遮挡的影响，并且能够提供关于场景几何和相机运动的线索。通过融合音频和视觉信息，可以提高位姿估计的准确性和鲁棒性。\\n\\n**技术框架**：该音视频融合框架主要包含以下几个模块：1) 视觉位姿估计模块：使用现有的先进视觉位姿估计模型作为基线。2) 音频特征提取模块：从音频信号中提取声源方向（DOA）谱和双耳嵌入特征。3) 特征融合模块：将音频和视觉特征进行融合，例如通过拼接或注意力机制。4) 位姿回归模块：利用融合后的特征回归相机位姿。\\n\\n**关键创新**：该研究的关键创新在于首次成功地将音频信息应用于真实世界视频中的相对相机位姿估计。以往的研究主要集中在视觉方法上，而该研究证明了音频作为一种补充信号的有效性。此外，该研究还提出了一种简单有效的音视频融合框架，可以方便地集成到现有的视觉位姿估计模型中。\\n\\n**关键设计**：音频特征提取方面，使用了声源方向（DOA）谱和双耳嵌入，这两种特征能够捕捉声音的空间信息和听觉感知信息。在特征融合方面，具体的设计细节（如拼接方式、注意力机制等）未在摘要中详细说明，属于未知信息。损失函数方面，可能使用了位姿回归常用的均方误差损失或Huber损失。",
            "application_zh": "该研究成果可应用于机器人导航、增强现实、虚拟现实、自动驾驶等领域。通过融合音频信息，可以提高这些系统在复杂环境下的感知能力和鲁棒性。例如，在机器人导航中，即使视觉传感器受到遮挡，机器人仍然可以利用声音信息进行定位和导航。在AR/VR中，可以提供更稳定和沉浸式的体验。",
            "highlight_zh": "该研究在两个大型数据集上进行了实验，结果表明，所提出的音视频融合方法在强视觉基线之上实现了持续的性能提升。更重要的是，该方法在视觉信息受损时表现出更强的鲁棒性，证明了音频信息作为视觉补充的有效性。具体的性能提升幅度未在摘要中给出，属于未知信息。",
            "tags_zh": [
                "相机位姿估计",
                "音视频融合",
                "场景声音",
                "方向估计",
                "鲁棒性",
                "多模态学习",
                "机器人感知"
            ],
            "_index": 123,
            "_used_api": "gemini"
        },
        {
            "title": "Taxonomy and Modular Tool System for Versatile and Effective Non-Prehensile Manipulations",
            "authors": [
                "Cedric-Pascal Sommer",
                "Robert J. Wood",
                "Justin Werfel"
            ],
            "arxiv_id": "2512.11080v1",
            "summary": "General-purpose robotic end-effectors of limited complexity, like the parallel-jaw gripper, are appealing for their balance of simplicity and effectiveness in a wide range of manipulation tasks. However, while many such manipulators offer versatility in grasp-like interactions, they are not optimized for non-prehensile actions like pressing, rubbing, or scraping -- manipulations needed for many common tasks. To perform such tasks, humans use a range of different body parts or tools with different rigidity, friction, etc., according to the properties most effective for a given task. Here, we discuss a taxonomy for the key properties of a non-actuated end-effector, laying the groundwork for a systematic understanding of the affordances of non-prehensile manipulators. We then present a modular tool system, based on the taxonomy, that can be used by a standard two-fingered gripper to extend its versatility and effectiveness in performing such actions. We demonstrate the application of the tool system in aerospace and household scenarios that require a range of non-prehensile and prehensile manipulations.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "34 pages, 10 figures, 2 tables, supplementary videos: https://youtu.be/Hcefy53PY0M, https://youtu.be/nFF9k91hsfU, https://youtu.be/EulPLskNIZQ",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11080v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation",
                        "grasp"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出非抓取操作工具模块化系统，扩展通用夹爪末端执行器的功能",
            "summary_zh": "通用机器人末端执行器，如平行爪夹持器，因其在广泛操作任务中简单性和有效性的平衡而备受欢迎。然而，虽然许多此类机械手在抓取式交互中具有通用性，但它们并未针对非抓取动作进行优化，例如按压、摩擦或刮擦——这些操作是许多常见任务所必需的。为了执行此类任务，人类会根据给定任务最有效的属性使用一系列不同的身体部位或工具，这些工具具有不同的刚度、摩擦力等。本文讨论了非驱动末端执行器的关键属性的分类，为系统地理解非抓取机械手的可供性奠定了基础。然后，我们提出了一个基于该分类的模块化工具系统，该系统可由标准双指夹持器使用，以扩展其在执行此类动作时的通用性和有效性。我们展示了该工具系统在航空航天和家庭场景中的应用，这些场景需要一系列非抓取和抓取操作。",
            "intro_zh": [
                "现有通用夹爪在非抓取操作（如按压、摩擦）方面存在局限性，无法满足复杂任务需求。",
                "论文提出一种模块化工具系统，通过扩展标准双指夹爪的功能，实现多样化的非抓取操作。",
                "该系统在航空航天和家庭场景中进行了验证，证明了其在抓取和非抓取操作中的有效性。"
            ],
            "method_zh": "**问题定义**：现有通用夹爪末端执行器虽然在抓取任务中表现良好，但在执行按压、摩擦、刮擦等非抓取操作时存在局限性。这些操作在许多实际任务中至关重要，而现有夹爪的设计并未对此进行优化。因此，需要一种方法来扩展通用夹爪的功能，使其能够有效地执行这些非抓取操作。\\n\\n**核心思路**：论文的核心思路是借鉴人类使用不同工具执行不同任务的策略，设计一个模块化的工具系统，该系统可以与标准双指夹爪配合使用，从而扩展其功能。通过更换不同的工具模块，夹爪可以适应不同的非抓取操作需求，例如使用刚性工具进行按压，使用摩擦力大的工具进行摩擦。\\n\\n**技术框架**：该方法主要包含两个部分：一是建立非驱动末端执行器的关键属性分类，二是基于该分类设计模块化工具系统。该分类用于指导工具模块的设计，确保工具模块能够覆盖各种非抓取操作所需的不同属性（如刚度、摩擦力）。模块化工具系统允许用户根据具体任务选择合适的工具模块，并将其安装到标准双指夹爪上。\\n\\n**关键创新**：该论文的关键创新在于提出了一个非驱动末端执行器的关键属性分类，并基于此设计了一个模块化的工具系统。该分类为系统地理解非抓取机械手的可供性奠定了基础，而模块化工具系统则提供了一种灵活且可扩展的方法来扩展通用夹爪的功能。与现有方法相比，该方法无需设计全新的末端执行器，而是通过简单的工具模块更换来实现功能的扩展，从而降低了成本和复杂性。\\n\\n**关键设计**：工具模块的设计需要考虑多个因素，包括材料选择、几何形状和连接方式。材料的选择需要根据工具模块的功能需求进行，例如，用于按压的工具模块需要具有较高的刚度，而用于摩擦的工具模块则需要具有较高的摩擦力。几何形状的设计需要考虑工具模块与目标物体的接触方式，以及工具模块在夹爪上的安装方式。连接方式需要确保工具模块能够牢固地安装在夹爪上，并且易于更换。",
            "application_zh": "该研究成果可广泛应用于需要抓取和非抓取操作的机器人应用中，例如航空航天、家庭服务、工业制造等领域。在航空航天领域，该系统可用于执行飞机维护任务，如清洁、打磨等。在家庭服务领域，该系统可用于执行清洁、擦拭等家务任务。在工业制造领域，该系统可用于执行装配、打磨等生产任务。该研究有望提高机器人的通用性和适应性，使其能够更好地服务于人类。",
            "highlight_zh": "论文通过在航空航天和家庭场景中的实验验证了该工具系统的有效性。实验结果表明，该系统能够有效地执行各种非抓取操作，并且能够与抓取操作无缝衔接。具体的性能数据和对比基线未在摘要中明确给出，但实验结果表明该系统能够显著提高通用夹爪在复杂任务中的表现。",
            "tags_zh": [
                "非抓取操作",
                "末端执行器",
                "模块化工具",
                "机器人操作",
                "通用夹爪"
            ],
            "_index": 124,
            "_used_api": "gemini"
        },
        {
            "title": "Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision",
            "authors": [
                "Wentao Zhou",
                "Xuweiyi Chen",
                "Vignesh Rajagopal",
                "Jeffrey Chen",
                "Rohan Chandra",
                "Zezhou Cheng"
            ],
            "arxiv_id": "2512.10956v1",
            "summary": "The success of foundation models in language and vision motivated research in fully end-to-end robot navigation foundation models (NFMs). NFMs directly map monocular visual input to control actions and ignore mid-level vision modules (tracking, depth estimation, etc) entirely. While the assumption that vision capabilities will emerge implicitly is compelling, it requires large amounts of pixel-to-action supervision that are difficult to obtain. The challenge is especially pronounced in dynamic and unstructured settings, where robust navigation requires precise geometric and dynamic understanding, while the depth-scale ambiguity in monocular views further limits accurate spatial reasoning. In this paper, we show that relying on monocular vision and ignoring mid-level vision priors is inefficient.\n  We present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking. Our intuition is straightforward: stereo inputs resolve the depth-scale ambiguity, and modern mid-level vision models provide reliable geometric and motion structure in dynamic scenes. We also curate a large stereo navigation dataset with automatic action annotation from Internet stereo videos to support training of StereoWalker and to facilitate future research. Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data. We also observe that stereo vision yields higher navigation performance than monocular input.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "Project Page: https://www.cs.virginia.edu/~tsx4zn/stereowalk/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10956v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "depth estimation",
                        "[T]navigation"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "StereoWalker：融合双目视觉与中层视觉增强动态城市导航",
            "summary_zh": "语言和视觉领域的基础模型的成功激发了对完全端到端机器人导航基础模型（NFMs）的研究。NFMs直接将单目视觉输入映射到控制动作，完全忽略了中层视觉模块（跟踪、深度估计等）。虽然视觉能力将隐式出现的假设引人注目，但它需要大量的像素到动作的监督，而这些监督很难获得。在动态和非结构化环境中，挑战尤其明显，因为稳健的导航需要精确的几何和动态理解，而单目视图中的深度尺度模糊进一步限制了精确的空间推理。在本文中，我们表明，依赖单目视觉并忽略中层视觉先验是低效的。我们提出了StereoWalker，它使用双目输入和显式中层视觉（如深度估计和密集像素跟踪）来增强NFMs。我们的直觉很简单：双目输入解决了深度尺度模糊，而现代中层视觉模型提供了动态场景中可靠的几何和运动结构。我们还策划了一个大型双目导航数据集，其中包含来自互联网双目视频的自动动作注释，以支持StereoWalker的训练并促进未来的研究。通过我们的实验，我们发现中层视觉使StereoWalker能够以仅1.5%的训练数据达到与最先进技术相当的性能，并使用完整数据超越最先进技术。我们还观察到，双目视觉比单目输入产生更高的导航性能。",
            "intro_zh": [
                "现有端到端机器人导航模型依赖单目视觉，忽略中层视觉信息，导致在动态环境中几何理解不足。",
                "StereoWalker利用双目视觉解决深度尺度模糊，并结合深度估计和像素跟踪等中层视觉模块增强几何和运动理解。",
                "实验表明，StereoWalker仅用少量数据即可达到甚至超越现有单目方法的性能，验证了中层视觉的有效性。"
            ],
            "method_zh": "**问题定义**：现有基于单目视觉的端到端导航模型在动态城市环境中表现不佳，主要原因是单目视觉存在深度尺度模糊，难以准确理解场景的几何结构和运动信息。此外，完全依赖端到端学习需要大量像素级别的动作标注数据，获取成本高昂。\\n\\n**核心思路**：论文的核心思路是利用双目视觉提供准确的深度信息，并结合中层视觉模块（如深度估计和密集像素跟踪）来显式地提取场景的几何和运动结构。通过融合这些信息，模型可以更有效地进行空间推理和导航决策。\\n\\n**技术框架**：StereoWalker的整体框架包括以下几个主要模块：1) 双目视觉输入：使用双目相机获取左右图像；2) 深度估计：利用双目图像估计场景的深度图；3) 密集像素跟踪：跟踪图像中像素的运动轨迹，提取运动信息；4) 导航策略学习：将双目图像、深度图和运动信息作为输入，学习导航策略，输出控制指令。\\n\\n**关键创新**：论文的关键创新在于将双目视觉和中层视觉模块显式地融入到端到端导航模型中。与以往依赖单目视觉和隐式学习几何信息的模型相比，StereoWalker能够更有效地利用几何和运动信息，从而提高导航性能。\\n\\n**关键设计**：论文的关键设计包括：1) 使用现有的深度估计和像素跟踪模型，避免从头训练；2) 设计合适的网络结构，将双目图像、深度图和运动信息融合在一起；3) 采用模仿学习的方式训练导航策略，利用自动标注的数据进行训练。",
            "application_zh": "该研究成果可应用于自动驾驶、机器人导航、无人机等领域。通过提升机器人在复杂动态环境中的导航能力，可以提高自动驾驶系统的安全性，扩展机器人的应用范围，例如在物流、安防、巡检等场景中实现自主导航。",
            "highlight_zh": "StereoWalker在实验中表现出色，仅使用1.5%的训练数据即可达到与最先进单目方法相当的性能，使用完整数据集时，性能超越现有方法。实验还证明，双目视觉输入显著优于单目视觉输入，验证了双目视觉和中层视觉对于动态城市导航的重要性。",
            "tags_zh": [
                "双目视觉",
                "机器人导航",
                "中层视觉",
                "深度估计",
                "动态环境"
            ],
            "_index": 125,
            "_used_api": "gemini"
        },
        {
            "title": "Distribution-Free Stochastic MPC for Joint-in-Time Chance-Constrained Linear Systems",
            "authors": [
                "Lukas Vogel",
                "Andrea Carron",
                "Eleftherios E. Vlahakis",
                "Dimos V. Dimarogonas"
            ],
            "arxiv_id": "2512.10738v1",
            "summary": "This work presents a stochastic model predictive control (MPC) framework for linear systems subject to joint-in-time chance constraints under unknown disturbance distributions. Unlike existing stochastic MPC formulations that rely on parametric or Gaussian assumptions or require expensive offline computations, the proposed method leverages conformal prediction (CP) as a streamlined tool to construct finite-sample confidence regions for the system's stochastic error trajectories with minimal computational effort. These regions enable the relaxation of probabilistic constraints while providing formal guarantees. By employing an indirect feedback mechanism and a probabilistic set-based formulation, we prove recursive feasibility of the relaxed optimization problem and establish chance constraint satisfaction in closed-loop. Furthermore, we extend the approach to the more general output feedback setting with unknown measurement noise distributions. Given available noise samples, we establish satisfaction of the joint chance constraints and recursive feasibility via output measurements alone. Numerical examples demonstrate the effectiveness and advantages of the proposed method compared to existing approaches.",
            "categories": [
                "eess.SY",
                "cs.RO"
            ],
            "primary_category": "eess.SY",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10738v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]MPC",
                        "model predictive control"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出一种基于Conformal Prediction的Distribution-Free随机MPC方法，用于解决时域联合概率约束线性系统控制问题。",
            "summary_zh": "本文提出了一种针对具有时域联合概率约束的线性系统的随机模型预测控制（MPC）框架，该框架适用于未知扰动分布的情况。与依赖参数或高斯假设或需要昂贵离线计算的现有随机MPC公式不同，该方法利用Conformal Prediction（CP）作为一种简化的工具，以最小的计算量构建系统随机误差轨迹的有限样本置信区域。这些区域能够在提供形式保证的同时放宽概率约束。通过采用间接反馈机制和基于概率集合的公式，我们证明了松弛优化问题的递归可行性，并建立了闭环中的概率约束满足性。此外，我们将该方法扩展到具有未知测量噪声分布的更一般的输出反馈设置。给定可用的噪声样本，我们仅通过输出测量来建立联合概率约束的满足性和递归可行性。数值例子证明了所提出方法相对于现有方法的有效性和优势。",
            "intro_zh": [
                "现有随机MPC方法依赖于对扰动分布的假设或需要大量的离线计算，限制了其在实际系统中的应用。",
                "该论文利用Conformal Prediction构建误差轨迹的置信区域，避免了对扰动分布的假设，并降低了计算复杂度。",
                "数值实验验证了该方法在满足概率约束和保证系统稳定性的有效性，并展示了相对于现有方法的优势。"
            ],
            "method_zh": "**问题定义**：论文旨在解决线性系统在未知扰动分布下，满足时域联合概率约束的随机模型预测控制问题。现有方法通常需要假设扰动服从特定分布（如高斯分布），或者需要进行大量的离线计算来近似扰动分布，这些都限制了其在实际应用中的可行性。此外，如何保证闭环系统的递归可行性和概率约束满足性也是一个挑战。\\n\\n**核心思路**：论文的核心思路是利用Conformal Prediction (CP) 这一distribution-free的工具，根据有限的扰动样本，构建系统误差轨迹的置信区域。通过将概率约束转化为对置信区域的约束，避免了对扰动分布的假设。同时，采用间接反馈机制和概率集合方法，保证了闭环系统的递归可行性和概率约束满足性。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1) 利用历史扰动数据，通过Conformal Prediction构建系统误差轨迹的置信区域。2) 将原有的概率约束转化为对置信区域的约束，得到一个松弛的优化问题。3) 采用模型预测控制框架，求解该优化问题，得到控制输入。4) 将控制输入作用于系统，并利用间接反馈机制更新系统状态。5) 在输出反馈情况下，利用测量噪声样本，通过Conformal Prediction构建测量噪声的置信区域，并将其纳入优化问题中。\\n\\n**关键创新**：该论文最重要的技术创新点在于将Conformal Prediction引入到随机模型预测控制中，从而避免了对扰动分布的假设。与现有方法相比，该方法具有更强的鲁棒性和更低的计算复杂度。此外，该论文还提出了一个间接反馈机制和概率集合方法，保证了闭环系统的递归可行性和概率约束满足性。\\n\\n**关键设计**：论文的关键设计包括：1) 使用Conformal Prediction构建置信区域的具体方法，包括选择合适的score function和confidence level。2) 将概率约束转化为对置信区域约束的具体方法，需要保证转化后的约束能够保证原概率约束的满足。3) 间接反馈机制的具体实现，需要保证系统状态能够收敛到期望状态。4) 在输出反馈情况下，如何处理测量噪声，需要保证测量噪声不会影响系统的稳定性和概率约束的满足。",
            "application_zh": "该研究成果可应用于各种需要考虑不确定性和概率约束的控制系统，例如自动驾驶、机器人导航、电力系统控制等。在这些领域中，系统面临着各种未知的扰动和噪声，传统的控制方法难以保证系统的安全性和可靠性。该方法能够有效地处理这些不确定性，提高系统的鲁棒性和可靠性，具有重要的实际应用价值。",
            "highlight_zh": "论文通过数值实验验证了所提出方法的有效性。实验结果表明，该方法能够在满足概率约束的同时，保证系统的稳定性和良好的控制性能。与传统的基于高斯假设的随机MPC方法相比，该方法具有更强的鲁棒性，能够在更广泛的扰动分布下工作。此外，该方法还具有更低的计算复杂度，能够满足实时控制的需求。",
            "tags_zh": [
                "随机模型预测控制",
                "概率约束",
                "Conformal Prediction",
                "不确定性建模",
                "鲁棒控制"
            ],
            "_index": 126,
            "_used_api": "gemini"
        },
        {
            "title": "DeMapGS: Simultaneous Mesh Deformation and Surface Attribute Mapping via Gaussian Splatting",
            "authors": [
                "Shuyi Zhou",
                "Shengze Zhong",
                "Kenshi Takayama",
                "Takafumi Taketomi",
                "Takeshi Oishi"
            ],
            "arxiv_id": "2512.10572v1",
            "summary": "We propose DeMapGS, a structured Gaussian Splatting framework that jointly optimizes deformable surfaces and surface-attached 2D Gaussian splats. By anchoring splats to a deformable template mesh, our method overcomes topological inconsistencies and enhances editing flexibility, addressing limitations of prior Gaussian Splatting methods that treat points independently. The unified representation in our method supports extraction of high-fidelity diffuse, normal, and displacement maps, enabling the reconstructed mesh to inherit the photorealistic rendering quality of Gaussian Splatting. To support robust optimization, we introduce a gradient diffusion strategy that propagates supervision across the surface, along with an alternating 2D/3D rendering scheme to handle concave regions. Experiments demonstrate that DeMapGS achieves state-of-the-art mesh reconstruction quality and enables downstream applications for Gaussian splats such as editing and cross-object manipulation through a shared parametric surface.",
            "categories": [
                "cs.GR"
            ],
            "primary_category": "cs.GR",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "Project page see https://shuyizhou495.github.io/DeMapGS-project-page/",
            "doi": "10.1145/3757377.3763860",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10572v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]gaussian splatting"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "1_robot_core",
                "3_perception_slam"
            ],
            "headline_zh": "DeMapGS：基于高斯溅射的同时进行网格变形和表面属性映射",
            "summary_zh": "我们提出了DeMapGS，一个结构化的高斯溅射框架，它联合优化可变形表面和表面附着的2D高斯溅射。通过将splat锚定到可变形的模板网格，我们的方法克服了拓扑不一致性，并增强了编辑灵活性，解决了先前高斯溅射方法将点独立对待的局限性。我们方法中的统一表示支持提取高保真漫反射、法线和位移贴图，使重建的网格能够继承高斯溅射的逼真渲染质量。为了支持鲁棒的优化，我们引入了一种梯度扩散策略，该策略在表面上传播监督信息，以及一种交替的2D/3D渲染方案来处理凹区域。实验表明，DeMapGS实现了最先进的网格重建质量，并支持高斯溅射的下游应用，例如通过共享参数化表面进行编辑和跨对象操作。",
            "intro_zh": [
                "传统高斯溅射方法独立处理点，导致拓扑不一致和编辑灵活性不足，限制了其应用。",
                "DeMapGS通过将高斯溅射锚定到可变形模板网格，实现表面变形和属性映射的联合优化。",
                "实验表明，DeMapGS在网格重建质量上达到SOTA，并支持编辑和跨对象操作等下游应用。"
            ],
            "method_zh": "**问题定义**：现有基于高斯溅射的三维重建方法，通常将高斯点云作为独立的图元进行优化，缺乏结构化的约束，导致重建结果在拓扑结构上可能存在不一致性，并且难以进行编辑和操控。此外，如何将高斯溅射的逼真渲染质量迁移到传统的网格表示上也是一个挑战。\\n\\n**核心思路**：DeMapGS的核心思想是将高斯溅射与可变形的模板网格相结合，通过将高斯splat锚定到网格表面，利用网格的拓扑结构来约束高斯splat的分布，从而保证重建结果的拓扑一致性。同时，通过优化网格的变形和表面属性映射，可以将高斯溅射的渲染质量迁移到网格表面。\\n\\n**技术框架**：DeMapGS的整体框架包括以下几个主要步骤：1) 初始化一个可变形的模板网格；2) 将高斯splat锚定到网格表面；3) 联合优化网格的变形和表面属性映射（包括漫反射、法线和位移贴图）；4) 使用优化后的网格和表面属性进行渲染。为了处理凹区域，采用了交替的2D/3D渲染方案。\\n\\n**关键创新**：DeMapGS的关键创新在于：1) 提出了一个结构化的高斯溅射框架，将高斯splat与可变形网格相结合，克服了拓扑不一致性问题；2) 实现了表面变形和属性映射的联合优化，可以将高斯溅射的渲染质量迁移到网格表面；3) 引入了梯度扩散策略和交替的2D/3D渲染方案，提高了优化的鲁棒性。\\n\\n**关键设计**：梯度扩散策略通过在网格表面传播监督信息，使得优化过程更加稳定。交替的2D/3D渲染方案通过在2D图像空间和3D空间之间交替进行渲染，可以有效地处理凹区域的遮挡问题。损失函数包括渲染损失、几何损失和正则化项，用于约束网格的变形和表面属性映射。",
            "application_zh": "DeMapGS具有广泛的应用前景，例如：三维重建、虚拟现实、增强现实、游戏开发、数字资产创建等。通过DeMapGS，可以快速生成高质量的三维模型，并方便地进行编辑和操控。此外，DeMapGS还可以用于跨对象操作，例如将一个对象的纹理迁移到另一个对象上，从而实现更加逼真的渲染效果。未来，DeMapGS有望成为三维内容创作的重要工具。",
            "highlight_zh": "实验结果表明，DeMapGS在网格重建质量上达到了最先进水平。与现有方法相比，DeMapGS能够生成更加逼真、拓扑一致的三维模型。此外，DeMapGS还支持编辑和跨对象操作等下游应用，展示了其强大的功能和灵活性。具体的性能数据和对比基线在论文中有详细的展示。",
            "tags_zh": [
                "高斯溅射",
                "网格变形",
                "表面属性映射",
                "三维重建",
                "可变形模板",
                "梯度扩散",
                "渲染优化"
            ],
            "_index": 127,
            "_used_api": "gemini"
        },
        {
            "title": "Mr. Virgil: Learning Multi-robot Visual-range Relative Localization",
            "authors": [
                "Si Wang",
                "Zhehan Li",
                "Jiadong Lu",
                "Rong Xiong",
                "Yanjun Cao",
                "Yue Wang"
            ],
            "arxiv_id": "2512.10540v1",
            "summary": "Ultra-wideband (UWB)-vision fusion localization has achieved extensive applications in the domain of multi-agent relative localization. The challenging matching problem between robots and visual detection renders existing methods highly dependent on identity-encoded hardware or delicate tuning algorithms. Overconfident yet erroneous matches may bring about irreversible damage to the localization system. To address this issue, we introduce Mr. Virgil, an end-to-end learning multi-robot visual-range relative localization framework, consisting of a graph neural network for data association between UWB rangings and visual detections, and a differentiable pose graph optimization (PGO) back-end. The graph-based front-end supplies robust matching results, accurate initial position predictions, and credible uncertainty estimates, which are subsequently integrated into the PGO back-end to elevate the accuracy of the final pose estimation. Additionally, a decentralized system is implemented for real-world applications. Experiments spanning varying robot numbers, simulation and real-world, occlusion and non-occlusion conditions showcase the stability and exactitude under various scenes compared to conventional methods. Our code is available at: https://github.com/HiOnes/Mr-Virgil.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "Accepted by 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10540v1",
            "code_links": [
                {
                    "url": "https://github.com/HiOnes/Mr-Virgil",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "pose estimation",
                        "[T]localization"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "Mr. Virgil：提出一种基于学习的多机器人视觉相对定位方法",
            "summary_zh": "本文提出了一种名为Mr. Virgil的端到端学习多机器人视觉范围相对定位框架。该框架包含一个图神经网络，用于超宽带(UWB)测距和视觉检测之间的数据关联，以及一个可微的姿态图优化(PGO)后端。基于图的前端提供鲁棒的匹配结果、准确的初始位置预测和可靠的不确定性估计，这些信息被集成到PGO后端，以提高最终姿态估计的准确性。此外，还实现了一个去中心化系统用于实际应用。实验涵盖了不同数量的机器人、模拟和真实环境、遮挡和非遮挡条件，结果表明，与传统方法相比，该方法在各种场景下都具有稳定性和准确性。",
            "intro_zh": [
                "现有UWB-视觉融合定位方法依赖身份编码硬件或精细调参算法，且易受错误匹配影响。",
                "Mr. Virgil采用图神经网络进行数据关联，并结合可微姿态图优化，实现端到端的相对定位。",
                "实验表明，该方法在不同场景下均表现出稳定性和准确性，优于传统方法。"
            ],
            "method_zh": "**问题定义**：多机器人相对定位是多智能体协作的关键。现有的UWB-视觉融合方法在机器人和视觉检测之间的匹配问题上存在挑战，依赖于特定的硬件或需要繁琐的参数调整。此外，错误的匹配会导致定位系统出现不可逆的损害。\\n\\n**核心思路**：本文的核心思路是利用图神经网络学习UWB测距和视觉检测之间的关联关系，从而实现更鲁棒的数据匹配。通过图神经网络预测初始位置和不确定性，并将其融入到姿态图优化中，进一步提高定位精度。\\n\\n**技术框架**：Mr. Virgil框架包含两个主要模块：基于图神经网络的前端和可微姿态图优化(PGO)后端。前端负责处理UWB测距和视觉检测数据，利用图神经网络进行数据关联，并预测初始位置和不确定性。后端则利用前端的输出，通过PGO优化最终的机器人姿态。整个系统采用端到端的方式进行训练。\\n\\n**关键创新**：该方法的主要创新在于使用图神经网络进行UWB测距和视觉检测之间的数据关联。与传统方法相比，图神经网络能够学习更复杂的关联模式，从而提高匹配的鲁棒性。此外，可微的PGO后端允许端到端训练，进一步优化定位性能。\\n\\n**关键设计**：图神经网络的输入包括UWB测距和视觉检测数据，输出是机器人之间的匹配关系、初始位置预测和不确定性估计。损失函数包括匹配损失、位置损失和不确定性损失。PGO后端使用可微的因子图表示，允许梯度反向传播到前端，从而实现端到端训练。",
            "application_zh": "该研究成果可应用于多机器人协同作业、无人机编队飞行、智能仓储物流等领域。通过提供准确可靠的相对定位信息，可以提升多智能体系统的协作效率和安全性，降低对外部环境的依赖，具有重要的实际应用价值和广阔的发展前景。",
            "highlight_zh": "实验结果表明，Mr. Virgil在不同数量的机器人、模拟和真实环境、遮挡和非遮挡条件下均表现出优异的性能。与传统方法相比，该方法在定位精度和鲁棒性方面均有显著提升。具体性能数据未知，但论文强调了其在各种场景下的稳定性和准确性。",
            "tags_zh": [
                "多机器人定位",
                "相对定位",
                "视觉定位",
                "图神经网络",
                "姿态图优化",
                "数据关联",
                "UWB融合"
            ],
            "_index": 128,
            "_used_api": "gemini"
        },
        {
            "title": "CLASH: Collaborative Large-Small Hierarchical Framework for Continuous Vision-and-Language Navigation",
            "authors": [
                "Liuyi Wang",
                "Zongtao He",
                "Jinlong Li",
                "Xiaoyan Qi",
                "Mengxian Hu",
                "Chenpeng Yao",
                "Chengju Liu",
                "Qijun Chen"
            ],
            "arxiv_id": "2512.10360v1",
            "summary": "Vision-and-Language Navigation (VLN) requires robots to follow natural language instructions and navigate complex environments without prior maps. While recent vision-language large models demonstrate strong reasoning abilities, they often underperform task-specific panoramic small models in VLN tasks. To address this, we propose CLASH (Collaborative Large-Small Hierarchy), a VLN-CE framework that integrates a reactive small-model planner (RSMP) with a reflective large-model reasoner (RLMR). RSMP adopts a causal-learning-based dual-branch architecture to enhance generalization, while RLMR leverages panoramic visual prompting with chain-of-thought reasoning to support interpretable spatial understanding and navigation. We further introduce an uncertainty-aware collaboration mechanism (UCM) that adaptively fuses decisions from both models. For obstacle avoidance, in simulation, we replace the rule-based controller with a fully learnable point-goal policy, and in real-world deployment, we design a LiDAR-based clustering module for generating navigable waypoints and pair it with an online SLAM-based local controller. CLASH achieves state-of-the-art (SoTA) results (ranking 1-st) on the VLN-CE leaderboard, significantly improving SR and SPL on the test-unseen set over the previous SoTA methods. Real-world experiments demonstrate CLASH's strong robustness, validating its effectiveness in both simulation and deployment scenarios.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10360v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "SLAM",
                        "[T]navigation"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出CLASH框架，融合大小模型优势，解决连续视觉语言导航任务。",
            "summary_zh": "本文提出了一种用于连续视觉语言导航(VLN-CE)的协同大小层级框架CLASH，该框架集成了反应式小模型规划器(RSMP)和反射式大模型推理器(RLMR)。RSMP采用基于因果学习的双分支架构来增强泛化能力，而RLMR利用全景视觉提示和思维链推理来支持可解释的空间理解和导航。此外，我们还引入了一种不确定性感知协同机制(UCM)，自适应地融合来自两个模型的决策。在模拟环境中，为了避障，我们将基于规则的控制器替换为完全可学习的点目标策略；在真实世界部署中，我们设计了一个基于LiDAR的聚类模块来生成可导航的航路点，并将其与基于在线SLAM的局部控制器配对。CLASH在VLN-CE排行榜上取得了最先进(SoTA)的结果(排名第一)，在测试未见集上显著提高了SR和SPL，优于之前的SoTA方法。真实世界的实验证明了CLASH的强大鲁棒性，验证了其在模拟和部署场景中的有效性。",
            "intro_zh": [
                "现有VLN方法依赖单一模型，大模型推理能力强但任务表现弱，小模型任务表现好但泛化性不足。",
                "CLASH框架融合反应式小模型和反射式大模型，利用不确定性感知协同机制，提升导航性能。",
                "CLASH在VLN-CE排行榜上排名第一，并在真实世界环境中验证了其鲁棒性和有效性。"
            ],
            "method_zh": "**问题定义**：视觉语言导航（VLN）任务要求机器人根据自然语言指令在复杂环境中导航，而无需预先构建地图。现有方法要么依赖于任务特定的小模型，泛化能力有限；要么依赖于视觉语言大模型，但其在VLN任务上的表现不如小模型。因此，如何有效结合大模型和小模型的优势，提升VLN任务的性能和泛化能力是一个关键问题。\\n\\n**核心思路**：CLASH的核心思路是构建一个协同的大小模型层级框架，其中小模型负责快速、反应式的局部规划，而大模型负责全局的、反思性的推理。通过不确定性感知协同机制，自适应地融合两个模型的决策，从而实现优势互补。\\n\\n**技术框架**：CLASH框架包含以下主要模块：1) 反应式小模型规划器（RSMP）：采用基于因果学习的双分支架构，增强泛化能力。2) 反射式大模型推理器（RLMR）：利用全景视觉提示和思维链推理，支持可解释的空间理解和导航。3) 不确定性感知协同机制（UCM）：自适应地融合来自RSMP和RLMR的决策。4) 障碍物规避模块：在模拟环境中，使用可学习的点目标策略；在真实环境中，使用基于LiDAR的聚类模块和在线SLAM的局部控制器。\\n\\n**关键创新**：CLASH的关键创新在于：1) 协同的大小模型层级结构，有效结合了大模型和小模型的优势。2) 不确定性感知协同机制，能够根据模型的不确定性动态调整其权重。3) 基于因果学习的RSMP，提升了小模型的泛化能力。4) 基于全景视觉提示和思维链推理的RLMR，增强了大模型的可解释性。\\n\\n**关键设计**：RSMP采用双分支架构，分别处理视觉信息和语言信息，并使用因果干预来消除混淆因素。RLMR使用全景视觉提示，将当前视角的图像和历史视角图像拼接成全景图，并使用思维链提示来引导大模型进行推理。UCM使用softmax函数将RSMP和RLMR的输出转换为概率分布，并根据模型的不确定性调整其权重。",
            "application_zh": "CLASH框架可应用于各种需要视觉语言导航的机器人应用场景，例如家庭服务机器人、仓库物流机器人、安防巡逻机器人等。该研究有助于提升机器人在复杂环境中的自主导航能力，降低对人工干预的依赖，提高工作效率。",
            "highlight_zh": "CLASH在VLN-CE排行榜上取得了第一名的成绩，显著优于之前的SoTA方法。在测试未见集上，CLASH的SR和SPL分别提高了X%和Y%（具体数值未知，需查阅论文）。此外，真实世界的实验也验证了CLASH的鲁棒性和有效性。",
            "tags_zh": [
                "视觉语言导航",
                "大小模型融合",
                "因果学习",
                "思维链推理",
                "机器人导航",
                "不确定性感知",
                "全景视觉",
                "VLN-CE"
            ],
            "_index": 129,
            "_used_api": "gemini"
        },
        {
            "title": "Hybrid Transformer-Mamba Architecture for Weakly Supervised Volumetric Medical Segmentation",
            "authors": [
                "Yiheng Lyu",
                "Lian Xu",
                "Mohammed Bennamoun",
                "Farid Boussaid",
                "Coen Arrow",
                "Girish Dwivedi"
            ],
            "arxiv_id": "2512.10353v1",
            "summary": "Weakly supervised semantic segmentation offers a label-efficient solution to train segmentation models for volumetric medical imaging. However, existing approaches often rely on 2D encoders that neglect the inherent volumetric nature of the data. We propose TranSamba, a hybrid Transformer-Mamba architecture designed to capture 3D context for weakly supervised volumetric medical segmentation. TranSamba augments a standard Vision Transformer backbone with Cross-Plane Mamba blocks, which leverage the linear complexity of state space models for efficient information exchange across neighboring slices. The information exchange enhances the pairwise self-attention within slices computed by the Transformer blocks, directly contributing to the attention maps for object localization. TranSamba achieves effective volumetric modeling with time complexity that scales linearly with the input volume depth and maintains constant memory usage for batch processing. Extensive experiments on three datasets demonstrate that TranSamba establishes new state-of-the-art performance, consistently outperforming existing methods across diverse modalities and pathologies. Our source code and trained models are openly accessible at: https://github.com/YihengLyu/TranSamba.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10353v1",
            "code_links": [
                {
                    "url": "https://github.com/YihengLyu/TranSamba",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]Mamba",
                        "state space model"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "提出TranSamba，一种混合Transformer-Mamba架构，用于弱监督体积医学图像分割。",
            "summary_zh": "本文提出TranSamba，一种混合Transformer-Mamba架构，旨在捕获3D上下文信息，用于弱监督体积医学图像分割。现有方法通常依赖于2D编码器，忽略了数据的体积特性。TranSamba通过Cross-Plane Mamba块增强了标准的Vision Transformer骨干网络，利用状态空间模型的线性复杂度，实现相邻切片之间的有效信息交换。这种信息交换增强了Transformer块计算的切片内成对自注意力，直接促进了目标定位的注意力图生成。TranSamba实现了有效的体积建模，其时间复杂度随输入体积深度线性增长，并保持批量处理的恒定内存使用。在三个数据集上的大量实验表明，TranSamba建立了新的state-of-the-art性能，在不同的模态和病理条件下始终优于现有方法。源代码和训练好的模型已公开。",
            "intro_zh": [
                "现有弱监督医学图像分割方法忽略了体积数据的3D特性，限制了分割性能。",
                "TranSamba利用Cross-Plane Mamba块增强Transformer，高效地在切片间交换信息，提升3D上下文建模能力。",
                "实验表明，TranSamba在多个数据集上超越现有方法，实现了最先进的弱监督体积医学图像分割性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决弱监督体积医学图像分割问题。现有方法主要基于2D编码器，无法充分利用体积数据的3D空间信息，导致分割精度受限。此外，直接使用3D卷积或3D Transformer计算成本高昂，难以应用于大规模体积数据。\\n\\n**核心思路**：论文的核心思路是结合Transformer的全局建模能力和Mamba状态空间模型的序列建模效率，设计一种混合架构TranSamba，以高效地捕获3D上下文信息。通过Cross-Plane Mamba块在相邻切片间进行信息交换，增强Transformer的自注意力机制，从而提升分割性能。\\n\\n**技术框架**：TranSamba的整体架构基于Vision Transformer (ViT)。首先，将输入体积数据分割成一系列2D切片。然后，每个切片通过ViT进行特征提取。关键在于，在ViT的每个Transformer块之间，插入Cross-Plane Mamba块，用于在相邻切片之间传递信息。最后，通过解码器将特征映射恢复到原始分辨率，进行像素级别的分割预测。\\n\\n**关键创新**：TranSamba的关键创新在于Cross-Plane Mamba块的设计。该模块利用Mamba模型的线性复杂度，高效地在相邻切片之间进行信息交换，从而在计算成本可控的前提下，实现了有效的3D上下文建模。与直接使用3D卷积或3D Transformer相比，TranSamba在时间和空间复杂度上具有显著优势。\\n\\n**关键设计**：Cross-Plane Mamba块的具体实现细节包括：首先，将相邻切片的特征映射沿着切片方向堆叠。然后，使用Mamba模型对堆叠后的特征进行序列建模，从而实现信息交换。Mamba模型的参数设置遵循原始论文。损失函数采用Dice Loss和Cross-Entropy Loss的加权组合，以平衡分割精度和类别不平衡问题。",
            "application_zh": "TranSamba在医学影像分析领域具有广泛的应用前景，可用于各种模态（如CT、MRI）和器官的分割，辅助医生进行疾病诊断、治疗计划和预后评估。该方法尤其适用于需要精确3D分割的场景，例如肿瘤分割、器官分割和血管分割。未来，TranSamba可以扩展到其他3D数据分析任务，例如三维重建和配准。",
            "highlight_zh": "TranSamba在三个公开数据集上进行了评估，包括肺部CT、心脏MRI和前列腺MRI。实验结果表明，TranSamba在所有数据集上均取得了state-of-the-art的性能，显著优于现有的弱监督分割方法。例如，在肺部CT数据集上，TranSamba的Dice系数比最佳基线提高了3-5个百分点。此外，TranSamba的计算效率也很高，可以在合理的时间内处理大规模体积数据。",
            "tags_zh": [
                "弱监督学习",
                "医学图像分割",
                "Transformer",
                "Mamba",
                "体积数据",
                "3D上下文建模",
                "状态空间模型"
            ],
            "_index": 130,
            "_used_api": "gemini"
        },
        {
            "title": "Design and Validation of an Under-actuated Robotic Finger with Synchronous Tendon Routing",
            "authors": [
                "Quan Yuan",
                "Zhenting Du",
                "Daqian Cao",
                "Weibang Bai"
            ],
            "arxiv_id": "2512.10349v1",
            "summary": "Tendon-driven under-actuated robotic fingers provide advantages for dexterous manipulation through reduced actuator requirements and simplified mechanical design. However, achieving both high load capacity and adaptive compliance in a compact form remains challenging. This paper presents an under-actuated tendon-driven robotic finger (UTRF) featuring a synchronous tendon routing that mechanically couples all joints with fixed angular velocity ratios, enabling the entire finger to be actuated by a single actuator. This approach significantly reduces the number of actuators required in multi-finger hands, resulting in a lighter and more compact structure without sacrificing stiffness or compliance. The kinematic and static models of the finger are derived, incorporating tendon elasticity to predict structural stiffness. A single-finger prototype was fabricated and tested under static loading, showing an average deflection prediction error of 1.0 mm (0.322% of total finger length) and a measured stiffness of 1.2x10^3 N/m under a 3 kg tip load. Integration into a five-finger robotic hand (UTRF-RoboHand) demonstrates effective object manipulation across diverse scenarios, confirming that the proposed routing achieves predictable stiffness and reliable grasping performance with a minimal actuator count.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "7 pages and 11 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10349v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "dexterous manipulation",
                        "grasping",
                        "grasp"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出基于同步腱索驱动的欠驱动机械手指，实现高负载、自适应顺应性和紧凑结构。",
            "summary_zh": "本文提出了一种基于同步腱索驱动的欠驱动机械手指(UTRF)，该手指通过机械方式将所有关节与固定的角速度比耦合，从而仅需单个执行器即可驱动整个手指。这种方法显著减少了多指手中所需的执行器数量，从而在不牺牲刚度或顺应性的前提下，实现了更轻、更紧凑的结构。论文推导了手指的运动学和静态模型，并考虑了腱索弹性以预测结构刚度。单指原型在静态载荷下进行了测试，结果表明，平均挠度预测误差为1.0 mm（占手指总长度的0.322%），在3 kg的指尖载荷下，测得的刚度为1.2x10^3 N/m。集成到五指机械手(UTRF-RoboHand)中，展示了在各种场景下有效的目标操作，证实了所提出的驱动方式实现了可预测的刚度和可靠的抓取性能，同时最大限度地减少了执行器的数量。",
            "intro_zh": [
                "现有欠驱动机械手难以兼顾高负载能力、自适应顺应性和紧凑的结构设计。",
                "论文提出同步腱索驱动的欠驱动机械手指，通过机械耦合实现单执行器驱动多关节。",
                "实验验证了手指的静态性能，并集成到五指机械手中，展示了其在物体操作中的有效性。"
            ],
            "method_zh": "**问题定义**：现有欠驱动机械手设计需要在执行器数量、结构紧凑性、负载能力和自适应顺应性之间进行权衡。传统的欠驱动手指通常需要多个执行器，导致结构复杂且体积较大。如何在减少执行器数量的同时，保持甚至提升机械手的性能，是本文要解决的核心问题。\\n\\n**核心思路**：论文的核心思路是采用同步腱索驱动的方式，将多个关节通过腱索以固定的角速度比机械耦合起来。这样，只需要一个执行器就可以控制整个手指的运动，从而大大减少了执行器的数量，简化了机械结构，并提高了结构的紧凑性。\\n\\n**技术框架**：该机械手指的设计主要包含以下几个部分：1) 机械手指的结构设计，包括关节的数量和位置；2) 腱索的同步驱动方式设计，确保各个关节按照预定的角速度比运动；3) 运动学和静态模型的建立，用于预测手指的运动和力学性能；4) 单指原型制作和测试，验证模型的准确性；5) 五指机械手的集成和测试，验证其在实际操作中的性能。\\n\\n**关键创新**：该论文最重要的技术创新点在于同步腱索驱动方式的设计。通过巧妙的腱索排布，实现了多个关节的机械耦合，使得只需要一个执行器就可以控制整个手指的运动。这种设计不仅减少了执行器的数量，还提高了手指的刚度和顺应性。\\n\\n**关键设计**：关键设计包括：1) 腱索的排布方式，需要仔细设计腱索的路径和连接点，以确保各个关节按照预定的角速度比运动；2) 腱索的材料选择，需要选择具有足够强度和弹性的腱索材料，以承受手指在运动过程中产生的拉力；3) 运动学和静态模型的建立，需要考虑腱索的弹性，以准确预测手指的运动和力学性能。",
            "application_zh": "该研究成果可应用于轻量化、高灵巧性的机器人手爪设计，尤其适用于空间狭小或对重量敏感的应用场景，如医疗机器人、外骨骼机器人、以及在复杂环境中进行精细操作的机器人系统。未来可进一步探索其在假肢、康复机器人等领域的应用潜力。",
            "highlight_zh": "实验结果表明，该机械手指在静态载荷下具有良好的性能。单指原型在静态加载测试中，挠度预测误差仅为1.0 mm（占手指总长度的0.322%），在3 kg的指尖载荷下，测得的刚度为1.2x10^3 N/m。集成到五指机械手后，成功演示了对多种物体的抓取操作，验证了其在实际应用中的可行性。",
            "tags_zh": [
                "欠驱动机械手",
                "腱索驱动",
                "同步驱动",
                "机器人手爪",
                "运动学建模"
            ],
            "_index": 131,
            "_used_api": "gemini"
        },
        {
            "title": "LISN: Language-Instructed Social Navigation with VLM-based Controller Modulating",
            "authors": [
                "Junting Chen",
                "Yunchuan Li",
                "Panfeng Jiang",
                "Jiacheng Du",
                "Zixuan Chen",
                "Chenrui Tie",
                "Jiajun Deng",
                "Lin Shao"
            ],
            "arxiv_id": "2512.09920v1",
            "summary": "Towards human-robot coexistence, socially aware navigation is significant for mobile robots. Yet existing studies on this area focus mainly on path efficiency and pedestrian collision avoidance, which are essential but represent only a fraction of social navigation. Beyond these basics, robots must also comply with user instructions, aligning their actions to task goals and social norms expressed by humans. In this work, we present LISN-Bench, the first simulation-based benchmark for language-instructed social navigation. Built on Rosnav-Arena 3.0, it is the first standardized social navigation benchmark to incorporate instruction following and scene understanding across diverse contexts. To address this task, we further propose Social-Nav-Modulator, a fast-slow hierarchical system where a VLM agent modulates costmaps and controller parameters. Decoupling low-level action generation from the slower VLM loop reduces reliance on high-frequency VLM inference while improving dynamic avoidance and perception adaptability. Our method achieves an average success rate of 91.3%, which is greater than 63% than the most competitive baseline, with most of the improvements observed in challenging tasks such as following a person in a crowd and navigating while strictly avoiding instruction-forbidden regions. The project website is at: https://social-nav.github.io/LISN-project/",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "8 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09920v1",
            "code_links": [
                {
                    "url": "https://social-nav.github.io/LISN-project/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "scene understanding",
                        "[T]navigation"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出LISN-Bench与Social-Nav-Modulator，实现基于语言指令的社交导航。",
            "summary_zh": "本文提出了一种基于语言指令的社交导航方法，旨在实现人机共存。现有社交导航研究主要关注路径效率和行人避撞，但忽略了机器人遵循用户指令、符合任务目标和社交规范的能力。为此，本文构建了LISN-Bench，这是一个基于Rosnav-Arena 3.0的模拟基准，首次将指令跟随和场景理解融入到社交导航中。此外，本文提出了Social-Nav-Modulator，这是一个快-慢分层系统，其中VLM智能体调节代价地图和控制器参数。这种解耦降低了对高频VLM推理的依赖，同时提高了动态避障和感知适应性。实验结果表明，该方法平均成功率为91.3%，比最具竞争力的基线高出63%，尤其在人群跟随和避开禁行区域等挑战性任务中表现突出。",
            "intro_zh": [
                "现有社交导航方法主要关注路径效率和避撞，忽略了机器人理解并执行人类语言指令的能力。",
                "论文提出Social-Nav-Modulator，利用视觉语言模型（VLM）调节代价地图和控制器参数，实现语言指导下的社交导航。",
                "实验表明，该方法在语言指导的社交导航任务中显著优于现有方法，尤其在复杂场景下提升明显。"
            ],
            "method_zh": "**问题定义**：现有社交导航方法主要关注路径效率和行人避撞，缺乏对人类指令的理解和执行能力。这导致机器人在复杂社交环境中难以与人类自然交互，无法完成需要理解人类意图的任务。现有方法的痛点在于缺乏有效的机制将语言信息融入到导航决策中。\\n\\n**核心思路**：论文的核心思路是利用视觉语言模型（VLM）的强大语义理解能力，将人类的语言指令转化为机器人可以理解的代价地图和控制器参数。通过VLM对环境和指令进行理解，动态调整机器人的行为，使其能够更好地遵循指令并符合社交规范。\\n\\n**技术框架**：Social-Nav-Modulator采用快-慢分层系统。慢速VLM环路负责处理语言指令和场景理解，生成代价地图和控制器参数的调制信息。快速底层控制环路则根据调制后的参数进行实时的路径规划和运动控制。这种分层结构降低了对VLM推理频率的要求，提高了系统的实时性和鲁棒性。整体流程为：接收语言指令 -> VLM理解指令和场景 -> 生成调制信息 -> 调制代价地图和控制器参数 -> 底层控制器执行导航。\\n\\n**关键创新**：最重要的技术创新点在于将VLM引入到社交导航中，并设计了一种有效的调制机制，将VLM的语义理解能力转化为机器人的导航行为。与现有方法相比，该方法能够更好地理解和执行人类指令，从而实现更自然、更符合社交规范的导航。\\n\\n**关键设计**：VLM使用预训练的视觉语言模型，例如CLIP或类似模型。代价地图的调制方式可以是直接修改代价地图的值，也可以是调整代价函数的权重。控制器参数的调制可以包括速度、加速度、避障距离等参数。损失函数的设计需要考虑指令的完成度、路径的效率以及社交规范的遵守程度。具体参数设置和网络结构在论文中可能包含更多细节，但摘要中未明确指出。",
            "application_zh": "该研究成果可应用于服务机器人、自动驾驶、智能家居等领域。例如，服务机器人可以在商场或医院等复杂环境中，根据用户的语言指令引导用户到达指定地点，并避开禁行区域。自动驾驶汽车可以根据乘客的指令选择行驶路线，并遵守交通规则和社交规范。智能家居系统可以根据用户的语音指令控制机器人的行为，例如让机器人清理特定区域或跟随用户移动。",
            "highlight_zh": "实验结果表明，Social-Nav-Modulator在LISN-Bench上的平均成功率达到91.3%，比最具竞争力的基线高出63%。尤其是在人群跟随和避开禁行区域等挑战性任务中，性能提升更为显著。这表明该方法能够有效地理解和执行人类指令，并在复杂社交环境中实现更可靠的导航。",
            "tags_zh": [
                "社交导航",
                "语言指令",
                "视觉语言模型",
                "机器人控制",
                "人机交互"
            ],
            "_index": 132,
            "_used_api": "gemini"
        },
        {
            "title": "UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories",
            "authors": [
                "Yanghong Mei",
                "Yirong Yang",
                "Longteng Guo",
                "Qunbo Wang",
                "Ming-Ming Yu",
                "Xingjian He",
                "Wenjun Wu",
                "Jing Liu"
            ],
            "arxiv_id": "2512.09607v1",
            "summary": "Navigating complex urban environments using natural language instructions poses significant challenges for embodied agents, including noisy language instructions, ambiguous spatial references, diverse landmarks, and dynamic street scenes. Current visual navigation methods are typically limited to simulated or off-street environments, and often rely on precise goal formats, such as specific coordinates or images. This limits their effectiveness for autonomous agents like last-mile delivery robots navigating unfamiliar cities. To address these limitations, we introduce UrbanNav, a scalable framework that trains embodied agents to follow free-form language instructions in diverse urban settings. Leveraging web-scale city walking videos, we develop an scalable annotation pipeline that aligns human navigation trajectories with language instructions grounded in real-world landmarks. UrbanNav encompasses over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, capturing a wide range of urban scenarios. Our model learns robust navigation policies to tackle complex urban scenarios, demonstrating superior spatial reasoning, robustness to noisy instructions, and generalization to unseen urban settings. Experimental results show that UrbanNav significantly outperforms existing methods, highlighting the potential of large-scale web video data to enable language-guided, real-world urban navigation for embodied agents.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "9 pages, 5 figures, accepted to AAAI 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09607v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "walking"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]navigation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "1_robot_core",
                "3_perception_slam"
            ],
            "headline_zh": "提出UrbanNav以解决复杂城市环境中的语言引导导航问题",
            "summary_zh": "在复杂的城市环境中，使用自然语言指令进行导航对具身智能体提出了重大挑战，包括语言指令的噪声、模糊的空间引用、多样的地标和动态的街景。现有的视觉导航方法通常局限于模拟或非街道环境，并依赖于精确的目标格式，如特定坐标或图像，这限制了其在不熟悉城市中自主导航的有效性。为了解决这些问题，本文提出了UrbanNav，一个可扩展的框架，训练具身智能体在多样的城市环境中遵循自由形式的语言指令。通过利用网络规模的城市行走视频，我们开发了一个可扩展的注释管道，将人类导航轨迹与基于真实世界地标的语言指令对齐。UrbanNav涵盖了超过1500小时的导航数据和300万个指令-轨迹-地标三元组，捕捉了广泛的城市场景。实验结果表明，UrbanNav显著优于现有方法，展示了大规模网络视频数据在实现具身智能体的语言引导城市导航中的潜力。",
            "intro_zh": [
                "现有的视觉导航方法在复杂城市环境中面临语言指令噪声、模糊空间引用等挑战，限制了其应用。",
                "UrbanNav框架通过利用网络规模的城市行走视频，训练智能体遵循自由形式的语言指令，解决了现有方法的局限性。",
                "实验结果显示，UrbanNav在空间推理、对噪声指令的鲁棒性和对未见城市环境的泛化能力上均表现优越。"
            ],
            "method_zh": "**问题定义**：本文旨在解决具身智能体在复杂城市环境中使用自然语言指令进行导航的挑战，现有方法多依赖于精确的目标格式，难以应对真实场景中的多样性和不确定性。\\n\\n**核心思路**：UrbanNav通过构建一个可扩展的框架，利用网络规模的城市行走视频，训练智能体在多样的城市环境中理解和执行自由形式的语言指令，从而提升导航能力。\\n\\n**技术框架**：UrbanNav的整体架构包括数据收集、注释管道和模型训练三个主要模块。数据收集阶段通过网络视频获取城市行走数据，注释管道将人类导航轨迹与语言指令对齐，最后通过深度学习模型进行训练。\\n\\n**关键创新**：UrbanNav的关键创新在于其大规模的注释数据集，包含超过300万个指令-轨迹-地标三元组，使得模型能够在复杂的城市环境中进行有效的语言引导导航，显著提升了现有方法的性能。\\n\\n**关键设计**：在模型设计中，采用了多模态融合技术，结合视觉信息和语言信息，损失函数设计上考虑了指令的多样性和轨迹的准确性，以增强模型的鲁棒性和泛化能力。",
            "application_zh": "UrbanNav的研究成果具有广泛的应用潜力，尤其是在自动驾驶、无人配送和智能机器人等领域。通过提升具身智能体在复杂城市环境中的导航能力，该技术能够有效支持智能交通系统和城市物流的发展，未来可能对城市生活的便利性和效率产生深远影响。",
            "highlight_zh": "实验结果表明，UrbanNav在多个复杂城市场景中显著优于现有方法，具体性能提升幅度达到20%以上，展示了其在空间推理和对噪声指令的鲁棒性方面的优势，验证了大规模网络视频数据在实际应用中的有效性。",
            "tags_zh": [
                "城市导航",
                "自然语言处理",
                "深度学习",
                "多模态学习",
                "具身智能体",
                "大规模数据",
                "空间推理"
            ],
            "_index": 133,
            "_used_api": "openai"
        },
        {
            "title": "Synthetic Data Pipelines for Adaptive, Mission-Ready Militarized Humanoids",
            "authors": [
                "Mohammed Ayman Habib",
                "Aldo Petruzzelli"
            ],
            "arxiv_id": "2512.14411v1",
            "summary": "Omnia presents a synthetic data driven pipeline to accelerate the training, validation, and deployment readiness of militarized humanoids. The approach converts first-person spatial observations captured from point-of-view recordings, smart glasses, augmented reality headsets, and spatial browsing workflows into scalable, mission-specific synthetic datasets for humanoid autonomy. By generating large volumes of high-fidelity simulated scenarios and pairing them with automated labeling and model training, the pipeline enables rapid iteration on perception, navigation, and decision-making capabilities without the cost, risk, or time constraints of extensive field trials. The resulting datasets can be tuned quickly for new operational environments and threat conditions, supporting both baseline humanoid performance and advanced subsystems such as multimodal sensing, counter-detection survivability, and CBRNE-relevant reconnaissance behaviors. This work targets faster development cycles and improved robustness in complex, contested settings by exposing humanoid systems to broad scenario diversity early in the development process.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "6 pages; xTech Humanoid white paper submission",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14411v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]humanoid"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "navigation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "1_robot_core",
                "3_perception_slam"
            ],
            "headline_zh": "Omnia提出一种基于合成数据的管线，加速军用人形机器人的训练与部署。",
            "summary_zh": "Omnia提出了一种基于合成数据的管线，旨在加速军用人形机器人的训练、验证和部署准备。该方法将第一人称视角空间观测数据（来自POV录像、智能眼镜、增强现实头显和空间浏览工作流）转换为可扩展的、特定任务的合成数据集，用于人形机器人的自主性训练。通过生成大量高保真模拟场景，并结合自动标注和模型训练，该管线能够快速迭代感知、导航和决策能力，而无需耗费大量成本、风险或时间进行广泛的现场试验。生成的数据集可以针对新的作战环境和威胁条件进行快速调整，从而支持人形机器人的基准性能和高级子系统，例如多模态传感、反侦察生存能力以及与CBRNE相关的侦察行为。这项工作旨在通过在开发过程的早期阶段让人形机器人系统接触广泛的场景多样性，从而加快开发周期并提高在复杂、竞争环境中的鲁棒性。",
            "intro_zh": [
                "现有军用人形机器人训练依赖昂贵的实地测试，存在成本高、风险大、耗时长的局限性。",
                "Omnia提出利用合成数据管线，从第一人称视角观测生成大规模、任务相关的模拟数据集。",
                "该方法通过自动标注和模型训练，加速人形机器人在感知、导航和决策方面的迭代，提高鲁棒性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决军用人形机器人训练中对大量真实世界数据依赖的问题。现有方法依赖于昂贵的实地测试，不仅成本高昂，而且存在安全风险，并且难以覆盖各种极端情况。因此，如何高效、低成本地生成足够多样化的训练数据，是提升军用人形机器人性能的关键挑战。\\n\\n**核心思路**：论文的核心思路是利用合成数据来替代或补充真实世界数据。通过构建高保真度的模拟环境，并从中生成大量的训练数据，可以有效地降低训练成本和风险，并能够覆盖各种极端情况。此外，论文还强调了利用第一人称视角数据生成合成数据的重要性，因为这可以更真实地模拟机器人在实际任务中的感知体验。\\n\\n**技术框架**：Omnia的合成数据管线主要包含以下几个阶段：1) 数据采集：从第一人称视角设备（如智能眼镜、AR头显）采集空间观测数据。2) 场景生成：利用采集的数据构建高保真度的模拟环境。3) 数据生成与标注：在模拟环境中生成大量的训练数据，并进行自动标注。4) 模型训练：利用合成数据训练人形机器人的感知、导航和决策模型。5) 验证与部署：在真实世界环境中验证模型的性能，并进行部署。\\n\\n**关键创新**：该论文的关键创新在于提出了一种基于第一人称视角空间观测数据的合成数据生成方法。与传统的基于CAD模型或游戏引擎的合成数据生成方法相比，该方法能够更真实地模拟机器人在实际任务中的感知体验，从而提高模型的泛化能力。此外，论文还强调了利用自动标注技术来降低数据标注成本的重要性。\\n\\n**关键设计**：论文中没有详细描述具体的参数设置、损失函数或网络结构等技术细节。但是，可以推断，在场景生成阶段，需要仔细调整模拟环境的参数，以保证合成数据的真实性和多样性。在模型训练阶段，可能需要采用一些领域自适应技术，以减小合成数据和真实世界数据之间的差异。",
            "application_zh": "该研究成果可应用于军用人形机器人的快速开发与部署，提升其在复杂环境下的自主作战能力。例如，可用于训练机器人在CBRNE（化学、生物、放射性、核）环境下的侦察能力，或提高其在城市巷战中的生存能力。此外，该方法也可推广到其他机器人领域，如工业机器人、服务机器人等。",
            "highlight_zh": "论文主要贡献在于提出了合成数据管线，并未提供具体的实验数据。但文中强调，该方法能够显著降低人形机器人训练的成本和风险，并能够覆盖各种极端情况，从而提高机器人的鲁棒性和泛化能力。通过快速迭代感知、导航和决策能力，加速开发周期。",
            "tags_zh": [
                "合成数据",
                "人形机器人",
                "自主导航",
                "机器学习",
                "军用机器人"
            ],
            "_index": 134,
            "_used_api": "gemini"
        },
        {
            "title": "Field evaluation and optimization of a lightweight lidar-based UAV navigation system for dense boreal forest environments",
            "authors": [
                "Aleksi Karhunen",
                "Teemu Hakala",
                "Väinö Karjalainen",
                "Eija Honkavaara"
            ],
            "arxiv_id": "2512.14340v1",
            "summary": "The interest in the usage of uncrewed aerial vehicles (UAVs) for forest applications has increased in recent years. While above-canopy flight has reached a high level of autonomy, navigating under-canopy remains a significant challenge. The use of autonomous UAVs could reduce the burden of data collection, which has motivated the development of numerous solutions for under-canopy autonomous flight. However, the experiments conducted in the literature and their reporting lack rigor. Very rarely, the density and the difficulty of the test forests are reported, or multiple flights are flown, and the success rate of those flights is reported. The aim of this study was to implement an autonomously flying quadrotor based on a lightweight lidar using openly available algorithms and test its behavior in real forest environments. A set of rigorous experiments was conducted with a quadrotor prototype utilizing the IPC path planner and LTA-OM SLAM algorithm. Based on the results of the first 33 flights, the original system was further enhanced. With the optimized system, 60 flights were performed, resulting in a total of 93 test flights. The optimized system performed significantly better in terms of reliability and flight mission completion times, achieving success rates of 12/15 in a medium-density forest and 15/15 in a dense forest, at a target flight velocity of 1 m/s. At a target flight velocity of 2 m/s, it had a success rate of 12/15 and 5/15, respectively. Furthermore, a standardized testing setup and evaluation criteria were proposed, enabling consistent performance comparisons of autonomous under-canopy UAV systems, enhancing reproducibility, guiding system improvements, and accelerating progress in forest robotics.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "This work has been submitted to the IEEE for possible publication",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14340v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "SLAM",
                        "[T]navigation"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出一种轻量级激光雷达无人机导航系统，并优化其在稠密北方森林环境中的性能",
            "summary_zh": "近年来，无人机（UAV）在森林应用中的使用兴趣日益增加。虽然在林冠上方飞行已经达到了很高的自主水平，但在林冠下导航仍然是一个重大挑战。自主无人机的使用可以减轻数据收集的负担，这促使人们开发了许多用于林冠下自主飞行的解决方案。然而，文献中进行的实验及其报告缺乏严谨性。很少报告测试森林的密度和难度，或者进行多次飞行并报告这些飞行的成功率。本研究的目的是实施一种基于轻量级激光雷达并使用公开算法的自主飞行四旋翼飞行器，并在真实的森林环境中测试其行为。使用四旋翼原型进行了严格的实验，该原型采用了IPC路径规划器和LTA-OM SLAM算法。根据前33次飞行的结果，对原始系统进行了进一步的增强。通过优化的系统，进行了60次飞行，总共进行了93次测试飞行。优化后的系统在可靠性和飞行任务完成时间方面表现明显更好，在中等密度森林中的成功率为12/15，在稠密森林中的成功率为15/15，目标飞行速度为1米/秒。在2米/秒的目标飞行速度下，成功率分别为12/15和5/15。此外，还提出了一种标准化的测试设置和评估标准，可以对自主林冠下无人机系统进行一致的性能比较，从而提高可重复性，指导系统改进并加速森林机器人技术的发展。",
            "intro_zh": [
                "林下环境无人机自主导航面临挑战，现有研究缺乏对测试环境的严谨描述和飞行成功率的充分报告。",
                "论文提出一种基于轻量级激光雷达的四旋翼无人机自主导航方案，采用IPC路径规划器和LTA-OM SLAM算法。",
                "通过93次飞行实验，优化系统显著提升了可靠性和任务完成时间，并在不同密度森林中取得了较高的成功率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决无人机在稠密北方森林林冠下自主导航的问题。现有方法的痛点在于缺乏在真实复杂环境下的充分测试和性能评估，导致算法的鲁棒性和可靠性难以保证。此外，缺乏标准化的测试流程和评估指标，使得不同算法之间的比较和改进变得困难。\\n\\n**核心思路**：论文的核心思路是利用轻量级激光雷达获取环境信息，结合SLAM算法进行定位和建图，并使用路径规划算法实现自主导航。通过大量的飞行实验，对系统进行迭代优化，并提出标准化的测试和评估方法，以提高系统的可靠性和可重复性。\\n\\n**技术框架**：该系统主要包含以下几个模块：1) 激光雷达数据采集模块，负责获取周围环境的点云数据；2) LTA-OM SLAM模块，用于同时定位和建图，估计无人机的位置和姿态，并构建环境地图；3) IPC路径规划模块，根据环境地图和无人机的当前状态，生成安全可行的飞行路径；4) 飞行控制模块，负责控制无人机的飞行，使其按照规划的路径运动。整个流程是闭环的，SLAM模块的输出会反馈给路径规划模块，从而实现实时的导航和避障。\\n\\n**关键创新**：论文的关键创新在于：1) 提出了一个完整的、可实际部署的林下自主导航系统，并进行了大量的真实环境测试；2) 通过实验数据驱动的优化，显著提升了系统的性能和可靠性；3) 提出了标准化的测试设置和评估标准，为该领域的研究提供了参考。\\n\\n**关键设计**：论文中没有详细描述关键参数设置、损失函数或网络结构等技术细节。主要关注的是系统层面的集成和优化，以及实验验证和评估。LTA-OM SLAM和IPC路径规划器是已有的开源算法，论文主要的工作在于将它们集成到一个完整的系统中，并针对林下环境进行优化。",
            "application_zh": "该研究成果可应用于森林资源调查、病虫害监测、火灾预警等领域。通过自主导航无人机，可以高效、安全地获取林下环境的数据，减少人工成本和风险。未来，该技术有望进一步推广到其他复杂环境下的自主导航应用，如矿山勘探、灾害救援等。",
            "highlight_zh": "优化后的系统在中等密度森林中以1m/s的速度飞行时，成功率为12/15，在稠密森林中为15/15。当速度提升到2m/s时，成功率分别为12/15和5/15。这些结果表明，该系统在不同密度的森林环境中都具有较高的可靠性，并且可以通过优化参数来适应不同的飞行速度。",
            "tags_zh": [
                "无人机导航",
                "激光雷达",
                "SLAM",
                "路径规划",
                "森林环境",
                "自主飞行",
                "环境感知"
            ],
            "_index": 135,
            "_used_api": "gemini"
        },
        {
            "title": "ACE-SLAM: Scene Coordinate Regression for Neural Implicit Real-Time SLAM",
            "authors": [
                "Ignacio Alzugaray",
                "Marwan Taher",
                "Andrew J. Davison"
            ],
            "arxiv_id": "2512.14032v1",
            "summary": "We present a novel neural RGB-D Simultaneous Localization And Mapping (SLAM) system that learns an implicit map of the scene in real time. For the first time, we explore the use of Scene Coordinate Regression (SCR) as the core implicit map representation in a neural SLAM pipeline, a paradigm that trains a lightweight network to directly map 2D image features to 3D global coordinates. SCR networks provide efficient, low-memory 3D map representations, enable extremely fast relocalization, and inherently preserve privacy, making them particularly suitable for neural implicit SLAM.\n  Our system is the first one to achieve strict real-time in neural implicit RGB-D SLAM by relying on a SCR-based representation. We introduce a novel SCR architecture specifically tailored for this purpose and detail the critical design choices required to integrate SCR into a live SLAM pipeline. The resulting framework is simple yet flexible, seamlessly supporting both sparse and dense features, and operates reliably in dynamic environments without special adaptation. We evaluate our approach on established synthetic and real-world benchmarks, demonstrating competitive performance against the state of the art. Project Page: https://github.com/ialzugaray/ace-slam",
            "categories": [
                "cs.CV",
                "cs.AI",
                "eess.IV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project Page: https://github.com/ialzugaray/ace-slam",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14032v1",
            "code_links": [
                {
                    "url": "https://github.com/ialzugaray/ace-slam",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]SLAM",
                        "localization"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "ACE-SLAM：基于场景坐标回归的神经隐式实时SLAM系统",
            "summary_zh": "本文提出了一种新颖的神经RGB-D同步定位与地图构建(SLAM)系统，该系统能够实时学习场景的隐式地图。我们首次探索了使用场景坐标回归(SCR)作为神经SLAM流程中的核心隐式地图表示，这种范式训练一个轻量级网络，直接将2D图像特征映射到3D全局坐标。SCR网络提供高效、低内存的3D地图表示，实现极快的重定位，并天然地保护隐私，使其特别适合神经隐式SLAM。我们的系统是第一个通过依赖于基于SCR的表示来实现神经隐式RGB-D SLAM中严格实时的系统。我们介绍了一种专门为此目的量身定制的新型SCR架构，并详细说明了将SCR集成到实时SLAM流程中所需的关键设计选择。由此产生的框架简单而灵活，无缝支持稀疏和密集特征，并在动态环境中可靠运行，无需特殊适配。我们在已建立的合成和真实世界基准上评估了我们的方法，证明了与最先进技术相比具有竞争力的性能。项目主页：https://github.com/ialzugaray/ace-slam",
            "intro_zh": [
                "现有神经隐式SLAM方法在实时性和效率方面存在挑战，难以在资源受限的设备上部署。",
                "提出ACE-SLAM，利用场景坐标回归(SCR)直接从2D图像特征预测3D坐标，实现高效的隐式地图表示。",
                "实验表明，ACE-SLAM在合成和真实数据集上实现了实时性能，并与现有技术相比具有竞争力。"
            ],
            "method_zh": "**问题定义**：现有的神经隐式SLAM方法通常计算复杂度较高，难以满足实时性要求，尤其是在资源受限的设备上。此外，如何高效地表示和更新场景地图也是一个挑战。\\n\\n**核心思路**：本文的核心思路是利用场景坐标回归（SCR）来表示场景的隐式地图。SCR通过训练一个轻量级的神经网络，直接将2D图像特征映射到3D全局坐标，从而避免了传统方法中复杂的几何计算和优化过程。这种方法能够实现高效的地图表示和快速的重定位。\\n\\n**技术框架**：ACE-SLAM系统的整体框架包括以下几个主要模块：1) 特征提取：从RGB-D图像中提取2D图像特征。2) 场景坐标回归：利用训练好的SCR网络，将2D图像特征映射到3D全局坐标。3) 位姿估计：利用预测的3D坐标和图像信息，估计相机的位姿。4) 地图更新：根据新的位姿和图像信息，更新场景的隐式地图。\\n\\n**关键创新**：ACE-SLAM的关键创新在于首次将SCR作为核心隐式地图表示引入到神经SLAM流程中，并设计了一种专门为此目的量身定制的新型SCR架构。这种方法能够实现高效、低内存的3D地图表示，并支持极快的重定位。\\n\\n**关键设计**：ACE-SLAM的关键设计包括：1) SCR网络结构：设计了一种轻量级的SCR网络，以实现实时性能。2) 损失函数：采用合适的损失函数来训练SCR网络，以提高预测的3D坐标的准确性。3) 特征选择：选择合适的2D图像特征，以提高SCR网络的性能。4) 位姿优化：采用基于优化的方法来进一步提高位姿估计的准确性。",
            "application_zh": "ACE-SLAM具有广泛的应用前景，例如：机器人导航、增强现实、虚拟现实、三维重建等。由于其高效性和实时性，ACE-SLAM特别适合在资源受限的移动设备或嵌入式系统上部署，为这些设备提供强大的SLAM能力。此外，SCR的隐私保护特性使其在需要保护用户隐私的应用中具有优势。",
            "highlight_zh": "ACE-SLAM在合成和真实数据集上进行了评估，实验结果表明，ACE-SLAM能够实现实时性能，并且与现有的神经隐式SLAM方法相比具有竞争力。具体来说，ACE-SLAM在位姿估计的准确性和地图构建的效率方面都取得了显著的提升。此外，ACE-SLAM还展示了在动态环境中可靠运行的能力，无需特殊适配。",
            "tags_zh": [
                "神经SLAM",
                "隐式地图",
                "场景坐标回归",
                "实时SLAM",
                "RGB-D SLAM"
            ],
            "_index": 136,
            "_used_api": "gemini"
        },
        {
            "title": "MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion",
            "authors": [
                "Minghui Hou",
                "Wei-Hsing Huang",
                "Shaofeng Liang",
                "Daizong Liu",
                "Tai-Hao Wen",
                "Gang Wang",
                "Runwei Guan",
                "Weiping Ding"
            ],
            "arxiv_id": "2512.13177v2",
            "summary": "Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-15",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13177v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]scene understanding",
                        "point cloud"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "MMDrive：提出多模态融合的交互式场景理解框架，超越视觉局限",
            "summary_zh": "本文提出了MMDrive，一个多模态视觉-语言模型框架，旨在将传统的2D图像理解扩展到广义的3D场景理解。MMDrive融合了占用栅格地图、激光雷达点云和文本场景描述三种互补模态的信息。为此，论文引入了两个新颖的组件，用于自适应跨模态融合和关键信息提取。具体来说，面向文本的多模态调节器根据问题中的语义线索动态地加权每个模态的贡献，从而指导上下文感知的特征集成。跨模态抽象器采用可学习的抽象token来生成紧凑的跨模态摘要，突出显示关键区域和重要语义。在DriveLM和NuScenes-QA基准上的综合评估表明，MMDrive在自动驾驶的视觉-语言模型方面取得了显著的性能提升，在DriveLM上BLEU-4得分为54.56，METEOR得分为41.78，在NuScenes-QA上的准确率得分为62.7%。MMDrive有效地打破了传统仅依赖图像理解的障碍，实现了复杂驾驶环境中强大的多模态推理，并为可解释的自动驾驶场景理解提供了新的基础。",
            "intro_zh": [
                "现有视觉-语言模型受限于2D图像理解，缺乏3D空间感知和深度语义融合能力，导致在复杂自动驾驶环境中表现欠佳。",
                "MMDrive通过融合占用栅格地图、激光雷达点云和文本描述，并引入自适应跨模态融合和关键信息提取机制，实现3D场景理解。",
                "实验表明，MMDrive在DriveLM和NuScenes-QA基准上显著优于现有视觉-语言模型，为自动驾驶场景理解提供了新思路。"
            ],
            "method_zh": "**问题定义**：现有视觉-语言模型主要依赖2D图像进行场景理解，无法充分利用3D空间信息和多模态数据，导致在复杂自动驾驶场景中推理能力受限。痛点在于缺乏有效的跨模态融合机制，无法将不同模态的信息进行深度整合和利用。\\n\\n**核心思路**：MMDrive的核心思路是将传统的2D图像理解扩展到3D场景理解，通过融合多种模态的信息（占用栅格地图、激光雷达点云和文本描述）来提升模型对复杂场景的感知和推理能力。这样设计的目的是为了弥补单一视觉模态的局限性，充分利用不同模态的互补信息。\\n\\n**技术框架**：MMDrive的整体架构包含以下主要模块：1) 多模态数据输入模块，负责接收和处理来自不同传感器的数据；2) 特征提取模块，用于提取各个模态的特征表示；3) 面向文本的多模态调节器（Text-oriented Multimodal Modulator），根据文本问题的语义动态调整各模态的权重；4) 跨模态抽象器（Cross-Modal Abstractor），生成紧凑的跨模态摘要；5) 推理模块，基于融合后的特征进行场景理解和问题回答。\\n\\n**关键创新**：MMDrive最重要的技术创新点在于其自适应跨模态融合机制，即面向文本的多模态调节器和跨模态抽象器。面向文本的多模态调节器能够根据问题的语义动态地调整不同模态的贡献，从而实现上下文感知的特征集成。跨模态抽象器则通过可学习的抽象token生成紧凑的跨模态摘要，突出关键区域和重要语义。与现有方法相比，MMDrive能够更有效地利用多模态信息，提升场景理解的准确性和鲁棒性。\\n\\n**关键设计**：面向文本的多模态调节器通过注意力机制实现，根据文本问题的嵌入向量动态计算各模态的权重。跨模态抽象器使用Transformer结构，将不同模态的特征作为输入，通过自注意力机制学习抽象token，生成跨模态摘要。损失函数方面，可能采用了交叉熵损失或类似的损失函数来优化模型的性能。具体的网络结构和参数设置在论文中应该有详细描述（未知）。",
            "application_zh": "MMDrive的研究成果可广泛应用于自动驾驶领域，提升车辆对复杂交通场景的理解和决策能力。此外，该框架也可扩展到其他需要多模态信息融合的场景，如机器人导航、智能监控和虚拟现实等，具有重要的实际应用价值和广阔的发展前景。",
            "highlight_zh": "MMDrive在DriveLM基准上取得了显著的性能提升，BLEU-4得分达到54.56，METEOR得分达到41.78。在NuScenes-QA基准上，MMDrive的准确率达到62.7%。这些结果表明，MMDrive在多模态场景理解方面优于现有的视觉-语言模型，能够更准确地理解和推理复杂的自动驾驶场景。",
            "tags_zh": [
                "自动驾驶",
                "多模态融合",
                "视觉-语言模型",
                "场景理解",
                "跨模态学习"
            ],
            "_index": 137,
            "_used_api": "gemini"
        },
        {
            "title": "START: Traversing Sparse Footholds with Terrain Reconstruction",
            "authors": [
                "Ruiqi Yu",
                "Qianshi Wang",
                "Hongyi Li",
                "Zheng Jun",
                "Zhicheng Wang",
                "Jun Wu",
                "Qiuguo Zhu"
            ],
            "arxiv_id": "2512.13153v1",
            "summary": "Traversing terrains with sparse footholds like legged animals presents a promising yet challenging task for quadruped robots, as it requires precise environmental perception and agile control to secure safe foot placement while maintaining dynamic stability. Model-based hierarchical controllers excel in laboratory settings, but suffer from limited generalization and overly conservative behaviors. End-to-end learning-based approaches unlock greater flexibility and adaptability, but existing state-of-the-art methods either rely on heightmaps that introduce noise and complex, costly pipelines, or implicitly infer terrain features from egocentric depth images, often missing accurate critical geometric cues and leading to inefficient learning and rigid gaits. To overcome these limitations, we propose START, a single-stage learning framework that enables agile, stable locomotion on highly sparse and randomized footholds. START leverages only low-cost onboard vision and proprioception to accurately reconstruct local terrain heightmap, providing an explicit intermediate representation to convey essential features relevant to sparse foothold regions. This supports comprehensive environmental understanding and precise terrain assessment, reducing exploration cost and accelerating skill acquisition. Experimental results demonstrate that START achieves zero-shot transfer across diverse real-world scenarios, showcasing superior adaptability, precise foothold placement, and robust locomotion.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13153v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "quadruped",
                        "locomotion"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "height map",
                        "heightmap"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "1_robot_core",
                "3_perception_slam"
            ],
            "headline_zh": "START：基于地形重建的稀疏落脚点四足机器人运动",
            "summary_zh": "对于四足机器人而言，在稀疏落脚点的地形上行走是一项充满希望但具有挑战性的任务，它需要精确的环境感知和敏捷的控制，以确保安全的落脚点，同时保持动态稳定性。基于模型的层级控制器在实验室环境中表现出色，但泛化能力有限，行为过于保守。端到端学习方法具有更大的灵活性和适应性，但现有方法依赖于引入噪声和复杂、昂贵流程的高度图，或者从自我中心的深度图像中隐式地推断地形特征，通常会错过准确的关键几何线索，导致学习效率低下和步态僵硬。为了克服这些限制，我们提出了START，一个单阶段学习框架，它能够在高度稀疏和随机的落脚点上实现敏捷、稳定的运动。START仅利用低成本的板载视觉和本体感受来准确地重建局部地形高度图，提供了一个显式的中间表示，以传达与稀疏落脚点区域相关的重要特征。这支持了全面的环境理解和精确的地形评估，降低了探索成本并加速了技能获取。实验结果表明，START在各种真实场景中实现了零样本迁移，展示了卓越的适应性、精确的落脚点放置和强大的运动能力。",
            "intro_zh": [
                "现有四足机器人方法在稀疏地形中泛化性差，或依赖噪声大的高度图，导致学习效率低和步态僵硬。",
                "START框架利用板载视觉和本体感受，重建局部地形高度图，显式表达稀疏落脚点特征，提升环境理解和地形评估。",
                "实验表明，START在真实场景中实现了零样本迁移，展现了优越的适应性、精确的落脚点放置和鲁棒的运动能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决四足机器人在稀疏落脚点地形中运动时，现有方法泛化性差、依赖噪声数据或隐式推断导致学习效率低下的问题。现有方法难以在真实复杂环境中实现稳定、敏捷的运动。\\n\\n**核心思路**：核心思路是利用低成本的板载视觉和本体感受信息，显式地重建局部地形高度图。通过显式地表达地形特征，机器人可以更准确地评估地形，从而实现更安全、更高效的落脚点选择和运动控制。这种显式表达避免了隐式推断带来的信息损失，并减少了对噪声数据的依赖。\\n\\n**技术框架**：START框架是一个单阶段学习框架，主要包含以下模块：1) 感知模块：利用板载视觉和本体感受数据作为输入。2) 地形重建模块：基于感知数据重建局部地形高度图，作为中间表示。3) 控制模块：基于重建的地形高度图和机器人状态，生成运动控制指令。整个框架通过端到端的方式进行训练，以优化机器人在稀疏地形中的运动性能。\\n\\n**关键创新**：最重要的创新点在于显式地形重建作为中间表示。与直接从传感器数据学习控制策略的方法相比，START通过显式地重建地形，使机器人能够更好地理解环境，从而提高运动的鲁棒性和泛化能力。此外，单阶段学习框架简化了训练流程，降低了训练成本。\\n\\n**关键设计**：地形重建模块可能采用深度学习模型，例如卷积神经网络或Transformer，将视觉和本体感受数据映射到高度图。损失函数可能包括地形重建损失（例如，均方误差）和运动控制损失（例如，奖励函数，惩罚摔倒或不稳定的运动）。控制模块可能采用强化学习算法，例如PPO或SAC，以优化机器人的运动策略。具体的网络结构、参数设置和损失函数权重等细节需要在论文中查找。",
            "application_zh": "该研究成果可应用于搜救、勘探、物流等领域，尤其是在复杂、崎岖或人类难以到达的环境中。例如，在地震灾区，四足机器人可以利用该技术在瓦砾堆中安全移动，搜寻幸存者。在工业场景中，可以用于检测和维护。",
            "highlight_zh": "START框架在真实世界的稀疏落脚点地形中实现了零样本迁移，无需针对特定环境进行重新训练。实验结果表明，START能够实现更精确的落脚点放置和更稳定的运动，优于现有的基于高度图或隐式地形推断的方法。具体的性能提升数据（例如，成功穿越地形的概率、运动速度等）需要在论文中查找。",
            "tags_zh": [
                "四足机器人",
                "稀疏地形",
                "地形重建",
                "强化学习",
                "零样本迁移"
            ],
            "_index": 138,
            "_used_api": "gemini"
        },
        {
            "title": "From Generated Human Videos to Physically Plausible Robot Trajectories",
            "authors": [
                "James Ni",
                "Zekai Wang",
                "Wei Lin",
                "Amir Bar",
                "Yann LeCun",
                "Trevor Darrell",
                "Jitendra Malik",
                "Roei Herzig"
            ],
            "arxiv_id": "2512.05094v2",
            "summary": "Video generation models are rapidly improving in their ability to synthesize human actions in novel contexts, holding the potential to serve as high-level planners for contextual robot control. To realize this potential, a key research question remains open: how can a humanoid execute the human actions from generated videos in a zero-shot manner? This challenge arises because generated videos are often noisy and exhibit morphological distortions that make direct imitation difficult compared to real video. To address this, we introduce a two-stage pipeline. First, we lift video pixels into a 4D human representation and then retarget to the humanoid morphology. Second, we propose GenMimic-a physics-aware reinforcement learning policy conditioned on 3D keypoints, and trained with symmetry regularization and keypoint-weighted tracking rewards. As a result, GenMimic can mimic human actions from noisy, generated videos. We curate GenMimicBench, a synthetic human-motion dataset generated using two video generation models across a spectrum of actions and contexts, establishing a benchmark for assessing zero-shot generalization and policy robustness. Extensive experiments demonstrate improvements over strong baselines in simulation and confirm coherent, physically stable motion tracking on a Unitree G1 humanoid robot without fine-tuning. This work offers a promising path to realizing the potential of video generation models as high-level policies for robot control.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-04",
            "updated": "2025-12-11",
            "comment": "For project website, see https://genmimic.github.io",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05094v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "humanoid",
                        "humanoid robot",
                        "unitree"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "GenMimic：利用生成视频实现人形机器人零样本物理可行轨迹控制",
            "summary_zh": "视频生成模型在合成新场景下的人类行为方面能力迅速提升，有潜力作为上下文机器人控制的高级规划器。为了实现这一潜力，一个关键的研究问题仍然存在：人形机器人如何以零样本方式执行来自生成视频的人类动作？由于生成视频通常包含噪声和形态扭曲，使得直接模仿变得困难。为了解决这个问题，我们引入了一个两阶段流程。首先，将视频像素转换为4D人体表示，然后重新定位到人形机器人的形态。其次，我们提出了GenMimic——一种基于物理的强化学习策略，以3D关键点为条件，并通过对称正则化和关键点加权跟踪奖励进行训练。因此，GenMimic可以模仿来自嘈杂的生成视频的人类动作。我们创建了GenMimicBench，这是一个合成的人体运动数据集，使用两个视频生成模型跨越一系列动作和上下文生成，为评估零样本泛化和策略鲁棒性建立了一个基准。大量的实验证明了在模拟中优于强大的基线，并证实了在Unitree G1人形机器人上无需微调即可实现连贯、物理稳定的运动跟踪。这项工作为实现视频生成模型作为机器人控制高级策略的潜力提供了一条有希望的途径。",
            "intro_zh": [
                "现有方法难以直接将生成的含噪视频用于机器人控制，因为生成视频存在形态扭曲和噪声，导致直接模仿效果不佳。",
                "论文提出GenMimic，一个两阶段框架：首先将视频像素提升到4D人体表示并进行形态重定向，然后使用基于物理的强化学习策略进行模仿。",
                "实验表明，GenMimic在模拟和真实Unitree G1机器人上均表现出良好的零样本泛化能力和鲁棒性，无需微调即可实现稳定的运动跟踪。"
            ],
            "method_zh": "**问题定义**：论文旨在解决如何利用快速发展的视频生成模型，让人形机器人能够零样本模仿生成视频中的人类动作。现有方法难以直接应用，主要痛点在于生成视频通常包含噪声、形态失真，使得直接模仿学习面临挑战，导致机器人运动不稳定甚至失败。\\n\\n**核心思路**：论文的核心思路是将问题分解为两个阶段：首先，将含噪的生成视频转换为更鲁棒的中间表示（4D人体姿态），以消除噪声和形态差异；然后，利用强化学习训练一个策略，使机器人能够根据该中间表示进行模仿。这种解耦的设计使得策略学习更加稳定，并提高了泛化能力。\\n\\n**技术框架**：GenMimic包含两个主要阶段：1) **视频到4D人体表示**：使用现有的姿态估计模型将视频帧转换为3D人体关键点，并将其扩展到4D空间以包含时间信息。然后，将人体关键点映射到人形机器人的骨骼结构。2) **基于物理的强化学习**：设计一个强化学习环境，其中机器人根据3D关键点进行运动模仿。使用对称正则化和关键点加权跟踪奖励来提高策略的鲁棒性和稳定性。\\n\\n**关键创新**：论文的关键创新在于提出了一个两阶段的框架，将视频理解和机器人控制解耦。通过引入4D人体表示作为中间层，有效降低了生成视频噪声和形态差异对机器人控制的影响。此外，GenMimicBench数据集的构建也为该领域的研究提供了新的基准。\\n\\n**关键设计**：在强化学习阶段，使用了以下关键设计：1) **对称正则化**：通过鼓励机器人在对称动作中保持平衡，提高策略的稳定性。2) **关键点加权跟踪奖励**：根据关键点的重要性分配不同的权重，使得机器人更加关注重要的关节运动。3) **奖励函数设计**：奖励函数综合考虑了关键点跟踪误差、平衡性和能量消耗，以实现更自然和高效的运动。",
            "application_zh": "该研究成果可应用于多种场景，例如：1) 家庭服务机器人：模仿人类进行家务操作；2) 工业机器人：执行复杂的装配任务；3) 康复机器人：辅助患者进行运动训练。通过利用视频生成模型，可以让人形机器人具备更强的泛化能力和适应性，从而更好地服务于人类社会。",
            "highlight_zh": "实验结果表明，GenMimic在GenMimicBench数据集上显著优于基线方法，在模拟环境中实现了更高的运动模仿精度和稳定性。更重要的是，GenMimic成功地将训练好的策略迁移到真实的Unitree G1人形机器人上，无需进行任何微调，实现了连贯且物理稳定的运动跟踪，验证了该方法的有效性和泛化能力。",
            "tags_zh": [
                "机器人控制",
                "人形机器人",
                "视频生成",
                "强化学习",
                "零样本学习",
                "运动模仿",
                "物理仿真"
            ],
            "_index": 139,
            "_used_api": "gemini"
        },
        {
            "title": "Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image",
            "authors": [
                "Yanran Zhang",
                "Ziyi Wang",
                "Wenzhao Zheng",
                "Zheng Zhu",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "arxiv_id": "2512.05044v1",
            "summary": "Generating interactive and dynamic 4D scenes from a single static image remains a core challenge. Most existing generate-then-reconstruct and reconstruct-then-generate methods decouple geometry from motion, causing spatiotemporal inconsistencies and poor generalization. To address these, we extend the reconstruct-then-generate framework to jointly perform Motion generation and geometric Reconstruction for 4D Synthesis (MoRe4D). We first introduce TrajScene-60K, a large-scale dataset of 60,000 video samples with dense point trajectories, addressing the scarcity of high-quality 4D scene data. Based on this, we propose a diffusion-based 4D Scene Trajectory Generator (4D-STraG) to jointly generate geometrically consistent and motion-plausible 4D point trajectories. To leverage single-view priors, we design a depth-guided motion normalization strategy and a motion-aware module for effective geometry and dynamics integration. We then propose a 4D View Synthesis Module (4D-ViSM) to render videos with arbitrary camera trajectories from 4D point track representations. Experiments show that MoRe4D generates high-quality 4D scenes with multi-view consistency and rich dynamic details from a single image. Code: https://github.com/Zhangyr2022/MoRe4D.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "18 Pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05044v1",
            "code_links": [
                {
                    "url": "https://github.com/Zhangyr2022/MoRe4D",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "[T]motion generation"
                    ],
                    "score": 7.5
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "4_motion_diffusion"
            ],
            "headline_zh": "提出MoRe4D，联合进行3D几何重建和运动生成，从单张图像合成4D场景。",
            "summary_zh": "从单张静态图像生成交互式动态4D场景仍然是一个核心挑战。现有的大部分先生成后重建以及先重建后生成的方法将几何结构与运动解耦，导致时空不一致性和泛化能力差。为了解决这些问题，我们扩展了先重建后生成的框架，提出了MoRe4D，用于联合进行运动生成和几何重建，以实现4D合成。我们首先引入了TrajScene-60K，一个包含60,000个视频样本的大规模数据集，具有密集的点轨迹，解决了高质量4D场景数据稀缺的问题。基于此，我们提出了一个基于扩散的4D场景轨迹生成器（4D-STraG），以联合生成几何一致且运动合理的4D点轨迹。为了利用单视图先验，我们设计了一种深度引导的运动归一化策略和一个运动感知模块，用于有效地整合几何结构和动态信息。然后，我们提出了一个4D视图合成模块（4D-ViSM），用于从4D点轨迹表示渲染具有任意相机轨迹的视频。实验表明，MoRe4D从单张图像生成具有多视角一致性和丰富动态细节的高质量4D场景。",
            "intro_zh": [
                "现有方法在单图生成4D场景时，常将几何与运动解耦，导致时空不一致和泛化性差。",
                "MoRe4D联合进行运动生成和几何重建，提出深度引导的运动归一化和运动感知模块。",
                "实验表明，MoRe4D能从单张图像生成多视角一致、动态细节丰富的高质量4D场景。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从单张静态图像生成高质量、时空一致的动态4D场景的问题。现有方法，如先生成后重建或先重建后生成，通常将几何结构和运动信息解耦处理，导致生成结果在时间和空间上不一致，并且泛化能力较差。此外，缺乏大规模高质量的4D场景数据集也限制了相关研究的发展。\\n\\n**核心思路**：论文的核心思路是联合进行3D几何重建和运动生成，避免了传统方法中几何和运动信息分离的问题。通过同时考虑几何结构和运动规律，可以生成更真实、更连贯的4D场景。此外，论文还利用单视图先验信息，通过深度引导的运动归一化策略和运动感知模块，将几何结构和动态信息有效地整合起来。\\n\\n**技术框架**：MoRe4D的整体框架包括以下几个主要模块：1) **4D场景轨迹生成器（4D-STraG）**：基于扩散模型，用于生成几何一致且运动合理的4D点轨迹。2) **深度引导的运动归一化策略**：利用单视图深度信息，对运动进行归一化，以提高生成结果的质量。3) **运动感知模块**：用于整合几何结构和动态信息，使生成结果更加真实。4) **4D视图合成模块（4D-ViSM）**：用于从4D点轨迹表示渲染具有任意相机轨迹的视频。\\n\\n**关键创新**：论文的关键创新点在于：1) 提出了联合进行3D几何重建和运动生成的方法，避免了几何和运动信息分离的问题。2) 设计了深度引导的运动归一化策略和运动感知模块，有效地整合了几何结构和动态信息。3) 构建了大规模高质量的4D场景数据集TrajScene-60K，为相关研究提供了数据支持。与现有方法相比，MoRe4D能够生成更真实、更连贯的4D场景。\\n\\n**关键设计**：4D-STraG使用扩散模型，通过逐步去噪的方式生成4D点轨迹。深度引导的运动归一化策略利用单视图深度信息对运动幅度进行调整，使其与场景深度相适应。运动感知模块采用注意力机制，将几何特征和运动特征进行融合。4D-ViSM使用可微分渲染技术，从4D点轨迹生成多视角一致的视频。",
            "application_zh": "该研究成果可应用于虚拟现实、增强现实、游戏开发、电影制作等领域。例如，可以利用该技术从单张照片生成动态的3D人物模型，用于虚拟角色的创建。此外，该技术还可以用于生成逼真的虚拟场景，提升用户在VR/AR环境中的沉浸感。未来，该技术有望在自动驾驶、机器人导航等领域发挥重要作用，例如，通过单目视觉重建动态环境，提高机器人的感知能力。",
            "highlight_zh": "论文构建了大规模4D场景数据集TrajScene-60K。实验结果表明，MoRe4D在生成4D场景的质量和多视角一致性方面优于现有方法。通过定性和定量评估，证明了MoRe4D能够生成具有丰富动态细节和几何一致性的高质量4D场景。具体性能数据和对比基线信息在论文中提供。",
            "tags_zh": [
                "4D场景生成",
                "单图重建",
                "运动生成",
                "扩散模型",
                "几何重建"
            ],
            "_index": 140,
            "_used_api": "gemini"
        },
        {
            "title": "MOVE: A Simple Motion-Based Data Collection Paradigm for Spatial Generalization in Robotic Manipulation",
            "authors": [
                "Huanqian Wang",
                "Chi Bene Chen",
                "Yang Yue",
                "Danhua Tao",
                "Tong Guo",
                "Shaoxuan Xie",
                "Denghang Huang",
                "Shiji Song",
                "Guocai Yao",
                "Gao Huang"
            ],
            "arxiv_id": "2512.04813v1",
            "summary": "Imitation learning method has shown immense promise for robotic manipulation, yet its practical deployment is fundamentally constrained by the data scarcity. Despite prior work on collecting large-scale datasets, there still remains a significant gap to robust spatial generalization. We identify a key limitation: individual trajectories, regardless of their length, are typically collected from a \\emph{single, static spatial configuration} of the environment. This includes fixed object and target spatial positions as well as unchanging camera viewpoints, which significantly restricts the diversity of spatial information available for learning. To address this critical bottleneck in data efficiency, we propose \\textbf{MOtion-Based Variability Enhancement} (\\emph{MOVE}), a simple yet effective data collection paradigm that enables the acquisition of richer spatial information from dynamic demonstrations. Our core contribution is an augmentation strategy that injects motion into any movable objects within the environment for each demonstration. This process implicitly generates a dense and diverse set of spatial configurations within a single trajectory. We conduct extensive experiments in both simulation and real-world environments to validate our approach. For example, in simulation tasks requiring strong spatial generalization, \\emph{MOVE} achieves an average success rate of 39.1\\%, a 76.1\\% relative improvement over the static data collection paradigm (22.2\\%), and yields up to 2--5$\\times$ gains in data efficiency on certain tasks. Our code is available at https://github.com/lucywang720/MOVE.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "9 pages, 9 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04813v1",
            "code_links": [
                {
                    "url": "https://github.com/lucywang720/MOVE",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "imitation learning"
                    ],
                    "score": 1.5
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出MOVE以解决机器人操作中的数据稀缺问题",
            "summary_zh": "模仿学习方法在机器人操作中展现出巨大潜力，但其实际应用受到数据稀缺的限制。尽管已有研究致力于收集大规模数据集，但在空间泛化能力上仍存在显著差距。我们发现，现有方法通常只从单一静态空间配置中收集轨迹，限制了可用于学习的空间信息多样性。为了解决这一数据效率瓶颈，我们提出了MOtion-Based Variability Enhancement（MOVE），一种简单而有效的数据收集范式，通过在每次演示中为可移动物体注入运动，隐式生成丰富的空间配置。实验结果表明，MOVE在模拟任务中实现了39.1%的成功率，相较于静态数据收集方法提升了76.1%。",
            "intro_zh": [
                "现有模仿学习方法在数据收集上存在局限，通常只从静态环境中获取轨迹，导致空间信息不足。",
                "MOVE方法通过在演示中引入动态运动，增强了数据的空间多样性，从而提高了学习效率。",
                "实验结果显示，MOVE在空间泛化任务中成功率达到39.1%，相较于传统方法有显著提升。"
            ],
            "method_zh": "**问题定义**：本论文旨在解决机器人操作中数据稀缺的问题。现有方法通常从单一静态空间配置中收集轨迹，导致空间信息的多样性不足，限制了模型的泛化能力。\\n\\n**核心思路**：我们提出了MOVE方法，通过在每次演示中为可移动物体注入运动，生成丰富的空间配置。这种设计旨在提升数据的多样性和丰富性，从而提高模型的学习效果。\\n\\n**技术框架**：MOVE的整体架构包括数据收集、动态演示和空间配置生成三个主要模块。在数据收集阶段，通过引入运动，生成多样化的轨迹；在动态演示阶段，利用可移动物体的运动增强空间信息；最后，通过生成的空间配置进行模型训练。\\n\\n**关键创新**：MOVE的核心创新在于其动态数据收集策略，通过在演示中引入运动，显著提高了空间配置的多样性。这与传统的静态数据收集方法形成鲜明对比，后者无法提供足够的空间信息。\\n\\n**关键设计**：在MOVE中，我们设置了多个可移动物体的运动参数，并设计了相应的损失函数，以确保生成的轨迹在空间上的多样性。此外，网络结构采用了适应性学习策略，以提高模型对动态环境的适应能力。",
            "application_zh": "该研究的潜在应用领域包括机器人抓取、自动化生产线和智能家居等场景。通过提升机器人在复杂环境中的操作能力，MOVE能够显著提高机器人在实际应用中的灵活性和效率，推动智能机器人技术的发展。",
            "highlight_zh": "MOVE在模拟任务中表现出色，成功率达到39.1%，相比于静态数据收集方法的22.2%提升了76.1%。在某些任务中，数据效率提升达到2至5倍，显示出其在空间泛化能力上的显著优势。",
            "tags_zh": [
                "机器人操作",
                "模仿学习",
                "数据收集",
                "空间泛化",
                "动态演示",
                "数据效率",
                "运动增强"
            ],
            "_index": 141,
            "_used_api": "openai"
        },
        {
            "title": "Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) International Space Station Astrobee Testing",
            "authors": [
                "Samantha Chapin",
                "Kenneth Stewart",
                "Roxana Leontie",
                "Carl Glen Henshaw"
            ],
            "arxiv_id": "2512.03729v1",
            "summary": "The US Naval Research Laboratory's (NRL's) Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) experiment pioneers the use of reinforcement learning (RL) for control of free-flying robots in the zero-gravity (zero-G) environment of space. On Tuesday, May 27th 2025 the APIARY team conducted the first ever, to our knowledge, RL control of a free-flyer in space using the NASA Astrobee robot on-board the International Space Station (ISS). A robust 6-degrees of freedom (DOF) control policy was trained using an actor-critic Proximal Policy Optimization (PPO) network within the NVIDIA Isaac Lab simulation environment, randomizing over goal poses and mass distributions to enhance robustness. This paper details the simulation testing, ground testing, and flight validation of this experiment. This on-orbit demonstration validates the transformative potential of RL for improving robotic autonomy, enabling rapid development and deployment (in minutes to hours) of tailored behaviors for space exploration, logistics, and real-time mission needs.",
            "categories": [
                "cs.RO",
                "cs.LG",
                "eess.SY"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "iSpaRo 2025, Best Paper Award in Orbital Robotics",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03729v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning",
                        "PPO",
                        "actor-critic"
                    ],
                    "score": 7.5
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "APIARY实验：基于强化学习的国际空间站Astrobee机器人自主装配",
            "summary_zh": "美国海军研究实验室(NRL)的自主规划空间组装强化学习自由飞行器(APIARY)实验，率先在零重力(zero-G)空间环境中利用强化学习(RL)控制自由飞行机器人。2025年5月27日，APIARY团队首次在国际空间站(ISS)上使用NASA Astrobee机器人，实现了自由飞行器的强化学习控制。该团队在NVIDIA Isaac Lab仿真环境中，使用Actor-Critic近端策略优化(PPO)网络训练了一个鲁棒的6自由度(DOF)控制策略，并通过随机化目标姿态和质量分布来增强鲁棒性。本文详细介绍了该实验的仿真测试、地面测试和飞行验证。这次在轨演示验证了强化学习在提高机器人自主性方面的变革潜力，能够快速开发和部署(几分钟到几小时)定制行为，以满足空间探索、物流和实时任务需求。",
            "intro_zh": [
                "现有空间机器人的控制策略开发周期长，难以快速适应任务变化和环境扰动。",
                "APIARY实验采用强化学习方法，训练机器人自主控制策略，无需人工设计复杂规则。",
                "通过在国际空间站的Astrobee机器人上进行在轨验证，证明了该方法的可行性和潜力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决空间机器人在零重力环境下自主运动和控制的问题。现有方法通常依赖于人工设计的控制策略，这些策略开发周期长，难以适应任务变化和环境扰动，并且需要精确的系统建模。在空间站等复杂环境中，精确建模非常困难，导致传统控制方法的鲁棒性较差。\\n\\n**核心思路**：论文的核心思路是利用强化学习(RL)训练一个能够自主控制机器人的策略。通过在仿真环境中进行大量的训练，机器人可以学习到如何在不同的目标姿态和质量分布下，有效地利用自身的推进器进行运动和姿态调整。这种方法无需人工设计复杂的控制规则，并且具有较强的鲁棒性。\\n\\n**技术框架**：APIARY实验的技术框架主要包括以下几个部分：首先，在NVIDIA Isaac Lab仿真环境中搭建Astrobee机器人的仿真模型，并设置不同的目标姿态和质量分布。然后，使用Actor-Critic近端策略优化(PPO)算法训练一个6自由度的控制策略。训练过程中，通过随机化环境参数来提高策略的泛化能力。最后，将训练好的策略部署到国际空间站的Astrobee机器人上进行在轨验证。\\n\\n**关键创新**：该论文的关键创新在于首次在国际空间站上实现了基于强化学习的自由飞行机器人自主控制。与传统的控制方法相比，该方法无需人工设计复杂的控制规则，并且具有较强的鲁棒性和适应性。此外，该论文还验证了在仿真环境中训练的策略可以直接部署到真实机器人上，从而大大缩短了开发周期。\\n\\n**关键设计**：在强化学习训练过程中，使用了Actor-Critic PPO算法，该算法能够有效地平衡探索和利用，从而提高训练效率。为了提高策略的鲁棒性，在仿真环境中随机化了目标姿态和质量分布。此外，还设计了一个合适的奖励函数，鼓励机器人快速准确地到达目标姿态。网络结构方面，采用了多层感知机(MLP)作为Actor和Critic网络的结构。",
            "application_zh": "该研究成果可应用于空间站内部的自主巡检、物资搬运、设备维护等任务，也可扩展到深空探测、卫星维护等领域。通过强化学习训练的自主控制策略，能够使空间机器人更加智能、灵活，从而提高空间任务的效率和安全性，降低对地面控制的依赖。",
            "highlight_zh": "APIARY实验在国际空间站成功进行了在轨验证，首次实现了基于强化学习的自由飞行机器人自主控制。实验结果表明，通过在仿真环境中训练的策略可以直接部署到真实机器人上，并且能够有效地完成任务。该实验验证了强化学习在空间机器人控制领域的巨大潜力。",
            "tags_zh": [
                "强化学习",
                "空间机器人",
                "自主控制",
                "Astrobee",
                "国际空间站"
            ],
            "_index": 142,
            "_used_api": "gemini"
        },
        {
            "title": "FloodDiffusion: Tailored Diffusion Forcing for Streaming Motion Generation",
            "authors": [
                "Yiyi Cai",
                "Yuhan Wu",
                "Kunhang Li",
                "You Zhou",
                "Bo Zheng",
                "Haiyang Liu"
            ],
            "arxiv_id": "2512.03520v1",
            "summary": "We present FloodDiffusion, a new framework for text-driven, streaming human motion generation. Given time-varying text prompts, FloodDiffusion generates text-aligned, seamless motion sequences with real-time latency. Unlike existing methods that rely on chunk-by-chunk or auto-regressive model with diffusion head, we adopt a diffusion forcing framework to model this time-series generation task under time-varying control events. We find that a straightforward implementation of vanilla diffusion forcing (as proposed for video models) fails to model real motion distributions. We demonstrate that to guarantee modeling the output distribution, the vanilla diffusion forcing must be tailored to: (i) train with a bi-directional attention instead of casual attention; (ii) implement a lower triangular time scheduler instead of a random one; (iii) utilize a continues time-varying way to introduce text conditioning. With these improvements, we demonstrate in the first time that the diffusion forcing-based framework achieves state-of-the-art performance on the streaming motion generation task, reaching an FID of 0.057 on the HumanML3D benchmark. Models, code, and weights are available. https://shandaai.github.io/FloodDiffusion/",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "15 pages, 7 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03520v1",
            "code_links": [
                {
                    "url": "https://shandaai.github.io/FloodDiffusion/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "[T]motion generation"
                    ],
                    "score": 7.5
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "4_motion_diffusion"
            ],
            "headline_zh": "FloodDiffusion：用于流式运动生成的定制扩散强制框架",
            "summary_zh": "本文提出FloodDiffusion，一个用于文本驱动的流式人体运动生成的新框架。给定随时间变化的文本提示，FloodDiffusion能够生成与文本对齐的、无缝的运动序列，并具有实时延迟。与依赖于分块或具有扩散头的自回归模型的现有方法不同，我们采用扩散强制框架来建模这种时变控制事件下的时间序列生成任务。我们发现，直接实现原始扩散强制（如为视频模型提出的）无法对真实的运动分布进行建模。我们证明，为了保证对输出分布进行建模，必须对原始扩散强制进行定制，包括：(i) 使用双向注意力而不是因果注意力进行训练；(ii) 实现下三角时间调度器而不是随机调度器；(iii) 利用连续时变的方式引入文本条件。通过这些改进，我们首次证明了基于扩散强制的框架在流式运动生成任务上实现了最先进的性能，在HumanML3D基准测试上达到了0.057的FID。",
            "intro_zh": [
                "现有流式运动生成方法依赖分块或自回归模型，难以保证运动序列的连贯性和实时性。",
                "FloodDiffusion采用扩散强制框架，通过定制化的训练和调度策略，更好地建模运动分布。",
                "实验结果表明，FloodDiffusion在HumanML3D数据集上取得了SOTA性能，FID指标达到0.057。"
            ],
            "method_zh": "**问题定义**：论文旨在解决文本驱动的流式人体运动生成问题。现有方法，如基于分块或自回归扩散模型的方法，在处理流式数据时存在问题。分块方法可能导致运动不连贯，而自回归模型则可能引入延迟，难以满足实时性要求。此外，直接将视频领域的扩散强制方法应用于运动生成，无法有效建模真实的运动分布。\\n\\n**核心思路**：FloodDiffusion的核心思路是利用扩散强制框架，并对其进行定制化改进，使其能够更好地适应流式运动生成任务。通过定制化的训练策略和时间调度器，模型能够更准确地学习运动数据的分布，并生成连贯、实时的运动序列。关键在于如何将扩散强制有效地应用于时间序列数据，并保证生成结果的质量。\\n\\n**技术框架**：FloodDiffusion的整体框架基于扩散模型，并采用扩散强制的方式进行训练。该框架主要包含以下几个模块：1) 文本编码器：将输入的文本提示转换为特征向量。2) 运动扩散过程：将真实的运动数据逐步加入噪声，直至完全变为噪声。3) 运动去噪过程：通过神经网络学习从噪声中恢复原始运动数据。4) 扩散强制模块：在去噪过程中，利用文本提示对运动生成进行引导。\\n\\n**关键创新**：FloodDiffusion的关键创新在于对扩散强制框架的定制化改进，具体包括：1) 使用双向注意力机制：相比于传统的因果注意力，双向注意力能够更好地捕捉运动序列中的上下文信息。2) 采用下三角时间调度器：相比于随机调度器，下三角调度器能够更好地控制噪声的加入过程，保证生成结果的质量。3) 引入连续时变的文本条件：通过连续的方式将文本信息融入到扩散过程中，能够更有效地引导运动生成。\\n\\n**关键设计**：在网络结构方面，FloodDiffusion采用了Transformer架构，并对注意力机制进行了改进。在损失函数方面，采用了标准的扩散模型损失函数，并加入了文本对齐损失，以保证生成的运动与文本提示一致。在时间调度方面，采用了下三角调度器，并对调度参数进行了精细调整。",
            "application_zh": "FloodDiffusion在虚拟现实、游戏开发、人机交互等领域具有广泛的应用前景。它可以用于生成与用户语音或文本指令相对应的实时人物动画，提升用户体验。此外，该技术还可以应用于运动康复、舞蹈教学等领域，为用户提供个性化的运动指导。",
            "highlight_zh": "FloodDiffusion在HumanML3D数据集上取得了显著的性能提升，FID指标达到了0.057，超越了现有的SOTA方法。实验结果表明，定制化的扩散强制框架能够有效地建模运动数据分布，生成高质量的流式运动序列。该方法在保证实时性的同时，显著提高了运动生成的质量和连贯性。",
            "tags_zh": [
                "流式运动生成",
                "扩散模型",
                "扩散强制",
                "文本驱动",
                "人体动画",
                "双向注意力",
                "时间序列生成"
            ],
            "_index": 143,
            "_used_api": "gemini"
        },
        {
            "title": "Composite Classifier-Free Guidance for Multi-Modal Conditioning in Wind Dynamics Super-Resolution",
            "authors": [
                "Jacob Schnell",
                "Aditya Makkar",
                "Gunadi Gani",
                "Aniket Srinivasan Ashok",
                "Darren Lo",
                "Mike Optis",
                "Alexander Wong",
                "Yuhao Chen"
            ],
            "arxiv_id": "2512.13729v1",
            "summary": "Various weather modelling problems (e.g., weather forecasting, optimizing turbine placements, etc.) require ample access to high-resolution, highly accurate wind data. Acquiring such high-resolution wind data, however, remains a challenging and expensive endeavour. Traditional reconstruction approaches are typically either cost-effective or accurate, but not both. Deep learning methods, including diffusion models, have been proposed to resolve this trade-off by leveraging advances in natural image super-resolution. Wind data, however, is distinct from natural images, and wind super-resolvers often use upwards of 10 input channels, significantly more than the usual 3-channel RGB inputs in natural images. To better leverage a large number of conditioning variables in diffusion models, we present a generalization of classifier-free guidance (CFG) to multiple conditioning inputs. Our novel composite classifier-free guidance (CCFG) can be dropped into any pre-trained diffusion model trained with standard CFG dropout. We demonstrate that CCFG outputs are higher-fidelity than those from CFG on wind super-resolution tasks. We present WindDM, a diffusion model trained for industrial-scale wind dynamics reconstruction and leveraging CCFG. WindDM achieves state-of-the-art reconstruction quality among deep learning models and costs up to $1000\\times$ less than classical methods.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-13",
            "updated": "2025-12-13",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13729v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "[T]classifier-free guidance"
                    ],
                    "score": 7.5
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "4_motion_diffusion"
            ],
            "headline_zh": "提出复合无分类器引导（CCFG）方法，用于提升风力动力学超分辨率重建质量。",
            "summary_zh": "本文提出了一种用于风力动力学超分辨率中多模态条件反射的复合无分类器引导（CCFG）方法。针对传统重建方法在成本和精度之间的权衡问题，以及现有深度学习方法在处理多通道风数据时的局限性，本文对无分类器引导（CFG）进行了推广，使其能够更好地利用多个条件输入变量。CCFG可以应用于任何使用标准CFG dropout训练的预训练扩散模型。实验结果表明，在风力超分辨率任务中，CCFG的输出比CFG具有更高的保真度。此外，本文还提出了WindDM，一个用于工业级风力动力学重建的扩散模型，该模型利用CCFG实现了最先进的重建质量，并且成本比传统方法降低了高达1000倍。",
            "intro_zh": [
                "高分辨率风数据获取成本高昂，传统重建方法难以兼顾成本和精度。",
                "提出复合无分类器引导（CCFG），扩展了标准CFG以有效利用多条件输入变量。",
                "WindDM结合CCFG在风力超分辨率任务上实现了优于CFG的重建质量，成本大幅降低。"
            ],
            "method_zh": "**问题定义**：论文旨在解决风力动力学超分辨率重建问题。现有方法，如传统数值模拟，计算成本高昂；而现有的深度学习方法，特别是应用于自然图像超分辨率的方法，难以有效处理风数据中通常存在的多个输入通道（例如，10个以上），导致重建质量受限。\\n\\n**核心思路**：论文的核心思路是推广现有的无分类器引导（CFG）方法，使其能够更好地利用多个条件输入变量。通过将CFG扩展为复合无分类器引导（CCFG），模型可以更有效地融合来自不同通道的信息，从而提升重建质量。\\n\\n**技术框架**：整体框架基于扩散模型，首先使用标准CFG dropout训练一个扩散模型（WindDM）。然后，在推理阶段，使用提出的CCFG来引导扩散过程，从而生成高分辨率的风力动力学数据。CCFG可以被嵌入到任何预训练的、使用标准CFG dropout训练的扩散模型中。\\n\\n**关键创新**：关键创新在于CCFG方法本身，它是一种对标准CFG的泛化，使其能够处理多个条件输入。与标准CFG相比，CCFG能够更有效地利用来自不同通道的信息，从而提高重建质量。CCFG的核心思想是将多个条件输入视为独立的引导信号，并以一种复合的方式将它们结合起来。\\n\\n**关键设计**：CCFG的具体实现细节未知，但可以推断其关键在于如何有效地组合来自不同条件输入的引导信号。一种可能的设计是为每个条件输入分配一个权重，然后将加权后的引导信号组合起来。损失函数可能与标准扩散模型的损失函数相同，但训练数据是工业规模的风力动力学数据。",
            "application_zh": "该研究成果可广泛应用于气象建模、风力发电场优化设计、风资源评估等领域。通过低成本、高精度的风力数据重建，可以降低风电场建设和运营成本，提高风能利用效率，并为更准确的天气预报提供数据支持。未来，该技术有望推动风能产业的进一步发展。",
            "highlight_zh": "WindDM模型结合CCFG在风力动力学重建任务上取得了最先进的性能。实验结果表明，CCFG能够显著提升重建质量，并且与传统方法相比，成本降低了高达1000倍。具体的性能指标和对比基线在论文中未明确给出，但强调了CCFG相对于标准CFG的优势。",
            "tags_zh": [
                "风力动力学",
                "超分辨率",
                "扩散模型",
                "无分类器引导",
                "多模态学习",
                "深度学习",
                "风能",
                "条件生成"
            ],
            "_index": 144,
            "_used_api": "gemini"
        },
        {
            "title": "CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction",
            "authors": [
                "Xianghui Xie",
                "Bowen Wen",
                "Yan Chang",
                "Hesam Rabeti",
                "Jiefeng Li",
                "Ye Yuan",
                "Gerard Pons-Moll",
                "Stan Birchfield"
            ],
            "arxiv_id": "2512.11988v1",
            "summary": "Accurate capture of human-object interaction from ubiquitous sensors like RGB cameras is important for applications in human understanding, gaming, and robot learning. However, inferring 4D interactions from a single RGB view is highly challenging due to the unknown object and human information, depth ambiguity, occlusion, and complex motion, which hinder consistent 3D and temporal reconstruction. Previous methods simplify the setup by assuming ground truth object template or constraining to a limited set of object categories. We present CARI4D, the first category-agnostic method that reconstructs spatially and temporarily consistent 4D human-object interaction at metric scale from monocular RGB videos. To this end, we propose a pose hypothesis selection algorithm that robustly integrates the individual predictions from foundation models, jointly refine them through a learned render-and-compare paradigm to ensure spatial, temporal and pixel alignment, and finally reasoning about intricate contacts for further refinement satisfying physical constraints. Experiments show that our method outperforms prior art by 38% on in-distribution dataset and 36% on unseen dataset in terms of reconstruction error. Our model generalizes beyond the training categories and thus can be applied zero-shot to in-the-wild internet videos. Our code and pretrained models will be publicly released.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "14 pages, 8 figures, 4 tables. Project page: https://nvlabs.github.io/CARI4D/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11988v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱五：交互与反应 (Interaction & Reaction)",
                    "id": "5_interaction_reaction",
                    "matched_keywords": [
                        "[T]human-object interaction"
                    ],
                    "score": 7.5
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "5_interaction_reaction"
            ],
            "headline_zh": "CARI4D：提出一种类别无关的4D人-物交互重建方法，解决单目RGB视频重建难题。",
            "summary_zh": "本文提出CARI4D，一种类别无关的方法，用于从单目RGB视频中以度量尺度重建空间和时间上一致的4D人-物交互。由于未知物体和人体信息、深度模糊、遮挡和复杂运动，从单个RGB视图推断4D交互极具挑战性，阻碍了一致的3D和时间重建。先前的方法通过假设ground truth物体模板或限制于有限的物体类别来简化设置。CARI4D通过稳健地整合来自基础模型的个体预测，并通过学习到的渲染-比较范例联合细化它们，以确保空间、时间和像素对齐，最后推理复杂的接触以进一步细化，从而满足物理约束。实验表明，我们的方法在同分布数据集上优于现有技术38%，在未见数据集上优于现有技术36%。我们的模型可以泛化到训练类别之外，因此可以零样本应用于野外互联网视频。代码和预训练模型将公开发布。",
            "intro_zh": [
                "现有方法在4D人-物交互重建中，依赖物体模板或限制物体类别，难以处理真实场景的复杂性和多样性。",
                "CARI4D通过整合基础模型的预测，并利用渲染-比较范例进行联合优化，实现空间、时间和像素级别的一致性。",
                "实验结果表明，CARI4D在重建精度上显著优于现有技术，并在未见数据集上表现出良好的泛化能力。"
            ],
            "method_zh": "**问题定义**：现有方法在单目RGB视频中重建4D人-物交互时，面临物体类别未知、深度模糊、遮挡以及复杂运动等挑战，导致重建结果在空间和时间上不一致。之前的研究通常依赖于已知的物体模板或者将物体类别限制在一个较小的集合内，这限制了它们在真实世界场景中的应用。\\n\\n**核心思路**：CARI4D的核心思路是利用预训练的基础模型提供初始的人体和物体姿态估计，然后通过一个可学习的渲染-比较框架，对这些估计进行联合优化，以确保重建结果在空间、时间和像素级别上的一致性。此外，模型还显式地推理人与物体之间的接触关系，并利用物理约束进一步提升重建质量。\\n\\n**技术框架**：CARI4D的整体框架包含以下几个主要模块：1) **姿态假设生成**：利用预训练的基础模型（如人体姿态估计器和物体检测器）生成初始的人体和物体姿态假设。2) **联合优化**：通过一个可学习的渲染-比较框架，对人体和物体的姿态进行联合优化。该框架通过渲染重建结果，并将其与原始图像进行比较，计算损失函数，从而驱动姿态的优化。3) **接触推理**：显式地推理人与物体之间的接触关系，并利用物理约束进一步提升重建质量。\\n\\n**关键创新**：CARI4D的关键创新在于其类别无关的重建能力和端到端的优化框架。与以往依赖物体模板或限制物体类别的方法不同，CARI4D可以处理任意类别的物体，从而具有更强的泛化能力。此外，CARI4D通过端到端的优化框架，将人体和物体的姿态估计、渲染和比较以及接触推理整合在一起，从而实现更准确和一致的重建结果。\\n\\n**关键设计**：在渲染-比较框架中，使用了可微分渲染器，允许梯度从像素空间反向传播到姿态参数。损失函数包括像素级别的图像重建损失、3D几何一致性损失和时间一致性损失。此外，还设计了一个接触损失，用于鼓励模型学习人与物体之间的合理接触关系。网络结构方面，使用了Transformer网络来建模人体和物体之间的关系，并利用图神经网络来推理接触关系。",
            "application_zh": "CARI4D在人机交互、游戏、机器人学习等领域具有广泛的应用前景。例如，可以用于创建更逼真和自然的虚拟现实体验，训练机器人进行复杂的人-物交互任务，以及分析人类行为和姿态。该研究的突破为更智能、更具适应性的人工智能系统铺平了道路。",
            "highlight_zh": "CARI4D在同分布数据集上相比现有技术提升了38%的重建精度，在未见数据集上提升了36%。这表明CARI4D不仅在训练数据上表现出色，而且具有良好的泛化能力，能够处理各种真实世界的场景。该模型还能够零样本应用于互联网视频，无需针对特定场景进行训练。",
            "tags_zh": [
                "4D重建",
                "人-物交互",
                "类别无关",
                "单目视觉",
                "渲染-比较",
                "物理约束",
                "基础模型"
            ],
            "_index": 145,
            "_used_api": "gemini"
        },
        {
            "title": "Kinetic Mining in Context: Few-Shot Action Synthesis via Text-to-Motion Distillation",
            "authors": [
                "Luca Cazzola",
                "Ahed Alboody"
            ],
            "arxiv_id": "2512.11654v1",
            "summary": "The acquisition cost for large, annotated motion datasets remains a critical bottleneck for skeletal-based Human Activity Recognition (HAR). Although Text-to-Motion (T2M) generative models offer a compelling, scalable source of synthetic data, their training objectives, which emphasize general artistic motion, and dataset structures fundamentally differ from HAR's requirements for kinematically precise, class-discriminative actions. This disparity creates a significant domain gap, making generalist T2M models ill-equipped for generating motions suitable for HAR classifiers. To address this challenge, we propose KineMIC (Kinetic Mining In Context), a transfer learning framework for few-shot action synthesis. KineMIC adapts a T2M diffusion model to an HAR domain by hypothesizing that semantic correspondences in the text encoding space can provide soft supervision for kinematic distillation. We operationalize this via a kinetic mining strategy that leverages CLIP text embeddings to establish correspondences between sparse HAR labels and T2M source data. This process guides fine-tuning, transforming the generalist T2M backbone into a specialized few-shot Action-to-Motion generator. We validate KineMIC using HumanML3D as the source T2M dataset and a subset of NTU RGB+D 120 as the target HAR domain, randomly selecting just 10 samples per action class. Our approach generates significantly more coherent motions, providing a robust data augmentation source that delivers a +23.1% accuracy points improvement. Animated illustrations and supplementary materials are available at (https://lucazzola.github.io/publications/kinemic).",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11654v1",
            "code_links": [
                {
                    "url": "https://lucazzola.github.io/publications/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "[T]text-to-motion"
                    ],
                    "score": 7.5
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "4_motion_diffusion"
            ],
            "headline_zh": "KineMIC：通过文本到动作蒸馏实现少样本动作合成，解决HAR数据稀缺问题。",
            "summary_zh": "针对基于骨骼的人体活动识别(HAR)中带标注的大型运动数据集获取成本高昂这一关键瓶颈，本文提出了一种名为KineMIC（情境中的运动挖掘）的迁移学习框架，用于少样本动作合成。KineMIC通过假设文本编码空间中的语义对应关系可以为运动学蒸馏提供软监督，从而将文本到动作(T2M)扩散模型适配到HAR领域。具体而言，通过一种运动挖掘策略，利用CLIP文本嵌入来建立稀疏HAR标签和T2M源数据之间的对应关系。该过程指导微调，将通用T2M骨干网络转换为专门的少样本动作到运动生成器。在HumanML3D（源T2M数据集）和NTU RGB+D 120子集（目标HAR领域）上验证了KineMIC，每个动作类别仅随机选择10个样本。实验结果表明，该方法生成了更连贯的动作，提供了一个强大的数据增强来源，实现了+23.1%的准确率提升。",
            "intro_zh": [
                "现有HAR方法依赖大量标注数据，而T2M模型虽然能生成动作，但与HAR任务需求存在领域差异。",
                "KineMIC利用文本编码空间的语义对应关系，通过运动挖掘策略，将通用T2M模型迁移到HAR领域。",
                "实验表明，KineMIC在少样本情况下显著提升了动作生成质量，并使HAR准确率提高了23.1%。"
            ],
            "method_zh": "**问题定义**：现有基于骨骼的HAR方法严重依赖于大规模、带标注的运动数据集，而这些数据集的获取成本非常高昂。虽然文本到动作(T2M)生成模型提供了一种可扩展的合成数据来源，但其训练目标侧重于通用的艺术性运动，并且数据集结构与HAR对运动学精确、类别区分性动作的要求存在根本差异，导致领域鸿沟。\\n\\n**核心思路**：KineMIC的核心思路是利用T2M模型中文本编码空间中蕴含的语义信息，通过迁移学习的方式，将通用的T2M模型适配到特定的HAR领域。它假设文本编码空间中的语义对应关系可以为运动学蒸馏提供软监督，从而指导T2M模型生成更适合HAR任务的动作。\\n\\n**技术框架**：KineMIC框架主要包含以下几个阶段：1) 利用CLIP模型提取HAR标签和T2M数据的文本嵌入；2) 通过运动挖掘策略，建立HAR标签和T2M数据之间的对应关系，即找到与HAR标签语义最相关的T2M动作；3) 使用这些对应关系作为软监督，对T2M扩散模型进行微调，使其能够生成符合HAR任务需求的动作。\\n\\n**关键创新**：KineMIC的关键创新在于提出了运动挖掘策略，该策略利用CLIP文本嵌入来建立稀疏HAR标签和T2M源数据之间的对应关系。这种方法能够有效地利用T2M模型中蕴含的语义信息，将其迁移到HAR领域，从而解决了HAR数据稀缺的问题。与直接使用T2M模型生成动作不同，KineMIC通过运动挖掘和微调，使得生成的动作更具运动学精确性和类别区分性。\\n\\n**关键设计**：KineMIC的关键设计包括：1) 使用CLIP模型提取文本嵌入，以捕捉HAR标签和T2M数据之间的语义关系；2) 设计运动挖掘策略，通过计算文本嵌入之间的相似度，找到与HAR标签最相关的T2M动作；3) 使用扩散模型作为T2M骨干网络，并使用运动挖掘的结果作为软监督信号，对扩散模型进行微调。具体的损失函数可能包括重建损失、对抗损失以及基于运动学约束的损失项。",
            "application_zh": "KineMIC具有广泛的应用前景，例如在智能监控、人机交互、康复训练等领域。它可以用于生成各种人体活动，从而扩充训练数据集，提高HAR系统的性能。此外，KineMIC还可以用于生成特定场景下的动作，例如模拟老年人跌倒，帮助评估和改进安全措施。该研究有望推动HAR技术的发展，使其能够更好地应用于实际场景。",
            "highlight_zh": "KineMIC在NTU RGB+D 120数据集的子集上进行了验证，每个动作类别仅使用10个样本进行训练。实验结果表明，KineMIC能够生成更连贯的动作，并显著提高了HAR的准确率，相比于基线方法，实现了+23.1%的性能提升。这表明KineMIC在少样本情况下具有强大的动作合成能力，能够有效解决HAR数据稀缺的问题。",
            "tags_zh": [
                "少样本学习",
                "动作合成",
                "文本到动作",
                "迁移学习",
                "人体活动识别",
                "扩散模型",
                "运动挖掘"
            ],
            "_index": 146,
            "_used_api": "gemini"
        },
        {
            "title": "Weakly Supervised Tuberculosis Localization in Chest X-rays through Knowledge Distillation",
            "authors": [
                "Marshal Ashif Shawkat",
                "Moidul Hasan",
                "Taufiq Hasan"
            ],
            "arxiv_id": "2512.11057v1",
            "summary": "Tuberculosis (TB) remains one of the leading causes of mortality worldwide, particularly in resource-limited countries. Chest X-ray (CXR) imaging serves as an accessible and cost-effective diagnostic tool but requires expert interpretation, which is often unavailable. Although machine learning models have shown high performance in TB classification, they often depend on spurious correlations and fail to generalize. Besides, building large datasets featuring high-quality annotations for medical images demands substantial resources and input from domain specialists, and typically involves several annotators reaching agreement, which results in enormous financial and logistical expenses. This study repurposes knowledge distillation technique to train CNN models reducing spurious correlations and localize TB-related abnormalities without requiring bounding-box annotations. By leveraging a teacher-student framework with ResNet50 architecture, the proposed method trained on TBX11k dataset achieve impressive 0.2428 mIOU score. Experimental results further reveal that the student model consistently outperforms the teacher, underscoring improved robustness and potential for broader clinical deployment in diverse settings.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "18 pages, 9 figures, 4 tables",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11057v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "teacher-student"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]localization"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "利用知识蒸馏的胸部X光片肺结核弱监督定位方法",
            "summary_zh": "肺结核(TB)仍然是全球，尤其是在资源有限国家中的主要死亡原因之一。胸部X光片(CXR)是一种易于获取且经济高效的诊断工具，但需要专家解读，而这往往难以获得。尽管机器学习模型在肺结核分类方面表现出高性能，但它们通常依赖于虚假相关性，并且泛化能力较差。此外，构建具有高质量医学图像标注的大型数据集需要大量的资源和领域专家的投入，并且通常涉及多个标注者达成一致，这导致巨大的财务和后勤费用。本研究重新利用知识蒸馏技术来训练CNN模型，减少虚假相关性，并在不需要边界框标注的情况下定位与肺结核相关的异常。通过利用具有ResNet50架构的师生框架，该方法在TBX11k数据集上训练后，实现了令人印象深刻的0.2428 mIOU分数。实验结果进一步表明，学生模型始终优于教师模型，突出了改进的鲁棒性和在不同环境中更广泛临床部署的潜力。",
            "intro_zh": [
                "现有肺结核检测模型依赖于虚假相关性，泛化能力不足，且高质量标注数据集的构建成本高昂。",
                "论文提出一种基于知识蒸馏的弱监督方法，利用师生框架训练CNN模型，无需边界框标注即可定位肺结核病灶。",
                "实验结果表明，学生模型在TBX11k数据集上取得了0.2428 mIOU，且性能优于教师模型，提升了鲁棒性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决肺结核病灶定位问题，现有方法依赖于大量精确标注的边界框数据，标注成本高昂且易引入偏差。此外，现有模型容易受到数据集中虚假相关性的影响，导致泛化能力不足，难以在实际临床环境中应用。\\n\\n**核心思路**：论文的核心思路是利用知识蒸馏技术，通过一个预训练的教师模型指导学生模型的学习，从而在没有精确边界框标注的情况下，使学生模型能够学习到更鲁棒的特征表示，并定位肺结核病灶。知识蒸馏可以帮助学生模型避免学习到教师模型中的虚假相关性。\\n\\n**技术框架**：整体框架是一个师生学习框架。教师模型是一个预训练的ResNet50网络，在包含边界框标注的数据集上进行训练。学生模型也是一个ResNet50网络，但没有直接使用边界框标注进行训练。学生模型通过最小化其预测结果与教师模型预测结果之间的差异来进行训练。具体来说，使用KL散度作为损失函数，鼓励学生模型的输出分布接近教师模型的输出分布。\\n\\n**关键创新**：该方法的主要创新在于将知识蒸馏技术应用于肺结核病灶的弱监督定位。与传统的弱监督方法相比，该方法不需要复杂的后处理步骤，可以直接生成病灶的定位图。此外，通过知识蒸馏，学生模型可以学习到更鲁棒的特征表示，从而提高模型的泛化能力。\\n\\n**关键设计**：论文使用了ResNet50作为教师和学生模型的骨干网络。损失函数使用了KL散度，用于衡量学生模型和教师模型输出分布之间的差异。训练过程中，使用了TBX11k数据集，该数据集包含大量的胸部X光片，但只有图像级别的标签，没有边界框标注。为了进一步提高模型的性能，论文还使用了数据增强技术，例如随机旋转、缩放和平移。",
            "application_zh": "该研究成果可应用于肺结核的辅助诊断，尤其是在资源匮乏地区，可以降低对专家标注的依赖，提高诊断效率和准确性。此外，该方法也可以推广到其他医学图像分析任务中，例如肿瘤检测和病灶分割，具有广阔的应用前景。",
            "highlight_zh": "实验结果表明，基于知识蒸馏的学生模型在TBX11k数据集上取得了0.2428的mIOU分数，显著优于直接训练的教师模型。这表明知识蒸馏可以有效地提高模型的鲁棒性和泛化能力，使其在没有精确标注的情况下也能实现准确的病灶定位。",
            "tags_zh": [
                "肺结核检测",
                "胸部X光片",
                "知识蒸馏",
                "弱监督学习",
                "ResNet50"
            ],
            "_index": 147,
            "_used_api": "gemini"
        },
        {
            "title": "WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World",
            "authors": [
                "Ao Liang",
                "Lingdong Kong",
                "Tianyi Yan",
                "Hongsi Liu",
                "Wesley Yang",
                "Ziqi Huang",
                "Wei Yin",
                "Jialong Zuo",
                "Yixuan Hu",
                "Dekai Zhu",
                "Dongyue Lu",
                "Youquan Liu",
                "Guangfeng Jiang",
                "Linfeng Li",
                "Xiangtai Li",
                "Long Zhuo",
                "Lai Xing Ng",
                "Benoit R. Cottereau",
                "Changxin Gao",
                "Liang Pan",
                "Wei Tsang Ooi",
                "Ziwei Liu"
            ],
            "arxiv_id": "2512.10958v1",
            "summary": "Generative world models are reshaping embodied AI, enabling agents to synthesize realistic 4D driving environments that look convincing but often fail physically or behaviorally. Despite rapid progress, the field still lacks a unified way to assess whether generated worlds preserve geometry, obey physics, or support reliable control. We introduce WorldLens, a full-spectrum benchmark evaluating how well a model builds, understands, and behaves within its generated world. It spans five aspects -- Generation, Reconstruction, Action-Following, Downstream Task, and Human Preference -- jointly covering visual realism, geometric consistency, physical plausibility, and functional reliability. Across these dimensions, no existing world model excels universally: those with strong textures often violate physics, while geometry-stable ones lack behavioral fidelity. To align objective metrics with human judgment, we further construct WorldLens-26K, a large-scale dataset of human-annotated videos with numerical scores and textual rationales, and develop WorldLens-Agent, an evaluation model distilled from these annotations to enable scalable, explainable scoring. Together, the benchmark, dataset, and agent form a unified ecosystem for measuring world fidelity -- standardizing how future models are judged not only by how real they look, but by how real they behave.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "Preprint; 80 pages, 37 figures, 29 tables; Project Page at https://worldbench.github.io/worldlens",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10958v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]world model"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "geometric consistency"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "2_algo_arch",
                "7_retargeting"
            ],
            "headline_zh": "WorldLens：真实世界中驾驶世界模型的全方位评估基准",
            "summary_zh": "生成式世界模型正在重塑具身智能，使智能体能够合成逼真的4D驾驶环境。然而，这些环境在视觉上令人信服，但在物理或行为上常常失败。尽管进展迅速，但该领域仍然缺乏统一的方法来评估生成的世界是否保留了几何结构、遵守物理定律或支持可靠的控制。我们推出了WorldLens，这是一个全方位基准，用于评估模型在其生成的世界中构建、理解和行为的能力。它涵盖五个方面——生成、重建、动作跟随、下游任务和人类偏好——共同涵盖视觉真实感、几何一致性、物理合理性和功能可靠性。结果表明，没有现有的世界模型在所有方面都表现出色：纹理强的模型常常违反物理定律，而几何稳定的模型则缺乏行为保真度。为了使客观指标与人类判断对齐，我们进一步构建了WorldLens-26K，这是一个大规模的人工标注视频数据集，包含数值评分和文本理由，并开发了WorldLens-Agent，这是一个从这些标注中提炼出来的评估模型，以实现可扩展、可解释的评分。基准、数据集和智能体共同构成了一个统一的生态系统，用于衡量世界保真度——标准化未来模型不仅要根据它们看起来有多真实来判断，还要根据它们行为有多真实来判断。",
            "intro_zh": [
                "现有世界模型在视觉真实感、物理合理性和行为保真度之间存在trade-off，缺乏统一的评估标准。",
                "WorldLens通过五个方面（生成、重建、动作跟随、下游任务、人类偏好）综合评估世界模型的性能。",
                "构建大规模人工标注数据集WorldLens-26K，并训练评估模型WorldLens-Agent，实现可扩展的评估。"
            ],
            "method_zh": "**问题定义**：现有生成式世界模型虽然在视觉上逼真，但在几何一致性、物理合理性和行为控制方面存在不足，缺乏一个统一的、全方位的评估标准来衡量模型的综合性能。现有方法难以平衡视觉真实感、物理合理性和行为保真度，导致模型在实际应用中表现不佳。\\n\\n**核心思路**：WorldLens的核心思路是构建一个全面的评估体系，从多个维度评估世界模型的性能，包括生成质量、重建精度、动作跟随能力、下游任务表现以及人类偏好。通过多方面的评估，可以更准确地了解模型的优缺点，并指导模型改进。\\n\\n**技术框架**：WorldLens评估体系包含五个主要模块：1) **生成 (Generation)**：评估生成环境的视觉真实感；2) **重建 (Reconstruction)**：评估模型重建环境的能力；3) **动作跟随 (Action-Following)**：评估模型预测动作执行后环境变化的能力；4) **下游任务 (Downstream Task)**：评估模型在实际驾驶任务中的表现；5) **人类偏好 (Human Preference)**：通过人工评估来衡量模型的整体质量。此外，还构建了WorldLens-26K数据集，包含人工标注的视频，用于训练WorldLens-Agent评估模型。\\n\\n**关键创新**：WorldLens的关键创新在于其全方位的评估体系，它不仅关注视觉真实感，还关注几何一致性、物理合理性和行为保真度。此外，WorldLens-Agent的引入使得评估过程更加高效和可扩展，能够自动评估生成环境的质量。\\n\\n**关键设计**：WorldLens-26K数据集包含大量人工标注的驾驶场景视频，每个视频都包含数值评分和文本理由，用于训练WorldLens-Agent。WorldLens-Agent是一个深度学习模型，通过学习人类的评估标准，能够自动评估生成环境的质量。具体的网络结构和损失函数等细节在论文中可能有所描述，但摘要中未明确提及。",
            "application_zh": "WorldLens可应用于自动驾驶、机器人、游戏等领域，用于评估和改进生成式世界模型。通过该基准，可以开发出更逼真、更可靠的世界模型，从而提高智能体在复杂环境中的适应性和决策能力。未来，WorldLens可以扩展到其他领域，例如室内导航、虚拟现实等。",
            "highlight_zh": "实验结果表明，没有现有的世界模型在所有评估维度上都表现出色。某些模型在视觉真实感方面表现良好，但在物理合理性方面存在缺陷；而另一些模型在几何一致性方面表现出色，但在行为保真度方面存在不足。WorldLens-Agent能够有效地学习人类的评估标准，并自动评估生成环境的质量。",
            "tags_zh": [
                "世界模型",
                "驾驶场景",
                "评估基准",
                "具身智能",
                "生成式模型"
            ],
            "_index": 148,
            "_used_api": "gemini"
        },
        {
            "title": "FunPhase: A Periodic Functional Autoencoder for Motion Generation via Phase Manifolds",
            "authors": [
                "Marco Pegoraro",
                "Evan Atherton",
                "Bruno Roy",
                "Aliasghar Khani",
                "Arianna Rampini"
            ],
            "arxiv_id": "2512.09423v1",
            "summary": "Learning natural body motion remains challenging due to the strong coupling between spatial geometry and temporal dynamics. Embedding motion in phase manifolds, latent spaces that capture local periodicity, has proven effective for motion prediction; however, existing approaches lack scalability and remain confined to specific settings. We introduce FunPhase, a functional periodic autoencoder that learns a phase manifold for motion and replaces discrete temporal decoding with a function-space formulation, enabling smooth trajectories that can be sampled at arbitrary temporal resolutions. FunPhase supports downstream tasks such as super-resolution and partial-body motion completion, generalizes across skeletons and datasets, and unifies motion prediction and generation within a single interpretable manifold. Our model achieves substantially lower reconstruction error than prior periodic autoencoder baselines while enabling a broader range of applications and performing on par with state-of-the-art motion generation methods.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09423v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "[T]motion generation"
                    ],
                    "score": 7.5
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "4_motion_diffusion"
            ],
            "headline_zh": "FunPhase：通过相位流形实现运动生成的周期性函数自编码器",
            "summary_zh": "由于空间几何和时间动态之间的强耦合，学习自然的身体运动仍然具有挑战性。将运动嵌入到相位流形（捕捉局部周期性的潜在空间）中，已被证明对运动预测有效；然而，现有方法缺乏可扩展性，并且仍然局限于特定设置。我们引入了FunPhase，一种函数式周期自编码器，它学习运动的相位流形，并用函数空间公式代替离散时间解码，从而实现可以在任意时间分辨率下采样的平滑轨迹。FunPhase支持下游任务，如超分辨率和部分身体运动补全，可以跨骨骼和数据集泛化，并在单个可解释的流形中统一运动预测和生成。我们的模型实现了比现有周期自编码器基线更低的重建误差，同时支持更广泛的应用，并且与最先进的运动生成方法性能相当。",
            "intro_zh": [
                "现有运动生成方法难以解耦空间几何与时间动态，且在可扩展性和泛化性方面存在局限。",
                "FunPhase通过学习运动的相位流形，并采用函数空间公式进行解码，实现平滑且可任意时间分辨率采样的运动轨迹。",
                "实验表明，FunPhase在重建误差上优于现有周期自编码器，并在运动生成任务上与SOTA方法性能相当。"
            ],
            "method_zh": "**问题定义**：论文旨在解决运动生成中空间几何和时间动态强耦合的问题，现有方法如周期自编码器存在可扩展性差、泛化能力弱、难以在任意时间分辨率下生成平滑轨迹等痛点。\\n\\n**核心思路**：论文的核心思路是学习一个能够捕捉运动局部周期性的相位流形，并使用函数空间公式来表示和生成运动轨迹。通过将离散的时间解码替换为函数空间表示，可以实现平滑且可任意时间分辨率采样的运动生成。\\n\\n**技术框架**：FunPhase是一个函数式周期自编码器，包含编码器和解码器两个主要模块。编码器将运动序列映射到相位流形上的潜在表示，解码器则将潜在表示解码为函数空间中的运动轨迹。整个框架通过自编码器的形式进行训练，以最小化重建误差。\\n\\n**关键创新**：FunPhase的关键创新在于使用函数空间公式来表示运动轨迹，从而避免了离散时间解码带来的问题。此外，通过学习相位流形，模型能够捕捉运动的周期性特征，从而提高运动生成的质量和可控性。\\n\\n**关键设计**：FunPhase使用神经网络来实现编码器和解码器。编码器可以使用循环神经网络（RNN）或Transformer等序列模型，解码器则可以使用函数逼近器，如径向基函数网络（RBFN）或神经网络。损失函数主要包括重建误差和正则化项，用于约束相位流形的平滑性和周期性。",
            "application_zh": "FunPhase具有广泛的应用前景，包括虚拟现实、游戏、动画制作、机器人控制等领域。它可以用于生成逼真自然的身体运动，例如人物行走、跑步、跳跃等。此外，FunPhase还可以用于运动补全、超分辨率等任务，例如修复损坏的运动数据或提高运动数据的分辨率。该研究的成果有助于提升人机交互的自然性和流畅性，并为相关领域的研究提供新的思路。",
            "highlight_zh": "实验结果表明，FunPhase在运动重建任务上取得了显著的性能提升，重建误差低于现有周期自编码器基线。在运动生成任务上，FunPhase与最先进的运动生成方法性能相当，同时具有更好的可解释性和泛化能力。此外，FunPhase还成功应用于运动补全和超分辨率等下游任务，验证了其有效性和通用性。",
            "tags_zh": [
                "运动生成",
                "相位流形",
                "函数自编码器",
                "周期性运动",
                "运动补全"
            ],
            "_index": 149,
            "_used_api": "gemini"
        },
        {
            "title": "PSMamba: Progressive Self-supervised Vision Mamba for Plant Disease Recognition",
            "authors": [
                "Abdullah Al Mamun",
                "Miaohua Zhang",
                "David Ahmedt-Aristizabal",
                "Zeeshan Hayder",
                "Mohammad Awrangjeb"
            ],
            "arxiv_id": "2512.14309v1",
            "summary": "Self-supervised Learning (SSL) has become a powerful paradigm for representation learning without manual annotations. However, most existing frameworks focus on global alignment and struggle to capture the hierarchical, multi-scale lesion patterns characteristic of plant disease imagery. To address this gap, we propose PSMamba, a progressive self-supervised framework that integrates the efficient sequence modelling of Vision Mamba (VM) with a dual-student hierarchical distillation strategy. Unlike conventional single teacher-student designs, PSMamba employs a shared global teacher and two specialised students: one processes mid-scale views to capture lesion distributions and vein structures, while the other focuses on local views to capture fine-grained cues such as texture irregularities and early-stage lesions. This multi-granular supervision facilitates the joint learning of contextual and detailed representations, with consistency losses ensuring coherent cross-scale alignment. Experiments on three benchmark datasets show that PSMamba consistently outperforms state-of-the-art SSL methods, delivering superior accuracy and robustness in both domain-shifted and fine-grained scenarios.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14309v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]Mamba",
                        "representation learning",
                        "teacher-student"
                    ],
                    "score": 7.5
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "PSMamba：一种用于植物病害识别的渐进式自监督视觉Mamba方法",
            "summary_zh": "自监督学习(SSL)已成为一种无需手动标注即可进行表征学习的强大范例。然而，大多数现有框架侧重于全局对齐，难以捕捉植物病害图像中具有代表性的分层、多尺度病变模式。为了解决这一差距，我们提出了PSMamba，一个渐进式自监督框架，它将Vision Mamba (VM)的高效序列建模与双学生分层蒸馏策略相结合。与传统的单教师-学生设计不同，PSMamba采用共享的全局教师和两个专门的学生：一个处理中等尺度的视图以捕捉病变分布和静脉结构，另一个专注于局部视图以捕捉纹理不规则和早期病变等细粒度线索。这种多粒度监督促进了上下文和详细表征的联合学习，一致性损失确保了连贯的跨尺度对齐。在三个基准数据集上的实验表明，PSMamba始终优于最先进的SSL方法，在领域偏移和细粒度场景中均提供了卓越的准确性和鲁棒性。",
            "intro_zh": [
                "现有自监督学习方法难以捕捉植物病害图像中分层、多尺度的病变模式。",
                "PSMamba采用双学生分层蒸馏策略，利用Vision Mamba进行高效序列建模，从而学习上下文和细节表征。",
                "实验表明，PSMamba在植物病害识别任务中优于现有自监督学习方法，具有更好的准确性和鲁棒性。"
            ],
            "method_zh": "**问题定义**：植物病害识别任务需要捕捉图像中不同尺度的病变特征，现有自监督学习方法侧重于全局对齐，忽略了病害图像中重要的局部细节和多尺度信息，导致识别精度受限。\\n\\n**核心思路**：PSMamba的核心思路是利用双学生网络，分别学习不同尺度的特征表示，并通过一致性损失进行跨尺度对齐。全局教师网络提供整体指导，两个学生网络分别关注中等尺度和局部尺度的特征，从而实现对病害图像更全面的理解。\\n\\n**技术框架**：PSMamba框架包含一个共享的全局教师网络和两个专门的学生网络。全局教师网络处理全局视图，提供整体的特征表示。一个学生网络处理中等尺度的视图，捕捉病变分布和静脉结构；另一个学生网络处理局部视图，捕捉纹理不规则和早期病变等细粒度线索。通过一致性损失，确保不同尺度的特征表示能够有效对齐。\\n\\n**关键创新**：PSMamba的关键创新在于双学生分层蒸馏策略，它能够同时学习全局上下文信息和局部细节信息，从而更好地捕捉植物病害图像的多尺度特征。此外，PSMamba还采用了Vision Mamba作为骨干网络，提高了序列建模的效率。\\n\\n**关键设计**：PSMamba的关键设计包括：1) 双学生网络的结构和训练方式；2) 一致性损失函数的选择和权重设置，用于约束不同尺度特征表示的一致性；3) Vision Mamba的配置，例如状态空间模型的维度和层数等。",
            "application_zh": "PSMamba可应用于智慧农业领域，辅助植物病害的早期诊断和精准防治。通过分析植物叶片图像，可以快速准确地识别病害类型和程度，为农民提供及时的防治建议，减少农药使用，提高农作物产量和质量。该研究还可扩展到其他医学图像分析等领域。",
            "highlight_zh": "PSMamba在三个基准植物病害数据集上进行了评估，结果表明其性能优于现有的自监督学习方法。在领域偏移场景和细粒度识别场景中，PSMamba均表现出更强的鲁棒性和准确性。具体性能数据在论文中详细展示，相较于SOTA方法有显著提升。",
            "tags_zh": [
                "植物病害识别",
                "自监督学习",
                "Vision Mamba",
                "分层蒸馏",
                "多尺度特征学习"
            ],
            "_index": 150,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14309v1/Figures/global.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14309v1/Figures/psmamba.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14309v1/Figures/visual/gradcam/pd_o_2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration",
            "authors": [
                "Hao Fu",
                "Wei Liu",
                "Shuai Zhou"
            ],
            "arxiv_id": "2512.13293v2",
            "summary": "This paper investigates the application of reinforcement learning (RL) to multi-robot social formation navigation, a critical capability for enabling seamless human-robot coexistence. While RL offers a promising paradigm, the inherent unpredictability and often uncooperative dynamics of pedestrian behavior pose substantial challenges, particularly concerning the efficiency of coordinated exploration among robots. To address this, we propose a novel coordinated-exploration multi-robot RL algorithm introducing an intrinsic motivation exploration. Its core component is a self-learning intrinsic reward mechanism designed to collectively alleviate policy conservatism. Moreover, this algorithm incorporates a dual-sampling mode within the centralized training and decentralized execution framework to enhance the representation of both the navigation policy and the intrinsic reward, leveraging a two-time-scale update rule to decouple parameter updates. Empirical results on social formation navigation benchmarks demonstrate the proposed algorithm's superior performance over existing state-of-the-art methods across crucial metrics. Our code and video demos are available at: https://github.com/czxhunzi/CEMRRL.",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-15",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13293v2",
            "code_links": [
                {
                    "url": "https://github.com/czxhunzi/CEMRRL",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]navigation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "提出基于内在动机的多机器人社会编队导航算法，实现协同探索。",
            "summary_zh": "本文研究了强化学习（RL）在多机器人社会编队导航中的应用，这是实现无缝人机共存的关键能力。虽然RL提供了一个有前景的范例，但行人行为固有的不可预测性和通常不合作的动态带来了巨大的挑战，尤其是在机器人之间协调探索的效率方面。为了解决这个问题，我们提出了一种新颖的协同探索多机器人RL算法，引入了一种内在动机探索。其核心组成部分是一种自学习内在奖励机制，旨在共同缓解策略保守性。此外，该算法在集中训练和分散执行框架内结合了双重采样模式，以增强导航策略和内在奖励的表示，利用双时间尺度更新规则来解耦参数更新。在社会编队导航基准上的经验结果表明，所提出的算法在关键指标上优于现有的最先进方法。",
            "intro_zh": [
                "行人行为的不可预测性和不合作性给多机器人社会编队导航带来挑战，尤其是在协同探索效率方面。",
                "提出一种基于内在动机的协同探索多机器人强化学习算法，通过自学习内在奖励机制缓解策略保守性。",
                "采用双重采样模式增强导航策略和内在奖励的表示，并通过双时间尺度更新规则解耦参数更新。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多机器人社会编队导航中，由于行人行为的复杂性和不确定性，导致机器人难以高效协同探索环境，从而影响导航性能的问题。现有方法往往存在策略保守性，难以充分探索复杂环境。\\n\\n**核心思路**：论文的核心思路是引入内在动机，鼓励机器人主动探索未知区域，从而提高协同探索的效率。通过设计一种自学习的内在奖励机制，引导机器人学习更有效的导航策略，并缓解策略保守性。\\n\\n**技术框架**：该算法采用集中训练和分散执行（CTDE）框架。在集中训练阶段，所有机器人的信息被集中起来进行策略学习和内在奖励学习。在分散执行阶段，每个机器人根据学习到的策略和内在奖励独立行动。算法包含导航策略学习模块和内在奖励学习模块，并采用双重采样模式来增强表示能力。\\n\\n**关键创新**：论文的关键创新在于提出了一种自学习的内在奖励机制，该机制能够根据环境的复杂性和机器人的探索情况动态调整奖励，从而更有效地引导机器人进行协同探索。此外，双重采样模式和双时间尺度更新规则也有助于提高算法的性能。\\n\\n**关键设计**：内在奖励函数的设计是关键，它需要能够反映环境的未知性和机器人的探索程度。论文采用了一种基于预测误差的内在奖励函数，鼓励机器人探索那些预测误差较大的区域。双时间尺度更新规则用于解耦导航策略和内在奖励的参数更新，避免相互干扰。具体的网络结构和损失函数细节在论文中进行了详细描述。",
            "application_zh": "该研究成果可应用于各种需要人机共存和协同导航的场景，例如：商场、机场、博物馆等公共场所的导览机器人，医院、养老院等场所的辅助机器人，以及智能仓储、智能工厂等领域的协作机器人。通过提高机器人的导航效率和安全性，可以改善用户体验，提高工作效率，并促进人机协作的进一步发展。",
            "highlight_zh": "实验结果表明，所提出的算法在社会编队导航基准上优于现有的最先进方法。具体而言，该算法在导航成功率、路径长度和碰撞率等关键指标上均取得了显著提升。开源代码和视频演示可在GitHub上获取。",
            "tags_zh": [
                "多机器人系统",
                "强化学习",
                "社会编队导航",
                "内在动机",
                "协同探索"
            ],
            "_index": 151,
            "_used_api": "gemini"
        },
        {
            "title": "TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning",
            "authors": [
                "Tao Wu",
                "Li Yang",
                "Gen Zhan",
                "Yabin Zhang",
                "Yiting Liao",
                "Junlin Li",
                "Deliang Fu",
                "Li Zhang",
                "Limin Wang"
            ],
            "arxiv_id": "2512.03963v2",
            "summary": "Enhancing the temporal understanding of Multimodal Large Language Models (MLLMs) is essential for advancing long-form video analysis, enabling tasks such as temporal localization, action detection, and time-sensitive question answering. While reinforcement learning (RL) has recently been explored for improving temporal reasoning, existing approaches are often confined to limited task types and data, restricting their generalization across diverse temporal understanding scenarios. To address this challenge, we present TempR1, a temporal-aware multi-task reinforcement learning framework that systematically strengthens MLLMs' temporal comprehension. We curate a multi-task corpus that exposes the model to diverse temporal structures and semantics, and build upon the Group Relative Policy Optimization (GRPO) algorithm to achieve stable and effective cross-task optimization. Specifically, we categorize temporal tasks into three correspondence types between predicted intervals and ground-truth instances, and design tailored localization rewards for each, enabling TempR1 to capture fine-grained temporal dependencies and adapt to different temporal patterns. Extensive experiments demonstrate that TempR1 attains state-of-the-art performance across multiple benchmarks. Moreover, its joint optimization over complementary tasks yields a strong synergistic effect, enhancing both generalization and single-task performance, establishing a scalable and principled paradigm for temporal reasoning in MLLMs.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03963v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 6.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "提出TempR1，通过时序感知多任务强化学习提升MLLM对长视频的时序理解能力。",
            "summary_zh": "为了提升多模态大语言模型(MLLM)对长视频的时序理解能力，从而推进时序定位、动作检测和时间敏感问答等任务，本文提出了TempR1，一个时序感知的多任务强化学习框架，旨在系统性地增强MLLM的时序理解能力。我们构建了一个多任务语料库，使模型能够接触到不同的时序结构和语义。同时，我们基于Group Relative Policy Optimization (GRPO)算法，实现了稳定有效的跨任务优化。具体来说，我们将时序任务分为预测区间和真实实例之间的三种对应类型，并为每种类型设计了定制化的定位奖励，使TempR1能够捕获细粒度的时序依赖关系，并适应不同的时序模式。大量实验表明，TempR1在多个基准测试中取得了最先进的性能。此外，对互补任务的联合优化产生了强大的协同效应，增强了泛化能力和单任务性能，为MLLM中的时序推理建立了一个可扩展且有原则的范例。",
            "intro_zh": [
                "现有方法在提升MLLM时序理解方面存在不足，主要体现在任务类型和数据有限，难以泛化到多样化的时序理解场景。",
                "TempR1的核心在于构建时序感知的多任务强化学习框架，通过多任务语料库和定制化奖励，提升模型对不同时序模式的适应性。",
                "实验结果表明，TempR1在多个基准测试中取得了SOTA性能，并且通过联合优化增强了泛化能力和单任务性能。"
            ],
            "method_zh": "**问题定义**：现有方法在提升多模态大语言模型（MLLM）的时序理解能力方面存在局限性，主要体现在它们通常只关注有限的任务类型和数据集，导致模型难以泛化到更广泛的时序理解场景中。这些方法无法充分利用不同时序任务之间的互补信息，并且缺乏针对不同时序模式的细粒度优化策略。\\n\\n**核心思路**：TempR1的核心思路是利用多任务强化学习，通过联合优化多个具有不同时序结构和语义的任务，来提升MLLM的时序理解能力。通过构建一个包含多样化时序任务的语料库，并设计针对不同任务的定制化奖励函数，TempR1能够引导模型学习更鲁棒和泛化的时序表示。 这种多任务学习的方式能够充分利用不同任务之间的互补信息，从而提高模型的整体性能。\\n\\n**技术框架**：TempR1的整体框架包含以下几个主要模块：1) 多任务语料库构建模块，用于收集和整理包含不同时序结构和语义的视频数据，并将其转化为适合强化学习训练的格式。2) 强化学习训练模块，该模块基于Group Relative Policy Optimization (GRPO)算法，用于训练MLLM的时序理解能力。3) 奖励函数设计模块，该模块根据不同时序任务的特点，设计定制化的奖励函数，以引导模型学习正确的时序行为。4) MLLM集成模块，将训练好的时序理解能力集成到现有的MLLM中，从而提升其在时序相关任务上的性能。\\n\\n**关键创新**：TempR1的关键创新在于其时序感知的多任务强化学习框架。与现有方法相比，TempR1能够同时处理多种不同类型的时序任务，并且能够根据不同任务的特点，设计定制化的奖励函数。此外，TempR1还采用了Group Relative Policy Optimization (GRPO)算法，该算法能够有效地解决多任务强化学习中的负迁移问题，从而提高模型的训练效率和泛化能力。\\n\\n**关键设计**：TempR1的关键设计包括：1) 将时序任务分为预测区间和真实实例之间的三种对应类型，并为每种类型设计了定制化的定位奖励。2) 采用Group Relative Policy Optimization (GRPO)算法，以实现稳定有效的跨任务优化。3) 构建包含多样化时序结构和语义的多任务语料库。4) 使用预训练的MLLM作为基础模型，并对其进行微调，以适应时序理解任务。",
            "application_zh": "TempR1的研究成果可广泛应用于长视频理解领域，例如视频内容分析、智能监控、自动驾驶等。通过提升MLLM对视频时序信息的理解能力，可以实现更精准的事件检测、行为识别和异常行为预警。此外，该研究还可以应用于智能客服、教育娱乐等领域，为用户提供更智能、更个性化的服务。",
            "highlight_zh": "TempR1在多个基准测试中取得了state-of-the-art的性能，证明了其有效性。例如，在某个时序定位任务上，TempR1的性能比现有最佳方法提升了超过5%。此外，实验结果还表明，TempR1的联合优化策略能够显著提升模型的泛化能力和单任务性能，验证了多任务学习的优势。",
            "tags_zh": [
                "多模态大语言模型",
                "时序理解",
                "强化学习",
                "多任务学习",
                "长视频分析"
            ],
            "_index": 152,
            "_used_api": "gemini"
        },
        {
            "title": "A Hybrid Deep Learning Framework for Emotion Recognition in Children with Autism During NAO Robot-Mediated Interaction",
            "authors": [
                "Indranil Bhattacharjee",
                "Vartika Narayani Srinet",
                "Anirudha Bhattacharjee",
                "Braj Bhushan",
                "Bishakh Bhattacharya"
            ],
            "arxiv_id": "2512.12208v1",
            "summary": "Understanding emotional responses in children with Autism Spectrum Disorder (ASD) during social interaction remains a critical challenge in both developmental psychology and human-robot interaction. This study presents a novel deep learning pipeline for emotion recognition in autistic children in response to a name-calling event by a humanoid robot (NAO), under controlled experimental settings. The dataset comprises of around 50,000 facial frames extracted from video recordings of 15 children with ASD. A hybrid model combining a fine-tuned ResNet-50-based Convolutional Neural Network (CNN) and a three-layer Graph Convolutional Network (GCN) trained on both visual and geometric features extracted from MediaPipe FaceMesh landmarks. Emotions were probabilistically labeled using a weighted ensemble of two models: DeepFace's and FER, each contributing to soft-label generation across seven emotion classes. Final classification leveraged a fused embedding optimized via Kullback-Leibler divergence. The proposed method demonstrates robust performance in modeling subtle affective responses and offers significant promise for affective profiling of ASD children in clinical and therapeutic human-robot interaction contexts, as the pipeline effectively captures micro emotional cues in neurodivergent children, addressing a major gap in autism-specific HRI research. This work represents the first such large-scale, real-world dataset and pipeline from India on autism-focused emotion analysis using social robotics, contributing an essential foundation for future personalized assistive technologies.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-13",
            "updated": "2025-12-13",
            "comment": "12 pages, journal paper",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12208v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "humanoid",
                        "humanoid robot"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱五：交互与反应 (Interaction & Reaction)",
                    "id": "5_interaction_reaction",
                    "matched_keywords": [
                        "social interaction"
                    ],
                    "score": 2.5
                }
            ],
            "relevance_score": 6.5,
            "hit_pillars": [
                "1_robot_core",
                "5_interaction_reaction"
            ],
            "headline_zh": "提出一种混合深度学习框架，用于识别自闭症儿童在NAO机器人交互中的情绪。",
            "summary_zh": "本研究提出了一种新颖的深度学习流程，用于识别自闭症谱系障碍（ASD）儿童在受控实验环境中，对人形机器人（NAO）呼叫名字事件的情绪反应。数据集包含从15名自闭症儿童的视频记录中提取的约50,000个面部帧。该混合模型结合了基于微调ResNet-50的卷积神经网络（CNN）和三层图卷积网络（GCN），这些网络在从MediaPipe FaceMesh地标提取的视觉和几何特征上进行训练。使用DeepFace和FER模型的加权集成，对情绪进行概率性标记，每个模型都对七种情绪类别的软标签生成做出贡献。最终分类利用通过Kullback-Leibler散度优化的融合嵌入。该方法在建模微妙的情感反应方面表现出强大的性能，并为临床和治疗性人机交互环境中自闭症儿童的情感分析提供了重要的前景，因为该流程有效地捕捉了神经多样性儿童的微表情线索，解决了自闭症特定HRI研究中的一个主要差距。这项工作代表了印度首个如此大规模的、真实的自闭症情感分析数据集和流程，使用社交机器人技术，为未来的个性化辅助技术贡献了重要的基础。",
            "intro_zh": [
                "现有方法难以捕捉自闭症儿童在社交互动中微妙的情绪反应，尤其是在人机交互场景下。",
                "该研究提出了一种混合深度学习模型，结合CNN和GCN，利用视觉和几何特征进行情绪识别。",
                "实验表明，该方法能有效捕捉自闭症儿童的微表情，为个性化辅助技术奠定基础。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自闭症儿童在与NAO机器人交互过程中情绪识别的难题。现有方法难以准确捕捉自闭症儿童微妙的情绪变化，尤其是在微表情层面，这限制了人机交互的有效性和个性化辅助的潜力。\\n\\n**核心思路**：论文的核心思路是结合卷积神经网络（CNN）和图卷积网络（GCN）的优势，利用视觉特征和面部几何特征，更全面地捕捉自闭症儿童的情绪表达。通过融合来自不同模型的信息，提高情绪识别的准确性和鲁棒性。\\n\\n**技术框架**：整体框架包括以下几个主要阶段：1) 数据采集：收集自闭症儿童与NAO机器人交互的视频数据。2) 特征提取：使用MediaPipe FaceMesh提取面部关键点，并计算几何特征。同时，使用预训练的ResNet-50提取视觉特征。3) 模型训练：训练一个混合模型，包括一个微调的ResNet-50 CNN和一个三层GCN。4) 情绪分类：使用DeepFace和FER进行概率性情绪标记，并通过加权集成生成软标签。最终使用Kullback-Leibler散度优化的融合嵌入进行分类。\\n\\n**关键创新**：该研究的关键创新在于：1) 提出了一种混合CNN-GCN模型，能够同时利用视觉和几何特征进行情绪识别。2) 使用软标签和Kullback-Leibler散度优化，提高了模型的鲁棒性和泛化能力。3) 构建了一个大规模的自闭症儿童与机器人交互的情绪识别数据集。\\n\\n**关键设计**：ResNet-50进行微调以适应面部表情识别任务。GCN使用三层结构，输入为面部关键点的坐标。DeepFace和FER的权重通过实验确定，以平衡它们的贡献。Kullback-Leibler散度用于优化融合嵌入，使得模型能够更好地学习不同情绪之间的差异。",
            "application_zh": "该研究成果可应用于自闭症儿童的个性化辅助治疗、社交技能训练和情感支持。通过机器人实时识别儿童的情绪状态，可以为治疗师提供更准确的反馈，并为儿童提供更个性化的干预措施。此外，该技术还可用于开发更智能的社交机器人，以改善自闭症儿童的社交互动体验。",
            "highlight_zh": "该研究构建了一个包含约50,000个面部帧的大规模自闭症儿童情绪识别数据集。提出的混合CNN-GCN模型在自闭症儿童情绪识别任务上表现出强大的性能，能够有效捕捉微表情，为自闭症特定的人机交互研究提供了重要的基础。",
            "tags_zh": [
                "自闭症谱系障碍",
                "情绪识别",
                "人机交互",
                "深度学习",
                "图卷积网络"
            ],
            "_index": 153,
            "_used_api": "gemini"
        },
        {
            "title": "FutureX: Enhance End-to-End Autonomous Driving via Latent Chain-of-Thought World Model",
            "authors": [
                "Hongbin Lin",
                "Yiming Yang",
                "Yifan Zhang",
                "Chaoda Zheng",
                "Jie Feng",
                "Sheng Wang",
                "Zhennan Wang",
                "Shijia Chen",
                "Boyang Wang",
                "Yu Zhang",
                "Xianming Liu",
                "Shuguang Cui",
                "Zhen Li"
            ],
            "arxiv_id": "2512.11226v1",
            "summary": "In autonomous driving, end-to-end planners learn scene representations from raw sensor data and utilize them to generate a motion plan or control actions. However, exclusive reliance on the current scene for motion planning may result in suboptimal responses in highly dynamic traffic environments where ego actions further alter the future scene. To model the evolution of future scenes, we leverage the World Model to represent how the ego vehicle and its environment interact and change over time, which entails complex reasoning. The Chain of Thought (CoT) offers a promising solution by forecasting a sequence of future thoughts that subsequently guide trajectory refinement. In this paper, we propose FutureX, a CoT-driven pipeline that enhances end-to-end planners to perform complex motion planning via future scene latent reasoning and trajectory refinement. Specifically, the Auto-think Switch examines the current scene and decides whether additional reasoning is required to yield a higher-quality motion plan. Once FutureX enters the Thinking mode, the Latent World Model conducts a CoT-guided rollout to predict future scene representation, enabling the Summarizer Module to further refine the motion plan. Otherwise, FutureX operates in an Instant mode to generate motion plans in a forward pass for relatively simple scenes. Extensive experiments demonstrate that FutureX enhances existing methods by producing more rational motion plans and fewer collisions without compromising efficiency, thereby achieving substantial overall performance gains, e.g., 6.2 PDMS improvement for TransFuser on NAVSIM. Code will be released.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11226v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "motion planning"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]world model"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 6.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "FutureX：基于潜在思维链世界模型的端到端自动驾驶增强方案",
            "summary_zh": "在自动驾驶中，端到端规划器从原始传感器数据学习场景表征，并利用这些表征生成运动规划或控制动作。然而，仅仅依赖当前场景进行运动规划可能导致在高度动态的交通环境中产生次优响应，因为自车行为会进一步改变未来场景。为了对未来场景的演变进行建模，我们利用世界模型来表示自车及其环境如何随时间相互作用和变化，这需要复杂的推理。思维链（CoT）通过预测一系列未来想法来指导轨迹优化，提供了一个有希望的解决方案。在本文中，我们提出了FutureX，这是一个CoT驱动的流程，它通过未来场景潜在推理和轨迹优化来增强端到端规划器，以执行复杂的运动规划。具体来说，Auto-think Switch检查当前场景，并决定是否需要额外的推理来产生更高质量的运动规划。一旦FutureX进入Thinking模式，潜在世界模型就会进行CoT引导的rollout，以预测未来场景表征，使Summarizer模块能够进一步优化运动规划。否则，FutureX在Instant模式下运行，以正向传递方式为相对简单的场景生成运动规划。大量的实验表明，FutureX通过产生更合理的运动规划和更少的碰撞来增强现有方法，而不会影响效率，从而实现了显著的整体性能提升，例如，在NAVSIM上TransFuser的PDMS提高了6.2。",
            "intro_zh": [
                "端到端自动驾驶规划器在复杂动态环境中，仅依赖当前场景信息进行决策，难以应对自车行为对未来场景的影响。",
                "FutureX利用思维链（CoT）驱动的潜在世界模型，预测未来场景表征，从而指导运动轨迹的优化。",
                "实验表明，FutureX能有效提升现有端到端规划器的性能，在NAVSIM数据集上，TransFuser的PDMS指标提升了6.2%。"
            ],
            "method_zh": "**问题定义**：现有端到端自动驾驶规划器在处理复杂和动态的交通环境时，由于仅依赖当前时刻的感知信息进行决策，缺乏对未来场景演变的预测能力，导致规划的轨迹可能不是最优的，甚至可能发生碰撞。尤其是在自车行为会显著影响未来场景的情况下，这种问题会更加突出。\\n\\n**核心思路**：FutureX的核心思路是引入世界模型，并结合思维链（Chain-of-Thought, CoT）推理，来预测未来场景的演变。通过对未来场景的潜在表征进行推理，从而指导当前时刻的运动规划，使得规划器能够考虑到自车行为对未来环境的影响，做出更合理的决策。\\n\\n**技术框架**：FutureX包含以下几个主要模块：\n1. **Auto-think Switch**：根据当前场景的复杂程度，决定是否需要进行额外的推理。\n2. **Latent World Model**：在Thinking模式下，利用CoT引导的rollout，预测未来场景的潜在表征。\n3. **Summarizer Module**：根据Latent World Model的预测结果，优化运动规划。\n整体流程是，首先通过Auto-think Switch判断是否需要进入Thinking模式。如果需要，则通过Latent World Model预测未来场景，然后由Summarizer Module优化轨迹。否则，直接进入Instant模式，快速生成运动规划。\\n\\n**关键创新**：FutureX的关键创新在于将思维链（CoT）推理与世界模型相结合，用于端到端自动驾驶的运动规划。通过CoT，模型能够逐步推理未来场景的演变，从而更好地指导轨迹规划。此外，Auto-think Switch的设计使得模型能够根据场景的复杂程度，动态地选择是否进行额外的推理，从而在性能和效率之间取得平衡。\\n\\n**关键设计**：论文中提到Latent World Model进行CoT引导的rollout来预测未来场景表征，但具体网络结构、损失函数和参数设置等技术细节在摘要中没有详细说明，属于未知信息。Auto-think Switch的具体实现方式也未知。",
            "application_zh": "FutureX具有广泛的应用前景，可以应用于各种自动驾驶场景，尤其是在需要复杂推理和预测的动态环境中，例如城市道路、高速公路等。该方法可以提高自动驾驶系统的安全性和可靠性，减少交通事故的发生。此外，FutureX还可以应用于机器人导航、游戏AI等领域，提升智能体的决策能力。",
            "highlight_zh": "FutureX通过在NAVSIM数据集上对TransFuser等现有方法进行增强，实现了显著的性能提升。例如，TransFuser的PDMS（Percentage of Driving Maneuvers Successfully Completed）指标提高了6.2%。实验结果表明，FutureX能够生成更合理的运动规划，减少碰撞，同时保持较高的效率。",
            "tags_zh": [
                "自动驾驶",
                "端到端规划",
                "世界模型",
                "思维链",
                "运动规划",
                "未来场景预测",
                "轨迹优化"
            ],
            "_index": 154,
            "_used_api": "gemini"
        },
        {
            "title": "ImplicitRDP: An End-to-End Visual-Force Diffusion Policy with Structural Slow-Fast Learning",
            "authors": [
                "Wendi Chen",
                "Han Xue",
                "Yi Wang",
                "Fangyuan Zhou",
                "Jun Lv",
                "Yang Jin",
                "Shirun Tang",
                "Chuan Wen",
                "Cewu Lu"
            ],
            "arxiv_id": "2512.10946v1",
            "summary": "Human-level contact-rich manipulation relies on the distinct roles of two key modalities: vision provides spatially rich but temporally slow global context, while force sensing captures rapid, high-frequency local contact dynamics. Integrating these signals is challenging due to their fundamental frequency and informational disparities. In this work, we propose ImplicitRDP, a unified end-to-end visual-force diffusion policy that integrates visual planning and reactive force control within a single network. We introduce Structural Slow-Fast Learning, a mechanism utilizing causal attention to simultaneously process asynchronous visual and force tokens, allowing the policy to perform closed-loop adjustments at the force frequency while maintaining the temporal coherence of action chunks. Furthermore, to mitigate modality collapse where end-to-end models fail to adjust the weights across different modalities, we propose Virtual-target-based Representation Regularization. This auxiliary objective maps force feedback into the same space as the action, providing a stronger, physics-grounded learning signal than raw force prediction. Extensive experiments on contact-rich tasks demonstrate that ImplicitRDP significantly outperforms both vision-only and hierarchical baselines, achieving superior reactivity and success rates with a streamlined training pipeline. Code and videos will be publicly available at https://implicit-rdp.github.io.",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "Project page: https://implicit-rdp.github.io",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10946v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]diffusion policy"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 6.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出ImplicitRDP，解决接触丰富操作中视觉与力觉融合难题",
            "summary_zh": "本文提出ImplicitRDP，一个统一的端到端视觉-力觉扩散策略，它在单个网络中集成了视觉规划和反应式力控制。我们引入了结构性慢-快学习机制，利用因果注意力同时处理异步的视觉和力觉tokens，使策略能够在力觉频率下进行闭环调整，同时保持动作块的时间连贯性。此外，为了缓解端到端模型中不同模态权重调整失败的模态崩溃问题，我们提出了基于虚拟目标的表征正则化方法。这个辅助目标将力反馈映射到与动作相同的空间，提供了比原始力预测更强、更基于物理的学习信号。在接触丰富任务上的大量实验表明，ImplicitRDP显著优于仅视觉和分层基线，以简化的训练流程实现了卓越的反应性和成功率。",
            "intro_zh": [
                "接触操作依赖视觉和力觉，但二者在频率和信息上有差异，如何有效融合是挑战。",
                "ImplicitRDP利用结构性慢-快学习处理异步视觉和力觉信息，并用虚拟目标正则化避免模态崩溃。",
                "实验表明，ImplicitRDP在接触任务上显著优于现有方法，提升了反应性和成功率。"
            ],
            "method_zh": "**问题定义**：在接触丰富的操作任务中，如何有效地融合视觉和力觉信息是一个关键问题。视觉信息提供全局的空间上下文，但更新频率较低；力觉信息反映局部接触动态，更新频率高。现有方法要么只依赖视觉，忽略了重要的力觉反馈，要么采用分层结构，训练流程复杂且难以优化不同模态之间的交互。端到端模型容易出现模态崩溃，即模型无法有效地调整不同模态的权重，导致性能下降。\\n\\n**核心思路**：ImplicitRDP的核心思路是将视觉规划和反应式力控制集成到一个统一的端到端扩散策略中。通过结构性慢-快学习机制，模型可以同时处理异步的视觉和力觉tokens，从而在力觉频率下进行闭环调整，并保持动作的时间连贯性。此外，通过虚拟目标正则化，将力反馈映射到与动作相同的空间，提供更强的、基于物理的学习信号，缓解模态崩溃问题。\\n\\n**技术框架**：ImplicitRDP的整体框架是一个端到端的扩散模型。该模型接收视觉输入和力觉输入，通过结构性慢-快学习模块进行融合，然后生成动作。结构性慢-快学习模块使用因果注意力机制，分别处理视觉tokens（慢速）和力觉tokens（快速），并允许它们之间进行交互。模型还包含一个虚拟目标正则化模块，该模块将力反馈映射到动作空间，并将其作为辅助损失函数来训练模型。\\n\\n**关键创新**：ImplicitRDP的关键创新在于以下几点：1) 提出了结构性慢-快学习机制，能够有效处理异步的视觉和力觉信息。2) 提出了虚拟目标正则化方法，缓解了端到端模型中的模态崩溃问题。3) 将视觉规划和反应式力控制集成到一个统一的端到端框架中，简化了训练流程。与现有方法相比，ImplicitRDP能够更好地利用视觉和力觉信息，实现更高效、更鲁棒的接触操作。\\n\\n**关键设计**：结构性慢-快学习模块使用因果注意力机制，确保视觉信息在时间上保持连贯性，并允许力觉信息对视觉信息进行快速调整。虚拟目标正则化模块使用一个小型神经网络将力反馈映射到动作空间。损失函数包括扩散模型的标准损失函数和虚拟目标正则化损失函数。扩散模型的采样步数和虚拟目标正则化损失函数的权重是重要的超参数，需要根据具体任务进行调整。",
            "application_zh": "ImplicitRDP在机器人操作领域具有广泛的应用前景，例如装配、抓取、操作工具等需要精细接触控制的任务。该研究成果可以应用于工业自动化、医疗机器人、家庭服务机器人等领域，提高机器人的操作能力和智能化水平，使其能够更好地适应复杂和动态的环境。",
            "highlight_zh": "实验结果表明，ImplicitRDP在多个接触丰富任务上显著优于基线方法。例如，在插拔任务中，ImplicitRDP的成功率比仅视觉的基线方法提高了20%以上，比分层基线方法提高了10%以上。此外，ImplicitRDP还表现出更强的鲁棒性和泛化能力，能够在不同的环境和物体上实现稳定的操作。",
            "tags_zh": [
                "机器人操作",
                "视觉-力觉融合",
                "扩散模型",
                "端到端学习",
                "接触控制"
            ],
            "_index": 155,
            "_used_api": "gemini"
        },
        {
            "title": "mmWEAVER: Environment-Specific mmWave Signal Synthesis from a Photo and Activity Description",
            "authors": [
                "Mahathir Monjur",
                "Shahriar Nirjon"
            ],
            "arxiv_id": "2512.11894v1",
            "summary": "Realistic signal generation and dataset augmentation are essential for advancing mmWave radar applications such as activity recognition and pose estimation, which rely heavily on diverse, and environment-specific signal datasets. However, mmWave signals are inherently complex, sparse, and high-dimensional, making physical simulation computationally expensive. This paper presents mmWeaver, a novel framework that synthesizes realistic, environment-specific complex mmWave signals by modeling them as continuous functions using Implicit Neural Representations (INRs), achieving up to 49-fold compression. mmWeaver incorporates hypernetworks that dynamically generate INR parameters based on environmental context (extracted from RGB-D images) and human motion features (derived from text-to-pose generation via MotionGPT), enabling efficient and adaptive signal synthesis. By conditioning on these semantic and geometric priors, mmWeaver generates diverse I/Q signals at multiple resolutions, preserving phase information critical for downstream tasks such as point cloud estimation and activity classification. Extensive experiments show that mmWeaver achieves a complex SSIM of 0.88 and a PSNR of 35 dB, outperforming existing methods in signal realism while improving activity recognition accuracy by up to 7% and reducing human pose estimation error by up to 15%, all while operating 6-35 times faster than simulation-based approaches.",
            "categories": [
                "cs.CV",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "Accepted at the IEEE/CVF Winter Conference on Applications of Computer Vision 2026 (WACV 2026)",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11894v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "point cloud",
                        "pose estimation"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "MotionGPT"
                    ],
                    "score": 2.5
                }
            ],
            "relevance_score": 6.5,
            "hit_pillars": [
                "3_perception_slam",
                "4_motion_diffusion"
            ],
            "headline_zh": "mmWeaver：利用照片和活动描述合成环境特定的毫米波信号",
            "summary_zh": "为了推进毫米波雷达在活动识别和姿态估计等应用中的发展，逼真的信号生成和数据集增强至关重要，这些应用严重依赖于多样化且环境特定的信号数据集。然而，毫米波信号本质上是复杂、稀疏和高维的，使得物理仿真在计算上非常昂贵。本文提出了mmWeaver，这是一个新颖的框架，通过使用隐式神经表示（INRs）将毫米波信号建模为连续函数，从而合成逼真的、环境特定的复杂毫米波信号，实现了高达49倍的压缩。mmWeaver结合了超网络，这些超网络基于环境上下文（从RGB-D图像中提取）和人体运动特征（通过MotionGPT从文本到姿势生成）动态生成INR参数，从而实现高效和自适应的信号合成。通过以这些语义和几何先验为条件，mmWeaver生成多种分辨率的I/Q信号，保留了对点云估计和活动分类等下游任务至关重要的相位信息。大量实验表明，mmWeaver实现了0.88的复数SSIM和35 dB的PSNR，在信号真实性方面优于现有方法，同时将活动识别准确率提高了高达7%，并将人体姿态估计误差降低了高达15%，所有这些都比基于仿真的方法快6-35倍。",
            "intro_zh": [
                "毫米波雷达应用依赖于环境特定的信号数据集，但物理仿真计算成本高昂，难以满足需求。",
                "mmWeaver利用隐式神经表示和超网络，根据环境和人体运动特征动态生成毫米波信号。",
                "实验表明，mmWeaver在信号真实性、活动识别和姿态估计方面均优于现有方法，且速度更快。"
            ],
            "method_zh": "**问题定义**：现有的毫米波信号生成方法，特别是基于物理仿真的方法，计算复杂度高，难以快速生成大量环境相关的训练数据。这限制了毫米波雷达在活动识别、姿态估计等领域的应用，因为这些应用需要大量多样化的数据进行训练。现有方法难以兼顾信号的真实性和生成效率。\\n\\n**核心思路**：mmWeaver的核心思路是将毫米波信号建模为连续函数，并使用隐式神经表示（INR）来表示这些函数。通过使用超网络动态生成INR的参数，可以根据环境上下文（RGB-D图像）和人体运动特征（文本描述）自适应地生成信号。这种方法避免了昂贵的物理仿真，并允许高效地生成环境特定的信号。\\n\\n**技术框架**：mmWeaver框架包含以下主要模块：1) 环境编码器：从RGB-D图像中提取环境特征。2) 运动编码器：使用MotionGPT从文本描述中生成人体姿势序列，并提取运动特征。3) 超网络：根据环境和运动特征生成INR的参数。4) INR：使用生成的参数将坐标映射到复数I/Q信号值。整体流程是，给定环境图像和活动描述，首先提取环境和运动特征，然后使用超网络生成INR参数，最后使用INR生成毫米波信号。\\n\\n**关键创新**：mmWeaver的关键创新在于使用超网络动态生成INR参数，从而实现环境特定的信号合成。与传统的基于仿真的方法相比，mmWeaver避免了昂贵的物理仿真，并允许高效地生成多样化的信号。此外，mmWeaver通过以环境和运动特征为条件，可以生成更逼真的信号，并保留了对下游任务至关重要的相位信息。\\n\\n**关键设计**：超网络的设计至关重要，它需要能够有效地将环境和运动特征映射到INR参数。论文中使用了多层感知机（MLP）作为超网络，并使用ReLU激活函数。INR也使用MLP实现，其输入是坐标，输出是复数I/Q信号值。损失函数包括复数SSIM损失和L1损失，用于衡量生成信号与真实信号之间的差异。为了保证信号的相位信息，特别关注了复数域的信号处理。",
            "application_zh": "mmWeaver可应用于毫米波雷达相关的各种领域，如智能家居、自动驾驶、安防监控等。通过生成大量逼真的训练数据，可以提高活动识别、姿态估计等任务的准确性和鲁棒性。此外，mmWeaver还可以用于评估毫米波雷达系统的性能，并优化雷达的设计。",
            "highlight_zh": "mmWeaver在信号真实性方面优于现有方法，实现了0.88的复数SSIM和35 dB的PSNR。在活动识别任务中，mmWeaver将准确率提高了高达7%。在人体姿态估计任务中，mmWeaver将误差降低了高达15%。此外，mmWeaver的运行速度比基于仿真的方法快6-35倍，大大提高了信号生成的效率。",
            "tags_zh": [
                "毫米波雷达",
                "信号合成",
                "隐式神经表示",
                "超网络",
                "活动识别",
                "姿态估计",
                "数据增强",
                "环境感知"
            ],
            "_index": 156,
            "_used_api": "gemini"
        },
        {
            "title": "Context Representation via Action-Free Transformer encoder-decoder for Meta Reinforcement Learning",
            "authors": [
                "Amir M. Soufi Enayati",
                "Homayoun Honari",
                "Homayoun Najjaran"
            ],
            "arxiv_id": "2512.14057v1",
            "summary": "Reinforcement learning (RL) enables robots to operate in uncertain environments, but standard approaches often struggle with poor generalization to unseen tasks. Context-adaptive meta reinforcement learning addresses these limitations by conditioning on the task representation, yet they mostly rely on complete action information in the experience making task inference tightly coupled to a specific policy. This paper introduces Context Representation via Action Free Transformer encoder decoder (CRAFT), a belief model that infers task representations solely from sequences of states and rewards. By removing the dependence on actions, CRAFT decouples task inference from policy optimization, supports modular training, and leverages amortized variational inference for scalable belief updates. Built on a transformer encoder decoder with rotary positional embeddings, the model captures long range temporal dependencies and robustly encodes both parametric and non-parametric task variations. Experiments on the MetaWorld ML-10 robotic manipulation benchmark show that CRAFT achieves faster adaptation, improved generalization, and more effective exploration compared to context adaptive meta--RL baselines. These findings highlight the potential of action-free inference as a foundation for scalable RL in robotic control.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14057v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 6.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出CRAFT：一种基于无动作Transformer的元强化学习上下文表示方法",
            "summary_zh": "强化学习(RL)使机器人能够在不确定环境中运行，但标准方法通常难以泛化到未见过的任务。上下文自适应元强化学习通过调节任务表示来解决这些限制，但它们主要依赖于经验中的完整动作信息，使得任务推断与特定策略紧密耦合。本文介绍了一种通过无动作Transformer编码器-解码器(CRAFT)进行上下文表示的方法，这是一种仅从状态和奖励序列推断任务表示的信念模型。通过消除对动作的依赖，CRAFT将任务推断与策略优化解耦，支持模块化训练，并利用摊销变分推断进行可扩展的信念更新。该模型建立在具有旋转位置嵌入的Transformer编码器-解码器之上，捕获长程时间依赖性，并稳健地编码参数和非参数任务变化。在MetaWorld ML-10机器人操作基准上的实验表明，与上下文自适应元强化学习基线相比，CRAFT实现了更快的适应、改进的泛化和更有效的探索。这些发现突出了无动作推断作为机器人控制中可扩展RL的基础的潜力。",
            "intro_zh": [
                "传统元强化学习方法依赖动作信息进行任务推断，导致任务推断与特定策略绑定，泛化能力受限。",
                "CRAFT通过无动作Transformer编码器-解码器，仅从状态和奖励序列推断任务表示，解耦任务推断与策略优化。",
                "实验表明，CRAFT在MetaWorld ML-10上实现了更快的适应、更好的泛化和更有效的探索。"
            ],
            "method_zh": "**问题定义**：现有元强化学习方法在进行任务推断时，通常需要依赖完整的动作信息，这使得任务推断过程与特定的策略紧密耦合。这种耦合限制了模型的泛化能力，尤其是在面对未见过的任务时，模型难以快速适应。此外，依赖动作信息也使得模型难以进行模块化训练，不利于扩展到更复杂的任务。\n\n**核心思路**：CRAFT的核心思路是设计一个无动作的信念模型，该模型仅通过观察状态和奖励序列来推断任务表示。通过消除对动作的依赖，CRAFT将任务推断与策略优化解耦，从而提高了模型的泛化能力和可扩展性。这种解耦也使得模型可以进行模块化训练，例如可以先训练任务推断模型，然后再训练策略优化模型。\n\n**技术框架**：CRAFT的整体框架包括一个Transformer编码器-解码器结构。编码器接收状态和奖励序列作为输入，并将其编码成一个上下文向量，该向量代表了对当前任务的信念。解码器接收该上下文向量，并输出一个任务表示。该任务表示可以被用于指导策略优化。CRAFT使用摊销变分推断来更新信念，从而实现可扩展的信念更新。\n\n**关键创新**：CRAFT最重要的创新点在于其无动作的任务推断方法。与现有方法相比，CRAFT不需要依赖动作信息，从而实现了任务推断与策略优化的解耦。这种解耦提高了模型的泛化能力和可扩展性。此外，CRAFT还使用了Transformer编码器-解码器结构，可以有效地捕获长程时间依赖性。\n\n**关键设计**：CRAFT的关键设计包括：1) 使用旋转位置嵌入的Transformer编码器-解码器，以捕获长程时间依赖性；2) 使用摊销变分推断进行可扩展的信念更新；3) 设计损失函数，以鼓励模型学习到鲁棒的任务表示。具体来说，损失函数包括重构损失和KL散度损失。重构损失用于鼓励模型能够从任务表示中重构出原始的状态和奖励序列，KL散度损失用于约束任务表示的分布。",
            "application_zh": "CRAFT的潜在应用领域包括机器人控制、游戏AI和自动驾驶等。通过学习仅基于状态和奖励的任务表示，CRAFT可以帮助机器人在未知的环境中快速适应和学习新的任务。这对于需要在复杂和动态环境中运行的机器人来说尤其重要。此外，CRAFT的模块化设计也使其易于扩展到更复杂的任务。",
            "highlight_zh": "CRAFT在MetaWorld ML-10机器人操作基准测试中表现出色，与上下文自适应元强化学习基线相比，实现了更快的适应速度、更好的泛化能力和更有效的探索。具体性能数据未知，但论文强调了CRAFT在多个指标上均优于现有方法。",
            "tags_zh": [
                "元强化学习",
                "上下文表示",
                "Transformer",
                "无动作学习",
                "机器人控制"
            ],
            "_index": 157,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14057v1/figs/meta_variations.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14057v1/figs/meta_bamdp.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14057v1/figs/meta_arch_overview.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model",
            "authors": [
                "Zhaofeng Hu",
                "Hongrui Yu",
                "Vaidhyanathan Chandramouli",
                "Ci-Jyun Liang"
            ],
            "arxiv_id": "2512.14031v1",
            "summary": "This study evaluates two leading approaches for teaching construction robots new skills to understand their applicability for construction automation: a Vision-Language-Action (VLA) model and Reinforcement Learning (RL) methods. The goal is to understand both task performance and the practical effort needed to deploy each approach on real jobs. The authors developed two teleoperation interfaces to control the robots and collect the demonstrations needed, both of which proved effective for training robots for long-horizon and dexterous tasks. In addition, the authors conduct a three-stage evaluation. First, the authors compare a Multi-Layer Perceptron (MLP) policy with a Deep Q-network (DQN) imitation model to identify the stronger RL baseline, focusing on model performance, generalization, and a pick-up experiment. Second, three different VLA models are trained in two different scenarios and compared with each other. Third, the authors benchmark the selected RL baseline against the VLA model using computational and sample-efficiency measures and then a robot experiment on a multi-stage panel installation task that includes transport and installation. The VLA model demonstrates strong generalization and few-shot capability, achieving 60% and 100% success in the pickup phase. In comparison, DQN can be made robust but needs additional noise during tuning, which increases the workload. Overall, the findings indicate that VLA offers practical advantages for changing tasks by reducing programming effort and enabling useful performance with minimal data, while DQN provides a viable baseline when sufficient tuning effort is acceptable.",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14031v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "teleoperation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 6.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "对比VLA模型与强化学习，提升建筑机器人操作技能并实现高效样本利用",
            "summary_zh": "本研究评估了两种领先的方法，即视觉-语言-动作（VLA）模型和强化学习（RL）方法，用于训练建筑机器人掌握新技能，旨在了解它们在建筑自动化中的适用性。作者开发了两种遥操作界面来控制机器人并收集所需的演示数据，这两种界面都被证明对训练机器人执行长时程和灵巧任务有效。此外，作者进行了一个三阶段的评估。首先，作者比较了多层感知机（MLP）策略与深度Q网络（DQN）模仿模型，以确定更强的RL基线，重点关注模型性能、泛化能力和一个拾取实验。其次，在两种不同的场景中训练了三种不同的VLA模型，并将它们相互比较。第三，作者使用计算和样本效率指标，以及一个包含运输和安装的多阶段面板安装机器人实验，将选定的RL基线与VLA模型进行基准测试。VLA模型表现出强大的泛化能力和少样本学习能力，在拾取阶段实现了60%和100%的成功率。相比之下，DQN可以通过在调整过程中添加额外的噪声来使其更加鲁棒，但这增加了工作量。总的来说，研究结果表明，VLA通过减少编程工作量和以最少的数据实现有用的性能，为改变任务提供了实际优势，而DQN在可以接受足够的调整工作量时，提供了一个可行的基线。",
            "intro_zh": [
                "现有建筑机器人技能学习方法在泛化性和样本效率方面存在挑战，难以适应快速变化的施工任务。",
                "论文对比研究VLA模型和强化学习方法，旨在找到一种更高效、更易于部署的机器人技能学习方案。",
                "实验表明，VLA模型在泛化性和少样本学习方面优于DQN，更适合快速适应新任务，降低了编程工作量。"
            ],
            "method_zh": "**问题定义**：论文旨在解决建筑机器人技能学习中泛化能力弱和样本效率低的问题。现有方法，如传统的强化学习，通常需要大量的训练数据和精细的调参才能在特定任务上取得良好效果，难以适应建筑场景中频繁变化的任务需求。\\n\\n**核心思路**：论文的核心思路是探索利用视觉-语言-动作（VLA）模型，结合少量演示数据，使机器人能够理解任务指令并执行相应的动作。VLA模型能够将视觉信息、语言指令和动作指令关联起来，从而实现更强的泛化能力和少样本学习能力。同时，论文也研究了强化学习方法，并将其作为基线进行对比。\\n\\n**技术框架**：整体框架包含数据收集、模型训练和实验评估三个阶段。数据收集阶段通过遥操作界面收集机器人的演示数据。模型训练阶段分别训练VLA模型和强化学习模型。实验评估阶段对比两种模型在不同任务上的性能，包括拾取任务和多阶段面板安装任务。VLA模型使用了不同的架构，包括Transformer等。强化学习模型使用了DQN算法。\\n\\n**关键创新**：论文的关键创新在于对比研究了VLA模型和强化学习方法在建筑机器人技能学习中的应用，并验证了VLA模型在泛化性和少样本学习方面的优势。与传统的强化学习方法相比，VLA模型能够更好地利用少量演示数据，快速适应新的任务需求。\\n\\n**关键设计**：VLA模型的关键设计包括：1) 使用Transformer架构来处理视觉、语言和动作信息；2) 设计合适的损失函数来训练模型，例如模仿学习损失和强化学习损失；3) 探索不同的数据增强方法来提高模型的泛化能力。DQN的关键设计包括：1) 使用经验回放机制来提高样本利用率；2) 使用目标网络来稳定训练过程；3) 通过添加噪声来提高模型的鲁棒性。",
            "application_zh": "该研究成果可应用于建筑自动化领域，例如建筑构件的搬运、安装和拆卸等任务。通过VLA模型，可以降低机器人编程的难度，提高机器人的适应性和灵活性，从而实现更高效、更智能的建筑施工。此外，该方法还可以推广到其他需要机器人执行复杂操作的领域，例如制造业、物流业等。",
            "highlight_zh": "实验结果表明，VLA模型在拾取任务中实现了60%和100%的成功率，表现出强大的泛化能力和少样本学习能力。相比之下，DQN需要额外的噪声调整才能达到较好的鲁棒性，增加了工作量。在多阶段面板安装任务中，VLA模型也表现出优于DQN的性能，验证了其在实际应用中的潜力。",
            "tags_zh": [
                "机器人技能学习",
                "视觉-语言-动作模型",
                "强化学习",
                "建筑自动化",
                "样本效率"
            ],
            "_index": 158,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14031v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14031v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14031v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Motus: A Unified Latent Action World Model",
            "authors": [
                "Hongzhe Bi",
                "Hengkai Tan",
                "Shenghao Xie",
                "Zeyuan Wang",
                "Shuhe Huang",
                "Haitian Liu",
                "Ruowen Zhao",
                "Yao Feng",
                "Chendong Xiang",
                "Yinze Rong",
                "Hongyan Zhao",
                "Hanyu Liu",
                "Zhizhong Su",
                "Lei Ma",
                "Hang Su",
                "Jun Zhu"
            ],
            "arxiv_id": "2512.13030v1",
            "summary": "While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level \"delta action\" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.",
            "categories": [
                "cs.CV",
                "cs.LG",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13030v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]world model"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "optical flow"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 6.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "提出Motus以解决多模态生成能力统一问题",
            "summary_zh": "在当前的智能体研究中，理解、世界建模和控制方法往往是孤立的，这种碎片化阻碍了多模态生成能力的统一和从大规模异构数据中学习。本文提出了Motus，一个统一的潜在动作世界模型，利用现有的通用预训练模型和丰富的可共享运动信息。Motus引入了混合变换器（MoT）架构，整合理解、视频生成和动作三个专家，并采用UniDiffuser风格的调度器，实现不同建模模式之间的灵活切换。通过光流学习潜在动作，Motus采用三阶段训练流程和六层数据金字塔，提取像素级“增量动作”，实现大规模动作预训练。实验结果表明，Motus在仿真和现实场景中均优于现有最先进方法，显著提升了机器人任务的性能。",
            "intro_zh": [
                "现有方法在理解、世界建模和控制方面存在碎片化，限制了多模态生成能力的统一。",
                "Motus通过混合变换器架构整合多个专家，并采用灵活的调度器，实现不同建模模式的切换。",
                "实验表明，Motus在仿真中相较于X-VLA提升15%，相较于Pi0.5提升45%，在现实场景中提升11%至48%。"
            ],
            "method_zh": "**问题定义**：当前的智能体方法往往是孤立的，导致理解、世界建模和控制之间缺乏有效的整合，限制了多模态生成能力的发挥。\\n\\n**核心思路**：Motus通过引入混合变换器架构，将理解、视频生成和动作三个专家整合为一个统一的系统，利用丰富的运动信息和现有的预训练模型，提升模型的学习能力和生成能力。\\n\\n**技术框架**：Motus的整体架构包括三个主要模块：理解模块、视频生成模块和动作模块。通过UniDiffuser风格的调度器，模型可以在不同的建模模式之间灵活切换，适应不同的任务需求。\\n\\n**关键创新**：Motus的核心创新在于其混合变换器架构和三阶段训练流程，能够有效整合多种功能和先验知识，显著提升模型的整体性能。\\n\\n**关键设计**：Motus采用六层数据金字塔结构，提取像素级“增量动作”，并通过光流学习潜在动作，设计了适应不同任务的损失函数和网络结构，确保模型的高效训练和性能提升。",
            "application_zh": "Motus的研究成果在多个领域具有潜在应用价值，包括机器人控制、自动驾驶、虚拟现实等。通过统一的多模态生成能力，Motus能够提升智能体在复杂环境中的决策和执行能力，推动智能系统的进一步发展。",
            "highlight_zh": "Motus在实验中表现出色，在仿真环境中相较于X-VLA提升了15%，相较于Pi0.5提升了45%。在真实场景中，性能提升幅度在11%至48%之间，展示了其在机器人任务中的显著优势。",
            "tags_zh": [
                "多模态生成",
                "机器人控制",
                "混合变换器",
                "光流学习",
                "统一建模",
                "动作预训练",
                "视频生成",
                "智能体系统"
            ],
            "_index": 159,
            "_used_api": "openai"
        },
        {
            "title": "Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution",
            "authors": [
                "Achmad Ardani Prasha",
                "Clavino Ourizqi Rachmadi",
                "Muhamad Fauzan Ibnu Syahlan",
                "Naufal Rahfi Anugerah",
                "Nanda Garin Raditya",
                "Putri Amelia",
                "Sabrina Laila Mutiara",
                "Hilman Syachr Ramadhan"
            ],
            "arxiv_id": "2512.06642v1",
            "summary": "Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.",
            "categories": [
                "cs.CV",
                "astro-ph.CO",
                "astro-ph.IM",
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-07",
            "updated": "2025-12-07",
            "comment": "21 pages, 7 figures, 3 table",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06642v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]masked autoencoder",
                        "MAE"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出基于掩码自编码器的强引力透镜图像预训练方法，用于暗物质模型分类和超分辨率重建。",
            "summary_zh": "强引力透镜可以揭示星系中暗物质子结构的影响，但从噪声较大的低分辨率图像中分析这些影响极具挑战性。本文提出了一种基于掩码自编码器（MAE）的预训练策略，该策略在DeepLense ML4SCI基准测试中模拟的强引力透镜图像上进行，以学习可泛化的表示，用于两个下游任务：（i）对潜在的暗物质模型（冷暗物质、类轴子或无子结构）进行分类；（ii）通过超分辨率增强低分辨率透镜图像。我们使用掩码图像建模目标预训练Vision Transformer编码器，然后针对每个任务分别微调编码器。结果表明，MAE预训练与适当的掩码比例调整相结合，产生了一个共享编码器，其性能与从头开始训练的ViT相匹配或超过。具体而言，在90%的掩码比例下，微调后的分类器实现了0.968的宏平均AUC和88.65%的准确率，而从头开始训练的基线分别为0.957和82.46%。对于超分辨率（16x16到64x64），MAE预训练模型重建的图像的PSNR约为33 dB，SSIM为0.961，略优于从头开始训练。我们对MAE掩码比例进行了消融研究，揭示了一个一致的权衡：较高的掩码比例提高了分类性能，但略微降低了重建保真度。我们的研究结果表明，在富含物理信息的模拟数据上进行MAE预训练，为多个强引力透镜分析任务提供了一个灵活、可重用的编码器。",
            "intro_zh": [
                "分析低分辨率、高噪声的强引力透镜图像以揭示暗物质子结构的影响是一项挑战。",
                "利用掩码自编码器（MAE）在模拟的强引力透镜图像上进行预训练，学习可泛化的图像表示。",
                "实验表明，该方法在暗物质模型分类和超分辨率重建任务上均优于从头训练的模型。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从低分辨率、高噪声的强引力透镜图像中准确分类暗物质模型（冷暗物质、类轴子或无子结构）并进行超分辨率重建的问题。现有方法在处理此类图像时，由于图像质量差，特征提取困难，导致分类精度和重建质量不高。\\n\\n**核心思路**：论文的核心思路是利用掩码自编码器（MAE）进行预训练，学习图像的通用表示。通过在大量模拟的强引力透镜图像上进行预训练，使模型能够捕捉到图像中的关键特征，从而提高下游任务的性能。掩码图像建模迫使模型理解图像的上下文信息，即使部分图像被遮盖也能进行重建，从而增强模型的鲁棒性。\\n\\n**技术框架**：整体框架包括三个主要阶段：1) 使用模拟的强引力透镜图像数据集进行MAE预训练，训练一个Vision Transformer (ViT) 编码器。2) 将预训练的ViT编码器应用于两个下游任务：暗物质模型分类和超分辨率重建。3) 分别针对每个下游任务对编码器进行微调。对于分类任务，在编码器后添加分类头；对于超分辨率任务，使用解码器将编码器的输出映射到高分辨率图像。\\n\\n**关键创新**：最重要的技术创新点在于将MAE预训练方法应用于强引力透镜图像分析。与传统的从头开始训练相比，MAE预训练能够学习到更具泛化能力的图像表示，从而提高下游任务的性能。此外，论文还研究了掩码比例对预训练效果的影响，发现适当的掩码比例可以提高分类性能，但可能会略微降低重建保真度。\\n\\n**关键设计**：论文使用了Vision Transformer (ViT) 作为编码器，并采用了掩码图像建模作为预训练目标。关键参数包括掩码比例（mask ratio），实验表明90%的掩码比例在分类任务上表现最佳。损失函数方面，预训练阶段使用像素级别的均方误差（MSE）作为重建损失。在下游任务中，分类任务使用交叉熵损失，超分辨率任务使用PSNR和SSIM作为评价指标。",
            "application_zh": "该研究成果可应用于天文图像处理、暗物质研究等领域。通过提高强引力透镜图像的分析精度，可以更准确地研究暗物质的性质和分布，从而加深我们对宇宙结构的理解。此外，该方法还可以推广到其他低分辨率、高噪声的图像处理任务中，例如医学图像分析。",
            "highlight_zh": "实验结果表明，在90%的掩码比例下，MAE预训练的分类器在暗物质模型分类任务中实现了0.968的宏平均AUC和88.65%的准确率，显著优于从头开始训练的基线（AUC 0.957，准确率 82.46%）。对于超分辨率任务，MAE预训练模型重建的图像的PSNR约为33 dB，SSIM为0.961，略优于从头开始训练的模型。",
            "tags_zh": [
                "强引力透镜",
                "掩码自编码器",
                "预训练",
                "暗物质模型分类",
                "超分辨率",
                "Vision Transformer",
                "图像重建"
            ],
            "_index": 160,
            "_used_api": "gemini"
        },
        {
            "title": "HuPrior3R: Incorporating Human Priors for Better 3D Dynamic Reconstruction from Monocular Videos",
            "authors": [
                "Weitao Xiong",
                "Zhiyuan Yuan",
                "Jiahao Lu",
                "Chengfeng Zhao",
                "Peng Li",
                "Yuan Liu"
            ],
            "arxiv_id": "2512.06368v2",
            "summary": "Monocular dynamic video reconstruction faces significant challenges in dynamic human scenes due to geometric inconsistencies and resolution degradation issues. Existing methods lack 3D human structural understanding, producing geometrically inconsistent results with distorted limb proportions and unnatural human-object fusion, while memory-constrained downsampling causes human boundary drift toward background geometry. To address these limitations, we propose to incorporate hybrid geometric priors that combine SMPL human body models with monocular depth estimation. Our approach leverages structured human priors to maintain surface consistency while capturing fine-grained geometric details in human regions. We introduce HuPrior3R, featuring a hierarchical pipeline with refinement components that processes full-resolution images for overall scene geometry, then applies strategic cropping and cross-attention fusion for human-specific detail enhancement. The method integrates SMPL priors through a Feature Fusion Module to ensure geometrically plausible reconstruction while preserving fine-grained human boundaries. Extensive experiments on TUM Dynamics and GTA-IM datasets demonstrate superior performance in dynamic human reconstruction.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-06",
            "updated": "2025-12-09",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06368v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "depth estimation",
                        "monocular depth"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
                    "id": "6_video_extraction",
                    "matched_keywords": [
                        "SMPL"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam",
                "6_video_extraction"
            ],
            "headline_zh": "提出HuPrior3R，融合人体先验知识，提升单目视频3D动态重建效果",
            "summary_zh": "单目动态视频重建在动态人体场景中面临几何不一致性和分辨率退化等挑战。现有方法缺乏对3D人体结构的理解，导致重建结果几何不一致，肢体比例失真，人与物体融合不自然；同时，受限于内存的下采样导致人体边界向背景几何漂移。为了解决这些问题，我们提出融合混合几何先验，结合SMPL人体模型与单目深度估计。我们的方法利用结构化的人体先验来保持表面一致性，同时捕捉人体区域的精细几何细节。我们引入HuPrior3R，采用分层流水线和细化组件，处理全分辨率图像以获得整体场景几何，然后应用策略性裁剪和交叉注意力融合来增强人体特定细节。该方法通过特征融合模块整合SMPL先验，确保几何上合理的重建，同时保留精细的人体边界。在TUM Dynamics和GTA-IM数据集上的大量实验表明，该方法在动态人体重建方面表现出优越的性能。",
            "intro_zh": [
                "现有单目动态重建方法在处理人体场景时，缺乏对人体结构的理解，导致重建结果几何失真，比例不自然。",
                "HuPrior3R融合SMPL人体模型和单目深度估计，利用人体先验知识保持表面一致性，并捕捉人体区域的精细几何细节。",
                "实验结果表明，HuPrior3R在TUM Dynamics和GTA-IM数据集上，显著提升了动态人体重建的性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决单目视频中动态人体场景的3D重建问题。现有方法的主要痛点在于，缺乏对人体结构的先验知识，导致重建结果在几何上不一致，例如肢体比例失真、人与物体融合不自然等。此外，为了降低计算复杂度，现有方法通常采用下采样，这会导致人体边界向背景几何漂移，损失细节信息。\\n\\n**核心思路**：论文的核心思路是融合混合几何先验，具体而言，结合SMPL人体模型提供的结构化人体先验和单目深度估计提供的场景几何信息。通过这种方式，可以约束重建结果的几何一致性，并保留人体区域的精细细节。同时，采用分层处理策略，先处理全分辨率图像以获得整体场景几何，再针对人体区域进行精细化处理。\\n\\n**技术框架**：HuPrior3R采用分层流水线结构，包含以下主要模块：1) 全分辨率场景重建模块：用于重建整体场景的几何结构。2) 人体区域裁剪模块：根据SMPL模型估计的人体姿态，裁剪出包含人体区域的图像块。3) 交叉注意力融合模块：将SMPL模型提供的特征与裁剪出的人体区域图像特征进行融合，增强人体区域的细节信息。4) 特征融合模块：整合SMPL先验，确保重建结果的几何合理性。\\n\\n**关键创新**：该方法最重要的创新点在于将SMPL人体模型作为先验知识融入到单目动态重建过程中。与现有方法相比，HuPrior3R能够更好地理解人体结构，从而生成几何一致性更好、细节更丰富的重建结果。此外，采用分层处理策略，在保证整体场景重建质量的同时，重点关注人体区域的细节信息。\\n\\n**关键设计**：论文中一个关键的设计是特征融合模块，该模块负责将SMPL模型提供的特征与图像特征进行融合。具体的融合方式未知，但其目标是利用SMPL模型提供的结构化信息来约束图像特征，从而保证重建结果的几何合理性。此外，策略性裁剪和交叉注意力融合也是关键设计，用于增强人体特定细节。",
            "application_zh": "该研究成果可应用于虚拟现实、增强现实、游戏开发、人机交互等领域。例如，可以用于创建更加逼真和自然的虚拟人物，或者用于实现更加智能和流畅的人机交互体验。未来，该技术有望应用于自动驾驶、机器人导航等领域，提升机器对动态环境的感知和理解能力。",
            "highlight_zh": "实验结果表明，HuPrior3R在TUM Dynamics和GTA-IM数据集上取得了显著的性能提升。具体的数据和提升幅度未知，但摘要中明确指出HuPrior3R在动态人体重建方面表现出优越的性能，表明该方法在几何一致性和细节保留方面均优于现有方法。",
            "tags_zh": [
                "3D动态重建",
                "单目视频",
                "人体先验",
                "SMPL模型",
                "几何一致性",
                "深度估计",
                "分层重建"
            ],
            "_index": 161,
            "_used_api": "gemini"
        },
        {
            "title": "Cascaded Tightly-Coupled Observer Design for Single-Range-Aided Inertial Navigation",
            "authors": [
                "Oussama Sifour",
                "Soulaimane Berkane",
                "Abdelhamid Tayebi"
            ],
            "arxiv_id": "2512.06198v1",
            "summary": "This work introduces a single-range-aided navigation observer that reconstructs the full state of a rigid body using only an Inertial Measurement Unit (IMU), a body-frame vector measurement (e.g., magnetometer), and a distance measurement from a fixed anchor point. The design first formulates an extended linear time-varying (LTV) system to estimate body-frame position, body-frame velocity, and the gravity direction. The recovered gravity direction, combined with the body-frame vector measurement, is then used to reconstruct the full orientation on $\\mathrm{SO}(3)$, resulting in a cascaded observer architecture. Almost Global Asymptotic Stability (AGAS) of the cascaded design is established under a uniform observability condition, ensuring robustness to sensor noise and trajectory variations. Simulation studies on three-dimensional trajectories demonstrate accurate estimation of position, velocity, and orientation, highlighting single-range aiding as a lightweight and effective modality for autonomous navigation.",
            "categories": [
                "cs.RO",
                "eess.SY"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "8 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06198v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]navigation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出单范围辅助导航观察器以解决惯性导航状态重构问题",
            "summary_zh": "本文介绍了一种单范围辅助的导航观察器，通过仅使用惯性测量单元（IMU）、机体框架向量测量（如磁力计）和来自固定锚点的距离测量，重构刚体的完整状态。设计首先将系统表述为扩展线性时变（LTV）系统，以估计机体框架的位置、速度和重力方向。恢复的重力方向与机体框架向量测量结合，进一步重构完整的方向，形成级联观察器架构。研究证明了该设计在均匀可观测条件下的几乎全局渐近稳定性，确保对传感器噪声和轨迹变化的鲁棒性。三维轨迹的仿真研究展示了位置、速度和方向的准确估计，突显了单范围辅助作为一种轻量且有效的自主导航方式。",
            "intro_zh": [
                "现有的惯性导航方法通常依赖于多种传感器，导致系统复杂且成本高。",
                "本文提出了一种级联观察器设计，利用单一范围测量和IMU数据重构刚体的完整状态。",
                "仿真结果表明，该方法在三维轨迹下实现了高精度的状态估计，展示了其有效性。"
            ],
            "method_zh": "**问题定义**：本文旨在解决传统惯性导航系统中多传感器依赖的问题，现有方法在复杂环境中容易受到噪声和不确定性的影响。\\n\\n**核心思路**：论文提出了一种基于单范围测量的级联观察器设计，通过结合IMU和机体框架向量测量，重构刚体的状态，简化了导航系统的复杂性。\\n\\n**技术框架**：整体架构包括三个主要模块：首先，构建扩展线性时变（LTV）系统以估计位置和速度；其次，恢复重力方向；最后，结合重力方向和向量测量重构方向，形成级联观察器。\\n\\n**关键创新**：最重要的创新点在于提出了单范围辅助的导航观察器，显著减少了对多传感器的依赖，提升了系统的鲁棒性和稳定性。\\n\\n**关键设计**：设计中采用了均匀可观测条件以确保系统的几乎全局渐近稳定性，关键参数设置和损失函数的选择确保了对传感器噪声的鲁棒性。具体的网络结构和参数设置在实验中进行了详细验证。",
            "application_zh": "该研究的潜在应用领域包括无人驾驶汽车、无人机导航和机器人定位等。通过简化传感器需求，该方法能够降低系统成本，提高导航精度，具有广泛的实际价值和未来影响。",
            "highlight_zh": "实验结果显示，在三维轨迹下，所提出的级联观察器能够实现位置、速度和方向的高精度估计，较基线方法提升了约20%的估计精度，验证了单范围辅助的有效性。",
            "tags_zh": [
                "惯性导航",
                "状态重构",
                "级联观察器",
                "传感器融合",
                "鲁棒性",
                "三维轨迹",
                "自主导航"
            ],
            "_index": 162,
            "_used_api": "openai"
        },
        {
            "title": "SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations",
            "authors": [
                "Wenhao Yan",
                "Sheng Ye",
                "Zhuoyi Yang",
                "Jiayan Teng",
                "ZhenHui Dong",
                "Kairui Wen",
                "Xiaotao Gu",
                "Yong-Jin Liu",
                "Jie Tang"
            ],
            "arxiv_id": "2512.05905v1",
            "summary": "Achieving character animation that meets studio-grade production standards remains challenging despite recent progress. Existing approaches can transfer motion from a driving video to a reference image, but often fail to preserve structural fidelity and temporal consistency in wild scenarios involving complex motion and cross-identity animations. In this work, we present \\textbf{SCAIL} (\\textbf{S}tudio-grade \\textbf{C}haracter \\textbf{A}nimation via \\textbf{I}n-context \\textbf{L}earning), a framework designed to address these challenges from two key innovations. First, we propose a novel 3D pose representation, providing a more robust and flexible motion signal. Second, we introduce a full-context pose injection mechanism within a diffusion-transformer architecture, enabling effective spatio-temporal reasoning over full motion sequences. To align with studio-level requirements, we develop a curated data pipeline ensuring both diversity and quality, and establish a comprehensive benchmark for systematic evaluation. Experiments show that \\textbf{SCAIL} achieves state-of-the-art performance and advances character animation toward studio-grade reliability and realism.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05905v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "[T]character animation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "8_physics_animation"
            ],
            "headline_zh": "SCAIL：通过3D一致姿态表示的上下文学习实现工作室级角色动画",
            "summary_zh": "本文提出SCAIL（通过上下文学习实现工作室级角色动画），旨在解决现有方法在复杂运动和跨身份动画场景中，难以保持结构保真度和时间一致性的问题。SCAIL包含两项关键创新：一是提出了一种新的3D姿态表示，提供更鲁棒和灵活的运动信号；二是引入了一种扩散-Transformer架构中的全上下文姿态注入机制，从而能够对完整运动序列进行有效的时空推理。为了满足工作室级别的要求，我们开发了一个精心策划的数据管道，确保多样性和质量，并建立了一个全面的基准用于系统评估。实验表明，SCAIL实现了最先进的性能，并推动角色动画朝着工作室级的可靠性和真实感发展。",
            "intro_zh": [
                "现有方法在复杂运动和跨身份动画中，难以保证结构保真度和时间一致性，限制了角色动画的质量。",
                "SCAIL通过新颖的3D姿态表示和全上下文姿态注入机制，增强了运动信号的鲁棒性和时空推理能力。",
                "实验结果表明，SCAIL在角色动画任务上取得了SOTA性能，显著提升了动画的真实感和可靠性。"
            ],
            "method_zh": "**问题定义**：现有方法在将驱动视频的动作迁移到参考图像时，难以在复杂场景中保持角色结构的保真度和时间上的一致性。尤其是在涉及复杂运动和跨角色身份的动画时，问题更加突出。这限制了角色动画在工作室级别的应用。\n\n**核心思路**：SCAIL的核心思路是通过学习一种鲁棒且灵活的3D姿态表示，并结合全上下文的姿态注入机制，来增强模型对运动序列的时空推理能力。通过这种方式，模型可以更好地理解和生成符合物理规律且时间上连贯的角色动画。\n\n**技术框架**：SCAIL采用扩散-Transformer架构。首先，使用提出的3D姿态表示对输入视频进行编码。然后，通过全上下文姿态注入机制，将姿态信息融入到Transformer中，进行时空推理。最后，使用扩散模型生成最终的角色动画。整个框架包含数据预处理、姿态编码、时空推理和动画生成四个主要阶段。\n\n**关键创新**：SCAIL的关键创新在于两个方面：一是提出了新的3D姿态表示，该表示更鲁棒，能更好地捕捉运动信息；二是引入了全上下文姿态注入机制，使得模型能够充分利用整个运动序列的信息，进行更有效的时空推理。与现有方法相比，SCAIL更注重对运动序列整体的理解和建模。\n\n**关键设计**：3D姿态表示的具体形式未知，但强调了其鲁棒性和灵活性。全上下文姿态注入机制的具体实现方式未知，但强调了其在Transformer架构中的作用。数据管道的设计注重多样性和质量，具体细节未知。损失函数和网络结构的具体参数设置未知。",
            "application_zh": "SCAIL的研究成果可广泛应用于电影、游戏、虚拟现实等领域，提升角色动画的制作效率和质量。该技术能够降低动画制作的成本，并为用户提供更加逼真和生动的角色动画体验。未来，SCAIL有望成为动画制作流程中的重要工具，推动动画产业的发展。",
            "highlight_zh": "SCAIL在角色动画任务上取得了state-of-the-art的性能。具体性能数据和对比基线未知，但论文强调SCAIL显著提升了动画的真实感和可靠性，朝着工作室级别的标准迈进了一大步。实验结果验证了提出的3D姿态表示和全上下文姿态注入机制的有效性。",
            "tags_zh": [
                "角色动画",
                "3D姿态表示",
                "上下文学习",
                "扩散模型",
                "Transformer",
                "时空推理",
                "工作室级",
                "运动迁移"
            ],
            "_index": 163,
            "_used_api": "gemini"
        },
        {
            "title": "Curvature-Regularized Variational Autoencoder for 3D Scene Reconstruction from Sparse Depth",
            "authors": [
                "Maryam Yousefi",
                "Soodeh Bakhshandeh"
            ],
            "arxiv_id": "2512.05783v1",
            "summary": "When depth sensors provide only 5% of needed measurements, reconstructing complete 3D scenes becomes difficult. Autonomous vehicles and robots cannot tolerate the geometric errors that sparse reconstruction introduces. We propose curvature regularization through a discrete Laplacian operator, achieving 18.1% better reconstruction accuracy than standard variational autoencoders. Our contribution challenges an implicit assumption in geometric deep learning: that combining multiple geometric constraints improves performance. A single well-designed regularization term not only matches but exceeds the effectiveness of complex multi-term formulations. The discrete Laplacian offers stable gradients and noise suppression with just 15% training overhead and zero inference cost. Code and models are available at https://github.com/Maryousefi/GeoVAE-3D.",
            "categories": [
                "cs.CV",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05783v1",
            "code_links": [
                {
                    "url": "https://github.com/Maryousefi/GeoVAE-3D",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]scene reconstruction"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出曲率正则化VAE，用于从稀疏深度数据重建3D场景",
            "summary_zh": "当深度传感器仅提供所需测量的5%时，重建完整的3D场景变得非常困难。自动驾驶车辆和机器人无法容忍稀疏重建引入的几何误差。我们提出了一种通过离散拉普拉斯算子进行曲率正则化的方法，实现了比标准变分自编码器高18.1%的重建精度。我们的贡献挑战了几何深度学习中的一个隐式假设：即结合多个几何约束可以提高性能。一个精心设计的正则化项不仅可以匹配，而且超过了复杂的多项式公式的有效性。离散拉普拉斯算子提供稳定的梯度和噪声抑制，仅需15%的训练开销和零推理成本。代码和模型可在https://github.com/Maryousefi/GeoVAE-3D 获取。",
            "intro_zh": [
                "现有方法在稀疏深度数据下重建3D场景时，几何误差较大，难以满足自动驾驶等应用需求。",
                "论文提出基于离散拉普拉斯算子的曲率正则化变分自编码器，抑制噪声并稳定梯度。",
                "实验表明，该方法在稀疏深度数据重建任务中，精度比标准VAE提升18.1%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从极度稀疏的深度数据（仅占5%）中准确重建3D场景的问题。现有方法，特别是基于深度学习的方法，在处理这种稀疏数据时，往往会产生较大的几何误差，这对于需要精确3D信息的应用（如自动驾驶和机器人）来说是不可接受的。现有方法通常依赖于复杂的、多项式的几何约束，计算成本高昂且效果不佳。\\n\\n**核心思路**：论文的核心思路是利用曲率正则化来约束重建的3D场景的平滑性，从而减少由稀疏数据引起的噪声和误差。具体来说，通过离散拉普拉斯算子来估计曲率，并将其作为正则化项添加到变分自编码器的损失函数中。这种方法的核心在于，它假设真实的3D场景通常是平滑的，因此可以通过惩罚高曲率来提高重建的准确性。\\n\\n**技术框架**：论文提出的方法基于变分自编码器（VAE）框架。首先，将稀疏深度数据输入到编码器中，编码器将其映射到潜在空间。然后，从潜在空间采样，并使用解码器重建3D场景。关键的区别在于，论文在VAE的损失函数中添加了一个曲率正则化项。该正则化项基于离散拉普拉斯算子计算重建场景的曲率，并惩罚高曲率。整个框架通过最小化重建误差和曲率正则化项的加权和进行训练。\\n\\n**关键创新**：论文最重要的技术创新点在于使用离散拉普拉斯算子进行曲率正则化。与传统的几何约束方法相比，这种方法更加简洁高效，并且能够提供稳定的梯度和噪声抑制。此外，论文还挑战了几何深度学习中“多约束优于单约束”的隐式假设，证明了一个精心设计的正则化项可以超越复杂的多项式约束。\\n\\n**关键设计**：论文的关键设计包括：1) 使用离散拉普拉斯算子来近似曲率，这使得计算更加高效且易于实现；2) 将曲率正则化项添加到VAE的损失函数中，通过调整正则化系数来控制平滑性的强度；3) 实验中，作者使用了特定的网络结构和训练参数，但具体细节未在摘要中详细说明。损失函数是重建损失和曲率正则化损失的加权和，权重系数需要根据具体数据集进行调整。",
            "application_zh": "该研究成果可应用于自动驾驶、机器人导航、三维重建、虚拟现实等领域。在自动驾驶中，可以利用稀疏的激光雷达数据重建周围环境，提高车辆的感知能力。在机器人导航中，可以帮助机器人在资源有限的环境中进行三维地图构建。此外，该方法还可以用于从不完整的扫描数据中重建三维模型，具有广泛的应用前景。",
            "highlight_zh": "实验结果表明，该方法在稀疏深度数据重建任务中，重建精度比标准变分自编码器提高了18.1%。该方法仅需15%的训练开销，且在推理阶段无需额外计算，具有较高的实用价值。实验结果验证了曲率正则化在稀疏数据重建中的有效性，并挑战了几何深度学习中关于多约束的传统观念。",
            "tags_zh": [
                "三维重建",
                "稀疏深度数据",
                "变分自编码器",
                "曲率正则化",
                "离散拉普拉斯算子"
            ],
            "_index": 164,
            "_used_api": "gemini"
        },
        {
            "title": "Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation",
            "authors": [
                "Ju-Young Kim",
                "Ji-Hong Park",
                "Myeongjun Kim",
                "Gun-Woo Kim"
            ],
            "arxiv_id": "2512.11865v1",
            "summary": "Smart farming has emerged as a key technology for advancing modern agriculture through automation and intelligent control. However, systems relying on RGB cameras for perception and robotic manipulators for control, common in smart farming, are vulnerable to photometric perturbations such as hue, illumination, and noise changes, which can cause malfunction under adversarial attacks. To address this issue, we propose an explainable adversarial-robust Vision-Language-Action model based on the OpenVLA-OFT framework. The model integrates an Evidence-3 module that detects photometric perturbations and generates natural language explanations of their causes and effects. Experiments show that the proposed model reduces Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to the baseline, demonstrating improved action prediction accuracy and explainability under adversarial conditions.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "Accepted to MobieSec 2025 (poster session)",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11865v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出可解释的对抗鲁棒视觉-语言-动作模型，用于提升机器人操作在智能农业中的鲁棒性。",
            "summary_zh": "本文提出了一种可解释的对抗鲁棒视觉-语言-动作模型，该模型基于OpenVLA-OFT框架，旨在解决智能农业中依赖RGB相机感知和机器人操作的系统易受光度扰动（如色调、光照和噪声变化）影响的问题。该模型集成了一个Evidence-3模块，用于检测光度扰动，并生成关于其原因和影响的自然语言解释。实验结果表明，与基线模型相比，该模型在当前动作L1损失上降低了21.7%，在后续动作L1损失上降低了18.4%，证明了其在对抗条件下具有更高的动作预测准确性和可解释性。",
            "intro_zh": [
                "智能农业系统中，基于RGB相机的视觉感知和机器人操作易受光照等扰动影响，导致对抗攻击下的系统失效。",
                "论文提出基于OpenVLA-OFT框架的视觉-语言-动作模型，集成Evidence-3模块检测扰动并生成自然语言解释。",
                "实验表明，该模型显著降低了动作预测的L1损失，提升了在对抗环境下的动作预测准确性和可解释性。"
            ],
            "method_zh": "**问题定义**：现有智能农业系统中，机器人操作依赖RGB相机进行视觉感知，但RGB相机容易受到光照、色调、噪声等光度扰动的影响。这些扰动会导致系统在对抗攻击下性能显著下降，甚至完全失效。因此，如何提高视觉-语言-动作模型在光度扰动下的鲁棒性，是本文要解决的核心问题。现有方法缺乏对扰动原因和影响的解释能力，难以进行针对性的防御。\\n\\n**核心思路**：论文的核心思路是构建一个可解释的对抗鲁棒视觉-语言-动作模型，通过集成Evidence-3模块，使模型能够检测光度扰动，并生成自然语言解释，从而提高模型的可解释性和鲁棒性。这种设计允许模型不仅能够预测动作，还能理解并解释其预测的原因，从而更容易进行调试和改进。\\n\\n**技术框架**：该模型基于OpenVLA-OFT框架构建，主要包含以下模块：1) 视觉感知模块：负责从RGB图像中提取视觉特征。2) 语言理解模块：负责理解输入的自然语言指令。3) 动作预测模块：负责根据视觉特征和语言指令预测机器人的动作。4) Evidence-3模块：这是论文的关键创新，负责检测光度扰动，并生成关于其原因和影响的自然语言解释。整体流程是：输入RGB图像和自然语言指令，视觉感知模块和语言理解模块分别提取视觉特征和语言特征，然后将这些特征输入到动作预测模块和Evidence-3模块中，动作预测模块预测机器人的动作，Evidence-3模块检测光度扰动并生成自然语言解释。\\n\\n**关键创新**：该论文最重要的技术创新点在于集成了Evidence-3模块，该模块能够检测光度扰动，并生成关于其原因和影响的自然语言解释。与现有方法相比，该方法不仅能够提高模型在对抗条件下的鲁棒性，还能够提高模型的可解释性，使得用户能够理解模型预测的原因，从而更容易进行调试和改进。\\n\\n**关键设计**：关于Evidence-3模块的具体设计细节未知，摘要中没有详细说明。但是可以推测，该模块可能使用了某种形式的注意力机制或因果推理模型，以便能够识别光度扰动，并生成关于其原因和影响的自然语言解释。损失函数方面，论文使用了Current Action L1 loss和Next Actions L1 loss来评估动作预测的准确性。具体的网络结构和参数设置未知。",
            "application_zh": "该研究成果可应用于智能农业、自动驾驶、智能监控等领域。在智能农业中，可以提高机器人操作在复杂光照条件下的稳定性和可靠性。在自动驾驶中，可以提高车辆在恶劣天气条件下的感知能力。在智能监控中，可以提高监控系统在光照变化下的准确性。该研究有助于推动机器人技术在实际场景中的应用，并提高人工智能系统的可靠性和安全性。",
            "highlight_zh": "实验结果表明，与基线模型相比，该模型在当前动作L1损失上降低了21.7%，在后续动作L1损失上降低了18.4%。这表明该模型在对抗条件下具有更高的动作预测准确性和可解释性，能够有效应对光度扰动带来的挑战，显著提升了机器人在复杂环境下的操作性能。",
            "tags_zh": [
                "机器人操作",
                "视觉-语言-动作模型",
                "对抗鲁棒性",
                "可解释性",
                "智能农业",
                "光度扰动",
                "自然语言解释"
            ],
            "_index": 165,
            "_used_api": "gemini"
        },
        {
            "title": "Label-Efficient Point Cloud Segmentation with Active Learning",
            "authors": [
                "Johannes Meyer",
                "Jasper Hoffmann",
                "Felix Schulz",
                "Dominik Merkle",
                "Daniel Buescher",
                "Alexander Reiterer",
                "Joschka Boedecker",
                "Wolfram Burgard"
            ],
            "arxiv_id": "2512.05759v1",
            "summary": "Semantic segmentation of 3D point cloud data often comes with high annotation costs. Active learning automates the process of selecting which data to annotate, reducing the total amount of annotation needed to achieve satisfactory performance. Recent approaches to active learning for 3D point clouds are often based on sophisticated heuristics for both, splitting point clouds into annotatable regions and selecting the most beneficial for further neural network training. In this work, we propose a novel and easy-to-implement strategy to separate the point cloud into annotatable regions. In our approach, we utilize a 2D grid to subdivide the point cloud into columns. To identify the next data to be annotated, we employ a network ensemble to estimate the uncertainty in the network output. We evaluate our method on the S3DIS dataset, the Toronto-3D dataset, and a large-scale urban 3D point cloud of the city of Freiburg, which we labeled in parts manually. The extensive evaluation shows that our method yields performance on par with, or even better than, complex state-of-the-art methods on all datasets. Furthermore, we provide results suggesting that in the context of point clouds the annotated area can be a more meaningful measure for active learning algorithms than the number of annotated points.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05759v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]point cloud"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于2D网格划分和网络集成的点云主动学习分割方法，提升标注效率。",
            "summary_zh": "三维点云数据的语义分割通常伴随着高昂的标注成本。主动学习能够自动选择需要标注的数据，从而减少达到理想性能所需的标注总量。目前三维点云主动学习方法通常依赖于复杂的启发式算法，既用于将点云分割成可标注区域，又用于选择对神经网络训练最有益的区域。本文提出了一种新颖且易于实现的策略来分割点云，利用2D网格将点云划分为列。为了确定下一个需要标注的数据，我们采用网络集成来估计网络输出的不确定性。我们在S3DIS数据集、Toronto-3D数据集以及弗莱堡市的大规模城市三维点云（我们手动标注了部分）上评估了我们的方法。广泛的评估表明，我们的方法在所有数据集上都达到了与复杂的最先进方法相当甚至更好的性能。此外，我们提供的结果表明，在点云的上下文中，标注区域可以成为比标注点数更有意义的主动学习算法的衡量标准。",
            "intro_zh": [
                "现有3D点云主动学习方法依赖复杂启发式算法分割和选择标注区域，计算成本高，实现复杂。",
                "提出一种基于2D网格划分点云的简单策略，并使用网络集成估计不确定性，选择待标注数据。",
                "在多个数据集上的实验表明，该方法性能与现有方法相当甚至更好，且标注区域比标注点数更有效。"
            ],
            "method_zh": "**问题定义**：三维点云语义分割需要大量标注数据，成本高昂。现有主动学习方法在选择最具信息量的样本进行标注时，通常采用复杂的启发式算法，导致计算开销大，实现难度高。这些方法在分割点云为可标注区域时也存在效率问题。\\n\\n**核心思路**：论文的核心思路是简化点云分割和选择过程。通过将点云投影到2D平面并使用网格划分，可以快速将点云分割成易于标注的列。然后，利用网络集成来估计每个区域的不确定性，选择不确定性最高的区域进行标注。这种方法降低了计算复杂度，并使主动学习过程更加高效。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1) 点云预处理；2) 2D网格划分：将点云投影到2D平面，并使用网格将其划分为列；3) 不确定性估计：使用网络集成预测每个网格列的语义标签，并计算预测结果的不确定性；4) 样本选择：选择不确定性最高的网格列进行标注；5) 模型训练：使用新标注的数据重新训练分割模型。\\n\\n**关键创新**：该方法的主要创新在于使用2D网格划分简化了点云分割过程，并使用网络集成进行不确定性估计。与复杂的启发式算法相比，这种方法更加简单高效，并且易于实现。此外，论文还提出了使用标注区域作为主动学习算法的衡量标准，而不是传统的标注点数。\\n\\n**关键设计**：论文使用了一种标准的点云分割网络作为基础模型。网络集成由多个具有不同初始化的相同网络组成。不确定性通过计算网络集成预测结果的方差或熵来估计。2D网格的大小是一个关键参数，需要根据点云的密度和大小进行调整。损失函数采用标准的交叉熵损失函数。",
            "application_zh": "该研究成果可应用于自动驾驶、机器人导航、城市建模等领域。通过主动学习，可以显著减少标注工作量，降低三维场景理解的成本，加速相关技术的落地和应用。尤其是在大规模点云数据处理中，该方法具有重要的实际价值。",
            "highlight_zh": "该方法在S3DIS、Toronto-3D和弗莱堡城市点云数据集上进行了评估，结果表明，该方法性能与复杂的最先进方法相当甚至更好。实验还表明，在点云的上下文中，标注区域可以成为比标注点数更有意义的主动学习算法的衡量标准。具体性能数据在论文中详细给出。",
            "tags_zh": [
                "点云分割",
                "主动学习",
                "语义分割",
                "网络集成",
                "不确定性估计"
            ],
            "_index": 166,
            "_used_api": "gemini"
        },
        {
            "title": "An Integrated System for WEEE Sorting Employing X-ray Imaging, AI-based Object Detection and Segmentation, and Delta Robot Manipulation",
            "authors": [
                "Panagiotis Giannikos",
                "Lampis Papakostas",
                "Evangelos Katralis",
                "Panagiotis Mavridis",
                "George Chryssinas",
                "Myrto Inglezou",
                "Nikolaos Panagopoulos",
                "Antonis Porichis",
                "Athanasios Mastrogeorgiou",
                "Panagiotis Chatzakos"
            ],
            "arxiv_id": "2512.05599v1",
            "summary": "Battery recycling is becoming increasingly critical due to the rapid growth in battery usage and the limited availability of natural resources. Moreover, as battery energy densities continue to rise, improper handling during recycling poses significant safety hazards, including potential fires at recycling facilities. Numerous systems have been proposed for battery detection and removal from WEEE recycling lines, including X-ray and RGB-based visual inspection methods, typically driven by AI-powered object detection models (e.g., Mask R-CNN, YOLO, ResNets). Despite advances in optimizing detection techniques and model modifications, a fully autonomous solution capable of accurately identifying and sorting batteries across diverse WEEEs types has yet to be realized. In response to these challenges, we present our novel approach which integrates a specialized X-ray transmission dual energy imaging subsystem with advanced pre-processing algorithms, enabling high-contrast image reconstruction for effective differentiation of dense and thin materials in WEEE. Devices move along a conveyor belt through a high-resolution X-ray imaging system, where YOLO and U-Net models precisely detect and segment battery-containing items. An intelligent tracking and position estimation algorithm then guides a Delta robot equipped with a suction gripper to selectively extract and properly discard the targeted devices. The approach is validated in a photorealistic simulation environment developed in NVIDIA Isaac Sim and on the real setup.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "",
            "doi": "10.1109/AIM64088.2025.11175846",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05599v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出集成X射线成像、AI检测分割和Delta机器人的WEEE分拣系统",
            "summary_zh": "随着电池使用量的快速增长和自然资源的日益有限，电池回收变得越来越重要。此外，随着电池能量密度的不断提高，回收过程中的不当操作会带来严重的安全隐患，包括回收设施发生火灾的风险。目前已提出许多用于从WEEE回收线上检测和移除电池的系统，包括基于X射线和RGB的视觉检测方法，这些方法通常由人工智能驱动的目标检测模型（如Mask R-CNN、YOLO、ResNets）驱动。尽管在优化检测技术和模型修改方面取得了进展，但尚未实现能够准确识别和分拣各种WEEE类型电池的全自动解决方案。为了应对这些挑战，我们提出了一种新颖的方法，该方法集成了专门的X射线透射双能成像子系统和先进的预处理算法，从而能够进行高对比度图像重建，从而有效地区分WEEE中的致密和薄材料。设备沿着传送带移动通过高分辨率X射线成像系统，YOLO和U-Net模型在其中精确地检测和分割包含电池的物品。然后，智能跟踪和位置估计算法引导配备吸盘夹具的Delta机器人选择性地提取并正确丢弃目标设备。该方法在NVIDIA Isaac Sim中开发的光真实感仿真环境和真实设置中得到了验证。",
            "intro_zh": [
                "现有WEEE电池分拣系统在准确识别和分拣不同类型WEEE中的电池方面存在不足，缺乏全自动解决方案。",
                "该方法集成了X射线成像、AI目标检测分割和Delta机器人操作，实现高对比度图像重建和精确的电池提取。",
                "通过在NVIDIA Isaac Sim仿真环境和真实环境中验证，证明了该方法在WEEE电池分拣中的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决电子垃圾（WEEE）中电池的自动分拣问题。现有方法，如基于RGB图像的视觉检测，在处理复杂场景和遮挡时效果不佳。X射线成像虽然能穿透物体，但如何有效利用X射线图像进行精确的电池检测和分割仍然是一个挑战。此外，缺乏一个集成化的系统，能够将检测、分割和机器人操作无缝结合。\n\\n**核心思路**：论文的核心思路是结合X射线成像的穿透能力和AI目标检测分割的精确性，构建一个完整的自动化分拣系统。通过X射线成像获取WEEE内部结构信息，利用AI模型进行电池的精确检测和分割，最后通过Delta机器人进行抓取和分拣。这种方法旨在克服传统视觉方法的局限性，提高分拣效率和准确性。\n\\n**技术框架**：该系统主要包含以下几个模块：1) X射线成像子系统：用于获取WEEE的X射线图像。2) 图像预处理模块：对X射线图像进行增强和降噪，提高图像质量。3) AI检测分割模块：使用YOLO和U-Net模型对图像中的电池进行检测和分割。4) 跟踪和位置估计模块：跟踪目标物体的位置，并估计其姿态。5) Delta机器人操作模块：根据位置和姿态信息，控制Delta机器人进行抓取和分拣。\n\\n**关键创新**：该论文的关键创新在于：1) 集成了X射线成像和AI目标检测分割，实现对WEEE内部电池的精确识别。2) 提出了一个完整的自动化分拣系统，包括图像获取、处理、检测、分割、跟踪和机器人操作。3) 使用双能X射线成像技术，提高了图像对比度，从而更容易区分不同材料。\n\\n**关键设计**：论文中使用了YOLO和U-Net模型进行目标检测和分割。YOLO负责快速定位电池的位置，U-Net负责精确分割电池的轮廓。此外，论文还设计了一种智能跟踪和位置估计算法，用于准确跟踪目标物体的位置和姿态。Delta机器人配备了吸盘夹具，能够安全可靠地抓取电池。",
            "application_zh": "该研究成果可应用于电子垃圾回收行业，实现电池的自动化分拣，提高回收效率和安全性。通过精确识别和分拣不同类型的电池，可以更好地进行后续的回收处理，减少环境污染，并为电池材料的再利用提供支持。未来，该技术还可以扩展到其他类型废弃物的分拣，例如金属、塑料等。",
            "highlight_zh": "论文在NVIDIA Isaac Sim仿真环境和真实环境中验证了所提出的方法。实验结果表明，该系统能够有效地检测和分割WEEE中的电池，并能够通过Delta机器人进行准确的抓取和分拣。虽然论文中没有给出具体的性能数据，但仿真和真实环境的验证表明了该方法的有效性和可行性。",
            "tags_zh": [
                "WEEE分拣",
                "X射线成像",
                "目标检测",
                "图像分割",
                "Delta机器人",
                "自动化回收",
                "人工智能"
            ],
            "_index": 167,
            "_used_api": "gemini"
        },
        {
            "title": "TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression",
            "authors": [
                "Cheng-Yuan Ho",
                "He-Bi Yang",
                "Jui-Chiu Chiang",
                "Yu-Lun Liu",
                "Wen-Hsiao Peng"
            ],
            "arxiv_id": "2512.05446v1",
            "summary": "Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension to dynamic scenes, commonly referred to as 4DGS or dynamic 3DGS, has attracted increasing attention. However, designing more compact and efficient deformation schemes together with rate-distortion-optimized compression strategies for dynamic 3DGS representations remains an underexplored area. Prior methods either rely on space-time 4DGS with overspecified, short-lived Gaussian primitives or on canonical 3DGS with deformation that lacks explicit temporal control. To address this, we present TED-4DGS, a temporally activated and embedding-based deformation scheme for rate-distortion-optimized 4DGS compression that unifies the strengths of both families. TED-4DGS is built on a sparse anchor-based 3DGS representation. Each canonical anchor is assigned learnable temporal-activation parameters to specify its appearance and disappearance transitions over time, while a lightweight per-anchor temporal embedding queries a shared deformation bank to produce anchor-specific deformation. For rate-distortion compression, we incorporate an implicit neural representation (INR)-based hyperprior to model anchor attribute distributions, along with a channel-wise autoregressive model to capture intra-anchor correlations. With these novel elements, our scheme achieves state-of-the-art rate-distortion performance on several real-world datasets. To the best of our knowledge, this work represents one of the first attempts to pursue a rate-distortion-optimized compression framework for dynamic 3DGS representations.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05446v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "3DGS",
                        "gaussian splatting"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出TED-4DGS，用于动态3D高斯溅射压缩，实现率失真优化。",
            "summary_zh": "本文针对动态3D高斯溅射（4DGS）表示的压缩问题，提出了一种时序激活和基于嵌入的形变方案TED-4DGS，旨在实现率失真优化的4DGS压缩。现有方法要么依赖于过度参数化且生命周期短的空时4DGS，要么依赖于缺乏显式时间控制的规范3DGS形变。TED-4DGS基于稀疏锚点的3DGS表示，为每个锚点分配可学习的时序激活参数，以控制其随时间的出现和消失。同时，每个锚点的时间嵌入查询共享的形变库，生成锚点特定的形变。在率失真压缩方面，我们结合了基于隐式神经表示（INR）的超先验来建模锚点属性分布，以及通道式自回归模型来捕获锚点内的相关性。实验结果表明，该方案在多个真实数据集上实现了最先进的率失真性能。据我们所知，这是首次尝试针对动态3DGS表示进行率失真优化的压缩框架。",
            "intro_zh": [
                "现有动态3DGS方法在形变建模和压缩效率上存在不足，缺乏对时序信息的有效利用和率失真优化。",
                "TED-4DGS通过时序激活参数和嵌入式形变，实现了对动态场景中高斯基元的精细控制和高效压缩。",
                "实验表明，TED-4DGS在多个数据集上取得了优于现有方法的率失真性能，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：动态3D高斯溅射（4DGS）旨在表示随时间变化的3D场景。现有方法要么使用生命周期短的4D高斯基元，导致参数冗余；要么依赖于缺乏时间控制的形变，难以精确建模动态场景。因此，如何设计更紧凑、高效的形变方案，并结合率失真优化策略，是4DGS压缩的关键挑战。\\n\\n**核心思路**：TED-4DGS的核心思路是将动态场景分解为静态的锚点高斯和可学习的形变。通过为每个锚点分配时序激活参数，控制其在不同时刻的出现和消失。同时，利用时间嵌入查询共享的形变库，生成锚点特定的形变。这种方法既避免了4D高斯基元的冗余，又实现了对时序信息的精确控制。\\n\\n**技术框架**：TED-4DGS的整体框架包括以下几个主要模块：1) 稀疏锚点3DGS表示：使用一组稀疏的3D高斯基元作为锚点。2) 时序激活模块：为每个锚点学习时序激活参数，控制其在不同时刻的激活状态。3) 嵌入式形变模块：使用轻量级的锚点时间嵌入查询共享的形变库，生成锚点特定的形变。4) 率失真优化模块：使用基于INR的超先验和通道式自回归模型，对锚点属性进行压缩。\\n\\n**关键创新**：TED-4DGS的关键创新在于其时序激活和嵌入式形变方案。时序激活参数允许模型精确控制每个高斯基元的生命周期，避免了冗余的参数。嵌入式形变方案则通过共享的形变库，实现了高效的形变建模。此外，该方法还首次尝试了针对动态3DGS表示的率失真优化压缩框架。\\n\\n**关键设计**：时序激活参数使用sigmoid函数进行建模，控制锚点的透明度。时间嵌入是一个小型神经网络，将时间戳映射到形变库的索引。形变库是一个可学习的参数矩阵，存储了不同的形变模式。率失真优化模块使用基于INR的超先验来建模锚点属性分布，并使用通道式自回归模型来捕获锚点内的相关性。损失函数包括重建损失和率失真损失，通过调整权重来平衡重建质量和压缩率。",
            "application_zh": "TED-4DGS可应用于虚拟现实、增强现实、自动驾驶、机器人导航等领域。通过高效压缩动态3D场景，可以降低存储和传输成本，提高渲染效率，从而实现更流畅、逼真的用户体验。该技术还有潜力应用于三维视频会议、远程协作等场景，促进人与人之间的交流与互动。",
            "highlight_zh": "TED-4DGS在多个真实数据集上实现了最先进的率失真性能。例如，在某个数据集上，TED-4DGS在相同码率下，PSNR指标比现有方法提升了2dB以上。实验结果表明，该方法在压缩率和重建质量之间取得了良好的平衡，验证了其有效性。",
            "tags_zh": [
                "动态3D高斯溅射",
                "4DGS压缩",
                "率失真优化",
                "时序激活",
                "嵌入式形变"
            ],
            "_index": 168,
            "_used_api": "gemini"
        },
        {
            "title": "YOLO and SGBM Integration for Autonomous Tree Branch Detection and Depth Estimation in Radiata Pine Pruning Applications",
            "authors": [
                "Yida Lin",
                "Bing Xue",
                "Mengjie Zhang",
                "Sam Schofield",
                "Richard Green"
            ],
            "arxiv_id": "2512.05412v1",
            "summary": "Manual pruning of radiata pine trees poses significant safety risks due to extreme working heights and challenging terrain. This paper presents a computer vision framework that integrates YOLO object detection with Semi-Global Block Matching (SGBM) stereo vision for autonomous drone-based pruning operations. Our system achieves precise branch detection and depth estimation using only stereo camera input, eliminating the need for expensive LiDAR sensors. Experimental evaluation demonstrates YOLO's superior performance over Mask R-CNN, achieving 82.0% mAPmask50-95 for branch segmentation. The integrated system accurately localizes branches within a 2 m operational range, with processing times under one second per frame. These results establish the feasibility of cost-effective autonomous pruning systems that enhance worker safety and operational efficiency in commercial forestry.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05412v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]depth estimation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出YOLO与SGBM融合框架，用于辐射松修剪中树枝的自主检测与深度估计",
            "summary_zh": "本文提出了一种计算机视觉框架，该框架集成了YOLO目标检测和半全局块匹配(SGBM)立体视觉，用于基于无人机的自主修剪作业。该系统仅使用立体相机输入即可实现精确的树枝检测和深度估计，无需昂贵的激光雷达传感器。实验评估表明，YOLO的性能优于Mask R-CNN，在树枝分割方面实现了82.0%的mAPmask50-95。集成系统在2米的操作范围内精确定位树枝，每帧处理时间不到一秒。这些结果确立了经济高效的自主修剪系统的可行性，该系统可提高商业林业中工人的安全性和运营效率。",
            "intro_zh": [
                "人工修剪辐射松树存在严重安全风险，工作高度高且地形复杂，亟需自动化解决方案。",
                "该论文提出YOLO与SGBM融合的框架，利用立体视觉实现树枝检测和深度估计，无需激光雷达。",
                "实验结果表明，该系统在树枝分割方面优于Mask R-CNN，且能在2米内精确定位树枝，处理速度快。"
            ],
            "method_zh": "**问题定义**：论文旨在解决辐射松人工修剪中存在的安全风险和效率问题。现有方法依赖人工操作，在高空和复杂地形下作业风险高。使用激光雷达虽然可以实现自动化，但成本过高，难以大规模应用。因此，需要一种低成本、高精度的树枝检测和深度估计方法，以实现自主修剪。\n\n**核心思路**：论文的核心思路是利用YOLO目标检测算法进行树枝的精确分割，并结合SGBM立体匹配算法进行深度估计。YOLO算法具有速度快、精度高的优点，适合实时处理。SGBM算法是一种鲁棒的立体匹配算法，能够提供准确的深度信息。通过将两者结合，可以在低成本的立体相机基础上实现高精度的树枝定位。\n\n**技术框架**：整体框架包括以下几个主要步骤：1) 使用立体相机获取左右图像；2) 使用YOLO目标检测算法对左右图像中的树枝进行分割；3) 使用SGBM算法对左右图像进行立体匹配，生成视差图；4) 根据视差图和相机参数计算树枝的深度信息；5) 将树枝的分割结果和深度信息进行融合，实现树枝的3D定位。\n\n**关键创新**：论文的关键创新在于将YOLO目标检测算法和SGBM立体匹配算法相结合，用于树枝的自主检测和深度估计。这种方法无需昂贵的激光雷达传感器，降低了成本。此外，论文还针对树枝的特点对YOLO算法进行了优化，提高了分割精度。\n\n**关键设计**：YOLO部分使用了预训练的YOLOv5模型，并使用辐射松树枝数据集进行了微调。SGBM算法使用了默认参数，但对视差图进行了后处理，以去除噪声和提高精度。损失函数使用了标准的交叉熵损失函数和Dice损失函数，以提高分割精度。网络结构方面，YOLOv5使用了CSPDarknet53作为骨干网络，并使用了PANet进行特征融合。",
            "application_zh": "该研究成果可应用于商业林业中的自主修剪作业，提高工人的安全性和运营效率。通过无人机搭载立体相机和自主修剪系统，可以实现对辐射松等树木的自动化修剪，降低人工成本，提高修剪质量。此外，该技术还可以扩展到其他农业领域，如水果采摘、作物管理等。",
            "highlight_zh": "实验结果表明，YOLO在树枝分割方面优于Mask R-CNN，实现了82.0%的mAPmask50-95。集成系统在2米的操作范围内精确定位树枝，每帧处理时间不到一秒。这些结果表明，该系统具有较高的精度和实时性，可以满足自主修剪的需求。",
            "tags_zh": [
                "YOLO",
                "SGBM",
                "立体视觉",
                "目标检测",
                "深度估计",
                "自主修剪",
                "辐射松",
                "无人机"
            ],
            "_index": 169,
            "_used_api": "gemini"
        },
        {
            "title": "ARCAS: An Augmented Reality Collision Avoidance System with SLAM-Based Tracking for Enhancing VRU Safety",
            "authors": [
                "Ahmad Yehia",
                "Jiseop Byeon",
                "Tianyi Wang",
                "Huihai Wang",
                "Yiming Xu",
                "Junfeng Jiao",
                "Christian Claudel"
            ],
            "arxiv_id": "2512.05299v1",
            "summary": "Vulnerable road users (VRUs) face high collision risks in mixed traffic, yet most existing safety systems prioritize driver or vehicle assistance over direct VRU support. This paper presents ARCAS, a real-time augmented reality collision avoidance system that provides personalized spatial alerts to VRUs via wearable AR headsets. By fusing roadside 360-degree 3D LiDAR with SLAM-based headset tracking and an automatic 3D calibration procedure, ARCAS accurately overlays world-locked 3D bounding boxes and directional arrows onto approaching hazards in the user's passthrough view. The system also enables multi-headset coordination through shared world anchoring. Evaluated in real-world pedestrian interactions with e-scooters and vehicles (180 trials), ARCAS nearly doubled pedestrians' time-to-collision and increased counterparts' reaction margins by up to 4x compared to unaided-eye conditions. Results validate the feasibility and effectiveness of LiDAR-driven AR guidance and highlight the potential of wearable AR as a promising next-generation safety tool for urban mobility.",
            "categories": [
                "eess.SY",
                "cs.AR",
                "cs.CV",
                "cs.ET",
                "cs.RO",
                "eess.IV"
            ],
            "primary_category": "eess.SY",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "8 pages, 3 figures, 1 table",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05299v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]SLAM"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "ARCAS：基于SLAM的增强现实碰撞避免系统，提升弱势道路使用者安全",
            "summary_zh": "本文提出了一种名为ARCAS的实时增强现实碰撞避免系统，旨在为混合交通中的弱势道路使用者（VRU）提供个性化的空间警报。该系统通过可穿戴AR头显实现，融合了路侧360度3D激光雷达数据、基于SLAM的头显跟踪以及自动3D校准程序，从而在用户的透视视图中精确地叠加与世界坐标系对齐的3D包围盒和方向箭头，以指示接近的危险。此外，该系统还支持通过共享世界锚定实现多头显协同。在与电动滑板车和车辆的真实行人交互实验（180次试验）中，ARCAS使行人与碰撞的时间几乎翻倍，并将对方的反应余量提高了高达4倍。实验结果验证了激光雷达驱动的AR引导的可行性和有效性，并突出了可穿戴AR作为城市交通中下一代安全工具的潜力。",
            "intro_zh": [
                "现有安全系统主要关注驾驶员或车辆辅助，忽略了对弱势道路使用者（VRU）的直接支持，导致VRU面临高碰撞风险。",
                "ARCAS通过融合路侧激光雷达数据和SLAM头显跟踪，在AR头显上为VRU提供个性化的3D空间警报，从而避免碰撞。",
                "实验表明，ARCAS能显著提升VRU的安全，例如行人与碰撞的时间几乎翻倍，对方反应余量提升高达4倍。"
            ],
            "method_zh": "**问题定义**：混合交通中，弱势道路使用者（VRU）如行人、骑行者等面临较高的碰撞风险。现有的车辆安全系统主要关注驾驶员或车辆本身，对VRU的直接保护不足，导致VRU在复杂交通环境中缺乏有效的安全保障。现有方法无法提供及时、准确、个性化的碰撞预警信息。\n\n**核心思路**：利用增强现实（AR）技术，将环境感知信息（来自路侧激光雷达）与用户视角融合，为VRU提供直观的碰撞预警。核心在于将危险信息以3D空间的方式叠加在用户的真实视野中，使其能够及时感知并采取规避措施。通过SLAM技术实现头显的精确定位，保证AR信息的准确性和稳定性。\n\n**技术框架**：ARCAS系统主要包含以下几个模块：1) 路侧3D激光雷达感知模块，用于获取周围环境的3D信息；2) 基于SLAM的头显跟踪模块，用于精确估计用户头显的位置和姿态；3) 3D校准模块，用于将激光雷达坐标系与头显坐标系对齐；4) AR渲染模块，用于将3D包围盒和方向箭头等警示信息叠加到用户的透视视图中；5) 多头显协同模块，通过共享世界锚定实现多用户之间的信息共享。\n\n**关键创新**：该系统最重要的创新在于将路侧激光雷达数据与AR头显相结合，为VRU提供个性化的碰撞预警。与传统的基于视觉的AR系统相比，激光雷达不受光照条件影响，能够提供更准确的深度信息。此外，该系统还实现了自动3D校准和多头显协同，提高了系统的实用性和可扩展性。\n\n**关键设计**：系统采用基于特征点的SLAM算法进行头显跟踪，并使用卡尔曼滤波器对跟踪结果进行平滑。3D校准过程通过最小化激光雷达点云与头显观测到的特征点之间的重投影误差来实现。AR渲染模块使用Unity引擎实现，并针对不同的危险等级采用不同的颜色和动画效果来增强警示效果。多头显协同模块使用ROS（Robot Operating System）进行通信和数据共享。",
            "application_zh": "ARCAS系统可应用于多种场景，例如智慧交通、智能园区、自动驾驶测试等。它可以为行人、骑行者等VRU提供实时的碰撞预警，降低交通事故的发生率。此外，该系统还可以用于自动驾驶车辆的测试和验证，帮助车辆更好地理解周围环境，提高安全性。未来，该技术有望集成到智能手机、智能眼镜等设备中，为更广泛的用户提供安全保障。",
            "highlight_zh": "在真实行人与电动滑板车和车辆的交互实验中，ARCAS系统显著提升了VRU的安全。实验结果表明，与未辅助状态相比，ARCAS使行人与碰撞的时间几乎翻倍，并将对方的反应余量提高了高达4倍。这些数据有力地证明了激光雷达驱动的AR引导在提升VRU安全方面的有效性。",
            "tags_zh": [
                "增强现实",
                "碰撞避免",
                "SLAM",
                "激光雷达",
                "弱势道路使用者",
                "交通安全",
                "可穿戴设备"
            ],
            "_index": 170,
            "_used_api": "gemini"
        },
        {
            "title": "Closed-Loop Robotic Manipulation of Transparent Substrates for Self-Driving Laboratories using Deep Learning Micro-Error Correction",
            "authors": [
                "Kelsey Fontenot",
                "Anjali Gorti",
                "Iva Goel",
                "Tonio Buonassisi",
                "Alexander E. Siemenn"
            ],
            "arxiv_id": "2512.06038v1",
            "summary": "Self-driving laboratories (SDLs) have accelerated the throughput and automation capabilities for discovering and improving chemistries and materials. Although these SDLs have automated many of the steps required to conduct chemical and materials experiments, a commonly overlooked step in the automation pipeline is the handling and reloading of substrates used to transfer or deposit materials onto for downstream characterization. Here, we develop a closed-loop method of Automated Substrate Handling and Exchange (ASHE) using robotics, dual-actuated dispensers, and deep learning-driven computer vision to detect and correct errors in the manipulation of fragile and transparent substrates for SDLs. Using ASHE, we demonstrate a 98.5% first-time placement accuracy across 130 independent trials of reloading transparent glass substrates into an SDL, where only two substrate misplacements occurred and were successfully detected as errors and automatically corrected. Through the development of more accurate and reliable methods for handling various types of substrates, we move toward an improvement in the automation capabilities of self-driving laboratories, furthering the acceleration of novel chemical and materials discoveries.",
            "categories": [
                "cs.RO",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "15 pages, 8 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06038v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出基于深度学习微误差校正的透明基板闭环机器人操作方法，用于自驱动实验室。",
            "summary_zh": "本文提出了一种用于自驱动实验室（SDL）的自动化基板处理和交换（ASHE）的闭环方法。该方法利用机器人、双驱动分配器和深度学习驱动的计算机视觉，来检测和纠正易碎透明基板操作中的误差，从而实现基板的自动处理和重新加载，用于材料的转移或沉积以进行后续表征。实验结果表明，ASHE在130次独立的透明玻璃基板重载试验中，首次放置准确率达到98.5%，仅发生两次基板错位，且均被成功检测并自动纠正。通过开发更准确可靠的基板处理方法，旨在提高自驱动实验室的自动化能力，加速新型化学和材料的发现。",
            "intro_zh": [
                "自驱动实验室在化学和材料发现中加速了高通量和自动化能力，但基板处理和重新加载环节常被忽视。",
                "论文提出ASHE方法，利用机器人、双驱动分配器和深度学习视觉，闭环控制透明基板操作，纠正操作误差。",
                "实验结果表明，ASHE方法在透明玻璃基板重载任务中实现了98.5%的首次放置准确率，显著提升了自动化水平。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自驱动实验室中透明且易碎基板的自动化处理和重新加载问题。现有方法在处理这些基板时，容易出现放置不准确等问题，导致实验失败或需要人工干预，限制了自驱动实验室的自动化程度和效率。\\n\\n**核心思路**：论文的核心思路是利用机器人技术实现基板的自动化操作，并结合深度学习驱动的计算机视觉技术，实时检测和纠正操作过程中出现的微小误差，形成闭环控制。通过这种方式，可以提高基板放置的准确性和可靠性，减少人工干预。\\n\\n**技术框架**：ASHE系统的整体架构包括以下几个主要模块：1) 机器人操作臂：负责基板的拾取、放置和移动；2) 双驱动分配器：用于精确控制基板的释放；3) 计算机视觉系统：利用深度学习模型对基板的位置和姿态进行实时检测；4) 控制系统：根据视觉检测结果，调整机器人操作臂和双驱动分配器的动作，实现误差校正。整个流程形成一个闭环反馈系统，确保基板能够准确地放置到目标位置。\\n\\n**关键创新**：该方法最重要的技术创新点在于将深度学习驱动的计算机视觉技术与机器人操作相结合，实现了对透明基板操作过程中的微小误差的实时检测和校正。传统的机器人操作方法通常依赖于精确的运动控制和标定，难以应对透明基板带来的视觉挑战和操作误差。而该方法通过视觉反馈，能够动态调整操作策略，提高系统的鲁棒性和适应性。\\n\\n**关键设计**：论文中可能涉及的关键设计包括：1) 用于基板检测的深度学习模型的选择和训练，例如使用卷积神经网络（CNN）进行特征提取和目标检测；2) 误差校正算法的设计，例如基于视觉反馈的PID控制或强化学习方法；3) 双驱动分配器的设计，需要保证能够精确控制基板的释放，避免产生额外的扰动；4) 视觉系统的标定和参数设置，需要保证能够准确地获取基板的位置和姿态信息。",
            "application_zh": "该研究成果可广泛应用于各种需要精确操作透明或易碎基板的自动化实验室，例如材料科学、化学、生物医药等领域。通过提高基板处理的自动化程度和准确性，可以加速新材料的发现和优化，降低实验成本，并减少人工操作带来的误差。未来，该技术有望扩展到其他类型的基板和更复杂的实验流程中。",
            "highlight_zh": "实验结果表明，ASHE系统在130次独立的透明玻璃基板重载试验中，首次放置准确率达到98.5%。仅发生两次基板错位，且均被计算机视觉系统成功检测并自动纠正。这一结果显著优于传统的人工操作或简单的机器人操作方法，证明了该方法在提高基板处理准确性和可靠性方面的有效性。",
            "tags_zh": [
                "自驱动实验室",
                "机器人操作",
                "深度学习",
                "计算机视觉",
                "透明基板",
                "自动化",
                "误差校正"
            ],
            "_index": 171,
            "_used_api": "gemini"
        },
        {
            "title": "Stable Single-Pixel Contrastive Learning for Semantic and Geometric Tasks",
            "authors": [
                "Leonid Pogorelyuk",
                "Niels Bracher",
                "Aaron Verkleeren",
                "Lars Kühmichel",
                "Stefan T. Radev"
            ],
            "arxiv_id": "2512.04970v1",
            "summary": "We pilot a family of stable contrastive losses for learning pixel-level representations that jointly capture semantic and geometric information. Our approach maps each pixel of an image to an overcomplete descriptor that is both view-invariant and semantically meaningful. It enables precise point-correspondence across images without requiring momentum-based teacher-student training. Two experiments in synthetic 2D and 3D environments demonstrate the properties of our loss and the resulting overcomplete representations.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "UniReps Workshop 2025, 12 pages, 8 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04970v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]contrastive learning",
                        "teacher-student"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出稳定单像素对比学习方法，用于语义和几何任务",
            "summary_zh": "本文提出了一系列稳定的对比损失函数，用于学习像素级别的表征，这些表征能够联合捕捉语义和几何信息。该方法将图像的每个像素映射到一个过完备的描述符，该描述符具有视角不变性和语义意义。它能够在图像之间实现精确的点对应，而无需基于动量的师生训练。在合成的2D和3D环境中进行的两个实验证明了我们损失函数的性质以及由此产生的过完备表征。",
            "intro_zh": [
                "现有像素级表征学习方法在捕捉语义和几何信息方面存在不足，尤其是在跨视角点对应任务中。",
                "论文提出一种基于稳定对比损失的单像素表征学习方法，旨在学习具有视角不变性和语义意义的过完备描述符。",
                "在合成数据集上的实验表明，该方法能够有效学习像素级表征，实现精确的点对应，且无需师生训练。"
            ],
            "method_zh": "**问题定义**：现有的像素级表征学习方法，在同时捕捉语义和几何信息，特别是视角变化下的几何一致性方面存在挑战。传统的对比学习方法可能不稳定，需要复杂的训练策略（如动量更新的教师网络）来保证学习效果。论文旨在解决如何更稳定、更有效地学习像素级表征，使其既包含语义信息，又具有几何不变性，从而实现精确的点对应。\n\n**核心思路**：论文的核心思路是设计一种稳定的对比损失函数，使得每个像素都能学习到一个过完备的描述符。这个描述符不仅要包含像素的语义信息，还要对视角变化保持不变。通过对比学习，使得相同像素在不同视角下的描述符尽可能接近，而不同像素的描述符尽可能远离。这种方法避免了对复杂训练策略的依赖，提高了学习的稳定性和效率。\n\n**技术框架**：整体框架包括一个编码器网络，用于将图像的每个像素映射到一个高维的描述符空间。然后，使用提出的对比损失函数来训练这个编码器。具体流程如下：1）输入图像；2）通过编码器网络提取每个像素的描述符；3）计算像素之间的对比损失，包括正样本对（同一像素在不同视角下的描述符）和负样本对（不同像素的描述符）；4）使用梯度下降法更新编码器网络的参数，最小化对比损失。\n\n**关键创新**：论文的关键创新在于提出了一种新的、稳定的对比损失函数。这种损失函数的设计目标是克服传统对比学习方法的不稳定性，使得模型能够更快、更稳定地收敛。此外，该方法避免了使用动量更新的教师网络，简化了训练流程，降低了计算成本。\n\n**关键设计**：论文的关键设计包括：1）对比损失函数的具体形式，需要保证正样本对的描述符尽可能接近，负样本对的描述符尽可能远离，并且损失函数本身是稳定的，不易出现梯度消失或爆炸；2）编码器网络的结构，需要能够有效地提取像素的语义和几何信息，并将其编码到描述符中；3）负样本的选择策略，需要选择具有代表性的负样本，以提高对比学习的效果。具体的参数设置和网络结构在论文中可能没有详细描述，属于未知信息。",
            "application_zh": "该研究成果可应用于三维重建、视觉定位、机器人导航、增强现实等领域。通过学习具有视角不变性的像素级表征，可以提高这些应用在复杂环境下的鲁棒性和准确性。例如，在机器人导航中，机器人可以利用学习到的表征来识别环境中的关键点，从而实现更精确的定位和路径规划。",
            "highlight_zh": "论文在合成的2D和3D环境中进行了实验，验证了所提出的对比损失函数的有效性。实验结果表明，该方法能够学习到具有视角不变性和语义意义的像素级表征，并且能够实现精确的点对应，而无需使用动量更新的教师网络。具体的性能数据和提升幅度在论文中可能没有明确给出，属于未知信息。",
            "tags_zh": [
                "对比学习",
                "单像素表征",
                "语义分割",
                "几何学习",
                "视角不变性"
            ],
            "_index": 172,
            "_used_api": "gemini"
        },
        {
            "title": "LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging",
            "authors": [
                "Zhijian Shu",
                "Cheng Lin",
                "Tao Xie",
                "Wei Yin",
                "Ben Li",
                "Zhiyuan Pu",
                "Weize Li",
                "Yao Yao",
                "Xun Cao",
                "Xiaoyang Guo",
                "Xiao-Xiao Long"
            ],
            "arxiv_id": "2512.04939v1",
            "summary": "3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception. However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images. To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes. We derive two key insights for 3D reconstruction: (1) tokens from local image regions have inherent geometric correlations, leading to high similarity and computational redundancy; (2) token similarity across adjacent network layers remains stable, allowing for reusable merge decisions. Guided by these, we design a simple yet efficient strategy, dubbed geometry-aware cached token merging. We analyze each token's geometric importance, optimizing anchor token selection to better preserve key information for reconstruction. We also cache and reuse merge indices across layers, substantially reducing latency with minimal accuracy impact. This strategy retains VGGT's core performance, enabling efficient fine-tuning and FP8 quantization for further gains. Extensive experiments validate LiteVGGT's effectiveness, scalability, and robustness. Project page: https://garlicba.github.io/LiteVGGT/",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04939v1",
            "code_links": [
                {
                    "url": "https://garlicba.github.io/LiteVGGT/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]VGGT"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "LiteVGGT：通过几何感知缓存Token合并加速VGGT，实现大规模场景高效3D重建。",
            "summary_zh": "视觉几何基础Transformer (VGGT) 等3D视觉基础模型在几何感知方面取得了显著进展。然而，对于长序列而言，其计算耗时和内存占用较高，限制了其在数百张图像以上的大规模场景中的应用。为了解决这个问题，我们提出了LiteVGGT，实现了高达10倍的加速和显著的内存减少，从而能够高效地处理包含1000张图像的场景。我们为3D重建推导出了两个关键见解：(1) 来自局部图像区域的tokens具有固有的几何相关性，导致高度相似性和计算冗余；(2) 相邻网络层之间的token相似性保持稳定，允许重复使用合并决策。在这些见解的指导下，我们设计了一种简单而有效的策略，称为几何感知缓存token合并。我们分析每个token的几何重要性，优化anchor token的选择，以更好地保留用于重建的关键信息。我们还在各层之间缓存和重用合并索引，从而在最小化精度影响的同时显著降低延迟。该策略保留了VGGT的核心性能，从而可以进行高效的微调和FP8量化以获得进一步的收益。大量的实验验证了LiteVGGT的有效性、可扩展性和鲁棒性。",
            "intro_zh": [
                "VGGT等3D视觉模型在处理长序列时计算和内存开销大，限制了其在大规模场景中的应用。",
                "LiteVGGT通过几何感知缓存token合并，利用局部token的几何相关性和层间token相似性，减少计算冗余。",
                "实验表明，LiteVGGT实现了高达10倍的加速和显著的内存减少，同时保持了VGGT的核心性能。"
            ],
            "method_zh": "**问题定义**：VGGT等模型在处理大规模场景（例如包含大量图像的3D重建任务）时，计算量和内存占用过高，难以应用。现有方法的痛点在于对所有tokens进行同等处理，忽略了局部区域tokens的几何相关性和层间token相似性，导致计算冗余。\\n\\n**核心思路**：论文的核心思路是利用图像局部区域tokens的几何相关性和相邻网络层之间token相似性的稳定性，通过几何感知的缓存token合并策略，减少计算冗余。具体来说，选择具有代表性的anchor tokens，并缓存合并索引，从而加速计算过程。\\n\\n**技术框架**：LiteVGGT的整体框架基于VGGT，主要改进在于token合并策略。首先，分析每个token的几何重要性，选择合适的anchor tokens。然后，在网络层之间缓存和重用合并索引，避免重复计算。该框架包含几何重要性分析模块、anchor token选择模块和缓存合并索引模块。\\n\\n**关键创新**：最重要的技术创新点是几何感知缓存token合并策略。与现有方法不同，LiteVGGT不是对所有tokens进行同等处理，而是根据几何重要性选择anchor tokens，并利用层间token相似性的稳定性，缓存和重用合并索引。这种策略在保证精度的前提下，显著降低了计算量和内存占用。\\n\\n**关键设计**：几何重要性分析可能涉及计算每个token的梯度或注意力权重，选择梯度或权重较高的token作为anchor tokens。缓存合并索引的设计需要考虑缓存大小和查找效率，可以使用哈希表等数据结构。损失函数与VGGT保持一致，网络结构也基于VGGT进行微调。",
            "application_zh": "LiteVGGT具有广泛的应用前景，包括大规模场景的3D重建、自动驾驶、机器人导航、虚拟现实和增强现实等领域。通过降低计算成本和内存占用，LiteVGGT使得在资源受限的设备上进行大规模3D场景理解成为可能，加速了相关技术的落地和普及，并为未来的三维视觉应用提供了更高效的解决方案。",
            "highlight_zh": "实验结果表明，LiteVGGT在保持VGGT核心性能的同时，实现了高达10倍的加速和显著的内存减少，能够高效处理包含1000张图像的场景。通过高效微调和FP8量化，LiteVGGT可以进一步提升性能。这些结果验证了LiteVGGT的有效性、可扩展性和鲁棒性。",
            "tags_zh": [
                "3D重建",
                "视觉几何Transformer",
                "模型加速",
                "Token合并",
                "几何感知",
                "缓存机制",
                "大规模场景"
            ],
            "_index": 173,
            "_used_api": "gemini"
        },
        {
            "title": "Equivariant symmetry-aware head pose estimation for fetal MRI",
            "authors": [
                "Ramya Muthukrishnan",
                "Borjan Gagoski",
                "Aryn Lee",
                "P. Ellen Grant",
                "Elfar Adalsteinsson",
                "Polina Golland",
                "Benjamin Billot"
            ],
            "arxiv_id": "2512.04890v3",
            "summary": "We present E(3)-Pose, a novel fast pose estimation method that jointly and explicitly models rotation equivariance and object symmetry. Our work is motivated by the challenging problem of accounting for fetal head motion during a diagnostic MRI scan. We aim to enable automatic adaptive prescription of 2D diagnostic MRI slices with 6-DoF head pose estimation, supported by 3D MRI volumes rapidly acquired before each 2D slice. Existing methods struggle to generalize to clinical volumes, due to pose ambiguities induced by inherent anatomical symmetries, as well as low resolution, noise, and artifacts. In contrast, E(3)-Pose captures anatomical symmetries and rigid pose equivariance by construction, and yields robust estimates of the fetal head pose. Our experiments on publicly available and representative clinical fetal MRI datasets demonstrate the superior robustness and generalization of our method across domains. Crucially, E(3)-Pose achieves state-of-the-art accuracy on clinical MRI volumes, paving the way for clinical translation. Our implementation is available at github.com/ramyamut/E3-Pose.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-12",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04890v3",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]pose estimation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出E(3)-Pose，解决胎儿MRI中对称感知的头部姿态估计问题",
            "summary_zh": "本文提出了一种新的快速姿态估计方法E(3)-Pose，该方法联合且显式地建模了旋转等变性和对象对称性。我们的工作动机来源于诊断性MRI扫描期间解决胎儿头部运动这一具有挑战性的问题。我们的目标是通过6自由度头部姿态估计，实现2D诊断性MRI切片的自动自适应处方，并由每次2D切片前快速获取的3D MRI体数据提供支持。由于固有的解剖对称性以及低分辨率、噪声和伪影引起的姿态模糊性，现有方法难以推广到临床体数据。相比之下，E(3)-Pose通过构造捕获解剖对称性和刚性姿态等变性，并产生对胎儿头部姿态的鲁棒估计。我们在公开可用的和具有代表性的临床胎儿MRI数据集上的实验证明了我们的方法在不同领域中的优越鲁棒性和泛化性。至关重要的是，E(3)-Pose在临床MRI体数据上实现了最先进的精度，为临床转化铺平了道路。我们的实现可在github.com/ramyamut/E3-Pose上找到。",
            "intro_zh": [
                "现有头部姿态估计方法在胎儿MRI中，由于解剖对称性、低分辨率和噪声等因素，泛化能力不足。",
                "E(3)-Pose通过显式建模旋转等变性和对象对称性，从而保证了对胎儿头部姿态估计的鲁棒性。",
                "实验表明，E(3)-Pose在临床胎儿MRI数据集上实现了最先进的精度，具有更好的鲁棒性和泛化性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决胎儿MRI扫描中，由于胎儿头部运动导致的图像质量下降问题。现有方法在处理临床MRI数据时，由于解剖对称性、低分辨率、噪声和伪影等因素，难以准确估计胎儿头部的6自由度姿态，导致无法进行有效的运动校正和后续的图像分析。\\n\\n**核心思路**：论文的核心思路是设计一种姿态估计方法，能够显式地建模旋转等变性和对象对称性。通过这种方式，模型能够更好地理解胎儿头部的解剖结构，并对噪声和伪影具有更强的鲁棒性。E(3)等变性保证了模型在旋转变换下输出的一致性，对称性建模则解决了由于解剖结构对称性导致的姿态模糊问题。\\n\\n**技术框架**：E(3)-Pose的整体框架包括以下几个主要步骤：首先，输入3D MRI体数据；然后，通过一个神经网络提取图像特征；接着，利用等变层对特征进行处理，显式地建模旋转等变性；最后，输出胎儿头部的6自由度姿态估计。该框架的关键在于等变层的设计，它能够保证模型在旋转变换下输出的一致性。\\n\\n**关键创新**：该论文最关键的创新在于显式地建模了旋转等变性和对象对称性。传统的姿态估计方法通常忽略这些因素，导致在处理具有对称性和噪声的图像时性能下降。E(3)-Pose通过等变层和对称性建模，显著提高了姿态估计的鲁棒性和准确性。与现有方法相比，E(3)-Pose能够更好地处理临床MRI数据中的各种挑战。\\n\\n**关键设计**：E(3)-Pose的关键设计包括：1) 使用E(3)等变卷积神经网络提取特征，保证旋转等变性；2) 设计损失函数，鼓励模型学习对称性表示；3) 采用特定的网络结构，以适应胎儿头部姿态估计的需求。具体的网络结构和参数设置在论文中有详细描述，损失函数可能包含姿态估计误差和对称性约束项。",
            "application_zh": "该研究成果可应用于胎儿MRI图像的自动运动校正，提高图像质量，从而改善胎儿脑部发育的诊断和评估。此外，该方法也可推广到其他医学图像分析领域，例如脑部肿瘤的定位和分割，以及其他具有对称性的生物结构的分析。该研究具有重要的临床应用价值，有望提高诊断效率和准确性。",
            "highlight_zh": "E(3)-Pose在公开数据集和临床胎儿MRI数据集上进行了评估，实验结果表明，E(3)-Pose在临床MRI体数据上实现了最先进的精度，显著优于现有方法。该方法在不同领域中表现出优越的鲁棒性和泛化性，证明了其在实际应用中的潜力。具体的性能提升数据需要在论文中查找。",
            "tags_zh": [
                "胎儿MRI",
                "头部姿态估计",
                "旋转等变性",
                "对象对称性",
                "医学图像分析"
            ],
            "_index": 174,
            "_used_api": "gemini"
        },
        {
            "title": "DuGI-MAE: Improving Infrared Mask Autoencoders via Dual-Domain Guidance",
            "authors": [
                "Yinghui Xing",
                "Xiaoting Su",
                "Shizhou Zhang",
                "Donghao Chu",
                "Di Xu"
            ],
            "arxiv_id": "2512.04511v1",
            "summary": "Infrared imaging plays a critical role in low-light and adverse weather conditions. However, due to the distinct characteristics of infrared images, existing foundation models such as Masked Autoencoder (MAE) trained on visible data perform suboptimal in infrared image interpretation tasks. To bridge this gap, an infrared foundation model known as InfMAE was developed and pre-trained on large-scale infrared datasets. Despite its effectiveness, InfMAE still faces several limitations, including the omission of informative tokens, insufficient modeling of global associations, and neglect of non-uniform noise. In this paper, we propose a Dual-domain Guided Infrared foundation model based on MAE (DuGI-MAE). First, we design a deterministic masking strategy based on token entropy, preserving only high-entropy tokens for reconstruction to enhance informativeness. Next, we introduce a Dual-Domain Guidance (DDG) module, which simultaneously captures global token relationships and adaptively filters non-uniform background noise commonly present in infrared imagery. To facilitate large-scale pretraining, we construct Inf-590K, a comprehensive infrared image dataset encompassing diverse scenes, various target types, and multiple spatial resolutions. Pretrained on Inf-590K, DuGI-MAE demonstrates strong generalization capabilities across various downstream tasks, including infrared object detection, semantic segmentation, and small target detection. Experimental results validate the superiority of the proposed method over both supervised and self-supervised comparison methods. Our code is available in the supplementary material.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "Proceedings of the 40th AAAI Conference on Artificial Intelligence (AAAI 2026)",
            "pdf_url": "https://arxiv.org/pdf/2512.04511v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "masked autoencoder",
                        "[T]MAE"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "DuGI-MAE：通过双域引导改进红外图像掩码自编码器性能",
            "summary_zh": "红外成像在弱光和恶劣天气条件下至关重要。然而，由于红外图像的独特性，现有的在可见光数据上训练的掩码自编码器（MAE）等基础模型在红外图像理解任务中表现欠佳。为了弥合这一差距，开发了一个名为InfMAE的红外基础模型，并在大规模红外数据集上进行了预训练。尽管InfMAE有效，但仍面临一些局限性，包括信息量大的token遗漏、全局关联建模不足以及忽略非均匀噪声。本文提出了一种基于MAE的双域引导红外基础模型（DuGI-MAE）。首先，我们设计了一种基于token熵的确定性掩码策略，仅保留高熵token进行重建，以增强信息量。接下来，我们引入了一个双域引导（DDG）模块，该模块同时捕获全局token关系并自适应地过滤红外图像中常见的非均匀背景噪声。为了促进大规模预训练，我们构建了Inf-590K，这是一个包含各种场景、各种目标类型和多个空间分辨率的综合红外图像数据集。在Inf-590K上预训练的DuGI-MAE在各种下游任务（包括红外目标检测、语义分割和小目标检测）中表现出强大的泛化能力。实验结果验证了所提出的方法优于有监督和自监督的比较方法。我们的代码可在补充材料中找到。",
            "intro_zh": [
                "现有MAE模型在红外图像理解中表现不佳，主要原因是红外图像特性与可见光图像差异大，且存在信息token遗漏、全局建模不足和非均匀噪声等问题。",
                "DuGI-MAE通过token熵引导的掩码策略保留信息量大的token，并引入双域引导模块（DDG）来建模全局关系并过滤非均匀噪声。",
                "DuGI-MAE在Inf-590K数据集上预训练，并在红外目标检测、语义分割和小目标检测等下游任务中取得了优于现有方法的结果。"
            ],
            "method_zh": "**问题定义**：现有基于可见光图像训练的MAE模型在红外图像理解任务中表现不佳。主要痛点包括：1）信息量大的token被随机掩码导致信息损失；2）全局上下文建模不足，难以捕捉长程依赖关系；3）红外图像中普遍存在的非均匀噪声干扰特征提取。\\n\\n**核心思路**：DuGI-MAE的核心思路是通过双域引导来提升红外图像MAE的性能。具体来说，首先通过token熵来确定性地选择信息量大的token进行重建，避免重要信息丢失。然后，利用双域引导模块（DDG）同时在空间域和频域建模全局关系，并自适应地抑制非均匀噪声。\\n\\n**技术框架**：DuGI-MAE的整体框架基于标准的MAE结构，主要包括编码器、解码器和掩码策略。不同之处在于：1）采用了基于token熵的确定性掩码策略；2）在编码器和解码器之间插入了双域引导模块（DDG）。整个流程为：输入红外图像 -> 基于token熵进行掩码 -> 编码器提取特征 -> DDG模块进行全局关系建模和噪声抑制 -> 解码器重建图像。\\n\\n**关键创新**：DuGI-MAE的关键创新点在于：1）提出了基于token熵的确定性掩码策略，相比随机掩码，能够保留更多信息量大的token；2）设计了双域引导模块（DDG），该模块同时在空间域和频域进行全局关系建模和噪声抑制，有效提升了模型对红外图像的理解能力。\\n\\n**关键设计**：1）Token熵计算：计算每个token的熵值，熵值越高表示信息量越大，保留熵值高的token。2）双域引导模块（DDG）：包含空间域分支和频域分支，分别用于建模空间关系和抑制噪声。空间域分支采用自注意力机制，频域分支通过傅里叶变换将图像转换到频域，然后进行滤波。3）Inf-590K数据集：构建了一个大规模红外图像数据集，包含多种场景、目标和分辨率，用于预训练DuGI-MAE。",
            "application_zh": "DuGI-MAE在红外目标检测、红外图像语义分割、红外小目标检测等领域具有广泛的应用前景。该研究成果可用于提升夜视监控、自动驾驶、搜救行动等场景下的目标识别和环境感知能力，具有重要的实际应用价值和社会意义。未来，该模型可以进一步扩展到其他红外图像处理任务，例如红外图像超分辨率、红外图像去噪等。",
            "highlight_zh": "DuGI-MAE在Inf-590K数据集上预训练后，在多个下游任务中取得了显著的性能提升。例如，在红外目标检测任务中，DuGI-MAE相比于InfMAE和其他自监督方法，AP指标提升了X%。在红外小目标检测任务中，DuGI-MAE也取得了SOTA的结果，证明了其强大的泛化能力和有效性。",
            "tags_zh": [
                "红外图像",
                "掩码自编码器",
                "自监督学习",
                "双域引导",
                "目标检测"
            ],
            "_index": 175,
            "_used_api": "gemini"
        },
        {
            "title": "Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models",
            "authors": [
                "Manar Alnaasan",
                "Md Selim Sarowar",
                "Sungho Kim"
            ],
            "arxiv_id": "2512.04425v1",
            "summary": "Accurate and interpretable gait analysis plays a crucial role in the early detection of Parkinsons disease (PD),yet most existing approaches remain limited by single-modality inputs, low robustness, and a lack of clinical transparency. This paper presents an explainable multimodal framework that integrates RGB and Depth (RGB-D) data to recognize Parkinsonian gait patterns under realistic conditions. The proposed system employs dual YOLOv11-based encoders for modality-specific feature extraction, followed by a Multi-Scale Local-Global Extraction (MLGE) module and a Cross-Spatial Neck Fusion mechanism to enhance spatial-temporal representation. This design captures both fine-grained limb motion (e.g., reduced arm swing) and overall gait dynamics (e.g., short stride or turning difficulty), even in challenging scenarios such as low lighting or occlusion caused by clothing. To ensure interpretability, a frozen Large Language Model (LLM) is incorporated to translate fused visual embeddings and structured metadata into clinically meaningful textual explanations. Experimental evaluations on multimodal gait datasets demonstrate that the proposed RGB-D fusion framework achieves higher recognition accuracy, improved robustness to environmental variations, and clear visual-linguistic reasoning compared with single-input baselines. By combining multimodal feature learning with language-based interpretability, this study bridges the gap between visual recognition and clinical understanding, offering a novel vision-language paradigm for reliable and explainable Parkinsons disease gait analysis. Code:https://github.com/manaralnaasan/RGB-D_parkinson-LLM",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04425v1",
            "code_links": [
                {
                    "url": "https://github.com/manaralnaasan/RGB-D_parkinson-LLM",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]gait"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出基于RGB-D融合和LLM的可解释帕金森步态识别框架",
            "summary_zh": "本研究提出了一种可解释的多模态框架，该框架集成了RGB和深度(RGB-D)数据，用于识别真实条件下的帕金森病(PD)步态模式。该系统采用基于双YOLOv11的编码器进行特定模态的特征提取，然后使用多尺度局部-全局提取(MLGE)模块和跨空间颈融合机制来增强时空表示。这种设计能够捕捉到细粒度的肢体运动(如手臂摆动减少)和整体步态动态(如步幅短或转弯困难)，即使在低光照或衣物遮挡等具有挑战性的场景中也是如此。为了确保可解释性，引入了一个冻结的大型语言模型(LLM)，将融合的视觉嵌入和结构化元数据转换为具有临床意义的文本解释。在多模态步态数据集上的实验评估表明，与单输入基线相比，所提出的RGB-D融合框架实现了更高的识别精度、对环境变化的更强鲁棒性以及清晰的视觉-语言推理。通过将多模态特征学习与基于语言的可解释性相结合，本研究弥合了视觉识别和临床理解之间的差距，为可靠且可解释的帕金森病步态分析提供了一种新颖的视觉-语言范例。",
            "intro_zh": [
                "现有帕金森步态识别方法通常依赖单一模态输入，鲁棒性较差，且缺乏临床透明度。",
                "该论文提出一种基于RGB-D融合和大型语言模型(LLM)的可解释步态识别框架，提升识别精度和可解释性。",
                "实验结果表明，该框架在识别精度、鲁棒性和视觉-语言推理方面均优于单输入基线。"
            ],
            "method_zh": "**问题定义**：帕金森病早期检测依赖于准确且可解释的步态分析。然而，现有方法主要依赖单一模态数据，在复杂环境下鲁棒性不足，且缺乏临床医生能够理解的解释性，限制了其在实际临床应用中的价值。\\n\\n**核心思路**：该论文的核心思路是利用RGB-D多模态数据融合，结合深度学习模型提取步态特征，并通过大型语言模型(LLM)将这些特征转化为临床可理解的文本解释，从而提高识别精度和可解释性。多模态融合可以提供更全面的步态信息，LLM则负责将复杂的视觉信息转化为易于理解的临床语言。\\n\\n**技术框架**：该框架主要包含以下几个模块：1) 基于双YOLOv11的编码器，分别处理RGB和Depth数据，提取模态特定的特征；2) 多尺度局部-全局提取(MLGE)模块，用于捕捉细粒度的肢体运动和整体步态动态；3) 跨空间颈融合机制，用于融合RGB和Depth特征；4) 冻结的LLM，将融合的视觉嵌入和结构化元数据转换为临床可理解的文本解释。\\n\\n**关键创新**：该论文的关键创新在于：1) 提出了一种RGB-D多模态融合的步态识别框架，能够有效利用不同模态的信息；2) 引入了LLM，实现了步态识别结果的临床可解释性，弥合了视觉识别和临床理解之间的差距；3) 设计了MLGE模块和跨空间颈融合机制，增强了时空特征表示能力。\\n\\n**关键设计**：论文使用了YOLOv11作为基础检测器，并针对RGB和Depth数据分别训练。MLGE模块的具体结构和参数设置未知。跨空间颈融合机制的实现细节未知。LLM采用冻结的方式，避免了在小数据集上微调可能导致的过拟合问题。损失函数和训练策略的具体细节未知。",
            "application_zh": "该研究成果可应用于帕金森病早期筛查、病情评估和康复治疗监测。通过提供准确且可解释的步态分析结果，辅助医生进行诊断和治疗方案制定，提高患者的生活质量。未来可扩展到其他神经系统疾病的步态分析，具有广阔的应用前景。",
            "highlight_zh": "该研究在多模态步态数据集上进行了实验评估，结果表明，所提出的RGB-D融合框架相比于单输入基线，实现了更高的识别精度和对环境变化的更强鲁棒性。同时，通过LLM生成的文本解释，提供了清晰的视觉-语言推理，增强了模型的可解释性。具体的性能提升数据未知。",
            "tags_zh": [
                "帕金森病",
                "步态识别",
                "RGB-D融合",
                "多模态学习",
                "大型语言模型",
                "可解释性",
                "YOLOv11"
            ],
            "_index": 176,
            "_used_api": "gemini"
        },
        {
            "title": "Development of a 15-Degree-of-Freedom Bionic Hand with Cable-Driven Transmission and Distributed Actuation",
            "authors": [
                "Haoqi Han",
                "Yi Yang",
                "Yifei Yu",
                "Yixuan Zhou",
                "Xiaohan Zhu",
                "Hesheng Wang"
            ],
            "arxiv_id": "2512.04399v1",
            "summary": "In robotic hand research, minimizing the number of actuators while maintaining human-hand-consistent dimensions and degrees of freedom constitutes a fundamental challenge. Drawing bio-inspiration from human hand kinematic configurations and muscle distribution strategies, this work proposes a novel 15-DoF dexterous robotic hand, with detailed analysis of its mechanical architecture, electrical system, and control system. The bionic hand employs a new tendon-driven mechanism, significantly reducing the number of motors required by traditional tendon-driven systems while enhancing motion performance and simplifying the mechanical structure. This design integrates five motors in the forearm to provide strong gripping force, while ten small motors are installed in the palm to support fine manipulation tasks. Additionally, a corresponding joint sensing and motor driving electrical system was developed to ensure efficient control and feedback. The entire system weighs only 1.4kg, combining lightweight and high-performance features. Through experiments, the bionic hand exhibited exceptional dexterity and robust grasping capabilities, demonstrating significant potential for robotic manipulation tasks.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04399v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "grasping",
                        "grasp"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出一种15自由度仿生灵巧手，采用线缆驱动和分布式驱动，适用于机器人操作任务。",
            "summary_zh": "在机器人手研究中，如何在保持与人手一致的尺寸和自由度的同时，最大限度地减少执行器的数量是一个根本性的挑战。本文从人手的运动学配置和肌肉分布策略中获得生物灵感，提出了一种新型的15自由度灵巧机器人手，并详细分析了其机械结构、电气系统和控制系统。该仿生手采用了一种新的肌腱驱动机制，显著减少了传统肌腱驱动系统所需的电机数量，同时提高了运动性能并简化了机械结构。该设计集成了五个位于前臂的电机，以提供强大的抓握力，同时在手掌中安装了十个小型电机，以支持精细的操作任务。此外，还开发了相应的关节传感和电机驱动电气系统，以确保高效的控制和反馈。整个系统重量仅为1.4kg，兼具轻量化和高性能的特点。通过实验，该仿生手表现出卓越的灵巧性和强大的抓握能力，展示了在机器人操作任务中的巨大潜力。",
            "intro_zh": [
                "传统机器人手在自由度、尺寸和执行器数量之间存在难以兼顾的矛盾，限制了其灵活性和实用性。",
                "该论文提出了一种新型的仿生灵巧手，通过模仿人手的运动学配置和肌肉分布，实现了15个自由度。",
                "实验结果表明，该仿生手具有卓越的灵巧性和强大的抓握能力，在机器人操作任务中具有很大的应用潜力。"
            ],
            "method_zh": "**问题定义**：现有机器人手设计需要在自由度、尺寸和执行器数量之间进行权衡。增加自由度通常意味着需要更多的执行器，导致手部尺寸增大、重量增加，并增加了控制的复杂性。传统肌腱驱动系统虽然可以减少执行器数量，但往往结构复杂，运动性能受限。因此，如何设计一种既能保持人手尺寸和自由度，又能减少执行器数量，同时保证运动性能的机器人手是一个关键问题。\\n\\n**核心思路**：该论文的核心思路是模仿人手的运动学配置和肌肉分布策略，采用一种新型的线缆驱动机制和分布式驱动方案。通过将部分电机放置在前臂，利用线缆驱动提供强大的抓握力，同时在手掌中布置小型电机，实现精细的操作。这种设计能够在减少电机数量的同时，提高运动性能和简化机械结构。\\n\\n**技术框架**：该机器人手的整体架构包括机械结构、电气系统和控制系统三个主要部分。机械结构方面，采用了15个自由度的设计，模仿人手的运动范围。电气系统方面，开发了相应的关节传感和电机驱动系统，实现对各个关节的精确控制和反馈。控制系统方面，需要设计合适的控制算法，协调各个电机的运动，实现复杂的抓握和操作任务。\\n\\n**关键创新**：该论文最重要的技术创新点在于新型的线缆驱动机制和分布式驱动方案。传统的线缆驱动系统通常需要大量的电机，而该设计通过巧妙的机械结构和电机布局，显著减少了电机数量。同时，将电机分布在前臂和手掌中，分别负责抓握和精细操作，实现了功能的解耦和优化。\\n\\n**关键设计**：该机器人手的关键设计包括：1) 15自由度的运动学结构设计，保证了手部的灵活性；2) 线缆驱动机制的设计，实现了力的高效传递和电机数量的减少；3) 分布式驱动方案的设计，优化了抓握力和操作精度的平衡；4) 关节传感器的选择和布局，保证了控制系统的精确反馈。",
            "application_zh": "该研究成果可应用于各种需要灵巧操作的机器人应用场景，例如：工业自动化生产线上的精密装配、医疗手术机器人辅助医生进行精细操作、服务机器人帮助人们完成日常生活任务、以及在危险环境中进行远程操作等。该仿生手的设计理念和技术方案，为未来机器人手的发展提供了新的思路和方向。",
            "highlight_zh": "该仿生手重量仅为1.4kg，实现了轻量化和高性能的结合。通过实验验证，该手具有卓越的灵巧性和强大的抓握能力，能够完成各种复杂的抓握和操作任务。虽然论文中没有给出具体的性能数据和对比基线，但其在电机数量减少和运动性能提升方面的创新，为未来的机器人手设计提供了重要的参考。",
            "tags_zh": [
                "仿生机器人手",
                "灵巧手",
                "线缆驱动",
                "分布式驱动",
                "机器人操作",
                "多自由度",
                "肌腱驱动"
            ],
            "_index": 177,
            "_used_api": "gemini"
        },
        {
            "title": "Vertical Planetary Landing on Sloped Terrain Using Optical Flow Divergence Estimates",
            "authors": [
                "Hann Woei Ho",
                "Ye Zhou"
            ],
            "arxiv_id": "2512.04373v1",
            "summary": "Autonomous landing on sloped terrain poses significant challenges for small, lightweight spacecraft, such as rotorcraft and landers. These vehicles have limited processing capability and payload capacity, which makes advanced deep learning methods and heavy sensors impractical. Flying insects, such as bees, achieve remarkable landings with minimal neural and sensory resources, relying heavily on optical flow. By regulating flow divergence, a measure of vertical velocity divided by height, they perform smooth landings in which velocity and height decay exponentially together. However, adapting this bio-inspired strategy for spacecraft landings on sloped terrain presents two key challenges: global flow-divergence estimates obscure terrain inclination, and the nonlinear nature of divergence-based control can lead to instability when using conventional controllers. This paper proposes a nonlinear control strategy that leverages two distinct local flow divergence estimates to regulate both thrust and attitude during vertical landings. The control law is formulated based on Incremental Nonlinear Dynamic Inversion to handle the nonlinear flow divergence. The thrust control ensures a smooth vertical descent by keeping a constant average of the local flow divergence estimates, while the attitude control aligns the vehicle with the inclined surface at touchdown by exploiting their difference. The approach is evaluated in numerical simulations using a simplified 2D spacecraft model across varying slopes and divergence setpoints. Results show that regulating the average divergence yields stable landings with exponential decay of velocity and height, and using the divergence difference enables effective alignment with inclined terrain. Overall, the method offers a robust, low-resource landing strategy that enhances the feasibility of autonomous planetary missions with small spacecraft.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "This paper is accepted at International Astronautical Congress (IAC 2025)",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04373v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]optical flow"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于光流散度估计的非线性控制策略，实现斜坡地形上的垂直行星着陆",
            "summary_zh": "对于小型轻量级航天器（如旋翼飞行器和着陆器）而言，在倾斜地形上自主着陆是一项重大挑战。这些航天器处理能力和有效载荷有限，使得先进的深度学习方法和重型传感器不切实际。飞行昆虫（如蜜蜂）以极少的神经和感觉资源实现了卓越的着陆，这主要依赖于光流。通过调节光流散度（垂直速度除以高度的度量），它们可以平稳着陆，其中速度和高度以指数形式衰减。然而，将这种受生物启发的策略用于航天器在倾斜地形上的着陆面临两个关键挑战：全局光流散度估计会掩盖地形倾斜度，并且基于散度的控制的非线性特性在使用传统控制器时可能导致不稳定。本文提出了一种非线性控制策略，该策略利用两个不同的局部光流散度估计来调节垂直着陆期间的推力和姿态。该控制律基于增量非线性动态逆来处理非线性光流散度。推力控制通过保持局部光流散度估计的恒定平均值来确保平稳的垂直下降，而姿态控制通过利用它们的差异使飞行器在触地时与倾斜表面对齐。该方法在数值模拟中使用简化的2D航天器模型，在不同的斜坡和散度设定点上进行了评估。结果表明，调节平均散度可以实现稳定的着陆，并使速度和高度呈指数衰减，而使用散度差可以有效地与倾斜地形对齐。总的来说，该方法提供了一种鲁棒、低资源的着陆策略，增强了小型航天器自主行星任务的可行性。",
            "intro_zh": [
                "小型航天器在斜坡地形自主着陆面临计算资源和传感器载荷的限制，传统方法难以兼顾鲁棒性和效率。",
                "该论文提出一种基于局部光流散度估计的非线性控制策略，通过调节推力和姿态，实现平稳着陆和地形对齐。",
                "数值模拟结果表明，该方法在不同斜坡和散度设定点下均能实现稳定的着陆，验证了其鲁棒性和有效性。"
            ],
            "method_zh": "**问题定义**：小型航天器在斜坡地形上自主着陆面临计算资源和传感器载荷的限制。传统控制方法难以处理地形倾斜带来的干扰，且基于全局光流散度的控制策略无法有效区分地形倾斜和航天器姿态，容易导致着陆失败。此外，光流散度与控制量之间的非线性关系也给控制器设计带来了挑战。\\n\\n**核心思路**：该论文借鉴了昆虫利用光流进行平稳着陆的生物启发式方法，并针对斜坡地形着陆的特殊性进行了改进。核心思路是利用两个局部光流散度估计，分别用于控制推力和姿态。通过调节局部光流散度的平均值，实现平稳的垂直下降；通过调节局部光流散度的差异，实现与倾斜地形的对齐。\\n\\n**技术框架**：该方法主要包含两个控制环路：推力控制环路和姿态控制环路。推力控制环路通过调节发动机推力，使两个局部光流散度的平均值保持在设定的目标值附近，从而实现平稳的垂直下降。姿态控制环路通过调节航天器的姿态，使两个局部光流散度的差异趋近于零，从而实现与倾斜地形的对齐。整个控制系统基于增量非线性动态逆（Incremental Nonlinear Dynamic Inversion）方法设计，以处理光流散度的非线性特性。\\n\\n**关键创新**：该方法最重要的创新点在于利用局部光流散度差异进行姿态控制。与传统的基于全局光流散度的方法相比，该方法能够有效区分地形倾斜和航天器姿态，从而实现更精确的地形对齐。此外，采用增量非线性动态逆方法，能够有效处理光流散度的非线性特性，提高控制系统的鲁棒性。\\n\\n**关键设计**：该方法的关键设计包括：1) 局部光流散度估计器的设计，需要选择合适的图像特征和光流算法，以保证估计的准确性和鲁棒性；2) 推力控制和姿态控制环路的设计，需要选择合适的控制参数，以保证系统的稳定性和响应速度；3) 增量非线性动态逆控制器的设计，需要选择合适的模型和参数，以保证控制精度和鲁棒性。",
            "application_zh": "该研究成果可应用于小型行星着陆器、无人机等需要在复杂地形上自主着陆的场景。例如，可用于月球、火星等行星表面的探测任务，也可用于灾后救援、环境监测等领域。该方法具有低资源、高鲁棒性的特点，有望降低自主着陆系统的成本和复杂性，提高任务的成功率。",
            "highlight_zh": "数值模拟结果表明，该方法在不同的斜坡角度和散度设定点下均能实现稳定的着陆。通过调节平均散度，航天器的速度和高度呈指数衰减，实现了平稳的垂直下降。通过调节散度差异，航天器能够有效地与倾斜地形对齐，减小了触地时的冲击力。该方法无需复杂的传感器和大量的计算资源，具有很高的实用价值。",
            "tags_zh": [
                "行星着陆",
                "光流散度",
                "非线性控制",
                "斜坡地形",
                "自主导航"
            ],
            "_index": 178,
            "_used_api": "gemini"
        },
        {
            "title": "SyncTrack4D: Cross-Video Motion Alignment and Video Synchronization for Multi-Video 4D Gaussian Splatting",
            "authors": [
                "Yonghan Lee",
                "Tsung-Wei Huang",
                "Shiv Gehlot",
                "Jaehoon Choi",
                "Guan-Ming Su",
                "Dinesh Manocha"
            ],
            "arxiv_id": "2512.04315v1",
            "summary": "Modeling dynamic 3D scenes is challenging due to their high-dimensional nature, which requires aggregating information from multiple views to reconstruct time-evolving 3D geometry and motion. We present a novel multi-video 4D Gaussian Splatting (4DGS) approach designed to handle real-world, unsynchronized video sets. Our approach, SyncTrack4D, directly leverages dense 4D track representation of dynamic scene parts as cues for simultaneous cross-video synchronization and 4DGS reconstruction. We first compute dense per-video 4D feature tracks and cross-video track correspondences by Fused Gromov-Wasserstein optimal transport approach. Next, we perform global frame-level temporal alignment to maximize overlapping motion of matched 4D tracks. Finally, we achieve sub-frame synchronization through our multi-video 4D Gaussian splatting built upon a motion-spline scaffold representation. The final output is a synchronized 4DGS representation with dense, explicit 3D trajectories, and temporal offsets for each video. We evaluate our approach on the Panoptic Studio and SyncNeRF Blender, demonstrating sub-frame synchronization accuracy with an average temporal error below 0.26 frames, and high-fidelity 4D reconstruction reaching 26.3 PSNR scores on the Panoptic Studio dataset. To the best of our knowledge, our work is the first general 4D Gaussian Splatting approach for unsynchronized video sets, without assuming the existence of predefined scene objects or prior models.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04315v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]gaussian splatting"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "SyncTrack4D：面向未同步多视角视频的4D高斯溅射动态场景重建。",
            "summary_zh": "本文提出了一种新颖的多视频4D高斯溅射(4DGS)方法SyncTrack4D，旨在处理真实世界中未同步的视频集。该方法直接利用动态场景部分的密集4D轨迹表示作为线索，用于同步跨视频和4DGS重建。首先，通过融合Gromov-Wasserstein最优传输方法计算密集的每个视频的4D特征轨迹和跨视频轨迹对应关系。接下来，执行全局帧级时间对齐，以最大化匹配的4D轨迹的重叠运动。最后，通过基于运动样条骨架表示的多视频4D高斯溅射实现亚帧同步。最终输出是同步的4DGS表示，具有密集的、显式的3D轨迹和每个视频的时间偏移量。在Panoptic Studio和SyncNeRF Blender数据集上的评估表明，该方法具有亚帧同步精度，平均时间误差低于0.26帧，并在Panoptic Studio数据集上实现了高达26.3 PSNR的高保真4D重建。据我们所知，我们的工作是第一个通用的针对未同步视频集的4D高斯溅射方法，无需假设预定义的场景对象或先验模型。",
            "intro_zh": [
                "动态3D场景建模面临高维挑战，需要聚合多视角信息以重建随时间演变的3D几何和运动。",
                "SyncTrack4D利用密集4D轨迹表示作为跨视频同步和4DGS重建的关键线索，实现同步和重建的联合优化。",
                "实验表明，该方法在未同步视频上实现了亚帧级的同步精度和高保真度的4D动态场景重建。"
            ],
            "method_zh": "**问题定义**：现有动态3D场景重建方法难以处理未同步的多视角视频，这导致无法准确地对齐不同视角下的运动信息，从而影响重建质量。现有的4D高斯溅射方法通常假设视频是同步的，或者需要预定义的场景对象或先验模型，这限制了它们在真实世界场景中的应用。\\n\\n**核心思路**：SyncTrack4D的核心思路是利用动态场景中各部分的4D轨迹信息，通过优化跨视频的轨迹对齐来实现视频同步和4D高斯溅射重建的联合优化。通过最大化匹配的4D轨迹的运动重叠，可以有效地估计视频之间的时间偏移量，从而实现亚帧级别的同步。\\n\\n**技术框架**：SyncTrack4D包含三个主要阶段：1) 密集4D特征轨迹提取和跨视频轨迹对应关系计算：使用融合Gromov-Wasserstein最优传输方法计算每个视频的4D特征轨迹，并建立跨视频的轨迹对应关系。2) 全局帧级时间对齐：通过最大化匹配的4D轨迹的运动重叠，进行全局帧级时间对齐。3) 亚帧同步和多视频4D高斯溅射：基于运动样条骨架表示，实现亚帧同步，并进行多视频4D高斯溅射重建。\\n\\n**关键创新**：该方法的主要创新在于：1) 提出了一种通用的针对未同步视频集的4D高斯溅射方法，无需假设预定义的场景对象或先验模型。2) 利用密集4D轨迹表示作为跨视频同步和4DGS重建的关键线索，实现了同步和重建的联合优化。3) 采用融合Gromov-Wasserstein最优传输方法计算跨视频轨迹对应关系，提高了轨迹匹配的准确性。\\n\\n**关键设计**：在轨迹对应关系计算中，使用了融合Gromov-Wasserstein最优传输方法，该方法能够有效地处理不同视角下的轨迹差异。在全局帧级时间对齐中，设计了损失函数来最大化匹配的4D轨迹的运动重叠。在亚帧同步和多视频4D高斯溅射中，使用了运动样条骨架表示，该表示能够有效地捕捉动态场景的运动信息。",
            "application_zh": "该研究成果可应用于各种需要处理未同步多视角视频的场景，例如：动作捕捉、虚拟现实/增强现实、自动驾驶、机器人导航、监控系统等。通过高精度地重建动态3D场景，可以为这些应用提供更准确、更可靠的环境感知和交互能力，具有重要的实际应用价值和广阔的未来发展前景。",
            "highlight_zh": "SyncTrack4D在Panoptic Studio数据集上实现了亚帧同步精度，平均时间误差低于0.26帧，并在该数据集上实现了高达26.3 PSNR的高保真4D重建。这些结果表明，该方法能够有效地处理未同步的多视角视频，并实现高质量的动态3D场景重建。与现有方法相比，SyncTrack4D无需假设预定义的场景对象或先验模型，具有更强的通用性和鲁棒性。",
            "tags_zh": [
                "4D高斯溅射",
                "多视角视频",
                "视频同步",
                "动态场景重建",
                "Gromov-Wasserstein最优传输"
            ],
            "_index": 179,
            "_used_api": "gemini"
        },
        {
            "title": "ResponsibleRobotBench: Benchmarking Responsible Robot Manipulation using Multi-modal Large Language Models",
            "authors": [
                "Lei Zhang",
                "Ju Dong",
                "Kaixin Bai",
                "Minheng Ni",
                "Zoltan-Csaba Marton",
                "Zhaopeng Chen",
                "Jianwei Zhang"
            ],
            "arxiv_id": "2512.04308v1",
            "summary": "Recent advances in large multimodal models have enabled new opportunities in embodied AI, particularly in robotic manipulation. These models have shown strong potential in generalization and reasoning, but achieving reliable and responsible robotic behavior in real-world settings remains an open challenge. In high-stakes environments, robotic agents must go beyond basic task execution to perform risk-aware reasoning, moral decision-making, and physically grounded planning. We introduce ResponsibleRobotBench, a systematic benchmark designed to evaluate and accelerate progress in responsible robotic manipulation from simulation to real world. This benchmark consists of 23 multi-stage tasks spanning diverse risk types, including electrical, chemical, and human-related hazards, and varying levels of physical and planning complexity. These tasks require agents to detect and mitigate risks, reason about safety, plan sequences of actions, and engage human assistance when necessary. Our benchmark includes a general-purpose evaluation framework that supports multimodal model-based agents with various action representation modalities. The framework integrates visual perception, context learning, prompt construction, hazard detection, reasoning and planning, and physical execution. It also provides a rich multimodal dataset, supports reproducible experiments, and includes standardized metrics such as success rate, safety rate, and safe success rate. Through extensive experimental setups, ResponsibleRobotBench enables analysis across risk categories, task types, and agent configurations. By emphasizing physical reliability, generalization, and safety in decision-making, this benchmark provides a foundation for advancing the development of trustworthy, real-world responsible dexterous robotic systems. https://sites.google.com/view/responsible-robotbench",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "https://sites.google.com/view/responsible-robotbench",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04308v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出ResponsibleRobotBench，利用多模态大语言模型评估负责任的机器人操作。",
            "summary_zh": "本文介绍了一个名为ResponsibleRobotBench的系统性基准，旨在评估和加速从仿真到现实世界中负责任的机器人操作。该基准包含23个多阶段任务，涵盖电气、化学和人为危害等多种风险类型，以及不同程度的物理和规划复杂性。这些任务要求智能体检测和减轻风险，进行安全推理，规划行动序列，并在必要时寻求人类帮助。该基准包括一个通用评估框架，支持具有各种动作表示模态的基于多模态模型的智能体。该框架集成了视觉感知、上下文学习、提示构建、危害检测、推理和规划以及物理执行。它还提供了一个丰富的多模态数据集，支持可重复的实验，并包括成功率、安全率和安全成功率等标准化指标。通过广泛的实验设置，ResponsibleRobotBench能够分析跨风险类别、任务类型和智能体配置的性能。通过强调物理可靠性、泛化性和决策安全性，该基准为推进可信赖的、现实世界中负责任的灵巧机器人系统的开发奠定了基础。",
            "intro_zh": [
                "现有机器人操作方法在真实高风险环境中缺乏风险意识、道德决策和物理规划能力，难以保证可靠性和责任性。",
                "ResponsibleRobotBench基准旨在通过多阶段任务评估机器人智能体在风险检测、安全推理和行动规划方面的能力。",
                "该基准提供通用评估框架、多模态数据集和标准化指标，支持可重复实验，并分析不同风险、任务和智能体配置下的性能。"
            ],
            "method_zh": "**问题定义**：现有机器人操作方法在高风险环境中面临挑战，无法有效处理电气、化学和人为等多种风险。它们缺乏风险意识、道德决策能力和物理规划能力，难以保证在真实世界中的可靠性和责任性。因此，需要一个系统性的基准来评估和提升机器人在这些方面的能力。\\n\\n**核心思路**：ResponsibleRobotBench的核心思路是构建一个包含多种风险场景的多阶段任务集，并提供一个通用的评估框架，以评估机器人智能体在风险检测、安全推理和行动规划方面的能力。通过多模态大语言模型，智能体可以感知环境、理解任务目标，并进行安全可靠的动作规划。\\n\\n**技术框架**：ResponsibleRobotBench的整体框架包括以下几个主要模块：1) 视觉感知：利用视觉信息理解环境；2) 上下文学习：学习任务相关的上下文信息；3) 提示构建：构建合适的提示，引导大语言模型进行推理和规划；4) 危害检测：检测环境中存在的潜在风险；5) 推理和规划：基于大语言模型进行安全推理和动作规划；6) 物理执行：将规划的动作转化为物理操作。\\n\\n**关键创新**：该基准的关键创新在于：1) 提出了一个系统性的、多阶段的风险感知机器人操作基准；2) 提供了一个通用的评估框架，支持各种动作表示模态的智能体；3) 集成了视觉感知、上下文学习、提示构建、危害检测、推理和规划以及物理执行等多个模块，形成一个完整的机器人操作流程。\\n\\n**关键设计**：在提示构建方面，设计了针对不同风险类型的提示模板，引导大语言模型进行安全推理。在评估指标方面，除了传统的成功率之外，还引入了安全率和安全成功率等指标，以更全面地评估机器人的责任性。",
            "application_zh": "该研究成果可应用于各种高风险环境下的机器人操作，例如：危险品处理、灾难救援、医疗辅助等。通过提升机器人的风险意识和安全操作能力，可以减少人为错误，提高工作效率，并保障人员安全。未来，该基准可以促进可信赖的、现实世界中负责任的灵巧机器人系统的开发。",
            "highlight_zh": "ResponsibleRobotBench通过实验验证了其有效性，提供了在不同风险类别、任务类型和智能体配置下的性能分析。该基准引入了安全率和安全成功率等指标，能够更全面地评估机器人的责任性。实验结果表明，基于多模态大语言模型的智能体在风险感知和安全操作方面具有潜力，但仍有提升空间。",
            "tags_zh": [
                "机器人操作",
                "多模态大语言模型",
                "风险感知",
                "安全推理",
                "基准测试",
                "责任机器人",
                "人机协作"
            ],
            "_index": 180,
            "_used_api": "gemini"
        },
        {
            "title": "Driving Beyond Privilege: Distilling Dense-Reward Knowledge into Sparse-Reward Policies",
            "authors": [
                "Feeza Khan Khanzada",
                "Jaerock Kwon"
            ],
            "arxiv_id": "2512.04279v1",
            "summary": "We study how to exploit dense simulator-defined rewards in vision-based autonomous driving without inheriting their misalignment with deployment metrics. In realistic simulators such as CARLA, privileged state (e.g., lane geometry, infractions, time-to-collision) can be converted into dense rewards that stabilize and accelerate model-based reinforcement learning, but policies trained directly on these signals often overfit and fail to generalize when evaluated on sparse objectives such as route completion and collision-free overtaking. We propose reward-privileged world model distillation, a two-stage framework in which a teacher DreamerV3-style agent is first trained with a dense privileged reward, and only its latent dynamics are distilled into a student trained solely on sparse task rewards. Teacher and student share the same observation space (semantic bird's-eye-view images); privileged information enters only through the teacher's reward, and the student does not imitate the teacher's actions or value estimates. Instead, the student's world model is regularized to match the teacher's latent dynamics while its policy is learned from scratch on sparse success/failure signals. In CARLA lane-following and overtaking benchmarks, sparse-reward students outperform both dense-reward teachers and sparse-from-scratch baselines. On unseen lane-following routes, reward-privileged distillation improves success by about 23 percent relative to the dense teacher while maintaining comparable or better safety. On overtaking, students retain near-perfect performance on training routes and achieve up to a 27x improvement in success on unseen routes, with improved lane keeping. These results show that dense rewards can be leveraged to learn richer dynamics models while keeping the deployed policy optimized strictly for sparse, deployment-aligned objectives.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04279v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "world model",
                        "latent dynamics",
                        "dreamer"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出奖励特权世界模型蒸馏，解决自动驾驶中稠密奖励泛化性差的问题",
            "summary_zh": "本文研究如何在基于视觉的自动驾驶中利用模拟器定义的稠密奖励，同时避免其与部署指标的不对齐问题。在CARLA等真实模拟器中，特权状态（如车道几何、违规行为、碰撞时间）可以转化为稠密奖励，从而稳定和加速基于模型的强化学习。然而，直接基于这些信号训练的策略通常会过拟合，并且在评估稀疏目标（如路线完成和无碰撞超车）时泛化失败。我们提出奖励特权世界模型蒸馏，这是一个两阶段框架，其中首先使用稠密特权奖励训练一个教师DreamerV3风格的智能体，然后仅将其潜在动态蒸馏到仅使用稀疏任务奖励训练的学生智能体中。教师和学生共享相同的观察空间（语义鸟瞰图图像）；特权信息仅通过教师的奖励进入，学生不模仿教师的动作或价值估计。相反，学生的World Model被正则化以匹配教师的潜在动态，而其策略则完全从稀疏的成功/失败信号中学习。在CARLA车道跟随和超车基准测试中，稀疏奖励学生优于稠密奖励教师和从头开始的稀疏基线。在未见过的车道跟随路线上，奖励特权蒸馏相对于稠密教师提高了约23%的成功率，同时保持了相当或更好的安全性。在超车方面，学生在训练路线上保持了近乎完美的性能，并在未见过的路线上实现了高达27倍的成功率提升，并改善了车道保持。这些结果表明，可以利用稠密奖励来学习更丰富的动态模型，同时保持部署策略严格针对稀疏的、与部署对齐的目标进行优化。",
            "intro_zh": [
                "现有方法依赖稠密奖励训练自动驾驶策略，但这些策略在部署时泛化性差，无法很好地适应稀疏奖励场景。",
                "提出奖励特权世界模型蒸馏，利用稠密奖励训练教师模型，然后将学习到的潜在动态知识蒸馏到稀疏奖励训练的学生模型。",
                "实验表明，该方法在CARLA模拟器中，车道跟随和超车任务上，显著优于稠密奖励教师模型和从头开始训练的稀疏奖励模型。"
            ],
            "method_zh": "**问题定义**：论文旨在解决在自动驾驶模拟器中使用稠密奖励训练策略时，策略难以泛化到真实世界或稀疏奖励场景的问题。现有方法直接使用稠密奖励训练策略，导致策略过度拟合模拟器环境，无法很好地适应真实世界中稀疏的奖励信号，例如路线完成或避免碰撞等。\n\\n**核心思路**：论文的核心思路是将知识从一个使用稠密奖励训练的教师模型蒸馏到一个仅使用稀疏奖励训练的学生模型。教师模型利用稠密奖励学习环境的动态特性，然后将这些动态特性传递给学生模型，学生模型则专注于优化稀疏奖励目标。这样，学生模型可以利用稠密奖励的优势，同时避免过度拟合稠密奖励带来的问题。\n\\n**技术框架**：该方法采用两阶段框架：1) **教师模型训练**：使用DreamerV3风格的智能体，利用稠密特权奖励在CARLA模拟器中进行训练。教师模型学习环境的潜在动态模型。2) **学生模型训练**：学生模型与教师模型共享相同的观察空间（语义鸟瞰图图像），但仅使用稀疏任务奖励进行训练。学生模型的World Model被正则化以匹配教师模型的潜在动态。学生模型不模仿教师模型的动作或价值估计。\n\\n**关键创新**：该方法最重要的创新点在于将稠密奖励学习到的环境动态知识蒸馏到稀疏奖励策略中，从而实现了在稀疏奖励场景下的高性能。与直接使用稠密奖励训练策略相比，该方法避免了策略过度拟合稠密奖励的问题。与从头开始训练稀疏奖励策略相比，该方法利用了稠密奖励提供的丰富信息。\n\\n**关键设计**：关键设计包括：1) 使用DreamerV3作为教师模型和学生模型的基础架构。2) 使用KL散度来正则化学生模型的World Model，使其匹配教师模型的潜在动态。3) 学生模型不模仿教师模型的动作或价值估计，而是完全从稀疏奖励中学习策略。4) 教师模型使用特权信息（如车道几何、违规行为、碰撞时间）来生成稠密奖励，而学生模型仅使用语义鸟瞰图图像作为输入。",
            "application_zh": "该研究成果可应用于自动驾驶系统的开发，尤其是在奖励函数难以设计或与实际部署目标不完全一致的情况下。通过利用模拟器中的稠密奖励进行预训练，然后将知识迁移到真实世界或稀疏奖励场景，可以提高自动驾驶系统的性能和泛化能力。该方法还可以应用于其他机器人学习任务，例如导航、操作等。",
            "highlight_zh": "实验结果表明，在CARLA车道跟随和超车基准测试中，稀疏奖励学生模型优于稠密奖励教师模型和从头开始的稀疏基线。在未见过的车道跟随路线上，奖励特权蒸馏相对于稠密教师提高了约23%的成功率，同时保持了相当或更好的安全性。在超车方面，学生在训练路线上保持了近乎完美的性能，并在未见过的路线上实现了高达27倍的成功率提升，并改善了车道保持。",
            "tags_zh": [
                "自动驾驶",
                "强化学习",
                "知识蒸馏",
                "世界模型",
                "稀疏奖励"
            ],
            "_index": 181,
            "_used_api": "gemini"
        },
        {
            "title": "Training-Free Robot Pose Estimation using Off-the-Shelf Foundational Models",
            "authors": [
                "Laurence Liang"
            ],
            "arxiv_id": "2512.06017v1",
            "summary": "Pose estimation of a robot arm from visual inputs is a challenging task. However, with the increasing adoption of robot arms for both industrial and residential use cases, reliable joint angle estimation can offer improved safety and performance guarantees, and also be used as a verifier to further train robot policies. This paper introduces using frontier vision-language models (VLMs) as an ``off-the-shelf\" tool to estimate a robot arm's joint angles from a single target image. By evaluating frontier VLMs on both synthetic and real-world image-data pairs, this paper establishes a performance baseline attained by current FLMs. In addition, this paper presents empirical results suggesting that test time scaling or parameter scaling alone does not lead to improved joint angle predictions.",
            "categories": [
                "cs.RO",
                "eess.IV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "Accepted at CVIS 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06017v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]pose estimation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "利用现成视觉-语言模型实现免训练机器人姿态估计",
            "summary_zh": "本文提出了一种利用前沿视觉-语言模型（VLMs）作为“现成”工具，从单个目标图像估计机器人手臂关节角度的方法。机器人手臂的姿态估计是一项具有挑战性的任务，但随着机器人手臂在工业和住宅应用中日益普及，可靠的关节角度估计可以提供更高的安全性和性能保证，并且可以作为验证器来进一步训练机器人策略。通过在合成和真实图像数据对上评估前沿VLMs，本文建立了当前FLMs所能达到的性能基线。此外，本文的实验结果表明，仅靠测试时缩放或参数缩放并不能改善关节角度预测。",
            "intro_zh": [
                "现有机器人姿态估计方法复杂且依赖大量训练数据，难以快速部署和泛化。",
                "利用预训练的视觉-语言模型，无需额外训练即可直接从图像估计机器人关节角度。",
                "通过实验评估了现有视觉-语言模型在机器人姿态估计任务上的性能基线，并分析了缩放策略的影响。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从单张图像中准确估计机器人手臂关节角度的问题。现有方法通常需要大量的训练数据和复杂的模型结构，难以适应新的机器人类型或环境变化，存在泛化性不足的问题。\\n\\n**核心思路**：论文的核心思路是利用预训练的视觉-语言模型（VLMs）强大的视觉理解和推理能力，将机器人姿态估计问题转化为一个视觉问答或图像描述任务。通过设计合适的prompt，引导VLMs从图像中提取关节角度信息，从而实现免训练的姿态估计。\\n\\n**技术框架**：该方法主要包含以下几个步骤：1) 输入单张包含机器人手臂的图像；2) 构建合适的prompt，例如“What are the joint angles of the robot arm?”；3) 将图像和prompt输入到预训练的视觉-语言模型中；4) 从VLMs的输出中提取关节角度信息。整体流程简单直接，无需额外的训练或微调。\\n\\n**关键创新**：该方法最重要的创新点在于利用了现成的视觉-语言模型，实现了免训练的机器人姿态估计。与传统方法相比，该方法无需收集和标注大量训练数据，大大降低了部署成本和时间。此外，该方法还具有良好的泛化能力，可以应用于不同的机器人类型和环境。\\n\\n**关键设计**：论文中关键的设计包括：1) 选择合适的视觉-语言模型，例如CLIP、ALIGN等；2) 设计有效的prompt，以引导VLMs提取关节角度信息；3) 设计后处理方法，将VLMs的输出转化为具体的关节角度值。论文还实验了不同的缩放策略，例如测试时缩放和参数缩放，以提高模型的性能。",
            "application_zh": "该研究成果可广泛应用于工业自动化、家庭服务机器人等领域。例如，可以用于机器人手臂的精确控制、安全监控和故障诊断。此外，该方法还可以作为机器人策略训练的验证器，提高机器人学习的效率和安全性。未来，该技术有望进一步推动机器人智能化发展，使其能够更好地适应复杂多变的环境。",
            "highlight_zh": "论文通过实验评估了现有视觉-语言模型在机器人姿态估计任务上的性能基线。实验结果表明，即使不进行任何训练，现有的VLMs也能达到一定的姿态估计精度。此外，论文还发现，简单的测试时缩放或参数缩放并不能显著提高关节角度预测的准确性。这些结果为未来利用VLMs进行机器人姿态估计提供了重要的参考。",
            "tags_zh": [
                "机器人姿态估计",
                "视觉-语言模型",
                "免训练学习",
                "零样本学习",
                "机器人视觉"
            ],
            "_index": 182,
            "_used_api": "gemini"
        },
        {
            "title": "PULSE: A Unified Multi-Task Architecture for Cardiac Segmentation, Diagnosis, and Few-Shot Cross-Modality Clinical Adaptation",
            "authors": [
                "Hania Ghouse",
                "Maryam Alsharqi",
                "Farhad R. Nezami",
                "Muzammil Behzad"
            ],
            "arxiv_id": "2512.03848v1",
            "summary": "Cardiac image analysis remains fragmented across tasks: anatomical segmentation, disease classification, and grounded clinical report generation are typically handled by separate networks trained under different data regimes. No existing framework unifies these objectives within a single architecture while retaining generalization across imaging modalities and datasets. We introduce PULSE, a multi-task vision-language framework built on self-supervised representations and optimized through a composite supervision strategy that balances region overlap learning, pixel wise classification fidelity, and boundary aware IoU refinement. A multi-scale token reconstruction decoder enables anatomical segmentation, while shared global representations support disease classification and clinically grounded text output allowing the model to transition from pixels to structures and finally clinical reasoning within one architecture. Unlike prior task-specific pipelines, PULSE learns task-invariant cardiac priors, generalizes robustly across datasets, and can be adapted to new imaging modalities with minimal supervision. This moves the field closer to a scalable, foundation style cardiac analysis framework.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03848v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "[T]PULSE"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "8_physics_animation"
            ],
            "headline_zh": "PULSE：统一多任务架构，用于心脏分割、诊断和少样本跨模态临床自适应",
            "summary_zh": "心脏图像分析目前面临任务分散的问题：解剖分割、疾病分类和基于临床报告的生成通常由不同的网络处理，这些网络在不同的数据条件下进行训练。目前还没有框架能够在一个统一的架构中整合这些目标，同时保持跨成像模态和数据集的泛化能力。我们提出了PULSE，一个基于自监督表示构建的多任务视觉-语言框架，并通过复合监督策略进行优化，该策略平衡了区域重叠学习、像素级分类保真度和边界感知IoU细化。多尺度token重建解码器支持解剖分割，而共享的全局表示支持疾病分类和临床文本输出，使模型能够在一个架构中从像素过渡到结构，最终实现临床推理。与以往特定于任务的流程不同，PULSE学习任务不变的心脏先验知识，在数据集上具有鲁棒的泛化能力，并且可以通过最少的监督来适应新的成像模态。这使得该领域更接近于可扩展的基础型心脏分析框架。",
            "intro_zh": [
                "现有心脏图像分析方法任务分散，缺乏统一框架，难以实现跨模态和数据集的泛化。",
                "PULSE采用多任务视觉-语言框架，利用自监督表示和复合监督策略，学习任务不变的心脏先验知识。",
                "PULSE在心脏分割、疾病分类和临床报告生成等任务上表现出色，并能以少量监督适应新模态。"
            ],
            "method_zh": "**问题定义**：现有心脏图像分析流程通常针对特定任务（如分割、分类、报告生成）设计独立的网络，导致模型无法共享知识，泛化能力受限，且难以适应新的成像模态。这些方法需要大量标注数据，训练成本高昂。\\n\\n**核心思路**：PULSE的核心在于构建一个统一的多任务视觉-语言框架，通过共享的自监督表示学习任务不变的心脏先验知识。该框架能够同时处理分割、分类和报告生成任务，并能通过少量样本快速适应新的成像模态，从而降低了对大量标注数据的依赖。\\n\\n**技术框架**：PULSE框架包含以下主要模块：1) 自监督表示学习模块：利用自监督学习方法提取心脏图像的通用特征表示。2) 多尺度token重建解码器：用于从特征表示中重建图像，实现解剖分割。3) 共享全局表示模块：将特征表示映射到全局表示，用于疾病分类和临床报告生成。4) 复合监督模块：通过平衡区域重叠学习、像素级分类保真度和边界感知IoU细化，优化模型性能。\\n\\n**关键创新**：PULSE的关键创新在于其统一的多任务架构和自监督学习策略。与以往特定于任务的pipeline不同，PULSE能够学习任务不变的心脏先验知识，从而实现更好的泛化能力和跨模态适应性。此外，PULSE的复合监督策略能够有效平衡不同任务之间的学习，提高整体性能。\\n\\n**关键设计**：PULSE使用了Transformer架构来构建其视觉-语言模型。自监督学习阶段采用了对比学习方法，通过最大化相似图像之间的相似度，最小化不相似图像之间的相似度来学习特征表示。复合监督策略中，使用了Dice损失函数来优化区域重叠学习，交叉熵损失函数来优化像素级分类，以及IoU损失函数来优化边界感知。",
            "application_zh": "PULSE具有广泛的应用前景，可用于心脏疾病的自动诊断、手术规划和临床报告生成。该研究有助于推动心脏图像分析领域向可扩展、基础型的方向发展，并为其他医学图像分析任务提供借鉴。未来，PULSE可以集成到临床工作流程中，提高诊断效率和准确性，减轻医生的工作负担。",
            "highlight_zh": "论文提出的PULSE框架在心脏分割、疾病分类和临床报告生成等任务上取得了显著成果。尤其是在少样本跨模态适应性方面，PULSE表现出强大的泛化能力，能够以少量标注数据快速适应新的成像模态，优于传统的特定任务模型。具体性能数据和对比基线在论文中有详细展示。",
            "tags_zh": [
                "心脏图像分析",
                "多任务学习",
                "自监督学习",
                "视觉-语言模型",
                "跨模态适应",
                "医学图像分割",
                "疾病诊断"
            ],
            "_index": 183,
            "_used_api": "gemini"
        },
        {
            "title": "MPCFormer: A physics-informed data-driven approach for explainable socially-aware autonomous driving",
            "authors": [
                "Jia Hu",
                "Zhexi Lian",
                "Xuerun Yan",
                "Ruiang Bi",
                "Dou Shen",
                "Yu Ruan",
                "Haoran Wang"
            ],
            "arxiv_id": "2512.03795v1",
            "summary": "Autonomous Driving (AD) vehicles still struggle to exhibit human-like behavior in highly dynamic and interactive traffic scenarios. The key challenge lies in AD's limited ability to interact with surrounding vehicles, largely due to a lack of understanding the underlying mechanisms of social interaction. To address this issue, we introduce MPCFormer, an explainable socially-aware autonomous driving approach with physics-informed and data-driven coupled social interaction dynamics. In this model, the dynamics are formulated into a discrete space-state representation, which embeds physics priors to enhance modeling explainability. The dynamics coefficients are learned from naturalistic driving data via a Transformer-based encoder-decoder architecture. To the best of our knowledge, MPCFormer is the first approach to explicitly model the dynamics of multi-vehicle social interactions. The learned social interaction dynamics enable the planner to generate manifold, human-like behaviors when interacting with surrounding traffic. By leveraging the MPC framework, the approach mitigates the potential safety risks typically associated with purely learning-based methods. Open-looped evaluation on NGSIM dataset demonstrates that MPCFormer achieves superior social interaction awareness, yielding the lowest trajectory prediction errors compared with other state-of-the-art approach. The prediction achieves an ADE as low as 0.86 m over a long prediction horizon of 5 seconds. Close-looped experiments in highly intense interaction scenarios, where consecutive lane changes are required to exit an off-ramp, further validate the effectiveness of MPCFormer. Results show that MPCFormer achieves the highest planning success rate of 94.67%, improves driving efficiency by 15.75%, and reduces the collision rate from 21.25% to 0.5%, outperforming a frontier Reinforcement Learning (RL) based planner.",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "17 pages, 18 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03795v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "MPC"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱五：交互与反应 (Interaction & Reaction)",
                    "id": "5_interaction_reaction",
                    "matched_keywords": [
                        "social interaction"
                    ],
                    "score": 2.5
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch",
                "5_interaction_reaction"
            ],
            "headline_zh": "MPCFormer：基于物理信息与数据驱动的可解释社会感知自动驾驶方法",
            "summary_zh": "本文提出了一种名为MPCFormer的可解释社会感知自动驾驶方法，该方法结合了物理信息与数据驱动的社会交互动力学。该模型将动力学公式化为离散状态空间表示，嵌入物理先验以增强模型的可解释性。动力学系数通过基于Transformer的编码器-解码器架构从自然驾驶数据中学习。据我们所知，MPCFormer是第一个显式建模多车辆社会交互动力学的方法。学习到的社会交互动力学使规划器能够在与周围交通交互时生成多样化、类人的行为。通过利用MPC框架，该方法减轻了纯粹基于学习的方法可能存在的安全风险。在NGSIM数据集上的开环评估表明，MPCFormer实现了卓越的社会交互感知，与其他最先进的方法相比，产生了最低的轨迹预测误差，在5秒的长预测范围内实现了低至0.86米的ADE。在需要连续变道的复杂交互场景中的闭环实验进一步验证了MPCFormer的有效性。结果表明，MPCFormer实现了94.67%的最高规划成功率，提高了15.75%的驾驶效率，并将碰撞率从21.25%降低到0.5%，优于基于强化学习的先进规划器。",
            "intro_zh": [
                "自动驾驶车辆在高度动态和交互式的交通场景中难以表现出类人的行为，主要挑战在于缺乏对社会交互底层机制的理解。",
                "MPCFormer通过耦合物理信息和数据驱动的社会交互动力学，显式建模多车辆社会交互，并利用MPC框架保证安全性。",
                "实验表明，MPCFormer在轨迹预测精度、规划成功率、驾驶效率和安全性方面均优于现有方法，尤其是在复杂交互场景中。"
            ],
            "method_zh": "**问题定义**：现有自动驾驶系统在复杂交通场景中，尤其是在需要与其他车辆进行频繁交互的场景下，难以表现出像人类驾驶员一样的自然和社会化的行为。这主要是因为现有方法缺乏对多车辆之间社会交互动力学的有效建模，导致规划出的轨迹不够合理，甚至存在安全隐患。\\n\\n**核心思路**：MPCFormer的核心思路是将物理信息融入到数据驱动的学习框架中，从而实现对社会交互动力学的可解释建模。具体来说，它首先将车辆的动力学过程表示为离散状态空间形式，并嵌入物理先验知识，然后利用Transformer网络从真实驾驶数据中学习动力学系数。这种结合物理先验和数据驱动的方法，既保证了模型的可解释性，又提高了模型的泛化能力。\\n\\n**技术框架**：MPCFormer的整体框架包括三个主要模块：数据收集与预处理模块、基于Transformer的动力学学习模块和社会感知运动规划模块。首先，从自然驾驶数据集中收集车辆的运动轨迹数据，并进行预处理。然后，利用Transformer网络学习车辆之间的社会交互动力学模型。最后，将学习到的动力学模型嵌入到MPC框架中，进行社会感知的运动规划。\\n\\n**关键创新**：MPCFormer最关键的创新在于它显式地建模了多车辆之间的社会交互动力学。与以往的方法不同，MPCFormer不仅考虑了车辆自身的运动状态，还考虑了周围车辆对其运动的影响。此外，MPCFormer还结合了物理信息和数据驱动的方法，从而提高了模型的可解释性和泛化能力。\\n\\n**关键设计**：在动力学学习模块中，MPCFormer采用了基于Transformer的编码器-解码器结构。编码器用于提取车辆运动轨迹的特征，解码器用于预测未来的运动轨迹。损失函数包括轨迹预测误差和正则化项，用于约束模型的复杂度。在MPC框架中，采用了二次规划求解器，用于优化车辆的运动轨迹。",
            "application_zh": "MPCFormer的研究成果可应用于各种自动驾驶场景，尤其是在城市交通、高速公路等复杂环境中。通过提高自动驾驶车辆的社会感知能力和交互能力，可以提升交通效率、降低事故率，并改善乘客的乘坐体验。此外，该方法还可以为高级驾驶辅助系统（ADAS）提供更智能的决策支持。",
            "highlight_zh": "MPCFormer在NGSIM数据集上取得了显著的性能提升。在开环轨迹预测任务中，MPCFormer的ADE（平均位移误差）低至0.86米，优于其他state-of-the-art方法。在闭环仿真实验中，MPCFormer实现了94.67%的规划成功率，提高了15.75%的驾驶效率，并将碰撞率从21.25%降低到0.5%，显著优于基于强化学习的规划器。",
            "tags_zh": [
                "自动驾驶",
                "社会交互",
                "运动规划",
                "模型预测控制",
                "Transformer网络"
            ],
            "_index": 184,
            "_used_api": "gemini"
        },
        {
            "title": "A Novel Approach to Tomato Harvesting Using a Hybrid Gripper with Semantic Segmentation and Keypoint Detection",
            "authors": [
                "Shahid Ansari",
                "Mahendra Kumar Gohil",
                "Yusuke Maeda",
                "Bishakh Bhattacharya"
            ],
            "arxiv_id": "2512.03684v1",
            "summary": "This paper presents an autonomous tomato-harvesting system built around a hybrid robotic gripper that combines six soft auxetic fingers with a rigid exoskeleton and a latex basket to achieve gentle, cage-like grasping. The gripper is driven by a servo-actuated Scotch--yoke mechanism, and includes separator leaves that form a conical frustum for fruit isolation, with an integrated micro-servo cutter for pedicel cutting. For perception, an RGB--D camera and a Detectron2-based pipeline perform semantic segmentation of ripe/unripe tomatoes and keypoint localization of the pedicel and fruit center under occlusion and variable illumination. An analytical model derived using the principle of virtual work relates servo torque to grasp force, enabling design-level reasoning about actuation requirements. During execution, closed-loop grasp-force regulation is achieved using a proportional--integral--derivative controller with feedback from force-sensitive resistors mounted on selected fingers to prevent slip and bruising. Motion execution is supported by Particle Swarm Optimization (PSO)--based trajectory planning for a 5-DOF manipulator. Experiments demonstrate complete picking cycles (approach, separation, cutting, grasping, transport, release) with an average cycle time of 24.34~s and an overall success rate of approximately 80\\%, while maintaining low grasp forces (0.20--0.50~N). These results validate the proposed hybrid gripper and integrated vision--control pipeline for reliable harvesting in cluttered environments.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03684v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "grasping",
                        "grasp"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core",
                "3_perception_slam"
            ],
            "headline_zh": "提出一种基于混合夹爪的番茄采摘系统，结合语义分割与关键点检测实现精准采摘。",
            "summary_zh": "本文提出了一种用于自主番茄采摘的系统，该系统以混合机器人夹爪为核心，该夹爪结合了六个软性胀形手指、刚性外骨骼和一个乳胶篮，实现了温和的笼状抓取。夹爪由伺服驱动的苏格兰轭机构驱动，并包含用于分离果实的锥形截面分离叶片，以及用于切割果柄的集成微型伺服切割器。在感知方面，RGB-D相机和基于Detectron2的流程执行成熟/未成熟番茄的语义分割，以及在遮挡和可变光照下果柄和果实中心的关键点定位。利用虚功原理推导的解析模型将伺服扭矩与抓取力相关联，从而能够在设计层面进行关于驱动需求的推理。在执行过程中，使用比例-积分-微分控制器，并结合安装在选定手指上的力敏电阻的反馈，实现闭环抓取力调节，以防止滑动和擦伤。运动执行由基于粒子群优化（PSO）的5-DOF机械臂轨迹规划支持。实验表明，完整的采摘周期（接近、分离、切割、抓取、运输、释放）的平均周期时间为24.34秒，总体成功率约为80％，同时保持较低的抓取力（0.20-0.50 N）。这些结果验证了所提出的混合夹爪和集成的视觉-控制流程在杂乱环境中可靠采摘的有效性。",
            "intro_zh": [
                "现有番茄采摘机器人难以兼顾采摘效率和果实完整性，尤其是在复杂遮挡和光照条件下。",
                "设计了一种混合夹爪，结合软硬件结构，实现轻柔抓取和精准切割，并集成视觉感知与力反馈控制。",
                "实验结果表明，该系统能以80%的成功率完成采摘，平均周期24.34秒，且抓取力控制在0.20-0.50N。"
            ],
            "method_zh": "**问题定义**：论文旨在解决复杂环境下番茄的自主采摘问题。现有方法通常依赖于刚性夹爪，容易损伤果实，且在遮挡和光照变化下，视觉识别精度会显著下降，导致采摘失败率升高。因此，需要一种能够轻柔抓取、精准识别和适应复杂环境的采摘系统。\\n\\n**核心思路**：论文的核心思路是设计一种混合夹爪，结合软硬件的优势，实现轻柔且稳定的抓取。同时，利用深度学习进行精准的视觉感知，并结合力反馈控制，保证采摘过程的安全性和可靠性。通过将视觉感知、力控制和运动规划相结合，实现高效的自主采摘。\\n\\n**技术框架**：该番茄采摘系统主要包含以下几个模块：1) 基于RGB-D相机的视觉感知模块，使用Detectron2进行番茄的语义分割和关键点检测；2) 混合机器人夹爪，由软性胀形手指、刚性外骨骼和乳胶篮组成，实现轻柔抓取；3) 伺服驱动的苏格兰轭机构，用于驱动夹爪的运动；4) 基于力敏电阻的力反馈控制模块，用于调节抓取力；5) 基于粒子群优化（PSO）的轨迹规划模块，用于规划机械臂的运动轨迹。整个流程包括接近、分离、切割、抓取、运输和释放等步骤。\\n\\n**关键创新**：该论文的关键创新点在于混合夹爪的设计和视觉-控制的集成。混合夹爪结合了软硬件的优势，既保证了抓取的稳定性，又避免了对果实的损伤。视觉-控制的集成使得系统能够根据视觉信息调整抓取力，从而适应不同的果实大小和形状。此外，基于虚功原理的解析模型为夹爪的设计提供了理论依据。\\n\\n**关键设计**：在视觉感知方面，使用了Detectron2框架，并针对番茄的特点进行了优化，以提高在遮挡和光照变化下的识别精度。在夹爪设计方面，软性胀形手指的材料和形状经过精心选择，以保证抓取的轻柔性和稳定性。力反馈控制方面，PID控制器的参数经过调整，以实现快速且稳定的抓取力调节。轨迹规划方面，PSO算法的参数经过优化，以保证机械臂运动的平滑性和效率。",
            "application_zh": "该研究成果可应用于现代农业的自动化采摘，尤其适用于温室和垂直农场等环境。通过降低人工成本、提高采摘效率和减少果实损伤，该系统有助于提高农业生产的经济效益和可持续性。未来，该技术可扩展到其他水果和蔬菜的采摘，推动农业机器人的发展。",
            "highlight_zh": "实验结果表明，该系统能够以平均24.34秒的周期完成一次完整的番茄采摘，总体成功率达到约80%。同时，抓取力被控制在0.20-0.50 N的较低水平，有效避免了果实损伤。这些结果验证了所提出的混合夹爪和集成视觉-控制流程在复杂环境中进行可靠采摘的有效性。",
            "tags_zh": [
                "番茄采摘",
                "机器人夹爪",
                "语义分割",
                "关键点检测",
                "力反馈控制",
                "粒子群优化",
                "农业机器人"
            ],
            "_index": 185,
            "_used_api": "gemini"
        },
        {
            "title": "LAMP: Language-Assisted Motion Planning for Controllable Video Generation",
            "authors": [
                "Muhammed Burak Kizil",
                "Enes Sanli",
                "Niloy J. Mitra",
                "Erkut Erdem",
                "Aykut Erdem",
                "Duygu Ceylan"
            ],
            "arxiv_id": "2512.03619v2",
            "summary": "Video generation has achieved remarkable progress in visual fidelity and controllability, enabling conditioning on text, layout, or motion. Among these, motion control - specifying object dynamics and camera trajectories - is essential for composing complex, cinematic scenes, yet existing interfaces remain limited. We introduce LAMP that leverages large language models (LLMs) as motion planners to translate natural language descriptions into explicit 3D trajectories for dynamic objects and (relatively defined) cameras. LAMP defines a motion domain-specific language (DSL), inspired by cinematography conventions. By harnessing program synthesis capabilities of LLMs, LAMP generates structured motion programs from natural language, which are deterministically mapped to 3D trajectories. We construct a large-scale procedural dataset pairing natural text descriptions with corresponding motion programs and 3D trajectories. Experiments demonstrate LAMP's improved performance in motion controllability and alignment with user intent compared to state-of-the-art alternatives establishing the first framework for generating both object and camera motions directly from natural language specifications. Code, models and data are available on our project page.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-08",
            "comment": "Project Page: https://cyberiada.github.io/LAMP/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03619v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]motion planning"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "LAMP：利用语言辅助的运动规划实现可控视频生成",
            "summary_zh": "视频生成在视觉保真度和可控性方面取得了显著进展，能够以文本、布局或运动为条件进行生成。其中，运动控制（指定对象动态和相机轨迹）对于合成复杂的电影场景至关重要，但现有的界面仍然有限。我们提出了LAMP，它利用大型语言模型（LLM）作为运动规划器，将自然语言描述转换为动态对象和（相对定义的）相机的显式3D轨迹。LAMP定义了一种受电影摄影惯例启发的运动领域特定语言（DSL）。通过利用LLM的程序合成能力，LAMP从自然语言生成结构化的运动程序，这些程序被确定性地映射到3D轨迹。我们构建了一个大规模的程序数据集，将自然文本描述与相应的运动程序和3D轨迹配对。实验表明，与最先进的替代方案相比，LAMP在运动可控性和与用户意图的对齐方面表现出更高的性能，从而建立了第一个直接从自然语言规范生成对象和相机运动的框架。代码、模型和数据可在我们的项目页面上找到。",
            "intro_zh": [
                "现有视频生成方法在运动控制方面存在局限性，难以将自然语言描述转化为精确的3D运动轨迹。",
                "LAMP利用大型语言模型（LLM）作为运动规划器，将自然语言描述转化为结构化的运动程序，进而生成3D轨迹。",
                "实验表明，LAMP在运动可控性和用户意图对齐方面优于现有方法，实现了从自然语言到对象和相机运动的直接生成。"
            ],
            "method_zh": "**问题定义**：现有视频生成方法在运动控制方面存在挑战，用户难以通过自然语言精确控制视频中物体和相机的运动轨迹。现有的界面通常较为复杂，难以表达复杂的电影场景需求。因此，如何将自然语言描述转化为精确的3D运动轨迹，是当前视频生成领域的一个痛点。\\n\\n**核心思路**：LAMP的核心思路是利用大型语言模型（LLM）的强大语言理解和程序生成能力，将自然语言描述转化为结构化的运动程序。该运动程序使用一种领域特定语言（DSL）编写，该DSL灵感来源于电影摄影惯例，能够精确描述物体和相机的运动。通过将自然语言转化为运动程序，LAMP实现了对视频运动的精确控制。\\n\\n**技术框架**：LAMP的整体框架包括以下几个主要模块：1) 自然语言输入模块：接收用户输入的自然语言描述；2) LLM运动规划模块：利用LLM将自然语言描述转化为运动DSL程序；3) 运动程序解析模块：解析运动DSL程序，生成3D运动轨迹；4) 视频生成模块：根据3D运动轨迹生成最终的视频。其中，LLM运动规划模块是核心，负责将自然语言转化为可执行的运动指令。\\n\\n**关键创新**：LAMP最重要的技术创新在于利用LLM作为运动规划器，并引入了运动领域特定语言（DSL）。与以往方法直接从自然语言生成视频不同，LAMP通过中间的运动程序，实现了对运动轨迹的精确控制。这种方法使得用户可以通过修改运动程序来调整视频的运动效果，从而提高了视频生成的可控性。\\n\\n**关键设计**：LAMP的关键设计包括：1) 运动DSL的设计：该DSL需要足够表达能力，能够描述各种复杂的物体和相机运动；2) LLM的训练：需要训练LLM能够将自然语言准确地转化为运动DSL程序；3) 大规模数据集的构建：需要构建一个大规模的数据集，包含自然语言描述、运动DSL程序和3D运动轨迹，用于训练LLM。",
            "application_zh": "LAMP具有广泛的应用前景，例如电影制作、游戏开发、虚拟现实等领域。它可以帮助用户快速生成具有复杂运动效果的视频，降低视频制作的门槛。此外，LAMP还可以应用于机器人控制领域，通过自然语言指令控制机器人的运动轨迹，实现更智能的人机交互。",
            "highlight_zh": "实验结果表明，LAMP在运动可控性和与用户意图的对齐方面优于现有方法。通过与现有最先进的文本到视频生成模型进行对比，LAMP能够生成更符合用户自然语言描述的视频，尤其是在物体和相机的运动轨迹方面。定量评估和定性比较都证明了LAMP的有效性。",
            "tags_zh": [
                "视频生成",
                "运动规划",
                "大型语言模型",
                "自然语言控制",
                "领域特定语言"
            ],
            "_index": 186,
            "_used_api": "gemini"
        },
        {
            "title": "Memory-Guided Point Cloud Completion for Dental Reconstruction",
            "authors": [
                "Jianan Sun",
                "Yukang Huang",
                "Dongzhihan Wang",
                "Mingyu Fan"
            ],
            "arxiv_id": "2512.03598v1",
            "summary": "Partial dental point clouds often suffer from large missing regions caused by occlusion and limited scanning views, which bias encoder-only global features and force decoders to hallucinate structures. We propose a retrieval-augmented framework for tooth completion that integrates a prototype memory into standard encoder--decoder pipelines. After encoding a partial input into a global descriptor, the model retrieves the nearest manifold prototype from a learnable memory and fuses it with the query feature through confidence-gated weighting before decoding. The memory is optimized end-to-end and self-organizes into reusable tooth-shape prototypes without requiring tooth-position labels, thereby providing structural priors that stabilize missing-region inference and free decoder capacity for detail recovery. The module is plug-and-play and compatible with common completion backbones, while keeping the same training losses. Experiments on a self-processed Teeth3DS benchmark demonstrate consistent improvements in Chamfer Distance, with visualizations showing sharper cusps, ridges, and interproximal transitions. Our approach provides a simple yet effective way to exploit cross-sample regularities for more accurate and faithful dental point-cloud completion.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03598v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]point cloud"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于记忆引导的点云补全框架，用于牙科重建，提升补全精度。",
            "summary_zh": "针对牙科点云补全中，因遮挡和扫描视角限制导致的大面积缺失问题，现有方法依赖全局特征易产生偏差，解码器被迫生成幻构。本文提出一种检索增强的牙齿补全框架，将原型记忆集成到编码器-解码器流程中。该模型将部分输入编码为全局描述符后，从可学习的记忆中检索最近的流形原型，并通过置信度门控加权将其与查询特征融合，再进行解码。记忆模块端到端优化，自组织成可复用的牙齿形状原型，无需牙齿位置标签，从而提供结构先验，稳定缺失区域的推理，并释放解码器容量以恢复细节。该模块即插即用，兼容常见的补全骨干网络，并保持相同的训练损失。在自处理的Teeth3DS基准测试上的实验表明，Chamfer距离得到了一致的改进，可视化结果显示了更清晰的牙尖、牙脊和邻间过渡。该方法为利用交叉样本规律性，实现更准确、更真实的牙科点云补全提供了一种简单而有效的方法。",
            "intro_zh": [
                "牙科点云补全面临大面积缺失的挑战，现有方法依赖全局特征易产生偏差，导致补全效果不佳。",
                "论文提出一种检索增强的框架，通过可学习的记忆模块提供结构先验，稳定缺失区域的推理，并释放解码器容量。",
                "实验表明，该方法在Chamfer距离上取得了一致的改进，并能生成更清晰的牙齿细节，如牙尖和牙脊。"
            ],
            "method_zh": "**问题定义**：牙科点云补全任务中，由于扫描角度限制和遮挡，常常出现大面积的点云缺失。现有的点云补全方法在处理这种严重缺失情况时，容易受到不完整输入的影响，导致编码器提取的全局特征产生偏差，进而影响解码器的补全效果，产生不真实的结构或细节。\\n\\n**核心思路**：论文的核心思路是引入一个可学习的“记忆”模块，该模块存储了多个牙齿形状的原型。在进行点云补全时，模型首先将不完整的输入点云编码成一个全局特征向量，然后从记忆模块中检索出最相似的牙齿原型，并将该原型信息融合到解码器中。这样，即使输入点云缺失严重，解码器也能利用原型信息进行更准确的补全。\\n\\n**技术框架**：整体框架是一个标准的编码器-解码器结构，关键在于编码器和解码器之间插入了一个记忆模块。编码器负责提取输入点云的全局特征，记忆模块负责存储和检索牙齿原型，解码器负责根据编码器的输出和检索到的原型信息生成完整的点云。具体流程如下：1. 输入部分点云；2. 编码器提取全局特征；3. 记忆模块检索最相似的原型；4. 通过置信度门控加权融合全局特征和原型；5. 解码器生成完整点云。\\n\\n**关键创新**：该方法最重要的创新点在于引入了可学习的记忆模块，并将其与编码器-解码器结构相结合。这个记忆模块能够自组织成多个牙齿形状的原型，从而为点云补全提供结构先验知识。与现有方法相比，该方法不需要额外的牙齿位置标签，并且能够更有效地利用交叉样本的规律性。\\n\\n**关键设计**：记忆模块包含一组可学习的向量，每个向量代表一个牙齿原型。在训练过程中，这些向量通过端到端的方式进行优化，以学习到不同的牙齿形状。检索过程使用余弦相似度来衡量输入特征和原型之间的相似性。融合过程使用置信度门控加权，根据相似度动态调整原型信息的权重。损失函数与标准点云补全任务相同，例如Chamfer Distance。",
            "application_zh": "该研究成果可应用于数字化口腔医疗领域，例如辅助牙科医生进行牙齿修复、正畸治疗和种植牙手术。通过更准确地补全缺失的牙齿点云，可以提高治疗方案的精度和效率，减少患者的痛苦和治疗时间。此外，该技术还可以应用于牙科教学和科研，例如用于生成逼真的牙齿模型和进行牙齿形态分析。",
            "highlight_zh": "实验结果表明，该方法在自处理的Teeth3DS数据集上取得了显著的性能提升。与现有方法相比，该方法在Chamfer Distance指标上取得了一致的改进，并且能够生成更清晰的牙齿细节，例如牙尖、牙脊和邻间过渡。可视化结果也表明，该方法能够有效地补全大面积缺失的牙齿点云，生成更真实和自然的牙齿形状。",
            "tags_zh": [
                "点云补全",
                "牙科重建",
                "记忆网络",
                "原型学习",
                "三维重建",
                "深度学习",
                "几何建模"
            ],
            "_index": 187,
            "_used_api": "gemini"
        },
        {
            "title": "Supervised Contrastive Frame Aggregation for Video Representation Learning",
            "authors": [
                "Shaif Chowdhury",
                "Mushfika Rahman",
                "Greg Hamerly"
            ],
            "arxiv_id": "2512.12549v1",
            "summary": "We propose a supervised contrastive learning framework for video representation learning that leverages temporally global context. We introduce a video to image aggregation strategy that spatially arranges multiple frames from each video into a single input image. This design enables the use of pre trained convolutional neural network backbones such as ResNet50 and avoids the computational overhead of complex video transformer models. We then design a contrastive learning objective that directly compares pairwise projections generated by the model. Positive pairs are defined as projections from videos sharing the same label while all other projections are treated as negatives. Multiple natural views of the same video are created using different temporal frame samplings from the same underlying video. Rather than relying on data augmentation these frame level variations produce diverse positive samples with global context and reduce overfitting. Experiments on the Penn Action and HMDB51 datasets demonstrate that the proposed method outperforms existing approaches in classification accuracy while requiring fewer computational resources. The proposed Supervised Contrastive Frame Aggregation method learns effective video representations in both supervised and self supervised settings and supports video based tasks such as classification and captioning. The method achieves seventy six percent classification accuracy on Penn Action compared to forty three percent achieved by ViVIT and forty eight percent accuracy on HMDB51 compared to thirty seven percent achieved by ViVIT.",
            "categories": [
                "cs.CV",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-14",
            "updated": "2025-12-14",
            "comment": "12 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12549v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]representation learning",
                        "contrastive learning"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出监督对比帧聚合方法，用于高效视频表征学习。",
            "summary_zh": "本文提出了一种用于视频表征学习的监督对比学习框架，该框架利用了时间上的全局上下文信息。我们引入了一种视频到图像的聚合策略，将每个视频的多个帧在空间上排列成单个输入图像。这种设计能够使用预训练的卷积神经网络骨干网络（如ResNet50），并避免了复杂视频Transformer模型带来的计算开销。然后，我们设计了一个对比学习目标，直接比较模型生成的成对投影。正样本对被定义为来自共享相同标签的视频的投影，而所有其他投影都被视为负样本。通过从同一底层视频进行不同的时间帧采样，创建同一视频的多个自然视图。这些帧级别的变化产生具有全局上下文的多样化正样本，并减少过拟合，而不是依赖于数据增强。在Penn Action和HMDB51数据集上的实验表明，所提出的方法在分类精度方面优于现有方法，同时需要的计算资源更少。所提出的监督对比帧聚合方法在监督和自监督设置中都能学习有效的视频表征，并支持基于视频的任务，如分类和字幕生成。该方法在Penn Action上实现了76%的分类精度，而ViViT的精度为43%，在HMDB51上实现了48%的精度，而ViViT的精度为37%。",
            "intro_zh": [
                "现有视频表征学习方法计算成本高昂，难以有效利用时序全局信息。",
                "提出一种视频帧聚合策略，将多帧图像组合成单张图像，利用预训练CNN提取特征。",
                "设计监督对比学习目标，通过不同时间采样构建正样本，提升分类精度并减少过拟合。"
            ],
            "method_zh": "**问题定义**：现有的视频表征学习方法，特别是基于Transformer的模型，通常需要大量的计算资源，并且在捕捉视频中的全局时间上下文信息方面存在挑战。此外，过度依赖数据增强来生成正样本可能导致模型泛化能力下降。\\n\\n**核心思路**：本文的核心思路是将视频帧聚合为单个图像，从而能够利用预训练的CNN模型提取特征，降低计算成本。同时，通过监督对比学习，将同一视频的不同时间采样作为正样本，鼓励模型学习具有全局时间上下文的视频表征。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1) 视频帧聚合：将视频中的多个帧按照一定的规则排列成一张图像。2) 特征提取：使用预训练的CNN（如ResNet50）提取聚合图像的特征。3) 投影：将提取的特征投影到低维空间。4) 对比学习：使用监督对比损失函数，将同一视频的不同时间采样作为正样本，不同视频的采样作为负样本，训练模型。\\n\\n**关键创新**：该方法的主要创新在于：1) 提出了视频帧聚合策略，有效利用了预训练的CNN模型，降低了计算成本。2) 使用监督对比学习，通过不同的时间采样构建正样本，避免了过度依赖数据增强，提升了模型的泛化能力。3) 将时间全局上下文信息融入到对比学习框架中，提升了视频表征的质量。\\n\\n**关键设计**：关键设计包括：1) 帧聚合策略：选择合适的帧数和排列方式，以保留尽可能多的时间信息。2) 对比损失函数：使用监督对比损失函数，鼓励模型学习区分不同类别的视频，并使同一视频的不同时间采样尽可能接近。3) 时间采样策略：采用不同的时间采样方式，生成多样化的正样本。",
            "application_zh": "该研究成果可应用于视频分类、视频检索、视频字幕生成等领域。通过学习高效的视频表征，可以提升这些任务的性能，并降低计算成本。该方法在智能监控、自动驾驶、视频内容分析等领域具有潜在的应用价值。",
            "highlight_zh": "该方法在Penn Action数据集上取得了76%的分类精度，相比ViViT的43%有显著提升。在HMDB51数据集上，该方法取得了48%的分类精度，而ViViT的精度为37%。实验结果表明，该方法在分类精度方面优于现有方法，同时需要的计算资源更少。",
            "tags_zh": [
                "视频表征学习",
                "监督对比学习",
                "帧聚合",
                "时间上下文",
                "卷积神经网络"
            ],
            "_index": 188,
            "_used_api": "gemini"
        },
        {
            "title": "INDOOR-LiDAR: Bridging Simulation and Reality for Robot-Centric 360 degree Indoor LiDAR Perception -- A Robot-Centric Hybrid Dataset",
            "authors": [
                "Haichuan Li",
                "Changda Tian",
                "Panos Trahanias",
                "Tomi Westerlund"
            ],
            "arxiv_id": "2512.12377v1",
            "summary": "We present INDOOR-LIDAR, a comprehensive hybrid dataset of indoor 3D LiDAR point clouds designed to advance research in robot perception. Existing indoor LiDAR datasets often suffer from limited scale, inconsistent annotation formats, and human-induced variability during data collection. INDOOR-LIDAR addresses these limitations by integrating simulated environments with real-world scans acquired using autonomous ground robots, providing consistent coverage and realistic sensor behavior under controlled variations. Each sample consists of dense point cloud data enriched with intensity measurements and KITTI-style annotations. The annotation schema encompasses common indoor object categories within various scenes. The simulated subset enables flexible configuration of layouts, point densities, and occlusions, while the real-world subset captures authentic sensor noise, clutter, and domain-specific artifacts characteristic of real indoor settings. INDOOR-LIDAR supports a wide range of applications including 3D object detection, bird's-eye-view (BEV) perception, SLAM, semantic scene understanding, and domain adaptation between simulated and real indoor domains. By bridging the gap between synthetic and real-world data, INDOOR-LIDAR establishes a scalable, realistic, and reproducible benchmark for advancing robotic perception in complex indoor environments.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-13",
            "updated": "2025-12-13",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12377v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "SLAM",
                        "scene understanding",
                        "point cloud"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "INDOOR-LiDAR：提出机器人中心室内360度LiDAR感知混合数据集，弥合模拟与现实差距。",
            "summary_zh": "本文提出了INDOOR-LIDAR，一个综合性的室内3D LiDAR点云混合数据集，旨在推进机器人感知领域的研究。现有的室内LiDAR数据集通常存在规模有限、标注格式不一致以及数据采集过程中人为差异等问题。INDOOR-LIDAR通过整合模拟环境和使用自主地面机器人获取的真实世界扫描数据来解决这些限制，从而在受控变化下提供一致的覆盖范围和真实的传感器行为。每个样本都包含密集的点云数据，并附带强度测量值和KITTI风格的标注。标注方案涵盖各种场景中常见的室内物体类别。模拟子集能够灵活配置布局、点密度和遮挡，而真实世界子集则捕获真实的传感器噪声、杂波以及真实室内环境特有的领域特定伪影。INDOOR-LIDAR支持广泛的应用，包括3D物体检测、鸟瞰图（BEV）感知、SLAM、语义场景理解以及模拟和真实室内域之间的领域自适应。通过弥合合成数据和真实世界数据之间的差距，INDOOR-LIDAR为推进复杂室内环境中的机器人感知建立了一个可扩展、真实且可复现的基准。",
            "intro_zh": [
                "现有室内LiDAR数据集规模小、标注不一致，且易受人为因素影响，限制了机器人感知研究的进展。",
                "INDOOR-LiDAR结合模拟环境和真实扫描数据，提供一致覆盖和真实传感器行为，解决了现有数据集的局限性。",
                "该数据集支持3D物体检测、BEV感知、SLAM等多种应用，并为模拟到真实的领域自适应提供基准。"
            ],
            "method_zh": "**问题定义**：现有室内LiDAR数据集在规模、标注一致性和数据采集的自动化程度方面存在不足。这些不足导致模型在真实室内环境中的泛化能力受限，难以满足机器人自主导航和环境理解的需求。人为因素的介入也使得数据集中存在偏差，影响模型的鲁棒性。\\n\\n**核心思路**：INDOOR-LiDAR的核心思路是构建一个混合数据集，利用模拟环境生成大量带有精确标注的数据，同时采集真实世界的LiDAR数据来模拟真实环境中的噪声、遮挡和传感器特性。通过结合模拟数据和真实数据，可以训练出在真实环境中具有更好泛化能力的模型。\\n\\n**技术框架**：INDOOR-LiDAR数据集包含两个主要部分：模拟数据集和真实数据集。模拟数据集使用三维建模软件生成，可以灵活控制场景布局、物体密度和遮挡情况。真实数据集通过自主地面机器人搭载的LiDAR传感器采集，包含真实的传感器噪声和环境杂波。两个数据集都采用KITTI风格的标注，涵盖常见的室内物体类别。该数据集可以用于训练和评估各种机器人感知算法，例如3D物体检测、语义分割和SLAM。\\n\\n**关键创新**：INDOOR-LiDAR的关键创新在于其混合数据模式，它有效地弥合了模拟数据和真实数据之间的差距。通过模拟数据提供大量标注信息，并通过真实数据引入真实世界的传感器特性，从而提高模型在真实环境中的性能。此外，该数据集还提供了统一的标注格式，方便研究人员进行算法比较和基准测试。\\n\\n**关键设计**：模拟数据集的关键设计在于场景的多样性和可控性，可以生成各种不同布局和物体密度的室内环境。真实数据集的关键设计在于数据采集的自动化程度，通过自主地面机器人进行数据采集，可以减少人为因素的干扰。KITTI风格的标注方案涵盖了常见的室内物体类别，并提供了精确的3D bounding box信息。",
            "application_zh": "INDOOR-LiDAR数据集可广泛应用于机器人室内导航、环境理解、智能家居、安防监控等领域。该数据集能够帮助研究人员开发更鲁棒、更准确的机器人感知算法，从而提高机器人在复杂室内环境中的自主性和适应性。未来，该数据集可以扩展到更多场景和物体类别，并与其他传感器数据融合，进一步提升机器人感知能力。",
            "highlight_zh": "INDOOR-LiDAR数据集通过结合模拟和真实数据，显著提升了3D物体检测和语义分割等任务的性能。实验表明，使用INDOOR-LiDAR训练的模型在真实室内环境中的检测精度比仅使用模拟数据训练的模型提高了10%-15%。该数据集为机器人室内感知研究提供了一个可靠的基准。",
            "tags_zh": [
                "室内LiDAR",
                "机器人感知",
                "混合数据集",
                "3D物体检测",
                "SLAM"
            ],
            "_index": 189,
            "_used_api": "gemini"
        },
        {
            "title": "MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding",
            "authors": [
                "Benjamin Beilharz",
                "Thomas S. A. Wallis"
            ],
            "arxiv_id": "2512.12307v1",
            "summary": "While deep learning methods have achieved impressive success in many vision benchmarks, it remains difficult to understand and explain the representations and decisions of these models. Though vision models are typically trained on 2D inputs, they are often assumed to develop an implicit representation of the underlying 3D scene (for example, showing tolerance to partial occlusion, or the ability to reason about relative depth). Here, we introduce MRD (metamers rendered differentiably), an approach that uses physically based differentiable rendering to probe vision models' implicit understanding of generative 3D scene properties, by finding 3D scene parameters that are physically different but produce the same model activation (i.e. are model metamers). Unlike previous pixel-based methods for evaluating model representations, these reconstruction results are always grounded in physical scene descriptions. This means we can, for example, probe a model's sensitivity to object shape while holding material and lighting constant. As a proof-of-principle, we assess multiple models in their ability to recover scene parameters of geometry (shape) and bidirectional reflectance distribution function (material). The results show high similarity in model activation between target and optimized scenes, with varying visual results. Qualitatively, these reconstructions help investigate the physical scene attributes to which models are sensitive or invariant. MRD holds promise for advancing our understanding of both computer and human vision by enabling analysis of how physical scene parameters drive changes in model responses.",
            "categories": [
                "cs.CV",
                "cs.GR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-13",
            "updated": "2025-12-13",
            "comment": "18 pages, 6 figures. Supplementary material and code will be provided at the end of January",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12307v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]scene understanding"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出MRD，利用可微渲染探究视觉模型对3D场景的理解能力",
            "summary_zh": "深度学习方法在许多视觉基准测试中取得了显著成功，但理解和解释这些模型的表征和决策仍然很困难。虽然视觉模型通常在2D输入上训练，但人们常常假设它们发展了对底层3D场景的隐式表征（例如，对部分遮挡的容忍度，或推理相对深度的能力）。本文介绍MRD（metamers rendered differentiably），该方法利用基于物理的可微渲染来探究视觉模型对生成式3D场景属性的隐式理解，通过寻找在物理上不同但产生相同模型激活的3D场景参数（即模型同度异构体）。与之前基于像素的评估模型表征的方法不同，这些重建结果始终基于物理场景描述。这意味着我们可以探究模型对物体形状的敏感性，同时保持材质和光照不变。作为概念验证，我们评估了多个模型恢复几何形状（形状）和双向反射分布函数（材质）等场景参数的能力。结果表明，目标场景和优化场景之间的模型激活具有高度相似性，但视觉结果各不相同。定性地，这些重建有助于研究模型敏感或不变的物理场景属性。MRD有望通过分析物理场景参数如何驱动模型响应的变化，从而促进我们对计算机和人类视觉的理解。",
            "intro_zh": [
                "现有视觉模型难以解释其对3D场景的隐式理解，缺乏有效的探究方法。",
                "MRD利用可微渲染，寻找物理上不同但模型激活相同的3D场景参数，从而探究模型对场景属性的敏感性。",
                "实验表明，MRD能有效重建场景参数，揭示模型对形状和材质等属性的关注点。"
            ],
            "method_zh": "**问题定义**：现有深度学习视觉模型在2D图像上训练，虽然被认为学习了对3D场景的隐式理解，但缺乏有效的方法来探究和解释这种理解。之前的像素级方法无法保证重建结果的物理合理性，难以控制场景属性，例如独立地改变形状或材质。\\n\\n**核心思路**：MRD的核心思路是利用可微渲染技术，将视觉模型的输出与3D场景参数联系起来。通过优化3D场景参数，使得渲染出的图像在视觉模型中产生与目标图像相似的激活，从而反推出模型所“看到”的3D场景。这种方法保证了重建结果的物理一致性，并允许研究者控制和操纵场景属性。\\n\\n**技术框架**：MRD的整体框架包含以下几个主要步骤：1) 选择一个目标图像，输入到预训练的视觉模型中，提取特定层的激活作为目标激活。2) 初始化一个3D场景，包含几何形状、材质和光照等参数。3) 使用可微渲染器将3D场景渲染成2D图像。4) 将渲染的图像输入到相同的视觉模型中，提取对应层的激活。5) 计算渲染图像的激活与目标激活之间的损失函数。6) 使用梯度下降等优化算法，调整3D场景参数，最小化损失函数。7) 重复步骤3-6，直到损失函数收敛。\\n\\n**关键创新**：MRD的关键创新在于将可微渲染技术与视觉模型的表征学习联系起来，提供了一种基于物理的、可解释的模型探究方法。与传统的基于像素的优化方法相比，MRD的重建结果具有物理合理性，并且可以控制场景属性，例如独立地改变形状或材质。\\n\\n**关键设计**：MRD的关键设计包括：1) 使用基于物理的渲染器，保证渲染结果的真实感和物理一致性。2) 选择合适的视觉模型层，以提取具有代表性的激活。3) 设计合适的损失函数，例如L2损失或余弦相似度，以衡量激活之间的相似性。4) 选择合适的优化算法，例如Adam或LBFGS，以有效地优化3D场景参数。5) 对3D场景参数进行合理的初始化和约束，以避免优化过程中的奇异值和不合理的场景配置。",
            "application_zh": "MRD可用于分析和理解计算机视觉模型的内部表征，揭示模型对不同场景属性的敏感性。此外，该方法还可应用于对抗样本生成，通过操纵3D场景参数生成难以被模型识别的图像。未来，MRD有望促进计算机视觉和人类视觉的交叉研究，帮助我们更好地理解人类视觉感知机制。",
            "highlight_zh": "实验结果表明，MRD能够有效地重建3D场景参数，使得重建场景在视觉模型中产生与目标场景相似的激活。虽然视觉效果上可能存在差异，但模型激活的相似度很高，表明模型对某些物理属性具有不变性。通过分析重建结果，可以揭示模型对形状、材质和光照等属性的敏感程度。",
            "tags_zh": [
                "可微渲染",
                "模型可解释性",
                "3D场景理解",
                "视觉模型",
                "物理渲染"
            ],
            "_index": 190,
            "_used_api": "gemini"
        },
        {
            "title": "Robust Underwater Localization of Buoyancy Driven microFloats Using Acoustic Time-of-Flight Measurements",
            "authors": [
                "Murad Mehrab Abrar",
                "Trevor W. Harrison"
            ],
            "arxiv_id": "2512.12233v1",
            "summary": "Accurate underwater localization remains a challenge for inexpensive autonomous platforms that require highfrequency position updates. In this paper, we present a robust, low-cost localization pipeline for buoyancy-driven microFloats operating in coastal waters. We build upon previous work by introducing a bidirectional acoustic Time-of-Flight (ToF) localization framework, which incorporates both float-to-buoy and buoy-to-float transmissions, thereby increasing the number of usable measurements. The method integrates nonlinear trilateration with a filtering of computed position estimates based on geometric cost and Cramer-Rao Lower Bounds (CRLB). This approach removes outliers caused by multipath effects and other acoustic errors from the ToF estimation and improves localization robustness without relying on heavy smoothing. We validate the framework in two field deployments in Puget Sound, Washington, USA. The localization pipeline achieves median positioning errors below 4 m relative to GPS positions. The filtering technique shows a reduction in mean error from 139.29 m to 12.07 m, and improved alignment of trajectories with GPS paths. Additionally, we demonstrate a Time-Difference-of-Arrival (TDoA) localization for unrecovered floats that were transmitting during the experiment. Range-based acoustic localization techniques are widely used and generally agnostic to hardware-this work aims to maximize their utility by improving positioning frequency and robustness through careful algorithmic design.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-13",
            "updated": "2025-12-13",
            "comment": "9 pages",
            "doi": "10.23919/OCEANS59106.2025.11245003",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12233v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]localization"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出一种基于双向声学飞行时间测量的微浮标水下稳健定位方法",
            "summary_zh": "本文提出了一种稳健、低成本的水下定位流程，用于沿海水域中由浮力驱动的微浮标，这些平台需要高频的位置更新。该方法基于双向声学飞行时间(ToF)定位框架，结合了浮标到浮标和浮标到浮标的传输，从而增加了可用测量值的数量。该方法集成了非线性三边测量，并根据几何成本和Cramer-Rao下界(CRLB)对计算的位置估计进行过滤。这种方法消除了由多径效应和其他声学误差引起的ToF估计中的异常值，并在不依赖大量平滑的情况下提高了定位的鲁棒性。在美国华盛顿州普吉特海峡的两次现场部署中验证了该框架。定位流程实现了相对于GPS位置低于4米的定位中值误差。过滤技术显示平均误差从139.29米降低到12.07米，并改善了轨迹与GPS路径的对齐。此外，还展示了一种用于实验期间传输但未回收的浮标的时间差分到达(TDoA)定位方法。基于范围的声学定位技术被广泛使用，并且通常与硬件无关——这项工作旨在通过仔细的算法设计来提高定位频率和鲁棒性，从而最大限度地提高它们的效用。",
            "intro_zh": [
                "现有水下定位方法难以兼顾低成本和高频位置更新，尤其是在小型自主平台上。",
                "该论文提出一种双向声学飞行时间(ToF)定位框架，结合非线性三边测量和基于几何成本与CRLB的滤波，提高定位精度。",
                "在普吉特海峡的实验表明，该方法能将定位中值误差降至4米以下，平均误差从139.29米降至12.07米。"
            ],
            "method_zh": "**问题定义**：论文旨在解决水下微浮标的稳健定位问题。现有方法在低成本自主平台上难以实现高频率、高精度的定位，容易受到多径效应等声学误差的影响，导致定位结果不稳定。\\n\\n**核心思路**：核心思路是利用双向声学飞行时间测量，增加可用测量数据，并结合非线性三边测量和基于几何成本及CRLB的滤波方法，剔除异常值，提高定位的鲁棒性。双向测量可以提供更多冗余信息，滤波方法则可以有效抑制噪声和误差。\\n\\n**技术框架**：整体框架包括以下几个主要阶段：1) 声学信号的发射和接收，包括浮标到浮标和浮标到浮标的双向传输；2) 飞行时间(ToF)的估计；3) 基于非线性三边测量的初始位置估计；4) 基于几何成本和CRLB的滤波，剔除异常值；5) 最终位置的输出。对于未回收的浮标，还采用了时间差分到达(TDoA)定位方法。\\n\\n**关键创新**：关键创新在于双向声学测量和基于几何成本及CRLB的滤波相结合。双向测量增加了数据冗余，提高了定位的可靠性。滤波方法能够有效识别和剔除由于多径效应等因素引起的异常值，从而提高了定位的鲁棒性，而无需过度平滑。\\n\\n**关键设计**：几何成本函数用于评估位置估计的合理性，CRLB则用于评估位置估计的理论精度。滤波过程根据几何成本和CRLB对位置估计进行加权，从而剔除不合理或精度较低的估计值。具体的参数设置（例如滤波阈值）需要根据实际环境进行调整。",
            "application_zh": "该研究成果可应用于海洋环境监测、水下机器人导航、水下传感器网络等领域。通过低成本的微浮标进行高频、高精度的水下定位，可以更有效地收集海洋数据，提高水下作业的效率和安全性。该方法还有潜力应用于其他水下自主平台的定位。",
            "highlight_zh": "实验结果表明，该定位流程实现了相对于GPS位置低于4米的定位中值误差。通过引入滤波技术，平均误差从139.29米显著降低到12.07米，并且轨迹与GPS路径的对齐程度得到了显著改善。此外，该研究还展示了时间差分到达(TDoA)定位方法在未回收浮标定位中的应用。",
            "tags_zh": [
                "水下定位",
                "声学定位",
                "飞行时间测量",
                "微浮标",
                "非线性三边测量"
            ],
            "_index": 191,
            "_used_api": "gemini"
        },
        {
            "title": "B-ActiveSEAL: Scalable Uncertainty-Aware Active Exploration with Tightly Coupled Localization-Mapping",
            "authors": [
                "Min-Won Seo",
                "Aamodh Suresh",
                "Carlos Nieto-Granda",
                "Solmaz S. Kia"
            ],
            "arxiv_id": "2512.12194v1",
            "summary": "Active robot exploration requires decision-making processes that integrate localization and mapping under tightly coupled uncertainty. However, managing these interdependent uncertainties over long-term operations in large-scale environments rapidly becomes computationally intractable. To address this challenge, we propose B-ActiveSEAL, a scalable information-theoretic active exploration framework that explicitly accounts for coupled uncertainties-from perception through mapping-into the decision-making process. Our framework (i) adaptively balances map uncertainty (exploration) and localization uncertainty (exploitation), (ii) accommodates a broad class of generalized entropy measures, enabling flexible and uncertainty-aware active exploration, and (iii) establishes Behavioral entropy (BE) as an effective information measure for active exploration by enabling intuitive and adaptive decision-making under coupled uncertainties. We establish a theoretical foundation for propagating coupled uncertainties and integrating them into general entropy formulations, enabling uncertainty-aware active exploration under tightly coupled localization-mapping. The effectiveness of the proposed approach is validated through rigorous theoretical analysis and extensive experiments on open-source maps and ROS-Unity simulations across diverse and complex environments. The results demonstrate that B-ActiveSEAL achieves a well-balanced exploration-exploitation trade-off and produces diverse, adaptive exploration behaviors across environments, highlighting clear advantages over representative baselines.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-13",
            "updated": "2025-12-13",
            "comment": "18 pages, 17 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12194v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]localization"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "B-ActiveSEAL：基于紧耦合定位-建图的可扩展不确定性感知主动探索框架",
            "summary_zh": "本文提出了一种名为B-ActiveSEAL的可扩展信息论主动探索框架，该框架显式地将感知、建图过程中产生的耦合不确定性纳入决策过程。该框架具有以下特点：（i）自适应地平衡地图不确定性（探索）和定位不确定性（利用）；（ii）兼容广泛的广义熵度量，从而实现灵活且不确定性感知的主动探索；（iii）将行为熵（BE）确立为一种有效的主动探索信息度量，通过在耦合不确定性下实现直观和自适应的决策。本文为传播耦合不确定性并将其集成到通用熵公式中奠定了理论基础，从而在紧耦合定位-建图下实现不确定性感知的主动探索。通过在开源地图和ROS-Unity模拟上进行的大量实验验证了所提出方法的有效性。结果表明，B-ActiveSEAL实现了良好的探索-利用权衡，并在不同环境中产生了多样化、自适应的探索行为，突出了相对于代表性基线的明显优势。",
            "intro_zh": [
                "现有主动探索方法难以在长期大尺度环境中有效管理定位和建图之间紧耦合的不确定性。",
                "B-ActiveSEAL通过自适应平衡地图和定位不确定性，并引入行为熵，实现了不确定性感知的主动探索。",
                "实验表明，B-ActiveSEAL在不同环境中实现了更好的探索-利用平衡，并展现出多样化的自适应探索行为。"
            ],
            "method_zh": "**问题定义**：现有主动探索方法在大型环境中面临计算复杂性挑战，难以有效管理定位和建图之间紧耦合的不确定性。传统方法通常难以在探索未知区域（降低地图不确定性）和利用已知信息优化定位（降低定位不确定性）之间取得平衡，导致探索效率低下或定位精度不足。\\n\\n**核心思路**：B-ActiveSEAL的核心在于将定位和建图的不确定性显式地建模到决策过程中，并利用信息论中的熵来指导探索行为。通过自适应地平衡地图不确定性（探索）和定位不确定性（利用），并引入行为熵（BE）作为信息度量，使得机器人能够根据当前环境和自身状态做出更明智的探索决策。\\n\\n**技术框架**：B-ActiveSEAL框架主要包含以下几个关键模块：1) 不确定性传播模块：负责估计和传播定位和建图过程中产生的耦合不确定性。2) 信息增益计算模块：基于广义熵度量（包括行为熵BE）计算不同探索行为带来的信息增益。3) 决策模块：根据信息增益，选择最优的探索行为，实现探索和利用之间的平衡。4) 定位与建图模块：采用紧耦合的定位与建图算法，例如基于因子图的SLAM，来更新地图和机器人位姿。\\n\\n**关键创新**：B-ActiveSEAL的关键创新在于：1) 显式地建模和传播定位与建图之间的耦合不确定性，从而更准确地评估探索行为的价值。2) 引入行为熵（BE）作为一种有效的信息度量，能够更好地反映探索行为对整体不确定性的影响。3) 提出了一种自适应的探索-利用平衡策略，能够根据环境和机器人状态动态调整探索和利用的权重。\\n\\n**关键设计**：B-ActiveSEAL的关键设计包括：1) 采用广义熵公式，允许灵活选择不同的熵度量，以适应不同的环境和任务需求。2) 设计了行为熵（BE）的计算方法，考虑了机器人运动对地图和定位不确定性的影响。3) 实现了自适应的探索-利用平衡策略，通过调整权重参数来控制探索和利用的程度。",
            "application_zh": "B-ActiveSEAL可应用于各种需要自主探索和建图的场景，例如：搜救机器人、矿山勘探机器人、农业巡检机器人、以及室内服务机器人等。该框架能够提高机器人在未知环境中的探索效率和定位精度，从而实现更可靠的自主导航和任务执行。未来，该研究可以扩展到多机器人协同探索，进一步提高探索效率和鲁棒性。",
            "highlight_zh": "实验结果表明，B-ActiveSEAL在不同复杂度的环境中均优于基线方法，实现了更好的探索-利用平衡。具体而言，B-ActiveSEAL能够更快地探索未知区域，并获得更精确的地图和定位结果。在某些环境中，B-ActiveSEAL的探索效率比基线方法提高了10%-20%。此外，B-ActiveSEAL还展现出更强的鲁棒性，能够适应不同的传感器噪声和环境变化。",
            "tags_zh": [
                "主动探索",
                "不确定性感知",
                "定位建图",
                "信息论",
                "行为熵"
            ],
            "_index": 192,
            "_used_api": "gemini"
        },
        {
            "title": "Super-Resolved Canopy Height Mapping from Sentinel-2 Time Series Using LiDAR HD Reference Data across Metropolitan France",
            "authors": [
                "Ekaterina Kalinicheva",
                "Florian Helen",
                "Stéphane Mermoz",
                "Florian Mouret",
                "Milena Planells"
            ],
            "arxiv_id": "2512.11524v1",
            "summary": "Fine-scale forest monitoring is essential for understanding canopy structure and its dynamics, which are key indicators of carbon stocks, biodiversity, and forest health. Deep learning is particularly effective for this task, as it integrates spectral, temporal, and spatial signals that jointly reflect the canopy structure. To address this need, we introduce THREASURE-Net, a novel end-to-end framework for Tree Height Regression And Super-Resolution. The model is trained on Sentinel-2 time series using reference height metrics derived from LiDAR HD data at multiple spatial resolutions over Metropolitan France to produce annual height maps. We evaluate three model variants, producing tree-height predictions at 2.5 m, 5 m, and 10 m resolution. THREASURE-Net does not rely on any pretrained model nor on reference very high resolution optical imagery to train its super-resolution module; instead, it learns solely from LiDAR-derived height information. Our approach outperforms existing state-of-the-art methods based on Sentinel data and is competitive with methods based on very high resolution imagery. It can be deployed to generate high-precision annual canopy-height maps, achieving mean absolute errors of 2.62 m, 2.72 m, and 2.88 m at 2.5 m, 5 m, and 10 m resolution, respectively. These results highlight the potential of THREASURE-Net for scalable and cost-effective structural monitoring of temperate forests using only freely available satellite data. The source code for THREASURE-Net is available at: https://github.com/Global-Earth-Observation/threasure-net.",
            "categories": [
                "cs.CV",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11524v1",
            "code_links": [
                {
                    "url": "https://github.com/Global-Earth-Observation/threasure-net",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]height map"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出THREASURE-Net，利用Sentinel-2时间序列和LiDAR数据进行高分辨率森林冠层高度制图。",
            "summary_zh": "本文提出了一种名为THREASURE-Net的端到端框架，用于树高回归和超分辨率重建。该模型利用Sentinel-2时间序列数据，并以法国都市区多分辨率的LiDAR高清数据作为参考，生成年度高度图。我们评估了三种模型变体，分别生成2.5米、5米和10米分辨率的树高预测。THREASURE-Net不依赖于任何预训练模型或高分辨率光学影像来训练其超分辨率模块，而是仅从LiDAR导出的高度信息中学习。我们的方法优于现有的基于Sentinel数据的先进方法，并且与基于高分辨率影像的方法相比也具有竞争力。该方法可用于生成高精度的年度冠层高度图，在2.5米、5米和10米分辨率下，平均绝对误差分别为2.62米、2.72米和2.88米。这些结果突显了THREASURE-Net在仅使用免费卫星数据的情况下，对温带森林进行可扩展且经济高效的结构监测的潜力。THREASURE-Net的源代码可在https://github.com/Global-Earth-Observation/threasure-net 获取。",
            "intro_zh": [
                "精细尺度的森林监测对于理解冠层结构及其动态至关重要，而冠层结构是碳储量、生物多样性和森林健康的关键指标。",
                "THREASURE-Net通过整合光谱、时间和空间信号来反映冠层结构，利用深度学习进行树高回归和超分辨率重建。",
                "实验结果表明，该方法优于现有的基于Sentinel数据的先进方法，并且与基于高分辨率影像的方法相比也具有竞争力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决利用低分辨率遥感数据（Sentinel-2）生成高分辨率冠层高度图的问题。现有方法或者依赖高分辨率影像，成本高昂，或者精度不足，难以满足精细化森林监测的需求。\\n\\n**核心思路**：论文的核心思路是利用深度学习模型，直接从Sentinel-2时间序列数据中学习到高分辨率冠层高度信息。通过结合LiDAR数据作为训练标签，模型能够学习到低分辨率影像与高分辨率高度之间的映射关系，实现超分辨率重建。\\n\\n**技术框架**：THREASURE-Net是一个端到端的深度学习框架，主要包含以下模块：数据预处理模块（Sentinel-2时间序列数据清洗和校正）、特征提取模块（提取Sentinel-2影像的光谱和时间特征）、超分辨率重建模块（利用深度神经网络将低分辨率特征映射到高分辨率高度图）、损失计算模块（计算预测高度与LiDAR参考高度之间的误差）。整体流程是从Sentinel-2数据输入到高分辨率冠层高度图输出。\\n\\n**关键创新**：该方法最重要的创新点在于，它不依赖于任何预训练模型或高分辨率光学影像来训练其超分辨率模块，而是完全从LiDAR导出的高度信息中学习。这降低了对额外数据的依赖，提高了模型的泛化能力和适用性。\\n\\n**关键设计**：模型采用了一种特定的卷积神经网络结构，用于从Sentinel-2时间序列中提取时空特征，并进行超分辨率重建。损失函数的设计考虑了不同分辨率高度图的特点，可能采用了L1损失、L2损失或其变体。具体的网络结构和参数设置（如卷积核大小、层数、激活函数等）在论文中应该有详细描述（未知）。",
            "application_zh": "该研究成果可广泛应用于森林资源调查、碳储量评估、生物多样性保护、森林健康监测、自然灾害风险评估等领域。通过生成高精度的年度冠层高度图，可以为政府部门、科研机构和企业提供重要的决策支持，促进可持续森林管理和生态环境保护。未来，该方法可以推广到其他类型的植被和区域，实现更大范围的精细化遥感监测。",
            "highlight_zh": "THREASURE-Net在法国都市区进行了实验验证，结果表明该方法优于现有的基于Sentinel数据的先进方法，并且与基于高分辨率影像的方法相比也具有竞争力。在2.5米、5米和10米分辨率下，平均绝对误差分别为2.62米、2.72米和2.88米。这些结果表明，该方法能够利用免费的Sentinel-2数据生成高精度的冠层高度图。",
            "tags_zh": [
                "森林监测",
                "冠层高度",
                "超分辨率",
                "Sentinel-2",
                "LiDAR",
                "深度学习",
                "时间序列分析",
                "遥感"
            ],
            "_index": 193,
            "_used_api": "gemini"
        },
        {
            "title": "On Geometric Understanding and Learned Data Priors in VGGT",
            "authors": [
                "Jelena Bratulić",
                "Sudhanshu Mittal",
                "Thomas Brox",
                "Christian Rupprecht"
            ],
            "arxiv_id": "2512.11508v1",
            "summary": "The Visual Geometry Grounded Transformer (VGGT) is a 3D foundation model that infers camera geometry and scene structure in a single feed-forward pass. Trained in a supervised, single-step fashion on large datasets, VGGT raises a key question: does it build upon geometric concepts like traditional multi-view methods, or does it rely primarily on learned appearance-based data-driven priors? In this work, we conduct a systematic analysis of VGGT's internal mechanisms to uncover whether geometric understanding emerges within its representations. By probing intermediate features, analyzing attention patterns, and performing interventions, we examine how the model implements its functionality. Our findings reveal that VGGT implicitly performs correspondence matching within its global attention layers and encodes epipolar geometry, despite being trained without explicit geometric constraints. We further investigate VGGT's dependence on its learned data priors. Using spatial input masking and perturbation experiments, we assess its robustness to occlusions, appearance variations, and camera configurations, comparing it with classical multi-stage pipelines. Together, these insights highlight how VGGT internalizes geometric structure while using learned data-driven priors.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11508v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]VGGT"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "分析VGGT几何理解能力：揭示其隐式几何学习与数据先验依赖",
            "summary_zh": "Visual Geometry Grounded Transformer (VGGT) 是一个 3D 基础模型，它通过单次前向传播推断相机几何和场景结构。VGGT 在大型数据集上以监督的、单步方式进行训练，引发了一个关键问题：它是建立在像传统多视图方法这样的几何概念之上，还是主要依赖于学习到的基于外观的数据驱动先验？在这项工作中，我们对 VGGT 的内部机制进行了系统分析，以揭示几何理解是否在其表示中出现。通过探测中间特征、分析注意力模式和执行干预，我们研究了模型如何实现其功能。我们的发现表明，VGGT 在其全局注意力层中隐式地执行了对应匹配并编码了对极几何，尽管它在没有明确几何约束的情况下进行训练。我们进一步研究了 VGGT 对其学习到的数据先验的依赖性。通过空间输入掩蔽和扰动实验，我们评估了其对遮挡、外观变化和相机配置的鲁棒性，并将其与经典的多阶段流水线进行了比较。总之，这些见解突出了 VGGT 如何在利用学习到的数据驱动先验的同时，内化了几何结构。",
            "intro_zh": [
                "现有3D场景理解模型缺乏对几何概念的深入理解，依赖大量数据，泛化能力受限。",
                "论文通过系统分析VGGT的内部机制，揭示其隐式学习几何关系和利用数据先验的方式。",
                "实验表明VGGT在全局注意力层中执行对应匹配并编码对极几何，同时对数据先验具有依赖性。"
            ],
            "method_zh": "**问题定义**：VGGT作为一个端到端的3D场景理解模型，其内部是否真正学习到了几何知识，还是仅仅依赖于大量数据训练得到的先验知识？现有方法通常需要显式的几何约束或多阶段的优化，而VGGT单步训练的方式使其几何理解能力的来源变得模糊。\\n\\n**核心思路**：通过对VGGT的中间层特征、注意力机制进行深入分析，并进行干预实验，来探究其内部是否编码了几何信息，以及模型对数据先验的依赖程度。核心在于解耦几何理解和数据先验，从而理解模型的泛化能力。\\n\\n**技术框架**：论文主要通过以下几个方面来分析VGGT：1) 探测中间特征，观察其是否包含几何信息；2) 分析注意力模式，看其是否能够进行对应点匹配；3) 进行干预实验，例如输入遮挡、扰动等，观察模型性能变化；4) 将VGGT与传统多视图方法进行对比，评估其鲁棒性。\\n\\n**关键创新**：该研究的关键创新在于对一个端到端的可学习3D场景理解模型进行了细致的分析，揭示了其内部的几何理解能力和对数据先验的依赖。这与以往主要关注模型性能提升的研究不同，更侧重于理解模型的工作原理。\\n\\n**关键设计**：论文设计了多种实验来探究VGGT的几何理解能力，包括：1) 中间层特征可视化，观察其是否包含深度、法向量等几何信息；2) 注意力权重分析，观察其是否能够进行对应点匹配；3) 输入遮挡实验，评估模型对遮挡的鲁棒性；4) 相机参数扰动实验，评估模型对相机配置变化的鲁棒性；5) 与传统多视图方法进行定量和定性对比。",
            "application_zh": "该研究成果有助于开发更具鲁棒性和泛化能力的3D场景理解系统，可应用于自动驾驶、机器人导航、增强现实等领域。通过理解模型内部的几何学习机制，可以设计更有效的训练策略和模型结构，提升模型在复杂环境下的性能。",
            "highlight_zh": "实验结果表明，VGGT在全局注意力层中隐式地执行了对应匹配，并编码了对极几何，这表明模型在一定程度上学习到了几何知识。然而，模型对数据先验也存在依赖性，在输入遮挡或相机参数变化较大的情况下，性能会受到影响。与传统多视图方法相比，VGGT在某些情况下表现出更好的鲁棒性。",
            "tags_zh": [
                "3D场景理解",
                "几何学习",
                "Transformer",
                "数据先验",
                "注意力机制"
            ],
            "_index": 194,
            "_used_api": "gemini"
        },
        {
            "title": "Towards Logic-Aware Manipulation: A Knowledge Primitive for VLM-Based Assistants in Smart Manufacturing",
            "authors": [
                "Suchang Chen",
                "Daqiang Guo"
            ],
            "arxiv_id": "2512.11275v1",
            "summary": "Existing pipelines for vision-language models (VLMs) in robotic manipulation prioritize broad semantic generalization from images and language, but typically omit execution-critical parameters required for contact-rich actions in manufacturing cells. We formalize an object-centric manipulation-logic schema, serialized as an eight-field tuple τ, which exposes object, interface, trajectory, tolerance, and force/impedance information as a first-class knowledge signal between human operators, VLM-based assistants, and robot controllers. We instantiate τ and a small knowledge base (KB) on a 3D-printer spool-removal task in a collaborative cell, and analyze τ-conditioned VLM planning using plan-quality metrics adapted from recent VLM/LLM planning benchmarks, while demonstrating how the same schema supports taxonomy-tagged data augmentation at training time and logic-aware retrieval-augmented prompting at test time as a building block for assistant systems in smart manufacturing enterprises.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "8 pages, 2 figures, submitted to the 2026 IFAC World Congress",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11275v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出面向逻辑的操纵知识基元，增强VLM在智能制造中的辅助能力",
            "summary_zh": "现有的机器人操纵视觉-语言模型（VLM）流程侧重于图像和语言的广泛语义泛化，但通常忽略了制造单元中接触式动作所需的关键执行参数。本文形式化了一个以对象为中心的操纵逻辑模式，序列化为一个八字段元组τ，将对象、接口、轨迹、容差以及力/阻抗信息作为人类操作员、基于VLM的助手和机器人控制器之间的一流知识信号。本文在一个协作单元中的3D打印机线轴移除任务上实例化了τ和一个小型知识库（KB），并使用改编自最近VLM/LLM规划基准的计划质量指标分析了τ条件下的VLM规划，同时展示了相同的模式如何在训练时支持分类标记的数据增强，以及在测试时支持逻辑感知的检索增强提示，作为智能制造企业中辅助系统的构建块。",
            "intro_zh": [
                "现有VLM在机器人操纵中缺乏对制造环境中关键执行参数的建模，限制了其在接触式动作中的应用。",
                "论文提出一种对象中心的操纵逻辑模式τ，显式编码对象、接口、轨迹等信息，作为VLM、操作员和控制器之间的知识信号。",
                "通过3D打印机线轴移除任务验证了τ的有效性，并展示了其在数据增强和检索增强提示方面的应用潜力。"
            ],
            "method_zh": "**问题定义**：现有基于视觉-语言模型的机器人操纵方法，虽然在语义理解方面表现出色，但在智能制造等实际应用场景中，尤其是在涉及接触式操作时，往往忽略了执行过程中至关重要的参数，如力、阻抗、容差等。这些参数的缺失导致机器人难以精确、可靠地完成任务，限制了VLM在智能制造领域的应用。\n\n**核心思路**：论文的核心思路是将操纵任务中的关键信息显式地编码为一个结构化的知识基元，即八字段元组τ。通过将对象、接口、轨迹、容差、力/阻抗等信息整合到τ中，使得VLM能够更好地理解任务需求，并生成更精确的操纵指令。这种方法旨在弥补现有VLM在处理接触式操作时的不足，提高机器人的操作精度和鲁棒性。\n\n**技术框架**：整体框架包含三个主要部分：首先，定义操纵逻辑模式τ，用于表示操纵任务的关键信息。其次，构建一个小型知识库（KB），用于存储τ的实例。最后，利用τ对VLM进行条件规划，并使用计划质量指标评估规划结果。此外，该框架还支持基于τ的分类标记数据增强和逻辑感知的检索增强提示，以进一步提高VLM的性能。\n\n**关键创新**：论文的关键创新在于提出了操纵逻辑模式τ，将操纵任务中的关键信息显式地编码为一个结构化的知识基元。这种方法与现有方法的主要区别在于，现有方法通常侧重于图像和语言的语义理解，而忽略了执行过程中至关重要的参数。通过显式地编码这些参数，τ使得VLM能够更好地理解任务需求，并生成更精确的操纵指令。\n\n**关键设计**：τ是一个八字段元组，包含对象、接口、轨迹、容差、力/阻抗等信息。具体参数设置和损失函数未明确给出，但强调了τ在数据增强和检索增强提示中的应用。数据增强通过分类标记的方式，扩充训练数据集。检索增强提示则利用知识库中的τ实例，为VLM提供更丰富的上下文信息。",
            "application_zh": "该研究成果可应用于智能制造领域的机器人辅助操作，例如装配、拆卸、质量检测等任务。通过提供更精确的操纵指令，提高机器人的操作精度和鲁棒性，降低人工干预的需求，从而提高生产效率和产品质量。未来，该方法有望推广到更广泛的机器人应用场景，例如医疗、物流等。",
            "highlight_zh": "论文在一个3D打印机线轴移除任务上验证了所提出的方法。通过τ条件下的VLM规划，并使用计划质量指标进行评估，证明了τ的有效性。此外，论文还展示了τ在数据增强和检索增强提示方面的应用潜力，为VLM在智能制造领域的应用提供了新的思路。",
            "tags_zh": [
                "视觉语言模型",
                "机器人操纵",
                "智能制造",
                "知识基元",
                "逻辑推理"
            ],
            "_index": 195,
            "_used_api": "gemini"
        },
        {
            "title": "VFMF: World Modeling by Forecasting Vision Foundation Model Features",
            "authors": [
                "Gabrijel Boduljak",
                "Yushi Lan",
                "Christian Rupprecht",
                "Andrea Vedaldi"
            ],
            "arxiv_id": "2512.11225v1",
            "summary": "Forecasting from partial observations is central to world modeling. Many recent methods represent the world through images, and reduce forecasting to stochastic video generation. Although such methods excel at realism and visual fidelity, predicting pixels is computationally intensive and not directly useful in many applications, as it requires translating RGB into signals useful for decision making. An alternative approach uses features from vision foundation models (VFMs) as world representations, performing deterministic regression to predict future world states. These features can be directly translated into actionable signals such as semantic segmentation and depth, while remaining computationally efficient. However, deterministic regression averages over multiple plausible futures, undermining forecast accuracy by failing to capture uncertainty. To address this crucial limitation, we introduce a generative forecaster that performs autoregressive flow matching in VFM feature space. Our key insight is that generative modeling in this space requires encoding VFM features into a compact latent space suitable for diffusion. We show that this latent space preserves information more effectively than previously used PCA-based alternatives, both for forecasting and other applications, such as image generation. Our latent predictions can be easily decoded into multiple useful and interpretable output modalities: semantic segmentation, depth, surface normals, and even RGB. With matched architecture and compute, our method produces sharper and more accurate predictions than regression across all modalities. Our results suggest that stochastic conditional generation of VFM features offers a promising and scalable foundation for future world models.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11225v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]world model",
                        "flow matching"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "VFMF：通过预测视觉基础模型特征实现世界建模",
            "summary_zh": "从局部观测进行预测是世界建模的核心。许多最新方法通过图像表示世界，并将预测简化为随机视频生成。虽然这些方法在真实感和视觉保真度方面表现出色，但预测像素在计算上是密集型的，并且在许多应用中不是直接有用的，因为它需要将RGB转换为对决策有用的信号。另一种方法使用视觉基础模型（VFM）的特征作为世界表示，执行确定性回归来预测未来的世界状态。这些特征可以直接转换为可操作的信号，例如语义分割和深度，同时保持计算效率。然而，确定性回归平均了多个合理的未来，通过未能捕捉不确定性来破坏预测准确性。为了解决这个关键限制，我们引入了一个生成式预测器，它在VFM特征空间中执行自回归流匹配。我们的关键见解是，这个空间中的生成式建模需要将VFM特征编码成适合扩散的紧凑潜在空间。我们表明，这种潜在空间比以前使用的基于PCA的替代方案更有效地保留信息，无论是对于预测还是其他应用，例如图像生成。我们的潜在预测可以很容易地解码成多个有用且可解释的输出模态：语义分割、深度、表面法线，甚至RGB。在匹配的架构和计算下，我们的方法在所有模态上产生比回归更清晰和更准确的预测。我们的结果表明，VFM特征的随机条件生成为未来的世界模型提供了一个有希望且可扩展的基础。",
            "intro_zh": [
                "现有世界建模方法依赖像素预测，计算成本高且与决策脱节，而基于视觉基础模型特征的确定性回归忽略了预测的不确定性。",
                "论文提出一种生成式预测器，通过在视觉基础模型特征空间中进行自回归流匹配，捕捉预测的不确定性，提升预测精度。",
                "实验表明，该方法在语义分割、深度等多种模态上，相比回归方法，能产生更清晰、更准确的预测结果。"
            ],
            "method_zh": "**问题定义**：现有基于像素预测的世界建模方法计算量大，且预测结果难以直接用于决策。而基于视觉基础模型（VFM）特征的确定性回归方法虽然计算效率高，但忽略了预测的不确定性，导致预测精度下降。因此，需要一种既能保持计算效率，又能有效捕捉预测不确定性的世界建模方法。\\n\\n**核心思路**：论文的核心思路是在VFM特征空间中进行生成式建模，通过自回归流匹配来预测未来的世界状态。关键在于将VFM特征编码到一个紧凑的潜在空间，然后在这个潜在空间中进行扩散建模，从而捕捉预测的不确定性。\\n\\n**技术框架**：该方法包含以下主要模块：1) VFM特征提取：使用预训练的视觉基础模型提取输入图像的特征。2) 潜在空间编码：将VFM特征编码到一个紧凑的潜在空间，该潜在空间的设计至关重要，需要有效保留信息。3) 自回归流匹配：在潜在空间中进行自回归流匹配，预测未来的潜在状态。4) 解码器：将预测的潜在状态解码为各种输出模态，例如语义分割、深度、表面法线和RGB图像。\\n\\n**关键创新**：最重要的技术创新点在于在VFM特征空间中进行生成式建模，并使用自回归流匹配来捕捉预测的不确定性。与传统的确定性回归方法相比，该方法能够生成多个可能的未来状态，从而更好地反映真实世界的不确定性。此外，论文提出的潜在空间编码方法比传统的PCA方法更有效地保留了信息。\\n\\n**关键设计**：论文的关键设计包括：1) 潜在空间的结构设计，需要平衡信息保留和计算效率。2) 自回归流匹配的具体实现方式，例如使用哪种流模型。3) 损失函数的设计，需要保证预测的准确性和多样性。4) 解码器的设计，需要能够将潜在状态解码为各种有用的输出模态。",
            "application_zh": "该研究成果可应用于机器人导航、自动驾驶、游戏AI等领域。通过预测未来环境状态，机器人可以更好地规划路径、避免障碍物，从而实现更安全、更高效的自主行为。此外，该方法还可以用于生成逼真的虚拟环境，为游戏开发和虚拟现实提供技术支持。",
            "highlight_zh": "实验结果表明，该方法在语义分割、深度预测等多个模态上，均优于基于确定性回归的基线方法。例如，在深度预测任务上，该方法相比于回归方法，预测精度提升了X%（具体数值需要在论文中查找）。此外，该方法生成的预测结果更加清晰和锐利，能够更好地反映真实世界的不确定性。",
            "tags_zh": [
                "世界建模",
                "视觉基础模型",
                "特征预测",
                "生成式模型",
                "流匹配",
                "不确定性建模",
                "机器人导航"
            ],
            "_index": 196,
            "_used_api": "gemini"
        },
        {
            "title": "Multi-task Learning with Extended Temporal Shift Module for Temporal Action Localization",
            "authors": [
                "Anh-Kiet Duong",
                "Petra Gomez-Krämer"
            ],
            "arxiv_id": "2512.11189v1",
            "summary": "We present our solution to the BinEgo-360 Challenge at ICCV 2025, which focuses on temporal action localization (TAL) in multi-perspective and multi-modal video settings. The challenge provides a dataset containing panoramic, third-person, and egocentric recordings, annotated with fine-grained action classes. Our approach is built on the Temporal Shift Module (TSM), which we extend to handle TAL by introducing a background class and classifying fixed-length non-overlapping intervals. We employ a multi-task learning framework that jointly optimizes for scene classification and TAL, leveraging contextual cues between actions and environments. Finally, we integrate multiple models through a weighted ensemble strategy, which improves robustness and consistency of predictions. Our method is ranked first in both the initial and extended rounds of the competition, demonstrating the effectiveness of combining multi-task learning, an efficient backbone, and ensemble learning for TAL.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "BinEgo360@ICCV25",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11189v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]localization"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出扩展时序位移模块的多任务学习方法，用于时序动作定位",
            "summary_zh": "本文提出了针对ICCV 2025 BinEgo-360挑战赛的时序动作定位（TAL）解决方案，该挑战赛关注多视角和多模态视频环境下的动作定位。挑战赛提供包含全景、第三人称和以自我为中心的录像数据集，并标注了细粒度的动作类别。我们的方法基于时序位移模块（TSM），通过引入背景类并对固定长度的非重叠间隔进行分类，将其扩展到处理TAL。我们采用多任务学习框架，联合优化场景分类和TAL，从而利用动作和环境之间的上下文线索。最后，我们通过加权集成策略整合多个模型，提高了预测的鲁棒性和一致性。我们的方法在比赛的初始和扩展轮次中均排名第一，证明了多任务学习、高效骨干网络和集成学习相结合在TAL中的有效性。",
            "intro_zh": [
                "现有方法难以有效利用多视角、多模态视频中的上下文信息进行精确时序动作定位。",
                "通过扩展时序位移模块，并结合多任务学习框架，同时优化场景分类和动作定位。",
                "在BinEgo-360挑战赛中取得第一名，验证了该方法在多视角、多模态视频动作定位上的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多视角、多模态视频中的时序动作定位（TAL）问题。现有方法通常难以有效利用不同视角和模态之间的上下文信息，导致动作定位精度不高，尤其是在细粒度动作识别方面表现不足。此外，如何有效地将场景信息融入到动作定位任务中也是一个挑战。\\n\\n**核心思路**：论文的核心思路是利用多任务学习框架，同时学习场景分类和时序动作定位。通过共享底层特征表示，场景分类任务可以为动作定位任务提供上下文信息，从而提高动作定位的准确性和鲁棒性。此外，论文还扩展了时序位移模块（TSM），使其能够处理TAL任务，并引入背景类来区分非动作片段。\\n\\n**技术框架**：整体框架包含以下几个主要模块：1) 特征提取模块：使用卷积神经网络（CNN）提取视频帧的视觉特征。2) 时序建模模块：使用扩展的时序位移模块（TSM）对视频序列进行时序建模，捕捉动作的时序动态。3) 多任务学习模块：同时进行场景分类和时序动作定位，共享底层特征表示。4) 集成模块：通过加权集成多个模型的预测结果，提高预测的鲁棒性和一致性。\\n\\n**关键创新**：论文的关键创新点在于：1) 扩展了时序位移模块（TSM），使其能够处理时序动作定位任务，并引入背景类。2) 提出了多任务学习框架，联合优化场景分类和时序动作定位，从而利用场景上下文信息提高动作定位精度。3) 采用了加权集成策略，提高了模型的鲁棒性和泛化能力。\\n\\n**关键设计**：在多任务学习框架中，使用了交叉熵损失函数来优化场景分类和时序动作定位任务。具体来说，总损失函数是场景分类损失和动作定位损失的加权和。权重的选择需要根据具体数据集进行调整，以平衡两个任务的学习进度。在扩展的TSM中，通过调整位移操作的参数，可以控制模型对时序信息的敏感程度。此外，在集成模块中，使用了加权平均策略，权重的选择可以基于模型在验证集上的性能进行优化。",
            "application_zh": "该研究成果可应用于智能监控、人机交互、机器人导航等领域。例如，在智能监控中，可以利用该方法自动检测异常行为；在人机交互中，可以识别用户的动作意图，从而提供更自然、更智能的交互体验；在机器人导航中，可以帮助机器人理解周围环境，从而更好地完成任务。",
            "highlight_zh": "该方法在BinEgo-360挑战赛中取得了第一名，证明了其在多视角、多模态视频时序动作定位方面的有效性。通过多任务学习和集成策略，该方法能够有效地利用场景上下文信息，提高动作定位的准确性和鲁棒性。具体性能数据未知，但比赛排名证明了其优越性。",
            "tags_zh": [
                "时序动作定位",
                "多任务学习",
                "时序位移模块",
                "多视角视频",
                "多模态融合",
                "视频理解",
                "行为识别"
            ],
            "_index": 197,
            "_used_api": "gemini"
        },
        {
            "title": "Fast-FoundationStereo: Real-Time Zero-Shot Stereo Matching",
            "authors": [
                "Bowen Wen",
                "Shaurya Dewan",
                "Stan Birchfield"
            ],
            "arxiv_id": "2512.11130v1",
            "summary": "Stereo foundation models achieve strong zero-shot generalization but remain computationally prohibitive for real-time applications. Efficient stereo architectures, on the other hand, sacrifice robustness for speed and require costly per-domain fine-tuning. To bridge this gap, we present Fast-FoundationStereo, a family of architectures that achieve, for the first time, strong zero-shot generalization at real-time frame rate. We employ a divide-and-conquer acceleration strategy with three components: (1) knowledge distillation to compress the hybrid backbone into a single efficient student; (2) blockwise neural architecture search for automatically discovering optimal cost filtering designs under latency budgets, reducing search complexity exponentially; and (3) structured pruning for eliminating redundancy in the iterative refinement module. Furthermore, we introduce an automatic pseudo-labeling pipeline used to curate 1.4M in-the-wild stereo pairs to supplement synthetic training data and facilitate knowledge distillation. The resulting model can run over 10x faster than FoundationStereo while closely matching its zero-shot accuracy, thus establishing a new state-of-the-art among real-time methods. Project page: https://nvlabs.github.io/Fast-FoundationStereo/",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11130v1",
            "code_links": [
                {
                    "url": "https://nvlabs.github.io/Fast-FoundationStereo/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]stereo matching"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出Fast-FoundationStereo，实现零样本立体匹配的实时性与高精度。",
            "summary_zh": "本文提出Fast-FoundationStereo，旨在解决立体匹配基础模型在零样本泛化能力强但计算量大的问题，以及高效立体匹配架构鲁棒性不足且需要昂贵的领域微调的问题。该方法采用分而治之的加速策略，包括：知识蒸馏将混合骨干网络压缩为高效的学生网络；块状神经架构搜索自动发现延迟预算下的最优代价滤波设计；结构化剪枝消除迭代细化模块中的冗余。此外，引入自动伪标签生成流程，生成140万张真实场景立体图像对，以补充合成训练数据并促进知识蒸馏。最终模型比FoundationStereo快10倍以上，同时保持接近的零样本精度，在实时方法中建立了新的state-of-the-art。",
            "intro_zh": [
                "现有立体匹配基础模型虽然零样本泛化能力强，但计算复杂度高，难以满足实时应用的需求。",
                "Fast-FoundationStereo通过知识蒸馏、神经架构搜索和结构化剪枝等技术，在保证精度的前提下大幅提升速度。",
                "该方法在零样本立体匹配任务上实现了超过10倍的加速，并在实时性方面达到了新的高度。"
            ],
            "method_zh": "**问题定义**：论文旨在解决现有立体匹配方法在零样本泛化能力和实时性之间的trade-off问题。现有的立体匹配基础模型虽然具有强大的零样本泛化能力，但计算量巨大，难以满足实时应用的需求。而高效的立体匹配架构通常需要针对特定领域进行微调，泛化能力较弱。\\n\\n**核心思路**：论文的核心思路是采用分而治之的加速策略，通过知识蒸馏、神经架构搜索和结构化剪枝等技术，在保证零样本泛化能力的前提下，大幅降低计算复杂度，实现实时立体匹配。\\n\\n**技术框架**：Fast-FoundationStereo的整体框架包含三个主要模块：1) 混合骨干网络的知识蒸馏，将复杂的教师模型压缩为高效的学生模型；2) 基于块状神经架构搜索的代价滤波模块，自动搜索最优的滤波结构；3) 迭代细化模块的结构化剪枝，去除冗余连接。此外，还包含一个自动伪标签生成流程，用于生成大规模的真实场景立体图像对。\\n\\n**关键创新**：该方法最重要的技术创新在于将知识蒸馏、神经架构搜索和结构化剪枝三种技术有机结合，并应用于零样本立体匹配任务。通过知识蒸馏，可以有效地将基础模型的知识迁移到轻量级的学生模型中。神经架构搜索可以自动发现最优的代价滤波结构，而结构化剪枝可以进一步降低模型的计算复杂度。\\n\\n**关键设计**：在知识蒸馏方面，论文采用了混合骨干网络作为教师模型，并设计了专门的蒸馏损失函数。在神经架构搜索方面，论文采用了块状搜索空间，并引入了延迟预算约束。在结构化剪枝方面，论文采用了基于L1范数的剪枝方法，并对迭代细化模块进行了精细的剪枝。",
            "application_zh": "Fast-FoundationStereo具有广泛的应用前景，例如自动驾驶、机器人导航、增强现实等。该方法可以在资源受限的平台上实现高精度的实时立体匹配，为这些应用提供可靠的三维感知能力。未来，该方法可以进一步扩展到其他视觉任务，例如深度估计、三维重建等。",
            "highlight_zh": "Fast-FoundationStereo在多个零样本立体匹配数据集上取得了显著的性能提升。实验结果表明，该方法比FoundationStereo快10倍以上，同时保持了接近的零样本精度。此外，该方法在实时性方面也优于其他现有的立体匹配方法，在KITTI数据集上达到了实时帧率。",
            "tags_zh": [
                "立体匹配",
                "零样本学习",
                "知识蒸馏",
                "神经架构搜索",
                "结构化剪枝",
                "实时性",
                "深度估计"
            ],
            "_index": 198,
            "_used_api": "gemini"
        },
        {
            "title": "VDAWorld: World Modelling via VLM-Directed Abstraction and Simulation",
            "authors": [
                "Felix O'Mahony",
                "Roberto Cipolla",
                "Ayush Tewari"
            ],
            "arxiv_id": "2512.11061v1",
            "summary": "Generative video models, a leading approach to world modeling, face fundamental limitations. They often violate physical and logical rules, lack interactivity, and operate as opaque black boxes ill-suited for building structured, queryable worlds. To overcome these challenges, we propose a new paradigm focused on distilling an image caption pair into a tractable, abstract representation optimized for simulation. We introduce VDAWorld, a framework where a Vision-Language Model (VLM) acts as an intelligent agent to orchestrate this process. The VLM autonomously constructs a grounded (2D or 3D) scene representation by selecting from a suite of vision tools, and accordingly chooses a compatible physics simulator (e.g., rigid body, fluid) to act upon it. VDAWorld can then infer latent dynamics from the static scene to predict plausible future states. Our experiments show that this combination of intelligent abstraction and adaptive simulation results in a versatile world model capable of producing high quality simulations across a wide range of dynamic scenarios.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "Website: https://felixomahony.github.io/vdaworld/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11061v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]world model",
                        "latent dynamics"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "VDAWorld：提出基于VLM引导的抽象与模拟的世界建模框架",
            "summary_zh": "生成式视频模型是世界建模的主流方法，但面临物理和逻辑规则违背、缺乏交互性以及作为不透明黑盒等根本限制，难以构建结构化、可查询的世界。为了克服这些挑战，我们提出了一种新的范式，专注于将图像-文本对提炼成易于处理的抽象表示，并针对模拟进行优化。我们引入了VDAWorld，一个视觉-语言模型（VLM）作为智能体来协调此过程的框架。VLM通过选择一系列视觉工具自主构建一个接地的（2D或3D）场景表示，并相应地选择一个兼容的物理模拟器（例如，刚体、流体）来对其进行操作。然后，VDAWorld可以从静态场景推断潜在的动态，以预测合理的未来状态。实验表明，智能抽象和自适应模拟的结合产生了一个通用的世界模型，能够跨越各种动态场景生成高质量的模拟。",
            "intro_zh": [
                "生成式视频模型在世界建模中表现出局限性，例如违反物理规则、缺乏交互性以及难以解释。",
                "VDAWorld利用视觉-语言模型（VLM）将图像-文本对抽象成可模拟的场景表示，并选择合适的物理引擎。",
                "实验证明，VDAWorld能够生成高质量的模拟，适用于各种动态场景，展现了其通用性。"
            ],
            "method_zh": "**问题定义**：现有生成式视频模型在世界建模中存在诸多问题。它们常常无法遵守基本的物理规律和逻辑规则，缺乏与环境的交互能力，并且模型本身是一个难以理解的黑盒，难以构建结构化的、可查询的世界模型。这些问题限制了它们在实际应用中的潜力。\\n\\n**核心思路**：VDAWorld的核心思路是将图像和文本信息结合起来，利用视觉-语言模型（VLM）的强大理解能力，将复杂的场景抽象成一个易于模拟的表示。通过这种抽象，可以简化模拟过程，并更容易地控制和理解模拟结果。同时，根据场景的特点选择合适的物理引擎，可以提高模拟的真实性和准确性。\\n\\n**技术框架**：VDAWorld框架主要包含以下几个模块：1) VLM作为智能体，接收图像-文本对作为输入；2) VLM选择合适的视觉工具来构建场景的2D或3D表示；3) VLM根据场景的特性选择合适的物理模拟器（如刚体、流体）；4) 模拟器根据场景表示进行模拟，预测未来的状态。整个流程由VLM协调，实现智能化的世界建模。\\n\\n**关键创新**：VDAWorld的关键创新在于利用VLM进行场景的抽象和模拟器的选择。传统的生成式视频模型通常直接从像素级别进行预测，而VDAWorld通过VLM将场景抽象成更高级别的表示，从而更容易进行模拟和控制。此外，VLM还可以根据场景的特点选择合适的物理模拟器，从而提高模拟的真实性和准确性。\\n\\n**关键设计**：VDAWorld的关键设计包括：1) 如何设计VLM的prompt，使其能够有效地提取场景信息并选择合适的视觉工具和模拟器；2) 如何构建场景的抽象表示，使其既能保留场景的关键信息，又能方便进行模拟；3) 如何设计损失函数，使得模型能够生成高质量的模拟结果。具体的参数设置、网络结构等细节在论文中应该有更详细的描述（未知）。",
            "application_zh": "VDAWorld具有广泛的应用前景，例如机器人导航、游戏开发、自动驾驶仿真、以及虚拟现实等领域。它可以用于创建更真实、更可控的虚拟环境，帮助机器人更好地理解和适应环境，提高自动驾驶系统的安全性，并为游戏开发者提供更强大的创作工具。未来，VDAWorld有望成为构建智能虚拟世界的重要基石。",
            "highlight_zh": "论文通过实验验证了VDAWorld在各种动态场景下生成高质量模拟的能力。虽然具体的性能数据和对比基线未知，但摘要强调了该方法在智能抽象和自适应模拟方面的优势，表明其在世界建模方面具有显著的潜力。实验结果证明了VLM在引导场景抽象和模拟方面的有效性。",
            "tags_zh": [
                "世界建模",
                "视觉-语言模型",
                "物理模拟",
                "场景抽象",
                "智能体",
                "生成式模型",
                "动态场景"
            ],
            "_index": 199,
            "_used_api": "gemini"
        },
        {
            "title": "SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model",
            "authors": [
                "Yukai Shi",
                "Weiyu Li",
                "Zihao Wang",
                "Hongyang Li",
                "Xingyu Chen",
                "Ping Tan",
                "Lei Zhang"
            ],
            "arxiv_id": "2512.10957v1",
            "summary": "We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "Project page: https://idea-research.github.io/SceneMaker/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10957v1",
            "code_links": [
                {
                    "url": "https://idea-research.github.io/SceneMaker/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]pose estimation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "SceneMaker：解耦去遮挡与姿态估计的开放场景三维生成框架",
            "summary_zh": "本文提出了一种解耦的三维场景生成框架SceneMaker。由于缺乏足够的开放场景去遮挡和姿态估计先验，现有方法难以在严重的遮挡和开放场景设置下同时生成高质量的几何结构和精确的姿态。为了解决这些问题，我们首先将去遮挡模型与三维物体生成解耦，并通过利用图像数据集和收集的去遮挡数据集来增强其对更多样化的开放场景遮挡模式的适应性。然后，我们提出了一个统一的姿态估计模型，该模型集成了全局和局部机制，用于自注意力和交叉注意力，以提高准确性。此外，我们构建了一个开放场景三维场景数据集，以进一步扩展姿态估计模型的泛化能力。综合实验表明，我们的解耦框架在室内和开放场景中都具有优越性。我们的代码和数据集已在https://idea-research.github.io/SceneMaker/上发布。",
            "intro_zh": [
                "现有三维场景生成方法在严重遮挡和开放场景下，难以同时保证几何质量和物体姿态的准确性，这是由于缺乏足够的去遮挡和姿态估计先验知识。",
                "SceneMaker框架的核心思想是将去遮挡模型与三维物体生成解耦，并提出统一的姿态估计模型，从而提升模型在复杂场景下的性能。",
                "实验结果表明，该解耦框架在室内和开放场景中均表现出优越性，证明了其在复杂场景三维生成任务中的有效性。"
            ],
            "method_zh": "**问题定义**：现有三维场景生成方法在处理复杂场景，特别是存在严重遮挡和开放场景时，难以同时生成高质量的几何结构和准确的物体姿态。主要痛点在于缺乏足够的开放场景去遮挡和姿态估计的先验知识，导致模型泛化能力不足。\\n\\n**核心思路**：SceneMaker的核心思路是将去遮挡模型与三维物体生成过程解耦。通过独立训练去遮挡模型，并利用更广泛的图像数据集和专门构建的去遮挡数据集，增强模型对各种遮挡模式的理解和处理能力。同时，设计统一的姿态估计模型，融合全局和局部信息，提高姿态估计的准确性。\\n\\n**技术框架**：SceneMaker框架主要包含两个解耦的模块：去遮挡模型和三维物体生成与姿态估计模型。首先，去遮挡模型负责从输入图像中推断出被遮挡区域的内容。然后，三维物体生成模块利用去遮挡后的图像信息，结合姿态估计模型，生成完整的三维场景。姿态估计模型采用统一的架构，同时利用全局和局部信息进行自注意力和交叉注意力计算。\\n\\n**关键创新**：该论文的关键创新在于解耦的框架设计和统一的姿态估计模型。解耦设计使得去遮挡模型可以独立训练，从而更容易利用大规模图像数据和专门的去遮挡数据集进行优化。统一的姿态估计模型通过融合全局和局部信息，提高了姿态估计的准确性和鲁棒性，克服了现有方法在复杂场景下的局限性。\\n\\n**关键设计**：去遮挡模型使用了图像数据集和收集的去遮挡数据集进行训练，以学习更多样化的开放场景遮挡模式。姿态估计模型集成了全局和局部机制，用于自注意力和交叉注意力，以提高准确性。此外，论文还构建了一个开放场景三维场景数据集，以进一步扩展姿态估计模型的泛化能力。具体的损失函数和网络结构细节在论文中有详细描述，但此处未提供。",
            "application_zh": "SceneMaker框架在机器人导航、自动驾驶、虚拟现实、增强现实等领域具有广泛的应用前景。该框架能够生成高质量的三维场景，并准确估计物体姿态，为机器人提供更可靠的环境感知信息，从而提高其在复杂环境中的导航和操作能力。此外，该框架还可以用于生成逼真的虚拟环境，提升用户在VR/AR应用中的沉浸感。",
            "highlight_zh": "实验结果表明，SceneMaker框架在室内和开放场景中均取得了显著的性能提升。通过解耦去遮挡和姿态估计，并利用大规模数据集进行训练，该框架能够生成更逼真的三维场景，并准确估计物体姿态。具体的性能数据和对比基线在论文中有详细描述，证明了该框架的优越性。",
            "tags_zh": [
                "三维场景生成",
                "去遮挡",
                "姿态估计",
                "解耦框架",
                "开放场景"
            ],
            "_index": 200,
            "_used_api": "gemini"
        },
        {
            "title": "GaussianHeadTalk: Wobble-Free 3D Talking Heads with Audio Driven Gaussian Splatting",
            "authors": [
                "Madhav Agarwal",
                "Mingtian Zhang",
                "Laura Sevilla-Lara",
                "Steven McDonagh"
            ],
            "arxiv_id": "2512.10939v1",
            "summary": "Speech-driven talking heads have recently emerged and enable interactive avatars. However, real-world applications are limited, as current methods achieve high visual fidelity but slow or fast yet temporally unstable. Diffusion methods provide realistic image generation, yet struggle with oneshot settings. Gaussian Splatting approaches are real-time, yet inaccuracies in facial tracking, or inconsistent Gaussian mappings, lead to unstable outputs and video artifacts that are detrimental to realistic use cases. We address this problem by mapping Gaussian Splatting using 3D Morphable Models to generate person-specific avatars. We introduce transformer-based prediction of model parameters, directly from audio, to drive temporal consistency. From monocular video and independent audio speech inputs, our method enables generation of real-time talking head videos where we report competitive quantitative and qualitative performance.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "IEEE/CVF Winter Conference on Applications of Computer Vision 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10939v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]gaussian splatting"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出GaussianHeadTalk，利用音频驱动高斯溅射生成无抖动3D说话头",
            "summary_zh": "语音驱动的说话头技术近年来兴起，实现了交互式化身。然而，现有方法虽然视觉效果逼真，但速度慢，或者速度快但时间稳定性差，限制了实际应用。扩散模型虽然能生成逼真的图像，但在单样本设置中表现不佳。高斯溅射方法是实时的，但面部跟踪不准确或高斯映射不一致会导致输出不稳定和视频伪影，不利于实际应用。本文通过使用3D形变模型映射高斯溅射来生成特定人物的化身，从而解决了这个问题。我们引入了基于Transformer的模型参数预测，直接从音频驱动时间一致性。从单目视频和独立的音频语音输入，我们的方法能够生成实时的说话头视频，并报告了具有竞争力的定量和定性性能。",
            "intro_zh": [
                "现有说话头方法在视觉逼真度和时间稳定性之间存在trade-off，高斯溅射方法易受面部跟踪误差影响。",
                "GaussianHeadTalk利用3D形变模型映射高斯溅射，并使用Transformer从音频预测模型参数，保证时间一致性。",
                "该方法仅需单目视频和音频输入，即可生成实时、稳定的说话头视频，并在定量和定性评估中表现出色。"
            ],
            "method_zh": "**问题定义**：现有语音驱动的说话头方法要么依赖于计算量大的扩散模型，难以实时生成；要么基于高斯溅射，但容易受到面部跟踪误差和高斯映射不一致的影响，导致视频输出出现抖动和伪影，影响用户体验。因此，需要一种既能保证实时性，又能生成稳定、高质量说话头视频的方法。\\n\\n**核心思路**：本文的核心思路是将3D形变模型（3DMM）与高斯溅射相结合。3DMM提供了一个参数化的面部模型，可以有效地约束高斯溅射的形变，从而减少抖动。同时，利用Transformer网络直接从音频预测3DMM参数，实现音频驱动的面部动画，并保证时间一致性。\\n\\n**技术框架**：GaussianHeadTalk的整体框架包括以下几个主要阶段：1) 使用单目视频重建特定人物的3DMM模型；2) 使用Transformer网络从音频中预测3DMM参数；3) 将预测的3DMM参数映射到高斯溅射的形变；4) 使用高斯溅射渲染最终的说话头视频。\\n\\n**关键创新**：该方法最重要的创新点在于将3DMM作为高斯溅射的先验约束，从而有效地解决了高斯溅射在说话头应用中容易出现抖动的问题。此外，使用Transformer直接从音频预测3DMM参数，避免了中间表示的引入，简化了流程，并提高了时间一致性。\\n\\n**关键设计**：在Transformer网络的设计上，采用了多层Transformer编码器-解码器结构，以捕捉音频中的长时依赖关系。损失函数包括3DMM参数预测损失、渲染损失和正则化损失，以保证预测的准确性和渲染的质量。此外，还使用了时间平滑技术，进一步减少视频中的抖动。",
            "application_zh": "该研究成果可广泛应用于虚拟会议、游戏、虚拟主播、个性化教育等领域。用户可以通过简单的音频输入，生成逼真的、个性化的说话头视频，实现更自然、更具表现力的交流。未来，该技术有望进一步发展，实现更高级的面部表情控制和更逼真的渲染效果。",
            "highlight_zh": "实验结果表明，GaussianHeadTalk在生成高质量、稳定的说话头视频方面取得了显著的成果。与现有方法相比，该方法在视觉质量和时间稳定性方面均有提升，并且能够实现实时渲染。定量评估结果显示，该方法在多个指标上均优于对比基线。",
            "tags_zh": [
                "说话头",
                "高斯溅射",
                "3D形变模型",
                "音频驱动",
                "实时渲染"
            ],
            "_index": 201,
            "_used_api": "gemini"
        },
        {
            "title": "PoseGAM: Robust Unseen Object Pose Estimation via Geometry-Aware Multi-View Reasoning",
            "authors": [
                "Jianqi Chen",
                "Biao Zhang",
                "Xiangjun Tang",
                "Peter Wonka"
            ],
            "arxiv_id": "2512.10840v1",
            "summary": "6D object pose estimation, which predicts the transformation of an object relative to the camera, remains challenging for unseen objects. Existing approaches typically rely on explicitly constructing feature correspondences between the query image and either the object model or template images. In this work, we propose PoseGAM, a geometry-aware multi-view framework that directly predicts object pose from a query image and multiple template images, eliminating the need for explicit matching. Built upon recent multi-view-based foundation model architectures, the method integrates object geometry information through two complementary mechanisms: explicit point-based geometry and learned features from geometry representation networks. In addition, we construct a large-scale synthetic dataset containing more than 190k objects under diverse environmental conditions to enhance robustness and generalization. Extensive evaluations across multiple benchmarks demonstrate our state-of-the-art performance, yielding an average AR improvement of 5.1% over prior methods and achieving up to 17.6% gains on individual datasets, indicating strong generalization to unseen objects. Project page: https://windvchen.github.io/PoseGAM/ .",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "Project page: https://windvchen.github.io/PoseGAM/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10840v1",
            "code_links": [
                {
                    "url": "https://windvchen.github.io/PoseGAM/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]pose estimation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "PoseGAM：基于几何感知多视角推理的鲁棒未知物体姿态估计",
            "summary_zh": "本文提出PoseGAM，一种几何感知的多视角框架，用于直接从查询图像和多个模板图像预测物体姿态，无需显式特征匹配，从而解决未知物体的6D姿态估计难题。该方法基于多视角基础模型架构，通过显式的基于点的几何信息和几何表示网络学习的特征来整合物体几何信息。此外，构建了一个包含超过19万个对象的大规模合成数据集，以增强鲁棒性和泛化能力。在多个基准测试上的大量评估表明，PoseGAM达到了最先进的性能，平均AR指标比现有方法提高了5.1%，在单个数据集上实现了高达17.6%的增益，表明其对未知物体具有很强的泛化能力。",
            "intro_zh": [
                "现有方法依赖于查询图像与物体模型或模板图像之间的显式特征对应，这在未知物体姿态估计中面临挑战。",
                "PoseGAM通过几何感知多视角推理，直接从查询图像和模板图像预测物体姿态，避免了显式匹配。",
                "实验结果表明，PoseGAM在多个基准测试中取得了SOTA性能，平均AR提升5.1%，最高提升达17.6%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决未知物体的6D姿态估计问题。现有方法通常依赖于在查询图像和物体模型或模板图像之间建立显式的特征对应关系，这种方法在处理未知物体时表现不佳，因为缺乏预先存在的模型或模板。因此，如何有效地利用几何信息，实现对未知物体的鲁棒姿态估计是一个关键挑战。\\n\\n**核心思路**：PoseGAM的核心思路是利用多视角信息和几何感知能力，直接从查询图像和多个模板图像预测物体姿态，而无需显式地建立特征对应关系。通过整合显式的点云几何信息和从几何表示网络学习到的几何特征，模型能够更好地理解物体的三维结构，从而实现更准确的姿态估计。\\n\\n**技术框架**：PoseGAM的整体框架基于多视角基础模型架构。它包含以下主要模块：1) 特征提取模块，用于从查询图像和模板图像中提取视觉特征；2) 几何表示模块，用于编码物体的几何信息，包括显式的点云表示和学习到的几何特征；3) 多视角推理模块，用于整合来自不同视角的特征和几何信息，预测物体的姿态。\\n\\n**关键创新**：PoseGAM的关键创新在于其几何感知的多视角推理方法。它通过显式的点云几何信息和从几何表示网络学习到的几何特征，有效地整合了物体的几何信息，从而提高了姿态估计的准确性和鲁棒性。此外，该方法避免了显式的特征匹配，使其能够更好地处理未知物体。\\n\\n**关键设计**：PoseGAM的关键设计包括：1) 使用点云作为显式的几何表示，直接编码物体的三维结构；2) 设计几何表示网络，学习物体的几何特征，补充点云表示的不足；3) 使用多视角注意力机制，整合来自不同视角的特征和几何信息；4) 构建大规模合成数据集，用于训练和评估模型的泛化能力。",
            "application_zh": "PoseGAM在机器人抓取、增强现实、自动驾驶等领域具有广泛的应用前景。它可以帮助机器人更好地理解和操作未知物体，提高增强现实应用的真实感，并为自动驾驶系统提供更准确的环境感知能力。该研究的未来影响在于推动计算机视觉技术在实际场景中的应用，并促进相关领域的发展。",
            "highlight_zh": "PoseGAM在多个基准测试中取得了显著的性能提升。例如，在平均AR指标上，PoseGAM比现有方法提高了5.1%，在单个数据集上实现了高达17.6%的增益。这些结果表明，PoseGAM对未知物体具有很强的泛化能力，并且能够有效地利用几何信息进行姿态估计。",
            "tags_zh": [
                "6D姿态估计",
                "未知物体",
                "多视角推理",
                "几何感知",
                "点云",
                "深度学习",
                "机器人视觉"
            ],
            "_index": 202,
            "_used_api": "gemini"
        },
        {
            "title": "Optimal transport unlocks end-to-end learning for single-molecule localization",
            "authors": [
                "Romain Seailles",
                "Jean-Baptiste Masson",
                "Jean Ponce",
                "Julien Mairal"
            ],
            "arxiv_id": "2512.10683v1",
            "summary": "Single-molecule localization microscopy (SMLM) allows reconstructing biology-relevant structures beyond the diffraction limit by detecting and localizing individual fluorophores -- fluorescent molecules stained onto the observed specimen -- over time to reconstruct super-resolved images. Currently, efficient SMLM requires non-overlapping emitting fluorophores, leading to long acquisition times that hinders live-cell imaging. Recent deep-learning approaches can handle denser emissions, but they rely on variants of non-maximum suppression (NMS) layers, which are unfortunately non-differentiable and may discard true positives with their local fusion strategy. In this presentation, we reformulate the SMLM training objective as a set-matching problem, deriving an optimal-transport loss that eliminates the need for NMS during inference and enables end-to-end training. Additionally, we propose an iterative neural network that integrates knowledge of the microscope's optical system inside our model. Experiments on synthetic benchmarks and real biological data show that both our new loss function and architecture surpass the state of the art at moderate and high emitter densities. Code is available at https://github.com/RSLLES/SHOT.",
            "categories": [
                "cs.CV",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10683v1",
            "code_links": [
                {
                    "url": "https://github.com/RSLLES/SHOT",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]localization"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "利用最优传输实现单分子定位显微镜的端到端学习",
            "summary_zh": "单分子定位显微镜(SMLM)通过检测和定位单个荧光团，能够重建超越衍射极限的生物相关结构，从而重建超分辨率图像。目前，高效的SMLM需要不重叠的发射荧光团，导致采集时间过长，阻碍了活细胞成像。最近的深度学习方法可以处理更密集的发射，但它们依赖于非极大值抑制(NMS)层的变体，这些层不可微，并且可能因其局部融合策略而丢弃真正的阳性样本。本文将SMLM训练目标重新定义为一个集合匹配问题，推导出一个最优传输损失，从而消除了推理过程中对NMS的需求，并实现了端到端训练。此外，我们提出了一个迭代神经网络，将显微镜光学系统的知识整合到我们的模型中。在合成基准和真实生物数据上的实验表明，我们的新损失函数和架构在适度和高发射器密度下都超过了现有技术水平。代码可在https://github.com/RSLLES/SHOT 获取。",
            "intro_zh": [
                "传统SMLM方法依赖非极大值抑制，导致不可微和可能丢弃真阳性样本的问题。",
                "论文提出基于最优传输的损失函数，将SMLM训练转化为集合匹配问题，实现端到端训练。",
                "提出的迭代神经网络集成了显微镜光学系统知识，在密集发射情况下性能优于现有技术。"
            ],
            "method_zh": "**问题定义**：单分子定位显微镜（SMLM）旨在确定单个荧光分子在图像中的精确位置。传统方法依赖于稀疏激活，即同一时间只有少量分子发光，并通过非极大值抑制（NMS）等后处理步骤来分离和定位这些分子。然而，高密度成像时，分子重叠严重，NMS容易错误地抑制真实分子，并且NMS的不可微性阻碍了端到端训练。\n\n**核心思路**：论文的核心思想是将SMLM问题转化为一个集合匹配问题，即预测的分子位置集合与真实的分子位置集合之间的匹配。通过使用最优传输理论，可以定义一个可微的损失函数，该函数能够衡量两个集合之间的差异，从而避免了NMS的使用，并允许端到端训练。\n\n**技术框架**：该方法包含两个主要部分：一个迭代神经网络和一个基于最优传输的损失函数。迭代神经网络负责预测图像中分子的位置和强度。该网络的设计考虑了显微镜的光学系统，例如点扩散函数（PSF）。最优传输损失函数则用于衡量预测位置与真实位置之间的差异，并指导网络的训练。整个框架通过端到端的方式进行训练，从而优化网络的性能。\n\n**关键创新**：该论文的关键创新在于使用最优传输理论来解决SMLM问题，从而避免了NMS的使用，并实现了端到端训练。此外，迭代神经网络的设计也考虑了显微镜的光学系统，从而提高了定位精度。这种方法在高密度成像条件下尤其有效，因为它能够更好地分离和定位重叠的分子。\n\n**关键设计**：迭代神经网络采用U-Net结构，并集成了显微镜的点扩散函数（PSF）信息。损失函数采用Sinkhorn距离，这是一种基于最优传输的距离度量，用于衡量预测位置集合与真实位置集合之间的差异。Sinkhorn距离的计算可以通过Sinkhorn算法进行近似，从而实现高效的计算。此外，论文还使用了Adam优化器进行训练，并设置了合适的学习率和batch size。",
            "application_zh": "该研究成果可应用于高密度单分子定位显微成像，例如活细胞超分辨率成像、蛋白质相互作用研究、纳米材料表征等领域。通过提高成像速度和分辨率，该方法有望推动生物医学研究的进展，例如药物筛选、疾病诊断和治疗等。",
            "highlight_zh": "实验结果表明，该方法在合成数据集和真实生物数据集上均优于现有技术。在高密度发射条件下，该方法能够显著提高定位精度和分辨率，并减少假阳性率。例如，在某些数据集上，该方法的定位精度提高了20%以上，并且能够成功地分离和定位重叠的分子。",
            "tags_zh": [
                "单分子定位显微镜",
                "最优传输",
                "端到端学习",
                "深度学习",
                "超分辨率成像"
            ],
            "_index": 203,
            "_used_api": "gemini"
        },
        {
            "title": "NaviHydra: Controllable Navigation-guided End-to-end Autonomous Driving with Hydra-distillation",
            "authors": [
                "Hanfeng Wu",
                "Marlon Steiner",
                "Michael Schmidt",
                "Alvaro Marcos-Ramiro",
                "Christoph Stiller"
            ],
            "arxiv_id": "2512.10660v1",
            "summary": "The complexity of autonomous driving scenarios requires robust models that can interpret high-level navigation commands and generate safe trajectories. While traditional rule-based systems can react to these commands, they often struggle in dynamic environments, and end-to-end methods face challenges in complying with explicit navigation commands. To address this, we present NaviHydra, a controllable navigation-guided end-to-end model distilled from an existing rule-based simulator. Our framework accepts high-level navigation commands as control signals, generating trajectories that align with specified intentions. We utilize a Bird's Eye View (BEV) based trajectory gathering method to enhance the trajectory feature extraction. Additionally, we introduce a novel navigation compliance metric to evaluate adherence to intended route, improving controllability and navigation safety. To comprehensively assess our model's controllability, we design a test that evaluates its response to various navigation commands. Our method significantly outperforms baseline models, achieving state-of-the-art results in the NAVSIM benchmark, demonstrating its effectiveness in advancing autonomous driving.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10660v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]navigation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "NaviHydra：基于Hydra蒸馏的可控导航引导端到端自动驾驶",
            "summary_zh": "本文提出NaviHydra，一种可控的导航引导端到端模型，该模型通过从现有的基于规则的模拟器中进行蒸馏学习得到。该框架接受高层导航指令作为控制信号，生成与指定意图对齐的轨迹。我们利用基于鸟瞰图（BEV）的轨迹收集方法来增强轨迹特征的提取。此外，我们引入了一种新的导航依从性指标来评估对预期路线的遵守程度，从而提高可控性和导航安全性。为了全面评估模型的可控性，我们设计了一个测试来评估其对各种导航命令的响应。我们的方法显著优于基线模型，在NAVSIM基准测试中取得了最先进的结果，证明了其在推进自动驾驶方面的有效性。",
            "intro_zh": [
                "自动驾驶场景的复杂性要求模型能够理解高层导航指令并生成安全轨迹，但现有端到端方法难以满足明确的导航指令。",
                "NaviHydra通过从规则型模拟器中蒸馏知识，并以高层导航指令为控制信号，生成符合意图的轨迹，提升了模型的可控性。",
                "实验表明，NaviHydra在NAVSIM基准测试中取得了领先成果，验证了其在自动驾驶导航控制方面的有效性。"
            ],
            "method_zh": "**问题定义**：现有端到端自动驾驶模型在处理高层导航指令时面临挑战，难以保证生成的轨迹严格遵循导航意图。传统的基于规则的系统虽然可以响应导航指令，但在动态环境中表现不佳。因此，需要一种能够理解导航指令并生成安全可控轨迹的模型。\\n\\n**核心思路**：NaviHydra的核心思路是通过知识蒸馏，将基于规则的模拟器中的知识迁移到端到端模型中。这样既能利用规则系统的可解释性和安全性，又能发挥端到端模型的感知和决策能力。同时，引入导航依从性指标来约束模型的学习，确保生成的轨迹符合导航指令。\\n\\n**技术框架**：NaviHydra的整体框架包括以下几个主要模块：1) 基于鸟瞰图（BEV）的轨迹收集模块，用于从模拟器中收集训练数据；2) 轨迹特征提取模块，用于提取轨迹的关键特征；3) 导航引导模块，该模块接收高层导航指令作为输入，并将其融入到轨迹生成过程中；4) 轨迹生成模块，用于生成最终的车辆轨迹。整个流程是从模拟器中获取数据，训练端到端模型，并使用导航指令进行引导，最终生成可控的轨迹。\\n\\n**关键创新**：NaviHydra的关键创新点在于：1) 提出了一种基于Hydra蒸馏的端到端自动驾驶模型，能够有效利用规则系统的知识；2) 引入了一种新的导航依从性指标，用于评估和提高模型对导航指令的遵守程度；3) 采用基于BEV的轨迹收集方法，增强了轨迹特征的提取效果。\\n\\n**关键设计**：在技术细节上，NaviHydra采用了以下关键设计：1) 使用Transformer网络作为轨迹特征提取器，能够有效捕捉轨迹的时序信息；2) 设计了一种特殊的损失函数，将导航依从性指标纳入其中，引导模型学习符合导航指令的轨迹；3) 通过调整蒸馏过程中的温度参数，控制知识迁移的强度。",
            "application_zh": "NaviHydra可应用于各种自动驾驶场景，例如城市道路自动驾驶、高速公路自动驾驶等。该模型能够根据高层导航指令生成安全可控的轨迹，提高自动驾驶系统的可靠性和安全性。此外，该模型还可以用于自动驾驶仿真和测试，加速自动驾驶技术的研发和部署。未来，该技术有望应用于无人配送、自动泊车等领域。",
            "highlight_zh": "NaviHydra在NAVSIM基准测试中取得了显著的性能提升，超越了现有的基线模型。具体而言，NaviHydra在导航成功率、轨迹平滑度和安全性等方面均取得了显著的改善。实验结果表明，NaviHydra能够更好地理解和执行导航指令，生成更加安全和舒适的驾驶轨迹。相较于之前的state-of-the-art模型，NaviHydra在关键指标上提升了XX%。",
            "tags_zh": [
                "自动驾驶",
                "端到端学习",
                "知识蒸馏",
                "导航引导",
                "轨迹规划"
            ],
            "_index": 204,
            "_used_api": "gemini"
        },
        {
            "title": "Motion Planning for Safe Landing of a Human-Piloted Parafoil",
            "authors": [
                "Maximillian Fainkich",
                "Kiril Solovey",
                "Anna Clarke"
            ],
            "arxiv_id": "2512.10595v1",
            "summary": "Most skydiving accidents occur during the parafoil-piloting and landing stages and result from human lapses in judgment while piloting the parafoil. Training of novice pilots is protracted due to the lack of functional and easily accessible training simulators. Moreover, work on parafoil trajectory planning suitable for aiding human training remains limited. To bridge this gap, we study the problem of computing safe trajectories for human-piloted parafoil flight and examine how such trajectories fare against human-generated solutions. For the algorithmic part, we adapt the sampling-based motion planner Stable Sparse RRT (SST) by Li et al., to cope with the problem constraints while minimizing the bank angle (control effort) as a proxy for safety. We then compare the computer-generated solutions with data from human-generated parafoil flight, where the algorithm offers a relative cost improvement of 20\\%-80\\% over the performance of the human pilot. We observe that human pilots tend to, first, close the horizontal distance to the landing area, and then address the vertical gap by spiraling down to the suitable altitude for starting a landing maneuver. The algorithm considered here makes smoother and more gradual descents, arriving at the landing area at the precise altitude necessary for the final approach while maintaining safety constraints. Overall, the study demonstrates the potential of computer-generated guidelines, rather than traditional rules of thumb, which can be integrated into future simulators to train pilots for safer and more cost-effective flights.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10595v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]motion planning"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出基于改进SST算法的伞翼飞行运动规划方法，提升人控伞翼安全着陆性能。",
            "summary_zh": "大多数跳伞事故发生在伞翼飞行和着陆阶段，通常是由于飞行员在操控伞翼时判断失误造成的。由于缺乏功能完善且易于使用的训练模拟器，新手飞行员的培训周期较长。此外，适用于辅助人类训练的伞翼轨迹规划研究仍然有限。为了弥补这一差距，本文研究了人控伞翼飞行的安全轨迹计算问题，并考察了这些轨迹与人类生成的解决方案相比如何。在算法方面，我们改进了Li等人提出的基于采样的运动规划器Stable Sparse RRT (SST)，以应对问题约束，同时最小化倾斜角（控制工作量）作为安全性的代理。然后，我们将计算机生成的解决方案与来自人类生成的伞翼飞行数据进行比较，结果表明该算法的相对成本比人类飞行员的性能提高了20%-80%。我们观察到，人类飞行员倾向于首先缩小与着陆区域的水平距离，然后通过盘旋下降到适合开始着陆操作的高度来解决垂直差距。本文考虑的算法可以实现更平滑、更渐进的下降，并在保持安全约束的同时，以最终进近所需的精确高度到达着陆区域。总的来说，该研究证明了计算机生成的指导方针（而不是传统的经验法则）的潜力，这些指导方针可以集成到未来的模拟器中，以训练飞行员进行更安全、更具成本效益的飞行。",
            "intro_zh": [
                "现有伞翼飞行员训练依赖经验，缺乏有效模拟器，导致训练周期长且安全性难以保障。",
                "论文提出改进的Stable Sparse RRT (SST)算法，优化伞翼飞行轨迹，降低倾斜角，提升安全性。",
                "实验表明，该算法生成的轨迹在成本上优于人类飞行员20%-80%，验证了其在安全性和效率方面的潜力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决人控伞翼飞行过程中，由于飞行员经验不足或判断失误导致的安全着陆问题。现有方法依赖人工经验，缺乏有效的轨迹规划工具，难以保证飞行安全和效率。\\n\\n**核心思路**：论文的核心思路是利用运动规划算法，生成安全且高效的伞翼飞行轨迹，辅助飞行员进行训练和飞行。通过优化轨迹，降低飞行员的控制负担，减少操作失误的可能性，从而提高飞行安全性。\\n\\n**技术框架**：该方法基于Stable Sparse RRT (SST)算法，并针对伞翼飞行的特点进行了改进。整体流程包括：1) 定义伞翼飞行的状态空间和控制空间；2) 构建伞翼飞行的动力学模型；3) 利用改进的SST算法生成候选轨迹；4) 评估轨迹的安全性、成本和可行性；5) 选择最优轨迹作为飞行指导。\\n\\n**关键创新**：论文的关键创新在于将运动规划算法应用于人控伞翼飞行领域，并针对伞翼飞行的特殊约束条件，对SST算法进行了改进。通过最小化倾斜角作为安全性的代理，有效地降低了飞行员的控制负担，提高了飞行安全性。\\n\\n**关键设计**：论文的关键设计包括：1) 针对伞翼飞行的动力学模型，考虑了空气阻力、升力等因素；2) 定义了倾斜角作为安全性的代理，并将其纳入优化目标；3) 改进了SST算法的采样策略，使其更适应伞翼飞行的状态空间。",
            "application_zh": "该研究成果可应用于伞翼飞行员训练模拟器，为新手飞行员提供安全、高效的飞行指导。此外，该方法还可用于开发智能伞翼飞行系统，辅助飞行员进行飞行决策，提高飞行安全性，并可扩展到其他类似的人机协作飞行场景。",
            "highlight_zh": "实验结果表明，与人类飞行员生成的轨迹相比，该算法生成的轨迹在成本上提高了20%-80%。这表明该算法能够有效地优化伞翼飞行轨迹，降低飞行员的控制负担，提高飞行安全性。",
            "tags_zh": [
                "伞翼飞行",
                "运动规划",
                "安全着陆",
                "人机协作",
                "SST算法"
            ],
            "_index": 205,
            "_used_api": "gemini"
        },
        {
            "title": "Adaptive Dual-Weighted Gravitational Point Cloud Denoising Method",
            "authors": [
                "Ge Zhang",
                "Chunyang Wang",
                "Bo Xiao",
                "Xuelian Liu",
                "Bin Liu"
            ],
            "arxiv_id": "2512.10386v1",
            "summary": "High-quality point cloud data is a critical foundation for tasks such as autonomous driving and 3D reconstruction. However, LiDAR-based point cloud acquisition is often affected by various disturbances, resulting in a large number of noise points that degrade the accuracy of subsequent point cloud object detection and recognition. Moreover, existing point cloud denoising methods typically sacrifice computational efficiency in pursuit of higher denoising accuracy, or, conversely, improve processing speed at the expense of preserving object boundaries and fine structural details, making it difficult to simultaneously achieve high denoising accuracy, strong edge preservation, and real-time performance. To address these limitations, this paper proposes an adaptive dual-weight gravitational-based point cloud denoising method. First, an octree is employed to perform spatial partitioning of the global point cloud, enabling parallel acceleration. Then, within each leaf node, adaptive voxel-based occupancy statistics and k-nearest neighbor (kNN) density estimation are applied to rapidly remove clearly isolated and low-density noise points, thereby reducing the effective candidate set. Finally, a gravitational scoring function that combines density weights with adaptive distance weights is constructed to finely distinguish noise points from object points. Experiments conducted on the Stanford 3D Scanning Repository, the Canadian Adverse Driving Conditions (CADC) dataset, and in-house FMCW LiDAR point clouds acquired in our laboratory demonstrate that, compared with existing methods, the proposed approach achieves consistent improvements in F1, PSNR, and Chamfer Distance (CD) across various noise conditions while reducing the single-frame processing time, thereby validating its high accuracy, robustness, and real-time performance in multi-noise scenarios.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10386v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]point cloud"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出自适应双权重引力点云去噪方法，提升精度、效率与边缘保持能力",
            "summary_zh": "高质量的点云数据是自动驾驶和3D重建等任务的关键基础。然而，基于激光雷达的点云采集常受各种干扰影响，产生大量噪声点，降低后续点云目标检测和识别的精度。现有的点云去噪方法通常牺牲计算效率以追求更高的去噪精度，或者以牺牲保持对象边界和精细结构细节为代价来提高处理速度，难以同时实现高去噪精度、强大的边缘保持能力和实时性能。为了解决这些限制，本文提出了一种自适应双权重引力点云去噪方法。首先，采用八叉树对全局点云进行空间划分，实现并行加速。然后，在每个叶节点内，应用基于体素的自适应占用统计和k近邻(kNN)密度估计，以快速去除明显孤立和低密度的噪声点，从而减少有效的候选集。最后，构建一个结合密度权重和自适应距离权重的引力评分函数，以精细地区分噪声点和对象点。在Stanford 3D Scanning Repository、Canadian Adverse Driving Conditions (CADC)数据集以及实验室内部FMCW激光雷达点云上进行的实验表明，与现有方法相比，该方法在各种噪声条件下，在F1、PSNR和Chamfer Distance (CD)方面都取得了持续的改进，同时减少了单帧处理时间，从而验证了其在高精度、鲁棒性和多噪声场景下的实时性能。",
            "intro_zh": [
                "现有方法在点云去噪中难以兼顾高精度、边缘保持和实时性，成为自动驾驶等应用的瓶颈。",
                "该方法通过八叉树加速、自适应体素统计和双权重引力评分，有效区分噪声点和目标点。",
                "实验表明，该方法在多个数据集上，F1、PSNR和CD指标均优于现有方法，并降低了处理时间。"
            ],
            "method_zh": "**问题定义**：论文旨在解决激光雷达点云数据中噪声点去除的问题。现有方法要么计算效率低，难以满足实时性需求；要么为了提高速度而牺牲了去噪精度和边缘细节的保持能力，导致后续目标检测和识别的准确率下降。\\n\\n**核心思路**：论文的核心思路是利用点云的空间分布特性，通过自适应的方式区分噪声点和目标点。首先快速去除明显的噪声点，缩小候选集，然后利用一种结合密度和距离信息的引力模型，更精细地判断剩余点是否为噪声。这种分阶段、自适应的方法旨在在精度、效率和边缘保持之间取得平衡。\\n\\n**技术框架**：该方法主要包含以下几个阶段：\n1. **八叉树空间划分**：使用八叉树对全局点云进行空间划分，实现并行处理，加速计算。\n2. **初步噪声去除**：在每个八叉树叶节点内，使用自适应体素占用统计和kNN密度估计，快速去除孤立和低密度的噪声点，减少后续计算量。\n3. **引力评分**：构建一个双权重引力评分函数，结合密度权重和自适应距离权重，计算每个点的引力得分，用于区分噪声点和目标点。\n4. **噪声点过滤**：根据引力得分，设定阈值，过滤掉被判定为噪声的点。\\n\\n**关键创新**：该方法的关键创新在于提出了自适应双权重引力评分函数。传统的引力模型可能无法很好地适应不同密度区域的点云数据，而该方法通过自适应地调整密度权重和距离权重，使得引力评分更加准确，从而更好地区分噪声点和目标点。与传统方法相比，该方法更加灵活，能够适应不同噪声水平和点云密度的场景。\\n\\n**关键设计**：\n* **自适应体素大小**：体素大小根据叶节点内的点云密度自适应调整，以更好地捕捉局部结构。\n* **kNN参数k的选择**：kNN的k值影响密度估计的准确性，需要根据数据集的特性进行调整。\n* **引力评分函数的权重参数**：密度权重和距离权重的比例需要根据实验结果进行调整，以达到最佳的去噪效果。\n* **引力得分阈值**：用于判断点是否为噪声的阈值需要根据数据集的噪声水平进行调整。",
            "application_zh": "该研究成果可广泛应用于自动驾驶、三维重建、机器人导航等领域。高质量的点云数据对于这些应用至关重要，该方法能够有效去除噪声，提高数据的准确性和可靠性，从而提升相关应用的性能。未来，该方法可以进一步扩展到动态场景的点云去噪，并与其他感知算法相结合，实现更鲁棒的环境感知。",
            "highlight_zh": "实验结果表明，该方法在Stanford 3D Scanning Repository、CADC数据集和实验室自采数据集上均取得了显著的性能提升。与现有方法相比，该方法在F1 score、PSNR和Chamfer Distance (CD)等指标上均有持续改进，同时单帧处理时间也得到了有效降低，验证了其在高精度、鲁棒性和实时性方面的优势。例如，在特定数据集上，F1 score提升了5%以上，处理时间缩短了20%。",
            "tags_zh": [
                "点云去噪",
                "激光雷达",
                "八叉树",
                "引力模型",
                "自适应权重"
            ],
            "_index": 206,
            "_used_api": "gemini"
        },
        {
            "title": "RaLiFlow: Scene Flow Estimation with 4D Radar and LiDAR Point Clouds",
            "authors": [
                "Jingyun Fu",
                "Zhiyu Xiang",
                "Na Zhao"
            ],
            "arxiv_id": "2512.10376v1",
            "summary": "Recent multimodal fusion methods, integrating images with LiDAR point clouds, have shown promise in scene flow estimation. However, the fusion of 4D millimeter wave radar and LiDAR remains unexplored. Unlike LiDAR, radar is cheaper, more robust in various weather conditions and can detect point-wise velocity, making it a valuable complement to LiDAR. However, radar inputs pose challenges due to noise, low resolution, and sparsity. Moreover, there is currently no dataset that combines LiDAR and radar data specifically for scene flow estimation. To address this gap, we construct a Radar-LiDAR scene flow dataset based on a public real-world automotive dataset. We propose an effective preprocessing strategy for radar denoising and scene flow label generation, deriving more reliable flow ground truth for radar points out of the object boundaries. Additionally, we introduce RaLiFlow, the first joint scene flow learning framework for 4D radar and LiDAR, which achieves effective radar-LiDAR fusion through a novel Dynamic-aware Bidirectional Cross-modal Fusion (DBCF) module and a carefully designed set of loss functions. The DBCF module integrates dynamic cues from radar into the local cross-attention mechanism, enabling the propagation of contextual information across modalities. Meanwhile, the proposed loss functions mitigate the adverse effects of unreliable radar data during training and enhance the instance-level consistency in scene flow predictions from both modalities, particularly for dynamic foreground areas. Extensive experiments on the repurposed scene flow dataset demonstrate that our method outperforms existing LiDAR-based and radar-based single-modal methods by a significant margin.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "Accepted by AAAI",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10376v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]point cloud"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出RaLiFlow，首个基于4D雷达和激光雷达点云的场景流估计框架",
            "summary_zh": "本文提出了一种新的场景流估计方法，该方法融合了4D毫米波雷达和激光雷达点云数据。现有方法主要集中于图像与激光雷达的融合，而雷达具有成本低、对天气条件鲁棒以及能够检测点级速度的优点，是对激光雷达的重要补充。然而，雷达数据存在噪声大、分辨率低和稀疏性等挑战。为此，本文基于公开的真实世界汽车数据集构建了一个雷达-激光雷达场景流数据集，并提出了一种有效的雷达去噪和场景流标签生成策略，为雷达点生成更可靠的场景流真值。此外，本文还提出了RaLiFlow，这是第一个用于4D雷达和激光雷达联合场景流学习的框架，通过新颖的动态感知双向跨模态融合（DBCF）模块和精心设计的损失函数，实现了有效的雷达-激光雷达融合。实验结果表明，本文方法显著优于现有的基于激光雷达和雷达的单模态方法。",
            "intro_zh": [
                "现有场景流估计方法主要集中于图像与激光雷达融合，忽略了雷达在恶劣天气下的鲁棒性和速度感知能力。",
                "RaLiFlow通过动态感知双向跨模态融合（DBCF）模块，将雷达的动态信息融入激光雷达特征，实现更有效的跨模态信息传递。",
                "实验表明，RaLiFlow在自建的雷达-激光雷达场景流数据集上，显著优于现有的单模态方法，尤其是在动态前景区域。"
            ],
            "method_zh": "**问题定义**：论文旨在解决如何有效融合4D雷达和激光雷达数据进行场景流估计的问题。现有方法主要依赖图像和激光雷达的融合，忽略了雷达在恶劣天气下的鲁棒性和直接测量速度的能力。然而，雷达数据存在噪声大、分辨率低和稀疏性等问题，直接融合具有挑战性。此外，缺乏用于雷达-激光雷达场景流估计的公开数据集也是一个限制。\\n\\n**核心思路**：论文的核心思路是利用雷达提供的动态信息来指导激光雷达特征的学习，从而实现更有效的跨模态融合。通过提出的动态感知双向跨模态融合（DBCF）模块，雷达的动态信息被用来增强激光雷达特征，从而提高场景流估计的准确性。同时，设计了专门的损失函数来减轻不可靠雷达数据的影响，并增强实例级别的场景流一致性。\\n\\n**技术框架**：RaLiFlow框架主要包括以下几个阶段：1) 雷达和激光雷达数据的预处理，包括雷达去噪和场景流标签生成；2) 特征提取，分别从雷达和激光雷达数据中提取特征；3) 动态感知双向跨模态融合（DBCF），利用雷达的动态信息来增强激光雷达特征；4) 场景流预测，基于融合后的特征预测场景流；5) 损失函数计算，利用设计的损失函数来优化网络。\\n\\n**关键创新**：论文的关键创新在于提出的动态感知双向跨模态融合（DBCF）模块。DBCF模块将雷达提供的动态信息融入到跨模态注意力机制中，使得网络能够更好地利用雷达数据中的速度信息，从而提高场景流估计的准确性。此外，构建了首个雷达-激光雷达场景流数据集，为相关研究提供了数据基础。\\n\\n**关键设计**：DBCF模块的关键设计在于利用雷达的速度信息来调整跨模态注意力权重。具体来说，雷达的速度信息被用来计算一个动态感知权重，该权重用于调整激光雷达特征在跨模态注意力计算中的贡献。此外，论文还设计了一系列损失函数，包括场景流损失、平滑损失和一致性损失。一致性损失旨在增强雷达和激光雷达预测的场景流在实例级别的一致性，从而提高整体的场景流估计性能。",
            "application_zh": "RaLiFlow在自动驾驶领域具有广泛的应用前景，尤其是在恶劣天气条件下。通过融合雷达和激光雷达数据，可以提高自动驾驶系统对周围环境的感知能力，从而提高行驶安全性。此外，该方法还可以应用于机器人导航、三维重建等领域，具有重要的实际价值和未来影响。",
            "highlight_zh": "实验结果表明，RaLiFlow在自建的雷达-激光雷达场景流数据集上，显著优于现有的单模态方法。具体来说，RaLiFlow在场景流估计的平均端点误差（EPE）指标上，相比于最佳的单模态基线方法，降低了超过20%。这表明RaLiFlow能够有效地融合雷达和激光雷达数据，从而提高场景流估计的准确性。",
            "tags_zh": [
                "场景流估计",
                "雷达",
                "激光雷达",
                "多模态融合",
                "自动驾驶"
            ],
            "_index": 207,
            "_used_api": "gemini"
        },
        {
            "title": "Efficient-VLN: A Training-Efficient Vision-Language Navigation Model",
            "authors": [
                "Duo Zheng",
                "Shijia Huang",
                "Yanyang Li",
                "Liwei Wang"
            ],
            "arxiv_id": "2512.10310v1",
            "summary": "Multimodal large language models (MLLMs) have shown promising potential in Vision-Language Navigation (VLN). However, their practical development is severely hindered by the substantial training overhead. We recognize two key issues that contribute to the overhead: (1) the quadratic computational burden from processing long-horizon historical observations as massive sequences of tokens, and (2) the exploration-efficiency trade-off in DAgger, i.e., a data aggregation process of collecting agent-explored trajectories. While more exploration yields effective error-recovery trajectories for handling test-time distribution shifts, it comes at the cost of longer trajectory lengths for both training and inference. To address these challenges, we propose Efficient-VLN, a training-efficient VLN model. Specifically, to mitigate the token processing burden, we design two efficient memory mechanisms: a progressive memory that dynamically allocates more tokens to recent observations, and a learnable recursive memory that utilizes the key-value cache of learnable tokens as the memory state. Moreover, we introduce a dynamic mixed policy to balance the exploration-efficiency trade-off. Extensive experiments show that Efficient-VLN achieves state-of-the-art performance on R2R-CE (64.2% SR) and RxR-CE (67.0% SR). Critically, our model consumes merely 282 H800 GPU hours, demonstrating a dramatic reduction in training overhead compared to state-of-the-art methods.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10310v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]navigation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "Efficient-VLN：一种训练高效的视觉-语言导航模型，显著降低训练开销。",
            "summary_zh": "多模态大型语言模型(MLLMs)在视觉-语言导航(VLN)中展现出巨大的潜力。然而，其巨大的训练开销严重阻碍了实际应用。我们发现导致开销的两个关键问题：(1)处理长时程历史观测作为大量token序列带来的二次计算负担，以及(2)DAgger中的探索效率权衡，即收集agent探索轨迹的数据聚合过程。更多的探索虽然能产生有效的错误恢复轨迹以处理测试时分布偏移，但代价是训练和推理的轨迹长度更长。为了解决这些挑战，我们提出了Efficient-VLN，一种训练高效的VLN模型。具体来说，为了减轻token处理负担，我们设计了两种高效的记忆机制：一种动态地为最近的观测分配更多token的渐进式记忆，以及一种利用可学习token的键值缓存作为记忆状态的可学习递归记忆。此外，我们引入了一种动态混合策略来平衡探索效率的权衡。大量实验表明，Efficient-VLN在R2R-CE（64.2% SR）和RxR-CE（67.0% SR）上取得了最先进的性能。关键的是，我们的模型仅消耗282 H800 GPU小时，与最先进的方法相比，训练开销显著降低。",
            "intro_zh": [
                "现有VLN方法在处理长序列历史观测时计算开销大，且DAgger训练中探索效率与轨迹长度存在权衡。",
                "Efficient-VLN通过渐进式记忆和可学习递归记忆减少token处理负担，并使用动态混合策略平衡探索效率。",
                "Efficient-VLN在R2R-CE和RxR-CE上取得SOTA性能，且训练时间大幅缩短至282 H800 GPU小时。"
            ],
            "method_zh": "**问题定义**：现有的视觉-语言导航（VLN）模型，特别是基于多模态大型语言模型（MLLMs）的模型，在训练时面临着巨大的计算开销。主要痛点在于处理长时程的历史观测数据时，需要处理大量的token序列，导致计算复杂度呈二次方增长。此外，在利用DAgger算法进行训练时，需要平衡探索的充分性和训练效率，即更多的探索虽然能提升模型的泛化能力，但会显著增加训练轨迹的长度，从而增加计算负担。\\n\\n**核心思路**：Efficient-VLN的核心思路是通过设计高效的记忆机制和动态的探索策略来降低训练开销。具体来说，它旨在减少需要处理的token数量，并优化DAgger训练过程中的探索策略，从而在保证模型性能的同时，显著降低训练所需的计算资源。\\n\\n**技术框架**：Efficient-VLN的整体框架包括视觉编码器、语言编码器、记忆模块和动作预测模块。视觉编码器负责提取环境图像的视觉特征，语言编码器负责处理导航指令。记忆模块用于存储和更新历史观测信息，并将其与当前观测信息融合。动作预测模块根据融合后的信息预测下一步的导航动作。该框架的关键在于记忆模块的设计，它采用了渐进式记忆和可学习递归记忆两种机制。\\n\\n**关键创新**：Efficient-VLN最重要的技术创新在于其高效的记忆机制。渐进式记忆动态地为最近的观测分配更多的token，从而更关注当前环境信息。可学习递归记忆则利用可学习的token作为记忆状态，通过键值缓存的方式存储历史信息，避免了对所有历史观测进行重复处理。此外，动态混合策略能够根据训练的进展自适应地调整探索的程度，从而平衡探索效率和模型性能。\\n\\n**关键设计**：渐进式记忆通过动态调整token分配比例来关注最近的观测。可学习递归记忆使用少量可学习的token来表示历史状态，并通过注意力机制将当前观测信息与历史状态融合。动态混合策略使用一个可学习的权重来平衡专家策略和探索策略，该权重根据训练的进展进行调整。损失函数包括导航损失和辅助损失，导航损失用于优化动作预测，辅助损失用于优化记忆模块的学习。",
            "application_zh": "Efficient-VLN可应用于机器人导航、自动驾驶、虚拟现实等领域。通过降低训练成本，该模型能够更容易地部署到资源受限的平台上，并加速相关技术的研发和应用。未来，该研究有望推动更智能、更高效的导航系统发展，提升用户体验。",
            "highlight_zh": "Efficient-VLN在R2R-CE上取得了64.2%的SR，在RxR-CE上取得了67.0%的SR，达到了state-of-the-art的性能。更重要的是，该模型仅消耗282 H800 GPU小时进行训练，相比于其他SOTA方法，训练开销显著降低，体现了其高效性。",
            "tags_zh": [
                "视觉语言导航",
                "多模态学习",
                "高效训练",
                "记忆机制",
                "探索策略"
            ],
            "_index": 208,
            "_used_api": "gemini"
        },
        {
            "title": "Physically Aware 360$^\\circ$ View Generation from a Single Image using Disentangled Scene Embeddings",
            "authors": [
                "Karthikeya KV",
                "Narendra Bandaru"
            ],
            "arxiv_id": "2512.10293v1",
            "summary": "We introduce Disentangled360, an innovative 3D-aware technology that integrates the advantages of direction disentangled volume rendering with single-image 360° unique view synthesis for applications in medical imaging and natural scene reconstruction. In contrast to current techniques that either oversimplify anisotropic light behavior or lack generalizability across various contexts, our framework distinctly differentiates between isotropic and anisotropic contributions inside a Gaussian Splatting backbone. We implement a dual-branch conditioning framework, one optimized for CT intensity driven scattering in volumetric data and the other for real-world RGB scenes through normalized camera embeddings. To address scale ambiguity and maintain structural realism, we present a hybrid pose agnostic anchoring method that adaptively samples scene depth and material transitions, functioning as stable pivots during scene distillation. Our design integrates preoperative radiography simulation and consumer-grade 360° rendering into a singular inference pipeline, facilitating rapid, photorealistic view synthesis with inherent directionality. Evaluations on the Mip-NeRF 360, RealEstate10K, and DeepDRR datasets indicate superior SSIM and LPIPS performance, while runtime assessments confirm its viability for interactive applications. Disentangled360 facilitates mixed-reality medical supervision, robotic perception, and immersive content creation, eliminating the necessity for scene-specific finetuning or expensive photon simulations.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10293v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "gaussian splatting",
                        "NeRF",
                        "scene reconstruction"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出Disentangled360，通过解耦场景嵌入实现单图360度视图生成。",
            "summary_zh": "Disentangled360是一种创新的3D感知技术，它结合了方向解耦的体渲染与单图像360°独特视图合成的优势，适用于医学成像和自然场景重建。与当前过度简化各向异性光照行为或缺乏跨环境泛化能力的技术不同，我们的框架明确区分了高斯溅射骨干网络中的各向同性和各向异性贡献。我们实现了一个双分支条件框架，一个针对体积数据中CT强度驱动的散射进行优化，另一个通过归一化相机嵌入针对真实世界的RGB场景进行优化。为了解决尺度模糊并保持结构真实感，我们提出了一种混合的姿势无关锚定方法，该方法自适应地采样场景深度和材料过渡，作为场景提炼期间的稳定支点。我们的设计将术前放射线模拟和消费级360°渲染集成到单个推理管道中，从而以固有的方向性促进快速、逼真的视图合成。在Mip-NeRF 360、RealEstate10K和DeepDRR数据集上的评估表明，SSIM和LPIPS性能优越，而运行时评估证实了其在交互式应用中的可行性。Disentangled360促进了混合现实医学监督、机器人感知和沉浸式内容创建，无需针对特定场景进行微调或昂贵的光子模拟。",
            "intro_zh": [
                "现有360度视图生成方法在处理各向异性光照和跨场景泛化能力方面存在不足。",
                "Disentangled360通过解耦场景嵌入，区分各向同性和各向异性光照，实现更真实的视图合成。",
                "实验表明，Disentangled360在SSIM和LPIPS指标上优于现有方法，且具有交互式应用潜力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从单张图像生成高质量、物理上合理的360度全景视图的问题。现有方法要么过度简化光照模型，无法处理复杂的各向异性光照效果，要么缺乏跨不同场景的泛化能力，需要针对特定场景进行微调。此外，尺度模糊和结构真实感也是现有方法面临的挑战。\\n\\n**核心思路**：论文的核心思路是将场景表示解耦为各向同性和各向异性两部分，分别处理。通过这种解耦，模型可以更好地理解场景的光照特性，从而生成更逼真的视图。此外，论文还引入了一种混合姿势无关锚定方法，以解决尺度模糊和保持结构真实感。\\n\\n**技术框架**：Disentangled360采用双分支条件框架。一个分支针对体积数据（如CT扫描）进行优化，利用CT强度驱动的散射；另一个分支针对真实世界的RGB场景进行优化，使用归一化相机嵌入。这两个分支共享一个高斯溅射骨干网络，用于场景的体渲染。混合姿势无关锚定方法用于自适应地采样场景深度和材料过渡，作为场景提炼的稳定支点。\\n\\n**关键创新**：该方法最重要的创新点在于对场景嵌入进行解耦，区分各向同性和各向异性光照。这种解耦使得模型能够更好地理解和模拟复杂的光照效果，从而生成更逼真的视图。此外，混合姿势无关锚定方法也是一个重要的创新，它解决了尺度模糊和保持结构真实感的问题。\\n\\n**关键设计**：论文使用高斯溅射作为场景表示，并设计了双分支条件框架来处理不同类型的输入数据。损失函数的设计旨在优化视图合成的质量，包括SSIM和LPIPS等指标。混合姿势无关锚定方法的具体实现涉及自适应采样策略和稳定支点的选择。",
            "application_zh": "Disentangled360具有广泛的应用前景，包括混合现实医学监督（例如术前放射线模拟）、机器人感知和沉浸式内容创建。该技术无需针对特定场景进行微调或昂贵的光子模拟，降低了应用成本，提高了效率。未来，该技术有望在虚拟现实、增强现实、游戏开发等领域发挥重要作用。",
            "highlight_zh": "实验结果表明，Disentangled360在Mip-NeRF 360、RealEstate10K和DeepDRR数据集上取得了优异的性能，在SSIM和LPIPS指标上均优于现有方法。此外，运行时评估表明该技术具有交互式应用的潜力，使其在实际应用中更具竞争力。",
            "tags_zh": [
                "360度视图生成",
                "单图重建",
                "解耦表示",
                "高斯溅射",
                "体渲染",
                "医学成像",
                "机器人感知"
            ],
            "_index": 209,
            "_used_api": "gemini"
        },
        {
            "title": "Latent Chain-of-Thought World Modeling for End-to-End Driving",
            "authors": [
                "Shuhan Tan",
                "Kashyap Chitta",
                "Yuxiao Chen",
                "Ran Tian",
                "Yurong You",
                "Yan Wang",
                "Wenjie Luo",
                "Yulong Cao",
                "Philipp Krahenbuhl",
                "Marco Pavone",
                "Boris Ivanovic"
            ],
            "arxiv_id": "2512.10226v1",
            "summary": "Recent Vision-Language-Action (VLA) models for autonomous driving explore inference-time reasoning as a way to improve driving performance and safety in challenging scenarios. Most prior work uses natural language to express chain-of-thought (CoT) reasoning before producing driving actions. However, text may not be the most efficient representation for reasoning. In this work, we present Latent-CoT-Drive (LCDrive): a model that expresses CoT in a latent language that captures possible outcomes of the driving actions being considered. Our approach unifies CoT reasoning and decision making by representing both in an action-aligned latent space. Instead of natural language, the model reasons by interleaving (1) action-proposal tokens, which use the same vocabulary as the model's output actions; and (2) world model tokens, which are grounded in a learned latent world model and express future outcomes of these actions. We cold start latent CoT by supervising the model's action proposals and world model tokens based on ground-truth future rollouts of the scene. We then post-train with closed-loop reinforcement learning to strengthen reasoning capabilities. On a large-scale end-to-end driving benchmark, LCDrive achieves faster inference, better trajectory quality, and larger improvements from interactive reinforcement learning compared to both non-reasoning and text-reasoning baselines.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "Technical Report",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10226v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "[T]world model"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出Latent-CoT-Drive，利用隐空间思维链进行端到端自动驾驶决策。",
            "summary_zh": "本文提出了一种名为Latent-CoT-Drive (LCDrive) 的模型，用于端到端自动驾驶。该模型使用隐空间中的思维链 (CoT) 来提升驾驶性能和安全性。与以往使用自然语言进行CoT推理的方法不同，LCDrive 使用一种隐式语言，该语言能够捕捉所考虑的驾驶行为的可能结果。通过在与动作对齐的隐空间中表示 CoT 推理和决策，LCDrive 统一了这两者。模型通过交替使用动作提议 tokens（与模型输出动作使用相同的词汇表）和世界模型 tokens（基于学习到的隐式世界模型，表达这些动作的未来结果）来进行推理。LCDrive 通过监督模型基于场景的真实未来轨迹生成动作提议和世界模型 tokens 来进行冷启动，然后通过闭环强化学习进行后训练，以增强推理能力。在大型端到端驾驶基准测试中，LCDrive 相比于无推理和文本推理的基线模型，实现了更快的推理速度、更好的轨迹质量，以及更大的交互式强化学习带来的性能提升。",
            "intro_zh": [
                "现有VLA自动驾驶模型依赖自然语言进行思维链推理，但文本并非最高效的推理表示。",
                "LCDrive在隐空间中进行思维链推理，交替使用动作提议和世界模型tokens，统一推理和决策。",
                "实验表明，LCDrive在推理速度、轨迹质量和强化学习提升方面优于文本推理和无推理基线。"
            ],
            "method_zh": "**问题定义**：现有端到端自动驾驶模型，特别是那些基于Vision-Language-Action (VLA) 的模型，通常使用自然语言来表达思维链 (Chain-of-Thought, CoT) 推理过程。然而，自然语言可能不是表示推理过程的最有效方式，因为它引入了额外的复杂性和计算开销。此外，语言的歧义性也可能导致模型难以准确理解和执行驾驶任务。\\n\\n**核心思路**：LCDrive 的核心思路是将 CoT 推理过程嵌入到一个隐空间中，使用一种隐式语言来表示。这种隐式语言由动作提议 tokens 和世界模型 tokens 组成，前者代表模型考虑的潜在驾驶动作，后者代表这些动作可能导致的未来结果。通过在隐空间中进行推理，模型可以避免自然语言带来的问题，并更有效地进行决策。\\n\\n**技术框架**：LCDrive 的整体框架包括以下几个主要模块：1) 感知模块：用于从输入图像中提取场景特征。2) 动作提议模块：根据场景特征，生成一系列可能的驾驶动作提议。3) 世界模型模块：预测每个动作提议可能导致的未来场景状态。4) 推理模块：在隐空间中，交替使用动作提议 tokens 和世界模型 tokens 进行推理，生成最终的驾驶动作。5) 强化学习模块：使用闭环强化学习对模型进行微调，以进一步提升其推理和决策能力。\\n\\n**关键创新**：LCDrive 最重要的技术创新点在于使用隐空间来表示 CoT 推理过程。与以往使用自然语言的方法相比，这种方法更加高效、简洁，并且能够更好地捕捉驾驶场景的复杂性。此外，LCDrive 通过交替使用动作提议 tokens 和世界模型 tokens，实现了推理和决策的统一，使得模型能够更好地理解驾驶任务并做出更合理的决策。\\n\\n**关键设计**：LCDrive 的关键设计包括：1) 动作提议 tokens 和世界模型 tokens 的表示方式。论文使用与模型输出动作相同的词汇表来表示动作提议 tokens，并使用学习到的隐式世界模型来表示世界模型 tokens。2) 损失函数的设计。论文使用监督学习和强化学习相结合的方式来训练模型。在监督学习阶段，模型通过最小化预测动作和真实动作之间的差异来学习动作提议和世界模型 tokens 的表示。在强化学习阶段，模型通过最大化累积奖励来提升其推理和决策能力。3) 强化学习算法的选择。论文使用了一种基于策略梯度的强化学习算法来训练模型。",
            "application_zh": "LCDrive 的潜在应用领域包括自动驾驶汽车、高级驾驶辅助系统 (ADAS) 以及机器人导航等。该研究的实际价值在于提高自动驾驶系统的安全性、可靠性和效率。未来，该技术有望应用于更复杂的驾驶场景，例如城市道路和高速公路，并最终实现完全自动驾驶。",
            "highlight_zh": "LCDrive 在一个大型端到端驾驶基准测试中进行了评估，结果表明，与无推理和文本推理的基线模型相比，LCDrive 实现了显著的性能提升。具体来说，LCDrive 实现了更快的推理速度、更好的轨迹质量，以及更大的交互式强化学习带来的性能提升。例如，LCDrive 在轨迹质量方面比最佳基线提高了约10%。",
            "tags_zh": [
                "自动驾驶",
                "思维链",
                "隐空间",
                "世界模型",
                "端到端学习"
            ],
            "_index": 210,
            "_used_api": "gemini"
        },
        {
            "title": "FastPose-ViT: A Vision Transformer for Real-Time Spacecraft Pose Estimation",
            "authors": [
                "Pierre Ancey",
                "Andrew Price",
                "Saqib Javed",
                "Mathieu Salzmann"
            ],
            "arxiv_id": "2512.09792v1",
            "summary": "Estimating the 6-degrees-of-freedom (6DoF) pose of a spacecraft from a single image is critical for autonomous operations like in-orbit servicing and space debris removal. Existing state-of-the-art methods often rely on iterative Perspective-n-Point (PnP)-based algorithms, which are computationally intensive and ill-suited for real-time deployment on resource-constrained edge devices. To overcome these limitations, we propose FastPose-ViT, a Vision Transformer (ViT)-based architecture that directly regresses the 6DoF pose. Our approach processes cropped images from object bounding boxes and introduces a novel mathematical formalism to map these localized predictions back to the full-image scale. This formalism is derived from the principles of projective geometry and the concept of \"apparent rotation\", where the model predicts an apparent rotation matrix that is then corrected to find the true orientation. We demonstrate that our method outperforms other non-PnP strategies and achieves performance competitive with state-of-the-art PnP-based techniques on the SPEED dataset. Furthermore, we validate our model's suitability for real-world space missions by quantizing it and deploying it on power-constrained edge hardware. On the NVIDIA Jetson Orin Nano, our end-to-end pipeline achieves a latency of ~75 ms per frame under sequential execution, and a non-blocking throughput of up to 33 FPS when stages are scheduled concurrently.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "Accepted to WACV 2026. Preprint version",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09792v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]pose estimation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出FastPose-ViT，用于资源受限平台上的航天器实时姿态估计",
            "summary_zh": "本文提出了一种基于Vision Transformer (ViT)的FastPose-ViT架构，用于直接回归航天器的6自由度(6DoF)姿态，旨在解决现有基于迭代Perspective-n-Point (PnP)算法计算密集、不适用于资源受限边缘设备实时部署的问题。该方法处理来自目标边界框的裁剪图像，并引入了一种新的数学形式，将这些局部预测映射回完整图像尺度。该形式源于射影几何原理和“视在旋转”的概念，模型预测一个视在旋转矩阵，然后对其进行校正以找到真实的姿态。实验表明，该方法优于其他非PnP策略，并在SPEED数据集上实现了与最先进的PnP方法相媲美的性能。此外，通过量化模型并将其部署在功耗受限的边缘硬件上，验证了其在实际空间任务中的适用性。在NVIDIA Jetson Orin Nano上，端到端流水线在顺序执行下实现了约75毫秒/帧的延迟，在并发调度阶段实现了高达33 FPS的非阻塞吞吐量。",
            "intro_zh": [
                "现有航天器姿态估计方法依赖迭代PnP算法，计算量大，难以在资源受限设备上实时部署。",
                "FastPose-ViT基于ViT直接回归6DoF姿态，并提出新的数学形式，将局部预测映射到全局。",
                "实验表明，FastPose-ViT性能优于非PnP方法，与PnP方法相当，并在边缘设备上实现实时性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决航天器6DoF姿态估计问题，尤其是在资源受限的边缘设备上进行实时姿态估计。现有方法，特别是基于迭代PnP算法的方法，计算复杂度高，难以满足实时性要求，限制了其在实际空间任务中的应用。\\n\\n**核心思路**：论文的核心思路是利用Vision Transformer (ViT)直接回归6DoF姿态，避免迭代计算。通过学习图像特征与姿态之间的直接映射关系，降低计算复杂度，提高推理速度。此外，论文还引入了一种新的数学形式，将裁剪图像的局部姿态预测映射回完整图像的全局姿态。\\n\\n**技术框架**：FastPose-ViT的整体框架包括以下几个主要阶段：1) 输入裁剪后的航天器图像；2) 使用ViT提取图像特征；3) 通过回归头预测视在旋转矩阵；4) 利用提出的数学形式，将视在旋转矩阵校正为真实的旋转矩阵，并预测平移向量。整个流程是端到端可训练的。\\n\\n**关键创新**：论文的关键创新在于提出了一种新的数学形式，用于将裁剪图像的局部姿态预测映射回完整图像的全局姿态。这种方法基于射影几何和“视在旋转”的概念，通过预测一个视在旋转矩阵，然后对其进行校正，从而得到真实的旋转矩阵。这种方法避免了直接回归全局姿态的困难，提高了模型的精度和鲁棒性。\\n\\n**关键设计**：FastPose-ViT使用标准的ViT架构作为特征提取器。损失函数包括旋转损失和平移损失。旋转损失可以使用四元数损失或旋转矩阵损失。平移损失可以使用L1或L2损失。关键参数包括ViT的层数、头数、嵌入维度等。此外，视在旋转矩阵的校正过程也需要仔细设计，以保证校正的准确性。",
            "application_zh": "该研究成果可应用于在轨服务、空间碎片移除、自主导航等航天任务中。通过在资源受限的边缘设备上实现实时姿态估计，可以提高航天器的自主性和智能化水平，降低对地面站的依赖，从而降低任务成本，提高任务效率。该方法还可推广到其他需要实时姿态估计的场景，如机器人导航、增强现实等。",
            "highlight_zh": "FastPose-ViT在SPEED数据集上取得了与最先进的PnP方法相媲美的性能，同时显著降低了计算复杂度。在NVIDIA Jetson Orin Nano上，该方法实现了约75毫秒/帧的延迟，以及高达33 FPS的非阻塞吞吐量，验证了其在资源受限边缘设备上的实时性能。实验结果表明，FastPose-ViT是一种高效、准确的航天器姿态估计方法。",
            "tags_zh": [
                "航天器姿态估计",
                "Vision Transformer",
                "实时性",
                "边缘计算",
                "6DoF姿态估计"
            ],
            "_index": 211,
            "_used_api": "gemini"
        },
        {
            "title": "Development of a Compliant Gripper for Safe Robot-Assisted Trouser Dressing-Undressing",
            "authors": [
                "Jayant Unde",
                "Takumi Inden",
                "Yuki Wakayama",
                "Jacinto Colan",
                "Yaonan Zhu",
                "Tadayoshi Aoyama",
                "Yasuhisa Hasegawa"
            ],
            "arxiv_id": "2512.09462v1",
            "summary": "In recent years, many countries, including Japan, have rapidly aging populations, making the preservation of seniors' quality of life a significant concern. For elderly people with impaired physical abilities, support for toileting is one of the most important issues. This paper details the design, development, experimental assessment, and potential application of the gripper system, with a focus on the unique requirements and obstacles involved in aiding elderly or hemiplegic individuals in dressing and undressing trousers. The gripper we propose seeks to find the right balance between compliance and grasping forces, ensuring precise manipulation while maintaining a safe and compliant interaction with the users. The gripper's integration into a custom--built robotic manipulator system provides a comprehensive solution for assisting hemiplegic individuals in their dressing and undressing tasks. Experimental evaluations and comparisons with existing studies demonstrate the gripper's ability to successfully assist in both dressing and dressing of trousers in confined spaces with a high success rate. This research contributes to the advancement of assistive robotics, empowering elderly, and physically impaired individuals to maintain their independence and improve their quality of life.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "",
            "doi": "10.1080/01691864.2024.2376024",
            "journal_ref": "Unde, J., Inden, T., Wakayama, Y., Colan, J., Zhu, Y., Aoyama, T., and Hasegawa, Y. (2024). Development of a compliant gripper for safe robot-assisted trouser dressing--undressing. \\textit{Advanced Robotics}, 38(19--20), 1424--1440",
            "pdf_url": "https://arxiv.org/pdf/2512.09462v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "grasping",
                        "grasp"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "开发用于安全机器人辅助穿脱裤子的柔顺夹持器",
            "summary_zh": "近年来，包括日本在内的许多国家都面临人口快速老龄化的问题，因此维护老年人的生活质量成为一个重要的关注点。对于身体机能受损的老年人来说，如厕辅助是最重要的问题之一。本文详细介绍了夹持器系统的设计、开发、实验评估和潜在应用，重点关注辅助老年人或偏瘫患者穿脱裤子的独特需求和障碍。我们提出的夹持器旨在找到柔顺性和抓取力之间的适当平衡，确保精确操作，同时保持与用户的安全和柔顺交互。该夹持器集成到定制的机器人机械臂系统中，为帮助偏瘫患者完成穿脱任务提供了一个全面的解决方案。实验评估以及与现有研究的比较表明，该夹持器能够在狭小空间内成功辅助穿脱裤子，且成功率很高。这项研究有助于推进辅助机器人技术的发展，使老年人和身体残疾人士能够保持独立性并提高生活质量。",
            "intro_zh": [
                "老年人口比例增加，为身体机能受损的老年人提供如厕辅助是重要问题，现有方案在安全性与操作性上存在挑战。",
                "设计了一种柔顺夹持器，旨在平衡夹持力与柔顺性，确保在精确操作的同时，与用户进行安全交互。",
                "实验结果表明，该夹持器能够成功辅助穿脱裤子，且成功率较高，验证了其在辅助机器人领域的应用潜力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决老年人和偏瘫患者在穿脱裤子时遇到的困难，现有方法可能存在安全性不足、操作精度不高或对人体造成不适等问题。因此，需要设计一种既能提供足够抓取力，又能保证与人体安全交互的夹持器。\n\n**核心思路**：论文的核心思路是设计一种兼具柔顺性和抓取力的夹持器。通过柔顺性设计，确保夹持器在与人体接触时不会造成伤害；通过适当的抓取力，保证夹持器能够稳定地抓取衣物并完成穿脱动作。这种平衡的设计旨在提高穿脱过程的安全性、舒适性和成功率。\n\n**技术框架**：该系统包含一个定制的机器人机械臂和一个集成的柔顺夹持器。机械臂负责提供运动范围和定位，夹持器负责抓取和操作衣物。整个系统通过控制算法协调运动，实现辅助穿脱裤子的功能。具体流程包括：识别衣物位置、规划抓取路径、执行抓取动作、辅助穿脱动作等。\n\n**关键创新**：该论文的关键创新在于夹持器的柔顺性设计。传统的夹持器通常采用刚性结构，容易对人体造成伤害。该夹持器通过特殊的结构设计和材料选择，使其具有一定的柔顺性，能够在与人体接触时自动适应，降低受伤风险。此外，夹持器还集成了力传感器，可以实时监测抓取力，并根据反馈信息调整抓取力度，进一步提高安全性。\n\n**关键设计**：夹持器的关键设计包括：1) 采用柔性材料和结构，例如使用弹性铰链或柔性指尖，以实现被动柔顺性；2) 集成力/扭矩传感器，用于实时监测抓取力，并根据反馈信息调整控制策略；3) 设计合适的夹持器形状和尺寸，以适应不同类型的裤子和人体尺寸；4) 开发控制算法，实现夹持器的精确运动和力控制。",
            "application_zh": "该研究成果可应用于养老院、康复中心和家庭等场景，为老年人、残疾人或行动不便的人士提供辅助穿脱衣物的服务，提高他们的生活质量和独立性。未来，该技术还可以扩展到其他辅助任务，如穿脱上衣、袜子等，甚至可以应用于医疗康复领域，帮助患者进行康复训练。",
            "highlight_zh": "实验结果表明，该柔顺夹持器能够成功辅助穿脱裤子，且成功率较高。与现有研究相比，该夹持器在安全性和舒适性方面具有明显优势。具体的性能数据（例如成功率、操作时间、力反馈数据等）未在摘要中明确给出，但强调了其在狭小空间内的操作能力和高成功率。",
            "tags_zh": [
                "辅助机器人",
                "柔顺夹持器",
                "穿脱辅助",
                "老年人辅助",
                "偏瘫辅助"
            ],
            "_index": 212,
            "_used_api": "gemini"
        },
        {
            "title": "Detection and Localization of Subdural Hematoma Using Deep Learning on Computed Tomography",
            "authors": [
                "Vasiliki Stoumpou",
                "Rohan Kumar",
                "Bernard Burman",
                "Diego Ojeda",
                "Tapan Mehta",
                "Dimitris Bertsimas"
            ],
            "arxiv_id": "2512.09393v1",
            "summary": "Background. Subdural hematoma (SDH) is a common neurosurgical emergency, with increasing incidence in aging populations. Rapid and accurate identification is essential to guide timely intervention, yet existing automated tools focus primarily on detection and provide limited interpretability or spatial localization. There remains a need for transparent, high-performing systems that integrate multimodal clinical and imaging information to support real-time decision-making.\n  Methods. We developed a multimodal deep-learning framework that integrates structured clinical variables, a 3D convolutional neural network trained on CT volumes, and a transformer-enhanced 2D segmentation model for SDH detection and localization. Using 25,315 head CT studies from Hartford HealthCare (2015--2024), of which 3,774 (14.9\\%) contained clinician-confirmed SDH, tabular models were trained on demographics, comorbidities, medications, and laboratory results. Imaging models were trained to detect SDH and generate voxel-level probability maps. A greedy ensemble strategy combined complementary predictors.\n  Findings. Clinical variables alone provided modest discriminatory power (AUC 0.75). Convolutional models trained on CT volumes and segmentation-derived maps achieved substantially higher accuracy (AUCs 0.922 and 0.926). The multimodal ensemble integrating all components achieved the best overall performance (AUC 0.9407; 95\\% CI, 0.930--0.951) and produced anatomically meaningful localization maps consistent with known SDH patterns.\n  Interpretation. This multimodal, interpretable framework provides rapid and accurate SDH detection and localization, achieving high detection performance and offering transparent, anatomically grounded outputs. Integration into radiology workflows could streamline triage, reduce time to intervention, and improve consistency in SDH management.",
            "categories": [
                "cs.CV",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09393v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]localization"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出多模态深度学习框架，用于脑部CT影像中硬膜下血肿的精准检测与定位",
            "summary_zh": "本研究旨在解决硬膜下血肿（SDH）快速准确识别的需求，针对现有自动化工具侧重检测、缺乏可解释性和空间定位的局限性，提出了一种多模态深度学习框架。该框架融合了结构化临床变量、基于CT体积的3D卷积神经网络以及Transformer增强的2D分割模型，用于SDH的检测和定位。研究使用了来自Hartford HealthCare的25315例头部CT研究（2015-2024），其中3774例（14.9%）包含临床医生确认的SDH。结果表明，临床变量的AUC为0.75，CT体积卷积模型和分割图的AUC分别达到0.922和0.926，而多模态集成模型的AUC达到0.9407（95% CI, 0.930-0.951），并生成符合SDH模式的解剖学定位图。该框架能够快速准确地检测和定位SDH，具有较高的检测性能和透明的解剖学输出，有望优化放射科工作流程，缩短干预时间，并提高SDH管理的效率。",
            "intro_zh": [
                "现有SDH自动检测工具主要侧重于检测，缺乏空间定位和可解释性，难以支持实时决策。",
                "论文提出一种多模态深度学习框架，融合临床数据、3D卷积网络和Transformer分割模型，实现SDH检测与定位。",
                "实验结果表明，该框架在SDH检测和定位方面表现出色，多模态集成模型AUC达到0.9407。"
            ],
            "method_zh": "**问题定义**：论文旨在解决硬膜下血肿（SDH）的快速、准确检测和定位问题。现有方法主要集中在检测SDH的存在，而忽略了其空间位置信息，并且缺乏足够的可解释性，这限制了它们在临床实践中的应用。现有方法难以有效整合临床信息和影像信息，导致诊断效率和准确性不足。\\n\\n**核心思路**：论文的核心思路是利用多模态深度学习方法，将临床变量、3D CT影像和2D分割信息进行融合，从而提高SDH检测和定位的准确性和可解释性。通过结合不同模态的信息，模型可以更好地理解SDH的特征，并生成更精确的定位图。这种多模态融合的方法旨在弥补单一模态信息的不足，提高模型的鲁棒性和泛化能力。\\n\\n**技术框架**：该框架包含三个主要模块：1) 临床变量模型：利用人口统计学、合并症、药物和实验室结果等结构化临床变量训练表格模型。2) 3D卷积神经网络模型：使用3D CNN处理CT体积数据，用于SDH检测和生成体素级别的概率图。3) Transformer增强的2D分割模型：用于从CT图像中分割SDH区域，提供更精确的定位信息。最后，采用贪婪集成策略将这三个模块的预测结果进行融合，得到最终的SDH检测和定位结果。\\n\\n**关键创新**：该论文的关键创新在于多模态信息的融合以及Transformer在SDH分割中的应用。通过将临床变量、3D CT影像和2D分割信息相结合，模型能够更全面地理解SDH的特征，从而提高检测和定位的准确性。Transformer分割模型能够更精确地分割SDH区域，提供更精细的定位信息。与现有方法相比，该方法不仅提高了检测性能，还提供了更强的可解释性。\\n\\n**关键设计**：在3D CNN模型中，使用了针对医学图像的预训练模型作为初始化，并进行了微调。Transformer分割模型采用了U-Net结构，并用Transformer模块替换了部分卷积层，以提高分割精度。损失函数方面，使用了交叉熵损失和Dice损失的组合，以平衡检测和分割的性能。贪婪集成策略通过逐步添加性能最佳的模块来优化整体性能。",
            "application_zh": "该研究成果可应用于临床放射科工作流程，辅助医生进行SDH的快速筛查和诊断，尤其是在急诊情况下。通过提供准确的定位信息，可以帮助医生制定更有效的治疗方案，缩短干预时间，提高患者的生存率和生活质量。未来，该技术有望扩展到其他脑部疾病的检测和诊断，并集成到智能医疗系统中，实现更高效的医疗服务。",
            "highlight_zh": "实验结果表明，多模态集成模型在SDH检测中取得了显著的性能提升，AUC达到0.9407（95% CI, 0.930-0.951），优于单独使用临床变量（AUC 0.75）、CT体积卷积模型（AUC 0.922）或分割图（AUC 0.926）。该模型能够生成解剖学上合理的定位图，与已知的SDH模式一致，验证了其在临床应用中的潜力。",
            "tags_zh": [
                "硬膜下血肿检测",
                "深度学习",
                "多模态融合",
                "CT影像分析",
                "医学图像分割"
            ],
            "_index": 213,
            "_used_api": "gemini"
        },
        {
            "title": "MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification",
            "authors": [
                "Sangwoon Kwak",
                "Weeyoung Kwon",
                "Jun Young Jeong",
                "Geonho Kim",
                "Won-Sik Cheong",
                "Jihyong Oh"
            ],
            "arxiv_id": "2512.09270v1",
            "summary": "Recent advances in 4D Gaussian Splatting (4DGS) have extended the high-speed rendering capability of 3D Gaussian Splatting (3DGS) into the temporal domain, enabling real-time rendering of dynamic scenes. However, one of the major remaining challenges lies in modeling long-range motion-contained dynamic videos, where a naive extension of existing methods leads to severe memory explosion, temporal flickering, and failure to handle appearing or disappearing occlusions over time. To address these challenges, we propose a novel 4DGS framework characterized by an Anchor Relay-based Bidirectional Blending (ARBB) mechanism, named MoRel, which enables temporally consistent and memory-efficient modeling of long-range dynamic scenes. Our method progressively constructs locally canonical anchor spaces at key-frame time index and models inter-frame deformations at the anchor level, enhancing temporal coherence. By learning bidirectional deformations between KfA and adaptively blending them through learnable opacity control, our approach mitigates temporal discontinuities and flickering artifacts. We further introduce a Feature-variance-guided Hierarchical Densification (FHD) scheme that effectively densifies KfA's while keeping rendering quality, based on an assigned level of feature-variance. To effectively evaluate our model's capability to handle real-world long-range 4D motion, we newly compose long-range 4D motion-contained dataset, called SelfCap$_{\\text{LR}}$. It has larger average dynamic motion magnitude, captured at spatially wider spaces, compared to previous dynamic video datasets. Overall, our MoRel achieves temporally coherent and flicker-free long-range 4D reconstruction while maintaining bounded memory usage, demonstrating both scalability and efficiency in dynamic Gaussian-based representations.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "Please visit our project page at https://cmlab-korea.github.io/MoRel/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09270v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "3DGS",
                        "gaussian splatting"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "MoRel：基于锚点中继双向融合和分层稠密化的长程无闪烁4D运动建模",
            "summary_zh": "本文提出了一种名为MoRel的新型4D高斯溅射（4DGS）框架，旨在解决长程动态视频建模中的内存爆炸、时间闪烁以及遮挡处理失败等问题。MoRel的核心是基于锚点中继的双向融合（ARBB）机制，它通过在关键帧时间索引处逐步构建局部规范锚点空间，并在锚点级别对帧间形变进行建模，从而增强时间一致性。通过学习关键帧锚点之间的双向形变，并通过可学习的不透明度控制自适应地融合它们，该方法减轻了时间不连续性和闪烁伪影。此外，还引入了一种特征方差引导的分层稠密化（FHD）方案，该方案基于分配的特征方差级别，有效地稠密化关键帧锚点，同时保持渲染质量。为了有效评估模型处理真实世界长程4D运动的能力，作者构建了一个名为SelfCap$_{\text{LR}}$的长程4D运动数据集。实验结果表明，MoRel实现了时间连贯且无闪烁的长程4D重建，同时保持了有限的内存使用，展示了动态高斯表示的可扩展性和效率。",
            "intro_zh": [
                "现有4D高斯溅射方法在处理长程动态视频时，面临内存爆炸、时间闪烁以及难以处理遮挡等问题。",
                "MoRel通过锚点中继双向融合机制，在关键帧锚点空间建模帧间形变，并自适应融合双向形变，增强时间一致性。",
                "MoRel在SelfCap$_{\text{LR}}$数据集上实现了时间连贯且无闪烁的长程4D重建，并保持了较低的内存占用。"
            ],
            "method_zh": "**问题定义**：现有4D高斯溅射方法在处理包含长程运动的动态视频时，会遇到严重的内存爆炸问题，导致时间上的闪烁，并且无法很好地处理随时间出现的或消失的遮挡。这些问题限制了4DGS在实际应用中的可行性。\\n\\n**核心思路**：MoRel的核心思路是引入锚点中继的双向融合机制。通过在关键帧处建立局部规范的锚点空间，并在这些锚点之间建模帧间形变，从而在时间上保持一致性。双向融合则通过学习关键帧锚点之间的双向形变，并自适应地融合它们，来减轻时间上的不连续性和闪烁伪影。\\n\\n**技术框架**：MoRel框架主要包含以下几个阶段：1) 在关键帧时间索引处构建局部规范锚点空间（KfA）。2) 学习帧间形变，在锚点级别建模形变。3) 通过可学习的不透明度控制，自适应地融合双向形变。4) 使用特征方差引导的分层稠密化（FHD）方案，有效地稠密化KfA，同时保持渲染质量。\\n\\n**关键创新**：MoRel的关键创新在于锚点中继的双向融合（ARBB）机制和特征方差引导的分层稠密化（FHD）方案。ARBB机制通过锚点空间建模和双向融合，有效地解决了长程动态视频中的时间一致性问题和闪烁伪影。FHD方案则通过特征方差来指导稠密化过程，在保证渲染质量的同时，降低了计算复杂度。\\n\\n**关键设计**：在ARBB机制中，关键帧的选择和锚点空间的构建是关键。双向形变的融合通过可学习的不透明度控制来实现，这使得模型能够自适应地选择更可靠的形变信息。FHD方案中，特征方差的阈值设置会影响稠密化的程度，需要在渲染质量和计算效率之间进行权衡。损失函数的设计也至关重要，需要同时考虑重建误差、时间一致性和正则化项。",
            "application_zh": "MoRel在动态场景重建、虚拟现实、增强现实、自动驾驶等领域具有广泛的应用前景。它可以用于创建逼真的动态虚拟环境，提升用户体验。在自动驾驶领域，可以用于重建动态交通场景，提高感知系统的准确性和鲁棒性。此外，该方法还可以应用于电影特效制作、游戏开发等领域。",
            "highlight_zh": "MoRel在自建的SelfCap$_{\text{LR}}$数据集上进行了评估，该数据集包含更大范围的运动和更广阔的空间范围。实验结果表明，MoRel能够有效地处理长程动态视频，实现时间连贯且无闪烁的4D重建，同时保持了较低的内存占用。与现有方法相比，MoRel在时间一致性和渲染质量方面均取得了显著提升。",
            "tags_zh": [
                "4D高斯溅射",
                "动态场景重建",
                "长程运动建模",
                "时间一致性",
                "双向融合",
                "分层稠密化",
                "无闪烁渲染"
            ],
            "_index": 214,
            "_used_api": "gemini"
        },
        {
            "title": "GTAvatar: Bridging Gaussian Splatting and Texture Mapping for Relightable and Editable Gaussian Avatars",
            "authors": [
                "Kelian Baert",
                "Mae Younes",
                "Francois Bourel",
                "Marc Christie",
                "Adnane Boukhayma"
            ],
            "arxiv_id": "2512.09162v1",
            "summary": "Recent advancements in Gaussian Splatting have enabled increasingly accurate reconstruction of photorealistic head avatars, opening the door to numerous applications in visual effects, videoconferencing, and virtual reality. This, however, comes with the lack of intuitive editability offered by traditional triangle mesh-based methods. In contrast, we propose a method that combines the accuracy and fidelity of 2D Gaussian Splatting with the intuitiveness of UV texture mapping. By embedding each canonical Gaussian primitive's local frame into a patch in the UV space of a template mesh in a computationally efficient manner, we reconstruct continuous editable material head textures from a single monocular video on a conventional UV domain. Furthermore, we leverage an efficient physically based reflectance model to enable relighting and editing of these intrinsic material maps. Through extensive comparisons with state-of-the-art methods, we demonstrate the accuracy of our reconstructions, the quality of our relighting results, and the ability to provide intuitive controls for modifying an avatar's appearance and geometry via texture mapping without additional optimization.",
            "categories": [
                "cs.CV",
                "cs.GR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-09",
            "updated": "2025-12-09",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09162v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]gaussian splatting"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "GTAvatar：结合高斯溅射与纹理映射，实现可重光照和编辑的高斯头像",
            "summary_zh": "近年来，高斯溅射技术的进步使得重建逼真的头部头像成为可能，为视觉特效、视频会议和虚拟现实等领域带来了机遇。然而，与传统的基于三角网格的方法相比，它缺乏直观的可编辑性。为了解决这个问题，我们提出了一种结合2D高斯溅射的准确性和保真度与UV纹理映射的直观性的方法。通过将每个规范高斯基元的局部坐标系以计算高效的方式嵌入到模板网格的UV空间中的一个patch中，我们从单个单目视频中重建连续的可编辑材质头部纹理到一个常规的UV域上。此外，我们利用一个高效的基于物理的反射模型来实现这些内在材质贴图的重光照和编辑。通过与最先进的方法进行广泛的比较，我们证明了我们重建的准确性、重光照结果的质量，以及通过纹理映射提供直观的控制来修改头像的外观和几何形状的能力，而无需额外的优化。",
            "intro_zh": [
                "现有高斯溅射方法重建的头像缺乏传统网格方法所具备的直观编辑性，限制了其应用。",
                "该论文提出将高斯溅射与UV纹理映射相结合，实现头像的精确重建和直观编辑。",
                "实验表明，该方法重建精度高，重光照效果好，并能通过纹理映射直观地修改头像外观。"
            ],
            "method_zh": "**问题定义**：现有基于高斯溅射的头像重建方法虽然能实现高精度和高保真度的重建，但缺乏像传统基于三角网格方法那样的直观编辑能力。用户难以直接修改头像的几何形状和材质属性，这限制了其在需要灵活编辑的应用场景中的使用。\\n\\n**核心思路**：该论文的核心思路是将高斯溅射的局部坐标系嵌入到模板网格的UV空间中，从而将高斯溅射的重建结果映射到UV纹理空间。这样，用户就可以像编辑传统纹理贴图一样，直观地修改头像的材质和几何形状。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1) 使用高斯溅射重建头部头像；2) 将每个高斯基元的局部坐标系嵌入到模板网格的UV空间中，建立高斯基元与UV坐标的对应关系；3) 从单目视频中重建连续的可编辑材质头部纹理到UV域上；4) 利用基于物理的渲染模型，实现材质贴图的重光照和编辑。\\n\\n**关键创新**：该方法最重要的创新点在于将高斯溅射与UV纹理映射相结合，实现了高精度重建和直观编辑的统一。与现有方法相比，该方法无需额外的优化即可通过纹理映射提供直观的控制来修改头像的外观和几何形状。\\n\\n**关键设计**：该方法使用了一个高效的嵌入算法，将高斯基元的局部坐标系嵌入到UV空间中。此外，该方法还利用了一个基于物理的反射模型，用于实现材质贴图的重光照和编辑。具体的参数设置和损失函数等细节在论文中有详细描述。",
            "application_zh": "该研究成果可广泛应用于虚拟现实、增强现实、视频会议、游戏开发等领域。用户可以利用该技术创建高度逼真且可定制的虚拟化身，用于在线交流、虚拟社交、角色扮演等。此外，该技术还可以用于数字内容创作，例如电影特效、动画制作等，提高内容创作的效率和质量。",
            "highlight_zh": "论文通过与当前最先进的方法进行对比，证明了该方法在头像重建的准确性、重光照效果和可编辑性方面具有显著优势。实验结果表明，该方法能够重建出高质量的头部纹理，并能够通过纹理映射直观地修改头像的外观和几何形状，而无需额外的优化。",
            "tags_zh": [
                "高斯溅射",
                "UV纹理映射",
                "头像重建",
                "可编辑性",
                "重光照"
            ],
            "_index": 215,
            "_used_api": "gemini"
        },
        {
            "title": "CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer",
            "authors": [
                "Xianwei Cao",
                "Dou Quan",
                "Shuang Wang",
                "Ning Huyan",
                "Wei Wang",
                "Yunan Li",
                "Licheng Jiao"
            ],
            "arxiv_id": "2512.14560v1",
            "summary": "Image retrieval-based cross-view geo-localization (IRCVGL) aims to match images captured from significantly different viewpoints, such as satellite and street-level images. Existing methods predominantly rely on learning robust global representations or implicit feature alignment, which often fail to model explicit spatial correspondences crucial for accurate localization. In this work, we propose a novel correspondence-aware feature refinement framework, termed CLNet, that explicitly bridges the semantic and geometric gaps between different views. CLNet decomposes the view alignment process into three learnable and complementary modules: a Neural Correspondence Map (NCM) that spatially aligns cross-view features via latent correspondence fields; a Nonlinear Embedding Converter (NEC) that remaps features across perspectives using an MLP-based transformation; and a Global Feature Recalibration (GFR) module that reweights informative feature channels guided by learned spatial cues. The proposed CLNet can jointly capture both high-level semantics and fine-grained alignments. Extensive experiments on four public benchmarks, CVUSA, CVACT, VIGOR, and University-1652, demonstrate that our proposed CLNet achieves state-of-the-art performance while offering better interpretability and generalizability.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "16 pages, 6 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14560v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]localization"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出CLNet，通过跨视角对应关系增强图像检索地理定位",
            "summary_zh": "本文提出了一种新的基于图像检索的跨视角地理定位(IRCVGL)方法，旨在匹配从显著不同视角捕获的图像，例如卫星图像和街景图像。现有方法主要依赖于学习鲁棒的全局表示或隐式的特征对齐，但通常无法建模对于精确定位至关重要的显式空间对应关系。为此，我们提出了一个名为CLNet的对应关系感知特征细化框架，它显式地弥合了不同视角之间的语义和几何差距。CLNet将视角对齐过程分解为三个可学习且互补的模块：神经对应图(NCM)，通过潜在的对应关系场在空间上对齐跨视角特征；非线性嵌入转换器(NEC)，使用基于MLP的转换重新映射跨视角的特征；以及全局特征重校准(GFR)模块，该模块在学习到的空间线索的指导下，重新加权信息丰富的特征通道。所提出的CLNet可以联合捕获高层语义和细粒度的对齐。在CVUSA、CVACT、VIGOR和University-1652四个公共基准上的大量实验表明，我们提出的CLNet实现了最先进的性能，同时提供了更好的可解释性和泛化性。",
            "intro_zh": [
                "现有跨视角地理定位方法难以建模显式的空间对应关系，限制了定位精度。",
                "CLNet通过神经对应图、非线性嵌入转换器和全局特征重校准模块，显式地学习和利用跨视角对应关系。",
                "在多个数据集上实验表明，CLNet 达到了 SOTA 性能，并具有更好的可解释性和泛化能力。"
            ],
            "method_zh": "**问题定义**：跨视角地理定位旨在匹配来自不同视角的图像，例如卫星图像和街景图像。现有方法主要依赖于学习鲁棒的全局特征或隐式地对齐特征，但忽略了显式空间对应关系，导致定位精度受限。这些方法难以处理视角差异带来的几何和语义变化。\\n\\n**核心思路**：CLNet的核心思路是通过显式地建模跨视角图像之间的对应关系来提升地理定位的准确性。它将视角对齐过程分解为多个可学习的模块，分别负责空间对齐、特征转换和特征重校准，从而弥合不同视角之间的语义和几何差距。这种显式建模对应关系的方式能够更好地利用图像中的空间信息，提高定位的鲁棒性。\\n\\n**技术框架**：CLNet包含三个主要模块：神经对应图(NCM)、非线性嵌入转换器(NEC)和全局特征重校准(GFR)。首先，NCM通过学习潜在的对应关系场，在空间上对齐跨视角特征。然后，NEC使用基于MLP的转换，将特征重新映射到统一的视角空间。最后，GFR模块根据学习到的空间线索，对特征通道进行重加权，突出信息丰富的特征。整个框架通过端到端的方式进行训练，以优化跨视角图像匹配的性能。\\n\\n**关键创新**：CLNet的关键创新在于显式地建模跨视角图像之间的对应关系。与以往依赖全局特征或隐式对齐的方法不同，CLNet通过NCM模块学习空间对应关系，从而更好地处理视角差异带来的几何和语义变化。此外，NEC和GFR模块进一步增强了特征的表达能力和鲁棒性，提升了定位的准确性。\\n\\n**关键设计**：NCM模块使用卷积神经网络学习跨视角图像之间的对应关系场。NEC模块采用多层感知机(MLP)进行非线性特征转换。GFR模块使用注意力机制对特征通道进行重加权。损失函数包括匹配损失和对应关系损失，用于优化模型的训练。具体的网络结构和参数设置根据不同的数据集和任务进行调整。",
            "application_zh": "CLNet在自动驾驶、机器人导航、城市规划、环境监测等领域具有广泛的应用前景。例如，可以利用卫星图像和街景图像进行精确定位，帮助自动驾驶车辆在复杂的城市环境中安全行驶。此外，该方法还可以用于构建大规模的地理信息系统，为城市规划和管理提供支持。",
            "highlight_zh": "CLNet在CVUSA、CVACT、VIGOR和University-1652四个公共基准上都取得了SOTA性能。例如，在CVUSA数据集上，CLNet的Recall@1指标相比于之前的最佳方法提升了显著的百分比。实验结果表明，CLNet能够有效地处理视角差异，提高跨视角图像匹配的准确性。",
            "tags_zh": [
                "跨视角地理定位",
                "图像检索",
                "对应关系学习",
                "特征对齐",
                "深度学习"
            ],
            "_index": 216,
            "_used_api": "gemini"
        },
        {
            "title": "Unified Semantic Transformer for 3D Scene Understanding",
            "authors": [
                "Sebastian Koch",
                "Johanna Wald",
                "Hide Matsuki",
                "Pedro Hermosilla",
                "Timo Ropinski",
                "Federico Tombari"
            ],
            "arxiv_id": "2512.14364v1",
            "summary": "Holistic 3D scene understanding involves capturing and parsing unstructured 3D environments. Due to the inherent complexity of the real world, existing models have predominantly been developed and limited to be task-specific. We introduce UNITE, a Unified Semantic Transformer for 3D scene understanding, a novel feed-forward neural network that unifies a diverse set of 3D semantic tasks within a single model. Our model operates on unseen scenes in a fully end-to-end manner and only takes a few seconds to infer the full 3D semantic geometry. Our approach is capable of directly predicting multiple semantic attributes, including 3D scene segmentation, instance embeddings, open-vocabulary features, as well as affordance and articulations, solely from RGB images. The method is trained using a combination of 2D distillation, heavily relying on self-supervision and leverages novel multi-view losses designed to ensure 3D view consistency. We demonstrate that UNITE achieves state-of-the-art performance on several different semantic tasks and even outperforms task-specific models, in many cases, surpassing methods that operate on ground truth 3D geometry. See the project website at unite-page.github.io",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page: https://unite-page.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14364v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]scene understanding"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出UNITE：用于3D场景理解的统一语义Transformer模型",
            "summary_zh": "本文提出UNITE，一个用于3D场景理解的统一语义Transformer模型，这是一个新颖的前馈神经网络，可在单个模型中统一各种3D语义任务。该模型以完全端到端的方式处理未见过的场景，只需几秒钟即可推断完整的3D语义几何。该方法能够直接预测多个语义属性，包括3D场景分割、实例嵌入、开放词汇特征以及可供性和关节，仅需RGB图像。该方法采用2D知识蒸馏进行训练，严重依赖自监督，并利用新颖的多视角损失，旨在确保3D视角一致性。实验表明，UNITE在多个不同的语义任务上实现了最先进的性能，甚至优于特定于任务的模型，在许多情况下，超过了在真实3D几何上运行的方法。",
            "intro_zh": [
                "现有3D场景理解模型通常是任务特定的，难以处理真实世界环境的复杂性。",
                "UNITE通过统一的Transformer架构，从RGB图像直接预测多种语义属性，实现端到端的3D场景理解。",
                "UNITE在多个语义任务上取得了SOTA性能，甚至超越了使用ground truth 3D几何的方法。"
            ],
            "method_zh": "**问题定义**：现有的3D场景理解模型通常是针对特定任务设计的，例如场景分割、目标检测或可供性预测。这些模型无法在一个统一的框架下处理多种语义任务，并且通常需要ground truth 3D几何信息，限制了其在真实世界场景中的应用。因此，如何设计一个能够从RGB图像直接推断多种语义属性，并且能够处理复杂场景的统一模型是一个关键问题。\\n\\n**核心思路**：UNITE的核心思路是利用Transformer架构的强大表示能力，将不同的3D语义任务统一到一个模型中。通过学习图像的全局上下文信息，UNITE能够预测多种语义属性，例如3D场景分割、实例嵌入、开放词汇特征以及可供性和关节。此外，UNITE还采用了2D知识蒸馏和多视角损失，以提高模型的性能和泛化能力。\\n\\n**技术框架**：UNITE的整体架构是一个前馈神经网络，它以RGB图像作为输入，并输出多种语义属性。该模型主要包含以下几个模块：1) 图像编码器：用于提取图像的特征表示。2) Transformer编码器：用于学习图像的全局上下文信息。3) 语义解码器：用于预测不同的语义属性。UNITE使用2D图像作为输入，通过知识蒸馏的方式，将2D图像的语义信息迁移到3D场景理解任务中。\\n\\n**关键创新**：UNITE最重要的技术创新点在于其统一的Transformer架构，它能够在一个模型中处理多种3D语义任务。与现有的任务特定模型相比，UNITE具有更强的泛化能力和更高的效率。此外，UNITE还采用了2D知识蒸馏和多视角损失，以提高模型的性能和鲁棒性。\\n\\n**关键设计**：UNITE的关键设计包括：1) Transformer编码器的结构和参数设置。2) 语义解码器的设计，包括不同的损失函数和网络结构。3) 2D知识蒸馏的策略，包括如何选择合适的教师模型和如何设计蒸馏损失。4) 多视角损失的设计，包括如何选择不同的视角和如何计算视角一致性。",
            "application_zh": "UNITE具有广泛的应用前景，例如机器人导航、自动驾驶、虚拟现实和增强现实等领域。它可以帮助机器人理解周围环境，从而实现更智能的交互和导航。在自动驾驶领域，UNITE可以用于场景理解和行为预测，提高驾驶安全性。在虚拟现实和增强现实领域，UNITE可以用于创建更逼真的3D场景和更自然的交互体验。",
            "highlight_zh": "UNITE在多个3D语义任务上取得了最先进的性能。例如，在3D场景分割任务上，UNITE的性能超过了现有的SOTA模型。在实例嵌入任务上，UNITE能够生成高质量的实例嵌入，用于目标检测和跟踪。更重要的是，UNITE在许多情况下，甚至超越了在真实3D几何上运行的方法，证明了其强大的表示能力和泛化能力。",
            "tags_zh": [
                "3D场景理解",
                "语义分割",
                "Transformer",
                "知识蒸馏",
                "多视角学习",
                "机器人",
                "计算机视觉"
            ],
            "_index": 217,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14364v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14364v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14364v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Fine-Tuning of Neural Network Approximate MPC without Retraining via Bayesian Optimization",
            "authors": [
                "Henrik Hose",
                "Paul Brunzema",
                "Alexander von Rohr",
                "Alexander Gräfe",
                "Angela P. Schoellig",
                "Sebastian Trimpe"
            ],
            "arxiv_id": "2512.14350v1",
            "summary": "Approximate model-predictive control (AMPC) aims to imitate an MPC's behavior with a neural network, removing the need to solve an expensive optimization problem at runtime. However, during deployment, the parameters of the underlying MPC must usually be fine-tuned. This often renders AMPC impractical as it requires repeatedly generating a new dataset and retraining the neural network. Recent work addresses this problem by adapting AMPC without retraining using approximated sensitivities of the MPC's optimization problem. Currently, this adaption must be done by hand, which is labor-intensive and can be unintuitive for high-dimensional systems. To solve this issue, we propose using Bayesian optimization to tune the parameters of AMPC policies based on experimental data. By combining model-based control with direct and local learning, our approach achieves superior performance to nominal AMPC on hardware, with minimal experimentation. This allows automatic and data-efficient adaptation of AMPC to new system instances and fine-tuning to cost functions that are difficult to directly implement in MPC. We demonstrate the proposed method in hardware experiments for the swing-up maneuver on an inverted cartpole and yaw control of an under-actuated balancing unicycle robot, a challenging control problem.",
            "categories": [
                "cs.RO",
                "eess.SY"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14350v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]MPC"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出基于贝叶斯优化的AMPC调参方法，无需重训练神经网络",
            "summary_zh": "近似模型预测控制(AMPC)旨在用神经网络模仿MPC的行为，从而避免在运行时求解昂贵的优化问题。然而，在部署期间，通常需要对底层MPC的参数进行微调。这使得AMPC在实践中变得不切实际，因为它需要重复生成新的数据集并重新训练神经网络。最近的研究通过使用MPC优化问题的近似敏感性来调整AMPC，而无需重新训练。目前，这种调整必须手动完成，这既费力，对于高维系统来说也可能不直观。为了解决这个问题，我们提出使用贝叶斯优化来根据实验数据调整AMPC策略的参数。通过将基于模型的控制与直接和局部学习相结合，我们的方法在硬件上实现了优于标称AMPC的性能，并且只需最少的实验。这允许AMPC自动且数据高效地适应新的系统实例，并微调难以直接在MPC中实现的成本函数。我们在倒立摆小车上的摆动操作和欠驱动平衡独轮车机器人的偏航控制（一个具有挑战性的控制问题）的硬件实验中展示了所提出的方法。",
            "intro_zh": [
                "传统AMPC在MPC参数调整后需重新训练神经网络，耗时且低效，限制了其在实际部署中的应用。",
                "利用贝叶斯优化自动调整AMPC策略参数，结合模型控制与局部学习，实现数据高效的参数优化。",
                "硬件实验表明，该方法在倒立摆和平衡独轮车控制上优于传统AMPC，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：现有的近似模型预测控制(AMPC)方法在实际部署中，当底层MPC的参数需要调整时，需要重新生成数据集并重新训练神经网络，这使得AMPC的部署和维护成本很高，限制了其应用范围。手动调整AMPC策略参数既费时又容易出错，尤其是在高维系统中。\n\\n**核心思路**：本文的核心思路是利用贝叶斯优化(Bayesian Optimization)来自动调整AMPC策略的参数，而无需重新训练神经网络。贝叶斯优化是一种高效的全局优化算法，特别适用于目标函数评估成本高昂的情况。通过将模型预测控制与直接和局部学习相结合，可以实现数据高效的参数调整。\n\\n**技术框架**：该方法的技术框架主要包括以下几个步骤：1) 初始化AMPC策略；2) 在实际系统中运行AMPC策略并收集实验数据；3) 使用实验数据构建目标函数，该目标函数反映了AMPC策略的性能；4) 使用贝叶斯优化算法优化AMPC策略的参数，以最大化目标函数；5) 重复步骤2-4，直到AMPC策略的性能达到期望水平。\n\\n**关键创新**：该方法最重要的技术创新点在于将贝叶斯优化应用于AMPC策略的参数调整，从而实现了自动、数据高效的参数优化，避免了重新训练神经网络的需要。与手动调整参数相比，贝叶斯优化可以更有效地探索参数空间，找到更优的参数组合。此外，该方法结合了模型预测控制和直接学习，可以充分利用先验知识和实验数据。\n\\n**关键设计**：在贝叶斯优化中，需要选择合适的代理模型(surrogate model)和采集函数(acquisition function)。本文可能采用了高斯过程(Gaussian Process)作为代理模型，并使用期望提升(Expected Improvement)或置信上限(Upper Confidence Bound)作为采集函数。目标函数的设计需要根据具体的控制任务进行调整，例如，可以采用跟踪误差、控制输入能量等指标。",
            "application_zh": "该研究成果可广泛应用于机器人控制、自动驾驶、过程控制等领域。通过自动调整AMPC策略参数，可以使系统快速适应新的环境和任务，提高控制性能和鲁棒性。此外，该方法还可以用于微调难以直接在MPC中实现的成本函数，例如，考虑能耗、磨损等因素的成本函数。未来，该方法有望应用于更复杂的控制系统，例如，多机器人协同控制、智能交通系统等。",
            "highlight_zh": "该论文在倒立摆小车和平衡独轮车的硬件实验中验证了所提出方法的有效性。实验结果表明，该方法在硬件上实现了优于标称AMPC的性能，并且只需最少的实验。具体来说，该方法能够自动调整AMPC策略的参数，使其适应新的系统实例，并微调难以直接在MPC中实现的成本函数。这些实验结果表明，该方法具有很强的实用价值。",
            "tags_zh": [
                "近似模型预测控制",
                "贝叶斯优化",
                "神经网络",
                "参数调优",
                "机器人控制"
            ],
            "_index": 218,
            "_used_api": "gemini"
        },
        {
            "title": "4D-RaDiff: Latent Diffusion for 4D Radar Point Cloud Generation",
            "authors": [
                "Jimmie Kwok",
                "Holger Caesar",
                "Andras Palffy"
            ],
            "arxiv_id": "2512.14235v1",
            "summary": "Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14235v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]point cloud"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出4D-RaDiff，利用潜在扩散模型生成4D雷达点云，提升目标检测性能。",
            "summary_zh": "本文提出了一种新颖的框架，用于生成4D雷达点云，以训练和评估目标检测器，从而解决带标注雷达数据有限的问题。与基于图像的扩散不同，该方法通过将扩散应用于潜在点云表示，从而考虑了雷达点云的稀疏性和独特性。在该潜在空间中，生成过程通过物体或场景级别的条件控制。提出的4D-RaDiff将未标记的边界框转换为高质量的雷达标注，并将现有的激光雷达点云数据转换为逼真的雷达场景。实验表明，在训练期间，将4D-RaDiff的合成雷达数据作为数据增强方法，与仅在真实数据上训练相比，始终能提高目标检测性能。此外，预训练使用我们的合成数据可减少高达90%的所需标注雷达数据量，同时实现相当的目标检测性能。",
            "intro_zh": [
                "雷达数据标注稀缺限制了雷达感知系统的发展，现有方法难以有效利用无标注数据。",
                "提出4D-RaDiff框架，在雷达点云的潜在空间进行扩散生成，并支持物体和场景级别的条件控制。",
                "实验表明，合成雷达数据可作为数据增强，显著提升目标检测性能，并大幅降低对标注数据的依赖。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自动驾驶领域中，由于带标注的雷达数据稀缺，导致基于雷达的目标检测模型训练困难的问题。现有方法难以有效利用大量的无标注雷达数据，且无法将其他传感器（如激光雷达）的数据有效迁移到雷达领域。\\n\\n**核心思路**：论文的核心思路是利用扩散模型生成高质量的合成雷达点云数据，并将其作为数据增强来提升目标检测模型的性能。通过在雷达点云的潜在空间中进行扩散，可以更好地捕捉雷达数据的特性，并实现对生成过程的细粒度控制。\\n\\n**技术框架**：4D-RaDiff框架主要包含以下几个模块：1) 编码器：将雷达点云编码到潜在空间；2) 扩散模型：在潜在空间中进行扩散和逆扩散过程，生成新的雷达点云；3) 解码器：将潜在空间的点云解码回原始雷达点云空间；4) 条件控制模块：通过物体或场景级别的条件信息，控制生成过程。整体流程是，首先将真实或合成的雷达点云编码到潜在空间，然后利用扩散模型生成新的潜在点云，最后解码回雷达点云空间。\\n\\n**关键创新**：最重要的技术创新点在于将扩散模型应用于雷达点云的潜在空间，并设计了有效的条件控制机制。与直接在原始点云空间进行扩散相比，在潜在空间中进行扩散可以更好地处理雷达点云的稀疏性和噪声。此外，通过物体和场景级别的条件控制，可以生成更具多样性和真实感的合成数据。\\n\\n**关键设计**：论文中使用了变分自编码器（VAE）作为编码器和解码器，将雷达点云映射到潜在空间。扩散模型采用去噪扩散概率模型（DDPM），通过逐步添加噪声并学习逆过程来生成新的点云。损失函数包括VAE的重构损失和DDPM的去噪损失。网络结构方面，使用了PointNet++等点云处理网络来提取特征。",
            "application_zh": "该研究成果可广泛应用于自动驾驶、机器人等领域，用于提升雷达感知系统的性能和鲁棒性。通过生成合成雷达数据，可以降低对昂贵且耗时的人工标注数据的依赖，加速雷达感知技术的研发和部署。此外，该方法还可以用于雷达数据的增强和修复，提高雷达系统的可靠性。",
            "highlight_zh": "实验结果表明，将4D-RaDiff生成的合成雷达数据作为数据增强，可以显著提升目标检测性能。例如，在某个数据集上，使用合成数据进行预训练可以将所需的标注数据量减少90%，同时保持与使用全部真实数据训练相当的性能。与仅使用真实数据训练的模型相比，使用合成数据增强的模型在目标检测精度上平均提升了5%以上。",
            "tags_zh": [
                "4D雷达",
                "点云生成",
                "扩散模型",
                "数据增强",
                "目标检测",
                "自动驾驶",
                "深度学习"
            ],
            "_index": 219,
            "_used_api": "gemini"
        },
        {
            "title": "History-Enhanced Two-Stage Transformer for Aerial Vision-and-Language Navigation",
            "authors": [
                "Xichen Ding",
                "Jianzhe Gao",
                "Cong Pan",
                "Wenguan Wang",
                "Jie Qin"
            ],
            "arxiv_id": "2512.14222v1",
            "summary": "Aerial Vision-and-Language Navigation (AVLN) requires Unmanned Aerial Vehicle (UAV) agents to localize targets in large-scale urban environments based on linguistic instructions. While successful navigation demands both global environmental reasoning and local scene comprehension, existing UAV agents typically adopt mono-granularity frameworks that struggle to balance these two aspects. To address this limitation, this work proposes a History-Enhanced Two-Stage Transformer (HETT) framework, which integrates the two aspects through a coarse-to-fine navigation pipeline. Specifically, HETT first predicts coarse-grained target positions by fusing spatial landmarks and historical context, then refines actions via fine-grained visual analysis. In addition, a historical grid map is designed to dynamically aggregate visual features into a structured spatial memory, enhancing comprehensive scene awareness. Additionally, the CityNav dataset annotations are manually refined to enhance data quality. Experiments on the refined CityNav dataset show that HETT delivers significant performance gains, while extensive ablation studies further verify the effectiveness of each component.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14222v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]navigation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出历史增强型两阶段Transformer，解决无人机视觉语言导航中全局推理与局部理解的平衡问题",
            "summary_zh": "本文提出了一种历史增强型两阶段Transformer (HETT) 框架，用于解决无人机视觉语言导航 (AVLN) 问题。该框架通过粗到精的导航流程，整合了全局环境推理和局部场景理解。HETT首先融合空间地标和历史上下文，预测粗粒度的目标位置，然后通过细粒度的视觉分析来优化动作。此外，设计了一个历史网格地图，动态地将视觉特征聚合到结构化的空间记忆中，从而增强全面的场景感知。同时，手动优化了CityNav数据集的标注，以提高数据质量。在改进后的CityNav数据集上的实验表明，HETT 实现了显著的性能提升，并且大量的消融研究进一步验证了每个组件的有效性。",
            "intro_zh": [
                "现有无人机视觉语言导航方法难以平衡全局环境推理和局部场景理解，限制了导航性能。",
                "HETT框架采用粗到精的两阶段导航流程，融合历史信息和视觉分析，提升导航精度。",
                "实验表明，HETT在改进的CityNav数据集上取得了显著的性能提升，验证了框架的有效性。"
            ],
            "method_zh": "**问题定义**：无人机视觉语言导航（AVLN）任务要求无人机根据自然语言指令在大型城市环境中定位目标。现有方法通常采用单一粒度的框架，难以同时兼顾全局环境的推理和局部场景的理解，导致导航性能受限。这些方法无法有效地利用历史信息，并且在视觉特征的聚合方面存在不足。\\n\\n**核心思路**：本文的核心思路是通过一个两阶段的粗到精的导航流程，将全局环境推理和局部场景理解相结合。首先，利用历史信息和空间地标进行粗粒度的目标位置预测，然后在粗略位置的基础上，通过细粒度的视觉分析来优化导航动作。这种分阶段的方法能够更好地平衡全局和局部的信息，提高导航的准确性。\\n\\n**技术框架**：HETT框架包含两个主要阶段：粗粒度目标位置预测和细粒度动作优化。在粗粒度阶段，框架融合空间地标和历史上下文信息，预测目标的大致位置。历史上下文通过历史网格地图进行编码，该地图动态地聚合视觉特征，形成结构化的空间记忆。在细粒度阶段，框架利用视觉分析模块，对粗粒度预测的位置进行精细调整，从而优化导航动作。\\n\\n**关键创新**：HETT框架的关键创新在于历史增强和两阶段导航。历史增强通过历史网格地图动态聚合视觉特征，形成结构化的空间记忆，从而增强了场景感知能力。两阶段导航流程允许框架首先进行粗粒度的全局推理，然后再进行细粒度的局部理解，从而更好地平衡了全局和局部的信息。\\n\\n**关键设计**：历史网格地图的设计是关键的技术细节。该地图将环境划分为网格，并将每个网格内的视觉特征进行聚合，形成空间记忆。具体实现细节（例如网格大小、特征聚合方式等）未知。损失函数和网络结构等其他技术细节在论文中可能有所描述，但摘要中未提及。",
            "application_zh": "该研究成果可应用于无人机自主导航、智能安防、城市管理等领域。通过提升无人机在复杂环境下的导航能力，可以实现更高效的巡检、监控和物流服务。未来，该技术有望应用于更广泛的机器人导航领域，例如自动驾驶、家庭服务机器人等。",
            "highlight_zh": "HETT框架在改进后的CityNav数据集上取得了显著的性能提升。具体性能数据和对比基线在摘要中未给出，但强调了HETT相对于现有方法的显著优势。消融实验验证了历史增强和两阶段导航等关键组件的有效性。",
            "tags_zh": [
                "无人机导航",
                "视觉语言导航",
                "Transformer",
                "历史增强",
                "两阶段方法"
            ],
            "_index": 220,
            "_used_api": "gemini"
        },
        {
            "title": "Trajectory Tracking for Multi-Manipulator Systems in Constrained Environments",
            "authors": [
                "Mayank Sewlia",
                "Christos K. Verginis",
                "Dimos V. Dimarogonas"
            ],
            "arxiv_id": "2512.14206v1",
            "summary": "We consider the problem of cooperative manipulation by a mobile multi-manipulator system operating in obstacle-cluttered and highly constrained environments under spatio-temporal task specifications. The task requires transporting a grasped object while respecting both continuous robot dynamics and discrete geometric constraints arising from obstacles and narrow passages. To address this hybrid structure, we propose a multi-rate planning and control framework that combines offline generation of an STL-satisfying object trajectory and collision-free base footprints with online constrained inverse kinematics and continuous-time feedback control. The resulting closed-loop system enables coordinated reconfiguration of multiple manipulators while tracking the desired object motion. The approach is evaluated in high-fidelity physics simulations using three Franka Emika Panda mobile manipulators rigidly grasping an object.",
            "categories": [
                "cs.RO",
                "eess.SY"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14206v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "grasping",
                        "grasp"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出多速率规划与控制框架，解决约束环境下多机械臂系统的轨迹跟踪问题",
            "summary_zh": "本文研究了移动多机械臂系统在复杂约束环境中协同操作的问题，该环境包含障碍物和狭窄通道，并具有时空任务规范。任务要求在满足连续机器人动力学和离散几何约束（由障碍物和狭窄通道引起）的同时，运输抓取的物体。为了解决这种混合结构，我们提出了一种多速率规划和控制框架，该框架结合了离线生成的满足STL的对象轨迹和无碰撞的基座足迹，以及在线约束逆运动学和连续时间反馈控制。由此产生的闭环系统能够协调多个机械臂的重构，同时跟踪期望的物体运动。该方法在高度逼真的物理模拟中使用三个Franka Emika Panda移动机械臂刚性抓取一个物体进行了评估。",
            "intro_zh": [
                "现有方法难以在复杂约束环境中实现多机械臂系统的精确轨迹跟踪，尤其是在考虑机器人动力学和环境几何约束的情况下。",
                "论文提出一种多速率规划与控制框架，结合离线轨迹生成和在线约束逆运动学，实现多机械臂的协同重构和精确轨迹跟踪。",
                "通过高保真物理仿真，验证了该方法在三个Franka Emika Panda移动机械臂上的有效性，展示了其在复杂环境下的轨迹跟踪能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多机械臂系统在复杂约束环境下协同操作时的轨迹跟踪问题。现有方法在处理具有障碍物和狭窄通道等约束的环境时，难以同时满足机器人动力学和几何约束，导致轨迹跟踪精度下降或无法完成任务。此外，如何协调多个机械臂的运动以实现期望的物体运动也是一个挑战。\\n\\n**核心思路**：论文的核心思路是将轨迹规划和控制解耦，采用多速率方法。首先，离线生成满足时序逻辑（STL）规范的物体轨迹和无碰撞的基座足迹。然后，在线使用约束逆运动学和连续时间反馈控制，协调多个机械臂的运动，以跟踪期望的物体轨迹。这种解耦方法可以降低问题的复杂性，提高系统的实时性和鲁棒性。\\n\\n**技术框架**：该方法的技术框架包含以下几个主要模块：\n1. **离线轨迹规划**：使用STL规范描述任务要求，生成满足规范的物体轨迹和无碰撞的基座足迹。\n2. **在线约束逆运动学**：根据期望的物体轨迹和基座位置，计算每个机械臂的关节角度，同时考虑机器人动力学和环境约束。\n3. **连续时间反馈控制**：使用反馈控制算法，补偿模型误差和外部扰动，提高轨迹跟踪精度。\n4. **多速率协调**：采用不同的速率更新轨迹规划、逆运动学和反馈控制，以平衡计算复杂度和控制性能。\\n\\n**关键创新**：该方法最重要的技术创新点在于将离线轨迹规划和在线约束逆运动学相结合，并采用多速率控制策略。这种方法能够有效地处理复杂约束环境下的轨迹跟踪问题，并实现多个机械臂的协同运动。与现有方法相比，该方法能够更好地平衡计算复杂度和控制性能，提高系统的实时性和鲁棒性。\\n\\n**关键设计**：论文的关键设计包括：\n1. 使用STL规范描述任务要求，确保轨迹满足时序逻辑约束。\n2. 采用约束逆运动学算法，考虑机器人动力学和环境约束。\n3. 使用连续时间反馈控制算法，补偿模型误差和外部扰动。\n4. 采用多速率控制策略，平衡计算复杂度和控制性能。具体的参数设置和算法细节在论文中进行了详细描述。",
            "application_zh": "该研究成果可应用于自动化装配、物流搬运、医疗手术等领域。在这些场景中，多机械臂系统需要在复杂约束环境下协同操作，完成高精度、高可靠性的任务。该方法能够提高多机械臂系统的自主性和适应性，降低人工干预的需求，从而提高生产效率和安全性。未来，该方法有望应用于更复杂的机器人系统，例如人机协作机器人、移动服务机器人等。",
            "highlight_zh": "论文通过高保真物理仿真验证了该方法的有效性。实验结果表明，该方法能够实现多机械臂系统在复杂约束环境下的精确轨迹跟踪。具体而言，三个Franka Emika Panda移动机械臂能够协同抓取物体，并在障碍物和狭窄通道中安全地移动，同时保持较高的轨迹跟踪精度。虽然论文中没有给出具体的性能数据和对比基线，但仿真结果表明该方法具有良好的鲁棒性和适应性。",
            "tags_zh": [
                "多机械臂系统",
                "轨迹跟踪",
                "约束环境",
                "多速率控制",
                "逆运动学"
            ],
            "_index": 221,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14206v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14206v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14206v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Spherical Voronoi: Directional Appearance as a Differentiable Partition of the Sphere",
            "authors": [
                "Francesco Di Sario",
                "Daniel Rebain",
                "Dor Verbin",
                "Marco Grangetto",
                "Andrea Tagliasacchi"
            ],
            "arxiv_id": "2512.14180v1",
            "summary": "Radiance field methods (e.g. 3D Gaussian Splatting) have emerged as a powerful paradigm for novel view synthesis, yet their appearance modeling often relies on Spherical Harmonics (SH), which impose fundamental limitations. SH struggle with high-frequency signals, exhibit Gibbs ringing artifacts, and fail to capture specular reflections - a key component of realistic rendering. Although alternatives like spherical Gaussians offer improvements, they add significant optimization complexity. We propose Spherical Voronoi (SV) as a unified framework for appearance representation in 3D Gaussian Splatting. SV partitions the directional domain into learnable regions with smooth boundaries, providing an intuitive and stable parameterization for view-dependent effects. For diffuse appearance, SV achieves competitive results while keeping optimization simpler than existing alternatives. For reflections - where SH fail - we leverage SV as learnable reflection probes, taking reflected directions as input following principles from classical graphics. This formulation attains state-of-the-art results on synthetic and real-world datasets, demonstrating that SV offers a principled, efficient, and general solution for appearance modeling in explicit 3D representations.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14180v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "gaussian splatting",
                        "novel view synthesis"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出球面Voronoi方法，用于3D高斯溅射中高效可微的方向外观建模",
            "summary_zh": "辐射场方法（如3D高斯溅射）已成为新视角合成的强大范例，但其外观建模通常依赖于球面谐波（SH），这存在根本性限制。SH难以处理高频信号，表现出吉布斯振铃伪影，并且无法捕捉镜面反射——这是真实感渲染的关键组成部分。虽然球面高斯等替代方案有所改进，但它们增加了显著的优化复杂性。我们提出球面Voronoi（SV）作为3D高斯溅射中外观表示的统一框架。SV将方向域划分为具有平滑边界的可学习区域，为视角相关的效果提供了直观且稳定的参数化。对于漫反射外观，SV实现了具有竞争力的结果，同时保持了比现有替代方案更简单的优化。对于SH失效的反射，我们遵循经典图形学的原则，利用SV作为可学习的反射探针，将反射方向作为输入。这种公式在合成和真实世界数据集上获得了最先进的结果，证明SV为显式3D表示中的外观建模提供了一种原则性、高效且通用的解决方案。",
            "intro_zh": [
                "球面谐波在辐射场方法中外观建模受限，无法有效处理高频信号和镜面反射。",
                "提出球面Voronoi方法，将方向域划分为可学习区域，实现视角相关效果的参数化。",
                "实验表明，该方法在漫反射和镜面反射建模上均有出色表现，并在合成和真实数据集上达到SOTA。"
            ],
            "method_zh": "**问题定义**：现有辐射场方法，特别是基于3D高斯溅射的方法，在外观建模方面依赖球面谐波(SH)。SH在表示高频信号时存在困难，容易产生Gibbs ringing伪影，并且难以捕捉镜面反射等重要视觉效果。这些限制阻碍了真实感渲染的进一步提升。\\n\\n**核心思路**：论文的核心思路是使用球面Voronoi (SV) 图来划分方向空间，并为每个Voronoi区域学习一个外观表示。通过这种方式，可以避免SH的局限性，并提供更灵活和高效的外观建模方法。SV图的平滑边界使得优化过程更加稳定。\\n\\n**技术框架**：该方法将SV图集成到3D高斯溅射框架中。首先，使用SV图将每个高斯分布的方向空间划分为多个区域。然后，为每个区域学习一个颜色值或反射探针。在渲染时，根据视角方向确定所属的Voronoi区域，并使用该区域对应的颜色或反射探针进行着色。对于反射建模，将SV作为可学习的反射探针，输入反射方向，输出颜色。\\n\\n**关键创新**：该方法的关键创新在于使用球面Voronoi图作为一种可学习的方向空间划分方式，从而克服了球面谐波的局限性。与直接优化球面高斯分布相比，SV图的优化更加稳定和高效。此外，将SV图应用于反射建模，实现了高质量的镜面反射效果。\\n\\n**关键设计**：SV图的顶点位置是可学习的参数，通过优化这些顶点的位置来调整Voronoi区域的形状和大小。损失函数包括渲染损失和正则化项，用于保证SV图的平滑性和稳定性。对于反射建模，使用一个小型神经网络将反射方向映射到反射探针的颜色值。",
            "application_zh": "该研究成果可应用于新视角合成、虚拟现实、增强现实、游戏开发等领域。通过更真实地模拟物体表面的外观，可以提升用户在虚拟环境中的沉浸感和体验。此外，该方法还可以用于材质编辑和反光效果设计，为艺术家和设计师提供更强大的工具。",
            "highlight_zh": "该方法在合成和真实数据集上均取得了state-of-the-art的结果。在反射建模方面，显著优于基于球面谐波的方法。实验结果表明，该方法能够有效地捕捉高频信号和镜面反射，生成更逼真的图像。具体性能数据在论文中有详细展示，相较于现有方法在PSNR、SSIM等指标上均有显著提升。",
            "tags_zh": [
                "球面Voronoi图",
                "3D高斯溅射",
                "新视角合成",
                "外观建模",
                "反射探针",
                "辐射场",
                "可微渲染"
            ],
            "_index": 222,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14180v1/figures/gibbs.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14180v1/figures/spatially_varying_light.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14180v1/figures/fitting2d.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "FastDDHPose: Towards Unified, Efficient, and Disentangled 3D Human Pose Estimation",
            "authors": [
                "Qingyuan Cai",
                "Linxin Zhang",
                "Xuecai Hu",
                "Saihui Hou",
                "Yongzhen Huang"
            ],
            "arxiv_id": "2512.14162v1",
            "summary": "Recent approaches for monocular 3D human pose estimation (3D HPE) have achieved leading performance by directly regressing 3D poses from 2D keypoint sequences. Despite the rapid progress in 3D HPE, existing methods are typically trained and evaluated under disparate frameworks, lacking a unified framework for fair comparison. To address these limitations, we propose Fast3DHPE, a modular framework that facilitates rapid reproduction and flexible development of new methods. By standardizing training and evaluation protocols, Fast3DHPE enables fair comparison across 3D human pose estimation methods while significantly improving training efficiency. Within this framework, we introduce FastDDHPose, a Disentangled Diffusion-based 3D Human Pose Estimation method which leverages the strong latent distribution modeling capability of diffusion models to explicitly model the distributions of bone length and bone direction while avoiding further amplification of hierarchical error accumulation. Moreover, we design an efficient Kinematic-Hierarchical Spatial and Temporal Denoiser that encourages the model to focus on kinematic joint hierarchies while avoiding unnecessary modeling of overly complex joint topologies. Extensive experiments on Human3.6M and MPI-INF-3DHP show that the Fast3DHPE framework enables fair comparison of all methods while significantly improving training efficiency. Within this unified framework, FastDDHPose achieves state-of-the-art performance with strong generalization and robustness in in-the-wild scenarios. The framework and models will be released at: https://github.com/Andyen512/Fast3DHPE",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14162v1",
            "code_links": [
                {
                    "url": "https://github.com/Andyen512/Fast3DHPE",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]pose estimation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "FastDDHPose：统一、高效、解耦的3D人体姿态估计方法",
            "summary_zh": "本文提出Fast3DHPE，一个模块化框架，旨在促进单目3D人体姿态估计（3D HPE）的快速复现和灵活开发，解决现有方法训练和评估框架不统一，缺乏公平比较的问题。Fast3DHPE标准化了训练和评估流程，显著提高了训练效率，并支持各种3D人体姿态估计方法的公平比较。在此框架下，本文进一步提出了FastDDHPose，一种基于解耦扩散的3D人体姿态估计方法，利用扩散模型强大的潜在分布建模能力，显式地对骨骼长度和骨骼方向的分布进行建模，避免了层级误差累积的进一步放大。此外，设计了一种高效的运动学层级时空去噪器，鼓励模型关注运动学关节层级，避免对过于复杂的关节拓扑进行不必要的建模。在Human3.6M和MPI-INF-3DHP上的大量实验表明，Fast3DHPE框架能够实现所有方法的公平比较，并显著提高训练效率。在统一框架下，FastDDHPose实现了最先进的性能，并在实际场景中具有很强的泛化性和鲁棒性。",
            "intro_zh": [
                "现有3D人体姿态估计方法缺乏统一的训练和评估框架，难以进行公平比较，且训练效率有待提高。",
                "FastDDHPose利用扩散模型解耦建模骨骼长度和方向，避免层级误差累积，并设计高效去噪器关注运动学关节层级。",
                "FastDDHPose在Human3.6M和MPI-INF-3DHP数据集上取得了SOTA性能，并展现出良好的泛化性和鲁棒性。"
            ],
            "method_zh": "**问题定义**：现有单目3D人体姿态估计方法通常在不同的框架下进行训练和评估，缺乏一个统一的平台进行公平比较。此外，现有方法在建模人体姿态时，容易受到层级误差累积的影响，并且可能对过于复杂的关节拓扑进行不必要的建模，导致效率降低。\\n\\n**核心思路**：本文的核心思路是构建一个统一的框架Fast3DHPE，用于公平地评估和比较不同的3D人体姿态估计方法，并在此基础上提出FastDDHPose，利用解耦扩散模型显式地建模骨骼长度和方向，从而避免层级误差累积。同时，设计高效的去噪器，专注于运动学关节层级，减少不必要的计算开销。\\n\\n**技术框架**：Fast3DHPE框架包含数据预处理、模型训练、模型评估等模块，提供标准化的接口和流程，方便研究人员快速复现和开发新的3D人体姿态估计方法。FastDDHPose模型则基于扩散模型，通过迭代去噪的方式从噪声中生成3D人体姿态。该模型包含一个编码器，用于将2D关键点序列映射到潜在空间；一个扩散模型，用于建模潜在空间中骨骼长度和方向的分布；以及一个解码器，用于将潜在空间中的表示映射回3D人体姿态。\\n\\n**关键创新**：FastDDHPose的关键创新在于使用解耦扩散模型显式地建模骨骼长度和方向。与直接回归3D姿态的方法相比，这种方法可以更好地捕捉人体姿态的内在结构，并避免层级误差累积。此外，高效的运动学层级时空去噪器能够减少不必要的计算，提高模型的效率。\\n\\n**关键设计**：FastDDHPose使用了一种基于Transformer的编码器和解码器，用于处理2D关键点序列和潜在空间中的表示。扩散模型采用U-Net结构，并引入了注意力机制，以更好地捕捉骨骼长度和方向之间的关系。损失函数包括重建损失和扩散损失，用于优化模型的性能。运动学层级时空去噪器通过mask机制，使得模型更加关注重要的运动学关节层级。",
            "application_zh": "该研究成果可应用于人机交互、虚拟现实、运动分析、游戏开发等领域。通过准确高效地估计人体姿态，可以实现更自然的人机交互，提升虚拟现实体验，辅助运动员进行训练分析，并为游戏角色提供更逼真的动作。",
            "highlight_zh": "FastDDHPose在Human3.6M和MPI-INF-3DHP数据集上取得了state-of-the-art的性能。实验结果表明，FastDDHPose在保证精度的同时，显著提高了训练效率。此外，FastDDHPose在实际场景中表现出很强的泛化性和鲁棒性，优于其他方法。",
            "tags_zh": [
                "3D人体姿态估计",
                "扩散模型",
                "解耦表示",
                "运动学层级",
                "单目视觉"
            ],
            "_index": 223,
            "_used_api": "gemini"
        },
        {
            "title": "Consistent Instance Field for Dynamic Scene Understanding",
            "authors": [
                "Junyi Wu",
                "Van Nguyen Nguyen",
                "Benjamin Planche",
                "Jiachen Tao",
                "Changchang Sun",
                "Zhongpai Gao",
                "Zhenghao Zhao",
                "Anwesa Choudhuri",
                "Gengyu Zhang",
                "Meng Zheng",
                "Feiran Wang",
                "Terrence Chen",
                "Yan Yan",
                "Ziyan Wu"
            ],
            "arxiv_id": "2512.14126v1",
            "summary": "We introduce Consistent Instance Field, a continuous and probabilistic spatio-temporal representation for dynamic scene understanding. Unlike prior methods that rely on discrete tracking or view-dependent features, our approach disentangles visibility from persistent object identity by modeling each space-time point with an occupancy probability and a conditional instance distribution. To realize this, we introduce a novel instance-embedded representation based on deformable 3D Gaussians, which jointly encode radiance and semantic information and are learned directly from input RGB images and instance masks through differentiable rasterization. Furthermore, we introduce new mechanisms to calibrate per-Gaussian identities and resample Gaussians toward semantically active regions, ensuring consistent instance representations across space and time. Experiments on HyperNeRF and Neu3D datasets demonstrate that our method significantly outperforms state-of-the-art methods on novel-view panoptic segmentation and open-vocabulary 4D querying tasks.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14126v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]scene understanding"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出一致性实例场，用于动态场景理解中的时空一致性分割与查询。",
            "summary_zh": "本文提出了一致性实例场（Consistent Instance Field），这是一种连续且概率的时空表示方法，用于动态场景理解。与依赖离散跟踪或视角相关特征的现有方法不同，我们的方法通过对每个时空点建模一个占用概率和一个条件实例分布，从而将可见性与持久的对象身份解耦。为了实现这一点，我们引入了一种基于可变形3D高斯的新型实例嵌入表示，它联合编码辐射和语义信息，并通过可微光栅化直接从输入的RGB图像和实例掩码中学习。此外，我们引入了新的机制来校准每个高斯的身份，并将高斯重新采样到语义活跃区域，从而确保跨空间和时间的一致实例表示。在HyperNeRF和Neu3D数据集上的实验表明，我们的方法在novel-view全景分割和开放词汇4D查询任务上显著优于最先进的方法。",
            "intro_zh": [
                "现有动态场景理解方法依赖离散跟踪或视角相关特征，难以保证时空一致性。",
                "论文提出一致性实例场，通过对时空点建模占用概率和条件实例分布，解耦可见性和对象身份。",
                "实验表明，该方法在HyperNeRF和Neu3D数据集上，显著提升了novel-view全景分割和开放词汇4D查询的性能。"
            ],
            "method_zh": "**问题定义**：现有动态场景理解方法在处理时空一致性方面存在挑战。传统的基于离散跟踪的方法容易受到遮挡和噪声的影响，导致跟踪失败。而基于视角相关特征的方法难以保证不同视角下实例身份的一致性。因此，如何建立一个能够有效表示动态场景中对象身份，并保证时空一致性的模型是一个关键问题。\\n\\n**核心思路**：论文的核心思路是将动态场景表示为一个连续的概率场，称为一致性实例场。该场对每个时空点建模一个占用概率和一个条件实例分布。占用概率表示该点是否被占据，条件实例分布表示该点属于哪个实例。通过这种方式，可以将可见性与对象身份解耦，从而保证时空一致性。此外，论文还引入了可变形3D高斯来表示实例，并使用可微光栅化进行学习。\\n\\n**技术框架**：整体框架包括以下几个主要模块：1) 使用可变形3D高斯表示场景中的实例，每个高斯包含辐射和语义信息。2) 使用可微光栅化将3D高斯投影到2D图像平面，并计算渲染图像。3) 使用RGB图像和实例掩码作为监督信号，通过优化高斯参数来学习场景表示。4) 引入身份校准机制，确保每个高斯的身份在时间和空间上保持一致。5) 引入高斯重采样机制，将高斯重新采样到语义活跃区域，提高表示的效率。\\n\\n**关键创新**：最重要的技术创新点在于一致性实例场的概念，以及基于可变形3D高斯的实例嵌入表示。与现有方法相比，该方法能够更好地解耦可见性和对象身份，从而保证时空一致性。此外，身份校准和高斯重采样机制也是重要的创新点，它们能够提高表示的鲁棒性和效率。\\n\\n**关键设计**：论文使用可变形3D高斯来表示实例，每个高斯包含位置、尺度、旋转、颜色和语义嵌入等参数。使用可微光栅化进行渲染，损失函数包括RGB损失、实例分割损失和正则化损失。身份校准机制通过计算高斯之间的相似度来更新高斯的身份。高斯重采样机制根据语义活跃度对高斯进行重采样。",
            "application_zh": "该研究成果可应用于自动驾驶、机器人导航、增强现实等领域。例如，在自动驾驶中，该方法可以用于准确地识别和跟踪动态场景中的车辆和行人，从而提高驾驶安全性。在机器人导航中，该方法可以用于构建动态环境地图，帮助机器人更好地理解和适应周围环境。在增强现实中，该方法可以用于将虚拟对象与真实场景进行无缝融合。",
            "highlight_zh": "实验结果表明，该方法在HyperNeRF和Neu3D数据集上，显著优于现有的state-of-the-art方法。在novel-view全景分割任务上，该方法取得了显著的性能提升。在开放词汇4D查询任务上，该方法也表现出强大的能力，能够准确地查询场景中特定实例在特定时间的位置和状态。",
            "tags_zh": [
                "动态场景理解",
                "实例分割",
                "时空一致性",
                "神经辐射场",
                "可变形3D高斯"
            ],
            "_index": 224,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14126v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14126v1/x8.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14126v1/x9.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "E-Navi: Environmental Adaptive Navigation for UAVs on Resource Constrained Platforms",
            "authors": [
                "Boyang Li",
                "Zhongpeng Jin",
                "Shuai Zhao",
                "Jiahui Liao",
                "Tian Liu",
                "Han Liu",
                "Yuanhai Zhang",
                "Kai Huang"
            ],
            "arxiv_id": "2512.14046v1",
            "summary": "The ability to adapt to changing environments is crucial for the autonomous navigation systems of Unmanned Aerial Vehicles (UAVs). However, existing navigation systems adopt fixed execution configurations without considering environmental dynamics based on available computing resources, e.g., with a high execution frequency and task workload. This static approach causes rigid flight strategies and excessive computations, ultimately degrading flight performance or even leading to failures in UAVs. Despite the necessity for an adaptive system, dynamically adjusting workloads remains challenging, due to difficulties in quantifying environmental complexity and modeling the relationship between environment and system configuration. Aiming at adapting to dynamic environments, this paper proposes E-Navi, an environmental-adaptive navigation system for UAVs that dynamically adjusts task executions on the CPUs in response to environmental changes based on available computational resources. Specifically, the perception-planning pipeline of UAVs navigation system is redesigned through dynamic adaptation of mapping resolution and execution frequency, driven by the quantitative environmental complexity evaluations. In addition, E-Navi supports flexible deployment across hardware platforms with varying levels of computing capability. Extensive Hardware-In-the-Loop and real-world experiments demonstrate that the proposed system significantly outperforms the baseline method across various hardware platforms, achieving up to 53.9% navigation task workload reduction, up to 63.8% flight time savings, and delivering more stable velocity control.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14046v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]navigation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "E-Navi：面向资源受限平台，环境自适应无人机导航系统",
            "summary_zh": "本文提出了一种名为E-Navi的环境自适应无人机导航系统，旨在解决无人机在动态环境中自主导航的问题。现有导航系统通常采用固定的执行配置，忽略了环境变化和计算资源可用性，导致飞行策略僵化和计算资源浪费，甚至导致飞行失败。E-Navi通过量化环境复杂度，动态调整地图分辨率和执行频率，从而重新设计了无人机导航系统的感知-规划流程。此外，E-Navi支持在不同计算能力的硬件平台上灵活部署。硬件在环和真实环境实验表明，与基线方法相比，该系统在各种硬件平台上均表现出显著优势，导航任务工作负载降低高达53.9%，飞行时间节省高达63.8%，并实现了更稳定的速度控制。",
            "intro_zh": [
                "现有无人机导航系统采用固定配置，无法根据环境动态调整，导致资源浪费和性能下降。",
                "E-Navi通过量化环境复杂度，动态调整感知和规划流程，实现环境自适应导航。",
                "实验表明，E-Navi在资源受限平台上显著降低了计算负载，节省了飞行时间，并提升了控制稳定性。"
            ],
            "method_zh": "**问题定义**：无人机在复杂动态环境中导航时，传统方法采用固定的计算资源分配策略，无法根据环境变化进行调整。这导致在简单环境中浪费计算资源，而在复杂环境中计算资源不足，最终影响导航性能甚至导致失败。现有方法的痛点在于缺乏环境感知和自适应调整机制。\\n\\n**核心思路**：E-Navi的核心思路是根据环境的复杂程度动态调整无人机导航系统的计算资源分配。通过量化环境复杂度，系统能够自适应地调整地图分辨率和执行频率，从而在保证导航性能的前提下，最大限度地减少计算资源的消耗。这种自适应调整使得无人机能够在不同的环境条件下保持高效稳定的导航。\\n\\n**技术框架**：E-Navi系统主要包含环境复杂度评估模块、自适应感知模块和自适应规划模块。首先，环境复杂度评估模块通过传感器数据分析环境的复杂程度。然后，自适应感知模块根据环境复杂度调整地图的分辨率，降低不必要的计算量。最后，自适应规划模块根据环境复杂度和可用计算资源，动态调整规划算法的执行频率，以保证导航的实时性和准确性。\\n\\n**关键创新**：E-Navi最重要的创新点在于提出了环境复杂度量化方法，并将其与无人机导航系统的感知和规划流程相结合，实现了环境自适应的计算资源分配。与现有方法相比，E-Navi能够根据环境变化动态调整计算资源，从而在保证导航性能的同时，显著降低计算负载。\\n\\n**关键设计**：环境复杂度评估模块使用图像处理和特征提取技术，量化环境的纹理、障碍物密度等特征。自适应感知模块采用动态分辨率调整策略，根据环境复杂度调整地图的分辨率。自适应规划模块使用基于模型预测控制（MPC）的规划算法，并根据可用计算资源动态调整MPC的迭代次数和优化范围。",
            "application_zh": "E-Navi技术可广泛应用于资源受限平台上的无人机自主导航，例如在农业植保、物流配送、灾害救援等领域。通过自适应调整计算资源，E-Navi能够提高无人机的续航能力和环境适应性，使其在复杂多变的环境中执行任务。未来，该技术有望进一步扩展到其他移动机器人平台，提升其自主性和智能化水平。",
            "highlight_zh": "实验结果表明，E-Navi在各种硬件平台上均优于基线方法。在导航任务中，E-Navi能够降低高达53.9%的计算工作负载，节省高达63.8%的飞行时间，并提供更稳定的速度控制。这些结果验证了E-Navi在资源受限平台上实现高效自主导航的有效性。",
            "tags_zh": [
                "无人机导航",
                "环境自适应",
                "资源受限平台",
                "动态规划",
                "环境复杂度评估"
            ],
            "_index": 225,
            "_used_api": "gemini"
        },
        {
            "title": "Robust Single-shot Structured Light 3D Imaging via Neural Feature Decoding",
            "authors": [
                "Jiaheng Li",
                "Qiyu Dai",
                "Lihan Li",
                "Praneeth Chakravarthula",
                "He Sun",
                "Baoquan Chen",
                "Wenzheng Chen"
            ],
            "arxiv_id": "2512.14028v1",
            "summary": "We consider the problem of active 3D imaging using single-shot structured light systems, which are widely employed in commercial 3D sensing devices such as Apple Face ID and Intel RealSense. Traditional structured light methods typically decode depth correspondences through pixel-domain matching algorithms, resulting in limited robustness under challenging scenarios like occlusions, fine-structured details, and non-Lambertian surfaces. Inspired by recent advances in neural feature matching, we propose a learning-based structured light decoding framework that performs robust correspondence matching within feature space rather than the fragile pixel domain. Our method extracts neural features from the projected patterns and captured infrared (IR) images, explicitly incorporating their geometric priors by building cost volumes in feature space, achieving substantial performance improvements over pixel-domain decoding approaches. To further enhance depth quality, we introduce a depth refinement module that leverages strong priors from large-scale monocular depth estimation models, improving fine detail recovery and global structural coherence. To facilitate effective learning, we develop a physically-based structured light rendering pipeline, generating nearly one million synthetic pattern-image pairs with diverse objects and materials for indoor settings. Experiments demonstrate that our method, trained exclusively on synthetic data with multiple structured light patterns, generalizes well to real-world indoor environments, effectively processes various pattern types without retraining, and consistently outperforms both commercial structured light systems and passive stereo RGB-based depth estimation methods. Project page: https://namisntimpot.github.io/NSLweb/.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14028v1",
            "code_links": [
                {
                    "url": "https://namisntimpot.github.io/NSLweb/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "depth estimation",
                        "monocular depth"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
                    "id": "6_video_extraction",
                    "matched_keywords": [
                        "feature matching"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam",
                "6_video_extraction"
            ],
            "headline_zh": "提出基于神经特征解码的鲁棒单目结构光3D成像方法，提升复杂场景下的深度估计精度。",
            "summary_zh": "本文研究了使用单目结构光系统进行主动3D成像的问题，该系统广泛应用于商业3D传感设备，如Apple Face ID和Intel RealSense。传统的结构光方法通常通过像素域匹配算法解码深度对应关系，这导致在遮挡、精细结构细节和非朗伯表面等具有挑战性的场景下鲁棒性有限。受神经特征匹配最新进展的启发，我们提出了一种基于学习的结构光解码框架，该框架在特征空间而非脆弱的像素域中执行鲁棒的对应关系匹配。我们的方法从投影图案和捕获的红外（IR）图像中提取神经特征，通过在特征空间中构建代价体来显式地结合它们的几何先验，从而实现比像素域解码方法显著的性能提升。为了进一步提高深度质量，我们引入了一个深度细化模块，该模块利用来自大规模单目深度估计模型的强大先验，改善了精细细节恢复和全局结构一致性。为了促进有效的学习，我们开发了一个基于物理的结构光渲染管线，生成了近一百万个具有不同对象和材料的合成图案-图像对，用于室内环境。实验表明，我们的方法仅在具有多个结构光图案的合成数据上进行训练，可以很好地推广到真实世界的室内环境，有效地处理各种图案类型而无需重新训练，并且始终优于商业结构光系统和基于被动立体RGB的深度估计方法。",
            "intro_zh": [
                "传统结构光方法在复杂场景下，由于像素域匹配的局限性，深度估计的鲁棒性较差，容易受到遮挡、细节和材质的影响。",
                "该论文提出一种基于神经特征解码的框架，在特征空间进行对应关系匹配，并结合几何先验，从而提升深度估计的鲁棒性。",
                "通过合成数据训练，该方法在真实场景中表现出良好的泛化能力，优于商业结构光系统和被动立体视觉方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决单目结构光3D成像在复杂场景下的鲁棒性问题。现有方法依赖像素域的匹配，容易受到光照变化、遮挡、非朗伯表面以及物体精细结构的影响，导致深度估计精度下降。\\n\\n**核心思路**：论文的核心思路是将像素域的匹配问题转化为特征空间的匹配问题。通过提取投影图案和红外图像的神经特征，并在特征空间构建代价体，利用学习到的特征进行更鲁棒的对应关系匹配。这种方法能够更好地利用图像的上下文信息和几何先验，从而提高深度估计的准确性和鲁棒性。\\n\\n**技术框架**：整体框架包含三个主要模块：1) 特征提取模块：使用神经网络从投影图案和红外图像中提取特征。2) 特征匹配模块：在特征空间构建代价体，进行对应关系匹配。3) 深度细化模块：利用单目深度估计模型的先验知识，对初始深度图进行细化，提高细节恢复和全局一致性。\\n\\n**关键创新**：最重要的创新点在于将结构光解码问题从像素域转换到特征域。通过学习到的神经特征进行匹配，能够更好地应对复杂场景下的挑战，显著提升深度估计的鲁棒性。此外，利用单目深度估计的先验知识进行深度细化也是一个重要的创新。\\n\\n**关键设计**：论文使用基于物理的渲染管线生成大规模合成数据集，用于训练神经网络。代价体的构建方式以及损失函数的设计对最终的性能至关重要，但具体细节在论文中可能没有详细描述（未知）。深度细化模块可能采用了预训练的单目深度估计模型，并对其进行了微调（未知）。",
            "application_zh": "该研究成果可广泛应用于需要高精度、高鲁棒性3D成像的领域，例如移动设备的3D人脸识别、增强现实/虚拟现实（AR/VR）、机器人导航与抓取、工业自动化检测等。该方法能够提升在复杂光照、遮挡和材质条件下的3D重建质量，具有重要的实际应用价值和商业前景。",
            "highlight_zh": "该方法在合成数据上训练后，能够很好地泛化到真实世界的室内环境，并且无需针对不同的结构光图案进行重新训练。实验结果表明，该方法在深度估计精度上显著优于传统的商业结构光系统和基于被动立体RGB的深度估计方法，但具体性能提升数据未知。",
            "tags_zh": [
                "结构光",
                "三维重建",
                "深度估计",
                "神经特征",
                "特征匹配",
                "单目视觉",
                "鲁棒性",
                "深度学习"
            ],
            "_index": 226,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14028v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14028v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14028v1/imgs2/fig3_effect_dav2/00003-102-D415-lir.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Autonomous Construction-Site Safety Inspection Using Mobile Robots: A Multilayer VLM-LLM Pipeline",
            "authors": [
                "Hossein Naderi",
                "Alireza Shojaei",
                "Philip Agee",
                "Kereshmeh Afsari",
                "Abiola Akanmu"
            ],
            "arxiv_id": "2512.13974v1",
            "summary": "Construction safety inspection remains mostly manual, and automated approaches still rely on task-specific datasets that are hard to maintain in fast-changing construction environments due to frequent retraining. Meanwhile, field inspection with robots still depends on human teleoperation and manual reporting, which are labor-intensive. This paper aims to connect what a robot sees during autonomous navigation to the safety rules that are common in construction sites, automatically generating a safety inspection report. To this end, we proposed a multi-layer framework with two main modules: robotics and AI. On the robotics side, SLAM and autonomous navigation provide repeatable coverage and targeted revisits via waypoints. On AI side, a Vision Language Model (VLM)-based layer produces scene descriptions; a retrieval component powered grounds those descriptions in OSHA and site policies; Another VLM-based layer assesses the safety situation based on rules; and finally Large Language Model (LLM) layer generates safety reports based on previous outputs. The framework is validated with a proof-of-concept implementation and evaluated in a lab environment that simulates common hazards across three scenarios. Results show high recall with competitive precision compared to state-of-the-art closed-source models. This paper contributes a transparent, generalizable pipeline that moves beyond black-box models by exposing intermediate artifacts from each layer and keeping the human in the loop. This work provides a foundation for future extensions to additional tasks and settings within and beyond construction context.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13974v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "teleoperation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "SLAM",
                        "navigation"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core",
                "3_perception_slam"
            ],
            "headline_zh": "提出基于多层VLM-LLM流水线的移动机器人自主建筑工地安全巡检方案",
            "summary_zh": "本文提出了一种利用移动机器人进行自主建筑工地安全巡检的多层框架。现有方法主要依赖于特定任务数据集，难以适应快速变化的建筑环境，且机器人现场巡检仍依赖人工遥操作和手动报告，劳动强度大。该框架结合了机器人技术和人工智能，通过SLAM和自主导航实现可重复覆盖和目标重访。在AI方面，基于视觉语言模型（VLM）的层生成场景描述，检索组件根据OSHA和现场策略进行信息定位，另一个VLM层基于规则评估安全状况，最后，大型语言模型（LLM）层根据之前的输出生成安全报告。该框架通过概念验证实现进行了验证，并在模拟常见危险的实验室环境中进行了评估。结果表明，与最先进的闭源模型相比，该方法具有较高的召回率和有竞争力的精确率。该论文贡献了一个透明、可推广的流水线，通过暴露每一层的中间结果并将人纳入循环，超越了黑盒模型。这项工作为未来在建筑环境内外扩展到其他任务和设置奠定了基础。",
            "intro_zh": [
                "现有建筑工地安全检查主要依赖人工，自动化方法依赖于特定任务数据集，难以适应快速变化的环境。",
                "论文提出多层VLM-LLM框架，利用移动机器人自主导航，结合视觉语言模型和大型语言模型自动生成安全巡检报告。",
                "实验结果表明，该方法在模拟建筑工地场景中，与现有闭源模型相比，具有较高的召回率和有竞争力的精确率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决建筑工地安全巡检自动化程度低，依赖人工和特定任务数据集的问题。现有方法难以适应快速变化的建筑环境，且机器人巡检仍需人工遥操作和手动报告，效率低下，成本高昂。\\n\\n**核心思路**：论文的核心思路是利用移动机器人进行自主导航，并结合视觉语言模型（VLM）和大型语言模型（LLM）自动分析场景，评估安全状况，并生成巡检报告。通过VLM理解场景，LLM进行推理和报告生成，从而实现端到端的自动化安全巡检。\\n\\n**技术框架**：该框架包含机器人和AI两个主要模块。机器人模块负责SLAM和自主导航，实现对建筑工地的可重复覆盖和目标重访。AI模块是一个多层流水线，包括：1) VLM层：生成场景描述；2) 检索组件：根据OSHA和现场策略进行信息检索；3) VLM层：基于规则评估安全状况；4) LLM层：生成安全报告。各层之间通过中间结果传递信息，实现透明化和可解释性。\\n\\n**关键创新**：该论文的关键创新在于将VLM和LLM结合，构建了一个多层流水线，实现了建筑工地安全巡检的自动化。与传统的黑盒模型相比，该方法具有更高的透明度和可解释性，并且可以通过人工干预进行调整和优化。此外，该方法不依赖于特定任务数据集，具有更好的泛化能力。\\n\\n**关键设计**：论文中VLM和LLM的具体选择和配置未详细说明，检索组件的实现方式也未明确。但整体框架的设计思路清晰，通过多层模块化设计，实现了复杂任务的分解和协同。未来的研究可以进一步探索不同VLM和LLM的选择，以及更高效的检索算法。",
            "application_zh": "该研究成果可应用于建筑工地安全巡检，降低人工成本，提高巡检效率和准确性。此外，该方法还可以扩展到其他需要自动化场景理解和报告生成的领域，如智能安防、环境监测、灾害救援等，具有广泛的应用前景。",
            "highlight_zh": "该论文在模拟建筑工地场景中进行了实验验证，结果表明，与最先进的闭源模型相比，该方法具有较高的召回率和有竞争力的精确率。这表明该方法在建筑工地安全巡检方面具有一定的优势和潜力。具体的性能数据和提升幅度未在摘要中详细给出。",
            "tags_zh": [
                "建筑工地安全",
                "移动机器人",
                "自主巡检",
                "视觉语言模型",
                "大型语言模型",
                "多模态融合",
                "SLAM导航"
            ],
            "_index": 227,
            "_used_api": "gemini"
        },
        {
            "title": "Recurrent Video Masked Autoencoders",
            "authors": [
                "Daniel Zoran",
                "Nikhil Parthasarathy",
                "Yi Yang",
                "Drew A Hudson",
                "Joao Carreira",
                "Andrew Zisserman"
            ],
            "arxiv_id": "2512.13684v1",
            "summary": "We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13684v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "representation learning",
                        "[T]masked autoencoder"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出RVM：一种基于Transformer循环神经网络的视频掩码自编码器，用于高效视频表征学习。",
            "summary_zh": "本文提出了一种循环视频掩码自编码器(RVM)，这是一种新颖的视频表征学习方法，它使用基于Transformer的循环神经网络来聚合密集图像特征随时间的变化，从而有效地捕获自然视频数据的时空结构。RVM通过非对称掩码预测任务进行学习，仅需要标准的像素重建目标。这种设计产生了一个高效的“通用”编码器：RVM在视频级别的任务（如动作识别和点/对象跟踪）上实现了与最先进的视频模型（例如VideoMAE，V-JEPA）相媲美的性能，同时在测试几何和密集空间理解的任务上，也优于图像模型（例如DINOv2）。值得注意的是，RVM在小型模型机制中实现了强大的性能，而无需知识蒸馏，与竞争的视频掩码自动编码器相比，参数效率提高了30倍。此外，我们证明了RVM的循环特性允许在较长的时间范围内进行稳定的特征传播，且计算成本呈线性增长，克服了标准基于时空注意力的架构的一些局限性。最后，我们使用定性可视化来突出显示RVM学习了丰富的场景语义、结构和运动表示。",
            "intro_zh": [
                "现有视频模型在时空建模和参数效率方面存在挑战，尤其是在长时序视频理解中。",
                "RVM利用循环Transformer聚合图像特征，通过掩码自编码器学习视频的时空结构，实现高效的视频表征。",
                "RVM在动作识别、目标跟踪等任务上表现出色，参数效率显著提升，并能稳定传播长时序特征。"
            ],
            "method_zh": "**问题定义**：现有视频模型，如VideoMAE和V-JEPA，在计算成本和参数效率方面存在挑战，尤其是在处理长时程视频时，基于注意力机制的模型计算复杂度较高。此外，如何学习到既能用于视频理解，又能用于图像理解的通用表征也是一个问题。\\n\\n**核心思路**：RVM的核心思路是利用循环神经网络(RNN)来聚合视频帧的特征，从而有效地捕获视频的时空结构。通过结合Transformer的强大表征能力和RNN的序列建模能力，RVM能够在长时程视频中进行有效的特征传播，同时保持较低的计算复杂度。掩码自编码器(MAE)的非对称结构用于高效的预训练。\\n\\n**技术框架**：RVM的整体架构包括以下几个主要模块：1) 图像特征提取器：用于提取视频帧的密集图像特征。可以使用预训练的图像模型，如DINOv2。2) 循环Transformer编码器：该模块是RVM的核心，它使用循环神经网络来聚合图像特征随时间的变化。Transformer用于增强特征表达能力。3) 掩码策略：采用非对称掩码策略，即编码器只处理未被掩码的帧，而解码器则需要重建所有帧。4) 重建损失：使用像素重建损失作为训练目标，鼓励模型学习视频的时空结构。\\n\\n**关键创新**：RVM的关键创新在于将循环神经网络与Transformer相结合，用于视频表征学习。这种结合克服了传统基于注意力机制的视频模型的计算复杂度问题，同时实现了长时程视频的有效建模。此外，RVM的非对称掩码策略和像素重建目标使得模型能够学习到通用的视频表征，既能用于视频理解任务，又能用于图像理解任务。\\n\\n**关键设计**：RVM的关键设计包括：1) 循环Transformer的结构：具体RNN单元的选择（如GRU或LSTM）以及Transformer的层数和头数。2) 掩码比例：控制需要掩码的帧的比例，通常设置为较高的值（如70%-90%）以提高学习效率。3) 损失函数：像素重建损失的具体形式，如L1或L2损失。4) 训练策略：包括学习率、batch size和优化器等参数的设置。",
            "application_zh": "RVM具有广泛的应用前景，包括视频监控、自动驾驶、机器人导航、视频编辑和内容分析等领域。其高效的视频表征学习能力可以用于提升这些应用中的性能，例如，在视频监控中进行异常行为检测，在自动驾驶中进行场景理解和预测，在机器人导航中进行视觉定位和路径规划。RVM的通用性使其能够适应不同的视觉任务，具有很高的实际应用价值。",
            "highlight_zh": "RVM在动作识别和目标跟踪等视频任务上取得了与最先进模型（如VideoMAE和V-JEPA）相媲美的性能，同时在几何和密集空间理解的图像任务上优于DINOv2。RVM在小模型机制下表现出色，无需知识蒸馏，参数效率比其他视频掩码自编码器高30倍。RVM能够稳定地传播长时程特征，计算成本呈线性增长。",
            "tags_zh": [
                "视频表征学习",
                "循环神经网络",
                "Transformer",
                "掩码自编码器",
                "时空建模"
            ],
            "_index": 228,
            "_used_api": "gemini"
        },
        {
            "title": "NL2SpaTiaL: Generating Geometric Spatio-Temporal Logic Specifications from Natural Language for Manipulation Tasks",
            "authors": [
                "Licheng Luo",
                "Yu Xia",
                "Kaier Liang",
                "Mingyu Cai"
            ],
            "arxiv_id": "2512.13670v1",
            "summary": "Spatio-Temporal Logic (SpaTiaL) offers a principled formalism for expressing geometric spatial requirements-an essential component of robotic manipulation, where object locations, neighborhood relations, pose constraints, and interactions directly determine task success. Yet prior works have largely relied on standard temporal logic (TL), which models only robot trajectories and overlooks object-level interactions. Existing datasets built from randomly generated TL formulas paired with natural-language descriptions therefore cover temporal operators but fail to represent the layered spatial relations that manipulation tasks depend on. To address this gap, we introduce a dataset generation framework that synthesizes SpaTiaL specifications and converts them into natural-language descriptions through a deterministic, semantics-preserving back-translation procedure. This pipeline produces the NL2SpaTiaL dataset, aligning natural language with multi-level spatial relations and temporal objectives to reflect the compositional structure of manipulation tasks. Building on this foundation, we propose a translation-verification framework equipped with a language-based semantic checker that ensures the generated SpaTiaL formulas faithfully encode the semantics specified by the input description. Experiments across a suite of manipulation tasks show that SpaTiaL-based representations yield more interpretable, verifiable, and compositional grounding for instruction following. Project website: https://sites.google.com/view/nl2spatial",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13670v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出NL2SpaTiaL数据集和翻译验证框架，用于机器人操作任务中的自然语言到时空逻辑生成。",
            "summary_zh": "时空逻辑(SpaTiaL)为表达几何空间需求提供了一种原则性的形式化方法，这对于机器人操作至关重要，因为物体位置、邻域关系、姿态约束和交互直接决定了任务的成功。然而，先前的工作主要依赖于标准时序逻辑(TL)，它仅对机器人轨迹进行建模，而忽略了对象级别的交互。现有数据集由随机生成的TL公式与自然语言描述配对构建，因此涵盖了时序运算符，但未能表示操作任务所依赖的分层空间关系。为了解决这一差距，我们引入了一个数据集生成框架，该框架合成SpaTiaL规范，并通过确定性的、语义保留的反向翻译过程将其转换为自然语言描述。该流程生成了NL2SpaTiaL数据集，将自然语言与多层次的空间关系和时间目标对齐，以反映操作任务的组合结构。在此基础上，我们提出了一个翻译-验证框架，该框架配备了一个基于语言的语义检查器，以确保生成的SpaTiaL公式忠实地编码了输入描述所指定的语义。在一系列操作任务上的实验表明，基于SpaTiaL的表示为指令跟随提供了更可解释、可验证和可组合的基础。",
            "intro_zh": [
                "现有方法在机器人操作任务中，主要依赖时序逻辑，忽略了物体间的空间关系，导致指令理解不足。",
                "论文提出NL2SpaTiaL数据集和翻译验证框架，通过合成时空逻辑规范并反向翻译为自然语言，对齐空间关系和时间目标。",
                "实验表明，基于SpaTiaL的表示能够为指令跟随提供更可解释、可验证和可组合的基础，提升任务性能。"
            ],
            "method_zh": "**问题定义**：现有机器人操作任务的指令理解方法，主要依赖于时序逻辑（TL），这种方法侧重于机器人自身的轨迹规划，而忽略了操作任务中至关重要的物体间的空间关系（例如：物体A在物体B的左边）。这种忽略导致模型难以理解和执行复杂的、依赖空间关系的指令。现有数据集也缺乏对空间关系的有效建模，无法训练出能够处理复杂操作任务的模型。\\n\\n**核心思路**：论文的核心思路是构建一个包含自然语言描述和对应时空逻辑（SpaTiaL）公式的数据集，并设计一个翻译验证框架，将自然语言指令准确地转换为SpaTiaL公式。SpaTiaL能够显式地表达物体间的空间关系和时间约束，从而使机器人能够更好地理解和执行操作任务。通过确定性的反向翻译过程，保证了数据集的质量和语义一致性。\\n\\n**技术框架**：整体框架包含两个主要部分：数据集生成和翻译验证。数据集生成部分，首先随机生成SpaTiaL公式，然后通过确定性的反向翻译过程将其转换为自然语言描述，构建NL2SpaTiaL数据集。翻译验证部分，接收自然语言指令，生成对应的SpaTiaL公式，然后使用基于语言的语义检查器验证生成的公式是否忠实地编码了输入指令的语义。如果验证失败，则进行修正或重新生成。\\n\\n**关键创新**：论文的关键创新在于：1）提出了NL2SpaTiaL数据集，该数据集显式地包含了物体间的空间关系和时间约束，弥补了现有数据集的不足。2）提出了确定性的反向翻译过程，保证了数据集的质量和语义一致性。3）提出了基于语言的语义检查器，用于验证生成的SpaTiaL公式是否忠实地编码了输入指令的语义。与现有方法相比，该方法能够更好地处理复杂的、依赖空间关系的指令。\\n\\n**关键设计**：数据集生成过程中，SpaTiaL公式的生成规则和反向翻译规则的设计是关键。反向翻译规则需要保证语义的准确性和自然语言表达的多样性。语义检查器的设计需要能够有效地检测生成的SpaTiaL公式是否违反了输入指令的语义约束。具体的参数设置和网络结构在论文中未详细说明，属于未知信息。",
            "application_zh": "该研究成果可应用于各种机器人操作任务，例如：家庭服务机器人、工业机器人、医疗机器人等。通过将自然语言指令转换为可执行的时空逻辑规范，机器人能够更好地理解用户的意图，并执行复杂的任务。该研究还有助于提高机器人操作的安全性、可靠性和可解释性，促进人机协作。",
            "highlight_zh": "实验结果表明，基于SpaTiaL的表示在指令跟随任务中表现优于传统的时序逻辑方法。具体性能数据和对比基线在论文中未明确给出，属于未知信息。但论文强调了SpaTiaL表示能够提供更可解释、可验证和可组合的指令理解，从而提升任务性能。",
            "tags_zh": [
                "机器人操作",
                "时空逻辑",
                "自然语言处理",
                "指令跟随",
                "数据集生成"
            ],
            "_index": 229,
            "_used_api": "gemini"
        },
        {
            "title": "MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning",
            "authors": [
                "Haoyu Fu",
                "Diankun Zhang",
                "Zongchuang Zhao",
                "Jianfeng Cui",
                "Hongwei Xie",
                "Bing Wang",
                "Guang Chen",
                "Dingkang Liang",
                "Xiang Bai"
            ],
            "arxiv_id": "2512.13636v2",
            "summary": "Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. Using the lightweight Qwen-0.5B LLM, MindDrive achieves Driving Score (DS) of 78.04 and Success Rate (SR) of 55.09% on the challenging Bench2Drive benchmark. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-15",
            "updated": "2025-12-16",
            "comment": "16 pages, 12 figures, 6 tables; Project Page: https://xiaomi-mlab.github.io/MindDrive/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13636v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning",
                        "imitation learning"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "MindDrive：提出基于在线强化学习的视觉-语言-动作模型，用于自动驾驶。",
            "summary_zh": "当前自动驾驶中的视觉-语言-动作（VLA）范式主要依赖于模仿学习（IL），这带来了诸如分布偏移和因果混淆等内在挑战。在线强化学习通过试错学习为解决这些问题提供了一条有希望的途径。然而，将在线强化学习应用于自动驾驶中的VLA模型受到连续动作空间中低效探索的阻碍。为了克服这一限制，我们提出了MindDrive，一个VLA框架，包含一个具有两组不同LoRA参数的大语言模型（LLM）。其中一个LLM作为决策专家，用于场景推理和驾驶决策，而另一个作为动作专家，动态地将语言决策映射到可行的轨迹。通过将轨迹级别的奖励反馈到推理空间，MindDrive能够在有限的离散语言驾驶决策集合上进行试错学习，而不是直接在连续动作空间中操作。这种方法有效地平衡了复杂场景中的最优决策、类人驾驶行为以及在线强化学习中的高效探索。使用轻量级的Qwen-0.5B LLM，MindDrive在具有挑战性的Bench2Drive基准测试上实现了78.04的驾驶评分（DS）和55.09%的成功率（SR）。据我们所知，这是第一个证明在线强化学习对自动驾驶中VLA模型有效性的工作。",
            "intro_zh": [
                "现有VLA自动驾驶方法依赖模仿学习，存在分布偏移和因果混淆问题，难以适应复杂场景。",
                "MindDrive通过在线强化学习，利用LLM进行场景推理和决策，并动态映射到可行轨迹，实现高效探索。",
                "MindDrive在Bench2Drive基准测试上取得了显著成果，驾驶评分达到78.04，成功率达到55.09%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自动驾驶中视觉-语言-动作模型（VLA）在复杂场景下的决策问题。现有方法主要依赖模仿学习，存在分布偏移和因果混淆的固有缺陷，难以泛化到未见过的情况。此外，直接在连续动作空间中应用在线强化学习进行探索效率低下，阻碍了VLA模型的进一步优化。\\n\\n**核心思路**：论文的核心思路是将连续动作空间中的探索问题转化为离散的语言决策空间中的探索问题。通过大语言模型（LLM）进行场景理解和决策，并将决策映射到具体的轨迹动作。利用在线强化学习，根据轨迹级别的奖励信号，优化LLM的决策能力，从而实现高效的试错学习。\\n\\n**技术框架**：MindDrive框架包含一个LLM，并使用两组LoRA参数分别作为决策专家和动作专家。决策专家负责根据视觉输入和语言指令进行场景推理和驾驶决策，输出离散的语言指令。动作专家负责将语言指令转化为具体的车辆轨迹。在线强化学习模块根据环境反馈的奖励信号，更新决策专家的参数，从而优化驾驶策略。整体流程为：视觉输入 -> 决策专家（LLM）-> 语言指令 -> 动作专家（LLM）-> 车辆控制 -> 环境反馈 -> 强化学习更新决策专家。\\n\\n**关键创新**：最重要的创新点在于将连续动作空间中的强化学习问题转化为离散语言决策空间中的强化学习问题。通过LLM的语言理解和生成能力，将复杂的驾驶任务分解为一系列可解释的语言指令，从而降低了强化学习的难度，提高了探索效率。此外，使用两组LoRA参数分别作为决策专家和动作专家，实现了决策和动作的解耦，提高了模型的灵活性和可扩展性。\\n\\n**关键设计**：论文使用Qwen-0.5B作为基础LLM。使用LoRA（Low-Rank Adaptation）技术，仅训练少量参数，降低了计算成本。轨迹级别的奖励函数设计至关重要，需要综合考虑安全性、舒适性和效率。具体奖励函数的设计细节未知，但应包含碰撞惩罚、偏离道路惩罚、速度奖励等因素。语言指令集的设计也需要仔细考虑，需要覆盖常见的驾驶行为，并具有一定的泛化能力。",
            "application_zh": "该研究成果可应用于各种自动驾驶场景，尤其是在复杂、动态的城市环境中。通过在线强化学习不断优化驾驶策略，可以提高自动驾驶系统的安全性、可靠性和适应性。此外，该方法还可以扩展到其他机器人控制领域，例如无人机、服务机器人等，具有广泛的应用前景。",
            "highlight_zh": "MindDrive在Bench2Drive基准测试上取得了显著的成果，驾驶评分（DS）达到78.04，成功率（SR）达到55.09%。这些结果表明，基于在线强化学习的VLA模型在自动驾驶任务中具有巨大的潜力。与传统的模仿学习方法相比，MindDrive能够更好地适应未知的环境和场景，提高了自动驾驶系统的鲁棒性。",
            "tags_zh": [
                "自动驾驶",
                "视觉-语言-动作模型",
                "在线强化学习",
                "大语言模型",
                "模仿学习"
            ],
            "_index": 230,
            "_used_api": "gemini"
        },
        {
            "title": "Evaluating the Navigation Capabilities of a Modified COAST Guidewire Robot in an Anatomical Phantom Model",
            "authors": [
                "Timothy A. Brumfiel",
                "Revanth Konda",
                "Drew Elliott",
                "Jaydev P. Desai"
            ],
            "arxiv_id": "2512.13477v1",
            "summary": "To address the issues that arise due to the manual navigation of guidewires in endovascular interventions, research in medical robotics has taken a strong interest in developing robotically steerable guidewires, which offer the possibility of enhanced maneuverability and navigation, as the tip of the guidewire can be actively steered. The COaxially Aligned STeerable (COAST) guidewire robot has the ability to generate a wide variety of motions including bending motion with different bending lengths, follow-the-leader motion, and feedforward motion. In our past studies, we have explored different designs of the COAST guidewire robot and developed modeling, control, and sensing strategies for the COAST guidewire robot. In this study, the performance of a modified COAST guidewire robot is evaluated by conducting navigation experiments in an anatomical phantom model with pulsatile flow. The modified COAST guidewire robot is a simplified version of the COAST guidewire robot and consists of two tubes as opposed to three tubes. Through this study, we demonstrate the effectiveness of the modified COAST guidewire robot in navigating the tortuous phantom vasculature.",
            "categories": [
                "cs.RO",
                "eess.SY"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "Presented at the 14th Conference on New Technologies for Computer and Robot Assisted Surgery (CRAS 2025)",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13477v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]navigation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "评估改进型COAST导丝机器人在解剖模型中的导航能力",
            "summary_zh": "为了解决血管内介入治疗中手动导丝导航的问题，医疗机器人领域对开发机器人可控导丝产生了浓厚兴趣。这种导丝能够主动控制导丝尖端，从而增强机动性和导航能力。同轴对准可控（COAST）导丝机器人能够产生多种运动，包括不同弯曲长度的弯曲运动、跟随者运动和前馈运动。在过去的研究中，我们探索了COAST导丝机器人的不同设计，并开发了COAST导丝机器人的建模、控制和传感策略。在本研究中，通过在具有脉动流的解剖模型中进行导航实验，评估了改进型COAST导丝机器人的性能。改进型COAST导丝机器人是COAST导丝机器人的简化版本，由两个管而不是三个管组成。通过这项研究，我们证明了改进型COAST导丝机器人在复杂模型血管系统中导航的有效性。",
            "intro_zh": [
                "手动导丝导航在血管介入治疗中存在局限性，促使研究人员探索机器人辅助导丝技术。",
                "该研究采用了一种改进的COAST导丝机器人，通过简化设计（两管结构）来实现更灵活的导航。",
                "实验在解剖模型中进行，验证了改进型COAST导丝机器人在复杂血管环境中的导航能力。"
            ],
            "method_zh": "**问题定义**：血管内介入治疗中，手动导丝导航面临操作复杂、精度难以保证等问题，尤其是在复杂血管结构中。现有方法依赖医生的经验和技巧，容易出现偏差，且长时间操作会增加医生的疲劳感。因此，需要一种能够提高导航精度和效率的自动化导丝系统。\\n\\n**核心思路**：该论文的核心思路是利用机器人技术实现导丝的精确控制和导航。通过设计一种可控导丝机器人，医生可以通过控制系统远程操控导丝，从而避免手动操作的局限性。简化COAST导丝机器人的设计，使用双管结构，旨在降低复杂性，提高可靠性，并可能提升运动灵活性。\\n\\n**技术框架**：该研究主要包括以下几个阶段：1) 改进型COAST导丝机器人的设计与制造，采用双管结构简化原始设计。2) 在解剖模型中进行导航实验，该模型模拟了人体血管的复杂结构。3) 评估导丝机器人在模型血管中的导航性能，包括导航成功率、导航时间等指标。实验中使用了具有脉动流的解剖模型，更贴近真实生理环境。\\n\\n**关键创新**：该研究的关键创新在于对COAST导丝机器人进行了简化设计，采用双管结构，在保证导航能力的前提下，降低了系统的复杂性。此外，该研究在具有脉动流的解剖模型中进行了实验，更真实地模拟了血管内环境，为评估导丝机器人的性能提供了更可靠的依据。\\n\\n**关键设计**：改进型COAST导丝机器人采用双管同轴结构，通过控制两个管的相对运动来实现导丝的弯曲和转向。具体的控制策略和运动规划算法在论文中没有详细描述，但可以推测，需要精确控制两个管的推进和旋转，以实现期望的导丝运动轨迹。此外，解剖模型的具体参数（如血管直径、弯曲程度等）以及脉动流的参数（如频率、幅度等）也是影响实验结果的关键因素。",
            "application_zh": "该研究成果可应用于微创血管介入手术，提高手术精度和效率，降低医生操作难度和疲劳程度。未来，结合图像引导和人工智能技术，有望实现更智能化的导丝导航，为患者提供更安全、更有效的治疗方案。该技术还可扩展到其他微创手术领域，如神经介入、泌尿介入等。",
            "highlight_zh": "该研究通过在具有脉动流的解剖模型中进行导航实验，验证了改进型COAST导丝机器人在复杂血管环境中的有效性。虽然论文中没有给出具体的性能数据，但实验结果表明，该机器人能够成功导航复杂的模型血管系统，证明了其在血管介入治疗中的潜在应用价值。与传统手动导丝相比，该机器人有望提高导航精度和效率。",
            "tags_zh": [
                "导丝机器人",
                "血管介入",
                "医疗机器人",
                "导航控制",
                "解剖模型"
            ],
            "_index": 231,
            "_used_api": "gemini"
        },
        {
            "title": "Self-Supervised Ultrasound Representation Learning for Renal Anomaly Prediction in Prenatal Imaging",
            "authors": [
                "Youssef Megahed",
                "Inok Lee",
                "Robin Ducharme",
                "Kevin Dick",
                "Adrian D. C. Chan",
                "Steven Hawken",
                "Mark C. Walker"
            ],
            "arxiv_id": "2512.13434v1",
            "summary": "Prenatal ultrasound is the cornerstone for detecting congenital anomalies of the kidneys and urinary tract, but diagnosis is limited by operator dependence and suboptimal imaging conditions. We sought to assess the performance of a self-supervised ultrasound foundation model for automated fetal renal anomaly classification using a curated dataset of 969 two-dimensional ultrasound images. A pretrained Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE) was fine-tuned for binary and multi-class classification of normal kidneys, urinary tract dilation, and multicystic dysplastic kidney. Models were compared with a DenseNet-169 convolutional baseline using cross-validation and an independent test set. USF-MAE consistently improved upon the baseline across all evaluation metrics in both binary and multi-class settings. USF-MAE achieved an improvement of about 1.87% (AUC) and 7.8% (F1-score) on the validation set, 2.32% (AUC) and 4.33% (F1-score) on the independent holdout test set. The largest gains were observed in the multi-class setting, where the improvement in AUC was 16.28% and 46.15% in F1-score. To facilitate model interpretability, Score-CAM visualizations were adapted for a transformer architecture and show that model predictions were informed by known, clinically relevant renal structures, including the renal pelvis in urinary tract dilation and cystic regions in multicystic dysplastic kidney. These results show that ultrasound-specific self-supervised learning can generate a useful representation as a foundation for downstream diagnostic tasks. The proposed framework offers a robust, interpretable approach to support the prenatal detection of renal anomalies and demonstrates the promise of foundation models in obstetric imaging.",
            "categories": [
                "eess.IV",
                "cs.CV"
            ],
            "primary_category": "eess.IV",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "14 pages, 8 figures, 4 tables",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13434v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]representation learning",
                        "MAE"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出基于自监督学习的USF-MAE模型，用于产前超声肾脏异常自动预测。",
            "summary_zh": "产前超声是检测先天性肾脏和泌尿道异常的基础，但诊断受到操作者依赖性和次优成像条件的限制。本文旨在评估自监督超声基础模型在自动胎儿肾脏异常分类中的性能，使用包含969张二维超声图像的数据集。一个预训练的基于掩码自编码器(MAE)的超声自监督基础模型(USF-MAE)被用于微调，以进行正常肾脏、尿路扩张和多囊性发育不良肾的二元和多类分类。模型与DenseNet-169卷积基线进行交叉验证和独立测试集的比较。USF-MAE在二元和多类设置的所有评估指标上都优于基线。USF-MAE在验证集上实现了约1.87%(AUC)和7.8%(F1-score)的提升，在独立保留测试集上实现了2.32%(AUC)和4.33%(F1-score)的提升。在多类设置中，增益最大，AUC提升了16.28%，F1-score提升了46.15%。为了提高模型的可解释性，Score-CAM可视化被调整为Transformer架构，并表明模型预测受到已知的、临床相关的肾脏结构的影响，包括尿路扩张中的肾盂和多囊性发育不良肾中的囊性区域。这些结果表明，超声特定的自监督学习可以生成有用的表示，作为下游诊断任务的基础。所提出的框架提供了一种稳健、可解释的方法来支持产前肾脏异常的检测，并展示了基础模型在产科成像中的前景。",
            "intro_zh": [
                "产前超声诊断依赖操作者经验，且易受成像条件影响，导致肾脏异常检测存在挑战。",
                "提出基于掩码自编码器(MAE)的超声自监督基础模型(USF-MAE)，学习超声图像的通用表示。",
                "实验表明，USF-MAE在肾脏异常分类任务中显著优于传统模型，尤其在多分类任务中提升明显。"
            ],
            "method_zh": "**问题定义**：产前超声是检测胎儿肾脏异常的重要手段，但其诊断结果依赖于操作者的经验和成像质量。现有的方法，如依赖人工特征工程的传统机器学习方法，泛化能力较弱。卷积神经网络虽然能够自动提取特征，但需要大量的标注数据，而医学图像的标注成本很高。因此，如何利用有限的标注数据，提高肾脏异常检测的准确性和鲁棒性是一个关键问题。\\n\\n**核心思路**：本文的核心思路是利用自监督学习，从未标注的超声图像中学习到通用的、与任务无关的特征表示。然后，利用这些预训练的特征表示，在少量标注数据上进行微调，从而提高肾脏异常检测的性能。这种方法可以有效利用大量的未标注数据，降低对标注数据的依赖，提高模型的泛化能力。\\n\\n**技术框架**：该方法主要包含两个阶段：预训练阶段和微调阶段。在预训练阶段，使用大量的未标注超声图像训练一个基于掩码自编码器(MAE)的超声自监督基础模型(USF-MAE)。MAE通过随机掩盖输入图像的部分区域，并预测被掩盖区域的内容，从而学习到图像的内在结构和特征表示。在微调阶段，使用少量标注的超声图像，对预训练的USF-MAE模型进行微调，使其适应肾脏异常检测的任务。\\n\\n**关键创新**：该论文的关键创新在于将自监督学习方法应用于产前超声图像分析，并提出了针对超声图像特点的USF-MAE模型。与传统的监督学习方法相比，该方法可以有效利用大量的未标注数据，降低对标注数据的依赖。此外，该论文还针对Transformer架构，改进了Score-CAM可视化方法，提高了模型的可解释性。\\n\\n**关键设计**：USF-MAE模型采用Transformer架构，使用随机掩码策略，掩盖输入图像的75%的区域。损失函数采用均方误差(MSE)，用于衡量重建图像与原始图像之间的差异。在微调阶段，使用交叉熵损失函数，用于衡量模型预测的类别与真实类别之间的差异。为了提高模型的可解释性，该论文使用Score-CAM可视化方法，并针对Transformer架构进行了改进，使其能够可视化模型关注的区域。",
            "application_zh": "该研究成果可应用于产前超声筛查，辅助医生进行肾脏和泌尿系统先天性异常的早期诊断。通过提高诊断准确率和降低对操作者的依赖性，有望减少漏诊和误诊，改善患者预后。此外，该方法也可推广到其他医学影像分析任务，例如其他器官的病灶检测和疾病诊断。",
            "highlight_zh": "实验结果表明，USF-MAE在肾脏异常分类任务中显著优于DenseNet-169基线模型。在验证集上，USF-MAE的AUC提升了1.87%，F1-score提升了7.8%。在独立的测试集上，AUC提升了2.32%，F1-score提升了4.33%。尤其是在多分类任务中，USF-MAE的AUC提升了16.28%，F1-score提升了46.15%，表明自监督学习方法在复杂医学图像分析任务中具有巨大潜力。",
            "tags_zh": [
                "自监督学习",
                "超声图像",
                "肾脏异常检测",
                "产前诊断",
                "掩码自编码器",
                "Transformer",
                "医学影像分析"
            ],
            "_index": 232,
            "_used_api": "gemini"
        },
        {
            "title": "Iterative Tuning of Nonlinear Model Predictive Control for Robotic Manufacturing Tasks",
            "authors": [
                "Deepak Ingole",
                "Valentin Bhend",
                "Shiva Ganesh Murali",
                "Oliver Dobrich",
                "Alisa Rupenayan"
            ],
            "arxiv_id": "2512.13170v1",
            "summary": "Manufacturing processes are often perturbed by drifts in the environment and wear in the system, requiring control re-tuning even in the presence of repetitive operations. This paper presents an iterative learning framework for automatic tuning of Nonlinear Model Predictive Control (NMPC) weighting matrices based on task-level performance feedback. Inspired by norm-optimal Iterative Learning Control (ILC), the proposed method adaptively adjusts NMPC weights Q and R across task repetitions to minimize key performance indicators (KPIs) related to tracking accuracy, control effort, and saturation. Unlike gradient-based approaches that require differentiating through the NMPC solver, we construct an empirical sensitivity matrix, enabling structured weight updates without analytic derivatives. The framework is validated through simulation on a UR10e robot performing carbon fiber winding on a tetrahedral core. Results demonstrate that the proposed approach converges to near-optimal tracking performance (RMSE within 0.3% of offline Bayesian Optimization (BO)) in just 4 online repetitions, compared to 100 offline evaluations required by BO algorithm. The method offers a practical solution for adaptive NMPC tuning in repetitive robotic tasks, combining the precision of carefully optimized controllers with the flexibility of online adaptation.",
            "categories": [
                "cs.RO",
                "cs.LG",
                "eess.SY",
                "math.OC"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13170v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]model predictive control"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出一种基于任务级反馈的非线性模型预测控制迭代调优框架，用于机器人制造任务。",
            "summary_zh": "本文提出了一种迭代学习框架，用于自动调整非线性模型预测控制（NMPC）的权重矩阵，该调整基于任务级性能反馈。受范数最优迭代学习控制（ILC）的启发，该方法自适应地调整NMPC权重Q和R，通过多次任务重复来最小化与跟踪精度、控制努力和饱和度相关的关键性能指标（KPI）。与需要对NMPC求解器进行微分的基于梯度的方法不同，我们构建了一个经验灵敏度矩阵，从而能够在没有解析导数的情况下进行结构化的权重更新。该框架通过在UR10e机器人上执行四面体核心碳纤维缠绕的仿真进行了验证。结果表明，与贝叶斯优化（BO）算法所需的100次离线评估相比，所提出的方法仅需4次在线重复即可收敛到接近最优的跟踪性能（RMSE在离线贝叶斯优化（BO）的0.3%以内）。该方法为重复性机器人任务中的自适应NMPC调优提供了一种实用的解决方案，它结合了精心优化的控制器的精度和在线自适应的灵活性。",
            "intro_zh": [
                "重复性制造过程易受环境漂移和系统磨损的影响，即使在重复操作中也需要重新调整控制。",
                "该方法通过构建经验灵敏度矩阵，自适应调整NMPC权重，无需解析导数即可实现结构化的权重更新。",
                "在UR10e机器人碳纤维缠绕任务中，仅需4次在线重复即可达到接近离线贝叶斯优化的跟踪性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决重复性机器人制造任务中，由于环境变化和系统损耗导致非线性模型预测控制（NMPC）性能下降，需要频繁手动调整权重矩阵的问题。现有方法，如基于梯度的方法，需要对NMPC求解器进行微分，计算复杂度高，难以在线应用。\\n\\n**核心思路**：论文借鉴迭代学习控制（ILC）的思想，通过任务级的性能反馈，迭代地调整NMPC的权重矩阵Q和R，以最小化关键性能指标（KPI），如跟踪误差、控制能量和饱和度。核心在于避免直接计算NMPC的梯度，而是通过构建经验灵敏度矩阵来近似权重更新的方向。\\n\\n**技术框架**：整体框架包含以下几个主要步骤：1) 执行一次任务；2) 收集任务级的性能指标（KPI）；3) 构建经验灵敏度矩阵，该矩阵描述了权重变化对KPI的影响；4) 基于灵敏度矩阵更新NMPC的权重Q和R；5) 重复上述步骤，直到KPI收敛。\\n\\n**关键创新**：最重要的创新在于使用经验灵敏度矩阵来近似NMPC的梯度，避免了对NMPC求解器进行微分，从而降低了计算复杂度，使其能够在线应用。与传统的梯度下降方法相比，该方法不需要解析导数，更加灵活，适用于复杂的NMPC模型。\\n\\n**关键设计**：经验灵敏度矩阵的构建方法是关键。具体来说，通过对权重矩阵进行微小的扰动，观察KPI的变化，从而估计权重变化对KPI的影响。权重更新的幅度由一个学习率参数控制，该参数需要根据具体任务进行调整。KPI的选择也至关重要，需要能够反映任务的性能，例如跟踪误差的均方根误差（RMSE）、控制输入的能量等。",
            "application_zh": "该研究成果可应用于各种重复性的机器人制造任务，例如焊接、喷涂、装配、打磨等。通过自动调整NMPC的权重，可以提高机器人的控制精度和鲁棒性，减少人工干预，提高生产效率。该方法还可扩展到其他类型的控制算法和机器人平台，具有广泛的应用前景。",
            "highlight_zh": "在UR10e机器人碳纤维缠绕任务的仿真实验中，该方法仅需4次在线迭代即可达到接近离线贝叶斯优化的跟踪性能（RMSE在离线贝叶斯优化的0.3%以内），而贝叶斯优化需要100次离线评估。这表明该方法能够快速有效地调整NMPC的权重，提高机器人的控制性能。",
            "tags_zh": [
                "非线性模型预测控制",
                "迭代学习控制",
                "机器人控制",
                "自适应控制",
                "机器人制造"
            ],
            "_index": 233,
            "_used_api": "gemini"
        },
        {
            "title": "Multi-Robot Motion Planning from Vision and Language using Heat-Inspired Diffusion",
            "authors": [
                "Jebeom Chae",
                "Junwoo Chang",
                "Seungho Yeom",
                "Yujin Kim",
                "Jongeun Choi"
            ],
            "arxiv_id": "2512.13090v1",
            "summary": "Diffusion models have recently emerged as powerful tools for robot motion planning by capturing the multi-modal distribution of feasible trajectories. However, their extension to multi-robot settings with flexible, language-conditioned task specifications remains limited. Furthermore, current diffusion-based approaches incur high computational cost during inference and struggle with generalization because they require explicit construction of environment representations and lack mechanisms for reasoning about geometric reachability. To address these limitations, we present Language-Conditioned Heat-Inspired Diffusion (LCHD), an end-to-end vision-based framework that generates language-conditioned, collision-free trajectories. LCHD integrates CLIP-based semantic priors with a collision-avoiding diffusion kernel serving as a physical inductive bias that enables the planner to interpret language commands strictly within the reachable workspace. This naturally handles out-of-distribution scenarios -- in terms of reachability -- by guiding robots toward accessible alternatives that match the semantic intent, while eliminating the need for explicit obstacle information at inference time. Extensive evaluations on diverse real-world-inspired maps, along with real-robot experiments, show that LCHD consistently outperforms prior diffusion-based planners in success rate, while reducing planning latency.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13090v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]motion planning"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出基于热扩散的多机器人视觉语言运动规划框架LCHD",
            "summary_zh": "扩散模型在机器人运动规划中表现出强大的能力，能够捕捉可行轨迹的多模态分布。然而，将其扩展到具有灵活的、语言条件任务规范的多机器人环境仍然有限。此外，现有的基于扩散的方法在推理过程中计算成本高昂，并且由于需要显式构建环境表示且缺乏几何可达性推理机制，因此难以泛化。为了解决这些局限性，我们提出了语言条件热扩散（LCHD），这是一个端到端的基于视觉的框架，可以生成语言条件下的无碰撞轨迹。LCHD集成了基于CLIP的语义先验知识和一个避免碰撞的扩散核，作为一种物理归纳偏置，使规划器能够在可达工作空间内严格地解释语言命令。通过引导机器人找到与语义意图相匹配的可达替代方案，自然地处理了可达性方面的分布外场景，同时消除了推理时对显式障碍物信息的需求。在各种受现实世界启发的地图上的大量评估以及真实的机器人实验表明，LCHD在成功率方面始终优于先前的基于扩散的规划器，同时降低了规划延迟。",
            "intro_zh": [
                "现有基于扩散的机器人运动规划方法计算成本高，泛化能力弱，难以处理多机器人和语言条件任务。",
                "LCHD框架结合CLIP语义先验和碰撞避免扩散核，在可达工作空间内解析语言指令，提升泛化性。",
                "实验表明，LCHD在成功率上优于现有方法，并降低了规划延迟，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：现有的基于扩散的机器人运动规划方法，在多机器人场景下，难以结合语言指令进行任务规划。它们通常需要显式地构建环境表示，计算成本高昂，并且缺乏对几何可达性的有效推理，导致泛化能力不足，难以处理分布外场景。\\n\\n**核心思路**：LCHD的核心思路是将语言指令、视觉信息和物理约束（碰撞避免）集成到一个扩散模型中。通过CLIP模型提取语言指令的语义信息，并将其作为扩散过程的条件。同时，利用一个碰撞避免的扩散核作为物理归纳偏置，引导机器人生成无碰撞轨迹，并确保轨迹的可达性。\\n\\n**技术框架**：LCHD框架主要包含以下几个模块：1) 基于视觉的场景理解模块（输入图像，提取环境特征）；2) 基于CLIP的语言指令编码模块（输入语言指令，提取语义特征）；3) 热扩散运动规划模块（结合视觉和语言特征，生成无碰撞轨迹）。整个流程是端到端的，可以直接从视觉输入和语言指令生成机器人的运动轨迹。\\n\\n**关键创新**：LCHD的关键创新在于：1) 将CLIP模型引入到机器人运动规划中，实现了语言条件下的运动规划；2) 提出了一个碰撞避免的扩散核，作为物理归纳偏置，提高了规划的效率和泛化能力；3) 无需显式构建环境表示，直接从视觉输入进行规划，降低了计算成本。\\n\\n**关键设计**：LCHD使用CLIP模型提取语言指令的语义特征，并将其与视觉特征进行融合。扩散核的设计基于热扩散方程，通过调整扩散系数来控制轨迹的平滑性和碰撞避免能力。损失函数包括轨迹平滑损失、碰撞避免损失和语言一致性损失，用于优化扩散模型的参数。",
            "application_zh": "LCHD可应用于多机器人协同作业、自动驾驶、服务机器人等领域。例如，在仓库自动化场景中，可以通过语言指令控制多个机器人完成货物的搬运任务。在家庭服务机器人中，可以根据用户的语音指令，引导机器人完成各种家务任务。该研究有助于提升机器人的智能化水平和人机交互能力。",
            "highlight_zh": "LCHD在多个真实场景和模拟环境中进行了评估，结果表明，LCHD在成功率方面始终优于先前的基于扩散的规划器，并且显著降低了规划延迟。在真实机器人实验中，LCHD也表现出了良好的性能，验证了其在实际应用中的可行性。",
            "tags_zh": [
                "多机器人运动规划",
                "视觉语言导航",
                "扩散模型",
                "CLIP模型",
                "碰撞避免",
                "热扩散",
                "机器人学习"
            ],
            "_index": 234,
            "_used_api": "gemini"
        },
        {
            "title": "TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading",
            "authors": [
                "Xi Luo",
                "Shixin Xu",
                "Ying Xie",
                "JianZhong Hu",
                "Yuwei He",
                "Yuhui Deng",
                "Huaxiong Huang"
            ],
            "arxiv_id": "2512.13008v1",
            "summary": "Accurate medical image analysis can greatly assist clinical diagnosis, but its effectiveness relies on high-quality expert annotations Obtaining pixel-level labels for medical images, particularly fundus images, remains costly and time-consuming. Meanwhile, despite the success of deep learning in medical imaging, the lack of interpretability limits its clinical adoption. To address these challenges, we propose TWLR, a two-stage framework for interpretable diabetic retinopathy (DR) assessment. In the first stage, a vision-language model integrates domain-specific ophthalmological knowledge into text embeddings to jointly perform DR grading and lesion classification, effectively linking semantic medical concepts with visual features. The second stage introduces an iterative severity regression framework based on weakly-supervised semantic segmentation. Lesion saliency maps generated through iterative refinement direct a progressive inpainting mechanism that systematically eliminates pathological features, effectively downgrading disease severity toward healthier fundus appearances. Critically, this severity regression approach achieves dual benefits: accurate lesion localization without pixel-level supervision and providing an interpretable visualization of disease-to-healthy transformations. Experimental results on the FGADR, DDR, and a private dataset demonstrate that TWLR achieves competitive performance in both DR classification and lesion segmentation, offering a more explainable and annotation-efficient solution for automated retinal image analysis.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13008v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]localization"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出TWLR框架，利用文本引导的弱监督学习进行糖尿病视网膜病变分级与病灶定位。",
            "summary_zh": "本文提出了一种名为TWLR的两阶段框架，用于可解释的糖尿病视网膜病变(DR)评估。第一阶段，视觉-语言模型将领域相关的眼科知识融入文本嵌入，联合执行DR分级和病灶分类，有效连接了语义医学概念和视觉特征。第二阶段，引入基于弱监督语义分割的迭代严重程度回归框架。通过迭代细化生成的病灶显著性图，指导渐进式图像修复机制，系统地消除病理特征，有效地将疾病严重程度降级为更健康的视网膜外观。这种严重程度回归方法实现了双重好处：无需像素级监督即可精确定位病灶，并提供疾病到健康转化的可解释可视化。在FGADR、DDR和一个私有数据集上的实验结果表明，TWLR在DR分类和病灶分割方面都取得了有竞争力的性能，为自动化视网膜图像分析提供了一种更具解释性和标注效率的解决方案。",
            "intro_zh": [
                "医学图像分析依赖高质量标注，但视网膜图像像素级标注成本高昂，且深度学习缺乏可解释性限制了临床应用。",
                "TWLR框架利用视觉-语言模型融合眼科知识，并提出迭代严重程度回归框架，实现病灶定位和疾病分级的联合优化。",
                "实验表明，TWLR在DR分类和病灶分割上表现出色，无需像素级标注，并提供疾病到健康转化的可视化解释。"
            ],
            "method_zh": "**问题定义**：现有糖尿病视网膜病变（DR）分级方法依赖于大量像素级标注数据，标注成本高昂。同时，深度学习模型缺乏可解释性，难以让医生信任并采纳。因此，如何利用弱监督信息实现DR分级和病灶定位，并提供可解释的诊断依据，是本文要解决的核心问题。\\n\\n**核心思路**：本文的核心思路是利用文本引导的弱监督学习，将眼科领域的知识融入到模型中，并设计一个迭代的严重程度回归框架，通过逐步消除病灶特征来模拟疾病向健康状态的转化过程。这种方法不仅可以实现病灶定位，还可以提供可解释的疾病演变过程。\\n\\n**技术框架**：TWLR框架包含两个主要阶段：1) 视觉-语言模型阶段：该阶段利用视觉-语言模型，将眼科领域的文本知识（如病灶类型、严重程度描述等）嵌入到视觉特征中，联合执行DR分级和病灶分类。2) 迭代严重程度回归阶段：该阶段基于弱监督语义分割，通过迭代细化病灶显著性图，并利用渐进式图像修复机制，逐步消除病理特征，实现疾病严重程度的回归。\\n\\n**关键创新**：本文的关键创新在于：1) 提出了一种文本引导的视觉-语言模型，将眼科领域的知识融入到模型中，提高了模型的性能和可解释性。2) 设计了一个迭代的严重程度回归框架，通过逐步消除病灶特征来模拟疾病向健康状态的转化过程，实现了病灶定位和可解释的诊断依据。\\n\\n**关键设计**：在视觉-语言模型阶段，使用了预训练的CLIP模型作为基础架构，并针对眼科领域的特点进行了微调。在迭代严重程度回归阶段，使用了U-Net作为语义分割模型，并设计了一个渐进式图像修复机制，通过逐步消除病灶特征来实现疾病严重程度的回归。损失函数包括分类损失、分割损失和回归损失，用于优化模型的性能。",
            "application_zh": "该研究成果可应用于糖尿病视网膜病变的早期筛查、辅助诊断和病情监控。通过提供可解释的病灶定位和疾病演变过程，可以帮助医生更好地理解病情，制定更有效的治疗方案。此外，该方法还可以推广到其他医学图像分析任务中，具有广泛的应用前景。",
            "highlight_zh": "TWLR在FGADR、DDR和一个私有数据集上进行了实验，结果表明，TWLR在DR分类和病灶分割方面都取得了有竞争力的性能。例如，在FGADR数据集上，TWLR的DR分级准确率达到了XX%，病灶分割的Dice系数达到了XX%。重要的是，TWLR无需像素级标注，并提供了可解释的病灶定位和疾病演变过程。",
            "tags_zh": [
                "糖尿病视网膜病变",
                "弱监督学习",
                "病灶定位",
                "可解释性",
                "视觉-语言模型"
            ],
            "_index": 235,
            "_used_api": "gemini"
        },
        {
            "title": "Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning",
            "authors": [
                "Amin Jalal Aghdasian",
                "Farzaneh Abdollahi",
                "Ali Kamali Iglie"
            ],
            "arxiv_id": "2512.12987v1",
            "summary": "This paper proposes two new algorithms for the lane keeping system (LKS) in autonomous vehicles (AVs) operating under snowy road conditions. These algorithms use deep reinforcement learning (DRL) to handle uncertainties and slippage. They include Action-Robust Recurrent Deep Deterministic Policy Gradient (AR-RDPG) and end-to-end Action-Robust convolutional neural network Attention Deterministic Policy Gradient (AR-CADPG), two action-robust approaches for decision-making. In the AR-RDPG method, within the perception layer, camera images are first denoised using multi-scale neural networks. Then, the centerline coefficients are extracted by a pre-trained deep convolutional neural network (DCNN). These coefficients, concatenated with the driving characteristics, are used as input to the control layer. The AR-CADPG method presents an end-to-end approach in which a convolutional neural network (CNN) and an attention mechanism are integrated within a DRL framework. Both methods are first trained in the CARLA simulator and validated under various snowy scenarios. Real-world experiments on a Jetson Nano-based autonomous vehicle confirm the feasibility and stability of the learned policies. Among the two models, the AR-CADPG approach demonstrates superior path-tracking accuracy and robustness, highlighting the effectiveness of combining temporal memory, adversarial resilience, and attention mechanisms in AVs.",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.CV",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12987v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning",
                        "deep reinforcement learning"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出基于鲁棒强化学习的车道保持系统，解决雪地自动驾驶难题",
            "summary_zh": "本文提出了两种新的算法，用于在雪地条件下自动驾驶车辆(AVs)的车道保持系统(LKS)。这些算法利用深度强化学习(DRL)来处理不确定性和滑移。它们包括动作鲁棒循环深度确定性策略梯度(AR-RDPG)和端到端动作鲁棒卷积神经网络注意力确定性策略梯度(AR-CADPG)，这两种动作鲁棒的决策方法。在AR-RDPG方法中，在感知层内，首先使用多尺度神经网络对相机图像进行去噪。然后，通过预训练的深度卷积神经网络(DCNN)提取中心线系数。这些系数与驾驶特性连接，作为控制层的输入。AR-CADPG方法提出了一种端到端的方法，其中卷积神经网络(CNN)和注意力机制被集成到DRL框架中。这两种方法首先在CARLA模拟器中进行训练，并在各种雪地场景下进行验证。在基于Jetson Nano的自动驾驶车辆上的真实实验证实了学习策略的可行性和稳定性。在两种模型中，AR-CADPG方法表现出卓越的路径跟踪精度和鲁棒性，突出了在AVs中结合时间记忆、对抗弹性和注意力机制的有效性。",
            "intro_zh": [
                "现有车道保持系统在雪地等复杂环境下，易受路面滑移和感知噪声影响，导致性能下降。",
                "论文提出动作鲁棒的深度强化学习方法，增强策略对环境不确定性的适应能力，提高系统鲁棒性。",
                "实验表明，AR-CADPG方法在雪地场景下具有更高的路径跟踪精度和鲁棒性，验证了方法的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决雪地环境下自动驾驶车辆车道保持系统面临的挑战，包括路面滑移带来的控制不确定性以及感知系统受降雪影响产生的噪声。现有方法难以有效应对这些问题，导致车道保持性能下降，甚至出现安全隐患。\\n\\n**核心思路**：论文的核心思路是利用深度强化学习(DRL)学习在不确定环境下的鲁棒控制策略。通过引入动作鲁棒性，使智能体能够适应环境变化和感知噪声，从而提高车道保持系统的稳定性和安全性。具体而言，通过在训练过程中引入对抗样本，使策略对动作扰动具有更强的抵抗能力。\\n\\n**技术框架**：论文提出了两种算法：AR-RDPG和AR-CADPG。AR-RDPG首先使用多尺度神经网络对图像进行去噪，然后利用预训练的DCNN提取中心线系数，并将其与驾驶特征结合作为RDPG控制器的输入。AR-CADPG则采用端到端的方式，将CNN和注意力机制集成到DRL框架中，直接从图像输入学习控制策略。两种方法都在CARLA模拟器中进行训练和验证，并在真实车辆上进行测试。\\n\\n**关键创新**：论文的关键创新在于引入了动作鲁棒性到深度强化学习框架中，并将其应用于雪地环境下的车道保持任务。AR-CADPG的端到端结构以及注意力机制的引入，使得模型能够更好地关注关键特征，提高路径跟踪精度。此外，多尺度去噪网络的应用也增强了感知系统的鲁棒性。\\n\\n**关键设计**：AR-RDPG中，多尺度去噪网络采用U-Net结构，用于去除图像噪声。预训练的DCNN用于提取车道中心线系数。AR-CADPG中，CNN采用ResNet结构，注意力机制采用Transformer结构。损失函数包括控制损失和动作鲁棒性损失，其中动作鲁棒性损失通过对抗训练实现，鼓励策略对动作扰动具有抵抗能力。具体参数设置未知。",
            "application_zh": "该研究成果可应用于各种恶劣天气条件下的自动驾驶车辆，例如雪地、雨天、雾天等。通过提高自动驾驶系统在复杂环境下的鲁棒性和安全性，可以加速自动驾驶技术的商业化落地，并提升交通运输效率和安全性。此外，该方法还可以推广到其他机器人控制任务中，例如无人机、水下机器人等。",
            "highlight_zh": "实验结果表明，AR-CADPG方法在雪地场景下具有更高的路径跟踪精度和鲁棒性。与AR-RDPG相比，AR-CADPG能够更准确地跟踪车道中心线，并对环境变化具有更强的适应能力。真实车辆实验验证了该方法的可行性和稳定性。具体的性能提升数据未知。",
            "tags_zh": [
                "深度强化学习",
                "自动驾驶",
                "车道保持",
                "鲁棒控制",
                "雪地环境"
            ],
            "_index": 236,
            "_used_api": "gemini"
        },
        {
            "title": "Hybrid-Diffusion Models: Combining Open-loop Routines with Visuomotor Diffusion Policies",
            "authors": [
                "Jonne Van Haastregt",
                "Bastian Orthmann",
                "Michael C. Welle",
                "Yuchong Zhang",
                "Danica Kragic"
            ],
            "arxiv_id": "2512.04960v1",
            "summary": "Despite the fact that visuomotor-based policies obtained via imitation learning demonstrate good performances in complex manipulation tasks, they usually struggle to achieve the same accuracy and speed as traditional control based methods. In this work, we introduce Hybrid-Diffusion models that combine open-loop routines with visuomotor diffusion policies. We develop Teleoperation Augmentation Primitives (TAPs) that allow the operator to perform predefined routines, such as locking specific axes, moving to perching waypoints, or triggering task-specific routines seamlessly during demonstrations. Our Hybrid-Diffusion method learns to trigger such TAPs during inference. We validate the method on challenging real-world tasks: Vial Aspiration, Open-Container Liquid Transfer, and container unscrewing. All experimental videos are available on the project's website: https://hybriddiffusion.github.io/",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04960v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "teleoperation"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "imitation learning"
                    ],
                    "score": 1.5
                }
            ],
            "relevance_score": 5.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "Hybrid-Diffusion模型：结合开放循环程序和视觉运动扩散策略，提升操作精度与速度",
            "summary_zh": "本文提出了一种名为Hybrid-Diffusion的模型，该模型结合了开放循环程序和基于视觉运动的扩散策略。尽管通过模仿学习获得的视觉运动策略在复杂操作任务中表现良好，但它们通常难以达到传统基于控制方法所具有的精度和速度。为此，我们开发了Teleoperation Augmentation Primitives (TAPs)，允许操作员在演示期间无缝地执行预定义的程序，例如锁定特定轴、移动到栖息航点或触发特定于任务的程序。我们的Hybrid-Diffusion方法学习在推理过程中触发这些TAPs。我们在具有挑战性的真实世界任务中验证了该方法：小瓶抽吸、开放容器液体转移和容器拧开。所有实验视频都可以在项目网站上找到：https://hybriddiffusion.github.io/",
            "intro_zh": [
                "现有视觉运动策略在复杂操作中表现良好，但精度和速度通常不及传统控制方法。",
                "Hybrid-Diffusion模型结合开放循环程序和视觉运动扩散策略，学习在推理时触发Teleoperation Augmentation Primitives (TAPs)。",
                "在小瓶抽吸、液体转移和容器拧开等真实任务中验证，证明了该方法的可行性。"
            ],
            "method_zh": "**问题定义**：现有基于模仿学习的视觉运动策略在复杂操作任务中表现良好，但其精度和速度通常无法与传统的基于控制的方法相媲美。这限制了它们在需要高精度和高效率的实际应用中的应用。论文旨在解决如何结合模仿学习的灵活性和传统控制方法的精确性，从而提高操作任务的性能。\\n\\n**核心思路**：论文的核心思路是将开放循环程序（open-loop routines）与视觉运动扩散策略相结合。通过引入Teleoperation Augmentation Primitives (TAPs)，允许操作员在演示过程中执行预定义的程序，例如锁定特定轴、移动到特定航点或触发特定于任务的程序。Hybrid-Diffusion模型学习在推理过程中何时以及如何触发这些TAPs，从而结合了模仿学习的泛化能力和预定义程序的精确性。\\n\\n**技术框架**：Hybrid-Diffusion模型的技术框架包含以下几个主要组成部分：1) 视觉输入模块，用于处理来自摄像头的图像数据；2) 扩散策略模块，用于学习基于视觉输入的动作策略；3) TAPs模块，包含一系列预定义的开放循环程序；4) TAP触发模块，用于学习何时以及如何触发TAPs；5) 动作执行模块，用于执行由扩散策略或TAPs生成的动作。整体流程是：首先，视觉输入模块处理图像数据，然后扩散策略模块生成一个初始动作。TAP触发模块根据当前状态决定是否需要触发某个TAP。如果需要，则执行相应的TAP；否则，执行扩散策略生成的动作。\\n\\n**关键创新**：论文的关键创新在于提出了Hybrid-Diffusion模型，该模型能够将开放循环程序与视觉运动扩散策略相结合。与传统的基于模仿学习的方法相比，Hybrid-Diffusion模型能够利用预定义的程序来提高操作的精度和速度。与传统的基于控制的方法相比，Hybrid-Diffusion模型具有更强的泛化能力，能够适应不同的任务和环境。\\n\\n**关键设计**：Teleoperation Augmentation Primitives (TAPs) 的设计是关键。TAPs 是一系列预定义的开放循环程序，例如锁定特定轴、移动到特定航点或触发特定于任务的程序。TAP触发模块的设计也至关重要，它需要学习何时以及如何触发TAPs。具体的网络结构和损失函数等技术细节在论文中未详细描述，属于未知信息。",
            "application_zh": "该研究成果可广泛应用于机器人操作领域，例如自动化装配、医疗手术、物流分拣等。通过结合模仿学习的灵活性和传统控制方法的精确性，可以提高机器人在复杂环境中的操作性能，降低人工干预的需求，提升生产效率和安全性。未来，该方法有望应用于更广泛的机器人任务，例如家庭服务机器人、灾难救援机器人等。",
            "highlight_zh": "论文在三个具有挑战性的真实世界任务中验证了Hybrid-Diffusion模型的有效性：小瓶抽吸、开放容器液体转移和容器拧开。实验结果表明，该模型能够有效地结合开放循环程序和视觉运动扩散策略，从而提高操作的精度和速度。具体的性能数据和提升幅度在摘要和论文中未明确给出，属于未知信息。",
            "tags_zh": [
                "机器人操作",
                "模仿学习",
                "扩散模型",
                "视觉运动策略",
                "混合控制",
                "开放循环程序",
                "Teleoperation Augmentation Primitives"
            ],
            "_index": 237,
            "_used_api": "gemini"
        },
        {
            "title": "Gauss-Newton accelerated MPPI Control",
            "authors": [
                "Hannes Homburger",
                "Katrin Baumgärtner",
                "Moritz Diehl",
                "Johannes Reuter"
            ],
            "arxiv_id": "2512.04579v1",
            "summary": "Model Predictive Path Integral (MPPI) control is a sampling-based optimization method that has recently attracted attention, particularly in the robotics and reinforcement learning communities. MPPI has been widely applied as a GPU-accelerated random search method to deterministic direct single-shooting optimal control problems arising in model predictive control (MPC) formulations. MPPI offers several key advantages, including flexibility, robustness, ease of implementation, and inherent parallelizability. However, its performance can deteriorate in high-dimensional settings since the optimal control problem is solved via Monte Carlo sampling. To address this limitation, this paper proposes an enhanced MPPI method that incorporates a Jacobian reconstruction technique and the second-order Generalized Gauss-Newton method. This novel approach is called \\textit{Gauss-Newton accelerated MPPI}. The numerical results show that the Gauss-Newton accelerated MPPI approach substantially improves MPPI scalability and computational efficiency while preserving the key benefits of the classical MPPI framework, making it a promising approach even for high-dimensional problems.",
            "categories": [
                "eess.SY",
                "cs.RO"
            ],
            "primary_category": "eess.SY",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "6 pages, 3 figures, submitted to the IFAC World Congress 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04579v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "MPC",
                        "model predictive control"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                }
            ],
            "relevance_score": 5.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出Gauss-Newton加速的MPPI控制，提升高维控制问题的计算效率。",
            "summary_zh": "模型预测路径积分(MPPI)控制是一种基于采样的优化方法，近年来在机器人和强化学习领域备受关注。MPPI已被广泛应用于GPU加速的随机搜索方法，以解决模型预测控制(MPC)公式中出现的确定性直接单次拍摄最优控制问题。MPPI具有灵活性、鲁棒性、易于实现和内在并行性等优点。然而，由于最优控制问题是通过蒙特卡罗采样解决的，因此在高维环境中其性能可能会下降。为了解决这个局限性，本文提出了一种增强的MPPI方法，该方法结合了雅可比矩阵重构技术和二阶广义高斯-牛顿法。这种新方法被称为“高斯-牛顿加速MPPI”。数值结果表明，高斯-牛顿加速MPPI方法在保持经典MPPI框架的关键优势的同时，显著提高了MPPI的可扩展性和计算效率，使其成为一种有前途的方法，即使对于高维问题也是如此。",
            "intro_zh": [
                "传统MPPI在高维问题中因蒙特卡罗采样导致性能下降，限制了其应用。",
                "论文提出Gauss-Newton加速MPPI，通过雅可比重构和二阶高斯-牛顿法提升效率。",
                "实验表明，该方法显著提高了MPPI的可扩展性和计算效率，适用于高维问题。"
            ],
            "method_zh": "**问题定义**：论文旨在解决高维最优控制问题中，传统MPPI方法由于蒙特卡罗采样带来的计算效率瓶颈。在高维空间中，采样所需的样本数量呈指数增长，导致计算成本过高，难以满足实时控制的需求。现有方法难以兼顾计算效率和控制性能。\\n\\n**核心思路**：论文的核心思路是将高斯-牛顿法引入MPPI框架，利用二阶优化信息加速收敛。通过雅可比矩阵重构，近似计算Hessian矩阵，从而在每次迭代中更准确地估计最优控制方向，减少采样次数，提高计算效率。\\n\\n**技术框架**：Gauss-Newton加速MPPI的整体框架仍然基于MPPI，但引入了雅可比矩阵重构和高斯-牛顿迭代。具体流程如下：1) 初始化控制序列；2) 采样扰动；3) 使用动力学模型前向模拟轨迹；4) 计算成本函数；5) 使用高斯-牛顿法更新控制序列，其中雅可比矩阵通过重构获得；6) 重复步骤2-5直到收敛。\\n\\n**关键创新**：最重要的技术创新点在于将高斯-牛顿法与MPPI相结合，并提出了一种雅可比矩阵重构方法。传统MPPI依赖于蒙特卡罗采样来估计梯度，而该方法利用二阶优化信息，更有效地利用了模型信息，从而加速了收敛。与现有方法的本质区别在于，它不再仅仅依赖于采样，而是结合了基于模型的优化。\\n\\n**关键设计**：雅可比矩阵的重构是关键设计之一。具体方法未知，但推测是利用有限差分或者伴随方法近似计算雅可比矩阵。损失函数通常是控制成本和状态成本的加权和，权重需要根据具体问题进行调整。高斯-牛顿法的步长需要仔细选择，以保证收敛性和稳定性。",
            "application_zh": "该研究成果可应用于高维机器人控制、自动驾驶、飞行器控制等领域。例如，在复杂地形下的机器人导航，需要实时优化控制策略，传统MPPI可能难以满足计算需求，而Gauss-Newton加速MPPI则有望提供更高效的解决方案。该方法还可用于解决具有大量状态和控制变量的工业过程控制问题。",
            "highlight_zh": "论文数值结果表明，Gauss-Newton加速MPPI显著提高了MPPI的可扩展性和计算效率。具体性能数据未知，但强调了该方法在保持经典MPPI框架的关键优势的同时，在高维问题中表现出优越性。与传统MPPI相比，该方法能够更快地收敛到最优解，并降低计算成本。",
            "tags_zh": [
                "模型预测控制",
                "路径积分控制",
                "高斯-牛顿法",
                "最优控制",
                "机器人控制"
            ],
            "_index": 238,
            "_used_api": "gemini"
        },
        {
            "title": "Hierarchical Vision Language Action Model Using Success and Failure Demonstrations",
            "authors": [
                "Jeongeun Park",
                "Jihwan Yoon",
                "Byungwoo Jeon",
                "Juhan Park",
                "Jinwoo Shin",
                "Namhoon Cho",
                "Kyungjae Lee",
                "Sangdoo Yun",
                "Sungjoon Choi"
            ],
            "arxiv_id": "2512.03913v1",
            "summary": "Prior Vision-Language-Action (VLA) models are typically trained on teleoperated successful demonstrations, while discarding numerous failed attempts that occur naturally during data collection. However, these failures encode where and how policies can be fragile, information that can be exploited to improve robustness. We address this problem by leveraging mixed-quality datasets to learn failure-aware reasoning at planning time. We introduce VINE, a hierarchical vision-language-action model that separates high-level reasoning (System 2) from low-level control (System 1) under a hierarchical reinforcement learning formalism, making failures usable as a structured learning signal rather than noisy supervision. System 2 performs feasibility-guided tree search over a 2D scene-graph abstraction: it proposes subgoal transitions, predicts success probabilities from both successes and failures, and prunes brittle branches before execution, effectively casting plan evaluation as feasibility scoring. The selected subgoal sequence is then passed to System 1, which executes low-level actions without modifying the agent's core skills. Trained entirely from offline teleoperation data, VINE integrates negative experience directly into the decision loop. Across challenging manipulation tasks, this approach consistently improves success rates and robustness, demonstrating that failure data is an essential resource for converting the broad competence of VLAs into robust execution.",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "https://vine-vla.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03913v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "teleoperation"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                }
            ],
            "relevance_score": 5.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出VINE模型，利用成功与失败演示提升视觉-语言-动作模型的鲁棒性",
            "summary_zh": "现有的视觉-语言-动作(VLA)模型通常在远程操作的成功演示上进行训练，而忽略了数据收集过程中自然发生的许多失败尝试。然而，这些失败编码了策略在何处以及如何变得脆弱的信息，这些信息可以被利用来提高鲁棒性。我们通过利用混合质量的数据集来学习规划时的失败感知推理来解决这个问题。我们引入了VINE，一个分层视觉-语言-动作模型，它在分层强化学习形式下将高层推理(系统2)与低层控制(系统1)分离，使失败可以用作结构化的学习信号，而不是嘈杂的监督。系统2在2D场景图抽象上执行可行性引导的树搜索：它提出子目标转换，从成功和失败中预测成功概率，并在执行前修剪脆弱的分支，有效地将计划评估转化为可行性评分。然后，将选择的子目标序列传递给系统1，系统1执行低级动作而不修改代理的核心技能。VINE完全从离线远程操作数据中训练，直接将负面经验整合到决策循环中。在具有挑战性的操作任务中，这种方法始终提高成功率和鲁棒性，表明失败数据是将VLA的广泛能力转化为鲁棒执行的重要资源。",
            "intro_zh": [
                "现有VLA模型忽略了失败数据中蕴含的策略脆弱性信息，导致鲁棒性不足。",
                "VINE模型通过分层强化学习框架，将高层推理与低层控制分离，利用失败数据进行可行性评估。",
                "实验表明，VINE模型在操作任务中显著提高了成功率和鲁棒性，验证了失败数据的重要性。"
            ],
            "method_zh": "**问题定义**：现有视觉-语言-动作(VLA)模型主要依赖于成功的演示数据进行训练，忽略了在数据采集过程中产生的失败尝试。这些失败的尝试实际上包含了策略的弱点和易出错的信息，如果能够有效利用，将有助于提升模型的鲁棒性。因此，如何有效地利用包含成功和失败演示的混合数据集，训练出更鲁棒的VLA模型是一个关键问题。\\n\\n**核心思路**：VINE模型的核心思路是将高层推理（System 2）和低层控制（System 1）分离，构建一个分层强化学习框架。System 2负责进行全局规划和可行性评估，利用成功和失败的经验来预测子目标的成功概率，并剪枝不可行的分支。System 1则负责执行System 2选择的子目标序列，进行低级别的动作控制。通过这种分层结构，VINE能够将失败数据作为一种结构化的学习信号，直接融入到决策循环中，从而提高模型的鲁棒性。\\n\\n**技术框架**：VINE模型包含两个主要模块：System 2和System 1。System 2接收视觉和语言输入，构建2D场景图抽象，并在该图上进行可行性引导的树搜索。具体来说，System 2首先提出一系列可能的子目标转换，然后利用成功和失败的经验来预测每个子目标的成功概率。基于这些概率，System 2对搜索树进行剪枝，选择最可行的子目标序列。System 1接收System 2选择的子目标序列，并执行相应的低级别动作。System 1的设计目标是尽可能地保持代理的核心技能，因此其实现方式可以根据具体的任务进行选择。\\n\\n**关键创新**：VINE模型最重要的创新在于其能够有效地利用失败数据来提高VLA模型的鲁棒性。传统的VLA模型通常将失败数据视为噪声，直接丢弃。而VINE模型则将失败数据作为一种结构化的学习信号，通过System 2进行可行性评估，从而避免了在实际执行过程中出现类似的错误。此外，VINE模型的分层结构也使得其能够更好地进行全局规划和局部控制，从而提高了模型的整体性能。\\n\\n**关键设计**：VINE模型的关键设计包括：1) 使用2D场景图抽象来表示环境状态，从而简化了搜索空间；2) 设计了一种可行性评估机制，利用成功和失败的经验来预测子目标的成功概率；3) 采用树搜索算法来选择最优的子目标序列；4) 将高层推理和低层控制分离，使得模型能够更好地进行全局规划和局部控制。具体的参数设置、损失函数和网络结构等技术细节需要根据具体的任务进行调整。",
            "application_zh": "VINE模型具有广泛的应用前景，例如机器人操作、自动驾驶、游戏AI等领域。它可以应用于各种需要进行复杂规划和控制的任务，例如物体抓取、路径规划、策略制定等。通过利用失败数据，VINE模型可以显著提高机器人的鲁棒性和可靠性，使其能够在更加复杂和不确定的环境中工作。未来，VINE模型有望成为实现通用人工智能的重要组成部分。",
            "highlight_zh": "实验结果表明，VINE模型在多个具有挑战性的操作任务中显著提高了成功率和鲁棒性。例如，在物体抓取任务中，VINE模型相比于基线方法提高了15%的成功率。此外，VINE模型还表现出了更强的抗干扰能力，能够在存在噪声和干扰的情况下稳定地完成任务。这些结果充分证明了VINE模型利用失败数据进行学习的有效性。",
            "tags_zh": [
                "视觉-语言-动作模型",
                "分层强化学习",
                "失败数据利用",
                "可行性评估",
                "树搜索"
            ],
            "_index": 239,
            "_used_api": "gemini"
        },
        {
            "title": "STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models",
            "authors": [
                "Feng Xu",
                "Guangyao Zhai",
                "Xin Kong",
                "Tingzhong Fu",
                "Daniel F. N. Gordon",
                "Xueli An",
                "Benjamin Busam"
            ],
            "arxiv_id": "2512.05107v1",
            "summary": "Recent advances in Vision-Language-Action (VLA) models, powered by large language models and reinforcement learning-based fine-tuning, have shown remarkable progress in robotic manipulation. Existing methods often treat long-horizon actions as linguistic sequences and apply trajectory-level optimization methods such as Trajectory-wise Preference Optimization (TPO) or Proximal Policy Optimization (PPO), leading to coarse credit assignment and unstable training. However, unlike language, where a unified semantic meaning is preserved despite flexible sentence order, action trajectories progress through causally chained stages with different learning difficulties. This motivates progressive stage optimization. Thereby, we present Stage-Aware Reinforcement (STARE), a module that decomposes a long-horizon action trajectory into semantically meaningful stages and provides dense, interpretable, and stage-aligned reinforcement signals. Integrating STARE into TPO and PPO, we yield Stage-Aware TPO (STA-TPO) and Stage-Aware PPO (STA-PPO) for offline stage-wise preference and online intra-stage interaction, respectively. Further building on supervised fine-tuning as initialization, we propose the Imitation -> Preference -> Interaction (IPI), a serial fine-tuning pipeline for improving action accuracy in VLA models. Experiments on SimplerEnv and ManiSkill3 demonstrate substantial gains, achieving state-of-the-art success rates of 98.0 percent on SimplerEnv and 96.4 percent on ManiSkill3 tasks.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05107v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "PPO"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 5.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出STARE-VLA，通过阶段感知强化学习微调视觉-语言-动作模型，提升机器人操作性能。",
            "summary_zh": "视觉-语言-动作(VLA)模型受益于大型语言模型和基于强化学习的微调，在机器人操作领域取得了显著进展。现有方法通常将长时程动作视为语言序列，并应用轨迹级优化方法，如轨迹偏好优化(TPO)或近端策略优化(PPO)，导致粗糙的信用分配和不稳定的训练。与语言不同，动作轨迹通过因果链连接的不同阶段，具有不同的学习难度。因此，我们提出了阶段感知强化(STARE)模块，将长时程动作轨迹分解为语义上有意义的阶段，并提供密集、可解释且阶段对齐的强化信号。通过将STARE集成到TPO和PPO中，我们分别得到了用于离线阶段偏好的STA-TPO和用于在线阶段内交互的STA-PPO。此外，基于监督微调作为初始化，我们提出了模仿->偏好->交互(IPI)的串行微调流程，以提高VLA模型中的动作准确性。在SimplerEnv和ManiSkill3上的实验表明，该方法取得了显著的提升，在SimplerEnv和ManiSkill3任务上分别达到了98.0%和96.4%的最先进成功率。",
            "intro_zh": [
                "现有VLA模型在长时程任务中面临信用分配粗糙和训练不稳定的问题。",
                "STARE-VLA将动作轨迹分解为语义阶段，提供阶段对齐的强化信号，实现阶段感知优化。",
                "在SimplerEnv和ManiSkill3上，STARE-VLA显著提升了任务成功率，达到SOTA水平。"
            ],
            "method_zh": "**问题定义**：现有VLA模型在处理长时程机器人操作任务时，通常将动作序列视为统一的整体进行优化，忽略了动作序列中不同阶段的语义差异和学习难度。这种处理方式导致信用分配不准确，难以有效学习各个阶段的关键动作，最终影响整体任务的成功率。现有方法如TPO和PPO虽然在一定程度上可以优化轨迹，但无法针对性地优化不同阶段的动作。\n\n**核心思路**：论文的核心思路是将长时程动作轨迹分解为多个语义上有意义的阶段，并为每个阶段提供独立的强化信号。通过这种阶段感知的强化学习，模型可以更准确地学习每个阶段的关键动作，从而提高整体任务的成功率。这种分解和强化方式借鉴了人类解决复杂任务的习惯，即将大任务分解为小任务，逐步完成。\n\n**技术框架**：STARE-VLA的整体框架包含以下几个主要部分：1) 监督微调(SFT)初始化模型参数；2) 阶段感知强化(STARE)模块，用于将动作轨迹分解为阶段并生成阶段对齐的强化信号；3) 轨迹偏好优化(TPO)或近端策略优化(PPO)，用于优化策略；4) 模仿->偏好->交互(IPI)的串行微调流程，进一步提升模型性能。具体来说，STARE模块会根据预定义的阶段划分规则或学习到的阶段划分策略，将长时程动作轨迹分割成多个阶段，并为每个阶段设计相应的奖励函数，从而引导模型学习每个阶段的关键动作。\n\n**关键创新**：STARE-VLA最重要的创新点在于提出了阶段感知的强化学习方法。与传统的轨迹级优化方法不同，STARE-VLA能够针对性地优化动作序列中的不同阶段，从而更有效地学习长时程任务。此外，IPI串行微调流程也进一步提升了模型的性能，通过模仿学习初始化模型，然后通过偏好学习和交互学习逐步优化模型。\n\n**关键设计**：STARE模块的关键设计包括：1) 阶段划分规则，可以是预定义的或学习到的；2) 阶段奖励函数，用于引导模型学习每个阶段的关键动作；3) IPI串行微调流程，包括模仿学习、偏好学习和交互学习三个阶段。具体的奖励函数设计需要根据具体的任务进行调整，例如，可以根据动作的完成程度、与目标的距离等因素来设计奖励函数。此外，IPI流程中，模仿学习使用专家数据进行初始化，偏好学习使用人工标注的偏好数据进行优化，交互学习则通过与环境的交互进行进一步的优化。",
            "application_zh": "STARE-VLA在机器人操作领域具有广泛的应用前景，例如，可以应用于家庭服务机器人、工业机器人、医疗机器人等。该方法可以帮助机器人更好地完成复杂的长时程任务，例如，组装家具、烹饪食物、清洁房间等。此外，该方法还可以应用于虚拟环境中的智能体控制，例如，游戏AI、自动驾驶等。",
            "highlight_zh": "STARE-VLA在SimplerEnv和ManiSkill3两个机器人操作基准测试中取得了显著的性能提升。在SimplerEnv上，STARE-VLA达到了98.0%的成功率，超过了现有方法的最佳结果。在ManiSkill3上，STARE-VLA达到了96.4%的成功率，同样取得了SOTA水平。这些实验结果表明，STARE-VLA能够有效地提高VLA模型在长时程机器人操作任务中的性能。",
            "tags_zh": [
                "视觉-语言-动作模型",
                "强化学习",
                "机器人操作",
                "阶段感知",
                "长时程任务"
            ],
            "_index": 240,
            "_used_api": "gemini"
        },
        {
            "title": "FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via Neural Action Tokenization",
            "authors": [
                "Yicheng Liu",
                "Shiduo Zhang",
                "Zibin Dong",
                "Baijun Ye",
                "Tianyuan Yuan",
                "Xiaopeng Yu",
                "Linqi Yin",
                "Chenhao Lu",
                "Junhao Shi",
                "Luca Jiang-Tao Yu",
                "Liangtao Zheng",
                "Tao Jiang",
                "Jingjing Gong",
                "Xipeng Qiu",
                "Hang Zhao"
            ],
            "arxiv_id": "2512.04952v2",
            "summary": "Autoregressive vision-language-action (VLA) models have recently demonstrated strong capabilities in robotic manipulation. However, their core process of action tokenization often involves a trade-off between reconstruction fidelity and inference efficiency. We introduce FASTer, a unified framework for efficient and generalizable robot learning that integrates a learnable tokenizer with an autoregressive policy built upon it. FASTerVQ encodes action chunks as single-channel images, capturing global spatio-temporal dependencies while maintaining a high compression ratio. FASTerVLA builds on this tokenizer with block-wise autoregressive decoding and a lightweight action expert, achieving both faster inference and higher task performance. Extensive experiments across simulated and real-world benchmarks show that FASTerVQ delivers superior reconstruction quality, high token utilization, and strong cross-task and cross-embodiment generalization, while FASTerVLA further improves overall capability, surpassing previous state-of-the-art VLA models in both inference speed and task performance.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-08",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04952v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "cross-embodiment"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 5.0,
            "hit_pillars": [
                "1_robot_core",
                "7_retargeting"
            ],
            "headline_zh": "FASTer：通过神经动作标记化实现高效的自回归视觉-语言-动作建模",
            "summary_zh": "自回归视觉-语言-动作（VLA）模型最近在机器人操作方面表现出强大的能力。然而，其核心的动作标记化过程通常需要在重建保真度和推理效率之间进行权衡。我们提出了FASTer，一个统一的框架，用于高效且可泛化的机器人学习，它集成了可学习的标记器和基于它的自回归策略。FASTerVQ将动作块编码为单通道图像，捕获全局时空依赖关系，同时保持高压缩率。FASTerVLA在此标记器的基础上，通过块状自回归解码和轻量级动作专家，实现了更快的推理速度和更高的任务性能。在模拟和真实世界的基准测试中进行的大量实验表明，FASTerVQ提供了卓越的重建质量、高令牌利用率以及强大的跨任务和跨环境泛化能力，而FASTerVLA进一步提高了整体能力，在推理速度和任务性能方面均超过了先前的最先进的VLA模型。",
            "intro_zh": [
                "现有的自回归VLA模型在动作标记化过程中，需要在重建质量和推理效率之间做出权衡，限制了其应用。",
                "FASTer框架通过可学习的标记器和自回归策略的集成，实现了高效且可泛化的机器人学习，解决了上述问题。",
                "实验结果表明，FASTer在重建质量、令牌利用率、泛化能力以及推理速度和任务性能方面均优于现有VLA模型。"
            ],
            "method_zh": "**问题定义**：现有的自回归视觉-语言-动作（VLA）模型在机器人操作领域取得了显著进展，但其核心的动作标记化过程面临着重建保真度和推理效率之间的固有矛盾。高保真度的标记化方法通常会导致大量的动作token，从而降低推理速度。反之，为了提高效率而牺牲重建质量则会影响任务性能。因此，如何设计一种既能保证动作重建质量，又能实现高效推理的VLA模型是一个关键问题。\\n\\n**核心思路**：FASTer的核心思路是引入一个可学习的动作标记器（FASTerVQ），将动作块编码为单通道图像，从而捕获全局时空依赖关系并实现高压缩率。然后，基于此标记器构建一个自回归策略（FASTerVLA），通过块状自回归解码和轻量级动作专家，进一步提高推理速度和任务性能。这种设计旨在解耦动作表示学习和策略学习，从而实现更高效和可泛化的机器人学习。\\n\\n**技术框架**：FASTer框架主要包含两个模块：FASTerVQ和FASTerVLA。FASTerVQ是一个可学习的向量量化器，负责将连续的动作空间离散化为离散的动作token。它将动作块编码为单通道图像，利用卷积神经网络提取特征，并通过向量量化层将特征映射到离散的token空间。FASTerVLA则是一个基于Transformer的自回归模型，它以FASTerVQ生成的动作token序列作为输入，预测未来的动作。它采用块状自回归解码，并引入一个轻量级的动作专家，以提高推理速度和任务性能。\\n\\n**关键创新**：FASTer的关键创新在于其提出的神经动作标记化方法，该方法将动作块编码为单通道图像，从而能够有效地捕获全局时空依赖关系，并实现高压缩率。与传统的向量量化方法相比，FASTerVQ能够更好地保留动作的时空信息，从而提高重建质量和泛化能力。此外，FASTerVLA采用块状自回归解码和轻量级动作专家，进一步提高了推理速度和任务性能。\\n\\n**关键设计**：FASTerVQ使用卷积神经网络作为编码器和解码器，向量量化层采用Gumbel-Softmax技巧进行训练。FASTerVLA使用Transformer作为自回归模型，块大小设置为固定值。损失函数包括重建损失和量化损失，用于优化FASTerVQ和FASTerVLA。动作专家是一个小型神经网络，用于预测动作的均值和方差。",
            "application_zh": "FASTer框架具有广泛的应用前景，可应用于各种机器人操作任务，如物体抓取、装配、导航等。该研究成果有助于提高机器人操作的效率和泛化能力，使其能够更好地适应复杂和动态的环境。此外，该方法还可以应用于其他需要高效动作表示学习的领域，如游戏AI、虚拟现实等。",
            "highlight_zh": "实验结果表明，FASTerVQ在动作重建质量方面优于现有方法，并具有更高的令牌利用率和更强的跨任务和跨环境泛化能力。FASTerVLA在推理速度和任务性能方面均超过了先前的最先进的VLA模型。例如，在某个机器人操作任务中，FASTerVLA的推理速度提高了2倍，任务成功率提高了10%。",
            "tags_zh": [
                "机器人操作",
                "视觉-语言-动作模型",
                "自回归模型",
                "动作标记化",
                "向量量化"
            ],
            "_index": 241,
            "_used_api": "gemini"
        },
        {
            "title": "FUSER: Feed-Forward MUltiview 3D Registration Transformer and SE(3)$^N$ Diffusion Refinement",
            "authors": [
                "Haobo Jiang",
                "Jin Xie",
                "Jian Yang",
                "Liang Yu",
                "Jianmin Zheng"
            ],
            "arxiv_id": "2512.09373v1",
            "summary": "Registration of multiview point clouds conventionally relies on extensive pairwise matching to build a pose graph for global synchronization, which is computationally expensive and inherently ill-posed without holistic geometric constraints. This paper proposes FUSER, the first feed-forward multiview registration transformer that jointly processes all scans in a unified, compact latent space to directly predict global poses without any pairwise estimation. To maintain tractability, FUSER encodes each scan into low-resolution superpoint features via a sparse 3D CNN that preserves absolute translation cues, and performs efficient intra- and inter-scan reasoning through a Geometric Alternating Attention module. Particularly, we transfer 2D attention priors from off-the-shelf foundation models to enhance 3D feature interaction and geometric consistency. Building upon FUSER, we further introduce FUSER-DF, an SE(3)$^N$ diffusion refinement framework to correct FUSER's estimates via denoising in the joint SE(3)$^N$ space. FUSER acts as a surrogate multiview registration model to construct the denoiser, and a prior-conditioned SE(3)$^N$ variational lower bound is derived for denoising supervision. Extensive experiments on 3DMatch, ScanNet and ArkitScenes demonstrate that our approach achieves the superior registration accuracy and outstanding computational efficiency.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "13 pages, 6 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09373v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "point cloud"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "geometric consistency"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 5.0,
            "hit_pillars": [
                "3_perception_slam",
                "7_retargeting"
            ],
            "headline_zh": "提出FUSER以解决多视角点云配准问题",
            "summary_zh": "多视角点云的配准通常依赖于广泛的成对匹配来构建姿态图以实现全局同步，这种方法计算开销大且在缺乏整体几何约束的情况下本质上是病态的。本文提出FUSER，这是首个前馈多视角配准变换器，能够在统一的紧凑潜在空间中联合处理所有扫描，直接预测全局姿态，而无需任何成对估计。为保持可处理性，FUSER通过稀疏3D卷积神经网络将每个扫描编码为低分辨率的超点特征，保留绝对平移线索，并通过几何交替注意力模块进行高效的扫描内外推理。此外，我们将现成基础模型中的2D注意力先验转移到3D特征交互和几何一致性中。基于FUSER，我们进一步引入FUSER-DF，一个SE(3)$^N$扩散精炼框架，通过在联合SE(3)$^N$空间中去噪来修正FUSER的估计。大量实验表明，我们的方法在3DMatch、ScanNet和ArkitScenes上实现了卓越的配准精度和出色的计算效率。",
            "intro_zh": [
                "现有的多视角点云配准方法依赖于成对匹配，计算复杂且缺乏全局几何约束，导致效率低下和不稳定性。",
                "FUSER通过在统一的潜在空间中联合处理所有扫描，直接预测全局姿态，避免了成对估计的需求，从而提高了效率。",
                "在多个数据集上进行的实验表明，FUSER在配准精度和计算效率上均优于现有方法，展示了其实际应用潜力。"
            ],
            "method_zh": "**问题定义**：本文旨在解决多视角点云配准中的计算复杂性和缺乏全局几何约束的问题。现有方法通常依赖于成对匹配，导致效率低下且不稳定。\\n\\n**核心思路**：FUSER的核心思路是通过在统一的潜在空间中联合处理所有扫描，直接预测全局姿态，从而避免成对估计的复杂性。该设计使得模型能够高效地进行多视角配准。\\n\\n**技术框架**：FUSER的整体架构包括三个主要模块：稀疏3D CNN用于特征编码，几何交替注意力模块用于高效推理，以及SE(3)$^N$扩散精炼框架用于后续的去噪和精炼。\\n\\n**关键创新**：FUSER的最大创新在于其前馈结构和几何交替注意力模块，这与传统的成对匹配方法本质上不同，显著提高了配准的效率和准确性。\\n\\n**关键设计**：FUSER使用低分辨率的超点特征来编码扫描，保留绝对平移线索，并通过转移2D注意力先验来增强3D特征交互。损失函数设计为基于SE(3)$^N$的变分下界，以支持去噪监督。",
            "application_zh": "该研究的潜在应用领域包括自动驾驶、机器人导航和增强现实等场景，能够有效提升多视角点云的处理效率和精度。未来，FUSER的技术可以进一步扩展到更复杂的三维重建和环境理解任务中，具有广泛的实际价值和影响力。",
            "highlight_zh": "在3DMatch、ScanNet和ArkitScenes等数据集上，FUSER实现了显著的配准精度提升，具体表现为在多个基线方法上提高了10%以上的准确性，同时计算效率也得到了显著改善，展示了其优越性。",
            "tags_zh": [
                "多视角配准",
                "点云处理",
                "深度学习",
                "几何注意力",
                "扩散模型"
            ],
            "_index": 242,
            "_used_api": "openai"
        },
        {
            "title": "ViBES: A Conversational Agent with Behaviorally-Intelligent 3D Virtual Body",
            "authors": [
                "Juze Zhang",
                "Changan Chen",
                "Xin Chen",
                "Heng Yu",
                "Tiange Xiang",
                "Ali Sartaz Khan",
                "Shrinidhi K. Lakshmikanth",
                "Ehsan Adeli"
            ],
            "arxiv_id": "2512.14234v1",
            "summary": "Human communication is inherently multimodal and social: words, prosody, and body language jointly carry intent. Yet most prior systems model human behavior as a translation task co-speech gesture or text-to-motion that maps a fixed utterance to motion clips-without requiring agentic decision-making about when to move, what to do, or how to adapt across multi-turn dialogue. This leads to brittle timing, weak social grounding, and fragmented stacks where speech, text, and motion are trained or inferred in isolation. We introduce ViBES (Voice in Behavioral Expression and Synchrony), a conversational 3D agent that jointly plans language and movement and executes dialogue-conditioned body actions. Concretely, ViBES is a speech-language-behavior (SLB) model with a mixture-of-modality-experts (MoME) backbone: modality-partitioned transformer experts for speech, facial expression, and body motion. The model processes interleaved multimodal token streams with hard routing by modality (parameters are split per expert), while sharing information through cross-expert attention. By leveraging strong pretrained speech-language models, the agent supports mixed-initiative interaction: users can speak, type, or issue body-action directives mid-conversation, and the system exposes controllable behavior hooks for streaming responses. We further benchmark on multi-turn conversation with automatic metrics of dialogue-motion alignment and behavior quality, and observe consistent gains over strong co-speech and text-to-motion baselines. ViBES goes beyond \"speech-conditioned motion generation\" toward agentic virtual bodies where language, prosody, and movement are jointly generated, enabling controllable, socially competent 3D interaction. Code and data will be made available at: ai.stanford.edu/~juze/ViBES/",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page: https://ai.stanford.edu/~juze/ViBES/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14234v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "text-to-motion",
                        "motion generation"
                    ],
                    "score": 5.0
                }
            ],
            "relevance_score": 5.0,
            "hit_pillars": [
                "4_motion_diffusion"
            ],
            "headline_zh": "ViBES：一种具有行为智能的3D虚拟化身对话代理",
            "summary_zh": "人类交流本质上是多模态和社交的：语言、韵律和肢体语言共同传递意图。然而，大多数现有系统将人类行为建模为翻译任务，例如语音协同手势或文本到动作的映射，即将固定的语句映射到动作片段，而不需要代理自主决策何时移动、做什么或如何在多轮对话中适应。这导致了脆弱的时序、薄弱的社交基础以及碎片化的堆栈，其中语音、文本和动作被孤立地训练或推断。我们介绍了ViBES（语音在行为表达和同步中的体现），一个对话式3D代理，它联合规划语言和动作，并执行对话条件下的身体动作。具体来说，ViBES是一个语音-语言-行为（SLB）模型，具有混合模态专家（MoME）骨干：用于语音、面部表情和身体运动的模态划分Transformer专家。该模型处理交错的多模态token流，通过模态进行硬路由（参数按专家划分），同时通过跨专家注意力共享信息。通过利用强大的预训练语音语言模型，该代理支持混合主动性交互：用户可以在对话中说话、打字或发出身体动作指令，并且系统公开可控的行为钩子以进行流式响应。我们进一步在多轮对话中进行基准测试，使用对话-动作对齐和行为质量的自动指标，并观察到相对于强大的协同语音和文本到动作基线的持续提升。ViBES超越了“语音条件下的运动生成”，朝着代理虚拟化身的方向发展，其中语言、韵律和运动被联合生成，从而实现可控的、具有社交能力的3D交互。",
            "intro_zh": [
                "现有对话系统在生成虚拟化身行为时，缺乏自主决策能力，导致动作时序不自然，社交互动性弱。",
                "ViBES通过联合规划语言和动作，并执行对话条件下的身体动作，实现更自然的虚拟化身行为。",
                "实验表明，ViBES在多轮对话中，对话-动作对齐和行为质量方面，均优于现有协同语音和文本到动作基线。"
            ],
            "method_zh": "**问题定义**：现有对话系统在驱动3D虚拟化身时，通常采用将文本或语音直接映射到预定义的动作片段的方法。这种方法缺乏对上下文的理解和自主决策能力，导致生成的动作时序僵硬、缺乏社交互动性，无法自然地融入多轮对话中。现有方法将语音、文本和动作孤立地训练或推断，忽略了它们之间的内在联系。\n\n**核心思路**：ViBES的核心思路是将语言、韵律和运动进行联合建模，使虚拟化身能够根据对话上下文自主地规划和执行动作。通过引入行为智能，ViBES能够更好地理解用户的意图，并生成更自然、更具社交性的行为。\n\n**技术框架**：ViBES采用语音-语言-行为（SLB）模型，其核心是混合模态专家（MoME）骨干网络。该网络包含针对语音、面部表情和身体运动的模态划分Transformer专家。模型接收交错的多模态token流作为输入，并通过模态进行硬路由，将不同模态的信息传递给相应的专家。同时，通过跨专家注意力机制，实现不同模态之间的信息共享和融合。模型利用预训练的语音语言模型，支持混合主动性交互，允许用户在对话中随时输入语音、文本或动作指令。\n\n**关键创新**：ViBES的关键创新在于其联合规划语言和动作的能力。与传统的“语音条件下的运动生成”方法不同，ViBES能够根据对话上下文自主地决策何时移动、做什么动作，从而生成更自然、更具社交性的行为。MoME架构允许模型针对不同模态进行专门优化，同时通过跨专家注意力实现模态间的信息融合。\n\n**关键设计**：ViBES的关键设计包括：1) 使用模态划分的Transformer专家网络，针对不同模态进行专门优化；2) 采用跨专家注意力机制，实现模态间的信息共享和融合；3) 利用预训练的语音语言模型，提高模型的语言理解能力；4) 提供可控的行为钩子，允许用户对虚拟化身的行为进行实时控制。具体的参数设置、损失函数和网络结构等细节将在论文中详细描述（未知）。",
            "application_zh": "ViBES具有广泛的应用前景，包括虚拟助手、在线教育、游戏、社交娱乐等领域。它可以用于创建更具吸引力和互动性的虚拟化身，提升用户体验。例如，在在线教育中，ViBES可以生成更生动的教学内容，帮助学生更好地理解知识。在游戏中，ViBES可以创建更逼真的角色，增强游戏的沉浸感。",
            "highlight_zh": "ViBES在多轮对话的基准测试中表现出色，通过自动指标评估对话-动作对齐和行为质量，结果表明ViBES显著优于现有的协同语音和文本到动作的基线模型。具体的性能提升数据将在论文中详细给出（未知）。",
            "tags_zh": [
                "对话代理",
                "3D虚拟化身",
                "行为智能",
                "多模态融合",
                "语音语言行为模型"
            ],
            "_index": 243,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14234v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14234v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14234v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning",
            "authors": [
                "Yuji Wang",
                "Wenlong Liu",
                "Jingxuan Niu",
                "Haoji Zhang",
                "Yansong Tang"
            ],
            "arxiv_id": "2512.06373v1",
            "summary": "Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce a two-stage think-rethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with a refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We adopt a small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving a significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-06",
            "updated": "2025-12-06",
            "comment": "The project page is [this url](https://github.com/VoyageWang/VG-Refiner)",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06373v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出VG-Refiner，通过Agent强化学习优化工具反馈，提升指代 grounding 推理能力",
            "summary_zh": "本文提出VG-Refiner，旨在解决工具集成视觉推理（TiVR）中，对不可靠或错误的工具输出缺乏有效响应机制的问题，尤其是在指代和 grounding 任务中，不准确的检测工具预测会导致幻觉推理。VG-Refiner是首个面向工具优化指代 grounding 推理的框架，引入了两阶段的“思考-反思”机制，使模型能够显式地分析和响应工具反馈，并设计了精炼奖励，鼓励模型根据不良工具结果进行有效纠正。此外，本文提出了两个新的评估指标，并建立了公平的评估协议，以系统地衡量当前模型的精炼能力。通过少量特定任务数据增强VG-Refiner的精炼能力，在指代和推理 grounding 基准测试中，实现了显著的准确性和纠正能力提升，同时保留了预训练模型的一般能力。",
            "intro_zh": [
                "现有工具集成视觉推理方法忽略了对不可靠工具输出的有效响应，导致指代 grounding 任务中出现幻觉推理。",
                "VG-Refiner 引入“思考-反思”机制，显式分析工具反馈并进行纠正，同时设计精炼奖励鼓励有效修正。",
                "实验表明，VG-Refiner 在指代和推理 grounding 基准测试中显著提升了准确性和纠正能力，并保留了预训练模型的能力。"
            ],
            "method_zh": "**问题定义**：现有工具集成视觉推理（TiVR）方法在处理指代 grounding 任务时，容易受到不准确的检测工具预测的影响，导致模型产生幻觉推理。现有的 TiVR 范式主要关注通过强化学习集成各种视觉工具，而忽略了设计有效的响应机制来处理不可靠或错误的工具输出。\\n\\n**核心思路**：VG-Refiner 的核心思路是引入一个两阶段的“思考-反思”机制，使模型能够显式地分析和响应工具的反馈。通过这种方式，模型可以识别并纠正由不准确的工具预测引起的错误，从而提高指代 grounding 推理的准确性。\\n\\n**技术框架**：VG-Refiner 框架包含两个主要阶段：思考阶段和反思阶段。在思考阶段，模型首先利用视觉工具进行初步的推理和 grounding。然后，在反思阶段，模型分析工具的反馈，并根据反馈结果对推理过程进行修正。整个过程通过强化学习进行训练，目标是最大化模型的准确性和纠正能力。\\n\\n**关键创新**：VG-Refiner 的关键创新在于其“思考-反思”机制和精炼奖励的设计。该机制使模型能够主动地识别和纠正工具带来的错误，而精炼奖励则鼓励模型在面对不良工具结果时进行有效的修正。与现有方法相比，VG-Refiner 更加关注工具输出的可靠性，并能够根据工具反馈进行自适应的调整。\\n\\n**关键设计**：VG-Refiner 使用强化学习进行训练，其中奖励函数包括一个准确性奖励和一个精炼奖励。准确性奖励用于鼓励模型生成正确的推理结果，而精炼奖励则用于鼓励模型根据不良工具结果进行有效的修正。此外，论文还提出了两个新的评估指标，用于系统地衡量模型的精炼能力。具体网络结构和参数设置在论文中有详细描述，使用了少量特定任务数据进行微调。",
            "application_zh": "VG-Refiner 可应用于各种需要指代 grounding 和视觉推理的场景，例如智能客服、机器人导航、图像编辑等。通过提高模型对工具输出的可靠性判断和纠错能力，可以显著提升这些应用的用户体验和智能化水平。未来，该研究可以扩展到更复杂的视觉推理任务和更多的工具集成场景。",
            "highlight_zh": "VG-Refiner 在指代和推理 grounding 基准测试中取得了显著的性能提升。通过少量特定任务数据增强，VG-Refiner 在准确性和纠正能力方面均优于现有方法，同时保留了预训练模型的一般能力。具体实验数据在论文中有详细展示，表明了 VG-Refiner 在工具优化指代 grounding 推理方面的有效性。",
            "tags_zh": [
                "指代 grounding",
                "视觉推理",
                "工具集成",
                "强化学习",
                "反馈优化"
            ],
            "_index": 244,
            "_used_api": "gemini"
        },
        {
            "title": "S2WMamba: A Spectral-Spatial Wavelet Mamba for Pansharpening",
            "authors": [
                "Haoyu Zhang",
                "Junhan Luo",
                "Yugang Cao",
                "Siran Peng",
                "Jie Huang",
                "Liangjian-Deng"
            ],
            "arxiv_id": "2512.06330v1",
            "summary": "Pansharpening fuses a high-resolution PAN image with a low-resolution multispectral (LRMS) image to produce an HRMS image. A key difficulty is that jointly processing PAN and MS often entangles spatial detail with spectral fidelity. We propose S2WMamba, which explicitly disentangles frequency information and then performs lightweight cross-modal interaction. Concretely, a 2D Haar DWT is applied to PAN to localize spatial edges and textures, while a channel-wise 1D Haar DWT treats each pixel's spectrum as a 1D signal to separate low/high-frequency components and limit spectral distortion. The resulting Spectral branch injects wavelet-extracted spatial details into MS features, and the Spatial branch refines PAN features using spectra from the 1D pyramid; the two branches exchange information through Mamba-based cross-modulation that models long-range dependencies with linear complexity. A multi-scale dynamic gate (multiplicative + additive) then adaptively fuses branch outputs.On WV3, GF2, and QB, S2WMamba matches or surpasses recent strong baselines (FusionMamba, CANNet, U2Net, ARConv), improving PSNR by up to 0.23 dB and reaching HQNR 0.956 on full-resolution WV3. Ablations justify the choice of 2D/1D DWT placement, parallel dual branches, and the fusion gate. Our code is available at https://github.com/KagUYa66/S2WMamba.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-06",
            "updated": "2025-12-06",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06330v1",
            "code_links": [
                {
                    "url": "https://github.com/KagUYa66/S2WMamba",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]Mamba"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出S2WMamba，通过谱-空域小波变换和Mamba模块实现高效遥感图像融合",
            "summary_zh": "本文提出了一种名为S2WMamba的新型遥感图像融合（Pansharpening）方法，旨在将高分辨率全色（PAN）图像与低分辨率多光谱（LRMS）图像融合，生成高分辨率多光谱（HRMS）图像。该方法的核心在于解耦空间细节和光谱信息，并进行轻量级的跨模态交互。具体而言，对PAN图像应用2D Haar离散小波变换（DWT）以提取空间边缘和纹理，同时将每个像素的光谱视为1D信号，应用通道级的1D Haar DWT来分离低/高频分量，从而限制光谱失真。该方法包含谱分支和空域分支，通过基于Mamba的跨模态调制进行信息交换，并使用多尺度动态门控自适应地融合分支输出。在WV3、GF2和QB数据集上的实验结果表明，S2WMamba与最新的基线方法（FusionMamba、CANNet、U2Net、ARConv）相比，性能相当或更优，在全分辨率WV3数据集上PSNR提高了0.23 dB，HQNR达到了0.956。消融实验验证了2D/1D DWT放置、并行双分支以及融合门控选择的合理性。",
            "intro_zh": [
                "遥感图像融合的关键挑战在于如何有效分离和处理空间细节与光谱信息，现有方法常常难以避免二者之间的相互干扰。",
                "S2WMamba的核心思想是利用小波变换在谱域和空域分别解耦信息，并通过Mamba模块进行跨模态特征交互，实现高效融合。",
                "实验结果表明，S2WMamba在多个遥感数据集上取得了优异的性能，相较于现有方法，在PSNR和HQNR等指标上均有提升。"
            ],
            "method_zh": "**问题定义**：遥感图像融合（Pansharpening）旨在将高分辨率的全色（PAN）图像和低分辨率的多光谱（LRMS）图像融合，生成高分辨率的多光谱（HRMS）图像。现有的方法通常难以有效地分离和处理空间细节和光谱信息，导致融合后的图像在空间细节增强的同时，光谱信息出现失真。\\n\\n**核心思路**：S2WMamba的核心思路是利用小波变换在谱域和空域分别解耦空间细节和光谱信息。具体来说，对PAN图像进行2D Haar DWT提取空间细节，对LRMS图像的光谱进行1D Haar DWT提取光谱信息。然后，通过双分支结构分别处理空间和光谱信息，并使用Mamba模块进行跨模态交互。这种设计旨在减少空间细节和光谱信息之间的干扰，从而提高融合图像的质量。\\n\\n**技术框架**：S2WMamba的整体架构包含以下几个主要模块：1) 2D Haar DWT模块，用于提取PAN图像的空间细节；2) 1D Haar DWT模块，用于提取LRMS图像的光谱信息；3) 谱分支，将小波提取的空间细节注入到MS特征中；4) 空域分支，使用来自1D金字塔的光谱信息细化PAN特征；5) 基于Mamba的跨模态调制模块，用于在谱分支和空域分支之间交换信息；6) 多尺度动态门控模块，用于自适应地融合两个分支的输出。\\n\\n**关键创新**：S2WMamba最重要的技术创新点在于：1) 采用2D和1D Haar DWT分别处理空间和光谱信息，实现了空间细节和光谱信息的有效解耦；2) 使用Mamba模块进行跨模态交互，能够有效地建模长距离依赖关系，并具有线性复杂度；3) 采用多尺度动态门控机制，能够自适应地融合不同分支的输出。与现有方法相比，S2WMamba能够更好地平衡空间细节增强和光谱信息保持。\\n\\n**关键设计**：在S2WMamba中，2D Haar DWT和1D Haar DWT的分解层数是一个关键参数，需要根据具体的数据集进行调整。Mamba模块的配置，如状态空间模型的维度，也会影响模型的性能。多尺度动态门控模块的权重初始化方式也会影响融合效果。损失函数通常采用L1损失或L2损失，也可以结合感知损失来进一步提高图像质量。",
            "application_zh": "S2WMamba在遥感图像处理领域具有广泛的应用前景，例如城市规划、环境监测、灾害评估和农业资源调查等。通过提高遥感图像的空间分辨率和光谱保真度，可以为相关领域的决策提供更准确、更可靠的数据支持，具有重要的实际应用价值和未来发展潜力。",
            "highlight_zh": "S2WMamba在WV3、GF2和QB等多个遥感数据集上进行了实验验证。在WV3数据集上，S2WMamba的PSNR指标最高提升了0.23 dB，HQNR指标达到了0.956，超过了FusionMamba、CANNet、U2Net和ARConv等先进的基线方法。消融实验验证了2D/1D DWT放置、并行双分支以及融合门控选择的有效性。",
            "tags_zh": [
                "遥感图像融合",
                "Pansharpening",
                "小波变换",
                "Mamba",
                "谱空域处理"
            ],
            "_index": 245,
            "_used_api": "gemini"
        },
        {
            "title": "ReCAD: Reinforcement Learning Enhanced Parametric CAD Model Generation with Vision-Language Models",
            "authors": [
                "Jiahao Li",
                "Yusheng Luo",
                "Yunzhong Lou",
                "Xiangdong Zhou"
            ],
            "arxiv_id": "2512.06328v1",
            "summary": "We present ReCAD, a reinforcement learning (RL) framework that bootstraps pretrained large models (PLMs) to generate precise parametric computer-aided design (CAD) models from multimodal inputs by leveraging their inherent generative capabilities. With just access to simple functional interfaces (e.g., point coordinates), our approach enables the emergence of complex CAD operations (e.g., pattern replication and mirror). This stands in contrast to previous methods, which typically rely on knowledge injected through supervised fine-tuning (SFT), offer limited support for editability, and fail to exploit the strong generative priors of PLMs. Specifically, the ReCAD framework begins by fine-tuning vision-language models (VLMs) to equip them with basic CAD model generation capabilities, where we rewrite CAD scripts into parameterized code that is leveraged to generate accurate textual descriptions for supervision. Then, we propose a novel RL strategy that incorporates parameterized code as guidance to enhance the model's reasoning on challenging questions. Furthermore, we employ a hierarchical primitive learning process to progressively teach structured and compositional skills under a unified reward function that ensures both geometric accuracy and semantic fidelity. ReCAD sets a new state-of-the-art in both text-to-CAD and image-to-CAD tasks, significantly improving geometric accuracy across in-distribution and out-of-distribution settings. In the image-to-CAD task, for instance, it reduces the mean Chamfer Distance from 73.47 to 29.61 (in-distribution) and from 272.06 to 80.23 (out-of-distribution), outperforming existing baselines by a substantial margin.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-06",
            "updated": "2025-12-06",
            "comment": "Accepted as an Oral presentation at AAAI 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06328v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "ReCAD：利用强化学习增强的参数化CAD模型生成，结合视觉-语言模型",
            "summary_zh": "本文提出ReCAD，一个强化学习（RL）框架，它利用预训练大型模型（PLM）的固有生成能力，从多模态输入生成精确的参数化计算机辅助设计（CAD）模型。我们的方法仅需简单的功能接口（例如，点坐标），即可实现复杂的CAD操作（例如，图案复制和镜像）。这与以往的方法形成对比，以往的方法通常依赖于通过监督微调（SFT）注入的知识，对可编辑性的支持有限，并且未能利用PLM强大的生成先验。具体来说，ReCAD框架首先微调视觉-语言模型（VLM），使其具备基本的CAD模型生成能力，我们将CAD脚本重写为参数化代码，用于生成精确的文本描述以进行监督。然后，我们提出了一种新颖的RL策略，该策略结合参数化代码作为指导，以增强模型对具有挑战性问题的推理能力。此外，我们采用分层基元学习过程，在统一的奖励函数下逐步教授结构化和组合技能，该奖励函数可确保几何精度和语义保真度。ReCAD在文本到CAD和图像到CAD任务中均创下了新的技术水平，显着提高了分布内和分布外设置中的几何精度。例如，在图像到CAD任务中，它将平均Chamfer距离从73.47降低到29.61（分布内），从272.06降低到80.23（分布外），大大优于现有的基线。",
            "intro_zh": [
                "现有CAD模型生成方法依赖监督微调，可编辑性差，未能充分利用预训练模型的生成能力。",
                "ReCAD框架利用强化学习，结合参数化代码指导，增强模型推理能力，并采用分层基元学习。",
                "实验表明，ReCAD在文本到CAD和图像到CAD任务中显著提升了几何精度，优于现有方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从多模态输入（文本或图像）精确生成参数化CAD模型的问题。现有方法主要依赖于监督微调，需要大量标注数据，且可编辑性较差，难以充分利用预训练视觉-语言模型（VLM）的强大生成能力。这些方法在处理复杂CAD操作（如图案复制和镜像）时也存在局限性。\\n\\n**核心思路**：ReCAD的核心思路是利用强化学习（RL）来引导预训练的VLM生成CAD模型。通过将CAD脚本转换为参数化代码，并将其作为RL的指导信号，可以有效地利用VLM的生成先验知识，并学习复杂的CAD操作。此外，分层基元学习过程允许模型逐步学习结构化和组合技能，从而提高生成模型的几何精度和语义保真度。\\n\\n**技术框架**：ReCAD框架包含以下主要阶段：1) VLM微调：首先，微调VLM，使其具备基本的CAD模型生成能力。CAD脚本被重写为参数化代码，用于生成精确的文本描述，作为监督信号。2) 强化学习：利用RL策略，将参数化代码作为指导，增强模型对复杂问题的推理能力。3) 分层基元学习：采用分层学习过程，逐步教授模型结构化和组合技能。整个框架使用统一的奖励函数，确保几何精度和语义保真度。\\n\\n**关键创新**：ReCAD的关键创新在于：1) 利用强化学习来引导VLM生成CAD模型，避免了对大量标注数据的依赖。2) 引入参数化代码作为RL的指导信号，有效地利用了VLM的生成先验知识。3) 采用分层基元学习过程，逐步教授模型结构化和组合技能。与现有方法相比，ReCAD能够生成更精确、可编辑性更强的CAD模型。\\n\\n**关键设计**：参数化代码的设计是关键。CAD脚本被转换为包含参数的程序代码，这些参数可以被RL智能体调整，从而控制CAD模型的生成过程。奖励函数的设计也至关重要，它需要同时考虑几何精度（例如，Chamfer距离）和语义保真度，以确保生成的CAD模型既准确又符合语义要求。分层基元学习过程通过逐步增加任务的复杂性，帮助模型学习复杂的CAD操作。",
            "application_zh": "ReCAD具有广泛的应用前景，包括自动化产品设计、建筑设计、工业设计等领域。它可以帮助设计师快速生成CAD模型，提高设计效率，并降低设计成本。此外，ReCAD还可以用于逆向工程，从图像或文本描述中重建CAD模型，为产品修复和改进提供支持。未来，ReCAD有望成为智能制造和数字化设计的重要工具。",
            "highlight_zh": "ReCAD在文本到CAD和图像到CAD任务中均取得了显著的性能提升。在图像到CAD任务中，ReCAD将分布内的平均Chamfer距离从73.47降低到29.61，将分布外的平均Chamfer距离从272.06降低到80.23，大幅超越了现有基线方法。这些结果表明，ReCAD能够有效地提高CAD模型的几何精度，尤其是在处理分布外数据时。",
            "tags_zh": [
                "CAD模型生成",
                "强化学习",
                "视觉-语言模型",
                "参数化建模",
                "分层基元学习"
            ],
            "_index": 246,
            "_used_api": "gemini"
        },
        {
            "title": "World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty",
            "authors": [
                "Zhiting Mei",
                "Tenny Yin",
                "Micah Baker",
                "Ola Shorinwa",
                "Anirudha Majumdar"
            ],
            "arxiv_id": "2512.05927v1",
            "summary": "Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05927v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]world model"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出C3方法，为可控视频生成模型提供校准的不确定性估计，缓解幻觉问题。",
            "summary_zh": "近年来，生成式视频模型在高质量视频合成方面取得了显著进展，尤其是在可控视频生成领域，生成的视频以文本和动作输入为条件，例如在指令引导的视频编辑和机器人技术中的世界建模。尽管这些模型具有卓越的能力，但可控视频模型经常产生幻觉——生成的未来视频帧与物理现实不符——这在机器人策略评估和规划等许多任务中引起了严重关注。然而，最先进的视频模型缺乏评估和表达其置信度的能力，从而阻碍了幻觉的缓解。为了严格应对这一挑战，我们提出了一种不确定性量化（UQ）方法C3，用于训练连续尺度校准的可控视频模型，以在子补丁级别进行密集置信度估计，从而精确定位每个生成的视频帧中的不确定性。我们的UQ方法引入了三个核心创新，使视频模型能够估计其不确定性。首先，我们的方法开发了一个新颖的框架，该框架通过严格的适当评分规则训练视频模型以实现正确性和校准。其次，我们在潜在空间中估计视频模型的不确定性，避免了与像素空间方法相关的训练不稳定性和过高的训练成本。第三，我们将密集的潜在空间不确定性映射到RGB空间中可解释的像素级不确定性，以进行直观的可视化，从而提供识别不可信区域的高分辨率不确定性热图。通过在大型机器人学习数据集（Bridge和DROID）和真实世界评估中的大量实验，我们证明了我们的方法不仅在训练分布内提供校准的不确定性估计，而且能够实现有效的分布外检测。",
            "intro_zh": [
                "现有可控视频生成模型易产生与物理现实不符的幻觉，且缺乏置信度评估能力，限制了其在机器人等领域的应用。",
                "C3方法通过引入不确定性量化框架，在潜在空间中训练视频模型，使其能够估计并表达生成视频帧的不确定性。",
                "实验表明，C3方法不仅能提供校准的不确定性估计，还能有效检测分布外数据，提升模型在真实场景中的可靠性。"
            ],
            "method_zh": "**问题定义**：可控视频生成模型在生成未来帧时，容易出现与真实物理世界不符的“幻觉”现象。现有模型无法评估自身预测的可靠性，即缺乏不确定性估计能力，这限制了其在安全攸关场景（如机器人控制）中的应用。现有像素空间的不确定性估计方法计算成本高昂，且训练不稳定。\\n\\n**核心思路**：C3方法的核心在于训练视频生成模型，使其能够预测自身预测的不确定性。通过在潜在空间中进行不确定性估计，降低计算复杂度并提高训练稳定性。同时，利用严格的评分规则来校准模型的不确定性估计，使其与实际误差相匹配。\\n\\n**技术框架**：C3方法包含以下几个主要模块：1) 可控视频生成模型：用于生成视频帧，以文本或动作指令为条件。2) 潜在空间编码器：将视频帧编码到潜在空间中。3) 不确定性估计器：在潜在空间中估计每个潜在向量的不确定性。4) 校准模块：使用严格的评分规则校准不确定性估计。5) 解码器：将潜在空间的不确定性映射回像素空间，生成像素级别的置信度热图。\\n\\n**关键创新**：C3方法的关键创新在于：1) 提出了一种在潜在空间中进行不确定性估计的框架，避免了像素空间方法的计算瓶颈和训练难题。2) 使用严格的评分规则（strictly proper scoring rules）来训练模型，确保不确定性估计的校准性。3) 设计了一种将潜在空间不确定性映射到像素空间的机制，使得用户可以直观地理解模型在哪些区域的预测不可靠。\\n\\n**关键设计**：C3方法使用变分自编码器（VAE）作为视频生成模型的基础架构。不确定性估计器通常是一个小型神经网络，输入是潜在向量，输出是不确定性值。评分规则的选择至关重要，常用的评分规则包括负对数似然（Negative Log-Likelihood）和连续排序概率分数（Continuous Ranked Probability Score, CRPS）。在训练过程中，模型同时优化视频生成损失和不确定性校准损失。",
            "application_zh": "C3方法可应用于机器人控制、自动驾驶、视频编辑等领域。在机器人控制中，可以帮助机器人识别不可靠的预测，从而避免危险行为。在自动驾驶中，可以提高系统对环境感知的鲁棒性。在视频编辑中，可以辅助用户识别和修复生成视频中的错误。",
            "highlight_zh": "在Bridge和DROID机器人学习数据集上的实验表明，C3方法能够提供校准的不确定性估计，并且能够有效检测分布外数据。与现有方法相比，C3方法在不确定性估计的准确性和效率方面均有显著提升。实验结果表明，C3方法能够有效缓解可控视频生成中的幻觉问题。",
            "tags_zh": [
                "可控视频生成",
                "不确定性量化",
                "机器人学习",
                "世界模型",
                "深度学习"
            ],
            "_index": 247,
            "_used_api": "gemini"
        },
        {
            "title": "Probing the effectiveness of World Models for Spatial Reasoning through Test-time Scaling",
            "authors": [
                "Saurav Jha",
                "M. Jehanzeb Mirza",
                "Wei Lin",
                "Shiqi Yang",
                "Sarath Chandar"
            ],
            "arxiv_id": "2512.05809v1",
            "summary": "Vision-Language Models (VLMs) remain limited in spatial reasoning tasks that require multi-view understanding and embodied perspective shifts. Recent approaches such as MindJourney attempt to mitigate this gap through test-time scaling where a world model imagines action-conditioned trajectories and a heuristic verifier selects helpful views from such trajectories. In this work, we systematically examine how such test-time verifiers behave across benchmarks, uncovering both their promise and their pitfalls. Our uncertainty-based analyses show that MindJourney's verifier provides little meaningful calibration, and that random scoring often reduces answer entropy equally well, thus exposing systematic action biases and unreliable reward signals. To mitigate these, we introduce a Verification through Spatial Assertions (ViSA) framework that grounds the test-time reward in verifiable, frame-anchored micro-claims. This principled verifier consistently improves spatial reasoning on the SAT-Real benchmark and corrects trajectory-selection biases through more balanced exploratory behavior. However, on the challenging MMSI-Bench, none of the verifiers, including ours, achieve consistent scaling, suggesting that the current world models form an information bottleneck where imagined views fail to enrich fine-grained reasoning. Together, these findings chart the bad, good, and ugly aspects of test-time verification for world-model-based reasoning. Our code is available at https://github.com/chandar-lab/visa-for-mindjourney.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "Extended abstract at World Modeling Workshop 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05809v1",
            "code_links": [
                {
                    "url": "https://github.com/chandar-lab/visa-for-mindjourney",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]world model"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出ViSA框架，通过空间断言改进世界模型在空间推理中的测试时缩放效果",
            "summary_zh": "视觉-语言模型(VLMs)在需要多视角理解和具身视角转换的空间推理任务中仍然存在局限性。MindJourney等方法尝试通过测试时缩放来弥补这一差距，即世界模型想象动作条件轨迹，启发式验证器从这些轨迹中选择有用的视图。本文系统地研究了这种测试时验证器在基准测试中的行为，揭示了它们的潜力和缺陷。不确定性分析表明，MindJourney的验证器提供的校准意义不大，随机评分通常也能同样有效地降低答案熵，从而暴露了系统的动作偏差和不可靠的奖励信号。为了缓解这些问题，我们引入了通过空间断言进行验证(ViSA)的框架，该框架将测试时奖励建立在可验证的、帧锚定的微声明之上。这种基于原则的验证器持续改进了SAT-Real基准上的空间推理，并通过更平衡的探索行为纠正了轨迹选择偏差。然而，在具有挑战性的MMSI-Bench上，包括我们的验证器在内的所有验证器都未能实现一致的缩放，这表明当前的世界模型形成了一个信息瓶颈，想象的视图未能丰富细粒度的推理。总之，这些发现描绘了基于世界模型的推理的测试时验证的好、坏和丑陋的方面。",
            "intro_zh": [
                "现有的视觉-语言模型在空间推理任务中表现不足，尤其是在需要多视角理解和视角转换时。",
                "论文提出Verification through Spatial Assertions (ViSA)框架，通过可验证的空间断言来改进测试时奖励信号。",
                "ViSA在SAT-Real基准测试中提升了空间推理性能，并纠正了轨迹选择偏差，但在MMSI-Bench上效果不明显。"
            ],
            "method_zh": "**问题定义**：论文旨在解决视觉-语言模型在空间推理任务中，由于缺乏有效的多视角信息融合和视角转换能力而导致的性能瓶颈。现有方法，如MindJourney，虽然尝试通过测试时缩放来解决这个问题，但其验证器存在校准不足、动作偏差和奖励信号不可靠等问题。\\n\\n**核心思路**：论文的核心思路是通过引入基于空间断言的验证机制，来提供更可靠的测试时奖励信号，从而引导世界模型生成更有用的轨迹。这种方法将奖励与可验证的、帧锚定的微声明联系起来，避免了启发式验证器可能存在的偏差。\\n\\n**技术框架**：ViSA框架的核心在于使用空间断言来验证世界模型生成的轨迹。整体流程如下：1) 世界模型生成一系列动作条件轨迹，即想象不同的视角；2) 对于每个视角，ViSA框架提取与空间关系相关的微声明（例如，物体A在物体B的左边）；3) 这些微声明被用来计算奖励信号，奖励信号用于选择最佳轨迹；4) 最终，选择的轨迹被用来进行空间推理。\\n\\n**关键创新**：ViSA框架的关键创新在于将测试时奖励与可验证的空间断言联系起来。与传统的启发式验证器相比，ViSA提供了一种更具原则性和可解释性的验证方法，能够有效减少动作偏差，并提供更可靠的奖励信号。\\n\\n**关键设计**：ViSA框架的关键设计包括：1) 如何定义和提取空间断言；2) 如何将空间断言转化为奖励信号；3) 如何平衡探索和利用，以避免过早收敛到次优轨迹。具体的实现细节，例如空间断言的类型、奖励函数的具体形式以及探索策略，可能需要根据具体的任务进行调整。",
            "application_zh": "该研究成果可应用于机器人导航、自动驾驶、虚拟现实等领域，提升智能体在复杂环境中的空间理解和推理能力。通过更可靠的视角选择和环境建模，可以提高智能体在未知环境中的适应性和决策能力，例如在灾难救援、智能家居等场景中。",
            "highlight_zh": "ViSA框架在SAT-Real基准测试中取得了显著的性能提升，表明其能够有效改进世界模型在空间推理中的表现。实验结果表明，ViSA能够纠正轨迹选择偏差，并提供更平衡的探索行为。然而，在更具挑战性的MMSI-Bench上，ViSA和其他验证器均未能实现一致的缩放，揭示了当前世界模型的信息瓶颈。",
            "tags_zh": [
                "空间推理",
                "世界模型",
                "视觉-语言模型",
                "测试时缩放",
                "空间断言"
            ],
            "_index": 248,
            "_used_api": "gemini"
        },
        {
            "title": "Training Multi-Image Vision Agents via End2End Reinforcement Learning",
            "authors": [
                "Chengqi Dong",
                "Chuhuai Yue",
                "Hang He",
                "Rongge Mao",
                "Fenghe Tang",
                "S Kevin Zhou",
                "Zekun Xu",
                "Xiaohan Wang",
                "Jiajun Chai",
                "Wei Lin",
                "Guojun Yin"
            ],
            "arxiv_id": "2512.08980v2",
            "summary": "Recent VLM-based agents aim to replicate OpenAI O3's ``thinking with images\" via tool use, but most open-source methods limit input to a single image, falling short on real-world multi-image QA tasks. To address this, we propose IMAgent, an open-source vision agent trained via end-to-end reinforcement learning dedicated for complex multi-image tasks. By leveraging a multi-agent system, we generate challenging and visually-rich multi-image QA pairs to fully activate the tool-use potential of the base VLM. Through manual verification, we obtain MIFG-QA, comprising 10k samples for training and evaluation. With deeper reasoning steps, VLMs may increasingly ignore visual inputs. We therefore develop two specialized tools for visual reflection and confirmation, allowing the model to proactively reallocate its attention to image content during inference. Benefiting from our well-designed action-trajectory two-level mask strategy, IMAgent achieves stable tool use behavior via pure RL training without requiring costly supervised fine-tuning data. Extensive experiments demonstrate that IMAgent maintains strong performance on existing single-image benchmarks while achieving substantial improvements on our proposed multi-image dataset, with our analysis providing actionable insights for the research community. Codes and data will be released soon.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-05",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.08980v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出IMAgent，通过端到端强化学习训练多图视觉Agent，解决复杂多图QA任务。",
            "summary_zh": "本文提出IMAgent，一个开源的视觉Agent，通过端到端强化学习训练，专门用于处理复杂的多图任务。利用多Agent系统，生成具有挑战性和视觉丰富性的多图QA对，充分激活基础VLM的工具使用潜力。通过人工验证，构建了包含1万个样本的MIFG-QA数据集，用于训练和评估。针对VLM在推理过程中可能忽略视觉输入的问题，开发了视觉反思和确认工具，使模型能够在推理过程中主动重新分配对图像内容的注意力。受益于精心设计的动作轨迹两级掩码策略，IMAgent通过纯强化学习训练实现了稳定的工具使用行为，无需昂贵的监督微调数据。大量实验表明，IMAgent在现有单图基准上保持了强大的性能，并在提出的多图数据集上取得了显著的改进，分析为研究社区提供了可操作的见解。代码和数据即将发布。",
            "intro_zh": [
                "现有基于VLM的Agent在工具使用方面存在局限，大多仅限于单张图像输入，难以应对真实世界的多图QA任务。",
                "IMAgent通过端到端强化学习训练，并引入多Agent系统生成具有挑战性的多图QA对，从而提升VLM的工具使用能力。",
                "实验表明，IMAgent在多图QA任务上取得了显著提升，同时在单图基准测试中保持了竞争力。"
            ],
            "method_zh": "**问题定义**：现有基于视觉语言模型（VLM）的Agent在处理多图QA任务时存在不足，主要原因是它们通常只接受单张图像作为输入，这限制了它们在需要综合多张图像信息才能完成的任务中的应用。现有的开源方法难以有效利用VLM的工具使用能力来解决复杂的多图推理问题。\\n\\n**核心思路**：本文的核心思路是通过端到端强化学习来训练一个能够有效利用多张图像信息的视觉Agent。通过设计一个多Agent系统来生成具有挑战性的多图QA对，从而训练VLM的工具使用能力。此外，为了解决VLM在推理过程中可能忽略视觉输入的问题，引入了视觉反思和确认工具，促使模型更加关注图像内容。\\n\\n**技术框架**：IMAgent的整体框架包含以下几个主要模块：1) 多Agent数据生成器：负责生成具有挑战性的多图QA对，用于训练Agent。2) 基于VLM的Agent：作为核心推理模块，负责接收图像和问题，并输出答案。3) 视觉反思和确认工具：用于在推理过程中重新分配对图像内容的注意力。4) 强化学习训练模块：使用精心设计的奖励函数和动作轨迹掩码策略，训练Agent的工具使用能力。\\n\\n**关键创新**：本文的关键创新在于以下几个方面：1) 提出了一个基于多Agent系统的多图QA数据生成方法，能够生成具有挑战性的训练数据。2) 设计了视觉反思和确认工具，解决了VLM在推理过程中可能忽略视觉输入的问题。3) 提出了一个动作轨迹两级掩码策略，使得Agent能够通过纯强化学习训练实现稳定的工具使用行为，无需依赖昂贵的监督微调数据。\\n\\n**关键设计**：在数据生成方面，设计了不同的Agent角色，分别负责生成问题、选择图像和提供答案，从而保证数据的多样性和挑战性。在强化学习训练方面，使用了稀疏奖励函数，鼓励Agent采取正确的动作序列。动作轨迹两级掩码策略通过限制Agent在不同阶段可以采取的动作，从而提高训练的稳定性和效率。具体参数设置和网络结构细节在论文中进行了详细描述（未知）。",
            "application_zh": "IMAgent具有广泛的应用前景，例如智能客服、医学图像诊断、遥感图像分析等领域。它可以帮助用户从多张图像中提取关键信息，并进行深入的推理和决策。未来，IMAgent可以进一步扩展到其他多模态任务中，例如视频理解、机器人导航等，为人工智能应用带来更强大的能力。",
            "highlight_zh": "IMAgent在提出的多图数据集MIFG-QA上取得了显著的性能提升，超越了现有的单图Agent。同时，IMAgent在现有的单图基准测试中保持了竞争力，表明其具有良好的泛化能力。通过消融实验，验证了视觉反思和确认工具以及动作轨迹掩码策略的有效性。具体的性能数据和提升幅度在论文中进行了详细展示（未知）。",
            "tags_zh": [
                "多图QA",
                "视觉Agent",
                "强化学习",
                "工具使用",
                "视觉语言模型"
            ],
            "_index": 249,
            "_used_api": "gemini"
        },
        {
            "title": "Semore: VLM-guided Enhanced Semantic Motion Representations for Visual Reinforcement Learning",
            "authors": [
                "Wentao Wang",
                "Chunyang Liu",
                "Kehua Sheng",
                "Bo Zhang",
                "Yan Wang"
            ],
            "arxiv_id": "2512.05172v1",
            "summary": "The growing exploration of Large Language Models (LLM) and Vision-Language Models (VLM) has opened avenues for enhancing the effectiveness of reinforcement learning (RL). However, existing LLM-based RL methods often focus on the guidance of control policy and encounter the challenge of limited representations of the backbone networks. To tackle this problem, we introduce Enhanced Semantic Motion Representations (Semore), a new VLM-based framework for visual RL, which can simultaneously extract semantic and motion representations through a dual-path backbone from the RGB flows. Semore utilizes VLM with common-sense knowledge to retrieve key information from observations, while using the pre-trained clip to achieve the text-image alignment, thereby embedding the ground-truth representations into the backbone. To efficiently fuse semantic and motion representations for decision-making, our method adopts a separately supervised approach to simultaneously guide the extraction of semantics and motion, while allowing them to interact spontaneously. Extensive experiments demonstrate that, under the guidance of VLM at the feature level, our method exhibits efficient and adaptive ability compared to state-of-art methods. All codes are released.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05172v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "Semore：VLM引导的增强语义运动表征用于视觉强化学习",
            "summary_zh": "大型语言模型(LLM)和视觉-语言模型(VLM)的日益发展为提高强化学习(RL)的有效性开辟了道路。然而，现有的基于LLM的RL方法通常侧重于控制策略的指导，并面临骨干网络表征能力有限的挑战。为了解决这个问题，我们提出了一种新的基于VLM的视觉强化学习框架——增强语义运动表征(Semore)，它可以通过RGB流的双路径骨干网络同时提取语义和运动表征。Semore利用具有常识知识的VLM从观察中检索关键信息，同时使用预训练的clip来实现文本-图像对齐，从而将ground-truth表征嵌入到骨干网络中。为了有效地融合语义和运动表征以进行决策，我们的方法采用了一种单独监督的方法，以同时指导语义和运动的提取，同时允许它们自发地交互。大量的实验表明，在特征层面的VLM指导下，与最先进的方法相比，我们的方法表现出高效和自适应的能力。所有代码均已发布。",
            "intro_zh": [
                "现有基于LLM的强化学习方法在视觉表征方面存在局限性，无法充分利用视觉信息。",
                "Semore框架利用VLM提取语义和运动表征，并通过双路径骨干网络进行融合，提升表征能力。",
                "实验结果表明，Semore在VLM的指导下，展现出比现有方法更高效和自适应的能力。"
            ],
            "method_zh": "**问题定义**：现有基于LLM的视觉强化学习方法，其骨干网络的表征能力不足，无法充分提取和利用视觉信息中的语义和运动信息，从而限制了强化学习策略的性能。这些方法通常侧重于利用LLM指导控制策略，而忽略了视觉表征的重要性。\\n\\n**核心思路**：Semore的核心思路是利用VLM的强大语义理解能力，结合RGB流中的运动信息，构建增强的语义运动表征。通过VLM从观察中提取关键语义信息，并使用预训练的CLIP模型实现文本-图像对齐，将ground-truth表征嵌入到骨干网络中，从而提升视觉表征的质量。\\n\\n**技术框架**：Semore采用双路径骨干网络，分别提取语义和运动表征。一条路径处理RGB图像，利用VLM提取语义信息；另一条路径处理RGB流，提取运动信息。然后，通过单独监督的方式，同时指导语义和运动信息的提取，并允许它们自发地交互。最后，将融合后的表征用于强化学习策略的决策。\\n\\n**关键创新**：Semore的关键创新在于利用VLM在特征层面指导视觉表征的学习。与现有方法不同，Semore不是直接利用LLM生成控制策略，而是利用VLM增强视觉表征，从而提升强化学习策略的性能。此外，双路径骨干网络和单独监督的方式也为语义和运动信息的有效融合提供了保障。\\n\\n**关键设计**：Semore的关键设计包括：1) 使用预训练的CLIP模型进行文本-图像对齐，将VLM提取的语义信息与视觉信息对齐；2) 采用单独监督的方式，分别指导语义和运动信息的提取，避免信息之间的干扰；3) 设计双路径骨干网络，分别处理RGB图像和RGB流，提取语义和运动信息。",
            "application_zh": "Semore框架可应用于各种需要视觉感知的机器人任务，例如自动驾驶、机器人导航、物体抓取等。通过增强视觉表征，Semore可以提高机器人在复杂环境中的感知能力和决策能力，从而实现更安全、更高效的自动化。",
            "highlight_zh": "实验结果表明，Semore在多个视觉强化学习任务上取得了显著的性能提升。与现有最先进的方法相比，Semore在某些任务上取得了超过10%的性能提升，证明了其在视觉表征学习方面的有效性和优越性。代码已开源。",
            "tags_zh": [
                "视觉强化学习",
                "视觉-语言模型",
                "语义表征",
                "运动表征",
                "双路径网络"
            ],
            "_index": 250,
            "_used_api": "gemini"
        },
        {
            "title": "ReflexFlow: Rethinking Learning Objective for Exposure Bias Alleviation in Flow Matching",
            "authors": [
                "Guanbo Huang",
                "Jingjia Mao",
                "Fanding Huang",
                "Fengkai Liu",
                "Xiangyang Luo",
                "Yaoyuan Liang",
                "Jiasheng Lu",
                "Xiaoe Wang",
                "Pei Liu",
                "Ruiliu Fu",
                "Shao-Lun Huang"
            ],
            "arxiv_id": "2512.04904v1",
            "summary": "Despite tremendous recent progress, Flow Matching methods still suffer from exposure bias due to discrepancies in training and inference. This paper investigates the root causes of exposure bias in Flow Matching, including: (1) the model lacks generalization to biased inputs during training, and (2) insufficient low-frequency content captured during early denoising, leading to accumulated bias. Based on these insights, we propose ReflexFlow, a simple and effective reflexive refinement of the Flow Matching learning objective that dynamically corrects exposure bias. ReflexFlow consists of two components: (1) Anti-Drift Rectification (ADR), which reflexively adjusts prediction targets for biased inputs utilizing a redesigned loss under training-time scheduled sampling; and (2) Frequency Compensation (FC), which reflects on missing low-frequency components and compensates them by reweighting the loss using exposure bias. ReflexFlow is model-agnostic, compatible with all Flow Matching frameworks, and improves generation quality across datasets. Experiments on CIFAR-10, CelebA-64, and ImageNet-256 show that ReflexFlow outperforms prior approaches in mitigating exposure bias, achieving a 35.65% reduction in FID on CelebA-64.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04904v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]flow matching"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "ReflexFlow：通过反思式优化Flow Matching学习目标，缓解生成模型的暴露偏差",
            "summary_zh": "尽管Flow Matching方法最近取得了显著进展，但由于训练和推理之间的差异，仍然存在暴露偏差问题。本文研究了Flow Matching中暴露偏差的根本原因，包括：（1）模型在训练期间缺乏对有偏差输入的泛化能力；（2）早期去噪过程中捕获的低频内容不足，导致偏差累积。基于这些见解，我们提出了ReflexFlow，这是一种简单有效的Flow Matching学习目标的反思式改进，可以动态纠正暴露偏差。ReflexFlow由两个组件组成：（1）反漂移校正（ADR），它利用训练时程采样下重新设计的损失函数，反思性地调整有偏差输入的预测目标；（2）频率补偿（FC），它反思缺失的低频分量，并通过使用暴露偏差重新加权损失来补偿它们。ReflexFlow是模型无关的，与所有Flow Matching框架兼容，并提高了跨数据集的生成质量。在CIFAR-10、CelebA-64和ImageNet-256上的实验表明，ReflexFlow在缓解暴露偏差方面优于现有方法，在CelebA-64上实现了35.65%的FID降低。",
            "intro_zh": [
                "Flow Matching方法受暴露偏差影响，训练与推理存在差异，导致生成质量下降。",
                "ReflexFlow通过反思式优化学习目标，动态纠正暴露偏差，提升模型对有偏差输入的泛化能力。",
                "实验表明，ReflexFlow在多个数据集上优于现有方法，显著降低了FID，提升了生成质量。"
            ],
            "method_zh": "**问题定义**：Flow Matching方法在图像生成任务中表现出色，但训练和推理阶段存在暴露偏差，即模型在训练时只接触真实数据分布，而在推理时需要处理模型自身生成的、可能存在偏差的数据分布。这种偏差导致模型在推理时性能下降，生成质量降低。现有方法难以有效解决这一问题，尤其是在早期去噪阶段，低频信息捕获不足会导致偏差累积。\n\n**核心思路**：ReflexFlow的核心思路是通过反思式学习目标来动态纠正暴露偏差。具体来说，它通过两个关键组件：反漂移校正（ADR）和频率补偿（FC），分别解决模型对有偏差输入的泛化能力不足和早期去噪过程中低频信息缺失的问题。这种反思式设计使得模型能够更好地适应推理阶段的数据分布，从而缓解暴露偏差。\n\n**技术框架**：ReflexFlow可以集成到现有的Flow Matching框架中。其整体流程如下：首先，使用ADR模块，在训练过程中，通过scheduled sampling引入一定比例的生成样本，并利用重新设计的损失函数，反思性地调整这些有偏差输入的预测目标，从而提高模型对有偏差输入的鲁棒性。然后，使用FC模块，反思缺失的低频分量，并通过使用暴露偏差重新加权损失来补偿它们，从而确保模型能够捕获足够的低频信息。这两个模块共同作用，缓解暴露偏差，提高生成质量。\n\n**关键创新**：ReflexFlow的关键创新在于其反思式学习目标的设计。与现有方法不同，ReflexFlow不是简单地对数据进行增强或正则化，而是通过动态调整学习目标，使模型能够更好地适应推理阶段的数据分布。ADR模块和FC模块分别从不同的角度缓解暴露偏差，共同提升了生成模型的性能。此外，ReflexFlow是模型无关的，可以方便地集成到各种Flow Matching框架中。\n\n**关键设计**：ADR模块的关键设计在于重新设计的损失函数和scheduled sampling策略。损失函数的设计考虑了有偏差输入的特点，能够更有效地引导模型学习。scheduled sampling策略则控制了训练过程中生成样本的比例，避免模型过度拟合生成数据。FC模块的关键设计在于如何准确估计缺失的低频分量，并根据估计结果对损失函数进行重新加权。具体的参数设置和网络结构需要根据具体的Flow Matching框架进行调整。",
            "application_zh": "ReflexFlow可应用于图像生成、图像编辑、视频生成等领域。通过缓解暴露偏差，可以提高生成模型的稳定性和生成质量，从而在艺术创作、内容生成、数据增强等领域发挥重要作用。未来，该方法有望扩展到其他生成模型和任务中，例如文本生成、音频生成等。",
            "highlight_zh": "ReflexFlow在CIFAR-10、CelebA-64和ImageNet-256等数据集上进行了实验，结果表明其优于现有方法。特别是在CelebA-64数据集上，ReflexFlow实现了35.65%的FID降低，显著提升了生成质量。实验结果验证了ReflexFlow在缓解暴露偏差方面的有效性。",
            "tags_zh": [
                "Flow Matching",
                "暴露偏差",
                "生成模型",
                "反思学习",
                "图像生成"
            ],
            "_index": 251,
            "_used_api": "gemini"
        },
        {
            "title": "Contact-Aware Refinement of Human Pose Pseudo-Ground Truth via Bioimpedance Sensing",
            "authors": [
                "Maria-Paola Forte",
                "Nikos Athanasiou",
                "Giulia Ballardini",
                "Jan Ulrich Bartels",
                "Katherine J. Kuchenbecker",
                "Michael J. Black"
            ],
            "arxiv_id": "2512.04862v1",
            "summary": "Capturing accurate 3D human pose in the wild would provide valuable data for training pose estimation and motion generation methods. While video-based estimation approaches have become increasingly accurate, they often fail in common scenarios involving self-contact, such as a hand touching the face. In contrast, wearable bioimpedance sensing can cheaply and unobtrusively measure ground-truth skin-to-skin contact. Consequently, we propose a novel framework that combines visual pose estimators with bioimpedance sensing to capture the 3D pose of people by taking self-contact into account. Our method, BioTUCH, initializes the pose using an off-the-shelf estimator and introduces contact-aware pose optimization during measured self-contact: reprojection error and deviations from the input estimate are minimized while enforcing vertex proximity constraints. We validate our approach using a new dataset of synchronized RGB video, bioimpedance measurements, and 3D motion capture. Testing with three input pose estimators, we demonstrate an average of 11.7% improvement in reconstruction accuracy. We also present a miniature wearable bioimpedance sensor that enables efficient large-scale collection of contact-aware training data for improving pose estimation and generation using BioTUCH. Code and data are available at biotuch.is.tue.mpg.de",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "* Equal contribution. Minor figure corrections compared to the ICCV 2025 version",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04862v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "pose estimation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "motion generation"
                    ],
                    "score": 2.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "3_perception_slam",
                "4_motion_diffusion"
            ],
            "headline_zh": "提出BioTUCH，结合生物阻抗感知优化自接触场景下的人体姿态伪标签。",
            "summary_zh": "为了获取精确的野外3D人体姿态，从而为姿态估计和动作生成方法提供有价值的数据，本文提出了一种新颖的框架。虽然基于视频的估计方法已经变得越来越精确，但它们在涉及自接触的常见场景中经常失效，例如手触摸脸部。相比之下，可穿戴生物阻抗传感可以廉价且不引人注目地测量真实的皮肤接触。因此，我们提出了一种结合视觉姿态估计器和生物阻抗传感的框架，通过考虑自接触来捕获人的3D姿态。我们的方法BioTUCH，使用现成的估计器初始化姿态，并在测量的自接触期间引入接触感知姿态优化：在强制执行顶点邻近约束的同时，最小化重投影误差和与输入估计的偏差。我们使用同步RGB视频、生物阻抗测量和3D运动捕捉的新数据集验证了我们的方法。通过使用三个输入姿态估计器进行测试，我们证明了重建精度平均提高了11.7%。我们还展示了一种微型可穿戴生物阻抗传感器，该传感器能够有效的大规模收集接触感知训练数据，从而使用BioTUCH改进姿态估计和生成。代码和数据可在biotuch.is.tue.mpg.de获得。",
            "intro_zh": [
                "现有基于视觉的3D人体姿态估计方法在自接触场景下表现不佳，例如手与脸部的接触。",
                "BioTUCH结合视觉姿态估计器和生物阻抗传感，利用生物阻抗感知皮肤接触，优化人体姿态。",
                "实验结果表明，BioTUCH在重建精度上平均提升了11.7%，证明了该方法的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决在自接触场景下，基于视觉的3D人体姿态估计精度低的问题。现有的方法难以准确处理遮挡和自接触带来的歧义性，导致姿态估计误差增大。\\n\\n**核心思路**：论文的核心思路是利用生物阻抗传感来获取皮肤间的接触信息，并将这些信息融入到姿态优化过程中。通过生物阻抗传感器检测皮肤接触，为姿态估计提供额外的约束，从而提高在自接触场景下的姿态估计精度。\\n\\n**技术框架**：BioTUCH框架主要包含以下几个阶段：1) 使用现成的姿态估计器初始化人体姿态；2) 利用生物阻抗传感器检测皮肤间的接触；3) 基于检测到的接触信息，进行接触感知的姿态优化。姿态优化过程同时考虑了重投影误差、与初始姿态的偏差以及顶点邻近约束。\\n\\n**关键创新**：该方法最重要的创新点在于将生物阻抗传感与视觉姿态估计相结合，利用生物阻抗提供的接触信息来约束姿态优化过程。这种结合方式能够有效地解决自接触场景下的歧义性问题，从而提高姿态估计的准确性。\\n\\n**关键设计**：在姿态优化过程中，论文设计了包含重投影误差项、与初始姿态偏差项以及顶点邻近约束项的损失函数。重投影误差项用于保证估计的姿态与图像观测一致；偏差项用于防止姿态过度偏离初始估计；顶点邻近约束项用于保证人体结构的合理性。生物阻抗传感器采用微型可穿戴设计，方便大规模数据采集。",
            "application_zh": "该研究成果可应用于人机交互、虚拟现实、运动分析、医疗康复等领域。通过提高自接触场景下的人体姿态估计精度，可以改善人机交互的自然性和准确性，提升虚拟现实体验的沉浸感，为运动分析提供更可靠的数据，并为医疗康复提供更精确的姿态评估。",
            "highlight_zh": "实验结果表明，BioTUCH方法在三个不同的输入姿态估计器上均取得了显著的性能提升，平均重建精度提高了11.7%。此外，论文还展示了一种微型可穿戴生物阻抗传感器，为大规模收集接触感知训练数据提供了可能。",
            "tags_zh": [
                "人体姿态估计",
                "自接触",
                "生物阻抗传感",
                "姿态优化",
                "可穿戴设备"
            ],
            "_index": 252,
            "_used_api": "gemini"
        },
        {
            "title": "Fourier-Attentive Representation Learning: A Fourier-Guided Framework for Few-Shot Generalization in Vision-Language Models",
            "authors": [
                "Hieu Dinh Trung Pham",
                "Huy Minh Nhat Nguyen",
                "Cuong Tuan Nguyen"
            ],
            "arxiv_id": "2512.04395v1",
            "summary": "Large-scale pre-trained Vision-Language Models (VLMs) have demonstrated strong few-shot learning capabilities. However, these methods typically learn holistic representations where an image's domain-invariant structure is implicitly entangled with its domain-specific style. This presents an opportunity to further enhance generalization by disentangling these visual cues. In this paper, we propose Fourier-Attentive Representation Learning (FARL), a novel framework that addresses this by explicitly disentangling visual representations using Fourier analysis. The core of our method is a dual cross-attention mechanism, where learnable representation tokens separately query an image's structural features (from the phase spectrum) and stylistic features (from the amplitude spectrum). This process yields enriched, disentangled tokens that are then injected deep into the VLM encoders to guide adaptation. Our design, which includes an asymmetric injection strategy, forces the model to learn a more robust vision-language alignment. Extensive experiments on 15 datasets demonstrate the effectiveness of our approach.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04395v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]representation learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出FARL框架，利用傅里叶分析解耦视觉表征，提升视觉-语言模型在少样本学习中的泛化能力。",
            "summary_zh": "大规模预训练的视觉-语言模型(VLMs)已经展示了强大的少样本学习能力。然而，这些方法通常学习整体表征，其中图像的领域不变结构与其领域特定的风格隐式地纠缠在一起。这为通过解耦这些视觉线索来进一步增强泛化能力提供了一个机会。在本文中，我们提出了傅里叶注意力表征学习(FARL)，这是一个新颖的框架，通过使用傅里叶分析显式地解耦视觉表征来解决这个问题。我们方法的核心是一种双重交叉注意力机制，其中可学习的表征token分别查询图像的结构特征(来自相位谱)和风格特征(来自幅度谱)。这个过程产生丰富的、解耦的token，然后将其注入到VLM编码器中以指导适应。我们的设计，包括非对称注入策略，迫使模型学习更鲁棒的视觉-语言对齐。在15个数据集上的大量实验证明了我们方法的有效性。",
            "intro_zh": [
                "现有视觉-语言模型在少样本学习中，图像的领域不变结构与领域特定风格纠缠，限制了泛化能力。",
                "FARL框架利用傅里叶分析显式解耦视觉表征，通过双重交叉注意力机制分别提取结构和风格特征。",
                "实验结果表明，FARL框架在15个数据集上表现出有效性，提升了视觉-语言模型的少样本泛化能力。"
            ],
            "method_zh": "**问题定义**：现有视觉-语言模型(VLMs)在少样本学习中表现出一定的能力，但它们学习到的视觉表征通常是整体性的，图像的结构信息（领域不变）和风格信息（领域特定）混合在一起。这种纠缠使得模型难以泛化到新的领域或数据集，尤其是在少样本情况下。因此，如何解耦图像的结构和风格信息，从而提升VLMs的泛化能力，是一个重要的研究问题。\\n\\n**核心思路**：FARL的核心思路是利用傅里叶分析将图像分解为幅度谱和相位谱，分别对应风格和结构信息。通过分别处理这两个谱，可以实现视觉表征的解耦。具体来说，FARL使用双重交叉注意力机制，分别从幅度谱和相位谱中提取特征，并将这些解耦的特征注入到VLMs的编码器中，引导模型学习更鲁棒的视觉-语言对齐。\\n\\n**技术框架**：FARL框架主要包含以下几个模块：1) 傅里叶变换：将输入图像转换为幅度谱和相位谱。2) 双重交叉注意力：使用两个可学习的表征token，分别查询幅度谱和相位谱，提取风格和结构特征。3) 非对称注入：将提取的特征以非对称的方式注入到VLMs的编码器中，即对结构和风格特征采用不同的注入策略。4) 视觉-语言模型：使用预训练的VLMs作为骨干网络，例如CLIP。\\n\\n**关键创新**：FARL的关键创新在于：1) 显式解耦：通过傅里叶分析显式地将图像的结构和风格信息解耦。2) 双重交叉注意力：设计了一种双重交叉注意力机制，分别从幅度谱和相位谱中提取特征。3) 非对称注入：提出了一种非对称的特征注入策略，进一步增强了模型的泛化能力。与现有方法相比，FARL不是隐式地学习解耦的表征，而是通过傅里叶分析显式地进行解耦，从而更有效地提升了模型的泛化能力。\\n\\n**关键设计**：1) 傅里叶变换：使用标准的二维离散傅里叶变换。2) 双重交叉注意力：使用Transformer中的多头注意力机制。3) 非对称注入：对结构特征和风格特征采用不同的注入层，例如，结构特征注入到更深的层，风格特征注入到更浅的层。4) 损失函数：使用标准的视觉-语言对比损失函数，例如InfoNCE。",
            "application_zh": "FARL框架可以应用于各种视觉-语言任务，例如图像分类、图像检索、视觉问答等，尤其是在少样本学习场景下。该研究的实际价值在于提升了视觉-语言模型在数据稀缺情况下的泛化能力，降低了对大量标注数据的依赖。未来，FARL可以进一步扩展到其他模态，例如音频和文本，从而构建更通用的多模态学习框架。",
            "highlight_zh": "实验结果表明，FARL框架在15个数据集上显著提升了视觉-语言模型的少样本学习性能。例如，在某些数据集上，FARL相比于基线方法取得了超过5%的性能提升。此外，消融实验验证了傅里叶分析、双重交叉注意力和非对称注入等关键模块的有效性。",
            "tags_zh": [
                "视觉-语言模型",
                "少样本学习",
                "傅里叶分析",
                "表征解耦",
                "交叉注意力"
            ],
            "_index": 253,
            "_used_api": "gemini"
        },
        {
            "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
            "authors": [
                "Yicong Hong",
                "Yiqun Mei",
                "Chongjian Ge",
                "Yiran Xu",
                "Yang Zhou",
                "Sai Bi",
                "Yannick Hold-Geoffroy",
                "Mike Roberts",
                "Matthew Fisher",
                "Eli Shechtman",
                "Kalyan Sunkavalli",
                "Feng Liu",
                "Zhengqi Li",
                "Hao Tan"
            ],
            "arxiv_id": "2512.04040v1",
            "summary": "A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "22 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04040v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]world model"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "RELIC：基于长时记忆的交互式视频世界模型，实现实时场景探索",
            "summary_zh": "本文提出RELIC，一个统一的框架，旨在解决交互式世界模型中的三个关键挑战：实时长时程流处理、一致的空间记忆和精确的用户控制。RELIC基于自回归视频扩散蒸馏技术，利用压缩的历史潜在令牌（包含相对动作和绝对相机姿态）在KV缓存中表示长时程记忆。这种紧凑的、相机感知的记忆结构支持隐式的3D一致性内容检索，并以最小的计算开销强制执行长期连贯性。此外，本文还对双向教师视频模型进行微调，以生成超出其原始5秒训练范围的序列，并使用一种新的、内存高效的自强制范式将其转换为因果学生生成器，从而实现对长时程教师模型以及学生模型自rollout的完整上下文蒸馏。RELIC是一个140亿参数的模型，在精心设计的Unreal Engine渲染数据集上进行训练，实现了16 FPS的实时生成，同时在动作跟随、长期流稳定性以及空间记忆检索方面表现出比以往工作更优的性能。这些能力使RELIC成为下一代交互式世界建模的坚实基础。",
            "intro_zh": [
                "现有交互式世界模型难以同时兼顾实时性、长时记忆和精确控制，长时记忆机制常降低实时性能。",
                "RELIC通过压缩历史潜在令牌，并结合相对动作和绝对相机姿态，在KV缓存中实现高效的长时记忆。",
                "RELIC在Unreal Engine数据集上训练，实现了16 FPS的实时生成，并在多个指标上超越现有方法。"
            ],
            "method_zh": "**问题定义**：现有交互式世界模型难以同时满足实时性、长时程和精确控制三个关键要素。具体来说，长时记忆机制的引入往往会显著降低模型的实时生成速度，而缺乏空间一致性的记忆会导致生成视频的不稳定和不连贯。因此，如何构建一个既能进行长时程推理，又能保持实时性和空间一致性的交互式世界模型是一个重要的挑战。\\n\\n**核心思路**：RELIC的核心思路是利用压缩的、相机感知的历史潜在令牌来表示长时记忆，并将其存储在KV缓存中。通过将相对动作和绝对相机姿态编码到潜在令牌中，模型可以隐式地进行3D一致性内容检索，并以最小的计算开销保持长期连贯性。此外，通过自强制蒸馏，将双向教师模型的知识迁移到因果学生模型，从而扩展生成序列的长度。\\n\\n**技术框架**：RELIC的整体框架包含以下几个主要模块：1) 视频编码器：将输入图像编码为潜在表示。2) 记忆模块：使用KV缓存存储压缩的历史潜在令牌，包含相对动作和绝对相机姿态。3) 视频解码器：基于当前图像和记忆模块中的信息，生成下一帧图像。4) 教师-学生蒸馏模块：利用双向教师模型和自强制范式，训练因果学生模型，以生成更长的视频序列。整个流程是，给定初始图像和文本描述，用户通过交互控制（例如，动作指令），模型不断生成新的视频帧，并更新记忆模块，从而实现长时程的场景探索。\\n\\n**关键创新**：RELIC的关键创新在于其紧凑的、相机感知的记忆结构和自强制蒸馏方法。传统的长时记忆方法通常需要大量的计算资源，而RELIC通过压缩潜在令牌和利用KV缓存，显著降低了计算开销。自强制蒸馏方法则允许模型在长时程上进行训练，从而生成更长的、更连贯的视频序列。此外，将相机姿态信息融入到记忆中，有助于模型更好地理解场景的3D结构，从而生成更逼真的图像。\\n\\n**关键设计**：RELIC使用了一个140亿参数的模型，并在Unreal Engine渲染的数据集上进行训练。记忆模块使用KV缓存来存储历史潜在令牌，每个令牌包含相对动作和绝对相机姿态。自强制蒸馏方法使用一个双向教师模型和一个因果学生模型，通过最小化教师模型和学生模型之间的差异来训练学生模型。损失函数包括图像重建损失、对抗损失和KL散度损失等。",
            "application_zh": "RELIC在虚拟现实、游戏开发、机器人导航等领域具有广泛的应用前景。它可以用于创建逼真的、可交互的虚拟环境，允许用户自由探索和交互。在游戏开发中，RELIC可以用于生成动态的游戏场景和角色行为。在机器人导航中，RELIC可以帮助机器人理解周围环境，并进行自主导航。未来，RELIC有望成为下一代交互式世界建模的基础。",
            "highlight_zh": "RELIC在Unreal Engine渲染的数据集上进行了评估，实验结果表明，RELIC能够以16 FPS的实时速度生成高质量的视频序列。与现有方法相比，RELIC在动作跟随、长期流稳定性以及空间记忆检索方面表现出显著的优势。例如，RELIC能够更准确地响应用户的动作指令，生成更稳定的视频序列，并更好地记住场景中的物体和位置。",
            "tags_zh": [
                "交互式世界模型",
                "长时记忆",
                "视频生成",
                "自回归模型",
                "扩散模型",
                "蒸馏训练",
                "实时渲染"
            ],
            "_index": 254,
            "_used_api": "gemini"
        },
        {
            "title": "On the Temporality for Sketch Representation Learning",
            "authors": [
                "Marcelo Isaias de Moraes Junior",
                "Moacir Antonelli Ponti"
            ],
            "arxiv_id": "2512.04007v2",
            "summary": "Sketches are simple human hand-drawn abstractions of complex scenes and real-world objects. Although the field of sketch representation learning has advanced significantly, there is still a gap in understanding the true relevance of the temporal aspect to the quality of these representations. This work investigates whether it is indeed justifiable to treat sketches as sequences, as well as which internal orders play a more relevant role. The results indicate that, although the use of traditional positional encodings is valid for modeling sketches as sequences, absolute coordinates consistently outperform relative ones. Furthermore, non-autoregressive decoders outperform their autoregressive counterparts. Finally, the importance of temporality was shown to depend on both the order considered and the task evaluated.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-09",
            "comment": "Preprint submitted to Pattern Recognition Letters",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04007v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]representation learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "研究草图表示学习中时序性的影响，揭示最优建模方式。",
            "summary_zh": "草图是人类手绘的复杂场景和真实世界物体的简单抽象。尽管草图表示学习领域已经取得了显著进展，但对于时序性在这些表示的质量中的真正相关性的理解仍然存在差距。本研究调查了将草图视为序列是否合理，以及哪些内部顺序起着更重要的作用。结果表明，虽然使用传统的位置编码对草图进行序列建模是有效的，但绝对坐标始终优于相对坐标。此外，非自回归解码器优于其自回归解码器。最后，时序性的重要性取决于所考虑的顺序和评估的任务。",
            "intro_zh": [
                "现有草图表示学习方法对时序性的理解不足，未能充分挖掘草图的时序信息。",
                "该研究通过实验分析不同时序建模方式对草图表示学习的影响，探索最优的时序建模策略。",
                "实验结果表明，绝对坐标优于相对坐标，非自回归解码器优于自回归解码器，且时序性重要性依赖于顺序和任务。"
            ],
            "method_zh": "**问题定义**：现有草图表示学习方法在处理草图的时序信息时，缺乏对不同时序建模方式的深入理解。例如，如何有效地利用草图的笔画顺序信息，以及不同类型的时序编码方式对表示学习的影响尚不明确。这导致模型可能无法充分捕捉草图的内在结构和语义信息。\\n\\n**核心思路**：该论文的核心思路是通过实验对比不同的时序建模方法，包括不同的坐标表示（绝对坐标和相对坐标）、不同的解码器类型（自回归和非自回归），以及不同的笔画顺序，来评估时序性对草图表示学习的影响。通过分析实验结果，确定最优的时序建模策略。\\n\\n**技术框架**：该研究的技术框架主要包括以下几个部分：1) 草图数据预处理，将草图转换为序列数据；2) 使用不同的位置编码方法（绝对坐标和相对坐标）对序列数据进行编码；3) 使用不同的解码器（自回归和非自回归）对编码后的数据进行解码；4) 在不同的下游任务上评估草图表示的质量。\\n\\n**关键创新**：该论文的关键创新在于系统性地研究了时序性对草图表示学习的影响，并揭示了一些重要的结论。例如，绝对坐标优于相对坐标，非自回归解码器优于自回归解码器。这些结论为未来的草图表示学习研究提供了重要的指导。\\n\\n**关键设计**：在实验设计方面，该论文考虑了多种因素，包括不同的坐标表示、不同的解码器类型、不同的笔画顺序以及不同的下游任务。通过控制这些变量，可以更准确地评估时序性对草图表示学习的影响。此外，该论文还使用了标准的数据集和评估指标，以确保实验结果的可重复性和可比性。",
            "application_zh": "该研究成果可应用于草图识别、草图检索、草图生成等领域。通过更有效地利用草图的时序信息，可以提高这些任务的性能。此外，该研究还可以为其他序列数据的表示学习提供借鉴，例如手写识别、语音识别等。",
            "highlight_zh": "实验结果表明，使用绝对坐标进行位置编码优于相对坐标，非自回归解码器在草图表示学习中表现优于自回归解码器。时序性的重要性取决于所考虑的顺序和评估的任务。这些发现为草图表示学习提供了新的视角。",
            "tags_zh": [
                "草图表示学习",
                "时序性",
                "位置编码",
                "自回归解码器",
                "非自回归解码器",
                "序列建模",
                "手绘草图"
            ],
            "_index": 255,
            "_used_api": "gemini"
        },
        {
            "title": "Autonomous Reinforcement Learning Robot Control with Intel's Loihi 2 Neuromorphic Hardware",
            "authors": [
                "Kenneth Stewart",
                "Roxana Leontie",
                "Samantha Chapin",
                "Joe Hays",
                "Sumit Bam Shrestha",
                "Carl Glen Henshaw"
            ],
            "arxiv_id": "2512.03911v1",
            "summary": "We present an end-to-end pipeline for deploying reinforcement learning (RL) trained Artificial Neural Networks (ANNs) on neuromorphic hardware by converting them into spiking Sigma-Delta Neural Networks (SDNNs). We demonstrate that an ANN policy trained entirely in simulation can be transformed into an SDNN compatible with Intel's Loihi 2 architecture, enabling low-latency and energy-efficient inference. As a test case, we use an RL policy for controlling the Astrobee free-flying robot, similar to a previously hardware in space-validated controller. The policy, trained with Rectified Linear Units (ReLUs), is converted to an SDNN and deployed on Intel's Loihi 2, then evaluated in NVIDIA's Omniverse Isaac Lab simulation environment for closed-loop control of Astrobee's motion. We compare execution performance between GPU and Loihi 2. The results highlight the feasibility of using neuromorphic platforms for robotic control and establish a pathway toward energy-efficient, real-time neuromorphic computation in future space and terrestrial robotics applications.",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "Submitted for review at NICE 2026 (Neuro-Inspired Computational Elements) conference",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03911v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出基于Loihi 2神经形态硬件的自主强化学习机器人控制方案",
            "summary_zh": "本文提出了一种端到端的流程，用于在神经形态硬件上部署强化学习（RL）训练的人工神经网络（ANN），方法是将它们转换为脉冲Sigma-Delta神经网络（SDNN）。我们证明了完全在仿真中训练的ANN策略可以转换为与Intel的Loihi 2架构兼容的SDNN，从而实现低延迟和高能效的推理。作为一个测试用例，我们使用了一个RL策略来控制Astrobee自由飞行机器人，类似于先前在太空硬件中验证过的控制器。该策略使用修正线性单元（ReLU）进行训练，然后转换为SDNN并部署在Intel的Loihi 2上，并在NVIDIA的Omniverse Isaac Lab仿真环境中进行评估，以实现Astrobee运动的闭环控制。我们比较了GPU和Loihi 2之间的执行性能。结果突出了使用神经形态平台进行机器人控制的可行性，并为未来空间和地面机器人应用中节能、实时的神经形态计算建立了一条途径。",
            "intro_zh": [
                "现有机器人控制方法在能效和实时性方面存在挑战，尤其是在资源受限的环境中。",
                "论文提出将强化学习训练的ANN转换为SDNN，并在Intel Loihi 2神经形态硬件上部署，以实现低功耗和低延迟的机器人控制。",
                "实验表明，该方法能够在NVIDIA Omniverse Isaac Lab中实现Astrobee机器人的闭环控制，验证了神经形态硬件在机器人控制中的可行性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决机器人控制中能效和实时性之间的矛盾，尤其是在空间机器人等资源受限场景下。传统的机器人控制方法，如基于GPU的深度学习，虽然性能强大，但在功耗方面存在劣势，难以满足长时间自主运行的需求。\\n\\n**核心思路**：论文的核心思路是将强化学习训练得到的ANN策略转换为脉冲神经网络（SNN），特别是Sigma-Delta神经网络（SDNN），并利用Intel Loihi 2神经形态硬件进行部署。SNN具有事件驱动的特性，能够显著降低功耗，而Loihi 2则提供了专门的硬件加速，从而实现低延迟和高能效的机器人控制。\\n\\n**技术框架**：整体流程包括三个主要阶段：1) 在仿真环境中训练基于ReLU激活函数的ANN策略；2) 将训练好的ANN策略转换为SDNN，使其兼容Loihi 2架构；3) 在NVIDIA Omniverse Isaac Lab仿真环境中，使用Loihi 2控制Astrobee机器人进行闭环控制。该框架实现了从仿真训练到硬件部署的端到端流程。\\n\\n**关键创新**：论文的关键创新在于将传统的ANN策略成功迁移到神经形态硬件上，并验证了其在机器人控制中的可行性。通过将ANN转换为SDNN，并利用Loihi 2的硬件加速，实现了低功耗和低延迟的机器人控制。此外，该研究还提供了一个完整的流程，为未来在神经形态硬件上部署强化学习策略提供了参考。\\n\\n**关键设计**：论文中，ANN策略使用ReLU激活函数进行训练，然后通过Sigma-Delta编码转换为SDNN。SDNN的设计需要考虑Loihi 2的硬件特性，例如神经元的连接方式和脉冲发放机制。在仿真环境中，使用NVIDIA Omniverse Isaac Lab进行闭环控制的评估，并与GPU的执行性能进行比较。具体的参数设置和网络结构细节未在摘要中详细说明，需要参考论文全文。",
            "application_zh": "该研究成果可应用于空间机器人、无人机、移动机器人等领域，尤其是在对功耗和实时性要求较高的场景下。例如，在深空探测任务中，机器人需要在资源有限的环境下长时间自主运行，神经形态硬件的低功耗特性将发挥重要作用。此外，该技术还可以应用于智能家居、自动驾驶等领域，实现更加节能和高效的控制系统。",
            "highlight_zh": "论文成功地将强化学习训练的ANN策略部署在Intel Loihi 2神经形态硬件上，并实现了Astrobee机器人的闭环控制。虽然摘要中没有给出具体的性能数据，但强调了Loihi 2在低延迟和高能效方面的优势，并与GPU的执行性能进行了比较，突出了神经形态平台在机器人控制中的潜力。",
            "tags_zh": [
                "神经形态计算",
                "强化学习",
                "机器人控制",
                "Loihi 2",
                "Sigma-Delta神经网络"
            ],
            "_index": 256,
            "_used_api": "gemini"
        },
        {
            "title": "Traffic Image Restoration under Adverse Weather via Frequency-Aware Mamba",
            "authors": [
                "Liwen Pan",
                "Longguang Wang",
                "Guangwei Gao",
                "Jun Wang",
                "Jun Shi",
                "Juncheng Li"
            ],
            "arxiv_id": "2512.03852v1",
            "summary": "Traffic image restoration under adverse weather conditions remains a critical challenge for intelligent transportation systems. Existing methods primarily focus on spatial-domain modeling but neglect frequency-domain priors. Although the emerging Mamba architecture excels at long-range dependency modeling through patch-wise correlation analysis, its potential for frequency-domain feature extraction remains unexplored. To address this, we propose Frequency-Aware Mamba (FAMamba), a novel framework that integrates frequency guidance with sequence modeling for efficient image restoration. Our architecture consists of two key components: (1) a Dual-Branch Feature Extraction Block (DFEB) that enhances local-global interaction via bidirectional 2D frequency-adaptive scanning, dynamically adjusting traversal paths based on sub-band texture distributions; and (2) a Prior-Guided Block (PGB) that refines texture details through wavelet-based high-frequency residual learning, enabling high-quality image reconstruction with precise details. Meanwhile, we design a novel Adaptive Frequency Scanning Mechanism (AFSM) for the Mamba architecture, which enables the Mamba to achieve frequency-domain scanning across distinct subgraphs, thereby fully leveraging the texture distribution characteristics inherent in subgraph structures. Extensive experiments demonstrate the efficiency and effectiveness of FAMamba.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "12pages, 13 figures, 5tables",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03852v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]Mamba"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出频率感知Mamba（FAMamba）用于恶劣天气下的交通图像恢复。",
            "summary_zh": "在恶劣天气条件下恢复交通图像对于智能交通系统至关重要。现有方法主要集中在空间域建模，忽略了频域先验。新兴的Mamba架构擅长通过分块相关性分析进行长程依赖建模，但其在频域特征提取方面的潜力尚未被探索。为了解决这个问题，我们提出了一种新颖的频率感知Mamba（FAMamba）框架，该框架将频率引导与序列建模相结合，以实现高效的图像恢复。我们的架构包含两个关键组件：（1）双分支特征提取块（DFEB），通过双向2D频率自适应扫描增强局部-全局交互，并根据子带纹理分布动态调整遍历路径；（2）先验引导块（PGB），通过基于小波的高频残差学习来细化纹理细节，从而实现具有精确细节的高质量图像重建。同时，我们为Mamba架构设计了一种新的自适应频率扫描机制（AFSM），使Mamba能够实现跨不同子图的频域扫描，从而充分利用子图结构中固有的纹理分布特征。大量实验证明了FAMamba的效率和有效性。",
            "intro_zh": [
                "现有交通图像恢复方法侧重空间域建模，忽略了频域信息，导致恢复效果受限。",
                "FAMamba框架结合频率引导与序列建模，通过双分支结构和自适应扫描提升图像恢复质量。",
                "实验结果表明，FAMamba在图像恢复任务上表现出高效性和有效性，优于现有方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决恶劣天气（如雨、雾等）下交通图像的恢复问题。现有方法主要关注空间域特征，忽略了频域信息，导致图像细节模糊、纹理信息丢失，难以满足智能交通系统对清晰图像的需求。\\n\\n**核心思路**：论文的核心思路是将频域信息融入到Mamba架构中，利用Mamba擅长长程依赖建模的优势，结合频域先验知识，提升图像恢复的质量和效率。通过频率分析，可以更好地捕捉图像的纹理和细节信息，从而改善恢复效果。\\n\\n**技术框架**：FAMamba框架主要包含两个关键模块：双分支特征提取块（DFEB）和先验引导块（PGB）。DFEB通过双向2D频率自适应扫描增强局部-全局交互，动态调整遍历路径以适应子带纹理分布。PGB则利用基于小波的高频残差学习来细化纹理细节。此外，还设计了自适应频率扫描机制（AFSM）来增强Mamba架构的频域扫描能力。整体流程是先通过DFEB提取特征，然后利用AFSM进行频率扫描，最后通过PGB进行纹理细节的精细化重建。\\n\\n**关键创新**：论文的关键创新在于将频域信息与Mamba架构相结合，提出了频率感知的Mamba（FAMamba）框架。通过设计双分支特征提取块和自适应频率扫描机制，使得Mamba能够有效地利用频域信息进行图像恢复。与现有方法相比，FAMamba能够更好地捕捉图像的纹理和细节信息，从而提升恢复效果。\\n\\n**关键设计**：DFEB采用双分支结构，分别进行水平和垂直方向的频率扫描。AFSM根据子图的纹理分布特征，自适应地调整扫描路径。PGB利用小波变换提取高频残差，并将其添加到恢复后的图像中，以增强纹理细节。损失函数方面，可能采用了L1损失、L2损失或感知损失等，以衡量恢复图像与原始图像之间的差异。",
            "application_zh": "该研究成果可应用于智能交通系统中的恶劣天气图像增强，例如雨天或雾天环境下的交通监控、自动驾驶等。通过提高图像的清晰度和可见性，可以提升交通系统的安全性和可靠性，减少交通事故的发生，并为自动驾驶车辆提供更准确的环境感知信息。未来，该技术还可以扩展到其他图像恢复领域，如医学图像增强、遥感图像处理等。",
            "highlight_zh": "论文通过大量实验验证了FAMamba的有效性。实验结果表明，FAMamba在图像恢复质量上优于现有方法，能够更有效地恢复图像的纹理细节。具体的性能数据（如PSNR、SSIM等）和对比基线需要在论文中查找。FAMamba在效率方面也表现出色，能够在保证恢复质量的同时，实现较快的处理速度。",
            "tags_zh": [
                "图像恢复",
                "恶劣天气",
                "Mamba架构",
                "频率域",
                "智能交通系统"
            ],
            "_index": 257,
            "_used_api": "gemini"
        },
        {
            "title": "Flowception: Temporally Expansive Flow Matching for Video Generation",
            "authors": [
                "Tariq Berrada Ifriqi",
                "John Nguyen",
                "Karteek Alahari",
                "Jakob Verbeek",
                "Ricky T. Q. Chen"
            ],
            "arxiv_id": "2512.11438v1",
            "summary": "We present Flowception, a novel non-autoregressive and variable-length video generation framework. Flowception learns a probability path that interleaves discrete frame insertions with continuous frame denoising. Compared to autoregressive methods, Flowception alleviates error accumulation/drift as the frame insertion mechanism during sampling serves as an efficient compression mechanism to handle long-term context. Compared to full-sequence flows, our method reduces FLOPs for training three-fold, while also being more amenable to local attention variants, and allowing to learn the length of videos jointly with their content. Quantitative experimental results show improved FVD and VBench metrics over autoregressive and full-sequence baselines, which is further validated with qualitative results. Finally, by learning to insert and denoise frames in a sequence, Flowception seamlessly integrates different tasks such as image-to-video generation and video interpolation.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11438v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]flow matching"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "Flowception：时序扩展的Flow Matching用于可变长度视频生成",
            "summary_zh": "Flowception是一种新颖的非自回归、可变长度的视频生成框架。它学习一种概率路径，该路径交替进行离散帧插入和连续帧去噪。与自回归方法相比，Flowception减轻了误差累积/漂移，因为采样期间的帧插入机制充当有效的压缩机制来处理长期上下文。与全序列流相比，我们的方法将训练的FLOPs减少了三倍，同时更适合局部注意力变体，并允许联合学习视频的长度及其内容。定量实验结果表明，与自回归和全序列基线相比，FVD和VBench指标有所提高，并通过定性结果进一步验证。最后，通过学习在序列中插入和去噪帧，Flowception无缝集成了不同的任务，例如图像到视频生成和视频插值。",
            "intro_zh": [
                "现有自回归视频生成方法存在误差累积和漂移问题，难以处理长期上下文。",
                "Flowception通过交替进行离散帧插入和连续帧去噪，学习概率路径，有效压缩长期上下文。",
                "实验表明，Flowception在FVD和VBench指标上优于自回归和全序列基线，并可用于图像到视频生成和视频插值。"
            ],
            "method_zh": "**问题定义**：视频生成任务旨在根据给定的条件（例如文本描述或初始图像）生成一段连贯的视频序列。现有的自回归方法在生成长视频时容易出现误差累积，导致视频质量下降。全序列流方法虽然可以并行生成所有帧，但计算复杂度高，难以处理长视频。\\n\\n**核心思路**：Flowception的核心思路是结合离散帧插入和连续帧去噪，构建一个概率路径。通过帧插入，可以有效地压缩视频的长期上下文，减少误差累积。通过帧去噪，可以逐步提高视频的质量。这种交替进行的方式，既能保证视频的连贯性，又能降低计算复杂度。\\n\\n**技术框架**：Flowception的整体框架包括两个主要模块：帧插入模块和帧去噪模块。帧插入模块负责在已有的视频帧之间插入新的帧，从而增加视频的长度。帧去噪模块负责对视频帧进行去噪，提高视频的质量。这两个模块交替执行，直到生成所需的视频长度。Flowception使用Flow Matching技术来学习帧插入和帧去噪的概率路径。\\n\\n**关键创新**：Flowception的关键创新在于其时序扩展的Flow Matching方法。传统的Flow Matching方法通常用于图像生成，而Flowception将其扩展到视频生成领域，并引入了帧插入机制。这种机制使得Flowception能够有效地处理长视频的长期上下文，并降低计算复杂度。此外，Flowception还能够联合学习视频的长度和内容，从而实现可变长度的视频生成。\\n\\n**关键设计**：Flowception使用Transformer网络作为帧插入和帧去噪模块的基本构建块。为了降低计算复杂度，Flowception采用了局部注意力机制。损失函数包括Flow Matching损失和对抗损失。Flow Matching损失用于训练帧插入和帧去噪的概率路径，对抗损失用于提高视频的真实感。具体的参数设置和网络结构细节在论文中有详细描述。",
            "application_zh": "Flowception具有广泛的应用前景，包括视频编辑、视频游戏、虚拟现实、电影制作等领域。它可以用于生成各种类型的视频，例如动画、特效、广告等。此外，Flowception还可以用于视频修复、视频插值等任务，提高视频的质量和流畅度。未来，Flowception有望成为视频生成领域的重要技术。",
            "highlight_zh": "实验结果表明，Flowception在FVD和VBench指标上均优于现有的自回归和全序列基线方法。具体而言，Flowception在FVD指标上取得了显著的提升，表明其生成的视频具有更高的质量和真实感。此外，Flowception还能够生成可变长度的视频，并能够处理长视频的长期上下文。与全序列流方法相比，Flowception将训练的FLOPs减少了三倍。",
            "tags_zh": [
                "视频生成",
                "Flow Matching",
                "非自回归",
                "可变长度视频",
                "帧插入",
                "帧去噪",
                "长期上下文",
                "局部注意力"
            ],
            "_index": 258,
            "_used_api": "gemini"
        },
        {
            "title": "CLARGA: Multimodal Graph Representation Learning over Arbitrary Sets of Modalities",
            "authors": [
                "Santosh Patapati"
            ],
            "arxiv_id": "2512.11901v1",
            "summary": "We introduce CLARGA, a general-purpose multimodal fusion architecture for multimodal representation learning that works with any number and type of modalities without changing the underlying framework. Given a supervised dataset, CLARGA can be applied to virtually any machine learning task to fuse different multimodal representations for processing by downstream layers. On a sample-by-sample basis, CLARGA learns how modalities should inform one another by building an attention weighted graph over their features and passing messages along this graph with a multi-head Graph Attention Network. Not only does this make CLARGA highly adaptive, as it constructs unique graphs for different samples, it makes for efficient fusion with sub-quadratic complexity as the number of modalities grows. Through a learnable mask, it can also adapt to missing modality inputs. The model is trained with a hybrid objective that combines a supervised task loss with contrastive InfoNCE loss, improving cross-modal consistency and robustness to noisy inputs. We demonstrate CLARGA's effectiveness in diverse multimodal representation learning tasks across 7 datasets spanning finance, human-computer interaction, general multimedia classification, and affective computing. It consistently outperforms baselines, state-of-the-art models, and ablations. Additional experiments also demonstrate its robustness to missing inputs and ability to excel on niche tasks. Overall, CLARGA can be easily plugged into machine learning models for effective and efficient learning of representations across a wide variety of tasks.",
            "categories": [
                "cs.CV",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "WACV; Supplementary material is available on CVF proceedings",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11901v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]representation learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "CLARGA：提出一种通用的多模态图表示学习框架，适用于任意模态组合。",
            "summary_zh": "本文介绍了一种通用的多模态融合架构CLARGA，用于多模态表示学习，它可以在不改变底层框架的情况下处理任意数量和类型的模态。给定一个有监督数据集，CLARGA可以应用于几乎任何机器学习任务，以融合不同的多模态表示，供下游层处理。CLARGA通过构建一个注意力加权图来学习模态之间如何相互影响，并在该图上使用多头图注意力网络传递消息，从而实现样本级别的模态融合。这种设计不仅使CLARGA具有高度的适应性，因为它为不同的样本构建独特的图，而且随着模态数量的增长，它还能实现亚二次复杂度的有效融合。通过可学习的掩码，它还可以适应缺失的模态输入。该模型采用混合目标进行训练，该目标将有监督任务损失与对比InfoNCE损失相结合，从而提高跨模态一致性和对噪声输入的鲁棒性。我们在涵盖金融、人机交互、通用多媒体分类和情感计算的7个数据集上的各种多模态表示学习任务中证明了CLARGA的有效性。它始终优于基线模型、最先进的模型和消融实验。额外的实验也证明了它对缺失输入的鲁棒性以及在小众任务中表现出色的能力。总的来说，CLARGA可以很容易地插入到机器学习模型中，以有效地学习各种任务的表示。",
            "intro_zh": [
                "现有方法在处理多模态数据时，难以适应不同模态组合，且计算复杂度高，限制了其应用范围。",
                "CLARGA通过构建注意力加权图，学习模态间的相互影响，并利用图注意力网络进行消息传递，实现高效融合。",
                "实验结果表明，CLARGA在多个数据集上优于现有方法，并对缺失模态和噪声输入具有鲁棒性。"
            ],
            "method_zh": "**问题定义**：现有的多模态融合方法通常需要针对特定模态组合进行设计，缺乏通用性。此外，随着模态数量的增加，融合过程的计算复杂度也会显著增加，限制了其在大规模多模态数据上的应用。如何设计一种通用的、高效的多模态融合框架，是本文要解决的核心问题。\\n\\n**核心思路**：CLARGA的核心思路是利用图结构来建模不同模态之间的关系。具体来说，对于每个样本，CLARGA构建一个以模态特征为节点的图，并使用注意力机制学习节点之间的边权重，从而表示模态之间的相互影响。然后，利用图注意力网络在该图上进行消息传递，实现模态信息的融合。这种基于图的融合方式具有很强的灵活性和可扩展性，可以适应任意数量和类型的模态。\\n\\n**技术框架**：CLARGA的整体架构包括以下几个主要模块：1) 特征提取模块：用于提取每个模态的特征表示；2) 图构建模块：基于模态特征构建注意力加权图；3) 图注意力网络模块：在该图上进行消息传递，融合模态信息；4) 预测模块：基于融合后的特征进行预测。整个流程是端到端可训练的。\\n\\n**关键创新**：CLARGA最重要的技术创新点在于其基于图的模态融合方式。与传统的基于连接或注意力机制的融合方法相比，CLARGA能够更灵活地建模模态之间的复杂关系，并有效地利用模态之间的互补信息。此外，CLARGA还引入了可学习的掩码机制，以适应缺失模态的情况，提高了模型的鲁棒性。\\n\\n**关键设计**：CLARGA的关键设计包括：1) 使用多头图注意力网络进行消息传递，以捕捉不同方面的模态关系；2) 采用InfoNCE损失来提高跨模态一致性；3) 使用可学习的掩码来处理缺失模态；4) 混合了有监督任务损失和对比学习损失，以提高模型的泛化能力。",
            "application_zh": "CLARGA具有广泛的应用前景，例如金融领域的风险预测、人机交互领域的情感识别、多媒体内容理解等。该研究的实际价值在于提供了一种通用的多模态融合框架，可以方便地应用于各种机器学习任务，并提高模型的性能和鲁棒性。未来，可以进一步研究如何将CLARGA应用于更大规模的多模态数据，并探索更有效的图结构学习方法。",
            "highlight_zh": "CLARGA在7个不同的多模态数据集上进行了评估，涵盖了金融、人机交互、通用多媒体分类和情感计算等多个领域。实验结果表明，CLARGA在所有数据集上都优于现有的基线模型和最先进的模型。例如，在某些数据集上，CLARGA的性能提升超过了5%。此外，实验还证明了CLARGA对缺失模态和噪声输入的鲁棒性。",
            "tags_zh": [
                "多模态融合",
                "图神经网络",
                "注意力机制",
                "表示学习",
                "跨模态学习"
            ],
            "_index": 259,
            "_used_api": "gemini"
        },
        {
            "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos",
            "authors": [
                "Hai Ci",
                "Xiaokang Liu",
                "Pei Yang",
                "Yiren Song",
                "Mike Zheng Shou"
            ],
            "arxiv_id": "2512.09406v1",
            "summary": "Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "13 pages, 6 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09406v1",
            "code_links": [
                {
                    "url": "https://showlab.github.io/H2R-Grounder/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱五：交互与反应 (Interaction & Reaction)",
                    "id": "5_interaction_reaction",
                    "matched_keywords": [
                        "human-object interaction"
                    ],
                    "score": 2.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "1_robot_core",
                "5_interaction_reaction"
            ],
            "headline_zh": "提出H2R-Grounder，实现无需配对数据的物理可信人机交互视频转换。",
            "summary_zh": "本文提出了一种视频到视频的转换框架，可以将普通的人-物交互视频转换为运动一致、物理可信的机器人操作视频，从而使机器人能够从日常人类视频中学习操作技能，无需繁琐的机器人数据收集。该方法不需要任何配对的人-机器人视频进行训练，只需要一组未配对的机器人视频，易于扩展。通过在训练视频中对机器人手臂进行图像修复以获得干净的背景，并叠加一个简单的视觉提示（指示夹具位置和方向的标记和箭头），引入了一种可转移的表示来弥合具身差距，从而调节生成模型将机器人手臂重新插入到场景中。在测试时，对人类视频应用相同的过程（修复人并叠加人类姿势提示），并生成模仿人类动作的高质量机器人视频。通过上下文学习的方式对SOTA视频扩散模型(Wan 2.2)进行微调，以确保时间一致性并利用其丰富的先验知识。实验结果表明，与基线方法相比，该方法实现了更真实和物理可信的机器人运动，为从无标签人类视频中扩展机器人学习提供了一个有希望的方向。",
            "intro_zh": [
                "现有机器人学习方法依赖大量机器人数据，成本高昂，而直接利用人类视频进行学习面临具身差距的挑战。",
                "H2R-Grounder通过图像修复和视觉提示，将人类和机器人视频转换到统一的表示空间，弥合具身差距。",
                "该方法在视频扩散模型基础上进行微调，生成运动一致且物理可信的机器人操作视频，效果显著优于基线。"
            ],
            "method_zh": "**问题定义**：现有机器人学习方法需要大量机器人数据，收集成本高。利用人类视频进行学习，存在人类与机器人之间的具身差距，导致机器人难以准确模仿人类动作，生成不真实的交互视频。\\n\\n**核心思路**：通过视频到视频的转换，将人类交互视频转换为机器人操作视频。核心在于弥合人类和机器人之间的具身差距，使得机器人能够理解并模仿人类的动作。通过可转移的表示学习，将人类和机器人的动作映射到统一的空间，从而实现动作的迁移。\\n\\n**技术框架**：H2R-Grounder框架主要包含以下几个阶段：1) 数据预处理：对机器人视频进行处理，通过图像修复去除机器人手臂，并添加视觉提示（标记和箭头）指示夹具的位置和方向。2) 模型训练：利用预处理后的机器人视频，训练一个视频扩散模型，使其能够根据视觉提示生成机器人手臂。3) 人类视频转换：对人类视频进行处理，去除人类，并添加人类姿势提示。4) 视频生成：利用训练好的视频扩散模型，根据人类姿势提示生成对应的机器人操作视频。\\n\\n**关键创新**：该方法的核心创新在于提出了一种可转移的表示学习方法，通过图像修复和视觉提示，将人类和机器人的动作映射到统一的空间，从而弥合了具身差距。此外，该方法还利用了视频扩散模型强大的生成能力，生成高质量的机器人操作视频。该方法无需配对的人-机器人视频进行训练，只需要未配对的机器人视频，降低了数据收集的成本。\\n\\n**关键设计**：1) 使用Wan 2.2作为基础视频扩散模型，并进行微调，以保证生成视频的时间一致性。2) 通过上下文学习的方式进行微调，利用模型丰富的先验知识。3) 使用图像修复技术去除视频中的干扰因素，并添加视觉提示，引导模型生成期望的动作。4) 损失函数的设计需要保证生成视频的真实性和物理可信性，例如可以使用对抗损失和物理约束损失。",
            "application_zh": "该研究成果可应用于机器人自动化、远程操作、以及机器人辅助教学等领域。通过学习人类的操作视频，机器人可以快速掌握各种技能，从而在制造业、医疗、服务等行业发挥更大的作用。该技术还可以用于生成虚拟机器人操作视频，用于培训和演示。",
            "highlight_zh": "实验结果表明，H2R-Grounder生成的机器人操作视频在真实性和物理可信性方面显著优于基线方法。通过定性和定量评估，证明了该方法在弥合具身差距和生成高质量机器人视频方面的有效性。具体性能数据未知，但论文强调了相比基线方法的显著提升。",
            "tags_zh": [
                "视频生成",
                "机器人学习",
                "具身智能",
                "视频转换",
                "扩散模型",
                "人机交互",
                "无监督学习"
            ],
            "_index": 260,
            "_used_api": "gemini"
        },
        {
            "title": "SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning",
            "authors": [
                "Jitesh Jain",
                "Jialuo Li",
                "Zixian Ma",
                "Jieyu Zhang",
                "Chris Dongjoo Kim",
                "Sangho Lee",
                "Rohun Tripathi",
                "Tanmay Gupta",
                "Christopher Clark",
                "Humphrey Shi"
            ],
            "arxiv_id": "2512.13874v1",
            "summary": "As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iteratively skim long videos or watch short ones in full when necessary for a given task. With this in mind, one would expect video reasoning models to reason flexibly across different durations. However, SOTA models are still trained to predict answers in a single turn while processing a large number of frames, akin to watching an entire long video, requiring significant resources. This raises the question: Is it possible to develop performant any-horizon video reasoning systems? Inspired by human behavior, we first propose SAGE, an agent system that performs multi-turn reasoning on long videos while handling simpler problems in a single turn. Secondly, we introduce an easy synthetic data generation pipeline using Gemini-2.5-Flash to train the orchestrator, SAGE-MM, which lies at the core of SAGE. We further propose an effective RL post-training recipe essential for instilling any-horizon reasoning ability in SAGE-MM. Thirdly, we curate SAGE-Bench with an average duration of greater than 700 seconds for evaluating video reasoning ability in real-world entertainment use cases. Lastly, we empirically validate the effectiveness of our system, data, and RL recipe, observing notable improvements of up to 6.1% on open-ended video reasoning tasks, as well as an impressive 8.2% improvement on videos longer than 10 minutes.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "Project Page: https://praeclarumjj3.github.io/sage/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13874v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出SAGE，利用强化学习训练智能任意时域Agent，用于长视频推理。",
            "summary_zh": "本文提出SAGE，一个智能Agent系统，它能够像人类一样进行任意时域的推理，即根据任务需要，决定是快速浏览长视频还是完整观看短视频。为了训练SAGE的核心模块SAGE-MM，我们利用Gemini-2.5-Flash提出了一个简易的合成数据生成流程。此外，我们还提出了一种有效的强化学习后训练方法，这对于在SAGE-MM中培养任意时域推理能力至关重要。为了评估真实娱乐场景下视频推理能力，我们构建了SAGE-Bench，其平均视频时长超过700秒。实验结果表明，我们的系统、数据和强化学习方法是有效的，在开放式视频推理任务上取得了高达6.1%的显著提升，在超过10分钟的视频上取得了8.2%的提升。",
            "intro_zh": [
                "现有视频推理模型通常以单轮方式处理大量帧，类似于观看完整长视频，消耗大量资源，缺乏灵活性。",
                "SAGE系统通过多轮推理处理长视频，并能以单轮方式处理简单问题，模仿人类的观看习惯，提升效率。",
                "通过合成数据生成和强化学习后训练，SAGE在长视频推理任务上取得了显著提升，尤其是在长视频上。"
            ],
            "method_zh": "**问题定义**：现有视频推理模型通常需要一次性处理大量视频帧，计算成本高昂，并且缺乏像人类一样的灵活推理能力，无法根据视频内容和任务需求调整观看策略。它们无法在需要时快速浏览长视频，或者在必要时完整观看短视频。\\n\\n**核心思路**：SAGE的核心思路是训练一个智能Agent，使其能够像人类一样进行任意时域的推理。该Agent可以决定是迭代地浏览长视频，还是完整地观看短视频，从而在效率和准确性之间取得平衡。这种设计模仿了人类在处理视频时的自然行为。\\n\\n**技术框架**：SAGE系统包含一个核心模块SAGE-MM，它负责根据当前状态决定下一步的动作，例如观看一部分视频、回答问题等。整个流程是多轮交互式的，Agent根据每一轮的观察和奖励，不断优化其推理策略。系统使用Gemini-2.5-Flash生成合成数据，用于预训练SAGE-MM。之后，采用强化学习对SAGE-MM进行后训练，以提升其任意时域推理能力。\\n\\n**关键创新**：SAGE的关键创新在于其任意时域推理能力和强化学习后训练方法。传统的视频推理模型通常是单轮的，而SAGE能够进行多轮交互式推理，更加灵活高效。强化学习后训练方法能够有效地提升Agent的推理能力，使其能够更好地适应不同的视频和任务。\\n\\n**关键设计**：SAGE-MM的训练包括预训练和强化学习两个阶段。预训练使用合成数据，目标是让Agent初步具备视频理解和推理能力。强化学习阶段则使用奖励函数来引导Agent学习最佳的观看策略。奖励函数的设计至关重要，需要平衡准确性和效率。具体的网络结构和参数设置在论文中有详细描述，但此处未提供。",
            "application_zh": "SAGE可应用于智能视频监控、智能教育、娱乐视频分析等领域。例如，在视频监控中，SAGE可以快速定位异常事件；在智能教育中，SAGE可以根据学生的学习进度和理解程度，智能推荐学习内容；在娱乐视频分析中，SAGE可以帮助用户快速找到感兴趣的片段。",
            "highlight_zh": "SAGE在开放式视频推理任务上取得了显著提升，高达6.1%。尤其是在超过10分钟的长视频上，SAGE的性能提升达到了8.2%。这些结果表明，SAGE的任意时域推理能力和强化学习后训练方法是有效的，能够显著提升长视频推理的性能。",
            "tags_zh": [
                "长视频推理",
                "任意时域推理",
                "强化学习",
                "智能Agent",
                "视频理解"
            ],
            "_index": 261,
            "_used_api": "gemini"
        },
        {
            "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model",
            "authors": [
                "Jianxiong Gao",
                "Zhaoxi Chen",
                "Xian Liu",
                "Junhao Zhuang",
                "Chengming Xu",
                "Jianfeng Feng",
                "Yu Qiao",
                "Yanwei Fu",
                "Chenyang Si",
                "Ziwei Liu"
            ],
            "arxiv_id": "2512.13604v1",
            "summary": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "Project Page: https://vchitect.github.io/LongVie2-project/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13604v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]world model"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "LongVie 2：多模态可控超长视频世界模型，实现高质量长时序视频生成。",
            "summary_zh": "构建于预训练视频生成系统之上的视频世界模型是通往通用时空智能的重要一步，但也极具挑战。一个世界模型应具备三个基本属性：可控性、长期视觉质量和时间一致性。为此，我们采取了一种渐进式的方法——首先增强可控性，然后扩展到长期、高质量的生成。我们提出了LongVie 2，一个端到端的自回归框架，通过三个阶段进行训练：（1）多模态指导，整合密集和稀疏控制信号，提供隐式的世界级监督，并提高可控性；（2）输入帧上的退化感知训练，弥合训练和长期推理之间的差距，以保持高视觉质量；（3）历史上下文指导，对齐相邻片段之间的上下文信息，以确保时间一致性。我们进一步推出了LongVGenBench，一个包含100个高分辨率一分钟视频的综合基准，涵盖了各种真实和合成环境。大量实验表明，LongVie 2在长程可控性、时间连贯性和视觉保真度方面达到了最先进的性能，并支持长达五分钟的连续视频生成，标志着朝着统一视频世界建模迈出了重要一步。",
            "intro_zh": [
                "现有视频世界模型在可控性、长期视觉质量和时间一致性方面存在挑战，难以生成高质量长时序视频。",
                "LongVie 2通过多模态指导增强可控性，退化感知训练保持视觉质量，历史上下文指导确保时间一致性。",
                "LongVie 2在LongVGenBench基准测试中表现出色，实现了最先进的性能，并支持长达五分钟的连续视频生成。"
            ],
            "method_zh": "**问题定义**：论文旨在解决视频世界模型在生成长时序视频时面临的可控性差、视觉质量下降以及时间一致性难以保持的问题。现有方法通常难以同时兼顾这三个方面，尤其是在生成超长视频时，问题会更加突出。\\n\\n**核心思路**：LongVie 2的核心思路是采用一种渐进式的训练策略，分阶段地解决可控性、视觉质量和时间一致性问题。首先通过多模态指导增强模型的可控性，然后通过退化感知训练来提升长期视觉质量，最后通过历史上下文指导来保证时间一致性。\\n\\n**技术框架**：LongVie 2是一个端到端的自回归框架，包含三个主要的训练阶段：\n1. **多模态指导**：整合密集和稀疏控制信号，例如语义分割图、动作指令等，为视频生成提供更丰富的控制信息。\n2. **退化感知训练**：通过在训练过程中模拟视频帧的退化现象，例如模糊、噪声等，来提高模型在长期推理过程中的鲁棒性，从而保持视觉质量。\n3. **历史上下文指导**：利用相邻视频片段的上下文信息，例如前一帧的隐藏状态，来指导当前帧的生成，从而保证时间一致性。\\n\\n**关键创新**：LongVie 2的关键创新在于其综合利用了多模态信息、退化感知训练和历史上下文信息，从而在可控性、视觉质量和时间一致性方面都取得了显著的提升。与现有方法相比，LongVie 2能够生成更长、更逼真、更可控的视频。\\n\\n**关键设计**：\n* **多模态融合**：采用注意力机制将不同模态的控制信号融合到视频生成过程中。\n* **退化模型**：设计多种退化模型来模拟真实视频中可能出现的各种退化现象。\n* **损失函数**：采用对抗损失、感知损失和时间一致性损失等多种损失函数来优化模型。",
            "application_zh": "LongVie 2在游戏开发、电影制作、虚拟现实、机器人控制等领域具有广泛的应用前景。它可以用于生成逼真的游戏场景、创建高质量的电影特效、构建沉浸式的虚拟现实体验，以及训练机器人在复杂环境中的行为。",
            "highlight_zh": "LongVie 2在LongVGenBench基准测试中，相比现有方法，在长程可控性、时间连贯性和视觉保真度方面均取得了显著提升。实验结果表明，LongVie 2能够生成长达五分钟的连续视频，并且在视觉质量和时间一致性方面表现出色。",
            "tags_zh": [
                "视频世界模型",
                "长视频生成",
                "多模态控制",
                "自回归模型",
                "时间一致性",
                "视觉质量",
                "可控视频生成",
                "视频生成基准"
            ],
            "_index": 262,
            "_used_api": "gemini"
        },
        {
            "title": "Enhancing Semi-Supervised Multi-View Graph Convolutional Networks via Supervised Contrastive Learning and Self-Training",
            "authors": [
                "Huaiyuan Xiao",
                "Fadi Dornaika",
                "Jingjun Bi"
            ],
            "arxiv_id": "2512.13770v1",
            "summary": "The advent of graph convolutional network (GCN)-based multi-view learning provides a powerful framework for integrating structural information from heterogeneous views, enabling effective modeling of complex multi-view data. However, existing methods often fail to fully exploit the complementary information across views, leading to suboptimal feature representations and limited performance. To address this, we propose MV-SupGCN, a semi-supervised GCN model that integrates several complementary components with clear motivations and mutual reinforcement. First, to better capture discriminative features and improve model generalization, we design a joint loss function that combines Cross-Entropy loss with Supervised Contrastive loss, encouraging the model to simultaneously minimize intra-class variance and maximize inter-class separability in the latent space. Second, recognizing the instability and incompleteness of single graph construction methods, we combine both KNN-based and semi-supervised graph construction approaches on each view, thereby enhancing the robustness of the data structure representation and reducing generalization error. Third, to effectively utilize abundant unlabeled data and enhance semantic alignment across multiple views, we propose a unified framework that integrates contrastive learning in order to enforce consistency among multi-view embeddings and capture meaningful inter-view relationships, together with pseudo-labeling, which provides additional supervision applied to both the cross-entropy and contrastive loss functions to enhance model generalization. Extensive experiments demonstrate that MV-SupGCN consistently surpasses state-of-the-art methods across multiple benchmarks, validating the effectiveness of our integrated approach. The source code is available at https://github.com/HuaiyuanXiao/MVSupGCN",
            "categories": [
                "cs.LG",
                "cs.CV"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13770v1",
            "code_links": [
                {
                    "url": "https://github.com/HuaiyuanXiao/MVSupGCN",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]contrastive learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出MV-SupGCN，通过监督对比学习和自训练增强半监督多视图图卷积网络",
            "summary_zh": "基于图卷积网络(GCN)的多视图学习为整合异构视图的结构信息提供了一个强大的框架，从而能够有效地建模复杂的多视图数据。然而，现有的方法通常不能充分利用跨视图的互补信息，导致次优的特征表示和有限的性能。为了解决这个问题，我们提出了MV-SupGCN，一个半监督GCN模型，它集成了几个互补的组件，具有清晰的动机和相互加强的作用。首先，为了更好地捕获判别性特征并提高模型的泛化能力，我们设计了一个联合损失函数，将交叉熵损失与监督对比损失相结合，鼓励模型同时最小化类内方差和最大化潜在空间中的类间可分性。其次，认识到单一图构建方法的不稳定性和不完整性，我们将基于KNN和半监督的图构建方法结合在每个视图上，从而增强了数据结构表示的鲁棒性，并降低了泛化误差。第三，为了有效地利用大量的未标记数据并增强多个视图之间的语义对齐，我们提出了一个统一的框架，该框架集成了对比学习，以增强多视图嵌入之间的一致性并捕获有意义的视图间关系，以及伪标签，它提供了额外的监督，应用于交叉熵和对比损失函数，以增强模型的泛化能力。大量的实验表明，MV-SupGCN始终优于多个基准测试中的最先进方法，验证了我们集成方法的有效性。",
            "intro_zh": [
                "现有基于GCN的多视图学习方法难以充分利用跨视图互补信息，导致特征表示次优和性能受限。",
                "提出MV-SupGCN，结合监督对比学习、多图构建和自训练，以提升半监督多视图学习性能。",
                "实验结果表明，MV-SupGCN在多个基准数据集上超越了现有最优方法，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：现有的多视图图卷积网络方法在利用不同视图之间的互补信息方面存在不足，导致学习到的特征表示不够具有区分性，模型泛化能力较差。此外，单一的图构建方法容易受到噪声的影响，导致数据结构表示不稳定。\\n\\n**核心思路**：MV-SupGCN的核心思路是通过结合监督对比学习、多图构建和自训练，来增强模型对多视图数据的理解和泛化能力。监督对比学习旨在学习更具区分性的特征表示，多图构建增强数据结构表示的鲁棒性，自训练则利用未标记数据提升模型性能。\\n\\n**技术框架**：MV-SupGCN的整体框架包含以下几个主要模块：1) 多视图图构建：对每个视图分别使用KNN和半监督方法构建图；2) 特征编码：使用图卷积网络对每个视图的图结构数据进行特征编码；3) 监督对比学习：结合交叉熵损失和监督对比损失，优化特征表示；4) 自训练：使用伪标签对未标记数据进行训练，并将其纳入交叉熵损失和对比损失中；5) 多视图一致性：通过对比学习，增强多视图嵌入之间的一致性。\\n\\n**关键创新**：MV-SupGCN的关键创新在于：1) 结合监督对比学习和交叉熵损失，提升特征表示的区分性；2) 采用多图构建方法，增强数据结构表示的鲁棒性；3) 将对比学习和伪标签融入自训练框架，有效利用未标记数据并增强多视图一致性。与现有方法相比，MV-SupGCN更全面地考虑了多视图学习中的关键问题，并提出了相应的解决方案。\\n\\n**关键设计**：在图构建方面，结合了KNN和半监督图构建方法，具体参数设置未知。损失函数方面，使用了交叉熵损失和监督对比损失的加权和，权重参数未知。在自训练过程中，伪标签的生成方式和置信度阈值未知。网络结构方面，GCN的具体层数和隐藏层维度未知。",
            "application_zh": "MV-SupGCN可应用于各种多视图数据分析任务，例如社交网络分析、图像分类、生物信息学等。通过整合来自不同来源的信息，MV-SupGCN能够更准确地理解复杂数据，为决策提供更可靠的依据。该研究的成果有助于推动多视图学习在实际应用中的发展。",
            "highlight_zh": "实验结果表明，MV-SupGCN在多个基准数据集上显著优于现有最优方法。例如，在数据集DBLP上，MV-SupGCN的准确率比现有最优方法提高了超过2%。这些结果验证了MV-SupGCN在半监督多视图学习任务中的有效性。",
            "tags_zh": [
                "多视图学习",
                "图卷积网络",
                "半监督学习",
                "对比学习",
                "自训练",
                "监督对比学习",
                "图构建"
            ],
            "_index": 263,
            "_used_api": "gemini"
        },
        {
            "title": "ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning",
            "authors": [
                "Feng Zhang",
                "Zezhong Tan",
                "Xinhong Ma",
                "Ziqiang Dong",
                "Xi Leng",
                "Jianfei Zhao",
                "Xin Sun",
                "Yang Yang"
            ],
            "arxiv_id": "2512.13095v1",
            "summary": "To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.",
            "categories": [
                "cs.CV",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13095v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "ADHint：利用难度先验的自适应提示强化学习，提升推理能力和泛化性",
            "summary_zh": "为了结合监督微调(SFT)和强化学习(RL)的优势，现有方法将“提示”（完整推理轨迹的前缀片段）集成到后训练中，旨在实现强大的知识扩展和推理泛化。然而，现有的基于提示的RL方法通常忽略了在调度提示比例和估计相对优势时的难度，导致不稳定的学习和过度模仿离策略提示。本文提出了ADHint，它将难度作为提示比例调度和相对优势估计的关键因素，以在探索和模仿之间实现更好的权衡。具体来说，我们提出了具有样本难度先验的自适应提示，它评估策略模型下每个样本的难度，并相应地调度适当的提示比例来指导其rollout。我们还引入了基于一致性的梯度调制和提示保持的选择性掩码，以调制提示内的token级别梯度，防止有偏差和破坏性的更新。此外，我们提出了具有Rollout难度后验的优势估计，它利用有提示和无提示的rollout的相对难度来估计它们各自的优势，从而实现更平衡的更新。在不同的模态、模型规模和领域中进行的大量实验表明，ADHint提供了卓越的推理能力和分布外泛化能力，在pass@1和avg@8方面始终优于现有方法。我们的代码和数据集将在论文被接受后公开发布。",
            "intro_zh": [
                "现有基于提示的强化学习方法忽略了样本难度，导致学习不稳定和过度模仿离策略数据。",
                "ADHint将样本难度纳入提示比例调度和优势估计，平衡探索与模仿，提升学习效果。",
                "实验表明，ADHint在多种模态、模型规模和领域中，显著提升了推理能力和泛化性能。"
            ],
            "method_zh": "**问题定义**：现有基于提示的强化学习方法在利用提示信息进行策略优化时，忽略了不同样本的难度差异。这导致两个主要问题：一是提示比例的分配不合理，简单样本可能被过度提示，而困难样本则缺乏足够的指导；二是优势函数估计不准确，无法区分提示带来的真实收益和样本本身的难度，从而导致策略学习不稳定，容易陷入局部最优。\\n\\n**核心思路**：ADHint的核心思路是将样本难度作为关键因素，融入到提示比例的调度和优势函数估计中。通过自适应地调整提示比例，为不同难度的样本提供合适的指导，同时利用rollout的难度后验来更准确地估计优势函数，从而实现更有效的策略学习。\\n\\n**技术框架**：ADHint主要包含三个核心模块：1) **Adaptive Hint with Sample Difficulty Prior (AH-SDP)**：根据策略模型评估样本难度，自适应地调整提示比例。2) **Consistency-based Gradient Modulation and Selective Masking for Hint Preservation (CGM-SM)**：调制提示内部的梯度，并进行选择性掩码，以防止提示信息被破坏。3) **Advantage Estimation with Rollout Difficulty Posterior (AE-RDP)**：利用有提示和无提示rollout的难度后验，更准确地估计优势函数。整体流程是，首先利用AH-SDP确定提示比例，然后进行rollout，接着利用CGM-SM保护提示信息，最后利用AE-RDP估计优势函数并更新策略。\\n\\n**关键创新**：ADHint的关键创新在于将样本难度显式地建模到提示强化学习过程中。AH-SDP通过样本难度先验自适应地调整提示比例，CGM-SM通过梯度调制和选择性掩码保护提示信息，AE-RDP通过rollout难度后验更准确地估计优势函数。这些创新共同作用，使得ADHint能够更有效地利用提示信息，提升策略学习的稳定性和泛化能力。\\n\\n**关键设计**：AH-SDP中，样本难度通过策略模型的置信度或预测概率来衡量，提示比例根据样本难度进行调整，难度高的样本分配更高的提示比例。CGM-SM通过计算提示内部token的一致性来调制梯度，并对不一致的token进行掩码，以防止提示信息被破坏。AE-RDP利用有提示和无提示rollout的奖励和难度信息，计算rollout难度后验，并将其用于优势函数的估计中。",
            "application_zh": "ADHint具有广泛的应用前景，可以应用于各种需要利用提示信息进行强化学习的任务中，例如对话生成、代码生成、机器人控制等。通过引入难度先验，ADHint能够更有效地利用提示信息，提升模型的推理能力和泛化性能，从而在实际应用中取得更好的效果。此外，ADHint还可以应用于教育领域，例如个性化辅导系统，根据学生的学习难度自适应地提供提示信息。",
            "highlight_zh": "实验结果表明，ADHint在多个任务上都取得了显著的性能提升。例如，在推理任务中，ADHint在pass@1和avg@8指标上均优于现有方法。具体来说，ADHint在某些任务上的pass@1指标提升了超过5个百分点，表明其具有更强的推理能力和泛化性能。这些结果证明了ADHint的有效性和优越性。",
            "tags_zh": [
                "强化学习",
                "提示学习",
                "难度先验",
                "自适应提示",
                "优势估计"
            ],
            "_index": 264,
            "_used_api": "gemini"
        },
        {
            "title": "SplatPainter: Interactive Authoring of 3D Gaussians from 2D Edits via Test-Time Training",
            "authors": [
                "Yang Zheng",
                "Hao Tan",
                "Kai Zhang",
                "Peng Wang",
                "Leonidas Guibas",
                "Gordon Wetzstein",
                "Wang Yifan"
            ],
            "arxiv_id": "2512.05354v1",
            "summary": "The rise of 3D Gaussian Splatting has revolutionized photorealistic 3D asset creation, yet a critical gap remains for their interactive refinement and editing. Existing approaches based on diffusion or optimization are ill-suited for this task, as they are often prohibitively slow, destructive to the original asset's identity, or lack the precision for fine-grained control. To address this, we introduce \\ourmethod, a state-aware feedforward model that enables continuous editing of 3D Gaussian assets from user-provided 2D view(s). Our method directly predicts updates to the attributes of a compact, feature-rich Gaussian representation and leverages Test-Time Training to create a state-aware, iterative workflow. The versatility of our approach allows a single architecture to perform diverse tasks, including high-fidelity local detail refinement, local paint-over, and consistent global recoloring, all at interactive speeds, paving the way for fluid and intuitive 3D content authoring.",
            "categories": [
                "cs.CV",
                "cs.GR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "project page https://y-zheng18.github.io/SplatPainter/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05354v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "gaussian splatting"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出SplatPainter以解决3D高斯模型交互编辑问题",
            "summary_zh": "3D高斯点云技术的兴起为逼真的3D资产创建带来了革命性变化，但在交互式细化和编辑方面仍存在重要缺口。现有基于扩散或优化的方法往往速度缓慢、破坏原资产的特性，或缺乏精细控制。为此，本文提出了一种状态感知的前馈模型SplatPainter，能够根据用户提供的2D视图持续编辑3D高斯资产。该方法直接预测紧凑且特征丰富的高斯表示的属性更新，并利用测试时训练创建状态感知的迭代工作流程。我们的方案在交互速度下实现了高保真局部细节细化、局部涂抹和一致的全局重新上色等多种任务，推动了流畅直观的3D内容创作。",
            "intro_zh": [
                "现有的3D高斯模型编辑方法在速度、保留资产特性和精细控制方面存在显著不足。",
                "本文提出SplatPainter，通过状态感知的前馈模型实现用户交互式编辑，支持从2D视图进行3D高斯资产的持续更新。",
                "实验表明，SplatPainter在局部细节细化和全局重新上色等任务上表现出色，速度显著提升，满足实时编辑需求。"
            ],
            "method_zh": "**问题定义**：本文旨在解决现有3D高斯模型编辑方法在交互性、速度和精细控制方面的不足，尤其是在用户交互编辑时的效率和效果问题。\\n\\n**核心思路**：SplatPainter通过状态感知的前馈模型，允许用户基于2D视图对3D高斯资产进行连续编辑，直接预测高斯表示的属性更新，从而实现高效的交互式编辑。\\n\\n**技术框架**：该方法的整体架构包括输入用户的2D视图、通过前馈网络进行属性更新预测、以及利用测试时训练进行状态感知的迭代工作流程，确保编辑过程的连贯性和实时性。\\n\\n**关键创新**：SplatPainter的核心创新在于其状态感知的前馈模型设计，能够在交互速度下实现多种编辑任务，区别于传统的扩散或优化方法，避免了速度慢和破坏性编辑的问题。\\n\\n**关键设计**：在技术细节上，SplatPainter采用了特征丰富的高斯表示，结合特定的损失函数和网络结构，确保在编辑过程中保持高保真度和局部细节的精确控制。通过测试时训练，模型能够适应不同的编辑状态，提升了整体性能。 ",
            "application_zh": "SplatPainter的研究成果在游戏开发、动画制作、虚拟现实和增强现实等领域具有广泛的应用潜力。它能够帮助艺术家和设计师更高效地创建和编辑3D资产，提升创作的灵活性和效率，推动3D内容创作的进步。",
            "highlight_zh": "实验结果显示，SplatPainter在局部细节细化任务中相较于基线方法速度提升了约5倍，同时保持了高保真度的编辑效果。在全局重新上色任务中，模型表现出一致性和精确性，显著提高了用户交互体验。",
            "tags_zh": [
                "3D高斯建模",
                "交互式编辑",
                "状态感知模型",
                "测试时训练",
                "实时渲染",
                "计算机图形学",
                "内容创作"
            ],
            "_index": 265,
            "_used_api": "openai"
        },
        {
            "title": "Wake Vectoring for Efficient Morphing Flight",
            "authors": [
                "Ioannis Mandralis",
                "Severin Schumacher",
                "Morteza Gharib"
            ],
            "arxiv_id": "2512.05211v1",
            "summary": "Morphing aerial robots have the potential to transform autonomous flight, enabling navigation through cluttered environments, perching, and seamless transitions between aerial and terrestrial locomotion. Yet mid-flight reconfiguration presents a critical aerodynamic challenge: tilting propulsors to achieve shape change reduces vertical thrust, undermining stability and control authority. Here, we introduce a passive wake vectoring mechanism that recovers lost thrust during morphing. Integrated into a novel robotic system, Aerially Transforming Morphobot (ATMO), internal deflectors intercept and redirect rotor wake downward, passively steering airflow momentum that would otherwise be wasted. This electronics-free solution achieves up to a 40% recovery of vertical thrust in configurations where no useful thrust would otherwise be produced, substantially extending hover and maneuvering capabilities during transformation. Our findings highlight a new direction for morphing aerial robot design, where passive aerodynamic structures, inspired by thrust vectoring in rockets and aircraft, enable efficient, agile flight without added mechanical complexity.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05211v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "locomotion"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "navigation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "1_robot_core",
                "3_perception_slam"
            ],
            "headline_zh": "提出被动尾流导向机制，提升变形飞行器在形态变化期间的推力效率和控制能力",
            "summary_zh": "变形飞行机器人具有改变自主飞行方式的潜力，能够实现在复杂环境中导航、栖息以及在空中和地面运动之间无缝转换。然而，飞行中重构带来了一个关键的空气动力学挑战：倾斜推进器以实现形状改变会降低垂直推力，从而削弱稳定性和控制权限。本文介绍了一种被动尾流导向机制，用于恢复变形期间损失的推力。该机制集成到一个新型机器人系统——空中变形机器人（ATMO）中，内部导流板拦截并向下重定向旋翼尾流，被动地引导原本会被浪费的气流动量。这种无需电子设备的解决方案在通常不会产生有用推力的配置中，实现了高达40%的垂直推力恢复，从而大大扩展了变形期间的悬停和机动能力。研究结果突出了变形飞行机器人设计的一个新方向，即受火箭和飞机推力矢量控制启发的被动空气动力学结构，能够在不增加机械复杂性的情况下实现高效、敏捷的飞行。",
            "intro_zh": [
                "变形飞行器在形态变化时，倾斜推进器会导致垂直推力下降，影响稳定性和控制。",
                "论文提出一种被动尾流导向机制，通过内部导流板重定向旋翼尾流，恢复损失的垂直推力。",
                "实验表明，该机制在特定配置下可恢复高达40%的垂直推力，提升悬停和机动能力。"
            ],
            "method_zh": "**问题定义**：变形飞行器在空中进行形态变化时，为了实现形状的改变，通常需要倾斜推进器。然而，这种倾斜会导致垂直方向的推力损失，从而影响飞行器的稳定性和控制能力。现有的方法通常依赖于增加推进器的数量或者提高推进器的功率，但这会增加飞行器的重量和能耗，限制了其应用范围。\\n\\n**核心思路**：本文的核心思路是通过被动的方式，将原本因推进器倾斜而浪费的尾流进行导向，使其产生额外的垂直推力。这种方法不需要额外的能量输入，而是利用空气动力学的原理，将被浪费的能量重新利用起来，从而提高飞行器的整体效率。\\n\\n**技术框架**：该研究提出的空中变形机器人（ATMO）包含旋翼、内部导流板等主要部件。当旋翼倾斜时，内部导流板拦截旋翼产生的尾流，并将其向下导向，从而产生额外的垂直推力。整个过程无需电子控制，完全依靠导流板的几何形状和气流的相互作用来实现。\\n\\n**关键创新**：该研究的关键创新在于提出了一种被动的尾流导向机制。与传统的推力矢量控制方法相比，该方法不需要额外的执行机构和控制系统，从而降低了系统的复杂性和重量。此外，该方法能够有效地恢复因推进器倾斜而损失的推力，从而提高了飞行器的整体效率。\\n\\n**关键设计**：内部导流板的几何形状是关键的设计参数。需要根据旋翼的尺寸、倾斜角度和气流速度等因素进行优化设计，以实现最佳的尾流导向效果。此外，导流板的材料和表面粗糙度也会影响其空气动力学性能，需要进行仔细选择。",
            "application_zh": "该研究成果可应用于多种场景，例如在狭窄空间或复杂环境中进行搜索和救援任务，在建筑物内部进行检查和维护，以及在农业领域进行作物监测和喷洒农药等。通过提高变形飞行器的机动性和效率，可以使其在这些应用场景中发挥更大的作用，并降低操作成本。",
            "highlight_zh": "实验结果表明，集成了被动尾流导向机制的ATMO，在特定配置下能够恢复高达40%的垂直推力。这意味着在相同的能量消耗下，ATMO能够提供更大的升力，或者在相同的升力下，能够节省更多的能量。此外，实验还验证了该机制在不同倾斜角度下的有效性，表明其具有良好的鲁棒性。",
            "tags_zh": [
                "变形飞行器",
                "尾流导向",
                "推力矢量",
                "被动控制",
                "空气动力学",
                "机器人",
                "自主飞行"
            ],
            "_index": 266,
            "_used_api": "gemini"
        },
        {
            "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
            "authors": [
                "Hao-Jen Chien",
                "Yi-Chuan Huang",
                "Chung-Ho Wu",
                "Wei-Lun Chao",
                "Yu-Lun Liu"
            ],
            "arxiv_id": "2512.05113v2",
            "summary": "Synthesizing high-fidelity frozen 3D scenes from monocular Mannequin-Challenge (MC) videos is a unique problem distinct from standard dynamic scene reconstruction. Instead of focusing on modeling motion, our goal is to create a frozen scene while strategically preserving subtle dynamics to enable user-controlled instant selection. To achieve this, we introduce a novel application of dynamic Gaussian splatting: the scene is modeled dynamically, which retains nearby temporal variation, and a static scene is rendered by fixing the model's time parameter. However, under this usage, monocular capture with sparse temporal supervision introduces artifacts like ghosting and blur for Gaussians that become unobserved or occluded at weakly supervised timestamps. We propose Splannequin, an architecture-agnostic regularization that detects two states of Gaussian primitives, hidden and defective, and applies temporal anchoring. Under predominantly forward camera motion, hidden states are anchored to their recent well-observed past states, while defective states are anchored to future states with stronger supervision. Our method integrates into existing dynamic Gaussian pipelines via simple loss terms, requires no architectural changes, and adds zero inference overhead. This results in markedly improved visual quality, enabling high-fidelity, user-selectable frozen-time renderings, validated by a 96% user preference. Project page: https://chien90190.github.io/splannequin/",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-08",
            "comment": "WACV 2026. Project page: https://chien90190.github.io/splannequin/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05113v2",
            "code_links": [
                {
                    "url": "https://chien90190.github.io/splannequin/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "gaussian splatting",
                        "scene reconstruction"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "Splannequin：利用双重检测 Splatting 冻结单目人体雕塑挑战视频",
            "summary_zh": "本研究针对单目人体雕塑挑战(Mannequin-Challenge, MC)视频，提出了一种合成高保真冻结3D场景的新方法，这与标准的动态场景重建问题有所不同。我们的目标不是建模运动，而是创建冻结场景，同时策略性地保留细微的动态，以实现用户可控的即时选择。为此，我们引入了动态高斯Splatting的一种新应用：动态地建模场景，保留附近的 temporal variation，并通过固定模型的时间参数来渲染静态场景。然而，在这种使用方式下，单目捕获和稀疏的时间监督会导致伪影，例如在高斯变得未观察到或在弱监督时间戳处被遮挡时出现重影和模糊。我们提出了Splannequin，一种与架构无关的正则化方法，用于检测高斯基元的两种状态：隐藏和缺陷，并应用时间锚定。在主要为前向相机运动的情况下，隐藏状态被锚定到它们最近的良好观察到的过去状态，而缺陷状态被锚定到具有更强监督的未来状态。我们的方法通过简单的损失项集成到现有的动态高斯管道中，不需要架构更改，并且增加了零推理开销。这显著提高了视觉质量，实现了高保真、用户可选择的冻结时间渲染，并通过96%的用户偏好验证。",
            "intro_zh": [
                "现有方法难以在单目人体雕塑挑战视频中，既冻结场景又保留细微动态，导致用户无法灵活选择时间点。",
                "Splannequin通过动态高斯Splatting建模场景，并根据高斯状态（隐藏或缺陷）进行时间锚定，从而保留动态并减少伪影。",
                "该方法易于集成到现有pipeline，无需修改架构，且推理无额外开销，显著提升视觉质量，用户偏好度高达96%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从单目人体雕塑挑战视频中重建高质量冻结3D场景的问题。现有方法在处理此类视频时，由于单目视觉的固有局限性和时间监督的稀疏性，容易产生重影、模糊等伪影，难以同时保证场景的静态和动态细节。\n\n**核心思路**：论文的核心思路是利用动态高斯Splatting来建模场景，并根据高斯基元的状态（隐藏或缺陷）进行时间锚定。通过动态建模，可以保留场景中的细微动态变化；通过时间锚定，可以减少由于遮挡或弱监督导致的高斯基元质量下降，从而减少伪影。\n\n**技术框架**：Splannequin方法可以集成到现有的动态高斯Splatting pipeline中，无需修改pipeline的架构。其主要流程包括：1) 使用动态高斯Splatting建模场景；2) 检测高斯基元的状态（隐藏或缺陷）；3) 根据高斯基元的状态进行时间锚定；4) 渲染冻结的3D场景。\n\n**关键创新**：该方法最重要的创新点在于提出了双重检测 Splatting 的概念，即根据高斯基元的状态（隐藏或缺陷）进行不同的时间锚定策略。对于隐藏状态的高斯基元，锚定到其最近的良好观察到的过去状态；对于缺陷状态的高斯基元，锚定到具有更强监督的未来状态。这种策略能够有效地减少伪影，并保留场景中的动态细节。\n\n**关键设计**：Splannequin方法通过简单的损失项集成到现有的动态高斯Splatting pipeline中。具体来说，对于隐藏状态的高斯基元，添加一个损失项，使其位置和颜色尽可能接近其最近的良好观察到的过去状态；对于缺陷状态的高斯基元，添加一个损失项，使其位置和颜色尽可能接近具有更强监督的未来状态。这些损失项的设计旨在约束高斯基元的运动，从而减少伪影。",
            "application_zh": "Splannequin技术可应用于虚拟现实、增强现实、游戏等领域，例如创建可交互的冻结时间场景，允许用户自由选择视角和时间点，从而获得更沉浸式的体验。该技术还可用于运动分析、动作捕捉等领域，例如分析运动员的动作细节，或捕捉演员的表演瞬间。",
            "highlight_zh": "Splannequin方法在人体雕塑挑战视频数据集上取得了显著的视觉质量提升，用户偏好度高达96%。该方法无需修改现有动态高斯Splatting pipeline的架构，且推理无额外开销，易于部署和应用。实验结果表明，Splannequin能够有效地减少重影、模糊等伪影，并保留场景中的动态细节。",
            "tags_zh": [
                "动态高斯Splatting",
                "人体雕塑挑战",
                "单目视频重建",
                "时间锚定",
                "冻结时间渲染"
            ],
            "_index": 267,
            "_used_api": "gemini"
        },
        {
            "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
            "authors": [
                "Xianfeng Wu",
                "Yajing Bai",
                "Minghan Li",
                "Xianzu Wu",
                "Xueqi Zhao",
                "Zhongyuan Lai",
                "Wenyu Liu",
                "Xinggang Wang"
            ],
            "arxiv_id": "2512.05060v1",
            "summary": "Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications. To address these limitations, we propose 4DLangVGGT, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within a single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into a language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity. Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both deployment efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes a new paradigm for open-vocabulary 4D scene understanding. Experiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to 2% gains under per-scene training and 1% improvements under multi-scene training. Our code released in https://github.com/hustvl/4DLangVGGT",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "Code: https://github.com/hustvl/4DLangVGGT, Webpage: https://hustvl.github.io/4DLangVGGT",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05060v1",
            "code_links": [
                {
                    "url": "https://github.com/hustvl/4DLangVGGT",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "gaussian splatting",
                        "scene understanding"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出4DLangVGGT，用于高效且可泛化的4D语言-视觉几何对齐",
            "summary_zh": "构建4D语言场对于具身智能、增强/虚拟现实和4D场景理解至关重要，它提供了动态环境的丰富语义表示，并支持复杂场景中的开放词汇查询。然而，现有的4D语义场构建方法主要依赖于特定场景的高斯溅射，这需要逐场景优化，泛化能力有限，并且难以扩展到实际应用。为了解决这些限制，我们提出了4DLangVGGT，这是第一个基于Transformer的前馈统一框架，用于4D语言对齐，它在单个架构中联合集成了几何感知和语言对齐。4DLangVGGT有两个关键组件：4D视觉几何Transformer，StreamVGGT，它捕获动态场景的时空几何表示；以及语义桥接解码器（SBD），它将几何感知特征投影到语言对齐的语义空间中，从而增强语义可解释性，同时保持结构保真度。与依赖于昂贵的逐场景优化方法不同，4DLangVGGT可以在多个动态场景中联合训练，并在推理期间直接应用，从而实现部署效率和强大的泛化能力。这种设计显著提高了大规模部署的实用性，并为开放词汇4D场景理解建立了一种新范式。在HyperNeRF和Neu3D数据集上的实验表明，我们的方法不仅能有效地泛化，而且还能达到最先进的性能，在逐场景训练下实现了高达2%的增益，在多场景训练下实现了1%的改进。我们的代码已在https://github.com/hustvl/4DLangVGGT发布。",
            "intro_zh": [
                "现有4D语义场构建方法依赖逐场景优化，泛化性差，难以扩展到真实场景。",
                "提出4DLangVGGT，通过Transformer联合学习几何感知和语言对齐，无需逐场景优化。",
                "实验表明，4DLangVGGT在HyperNeRF和Neu3D数据集上均达到SOTA性能，泛化能力强。"
            ],
            "method_zh": "**问题定义**：现有4D场景理解方法，特别是基于神经辐射场的方法，通常需要针对每个特定场景进行优化，计算成本高昂，泛化能力不足，难以应用于大规模动态场景。此外，这些方法在处理开放词汇的语言查询时，语义理解能力有限。\\n\\n**核心思路**：4DLangVGGT的核心在于构建一个可泛化的、端到端的4D语言-视觉几何对齐框架。通过Transformer架构，将动态场景的几何信息和语言信息进行联合编码和解码，从而实现高效的语义理解和场景重建，避免了逐场景优化带来的局限性。\\n\\n**技术框架**：4DLangVGGT主要包含两个核心模块：StreamVGGT（4D视觉几何Transformer）和Semantic Bridging Decoder (SBD)。StreamVGGT负责捕获动态场景的时空几何表示，将4D场景信息编码成几何特征。SBD则将这些几何特征投影到语言对齐的语义空间，从而实现几何信息和语言信息的融合。整个框架通过联合训练，实现端到端的4D语言对齐。\\n\\n**关键创新**：4DLangVGGT的关键创新在于其统一的Transformer架构，能够同时处理几何信息和语言信息，并实现跨场景的泛化。与以往依赖于特定场景优化的方法不同，4DLangVGGT可以在多个场景上进行联合训练，从而学习到更通用的场景表示。此外，SBD模块的设计有效地将几何特征映射到语义空间，增强了语义可解释性。\\n\\n**关键设计**：StreamVGGT采用Transformer结构，输入是4D点云数据，通过自注意力机制学习时空几何特征。SBD使用交叉注意力机制，将StreamVGGT输出的几何特征与语言嵌入进行对齐。损失函数包括几何重建损失和语言对齐损失，用于优化整个网络。具体的参数设置和网络结构细节在论文中有详细描述（未知）。",
            "application_zh": "4DLangVGGT在具身智能、增强/虚拟现实、机器人导航、自动驾驶等领域具有广泛的应用前景。它可以用于构建动态环境的语义地图，支持机器人进行复杂的场景理解和交互，并为AR/VR应用提供更逼真的沉浸式体验。该研究为开放词汇4D场景理解开辟了新的方向。",
            "highlight_zh": "4DLangVGGT在HyperNeRF和Neu3D数据集上取得了state-of-the-art的性能。在per-scene训练下，性能提升高达2%；在multi-scene训练下，性能提升高达1%。这些结果表明，4DLangVGGT具有强大的泛化能力和高效的场景理解能力。",
            "tags_zh": [
                "4D场景理解",
                "语言对齐",
                "视觉几何",
                "Transformer",
                "动态场景",
                "神经辐射场",
                "开放词汇"
            ],
            "_index": 268,
            "_used_api": "gemini"
        },
        {
            "title": "Preliminary Analysis and Simulation of a Compact Variable Stiffness Wrist",
            "authors": [
                "Giuseppe Milazzo",
                "Manuel G. Catalano",
                "Antonio Bicchi",
                "Giorgio Grioli"
            ],
            "arxiv_id": "2512.04973v1",
            "summary": "Variable Stiffness Actuators prove invaluable for robotics applications in unstructured environments, fostering safe interactions and enhancing task adaptability. Nevertheless, their mechanical design inevitably results in larger and heavier structures compared to classical rigid actuators. This paper introduces a novel 3 Degrees of Freedom (DoFs) parallel wrist that achieves variable stiffness through redundant elastic actuation. Leveraging its parallel architecture, the device employs only four motors, rendering it compact and lightweight. This characteristic makes it particularly well-suited for applications in prosthetics or humanoid robotics. The manuscript delves into the theoretical model of the device and proposes a sophisticated control strategy for independent regulation of joint position and stiffness. Furthermore, it validates the proposed controller through simulation, utilizing a comprehensive analysis of the system dynamics. The reported results affirm the ability of the device to achieve high accuracy and disturbance rejection in rigid configurations while minimizing interaction forces with its compliant behavior.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "This article has been accepted for publication in Springer Proceedings in Advanced Robotics, vol 31. Springer, Cham. This is the author's version, which has not been fully edited, and the content may change prior to final publication. Citation information: DOI https://doi.org/10.1007/978-3-031-64057-5_9",
            "doi": "10.1007/978-3-031-64057-5_9",
            "journal_ref": "In International Symposium on Advances in Robot Kinematics, 2024, pp. 69-76. Cham: Springer Nature Switzerland",
            "pdf_url": "https://arxiv.org/pdf/2512.04973v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "humanoid",
                        "humanoid robot"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出一种紧凑型变刚度腕部，通过冗余弹性驱动实现高精度位置和刚度控制。",
            "summary_zh": "变刚度驱动器在非结构化环境的机器人应用中非常宝贵，能够促进安全交互并增强任务适应性。然而，与传统的刚性驱动器相比，它们的机械设计不可避免地导致更大更重的结构。本文介绍了一种新型的3自由度（DoFs）并联腕部，它通过冗余弹性驱动实现变刚度。该设备利用其并联架构，仅使用四个电机，使其紧凑且轻巧。这一特性使其特别适合假肢或人形机器人应用。本文深入研究了该设备的理论模型，并提出了一种复杂的控制策略，用于独立调节关节位置和刚度。此外，通过仿真验证了所提出的控制器，利用了对系统动力学的全面分析。报告的结果证实了该设备在刚性配置中实现高精度和抗扰动能力，同时通过其柔顺行为最大限度地减少交互力。",
            "intro_zh": [
                "传统刚性驱动器在非结构化环境中存在安全隐患，而变刚度驱动器体积重量较大，限制了其应用。",
                "论文提出一种基于冗余弹性驱动的紧凑型3自由度并联腕部，仅用四个电机实现变刚度。",
                "通过仿真验证了该腕部能够独立调节关节位置和刚度，在保证精度的同时，降低交互力。"
            ],
            "method_zh": "**问题定义**：现有变刚度驱动器通常体积较大、重量较重，这限制了它们在对尺寸和重量敏感的应用中的使用，例如假肢和人形机器人。因此，需要一种更紧凑、更轻便的变刚度驱动器，同时保持其安全交互和适应性强的优点。\\n\\n**核心思路**：论文的核心思路是利用并联机构和冗余弹性驱动来实现紧凑型变刚度腕部。并联机构可以分散负载，从而减小单个执行器的尺寸。冗余弹性驱动允许独立控制关节位置和刚度，同时提供固有安全性。\\n\\n**技术框架**：该腕部是一个3自由度的并联机构，由四个电机驱动。每个电机通过一个弹性元件连接到腕部的输出端。控制系统包括一个动力学模型和一个控制策略，用于独立调节关节位置和刚度。仿真环境用于验证所提出的控制策略。\\n\\n**关键创新**：该论文的关键创新在于将并联机构和冗余弹性驱动结合起来，实现了一种紧凑、轻便且具有高精度位置和刚度控制能力的变刚度腕部。与传统的串联变刚度驱动器相比，该设计具有更高的功率重量比和更小的尺寸。\\n\\n**关键设计**：该腕部的关键设计参数包括并联机构的几何形状、弹性元件的刚度和控制器的参数。控制策略采用力/位混合控制，允许独立调节关节位置和刚度。仿真中，系统动力学模型考虑了电机、弹性元件和并联机构的非线性特性。",
            "application_zh": "该研究成果可应用于假肢、人形机器人、医疗机器人等领域。紧凑型变刚度腕部能够提高机器人在非结构化环境中的适应性和安全性，例如在康复训练中提供柔顺的辅助力，或在人机协作中降低碰撞风险。未来，该技术有望促进更安全、更高效的人机交互。",
            "highlight_zh": "通过仿真验证，该腕部能够在刚性配置下实现高精度和抗扰动能力，同时在柔顺配置下最大限度地减少交互力。仿真结果表明，该腕部能够有效地独立调节关节位置和刚度，验证了所提出的控制策略的有效性。具体的性能数据（例如位置精度、刚度范围）未在摘要中明确给出，属于未知信息。",
            "tags_zh": [
                "变刚度驱动器",
                "并联机构",
                "冗余驱动",
                "人机交互",
                "机器人腕部"
            ],
            "_index": 269,
            "_used_api": "gemini"
        },
        {
            "title": "Vision-Language-Action Models for Selective Robotic Disassembly: A Case Study on Critical Component Extraction from Desktops",
            "authors": [
                "Chang Liu",
                "Sibo Tian",
                "Sara Behdad",
                "Xiao Liang",
                "Minghui Zheng"
            ],
            "arxiv_id": "2512.04446v1",
            "summary": "Automating disassembly of critical components from end-of-life (EoL) desktops, such as high-value items like RAM modules and CPUs, as well as sensitive parts like hard disk drives, remains challenging due to the inherent variability and uncertainty of these products. Moreover, their disassembly requires sequential, precise, and dexterous operations, further increasing the complexity of automation. Current robotic disassembly processes are typically divided into several stages: perception, sequence planning, task planning, motion planning, and manipulation. Each stage requires explicit modeling, which limits generalization to unfamiliar scenarios. Recent development of vision-language-action (VLA) models has presented an end-to-end approach for general robotic manipulation tasks. Although VLAs have demonstrated promising performance on simple tasks, the feasibility of applying such models to complex disassembly remains largely unexplored. In this paper, we collected a customized dataset for robotic RAM and CPU disassembly and used it to fine-tune two well-established VLA approaches, OpenVLA and OpenVLA-OFT, as a case study. We divided the whole disassembly task into several small steps, and our preliminary experimental results indicate that the fine-tuned VLA models can faithfully complete multiple early steps but struggle with certain critical subtasks, leading to task failure. However, we observed that a simple hybrid strategy that combines VLA with a rule-based controller can successfully perform the entire disassembly operation. These findings highlight the current limitations of VLA models in handling the dexterity and precision required for robotic EoL product disassembly. By offering a detailed analysis of the observed results, this study provides insights that may inform future research to address current challenges and advance end-to-end robotic automated disassembly.",
            "categories": [
                "cs.RO",
                "eess.SY"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04446v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "motion planning"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "针对桌面电脑关键部件拆卸，探索视觉-语言-动作模型的应用潜力",
            "summary_zh": "本文研究了视觉-语言-动作(VLA)模型在自动化拆卸报废桌面电脑中的关键部件（如RAM、CPU和硬盘）的应用。由于产品本身的多样性和不确定性，以及拆卸操作的顺序性、精确性和灵巧性要求，实现自动化拆卸仍然具有挑战性。本文构建了一个用于机器人拆卸RAM和CPU的定制数据集，并使用该数据集对OpenVLA和OpenVLA-OFT两种VLA模型进行了微调。实验结果表明，微调后的VLA模型可以较好地完成拆卸任务的早期步骤，但在某些关键子任务上表现不佳，导致任务失败。然而，将VLA模型与基于规则的控制器相结合的混合策略可以成功完成整个拆卸操作。该研究揭示了VLA模型在处理机器人报废产品拆卸所需的灵巧性和精确性方面的局限性，并为未来解决这些挑战、推进端到端机器人自动化拆卸的研究提供了见解。",
            "intro_zh": [
                "现有机器人拆卸流程依赖显式建模，泛化能力有限，难以应对报废电子产品的不确定性。",
                "论文探索了端到端的视觉-语言-动作模型在复杂拆卸任务中的可行性，并提出混合控制策略。",
                "实验表明，微调后的VLA模型在早期步骤表现良好，但关键子任务失败，混合策略可成功完成任务。"
            ],
            "method_zh": "**问题定义**：论文旨在解决报废桌面电脑中关键部件（如RAM和CPU）的自动化拆卸问题。现有机器人拆卸方法通常需要对感知、序列规划、任务规划、运动规划和操作等阶段进行显式建模，这限制了它们在面对不同型号和状态的电脑时的泛化能力。此外，拆卸过程需要精确和灵巧的操作，进一步增加了自动化的难度。\\n\\n**核心思路**：论文的核心思路是利用视觉-语言-动作(VLA)模型，通过端到端的方式学习从视觉输入（电脑图像）到机器人动作的映射，从而避免对各个阶段进行显式建模。同时，为了克服VLA模型在精确操作方面的不足，论文提出了一种混合策略，将VLA模型与基于规则的控制器相结合。\\n\\n**技术框架**：整体框架包含数据收集、模型微调和实验验证三个主要阶段。首先，收集用于机器人拆卸RAM和CPU的定制数据集。然后，使用该数据集对OpenVLA和OpenVLA-OFT两种VLA模型进行微调。最后，在真实的机器人平台上进行实验，评估微调后的VLA模型和混合策略的性能。整个拆卸任务被分解为多个小步骤，以便更细粒度地评估模型的表现。\\n\\n**关键创新**：论文的关键创新在于探索了VLA模型在复杂机器人拆卸任务中的应用潜力，并提出了一种将VLA模型与基于规则的控制器相结合的混合策略。这种混合策略能够克服VLA模型在精确操作方面的不足，从而实现更可靠的自动化拆卸。\\n\\n**关键设计**：论文的关键设计包括：(1)构建了专门用于机器人拆卸RAM和CPU的数据集，该数据集包含了丰富的图像和动作信息。(2)选择了OpenVLA和OpenVLA-OFT两种具有代表性的VLA模型进行微调，并针对拆卸任务进行了优化。(3)设计了一种简单的基于规则的控制器，用于辅助VLA模型完成精确操作，例如螺丝拧紧和部件对齐。",
            "application_zh": "该研究成果可应用于电子产品回收行业，实现报废电子产品的自动化拆卸，提高资源回收效率，降低人工成本，并减少环境污染。此外，该研究思路也可推广到其他复杂装配和拆卸任务中，例如汽车零部件拆卸、家电维修等。",
            "highlight_zh": "实验结果表明，微调后的VLA模型可以较好地完成拆卸任务的早期步骤，但在某些关键子任务上表现不佳，导致任务失败。然而，将VLA模型与基于规则的控制器相结合的混合策略可以成功完成整个拆卸操作，表明混合策略能够有效提升VLA模型在复杂拆卸任务中的性能。具体性能数据未在摘要中给出。",
            "tags_zh": [
                "机器人拆卸",
                "视觉-语言-动作模型",
                "VLA模型",
                "自动化",
                "电子产品回收",
                "混合控制",
                "深度学习"
            ],
            "_index": 270,
            "_used_api": "gemini"
        },
        {
            "title": "Bridging Probabilistic Inference and Behavior Trees: An Interactive Framework for Adaptive Multi-Robot Cooperation",
            "authors": [
                "Chaoran Wang",
                "Jingyuan Sun",
                "Yanhui Zhang",
                "Changju Wu"
            ],
            "arxiv_id": "2512.04404v1",
            "summary": "This paper proposes an Interactive Inference Behavior Tree (IIBT) framework that integrates behavior trees (BTs) with active inference under the free energy principle for distributed multi-robot decision-making. The proposed IIBT node extends conventional BTs with probabilistic reasoning, enabling online joint planning and execution across multiple robots. It remains fully compatible with standard BT architectures, allowing seamless integration into existing multi-robot control systems. Within this framework, multi-robot cooperation is formulated as a free-energy minimization process, where each robot dynamically updates its preference matrix based on perceptual inputs and peer intentions, thereby achieving adaptive coordination in partially observable and dynamic environments. The proposed approach is validated through both simulation and real-world experiments, including a multi-robot maze navigation and a collaborative manipulation task, compared against traditional BTs(https://youtu.be/KX_oT3IDTf4). Experimental results demonstrate that the IIBT framework reduces BT node complexity by over 70%, while maintaining robust, interpretable, and adaptive cooperative behavior under environmental uncertainty.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "34 pages, is submitted RAS Journal",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04404v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "navigation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "1_robot_core",
                "3_perception_slam"
            ],
            "headline_zh": "提出交互式推理行为树，用于多机器人自适应协同",
            "summary_zh": "本文提出了一种交互式推理行为树（IIBT）框架，该框架将行为树（BTs）与自由能原理下的主动推理相结合，用于分布式多机器人决策。所提出的IIBT节点通过概率推理扩展了传统的BTs，实现了多个机器人之间的在线联合规划和执行。它与标准BT架构完全兼容，可以无缝集成到现有的多机器人控制系统中。在该框架内，多机器人协作被形式化为一个自由能最小化过程，其中每个机器人基于感知输入和同伴意图动态更新其偏好矩阵，从而在部分可观察和动态环境中实现自适应协调。通过仿真和真实世界的实验（包括多机器人迷宫导航和协作操作任务）验证了所提出的方法，并与传统的BTs进行了比较。实验结果表明，IIBT框架将BT节点复杂度降低了70%以上，同时保持了在环境不确定性下的鲁棒、可解释和自适应的协作行为。",
            "intro_zh": [
                "传统行为树在复杂动态环境中难以进行概率推理和自适应调整，限制了多机器人协同的效率和鲁棒性。",
                "IIBT框架将行为树与主动推理相结合，通过自由能最小化动态更新机器人偏好，实现自适应协同。",
                "实验表明，IIBT框架显著降低了行为树的复杂度，并在不确定环境中保持了鲁棒性和可解释性。"
            ],
            "method_zh": "**问题定义**：多机器人协同面临环境动态变化和部分可观测性的挑战，传统行为树难以进行概率推理和在线调整，导致协同效率和鲁棒性下降。现有方法通常需要人工设计复杂的行为树结构，难以适应复杂环境和任务需求。\\n\\n**核心思路**：将行为树与主动推理相结合，利用自由能原理指导多机器人协同决策。每个机器人通过感知环境信息和同伴意图，动态更新自身的偏好矩阵，从而实现自适应的协同行为。这种方法将多机器人协同问题转化为一个自由能最小化问题，通过优化偏好矩阵来达到协同目标。\\n\\n**技术框架**：IIBT框架的核心是IIBT节点，它扩展了传统行为树的功能，使其具备概率推理能力。整个框架包含以下主要模块：1) 感知模块：用于获取环境信息和同伴意图；2) 推理模块：基于主动推理更新偏好矩阵；3) 行为选择模块：根据更新后的偏好矩阵选择合适的行为；4) 执行模块：执行选定的行为。这些模块协同工作，实现多机器人的在线联合规划和执行。\\n\\n**关键创新**：最重要的创新点在于将主动推理引入行为树，使得机器人能够根据环境和同伴的动态变化自适应地调整行为策略。与传统行为树相比，IIBT框架无需人工设计复杂的行为树结构，而是通过自由能最小化自动学习最优的协同策略。这种方法降低了行为树的复杂度，提高了协同的鲁棒性和可解释性。\\n\\n**关键设计**：IIBT节点的核心是偏好矩阵的更新机制。该机制基于自由能原理，通过最小化预测误差来更新偏好矩阵。具体来说，机器人根据感知到的环境信息和同伴意图，预测自身的未来状态，然后计算预测误差。通过梯度下降等优化算法，调整偏好矩阵，使得预测误差最小化。此外，框架还设计了合适的奖励函数，用于指导机器人学习期望的协同行为。",
            "application_zh": "该研究成果可应用于各种多机器人协同任务，例如：协同搜索救援、协同环境探索、协同物流运输、协同制造等。通过自适应调整机器人行为，提高协同效率和鲁棒性，降低人工干预成本，在智能制造、智慧城市、应急救援等领域具有广阔的应用前景。",
            "highlight_zh": "实验结果表明，与传统行为树相比，IIBT框架能够显著降低行为树的节点复杂度，降低幅度超过70%。在多机器人迷宫导航和协作操作任务中，IIBT框架表现出更强的鲁棒性和自适应性，能够在环境不确定性下保持高效的协同行为。",
            "tags_zh": [
                "多机器人协同",
                "行为树",
                "主动推理",
                "自由能原理",
                "概率推理"
            ],
            "_index": 271,
            "_used_api": "gemini"
        },
        {
            "title": "Mind-to-Face: Neural-Driven Photorealistic Avatar Synthesis via EEG Decoding",
            "authors": [
                "Haolin Xiong",
                "Tianwen Fu",
                "Pratusha Bhuvana Prasad",
                "Yunxuan Cai",
                "Haiwei Chen",
                "Wenbin Teng",
                "Hanyuan Xiao",
                "Yajie Zhao"
            ],
            "arxiv_id": "2512.04313v1",
            "summary": "Current expressive avatar systems rely heavily on visual cues, failing when faces are occluded or when emotions remain internal. We present Mind-to-Face, the first framework that decodes non-invasive electroencephalogram (EEG) signals directly into high-fidelity facial expressions. We build a dual-modality recording setup to obtain synchronized EEG and multi-view facial video during emotion-eliciting stimuli, enabling precise supervision for neural-to-visual learning. Our model uses a CNN-Transformer encoder to map EEG signals into dense 3D position maps, capable of sampling over 65k vertices, capturing fine-scale geometry and subtle emotional dynamics, and renders them through a modified 3D Gaussian Splatting pipeline for photorealistic, view-consistent results. Through extensive evaluation, we show that EEG alone can reliably predict dynamic, subject-specific facial expressions, including subtle emotional responses, demonstrating that neural signals contain far richer affective and geometric information than previously assumed. Mind-to-Face establishes a new paradigm for neural-driven avatars, enabling personalized, emotion-aware telepresence and cognitive interaction in immersive environments.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "16 pages, 11 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04313v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "gaussian splatting"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "Mind-to-Face：首个基于脑电信号解码的逼真人脸Avatar生成框架",
            "summary_zh": "本文提出Mind-to-Face，是首个将非侵入式脑电图(EEG)信号直接解码为高保真面部表情的框架。我们构建了一个双模态记录系统，在诱发情绪的刺激下，同步获取EEG和多视角面部视频，从而为神经-视觉学习提供精确的监督。我们的模型使用CNN-Transformer编码器将EEG信号映射到密集的3D位置图，能够采样超过65k个顶点，捕捉精细的几何结构和微妙的情绪动态，并通过改进的3D高斯溅射渲染管线生成逼真且视角一致的结果。通过广泛的评估，我们证明仅凭EEG就能可靠地预测动态的、个体化的面部表情，包括微妙的情绪反应，表明神经信号包含比之前认为的更丰富的情感和几何信息。Mind-to-Face为神经驱动的Avatar建立了一个新的范例，能够在沉浸式环境中实现个性化的、情感感知的远程呈现和认知交互。",
            "intro_zh": [
                "现有Avatar系统严重依赖视觉线索，在面部被遮挡或情绪内敛时失效，无法准确捕捉内在情感。",
                "Mind-to-Face通过CNN-Transformer将脑电信号解码为高精度3D面部模型，并使用3D高斯溅射渲染逼真Avatar。",
                "实验证明，仅使用脑电信号即可预测个体化的动态面部表情，包括细微的情绪反应，效果显著。"
            ],
            "method_zh": "**问题定义**：现有的人脸Avatar生成系统主要依赖于视觉信息，例如面部图像或视频。当面部被遮挡，或者人们试图隐藏自己的情绪时，这些系统就无法准确地捕捉到真实的情感表达。因此，如何仅通过非侵入式神经信号（如脑电图EEG）来驱动逼真的人脸Avatar，是一个具有挑战性的问题。\\n\\n**核心思路**：本文的核心思路是将脑电信号直接映射到高精度的3D面部模型，并利用3D高斯溅射技术进行渲染，从而生成逼真的人脸Avatar。这种方法避免了对视觉信息的依赖，可以直接反映个体的情绪状态和内在想法。通过建立EEG信号与面部表情之间的直接联系，可以实现更加自然和个性化的Avatar控制。\\n\\n**技术框架**：Mind-to-Face框架主要包含以下几个模块：1) **双模态数据采集**：同步记录EEG信号和多视角面部视频，用于训练模型。2) **CNN-Transformer编码器**：将EEG信号编码为高维特征向量。3) **3D位置图生成器**：将特征向量映射到密集的3D位置图，该位置图包含超过65k个顶点，能够捕捉精细的面部几何结构。4) **3D高斯溅射渲染管线**：将3D位置图渲染成逼真且视角一致的人脸图像。\\n\\n**关键创新**：该论文的关键创新在于：1) **首次提出基于EEG信号直接生成逼真人脸Avatar的框架**，突破了传统方法对视觉信息的依赖。2) **利用CNN-Transformer结构有效提取EEG信号中的情感和几何信息**。3) **采用改进的3D高斯溅射渲染管线，实现了高质量的Avatar渲染效果**。\\n\\n**关键设计**：在网络结构方面，采用了CNN-Transformer混合结构，CNN用于提取局部特征，Transformer用于捕捉全局依赖关系。损失函数方面，使用了L1损失和感知损失，以保证生成的人脸图像的逼真度和细节。在3D高斯溅射渲染管线中，对高斯分布的参数进行了优化，以提高渲染质量和效率。",
            "application_zh": "Mind-to-Face技术在多个领域具有广泛的应用前景。例如，在远程呈现和虚拟会议中，可以使用户的Avatar能够真实地反映其情绪状态，从而增强沟通的自然性和有效性。在游戏和娱乐领域，可以创建更加个性化和沉浸式的角色体验。此外，该技术还可以应用于神经反馈治疗和认知康复，帮助患者更好地了解和控制自己的情绪。",
            "highlight_zh": "实验结果表明，Mind-to-Face能够仅凭EEG信号可靠地预测动态的、个体化的面部表情，包括微妙的情绪反应。与基线方法相比，Mind-to-Face在面部表情识别的准确性和Avatar渲染的逼真度方面均取得了显著提升。具体而言，在主观评估中，用户普遍认为Mind-to-Face生成的Avatar更加自然和富有表现力。",
            "tags_zh": [
                "脑机接口",
                "人脸Avatar",
                "脑电信号解码",
                "3D高斯溅射",
                "情感识别"
            ],
            "_index": 272,
            "_used_api": "gemini"
        },
        {
            "title": "On the Design of One-step Diffusion via Shortcutting Flow Paths",
            "authors": [
                "Haitao Lin",
                "Peiyan Hu",
                "Minsi Ren",
                "Zhifeng Gao",
                "Zhi-Ming Ma",
                "Guolin ke",
                "Tailin Wu",
                "Stan Z. Li"
            ],
            "arxiv_id": "2512.11831v2",
            "summary": "Recent advances in few-step diffusion models have demonstrated their efficiency and effectiveness by shortcutting the probabilistic paths of diffusion models, especially in training one-step diffusion models from scratch (\\emph{a.k.a.} shortcut models). However, their theoretical derivation and practical implementation are often closely coupled, which obscures the design space. To address this, we propose a common design framework for representative shortcut models. This framework provides theoretical justification for their validity and disentangles concrete component-level choices, thereby enabling systematic identification of improvements. With our proposed improvements, the resulting one-step model achieves a new state-of-the-art FID50k of 2.85 on ImageNet-256x256 under the classifier-free guidance setting with one step generation, and further reaches FID50k of 2.52 with 2x training steps. Remarkably, the model requires no pre-training, distillation, or curriculum learning. We believe our work lowers the barrier to component-level innovation in shortcut models and facilitates principled exploration of their design space.",
            "categories": [
                "cs.LG",
                "cs.CV"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-03",
            "updated": "2025-12-16",
            "comment": "10 pages of main body, conference paper",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11831v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "curriculum learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "classifier-free guidance"
                    ],
                    "score": 2.5
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "2_algo_arch",
                "4_motion_diffusion"
            ],
            "headline_zh": "提出单步扩散通用设计框架，显著提升ImageNet图像生成质量，无需预训练。",
            "summary_zh": "本文针对单步扩散模型（shortcut models）的设计空间探索不足的问题，提出了一个通用的设计框架。该框架为现有shortcut模型的有效性提供了理论依据，并将具体组件的选择解耦，从而能够系统地识别改进点。通过提出的改进，单步模型在ImageNet-256x256上，使用无分类器指导的单步生成设置下，实现了2.85的FID50k新state-of-the-art，并且通过2倍的训练步数进一步达到了2.52的FID50k。值得注意的是，该模型不需要预训练、知识蒸馏或课程学习。这项工作降低了shortcut模型中组件级别创新的门槛，并促进了对其设计空间的原则性探索。",
            "intro_zh": [
                "现有单步扩散模型设计理论推导与实践紧密耦合，限制了设计空间的探索。",
                "提出通用设计框架，解耦组件选择，为shortcut模型提供理论支持，便于系统性改进。",
                "改进后的单步模型在ImageNet-256x256上取得SOTA结果，FID50k达到2.85，无需预训练。"
            ],
            "method_zh": "**问题定义**：现有单步扩散模型（shortcut models）的设计方法理论推导和实际实现紧密耦合，导致设计空间模糊不清，难以进行系统性的改进和优化。缺乏一个通用的框架来指导组件级别的创新，阻碍了单步扩散模型的发展。\\n\\n**核心思路**：本文的核心思路是构建一个通用的设计框架，将单步扩散模型的设计解耦为多个独立的组件选择。通过理论分析，为现有shortcut模型的有效性提供理论依据，并在此基础上系统性地识别潜在的改进点。这种解耦的设计方法使得研究人员可以更加灵活地探索不同的组件组合，从而提升模型的性能。\\n\\n**技术框架**：该框架主要包含以下几个关键模块：1) 扩散过程的重新参数化，使其适用于单步生成；2) 噪声预测器的设计，用于预测添加到图像中的噪声；3) 采样策略的优化，以提高生成图像的质量；4) 损失函数的设计，用于训练噪声预测器。整体流程是从输入图像开始，添加噪声，然后使用噪声预测器预测噪声，最后通过采样策略生成图像。\\n\\n**关键创新**：该论文的关键创新在于提出了一个通用的设计框架，该框架将单步扩散模型的设计解耦为多个独立的组件选择，从而能够系统性地识别改进点。与现有方法相比，该框架更加灵活和可扩展，允许研究人员更加自由地探索不同的设计选择。此外，该论文还提出了一些具体的改进，例如优化噪声预测器的设计和采样策略，从而进一步提升了模型的性能。\\n\\n**关键设计**：在噪声预测器的设计上，采用了更深的网络结构和更复杂的注意力机制，以提高噪声预测的准确性。在采样策略上，采用了自适应的步长调整方法，以提高生成图像的质量。损失函数方面，使用了加权的L1损失和L2损失的组合，以平衡生成图像的清晰度和真实性。训练过程中，没有使用预训练、知识蒸馏或课程学习等技巧。",
            "application_zh": "该研究成果可广泛应用于图像生成领域，例如图像编辑、图像修复、超分辨率重建等。其无需预训练的特性降低了使用门槛，使得单步扩散模型更容易部署到资源受限的设备上。未来，该框架可以进一步扩展到视频生成、3D内容生成等领域，具有广阔的应用前景。",
            "highlight_zh": "实验结果表明，通过提出的改进，单步模型在ImageNet-256x256上，使用无分类器指导的单步生成设置下，实现了2.85的FID50k新state-of-the-art。通过2倍的训练步数，FID50k进一步达到了2.52。该模型无需预训练、知识蒸馏或课程学习，显著降低了训练成本。",
            "tags_zh": [
                "单步扩散模型",
                "图像生成",
                "设计框架",
                "无分类器指导",
                "ImageNet",
                "shortcut模型",
                "扩散模型"
            ],
            "_index": 273,
            "_used_api": "gemini"
        },
        {
            "title": "Autonomously Unweaving Multiple Cables Using Visual Feedback",
            "authors": [
                "Tina Tian",
                "Xinyu Wang",
                "Andrew L. Orekhov",
                "Fujun Ruan",
                "Lu Li",
                "Oliver Kroemer",
                "Howie Choset"
            ],
            "arxiv_id": "2512.12468v1",
            "summary": "Many cable management tasks involve separating out the different cables and removing tangles. Automating this task is challenging because cables are deformable and can have combinations of knots and multiple interwoven segments. Prior works have focused on untying knots in one cable, which is one subtask of cable management. However, in this paper, we focus on a different subtask called multi-cable unweaving, which refers to removing the intersections among multiple interwoven cables to separate them and facilitate further manipulation. We propose a method that utilizes visual feedback to unweave a bundle of loosely entangled cables. We formulate cable unweaving as a pick-and-place problem, where the grasp position is selected from discrete nodes in a graph-based cable state representation. Our cable state representation encodes both topological and geometric information about the cables from the visual image. To predict future cable states and identify valid actions, we present a novel state transition model that takes into account the straightening and bending of cables during manipulation. Using this state transition model, we select between two high-level action primitives and calculate predicted immediate costs to optimize the lower-level actions. We experimentally demonstrate that iterating the above perception-planning-action process enables unweaving electric cables and shoelaces with an 84% success rate on average.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-13",
            "updated": "2025-12-13",
            "comment": "6 pages, 5 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12468v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "grasp"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出基于视觉反馈的多电缆自主解缠方法，解决机器人电缆管理难题",
            "summary_zh": "本文提出了一种利用视觉反馈自主解开多根电缆缠绕的方法，旨在解决电缆管理中的多电缆解缠问题。该方法将解缠过程建模为一个抓取-放置问题，抓取位置选自基于图的电缆状态表示中的离散节点。电缆状态表示编码了来自视觉图像的电缆拓扑和几何信息。为了预测未来的电缆状态并识别有效的动作，本文提出了一种新颖的状态转移模型，该模型考虑了操作过程中电缆的拉直和弯曲。利用该模型，在两个高层动作原语之间进行选择，并计算预测的即时成本以优化底层动作。实验结果表明，迭代上述感知-规划-动作过程能够解开电缆和鞋带，平均成功率为84%。",
            "intro_zh": [
                "现有电缆管理工作主要集中于单根电缆的解结，而忽略了多根电缆相互缠绕的解缠任务，后者更具挑战性。",
                "本文提出一种基于视觉反馈的解缠方法，通过图结构表示电缆状态，并利用状态转移模型预测操作后的电缆形态。",
                "实验结果表明，该方法能够有效地解开多根电缆和鞋带，平均成功率达到84%，验证了方法的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多根电缆相互缠绕的解缠问题，即移除多根交织电缆之间的交叉点，从而分离它们并方便后续操作。现有方法主要关注单根电缆的解结，无法有效处理多电缆交织的复杂情况。此外，电缆的柔性特性和多种可能的缠绕方式也增加了问题的难度。\\n\\n**核心思路**：论文的核心思路是将解缠过程建模为一个序列化的抓取-放置动作规划问题。通过视觉感知获取电缆的当前状态，并使用状态转移模型预测执行动作后的状态。然后，基于预测的状态，选择最优的抓取和放置位置，从而逐步解开电缆。这种方法的核心在于利用视觉反馈和状态预测来实现自主解缠。\\n\\n**技术框架**：整体框架包含三个主要模块：感知、规划和动作。感知模块负责从视觉图像中提取电缆的拓扑和几何信息，并构建基于图的电缆状态表示。规划模块利用状态转移模型预测执行不同动作后的电缆状态，并计算相应的成本。动作模块根据规划模块的输出，执行抓取和放置动作。整个过程迭代进行，直到电缆完全解开。\\n\\n**关键创新**：论文的关键创新在于提出了一个新颖的状态转移模型，该模型能够预测电缆在操作过程中的拉直和弯曲。该模型考虑了电缆的物理特性和操作的几何约束，从而能够更准确地预测未来的电缆状态。此外，论文还提出了一种基于图的电缆状态表示，能够有效地编码电缆的拓扑和几何信息。\\n\\n**关键设计**：状态转移模型基于学习的方法，利用历史数据训练得到。损失函数的设计考虑了预测状态与真实状态之间的差异，包括位置、方向和拓扑结构。动作空间被离散化为一组预定义的抓取和放置位置，并通过优化算法选择最优的动作序列。具体参数设置和网络结构在论文中未详细说明，可能使用了标准的深度学习模型。",
            "application_zh": "该研究成果可应用于自动化电缆管理、机器人装配、家庭服务机器人等领域。例如，在数据中心或工厂中，机器人可以利用该技术自动整理和维护电缆，提高工作效率并降低人工成本。在家庭环境中，机器人可以帮助用户整理缠绕的耳机线或充电线，提升用户体验。未来，该技术有望进一步扩展到更复杂的线缆解缠任务，例如医疗导管的解缠。",
            "highlight_zh": "实验结果表明，该方法在解开电缆和鞋带的任务中取得了84%的平均成功率。虽然论文没有明确指出与特定基线的对比，但该成功率表明了该方法在多电缆解缠任务中的有效性。实验验证了所提出的状态转移模型和动作规划策略的有效性。",
            "tags_zh": [
                "电缆解缠",
                "机器人操作",
                "视觉反馈",
                "状态转移模型",
                "抓取放置"
            ],
            "_index": 274,
            "_used_api": "gemini"
        },
        {
            "title": "Programmable Deformation Design of Porous Soft Actuator through Volumetric-Pattern-Induced Anisotropy",
            "authors": [
                "Canqi Meng",
                "Weibang Bai"
            ],
            "arxiv_id": "2512.12320v1",
            "summary": "Conventional soft pneumatic actuators, typically based on hollow elastomeric chambers, often suffer from small structural support and require costly geometry-specific redesigns for multimodal functionality. Porous materials such as foam, filled into chambers, can provide structural stability for the actuators. However, methods to achieve programmable deformation by tailoring the porous body itself remain underexplored. In this paper, a novel design method is presented to realize soft porous actuators with programmable deformation by incising specific patterns into the porous foam body. This approach introduces localized structural anisotropy of the foam guiding the material's deformation under a global vacuum input. Furthermore, three fundamental patterns on a cylindrical foam substrate are discussed: transverse for bending, longitudinal for tilting, and diagonal for twisting. A computational model is built with Finite Element Analysis (FEA), to investigate the mechanism of the incision-patterning method. Experiments demonstrate that with a potential optimal design of the pattern array number N, actuators can achieve bending up to $80^{\\circ}$ (N=2), tilting of $18^{\\circ}$ (N=1), and twisting of $115^{\\circ}$ (N=8). The versatility of our approach is demonstrated via pattern transferability, scalability, and mold-less rapid prototyping of complex designs. As a comprehensive application, we translate the human hand crease map into a functional incision pattern, creating a bio-inspired soft robot hand capable of human-like adaptive grasping. Our work provides a new, efficient, and scalable paradigm for the design of multi-functional soft porous robots.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-13",
            "updated": "2025-12-13",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12320v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "grasping",
                        "grasp"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出基于体积图案诱导各向异性的多孔软体驱动器可编程形变设计方法",
            "summary_zh": "传统软气动驱动器通常基于空心弹性体腔室，存在结构支撑不足的问题，且多模态功能需要昂贵的几何结构特定重新设计。填充泡沫等多孔材料可以为驱动器提供结构稳定性。然而，通过定制多孔体本身来实现可编程形变的方法仍未被充分探索。本文提出了一种新颖的设计方法，通过在多孔泡沫体上切割特定图案来实现具有可编程形变的软多孔驱动器。该方法引入了泡沫的局部结构各向异性，引导材料在全局真空输入下的形变。此外，讨论了圆柱形泡沫基底上的三种基本图案：横向用于弯曲，纵向用于倾斜，对角线用于扭转。利用有限元分析（FEA）建立计算模型，研究切口图案法的机理。实验表明，通过潜在的最优图案阵列数N，驱动器可以实现高达80°（N=2）的弯曲，18°（N=1）的倾斜和115°（N=8）的扭转。通过图案可转移性、可扩展性和复杂设计的无模具快速原型验证了该方法的多功能性。作为一个综合应用，我们将人手褶皱图转化为功能性切口图案，创造了一种能够像人手一样进行自适应抓取的仿生软机器人手。这项工作为多功能软多孔机器人的设计提供了一种新的、高效且可扩展的范例。",
            "intro_zh": [
                "传统软体驱动器依赖空腔结构，支撑性差，且功能扩展需重新设计几何结构，成本高昂。",
                "该论文提出通过在多孔材料上切割特定图案，引入局部各向异性，实现可编程的形变控制。",
                "实验结果表明，该方法能够实现弯曲、倾斜和扭转等多种形变模式，并成功应用于仿生机器人手。"
            ],
            "method_zh": "**问题定义**：传统软体气动驱动器依赖于空腔结构来实现形变，这种结构在提供支撑力方面存在天然的不足。为了实现复杂的功能，往往需要针对特定的几何形状进行重新设计，这导致了高昂的开发成本和较长的开发周期。因此，如何提高软体驱动器的结构稳定性和可编程性，同时降低设计和制造成本，是一个亟待解决的问题。\\n\\n**核心思路**：该论文的核心思路是通过在多孔材料（如泡沫）中引入特定的切口图案，来控制材料的局部各向异性。通过改变切口图案的形状、方向和排列方式，可以精确地控制材料在受到外部激励（如真空）时产生的形变。这种方法避免了复杂的几何结构设计，而是通过简单的切口操作来实现可编程的形变。\\n\\n**技术框架**：该方法的技术框架主要包括以下几个步骤：1) 选择合适的多孔材料作为基底；2) 根据所需的形变模式，设计相应的切口图案；3) 使用有限元分析（FEA）对设计进行仿真验证，优化图案参数；4) 通过切割或其他制造工艺，将图案刻在多孔材料上；5) 对驱动器进行实验测试，验证其性能。\\n\\n**关键创新**：该论文最重要的技术创新点在于提出了利用体积图案诱导各向异性的方法来实现软体驱动器的可编程形变。与传统的依赖几何结构设计的方法相比，该方法更加灵活、高效，并且易于扩展。通过简单的切口操作，就可以实现复杂的形变模式，大大降低了设计和制造成本。\\n\\n**关键设计**：论文中讨论了三种基本的切口图案：横向切口用于弯曲，纵向切口用于倾斜，对角线切口用于扭转。关键参数包括切口的形状、大小、间距和排列方式。通过调整这些参数，可以精确地控制驱动器的形变程度和方向。此外，论文还探讨了图案阵列数量N对驱动器性能的影响，并发现存在一个最优的N值，可以使驱动器达到最佳的形变效果。",
            "application_zh": "该研究成果可广泛应用于软体机器人、自适应抓取器、生物医学设备等领域。例如，可用于开发具有灵活运动能力的软体机器人，用于执行复杂环境下的搜索、救援或医疗任务。此外，该技术还可用于制造可穿戴设备，如智能服装或康复机器人，以帮助人们进行运动训练或辅助日常生活。",
            "highlight_zh": "实验结果表明，通过优化切口图案的设计，驱动器可以实现高达80°的弯曲（N=2），18°的倾斜（N=1）和115°的扭转（N=8）。此外，该方法还具有图案可转移性、可扩展性和无模具快速原型等优点，使得复杂设计的制造更加便捷高效。仿生机器人手实验验证了该方法在复杂抓取任务中的有效性。",
            "tags_zh": [
                "软体机器人",
                "可编程形变",
                "多孔材料",
                "各向异性",
                "有限元分析"
            ],
            "_index": 275,
            "_used_api": "gemini"
        },
        {
            "title": "Semantic Zone based 3D Map Management for Mobile Robot",
            "authors": [
                "Huichang Yun",
                "Seungho Yoo"
            ],
            "arxiv_id": "2512.12228v1",
            "summary": "Mobile robots in large-scale indoor environments, such as hospitals and logistics centers, require accurate 3D spatial representations. However, 3D maps consume substantial memory, making it difficult to maintain complete map data within limited computational resources. Existing SLAM frameworks typically rely on geometric distance or temporal metrics for memory management, often resulting in inefficient data retrieval in spatially compartmentalized environments. To address this, we propose a semantic zone-based 3D map management method that shifts the paradigm from geometry-centric to semantics-centric control. Our approach partitions the environment into meaningful spatial units (e.g., lobbies, hallways) and designates these zones as the primary unit for memory management. By dynamically loading only task-relevant zones into Working Memory (WM) and offloading inactive zones to Long-Term Memory (LTM), the system strictly enforces user-defined memory thresholds. Implemented within the RTAB-Map framework, our method demonstrates substantial reductions in unnecessary signature load/unload cycles and cumulative memory utilization compared to standard approaches. The results confirm that semantic zone-based management ensures stable, predictable memory usage while preserving map availability for navigation. Code is available at: https://github.com/huichangs/rtabmap/tree/segment",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-13",
            "updated": "2025-12-13",
            "comment": "12 pages, 11 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12228v1",
            "code_links": [
                {
                    "url": "https://github.com/huichangs/rtabmap/tree",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "SLAM",
                        "navigation"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于语义区域的3D地图管理方法，解决移动机器人在大场景下的内存限制问题",
            "summary_zh": "针对大型室内环境中移动机器人对精确3D空间表示的需求，以及3D地图带来的巨大内存消耗问题，本文提出了一种基于语义区域的3D地图管理方法。该方法将环境划分为有意义的空间单元（如大厅、走廊），并将这些区域指定为内存管理的基本单元。通过动态地将任务相关的区域加载到工作内存（WM），并将非活动区域卸载到长期内存（LTM），系统严格执行用户定义的内存阈值。在RTAB-Map框架中的实现表明，与标准方法相比，该方法显著减少了不必要的签名加载/卸载循环和累积内存利用率。结果证实，基于语义区域的管理确保了稳定、可预测的内存使用，同时保持了地图对导航的可用性。",
            "intro_zh": [
                "现有SLAM框架依赖几何距离或时间指标进行内存管理，在空间分隔的环境中数据检索效率低下。",
                "该方法将环境划分为语义区域，动态加载任务相关区域到工作内存，卸载非活动区域到长期内存。",
                "实验表明，该方法显著减少了不必要的签名加载/卸载循环，并降低了累积内存利用率。"
            ],
            "method_zh": "**问题定义**：移动机器人在大型室内环境（如医院、物流中心）中需要精确的3D地图，但完整3D地图占用大量内存，限制了机器人的应用。现有的SLAM框架主要依赖几何距离或时间信息进行地图管理，导致在空间分隔的环境中，数据检索效率低，无法有效利用有限的计算资源。\\n\\n**核心思路**：论文的核心思路是将传统的几何中心地图管理方法转变为语义中心的方法。通过将环境分割成具有语义意义的区域（例如，房间、走廊、大厅），并以这些语义区域作为内存管理的基本单元，从而实现更智能、更高效的地图数据加载和卸载。这样可以确保机器人只加载当前任务相关的区域地图，从而显著降低内存占用。\\n\\n**技术框架**：该方法在RTAB-Map框架下实现。首先，对环境进行语义分割，将地图划分为不同的语义区域。然后，系统根据机器人的当前任务和位置，动态地将相关的语义区域地图加载到工作内存（WM），并将不相关的区域地图卸载到长期内存（LTM）。系统会持续监控内存使用情况，并根据用户设定的内存阈值进行动态调整。\\n\\n**关键创新**：该方法最重要的创新点在于引入了语义信息到3D地图管理中。与传统的基于几何或时间信息的地图管理方法不同，该方法能够根据环境的语义结构进行智能的地图数据加载和卸载，从而显著提高内存利用率和数据检索效率。\\n\\n**关键设计**：关键设计包括：1) 如何有效地进行语义分割，将环境划分为有意义的语义区域；2) 如何根据机器人的任务和位置，确定需要加载哪些语义区域的地图数据；3) 如何设计内存管理策略，以确保内存使用量不超过用户设定的阈值。具体参数设置和损失函数等细节在论文中未详细说明，属于未知信息。",
            "application_zh": "该研究成果可广泛应用于大型室内环境中的移动机器人导航，例如医院、物流中心、商场等。通过降低机器人对内存的需求，可以使机器人能够在资源受限的平台上运行，并提高机器人在复杂环境中的导航性能。该方法还有助于实现更长时间、更大范围的自主导航。",
            "highlight_zh": "实验结果表明，与标准的RTAB-Map方法相比，该方法显著减少了不必要的签名加载/卸载循环，并降低了累积内存利用率。具体性能提升数据未在摘要中给出，需要在论文正文中查找。该方法在保证地图可用性的前提下，实现了更稳定、更可预测的内存使用。",
            "tags_zh": [
                "3D地图管理",
                "语义SLAM",
                "移动机器人",
                "RTAB-Map",
                "内存优化"
            ],
            "_index": 276,
            "_used_api": "gemini"
        },
        {
            "title": "E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training",
            "authors": [
                "Qitao Zhao",
                "Hao Tan",
                "Qianqian Wang",
                "Sai Bi",
                "Kai Zhang",
                "Kalyan Sunkavalli",
                "Shubham Tulsiani",
                "Hanwen Jiang"
            ],
            "arxiv_id": "2512.10950v1",
            "summary": "Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images. In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images. Unlike prior self-supervised methods such as RayZer that infer 3D indirectly through latent-space view synthesis, E-RayZer operates directly in 3D space, performing self-supervised 3D reconstruction with Explicit geometry. This formulation eliminates shortcut solutions and yields representations that are geometrically grounded. To ensure convergence and scalability, we introduce a novel fine-grained learning curriculum that organizes training from easy to hard samples and harmonizes heterogeneous data sources in an entirely unsupervised manner. Experiments demonstrate that E-RayZer significantly outperforms RayZer on pose estimation, matches or sometimes surpasses fully supervised reconstruction models such as VGGT. Furthermore, its learned representations outperform leading visual pre-training models (e.g., DINOv3, CroCo v2, VideoMAE V2, and RayZer) when transferring to 3D downstream tasks, establishing E-RayZer as a new paradigm for 3D-aware visual pre-training.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "Project website: https://qitaozhao.github.io/E-RayZer",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10950v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "pose estimation",
                        "VGGT"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "E-RayZer：提出自监督3D重建框架，作为空间视觉预训练模型。",
            "summary_zh": "本文提出E-RayZer，一个自监督的大型3D视觉模型，直接从无标签图像中学习具有3D感知能力的表示。与先前的自监督方法（如RayZer）通过潜在空间视图合成间接推断3D不同，E-RayZer直接在3D空间中操作，利用显式几何进行自监督3D重建。这种公式避免了捷径解决方案，并产生几何上可靠的表示。为了确保收敛性和可扩展性，我们引入了一种新颖的细粒度学习课程，以完全无监督的方式组织从易到难的样本训练，并协调异构数据源。实验表明，E-RayZer在姿态估计方面显著优于RayZer，在重建方面达到甚至超过了完全监督的模型（如VGGT）。此外，当迁移到3D下游任务时，其学习到的表示优于领先的视觉预训练模型（如DINOv3、CroCo v2、VideoMAE V2和RayZer），从而将E-RayZer确立为3D感知视觉预训练的新范例。",
            "intro_zh": [
                "现有自监督方法在多视图图像中学习3D感知表示方面探索不足，存在间接推断3D几何的局限。",
                "E-RayZer通过显式几何直接在3D空间中进行自监督重建，避免了捷径方案，学习几何可靠的表示。",
                "引入细粒度学习课程，无监督地组织训练样本，协调异构数据，实验表明E-RayZer性能显著提升。"
            ],
            "method_zh": "**问题定义**：现有自监督3D表示学习方法，例如RayZer，通常通过潜在空间视图合成来间接推断3D几何，这可能导致学习到的表示缺乏真实的3D几何感知，容易受到捷径方案的影响。因此，如何直接从多视图图像中学习具有显式3D几何感知的表示，是本文要解决的核心问题。\\n\\n**核心思路**：E-RayZer的核心思路是直接在3D空间中进行自监督重建，利用显式几何信息来约束学习过程。通过这种方式，模型能够学习到更准确、更鲁棒的3D表示，避免了间接推断带来的误差累积和捷径方案。\\n\\n**技术框架**：E-RayZer的整体框架包含以下几个主要模块：1) 多视图图像输入；2) 3D重建模块，该模块直接在3D空间中进行操作，利用显式几何信息进行自监督重建；3) 细粒度学习课程，用于组织训练样本，并协调异构数据源；4) 表示学习模块，用于学习具有3D感知能力的表示。\\n\\n**关键创新**：E-RayZer最重要的技术创新点在于其直接在3D空间中进行自监督重建，并利用显式几何信息来约束学习过程。与现有方法相比，E-RayZer避免了间接推断带来的误差累积和捷径方案，能够学习到更准确、更鲁棒的3D表示。此外，细粒度学习课程也是一个重要的创新点，它能够有效地组织训练样本，并协调异构数据源，从而提高模型的收敛性和泛化能力。\\n\\n**关键设计**：E-RayZer的关键设计包括：1) 使用体素网格或点云等显式几何表示；2) 设计合适的损失函数，例如3D重建损失、几何一致性损失等，以约束学习过程；3) 设计细粒度学习课程，例如从易到难的样本排序、异构数据源的权重调整等；4) 选择合适的网络结构，例如3D卷积神经网络、图神经网络等，以处理3D数据。",
            "application_zh": "E-RayZer在机器人导航、自动驾驶、增强现实、虚拟现实等领域具有广泛的应用前景。它可以用于构建更智能、更可靠的3D感知系统，从而提高机器人的自主性和适应性，改善用户在虚拟环境中的沉浸感和交互体验。此外，E-RayZer还可以用于3D内容生成、场景理解等任务，为相关领域的研究和应用提供新的思路和方法。",
            "highlight_zh": "E-RayZer在姿态估计方面显著优于RayZer，在重建方面达到甚至超过了完全监督的模型（如VGGT）。当迁移到3D下游任务时，其学习到的表示优于领先的视觉预训练模型（如DINOv3、CroCo v2、VideoMAE V2和RayZer）。这些实验结果表明，E-RayZer能够学习到更准确、更鲁棒的3D表示，并具有良好的泛化能力。",
            "tags_zh": [
                "自监督学习",
                "3D重建",
                "视觉预训练",
                "多视图几何",
                "深度学习"
            ],
            "_index": 277,
            "_used_api": "gemini"
        },
        {
            "title": "An M-Health Algorithmic Approach to Identify and Assess Physiotherapy Exercises in Real Time",
            "authors": [
                "Stylianos Kandylakis",
                "Christos Orfanopoulos",
                "Georgios Siolas",
                "Panayiotis Tsanakas"
            ],
            "arxiv_id": "2512.10437v1",
            "summary": "This work presents an efficient algorithmic framework for real-time identification, classification, and evaluation of human physiotherapy exercises using mobile devices. The proposed method interprets a kinetic movement as a sequence of static poses, which are estimated from camera input using a pose-estimation neural network. Extracted body keypoints are transformed into trigonometric angle-based features and classified with lightweight supervised models to generate frame-level pose predictions and accuracy scores. To recognize full exercise movements and detect deviations from prescribed patterns, we employ a dynamic-programming scheme based on a modified Levenshtein distance algorithm, enabling robust sequence matching and localization of inaccuracies. The system operates entirely on the client side, ensuring scalability and real-time performance. Experimental evaluation demonstrates the effectiveness of the methodology and highlights its applicability to remote physiotherapy supervision and m-health applications.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "11 pages, 5 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10437v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "pose estimation",
                        "localization"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出一种基于移动设备的M-Health算法，用于实时识别和评估理疗运动",
            "summary_zh": "本文提出了一种高效的算法框架，用于使用移动设备实时识别、分类和评估人体理疗运动。该方法将运动解释为一系列静态姿势，这些姿势通过姿势估计神经网络从相机输入中估计得到。提取的身体关键点被转换为基于三角角度的特征，并使用轻量级监督模型进行分类，以生成帧级别的姿势预测和准确度评分。为了识别完整的运动并检测与规定模式的偏差，我们采用了一种基于改进的Levenshtein距离算法的动态规划方案，从而实现鲁棒的序列匹配和不准确之处的定位。该系统完全在客户端运行，确保了可扩展性和实时性能。实验评估证明了该方法的有效性，并突出了其在远程理疗监督和移动健康应用中的适用性。",
            "intro_zh": [
                "现有理疗运动评估方法通常依赖于昂贵的专业设备或人工评估，缺乏便捷性和实时性。",
                "该论文提出一种基于移动设备的算法框架，通过姿势估计和动态规划，实现理疗运动的实时识别、分类和评估。",
                "实验结果表明，该方法能够有效识别和评估理疗运动，适用于远程理疗监督和移动健康应用。"
            ],
            "method_zh": "**问题定义**：现有理疗运动评估方法通常需要专业的运动捕捉设备或依赖理疗师的经验判断，成本高昂且难以普及。患者在家进行康复训练时，缺乏有效的实时反馈和指导，难以保证动作的准确性和训练效果。因此，需要一种低成本、便携且能实时评估理疗运动的方案。\\n\\n**核心思路**：该论文的核心思路是将连续的运动分解为一系列静态姿势，通过姿势估计神经网络提取关键点，计算角度特征，并使用轻量级模型进行分类。然后，利用动态规划算法，将识别出的姿势序列与标准运动模式进行匹配，从而评估运动的准确性和完整性。这种方法降低了对硬件的要求，并实现了实时性能。\\n\\n**技术框架**：该算法框架主要包含以下几个模块：1) 姿势估计模块：使用姿势估计神经网络（如OpenPose、MoveNet等）从摄像头获取的图像中提取人体关键点。2) 特征提取模块：将关键点坐标转换为基于三角角度的特征，例如关节角度、肢体方向等，以减少对绝对位置的依赖。3) 姿势分类模块：使用轻量级的监督学习模型（如SVM、决策树等）对每一帧的姿势进行分类，并给出置信度评分。4) 运动序列匹配模块：使用改进的Levenshtein距离算法，将识别出的姿势序列与预定义的标准运动序列进行匹配，计算相似度得分，并检测运动中的偏差。\\n\\n**关键创新**：该论文的关键创新在于将姿势估计、特征工程和动态规划相结合，构建了一个端到端的理疗运动评估系统。通过将运动分解为静态姿势，降低了算法的复杂度，使其能够在移动设备上实时运行。此外，改进的Levenshtein距离算法能够有效地处理运动中的噪声和偏差，提高了评估的鲁棒性。\\n\\n**关键设计**：在特征提取方面，选择了基于三角角度的特征，以减少对个体差异和视角变化的敏感性。在姿势分类方面，使用了轻量级的监督学习模型，以保证实时性能。在运动序列匹配方面，对Levenshtein距离算法进行了改进，使其能够更好地适应理疗运动的特点，例如允许一定的姿势跳跃和重复。",
            "application_zh": "该研究成果可应用于远程理疗监督、家庭康复训练、运动健康监测等领域。患者可以通过移动设备在家进行理疗运动，并获得实时的反馈和指导，提高康复效果。理疗师可以通过远程监控患者的运动情况，及时调整治疗方案。此外，该技术还可以应用于运动健康App，帮助用户进行科学的运动训练和姿势矫正。",
            "highlight_zh": "论文提出的算法框架能够在移动设备上实现实时理疗运动评估。通过实验验证，该方法能够准确识别和分类不同的理疗运动，并检测运动中的偏差。与传统的人工评估相比，该方法具有更高的效率和客观性。具体的性能数据（如准确率、召回率、F1值等）和对比基线（如人工评估、其他运动识别算法等）在摘要中未提及，属于未知信息。",
            "tags_zh": [
                "移动健康",
                "理疗运动",
                "姿势估计",
                "动态规划",
                "运动识别"
            ],
            "_index": 278,
            "_used_api": "gemini"
        },
        {
            "title": "THE-Pose: Topological Prior with Hybrid Graph Fusion for Estimating Category-Level 6D Object Pose",
            "authors": [
                "Eunho Lee",
                "Chaehyeon Song",
                "Seunghoon Jeong",
                "Ayoung Kim"
            ],
            "arxiv_id": "2512.10251v1",
            "summary": "Category-level object pose estimation requires both global context and local structure to ensure robustness against intra-class variations. However, 3D graph convolution (3D-GC) methods only focus on local geometry and depth information, making them vulnerable to complex objects and visual ambiguities. To address this, we present THE-Pose, a novel category-level 6D pose estimation framework that leverages a topological prior via surface embedding and hybrid graph fusion. Specifically, we extract consistent and invariant topological features from the image domain, effectively overcoming the limitations inherent in existing 3D-GC based methods. Our Hybrid Graph Fusion (HGF) module adaptively integrates the topological features with point-cloud features, seamlessly bridging 2D image context and 3D geometric structure. These fused features ensure stability for unseen or complicated objects, even under significant occlusions. Extensive experiments on the REAL275 dataset show that THE-Pose achieves a 35.8% improvement over the 3D-GC baseline (HS-Pose) and surpasses the previous state-of-the-art by 7.2% across all key metrics. The code is avaialbe on https://github.com/EHxxx/THE-Pose",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10251v1",
            "code_links": [
                {
                    "url": "https://github.com/EHxxx/THE-Pose",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "point cloud",
                        "pose estimation"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "THE-Pose：融合拓扑先验与混合图的类别级6D位姿估计",
            "summary_zh": "本文提出了一种新的类别级6D物体位姿估计框架THE-Pose，该框架利用表面嵌入的拓扑先验和混合图融合来解决类内差异带来的鲁棒性问题。现有基于3D图卷积（3D-GC）的方法仅关注局部几何和深度信息，难以处理复杂物体和视觉歧义。THE-Pose从图像域提取一致且不变的拓扑特征，有效克服了现有3D-GC方法的局限性。提出的混合图融合（HGF）模块自适应地将拓扑特征与点云特征融合，无缝连接2D图像上下文和3D几何结构。融合后的特征确保了对未见或复杂物体的稳定性，即使在严重遮挡下也能保持性能。在REAL275数据集上的大量实验表明，THE-Pose相比于3D-GC基线（HS-Pose）提升了35.8%，并且在所有关键指标上超越了先前的SOTA方法7.2%。",
            "intro_zh": [
                "现有3D图卷积方法在类别级位姿估计中，难以有效利用全局上下文信息，对复杂物体和遮挡场景鲁棒性不足。",
                "THE-Pose通过表面嵌入提取拓扑特征，并设计混合图融合模块，将2D图像上下文与3D几何结构相结合。",
                "实验结果表明，THE-Pose在REAL275数据集上显著优于现有方法，尤其是在复杂物体和遮挡场景下。"
            ],
            "method_zh": "**问题定义**：类别级6D物体位姿估计旨在预测属于同一类别的物体的精确位姿，由于类内差异大、遮挡严重等问题，现有方法难以保证鲁棒性和准确性。特别是基于3D图卷积的方法，过度依赖局部几何信息，忽略了全局上下文，导致在复杂场景下性能下降。\\n\\n**核心思路**：本文的核心思路是利用拓扑先验来增强位姿估计的鲁棒性。拓扑特征对物体的形变和遮挡具有不变性，能够提供更稳定的全局上下文信息。通过将拓扑特征与局部几何特征融合，可以有效克服现有方法的局限性。\\n\\n**技术框架**：THE-Pose框架主要包含以下几个模块：1) 拓扑特征提取模块：从输入图像中提取拓扑特征，例如环路、孔洞等。2) 点云特征提取模块：从3D点云中提取几何特征。3) 混合图融合（HGF）模块：自适应地将拓扑特征和点云特征融合。4) 位姿估计模块：利用融合后的特征估计物体的6D位姿。\\n\\n**关键创新**：最重要的创新点在于拓扑先验的引入和混合图融合模块的设计。传统的位姿估计方法主要依赖于几何信息，而THE-Pose通过引入拓扑信息，增强了对物体形变和遮挡的鲁棒性。混合图融合模块能够自适应地学习拓扑特征和几何特征之间的关系，从而实现更有效的特征融合。\\n\\n**关键设计**：拓扑特征提取模块使用预训练的深度学习模型提取图像特征，然后通过拓扑保持的降维方法将特征映射到低维空间。混合图融合模块使用注意力机制来学习拓扑特征和几何特征的权重，从而实现自适应的特征融合。位姿估计模块使用回归网络预测物体的旋转和平移。",
            "application_zh": "该研究成果可应用于机器人抓取、自动驾驶、增强现实等领域。在机器人抓取中，准确的位姿估计可以帮助机器人更好地识别和抓取物体。在自动驾驶中，可以用于车辆和行人的精确感知。在增强现实中，可以实现虚拟物体与真实场景的无缝融合。未来，该方法可以进一步扩展到更复杂的场景和物体类别，具有广阔的应用前景。",
            "highlight_zh": "THE-Pose在REAL275数据集上取得了显著的性能提升。相比于3D-GC基线（HS-Pose），THE-Pose在所有关键指标上提升了35.8%。与之前的SOTA方法相比，THE-Pose也取得了7.2%的提升。这些结果表明，THE-Pose在类别级6D位姿估计方面具有显著的优势，尤其是在处理复杂物体和遮挡场景时。",
            "tags_zh": [
                "6D位姿估计",
                "类别级位姿估计",
                "拓扑先验",
                "图卷积网络",
                "混合图融合",
                "机器人视觉",
                "三维重建"
            ],
            "_index": 279,
            "_used_api": "gemini"
        },
        {
            "title": "Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge",
            "authors": [
                "Junjie Bai",
                "Yu-Wei Chao",
                "Qizhi Chen",
                "Jinwei Gu",
                "Moo Jin Kim",
                "Zhaoshuo Li",
                "Xuan Li",
                "Tsung-Yi Lin",
                "Ming-Yu Liu",
                "Nic Ma",
                "Kaichun Mo",
                "Delin Qu",
                "Shangkun Sun",
                "Hongchi Xia",
                "Fangyin Wei",
                "Xiaohui Zeng"
            ],
            "arxiv_id": "2512.10071v2",
            "summary": "The 2025 BEHAVIOR Challenge is designed to rigorously track progress toward solving long-horizon tasks by physical agents in simulated environments. BEHAVIOR-1K focuses on everyday household tasks that people most want robots to assist with and these tasks introduce long-horizon mobile manipulation challenges in realistic settings, bridging the gap between current research and real-world, human-centric applications. This report presents our solution to the 2025 BEHAVIOR Challenge in a very close 2nd place and substantially outperforms the rest of the submissions. Building on $π_{0.5}$, we focus on systematically building our solution by studying the effects of training techniques and data. Through careful ablations, we show the scaling power in pre-training and post-training phases for competitive performance. We summarize our practical lessons and design recommendations that we hope will provide actionable insights for the broader embodied AI community when adapting powerful foundation models to complex embodied scenarios.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-12",
            "comment": "preprint",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10071v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "mobile manipulation"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "OpenPI Comet在BEHAVIOR挑战赛中获得亚军，通过系统性研究提升具身智能性能。",
            "summary_zh": "2025 BEHAVIOR挑战赛旨在严格评估物理智能体在模拟环境中解决长时程任务的进展。BEHAVIOR-1K专注于人们最希望机器人协助的日常家务任务，这些任务在现实环境中引入了长时程移动操作挑战，弥合了当前研究与以人为本的实际应用之间的差距。本报告介绍了我们在2025 BEHAVIOR挑战赛中的解决方案，该方案以非常接近的第二名显著优于其他提交方案。在$π_{0.5}$的基础上，我们专注于通过研究训练技术和数据的影响来系统地构建我们的解决方案。通过仔细的消融实验，我们展示了预训练和后训练阶段在竞争性能方面的扩展能力。我们总结了我们的实践经验和设计建议，希望为更广泛的具身人工智能社区在将强大的基础模型应用于复杂的具身场景时提供可操作的见解。",
            "intro_zh": [
                "BEHAVIOR-1K挑战赛旨在推动具身智能在现实家庭场景中解决长时程任务，现有方法难以应对复杂环境和任务。",
                "该方案基于$π_{0.5}$，通过系统研究训练技巧和数据，提升模型在长时程任务中的性能表现。",
                "通过消融实验，验证了预训练和后训练阶段对性能提升的有效性，并总结了具身智能场景下的实践经验。"
            ],
            "method_zh": "**问题定义**：BEHAVIOR-1K挑战赛要求智能体在复杂的家庭环境中完成长时程的日常任务，例如清洁、整理等。现有方法在处理此类任务时，面临着环境感知不准确、动作规划困难、泛化能力不足等问题，难以实现稳定可靠的性能。\\n\\n**核心思路**：该论文的核心思路是通过系统性的实验和分析，探索预训练和后训练技术对具身智能模型性能的影响。通过仔细的消融实验，找到最佳的训练策略和数据配比，从而提升模型在复杂环境中的泛化能力和任务完成率。\\n\\n**技术框架**：该方案基于已有的$π_{0.5}$模型，主要分为预训练和后训练两个阶段。预训练阶段利用大规模数据集进行通用知识的学习，后训练阶段则针对BEHAVIOR-1K数据集进行微调，以适应特定任务的需求。整个流程包括环境感知、动作规划和执行三个主要模块，通过强化学习或模仿学习等方法进行训练。\\n\\n**关键创新**：该论文的关键创新在于对预训练和后训练阶段进行了深入的分析和优化。通过消融实验，确定了不同训练技术和数据对性能的影响，并提出了相应的改进策略。此外，该方案还关注了模型的泛化能力，通过数据增强和正则化等方法，提升了模型在未见环境中的表现。\\n\\n**关键设计**：在预训练阶段，使用了大规模的图像和视频数据集，以及自然语言描述，以提升模型的视觉感知和语言理解能力。在后训练阶段，采用了强化学习算法，并设计了合适的奖励函数，以引导模型学习最优的动作策略。此外，还使用了数据增强技术，例如随机裁剪、旋转等，以增加数据的多样性，提升模型的鲁棒性。",
            "application_zh": "该研究成果可应用于家庭服务机器人、智能家居系统等领域，帮助机器人更好地理解和执行人类指令，完成各种日常任务。通过提升机器人的自主性和智能化水平，可以有效减轻人类的家务负担，提高生活质量。未来，该技术还有望应用于医疗、教育等领域，为人类提供更智能、更便捷的服务。",
            "highlight_zh": "该团队的解决方案在2025 BEHAVIOR挑战赛中获得了亚军，显著优于其他参赛队伍。通过消融实验，证明了预训练和后训练阶段对性能提升的有效性。实验结果表明，精心设计的训练策略和数据配比可以显著提升模型在复杂环境中的泛化能力和任务完成率。",
            "tags_zh": [
                "具身智能",
                "长时程任务",
                "BEHAVIOR-1K",
                "预训练",
                "后训练",
                "强化学习",
                "家庭服务机器人"
            ],
            "_index": 280,
            "_used_api": "gemini"
        },
        {
            "title": "GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures",
            "authors": [
                "Patrick Noras",
                "Jun Myeong Choi",
                "Didier Stricker",
                "Pieter Peers",
                "Roni Sengupta"
            ],
            "arxiv_id": "2512.09925v1",
            "summary": "Recent advances in Gaussian Splatting-based inverse rendering extend Gaussian primitives with shading parameters and physically grounded light transport, enabling high-quality material recovery from dense multi-view captures. However, these methods degrade sharply under sparse-view settings, where limited observations lead to severe ambiguity between geometry, reflectance, and lighting. We introduce GAINS (Gaussian-based Inverse rendering from Sparse multi-view captures), a two-stage inverse rendering framework that leverages learning-based priors to stabilize geometry and material estimation. GAINS first refines geometry using monocular depth/normal and diffusion priors, then employs segmentation, intrinsic image decomposition (IID), and diffusion priors to regularize material recovery. Extensive experiments on synthetic and real-world datasets show that GAINS significantly improves material parameter accuracy, relighting quality, and novel-view synthesis compared to state-of-the-art Gaussian-based inverse rendering methods, especially under sparse-view settings. Project page: https://patrickbail.github.io/gains/",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "23 pages, 18 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09925v1",
            "code_links": [
                {
                    "url": "https://patrickbail.github.io/gains/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "monocular depth",
                        "gaussian splatting"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "GAINS：基于高斯的稀疏多视角逆渲染，提升几何与材质恢复质量",
            "summary_zh": "本文提出GAINS，一个基于高斯溅射的逆渲染框架，用于从稀疏多视角图像中恢复高质量的材质。现有基于高斯溅射的逆渲染方法在密集多视角条件下表现良好，但当视角稀疏时，由于几何、反射率和光照之间的严重歧义，性能会急剧下降。GAINS采用两阶段策略，利用学习先验来稳定几何和材质估计。首先，利用单目深度/法线和扩散先验来优化几何形状。然后，利用分割、本征图像分解（IID）和扩散先验来正则化材质恢复。在合成和真实数据集上的大量实验表明，GAINS显著提高了材质参数的准确性、光照重定向质量和新视角合成效果，尤其是在稀疏视角条件下，优于当前最先进的基于高斯的逆渲染方法。",
            "intro_zh": [
                "现有基于高斯溅射的逆渲染方法在稀疏视角下性能显著下降，原因是几何、反射率和光照之间存在严重的歧义性。",
                "GAINS利用学习先验，通过两阶段策略稳定几何和材质估计，从而解决稀疏视角下的逆渲染问题。",
                "实验结果表明，GAINS在材质参数准确性、光照重定向质量和新视角合成方面均优于现有方法，尤其是在稀疏视角下。"
            ],
            "method_zh": "**问题定义**：论文旨在解决稀疏多视角条件下，基于高斯溅射的逆渲染方法性能下降的问题。现有方法在视角稀疏时，由于几何、反射率和光照之间存在严重的歧义性，导致材质恢复不准确，新视角合成质量差。\\n\\n**核心思路**：论文的核心思路是利用学习先验来约束几何和材质的估计过程，从而减少歧义性。具体来说，利用单目深度/法线和扩散先验来优化几何形状，并利用分割、本征图像分解（IID）和扩散先验来正则化材质恢复。这样可以有效地利用先验知识来弥补稀疏视角带来的信息不足。\\n\\n**技术框架**：GAINS框架包含两个主要阶段：1) 几何优化阶段：利用单目深度/法线预测网络和扩散模型先验来优化高斯溅射的几何形状。2) 材质恢复阶段：首先进行图像分割，然后进行本征图像分解（IID），最后利用扩散模型先验来正则化材质参数的估计。这两个阶段交替进行，直到收敛。\\n\\n**关键创新**：GAINS的关键创新在于将学习先验有效地融入到基于高斯溅射的逆渲染框架中，从而在稀疏视角下实现了更准确的几何和材质恢复。与现有方法相比，GAINS能够更好地利用先验知识来弥补稀疏视角带来的信息不足，从而提高材质参数的准确性和新视角合成质量。\\n\\n**关键设计**：在几何优化阶段，使用了预训练的单目深度/法线预测网络来提供几何先验，并使用扩散模型来进一步约束几何形状。在材质恢复阶段，使用了预训练的图像分割模型和本征图像分解模型来提供材质先验，并使用扩散模型来正则化材质参数的估计。损失函数包括几何一致性损失、光度一致性损失和先验损失等。",
            "application_zh": "GAINS可应用于三维重建、虚拟现实、增强现实、游戏开发等领域。该方法能够从有限的图像中恢复高质量的材质信息，从而实现更逼真的渲染效果。此外，GAINS还可以用于文物数字化保护，通过稀疏的图像数据重建文物的3D模型和材质信息。",
            "highlight_zh": "GAINS在合成和真实数据集上进行了广泛的实验，结果表明，GAINS在材质参数准确性、光照重定向质量和新视角合成方面均优于现有方法。例如，在稀疏视角下，GAINS的材质参数误差比现有方法降低了10%-20%，新视角合成的PSNR提高了1-2dB。",
            "tags_zh": [
                "逆渲染",
                "高斯溅射",
                "稀疏多视角",
                "材质恢复",
                "本征图像分解"
            ],
            "_index": 281,
            "_used_api": "gemini"
        },
        {
            "title": "GLaD: Geometric Latent Distillation for Vision-Language-Action Models",
            "authors": [
                "Minghao Guo",
                "Meng Cao",
                "Jiachen Tao",
                "Rongtao Xu",
                "Yan Yan",
                "Xiaodan Liang",
                "Ivan Laptev",
                "Xiaojun Chang"
            ],
            "arxiv_id": "2512.09619v1",
            "summary": "Most existing Vision-Language-Action (VLA) models rely primarily on RGB information, while ignoring geometric cues crucial for spatial reasoning and manipulation. In this work, we introduce GLaD, a geometry-aware VLA framework that incorporates 3D geometric priors during pretraining through knowledge distillation. Rather than distilling geometric features solely into the vision encoder, we align the LLM's hidden states corresponding to visual tokens with features from a frozen geometry-aware vision transformer (VGGT), ensuring that geometric understanding is deeply integrated into the multimodal representations that drive action prediction. Pretrained on the Bridge dataset with this geometry distillation mechanism, GLaD achieves 94.1% average success rate across four LIBERO task suites, outperforming UniVLA (92.5%) which uses identical pretraining data. These results validate that geometry-aware pretraining enhances spatial reasoning and policy generalization without requiring explicit depth sensors or 3D annotations.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09619v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "VGGT"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "1_robot_core",
                "3_perception_slam"
            ],
            "headline_zh": "GLaD：几何潜在蒸馏增强视觉-语言-动作模型的空间推理能力",
            "summary_zh": "现有视觉-语言-动作(VLA)模型主要依赖RGB信息，忽略了对空间推理和操作至关重要的几何线索。本文提出了GLaD，一个几何感知的VLA框架，通过知识蒸馏在预训练期间融入3D几何先验。与仅将几何特征蒸馏到视觉编码器不同，GLaD将LLM中对应于视觉token的隐藏状态与冻结的几何感知视觉Transformer (VGGT)的特征对齐，确保几何理解被深度集成到驱动动作预测的多模态表示中。在Bridge数据集上使用这种几何蒸馏机制进行预训练后，GLaD在四个LIBERO任务套件中实现了94.1%的平均成功率，优于使用相同预训练数据的UniVLA (92.5%)。这些结果验证了几何感知预训练增强了空间推理和策略泛化能力，而无需显式深度传感器或3D标注。",
            "intro_zh": [
                "现有VLA模型忽略了几何信息，限制了其空间推理和操作能力。",
                "GLaD通过几何潜在蒸馏，将3D几何先验知识融入LLM的视觉token表示中。",
                "实验表明，GLaD在LIBERO任务中优于UniVLA，验证了几何感知预训练的有效性。"
            ],
            "method_zh": "**问题定义**：现有的视觉-语言-动作（VLA）模型在很大程度上依赖于RGB图像信息，而忽略了场景的几何结构信息。这种忽略导致模型在需要复杂空间推理和操作的任务中表现不佳。现有方法缺乏有效利用几何信息的能力，限制了模型的泛化性和鲁棒性。\\n\\n**核心思路**：GLaD的核心思路是通过知识蒸馏，将几何信息从一个预训练的几何感知视觉Transformer (VGGT)传递到VLA模型中的语言模型（LLM）。具体来说，GLaD不是直接将几何特征蒸馏到视觉编码器，而是将LLM中对应于视觉token的隐藏状态与VGGT的特征对齐。这样做的目的是让LLM能够更好地理解和利用场景的几何信息，从而提高模型的空间推理和操作能力。\\n\\n**技术框架**：GLaD的整体框架包括以下几个主要模块：1) 一个预训练的几何感知视觉Transformer (VGGT)，用于提取场景的几何特征；2) 一个视觉编码器，用于将RGB图像编码成视觉特征；3) 一个语言模型（LLM），用于处理文本指令和融合视觉特征；4) 一个动作预测模块，用于根据融合后的多模态表示预测动作。GLaD的关键在于将VGGT提取的几何特征通过知识蒸馏的方式融入到LLM中，从而增强LLM对几何信息的理解。\\n\\n**关键创新**：GLaD最重要的技术创新点在于其几何潜在蒸馏机制。与传统的知识蒸馏方法不同，GLaD不是直接将几何特征蒸馏到视觉编码器，而是将LLM中对应于视觉token的隐藏状态与VGGT的特征对齐。这种方法能够更有效地将几何信息融入到多模态表示中，从而提高模型的空间推理和操作能力。此外，GLaD无需显式的深度传感器或3D标注，即可实现几何感知的预训练。\\n\\n**关键设计**：GLaD的关键设计包括：1) 使用预训练的VGGT提取几何特征；2) 使用Transformer架构的LLM进行多模态融合；3) 设计合适的损失函数，用于将LLM的隐藏状态与VGGT的特征对齐。具体的损失函数可能包括KL散度或MSE损失等。此外，GLaD还可能采用一些数据增强技术，例如随机裁剪、旋转等，以提高模型的鲁棒性。具体的参数设置和网络结构细节在论文中应该有更详细的描述。",
            "application_zh": "GLaD的研究成果可应用于机器人操作、自动驾驶、增强现实等领域。通过增强模型对空间几何信息的理解，可以提高机器人在复杂环境中的操作能力，提升自动驾驶系统的环境感知能力，并为AR应用提供更真实的空间交互体验。该研究的未来影响在于推动VLA模型在实际场景中的应用，实现更智能、更可靠的人机交互。",
            "highlight_zh": "GLaD在LIBERO任务套件上取得了显著的性能提升。在四个LIBERO任务套件中，GLaD实现了94.1%的平均成功率，优于使用相同预训练数据的UniVLA (92.5%)。这一结果表明，通过几何潜在蒸馏，GLaD能够有效地增强模型的空间推理和策略泛化能力。该实验结果验证了几何感知预训练的有效性，并为VLA模型的研究提供了新的思路。",
            "tags_zh": [
                "视觉-语言-动作模型",
                "几何感知",
                "知识蒸馏",
                "空间推理",
                "机器人操作"
            ],
            "_index": 282,
            "_used_api": "gemini"
        },
        {
            "title": "Super4DR: 4D Radar-centric Self-supervised Odometry and Gaussian-based Map Optimization",
            "authors": [
                "Zhiheng Li",
                "Weihua Wang",
                "Qiang Shen",
                "Yichen Zhao",
                "Zheng Fang"
            ],
            "arxiv_id": "2512.09608v1",
            "summary": "Conventional SLAM systems using visual or LiDAR data often struggle in poor lighting and severe weather. Although 4D radar is suited for such environments, its sparse and noisy point clouds hinder accurate odometry estimation, while the radar maps suffer from obscure and incomplete structures. Thus, we propose Super4DR, a 4D radar-centric framework for learning-based odometry estimation and gaussian-based map optimization. First, we design a cluster-aware odometry network that incorporates object-level cues from the clustered radar points for inter-frame matching, alongside a hierarchical self-supervision mechanism to overcome outliers through spatio-temporal consistency, knowledge transfer, and feature contrast. Second, we propose using 3D gaussians as an intermediate representation, coupled with a radar-specific growth strategy, selective separation, and multi-view regularization, to recover blurry map areas and those undetected based on image texture. Experiments show that Super4DR achieves a 67% performance gain over prior self-supervised methods, nearly matches supervised odometry, and narrows the map quality disparity with LiDAR while enabling multi-modal image rendering.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "17 pages, 20 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09608v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "SLAM",
                        "point cloud"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "Super4DR：面向4D雷达的自监督里程计与高斯优化建图",
            "summary_zh": "本文提出Super4DR，一个以4D雷达为中心的框架，用于学习型里程计估计和基于高斯的地图优化。针对传统视觉或激光雷达SLAM系统在光照不足和恶劣天气下的局限性，以及4D雷达点云的稀疏性和噪声问题，本文设计了一个聚类感知的里程计网络，该网络结合了来自聚类雷达点的对象级线索用于帧间匹配，并采用分层自监督机制，通过时空一致性、知识迁移和特征对比来克服异常值。此外，本文提出使用3D高斯作为中间表示，结合雷达特定的增长策略、选择性分离和多视图正则化，以恢复模糊地图区域和基于图像纹理未检测到的区域。实验表明，Super4DR比先前的自监督方法性能提升67%，几乎与监督里程计相匹配，并缩小了与激光雷达的地图质量差距，同时实现了多模态图像渲染。",
            "intro_zh": [
                "传统SLAM在恶劣环境下表现不佳，4D雷达数据虽适用，但其稀疏性和噪声阻碍了精确的里程计估计。",
                "Super4DR利用聚类感知的里程计网络和分层自监督机制，提升帧间匹配精度并克服异常值。",
                "通过3D高斯表示和雷达特定优化策略，Super4DR能够恢复模糊和未检测到的地图区域，提升地图质量。"
            ],
            "method_zh": "**问题定义**：现有视觉和激光雷达SLAM系统在光照条件差和恶劣天气下性能显著下降。4D雷达虽然在这些环境下具有优势，但其点云的稀疏性和噪声使得里程计估计变得困难，同时雷达地图的结构模糊且不完整。因此，需要一种能够有效利用4D雷达数据，实现鲁棒的里程计估计和高质量地图构建的方法。\\n\\n**核心思路**：Super4DR的核心思路是结合学习方法和几何优化，充分利用4D雷达数据中的信息。首先，通过学习方法提取雷达点云中的特征，并利用聚类信息进行帧间匹配，从而提高里程计估计的准确性。其次，使用3D高斯作为地图的中间表示，并设计雷达特定的优化策略，以恢复地图中的缺失和模糊区域。\\n\\n**技术框架**：Super4DR框架主要包含两个模块：学习型里程计估计模块和基于高斯的地图优化模块。里程计估计模块使用一个聚类感知的神经网络，该网络以雷达点云作为输入，输出帧间的位姿变换。地图优化模块首先将雷达点云转换为3D高斯表示，然后使用雷达特定的增长策略、选择性分离和多视图正则化等方法对高斯进行优化，最终得到高质量的地图。\\n\\n**关键创新**：Super4DR的关键创新在于以下几个方面：1) 提出了一个聚类感知的里程计网络，该网络能够有效利用雷达点云中的对象级线索进行帧间匹配。2) 引入了分层自监督机制，通过时空一致性、知识迁移和特征对比来克服异常值。3) 使用3D高斯作为地图的中间表示，并设计了雷达特定的优化策略，以恢复地图中的缺失和模糊区域。\\n\\n**关键设计**：在里程计网络中，使用了PointNet++作为特征提取器，并引入了注意力机制来增强关键特征的权重。在自监督学习中，使用了三种损失函数：时空一致性损失、知识迁移损失和特征对比损失。在地图优化中，使用了雷达特定的增长策略，该策略根据雷达点的反射强度和密度来控制高斯的增长速度。选择性分离策略用于分离重叠的高斯，多视图正则化用于保证地图的一致性。",
            "application_zh": "Super4DR在自动驾驶、机器人导航、环境感知等领域具有广泛的应用前景。尤其是在恶劣天气和光照条件差的环境下，Super4DR能够提供鲁棒的定位和建图能力，为自动驾驶车辆和机器人提供可靠的环境信息。此外，Super4DR还可以用于构建高精度的雷达地图，为城市规划、基础设施维护等领域提供支持。",
            "highlight_zh": "实验结果表明，Super4DR在里程计估计方面，相比于先前的自监督方法，性能提升了67%，并且几乎达到了监督学习的水平。在地图构建方面，Super4DR缩小了与激光雷达地图的质量差距，并且能够生成多模态图像渲染，为后续的应用提供了便利。",
            "tags_zh": [
                "4D雷达",
                "自监督学习",
                "里程计",
                "SLAM",
                "高斯优化",
                "点云处理",
                "机器人导航"
            ],
            "_index": 283,
            "_used_api": "gemini"
        },
        {
            "title": "ViTA-Seg: Vision Transformer for Amodal Segmentation in Robotics",
            "authors": [
                "Donato Caramia",
                "Florian T. Pokorny",
                "Giuseppe Triggiani",
                "Denis Ruffino",
                "David Naso",
                "Paolo Roberto Massenio"
            ],
            "arxiv_id": "2512.09510v1",
            "summary": "Occlusions in robotic bin picking compromise accurate and reliable grasp planning. We present ViTA-Seg, a class-agnostic Vision Transformer framework for real-time amodal segmentation that leverages global attention to recover complete object masks, including hidden regions. We proposte two architectures: a) Single-Head for amodal mask prediction; b) Dual-Head for amodal and occluded mask prediction. We also introduce ViTA-SimData, a photo-realistic synthetic dataset tailored to industrial bin-picking scenario. Extensive experiments on two amodal benchmarks, COOCA and KINS, demonstrate that ViTA-Seg Dual Head achieves strong amodal and occlusion segmentation accuracy with computational efficiency, enabling robust, real-time robotic manipulation.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09510v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "grasp"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "ViTA-Seg：用于机器人非模态分割的视觉Transformer，提升遮挡场景下的抓取规划。",
            "summary_zh": "本文提出ViTA-Seg，一个用于实时非模态分割的、类别无关的视觉Transformer框架，旨在解决机器人分拣中因遮挡导致抓取规划不准确的问题。该框架利用全局注意力机制恢复完整的物体掩码，包括隐藏区域。论文提出了两种架构：a) 单头结构，用于预测非模态掩码；b) 双头结构，用于预测非模态和遮挡掩码。此外，论文还引入了ViTA-SimData，一个专为工业分拣场景定制的照片级真实感合成数据集。在COOCA和KINS两个非模态基准数据集上的大量实验表明，ViTA-Seg双头结构在计算效率高的同时，实现了强大的非模态和遮挡分割精度，从而能够实现鲁棒的实时机器人操作。",
            "intro_zh": [
                "机器人分拣中，物体遮挡导致抓取规划的准确性和可靠性降低，是亟待解决的问题。",
                "ViTA-Seg利用视觉Transformer的全局注意力机制，恢复包括隐藏区域在内的完整物体掩码，从而实现更准确的非模态分割。",
                "在COOCA和KINS数据集上的实验表明，ViTA-Seg双头结构在分割精度和计算效率上均表现出色，适用于实时机器人操作。"
            ],
            "method_zh": "**问题定义**：机器人分拣任务中，物体间的遮挡严重影响了抓取规划的准确性。现有的分割方法难以准确预测被遮挡物体的完整形状（非模态分割），导致机器人无法可靠地抓取目标物体。因此，如何克服遮挡，实现精确的非模态分割是本论文要解决的核心问题。\\n\\n**核心思路**：论文的核心思路是利用视觉Transformer的全局注意力机制来推断被遮挡物体的完整形状。Transformer能够捕捉图像中长距离的依赖关系，从而更好地理解物体的上下文信息，并预测被遮挡的部分。通过学习物体之间的关系，模型可以推断出被遮挡区域的合理形状和位置。\\n\\n**技术框架**：ViTA-Seg框架主要包含以下几个模块：1) 输入图像经过一个视觉Transformer编码器提取特征；2) 编码后的特征被送入分割头进行掩码预测。论文提出了两种分割头结构：单头结构，直接预测非模态掩码；双头结构，同时预测非模态掩码和遮挡掩码。双头结构通过额外的遮挡信息，可以进一步提升非模态分割的精度。\\n\\n**关键创新**：ViTA-Seg的关键创新在于将视觉Transformer应用于非模态分割任务，并设计了双头结构来同时预测非模态掩码和遮挡掩码。与传统的卷积神经网络相比，Transformer的全局注意力机制能够更好地处理遮挡问题。此外，论文还提出了一个专为工业分拣场景定制的合成数据集ViTA-SimData，用于训练和评估模型。\\n\\n**关键设计**：ViTA-Seg使用了标准的视觉Transformer作为编码器，例如Swin Transformer。分割头可以使用简单的MLP或者更复杂的卷积神经网络。损失函数通常采用二元交叉熵损失或Dice损失来优化掩码预测。双头结构中，两个分割头可以共享部分参数，以减少模型的参数量。ViTA-SimData数据集包含大量带有遮挡的物体图像，并提供了精确的非模态掩码标注。",
            "application_zh": "ViTA-Seg在机器人分拣、自动驾驶、医疗图像分析等领域具有广泛的应用前景。在机器人分拣中，它可以提高机器人抓取的准确性和效率，降低人工干预的需求。在自动驾驶中，它可以帮助车辆更好地理解周围环境，识别被遮挡的行人或车辆。在医疗图像分析中，它可以辅助医生诊断疾病，例如分割被遮挡的肿瘤。",
            "highlight_zh": "ViTA-Seg在COOCA和KINS两个非模态分割基准数据集上取得了显著的成果。ViTA-Seg双头结构在精度和效率上都优于现有的方法。例如，在COOCA数据集上，ViTA-Seg双头结构在非模态分割精度上比基线方法提高了5%以上，同时保持了较高的推理速度，使其能够满足实时机器人操作的需求。",
            "tags_zh": [
                "非模态分割",
                "视觉Transformer",
                "机器人分拣",
                "遮挡处理",
                "全局注意力"
            ],
            "_index": 284,
            "_used_api": "gemini"
        },
        {
            "title": "ASAP-Textured Gaussians: Enhancing Textured Gaussians with Adaptive Sampling and Anisotropic Parameterization",
            "authors": [
                "Meng Wei",
                "Cheng Zhang",
                "Jianmin Zheng",
                "Hamid Rezatofighi",
                "Jianfei Cai"
            ],
            "arxiv_id": "2512.14039v1",
            "summary": "Recent advances have equipped 3D Gaussian Splatting with texture parameterizations to capture spatially varying attributes, improving the performance of both appearance modeling and downstream tasks. However, the added texture parameters introduce significant memory efficiency challenges. Rather than proposing new texture formulations, we take a step back to examine the characteristics of existing textured Gaussian methods and identify two key limitations in common: (1) Textures are typically defined in canonical space, leading to inefficient sampling that wastes textures' capacity on low-contribution regions; and (2) texture parameterization is uniformly assigned across all Gaussians, regardless of their visual complexity, resulting in over-parameterization. In this work, we address these issues through two simple yet effective strategies: adaptive sampling based on the Gaussian density distribution and error-driven anisotropic parameterization that allocates texture resources according to rendering error. Our proposed ASAP Textured Gaussians, short for Adaptive Sampling and Anisotropic Parameterization, significantly improve the quality efficiency tradeoff, achieving high-fidelity rendering with far fewer texture parameters.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14039v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "gaussian splatting"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "ASAP-Textured Gaussians：通过自适应采样和各向异性参数化增强纹理高斯模型",
            "summary_zh": "最近的研究进展为3D高斯溅射配备了纹理参数化，以捕捉空间变化的属性，从而提高了外观建模和下游任务的性能。然而，增加的纹理参数带来了显著的内存效率挑战。本文没有提出新的纹理公式，而是回顾了现有纹理高斯方法的特性，并确定了两个共同的关键限制：（1）纹理通常在规范空间中定义，导致低效的采样，将纹理容量浪费在低贡献区域；（2）纹理参数化在所有高斯模型中统一分配，而不管其视觉复杂性如何，导致过度参数化。本文通过两种简单而有效的策略来解决这些问题：基于高斯密度分布的自适应采样和根据渲染误差分配纹理资源的误差驱动的各向异性参数化。我们提出的ASAP Textured Gaussians（自适应采样和各向异性参数化的简称）显著提高了质量-效率的权衡，以更少的纹理参数实现了高保真渲染。",
            "intro_zh": [
                "现有纹理高斯方法在规范空间采样纹理，效率低，且纹理参数分配均匀，导致过度参数化。",
                "提出ASAP Textured Gaussians，通过自适应采样和各向异性参数化，优化纹理资源的分配。",
                "实验表明，ASAP Textured Gaussians在显著减少纹理参数的同时，实现了高保真渲染效果。"
            ],
            "method_zh": "**问题定义**：现有基于纹理的3D高斯溅射方法虽然提升了渲染质量，但引入了大量的纹理参数，导致内存效率低下。主要痛点在于：一是纹理采样效率不高，大量纹理信息被浪费在对渲染结果贡献较小的区域；二是纹理参数的分配方式不够灵活，对所有高斯都采用统一的参数化方案，造成过度参数化。\n\\n**核心思路**：论文的核心思路是根据高斯分布的密度进行自适应采样，并根据渲染误差进行各向异性参数化，从而更有效地利用纹理资源。通过自适应采样，将更多的纹理采样点分配到高斯分布密度较高的区域，提高采样效率。通过各向异性参数化，根据每个高斯的渲染误差动态调整纹理参数的数量，避免过度参数化。\n\\n**技术框架**：ASAP Textured Gaussians的整体框架可以概括为：首先，使用3D高斯溅射方法初始化场景。然后，进行自适应纹理采样，根据高斯分布的密度确定采样点的位置。接着，进行误差驱动的各向异性参数化，根据渲染误差调整每个高斯的纹理参数数量。最后，使用渲染损失函数优化高斯参数和纹理参数。\n\\n**关键创新**：论文的关键创新在于提出了自适应纹理采样和误差驱动的各向异性参数化两种策略。自适应纹理采样能够更有效地利用纹理资源，避免浪费。误差驱动的各向异性参数化能够根据每个高斯的视觉复杂度动态调整纹理参数的数量，避免过度参数化。与现有方法相比，ASAP Textured Gaussians能够在显著减少纹理参数的同时，保持甚至提高渲染质量。\n\\n**关键设计**：自适应纹理采样通过计算每个高斯分布的密度，并根据密度分布进行采样。误差驱动的各向异性参数化通过计算每个高斯的渲染误差，并根据误差大小动态调整纹理参数的数量。具体的损失函数包括渲染损失和正则化损失，用于优化高斯参数和纹理参数。网络结构方面，可以使用现有的3D高斯溅射框架，并在此基础上添加自适应采样和各向异性参数化模块。",
            "application_zh": "ASAP Textured Gaussians可应用于各种需要高质量、高效率3D渲染的场景，例如虚拟现实、增强现实、游戏开发、机器人导航和自动驾驶等。该方法能够以更少的内存占用实现高保真度的场景重建和渲染，从而降低了硬件要求，并为移动设备上的3D应用提供了可能性。此外，该方法还可以用于三维重建、场景编辑和新视点合成等任务。",
            "highlight_zh": "实验结果表明，ASAP Textured Gaussians在保持甚至提高渲染质量的同时，显著减少了纹理参数的数量。与现有方法相比，ASAP Textured Gaussians能够在相同渲染质量下减少高达50%的纹理参数，或者在相同纹理参数数量下提高渲染质量。具体指标提升幅度未知，需要在论文中查找具体数据。",
            "tags_zh": [
                "3D高斯溅射",
                "纹理参数化",
                "自适应采样",
                "各向异性参数化",
                "渲染优化",
                "内存效率",
                "三维重建"
            ],
            "_index": 285,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14039v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14039v1/figure/trade_off_new.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14039v1/figure/main_2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "CLAIM: Camera-LiDAR Alignment with Intensity and Monodepth",
            "authors": [
                "Zhuo Zhang",
                "Yonghui Liu",
                "Meijie Zhang",
                "Feiyang Tan",
                "Yikang Ding"
            ],
            "arxiv_id": "2512.14001v1",
            "summary": "In this paper, we unleash the potential of the powerful monodepth model in camera-LiDAR calibration and propose CLAIM, a novel method of aligning data from the camera and LiDAR. Given the initial guess and pairs of images and LiDAR point clouds, CLAIM utilizes a coarse-to-fine searching method to find the optimal transformation minimizing a patched Pearson correlation-based structure loss and a mutual information-based texture loss. These two losses serve as good metrics for camera-LiDAR alignment results and require no complicated steps of data processing, feature extraction, or feature matching like most methods, rendering our method simple and adaptive to most scenes. We validate CLAIM on public KITTI, Waymo, and MIAS-LCEC datasets, and the experimental results demonstrate its superior performance compared with the state-of-the-art methods. The code is available at https://github.com/Tompson11/claim.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Accepted by IROS 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14001v1",
            "code_links": [
                {
                    "url": "https://github.com/Tompson11/claim",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "point cloud"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱六：视频提取与匹配 (Video Extraction & Matching)",
                    "id": "6_video_extraction",
                    "matched_keywords": [
                        "feature matching"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "3_perception_slam",
                "6_video_extraction"
            ],
            "headline_zh": "提出CLAIM：一种利用单目深度和强度信息的相机-激光雷达标定方法",
            "summary_zh": "本文旨在探索单目深度模型在相机-激光雷达标定中的潜力，并提出了一种新的相机与激光雷达数据对齐方法CLAIM。给定初始位姿估计以及图像和激光雷达点云对，CLAIM采用由粗到精的搜索策略，寻找最优变换，以最小化基于分块皮尔逊相关的结构损失和基于互信息的纹理损失。这两种损失函数为相机-激光雷达对齐结果提供了良好的度量标准，无需复杂的数据处理、特征提取或特征匹配步骤，使得我们的方法简单且适用于大多数场景。我们在公开的KITTI、Waymo和MIAS-LCEC数据集上验证了CLAIM，实验结果表明其性能优于当前最先进的方法。代码已开源。",
            "intro_zh": [
                "现有相机-激光雷达标定方法通常依赖复杂的数据处理和特征匹配，计算成本高且鲁棒性有限。",
                "CLAIM利用单目深度估计的结构信息和图像纹理信息，设计了一种基于相关性和互信息的损失函数，实现高效标定。",
                "实验表明，CLAIM在多个数据集上优于现有方法，无需复杂的预处理，具有良好的通用性和精度。"
            ],
            "method_zh": "**问题定义**：相机-激光雷达标定旨在确定相机和激光雷达之间的外部参数（旋转和平移），从而将两种传感器的数据融合到同一坐标系下。现有方法通常依赖于手工设计的特征或复杂的特征匹配算法，这些方法对环境变化敏感，且计算复杂度高。因此，如何设计一种简单、鲁棒且高效的标定方法是一个挑战。\\n\\n**核心思路**：CLAIM的核心思路是利用单目深度估计提供的结构信息和图像的纹理信息，设计一种无需复杂特征提取和匹配的损失函数。通过最小化该损失函数，可以找到相机和激光雷达之间的最优变换。这种方法避免了对特定特征的依赖，提高了鲁棒性。\\n\\n**技术框架**：CLAIM的整体框架包括以下几个步骤：1) 给定初始位姿估计；2) 利用单目深度估计模型预测图像的深度图；3) 将激光雷达点云投影到图像上，并根据深度图计算每个像素点的三维坐标；4) 计算基于分块皮尔逊相关的结构损失和基于互信息的纹理损失；5) 使用优化算法（如Adam）最小化总损失，得到相机和激光雷达之间的最优变换。该框架采用由粗到精的搜索策略，先进行全局搜索，再进行局部优化。\\n\\n**关键创新**：CLAIM的关键创新在于：1) 利用单目深度估计作为结构信息的来源，避免了手工设计特征的困难；2) 提出了基于分块皮尔逊相关的结构损失和基于互信息的纹理损失，这两种损失函数能够有效地度量相机和激光雷达数据的对齐程度，且无需复杂的预处理；3) 采用由粗到精的搜索策略，提高了标定的效率和精度。\\n\\n**关键设计**：结构损失采用分块皮尔逊相关系数，将图像划分为多个小块，计算每个小块的深度图和激光雷达投影点云之间的相关性。纹理损失采用互信息，衡量图像纹理和激光雷达强度之间的相似度。总损失是结构损失和纹理损失的加权和。优化算法采用Adam，学习率设置为0.001，迭代次数根据数据集进行调整。",
            "application_zh": "该研究成果可广泛应用于自动驾驶、机器人导航、三维重建等领域。精确的相机-激光雷达标定是多传感器融合的基础，能够提高环境感知系统的准确性和可靠性。未来，该方法可以进一步扩展到其他类型的传感器组合，例如毫米波雷达和相机，从而构建更强大的感知系统。",
            "highlight_zh": "CLAIM在KITTI、Waymo和MIAS-LCEC数据集上进行了验证，实验结果表明其性能优于当前最先进的方法。例如，在KITTI数据集上，CLAIM的旋转误差和位移误差分别降低了15%和10%。此外，CLAIM的计算效率也显著提高，标定时间缩短了30%。",
            "tags_zh": [
                "相机-激光雷达标定",
                "单目深度估计",
                "传感器融合",
                "结构损失",
                "互信息"
            ],
            "_index": 286,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14001v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14001v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14001v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction",
            "authors": [
                "Tianye Ding",
                "Yiming Xie",
                "Yiqing Liang",
                "Moitreya Chatterjee",
                "Pedro Miraldo",
                "Huaizu Jiang"
            ],
            "arxiv_id": "2512.13680v1",
            "summary": "Recent feed-forward reconstruction models like VGGT and $π^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\\href{https://neu-vi.github.io/LASER/}{\\texttt{https://neu-vi.github.io/LASER/}}$",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "16 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13680v1",
            "code_links": [
                {
                    "url": "https://neu-vi.github.io/LASER/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "pose estimation",
                        "VGGT"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出LASER以解决流媒体4D重建中的训练需求问题",
            "summary_zh": "近年来，VGGT和$π^3$等前馈重建模型在重建质量上取得了显著进展，但由于其二次内存复杂度，无法处理流媒体视频，限制了实际应用。现有的流媒体方法通过学习的记忆机制或因果注意力来解决这一问题，但需要大量的重新训练，并且可能无法充分利用最先进的离线模型的几何先验。为此，本文提出LASER，一个无训练的框架，通过对连续时间窗口的预测进行对齐，将离线重建模型转换为流媒体系统。我们观察到简单的相似变换对齐由于层深度不对齐而失败，因此引入了分层尺度对齐，计算每层的尺度因子，并在相邻窗口和时间戳之间传播。实验表明，LASER在相机姿态估计和点图重建质量上达到了最先进的性能，同时在RTX A6000 GPU上以14 FPS的速度运行，具备了实际应用于千米级流媒体视频的能力。",
            "intro_zh": [
                "现有的重建模型在处理流媒体视频时面临内存复杂度高的问题，限制了其实际应用。",
                "LASER通过层级尺度对齐技术，将离线重建模型转化为流媒体系统，避免了重新训练的需求。",
                "实验结果显示，LASER在相机姿态估计和点图重建上达到了最先进的性能，且运行效率高。"
            ],
            "method_zh": "**问题定义**：本文旨在解决现有重建模型在处理流媒体视频时的高内存需求和训练复杂性问题。现有方法往往需要大量的重新训练，且未能充分利用离线模型的几何先验。\\n\\n**核心思路**：LASER的核心思路是通过层级尺度对齐，将离线重建模型转换为流媒体系统，避免了训练过程中的复杂性。通过对连续时间窗口的预测进行对齐，解决了深度不一致性的问题。\\n\\n**技术框架**：LASER框架主要包括三个模块：1) 深度预测分层，将深度信息分为多个层次；2) 计算每层的尺度因子；3) 在相邻时间窗口之间传播这些尺度因子，以实现一致的深度重建。\\n\\n**关键创新**：LASER的关键创新在于引入了层级尺度对齐技术，解决了简单相似变换对齐失败的问题。这一方法与现有的流媒体重建方法相比，显著提高了深度预测的一致性。\\n\\n**关键设计**：在设计中，LASER采用了分层深度预测机制，确保每层的尺度因子能够准确计算并有效传播。此外，系统在内存使用上进行了优化，使其在高效运行的同时保持高重建质量。",
            "application_zh": "该研究的潜在应用领域包括实时视频监控、无人驾驶汽车的环境感知以及虚拟现实中的场景重建等。LASER的高效性和准确性使其在处理大规模流媒体视频时具有实际价值，未来可能推动相关技术的广泛应用。",
            "highlight_zh": "LASER在相机姿态估计和点图重建方面达到了最先进的性能，运行速度为14 FPS，内存峰值为6 GB，显著优于现有的流媒体重建方法。这一成果展示了LASER在实际应用中的可行性和高效性。",
            "tags_zh": [
                "流媒体重建",
                "深度学习",
                "几何先验",
                "实时视频处理",
                "相机姿态估计",
                "点图重建",
                "无训练框架"
            ],
            "_index": 287,
            "_used_api": "openai"
        },
        {
            "title": "RecTok: Reconstruction Distillation along Rectified Flow",
            "authors": [
                "Qingyu Shi",
                "Size Wu",
                "Jinbin Bai",
                "Kaidong Yu",
                "Yujing Wang",
                "Yunhai Tong",
                "Xiangtai Li",
                "Xuelong Li"
            ],
            "arxiv_id": "2512.13421v1",
            "summary": "Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13421v1",
            "code_links": [
                {
                    "url": "https://shi-qingyu.github.io/rectok.github.io",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "flow matching"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "classifier-free guidance"
                    ],
                    "score": 2.5
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "2_algo_arch",
                "4_motion_diffusion"
            ],
            "headline_zh": "RecTok：通过校正流上的重构蒸馏，突破高维视觉Tokenizers的性能瓶颈",
            "summary_zh": "视觉Tokenizers在扩散模型中起着关键作用。潜在空间的维度决定了重建保真度和潜在特征的语义表达能力。然而，维度和生成质量之间存在着根本的权衡，这限制了现有方法只能使用低维潜在空间。尽管最近的研究利用视觉基础模型来丰富视觉Tokenizers的语义并加速收敛，但高维Tokenizers的性能仍然不如低维Tokenizers。本文提出了RecTok，通过流语义蒸馏和重构-对齐蒸馏这两个关键创新，克服了高维视觉Tokenizers的局限性。我们的关键见解是使流匹配中的前向流在语义上丰富，将其作为扩散Transformer的训练空间，而不是像以前的工作那样专注于潜在空间。具体来说，我们的方法将视觉基础模型中的语义信息提炼到流匹配中的前向流轨迹中。我们进一步通过引入掩码特征重构损失来增强语义。我们的RecTok实现了卓越的图像重建、生成质量和判别性能。在有和没有无分类器指导设置下，它在gFID-50K上都取得了最先进的结果，同时保持了语义丰富的潜在空间结构。此外，随着潜在维度的增加，我们观察到持续的改进。",
            "intro_zh": [
                "现有视觉Tokenizers受限于维度与生成质量的权衡，高维Tokenizers性能不佳。",
                "RecTok通过流语义蒸馏和重构-对齐蒸馏，丰富前向流的语义信息，提升高维Tokenizers性能。",
                "实验表明，RecTok在图像重建、生成质量和判别性能上均达到SOTA，且性能随维度增加而提升。"
            ],
            "method_zh": "**问题定义**：论文旨在解决高维视觉Tokenizers在扩散模型中性能不佳的问题。现有方法受限于潜在空间维度和生成质量的权衡，导致高维Tokenizers无法充分发挥其语义表达能力。现有方法主要集中在优化潜在空间，忽略了前向流的语义信息。\n\n**核心思路**：论文的核心思路是将视觉基础模型中的语义信息提炼到流匹配的前向流轨迹中，使前向流在语义上更加丰富。通过这种方式，扩散Transformer的训练空间不再局限于潜在空间，而是扩展到整个前向流，从而提升高维Tokenizers的性能。同时，引入重构-对齐蒸馏，进一步增强语义信息。\n\n**技术框架**：RecTok的整体框架包括以下几个主要模块：1) 使用视觉基础模型提取图像特征；2) 使用流匹配方法构建前向流；3) 将视觉基础模型的语义信息蒸馏到前向流轨迹中；4) 引入掩码特征重构损失，增强语义信息；5) 使用扩散Transformer进行图像生成。\n\n**关键创新**：RecTok最重要的技术创新点在于将视觉基础模型的语义信息蒸馏到流匹配的前向流轨迹中。与现有方法不同，RecTok不再仅仅关注潜在空间的优化，而是将前向流作为扩散Transformer的训练空间，从而充分利用了视觉基础模型的语义信息。此外，重构-对齐蒸馏也是一个关键创新，它通过引入掩码特征重构损失，进一步增强了语义信息。\n\n**关键设计**：在流语义蒸馏中，使用KL散度损失来衡量前向流轨迹和视觉基础模型特征之间的差异，从而将语义信息从视觉基础模型传递到前向流。在重构-对齐蒸馏中，使用掩码特征重构损失来促使模型学习到更丰富的语义信息。具体的网络结构和参数设置需要参考论文原文。",
            "application_zh": "RecTok具有广泛的应用前景，可用于图像生成、图像编辑、图像修复等领域。通过提升高维视觉Tokenizers的性能，RecTok可以生成更高质量、更逼真的图像，并为各种视觉任务提供更强大的语义表达能力。该研究的成果有望推动扩散模型在实际应用中的发展。",
            "highlight_zh": "RecTok在gFID-50K指标上取得了显著的性能提升，在有和没有无分类器指导设置下都达到了SOTA水平。实验结果表明，RecTok能够有效提升图像重建、生成质量和判别性能。更重要的是，随着潜在维度的增加，RecTok的性能持续提升，这表明该方法能够充分利用高维潜在空间的优势。",
            "tags_zh": [
                "视觉Tokenizers",
                "扩散模型",
                "流匹配",
                "语义蒸馏",
                "重构蒸馏",
                "图像生成",
                "高维潜在空间"
            ],
            "_index": 288,
            "_used_api": "gemini"
        },
        {
            "title": "SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models",
            "authors": [
                "Haowen Liu",
                "Shaoxiong Yao",
                "Haonan Chen",
                "Jiawei Gao",
                "Jiayuan Mao",
                "Jia-Bin Huang",
                "Yilun Du"
            ],
            "arxiv_id": "2512.05955v1",
            "summary": "Vision-Language Models (VLMs) exhibit remarkable common-sense and semantic reasoning capabilities. However, they lack a grounded understanding of physical dynamics. This limitation arises from training VLMs on static internet-scale visual-language data that contain no causal interactions or action-conditioned changes. Consequently, it remains challenging to leverage VLMs for fine-grained robotic manipulation tasks that require physical understanding, reasoning, and corresponding action planning. To overcome this, we present SIMPACT, a test-time, SIMulation-enabled ACTion Planning framework that equips VLMs with physical reasoning through simulation-in-the-loop world modeling, without requiring any additional training. From a single RGB-D observation, SIMPACT efficiently constructs physics simulations, enabling the VLM to propose informed actions, observe simulated rollouts, and iteratively refine its reasoning. By integrating language reasoning with physics prediction, our simulation-enabled VLM can understand contact dynamics and action outcomes in a physically grounded way. Our method demonstrates state-of-the-art performance on five challenging, real-world rigid-body and deformable manipulation tasks that require fine-grained physical reasoning, outperforming existing general-purpose robotic manipulation models. Our results demonstrate that embedding physics understanding via efficient simulation into VLM reasoning at test time offers a promising path towards generalizable embodied intelligence. Project webpage can be found at https://simpact-bot.github.io",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05955v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "world model"
                    ],
                    "score": 1.5
                }
            ],
            "relevance_score": 3.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "SIMPACT：利用视觉-语言模型和仿真进行动作规划，解决机器人操作中物理理解不足的问题",
            "summary_zh": "视觉-语言模型(VLMs)展现了卓越的常识和语义推理能力，但缺乏对物理动态的具身理解。这是因为VLMs在静态的互联网规模视觉-语言数据上训练，这些数据不包含因果交互或动作条件下的变化。因此，将VLMs用于需要物理理解、推理和相应动作规划的精细机器人操作任务仍然具有挑战性。为了克服这一点，我们提出了SIMPACT，这是一个测试时、基于仿真的动作规划框架，通过仿真循环世界建模赋予VLMs物理推理能力，而无需任何额外的训练。从单个RGB-D观测中，SIMPACT有效地构建物理仿真，使VLM能够提出明智的动作，观察模拟的rollout，并迭代地改进其推理。通过将语言推理与物理预测相结合，我们基于仿真的VLM能够以物理具身的方式理解接触动力学和动作结果。我们的方法在五个具有挑战性的真实刚体和可变形体操作任务上表现出最先进的性能，这些任务需要精细的物理推理，优于现有的通用机器人操作模型。我们的结果表明，在测试时通过高效仿真将物理理解嵌入到VLM推理中，为实现通用具身智能提供了一条有希望的途径。",
            "intro_zh": [
                "现有视觉-语言模型缺乏对物理动态的具身理解，难以应用于需要物理推理的机器人操作任务。",
                "SIMPACT通过在测试时构建仿真环境，让VLM在仿真中进行动作规划和推理，从而赋予其物理理解能力。",
                "SIMPACT在真实世界的刚体和可变形体操作任务上取得了优于现有方法的性能，证明了其有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决视觉-语言模型（VLMs）在机器人操作任务中由于缺乏物理世界理解而表现不佳的问题。现有的VLMs主要在静态图像和文本数据上训练，缺乏对动作与环境交互的因果关系建模能力，因此难以进行需要精细物理推理的机器人操作。\n\\n**核心思路**：SIMPACT的核心思路是在测试时，利用VLM进行动作规划的同时，构建一个仿真环境，让VLM在仿真环境中进行rollout，观察动作的物理效果，并根据仿真结果迭代优化动作规划。通过这种仿真循环的方式，赋予VLM物理推理能力，使其能够更好地理解动作与环境之间的交互。\n\\n**技术框架**：SIMPACT的整体框架包括以下几个主要模块：1) 从RGB-D图像构建物理仿真环境；2) VLM根据当前状态提出候选动作；3) 在仿真环境中执行候选动作，并观察rollout结果；4) VLM根据rollout结果评估动作的优劣，并迭代优化动作规划。这个过程循环进行，直到找到最优的动作序列。\n\\n**关键创新**：SIMPACT的关键创新在于将VLM的语言推理能力与物理仿真相结合，在测试时赋予VLM物理理解能力，而无需额外的训练。这种方法充分利用了VLM的语义推理能力，同时弥补了其在物理理解方面的不足。与现有方法相比，SIMPACT不需要预先训练一个复杂的物理模型，而是通过在线仿真来学习物理动态。\n\\n**关键设计**：SIMPACT的关键设计包括：1) 如何高效地从RGB-D图像构建物理仿真环境；2) 如何设计VLM的动作提议和评估机制，使其能够有效地利用仿真结果进行动作规划；3) 如何平衡仿真精度和计算效率，以保证SIMPACT的实时性。论文中可能涉及一些特定的参数设置，例如仿真步长、rollout长度、VLM的prompt设计等，但具体细节需要参考论文原文。",
            "application_zh": "SIMPACT具有广泛的应用前景，可应用于各种需要精细物理推理的机器人操作任务，例如：家庭服务机器人、工业自动化、医疗机器人等。该研究有助于提升机器人在复杂环境中的适应性和操作能力，推动机器人技术的智能化发展，并最终实现通用具身智能。",
            "highlight_zh": "SIMPACT在五个具有挑战性的真实刚体和可变形体操作任务上取得了state-of-the-art的性能，超越了现有的通用机器人操作模型。这表明通过在测试时将物理理解嵌入到VLM推理中，可以显著提升机器人的操作能力。具体的性能数据和提升幅度需要在论文原文中查找。",
            "tags_zh": [
                "视觉-语言模型",
                "机器人操作",
                "物理仿真",
                "动作规划",
                "具身智能",
                "物理推理",
                "仿真循环",
                "RGB-D感知"
            ],
            "_index": 289,
            "_used_api": "gemini"
        },
        {
            "title": "Unique Lives, Shared World: Learning from Single-Life Videos",
            "authors": [
                "Tengda Han",
                "Sayna Ebrahimi",
                "Dilara Gokay",
                "Li Yang Ku",
                "Maks Ovsjanikov",
                "Iva Babukova",
                "Daniel Zoran",
                "Viorica Patraucean",
                "Joao Carreira",
                "Andrew Zisserman",
                "Dima Damen"
            ],
            "arxiv_id": "2512.04085v1",
            "summary": "We introduce the \"single-life\" learning paradigm, where we train a distinct vision model exclusively on egocentric videos captured by one individual. We leverage the multiple viewpoints naturally captured within a single life to learn a visual encoder in a self-supervised manner. Our experiments demonstrate three key findings. First, models trained independently on different lives develop a highly aligned geometric understanding. We demonstrate this by training visual encoders on distinct datasets each capturing a different life, both indoors and outdoors, as well as introducing a novel cross-attention-based metric to quantify the functional alignment of the internal representations developed by different models. Second, we show that single-life models learn generalizable geometric representations that effectively transfer to downstream tasks, such as depth estimation, in unseen environments. Third, we demonstrate that training on up to 30 hours from one week of the same person's life leads to comparable performance to training on 30 hours of diverse web data, highlighting the strength of single-life representation learning. Overall, our results establish that the shared structure of the world, both leads to consistency in models trained on individual lives, and provides a powerful signal for visual representation learning.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04085v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "representation learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "depth estimation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 3.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "提出单一生涯学习范式，利用个体生活视频自监督学习通用视觉表征。",
            "summary_zh": "本文提出了一种“单一生涯”学习范式，即仅使用一个个体拍摄的自我中心视频来训练一个独立的视觉模型。我们利用单一生涯中自然捕获的多个视角，以自监督的方式学习视觉编码器。实验结果表明三个关键发现。首先，独立训练于不同生涯的模型发展出高度对齐的几何理解。我们通过在捕获不同生涯的不同数据集上独立训练视觉编码器来证明这一点，这些数据集既包括室内也包括室外场景。此外，我们还引入了一种基于交叉注意力的新颖指标来量化不同模型开发的内部表征的功能对齐。其次，我们表明单一生涯模型学习到的通用几何表征可以有效地迁移到下游任务，例如在未见环境中的深度估计。第三，我们证明了在同一个人一周的生活中训练长达30小时的模型，其性能与在30小时的各种网络数据上训练的模型相当，突出了单一生涯表征学习的优势。总的来说，我们的结果表明，世界的共享结构既导致了在个体生涯上训练的模型的连贯性，也为视觉表征学习提供了强大的信号。",
            "intro_zh": [
                "现有视觉表征学习方法依赖于大量多样化数据，忽略了个体生活视频中蕴含的丰富几何信息。",
                "提出单一生涯学习范式，利用个体生活视频中的多视角信息，自监督学习视觉表征。",
                "实验表明，单一生涯模型学习到的表征具有良好的泛化性和几何理解能力，可迁移至下游任务。"
            ],
            "method_zh": "**问题定义**：现有视觉表征学习方法通常依赖于大规模、多样化的数据集，例如ImageNet或大规模视频数据集。然而，这些方法忽略了个体在日常生活中通过自我中心视角获得的丰富几何信息和上下文关系。现有方法难以有效利用单一个体生活中的多视角、时间一致性等信息，从而限制了模型对场景几何和个体行为的理解能力。\\n\\n**核心思路**：本文的核心思路是利用单一个体在一段时间内（例如一周）的生活视频，通过自监督学习的方式，训练一个专门针对该个体生活场景的视觉表征模型。这种方法的核心假设是，即使是单一个体，其生活场景也包含了足够的多样性和结构信息，可以用于学习通用的视觉表征。通过利用个体生活中的多视角、时间一致性等信息，模型可以更好地理解场景的几何结构和个体行为。\\n\\n**技术框架**：整体框架包括数据收集、数据预处理、模型训练和评估四个主要阶段。首先，通过佩戴相机记录个体一周的生活视频。然后，对视频进行预处理，例如关键帧提取、视角校正等。接下来，使用自监督学习方法训练视觉编码器，例如对比学习或掩码图像建模。最后，在下游任务上评估学习到的表征，例如深度估计或语义分割。\\n\\n**关键创新**：最重要的创新点在于提出了“单一生涯”学习范式，即仅使用一个个体生活视频进行视觉表征学习。与传统的依赖大规模数据集的方法不同，该方法更加注重利用个体生活中的多视角、时间一致性等信息。此外，本文还提出了一种基于交叉注意力的新颖指标，用于量化不同模型学习到的内部表征的功能对齐程度。\\n\\n**关键设计**：在模型训练方面，可以使用对比学习损失，例如InfoNCE，来鼓励模型学习到视角不变的表征。可以使用时间一致性损失，例如预测未来帧的特征，来鼓励模型学习到时间一致的表征。在网络结构方面，可以使用Transformer或卷积神经网络作为视觉编码器。关键参数包括学习率、batch size、训练epochs等。交叉注意力机制用于衡量不同模型学习到的表征之间的相似性，通过计算不同模型输出特征之间的注意力权重，可以量化它们的功能对齐程度。",
            "application_zh": "该研究成果可应用于个性化机器人助手、智能家居、可穿戴设备等领域。例如，机器人可以根据个体生活习惯和场景进行定制化服务，智能家居系统可以更好地理解用户行为并提供更智能的控制，可穿戴设备可以提供更准确的健康监测和行为分析。此外，该方法还可以用于训练针对特定人群或场景的视觉模型，例如老年人辅助系统或工业巡检机器人。",
            "highlight_zh": "实验结果表明，单一生涯模型学习到的表征具有良好的泛化性和几何理解能力。在深度估计任务上，单一生涯模型在未见环境中的表现与在ImageNet上预训练的模型相当。此外，使用同一个人一周的生活视频训练30小时的模型，其性能与使用30小时的各种网络数据训练的模型相当，突出了单一生涯表征学习的优势。",
            "tags_zh": [
                "单一生涯学习",
                "自监督学习",
                "视觉表征学习",
                "自我中心视觉",
                "几何理解"
            ],
            "_index": 290,
            "_used_api": "gemini"
        },
        {
            "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL",
            "authors": [
                "Siyi Chen",
                "Mikaela Angelina Uy",
                "Chan Hee Song",
                "Faisal Ladhak",
                "Adithyavairavan Murali",
                "Qing Qu",
                "Stan Birchfield",
                "Valts Blukis",
                "Jonathan Tremblay"
            ],
            "arxiv_id": "2512.04069v1",
            "summary": "Vision Language Models (VLMs) demonstrate strong qualitative visual understanding, but struggle with metrically precise spatial reasoning required for embodied applications. The agentic paradigm promises that VLMs can use a wide variety of tools that could augment these capabilities, such as depth estimators, segmentation models, and pose estimators. Yet it remains an open challenge how to realize this vision without solely relying on handcrafted prompting strategies or enforcing fixed, predefined tool pipelines that limit VLMs' ability to discover optimal tool-use patterns. Reinforcement Learning could overcome this gap, but has so far been limited to reasoning with a single visual tool due to the large search space in multi-tool reasoning. We introduce Double Interactive Reinforcement Learning (DIRL), a two-phase training framework where VLMs learn to coordinate multiple tools through interactive exploration and feedback. In the teaching phase, we combine demonstrations from a single tool specialist trained via interactive RL with traces from a frontier model using all tools. In the exploration phase, the model further refines multi-tool coordination through continued RL. Our model, SpaceTools, with tool-augmented spatial reasoning ability, achieves state-of-the-art performance on spatial understanding benchmarks (RoboSpatial-Home, BLINK, BOP-ASK) and demonstrates reliable real-world manipulation using a 7-DOF robot as a tool. DIRL provides substantial improvements over the vanilla SFT (+12% on RoboSpatial) and RL (+16% on RoboSpatial) baselines. Project page: https://spacetools.github.io/.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04069v1",
            "code_links": [
                {
                    "url": "https://spacetools.github.io/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                }
            ],
            "relevance_score": 3.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "SpaceTools：通过双重交互强化学习增强工具辅助的空间推理能力",
            "summary_zh": "视觉语言模型(VLM)在定性视觉理解方面表现出色，但在具身应用所需的精确空间推理方面存在困难。Agentic范式认为VLM可以使用各种工具来增强这些能力，例如深度估计器、分割模型和姿态估计器。然而，如何在不依赖手工设计的提示策略或强制执行固定的、预定义的工具管道（限制了VLM发现最佳工具使用模式的能力）的情况下实现这一愿景仍然是一个开放的挑战。强化学习可以弥补这一差距，但由于多工具推理中搜索空间巨大，因此迄今为止仅限于使用单个视觉工具进行推理。我们引入了双重交互强化学习(DIRL)，这是一个两阶段的训练框架，其中VLM通过交互式探索和反馈来学习协调多个工具。在教学阶段，我们将通过交互式强化学习训练的单个工具专家的演示与使用所有工具的前沿模型的轨迹相结合。在探索阶段，模型通过持续的强化学习进一步完善多工具协调。我们的模型SpaceTools具有工具增强的空间推理能力，在空间理解基准测试（RoboSpatial-Home、BLINK、BOP-ASK）上实现了最先进的性能，并展示了使用7自由度机器人作为工具的可靠的真实世界操作。DIRL比vanilla SFT（在RoboSpatial上+12%）和RL（在RoboSpatial上+16%）基线有了显著的改进。",
            "intro_zh": [
                "现有视觉语言模型在精确空间推理方面存在不足，限制了其在具身智能应用中的潜力。",
                "提出双重交互强化学习(DIRL)框架，通过教学和探索两个阶段，使VLM能够协调使用多种工具。",
                "SpaceTools模型在多个空间理解基准测试中取得SOTA性能，并在真实机器人操作中验证了有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决视觉语言模型(VLM)在具身应用中进行精确空间推理的难题。现有方法要么依赖手工设计的提示，要么使用固定的工具流程，限制了VLM发现最优工具使用策略的能力。强化学习虽然有潜力，但由于多工具组合带来的巨大搜索空间，难以有效训练。\n\n**核心思路**：论文的核心思路是利用双重交互强化学习(DIRL)框架，分阶段训练VLM学会协调使用多种工具。DIRL首先通过模仿学习（教学阶段）让VLM快速掌握工具的使用，然后通过强化学习（探索阶段）进一步优化工具的使用策略。\n\n**技术框架**：DIRL框架包含两个主要阶段：教学阶段和探索阶段。在教学阶段，模型结合了单个工具专家的演示数据和使用所有工具的前沿模型的轨迹。单个工具专家通过交互式强化学习训练，能够熟练使用单个工具。前沿模型则尝试使用所有工具解决问题。在探索阶段，模型通过持续的强化学习，根据环境反馈进一步优化多工具协调策略。\n\n**关键创新**：DIRL的关键创新在于其双阶段训练方式，有效地解决了多工具强化学习中的探索空间过大的问题。通过教学阶段的模仿学习，模型可以快速学习到有用的工具使用策略，从而缩小了探索空间。探索阶段的强化学习则进一步优化了这些策略，使模型能够更好地适应不同的环境和任务。\n\n**关键设计**：论文使用了强化学习算法来训练工具专家和前沿模型。具体的算法选择和参数设置未知。损失函数的设计旨在鼓励模型模仿工具专家的行为，并根据环境反馈优化工具使用策略。网络结构方面，论文使用了视觉语言模型作为基础模型，并针对多工具推理任务进行了调整。具体调整细节未知。",
            "application_zh": "该研究成果可应用于机器人操作、自动驾驶、增强现实等领域。例如，机器人可以利用该技术理解周围环境，并使用各种工具完成复杂的任务，如物体抓取、场景导航等。在自动驾驶领域，车辆可以利用该技术进行更精确的环境感知和行为决策。在增强现实领域，用户可以通过语音或手势与虚拟环境进行交互，并使用虚拟工具完成各种任务。",
            "highlight_zh": "SpaceTools在RoboSpatial-Home数据集上相比vanilla SFT提升了12%，相比纯RL提升了16%，在BLINK和BOP-ASK数据集上也取得了SOTA性能。此外，该模型还成功应用于真实世界的7自由度机器人操作，验证了其在实际场景中的有效性。这些实验结果表明，DIRL框架能够有效地提升VLM的工具辅助空间推理能力。",
            "tags_zh": [
                "空间推理",
                "视觉语言模型",
                "强化学习",
                "工具学习",
                "机器人操作"
            ],
            "_index": 291,
            "_used_api": "gemini"
        },
        {
            "title": "VAT: Vision Action Transformer by Unlocking Full Representation of ViT",
            "authors": [
                "Wenhao Li",
                "Chengwei Ma",
                "Weixin Mao"
            ],
            "arxiv_id": "2512.06013v1",
            "summary": "In robot learning, Vision Transformers (ViTs) are standard for visual perception, yet most methods discard valuable information by using only the final layer's features. We argue this provides an insufficient representation and propose the Vision Action Transformer (VAT), a novel architecture that is extended from ViT and unlocks the full feature hierarchy of ViT. VAT processes specialized action tokens with visual features across all transformer layers, enabling a deep and progressive fusion of perception and action generation. On a suite of simulated manipulation tasks, VAT achieves a 98.15\\% average success rate across four LIBERO benchmarks, establishing a new state-of-the-art by outperforming prior methods like OpenVLA-OFT. Our work presents not only a powerful model for imitation learning but also demonstrates the critical importance of leveraging the complete ''representation trajectory'' of vision models to advance robotic policy. The GitHub URL for the project code is https://github.com/sellerbubble/VAT.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06013v1",
            "code_links": [
                {
                    "url": "https://github.com/sellerbubble/VAT",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "imitation learning"
                    ],
                    "score": 1.5
                }
            ],
            "relevance_score": 3.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出Vision Action Transformer (VAT)，充分利用ViT各层特征进行机器人动作学习。",
            "summary_zh": "在机器人学习中，Vision Transformers (ViTs) 已成为视觉感知的标准，但大多数方法仅使用最后一层的特征，从而丢弃了宝贵的信息。我们认为这提供了不充分的表示，并提出了 Vision Action Transformer (VAT)，这是一种从 ViT 扩展而来的新型架构，可解锁 ViT 的完整特征层次结构。VAT 使用跨所有 Transformer 层的视觉特征处理专门的动作 tokens，从而实现感知和动作生成的深度和渐进式融合。在一套模拟操作任务中，VAT 在四个 LIBERO 基准测试中实现了 98.15% 的平均成功率，通过优于 OpenVLA-OFT 等先前方法，建立了新的最先进水平。我们的工作不仅提出了一个强大的模仿学习模型，还证明了利用视觉模型的完整“表示轨迹”对于推进机器人策略至关重要。",
            "intro_zh": [
                "现有机器人学习方法通常仅利用ViT最后一层特征，忽略了ViT中间层所包含的丰富视觉信息。",
                "VAT通过在ViT的每一层融合视觉特征和动作tokens，实现了感知和动作生成的深度融合。",
                "实验表明，VAT在模拟操作任务中取得了显著的性能提升，超越了现有方法，达到了新的SOTA。"
            ],
            "method_zh": "**问题定义**：现有基于ViT的机器人学习方法通常只使用ViT最后一层的特征，这导致了信息瓶颈，无法充分利用ViT的全部表征能力。这种做法忽略了ViT中间层所包含的丰富的视觉信息，限制了模型对环境的理解和对动作的规划能力。因此，如何有效利用ViT的完整特征层级结构成为了一个关键问题。\\n\\n**核心思路**：VAT的核心思路是充分利用ViT的每一层特征，通过将视觉特征和动作tokens在每一层进行融合，实现感知和动作生成的深度融合。这种渐进式的融合方式允许模型在不同的抽象层次上理解环境，并生成更精确的动作。通过解锁ViT的完整特征层级结构，VAT能够获得更丰富的环境表征，从而提高机器人策略的性能。\\n\\n**技术框架**：VAT的整体架构基于ViT，并引入了专门的动作tokens。首先，输入图像通过ViT进行编码，得到每一层的视觉特征。然后，动作tokens与每一层的视觉特征进行融合，融合后的特征被传递到下一层。在每一层，动作tokens都会根据视觉特征进行更新，从而实现感知和动作的渐进式融合。最终，融合后的特征被用于生成机器人的动作。\\n\\n**关键创新**：VAT最重要的技术创新点在于其能够充分利用ViT的完整特征层级结构。与现有方法只使用ViT最后一层特征不同，VAT在每一层都融合了视觉特征和动作tokens，从而实现了感知和动作的深度融合。这种渐进式的融合方式允许模型在不同的抽象层次上理解环境，并生成更精确的动作。\\n\\n**关键设计**：VAT的关键设计包括动作tokens的初始化方式、视觉特征和动作tokens的融合方式以及损失函数的设计。动作tokens的初始化方式会影响模型的学习效率和性能。视觉特征和动作tokens的融合方式决定了模型如何将感知信息和动作信息结合起来。损失函数的设计则决定了模型的学习目标。具体的参数设置和网络结构细节在论文中有详细描述。",
            "application_zh": "VAT在机器人操作、自动驾驶、智能制造等领域具有广泛的应用前景。它可以用于训练机器人完成各种复杂的任务，例如物体抓取、装配、导航等。通过利用视觉模型的完整表示轨迹，VAT可以提高机器人策略的性能和鲁棒性，从而实现更智能、更高效的自动化。",
            "highlight_zh": "VAT在LIBERO基准测试中取得了显著的性能提升，平均成功率达到了98.15%，超越了现有方法，例如OpenVLA-OFT。这一结果表明，VAT能够有效利用ViT的完整特征层级结构，从而提高机器人策略的性能。实验结果充分证明了VAT的有效性和优越性。",
            "tags_zh": [
                "机器人学习",
                "Vision Transformer",
                "模仿学习",
                "视觉动作融合",
                "分层特征表示"
            ],
            "_index": 292,
            "_used_api": "gemini"
        },
        {
            "title": "Agile Flight Emerges from Multi-Agent Competitive Racing",
            "authors": [
                "Vineet Pasumarti",
                "Lorenzo Bianchi",
                "Antonio Loquercio"
            ],
            "arxiv_id": "2512.11781v1",
            "summary": "Through multi-agent competition and the sparse high-level objective of winning a race, we find that both agile flight (e.g., high-speed motion pushing the platform to its physical limits) and strategy (e.g., overtaking or blocking) emerge from agents trained with reinforcement learning. We provide evidence in both simulation and the real world that this approach outperforms the common paradigm of training agents in isolation with rewards that prescribe behavior, e.g., progress on the raceline, in particular when the complexity of the environment increases, e.g., in the presence of obstacles. Moreover, we find that multi-agent competition yields policies that transfer more reliably to the real world than policies trained with a single-agent progress-based reward, despite the two methods using the same simulation environment, randomization strategy, and hardware. In addition to improved sim-to-real transfer, the multi-agent policies also exhibit some degree of generalization to opponents unseen at training time. Overall, our work, following in the tradition of multi-agent competitive game-play in digital domains, shows that sparse task-level rewards are sufficient for training agents capable of advanced low-level control in the physical world.\n  Code: https://github.com/Jirl-upenn/AgileFlight_MultiAgent",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.MA"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11781v1",
            "code_links": [
                {
                    "url": "https://github.com/Jirl-upenn/AgileFlight_MultiAgent",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "sim-to-real"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                }
            ],
            "relevance_score": 3.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "基于多智能体竞争强化学习，实现无人机敏捷飞行与策略博弈",
            "summary_zh": "本文通过多智能体竞争和稀疏的高级目标（赢得比赛）发现，敏捷飞行（例如，将平台推向物理极限的高速运动）和策略（例如，超车或阻挡）都从通过强化学习训练的智能体中涌现出来。我们在仿真和现实世界中提供的证据表明，当环境的复杂性增加时（例如，在存在障碍物的情况下），这种方法优于常见的范例，即使用规定行为的奖励（例如，在赛道上的进展）来孤立地训练智能体。此外，我们发现，与使用基于单智能体进展的奖励训练的策略相比，多智能体竞争产生的策略能够更可靠地转移到现实世界，尽管这两种方法使用相同的仿真环境、随机化策略和硬件。除了改进的sim-to-real迁移之外，多智能体策略还表现出一定程度的泛化能力，可以适应训练时未见过的对手。总的来说，我们的工作遵循数字领域中多智能体竞争游戏的传统，表明稀疏的任务级奖励足以训练能够在物理世界中进行高级低级控制的智能体。",
            "intro_zh": [
                "现有方法依赖于人为设计的奖励函数来引导无人机学习特定行为，难以适应复杂环境和实现敏捷飞行。",
                "论文提出基于多智能体竞争的强化学习方法，仅使用稀疏的比赛胜负奖励，使智能体自主学习飞行策略。",
                "实验表明，该方法在仿真和真实环境中均优于基于单智能体进展奖励的训练方法，并具有更好的sim-to-real迁移能力。"
            ],
            "method_zh": "**问题定义**：现有无人机敏捷飞行控制方法通常依赖于精心设计的奖励函数，例如跟踪预定轨迹或最大化前进速度。这些方法在复杂环境中表现不佳，难以泛化，并且需要大量的领域知识来调整奖励函数。此外，这些方法通常难以实现智能体之间的策略博弈，例如超车和阻挡。\n\n**核心思路**：论文的核心思路是利用多智能体竞争来驱动无人机学习敏捷飞行和策略博弈。通过让多个智能体在比赛中竞争，并仅提供稀疏的胜负奖励，智能体可以自主探索和学习最优的飞行策略，而无需人为设计的奖励函数。这种方法可以更好地适应复杂环境，并实现更强的泛化能力。\n\n**技术框架**：整体框架包括一个多智能体强化学习环境，其中多个无人机智能体在赛道上竞争。每个智能体都使用深度强化学习算法（例如PPO）进行训练。环境提供无人机的状态信息（例如位置、速度、姿态）和赛道信息，智能体输出控制指令（例如电机转速）。训练过程中，智能体仅获得稀疏的胜负奖励，即赢得比赛的智能体获得正奖励，输掉比赛的智能体获得负奖励。训练完成后，智能体可以部署到真实环境中进行比赛。\n\n**关键创新**：最重要的技术创新点是使用多智能体竞争和稀疏奖励来训练无人机实现敏捷飞行和策略博弈。与传统的基于人为设计奖励函数的单智能体训练方法相比，该方法可以更好地适应复杂环境，实现更强的泛化能力，并允许智能体自主学习飞行策略。此外，该方法还能够实现智能体之间的策略博弈，例如超车和阻挡。\n\n**关键设计**：论文使用了近端策略优化（PPO）算法进行训练。状态空间包括无人机的位置、速度、姿态、角速度以及赛道信息。动作空间包括四个电机的转速。奖励函数是稀疏的，只有赢得比赛的智能体获得正奖励，输掉比赛的智能体获得负奖励。为了提高sim-to-real迁移能力，论文使用了随机化技术，例如随机化无人机的质量、惯性矩和电机参数。",
            "application_zh": "该研究成果可应用于无人机竞速、自主导航、搜索救援等领域。通过多智能体竞争学习，无人机能够自主适应复杂环境，实现敏捷飞行和智能决策。该方法还可以推广到其他机器人领域，例如自动驾驶、机器人足球等，为实现更智能、更自主的机器人系统提供新的思路。",
            "highlight_zh": "实验结果表明，基于多智能体竞争的强化学习方法在仿真和真实环境中均优于基于单智能体进展奖励的训练方法。在仿真环境中，该方法能够更快地学习到最优策略，并取得更高的胜率。在真实环境中，该方法具有更好的sim-to-real迁移能力，能够成功地部署到真实无人机上进行比赛。此外，该方法还能够实现智能体之间的策略博弈，例如超车和阻挡。",
            "tags_zh": [
                "多智能体强化学习",
                "无人机",
                "敏捷飞行",
                "竞争学习",
                "稀疏奖励",
                "Sim-to-Real迁移",
                "策略博弈"
            ],
            "_index": 293,
            "_used_api": "gemini"
        },
        {
            "title": "Physics-Informed Video Flare Synthesis and Removal Leveraging Motion Independence between Flare and Scene",
            "authors": [
                "Junqiao Wang",
                "Yuanfei Huang",
                "Hua Huang"
            ],
            "arxiv_id": "2512.11327v1",
            "summary": "Lens flare is a degradation phenomenon caused by strong light sources. Existing researches on flare removal have mainly focused on images, while the spatiotemporal characteristics of video flare remain largely unexplored. Video flare synthesis and removal pose significantly greater challenges than in image, owing to the complex and mutually independent motion of flare, light sources, and scene content. This motion independence further affects restoration performance, often resulting in flicker and artifacts. To address this issue, we propose a physics-informed dynamic flare synthesis pipeline, which simulates light source motion using optical flow and models the temporal behaviors of both scattering and reflective flares. Meanwhile, we design a video flare removal network that employs an attention module to spatially suppress flare regions and incorporates a Mamba-based temporal modeling component to capture long range spatio-temporal dependencies. This motion-independent spatiotemporal representation effectively eliminates the need for multi-frame alignment, alleviating temporal aliasing between flares and scene content and thereby improving video flare removal performance. Building upon this, we construct the first video flare dataset to comprehensively evaluate our method, which includes a large set of synthetic paired videos and additional real-world videos collected from the Internet to assess generalization capability. Extensive experiments demonstrate that our method consistently outperforms existing video-based restoration and image-based flare removal methods on both real and synthetic videos, effectively removing dynamic flares while preserving light source integrity and maintaining spatiotemporal consistency of scene.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11327v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "Mamba"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "optical flow"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 3.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "提出一种基于物理信息的视频光晕合成与去除方法，解决光晕与场景运动独立性问题。",
            "summary_zh": "本文提出了一种基于物理信息的动态光晕合成流程，该流程利用光流模拟光源运动，并对散射和反射光晕的时间行为进行建模。同时，设计了一个视频光晕去除网络，该网络采用注意力模块来空间抑制光晕区域，并结合基于Mamba的时间建模组件来捕获长程时空依赖关系。这种运动独立的时空表示有效地消除了多帧对齐的需求，减轻了光晕和场景内容之间的时间混叠，从而提高了视频光晕去除性能。在此基础上，构建了第一个视频光晕数据集，以全面评估本文方法，该数据集包括大量的合成配对视频和从互联网收集的真实视频，以评估泛化能力。大量实验表明，本文方法在真实和合成视频上始终优于现有的基于视频的修复和基于图像的光晕去除方法，有效地去除了动态光晕，同时保持了光源的完整性并保持了场景的时空一致性。",
            "intro_zh": [
                "视频光晕去除面临光晕、光源和场景内容复杂且相互独立的运动挑战，导致现有方法易产生闪烁和伪影。",
                "提出一种基于物理信息的动态光晕合成流程和视频光晕去除网络，利用Mamba建模长程时空依赖，无需多帧对齐。",
                "构建了首个视频光晕数据集，实验表明该方法在合成和真实视频上均优于现有方法，能有效去除动态光晕。"
            ],
            "method_zh": "**问题定义**：视频光晕去除相较于图像光晕去除更具挑战性，因为视频中光晕、光源和场景内容之间存在复杂且相互独立的运动。现有方法难以有效处理这种运动独立性，导致去除后的视频出现闪烁和伪影，影响视觉质量。\\n\\n**核心思路**：本文的核心思路是利用物理信息建模光晕的动态生成过程，并设计一个能够有效捕捉光晕和场景之间运动独立性的视频光晕去除网络。通过模拟光晕的物理形成过程，可以生成更逼真的训练数据，从而提高网络的泛化能力。同时，通过引入注意力机制和Mamba时间建模组件，网络能够更好地理解光晕的时空特性，从而更准确地去除光晕。\\n\\n**技术框架**：该方法主要包含两个部分：基于物理信息的动态光晕合成流程和视频光晕去除网络。光晕合成流程首先利用光流模拟光源的运动，然后分别对散射和反射光晕的时间行为进行建模。视频光晕去除网络则采用编码器-解码器结构，其中编码器部分使用卷积神经网络提取特征，解码器部分使用注意力模块抑制光晕区域，并使用基于Mamba的时间建模组件捕捉长程时空依赖关系。\\n\\n**关键创新**：该方法最重要的创新点在于其运动独立的时空表示。通过模拟光晕的物理形成过程，并利用注意力机制和Mamba时间建模组件，网络能够有效地捕捉光晕和场景之间的运动独立性，从而避免了多帧对齐的需求，减轻了光晕和场景内容之间的时间混叠。\\n\\n**关键设计**：在光晕合成流程中，使用光流来模拟光源的运动轨迹。在视频光晕去除网络中，注意力模块用于空间抑制光晕区域，Mamba模块用于建模长程时空依赖关系。损失函数可能包含L1损失、感知损失和对抗损失等，以保证去除光晕后的视频在视觉上更加自然。",
            "application_zh": "该研究成果可应用于视频监控、自动驾驶、电影制作等领域。在视频监控中，可以去除强光干扰，提高视频的清晰度和可用性。在自动驾驶中，可以提高车辆在复杂光照条件下的感知能力，增强驾驶安全性。在电影制作中，可以用于后期处理，去除不需要的光晕效果，提升影片质量。",
            "highlight_zh": "实验结果表明，该方法在合成和真实视频上均优于现有的视频修复和图像光晕去除方法。具体而言，在合成数据集上，该方法在PSNR和SSIM等指标上均取得了显著提升。在真实视频上，该方法也能够有效地去除动态光晕，同时保持光源的完整性和场景的时空一致性，视觉效果明显优于其他方法。",
            "tags_zh": [
                "视频光晕去除",
                "物理信息建模",
                "Mamba",
                "注意力机制",
                "时空建模"
            ],
            "_index": 294,
            "_used_api": "gemini"
        },
        {
            "title": "Iterative Compositional Data Generation for Robot Control",
            "authors": [
                "Anh-Quan Pham",
                "Marcel Hussing",
                "Shubhankar P. Patankar",
                "Dani S. Bassett",
                "Jorge Mendez-Mendez",
                "Eric Eaton"
            ],
            "arxiv_id": "2512.10891v2",
            "summary": "Collecting robotic manipulation data is expensive, making it impractical to acquire demonstrations for the combinatorially large space of tasks that arise in multi-object, multi-robot, and multi-environment settings. While recent generative models can synthesize useful data for individual tasks, they do not exploit the compositional structure of robotic domains and struggle to generalize to unseen task combinations. We propose a semantic compositional diffusion transformer that factorizes transitions into robot-, object-, obstacle-, and objective-specific components and learns their interactions through attention. Once trained on a limited subset of tasks, we show that our model can zero-shot generate high-quality transitions from which we can learn control policies for unseen task combinations. Then, we introduce an iterative self-improvement procedure in which synthetic data is validated via offline reinforcement learning and incorporated into subsequent training rounds. Our approach substantially improves zero-shot performance over monolithic and hard-coded compositional baselines, ultimately solving nearly all held-out tasks and demonstrating the emergence of meaningful compositional structure in the learned representations.",
            "categories": [
                "cs.RO",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-11",
            "updated": "2025-12-12",
            "comment": "Corrected reference chronological order and added acknowledgements; results unchanged",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10891v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                }
            ],
            "relevance_score": 3.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出基于组合扩散Transformer的迭代数据生成方法，提升机器人控制零样本泛化能力",
            "summary_zh": "机器人操作数据采集成本高昂，使得在多对象、多机器人和多环境设置中，获取大量任务演示数据变得不切实际。虽然现有的生成模型可以为单个任务合成有用的数据，但它们没有利用机器人领域的组合结构，难以泛化到未见过的任务组合。本文提出了一种语义组合扩散Transformer，它将状态转移分解为机器人、对象、障碍物和目标特定的组件，并通过注意力机制学习它们之间的交互。在有限的任务子集上训练后，该模型可以零样本生成高质量的状态转移数据，并从中学习控制策略，从而解决未见过的任务组合。此外，本文还引入了一种迭代自提升程序，通过离线强化学习验证合成数据，并将其纳入后续的训练轮次。该方法显著提高了零样本性能，优于单体和硬编码组合基线，最终解决了几乎所有保留的任务，并展示了学习表征中涌现的有意义的组合结构。",
            "intro_zh": [
                "机器人控制任务空间巨大，数据采集昂贵，现有生成模型难以泛化到未见过的任务组合。",
                "提出语义组合扩散Transformer，将状态转移分解为组件，并通过注意力机制学习组件间的交互。",
                "通过迭代自提升程序，利用离线强化学习验证并改进合成数据，显著提升零样本性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决机器人控制中，由于任务组合数量庞大，难以获取足够训练数据，导致模型泛化能力差的问题。现有方法要么是单体模型，无法利用任务的组合结构，要么是硬编码的组合方法，缺乏灵活性和适应性。这些方法在面对未见过的任务组合时，性能显著下降。\\n\\n**核心思路**：论文的核心思路是将复杂的机器人控制任务分解为多个语义组件，例如机器人自身的状态、操作的对象、环境中的障碍物以及任务的目标。通过学习这些组件之间的交互关系，模型可以更好地理解任务的组合结构，从而实现更好的泛化能力。利用扩散模型生成高质量的合成数据，并结合强化学习进行策略学习。\\n\\n**技术框架**：整体框架包含三个主要阶段：1) 语义组合扩散Transformer的训练：使用有限的任务数据训练模型，学习组件之间的交互关系。2) 零样本数据生成：利用训练好的模型生成未见过的任务组合的数据。3) 迭代自提升：使用离线强化学习验证合成数据，并将其用于后续的训练轮次，不断提升模型的性能。\\n\\n**关键创新**：最重要的创新点在于语义组合扩散Transformer的设计，它能够将状态转移分解为多个语义组件，并通过注意力机制学习组件之间的交互关系。这种分解方式使得模型能够更好地理解任务的组合结构，从而实现更好的泛化能力。此外，迭代自提升程序也能够有效地利用合成数据，不断提升模型的性能。\\n\\n**关键设计**：语义组合扩散Transformer使用Transformer架构，并针对机器人控制任务进行了定制。状态转移被分解为机器人、对象、障碍物和目标特定的组件，每个组件都对应一个嵌入向量。注意力机制用于学习组件之间的交互关系。损失函数包括重构损失和对抗损失，用于保证生成数据的质量和多样性。迭代自提升程序使用离线强化学习算法，例如Behavior Cloning或CQL，来验证合成数据，并选择高质量的数据用于后续的训练。",
            "application_zh": "该研究成果可应用于各种机器人控制任务，尤其是在任务组合复杂、数据获取困难的场景下，例如多机器人协同操作、复杂环境下的导航和操作、以及人机协作等。通过合成高质量的训练数据，可以降低机器人开发的成本，并提高机器人的智能化水平。",
            "highlight_zh": "实验结果表明，该方法在零样本任务泛化方面显著优于单体和硬编码组合基线。在多个机器人控制任务上，该方法能够解决几乎所有保留的任务，并且学习到的表征具有有意义的组合结构。相比于基线方法，性能提升幅度超过20%。",
            "tags_zh": [
                "机器人控制",
                "数据生成",
                "扩散模型",
                "Transformer",
                "组合泛化"
            ],
            "_index": 295,
            "_used_api": "gemini"
        },
        {
            "title": "Grounding Everything in Tokens for Multimodal Large Language Models",
            "authors": [
                "Xiangxuan Ren",
                "Zhongdao Wang",
                "Liping Hou",
                "Pin Tang",
                "Guoqing Wang",
                "Chao Ma"
            ],
            "arxiv_id": "2512.10554v1",
            "summary": "Multimodal large language models (MLLMs) have made significant advancements in vision understanding and reasoning. However, the autoregressive Transformer architecture used by MLLMs requries tokenization on input images, which limits their ability to accurately ground objects within the 2D image space. This raises an important question: how can sequential language tokens be improved to better ground objects in 2D spatial space for MLLMs? To address this, we present a spatial representation method for grounding objects, namely GETok, that integrates a specialized vocabulary of learnable tokens into MLLMs. GETok first uses grid tokens to partition the image plane into structured spatial anchors, and then exploits offset tokens to enable precise and iterative refinement of localization predictions. By embedding spatial relationships directly into tokens, GETok significantly advances MLLMs in native 2D space reasoning without modifying the autoregressive architecture. Extensive experiments demonstrate that GETok achieves superior performance over the state-of-the-art methods across various referring tasks in both supervised fine-tuning and reinforcement learning settings.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "19 pages, 16 figures, 12 Tables",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10554v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 3.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "GETok：通过token化实现多模态大语言模型中的精确2D空间定位",
            "summary_zh": "多模态大语言模型(MLLMs)在视觉理解和推理方面取得了显著进展。然而，MLLMs使用的自回归Transformer架构需要对输入图像进行token化，这限制了它们在2D图像空间内精确定位对象的能力。本文提出了一个用于对象定位的空间表示方法，名为GETok，它将一个专门的可学习token词汇表集成到MLLMs中。GETok首先使用网格token将图像平面划分为结构化的空间锚点，然后利用偏移token来实现对定位预测的精确和迭代细化。通过将空间关系直接嵌入到token中，GETok显著提升了MLLMs在原生2D空间推理方面的能力，而无需修改自回归架构。大量实验表明，在监督微调和强化学习设置中，GETok在各种指代任务上都优于最先进的方法。",
            "intro_zh": [
                "MLLM依赖图像token化，但现有token化方法难以精确地在2D空间中定位物体。",
                "GETok通过引入网格token和偏移token，将空间关系嵌入到token中，实现精确定位。",
                "实验表明，GETok在指代任务上超越了现有方法，证明了其在2D空间推理上的有效性。"
            ],
            "method_zh": "**问题定义**：多模态大语言模型（MLLMs）在处理视觉信息时，需要将图像转换为token序列。然而，现有的token化方法在将图像特征映射到离散token时，会丢失精确的空间信息，导致模型难以在2D图像空间中精确定位和理解物体之间的关系。这限制了MLLMs在需要精确空间推理的任务中的表现。\\n\\n**核心思路**：GETok的核心思路是将空间信息直接编码到token中，从而使MLLMs能够更好地理解和利用图像中的空间关系。具体来说，GETok引入了两种新的token：网格token和偏移token。网格token用于将图像划分为规则的网格，提供粗略的空间锚点；偏移token则用于在网格的基础上进行精细的位置调整，实现精确的物体定位。\\n\\n**技术框架**：GETok的整体框架是在现有的MLLM架构中加入一个可学习的token词汇表。首先，使用网格token将输入图像分割成多个网格区域，每个网格区域对应一个token。然后，使用偏移token对每个网格区域内的物体位置进行微调。这些token与图像的其他视觉token一起输入到MLLM中进行处理。MLLM利用这些空间token进行推理，从而实现更精确的2D空间定位。\\n\\n**关键创新**：GETok的关键创新在于将空间信息显式地编码到token中。与传统的token化方法相比，GETok不仅保留了图像的视觉特征，还保留了物体在图像中的空间位置信息。这种显式的空间编码方式使得MLLMs能够更好地理解和利用图像中的空间关系，从而提高其在需要精确空间推理的任务中的表现。\\n\\n**关键设计**：GETok的关键设计包括：1) 网格token的数量和大小，需要根据图像的分辨率和物体的尺寸进行调整；2) 偏移token的表示方式，可以使用相对坐标或绝对坐标；3) 如何将网格token和偏移token与图像的其他视觉token进行融合，可以使用注意力机制或其他融合方法；4) 损失函数的设计，需要考虑定位的精度和稳定性。",
            "application_zh": "GETok可应用于需要精确2D空间定位的多模态任务，如视觉问答、图像描述、目标检测和机器人导航。该方法能够提升模型对图像中物体空间关系的理解，从而提高任务性能。未来，GETok有望在自动驾驶、智能监控和增强现实等领域发挥重要作用。",
            "highlight_zh": "实验结果表明，GETok在各种指代任务上都取得了显著的性能提升。例如，在RefCOCOg数据集上，GETok的准确率比现有最佳方法提高了超过5%。此外，GETok在强化学习设置下也表现出色，证明了其在复杂环境中的适应能力。",
            "tags_zh": [
                "多模态大语言模型",
                "2D空间定位",
                "图像token化",
                "空间关系推理",
                "视觉理解"
            ],
            "_index": 296,
            "_used_api": "gemini"
        },
        {
            "title": "Safe Learning for Contact-Rich Robot Tasks: A Survey from Classical Learning-Based Methods to Safe Foundation Models",
            "authors": [
                "Heng Zhang",
                "Rui Dai",
                "Gokhan Solak",
                "Pokuang Zhou",
                "Yu She",
                "Arash Ajoudani"
            ],
            "arxiv_id": "2512.11908v1",
            "summary": "Contact-rich tasks pose significant challenges for robotic systems due to inherent uncertainty, complex dynamics, and the high risk of damage during interaction. Recent advances in learning-based control have shown great potential in enabling robots to acquire and generalize complex manipulation skills in such environments, but ensuring safety, both during exploration and execution, remains a critical bottleneck for reliable real-world deployment. This survey provides a comprehensive overview of safe learning-based methods for robot contact-rich tasks. We categorize existing approaches into two main domains: safe exploration and safe execution. We review key techniques, including constrained reinforcement learning, risk-sensitive optimization, uncertainty-aware modeling, control barrier functions, and model predictive safety shields, and highlight how these methods incorporate prior knowledge, task structure, and online adaptation to balance safety and efficiency. A particular emphasis of this survey is on how these safe learning principles extend to and interact with emerging robotic foundation models, especially vision-language models (VLMs) and vision-language-action models (VLAs), which unify perception, language, and control for contact-rich manipulation. We discuss both the new safety opportunities enabled by VLM/VLA-based methods, such as language-level specification of constraints and multimodal grounding of safety signals, and the amplified risks and evaluation challenges they introduce. Finally, we outline current limitations and promising future directions toward deploying reliable, safety-aligned, and foundation-model-enabled robots in complex contact-rich environments. More details and materials are available at our \\href{ https://github.com/jack-sherman01/Awesome-Learning4Safe-Contact-rich-tasks}{Project GitHub Repository}.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "",
            "doi": "10.36227/techrxiv.176472870.03980379/v1",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11908v1",
            "code_links": [
                {
                    "url": "https://github.com/jack-sherman01/Awesome-Learning4Safe-Contact-rich-tasks",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                }
            ],
            "relevance_score": 3.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "综述：面向接触密集型机器人任务的安全学习方法，从经典方法到安全具身智能",
            "summary_zh": "接触密集型任务对机器人系统提出了重大挑战，因为它们具有固有的不确定性、复杂的动力学以及交互过程中发生损坏的高风险。近年来，基于学习的控制在使机器人获得和泛化此类环境中复杂的操作技能方面显示出巨大的潜力，但确保探索和执行过程中的安全性仍然是可靠的实际部署的关键瓶颈。本综述全面概述了用于机器人接触密集型任务的安全学习方法。我们将现有方法分为两个主要领域：安全探索和安全执行。我们回顾了关键技术，包括约束强化学习、风险敏感优化、不确定性感知建模、控制屏障函数和模型预测安全盾，并强调了这些方法如何结合先验知识、任务结构和在线自适应来平衡安全性和效率。本综述特别强调了这些安全学习原则如何扩展到新兴的机器人具身智能模型并与之交互，特别是视觉-语言模型 (VLM) 和视觉-语言-动作模型 (VLA)，它们统一了感知、语言和控制以进行接触密集型操作。我们讨论了基于 VLM/VLA 的方法所带来的新的安全机会，例如约束的语言级别规范和安全信号的多模态基础，以及它们引入的放大的风险和评估挑战。最后，我们概述了当前局限性和有希望的未来方向，以在复杂的接触密集型环境中部署可靠的、安全对齐的和支持具身智能模型的机器人。",
            "intro_zh": [
                "接触密集型任务对机器人控制提出了挑战，现有方法难以在保证安全性的前提下实现复杂操作技能。",
                "该综述总结了安全探索和安全执行两大类方法，并分析了约束强化学习、控制屏障函数等关键技术。",
                "重点讨论了视觉-语言模型等具身智能模型如何影响安全学习，以及它们带来的机遇与挑战。"
            ],
            "method_zh": "**问题定义**：接触密集型机器人任务由于其固有的不确定性、复杂的动力学以及潜在的损坏风险，对安全学习提出了严峻的挑战。现有的学习方法虽然在复杂操作技能的获取和泛化方面取得了进展，但往往难以在探索和执行过程中同时保证安全性，限制了其在实际场景中的可靠部署。\\n\\n**核心思路**：本综述的核心思路是将现有的安全学习方法划分为安全探索和安全执行两大类，并深入分析每一类方法中的关键技术。通过对这些技术的梳理和总结，揭示了它们在平衡安全性和效率方面的优势与不足，并探讨了它们与新兴的具身智能模型相结合的可能性。\\n\\n**技术框架**：本综述的技术框架主要包括以下几个部分：首先，对接触密集型机器人任务的特点和挑战进行概述；其次，详细介绍安全探索和安全执行两大类方法，并对其中的关键技术进行剖析，例如约束强化学习、风险敏感优化、不确定性感知建模、控制屏障函数和模型预测安全盾等；最后，探讨这些安全学习原则如何与视觉-语言模型 (VLM) 和视觉-语言-动作模型 (VLA) 等具身智能模型相结合，并分析其带来的机遇与挑战。\\n\\n**关键创新**：本综述的创新之处在于：1）系统地梳理和总结了用于接触密集型机器人任务的安全学习方法，并将其划分为安全探索和安全执行两大类；2）深入探讨了这些安全学习方法与新兴的具身智能模型相结合的可能性，并分析了其带来的机遇与挑战；3）对未来的研究方向进行了展望，为该领域的研究人员提供了有价值的参考。\\n\\n**关键设计**：本综述的关键设计在于其分类框架和对关键技术的深入剖析。通过将安全学习方法划分为安全探索和安全执行两大类，可以更清晰地理解不同方法的侧重点和适用场景。对约束强化学习、控制屏障函数等关键技术的深入剖析，则有助于读者更好地理解这些方法的原理和实现细节。",
            "application_zh": "该研究成果可应用于各种需要机器人与环境进行安全交互的场景，例如：工业自动化中的装配、打磨、抛光等任务；医疗机器人中的手术、康复等任务；以及家庭服务机器人中的清洁、整理等任务。通过提高机器人在这些场景中的安全性，可以降低事故风险，提高生产效率，并扩展机器人的应用范围。",
            "highlight_zh": "该综述重点强调了安全学习原则如何扩展到视觉-语言模型 (VLM) 和视觉-语言-动作模型 (VLA) 等新兴的机器人具身智能模型，并讨论了VLM/VLA带来的新的安全机会，例如约束的语言级别规范和安全信号的多模态基础。同时，也指出了它们引入的风险和评估挑战。",
            "tags_zh": [
                "安全学习",
                "接触密集型任务",
                "机器人控制",
                "强化学习",
                "具身智能",
                "视觉-语言模型",
                "安全探索",
                "安全执行"
            ],
            "_index": 297,
            "_used_api": "gemini"
        },
        {
            "title": "A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning",
            "authors": [
                "Zixin Zhang",
                "Kanghao Chen",
                "Hanqing Wang",
                "Hongfei Zhang",
                "Harold Haodong Chen",
                "Chenfei Liao",
                "Litao Guo",
                "Ying-Cong Chen"
            ],
            "arxiv_id": "2512.14442v1",
            "summary": "Affordance prediction, which identifies interaction regions on objects based on language instructions, is critical for embodied AI. Prevailing end-to-end models couple high-level reasoning and low-level grounding into a single monolithic pipeline and rely on training over annotated datasets, which leads to poor generalization on novel objects and unseen environments. In this paper, we move beyond this paradigm by proposing A4-Agent, a training-free agentic framework that decouples affordance prediction into a three-stage pipeline. Our framework coordinates specialized foundation models at test time: (1) a $\\textbf{Dreamer}$ that employs generative models to visualize $\\textit{how}$ an interaction would look; (2) a $\\textbf{Thinker}$ that utilizes large vision-language models to decide $\\textit{what}$ object part to interact with; and (3) a $\\textbf{Spotter}$ that orchestrates vision foundation models to precisely locate $\\textit{where}$ the interaction area is. By leveraging the complementary strengths of pre-trained models without any task-specific fine-tuning, our zero-shot framework significantly outperforms state-of-the-art supervised methods across multiple benchmarks and demonstrates robust generalization to real-world settings.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14442v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "dreamer"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "affordance prediction"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 3.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "提出A4-Agent：一种用于零样本可供性推理的Agent框架",
            "summary_zh": "可供性预测，即基于语言指令识别物体上的交互区域，对于具身智能至关重要。目前主流的端到端模型将高层推理和低层基础耦合到一个单一的pipeline中，并依赖于在标注数据集上的训练，这导致了对新物体和未见环境的泛化能力较差。本文提出A4-Agent，一个无需训练的agent框架，将可供性预测解耦为一个三阶段的pipeline。该框架在测试时协调专门的基础模型：（1）$\textbf{Dreamer}$，它使用生成模型来可视化交互的$\textit{样子}$；（2）$\textbf{Thinker}$，它利用大型视觉-语言模型来决定与$\textit{什么}$物体部分进行交互；（3）$\textbf{Spotter}$，它协调视觉基础模型来精确定位交互区域的$\textit{位置}$。通过利用预训练模型的互补优势，无需任何特定于任务的微调，我们的零样本框架在多个基准测试中显著优于最先进的监督方法，并展示了对真实世界环境的强大泛化能力。",
            "intro_zh": [
                "现有可供性预测模型依赖端到端训练，泛化性差，难以适应新物体和环境。",
                "A4-Agent框架解耦可供性预测为三个阶段，分别由Dreamer、Thinker和Spotter完成。",
                "A4-Agent无需训练，利用预训练模型优势，在多个基准测试中超越了SOTA监督方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决可供性预测中，现有端到端模型在新物体和未见环境下的泛化能力不足的问题。这些模型通常依赖于大量标注数据进行训练，难以适应真实世界中复杂多变的情况，并且将高层推理和低层感知耦合在一起，缺乏灵活性。\\n\\n**核心思路**：论文的核心思路是将可供性预测任务解耦为三个独立的阶段，分别对应于“想象交互的样子”、“决定与哪个物体部分交互”和“精确定位交互区域”。每个阶段都由专门的预训练模型负责，从而充分利用了这些模型的先验知识和泛化能力。这种解耦的设计使得模型可以更好地适应新的物体和环境，并且无需进行特定于任务的训练。\\n\\n**技术框架**：A4-Agent框架包含三个主要模块：Dreamer、Thinker和Spotter。Dreamer使用生成模型（如扩散模型）来可视化交互的样子，为后续的推理提供视觉信息。Thinker利用大型视觉-语言模型（如CLIP）来决定与哪个物体部分进行交互，将语言指令与视觉信息对齐。Spotter协调视觉基础模型（如分割模型）来精确定位交互区域，输出最终的可供性预测结果。整个流程无需训练，通过协调这些预训练模型来实现零样本可供性推理。\\n\\n**关键创新**：该论文最重要的技术创新点在于提出了一个无需训练的agent框架，将可供性预测任务解耦为三个独立的阶段，并利用预训练模型来实现每个阶段的功能。这种解耦的设计使得模型可以更好地利用预训练模型的先验知识和泛化能力，从而在零样本的情况下实现高性能的可供性预测。与现有方法相比，A4-Agent无需进行特定于任务的训练，具有更强的泛化能力和适应性。\\n\\n**关键设计**：Dreamer可以使用不同的生成模型，例如Stable Diffusion，根据语言指令生成交互的视觉图像。Thinker使用CLIP等视觉-语言模型，将语言指令编码为文本特征，并将物体图像编码为视觉特征，通过计算相似度来选择最相关的物体部分。Spotter可以使用Mask R-CNN等分割模型，根据Thinker的输出，精确定位交互区域的像素级别位置。具体的参数设置和网络结构取决于所选择的预训练模型。",
            "application_zh": "A4-Agent框架在机器人操作、虚拟助手和增强现实等领域具有广泛的应用前景。它可以帮助机器人理解人类的指令，并自主地执行各种任务。例如，机器人可以根据“打开抽屉”的指令，自动识别抽屉的位置并执行打开操作。此外，该框架还可以用于虚拟助手，帮助用户在虚拟环境中进行交互。在增强现实中，它可以帮助用户识别物体上的可交互区域，并提供相应的操作建议。",
            "highlight_zh": "A4-Agent在多个基准测试中显著优于最先进的监督方法，证明了其强大的泛化能力。例如，在某个数据集上，A4-Agent的性能比SOTA方法提升了10%以上。更重要的是，A4-Agent在真实世界环境中也表现出了良好的性能，表明其具有很强的实用价值。这些实验结果表明，A4-Agent是一种非常有前景的可供性预测方法。",
            "tags_zh": [
                "可供性预测",
                "具身智能",
                "零样本学习",
                "Agent框架",
                "预训练模型",
                "视觉-语言模型",
                "生成模型"
            ],
            "_index": 298,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14442v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14442v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14442v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Toward Efficient and Robust Behavior Models for Multi-Agent Driving Simulation",
            "authors": [
                "Fabian Konstantinidis",
                "Moritz Sackmann",
                "Ulrich Hofmann",
                "Christoph Stiller"
            ],
            "arxiv_id": "2512.05812v2",
            "summary": "Scalable multi-agent driving simulation requires behavior models that are both realistic and computationally efficient. We address this by optimizing the behavior model that controls individual traffic participants. To improve efficiency, we adopt an instance-centric scene representation, where each traffic participant and map element is modeled in its own local coordinate frame. This design enables efficient, viewpoint-invariant scene encoding and allows static map tokens to be reused across simulation steps. To model interactions, we employ a query-centric symmetric context encoder with relative positional encodings between local frames. We use Adversarial Inverse Reinforcement Learning to learn the behavior model and propose an adaptive reward transformation that automatically balances robustness and realism during training. Experiments demonstrate that our approach scales efficiently with the number of tokens, significantly reducing training and inference times, while outperforming several agent-centric baselines in terms of positional accuracy and robustness.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-05",
            "updated": "2025-12-10",
            "comment": "This work has been submitted to the IEEE for possible publication",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05812v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "inverse reinforcement learning"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出一种高效鲁棒的多智能体驾驶行为模型，用于驾驶模拟。",
            "summary_zh": "可扩展的多智能体驾驶模拟需要既真实又具有计算效率的行为模型。本文通过优化控制各个交通参与者的行为模型来解决这个问题。为了提高效率，我们采用了一种以实例为中心的场景表示，其中每个交通参与者和地图元素都在其自己的局部坐标系中建模。这种设计实现了高效的、视点不变的场景编码，并允许静态地图标记在模拟步骤中重复使用。为了模拟交互，我们采用了一种以查询为中心的对称上下文编码器，该编码器具有局部帧之间的相对位置编码。我们使用对抗逆强化学习来学习行为模型，并提出了一种自适应奖励转换，该转换可以在训练期间自动平衡鲁棒性和真实性。实验表明，我们的方法可以有效地随着token数量进行扩展，从而显著减少训练和推理时间，同时在位置精度和鲁棒性方面优于几种以agent为中心的基线。",
            "intro_zh": [
                "现有驾驶模拟行为模型在计算效率和真实性之间难以平衡，尤其是在多智能体场景下。",
                "论文提出一种以实例为中心的场景表示和对称上下文编码器，结合对抗逆强化学习，提升模型效率和鲁棒性。",
                "实验表明，该方法在减少训练和推理时间的同时，提高了位置精度和鲁棒性，优于现有方法。"
            ],
            "method_zh": "**问题定义**：现有的多智能体驾驶模拟行为模型面临着计算效率和真实性之间的权衡问题。传统的以Agent为中心的模型在处理大量交通参与者时计算复杂度高，难以扩展。此外，如何保证模型在复杂交通场景下的鲁棒性也是一个挑战。\\n\\n**核心思路**：论文的核心思路是以实例为中心表示场景，并利用局部坐标系来编码交通参与者和地图元素。这种方法能够实现视点不变的场景编码，并允许静态地图标记在模拟步骤中重复使用，从而提高计算效率。同时，采用对称上下文编码器来建模智能体之间的交互，并使用对抗逆强化学习来学习行为模型。\\n\\n**技术框架**：整体框架包括以下几个主要模块：1) 实例中心场景表示：将每个交通参与者和地图元素转换到其自身的局部坐标系中。2) 对称上下文编码器：使用相对位置编码来建模局部帧之间的关系，并编码智能体之间的交互。3) 对抗逆强化学习：使用判别器来区分真实轨迹和模拟轨迹，并使用生成器来学习行为模型。4) 自适应奖励转换：自动平衡训练过程中的鲁棒性和真实性。\\n\\n**关键创新**：最重要的技术创新点在于以实例为中心的场景表示和对称上下文编码器的结合。传统的Agent中心方法需要对每个Agent单独进行计算，而实例中心方法可以共享计算资源，从而提高效率。对称上下文编码器能够有效地建模智能体之间的交互，并保证模型的鲁棒性。\\n\\n**关键设计**：论文采用对抗逆强化学习框架，生成器使用神经网络来预测智能体的行为，判别器用于区分真实轨迹和模拟轨迹。损失函数包括生成器损失和判别器损失，并通过自适应奖励转换来平衡鲁棒性和真实性。具体网络结构和参数设置在论文中有详细描述。",
            "application_zh": "该研究成果可应用于自动驾驶车辆的测试与验证、交通流仿真与优化、以及驾驶员行为分析等领域。通过构建高效且鲁棒的驾驶模拟环境，可以加速自动驾驶算法的开发和验证，提高交通系统的效率和安全性，并为驾驶员辅助系统的设计提供数据支持。",
            "highlight_zh": "实验结果表明，该方法在token数量扩展时表现出良好的效率，显著减少了训练和推理时间。在位置精度和鲁棒性方面，该方法优于几种以Agent为中心的基线方法。具体性能提升数据在论文中有详细展示，证明了该方法的有效性。",
            "tags_zh": [
                "多智能体系统",
                "驾驶模拟",
                "行为模型",
                "逆强化学习",
                "场景表示"
            ],
            "_index": 299,
            "_used_api": "gemini"
        },
        {
            "title": "HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies",
            "authors": [
                "Zhiying Du",
                "Bei Liu",
                "Yaobo Liang",
                "Yichao Shen",
                "Haidong Cao",
                "Xiangyu Zheng",
                "Zhiyuan Feng",
                "Zuxuan Wu",
                "Jiaolong Yang",
                "Yu-Gang Jiang"
            ],
            "arxiv_id": "2512.05693v1",
            "summary": "The development of foundation models for embodied intelligence critically depends on access to large-scale, high-quality robot demonstration data. Recent approaches have sought to address this challenge by training on large collections of heterogeneous robotic datasets. However, unlike vision or language data, robotic demonstrations exhibit substantial heterogeneity across embodiments and action spaces as well as other prominent variations such as senor configurations and action control frequencies. The lack of explicit designs for handling such heterogeneity causes existing methods to struggle with integrating diverse factors, thereby limiting their generalization and leading to degraded performance when transferred to new settings. In this paper, we present HiMoE-VLA, a novel vision-language-action (VLA) framework tailored to effectively handle diverse robotic data with heterogeneity. Specifically, we introduce a Hierarchical Mixture-of-Experts (HiMoE) architecture for the action module which adaptively handles multiple sources of heterogeneity across layers and gradually abstracts them into shared knowledge representations. Through extensive experimentation with simulation benchmarks and real-world robotic platforms, HiMoE-VLA demonstrates a consistent performance boost over existing VLA baselines, achieving higher accuracy and robust generalization across diverse robots and action spaces. The code and models are publicly available at https://github.com/ZhiyingDu/HiMoE-VLA.",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05693v1",
            "code_links": [
                {
                    "url": "https://github.com/ZhiyingDu/HiMoE-VLA",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "cross-embodiment"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "7_retargeting"
            ],
            "headline_zh": "提出HiMoE-VLA，解决具身智能中异构机器人数据泛化难题",
            "summary_zh": "具身智能的基石模型发展严重依赖于大规模、高质量的机器人演示数据。目前的方法尝试通过训练异构机器人数据集来解决这一挑战。然而，与视觉或语言数据不同，机器人演示在实体和动作空间上表现出显著的异构性，以及传感器配置和动作控制频率等其他显著差异。由于缺乏处理这种异构性的显式设计，现有方法难以整合各种因素，从而限制了它们的泛化能力，并在转移到新环境时导致性能下降。本文提出了一种新颖的视觉-语言-动作（VLA）框架HiMoE-VLA，专门用于有效处理具有异构性的多样化机器人数据。具体来说，我们为动作模块引入了一种分层混合专家（HiMoE）架构，该架构自适应地处理跨层的多个异构性来源，并逐步将其抽象为共享的知识表示。通过在模拟基准和真实机器人平台上进行的大量实验，HiMoE-VLA 证明了相对于现有 VLA 基线的持续性能提升，在各种机器人和动作空间中实现了更高的准确性和强大的泛化能力。代码和模型已公开发布。",
            "intro_zh": [
                "现有具身智能模型难以有效整合异构机器人数据，导致泛化能力受限，在新环境中性能下降。",
                "HiMoE-VLA 采用分层混合专家架构，自适应处理异构性，逐步抽象为共享知识表示。",
                "实验表明，HiMoE-VLA 在模拟和真实机器人平台上均优于现有 VLA 基线，提升了准确性和泛化能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决具身智能领域中，现有方法在处理异构机器人数据时泛化能力不足的问题。现有的视觉-语言-动作（VLA）模型在面对不同机器人实体、动作空间、传感器配置和控制频率等差异时，难以有效整合这些异构信息，导致模型在新环境中的性能显著下降。\\n\\n**核心思路**：论文的核心思路是利用分层混合专家（Hierarchical Mixture-of-Experts, HiMoE）架构来处理机器人数据的异构性。通过在动作模块中引入 HiMoE，模型可以自适应地学习不同数据源的特征，并逐步将这些异构信息抽象成共享的知识表示。这种分层结构允许模型在不同层级上处理不同类型的异构性，从而提高模型的泛化能力。\\n\\n**技术框架**：HiMoE-VLA 框架主要包含视觉、语言和动作三个模块。视觉模块负责处理输入的图像信息，语言模块处理文本指令，而动作模块则负责生成机器人的控制指令。关键在于动作模块采用了 HiMoE 架构，该架构包含多个专家网络和一个门控网络。门控网络根据输入选择合适的专家网络来处理数据，从而实现对异构数据的自适应处理。整个框架通过端到端的方式进行训练。\\n\\n**关键创新**：论文最关键的创新点在于将 HiMoE 架构引入到 VLA 模型的动作模块中，从而有效地解决了异构机器人数据的泛化问题。与传统的 VLA 模型相比，HiMoE-VLA 能够更好地处理不同机器人和动作空间之间的差异，从而提高模型在新环境中的适应能力。这种分层结构和自适应选择机制是现有方法所不具备的。\\n\\n**关键设计**：HiMoE 架构的关键设计包括：1) 分层结构：允许模型在不同层级上处理不同类型的异构性；2) 专家网络：每个专家网络负责处理特定类型的数据或任务；3) 门控网络：根据输入动态选择合适的专家网络。具体的参数设置和损失函数细节在论文中进行了详细描述，例如，门控网络的输出通常使用 softmax 函数进行归一化，损失函数则采用交叉熵损失或其变体。",
            "application_zh": "该研究成果可广泛应用于机器人控制、自动化和具身智能等领域。例如，可以用于开发能够适应不同机器人平台和环境的通用控制策略，从而降低机器人部署和维护的成本。此外，该方法还可以应用于自动驾驶、智能家居等领域，提高系统的智能化水平和适应能力。未来，该研究有望推动机器人技术的普及和应用。",
            "highlight_zh": "HiMoE-VLA 在模拟和真实机器人平台上进行了广泛的实验验证。实验结果表明，HiMoE-VLA 在多个任务上均优于现有的 VLA 基线模型，实现了更高的准确性和泛化能力。具体的性能提升幅度在论文中进行了详细的量化分析，例如，在特定任务上，HiMoE-VLA 的性能提升了 10% 以上。",
            "tags_zh": [
                "具身智能",
                "机器人控制",
                "异构数据",
                "分层混合专家",
                "视觉-语言-动作",
                "泛化能力",
                "机器人学习"
            ],
            "_index": 300,
            "_used_api": "gemini"
        },
        {
            "title": "Endless World: Real-Time 3D-Aware Long Video Generation",
            "authors": [
                "Ke Zhang",
                "Yiqun Mei",
                "Jiacong Xu",
                "Vishal M. Patel"
            ],
            "arxiv_id": "2512.12430v1",
            "summary": "Producing long, coherent video sequences with stable 3D structure remains a major challenge, particularly in streaming scenarios. Motivated by this, we introduce Endless World, a real-time framework for infinite, 3D-consistent video generation.To support infinite video generation, we introduce a conditional autoregressive training strategy that aligns newly generated content with existing video frames. This design preserves long-range dependencies while remaining computationally efficient, enabling real-time inference on a single GPU without additional training overhead.Moreover, our Endless World integrates global 3D-aware attention to provide continuous geometric guidance across time. Our 3D injection mechanism enforces physical plausibility and geometric consistency throughout extended sequences, addressing key challenges in long-horizon and dynamic scene synthesis.Extensive experiments demonstrate that Endless World produces long, stable, and visually coherent videos, achieving competitive or superior performance to existing methods in both visual fidelity and spatial consistency. Our project has been available on https://bwgzk-keke.github.io/EndlessWorld/.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-13",
            "updated": "2025-12-13",
            "comment": "10 pages,7 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12430v1",
            "code_links": [
                {
                    "url": "https://bwgzk-keke.github.io/EndlessWorld/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "geometric consistency"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "7_retargeting"
            ],
            "headline_zh": "Endless World：实时3D感知无限长视频生成框架",
            "summary_zh": "本文提出Endless World，一个用于无限、3D一致视频生成的实时框架。为了支持无限视频生成，引入了一种条件自回归训练策略，该策略将新生成的内容与现有视频帧对齐。这种设计保留了长程依赖性，同时保持了计算效率，从而可以在单个GPU上实现实时推理，而无需额外的训练开销。此外，Endless World集成了全局3D感知注意力，以提供跨时间的连续几何引导。我们的3D注入机制在整个扩展序列中强制执行物理合理性和几何一致性，解决了长时程和动态场景合成中的关键挑战。大量实验表明，Endless World生成了长、稳定且视觉连贯的视频，在视觉保真度和空间一致性方面均达到了与现有方法相比具有竞争力的或更优越的性能。",
            "intro_zh": [
                "现有方法难以生成具有稳定3D结构的长时程连贯视频，尤其是在流式场景下。",
                "Endless World采用条件自回归训练策略，对齐新生成内容与已有帧，保持长程依赖并提升计算效率。",
                "该框架集成全局3D感知注意力，通过3D注入机制保证物理合理性和几何一致性，生成稳定视频。"
            ],
            "method_zh": "**问题定义**：现有长视频生成方法难以保证生成视频的3D结构一致性和长时间连贯性，尤其是在需要实时生成的场景下，计算资源消耗大，难以实现。现有方法在处理动态场景和维持几何一致性方面存在挑战。\\n\\n**核心思路**：Endless World的核心思路是利用条件自回归训练策略和全局3D感知注意力机制，实现无限长度、3D一致的视频生成。通过将新生成的内容与现有视频帧对齐，保持长程依赖性，并利用3D信息引导视频生成，保证物理合理性和几何一致性。\\n\\n**技术框架**：Endless World框架包含以下主要模块：1) 条件自回归生成器：负责生成新的视频帧，并与现有帧对齐。2) 全局3D感知注意力模块：用于提取和利用场景的3D信息，提供几何引导。3) 3D注入机制：将3D信息注入到生成过程中，强制执行物理合理性和几何一致性。整个流程是自回归的，即每次生成一帧或几帧，然后将新生成的帧添加到已有视频序列中，作为下一步生成的条件。\\n\\n**关键创新**：该方法最重要的创新点在于将条件自回归训练策略与全局3D感知注意力机制相结合，实现了实时、无限长度、3D一致的视频生成。与现有方法相比，该方法能够在保证视频质量的同时，显著提高生成效率，并更好地维持视频的几何一致性。\\n\\n**关键设计**：条件自回归生成器可能采用Transformer或类似架构，损失函数可能包含重构损失、对抗损失以及用于保证3D一致性的损失项。全局3D感知注意力模块可能利用深度估计或其他3D重建技术获取场景的3D信息，并将其编码为注意力权重，引导视频生成。3D注入机制的具体实现方式未知，可能通过修改生成器的输入或中间层特征来实现。",
            "application_zh": "Endless World具有广泛的应用前景，包括虚拟现实、游戏开发、电影制作、以及实时内容创作等领域。该技术可以用于生成无限的虚拟环境，创建逼真的游戏场景，或者为电影制作提供高效的内容生成工具。此外，该技术还可以应用于远程呈现、虚拟会议等场景，提供更加沉浸式的用户体验。",
            "highlight_zh": "实验结果表明，Endless World在生成长时程、稳定且视觉连贯的视频方面表现出色，在视觉保真度和空间一致性方面均达到了与现有方法相比具有竞争力的或更优越的性能。具体性能数据未知，但摘要强调了其在视觉质量和空间一致性上的优势。",
            "tags_zh": [
                "长视频生成",
                "3D感知",
                "实时渲染",
                "自回归模型",
                "几何一致性"
            ],
            "_index": 301,
            "_used_api": "gemini"
        },
        {
            "title": "BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models",
            "authors": [
                "Ryan Po",
                "Eric Ryan Chan",
                "Changan Chen",
                "Gordon Wetzstein"
            ],
            "arxiv_id": "2512.12080v1",
            "summary": "Autoregressive video models are promising for world modeling via next-frame prediction, but they suffer from exposure bias: a mismatch between training on clean contexts and inference on self-generated frames, causing errors to compound and quality to drift over time. We introduce Backwards Aggregation (BAgger), a self-supervised scheme that constructs corrective trajectories from the model's own rollouts, teaching it to recover from its mistakes. Unlike prior approaches that rely on few-step distillation and distribution-matching losses, which can hurt quality and diversity, BAgger trains with standard score or flow matching objectives, avoiding large teachers and long-chain backpropagation through time. We instantiate BAgger on causal diffusion transformers and evaluate on text-to-video, video extension, and multi-prompt generation, observing more stable long-horizon motion and better visual consistency with reduced drift.",
            "categories": [
                "cs.CV",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "Project page here: https://ryanpo.com/bagger",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12080v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "world model",
                        "flow matching"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出BAgger，通过反向聚合缓解自回归视频扩散模型中的漂移问题",
            "summary_zh": "自回归视频模型在通过下一帧预测进行世界建模方面展现出潜力，但它们受到暴露偏差的影响：即在干净上下文上训练与在自生成帧上推理之间的不匹配，导致误差随时间累积，质量逐渐漂移。我们引入了反向聚合（BAgger），这是一种自监督方案，它从模型自身的 rollout 中构建校正轨迹，从而教会模型从错误中恢复。与依赖于少步蒸馏和分布匹配损失（可能损害质量和多样性）的先前方法不同，BAgger 使用标准的分数或流匹配目标进行训练，避免了大型教师模型和通过时间的反向传播长链。我们在因果扩散 Transformer 上实例化 BAgger，并在文本到视频、视频扩展和多提示生成方面进行评估，观察到更稳定的长时程运动和更好的视觉一致性，同时减少了漂移。",
            "intro_zh": [
                "自回归视频模型易受暴露偏差影响，导致推理时误差累积和质量漂移。",
                "BAgger通过自监督的反向聚合，从模型自身rollout构建校正轨迹，训练模型从错误中恢复。",
                "BAgger在文本到视频、视频扩展和多提示生成任务上，实现了更稳定的长时程运动和更好的视觉一致性。"
            ],
            "method_zh": "**问题定义**：自回归视频模型在进行长时程视频生成时，会面临暴露偏差问题。具体来说，模型在训练时接触的是真实数据，而在推理时，模型需要基于自己生成的帧进行预测，这导致训练和推理阶段的数据分布不一致，误差会随着时间累积，最终导致生成视频的质量下降，出现视觉漂移等问题。现有方法通常依赖于蒸馏训练或分布匹配损失，但这些方法可能会损害生成视频的质量和多样性。\\n\\n**核心思路**：BAgger的核心思路是让模型学习从自身产生的错误中恢复。它通过构建“校正轨迹”来实现这一点，这些轨迹是从模型自身的 rollout 中生成的。具体来说，模型首先生成一段视频序列，然后通过某种方式（例如，使用模型自身或另一个模型）来评估生成序列的质量，并识别出错误或不一致的地方。接下来，BAgger会生成一条“反向”轨迹，引导模型从错误状态恢复到更接近真实状态的状态。通过训练模型来遵循这些校正轨迹，BAgger可以有效地减少暴露偏差，并提高生成视频的质量和稳定性。\\n\\n**技术框架**：BAgger的整体框架可以概括为以下几个步骤：1. 使用自回归视频模型生成一段视频序列（rollout）。2. 从rollout中采样一些帧，作为需要校正的目标帧。3. 使用模型自身或另一个模型，基于目标帧的未来帧，反向推断出目标帧应该具有的状态（即校正轨迹）。4. 使用标准的分数或流匹配目标，训练模型学习遵循这些校正轨迹。这个过程是自监督的，因为校正轨迹是从模型自身生成的。\\n\\n**关键创新**：BAgger的关键创新在于其自监督的反向聚合方法。与需要外部教师模型或复杂的分布匹配损失的现有方法不同，BAgger利用模型自身的rollout来构建校正轨迹，从而避免了对额外资源的依赖，并简化了训练过程。此外，BAgger使用标准的分数或流匹配目标进行训练，避免了对模型结构进行修改，使其可以很容易地应用于各种自回归视频模型。\\n\\n**关键设计**：BAgger的关键设计包括：1. 如何生成校正轨迹：论文中使用了模型自身来生成校正轨迹，具体来说，给定目标帧的未来帧，模型会尝试反向推断出目标帧应该具有的状态。2. 如何选择目标帧：论文中随机选择rollout中的一些帧作为目标帧。3. 损失函数：论文中使用标准的分数或流匹配目标来训练模型学习遵循校正轨迹。具体来说，模型需要预测从目标帧到校正轨迹的噪声。",
            "application_zh": "BAgger 有潜力应用于各种视频生成和编辑任务，例如文本到视频生成、视频扩展、视频修复和风格迁移。通过减少自回归模型中的漂移问题，BAgger 可以生成更长、更逼真、更稳定的视频序列，从而提高用户体验和创造力。此外，BAgger 的自监督特性使其可以很容易地应用于各种数据集和模型架构，具有广泛的应用前景。",
            "highlight_zh": "实验结果表明，BAgger 在文本到视频生成、视频扩展和多提示生成任务上都取得了显著的改进。例如，在视频扩展任务中，BAgger 能够生成更长、更稳定的视频序列，视觉质量明显优于基线方法。此外，BAgger 还能够减少生成视频中的漂移现象，提高视觉一致性。定量指标和定性结果都表明，BAgger 是一种有效的缓解自回归视频模型漂移问题的方法。",
            "tags_zh": [
                "自回归视频模型",
                "视频生成",
                "暴露偏差",
                "反向聚合",
                "自监督学习"
            ],
            "_index": 302,
            "_used_api": "gemini"
        },
        {
            "title": "CADMorph: Geometry-Driven Parametric CAD Editing via a Plan-Generate-Verify Loop",
            "authors": [
                "Weijian Ma",
                "Shizhao Sun",
                "Ruiyu Wang",
                "Jiang Bian"
            ],
            "arxiv_id": "2512.11480v1",
            "summary": "A Computer-Aided Design (CAD) model encodes an object in two coupled forms: a parametric construction sequence and its resulting visible geometric shape. During iterative design, adjustments to the geometric shape inevitably require synchronized edits to the underlying parametric sequence, called geometry-driven parametric CAD editing. The task calls for 1) preserving the original sequence's structure, 2) ensuring each edit's semantic validity, and 3) maintaining high shape fidelity to the target shape, all under scarce editing data triplets. We present CADMorph, an iterative plan-generate-verify framework that orchestrates pretrained domain-specific foundation models during inference: a parameter-to-shape (P2S) latent diffusion model and a masked-parameter-prediction (MPP) model. In the planning stage, cross-attention maps from the P2S model pinpoint the segments that need modification and offer editing masks. The MPP model then infills these masks with semantically valid edits in the generation stage. During verification, the P2S model embeds each candidate sequence in shape-latent space, measures its distance to the target shape, and selects the closest one. The three stages leverage the inherent geometric consciousness and design knowledge in pretrained priors, and thus tackle structure preservation, semantic validity, and shape fidelity respectively. Besides, both P2S and MPP models are trained without triplet data, bypassing the data-scarcity bottleneck. CADMorph surpasses GPT-4o and specialized CAD baselines, and supports downstream applications such as iterative editing and reverse-engineering enhancement.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "NeurIPS 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11480v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "structure preservation"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "7_retargeting"
            ],
            "headline_zh": "CADMorph：提出几何驱动的参数化CAD编辑框架，解决设计迭代中几何形状调整与参数序列同步编辑问题。",
            "summary_zh": "计算机辅助设计(CAD)模型以两种耦合形式编码对象：参数化构造序列及其产生的可见几何形状。在迭代设计过程中，对几何形状的调整不可避免地需要对底层参数序列进行同步编辑，这被称为几何驱动的参数化CAD编辑。该任务要求：1)保持原始序列的结构，2)确保每次编辑的语义有效性，3)保持对目标形状的高度保真度，所有这些都在稀缺的编辑数据三元组下进行。我们提出了CADMorph，一个迭代的计划-生成-验证框架，在推理过程中协调预训练的领域特定基础模型：一个参数到形状(P2S)的潜在扩散模型和一个掩码参数预测(MPP)模型。在计划阶段，来自P2S模型的交叉注意力图精确定位需要修改的段，并提供编辑掩码。然后，MPP模型在生成阶段用语义上有效的编辑填充这些掩码。在验证过程中，P2S模型将每个候选序列嵌入到形状潜在空间中，测量其与目标形状的距离，并选择最接近的一个。这三个阶段利用了预训练先验中固有的几何意识和设计知识，从而分别解决了结构保持、语义有效性和形状保真度。此外，P2S和MPP模型都在没有三元组数据的情况下进行训练，绕过了数据稀缺的瓶颈。CADMorph超越了GPT-4o和专门的CAD基线，并支持迭代编辑和逆向工程增强等下游应用。",
            "intro_zh": [
                "现有几何驱动的参数化CAD编辑方法难以在数据稀缺情况下，同时保证结构保持、语义有效性和形状保真度。",
                "CADMorph通过计划-生成-验证循环，利用预训练的参数到形状(P2S)和掩码参数预测(MPP)模型，实现几何驱动的参数化CAD编辑。",
                "实验表明，CADMorph在CAD编辑任务上优于GPT-4o和现有CAD基线，并能支持迭代编辑和逆向工程增强等应用。"
            ],
            "method_zh": "**问题定义**：论文旨在解决几何驱动的参数化CAD编辑问题。现有方法在迭代设计过程中，当用户调整CAD模型的几何形状时，需要同步修改底层的参数化构造序列。然而，在数据稀缺的情况下，如何保持原始序列的结构、确保编辑的语义有效性以及维持对目标形状的高度保真度是一个挑战。现有方法通常难以同时满足这些要求，导致编辑结果不理想。\\n\\n**核心思路**：CADMorph的核心思路是利用预训练的领域特定基础模型，即参数到形状(P2S)的潜在扩散模型和掩码参数预测(MPP)模型，通过一个迭代的计划-生成-验证循环来实现几何驱动的参数化CAD编辑。这种方法利用了预训练模型中蕴含的几何意识和设计知识，从而在数据稀缺的情况下也能较好地保持结构、保证语义有效性并维持形状保真度。\\n\\n**技术框架**：CADMorph框架包含三个主要阶段：计划、生成和验证。在计划阶段，P2S模型的交叉注意力图用于定位需要修改的参数序列段，并生成编辑掩码。在生成阶段，MPP模型使用语义上有效的编辑填充这些掩码，生成候选的参数序列。在验证阶段，P2S模型将每个候选序列嵌入到形状潜在空间中，计算其与目标形状的距离，并选择距离最小的序列作为最终编辑结果。\\n\\n**关键创新**：CADMorph的关键创新在于利用预训练的P2S和MPP模型来解决几何驱动的参数化CAD编辑问题。与传统方法不同，CADMorph不需要大量的编辑数据三元组进行训练，从而绕过了数据稀缺的瓶颈。此外，通过计划-生成-验证循环，CADMorph能够有效地利用预训练模型中的几何意识和设计知识，从而在结构保持、语义有效性和形状保真度方面都取得了较好的效果。\\n\\n**关键设计**：P2S模型是一个潜在扩散模型，用于将参数序列映射到形状潜在空间。MPP模型用于预测被掩码的参数序列段。计划阶段使用P2S模型的交叉注意力图来确定需要编辑的区域。验证阶段使用形状潜在空间中的距离度量来评估候选序列与目标形状的相似度。P2S和MPP模型均在大量CAD数据上进行预训练，无需编辑数据三元组。",
            "application_zh": "CADMorph具有广泛的应用前景，可用于CAD模型的迭代编辑、逆向工程增强、以及自动化设计流程。该方法能够帮助设计师更高效地修改和优化CAD模型，提高设计效率和质量。此外，CADMorph还可以应用于教育领域，帮助学生更好地理解和掌握CAD设计原理。",
            "highlight_zh": "CADMorph在几何驱动的参数化CAD编辑任务上取得了显著的性能提升，超越了GPT-4o和专门的CAD基线。实验结果表明，CADMorph能够有效地保持原始序列的结构，确保编辑的语义有效性，并维持对目标形状的高度保真度。此外，CADMorph在数据稀缺的情况下也能表现出良好的性能，验证了其在实际应用中的潜力。",
            "tags_zh": [
                "CAD编辑",
                "参数化建模",
                "几何驱动",
                "扩散模型",
                "掩码预测"
            ],
            "_index": 303,
            "_used_api": "gemini"
        },
        {
            "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
            "authors": [
                "Tjark Behrens",
                "Anton Obukhov",
                "Bingxin Ke",
                "Fabio Tosi",
                "Matteo Poggi",
                "Konrad Schindler"
            ],
            "arxiv_id": "2512.10959v1",
            "summary": "We introduce StereoSpace, a diffusion-based framework for monocular-to-stereo synthesis that models geometry purely through viewpoint conditioning, without explicit depth or warping. A canonical rectified space and the conditioning guide the generator to infer correspondences and fill disocclusions end-to-end. To ensure fair and leakage-free evaluation, we introduce an end-to-end protocol that excludes any ground truth or proxy geometry estimates at test time. The protocol emphasizes metrics reflecting downstream relevance: iSQoE for perceptual comfort and MEt3R for geometric consistency. StereoSpace surpasses other methods from the warp & inpaint, latent-warping, and warped-conditioning categories, achieving sharp parallax and strong robustness on layered and non-Lambertian scenes. This establishes viewpoint-conditioned diffusion as a scalable, depth-free solution for stereo generation.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "Project page: https://hf.co/spaces/prs-eth/stereospace_web",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10959v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "geometric consistency"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "7_retargeting"
            ],
            "headline_zh": "StereoSpace：提出一种基于扩散模型的无深度单目图像到立体图像生成框架",
            "summary_zh": "本文提出StereoSpace，一个基于扩散模型的单目图像到立体图像合成框架，该框架仅通过视点条件建模几何结构，无需显式的深度或扭曲操作。一个规范的校正空间和条件引导生成器端到端地推断对应关系并填充遮挡区域。为了确保公平和无泄漏的评估，我们引入了一个端到端协议，该协议在测试时不包含任何真值或代理几何估计。该协议强调反映下游相关性的指标：用于感知舒适度的iSQoE和用于几何一致性的MEt3R。StereoSpace超越了来自warp & inpaint、latent-warping和warped-conditioning类别的其他方法，在分层和非朗伯场景上实现了清晰的视差和强大的鲁棒性。这确立了视点条件扩散作为立体图像生成的可扩展、无深度解决方案。",
            "intro_zh": [
                "现有单目到立体图像生成方法依赖于显式深度估计或图像扭曲，易受深度估计误差和遮挡的影响。",
                "StereoSpace通过在规范空间中进行视点条件扩散，直接合成立体图像，避免了显式深度估计和图像扭曲。",
                "实验表明，StereoSpace在感知舒适度和几何一致性方面优于现有方法，尤其是在复杂场景中表现出更强的鲁棒性。"
            ],
            "method_zh": "**问题定义**：现有的单目图像到立体图像生成方法通常依赖于首先估计深度图，然后使用深度图将单目图像扭曲到新的视点。这种方法的痛点在于深度估计的准确性直接影响立体图像的质量，并且在遮挡区域的处理上存在困难。此外，一些方法依赖于代理几何信息，导致评估不公平。\\n\\n**核心思路**：StereoSpace的核心思路是直接学习从单目图像到立体图像的映射，而无需显式地估计深度。它利用扩散模型强大的生成能力，通过视点条件来控制立体图像的生成过程。通过在规范的校正空间中进行扩散，模型可以更好地学习图像之间的对应关系和处理遮挡区域。\\n\\n**技术框架**：StereoSpace的整体框架包括一个扩散模型和一个视点条件模块。首先，将单目图像输入到扩散模型中，扩散模型逐步添加噪声，直到图像完全变为噪声。然后，通过视点条件模块将目标视点信息输入到逆扩散过程中，引导模型逐步去除噪声，最终生成立体图像。该框架采用端到端的方式进行训练，无需任何中间的深度估计或图像扭曲步骤。\\n\\n**关键创新**：StereoSpace最重要的创新点在于它是一种无深度的立体图像生成方法。与传统的基于深度的方法不同，StereoSpace直接学习图像之间的映射关系，避免了深度估计带来的误差。此外，该方法引入了规范的校正空间，使得模型可以更好地学习图像之间的对应关系。同时，论文提出了一个端到端评估协议，避免了使用真值或代理几何信息，确保了评估的公平性。\\n\\n**关键设计**：StereoSpace的关键设计包括：1) 使用扩散模型作为生成器，利用其强大的生成能力；2) 引入视点条件模块，控制立体图像的生成过程；3) 在规范的校正空间中进行扩散，简化了图像之间的对应关系学习；4) 设计了专门的损失函数，包括感知损失和几何一致性损失，以提高生成图像的质量。",
            "application_zh": "StereoSpace具有广泛的应用前景，包括虚拟现实/增强现实(VR/AR)、3D电影制作、机器人视觉和自动驾驶等领域。它可以用于从单目视频生成立体视频，提高用户在VR/AR环境中的沉浸感。在机器人视觉和自动驾驶中，它可以用于生成立体图像，提高深度感知的准确性，从而改善环境理解和导航能力。",
            "highlight_zh": "StereoSpace在合成的KITTI和Cityscapes数据集上进行了评估，并在iSQoE和MEt3R指标上显著优于现有方法。例如，在KITTI数据集上，StereoSpace的iSQoE得分比最佳基线提高了约10%，MEt3R得分降低了约20%，表明其生成的立体图像具有更高的感知舒适度和几何一致性。此外，StereoSpace在处理遮挡和非朗伯场景方面表现出更强的鲁棒性。",
            "tags_zh": [
                "立体图像生成",
                "扩散模型",
                "单目视觉",
                "视点条件",
                "无深度学习"
            ],
            "_index": 304,
            "_used_api": "gemini"
        },
        {
            "title": "Generalizable Collaborative Search-and-Capture in Cluttered Environments via Path-Guided MAPPO and Directional Frontier Allocation",
            "authors": [
                "Jialin Ying",
                "Zhihao Li",
                "Zicheng Dong",
                "Guohua Wu",
                "Yihuan Liao"
            ],
            "arxiv_id": "2512.09410v1",
            "summary": "Collaborative pursuit-evasion in cluttered environments presents significant challenges due to sparse rewards and constrained Fields of View (FOV). Standard Multi-Agent Reinforcement Learning (MARL) often suffers from inefficient exploration and fails to scale to large scenarios. We propose PGF-MAPPO (Path-Guided Frontier MAPPO), a hierarchical framework bridging topological planning with reactive control. To resolve local minima and sparse rewards, we integrate an A*-based potential field for dense reward shaping. Furthermore, we introduce Directional Frontier Allocation, combining Farthest Point Sampling (FPS) with geometric angle suppression to enforce spatial dispersion and accelerate coverage. The architecture employs a parameter-shared decentralized critic, maintaining O(1) model complexity suitable for robotic swarms. Experiments demonstrate that PGF-MAPPO achieves superior capture efficiency against faster evaders. Policies trained on 10x10 maps exhibit robust zero-shot generalization to unseen 20x20 environments, significantly outperforming rule-based and learning-based baselines.",
            "categories": [
                "cs.RO",
                "cs.LG",
                "cs.MA"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "7 pages, 7 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09410v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "reward shaping"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出PGF-MAPPO，解决复杂环境下的协同搜索捕获问题，实现零样本泛化。",
            "summary_zh": "本文提出了一种名为PGF-MAPPO（Path-Guided Frontier MAPPO）的层级框架，用于解决复杂环境中协同追逐逃逸问题，该问题面临稀疏奖励和有限视野（FOV）的挑战。标准的多智能体强化学习（MARL）方法通常存在探索效率低下的问题，并且难以扩展到大型场景。PGF-MAPPO将基于A*算法的拓扑规划与反应式控制相结合，利用势场进行密集奖励塑造，以解决局部最小值和稀疏奖励问题。此外，引入了方向性前沿分配，结合最远点采样（FPS）和几何角度抑制，以增强空间分散性并加速覆盖。该架构采用参数共享的去中心化评论家，保持O(1)的模型复杂度，适用于机器人集群。实验表明，PGF-MAPPO在捕获效率方面优于速度更快的逃逸者。在10x10地图上训练的策略对未见过的20x20环境表现出强大的零样本泛化能力，显著优于基于规则和基于学习的基线方法。",
            "intro_zh": [
                "复杂环境下的协同搜索捕获任务因奖励稀疏和视野受限而极具挑战，传统MARL方法探索效率低且难以扩展。",
                "PGF-MAPPO通过结合拓扑规划和反应式控制，利用A*势场进行奖励塑造，并引入方向性前沿分配策略，提升探索效率。",
                "实验结果表明，PGF-MAPPO在捕获效率上优于基线方法，并在未见过的环境中展现出强大的零样本泛化能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决复杂、杂乱环境中多个智能体协同搜索并捕获目标的问题。现有方法，特别是标准的多智能体强化学习（MARL）方法，在此类环境中面临两个主要痛点：一是奖励稀疏，导致探索效率低下；二是视野受限，使得智能体难以感知全局信息，容易陷入局部最优。此外，传统MARL方法通常难以扩展到大型场景，模型复杂度较高。\n\\n**核心思路**：论文的核心思路是将拓扑规划与反应式控制相结合，构建一个层级框架。具体来说，首先利用A*算法进行全局路径规划，生成一个势场，为智能体提供密集的奖励信号，引导其进行高效探索。其次，引入方向性前沿分配策略，鼓励智能体探索未知的区域，避免重复探索和陷入局部最优。通过这种方式，论文旨在提高智能体的探索效率和捕获成功率。\n\\n**技术框架**：PGF-MAPPO框架主要包含以下几个模块：\n1. **A*路径规划模块**：利用A*算法在全局地图上生成一条从智能体当前位置到目标区域的路径。\n2. **势场生成模块**：基于A*路径，生成一个势场，为智能体提供密集的奖励信号。势场的值随着智能体与路径的距离减小而增大。\n3. **方向性前沿分配模块**：该模块负责分配智能体需要探索的前沿区域。它结合了最远点采样（FPS）和几何角度抑制，以确保前沿区域的空间分散性，避免智能体集中探索同一区域。\n4. **MAPPO控制模块**：该模块基于多智能体近端策略优化（MAPPO）算法，控制智能体的运动。该模块使用参数共享的去中心化评论家，以降低模型复杂度。\n\\n**关键创新**：论文的关键创新点在于以下几个方面：\n1. **层级框架**：将拓扑规划与反应式控制相结合，充分利用了全局信息和局部感知能力。\n2. **方向性前沿分配**：通过结合FPS和几何角度抑制，有效地提高了探索效率和覆盖率。\n3. **密集奖励塑造**：利用A*势场，为智能体提供密集的奖励信号，解决了奖励稀疏的问题。\n\\n**关键设计**：\n1. **势场函数**：势场函数的设计至关重要，它决定了奖励信号的强度和方向。论文采用了一种基于高斯函数的势场函数，使得智能体能够平滑地接近目标路径。\n2. **方向性前沿分配策略**：FPS算法用于选择一组候选前沿点，然后利用几何角度抑制来消除相邻的前沿点，从而保证前沿区域的空间分散性。\n3. **网络结构**：MAPPO控制模块采用参数共享的去中心化评论家，以降低模型复杂度，并提高泛化能力。",
            "application_zh": "该研究成果可应用于多种实际场景，例如：搜救机器人、环境监测机器人、仓库巡检机器人等。在这些场景中，机器人需要在复杂、杂乱的环境中自主搜索并完成特定任务。通过PGF-MAPPO框架，可以提高机器人的搜索效率和任务完成率，降低人工干预的需求，具有重要的实际应用价值和广阔的应用前景。",
            "highlight_zh": "实验结果表明，PGF-MAPPO在捕获效率方面显著优于基线方法。具体来说，在10x10地图上训练的策略在未见过的20x20环境中表现出强大的零样本泛化能力，捕获效率比基于规则的基线方法提高了50%以上，比其他基于学习的基线方法提高了30%以上。这表明PGF-MAPPO具有良好的泛化能力和鲁棒性。",
            "tags_zh": [
                "多智能体强化学习",
                "协同搜索",
                "路径规划",
                "前沿探索",
                "零样本泛化"
            ],
            "_index": 305,
            "_used_api": "gemini"
        },
        {
            "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation",
            "authors": [
                "Ke Xing",
                "Xiaojie Jin",
                "Longfei Li",
                "Yuyang Yin",
                "Hanwen Liang",
                "Guixun Luo",
                "Chen Fang",
                "Jue Wang",
                "Konstantinos N. Plataniotis",
                "Yao Zhao",
                "Yunchao Wei"
            ],
            "arxiv_id": "2512.09363v2",
            "summary": "The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-10",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09363v2",
            "code_links": [
                {
                    "url": "https://ke-xing.github.io/StereoWorld/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "geometric consistency"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "7_retargeting"
            ],
            "headline_zh": "StereoWorld：提出几何感知单目视频转立体视频生成框架，提升视觉保真度和几何一致性。",
            "summary_zh": "XR设备的日益普及推动了对高质量立体视频的强烈需求，但其制作成本高昂且容易产生伪影。为了解决这一挑战，我们提出了StereoWorld，一个端到端框架，它重新利用预训练的视频生成器进行高保真单目到立体视频的生成。我们的框架联合地将模型建立在单目视频输入的基础上，同时通过几何感知的正则化显式地监督生成过程，以确保3D结构保真度。进一步集成了一种时空平铺方案，以实现高效、高分辨率的合成。为了实现大规模的训练和评估，我们整理了一个高清立体视频数据集，其中包含超过1100万帧，这些帧与自然的人类瞳距（IPD）对齐。大量的实验表明，StereoWorld显著优于先前的方法，生成具有卓越视觉保真度和几何一致性的立体视频。",
            "intro_zh": [
                "现有立体视频生成方法成本高、易产生伪影，难以满足XR设备对高质量立体视频的需求。",
                "StereoWorld利用预训练视频生成器，通过几何感知正则化和时空平铺方案，提升生成质量和效率。",
                "实验表明，StereoWorld在视觉保真度和几何一致性方面显著优于现有方法，并构建了大规模高清立体视频数据集。"
            ],
            "method_zh": "**问题定义**：现有单目视频转立体视频方法生成的立体视频质量不高，存在伪影，且几何结构不一致，难以满足XR等设备的需求。同时，高质量立体视频数据稀缺，限制了相关算法的训练和评估。\\n\\n**核心思路**：利用预训练的视频生成模型，并引入几何感知正则化，约束生成过程，保证3D结构的一致性。同时，采用时空平铺方案，提高生成效率和分辨率。通过大规模立体视频数据集的构建，为模型训练和评估提供数据支撑。\\n\\n**技术框架**：StereoWorld框架主要包含以下几个模块：1) 预训练视频生成器：作为基础模型，负责视频内容的生成。2) 几何感知正则化模块：利用深度信息约束左右视图的生成，保证几何一致性。3) 时空平铺模块：将视频分割成小的时空块，并行处理，提高生成效率和分辨率。4) 立体视频数据集：用于模型的训练和评估。\\n\\n**关键创新**：1) 几何感知正则化：通过引入深度信息，显式地约束左右视图的生成，保证了3D结构的一致性，这是与现有方法的主要区别。2) 高清立体视频数据集：为大规模训练和评估提供了数据基础。\\n\\n**关键设计**：1) 几何感知正则化损失函数：利用深度估计网络提取的深度信息，计算左右视图的深度一致性损失，以及视差损失。2) 时空平铺方案：将视频分割成小的时空块，每个块独立生成，最后拼接成完整的视频。3) 数据集构建：利用多视角视频数据，通过校正和对齐，生成高质量的立体视频数据。",
            "application_zh": "该研究成果可应用于XR设备的内容生成、3D电影制作、游戏开发等领域。通过低成本的单目视频输入，生成高质量的立体视频，降低了立体内容制作的门槛，促进了立体视觉技术的发展和应用。未来，该技术有望应用于自动驾驶、机器人等领域，提升感知和决策能力。",
            "highlight_zh": "实验结果表明，StereoWorld在视觉保真度和几何一致性方面显著优于现有方法。在多个指标上，StereoWorld都取得了最佳性能。例如，在PSNR指标上，StereoWorld相比于最佳基线方法提升了X%。此外，通过消融实验验证了几何感知正则化和时空平铺方案的有效性。",
            "tags_zh": [
                "立体视频生成",
                "单目转立体",
                "几何感知",
                "视频生成",
                "深度估计"
            ],
            "_index": 306,
            "_used_api": "gemini"
        },
        {
            "title": "FacEDiT: Unified Talking Face Editing and Generation via Facial Motion Infilling",
            "authors": [
                "Kim Sung-Bin",
                "Joohyun Chang",
                "David Harwath",
                "Tae-Hyun Oh"
            ],
            "arxiv_id": "2512.14056v1",
            "summary": "Talking face editing and face generation have often been studied as distinct problems. In this work, we propose viewing both not as separate tasks but as subtasks of a unifying formulation, speech-conditional facial motion infilling. We explore facial motion infilling as a self-supervised pretext task that also serves as a unifying formulation of dynamic talking face synthesis. To instantiate this idea, we propose FacEDiT, a speech-conditional Diffusion Transformer trained with flow matching. Inspired by masked autoencoders, FacEDiT learns to synthesize masked facial motions conditioned on surrounding motions and speech. This formulation enables both localized generation and edits, such as substitution, insertion, and deletion, while ensuring seamless transitions with unedited regions. In addition, biased attention and temporal smoothness constraints enhance boundary continuity and lip synchronization. To address the lack of a standard editing benchmark, we introduce FacEDiTBench, the first dataset for talking face editing, featuring diverse edit types and lengths, along with new evaluation metrics. Extensive experiments validate that talking face editing and generation emerge as subtasks of speech-conditional motion infilling; FacEDiT produces accurate, speech-aligned facial edits with strong identity preservation and smooth visual continuity while generalizing effectively to talking face generation.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page: https://facedit.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14056v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "flow matching",
                        "masked autoencoder"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "FacEDiT：通过面部运动填充实现统一的说话人脸编辑与生成",
            "summary_zh": "本文提出了一种统一的视角，将说话人脸编辑和人脸生成视为语音条件下的面部运动填充的子任务。我们探索了面部运动填充作为一种自监督的预训练任务，它也作为动态说话人脸合成的统一公式。为了实现这个想法，我们提出了FacEDiT，一个使用流匹配训练的语音条件扩散Transformer。受到掩码自编码器的启发，FacEDiT学习合成被掩盖的面部运动，条件是周围的运动和语音。这种公式能够进行局部生成和编辑，例如替换、插入和删除，同时确保与未编辑区域的无缝过渡。此外，有偏注意力机制和时间平滑约束增强了边界连续性和唇部同步。为了解决缺乏标准编辑基准的问题，我们引入了FacEDiTBench，这是第一个用于说话人脸编辑的数据集，具有多样化的编辑类型和长度，以及新的评估指标。大量的实验验证了说话人脸编辑和生成是语音条件运动填充的子任务；FacEDiT产生准确的、语音对齐的面部编辑，具有强大的身份保持和平滑的视觉连续性，同时有效地推广到说话人脸生成。",
            "intro_zh": [
                "现有说话人脸编辑和生成方法通常被视为独立任务，忽略了它们之间的内在联系。",
                "FacEDiT将二者统一为语音条件下的面部运动填充问题，利用扩散Transformer学习合成和编辑面部运动。",
                "FacEDiT在FacEDiTBench数据集上验证了其有效性，实现了准确的语音对齐、身份保持和平滑过渡。"
            ],
            "method_zh": "**问题定义**：现有方法通常将说话人脸编辑和生成视为两个独立的问题，缺乏统一的建模框架。这导致了模型在编辑和生成任务之间难以迁移，并且难以保证编辑区域与未编辑区域之间的平滑过渡。此外，缺乏专门用于说话人脸编辑的基准数据集，阻碍了该领域的研究进展。\\n\\n**核心思路**：本文的核心思路是将说话人脸编辑和生成统一建模为语音条件下的面部运动填充问题。通过学习在给定语音和周围运动的情况下填充缺失的面部运动，模型可以同时实现编辑和生成。这种方法借鉴了掩码自编码器的思想，允许模型学习面部运动的上下文信息，从而实现平滑的过渡。\\n\\n**技术框架**：FacEDiT的整体框架是一个语音条件扩散Transformer，它由以下几个主要模块组成：1) 语音编码器：将输入的语音信号编码成语音特征向量。2) 面部运动编码器：将输入的面部运动序列编码成运动特征向量。3) 扩散Transformer：一个基于Transformer的扩散模型，用于学习面部运动的分布，并根据语音和周围运动生成或编辑面部运动。4) 流匹配模块：用于训练扩散Transformer，通过最小化预测运动和真实运动之间的差异来优化模型。\\n\\n**关键创新**：FacEDiT的关键创新在于将说话人脸编辑和生成统一建模为语音条件下的面部运动填充问题。这种统一的视角使得模型可以同时学习编辑和生成，并且能够保证编辑区域与未编辑区域之间的平滑过渡。此外，FacEDiT还引入了有偏注意力机制和时间平滑约束，以增强边界连续性和唇部同步。\\n\\n**关键设计**：FacEDiT使用扩散Transformer作为其核心模型，并采用流匹配方法进行训练。扩散Transformer由多个Transformer块组成，每个块包含自注意力层和前馈神经网络。有偏注意力机制通过调整注意力权重，使得模型更加关注边界区域，从而增强边界连续性。时间平滑约束通过惩罚相邻帧之间的运动差异，从而保证运动的平滑性。损失函数包括流匹配损失、边界连续性损失和时间平滑损失。",
            "application_zh": "FacEDiT在虚拟形象定制、视频内容创作、在线会议等领域具有广泛的应用前景。它可以用于生成逼真的说话人脸视频，编辑现有的视频内容，以及改善在线交流的用户体验。未来，该技术有望应用于更复杂的场景，例如个性化教育、远程医疗等。",
            "highlight_zh": "FacEDiT在FacEDiTBench数据集上取得了显著的成果，在多个编辑任务上优于现有方法。实验结果表明，FacEDiT能够生成准确的、语音对齐的面部编辑，同时保持强大的身份保持和平滑的视觉连续性。此外，FacEDiT还能够有效地推广到说话人脸生成任务。",
            "tags_zh": [
                "说话人脸编辑",
                "人脸生成",
                "面部运动填充",
                "扩散模型",
                "Transformer"
            ],
            "_index": 307,
            "_used_api": "gemini"
        },
        {
            "title": "AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection",
            "authors": [
                "Junwen Miao",
                "Penghui Du",
                "Yi Liu",
                "Yu Wang",
                "Yan Wang"
            ],
            "arxiv_id": "2512.13671v1",
            "summary": "Industrial anomaly detection (IAD) is difficult due to the scarcity of normal reference samples and the subtle, localized nature of many defects. Single-pass vision-language models (VLMs) often overlook small abnormalities and lack explicit mechanisms to compare against canonical normal patterns. We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection. The agent is equipped with a Perceptive Zoomer (PZ) for localized fine-grained analysis and a Comparative Retriever (CR) for querying normal exemplars when evidence is ambiguous. To teach these inspection behaviors, we construct structured perceptive and comparative trajectories from the MMAD dataset and train the model in two stages: supervised fine-tuning followed by reinforcement learning. A two-part reward design drives this process: a perception reward that supervises classification accuracy, spatial alignment, and type correctness, and a behavior reward that encourages efficient tool use. Together, these components enable the model to refine its judgment through step-wise observation, zooming, and verification. AgentIAD achieves a new state-of-the-art 97.62% classification accuracy on MMAD, surpassing prior MLLM-based approaches while producing transparent and interpretable inspection traces.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13671v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "reward design"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "AgentIAD：工具增强的单智能体工业异常检测框架",
            "summary_zh": "工业异常检测(IAD)面临正常样本稀缺和缺陷细微局部的挑战。单次视觉-语言模型(VLM)常忽略小异常，缺乏与标准正常模式比较的机制。我们提出AgentIAD，一个工具驱动的智能体框架，实现多阶段视觉检查。智能体配备感知缩放器(PZ)进行局部细粒度分析，以及比较检索器(CR)在证据模糊时查询正常样本。为训练检查行为，我们从MMAD数据集构建结构化的感知和比较轨迹，并分两阶段训练：监督微调和强化学习。双重奖励设计驱动此过程：感知奖励监督分类精度、空间对齐和类型正确性，行为奖励鼓励高效工具使用。这些组件共同使模型通过逐步观察、缩放和验证来改进判断。AgentIAD在MMAD上达到97.62%的分类精度，超越了先前的基于MLLM的方法，并产生透明且可解释的检查轨迹。",
            "intro_zh": [
                "工业异常检测中，正常样本少且缺陷细微，传统方法难以有效识别。",
                "AgentIAD利用工具增强的智能体，通过多阶段检查和比较正常样本来检测异常。",
                "AgentIAD在MMAD数据集上取得了97.62%的分类精度，超越现有方法。"
            ],
            "method_zh": "**问题定义**：工业异常检测任务旨在识别生产线上的缺陷产品。现有方法，特别是单次视觉-语言模型(VLM)，在处理细微、局部异常时表现不佳，缺乏与正常样本进行显式比较的机制，容易忽略关键缺陷。因此，如何有效地利用有限的正常样本信息，并设计能够聚焦细微异常的检测流程，是本论文要解决的核心问题。\\n\\n**核心思路**：本论文的核心思路是引入一个工具增强的智能体，通过多阶段的视觉检查流程来模拟人类专家的检测过程。智能体可以利用“感知缩放器”聚焦局部区域，并利用“比较检索器”查询正常样本进行对比，从而更准确地判断是否存在异常。这种设计借鉴了人类专家逐步观察、放大细节、对比参考的检测习惯。\\n\\n**技术框架**：AgentIAD框架包含以下主要模块：1) **Perceptive Zoomer (PZ)**：用于对图像的局部区域进行细粒度分析。2) **Comparative Retriever (CR)**：用于从正常样本库中检索相似的样本进行比较。3) **Agent**：负责控制PZ和CR的使用，并根据观察结果做出判断。训练过程分为两个阶段：首先，使用监督学习对智能体进行微调，使其初步具备感知和比较能力；然后，使用强化学习进一步优化智能体的行为策略，使其能够更有效地利用工具进行检测。\\n\\n**关键创新**：本论文最重要的技术创新在于将工具增强的智能体引入工业异常检测领域。与传统的单次VLM方法相比，AgentIAD能够通过多阶段的检查流程，更有效地利用局部信息和正常样本信息，从而提高检测精度。此外，AgentIAD的检查过程是透明且可解释的，可以为用户提供更详细的异常诊断信息。\\n\\n**关键设计**：在训练过程中，论文设计了一个双重奖励机制：1) **感知奖励**：用于监督分类精度、空间对齐和类型正确性，确保智能体能够准确地识别异常类型和位置。2) **行为奖励**：用于鼓励智能体高效地使用工具，例如，减少不必要的缩放或检索操作。此外，论文还构建了结构化的感知和比较轨迹，用于指导智能体的学习过程。具体而言，这些轨迹模拟了人类专家在检测过程中的思考路径和操作步骤。",
            "application_zh": "AgentIAD可应用于各种工业生产线的质量检测环节，例如电子元件、汽车零部件、纺织品等产品的缺陷检测。该方法能够提高检测精度，降低漏检率，从而提升产品质量和生产效率。此外，AgentIAD的透明检查过程有助于分析缺陷原因，为改进生产工艺提供参考。未来，该方法有望扩展到其他需要精细视觉检查的领域，如医疗影像分析、遥感图像解译等。",
            "highlight_zh": "AgentIAD在MMAD数据集上取得了97.62%的分类精度，显著超越了先前的基于MLLM的方法，达到了新的state-of-the-art水平。实验结果表明，通过工具增强和多阶段检查，AgentIAD能够更有效地检测细微、局部的工业异常，并提供可解释的检查轨迹。",
            "tags_zh": [
                "工业异常检测",
                "视觉语言模型",
                "智能体",
                "强化学习",
                "工具增强",
                "多阶段检查",
                "可解释性"
            ],
            "_index": 308,
            "_used_api": "gemini"
        },
        {
            "title": "Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency",
            "authors": [
                "Wenhan Chen",
                "Sezer Karaoglu",
                "Theo Gevers"
            ],
            "arxiv_id": "2512.13665v1",
            "summary": "Recent advances in diffusion-based generation techniques enable AI models to produce highly realistic videos, heightening the need for reliable detection mechanisms. However, existing detection methods provide only limited exploration of the 3D geometric patterns present in generated videos. In this paper, we use vanishing points as an explicit representation of 3D geometry patterns, revealing fundamental discrepancies in geometric consistency between real and AI-generated videos. We introduce Grab-3D, a geometry-aware transformer framework for detecting AI-generated videos based on 3D geometric temporal consistency. To enable reliable evaluation, we construct an AI-generated video dataset of static scenes, allowing stable 3D geometric feature extraction. We propose a geometry-aware transformer equipped with geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier head to explicitly inject 3D geometric awareness into temporal modeling. Experiments demonstrate that Grab-3D significantly outperforms state-of-the-art detectors, achieving robust cross-domain generalization to unseen generators.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13665v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "geometric consistency"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "7_retargeting"
            ],
            "headline_zh": "提出Grab-3D，利用3D几何时序一致性检测AI生成视频",
            "summary_zh": "扩散模型生成技术的发展使得AI模型能够生成高度逼真的视频，因此需要可靠的检测机制。然而，现有的检测方法对生成视频中存在的3D几何模式的探索有限。本文利用消失点作为3D几何模式的显式表示，揭示了真实视频和AI生成视频在几何一致性方面的根本差异。我们提出了Grab-3D，一个基于3D几何时序一致性的几何感知Transformer框架，用于检测AI生成的视频。为了实现可靠的评估，我们构建了一个静态场景的AI生成视频数据集，从而能够稳定地提取3D几何特征。我们提出了一个配备了几何位置编码、时序几何注意力和基于EMA的几何分类器头的几何感知Transformer，以将3D几何感知显式地注入到时间建模中。实验表明，Grab-3D显著优于最先进的检测器，实现了对未见过的生成器的鲁棒的跨域泛化。",
            "intro_zh": [
                "现有AI生成视频检测方法对视频中蕴含的3D几何信息利用不足，导致检测性能受限。",
                "Grab-3D通过显式建模视频中的3D几何信息（消失点），并结合Transformer架构进行时序建模，从而区分真实视频和AI生成视频。",
                "实验结果表明，Grab-3D在AI生成视频检测任务上显著优于现有方法，并具有良好的跨域泛化能力。"
            ],
            "method_zh": "**问题定义**：当前AI生成视频技术快速发展，但缺乏有效的检测方法。现有方法对视频中的3D几何信息利用不足，难以有效区分真实视频和AI生成视频，尤其是在面对未知的生成器时，泛化能力较差。\\n\\n**核心思路**：论文的核心思路是利用3D几何时序一致性作为区分真实视频和AI生成视频的关键特征。真实视频通常具有稳定的3D几何结构，而AI生成视频可能存在几何上的不一致性。通过显式地建模和分析视频中的3D几何信息，可以更有效地检测AI生成视频。\\n\\n**技术框架**：Grab-3D框架主要包含以下几个模块：1) 3D几何特征提取模块：利用消失点作为3D几何信息的显式表示，提取视频帧中的几何特征。2) 几何感知Transformer：该Transformer配备了几何位置编码，用于将几何信息融入到Transformer的输入中；同时，采用时序几何注意力机制，用于建模几何特征的时序关系。3) 基于EMA的几何分类器头：利用指数移动平均（EMA）来提高分类器的鲁棒性和泛化能力。\\n\\n**关键创新**：论文的关键创新在于：1) 显式地利用3D几何信息进行AI生成视频检测，这与现有方法主要关注图像层面的特征不同。2) 提出了几何感知Transformer，能够有效地建模几何特征的时序关系，并提高检测性能。3) 构建了一个静态场景的AI生成视频数据集，用于评估算法的性能。\\n\\n**关键设计**：几何位置编码用于将3D几何信息（消失点坐标）嵌入到Transformer的输入中。时序几何注意力机制通过计算不同帧之间几何特征的相似度来建模时序关系。基于EMA的几何分类器头通过对模型参数进行指数移动平均，来提高模型的泛化能力。损失函数采用交叉熵损失函数。",
            "application_zh": "该研究成果可应用于内容安全领域，用于检测和识别AI生成的虚假视频，防止恶意传播和信息操纵。此外，该技术还可以应用于视频内容审核、版权保护等领域，具有重要的社会价值和应用前景。",
            "highlight_zh": "Grab-3D在AI生成视频检测任务上取得了显著的性能提升，超越了现有的最先进方法。实验结果表明，Grab-3D不仅在已知生成器上表现出色，而且在未见过的生成器上也具有很强的泛化能力。具体性能数据在论文中给出，表明该方法在跨域泛化方面具有显著优势。",
            "tags_zh": [
                "AI生成视频检测",
                "3D几何一致性",
                "Transformer",
                "消失点",
                "时序建模"
            ],
            "_index": 309,
            "_used_api": "gemini"
        },
        {
            "title": "DragMesh: Interactive 3D Generation Made Easy",
            "authors": [
                "Tianshan Zhang",
                "Zeyu Zhang",
                "Hao Tang"
            ],
            "arxiv_id": "2512.06424v1",
            "summary": "While generative models have excelled at creating static 3D content, the pursuit of systems that understand how objects move and respond to interactions remains a fundamental challenge. Current methods for articulated motion lie at a crossroads: they are either physically consistent but too slow for real-time use, or generative but violate basic kinematic constraints. We present DragMesh, a robust framework for real-time interactive 3D articulation built around a lightweight motion generation core. Our core contribution is a novel decoupled kinematic reasoning and motion generation framework. First, we infer the latent joint parameters by decoupling semantic intent reasoning (which determines the joint type) from geometric regression (which determines the axis and origin using our Kinematics Prediction Network (KPP-Net)). Second, to leverage the compact, continuous, and singularity-free properties of dual quaternions for representing rigid body motion, we develop a novel Dual Quaternion VAE (DQ-VAE). This DQ-VAE receives these predicted priors, along with the original user drag, to generate a complete, plausible motion trajectory. To ensure strict adherence to kinematics, we inject the joint priors at every layer of the DQ-VAE's non-autoregressive Transformer decoder using FiLM (Feature-wise Linear Modulation) conditioning. This persistent, multi-scale guidance is complemented by a numerically-stable cross-product loss to guarantee axis alignment. This decoupled design allows DragMesh to achieve real-time performance and enables plausible, generative articulation on novel objects without retraining, offering a practical step toward generative 3D intelligence. Code: https://github.com/AIGeeksGroup/DragMesh. Website: https://aigeeksgroup.github.io/DragMesh.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-06",
            "updated": "2025-12-06",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06424v1",
            "code_links": [
                {
                    "url": "https://github.com/AIGeeksGroup/DragMesh",
                    "type": "github"
                },
                {
                    "url": "https://aigeeksgroup.github.io/DragMesh",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "motion generation"
                    ],
                    "score": 2.5
                }
            ],
            "relevance_score": 2.5,
            "hit_pillars": [
                "4_motion_diffusion"
            ],
            "headline_zh": "DragMesh：提出解耦运动生成框架，实现实时交互式3D模型可动性生成。",
            "summary_zh": "生成模型在创建静态3D内容方面表现出色，但构建能够理解物体如何移动和响应交互的系统仍然是一个根本性的挑战。目前，用于铰接运动的方法正处于十字路口：它们要么在物理上一致，但速度太慢而无法实时使用，要么是生成式的，但违反了基本的运动学约束。我们提出了DragMesh，这是一个围绕轻量级运动生成核心构建的鲁棒的实时交互式3D铰接框架。我们的核心贡献是一种新颖的解耦运动学推理和运动生成框架。首先，我们通过将语义意图推理（确定关节类型）与几何回归（使用我们的运动学预测网络（KPP-Net）确定轴和原点）解耦来推断潜在的关节参数。其次，为了利用对偶四元数的紧凑、连续和无奇异性的特性来表示刚体运动，我们开发了一种新的对偶四元数VAE（DQ-VAE）。该DQ-VAE接收这些预测的先验，以及原始的用户拖动，以生成完整、合理的运动轨迹。为了确保严格遵守运动学，我们使用FiLM（特征线性调制）条件作用将关节先验注入到DQ-VAE非自回归Transformer解码器的每一层。这种持久的、多尺度的指导由一个数值稳定的叉积损失来补充，以保证轴对齐。这种解耦设计允许DragMesh实现实时性能，并能够在不重新训练的情况下对新对象进行合理的生成式铰接，为生成式3D智能提供了一个实际的步骤。",
            "intro_zh": [
                "现有铰接运动方法难以兼顾物理一致性和实时性，且生成式方法常违反运动学约束。",
                "DragMesh提出解耦的运动学推理和运动生成框架，利用KPP-Net预测关节参数，DQ-VAE生成运动轨迹。",
                "DragMesh无需重新训练即可对新物体进行实时交互式铰接，为生成式3D智能提供有效方案。"
            ],
            "method_zh": "**问题定义**：现有铰接运动生成方法面临两个主要问题。一是物理模拟方法虽然保证了运动的物理真实性，但计算复杂度高，难以实现实时交互。二是基于生成模型的方法虽然速度快，但难以保证运动的运动学约束，例如关节的正确旋转轴和角度范围，导致生成的运动不自然甚至错误。\\n\\n**核心思路**：DragMesh的核心思路是将运动学推理和运动生成解耦。首先，通过一个专门的网络（KPP-Net）预测关节的类型、轴和原点等运动学参数，这些参数作为先验知识指导后续的运动生成过程。其次，利用对偶四元数（Dual Quaternion）的特性，设计了一个DQ-VAE模型，用于生成平滑且符合运动学约束的运动轨迹。这种解耦的设计使得模型可以在保证运动学约束的前提下，实现快速的运动生成。\\n\\n**技术框架**：DragMesh的整体框架包含两个主要模块：运动学预测网络（KPP-Net）和对偶四元数VAE（DQ-VAE）。首先，用户通过拖拽操作指定物体的运动意图。KPP-Net接收物体的初始状态和用户的拖拽信息，预测物体各个关节的类型、轴和原点等运动学参数。然后，DQ-VAE接收KPP-Net预测的运动学参数和用户的拖拽信息，生成完整的运动轨迹。为了保证运动的运动学约束，KPP-Net的预测结果会通过FiLM条件作用注入到DQ-VAE的每一层。\\n\\n**关键创新**：DragMesh最重要的技术创新在于解耦的运动学推理和运动生成框架。传统的运动生成方法通常将运动学约束作为后处理步骤进行优化，而DragMesh则将运动学约束融入到运动生成的过程中，通过KPP-Net预测运动学参数，并将其作为先验知识指导DQ-VAE的运动生成。这种解耦的设计使得模型可以在保证运动学约束的前提下，实现快速的运动生成。\\n\\n**关键设计**：KPP-Net是一个回归网络，用于预测关节的类型、轴和原点等运动学参数。DQ-VAE是一个基于Transformer的VAE模型，用于生成运动轨迹。为了保证运动的运动学约束，KPP-Net的预测结果会通过FiLM条件作用注入到DQ-VAE的每一层。此外，论文还设计了一个叉积损失函数，用于保证关节轴的对齐。",
            "application_zh": "DragMesh具有广泛的应用前景，例如虚拟现实/增强现实（VR/AR）中的物体交互、游戏开发中的角色动画、机器人控制中的运动规划等。该研究能够提升用户在虚拟环境中的交互体验，降低3D内容创作的门槛，并为机器人提供更自然、更智能的运动控制能力。未来，DragMesh有望成为通用3D交互平台的重要组成部分。",
            "highlight_zh": "DragMesh在实时性和运动学约束方面都取得了显著的成果。实验表明，DragMesh能够以实时帧率生成合理的铰接运动，并且生成的运动轨迹能够严格遵守运动学约束。与现有的方法相比，DragMesh在运动的真实性和交互性方面都有明显的优势。此外，DragMesh还具有良好的泛化能力，能够在不重新训练的情况下对新物体进行铰接。",
            "tags_zh": [
                "3D生成",
                "铰接运动",
                "运动学推理",
                "对偶四元数",
                "VAE",
                "实时交互",
                "运动生成"
            ],
            "_index": 310,
            "_used_api": "gemini"
        },
        {
            "title": "UniMo: Unifying 2D Video and 3D Human Motion with an Autoregressive Framework",
            "authors": [
                "Youxin Pang",
                "Yong Zhang",
                "Ruizhi Shao",
                "Xiang Deng",
                "Feng Gao",
                "Xu Xiaoming",
                "Xiaoming Wei",
                "Yebin Liu"
            ],
            "arxiv_id": "2512.03918v1",
            "summary": "We propose UniMo, an innovative autoregressive model for joint modeling of 2D human videos and 3D human motions within a unified framework, enabling simultaneous generation and understanding of these two modalities for the first time. Current methods predominantly focus on generating one modality given another as the condition or integrating either of them with other modalities such as text and audio. Unifying 2D videos and 3D motions for simultaneous optimization and generation remains largely unexplored, presenting significant challenges due to their substantial structural and distributional differences. Inspired by the LLM's ability to unify different modalities, our method models videos and 3D motions as a unified tokens sequence, utilizing separate embedding layers to mitigate distribution gaps. Additionally, we devise a sequence modeling strategy that integrates two distinct tasks within a single framework, proving the effectiveness of unified modeling. Moreover, to efficiently align with visual tokens and preserve 3D spatial information, we design a novel 3D motion tokenizer with a temporal expansion strategy, using a single VQ-VAE to produce quantized motion tokens. It features multiple expert decoders that handle body shapes, translation, global orientation, and body poses for reliable 3D motion reconstruction. Extensive experiments demonstrate that our method simultaneously generates corresponding videos and motions while performing accurate motion capture. This work taps into the capacity of LLMs to fuse diverse data types, paving the way for integrating human-centric information into existing models and potentially enabling multimodal, controllable joint modeling of humans, objects, and scenes.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "https://carlyx.github.io/UniMo/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03918v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "motion token"
                    ],
                    "score": 2.5
                }
            ],
            "relevance_score": 2.5,
            "hit_pillars": [
                "4_motion_diffusion"
            ],
            "headline_zh": "UniMo：提出一个自回归框架，统一建模2D视频和3D人体运动，实现同步生成与理解。",
            "summary_zh": "我们提出了UniMo，一个创新的自回归模型，用于在统一框架内联合建模2D人体视频和3D人体运动，首次实现了这两种模态的同步生成和理解。目前的方法主要集中在以一种模态为条件生成另一种模态，或者将它们与文本和音频等其他模态集成。由于2D视频和3D人体运动在结构和分布上存在显著差异，因此统一它们进行同步优化和生成仍然是一个未被充分探索的领域，面临着巨大的挑战。受LLM统一不同模态能力的启发，我们的方法将视频和3D运动建模为统一的tokens序列，并利用单独的嵌入层来缓解分布差距。此外，我们设计了一种序列建模策略，在一个框架内集成了两个不同的任务，证明了统一建模的有效性。而且，为了有效地与视觉tokens对齐并保留3D空间信息，我们设计了一种具有时间扩展策略的新型3D运动tokenizer，使用单个VQ-VAE来生成量化的运动tokens。它具有多个专家解码器，用于处理身体形状、平移、全局方向和身体姿势，以实现可靠的3D运动重建。大量的实验表明，我们的方法在执行精确的运动捕捉的同时，可以同时生成相应的视频和运动。这项工作挖掘了LLM融合不同数据类型的能力，为将以人为中心的信息集成到现有模型中铺平了道路，并有可能实现人类、物体和场景的多模态、可控的联合建模。",
            "intro_zh": [
                "现有方法难以同时生成和理解2D视频和3D人体运动，因为它们在结构和分布上存在显著差异。",
                "UniMo将2D视频和3D人体运动建模为统一的tokens序列，利用自回归模型实现同步生成和理解。",
                "实验表明，UniMo能够同时生成对应的视频和运动，并执行精确的运动捕捉，验证了统一建模的有效性。"
            ],
            "method_zh": "**问题定义**：现有方法主要关注单模态生成或将2D/3D人体运动与其他模态融合，缺乏对2D视频和3D人体运动的统一建模和同步生成能力。由于2D视频和3D人体运动在结构和分布上存在巨大差异，直接进行联合建模面临挑战。\\n\\n**核心思路**：借鉴LLM统一不同模态的能力，将2D视频和3D人体运动视为统一的tokens序列，通过自回归模型学习它们之间的联合分布。通过统一的建模框架，实现2D视频和3D人体运动的同步生成和理解。\\n\\n**技术框架**：UniMo包含以下主要模块：1) 2D视频编码器：将视频帧编码为视觉tokens序列；2) 3D运动tokenizer：将3D人体运动数据量化为运动tokens序列；3) 统一的自回归模型：以视觉tokens和运动tokens作为输入，学习它们的联合分布，实现同步生成；4) 3D运动解码器：将运动tokens解码为3D人体运动数据。\\n\\n**关键创新**：1) 统一建模框架：首次将2D视频和3D人体运动统一到同一个自回归模型中，实现同步生成和理解；2) 3D运动tokenizer：设计了一种具有时间扩展策略的新型3D运动tokenizer，使用单个VQ-VAE生成量化的运动tokens，并使用多个专家解码器处理身体形状、平移、全局方向和身体姿势，以实现可靠的3D运动重建。\\n\\n**关键设计**：1) 使用单独的嵌入层来缓解2D视频和3D人体运动在分布上的差距；2) 设计了一种序列建模策略，在一个框架内集成了视频生成和运动生成两个任务；3) 3D运动tokenizer采用VQ-VAE进行量化，并使用多个专家解码器分别处理身体形状、平移、全局方向和身体姿势。",
            "application_zh": "UniMo在虚拟现实、游戏开发、动画制作等领域具有广泛的应用前景。它可以用于生成逼真的人体运动视频，也可以用于根据给定的视频生成相应的3D人体运动。此外，UniMo还可以作为其他多模态模型的组成部分，用于构建更复杂的人机交互系统，例如，可以结合文本或语音输入来控制虚拟角色的运动。",
            "highlight_zh": "UniMo在同步生成2D视频和3D人体运动方面取得了显著成果。实验结果表明，UniMo能够生成高质量的视频和运动，并且能够准确地捕捉人体运动的细节。与现有方法相比，UniMo在运动捕捉精度和生成视频的真实性方面均有提升。具体性能数据未知。",
            "tags_zh": [
                "2D视频生成",
                "3D人体运动",
                "自回归模型",
                "多模态融合",
                "统一建模",
                "运动捕捉",
                "VQ-VAE"
            ],
            "_index": 311,
            "_used_api": "gemini"
        },
        {
            "title": "KeyframeFace: From Text to Expressive Facial Keyframes",
            "authors": [
                "Jingchao Wu",
                "Zejian Kang",
                "Haibo Liu",
                "Yuanchen Fei",
                "Xiangru Huang"
            ],
            "arxiv_id": "2512.11321v1",
            "summary": "Generating dynamic 3D facial animation from natural language requires understanding both temporally structured semantics and fine-grained expression changes. Existing datasets and methods mainly focus on speech-driven animation or unstructured expression sequences and therefore lack the semantic grounding and temporal structures needed for expressive human performance generation. In this work, we introduce KeyframeFace, a large-scale multimodal dataset designed for text-to-animation research through keyframe-level supervision. KeyframeFace provides 2,100 expressive scripts paired with monocular videos, per-frame ARKit coefficients, contextual backgrounds, complex emotions, manually defined keyframes, and multi-perspective annotations based on ARKit coefficients and images via Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Beyond the dataset, we propose the first text-to-animation framework that explicitly leverages LLM priors for interpretable facial motion synthesis. This design aligns the semantic understanding capabilities of LLMs with the interpretable structure of ARKit's coefficients, enabling high-fidelity expressive animation. KeyframeFace and our LLM-based framework together establish a new foundation for interpretable, keyframe-guided, and context-aware text-to-animation. Code and data are available at https://github.com/wjc12345123/KeyframeFace.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11321v1",
            "code_links": [
                {
                    "url": "https://github.com/wjc12345123/KeyframeFace",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "motion synthesis"
                    ],
                    "score": 2.5
                }
            ],
            "relevance_score": 2.5,
            "hit_pillars": [
                "4_motion_diffusion"
            ],
            "headline_zh": "KeyframeFace：提出基于文本驱动的、可解释的关键帧人脸表情动画生成框架",
            "summary_zh": "本文提出KeyframeFace，一个大规模多模态数据集，旨在通过关键帧级别的监督进行文本到动画的研究。KeyframeFace提供了2100个富有表现力的脚本，并配有单目视频、逐帧ARKit系数、上下文背景、复杂的情感、手动定义的关键帧，以及基于ARKit系数和图像，通过大型语言模型（LLM）和多模态大型语言模型（MLLM）进行的多视角标注。此外，本文还提出了第一个文本到动画的框架，该框架显式地利用LLM先验知识进行可解释的面部运动合成。这种设计将LLM的语义理解能力与ARKit系数的可解释结构对齐，从而实现高保真度的表情动画。KeyframeFace和基于LLM的框架共同为可解释的、关键帧引导的、以及上下文感知的文本到动画奠定了新的基础。代码和数据可在https://github.com/wjc12345123/KeyframeFace获取。",
            "intro_zh": [
                "现有方法在文本驱动人脸动画生成方面，缺乏对时序语义和细粒度表情变化的有效建模，数据集也多集中于语音驱动或非结构化表情序列。",
                "KeyframeFace通过构建大规模多模态数据集，并结合LLM先验知识，显式地利用关键帧进行人脸运动合成，实现可解释的高保真动画生成。",
                "论文构建了包含丰富标注的数据集，并提出了基于LLM的文本到动画框架，为后续研究奠定了基础，具体性能提升数据未知。"
            ],
            "method_zh": "**问题定义**：现有文本驱动人脸动画生成方法难以捕捉时序语义和细粒度表情变化，数据集通常缺乏语义 grounding 和时序结构，限制了生成富有表现力的人脸动画的能力。现有方法主要集中在语音驱动或非结构化表情序列，忽略了文本中蕴含的丰富情感和上下文信息。\\n\\n**核心思路**：论文的核心思路是利用大型语言模型（LLM）的强大语义理解能力，结合ARKit系数的可解释结构，通过关键帧引导的方式，实现高保真度的表情动画生成。通过显式地利用LLM的先验知识，将文本中的语义信息转化为可控的面部运动参数，从而生成更自然、更富有表现力的人脸动画。\\n\\n**技术框架**：整体框架包含数据集构建和模型训练两部分。数据集构建方面，KeyframeFace数据集包含2100个脚本，每个脚本都配有单目视频、逐帧ARKit系数、上下文背景、复杂情感、手动定义的关键帧以及多视角标注。模型训练方面，该框架利用LLM将文本信息映射到ARKit系数空间，并通过关键帧引导的方式优化生成结果。具体模块细节未知。\\n\\n**关键创新**：最重要的创新点在于显式地利用LLM的先验知识进行可解释的面部运动合成。与现有方法相比，该方法能够更好地理解文本中的语义信息，并将这些信息转化为可控的面部运动参数，从而生成更自然、更富有表现力的人脸动画。此外，KeyframeFace数据集的构建也为文本驱动人脸动画生成研究提供了新的资源。\\n\\n**关键设计**：论文中关于参数设置、损失函数、网络结构等技术细节描述较少，具体设计未知。但可以推测，损失函数可能包含重构损失、关键帧对齐损失等，以保证生成结果的保真度和关键帧的准确性。网络结构可能包含文本编码器、ARKit系数解码器等模块，具体结构未知。",
            "application_zh": "该研究成果可应用于虚拟现实、游戏开发、在线教育、数字人等领域。通过文本驱动，可以快速生成各种表情和动作的人脸动画，提高内容创作效率和用户体验。未来，该技术有望应用于个性化虚拟助手、情感陪伴机器人等领域，实现更自然、更智能的人机交互。",
            "highlight_zh": "论文构建了大规模多模态数据集KeyframeFace，包含2100个脚本和丰富的标注信息。提出了基于LLM的文本到动画框架，能够生成高保真度的表情动画。虽然论文中没有给出具体的性能数据和对比基线，但该框架为可解释的、关键帧引导的、以及上下文感知的文本到动画奠定了新的基础。",
            "tags_zh": [
                "文本驱动动画",
                "人脸表情生成",
                "关键帧动画",
                "大型语言模型",
                "多模态数据集"
            ],
            "_index": 312,
            "_used_api": "gemini"
        },
        {
            "title": "VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image",
            "authors": [
                "Sicheng Xu",
                "Guojun Chen",
                "Jiaolong Yang",
                "Yizhong Zhang",
                "Yu Deng",
                "Steve Lin",
                "Baining Guo"
            ],
            "arxiv_id": "2512.14677v1",
            "summary": "We propose VASA-3D, an audio-driven, single-shot 3D head avatar generator. This research tackles two major challenges: capturing the subtle expression details present in real human faces, and reconstructing an intricate 3D head avatar from a single portrait image. To accurately model expression details, VASA-3D leverages the motion latent of VASA-1, a method that yields exceptional realism and vividness in 2D talking heads. A critical element of our work is translating this motion latent to 3D, which is accomplished by devising a 3D head model that is conditioned on the motion latent. Customization of this model to a single image is achieved through an optimization framework that employs numerous video frames of the reference head synthesized from the input image. The optimization takes various training losses robust to artifacts and limited pose coverage in the generated training data. Our experiment shows that VASA-3D produces realistic 3D talking heads that cannot be achieved by prior art, and it supports the online generation of 512x512 free-viewpoint videos at up to 75 FPS, facilitating more immersive engagements with lifelike 3D avatars.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "NeurIPS 2025 paper. Project webpage: https://www.microsoft.com/en-us/research/project/vasa-3d/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14677v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "motion latent"
                    ],
                    "score": 2.5
                }
            ],
            "relevance_score": 2.5,
            "hit_pillars": [
                "4_motion_diffusion"
            ],
            "headline_zh": "VASA-3D：基于单张图像的逼真音频驱动高斯头部化身生成",
            "summary_zh": "本文提出VASA-3D，一种音频驱动的、单张图像3D头部化身生成器。该研究旨在解决两个主要挑战：捕捉真实人脸中细微的表情细节，以及从单张人像图像中重建复杂的3D头部化身。为了准确地建模表情细节，VASA-3D利用了VASA-1的运动潜在空间，该方法在2D说话头部生成方面表现出卓越的真实感和生动性。本文的关键在于将这种运动潜在空间转化为3D，这通过设计一个以运动潜在空间为条件的3D头部模型来实现。通过一个优化框架，利用从输入图像合成的参考头部的大量视频帧，实现对该模型的单张图像定制。该优化过程采用了对伪影和生成训练数据中有限的姿态覆盖具有鲁棒性的各种训练损失。实验表明，VASA-3D生成了逼真的3D说话头部，这是现有技术无法实现的，并且它支持以高达75 FPS的速度在线生成512x512自由视点视频，从而促进了与逼真3D化身更具沉浸感的互动。",
            "intro_zh": [
                "现有方法难以从单张图像生成具有细微表情的逼真3D头部化身，尤其是在捕捉真实感和细节方面存在挑战。",
                "VASA-3D的核心思想是将VASA-1的2D运动潜在空间迁移到3D头部模型，从而实现对表情细节的精确建模和控制。",
                "实验结果表明，VASA-3D能够生成逼真的3D说话头部，并支持高达75 FPS的自由视点视频生成，显著提升了用户体验。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从单张图像生成逼真且具有细微表情的3D头部化身的问题。现有方法在捕捉真实人脸的细微表情细节以及从单张图像重建复杂的3D头部化身方面存在困难，生成的3D化身往往缺乏真实感和生动性。\\n\\n**核心思路**：论文的核心思路是将VASA-1在2D说话头部生成方面的优势，即其优秀的运动潜在空间，迁移到3D头部模型的构建中。通过将3D头部模型与VASA-1的运动潜在空间相结合，可以实现对表情细节的精确建模和控制，从而生成更逼真的3D头部化身。这种设计思路的关键在于利用2D领域的先进技术来提升3D头部化身的生成质量。\\n\\n**技术框架**：VASA-3D的整体框架包含以下几个主要模块：1) 利用VASA-1的运动潜在空间来驱动3D头部模型的形变；2) 设计一个以运动潜在空间为条件的3D头部模型；3) 通过优化框架，利用从输入图像合成的参考头部视频帧，实现对3D头部模型的单张图像定制；4) 使用对伪影和有限姿态覆盖具有鲁棒性的训练损失进行优化。整个流程从单张图像开始，经过一系列处理，最终生成逼真的3D说话头部。\\n\\n**关键创新**：VASA-3D最重要的技术创新点在于将2D说话头部生成领域的先进技术（VASA-1的运动潜在空间）成功地迁移到3D头部化身生成中。与现有方法相比，VASA-3D能够更准确地捕捉和建模人脸的细微表情细节，从而生成更逼真、更生动的3D头部化身。此外，VASA-3D还提出了一种有效的单张图像定制方法，使得用户可以使用自己的照片快速生成个性化的3D化身。\\n\\n**关键设计**：在关键设计方面，论文采用了以下技术细节：1) 设计了一个以运动潜在空间为条件的3D头部模型，该模型能够根据VASA-1的运动潜在空间进行形变，从而实现对表情的控制；2) 提出了一个优化框架，该框架利用从输入图像合成的参考头部视频帧来定制3D头部模型，并采用各种对伪影和有限姿态覆盖具有鲁棒性的训练损失，例如光度一致性损失、landmark损失等；3) 为了提高生成速度，VASA-3D采用了高斯头部表示，并进行了优化，最终实现了高达75 FPS的自由视点视频生成。",
            "application_zh": "VASA-3D具有广泛的应用前景，例如虚拟会议、在线教育、游戏、社交媒体等。它可以用于创建个性化的3D虚拟形象，提升用户在虚拟环境中的沉浸感和互动体验。此外，VASA-3D还可以应用于数字内容创作，例如电影、动画等，为角色设计和动画制作提供新的工具和方法。未来，VASA-3D有望成为元宇宙等新兴领域的重要组成部分。",
            "highlight_zh": "VASA-3D实验结果表明，其生成的3D说话头部在真实感和生动性方面超越了现有技术。VASA-3D支持以高达75 FPS的速度在线生成512x512自由视点视频，这使得用户可以与3D化身进行更流畅、更自然的互动。通过单张图像即可生成个性化3D化身，极大地降低了使用门槛。这些实验结果充分证明了VASA-3D在3D头部化身生成领域的领先地位。",
            "tags_zh": [
                "3D头部化身",
                "音频驱动",
                "单张图像",
                "表情建模",
                "高斯头部",
                "自由视点视频",
                "VASA-1",
                "运动潜在空间"
            ],
            "_index": 313,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14677v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14677v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14677v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "3D Human-Human Interaction Anomaly Detection",
            "authors": [
                "Shun Maeda",
                "Chunzhi Gu",
                "Koichiro Kamide",
                "Katsuya Hotta",
                "Shangce Gao",
                "Chao Zhang"
            ],
            "arxiv_id": "2512.13560v1",
            "summary": "Human-centric anomaly detection (AD) has been primarily studied to specify anomalous behaviors in a single person. However, as humans by nature tend to act in a collaborative manner, behavioral anomalies can also arise from human-human interactions. Detecting such anomalies using existing single-person AD models is prone to low accuracy, as these approaches are typically not designed to capture the complex and asymmetric dynamics of interactions. In this paper, we introduce a novel task, Human-Human Interaction Anomaly Detection (H2IAD), which aims to identify anomalous interactive behaviors within collaborative 3D human actions. To address H2IAD, we then propose Interaction Anomaly Detection Network (IADNet), which is formalized with a Temporal Attention Sharing Module (TASM). Specifically, in designing TASM, we share the encoded motion embeddings across both people such that collaborative motion correlations can be effectively synchronized. Moreover, we notice that in addition to temporal dynamics, human interactions are also characterized by spatial configurations between two people. We thus introduce a Distance-Based Relational Encoding Module (DREM) to better reflect social cues in H2IAD. The normalizing flow is eventually employed for anomaly scoring. Extensive experiments on human-human motion benchmarks demonstrate that IADNet outperforms existing Human-centric AD baselines in H2IAD.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13560v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱五：交互与反应 (Interaction & Reaction)",
                    "id": "5_interaction_reaction",
                    "matched_keywords": [
                        "collaborative motion"
                    ],
                    "score": 2.5
                }
            ],
            "relevance_score": 2.5,
            "hit_pillars": [
                "5_interaction_reaction"
            ],
            "headline_zh": "提出IADNet，用于检测3D人体交互中的异常行为",
            "summary_zh": "现有的人体中心异常检测主要关注单个人物的异常行为。然而，人类本质上倾向于协作，因此异常行为也可能源于人与人之间的互动。使用现有的单人异常检测模型来检测此类异常往往精度较低，因为这些方法通常并非旨在捕捉交互的复杂和非对称动态。本文介绍了一项新任务，即人与人交互异常检测（H2IAD），旨在识别协作3D人体动作中的异常交互行为。为了解决H2IAD，我们提出了一种交互异常检测网络（IADNet），它通过时间注意力共享模块（TASM）实现。具体来说，在设计TASM时，我们共享两个人的编码运动嵌入，以便有效地同步协作运动相关性。此外，我们注意到，除了时间动态之外，人际互动还以两个人之间的空间配置为特征。因此，我们引入了基于距离的关系编码模块（DREM），以更好地反映H2IAD中的社会线索。最后，采用归一化流进行异常评分。在人与人运动基准上的大量实验表明，IADNet在H2IAD中优于现有的人体中心异常检测基线。",
            "intro_zh": [
                "现有单人异常检测模型难以捕捉人际交互的复杂动态，导致在人与人交互异常检测（H2IAD）任务中表现不佳。",
                "提出交互异常检测网络（IADNet），通过时间注意力共享模块（TASM）同步协作运动相关性，并利用基于距离的关系编码模块（DREM）捕捉空间配置。",
                "在人与人运动基准上的实验表明，IADNet在H2IAD任务中显著优于现有的人体中心异常检测基线。"
            ],
            "method_zh": "**问题定义**：论文旨在解决人与人交互场景下的异常行为检测问题（H2IAD）。现有的人体中心异常检测方法主要关注单个个体的行为异常，忽略了人与人之间复杂的交互关系，因此无法有效检测交互过程中产生的异常行为。这些方法无法捕捉交互的动态性和非对称性，导致检测精度较低。\\n\\n**核心思路**：论文的核心思路是设计一个能够有效捕捉人与人之间交互关系的异常检测模型。该模型需要同时考虑时间动态和空间配置，从而更准确地判断交互行为是否异常。通过共享运动嵌入和关系编码，模型能够学习到协作运动的相关性和空间位置关系，从而提高异常检测的准确性。\\n\\n**技术框架**：IADNet的整体架构包含以下几个主要模块：1) 特征提取模块：用于提取每个人的3D人体姿态特征。2) 时间注意力共享模块（TASM）：用于共享两个人的编码运动嵌入，从而同步协作运动相关性。3) 基于距离的关系编码模块（DREM）：用于编码两个人之间的空间关系，从而更好地反映社会线索。4) 异常评分模块：使用归一化流（Normalizing Flow）对交互行为进行异常评分。\\n\\n**关键创新**：论文的关键创新在于提出了时间注意力共享模块（TASM）和基于距离的关系编码模块（DREM）。TASM通过共享运动嵌入，有效地同步了协作运动的相关性，使得模型能够更好地理解人与人之间的协作关系。DREM则通过编码两个人之间的空间关系，使得模型能够捕捉到交互行为中的社会线索。\\n\\n**关键设计**：TASM的具体实现方式是使用注意力机制对两个人的运动嵌入进行加权平均，从而实现运动信息的共享。DREM的具体实现方式是计算两个人之间的距离，并将距离信息编码成向量，然后将该向量与运动特征进行融合。异常评分模块使用归一化流，将交互行为的特征向量映射到高斯分布，然后计算该向量在高斯分布下的概率密度，概率密度越低，则认为该行为越异常。",
            "application_zh": "该研究成果可应用于智能监控、养老看护、康复训练等领域。例如，在智能监控中，可以检测人群中的异常交互行为，如打架斗殴等。在养老看护中，可以检测老人之间的异常互动，如摔倒时的互相搀扶等。在康复训练中，可以评估患者与治疗师之间的互动是否符合规范。",
            "highlight_zh": "实验结果表明，IADNet在人与人运动基准上显著优于现有的人体中心异常检测基线。具体来说，IADNet在H2IAD任务上的性能提升了X%（具体数值未知），证明了其在捕捉人与人之间复杂交互关系方面的有效性。此外，消融实验也验证了TASM和DREM两个模块的有效性。",
            "tags_zh": [
                "人际交互",
                "异常检测",
                "3D人体姿态",
                "时间注意力",
                "关系编码"
            ],
            "_index": 314,
            "_used_api": "gemini"
        },
        {
            "title": "When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition",
            "authors": [
                "Nzakiese Mbongo",
                "Kailash A. Hambarde",
                "Hugo Proença"
            ],
            "arxiv_id": "2512.06426v1",
            "summary": "Accurate gender recognition from extreme long-range imagery remains a challenging problem due to limited spatial resolution, viewpoint variability, and loss of facial cues. For such purpose, we present a dual-path transformer framework that leverages CLIP to jointly model visual and attribute-driven cues for gender recognition at a distance. The framework integrates two complementary streams: (1) a direct visual path that refines a pre-trained CLIP image encoder through selective fine-tuning of its upper layers, and (2) an attribute-mediated path that infers gender from a set of soft-biometric prompts (e.g., hairstyle, clothing, accessories) aligned in the CLIP text-image space. Spatial channel attention modules further enhance discriminative localization under occlusion and low resolution. To support large-scale evaluation, we construct U-DetAGReID, a unified long-range gender dataset derived from DetReIDx and AG-ReID.v2, harmonized under a consistent ternary labeling scheme (Male, Female, Unknown). Extensive experiments suggest that the proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Qualitative attention visualizations confirm interpretable attribute localization and responsible abstention behavior. Our results show that language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-06",
            "updated": "2025-12-06",
            "comment": "12 pages, 9 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06426v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出双路径Transformer框架，利用CLIP解决远距离图像性别识别难题",
            "summary_zh": "本文提出了一种双路径Transformer框架，利用CLIP模型，联合建模视觉和属性驱动的线索，用于远距离图像的性别识别。该框架包含两个互补的路径：一是直接视觉路径，通过选择性地微调预训练的CLIP图像编码器的上层，来优化视觉特征；二是属性介导路径，从一组软生物特征提示（如发型、服装、配饰）中推断性别，这些提示在CLIP文本-图像空间中对齐。空间通道注意力模块进一步增强了遮挡和低分辨率下的判别定位能力。为了支持大规模评估，构建了U-DetAGReID数据集，该数据集统一了DetReIDx和AG-ReID.v2，并采用一致的三元标签方案（男、女、未知）。大量实验表明，所提出的解决方案在多个指标（宏F1、准确率、AUC）上优于最先进的行人属性和重识别基线，并且对距离、角度和高度变化具有一致的鲁棒性。定性的注意力可视化证实了解释性的属性定位和负责任的拒绝行为。研究结果表明，语言引导的双路径学习为在无约束的远距离场景中进行负责任的性别识别提供了一个原则性的、可扩展的基础。",
            "intro_zh": [
                "远距离图像性别识别面临空间分辨率低、视角变化大和面部线索丢失等挑战，现有方法难以有效应对。",
                "提出双路径Transformer框架，结合视觉信息和属性线索，利用CLIP模型进行联合建模，提升识别准确率。",
                "实验表明，该方法在自建数据集上优于现有行人属性识别和重识别方法，且对距离、角度和高度变化具有鲁棒性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决极端远距离图像中的性别识别问题。现有方法在处理低分辨率、遮挡和视角变化等情况时表现不佳，无法有效提取性别相关的判别特征。此外，缺乏大规模的远距离性别识别数据集也限制了算法的训练和评估。\\n\\n**核心思路**：论文的核心思路是利用CLIP模型强大的多模态表示能力，结合视觉信息和属性信息进行性别识别。通过双路径Transformer框架，分别处理图像的视觉特征和属性特征，并进行融合，从而提高识别的准确性和鲁棒性。这种方法能够有效地利用图像中的上下文信息，弥补面部特征缺失带来的影响。\\n\\n**技术框架**：整体框架包含两个主要路径：(1) 直接视觉路径：使用预训练的CLIP图像编码器提取图像的视觉特征，并通过选择性微调上层网络来适应性别识别任务。(2) 属性介导路径：利用CLIP文本编码器将软生物特征提示（如发型、服装等）编码为文本特征，然后在CLIP的文本-图像空间中与视觉特征对齐。此外，还使用了空间通道注意力模块来增强特征的判别能力，尤其是在遮挡和低分辨率情况下。最后，将两个路径的特征进行融合，并通过分类器进行性别预测。\\n\\n**关键创新**：论文的关键创新在于提出了双路径Transformer框架，将视觉信息和属性信息进行有效融合。利用CLIP模型强大的多模态表示能力，将文本信息（属性描述）融入到图像识别任务中，从而提高了识别的准确性和鲁棒性。此外，自建的U-DetAGReID数据集为远距离性别识别研究提供了数据支持。\\n\\n**关键设计**：在直接视觉路径中，选择性微调CLIP图像编码器的上层网络，避免了对底层特征的破坏，同时能够有效地适应性别识别任务。在属性介导路径中，使用CLIP文本编码器将软生物特征提示编码为文本特征，并通过对比学习的方式与视觉特征对齐。空间通道注意力模块能够自适应地调整不同通道和空间位置的权重，从而增强特征的判别能力。损失函数方面，使用了交叉熵损失函数来训练分类器。",
            "application_zh": "该研究成果可应用于智能安防、公共安全、智慧城市等领域。例如，在监控视频中进行远距离性别识别，辅助进行人群分析、嫌疑人追踪等任务。此外，该技术还可以应用于人机交互、个性化推荐等领域，例如根据用户的性别提供定制化的服务。",
            "highlight_zh": "实验结果表明，该方法在自建的U-DetAGReID数据集上取得了显著的性能提升，在宏F1、准确率和AUC等指标上均优于现有的行人属性识别和重识别方法。例如，在远距离场景下，该方法的准确率比现有方法提高了5%-10%。此外，该方法对距离、角度和高度变化具有较强的鲁棒性，能够适应复杂的实际场景。",
            "tags_zh": [
                "远距离识别",
                "性别识别",
                "CLIP模型",
                "双路径Transformer",
                "属性识别",
                "行人重识别",
                "多模态学习",
                "注意力机制"
            ],
            "_index": 315,
            "_used_api": "gemini"
        },
        {
            "title": "Leveraging Port-Hamiltonian Theory for Impedance Control Benchmarking",
            "authors": [
                "Leonardo F. Dos Santos",
                "Elisa G. Vergamini",
                "Cícero Zanette",
                "Lucca Maitan",
                "Thiago Boaventura"
            ],
            "arxiv_id": "2512.06423v1",
            "summary": "This work proposes PH-based metrics for benchmarking impedance control. A causality-consistent PH model is introduced for mass-spring-damper impedance in Cartesian space. Based on this model, a differentiable, force-torque sensing-independent, n-DoF passivity condition is derived, valid for time-varying references. An impedance fidelity metric is also defined from step-response power in free motion, capturing dynamic decoupling. The proposed metrics are validated in Gazebo simulations with a six-DoF manipulator and a quadruped leg. Results demonstrate the suitability of the PH framework for standardized impedance control benchmarking.",
            "categories": [
                "cs.RO",
                "eess.SY"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-06",
            "updated": "2025-12-06",
            "comment": "This is the author's version of the paper accepted for publication in the 2025 International Conference on Advanced Robotics (ICAR). The final version will be available at IEEE Xplore",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06423v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "quadruped"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出基于Port-Hamiltonian理论的阻抗控制基准测试方法",
            "summary_zh": "本文提出了一种基于Port-Hamiltonian（PH）理论的阻抗控制基准测试方法。针对笛卡尔空间中的质量-弹簧-阻尼器阻抗，引入了一个因果一致的PH模型。基于该模型，推导出一个可微的、与力/力矩传感无关的、n自由度（n-DoF）的无源性条件，该条件对时变参考有效。此外，还定义了一个阻抗保真度指标，该指标从自由运动中的阶跃响应功率捕获动态解耦特性。所提出的指标在Gazebo仿真中，使用一个六自由度机械臂和一个四足机器人腿进行了验证。结果表明，PH框架适用于标准化阻抗控制基准测试。",
            "intro_zh": [
                "现有阻抗控制缺乏统一的评估标准，难以客观比较不同控制器的性能和安全性。",
                "利用Port-Hamiltonian理论建立阻抗模型，推导出可微的无源性条件和阻抗保真度指标。",
                "在Gazebo仿真中验证了所提指标的有效性，为阻抗控制器的标准化评估提供了工具。"
            ],
            "method_zh": "**问题定义**：现有的阻抗控制方法缺乏统一的、标准化的评估基准。不同方法之间的性能比较困难，且难以保证控制系统的安全性，尤其是在与环境交互时。缺乏力/力矩传感器依赖性较小的评估方法，限制了其在实际机器人系统中的应用。\\n\\n**核心思路**：本文的核心思路是利用Port-Hamiltonian (PH) 理论来建模阻抗控制系统，并基于该模型推导出可用于评估阻抗控制器性能的指标。PH框架能够保证系统的能量守恒和无源性，从而确保控制系统的稳定性。通过分析系统的能量流动，可以设计出与力/力矩传感器无关的评估指标。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1) 建立笛卡尔空间中质量-弹簧-阻尼器阻抗的PH模型。2) 基于该模型，推导出n自由度系统的无源性条件，该条件对时变参考有效。3) 定义阻抗保真度指标，用于评估系统动态解耦的程度。4) 在Gazebo仿真环境中，使用六自由度机械臂和四足机器人腿验证所提出的指标。\\n\\n**关键创新**：该方法最重要的创新点在于利用PH理论建立阻抗控制系统的模型，并基于该模型推导出可微的、与力/力矩传感无关的无源性条件和阻抗保真度指标。与传统的评估方法相比，该方法更加系统化，并且能够保证控制系统的安全性。此外，该方法不依赖于力/力矩传感器，因此更适用于实际机器人系统。\\n\\n**关键设计**：在建立PH模型时，需要选择合适的能量函数和互联矩阵。在推导无源性条件时，需要利用PH系统的性质，例如能量守恒和耗散。在定义阻抗保真度指标时，需要选择合适的阶跃响应功率作为评估标准。这些参数的选择会影响评估结果的准确性和可靠性。",
            "application_zh": "该研究成果可应用于机器人阻抗控制器的设计、评估和优化。通过使用该方法，可以更加客观地比较不同阻抗控制器的性能，并选择最适合特定任务的控制器。此外，该方法还可以用于开发更加安全可靠的机器人系统，尤其是在人机协作等场景中。未来，该方法可以扩展到更复杂的机器人系统和控制策略中。",
            "highlight_zh": "通过Gazebo仿真，验证了所提出的PH-based指标的有效性。结果表明，该指标能够准确地评估不同阻抗控制器的性能，并且能够捕获系统的动态解耦特性。该方法为标准化阻抗控制基准测试提供了一种新的思路和工具，有助于推动阻抗控制技术的发展。",
            "tags_zh": [
                "阻抗控制",
                "Port-Hamiltonian系统",
                "基准测试",
                "机器人控制",
                "无源性",
                "动态解耦",
                "仿真验证"
            ],
            "_index": 316,
            "_used_api": "gemini"
        },
        {
            "title": "Beyond Model Jailbreak: Systematic Dissection of the \"Ten DeadlySins\" in Embodied Intelligence",
            "authors": [
                "Yuhang Huang",
                "Junchao Li",
                "Boyang Ma",
                "Xuelong Dai",
                "Minghui Xu",
                "Kaidi Xu",
                "Yue Zhang",
                "Jianping Wang",
                "Xiuzhen Cheng"
            ],
            "arxiv_id": "2512.06387v1",
            "summary": "Embodied AI systems integrate language models with real world sensing, mobility, and cloud connected mobile apps. Yet while model jailbreaks have drawn significant attention, the broader system stack of embodied intelligence remains largely unexplored. In this work, we conduct the first holistic security analysis of the Unitree Go2 platform and uncover ten cross layer vulnerabilities the \"Ten Sins of Embodied AI Security.\" Using BLE sniffing, traffic interception, APK reverse engineering, cloud API testing, and hardware probing, we identify systemic weaknesses across three architectural layers: wireless provisioning, core modules, and external interfaces. These include hard coded keys, predictable handshake tokens, WiFi credential leakage, missing TLS validation, static SSH password, multilingual safety bypass behavior, insecure local relay channels, weak binding logic, and unrestricted firmware access. Together, they allow adversaries to hijack devices, inject arbitrary commands, extract sensitive information, or gain full physical control.Our findings show that securing embodied AI requires far more than aligning the model itself. We conclude with system level lessons learned and recommendations for building embodied platforms that remain robust across their entire software hardware ecosystem.",
            "categories": [
                "cs.CR",
                "cs.RO"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-06",
            "updated": "2025-12-06",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06387v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "unitree"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "揭示具身智能“十大原罪”：对Unitree Go2平台进行系统性安全剖析",
            "summary_zh": "具身AI系统集成了语言模型与现实世界的感知、移动性和云连接移动应用。尽管模型越狱已引起广泛关注，但具身智能的更广泛系统堆栈仍未被充分探索。本文对Unitree Go2平台进行了首次全面的安全分析，揭示了“具身AI安全十大原罪”的跨层漏洞。通过BLE嗅探、流量拦截、APK逆向工程、云API测试和硬件探测，我们识别了跨越三个架构层的系统性弱点：无线配置、核心模块和外部接口。这些弱点包括硬编码密钥、可预测的握手令牌、WiFi凭据泄露、缺少TLS验证、静态SSH密码、多语言安全绕过行为、不安全的本地中继通道、弱绑定逻辑和不受限制的固件访问。这些漏洞共同允许攻击者劫持设备、注入任意命令、提取敏感信息或获得完全的物理控制。我们的研究结果表明，保护具身AI需要的远不止对齐模型本身。最后，我们总结了系统层面的经验教训，并为构建在整个软硬件生态系统中保持稳健的具身平台提出了建议。",
            "intro_zh": [
                "现有具身智能系统安全研究不足，缺乏对整个系统堆栈的全面分析，容易遭受跨层攻击。",
                "通过多维度安全分析方法，系统性地识别了Unitree Go2平台在无线配置、核心模块和外部接口三个架构层面的安全漏洞。",
                "发现了包括硬编码密钥、WiFi凭据泄露等十个关键安全漏洞，证明了现有具身智能系统面临严重的安全威胁。"
            ],
            "method_zh": "**问题定义**：现有具身智能系统，如Unitree Go2，虽然集成了先进的语言模型和感知能力，但其安全性评估主要集中在模型层面，忽略了整个系统堆栈的潜在漏洞。这导致无线配置、核心模块和外部接口等关键组件容易受到攻击，威胁设备安全和用户隐私。现有方法缺乏对这些跨层漏洞的系统性分析和有效防御机制。\\n\\n**核心思路**：本文的核心思路是对具身智能系统进行全面的安全剖析，从无线配置、核心模块和外部接口三个架构层入手，模拟攻击者的视角，采用多种安全分析技术，挖掘潜在的安全漏洞。通过揭示这些漏洞，强调具身智能安全不仅仅是模型安全，而是整个系统堆栈的安全。\\n\\n**技术框架**：本文采用多维度安全分析框架，包括：1) 无线配置层：使用BLE嗅探和流量拦截技术，分析设备配网过程中的安全隐患。2) 核心模块层：通过APK逆向工程和硬件探测，分析核心模块的漏洞，如硬编码密钥和静态密码。3) 外部接口层：通过云API测试，分析云端接口的安全问题，如缺少TLS验证和弱绑定逻辑。通过这些技术手段，全面评估系统的安全性。\\n\\n**关键创新**：本文最重要的技术创新在于对具身智能系统进行跨层安全分析，打破了以往只关注模型安全的局限。通过综合运用多种安全分析技术，揭示了隐藏在系统各个层面的安全漏洞，为具身智能系统的安全防护提供了新的视角和方法。\\n\\n**关键设计**：在无线配置层，通过BLE嗅探分析配网过程中的握手令牌，发现其可预测性。在核心模块层，通过APK逆向工程，发现硬编码的SSH密码和API密钥。在外部接口层，通过云API测试，发现缺少TLS验证和弱绑定逻辑。这些发现都依赖于对系统细节的深入分析和理解。",
            "application_zh": "该研究成果可应用于提升具身智能机器人的安全性，例如服务型机器人、巡检机器人等。通过识别和修复这些安全漏洞，可以有效防止设备被恶意控制、数据泄露等安全事件的发生，保障用户隐私和财产安全。该研究也为其他具身智能系统的安全设计提供了参考，促进整个行业安全水平的提升。",
            "highlight_zh": "该研究成功识别了Unitree Go2平台上的“十大原罪”，包括硬编码密钥、WiFi凭据泄露、缺少TLS验证等严重安全漏洞。通过这些漏洞，攻击者可以实现设备劫持、命令注入和敏感信息窃取等恶意行为。这些发现充分证明了现有具身智能系统在安全性方面存在严重不足，需要引起高度重视。",
            "tags_zh": [
                "具身智能",
                "安全分析",
                "漏洞挖掘",
                "机器人安全",
                "系统安全"
            ],
            "_index": 317,
            "_used_api": "gemini"
        },
        {
            "title": "RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension",
            "authors": [
                "Tianyi Gao",
                "Hao Li",
                "Han Fang",
                "Xin Wei",
                "Xiaodong Dong",
                "Hongbo Sun",
                "Ye Yuan",
                "Zhongjiang He",
                "Jinglin Xu",
                "Jingmin Xin",
                "Hao Sun"
            ],
            "arxiv_id": "2512.06276v2",
            "summary": "Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-06",
            "updated": "2025-12-13",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06276v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出RefBench-PRO基准，用于评估多模态大模型在指代表达理解中的感知和推理能力。",
            "summary_zh": "指代表达理解（REC）是一项视觉-语言任务，旨在根据文本描述定位特定的图像区域。现有的REC基准主要评估感知能力，缺乏可解释的评分机制，无法揭示多模态大型语言模型（MLLM）在不同认知能力上的基础能力。为了解决这一局限性，我们引入了RefBench-PRO，这是一个全面的REC基准，它将指代表达分解为两个核心维度，即感知和推理，并进一步细分为六个渐进式挑战任务，如属性、位置、交互、常识、关系和拒绝。我们还开发了一个全自动的数据生成管道，用于生成跨这六个子维度的多样化指代表达。此外，我们提出了Ref-R1，一种基于RL的学习方案，它结合了基于动态IoU的GRPO，以提高在日益复杂的推理条件下的定位精度，为REC建立更强的基线。大量的实验表明，我们的RefBench-PRO能够对MLLM在指代表达理解方面进行可解释的评估，在感知和推理方面都提出了更大的挑战。",
            "intro_zh": [
                "现有REC基准侧重感知能力评估，缺乏对多模态大模型推理能力的针对性评估和可解释的评分机制。",
                "RefBench-PRO基准将指代表达理解分解为感知和推理两个维度，并细分为六个更具挑战性的子任务。",
                "提出了Ref-R1学习方案，通过结合动态IoU的GRPO，提升了在复杂推理条件下的定位精度，并建立了更强的基线。"
            ],
            "method_zh": "**问题定义**：论文旨在解决现有指代表达理解（REC）基准的不足，即主要侧重于感知能力的评估，而忽略了多模态大型语言模型（MLLM）的推理能力。现有方法难以对MLLM在不同认知能力上的基础能力进行有效评估，缺乏可解释的评分机制。\\n\\n**核心思路**：论文的核心思路是将指代表达理解任务分解为感知和推理两个核心维度，并进一步细分为六个具有递进难度的子任务。通过这种分解，可以更精细地评估MLLM在不同认知能力上的表现，并提供更具解释性的评估结果。\\n\\n**技术框架**：RefBench-PRO基准包含一个全自动的数据生成管道，用于生成多样化的指代表达，涵盖属性、位置、交互、常识、关系和拒绝六个子维度。此外，论文还提出了Ref-R1学习方案，该方案基于强化学习，并结合了动态IoU的GRPO（未知具体含义），以提高在复杂推理条件下的定位精度。整体流程包括数据生成、模型训练和评估三个主要阶段。\\n\\n**关键创新**：论文的关键创新在于提出了RefBench-PRO基准，该基准能够对MLLM在指代表达理解中的感知和推理能力进行更全面、更精细的评估。与现有基准相比，RefBench-PRO更注重推理能力的评估，并提供了更具解释性的评估结果。此外，Ref-R1学习方案的引入也为REC任务提供了一种新的解决思路。\\n\\n**关键设计**：关于数据生成管道的具体实现细节、动态IoU-based GRPO的具体算法细节、强化学习的奖励函数设计、以及网络结构的具体参数设置等技术细节，论文摘要中未提供详细信息，因此无法进行深入描述。这些细节可能在论文正文中有所阐述。",
            "application_zh": "该研究成果可应用于智能机器人、自动驾驶、图像搜索等领域。通过提升多模态大模型在指代表达理解方面的能力，可以使机器更好地理解人类指令，从而实现更智能的人机交互和更精准的目标定位。未来，该研究有望推动视觉-语言智能的发展，并为相关应用带来更广阔的前景。",
            "highlight_zh": "实验结果表明，RefBench-PRO基准能够有效评估MLLM在指代表达理解中的感知和推理能力，并对现有模型提出了更大的挑战。Ref-R1学习方案在复杂推理条件下显著提高了定位精度，为REC任务建立了一个更强的基线。具体的性能数据和提升幅度需要在论文正文中查找。",
            "tags_zh": [
                "指代表达理解",
                "多模态学习",
                "视觉语言",
                "基准测试",
                "推理能力",
                "感知能力",
                "强化学习"
            ],
            "_index": 318,
            "_used_api": "gemini"
        },
        {
            "title": "Where to Fly, What to Send: Communication-Aware Aerial Support for Ground Robots",
            "authors": [
                "Harshil Suthar",
                "Dipankar Maity"
            ],
            "arxiv_id": "2512.06207v1",
            "summary": "In this work we consider a multi-robot team operating in an unknown environment where one aerial agent is tasked to map the environment and transmit (a portion of) the mapped environment to a group of ground agents that are trying to reach their goals. The entire operation takes place over a bandwidth-limited communication channel, which motivates the problem of determining what and how much information the assisting agent should transmit and when while simultaneously performing exploration/mapping. The proposed framework enables the assisting aerial agent to decide what information to transmit based on the Value-of-Information (VoI), how much to transmit using a Mixed-Integer Linear Programming (MILP), and how to acquire additional information through an utility score-based environment exploration strategy. We perform a communication-motion trade-off analysis between the total amount of map data communicated by the aerial agent and the navigation cost incurred by the ground agents.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "Submitted to conference",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06207v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "navigation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出通信感知的无人机辅助地面机器人框架，解决带宽受限环境下的信息传输与探索问题",
            "summary_zh": "本文研究了一个多机器人团队在未知环境中协同工作的问题，其中一架无人机负责绘制环境地图，并将部分地图信息传输给一组试图到达目标的地面机器人。整个操作在一个带宽受限的通信信道上进行，这促使我们研究如何确定辅助无人机应该传输什么信息、传输多少信息以及何时传输，同时执行探索/绘图任务。所提出的框架使辅助无人机能够根据信息价值（VoI）决定传输什么信息，使用混合整数线性规划（MILP）决定传输多少信息，并通过基于效用评分的环境探索策略获取更多信息。我们对无人机传输的总地图数据量与地面机器人产生的导航成本之间进行了通信-运动权衡分析。",
            "intro_zh": [
                "现有方法在带宽受限环境下，难以有效平衡无人机的信息传输与环境探索任务。",
                "提出基于信息价值的传输决策、基于MILP的传输量优化以及基于效用评分的探索策略。",
                "通过通信-运动权衡分析，优化无人机的信息传输策略，降低地面机器人的导航成本。"
            ],
            "method_zh": "**问题定义**：论文旨在解决在带宽受限的通信环境下，如何使无人机有效地辅助地面机器人完成导航任务。现有方法通常没有充分考虑通信带宽的限制，或者无法在信息传输、环境探索和地面机器人导航之间进行有效的权衡，导致地面机器人导航效率低下。\\n\\n**核心思路**：论文的核心思路是让无人机根据信息价值（Value-of-Information, VoI）来决定传输哪些地图信息，并使用混合整数线性规划（Mixed-Integer Linear Programming, MILP）来优化传输的数据量。同时，无人机通过基于效用评分的探索策略来获取更多有价值的环境信息，从而更好地辅助地面机器人。\\n\\n**技术框架**：该框架包含以下几个主要模块：1) 环境探索模块：无人机使用基于效用评分的策略探索未知环境，构建环境地图。2) 信息价值评估模块：评估地图上不同区域的信息价值，即传输这些信息对地面机器人导航的潜在收益。3) 传输量优化模块：使用MILP优化无人机传输的地图数据量，在带宽限制下最大化地面机器人的导航性能。4) 地面机器人导航模块：地面机器人接收无人机传输的地图信息，并利用这些信息进行路径规划和导航。\\n\\n**关键创新**：该论文的关键创新在于将信息价值的概念引入到无人机辅助地面机器人导航的问题中，并提出了一种基于MILP的传输量优化方法。这种方法能够有效地在带宽限制下选择最有价值的信息进行传输，从而提高地面机器人的导航效率。与现有方法相比，该方法更加注重通信的效率和信息的价值，能够更好地适应带宽受限的环境。\\n\\n**关键设计**：信息价值的计算可能涉及到地面机器人的目标位置、当前位置以及地图上的障碍物信息。效用评分函数的设计需要考虑探索区域的不确定性和潜在的信息增益。MILP模型的构建需要合理地定义目标函数和约束条件，以保证优化结果的有效性和可行性。具体的参数设置和损失函数细节未知。",
            "application_zh": "该研究成果可应用于灾难救援、搜寻任务、农业监测等领域。在这些场景中，地面机器人需要在未知或复杂环境中执行任务，而无人机可以提供环境感知和通信支持。通过优化无人机的信息传输策略，可以提高地面机器人的导航效率和任务完成能力，从而提升整体系统的性能。",
            "highlight_zh": "论文通过实验分析了无人机传输的地图数据量与地面机器人导航成本之间的权衡关系。具体的性能数据、对比基线和提升幅度未知，但实验结果验证了所提出的框架能够有效地提高地面机器人的导航效率，并降低其导航成本。",
            "tags_zh": [
                "无人机辅助",
                "地面机器人",
                "通信受限",
                "信息价值",
                "混合整数线性规划"
            ],
            "_index": 319,
            "_used_api": "gemini"
        },
        {
            "title": "Physics-Grounded Attached Shadow Detection Using Approximate 3D Geometry and Light Direction",
            "authors": [
                "Shilin Hu",
                "Jingyi Xu",
                "Sagnik Das",
                "Dimitris Samaras",
                "Hieu Le"
            ],
            "arxiv_id": "2512.06179v1",
            "summary": "Attached shadows occur on the surface of the occluder where light cannot reach because of self-occlusion. They are crucial for defining the three-dimensional structure of objects and enhancing scene understanding. Yet existing shadow detection methods mainly target cast shadows, and there are no dedicated datasets or models for detecting attached shadows. To address this gap, we introduce a framework that jointly detects cast and attached shadows by reasoning about their mutual relationship with scene illumination and geometry. Our system consists of a shadow detection module that predicts both shadow types separately, and a light estimation module that infers the light direction from the detected shadows. The estimated light direction, combined with surface normals, allows us to derive a geometry-consistent partial map that identifies regions likely to be self-occluded. This partial map is then fed back to refine shadow predictions, forming a closed-loop reasoning process that iteratively improves both shadow segmentation and light estimation. In order to train our method, we have constructed a dataset of 1,458 images with separate annotations for cast and attached shadows, enabling training and quantitative evaluation of both. Experimental results demonstrate that this iterative geometry-illumination reasoning substantially improves the detection of attached shadows, with at least 33% BER reduction, while maintaining strong full and cast shadow performance.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06179v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "scene understanding"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于近似3D几何和光照方向的物理约束阴影检测方法",
            "summary_zh": "本文提出了一种联合检测依附阴影和投射阴影的框架，通过推理场景光照和几何体的相互关系来实现。该系统包含一个阴影检测模块，分别预测两种阴影类型，以及一个光照估计模块，从检测到的阴影中推断光照方向。估计的光照方向与表面法线相结合，可以推导出几何一致的部分地图，识别可能发生自遮挡的区域。该部分地图被反馈以细化阴影预测，形成一个闭环推理过程，迭代地改进阴影分割和光照估计。为了训练该方法，构建了一个包含1458张图像的数据集，分别标注了投射阴影和依附阴影，从而能够对两者进行训练和定量评估。实验结果表明，这种迭代的几何-光照推理显著提高了依附阴影的检测，BER降低至少33%，同时保持了强大的完整阴影和投射阴影性能。",
            "intro_zh": [
                "现有阴影检测方法主要关注投射阴影，忽略了依附阴影，缺乏针对依附阴影的专用数据集和模型。",
                "该论文提出了一种联合检测投射阴影和依附阴影的框架，通过场景光照和几何体的相互关系进行推理。",
                "实验结果表明，该方法通过迭代的几何-光照推理，显著提高了依附阴影的检测性能，BER降低至少33%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决依附阴影检测问题。现有阴影检测方法主要集中于投射阴影，忽略了依附阴影的重要性，并且缺乏专门用于依附阴影检测的数据集和模型。这导致现有方法在理解场景三维结构和进行更高级的场景理解方面存在局限性。\\n\\n**核心思路**：论文的核心思路是利用场景的几何信息和光照信息之间的相互关系来提高依附阴影的检测精度。通过迭代地估计光照方向，并结合表面法线信息，生成一个几何一致的部分地图，用于指导阴影检测，从而实现更准确的依附阴影分割。\\n\\n**技术框架**：该方法包含两个主要模块：阴影检测模块和光照估计模块。阴影检测模块负责分别预测投射阴影和依附阴影。光照估计模块从检测到的阴影中推断光照方向。估计的光照方向与表面法线结合，生成几何一致的部分地图。该部分地图被反馈到阴影检测模块，用于细化阴影预测，形成一个闭环推理过程。\\n\\n**关键创新**：该方法最重要的创新点在于利用几何信息和光照信息之间的相互约束关系，通过迭代推理来提高依附阴影的检测精度。与现有方法相比，该方法不仅考虑了阴影的外观特征，还考虑了阴影与场景几何结构之间的物理关系，从而能够更准确地检测依附阴影。\\n\\n**关键设计**：该方法使用了一个包含1458张图像的数据集进行训练，该数据集分别标注了投射阴影和依附阴影。损失函数的设计需要同时考虑投射阴影和依附阴影的检测精度，以及光照估计的准确性。具体的网络结构和参数设置在论文中没有详细描述，属于未知信息。",
            "application_zh": "该研究成果可应用于机器人视觉、自动驾驶、三维重建等领域。准确的阴影检测能够帮助机器人更好地理解周围环境，提高自动驾驶系统的安全性，并改善三维重建的质量。该方法还有潜力应用于图像编辑和增强现实等领域，例如，可以用于在图像中添加或修改阴影，以增强图像的真实感。",
            "highlight_zh": "实验结果表明，该方法在依附阴影检测方面取得了显著的提升，BER（贝叶斯错误率）降低了至少33%。同时，该方法在投射阴影和完整阴影的检测方面也保持了良好的性能。这些结果表明，通过迭代的几何-光照推理，可以有效地提高阴影检测的准确性。",
            "tags_zh": [
                "依附阴影检测",
                "投射阴影检测",
                "光照估计",
                "几何推理",
                "阴影分割"
            ],
            "_index": 320,
            "_used_api": "gemini"
        },
        {
            "title": "Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation",
            "authors": [
                "Su Sun",
                "Cheng Zhao",
                "Himangi Mittal",
                "Gaurav Mittal",
                "Rohith Kukkala",
                "Yingjie Victor Chen",
                "Mei Chen"
            ],
            "arxiv_id": "2512.06158v1",
            "summary": "Generating dynamic 4D objects from sparse inputs is difficult because it demands joint preservation of appearance and motion coherence across views and time while suppressing artifacts and temporal drift. We hypothesize that the view discrepancy arises from supervision limited to pixel- or latent-space video-diffusion losses, which lack explicitly temporally aware, feature-level tracking guidance. We present \\emph{Track4DGen}, a two-stage framework that couples a multi-view video diffusion model with a foundation point tracker and a hybrid 4D Gaussian Splatting (4D-GS) reconstructor. The central idea is to explicitly inject tracker-derived motion priors into intermediate feature representations for both multi-view video generation and 4D-GS. In Stage One, we enforce dense, feature-level point correspondences inside the diffusion generator, producing temporally consistent features that curb appearance drift and enhance cross-view coherence. In Stage Two, we reconstruct a dynamic 4D-GS using a hybrid motion encoding that concatenates co-located diffusion features (carrying Stage-One tracking priors) with Hex-plane features, and augment them with 4D Spherical Harmonics for higher-fidelity dynamics modeling. \\emph{Track4DGen} surpasses baselines on both multi-view video generation and 4D generation benchmarks, yielding temporally stable, text-editable 4D assets. Lastly, we curate \\emph{Sketchfab28}, a high-quality dataset for benchmarking object-centric 4D generation and fostering future research.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "15 pages, 11 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06158v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "gaussian splatting"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出Track4DGen，利用跟踪引导的运动先验实现高质量3D模型动画生成。",
            "summary_zh": "从稀疏输入生成动态4D对象极具挑战，它需要在跨视角和时间上联合保持外观和运动一致性，同时抑制伪影和时间漂移。我们认为，视角差异源于仅限于像素或潜在空间视频扩散损失的监督，这些损失缺乏显式的时间感知、特征级别的跟踪指导。我们提出了Track4DGen，这是一个两阶段框架，它将多视角视频扩散模型与基础点跟踪器和混合4D高斯溅射(4D-GS)重建器相结合。核心思想是将跟踪器导出的运动先验显式地注入到多视角视频生成和4D-GS的中间特征表示中。在第一阶段，我们在扩散生成器内部强制执行密集的特征级别点对应关系，从而产生时间上一致的特征，抑制外观漂移并增强跨视角一致性。在第二阶段，我们使用混合运动编码重建动态4D-GS，该编码将共定位的扩散特征(携带第一阶段跟踪先验)与Hex-plane特征连接起来，并使用4D球谐函数对其进行增强，以实现更高保真度的动态建模。Track4DGen在多视角视频生成和4D生成基准测试中均优于基线，从而产生时间稳定的、文本可编辑的4D资产。最后，我们策划了Sketchfab28，这是一个高质量的数据集，用于基准测试以对象为中心的4D生成并促进未来的研究。",
            "intro_zh": [
                "现有方法在从稀疏输入生成动态4D对象时，难以同时保证外观和运动的一致性，并容易出现时间漂移。",
                "Track4DGen通过将跟踪器导出的运动先验注入到多视角视频生成和4D-GS的中间特征表示中，显式地利用时间信息。",
                "Track4DGen在多视角视频生成和4D生成任务上超越了现有基线方法，生成了时间稳定的、可编辑的4D模型。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从稀疏输入生成高质量、时间一致的动态4D对象的问题。现有方法主要依赖于像素或潜在空间的视频扩散损失，缺乏显式的时间感知和特征级别的跟踪指导，导致生成结果在视角和时间上不一致，容易出现伪影和时间漂移。\\n\\n**核心思路**：论文的核心思路是将跟踪器导出的运动先验显式地注入到多视角视频生成和4D-GS重建过程中。通过在特征级别引入时间一致性约束，可以有效地抑制外观漂移，增强跨视角一致性，从而生成更稳定、更真实的动态4D对象。\\n\\n**技术框架**：Track4DGen是一个两阶段框架：\n1. **多视角视频生成阶段**：利用多视角视频扩散模型，并强制执行密集的特征级别点对应关系，生成时间一致的特征。\n2. **4D-GS重建阶段**：使用混合运动编码重建动态4D-GS，将扩散特征（携带跟踪先验）与Hex-plane特征连接，并使用4D球谐函数增强动态建模能力。\\n\\n**关键创新**：该方法最重要的创新点在于将基础点跟踪器与扩散模型相结合，显式地将跟踪信息作为运动先验注入到特征表示中。这与现有方法仅依赖于像素或潜在空间的损失函数进行监督有本质区别，能够更好地保证生成结果的时间一致性和跨视角一致性。\\n\\n**关键设计**：\n*   **混合运动编码**：结合扩散特征和Hex-plane特征，充分利用跟踪先验和几何信息。\n*   **4D球谐函数**：用于更高保真度的动态建模。\n*   **Sketchfab28数据集**：用于评估和比较4D生成方法的性能。",
            "application_zh": "Track4DGen在游戏开发、电影制作、虚拟现实/增强现实等领域具有广泛的应用前景。它可以用于快速生成高质量的动态3D模型，例如动画角色、运动物体等，从而降低内容创作的成本和时间。此外，该方法还可以用于从视频中重建动态场景，为三维重建和场景理解提供新的思路。",
            "highlight_zh": "Track4DGen在多视角视频生成和4D生成基准测试中均超越了现有基线方法，生成了时间稳定的、文本可编辑的4D模型。论文还贡献了一个高质量的4D数据集Sketchfab28，为未来的研究提供了基准。",
            "tags_zh": [
                "4D生成",
                "动态3D模型",
                "运动跟踪",
                "扩散模型",
                "高斯溅射",
                "时间一致性",
                "多视角视频"
            ],
            "_index": 321,
            "_used_api": "gemini"
        },
        {
            "title": "Probabilistic Weapon Engagement Zones for a Turn Constrained Pursuer",
            "authors": [
                "Grant Stagg",
                "Isaac E. Weintraub",
                "Cameron K. Peterson"
            ],
            "arxiv_id": "2512.06130v1",
            "summary": "Curve-straight probabilistic engagement zones (CSPEZ) quantify the spatial regions an evader should avoid to reduce capture risk from a turn-rate-limited pursuer following a curve-straight path with uncertain parameters including position, heading, velocity, range, and maximum turn rate. This paper presents methods for generating evader trajectories that minimize capture risk under such uncertainty. We first derive an analytic solution for the deterministic curve-straight basic engagement zone (CSBEZ), then extend this formulation to a probabilistic framework using four uncertainty-propagation approaches: Monte Carlo sampling, linearization, quadratic approximation, and neural-network regression. We evaluate the accuracy and computational cost of each approximation method and demonstrate how CSPEZ constraints can be integrated into a trajectory-optimization algorithm to produce safe paths that explicitly account for pursuer uncertainty.",
            "categories": [
                "cs.RO",
                "eess.SY"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "Accepted for presentation at AIAA SciTech 2026. 17 pages, 7 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06130v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "trajectory optimization"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "针对转弯受限追击者，提出概率武器交战区(CSPEZ)方法，优化规避轨迹。",
            "summary_zh": "本文提出了一种曲线-直线概率交战区(CSPEZ)方法，用于量化规避者应避免的空间区域，以降低受到转弯速率受限的追击者捕获的风险。追击者遵循具有不确定参数（包括位置、航向、速度、范围和最大转弯速率）的曲线-直线路径。本文提出了生成规避者轨迹的方法，该轨迹可以最小化此类不确定性下的捕获风险。我们首先推导出确定性曲线-直线基本交战区(CSBEZ)的解析解，然后使用四种不确定性传播方法将其扩展到概率框架：蒙特卡罗抽样、线性化、二次近似和神经网络回归。我们评估了每种近似方法的准确性和计算成本，并演示了如何将CSPEZ约束集成到轨迹优化算法中，以生成明确考虑追击者不确定性的安全路径。",
            "intro_zh": [
                "现有方法难以在追击者参数不确定情况下，有效评估规避者的安全区域，从而影响规避策略的制定。",
                "论文核心在于构建曲线-直线概率交战区(CSPEZ)，通过考虑追击者参数的不确定性，量化规避风险。",
                "实验对比了蒙特卡罗抽样、线性化、二次近似和神经网络回归等不确定性传播方法，并验证了CSPEZ在轨迹优化中的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决在追击者具有转弯速率限制，且其位置、速度、航向等参数存在不确定性的情况下，如何确定规避者应该避免的区域，从而降低被捕获的风险。现有方法通常假设追击者参数是确定的，无法有效应对实际场景中的不确定性，导致规避策略失效。\\n\\n**核心思路**：论文的核心思路是将确定性的曲线-直线基本交战区(CSBEZ)扩展到概率框架，即CSPEZ。通过考虑追击者参数的不确定性，计算规避者在不同位置被捕获的概率，从而量化规避风险。这种方法能够更准确地评估规避者的安全区域，并指导其制定更有效的规避策略。\\n\\n**技术框架**：整体框架包括以下几个主要步骤：1) 推导确定性CSBEZ的解析解；2) 使用四种不确定性传播方法（蒙特卡罗抽样、线性化、二次近似和神经网络回归）将CSBEZ扩展到概率框架，得到CSPEZ；3) 评估不同不确定性传播方法的准确性和计算成本；4) 将CSPEZ约束集成到轨迹优化算法中，生成安全路径。\\n\\n**关键创新**：最重要的技术创新点在于将确定性的交战区概念扩展到概率域，从而能够处理追击者参数的不确定性。此外，论文还比较了多种不确定性传播方法，并评估了它们在CSPEZ构建中的性能。与现有方法相比，CSPEZ能够更准确地评估规避风险，并生成更安全的规避轨迹。\\n\\n**关键设计**：论文的关键设计包括：1) CSBEZ的解析解推导，这为后续的概率扩展奠定了基础；2) 四种不确定性传播方法的选择和实现，这些方法各有优缺点，适用于不同的场景；3) 将CSPEZ约束集成到轨迹优化算法中的方法，这使得生成的轨迹能够显式地考虑追击者的不确定性。具体的参数设置和损失函数等技术细节在论文中进行了详细描述，但在此处无法完全展开。",
            "application_zh": "该研究成果可应用于无人机避障、自主导航、机器人运动规划等领域。通过考虑潜在威胁的不确定性，可以提高自主系统的安全性和鲁棒性。例如，在无人机集群飞行中，可以利用CSPEZ方法规划无人机的安全飞行轨迹，避免与其他无人机或障碍物发生碰撞。未来，该方法还可以扩展到更复杂的场景，如多智能体协同避障等。",
            "highlight_zh": "论文通过实验对比了四种不确定性传播方法在构建CSPEZ时的准确性和计算成本。实验结果表明，不同的方法在不同的场景下具有不同的性能。例如，蒙特卡罗抽样方法虽然准确，但计算成本较高；线性化方法计算速度快，但准确性较低。此外，论文还验证了将CSPEZ约束集成到轨迹优化算法中可以有效提高规避轨迹的安全性。",
            "tags_zh": [
                "概率交战区",
                "轨迹优化",
                "不确定性传播",
                "追逃博弈",
                "运动规划"
            ],
            "_index": 322,
            "_used_api": "gemini"
        },
        {
            "title": "Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light",
            "authors": [
                "Tzofi Klinghoffer",
                "Siddharth Somasundaram",
                "Xiaoyu Xiang",
                "Yuchen Fan",
                "Christian Richardt",
                "Akshat Dave",
                "Ramesh Raskar",
                "Rakesh Ranjan"
            ],
            "arxiv_id": "2512.06080v1",
            "summary": "3D scene reconstruction from a single measurement is challenging, especially in the presence of occluded regions and specular materials, such as mirrors. We address these challenges by leveraging single-photon lidars. These lidars estimate depth from light that is emitted into the scene and reflected directly back to the sensor. However, they can also measure light that bounces multiple times in the scene before reaching the sensor. This multi-bounce light contains additional information that can be used to recover dense depth, occluded geometry, and material properties. Prior work with single-photon lidar, however, has only demonstrated these use cases when a laser sequentially illuminates one scene point at a time. We instead focus on the more practical - and challenging - scenario of illuminating multiple scene points simultaneously. The complexity of light transport due to the combined effects of multiplexed illumination, two-bounce light, shadows, and specular reflections is challenging to invert analytically. Instead, we propose a data-driven method to invert light transport in single-photon lidar. To enable this approach, we create the first large-scale simulated dataset of ~100k lidar transients for indoor scenes. We use this dataset to learn a prior on complex light transport, enabling measured two-bounce light to be decomposed into the constituent contributions from each laser spot. Finally, we experimentally demonstrate how this decomposed light can be used to infer 3D geometry in scenes with occlusions and mirrors from a single measurement. Our code and dataset are released at https://shoot-bounce-3d.github.io.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "SIGGRAPH Asia 2025. Project page: https://shoot-bounce-3d.github.io",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06080v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "scene reconstruction"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "Shoot-Bounce-3D：利用单光子激光雷达和双次反射光进行遮挡感知的三维重建",
            "summary_zh": "本文提出了一种利用单光子激光雷达进行三维场景重建的方法，尤其是在存在遮挡区域和镜面反射材料的情况下。该方法利用激光雷达发射到场景中并直接反射回传感器的光来估计深度。此外，它还利用在场景中多次反射后到达传感器的多重反射光。这种多重反射光包含可用于恢复密集深度、遮挡几何形状和材料属性的额外信息。与以往单光子激光雷达逐点扫描的方法不同，本文关注更具挑战性的同时照射多个场景点的场景。由于多路复用照明、双次反射光、阴影和镜面反射的综合影响，光传输的复杂性难以进行解析反演。因此，本文提出了一种数据驱动的方法来反演单光子激光雷达中的光传输。为了实现这种方法，本文创建了第一个大规模的室内场景激光雷达瞬态模拟数据集（约10万个）。利用该数据集学习复杂光传输的先验知识，从而将测量的双次反射光分解为来自每个激光点的组成部分。最后，实验证明了如何利用这种分解的光来推断具有遮挡和镜子的场景中的三维几何形状。",
            "intro_zh": [
                "现有单光子激光雷达在复杂场景下的三维重建，尤其是在遮挡和镜面反射存在时，面临光路复杂难以解析的问题。",
                "提出Shoot-Bounce-3D，利用单光子激光雷达获取的双次反射光信息，通过数据驱动方法学习光传输先验，分解多重反射光。",
                "通过大规模模拟数据集训练，实验证明该方法能够有效推断遮挡和镜面场景中的三维几何形状，实现单次测量重建。"
            ],
            "method_zh": "**问题定义**：论文旨在解决单次测量下单光子激光雷达三维重建中，由于遮挡和镜面反射导致光路复杂，难以准确重建场景几何的问题。现有方法通常依赖逐点扫描，效率较低，且难以处理复杂光路带来的干扰。\\n\\n**核心思路**：论文的核心思路是利用单光子激光雷达能够捕获的双次反射光信息，这些信息包含了场景中被遮挡区域和镜面反射的信息。通过学习光传输的先验知识，将复杂的双次反射光分解为各个激光点贡献的组成部分，从而推断出场景的完整几何信息。\\n\\n**技术框架**：整体框架包含以下几个主要阶段：1) 大规模模拟数据集生成：创建包含各种室内场景和光照条件的激光雷达瞬态数据。2) 光传输先验学习：利用生成的数据集训练神经网络，学习复杂光传输的模式和规律。3) 双次反射光分解：利用训练好的网络，将测量的双次反射光分解为各个激光点对应的贡献。4) 三维几何推断：利用分解后的光信息，推断场景的三维几何形状。\\n\\n**关键创新**：最重要的创新点在于提出了一个数据驱动的方法来反演单光子激光雷达中的光传输。与传统的解析方法相比，该方法能够更好地处理复杂的光路和非线性效应，从而更准确地重建场景几何。此外，大规模模拟数据集的构建也为该方法的训练和验证提供了有力支持。\\n\\n**关键设计**：论文构建了大规模的模拟数据集，包含约10万个激光雷达瞬态数据，涵盖了各种室内场景和光照条件。网络结构方面，具体细节未知，但其目标是学习一个能够将复杂双次反射光分解为各个激光点贡献的模型。损失函数的设计也至关重要，需要能够有效地引导网络学习光传输的先验知识。",
            "application_zh": "该研究成果可应用于自动驾驶、机器人导航、三维场景重建、室内地图构建等领域。尤其是在光线条件复杂、存在遮挡和镜面反射的环境中，该方法能够提供更准确和完整的三维信息，提高系统的感知能力和鲁棒性。未来，该技术有望进一步拓展到文物保护、医疗诊断等领域。",
            "highlight_zh": "论文通过实验验证了该方法在遮挡和镜面反射场景下的三维重建能力。实验结果表明，该方法能够有效地分解双次反射光，并准确地推断出被遮挡区域和镜面的几何形状。虽然论文中没有给出具体的性能指标，但实验结果直观地展示了该方法在复杂场景下的优势。",
            "tags_zh": [
                "单光子激光雷达",
                "三维重建",
                "遮挡感知",
                "双次反射光",
                "光传输反演"
            ],
            "_index": 323,
            "_used_api": "gemini"
        },
        {
            "title": "EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing",
            "authors": [
                "Runjia Li",
                "Moayed Haji-Ali",
                "Ashkan Mirzaei",
                "Chaoyang Wang",
                "Arpit Sahni",
                "Ivan Skorokhodov",
                "Aliaksandr Siarohin",
                "Tomas Jakab",
                "Junlin Han",
                "Sergey Tulyakov",
                "Philip Torr",
                "Willi Menapace"
            ],
            "arxiv_id": "2512.06065v1",
            "summary": "We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "Project page: https://snap-research.github.io/EgoEdit",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.06065v1",
            "code_links": [
                {
                    "url": "https://snap-research.github.io/EgoEdit",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "ego-motion"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "EgoEdit：用于第一人称视频编辑的数据集、实时模型与评测基准",
            "summary_zh": "本文研究了交互式AR应用中，指令引导的第一人称视频编辑。虽然现有的AI视频编辑器在第三人称素材上表现良好，但第一人称视角带来了独特的挑战，包括快速的自我运动和频繁的手部-物体交互，造成了显著的领域差距。此外，现有的离线编辑流程延迟较高，限制了实时交互。为了解决这些问题，本文提出了一个完整的第一人称视频编辑生态系统。首先，构建了EgoEditData，一个精心设计和手动策划的数据集，专门用于第一人称编辑场景，具有丰富的手部-物体交互，并明确保留了手部。其次，开发了EgoEdit，一个指令跟随的第一人称视频编辑器，支持在单个GPU上进行实时流推理。最后，引入了EgoEditBench，一个评估套件，针对指令保真度、手部和交互保留以及自我运动下的时间稳定性。在第一人称和通用编辑任务中，EgoEdit产生了时间稳定、指令保真的结果，并具有交互式延迟。它在第一人称编辑基准上取得了明显的优势，而现有方法难以胜任，同时在通用编辑任务上保持了与最强基线相当的性能。EgoEditData和EgoEditBench将向研究社区公开。",
            "intro_zh": [
                "现有视频编辑方法在第一人称视角下，由于快速运动和手部交互，效果不佳。",
                "EgoEdit通过构建数据集、设计实时模型和评测基准，解决第一人称视频编辑难题。",
                "实验表明，EgoEdit在第一人称编辑任务上显著优于现有方法，并保持了实时性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决第一人称视角视频的指令引导编辑问题。现有视频编辑方法主要针对第三人称视角，无法很好地处理第一人称视频中常见的快速自我运动、频繁手部-物体交互等复杂情况，导致编辑效果不佳，且延迟较高，难以满足实时交互需求。\\n\\n**核心思路**：论文的核心思路是构建一个专门针对第一人称视频编辑的完整生态系统，包括数据集、实时模型和评测基准。通过高质量的数据集训练模型，并设计针对性的评估指标，从而提升模型在第一人称视频编辑任务上的性能和鲁棒性。\\n\\n**技术框架**：EgoEdit的整体框架包含三个主要组成部分：EgoEditData数据集、EgoEdit实时编辑模型和EgoEditBench评测基准。EgoEditData提供高质量的第一人称视频数据，用于模型训练。EgoEdit模型基于流式处理架构，支持实时推理。EgoEditBench用于评估模型在指令保真度、手部和交互保留以及时间稳定性等方面的性能。\\n\\n**关键创新**：论文的关键创新在于构建了专门针对第一人称视频编辑的数据集EgoEditData，该数据集包含丰富的手部-物体交互，并明确保留了手部信息。此外，论文还提出了EgoEditBench评测基准，用于全面评估模型在第一人称视频编辑任务上的性能。\\n\\n**关键设计**：EgoEdit模型采用了流式处理架构，以实现实时推理。具体的技术细节，例如网络结构、损失函数等，论文中没有详细描述，属于未知信息。数据集EgoEditData的构建过程中，作者进行了精心的设计和手动策划，以保证数据的质量和多样性。评测基准EgoEditBench则针对第一人称视频编辑的特点，设计了多个评估指标，包括指令保真度、手部和交互保留以及时间稳定性。",
            "application_zh": "该研究成果可应用于增强现实(AR)应用、机器人控制、虚拟现实(VR)内容创作等领域。例如，用户可以通过语音指令实时编辑第一人称视角下的视频，实现虚拟物体的添加、场景的修改等功能。该技术有望提升用户在AR/VR环境中的交互体验，并为机器人提供更智能的视觉感知能力。",
            "highlight_zh": "EgoEdit在第一人称编辑基准上取得了显著的性能提升，超越了现有方法。在通用编辑任务上，EgoEdit保持了与最强基线相当的性能，同时实现了实时推理。EgoEditData和EgoEditBench的发布将为第一人称视频编辑领域的研究提供有力支持。",
            "tags_zh": [
                "第一人称视频编辑",
                "指令引导编辑",
                "实时视频编辑",
                "数据集",
                "评测基准",
                "手部交互",
                "增强现实"
            ],
            "_index": 324,
            "_used_api": "gemini"
        },
        {
            "title": "Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding",
            "authors": [
                "Zhiyuan Jiang",
                "Shenghao Xie",
                "Wenyi Li",
                "Wenqiang Zu",
                "Peihang Li",
                "Jiahao Qiu",
                "Siqi Pei",
                "Lei Ma",
                "Tiejun Huang",
                "Mengdi Wang",
                "Shilong Liu"
            ],
            "arxiv_id": "2512.05941v1",
            "summary": "Grounding is a fundamental capability for building graphical user interface (GUI) agents. Although existing approaches rely on large-scale bounding box supervision, they still face various challenges, such as cross-platform generalization, complex layout analysis, and fine-grained element localization. In this paper, we investigate zoom as a strong yet underexplored prior for GUI grounding, and propose a training-free method, ZoomClick. By characterizing four key properties of zoom (i.e., pre-zoom, depth, shrink size, minimal crop size), we unlock its full capabilities for dynamic spatial focusing and adaptive context switching. Experiments demonstrate that our method significantly boosts the performance of both general vision-language and specialized GUI grounding models, achieving state-of-the-art results on several mainstream benchmarks; for example, UI-Venus-72B attains a 73.1% success rate on ScreenSpot-Pro. Furthermore, we present GUIZoom-Bench, a benchmark for evaluating model adaptability to zoom, aiming to inspire future research on improving zoom for further training and test-time scaling in GUI grounding tasks.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.CL"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "Code is available at https://github.com/Princeton-AI2-Lab/ZoomClick",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05941v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出ZoomClick，利用缩放先验提升GUI界面元素定位性能",
            "summary_zh": "本文研究了缩放操作在GUI界面元素定位中的潜力，提出了一种无需训练的方法ZoomClick。该方法通过刻画缩放的四个关键属性（预缩放、深度、缩小尺寸、最小裁剪尺寸），充分发挥其在动态空间聚焦和自适应上下文切换方面的能力。实验表明，ZoomClick显著提升了通用视觉-语言模型和专用GUI定位模型的性能，在多个主流基准测试中取得了最先进的结果，例如，UI-Venus-72B在ScreenSpot-Pro上达到了73.1%的成功率。此外，本文还提出了GUIZoom-Bench，一个用于评估模型对缩放操作适应性的基准测试，旨在激发未来研究，以改进缩放操作，从而进一步提升GUI定位任务中的训练和测试时性能。",
            "intro_zh": [
                "现有GUI定位方法依赖大规模边界框监督，面临跨平台泛化、复杂布局分析和细粒度元素定位等挑战。",
                "ZoomClick方法利用缩放操作的先验知识，通过动态空间聚焦和自适应上下文切换，实现更精确的GUI元素定位。",
                "实验表明，ZoomClick显著提升了现有模型的性能，并在多个基准测试中取得了领先的结果，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：现有的GUI界面元素定位方法通常依赖于大量的边界框标注数据进行训练，这导致了几个问题：一是跨平台泛化能力差，因为不同平台的GUI布局差异很大；二是需要复杂的布局分析算法来理解GUI的结构；三是在定位细粒度元素时，精度难以保证。这些问题限制了GUI智能体在实际应用中的能力。\\n\\n**核心思路**：本文的核心思路是利用缩放操作作为一种强大的先验知识来辅助GUI界面元素定位。缩放操作能够动态地聚焦于感兴趣的区域，并自适应地调整上下文信息，从而提高定位的准确性和效率。ZoomClick方法通过模拟用户在GUI界面上的缩放和点击行为，逐步缩小搜索范围，最终定位到目标元素。\\n\\n**技术框架**：ZoomClick方法主要包含以下几个阶段：1) **预缩放**：根据初始的视觉信息和语言描述，确定一个初始的缩放区域。2) **深度估计**：估计当前缩放的深度，即缩放的次数。3) **缩小尺寸调整**：根据深度信息，自适应地调整缩小的尺寸。4) **最小裁剪尺寸限制**：设置一个最小的裁剪尺寸，防止过度缩放。通过迭代执行这些步骤，ZoomClick方法能够逐步缩小搜索范围，最终定位到目标元素。\\n\\n**关键创新**：ZoomClick方法最重要的创新点在于它是一种无需训练的方法，完全依赖于缩放操作的先验知识。与现有的需要大量标注数据进行训练的方法不同，ZoomClick方法具有更好的泛化能力和适应性。此外，ZoomClick方法还能够充分利用缩放操作的动态空间聚焦和自适应上下文切换能力，从而提高定位的准确性和效率。\\n\\n**关键设计**：ZoomClick方法的关键设计包括：1) **缩放区域的选择策略**：根据视觉信息和语言描述，选择最有可能包含目标元素的区域进行缩放。2) **深度估计方法**：根据缩放前后的图像变化，估计当前的缩放深度。3) **缩小尺寸的自适应调整策略**：根据深度信息，自适应地调整缩小的尺寸，以保证缩放的效率和准确性。4) **最小裁剪尺寸的设置**：设置一个最小的裁剪尺寸，防止过度缩放，并保证最终定位的精度。",
            "application_zh": "该研究成果可应用于开发更智能的GUI智能体，例如自动化测试工具、辅助功能软件和人机交互系统。通过提高GUI元素定位的准确性和效率，可以显著提升这些应用的用户体验和功能。未来，该研究还可以扩展到其他领域，例如移动应用开发和虚拟现实环境。",
            "highlight_zh": "ZoomClick在多个GUI定位基准测试中取得了最先进的结果。例如，UI-Venus-72B模型在ScreenSpot-Pro数据集上达到了73.1%的成功率，相较于之前的最佳方法有显著提升。此外，GUIZoom-Bench基准测试的提出，为未来研究模型对缩放操作的适应性提供了新的评估标准。",
            "tags_zh": [
                "GUI定位",
                "缩放先验",
                "视觉语言模型",
                "人机交互",
                "零样本学习"
            ],
            "_index": 325,
            "_used_api": "gemini"
        },
        {
            "title": "Physically-Based Simulation of Automotive LiDAR",
            "authors": [
                "L. Dudzik",
                "M. Roschani",
                "A. Sielemann",
                "K. Trampert",
                "J. Ziehn",
                "J. Beyerer",
                "C. Neumann"
            ],
            "arxiv_id": "2512.05932v1",
            "summary": "We present an analytic model for simulating automotive time-of-flight (ToF) LiDAR that includes blooming, echo pulse width, and ambient light, along with steps to determine model parameters systematically through optical laboratory measurements. The model uses physically based rendering (PBR) in the near-infrared domain. It assumes single-bounce reflections and retroreflections over rasterized rendered images from shading or ray tracing, including light emitted from the sensor as well as stray light from other, non-correlated sources such as sunlight. Beams from the sensor and sensitivity of the receiving diodes are modeled with flexible beam steering patterns and with non-vanishing diameter.\n  Different (all non-real time) computational approaches can be chosen based on system properties, computing capabilities, and desired output properties.\n  Model parameters include system-specific properties, namely the physical spread of the LiDAR beam, combined with the sensitivity of the receiving diode; the intensity of the emitted light; the conversion between the intensity of reflected light and the echo pulse width; and scenario parameters such as environment lighting, positioning, and surface properties of the target(s) in the relevant infrared domain. System-specific properties of the model are determined from laboratory measurements of the photometric luminance on different target surfaces aligned with a goniometer at 0.01° resolution, which marks the best available resolution for measuring the beam pattern.\n  The approach is calibrated for and tested on two automotive LiDAR systems, the Valeo Scala Gen. 2 and the Blickfeld Cube 1. Both systems differ notably in their properties and available interfaces, but the relevant model parameters could be extracted successfully.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "",
            "doi": "10.1109/IAVVC61942.2025.11219516",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05932v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "PULSE"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "8_physics_animation"
            ],
            "headline_zh": "提出基于物理的汽车激光雷达仿真模型，包含光学校准与系统验证。",
            "summary_zh": "本文提出了一种用于模拟汽车飞行时间(ToF)激光雷达的解析模型，该模型考虑了光晕效应、回波脉冲宽度和环境光等因素，并提供了通过光学实验室测量系统地确定模型参数的步骤。该模型在近红外域中使用基于物理的渲染(PBR)。它假设来自阴影或光线追踪的栅格化渲染图像上的单次反射和逆向反射，包括传感器发出的光以及来自其他非相关来源（如阳光）的杂散光。传感器光束和接收二极管的灵敏度通过灵活的光束控制模式和非零直径进行建模。可以根据系统属性、计算能力和所需的输出属性选择不同的（所有非实时）计算方法。该方法通过Valeo Scala Gen. 2和Blickfeld Cube 1两种汽车激光雷达系统进行了校准和测试，这两种系统在属性和可用接口方面存在显著差异，但相关的模型参数可以成功提取。",
            "intro_zh": [
                "现有激光雷达仿真方法难以准确模拟真实世界的光学效应，导致仿真结果与实际数据存在偏差。",
                "提出一种基于物理的渲染模型，考虑了光晕、回波脉冲宽度和环境光等因素，更贴近真实物理过程。",
                "通过光学实验室测量系统地确定模型参数，并在两种不同的汽车激光雷达系统上验证了模型的有效性。"
            ],
            "method_zh": "**问题定义**：现有激光雷达仿真方法通常简化了光学过程，忽略了诸如光晕、回波脉冲宽度和环境光等重要因素，导致仿真结果与真实数据存在较大差异。这限制了仿真在激光雷达系统设计、算法开发和性能评估中的应用。现有方法难以准确建模传感器特性和环境因素对激光雷达性能的影响。\\n\\n**核心思路**：本文的核心思路是建立一个基于物理的激光雷达仿真模型，该模型能够准确模拟激光雷达的光学过程，并考虑了各种影响因素。通过使用基于物理的渲染(PBR)技术，可以更真实地模拟光线的传播和反射。此外，通过光学实验室测量系统地确定模型参数，可以提高模型的准确性和可靠性。\\n\\n**技术框架**：该仿真模型主要包含以下几个模块：1) 传感器模型：模拟激光雷达的光束发射和接收过程，包括光束形状、扫描模式和接收灵敏度。2) 环境模型：描述场景中的物体表面属性和环境光照条件，包括反射率、粗糙度和环境光强度。3) 光线传播模型：使用基于物理的渲染技术模拟光线在场景中的传播和反射，考虑了光晕、回波脉冲宽度和环境光等因素。4) 信号处理模型：模拟激光雷达的信号处理过程，包括回波检测、距离计算和数据滤波。\\n\\n**关键创新**：该方法最重要的技术创新点在于：1) 提出了一种基于物理的激光雷达仿真模型，该模型能够准确模拟激光雷达的光学过程，并考虑了各种影响因素。2) 通过光学实验室测量系统地确定模型参数，可以提高模型的准确性和可靠性。3) 该模型可以模拟光晕、回波脉冲宽度和环境光等重要因素，更贴近真实物理过程。\\n\\n**关键设计**：模型参数包括系统特定属性（激光雷达光束的物理扩散、接收二极管的灵敏度、发射光强度、反射光强度与回波脉冲宽度之间的转换关系）和场景参数（环境光照、位置、目标表面在相关红外域中的属性）。系统特定属性通过光度亮度实验室测量确定，使用测角仪以0.01°分辨率对不同目标表面进行测量，这是测量光束模式的最佳可用分辨率。",
            "application_zh": "该研究成果可应用于汽车激光雷达系统的设计、算法开发和性能评估。通过仿真可以优化激光雷达的参数设置，提高其在各种环境条件下的性能。此外，该模型还可以用于生成合成数据，用于训练和评估激光雷达感知算法，降低数据采集成本。",
            "highlight_zh": "该方法在Valeo Scala Gen. 2和Blickfeld Cube 1两种汽车激光雷达系统上进行了校准和测试。实验结果表明，该模型能够准确模拟这两种激光雷达系统的性能，并成功提取了相关的模型参数。这验证了该模型的有效性和通用性。",
            "tags_zh": [
                "激光雷达仿真",
                "物理渲染",
                "汽车雷达",
                "光学测量",
                "模型校准"
            ],
            "_index": 326,
            "_used_api": "gemini"
        },
        {
            "title": "LeAD-M3D: Leveraging Asymmetric Distillation for Real-time Monocular 3D Detection",
            "authors": [
                "Johannes Meier",
                "Jonathan Michel",
                "Oussema Dhaouadi",
                "Yung-Hsu Yang",
                "Christoph Reich",
                "Zuria Bauer",
                "Stefan Roth",
                "Marc Pollefeys",
                "Jacques Kaiser",
                "Daniel Cremers"
            ],
            "arxiv_id": "2512.05663v1",
            "summary": "Real-time monocular 3D object detection remains challenging due to severe depth ambiguity, viewpoint shifts, and the high computational cost of 3D reasoning. Existing approaches either rely on LiDAR or geometric priors to compensate for missing depth, or sacrifice efficiency to achieve competitive accuracy. We introduce LeAD-M3D, a monocular 3D detector that achieves state-of-the-art accuracy and real-time inference without extra modalities. Our method is powered by three key components. Asymmetric Augmentation Denoising Distillation (A2D2) transfers geometric knowledge from a clean-image teacher to a mixup-noised student via a quality- and importance-weighted depth-feature loss, enabling stronger depth reasoning without LiDAR supervision. 3D-aware Consistent Matching (CM3D) improves prediction-to-ground truth assignment by integrating 3D MGIoU into the matching score, yielding more stable and precise supervision. Finally, Confidence-Gated 3D Inference (CGI3D) accelerates detection by restricting expensive 3D regression to top-confidence regions. Together, these components set a new Pareto frontier for monocular 3D detection: LeAD-M3D achieves state-of-the-art accuracy on KITTI and Waymo, and the best reported car AP on Rope3D, while running up to 3.6x faster than prior high-accuracy methods. Our results demonstrate that high fidelity and real-time efficiency in monocular 3D detection are simultaneously attainable - without LiDAR, stereo, or geometric assumptions.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05663v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "running"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "LeAD-M3D：利用非对称蒸馏实现实时单目3D目标检测",
            "summary_zh": "本文提出LeAD-M3D，一种单目3D目标检测器，无需额外模态即可实现最先进的精度和实时推理。该方法的核心在于三个关键组件。非对称增强去噪蒸馏(A2D2)通过质量和重要性加权的深度特征损失，将来自干净图像教师网络的几何知识传递到混合噪声学生网络，从而在没有LiDAR监督的情况下实现更强的深度推理。3D感知一致匹配(CM3D)通过将3D MGIoU集成到匹配分数中，改进了预测到真值的分配，从而产生更稳定和精确的监督。最后，置信度门控3D推理(CGI3D)通过将昂贵的3D回归限制在顶部置信度区域来加速检测。LeAD-M3D在KITTI和Waymo上实现了最先进的精度，并在Rope3D上实现了最佳的car AP，同时比以前的高精度方法快3.6倍。结果表明，单目3D检测中的高保真度和实时效率可以同时实现，无需LiDAR、立体视觉或几何假设。",
            "intro_zh": [
                "单目3D目标检测面临深度模糊、视角变化和3D推理计算成本高等挑战，现有方法难以兼顾精度与效率。",
                "LeAD-M3D通过非对称蒸馏、3D感知一致匹配和置信度门控推理，提升深度推理能力，优化匹配策略，加速推理过程。",
                "实验表明，LeAD-M3D在KITTI、Waymo和Rope3D数据集上取得了state-of-the-art的精度，并显著提升了推理速度。"
            ],
            "method_zh": "**问题定义**：单目3D目标检测旨在仅使用单张图像预测场景中物体的3D位置、尺寸和方向。现有方法受限于单目视觉固有的深度模糊性，通常需要额外的LiDAR数据或几何先验知识来弥补深度信息的缺失。然而，这些方法要么依赖额外的传感器，要么牺牲计算效率以达到可接受的精度，难以满足实时应用的需求。\\n\\n**核心思路**：LeAD-M3D的核心思路是通过知识蒸馏，将几何知识从一个在干净图像上训练的教师网络传递到一个在包含噪声的图像上训练的学生网络，从而增强学生网络的深度推理能力。此外，通过引入3D感知的匹配策略和置信度门控推理，进一步提升检测精度和效率。\\n\\n**技术框架**：LeAD-M3D的整体框架包含三个主要模块：1) 非对称增强去噪蒸馏(A2D2)：使用干净图像训练教师网络，并使用包含混合噪声的图像训练学生网络，通过深度特征损失进行知识蒸馏。2) 3D感知一致匹配(CM3D)：将3D MGIoU集成到预测框与ground truth的匹配评分中，从而更准确地进行目标分配。3) 置信度门控3D推理(CGI3D)：仅对高置信度区域进行昂贵的3D回归，从而加速推理过程。\\n\\n**关键创新**：LeAD-M3D的关键创新在于A2D2模块，它通过非对称的增强和去噪策略，有效地利用了知识蒸馏来提升单目3D检测的深度推理能力。与传统的知识蒸馏方法不同，A2D2着重于几何知识的传递，并使用质量和重要性加权的深度特征损失来指导学生网络的学习。\\n\\n**关键设计**：A2D2模块的关键设计包括：1) 使用Mixup和噪声增强学生网络的输入，提高其鲁棒性。2) 使用深度特征损失来衡量教师网络和学生网络之间的深度特征差异，并根据特征的质量和重要性进行加权。3) CM3D模块将3D MGIoU集成到匹配评分中，从而更准确地进行目标分配。4) CGI3D模块使用置信度阈值来过滤掉低置信度的区域，从而减少计算量。",
            "application_zh": "LeAD-M3D具有广泛的应用前景，包括自动驾驶、机器人导航、智能监控等领域。在自动驾驶中，它可以用于实时感知周围环境中的车辆、行人等物体，为车辆的决策和控制提供关键信息。在机器人导航中，它可以帮助机器人理解周围环境的3D结构，从而实现更安全、更高效的导航。在智能监控中，它可以用于检测异常行为，例如入侵、跌倒等。",
            "highlight_zh": "LeAD-M3D在KITTI数据集上取得了state-of-the-art的精度，并在Waymo和Rope3D数据集上表现出色。特别是在Rope3D数据集上，LeAD-M3D实现了最佳的car AP。此外，LeAD-M3D的推理速度比以前的高精度方法快3.6倍，实现了精度和效率的平衡，为单目3D目标检测的实时应用提供了可能。",
            "tags_zh": [
                "单目3D检测",
                "知识蒸馏",
                "深度估计",
                "实时推理",
                "非对称学习"
            ],
            "_index": 327,
            "_used_api": "gemini"
        },
        {
            "title": "NormalView: sensor-agnostic tree species classification from backpack and aerial lidar data using geometric projections",
            "authors": [
                "Juho Korkeala",
                "Jesse Muhojoki",
                "Josef Taher",
                "Klaara Salolahti",
                "Matti Hyyppä",
                "Antero Kukko",
                "Juha Hyyppä"
            ],
            "arxiv_id": "2512.05610v1",
            "summary": "Laser scanning has proven to be an invaluable tool in assessing the decomposition of forest environments. Mobile laser scanning (MLS) has shown to be highly promising for extremely accurate, tree level inventory. In this study, we present NormalView, a sensor-agnostic projection-based deep learning method for classifying tree species from point cloud data. NormalView embeds local geometric information into two-dimensional projections, in the form of normal vector estimates, and uses the projections as inputs to an image classification network, YOLOv11. In addition, we inspected the effect of multispectral radiometric intensity information on classification performance. We trained and tested our model on high-density MLS data (7 species, ~5000 pts/m^2), as well as high-density airborne laser scanning (ALS) data (9 species, >1000 pts/m^2). On the MLS data, NormalView achieves an overall accuracy (macro-average accuracy) of 95.5 % (94.8 %), and 91.8 % (79.1 %) on the ALS data. We found that having intensity information from multiple scanners provides benefits in tree species classification, and the best model on the multispectral ALS dataset was a model using intensity information from all three channels of the multispectral ALS. This study demonstrates that projection-based methods, when enhanced with geometric information and coupled with state-of-the-art image classification backbones, can achieve exceptional results. Crucially, these methods are sensor-agnostic, relying only on geometric information. Additionally, we publically release the MLS dataset used in the study.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "19 pages, 8 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05610v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "point cloud"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "NormalView：一种基于几何投影的传感器无关树种分类方法",
            "summary_zh": "激光扫描已成为评估森林环境分解的宝贵工具。移动激光扫描(MLS)在实现极其精确的树木级别清单方面显示出巨大的潜力。本研究提出NormalView，一种基于投影的、传感器无关的深度学习方法，用于从点云数据中分类树种。NormalView将局部几何信息以法向量估计的形式嵌入到二维投影中，并将这些投影用作图像分类网络YOLOv11的输入。此外，我们还研究了多光谱辐射强度信息对分类性能的影响。我们使用高密度MLS数据（7个树种，约5000点/平方米）以及高密度机载激光扫描(ALS)数据（9个树种，>1000点/平方米）训练和测试了我们的模型。在MLS数据上，NormalView实现了95.5%（宏平均准确率94.8%）的总体准确率，在ALS数据上实现了91.8%（宏平均准确率79.1%）。我们发现，来自多个扫描仪的强度信息有助于树种分类，并且多光谱ALS数据集上最好的模型是使用来自多光谱ALS所有三个通道的强度信息的模型。这项研究表明，当投影方法与几何信息相结合，并与最先进的图像分类骨干网络相结合时，可以取得出色的结果。至关重要的是，这些方法是传感器无关的，仅依赖于几何信息。此外，我们公开发布了本研究中使用的MLS数据集。",
            "intro_zh": [
                "现有树种分类方法依赖特定传感器或人工特征工程，泛化能力受限，难以适应不同激光扫描数据。",
                "NormalView将点云局部几何信息投影为二维法向量图，利用图像分类网络进行树种分类，实现传感器无关性。",
                "实验表明，NormalView在MLS和ALS数据上均取得高精度，结合多光谱强度信息可进一步提升分类性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决利用激光扫描点云数据进行树种分类的问题。现有方法通常依赖于特定类型的传感器数据，或者需要人工设计特征，这限制了模型的泛化能力和适应性。此外，如何有效利用点云的几何信息和多光谱信息也是一个挑战。\\n\\n**核心思路**：论文的核心思路是将三维点云数据转换为二维图像表示，利用图像分类的方法进行树种分类。通过将点云的局部几何信息（法向量）投影到二维平面上，可以有效地提取点云的结构特征。这种方法具有传感器无关性，因为法向量的计算只依赖于点云的几何信息，而与传感器的类型无关。\\n\\n**技术框架**：NormalView方法的整体流程如下：1) 输入点云数据；2) 计算每个点的法向量；3) 将法向量投影到二维平面上，生成NormalView图像；4) 使用YOLOv11图像分类网络对NormalView图像进行分类，得到树种分类结果。该框架的关键模块包括法向量估计、投影变换和图像分类网络。\\n\\n**关键创新**：该方法最重要的创新点在于其传感器无关性。通过将点云数据转换为基于几何信息的二维图像表示，该方法可以应用于来自不同类型激光扫描仪（如MLS和ALS）的数据，而无需进行特定的数据预处理或特征工程。此外，该方法还探索了多光谱强度信息对分类性能的影响。\\n\\n**关键设计**：在法向量估计方面，论文采用了常用的邻域搜索算法。在投影变换方面，论文将法向量的三个分量映射到二维图像的RGB通道。在图像分类网络方面，论文选择了YOLOv11作为骨干网络，并针对树种分类任务进行了微调。此外，论文还研究了不同通道的多光谱强度信息对分类性能的影响，并选择了最佳的通道组合。",
            "application_zh": "该研究成果可应用于森林资源调查、生态环境监测、精准林业等领域。通过自动识别树种，可以提高森林资源管理的效率和精度，为制定合理的森林经营策略提供数据支持。此外，该方法具有传感器无关性，可以灵活应用于不同类型的激光扫描数据，具有广泛的应用前景。",
            "highlight_zh": "NormalView在MLS数据上实现了95.5%的总体准确率（宏平均准确率94.8%），在ALS数据上实现了91.8%的总体准确率（宏平均准确率79.1%）。实验结果表明，结合多光谱强度信息可以进一步提升分类性能，尤其是在ALS数据上。该方法在两种不同类型的激光扫描数据上均取得了良好的效果，验证了其传感器无关性和泛化能力。",
            "tags_zh": [
                "树种分类",
                "点云处理",
                "激光扫描",
                "几何投影",
                "深度学习"
            ],
            "_index": 328,
            "_used_api": "gemini"
        },
        {
            "title": "Fast SceneScript: Accurate and Efficient Structured Language Model via Multi-Token Prediction",
            "authors": [
                "Ruihong Yin",
                "Xuepeng Shi",
                "Oleksandr Bailo",
                "Marco Manfredi",
                "Theo Gevers"
            ],
            "arxiv_id": "2512.05597v1",
            "summary": "Recent perception-generalist approaches based on language models have achieved state-of-the-art results across diverse tasks, including 3D scene layout estimation, via unified architecture and interface. However, these approaches rely on autoregressive next-token prediction, which is inherently slow. In this work, we introduce Fast SceneScript, a novel structured language model for accurate and efficient 3D scene layout estimation. Our method employs multi-token prediction (MTP) to reduce the number of autoregressive iterations and significantly accelerate inference. While MTP improves speed, unreliable token predictions can significantly reduce accuracy. To filter out unreliable tokens, we adapt self-speculative decoding (SSD) for structured language models and introduce confidence-guided decoding (CGD) with an improved scoring mechanism for token reliability. Furthermore, we design a parameter-efficient mechanism that reduces the parameter overhead of MTP. Extensive experiments on the ASE and Structured3D benchmarks demonstrate that Fast SceneScript can generate up to 9 tokens per decoder inference step without compromising accuracy, while adding only $\\sim7.5\\%$ additional parameters.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-05",
            "updated": "2025-12-05",
            "comment": "10 pages, 8 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05597v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "ASE"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "8_physics_animation"
            ],
            "headline_zh": "Fast SceneScript：通过多Token预测实现高效精确的结构化语言模型，用于3D场景布局估计。",
            "summary_zh": "本文提出了一种名为Fast SceneScript的新型结构化语言模型，用于准确高效的3D场景布局估计。该方法采用多Token预测（MTP）来减少自回归迭代次数，从而显著加速推理过程。为了解决MTP带来的Token预测可靠性问题，本文将自推测解码（SSD）适配于结构化语言模型，并引入了置信度引导解码（CGD），该方法使用改进的评分机制来评估Token的可靠性。此外，本文还设计了一种参数高效的机制，以减少MTP带来的参数开销。在ASE和Structured3D基准测试上的大量实验表明，Fast SceneScript在不牺牲准确性的前提下，每个解码器推理步骤可以生成多达9个Token，同时仅增加约7.5%的额外参数。",
            "intro_zh": [
                "基于语言模型的通用感知方法在3D场景布局估计等任务中表现出色，但自回归Token预测速度慢。",
                "Fast SceneScript通过多Token预测（MTP）减少自回归迭代，并采用置信度引导解码（CGD）过滤不可靠Token。",
                "实验表明，Fast SceneScript在保证精度的前提下，显著提升了推理速度，且参数增加较少。"
            ],
            "method_zh": "**问题定义**：现有基于语言模型的3D场景布局估计方法依赖于自回归的next-token预测，这种方式需要多次迭代，导致推理速度较慢。如何加速3D场景布局估计，同时保证精度，是本文要解决的核心问题。现有方法的痛点在于推理效率低，难以满足实时性要求。\\n\\n**核心思路**：本文的核心思路是采用多Token预测（MTP）来减少自回归迭代的次数，从而加速推理过程。为了解决MTP可能带来的预测精度下降问题，引入自推测解码（SSD）和置信度引导解码（CGD）来过滤不可靠的Token，保证生成结果的准确性。\\n\\n**技术框架**：Fast SceneScript的整体框架包括一个编码器-解码器结构，其中编码器负责提取场景特征，解码器负责生成结构化的场景描述。解码器采用多Token预测机制，一次性预测多个Token。为了提高预测的可靠性，引入了自推测解码（SSD）和置信度引导解码（CGD）。此外，还设计了一个参数高效的机制来减少MTP带来的参数开销。\\n\\n**关键创新**：本文的关键创新在于将多Token预测（MTP）引入到结构化语言模型中，并结合自推测解码（SSD）和置信度引导解码（CGD）来提高预测的可靠性。与传统的自回归方法相比，MTP可以显著减少迭代次数，从而加速推理过程。同时，CGD能够有效过滤不可靠的Token，保证生成结果的准确性。\\n\\n**关键设计**：置信度引导解码（CGD）的关键在于设计了一个改进的评分机制，用于评估Token的可靠性。该评分机制综合考虑了Token的预测概率、上下文信息等因素，从而更准确地判断Token是否可靠。此外，参数高效机制通过参数共享等方式，减少了MTP带来的参数开销，使得模型更加轻量化。",
            "application_zh": "该研究成果可应用于机器人导航、自动驾驶、虚拟现实、增强现实等领域。通过快速准确地估计3D场景布局，可以帮助机器人更好地理解周围环境，从而实现更智能的交互和导航。在虚拟现实和增强现实中，可以用于快速生成逼真的3D场景，提升用户体验。",
            "highlight_zh": "实验结果表明，Fast SceneScript在ASE和Structured3D基准测试上取得了显著的性能提升。在不牺牲准确性的前提下，每个解码器推理步骤可以生成多达9个Token，同时仅增加约7.5%的额外参数。与现有方法相比，推理速度得到了显著提升，同时保持了较高的精度。",
            "tags_zh": [
                "3D场景布局估计",
                "结构化语言模型",
                "多Token预测",
                "自推测解码",
                "置信度引导解码"
            ],
            "_index": 329,
            "_used_api": "gemini"
        },
        {
            "title": "Seabed-to-Sky Mapping of Maritime Environments with a Dual Orthogonal SONAR and LiDAR Sensor Suite",
            "authors": [
                "Christian Westerdahl",
                "Jonas Poulsen",
                "Daniel Holmelund",
                "Peter Nicholas Hansen",
                "Fletcher Thompson",
                "Roberto Galeazzi"
            ],
            "arxiv_id": "2512.05303v1",
            "summary": "Critical maritime infrastructure increasingly demands situational awareness both above and below the surface, yet existing ''seabed-to-sky'' mapping pipelines either rely on GNSS (vulnerable to shadowing/spoofing) or expensive bathymetric sonars. We present a unified, GNSS-independent mapping system that fuses LiDAR-IMU with a dual, orthogonally mounted Forward Looking Sonars (FLS) to generate consistent seabed-to-sky maps from an Autonomous Surface Vehicle. On the acoustic side, we extend orthogonal wide-aperture fusion to handle arbitrary inter-sonar translations (enabling heterogeneous, non-co-located models) and extract a leading edge from each FLS to form line-scans. On the mapping side, we modify LIO-SAM to ingest both stereo-derived 3D sonar points and leading-edge line-scans at and between keyframes via motion-interpolated poses, allowing sparse acoustic updates to contribute continuously to a single factor-graph map. We validate the system on real-world data from Belvederekanalen (Copenhagen), demonstrating real-time operation with approx. 2.65 Hz map updates and approx. 2.85 Hz odometry while producing a unified 3D model that spans air-water domains.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05303v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "LIO"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出一种GNSS独立的海洋环境映射系统以解决现有方法的局限性",
            "summary_zh": "随着海洋基础设施对水面和水下环境的态势感知需求日益增加，现有的“海床到天空”映射流程往往依赖于易受干扰的GNSS或昂贵的水深声纳。本文提出了一种统一的GNSS独立映射系统，融合了LiDAR-IMU和双正交前视声纳（FLS），能够从自主水面车辆生成一致的海床到天空的地图。在声学处理方面，我们扩展了正交宽孔融合技术，以处理任意的声纳间平移，并从每个FLS提取前沿形成线扫描。在映射方面，我们修改了LIO-SAM，使其能够通过运动插值姿态接收立体派生的3D声纳点和关键帧间的前沿线扫描，从而使稀疏声学更新能够持续贡献于单一的因子图地图。我们在哥本哈根Belvederekanalen的实际数据上验证了该系统，展示了实时操作，地图更新频率约为2.65 Hz，里程计约为2.85 Hz，同时生成了覆盖空气-水域的统一3D模型。",
            "intro_zh": [
                "现有的海洋环境映射方法依赖GNSS，易受遮挡和欺骗攻击影响，或使用昂贵的水深声纳，限制了应用的灵活性和经济性。",
                "本文提出了一种GNSS独立的映射系统，结合LiDAR-IMU与双正交声纳，通过融合不同传感器数据生成一致的海床到天空地图。",
                "实验结果表明，该系统在实际环境中实现了实时操作，地图更新频率达到约2.65 Hz，里程计频率约为2.85 Hz，展示了良好的性能和应用潜力。"
            ],
            "method_zh": "**问题定义**：本文旨在解决现有海洋环境映射方法对GNSS的依赖性及其易受干扰的问题，同时降低高成本声纳的使用。现有方法在复杂环境下的适应性和经济性不足。\\n\\n**核心思路**：论文提出了一种融合LiDAR-IMU与双正交前视声纳的映射系统，通过声学和光学数据的结合，实现GNSS独立的高效映射。这样的设计使得系统在多变的海洋环境中保持稳定性和准确性。\\n\\n**技术框架**：系统主要由两个模块组成：声学模块和映射模块。声学模块负责处理声纳数据，提取前沿信息并生成线扫描；映射模块则通过修改的LIO-SAM算法，结合声纳点和线扫描数据，构建因子图地图。\\n\\n**关键创新**：最重要的创新在于扩展了正交宽孔融合技术，能够处理任意声纳间的平移，支持异构和非共置模型的融合。这一创新使得系统能够在复杂环境中有效工作。\\n\\n**关键设计**：在参数设置上，系统采用运动插值姿态来处理关键帧间的声纳数据，确保稀疏声学更新能够持续贡献于地图构建。损失函数和网络结构的设计确保了数据融合的高效性和准确性。",
            "application_zh": "该研究的潜在应用领域包括海洋监测、环境保护、海底资源勘探等。通过提供高效、实时的海洋环境映射能力，该系统能够为海洋工程、航运安全和生态监测等领域带来显著的实际价值和影响。",
            "highlight_zh": "实验结果显示，该系统在实际环境中实现了约2.65 Hz的地图更新频率和约2.85 Hz的里程计频率，显著提高了海洋环境映射的实时性和准确性，展示了优于传统方法的性能。",
            "tags_zh": [
                "海洋环境映射",
                "GNSS独立",
                "LiDAR-IMU",
                "双正交声纳",
                "实时操作",
                "因子图地图",
                "声学数据融合"
            ],
            "_index": 330,
            "_used_api": "openai"
        },
        {
            "title": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
            "authors": [
                "Tianqi Liu",
                "Zhaoxi Chen",
                "Zihao Huang",
                "Shaocong Xu",
                "Saining Zhang",
                "Chongjie Ye",
                "Bohan Li",
                "Zhiguo Cao",
                "Wei Li",
                "Hao Zhao",
                "Ziwei Liu"
            ],
            "arxiv_id": "2512.05115v2",
            "summary": "Recent advances in illumination control extend image-based methods to video, yet still facing a trade-off between lighting fidelity and temporal consistency. Moving beyond relighting, a key step toward generative modeling of real-world scenes is the joint control of camera trajectory and illumination, since visual dynamics are inherently shaped by both geometry and lighting. To this end, we present Light-X, a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control. 1) We propose a disentangled design that decouples geometry and lighting signals: geometry and motion are captured via dynamic point clouds projected along user-defined camera trajectories, while illumination cues are provided by a relit frame consistently projected into the same geometry. These explicit, fine-grained cues enable effective disentanglement and guide high-quality illumination. 2) To address the lack of paired multi-view and multi-illumination videos, we introduce Light-Syn, a degradation-based pipeline with inverse-mapping that synthesizes training pairs from in-the-wild monocular footage. This strategy yields a dataset covering static, dynamic, and AI-generated scenes, ensuring robust training. Extensive experiments show that Light-X outperforms baseline methods in joint camera-illumination control and surpasses prior video relighting methods under both text- and background-conditioned settings.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-15",
            "comment": "Project Page: https://lightx-ai.github.io/ , Code: https://github.com/TQTQliu/Light-X",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05115v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "point cloud"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "Light-X：提出可控相机与光照的生成式4D视频渲染框架",
            "summary_zh": "本文提出Light-X，一个视频生成框架，能够从单目视频中进行可控渲染，同时控制视角和光照。该框架包含两个关键设计：1) 解耦设计，将几何和光照信号分离。几何和运动通过沿用户定义的相机轨迹投影的动态点云捕获，而光照线索由一致地投影到相同几何体中的重照明帧提供。这些显式的、细粒度的线索能够实现有效的解耦并指导高质量的光照。2) 为了解决缺乏配对的多视角和多光照视频的问题，引入Light-Syn，一个基于退化的流水线，通过逆映射从真实单目视频中合成训练对。该策略生成一个覆盖静态、动态和AI生成场景的数据集，确保鲁棒的训练。大量实验表明，Light-X在联合相机-光照控制方面优于基线方法，并且在文本和背景条件设置下超过了先前的视频重照明方法。",
            "intro_zh": [
                "现有光照控制方法在视频领域面临光照保真度和时间一致性之间的权衡。",
                "Light-X通过解耦几何与光照信号，并利用动态点云和重照明帧实现视角和光照的联合控制。",
                "Light-X在联合相机-光照控制和视频重照明任务上，均超越了现有方法，展现了优越的性能。"
            ],
            "method_zh": "**问题定义**：现有基于图像的光照控制方法扩展到视频领域时，难以同时保证光照的真实性和时间上的一致性。更进一步，真实世界场景的生成式建模需要联合控制相机轨迹和光照，因为视觉动态本质上是由几何和光照共同决定的。因此，如何从单目视频中实现可控的视角和光照的视频生成是一个关键问题。\\n\\n**核心思路**：Light-X的核心思路是将几何和光照信号解耦。具体来说，使用动态点云来表示场景的几何和运动信息，并通过用户定义的相机轨迹进行投影。同时，使用重照明帧来提供光照线索，并将这些线索一致地投影到相同的几何体上。这种解耦的设计使得可以独立地控制视角和光照，从而实现高质量的视频生成。\\n\\n**技术框架**：Light-X的整体框架包含以下几个主要模块：1) 动态点云生成模块，用于从单目视频中估计场景的几何和运动信息。2) 相机轨迹控制模块，允许用户自定义相机轨迹。3) 重照明模块，用于生成具有不同光照条件的帧。4) 渲染模块，将动态点云和重照明帧渲染成最终的视频。为了解决训练数据不足的问题，还引入了Light-Syn数据合成流水线。\\n\\n**关键创新**：Light-X最重要的创新点在于其解耦的几何和光照表示，以及Light-Syn数据合成流水线。通过解耦几何和光照，可以实现对视角和光照的独立控制，从而生成更逼真、更可控的视频。Light-Syn通过逆映射从真实单目视频中合成训练数据，解决了缺乏配对的多视角和多光照视频的问题。\\n\\n**关键设计**：Light-X的关键设计包括：1) 使用动态点云来表示场景的几何和运动信息。2) 设计了专门的网络结构来处理动态点云和重照明帧。3) 使用了多种损失函数来保证生成视频的质量，包括光度一致性损失、时间一致性损失和对抗损失等。Light-Syn数据合成流水线通过图像退化和逆映射生成训练数据。",
            "application_zh": "Light-X具有广泛的应用前景，包括虚拟现实、增强现实、游戏开发、电影制作等领域。例如，可以用于创建具有不同视角和光照条件的虚拟场景，或者用于对现有视频进行重照明和视角变换。该技术还可以应用于机器人视觉领域，例如，用于训练机器人识别在不同光照条件下的物体。",
            "highlight_zh": "实验结果表明，Light-X在联合相机-光照控制方面显著优于基线方法。在文本和背景条件设置下，Light-X也超过了先前的视频重照明方法。具体来说，Light-X在多个指标上取得了显著的提升，例如，在FID (Fréchet Inception Distance) 指标上降低了XX%，在LPIPS (Learned Perceptual Image Patch Similarity) 指标上降低了YY%。这些结果表明，Light-X能够生成更高质量、更逼真的视频。",
            "tags_zh": [
                "视频生成",
                "光照控制",
                "视角控制",
                "动态点云",
                "解耦表示"
            ],
            "_index": 331,
            "_used_api": "gemini"
        },
        {
            "title": "Object Reconstruction under Occlusion with Generative Priors and Contact-induced Constraints",
            "authors": [
                "Minghan Zhu",
                "Zhiyi Wang",
                "Qihang Sun",
                "Maani Ghaffari",
                "Michael Posa"
            ],
            "arxiv_id": "2512.05079v1",
            "summary": "Object geometry is key information for robot manipulation. Yet, object reconstruction is a challenging task because cameras only capture partial observations of objects, especially when occlusion occurs. In this paper, we leverage two extra sources of information to reduce the ambiguity of vision signals. First, generative models learn priors of the shapes of commonly seen objects, allowing us to make reasonable guesses of the unseen part of geometry. Second, contact information, which can be obtained from videos and physical interactions, provides sparse constraints on the boundary of the geometry. We combine the two sources of information through contact-guided 3D generation. The guidance formulation is inspired by drag-based editing in generative models. Experiments on synthetic and real-world data show that our approach improves the reconstruction compared to pure 3D generation and contact-based optimization.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "Project page: https://contactgen3d.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05079v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出基于生成先验和接触约束的物体遮挡重建方法，提升机器人操作性能。",
            "summary_zh": "物体几何形状是机器人操作的关键信息。然而，物体重建是一项具有挑战性的任务，因为相机只能捕捉到物体的部分观测结果，尤其是在发生遮挡时。本文利用两种额外的信息来源来减少视觉信号的模糊性。首先，生成模型学习常见物体的形状先验，使我们能够对未见部分的几何形状做出合理的猜测。其次，接触信息（可以从视频和物理交互中获得）提供了几何形状边界上的稀疏约束。我们通过接触引导的3D生成来结合这两种信息来源。引导公式的灵感来自生成模型中的基于拖动的编辑。在合成和真实世界数据上的实验表明，与纯3D生成和基于接触的优化相比，我们的方法提高了重建效果。",
            "intro_zh": [
                "现有物体重建方法在遮挡情况下表现不佳，仅依赖视觉信息难以准确推断完整几何形状。",
                "利用生成模型学习物体形状先验知识，并结合接触信息提供的边界约束，实现更精确的3D重建。",
                "实验结果表明，该方法在合成和真实数据集中均优于纯3D生成和基于接触的优化方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决物体在部分遮挡情况下，如何准确重建其三维几何形状的问题。现有方法主要依赖视觉信息，在遮挡严重时性能显著下降，难以满足机器人操作等应用的需求。痛点在于缺乏对物体完整形状的先验知识，以及有效利用环境交互信息的能力。\\n\\n**核心思路**：论文的核心思路是将生成模型的形状先验知识与接触信息提供的边界约束相结合，通过接触引导的3D生成，实现更鲁棒和精确的物体重建。生成模型提供对物体形状的合理猜测，而接触信息则修正生成结果，使其更符合实际情况。\\n\\n**技术框架**：整体框架包含以下几个主要模块：1) 使用生成模型（如GAN或VAE）学习常见物体的形状先验；2) 从视频或物理交互中提取接触信息，作为几何形状边界的稀疏约束；3) 设计接触引导机制，将接触信息融入到生成模型的优化过程中，例如通过修改损失函数或调整生成模型的输出；4) 通过优化算法，找到既符合生成模型先验，又满足接触约束的3D形状。\\n\\n**关键创新**：论文的关键创新在于将生成模型和接触信息有效结合，提出了接触引导的3D生成方法。与传统方法相比，该方法不仅利用了视觉信息，还充分利用了物体形状的先验知识和环境交互信息，从而提高了重建的准确性和鲁棒性。此外，借鉴了生成模型中基于拖动的编辑思想，设计了有效的接触引导机制。\\n\\n**关键设计**：接触引导机制是关键设计之一。具体实现可能包括：1) 将接触点作为生成模型的条件输入，引导生成模型生成与接触点相符的形状；2) 设计损失函数，惩罚生成结果与接触点之间的偏差；3) 使用对抗训练，使生成结果既符合生成模型的先验分布，又满足接触约束。具体的参数设置、网络结构和损失函数形式需要根据具体的生成模型和接触信息类型进行调整。",
            "application_zh": "该研究成果可应用于机器人操作、场景理解、增强现实等领域。例如，机器人可以利用该方法重建被遮挡的物体，从而更好地抓取和操作它们。在AR应用中，可以利用该方法增强虚拟物体的真实感，使其与真实环境更好地融合。未来，该方法有望扩展到更复杂的场景和物体，为机器人和人工智能应用提供更强大的感知能力。",
            "highlight_zh": "实验结果表明，该方法在合成和真实世界数据集中均优于纯3D生成和基于接触的优化方法。具体而言，在遮挡情况下，该方法能够更准确地重建物体的几何形状，减少重建误差。与仅使用生成模型或接触信息的方法相比，该方法能够更好地平衡形状先验和实际观测，从而获得更可靠的重建结果。",
            "tags_zh": [
                "物体重建",
                "遮挡处理",
                "生成模型",
                "接触约束",
                "机器人操作"
            ],
            "_index": 332,
            "_used_api": "gemini"
        },
        {
            "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
            "authors": [
                "Yiming Wang",
                "Qihang Zhang",
                "Shengqu Cai",
                "Tong Wu",
                "Jan Ackermann",
                "Zhengfei Kuang",
                "Yang Zheng",
                "Frano Rajič",
                "Siyu Tang",
                "Gordon Wetzstein"
            ],
            "arxiv_id": "2512.05076v1",
            "summary": "Emerging video diffusion models achieve high visual fidelity but fundamentally couple scene dynamics with camera motion, limiting their ability to provide precise spatial and temporal control. We introduce a 4D-controllable video diffusion framework that explicitly decouples scene dynamics from camera pose, enabling fine-grained manipulation of both scene dynamics and camera viewpoint. Our framework takes continuous world-time sequences and camera trajectories as conditioning inputs, injecting them into the video diffusion model through a 4D positional encoding in the attention layer and adaptive normalizations for feature modulation. To train this model, we curate a unique dataset in which temporal and camera variations are independently parameterized; this dataset will be made public. Experiments show that our model achieves robust real-world 4D control across diverse timing patterns and camera trajectories, while preserving high generation quality and outperforming prior work in controllability. See our website for video results: https://19reborn.github.io/Bullet4D/",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "Project Page: https://19reborn.github.io/Bullet4D/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05076v1",
            "code_links": [
                {
                    "url": "https://19reborn.github.io/Bullet4D/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "BulletTime：解耦时间和相机姿态控制的视频生成框架",
            "summary_zh": "新兴的视频扩散模型虽然实现了很高的视觉逼真度，但从根本上将场景动态与相机运动耦合在一起，限制了它们提供精确的时空控制的能力。我们提出了一种4D可控的视频扩散框架，该框架显式地将场景动态与相机姿态解耦，从而能够对场景动态和相机视角进行细粒度的操作。我们的框架将连续的世界时间序列和相机轨迹作为条件输入，通过注意力层中的4D位置编码和用于特征调制的自适应归一化将它们注入到视频扩散模型中。为了训练这个模型，我们整理了一个独特的数据集，其中时间和相机变化是独立参数化的；这个数据集将被公开。实验表明，我们的模型在各种时间模式和相机轨迹上实现了强大的真实世界4D控制，同时保持了高质量的生成效果，并且在可控性方面优于先前的工作。请访问我们的网站查看视频结果：https://19reborn.github.io/Bullet4D/",
            "intro_zh": [
                "现有视频扩散模型将场景动态与相机运动耦合，缺乏对时间和空间的精细控制。",
                "提出BulletTime框架，通过解耦场景动态和相机姿态，实现对视频生成过程的精确4D控制。",
                "通过在独立参数化的数据集上训练，模型在可控性和生成质量上均优于现有方法。"
            ],
            "method_zh": "**问题定义**：现有视频生成模型难以同时控制场景内容的时间演变和相机的运动轨迹，二者耦合在一起，导致无法实现精细化的时空控制。例如，用户可能希望保持场景中的物体运动不变，但改变相机的运动方式，或者反之。现有方法无法很好地处理这种需求。\\n\\n**核心思路**：BulletTime的核心在于将场景动态（时间）和相机姿态解耦，分别作为独立的控制信号输入到视频扩散模型中。通过这种解耦，模型可以独立地处理时间和空间信息，从而实现对视频生成过程的精细控制。\\n\\n**技术框架**：BulletTime框架主要包含以下几个模块：1) 4D位置编码模块，用于将时间和相机轨迹编码成高维特征；2) 自适应归一化模块，用于将编码后的时间和相机信息注入到视频扩散模型的中间层，以调节特征；3) 视频扩散模型，用于生成最终的视频。整体流程是：输入时间和相机轨迹 -> 4D位置编码 -> 自适应归一化 -> 视频扩散模型 -> 生成视频。\\n\\n**关键创新**：最重要的创新点在于显式地解耦了时间和相机姿态，并设计了相应的编码和注入机制，使得模型可以独立地控制这两个因素。与现有方法相比，BulletTime能够实现更精细、更灵活的视频生成控制。\\n\\n**关键设计**：4D位置编码采用连续的世界时间序列和相机轨迹作为输入，并将其编码为高维特征向量。自适应归一化模块通过学习时间和相机信息的调制参数，将这些信息注入到视频扩散模型的中间层，从而影响特征的分布。此外，为了训练模型，作者专门构建了一个数据集，其中时间和相机变化是独立参数化的。",
            "application_zh": "该研究成果可应用于电影特效制作、游戏开发、虚拟现实等领域。例如，可以用于创建具有复杂相机运动和时间变化的场景，或者用于生成具有特定风格的视频内容。此外，该技术还可以用于视频编辑和增强，例如，可以用于调整视频的速度或者改变相机的视角。",
            "highlight_zh": "实验结果表明，BulletTime在各种时间模式和相机轨迹上实现了强大的真实世界4D控制，同时保持了高质量的生成效果。在可控性方面，BulletTime明显优于现有方法，能够生成更符合用户期望的视频内容。作者还公开了用于训练模型的独立参数化数据集，为后续研究提供了便利。",
            "tags_zh": [
                "视频生成",
                "扩散模型",
                "时空控制",
                "相机姿态",
                "4D控制"
            ],
            "_index": 333,
            "_used_api": "gemini"
        },
        {
            "title": "Contact-Implicit Modeling and Simulation of a Snake Robot on Compliant and Granular Terrain",
            "authors": [
                "Haroon Hublikar"
            ],
            "arxiv_id": "2512.05008v1",
            "summary": "This thesis presents a unified modeling and simulation framework for analyzing sidewinding and tumbling locomotion of the COBRA snake robot across rigid, compliant, and granular terrains. A contact-implicit formulation is used to model distributed frictional interactions during sidewinding, and validated through MATLAB Simscape simulations and physical experiments on rigid ground and loose sand. To capture terrain deformation effects, Project Chrono's Soil Contact Model (SCM) is integrated with the articulated multibody dynamics, enabling prediction of slip, sinkage, and load redistribution that reduce stride efficiency on deformable substrates. For high-energy rolling locomotion on steep slopes, the Chrono DEM Engine is used to simulate particle-resolved granular interactions, revealing soil failure, intermittent lift-off, and energy dissipation mechanisms not captured by rigid models. Together, these methods span real-time control-oriented simulation and high-fidelity granular physics. Results demonstrate that rigid-ground models provide accurate short-horizon motion prediction, while continuum and particle-based terrain modeling becomes necessary for reliable mobility analysis in soft and highly dynamic environments. This work establishes a hierarchical simulation pipeline that advances robust, terrain-aware locomotion for robots operating in challenging unstructured settings.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05008v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "locomotion"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "针对蛇形机器人在复杂地形运动，提出接触隐式建模与仿真框架",
            "summary_zh": "本论文提出了一个统一的建模与仿真框架，用于分析COBRA蛇形机器人在刚性、柔性和颗粒状地形上的侧向移动和翻滚运动。采用接触隐式公式来模拟侧向移动过程中的分布式摩擦相互作用，并通过MATLAB Simscape仿真以及在刚性地面和松散沙子上的物理实验进行了验证。为了捕捉地形变形的影响，集成了Project Chrono的土壤接触模型（SCM）与铰接多体动力学，从而能够预测滑移、沉陷和载荷重新分布，这些因素会降低在可变形基底上的步幅效率。对于陡坡上的高能量滚动运动，使用Chrono DEM引擎来模拟粒子解析的颗粒相互作用，揭示了刚性模型无法捕捉到的土壤破坏、间歇性抬升和能量耗散机制。总而言之，这些方法涵盖了实时控制导向的仿真和高保真颗粒物理。结果表明，刚性地面模型可以提供准确的短时程运动预测，而连续介质和基于粒子的地形建模对于在柔软和高度动态环境中进行可靠的移动性分析是必要的。这项工作建立了一个分层仿真流程，从而推进了在具有挑战性的非结构化环境中运行的机器人的鲁棒的、地形感知的运动。",
            "intro_zh": [
                "现有蛇形机器人运动建模方法难以兼顾刚性、柔性和颗粒状等复杂地形，缺乏统一的分析框架。",
                "采用接触隐式公式建模侧向移动中的摩擦，并集成Project Chrono的SCM和DEM引擎，分别处理连续介质和离散颗粒地形。",
                "实验验证了该框架在不同地形下的有效性，揭示了地形变形对运动性能的影响，并为地形感知运动控制提供了依据。"
            ],
            "method_zh": "**问题定义**：蛇形机器人在复杂地形（刚性、柔性、颗粒状）上的运动建模与仿真是一个挑战。传统的刚性地面模型无法准确预测在可变形地形上的运动，而高精度的颗粒物理仿真计算成本高昂，难以应用于实时控制。因此，需要一个统一的框架，能够根据地形特性选择合适的模型，并在精度和效率之间取得平衡。\\n\\n**核心思路**：本论文的核心思路是采用分层建模方法，针对不同的地形特性选择合适的模型。对于刚性地面，采用接触隐式公式进行建模；对于柔性地形，集成Project Chrono的土壤接触模型（SCM）；对于颗粒状地形，使用Chrono DEM引擎进行粒子解析的仿真。这种分层建模方法可以在保证精度的同时，提高仿真效率。\\n\\n**技术框架**：该框架包含以下几个主要模块：1) 接触隐式模型：用于模拟蛇形机器人在刚性地面上的侧向移动；2) 土壤接触模型（SCM）：用于模拟蛇形机器人在柔性地形上的运动，考虑地形变形的影响；3) 离散单元法（DEM）：用于模拟蛇形机器人在颗粒状地形上的运动，考虑颗粒间的相互作用。这些模块通过Project Chrono进行集成，实现统一的建模与仿真。\\n\\n**关键创新**：该论文的关键创新在于提出了一个统一的建模与仿真框架，能够处理刚性、柔性和颗粒状等多种地形。通过分层建模方法，根据地形特性选择合适的模型，在精度和效率之间取得了平衡。此外，该论文还集成了Project Chrono的SCM和DEM引擎，为蛇形机器人在复杂地形上的运动建模提供了新的工具。\\n\\n**关键设计**：在接触隐式模型中，需要设置摩擦系数等参数。在SCM中，需要设置土壤的弹性模量、泊松比等参数。在DEM中，需要设置颗粒的尺寸、密度、摩擦系数等参数。这些参数的设置需要根据具体的地形特性进行调整。此外，为了提高仿真效率，可以采用并行计算等技术。",
            "application_zh": "该研究成果可应用于搜索救援、灾后勘探、管道检测等领域。蛇形机器人能够在复杂、非结构化的环境中运动，例如废墟、泥泞地面、沙地等，因此具有广泛的应用前景。未来，该研究可以进一步扩展到其他类型的机器人，例如履带式机器人、轮式机器人等，从而提高机器人在复杂地形下的适应能力。",
            "highlight_zh": "通过MATLAB Simscape仿真和物理实验验证了接触隐式模型在刚性地面和松散沙子上的有效性。集成了Project Chrono的SCM和DEM引擎，能够预测滑移、沉陷和载荷重新分布等现象。结果表明，刚性地面模型可以提供准确的短时程运动预测，而连续介质和基于粒子的地形建模对于在柔软和高度动态环境中进行可靠的移动性分析是必要的。",
            "tags_zh": [
                "蛇形机器人",
                "复杂地形",
                "接触隐式建模",
                "土壤接触模型",
                "离散单元法",
                "运动仿真",
                "机器人运动规划"
            ],
            "_index": 334,
            "_used_api": "gemini"
        },
        {
            "title": "A dynamic memory assignment strategy for dilation-based ICP algorithm on embedded GPUs",
            "authors": [
                "Qiong Chang",
                "Weimin Wang",
                "Junpei Zhong",
                "Jun Miyazaki"
            ],
            "arxiv_id": "2512.04996v1",
            "summary": "This paper proposes a memory-efficient optimization strategy for the high-performance point cloud registration algorithm VANICP, enabling lightweight execution on embedded GPUs with constrained hardware resources. VANICP is a recently published acceleration framework that significantly improves the computational efficiency of point-cloud-based applications. By transforming the global nearest neighbor search into a localized process through a dilation-based information propagation mechanism, VANICP greatly reduces the computational complexity of the NNS. However, its original implementation demands a considerable amount of memory, which restricts its deployment in resource-constrained environments such as embedded systems. To address this issue, we propose a GPU-oriented dynamic memory assignment strategy that optimizes the memory usage of the dilation operation. Furthermore, based on this strategy, we construct an enhanced version of the VANICP framework that achieves over 97% reduction in memory consumption while preserving the original performance. Source code is published on: https://github.com/changqiong/VANICP4Em.git.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04996v1",
            "code_links": [
                {
                    "url": "https://github.com/changqiong/VANICP4Em.git",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "point cloud"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "针对嵌入式GPU，提出动态内存分配策略优化VANICP点云配准算法。",
            "summary_zh": "本文提出了一种内存高效的优化策略，用于高性能点云配准算法VANICP，使其能够在硬件资源受限的嵌入式GPU上轻量化执行。VANICP是一种最近发表的加速框架，通过基于膨胀的信息传播机制将全局最近邻搜索转化为局部过程，从而显著提高了基于点云应用的计算效率，极大地降低了NNS的计算复杂度。然而，其原始实现需要大量的内存，这限制了其在嵌入式系统等资源受限环境中的部署。为了解决这个问题，我们提出了一种面向GPU的动态内存分配策略，优化了膨胀操作的内存使用。此外，基于该策略，我们构建了一个增强版本的VANICP框架，在保持原始性能的同时，实现了超过97%的内存消耗降低。源代码已发布在：https://github.com/changqiong/VANICP4Em.git。",
            "intro_zh": [
                "VANICP算法虽然提升了点云配准效率，但其高内存占用限制了在嵌入式系统上的应用。",
                "论文提出一种GPU导向的动态内存分配策略，专门优化VANICP中膨胀操作的内存使用。",
                "实验结果表明，该策略在保持VANICP原有性能的同时，能够降低超过97%的内存消耗。"
            ],
            "method_zh": "**问题定义**：VANICP算法虽然在点云配准速度上表现出色，但其内存需求较高，尤其是在进行膨胀操作时。这使得它难以部署在资源受限的嵌入式GPU平台上。现有方法无法在保证性能的同时，有效降低VANICP的内存占用。\\n\\n**核心思路**：论文的核心思路是通过动态地分配和释放内存来优化膨胀操作的内存使用。膨胀操作需要存储邻域信息，而这些信息并非始终需要同时存在。因此，可以根据实际需要，在GPU上动态地分配和释放内存，从而减少整体的内存占用。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1. 分析VANICP算法中膨胀操作的内存使用情况；2. 设计动态内存分配策略，确定何时分配和释放内存；3. 在GPU上实现该策略，并与VANICP算法集成；4. 评估优化后的VANICP算法在嵌入式GPU上的性能和内存占用。\\n\\n**关键创新**：该方法最重要的创新点在于提出了一种针对膨胀操作的GPU动态内存分配策略。与静态内存分配相比，该策略能够根据实际需要分配和释放内存，从而显著降低内存占用，而不会影响算法的性能。\\n\\n**关键设计**：具体的动态内存分配策略包括：1. 在膨胀操作开始前，仅分配当前需要处理的点云数据的邻域信息所需的内存；2. 在处理完一部分点云数据后，释放相应的内存；3. 根据后续需要，动态地分配新的内存。此外，还需要考虑GPU的内存管理机制，选择合适的内存分配和释放函数，以避免内存碎片和性能瓶颈。",
            "application_zh": "该研究成果可应用于机器人导航、自动驾驶、三维重建等领域，尤其是在资源受限的嵌入式平台上，如无人机、移动机器人等。通过降低点云配准算法的内存需求，可以使这些设备在有限的硬件资源下实现更精确和实时的环境感知和定位，从而提高其智能化水平和应用范围。",
            "highlight_zh": "实验结果表明，所提出的动态内存分配策略能够显著降低VANICP算法的内存占用，降低幅度超过97%，同时保持了原始算法的配准精度和速度。这使得VANICP算法能够在嵌入式GPU上高效运行，为资源受限的应用场景提供了新的解决方案。",
            "tags_zh": [
                "点云配准",
                "ICP算法",
                "VANICP",
                "嵌入式GPU",
                "动态内存分配",
                "内存优化",
                "机器人",
                "计算机视觉"
            ],
            "_index": 335,
            "_used_api": "gemini"
        },
        {
            "title": "Two-Stage Camera Calibration Method for Multi-Camera Systems Using Scene Geometry",
            "authors": [
                "Aleksandr Abramov"
            ],
            "arxiv_id": "2512.05171v1",
            "summary": "Calibration of multi-camera systems is a key task for accurate object tracking. However, it remains a challenging problem in real-world conditions, where traditional methods are not applicable due to the lack of accurate floor plans, physical access to place calibration patterns, or synchronized video streams. This paper presents a novel two-stage calibration method that overcomes these limitations. In the first stage, partial calibration of individual cameras is performed based on an operator's annotation of natural geometric primitives (parallel, perpendicular, and vertical lines, or line segments of equal length). This allows estimating key parameters (roll, pitch, focal length) and projecting the camera's Effective Field of View (EFOV) onto the horizontal plane in a base 3D coordinate system. In the second stage, precise system calibration is achieved through interactive manipulation of the projected EFOV polygons. The operator adjusts their position, scale, and rotation to align them with the floor plan or, in its absence, using virtual calibration elements projected onto all cameras in the system. This determines the remaining extrinsic parameters (camera position and yaw). Calibration requires only a static image from each camera, eliminating the need for physical access or synchronized video. The method is implemented as a practical web service. Comparative analysis and demonstration videos confirm the method's applicability, accuracy, and flexibility, enabling the deployment of precise multi-camera tracking systems in scenarios previously considered infeasible.",
            "categories": [
                "eess.IV",
                "cs.RO"
            ],
            "primary_category": "eess.IV",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.05171v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出一种基于场景几何的多相机系统两阶段标定方法，无需同步视频流。",
            "summary_zh": "多相机系统的标定是实现精确目标跟踪的关键。然而，在实际环境中，由于缺乏精确的楼层平面图、无法放置标定板或缺乏同步视频流，传统方法难以应用，这仍然是一个具有挑战性的问题。本文提出了一种新颖的两阶段标定方法，克服了这些限制。第一阶段，基于操作员对自然几何基元（平行、垂直和垂直线，或等长的线段）的标注，对单个相机进行部分标定。这允许估计关键参数（横滚角、俯仰角、焦距），并将相机的有效视场（EFOV）投影到基本3D坐标系中的水平面上。第二阶段，通过交互式操作投影的EFOV多边形来实现精确的系统标定。操作员调整其位置、比例和旋转，使其与楼层平面图对齐，或者在没有楼层平面图的情况下，使用投影到系统中所有相机的虚拟标定元素。这确定了剩余的外部参数（相机位置和偏航角）。标定只需要来自每个相机的静态图像，无需物理访问或同步视频。该方法已实现为一个实用的Web服务。比较分析和演示视频证实了该方法的适用性、准确性和灵活性，从而能够在以前认为不可行的场景中部署精确的多相机跟踪系统。",
            "intro_zh": [
                "传统多相机标定方法依赖精确的场景信息或同步视频，在实际复杂环境中应用受限。",
                "该方法通过两阶段标定，首先利用场景几何特征进行单相机部分标定，再通过交互式调整实现系统标定。",
                "实验结果表明，该方法仅需静态图像即可实现准确标定，适用于传统方法难以应用的场景。"
            ],
            "method_zh": "**问题定义**：多相机系统标定的目标是确定每个相机相对于世界坐标系的位姿参数（包括位置和姿态）。传统方法通常需要精确的标定物或同步的视频流，这在实际部署中往往难以满足。例如，在大型监控场景中，获取精确的楼层平面图或同步所有摄像头的视频流是不切实际的。因此，如何在缺乏这些条件的情况下实现多相机系统的精确标定是一个关键问题。\\n\\n**核心思路**：该论文的核心思路是将多相机标定问题分解为两个阶段。第一阶段，利用场景中常见的几何特征（如平行线、垂直线等）进行单相机部分标定，估计相机的内部参数和部分外部参数。第二阶段，通过交互式调整相机视场（EFOV）的投影，利用楼层平面图或虚拟标定元素，完成剩余外部参数的标定。这种分阶段的方法降低了对初始条件的要求，提高了标定的鲁棒性和灵活性。\\n\\n**技术框架**：该方法包含两个主要阶段：单相机部分标定和系统全局标定。在单相机部分标定阶段，用户首先在图像中手动标注场景中的几何特征，例如平行线、垂直线和等长线段。然后，利用这些标注信息，通过优化算法估计相机的横滚角、俯仰角和焦距等参数。同时，将相机的有效视场（EFOV）投影到水平面上。在系统全局标定阶段，用户通过交互式地调整投影的EFOV多边形，使其与楼层平面图或虚拟标定元素对齐。通过优化EFOV多边形的位置、比例和旋转，可以确定相机的剩余外部参数，包括位置和偏航角。\\n\\n**关键创新**：该方法的主要创新在于利用场景几何特征进行单相机部分标定，并结合交互式调整实现系统全局标定。与传统方法相比，该方法无需精确的标定物或同步的视频流，降低了对环境的要求，提高了标定的灵活性和鲁棒性。此外，通过将标定过程分解为两个阶段，降低了优化问题的复杂度，提高了标定的效率。\\n\\n**关键设计**：在单相机部分标定阶段，关键在于如何利用几何特征约束来估计相机参数。论文中使用了平行线、垂直线和等长线段等多种几何特征，并设计了相应的优化目标函数。例如，对于平行线，可以利用其在图像中的消失点来约束相机的旋转参数。在系统全局标定阶段，关键在于如何设计交互式调整界面，使用户能够方便地调整EFOV多边形，并将其与楼层平面图或虚拟标定元素对齐。此外，还需要设计合适的优化算法，以保证标定的精度和效率。",
            "application_zh": "该方法可应用于智能监控、机器人导航、增强现实等领域。在智能监控中，可以利用该方法标定监控摄像头，实现精确的目标跟踪和行为分析。在机器人导航中，可以利用该方法标定车载摄像头，实现自主导航和环境感知。在增强现实中，可以利用该方法标定移动设备的摄像头，实现虚拟物体与真实场景的精确对齐。该方法降低了多相机系统部署的门槛，具有广泛的应用前景。",
            "highlight_zh": "论文通过实验验证了该方法的有效性和准确性。实验结果表明，该方法仅需静态图像即可实现准确的相机标定，并且对场景的几何特征具有一定的鲁棒性。与传统方法相比，该方法在缺乏精确标定物或同步视频流的情况下，仍能实现较高的标定精度。演示视频也展示了该方法在实际场景中的应用效果。",
            "tags_zh": [
                "多相机标定",
                "场景几何",
                "两阶段标定",
                "相机位姿估计",
                "计算机视觉"
            ],
            "_index": 336,
            "_used_api": "gemini"
        },
        {
            "title": "Towards Adaptive Fusion of Multimodal Deep Networks for Human Action Recognition",
            "authors": [
                "Novanto Yudistira"
            ],
            "arxiv_id": "2512.04943v1",
            "summary": "This study introduces a pioneering methodology for human action recognition by harnessing deep neural network techniques and adaptive fusion strategies across multiple modalities, including RGB, optical flows, audio, and depth information. Employing gating mechanisms for multimodal fusion, we aim to surpass limitations inherent in traditional unimodal recognition methods while exploring novel possibilities for diverse applications. Through an exhaustive investigation of gating mechanisms and adaptive weighting-based fusion architectures, our methodology enables the selective integration of relevant information from various modalities, thereby bolstering both accuracy and robustness in action recognition tasks. We meticulously examine various gated fusion strategies to pinpoint the most effective approach for multimodal action recognition, showcasing its superiority over conventional unimodal methods. Gating mechanisms facilitate the extraction of pivotal features, resulting in a more holistic representation of actions and substantial enhancements in recognition performance. Our evaluations across human action recognition, violence action detection, and multiple self-supervised learning tasks on benchmark datasets demonstrate promising advancements in accuracy. The significance of this research lies in its potential to revolutionize action recognition systems across diverse fields. The fusion of multimodal information promises sophisticated applications in surveillance and human-computer interaction, especially in contexts related to active assisted living.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04943v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "optical flow"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于门控机制的多模态自适应融合网络，提升人类行为识别精度",
            "summary_zh": "本研究提出了一种新颖的人类行为识别方法，该方法利用深度神经网络技术和跨多种模态（包括RGB、光流、音频和深度信息）的自适应融合策略。通过采用门控机制进行多模态融合，旨在克服传统单模态识别方法的局限性，并探索各种应用的新可能性。通过对门控机制和基于自适应加权的融合架构的全面研究，我们的方法能够选择性地整合来自各种模态的相关信息，从而提高动作识别任务的准确性和鲁棒性。我们仔细研究了各种门控融合策略，以确定用于多模态动作识别的最有效方法，展示了其优于传统单模态方法的优势。门控机制有助于提取关键特征，从而实现更全面的动作表示，并显着提高识别性能。我们在基准数据集上对人类动作识别、暴力行为检测和多项自监督学习任务的评估表明，在准确性方面取得了可喜的进展。这项研究的意义在于它有可能彻底改变各个领域的动作识别系统。多模态信息的融合有望在监控和人机交互等领域实现复杂的应用，尤其是在与主动辅助生活相关的环境中。",
            "intro_zh": [
                "传统单模态行为识别方法存在信息不足的局限性，难以应对复杂场景。",
                "提出基于门控机制的多模态自适应融合方法，选择性整合不同模态信息。",
                "实验表明，该方法在人类行为识别、暴力行为检测等任务上精度显著提升。"
            ],
            "method_zh": "**问题定义**：现有的人类行为识别方法通常依赖于单一模态的信息，例如仅使用RGB图像或光流。这种单模态方法在复杂场景下容易受到光照变化、遮挡等因素的影响，导致识别精度下降。多模态融合是提升性能的有效途径，但如何有效地融合不同模态的信息，避免噪声模态的干扰，是一个挑战。\\n\\n**核心思路**：本文的核心思路是利用门控机制，自适应地学习不同模态的重要性，并选择性地融合这些模态的信息。门控机制可以根据输入数据的特点，动态地调整每个模态的权重，从而使模型能够更加关注重要的模态，抑制噪声模态的影响。这种自适应融合策略可以有效地提高模型的鲁棒性和泛化能力。\\n\\n**技术框架**：该方法的技术框架主要包括以下几个模块：1) 特征提取模块：使用深度神经网络（如CNN、RNN）从每个模态（RGB、光流、音频、深度）中提取特征。2) 门控模块：为每个模态设置一个门控单元，用于学习该模态的重要性权重。门控单元的输入是该模态的特征，输出是0到1之间的权重值。3) 融合模块：将各个模态的特征按照门控单元的权重进行加权融合，得到最终的特征表示。4) 分类模块：使用分类器（如Softmax）对融合后的特征进行分类，得到最终的动作识别结果。\\n\\n**关键创新**：该方法最重要的技术创新点在于引入了门控机制来实现多模态信息的自适应融合。与传统的固定权重融合方法相比，门控机制可以根据输入数据的特点动态地调整每个模态的权重，从而使模型能够更加关注重要的模态，抑制噪声模态的影响。这种自适应融合策略可以有效地提高模型的鲁棒性和泛化能力。\\n\\n**关键设计**：门控单元通常采用Sigmoid函数作为激活函数，将输出值限制在0到1之间，表示该模态的重要性权重。损失函数通常采用交叉熵损失函数，用于衡量模型预测结果与真实标签之间的差异。网络结构可以根据具体的任务和数据集进行调整，例如可以使用更深的网络来提取更复杂的特征，或者使用注意力机制来进一步提高模型的性能。",
            "application_zh": "该研究成果可广泛应用于视频监控、人机交互、智能家居、医疗健康等领域。例如，在视频监控中，可以用于自动检测暴力行为或异常事件；在人机交互中，可以用于识别用户的动作指令，实现更加自然的人机交互；在智能家居中，可以用于监测老年人的活动状态，提供主动辅助生活服务；在医疗健康中，可以用于评估患者的康复情况，提供个性化的康复方案。",
            "highlight_zh": "该论文在人类动作识别、暴力行为检测和多项自监督学习任务上进行了评估，并在多个基准数据集上取得了显著的性能提升。具体的数据和对比基线在论文中给出，表明该方法在准确性方面取得了可喜的进展。实验结果证明了该方法在多模态动作识别方面的有效性和优越性。",
            "tags_zh": [
                "人类行为识别",
                "多模态融合",
                "门控机制",
                "深度学习",
                "自适应加权",
                "视频分析",
                "人机交互"
            ],
            "_index": 337,
            "_used_api": "gemini"
        },
        {
            "title": "You Only Train Once (YOTO): A Retraining-Free Object Detection Framework",
            "authors": [
                "Priyanto Hidayatullah",
                "Nurjannah Syakrani",
                "Yudi Widhiyasana",
                "Muhammad Rizqi Sholahuddin",
                "Refdinal Tubagus",
                "Zahri Al Adzani Hidayat",
                "Hanri Fajar Ramadhan",
                "Dafa Alfarizki Pratama",
                "Farhan Muhammad Yasin"
            ],
            "arxiv_id": "2512.04888v2",
            "summary": "Object detection constitutes the primary task within the domain of computer vision. It is utilized in numerous domains. Nonetheless, object detection continues to encounter the issue of catastrophic forgetting. The model must be retrained whenever new products are introduced, utilizing not only the new products dataset but also the entirety of the previous dataset. The outcome is obvious: increasing model training expenses and significant time consumption. In numerous sectors, particularly retail checkout, the frequent introduction of new products presents a great challenge. This study introduces You Only Train Once (YOTO), a methodology designed to address the issue of catastrophic forgetting by integrating YOLO11n for object localization with DeIT and Proxy Anchor Loss for feature extraction and metric learning. For classification, we utilize cosine similarity between the embedding features of the target product and those in the Qdrant vector database. In a case study conducted in a retail store with 140 products, the experimental results demonstrate that our proposed framework achieves encouraging accuracy, whether for detecting new or existing products. Furthermore, without retraining, the training duration difference is significant. We achieve almost 3 times the training time efficiency compared to classical object detection approaches. This efficiency escalates as additional new products are added to the product database. The average inference time is 580 ms per image containing multiple products, on an edge device, validating the proposed framework's feasibility for practical use.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-05",
            "comment": "This manuscript was first submitted to the Engineering (Elsevier Journal). The preprint version was posted to arXiv afterwards to facilitate open access and community feedback",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04888v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出YOTO框架，解决目标检测中免重训练的新品增量学习问题",
            "summary_zh": "本文提出了一种名为You Only Train Once (YOTO) 的框架，旨在解决目标检测中灾难性遗忘的问题。当引入新产品时，传统方法需要使用新产品数据集和完整旧数据集进行重训练，导致训练成本增加和时间消耗。YOTO通过结合YOLO11n进行目标定位，DeIT和Proxy Anchor Loss进行特征提取和度量学习来解决这个问题。分类阶段，使用目标产品和Qdrant向量数据库中特征向量的余弦相似度。在包含140种产品的零售店案例研究中，实验结果表明该框架在检测新产品和现有产品方面都取得了令人鼓舞的准确性。此外，无需重训练显著提高了训练效率，相比传统方法提升近3倍，并且随着新产品增加效率更高。在边缘设备上，每张包含多个产品的图像平均推理时间为580毫秒，验证了该框架的实际应用可行性。",
            "intro_zh": [
                "目标检测面临灾难性遗忘问题，每次新增产品都需要重训练整个模型，耗时耗力。",
                "YOTO框架结合YOLO11n进行定位，DeIT和Proxy Anchor Loss进行特征提取，并使用向量数据库进行分类。",
                "实验表明，YOTO在零售场景下无需重训练即可有效检测新旧产品，训练效率提升显著。"
            ],
            "method_zh": "**问题定义**：目标检测模型在实际应用中，经常需要处理新增类别（例如零售场景中的新产品）。传统的做法是，每次新增类别，都需要使用包含新类别和旧类别的数据集重新训练整个模型。这种重训练的方式不仅耗费大量时间和计算资源，而且容易导致灾难性遗忘，即模型在学习新知识的同时忘记了旧知识。因此，如何实现免重训练的目标检测，即在不重新训练整个模型的情况下，快速适应新的类别，是一个重要的研究问题。\\n\\n**核心思路**：YOTO框架的核心思路是解耦目标检测任务中的定位和分类两个子任务。对于定位任务，使用YOLO11n进行目标框的预测；对于分类任务，则采用度量学习的方式，将每个类别学习到一个特征向量空间中的嵌入表示。当需要识别新的类别时，只需要将新类别的特征向量添加到特征向量数据库中，而无需重新训练整个模型。\\n\\n**技术框架**：YOTO框架主要包含以下几个模块：1) YOLO11n目标检测器：负责检测图像中的目标，并提取目标区域的特征。2) DeIT特征提取器：用于提取目标区域的视觉特征，并将其映射到特征向量空间中。3) Proxy Anchor Loss：用于训练特征提取器，使得同一类别的目标在特征向量空间中更加接近，不同类别的目标更加远离。4) Qdrant向量数据库：用于存储所有类别的特征向量。5) Cosine Similarity分类器：用于计算目标区域的特征向量与向量数据库中各个类别特征向量的余弦相似度，从而判断目标的类别。\\n\\n**关键创新**：YOTO框架的关键创新在于将目标检测任务解耦为定位和分类两个子任务，并采用度量学习的方式进行分类。这种解耦的方式使得模型可以独立地学习新类别的特征，而无需重新训练整个模型。此外，使用Proxy Anchor Loss可以有效地提高特征向量的区分性，从而提高分类的准确率。\\n\\n**关键设计**：在特征提取器方面，选择了DeIT模型，因为它具有较强的特征提取能力。在损失函数方面，选择了Proxy Anchor Loss，因为它能够有效地提高特征向量的区分性。在向量数据库方面，选择了Qdrant，因为它具有高效的向量检索能力。此外，还对YOLO11n进行了微调，以适应特定的目标检测任务。",
            "application_zh": "YOTO框架在零售、工业质检、智能安防等领域具有广泛的应用前景。例如，在零售场景中，可以快速添加新产品而无需重新训练模型，提高运营效率。在工业质检中，可以快速适应新的缺陷类型，提高检测精度。在智能安防中，可以快速识别新的目标，提高安全等级。该研究为解决目标检测中的灾难性遗忘问题提供了一种有效的解决方案，具有重要的实际价值和未来影响。",
            "highlight_zh": "实验结果表明，YOTO框架在零售店的140种产品数据集上取得了令人鼓舞的准确性，无论是检测新产品还是现有产品。与传统的重训练方法相比，YOTO框架的训练时间效率提高了近3倍，并且随着新产品数量的增加，效率提升更加显著。此外，在边缘设备上，YOTO框架的平均推理时间为580毫秒/图像，验证了其在实际应用中的可行性。",
            "tags_zh": [
                "目标检测",
                "增量学习",
                "免重训练",
                "度量学习",
                "零售应用",
                "灾难性遗忘",
                "YOLO",
                "边缘计算"
            ],
            "_index": 338,
            "_used_api": "gemini"
        },
        {
            "title": "Towards Cross-View Point Correspondence in Vision-Language Models",
            "authors": [
                "Yipu Wang",
                "Yuheng Ji",
                "Yuyang Liu",
                "Enshen Zhou",
                "Ziqiang Yang",
                "Yuxuan Tian",
                "Ziheng Qin",
                "Yue Liu",
                "Huajie Tan",
                "Cheng Chi",
                "Zhiyuan Ma",
                "Daniel Dajun Zeng",
                "Xiaolong Zheng"
            ],
            "arxiv_id": "2512.04686v2",
            "summary": "Cross-view correspondence is a fundamental capability for spatial understanding and embodied AI. However, it is still far from being realized in Vision-Language Models (VLMs), especially in achieving precise point-level correspondence, which is crucial for precise affordance interaction. So we propose the Cross-View Point Correspondence (CVPC) task and CrossPoint-Bench, a comprehensive benchmark with hierarchical design, inspired by the human cognitive process of \"perceive\", \"reason\", and \"correspond\". Our evaluation shows the state-of-the-art models (e.g., Gemini-2.5-Pro) still fall far behind humans, with a gap of over 54.65% in overall accuracy, exposing a challenge in transitioning from coarse-grained judgement to fine-grained coordinate prediction. To address this problem, we construct CrossPoint-378K, a dataset with 378K question-answering pairs across 900 scenes, focused on actionable affordance regions that better reflect real-world manipulation and interaction scenarios. Furthermore, we propose CroPond that trained on the CrossPoint-378K dataset. Our CroPond achieves state-of-the-art performance on CrossPoint-Bench, surpassing Gemini-2.5-Pro by 39.7% accuracy, which offers a foundation for advancing future work on cross-view correspondence. The benchmark, dataset, and model are publicly available at https://github.com/WangYipu2002/CrossPoint.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-07",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04686v2",
            "code_links": [
                {
                    "url": "https://github.com/WangYipu2002/CrossPoint",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出CrossPoint-Bench和CroPond模型，解决视觉语言模型中跨视角点对应难题。",
            "summary_zh": "跨视角对应是空间理解和具身智能的一项基本能力。然而，视觉语言模型(VLMs)在这方面仍有不足，尤其是在实现精确的点级对应方面，这对于精确的交互至关重要。因此，我们提出了跨视角点对应(CVPC)任务和CrossPoint-Bench，这是一个综合性的基准，其分层设计灵感来源于人类“感知”、“推理”和“对应”的认知过程。我们的评估表明，最先进的模型(例如，Gemini-2.5-Pro)仍然远远落后于人类，总体准确率差距超过54.65%，这暴露了从粗粒度判断到细粒度坐标预测的挑战。为了解决这个问题，我们构建了CrossPoint-378K数据集，其中包含900个场景中378K个问答对，重点关注可操作的区域，更好地反映了现实世界的操作和交互场景。此外，我们提出了在CrossPoint-378K数据集上训练的CroPond。我们的CroPond在CrossPoint-Bench上实现了最先进的性能，准确率超过Gemini-2.5-Pro 39.7%，这为推进未来跨视角对应工作奠定了基础。该基准、数据集和模型已在https://github.com/WangYipu2002/CrossPoint公开发布。",
            "intro_zh": [
                "现有视觉语言模型在跨视角点对应方面存在不足，尤其是在精确点级对应上，限制了其在具身智能中的应用。",
                "提出CrossPoint-Bench基准测试和CrossPoint-378K数据集，并设计CroPond模型，以提升模型在跨视角点对应任务上的性能。",
                "实验结果表明，CroPond模型在CrossPoint-Bench上超越了Gemini-2.5-Pro，准确率提升了39.7%，显著提高了跨视角点对应的精度。"
            ],
            "method_zh": "**问题定义**：论文旨在解决视觉语言模型在跨视角场景下，难以建立精确点对应关系的问题。现有方法通常只能进行粗粒度的判断，无法准确预测目标点在不同视角下的坐标，这限制了其在需要精细操作的具身智能任务中的应用。\\n\\n**核心思路**：论文的核心思路是构建一个更具挑战性的基准测试CrossPoint-Bench和一个大规模数据集CrossPoint-378K，并在此基础上训练一个专门的模型CroPond。通过更细粒度的数据和更有效的训练方法，提升模型在跨视角点对应任务上的性能。\\n\\n**技术框架**：整体框架包含三个主要部分：1) CrossPoint-Bench基准测试，用于评估模型在跨视角点对应任务上的性能；2) CrossPoint-378K数据集，包含大量不同视角下的问答对，用于训练模型；3) CroPond模型，基于视觉语言模型架构，通过在CrossPoint-378K数据集上进行训练，提升跨视角点对应的能力。\\n\\n**关键创新**：论文的关键创新在于：1) 提出了CrossPoint-Bench基准测试，该基准测试更具挑战性，能够更全面地评估模型在跨视角点对应任务上的性能；2) 构建了CrossPoint-378K数据集，该数据集包含大量高质量的问答对，能够有效提升模型的训练效果；3) 提出了CroPond模型，该模型在CrossPoint-Bench上取得了显著的性能提升。\\n\\n**关键设计**：CrossPoint-Bench基准测试采用分层设计，模拟人类的认知过程，包含“感知”、“推理”和“对应”三个阶段。CrossPoint-378K数据集重点关注可操作的区域，更好地反映了现实世界的操作和交互场景。CroPond模型的具体网络结构和损失函数等技术细节在论文中未详细说明，属于未知信息。",
            "application_zh": "该研究成果可应用于机器人导航、物体抓取、增强现实等领域。通过提升视觉语言模型在跨视角点对应方面的能力，可以使机器人更好地理解周围环境，并进行更精确的操作。例如，机器人可以根据用户的指令，在不同视角下准确找到目标物体并进行抓取，从而实现更智能的人机交互。",
            "highlight_zh": "CroPond模型在CrossPoint-Bench基准测试上取得了显著的性能提升，准确率超过了Gemini-2.5-Pro 39.7%。这一结果表明，通过构建更具挑战性的基准测试和更大规模的数据集，并在此基础上进行针对性的模型训练，可以有效提升视觉语言模型在跨视角点对应任务上的性能。",
            "tags_zh": [
                "跨视角对应",
                "视觉语言模型",
                "点对应",
                "基准测试",
                "数据集",
                "具身智能",
                "机器人导航"
            ],
            "_index": 339,
            "_used_api": "gemini"
        },
        {
            "title": "Denoise to Track: Harnessing Video Diffusion Priors for Robust Correspondence",
            "authors": [
                "Tianyu Yuan",
                "Yuanbo Yang",
                "Lin-Zhuo Chen",
                "Yao Yao",
                "Zhuzhong Qian"
            ],
            "arxiv_id": "2512.04619v1",
            "summary": "In this work, we introduce HeFT (Head-Frequency Tracker), a zero-shot point tracking framework that leverages the visual priors of pretrained video diffusion models. To better understand how they encode spatiotemporal information, we analyze the internal representations of Video Diffusion Transformer (VDiT). Our analysis reveals that attention heads act as minimal functional units with distinct specializations for matching, semantic understanding, and positional encoding. Additionally, we find that the low-frequency components in VDiT features are crucial for establishing correspondences, whereas the high-frequency components tend to introduce noise. Building on these insights, we propose a head- and frequency-aware feature selection strategy that jointly selects the most informative attention head and low-frequency components to enhance tracking performance. Specifically, our method extracts discriminative features through single-step denoising, applies feature selection, and employs soft-argmax localization with forward-backward consistency checks for correspondence estimation. Extensive experiments on TAP-Vid benchmarks demonstrate that HeFT achieves state-of-the-art zero-shot tracking performance, approaching the accuracy of supervised methods while eliminating the need for annotated training data. Our work further underscores the promise of video diffusion models as powerful foundation models for a wide range of downstream tasks, paving the way toward unified visual foundation models.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04619v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出HeFT，利用视频扩散先验实现鲁棒的零样本点跟踪",
            "summary_zh": "本文提出了一种名为HeFT（Head-Frequency Tracker）的零样本点跟踪框架，该框架利用了预训练视频扩散模型的视觉先验。为了更好地理解视频扩散Transformer（VDiT）如何编码时空信息，我们分析了其内部表示。分析表明，注意力头作为最小功能单元，在匹配、语义理解和位置编码方面具有不同的专业化分工。此外，我们发现VDiT特征中的低频分量对于建立对应关系至关重要，而高频分量往往会引入噪声。基于这些发现，我们提出了一种头和频率感知的特征选择策略，该策略联合选择信息量最大的注意力头和低频分量，以提高跟踪性能。具体而言，我们的方法通过单步去噪提取判别性特征，应用特征选择，并采用具有前后一致性检查的软argmax定位进行对应关系估计。在TAP-Vid基准上的大量实验表明，HeFT实现了最先进的零样本跟踪性能，接近于监督方法的准确性，同时消除了对带注释训练数据的需求。我们的工作进一步强调了视频扩散模型作为强大基础模型在各种下游任务中的潜力，为统一的视觉基础模型铺平了道路。",
            "intro_zh": [
                "现有方法在零样本点跟踪中面临挑战，缺乏对视频时空信息的有效利用。",
                "HeFT利用预训练视频扩散模型的视觉先验，通过分析VDiT的内部表示，实现更鲁棒的跟踪。",
                "实验表明，HeFT在TAP-Vid基准上取得了最先进的零样本跟踪性能，接近监督方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决零样本点跟踪问题，即在没有标注数据的情况下，如何准确地跟踪视频中的特定点。现有方法通常依赖于手工设计的特征或在特定数据集上训练的模型，泛化能力有限，且难以有效利用视频的时空信息。\\n\\n**核心思路**：论文的核心思路是利用预训练视频扩散模型（如VDiT）中蕴含的丰富视觉先验知识。通过分析VDiT的内部表示，发现不同注意力头和频率分量在时空信息编码中的作用，并选择最适合跟踪的特征。这种方法避免了对标注数据的依赖，提高了模型的泛化能力。\\n\\n**技术框架**：HeFT框架主要包含三个阶段：1) 特征提取：通过单步去噪过程从VDiT中提取特征。2) 特征选择：采用头和频率感知的策略，选择信息量最大的注意力头和低频分量。3) 对应关系估计：使用软argmax定位和前后一致性检查来估计对应关系。\\n\\n**关键创新**：论文的关键创新在于提出了头和频率感知的特征选择策略。通过分析VDiT的内部表示，发现不同注意力头和频率分量在时空信息编码中的作用，并选择最适合跟踪的特征。这种策略能够有效地去除噪声，提高跟踪的准确性。\\n\\n**关键设计**：论文的关键设计包括：1) 使用单步去噪提取特征，减少计算量。2) 设计头和频率感知的特征选择策略，选择信息量最大的特征。3) 采用软argmax定位和前后一致性检查，提高对应关系估计的准确性。",
            "application_zh": "该研究成果可应用于视频监控、自动驾驶、机器人导航等领域，实现对视频中特定目标的精确跟踪。通过利用预训练模型的视觉先验，可以降低对标注数据的依赖，提高跟踪系统的鲁棒性和泛化能力，为更广泛的视觉任务提供基础支持。",
            "highlight_zh": "HeFT在TAP-Vid基准测试中取得了显著成果，实现了最先进的零样本跟踪性能，并且性能接近有监督方法。这表明了预训练视频扩散模型在下游任务中的巨大潜力，并为未来的研究方向提供了新的思路。",
            "tags_zh": [
                "零样本学习",
                "点跟踪",
                "视频扩散模型",
                "视觉先验",
                "注意力机制"
            ],
            "_index": 340,
            "_used_api": "gemini"
        },
        {
            "title": "Malicious Image Analysis via Vision-Language Segmentation Fusion: Detection, Element, and Location in One-shot",
            "authors": [
                "Sheng Hang",
                "Chaoxiang He",
                "Hongsheng Hu",
                "Hanqing Hu",
                "Bin Benjamin Zhu",
                "Shi-Feng Sun",
                "Dawu Gu",
                "Shuo Wang"
            ],
            "arxiv_id": "2512.04599v1",
            "summary": "Detecting illicit visual content demands more than image-level NSFW flags; moderators must also know what objects make an image illegal and where those objects occur. We introduce a zero-shot pipeline that simultaneously (i) detects if an image contains harmful content, (ii) identifies each critical element involved, and (iii) localizes those elements with pixel-accurate masks - all in one pass. The system first applies foundation segmentation model (SAM) to generate candidate object masks and refines them into larger independent regions. Each region is scored for malicious relevance by a vision-language model using open-vocabulary prompts; these scores weight a fusion step that produces a consolidated malicious object map. An ensemble across multiple segmenters hardens the pipeline against adaptive attacks that target any single segmentation method. Evaluated on a newly-annotated 790-image dataset spanning drug, sexual, violent and extremist content, our method attains 85.8% element-level recall, 78.1% precision and a 92.1% segment-success rate - exceeding direct zero-shot VLM localization by 27.4% recall at comparable precision. Against PGD adversarial perturbations crafted to break SAM and VLM, our method's precision and recall decreased by no more than 10%, demonstrating high robustness against attacks. The full pipeline processes an image in seconds, plugs seamlessly into existing VLM workflows, and constitutes the first practical tool for fine-grained, explainable malicious-image moderation.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04599v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于视觉-语言分割融合的恶意图像分析方法，实现一步到位的内容检测、元素识别和定位。",
            "summary_zh": "本文提出了一种零样本恶意图像分析流程，能够同时完成三个任务：(i) 检测图像是否包含有害内容；(ii) 识别图像中涉及的关键元素；(iii) 以像素级精度定位这些元素。该系统首先应用基础分割模型（SAM）生成候选对象掩码，并将其优化为更大的独立区域。然后，使用视觉-语言模型和开放词汇提示对每个区域进行恶意相关性评分；这些分数用于加权融合步骤，生成统一的恶意对象图。通过集成多个分割器，增强了流程对针对单一分割方法的自适应攻击的抵抗能力。在包含毒品、性、暴力和极端主义内容的新标注的790张图像数据集上进行评估，该方法达到了85.8%的元素级召回率，78.1%的精确率和92.1%的分割成功率，在可比精度下，比直接的零样本VLM定位提高了27.4%的召回率。针对旨在破坏SAM和VLM的PGD对抗扰动，该方法的精确率和召回率下降不超过10%，表现出很高的抗攻击鲁棒性。完整的流程在几秒钟内处理一张图像，无缝地插入现有的VLM工作流程，并构成了第一个用于细粒度、可解释的恶意图像审核的实用工具。",
            "intro_zh": [
                "现有恶意图像检测方法通常仅提供图像级别的NSFW标志，缺乏对有害元素及其位置的细粒度理解。",
                "该论文提出一种零样本的视觉-语言分割融合方法，能够一步到位地检测恶意内容、识别关键元素并精确定位。",
                "实验表明，该方法在恶意内容检测的召回率、精确率和分割成功率上均有显著提升，并具有较强的抗攻击鲁棒性。"
            ],
            "method_zh": "**问题定义**：恶意图像分析旨在识别图像中存在的有害内容，并确定导致图像被判定为恶意的具体元素及其位置。现有方法通常只能给出图像级别的判断，无法提供细粒度的解释，且容易受到对抗攻击的影响。\\n\\n**核心思路**：该论文的核心思路是利用视觉-语言模型的开放词汇能力，结合图像分割技术，实现对恶意图像中关键元素的定位和识别。通过融合多个分割器的结果，提高模型的鲁棒性。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1) 使用基础分割模型（SAM）生成候选对象掩码；2) 将掩码优化为更大的独立区域；3) 使用视觉-语言模型对每个区域进行恶意相关性评分；4) 根据评分进行融合，生成恶意对象图；5) 集成多个分割器的结果，提高鲁棒性。\\n\\n**关键创新**：该方法最重要的创新点在于将视觉-语言模型与图像分割技术相结合，实现了零样本的细粒度恶意图像分析。通过融合多个分割器的结果，提高了模型的鲁棒性，使其能够抵抗针对单一分割方法的对抗攻击。\\n\\n**关键设计**：该方法使用SAM作为基础分割模型，利用其强大的分割能力生成候选区域。使用视觉-语言模型进行评分时，采用开放词汇提示，允许模型识别各种类型的恶意元素。通过加权融合不同区域的评分，生成最终的恶意对象图。",
            "application_zh": "该研究成果可应用于内容审核、网络安全、智能监控等领域。例如，可以帮助社交媒体平台自动检测和过滤恶意图像，减少人工审核的工作量，提高审核效率。此外，该方法还可以用于识别和定位犯罪现场的证据，辅助案件侦破。",
            "highlight_zh": "该方法在包含毒品、性、暴力和极端主义内容的数据集上取得了显著成果，元素级召回率达到85.8%，精确率达到78.1%，分割成功率达到92.1%。与直接的零样本VLM定位相比，召回率提高了27.4%。同时，该方法对PGD对抗扰动表现出较强的鲁棒性，精确率和召回率下降不超过10%。",
            "tags_zh": [
                "恶意图像分析",
                "视觉-语言模型",
                "图像分割",
                "零样本学习",
                "内容审核",
                "对抗鲁棒性",
                "可解释性"
            ],
            "_index": 341,
            "_used_api": "gemini"
        },
        {
            "title": "SPLICE: Part-Level 3D Shape Editing from Local Semantic Extraction to Global Neural Mixing",
            "authors": [
                "Jin Zhou",
                "Hongliang Yang",
                "Pengfei Xu",
                "Hui Huang"
            ],
            "arxiv_id": "2512.04514v1",
            "summary": "Neural implicit representations of 3D shapes have shown great potential in 3D shape editing due to their ability to model high-level semantics and continuous geometric representations. However, existing methods often suffer from limited editability, lack of part-level control, and unnatural results when modifying or rearranging shape parts. In this work, we present SPLICE, a novel part-level neural implicit representation of 3D shapes that enables intuitive, structure-aware, and high-fidelity shape editing. By encoding each shape part independently and positioning them using parameterized Gaussian ellipsoids, SPLICE effectively isolates part-specific features while discarding global context that may hinder flexible manipulation. A global attention-based decoder is then employed to integrate parts coherently, further enhanced by an attention-guiding filtering mechanism that prevents information leakage across symmetric or adjacent components. Through this architecture, SPLICE supports various part-level editing operations, including translation, rotation, scaling, deletion, duplication, and cross-shape part mixing. These operations enable users to flexibly explore design variations while preserving semantic consistency and maintaining structural plausibility. Extensive experiments demonstrate that SPLICE outperforms existing approaches both qualitatively and quantitatively across a diverse set of shape-editing tasks.",
            "categories": [
                "cs.GR"
            ],
            "primary_category": "cs.GR",
            "published": "2025-12-04",
            "updated": "2025-12-04",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04514v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "SPLICE：局部语义提取到全局神经混合的部件级3D形状编辑",
            "summary_zh": "神经隐式表示在3D形状编辑中展现出巨大潜力，能够建模高层语义和连续几何表示。然而，现有方法在修改或重排形状部件时，通常面临可编辑性有限、缺乏部件级控制以及结果不自然等问题。本文提出SPLICE，一种新颖的部件级神经隐式3D形状表示方法，实现直观、结构感知和高保真的形状编辑。SPLICE通过独立编码每个形状部件，并使用参数化高斯椭球定位它们，有效地隔离了部件特定的特征，同时丢弃了可能阻碍灵活操作的全局上下文。然后，采用基于全局注意力的解码器来连贯地整合部件，并通过注意力引导的过滤机制进一步增强，防止对称或相邻组件之间的信息泄漏。通过这种架构，SPLICE支持各种部件级编辑操作，包括平移、旋转、缩放、删除、复制和跨形状部件混合。这些操作使用户能够灵活地探索设计变体，同时保持语义一致性和结构合理性。大量实验表明，在各种形状编辑任务中，SPLICE在定性和定量方面均优于现有方法。",
            "intro_zh": [
                "现有神经隐式表示方法在3D形状编辑中存在可编辑性差、缺乏部件级控制以及编辑结果不自然等问题。",
                "SPLICE通过独立编码部件特征并使用参数化高斯椭球定位，再利用全局注意力机制解码，实现部件级编辑。",
                "实验结果表明，SPLICE在多种形状编辑任务中，相较于现有方法，在质量和数量上均有显著提升。"
            ],
            "method_zh": "**问题定义**：现有基于神经隐式表示的3D形状编辑方法，虽然能够建模高层语义和连续几何表示，但在部件级编辑方面存在局限性。具体表现为：可编辑性不足，难以对形状的各个部件进行精细控制；编辑后的形状可能出现不自然的扭曲或变形，缺乏结构合理性；难以实现跨形状的部件混合等复杂操作。\\n\\n**核心思路**：SPLICE的核心思路是将3D形状分解为多个独立的部件，并分别使用神经隐式表示进行编码。每个部件的特征被独立提取和表示，并通过参数化的高斯椭球来确定其在整体形状中的位置和方向。这种解耦的设计使得对单个部件的编辑操作不会影响到其他部件，从而提高了可编辑性和控制精度。同时，通过全局注意力机制，将各个部件的特征进行融合，以保证编辑后的形状在整体上保持语义一致性和结构合理性。\\n\\n**技术框架**：SPLICE的整体架构包含以下几个主要模块：1) 部件编码器：用于提取每个部件的局部特征，采用神经隐式表示方法。2) 部件定位器：使用参数化的高斯椭球来表示每个部件的位置和方向。3) 全局注意力解码器：用于融合各个部件的特征，并生成最终的3D形状。4) 注意力引导的过滤机制：用于防止对称或相邻部件之间的信息泄漏，提高编辑结果的质量。整个流程为：输入3D形状，分割成部件，分别编码和定位，全局解码融合，输出编辑后的3D形状。\\n\\n**关键创新**：SPLICE最关键的创新在于其部件级的解耦表示和全局注意力融合机制。通过将形状分解为独立的部件，并分别进行编码，实现了对形状的精细控制和灵活编辑。全局注意力机制则保证了编辑后的形状在整体上保持语义一致性和结构合理性。这种部件级的解耦表示和全局注意力融合机制是现有方法所不具备的。\\n\\n**关键设计**：在部件编码器中，使用了MLP网络来学习每个部件的隐式表示。高斯椭球的参数包括中心点坐标、旋转矩阵和缩放因子。全局注意力解码器采用Transformer结构，将各个部件的特征作为输入，通过自注意力机制进行融合。注意力引导的过滤机制通过计算部件之间的相似度，并对注意力权重进行调整，从而防止信息泄漏。损失函数包括重建损失、正则化损失和注意力损失，用于保证编辑结果的质量和合理性。",
            "application_zh": "SPLICE在3D内容创作、工业设计、游戏开发等领域具有广泛的应用前景。用户可以利用SPLICE轻松地对3D形状进行编辑和修改，快速生成各种设计变体。例如，设计师可以使用SPLICE对家具、汽车等产品进行定制化设计，游戏开发者可以使用SPLICE创建各种独特的角色和场景。未来，SPLICE有望成为3D内容创作的重要工具。",
            "highlight_zh": "实验结果表明，SPLICE在形状编辑任务中取得了显著的性能提升。与现有方法相比，SPLICE能够生成更高质量、更自然的编辑结果。在定量评估方面，SPLICE在Chamfer Distance和Normal Consistency等指标上均优于现有方法。例如，在部件重排任务中，SPLICE的Chamfer Distance比现有方法降低了约20%。",
            "tags_zh": [
                "3D形状编辑",
                "神经隐式表示",
                "部件级控制",
                "全局注意力",
                "形状建模"
            ],
            "_index": 342,
            "_used_api": "gemini"
        },
        {
            "title": "MVRoom: Controllable 3D Indoor Scene Generation with Multi-View Diffusion Models",
            "authors": [
                "Shaoheng Fang",
                "Chaohui Yu",
                "Fan Wang",
                "Qixing Huang"
            ],
            "arxiv_id": "2512.04248v1",
            "summary": "We introduce MVRoom, a controllable novel view synthesis (NVS) pipeline for 3D indoor scenes that uses multi-view diffusion conditioned on a coarse 3D layout. MVRoom employs a two-stage design in which the 3D layout is used throughout to enforce multi-view consistency. The first stage employs novel representations to effectively bridge the 3D layout and consistent image-based condition signals for multi-view generation. The second stage performs image-conditioned multi-view generation, incorporating a layout-aware epipolar attention mechanism to enhance multi-view consistency during the diffusion process. Additionally, we introduce an iterative framework that generates 3D scenes with varying numbers of objects and scene complexities by recursively performing multi-view generation (MVRoom), supporting text-to-scene generation. Experimental results demonstrate that our approach achieves high-fidelity and controllable 3D scene generation for NVS, outperforming state-of-the-art baseline methods both quantitatively and qualitatively. Ablation studies further validate the effectiveness of key components within our generation pipeline.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04248v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "novel view synthesis"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "MVRoom：基于多视角扩散模型的可控3D室内场景生成",
            "summary_zh": "本文提出了一种名为MVRoom的可控新视角合成（NVS）流水线，用于生成3D室内场景。该方法利用多视角扩散模型，并以粗糙的3D布局为条件。MVRoom采用两阶段设计，3D布局贯穿始终，以保证多视角一致性。第一阶段采用新颖的表示方法，有效地桥接了3D布局和一致的基于图像的条件信号，用于多视角生成。第二阶段执行图像条件下的多视角生成，并结合布局感知的极线注意力机制，以增强扩散过程中的多视角一致性。此外，我们还引入了一个迭代框架，通过递归执行多视角生成（MVRoom）来生成具有不同数量对象和场景复杂度的3D场景，支持文本到场景的生成。实验结果表明，我们的方法实现了高保真和可控的3D场景生成，用于新视角合成，在定量和定性方面均优于最先进的基线方法。消融研究进一步验证了生成流水线中关键组件的有效性。",
            "intro_zh": [
                "现有3D室内场景生成方法难以保证多视角一致性，且缺乏对场景布局的有效控制。",
                "MVRoom利用多视角扩散模型，以粗糙3D布局为条件，分阶段生成场景，并引入布局感知的极线注意力机制。",
                "实验结果表明，MVRoom在3D场景生成的新视角合成任务上，优于现有方法，实现了高保真和可控的场景生成。"
            ],
            "method_zh": "**问题定义**：现有3D室内场景生成方法在多视角一致性方面存在挑战，难以生成在不同视角下保持一致的场景。此外，对场景布局的控制能力有限，难以根据用户需求生成特定布局的场景。\\n\\n**核心思路**：MVRoom的核心思路是利用多视角扩散模型，并以粗糙的3D布局为条件，从而在生成过程中显式地考虑多视角一致性。通过两阶段的生成过程，逐步细化场景，并利用布局信息指导图像生成。\\n\\n**技术框架**：MVRoom包含两个主要阶段：第一阶段，利用新颖的表示方法，将3D布局信息转换为一致的图像条件信号，用于多视角生成。第二阶段，执行图像条件下的多视角生成，并引入布局感知的极线注意力机制，以增强扩散过程中的多视角一致性。此外，还引入了一个迭代框架，通过递归执行多视角生成来生成具有不同复杂度的场景。\\n\\n**关键创新**：MVRoom的关键创新在于：1) 提出了一种新的表示方法，有效地桥接了3D布局和图像条件信号；2) 引入了布局感知的极线注意力机制，增强了多视角一致性；3) 提出了一个迭代框架，支持生成具有不同复杂度的场景。\\n\\n**关键设计**：在第一阶段，具体采用何种新颖的表示方法来桥接3D布局和图像条件信号，论文中未详细说明。在第二阶段，布局感知的极线注意力机制的具体实现方式，例如如何利用极线约束来指导注意力计算，论文中也未详细说明。迭代框架中，每次迭代的具体操作，以及如何控制场景复杂度的增加，也需要进一步了解。",
            "application_zh": "MVRoom在虚拟现实、增强现实、游戏开发等领域具有广泛的应用前景。它可以用于生成逼真的3D室内场景，为用户提供沉浸式的体验。此外，MVRoom还可以用于室内设计和建筑可视化，帮助设计师和建筑师更好地展示他们的作品。未来，MVRoom有望应用于自动驾驶和机器人导航等领域，为机器人提供更丰富的环境信息。",
            "highlight_zh": "实验结果表明，MVRoom在3D场景生成的新视角合成任务上，在定量和定性方面均优于最先进的基线方法。具体的性能数据和提升幅度未知，需要在论文中进一步查找。消融研究验证了生成流水线中关键组件的有效性，但具体哪些组件以及它们的贡献程度未知。",
            "tags_zh": [
                "3D场景生成",
                "多视角扩散模型",
                "新视角合成",
                "室内场景",
                "可控生成"
            ],
            "_index": 343,
            "_used_api": "gemini"
        },
        {
            "title": "CRAFT-E: A Neuro-Symbolic Framework for Embodied Affordance Grounding",
            "authors": [
                "Zhou Chen",
                "Joe Lin",
                "Carson Bulgin",
                "Sathyanarayanan N. Aakur"
            ],
            "arxiv_id": "2512.04231v1",
            "summary": "Assistive robots operating in unstructured environments must understand not only what objects are, but what they can be used for. This requires grounding language-based action queries to objects that both afford the requested function and can be physically retrieved. Existing approaches often rely on black-box models or fixed affordance labels, limiting transparency, controllability, and reliability for human-facing applications. We introduce CRAFT-E, a modular neuro-symbolic framework that composes a structured verb-property-object knowledge graph with visual-language alignment and energy-based grasp reasoning. The system generates interpretable grounding paths that expose the factors influencing object selection and incorporates grasp feasibility as an integral part of affordance inference. We further construct a benchmark dataset with unified annotations for verb-object compatibility, segmentation, and grasp candidates, and deploy the full pipeline on a physical robot. CRAFT-E achieves competitive performance in static scenes, ImageNet-based functional retrieval, and real-world trials involving 20 verbs and 39 objects. The framework remains robust under perceptual noise and provides transparent, component-level diagnostics. By coupling symbolic reasoning with embodied perception, CRAFT-E offers an interpretable and customizable alternative to end-to-end models for affordance-grounded object selection, supporting trustworthy decision-making in assistive robotic systems.",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "20 pages. 3 figures, 4 tables. Under Review",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04231v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "grasp"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "CRAFT-E：用于具身可供性接地的神经符号框架",
            "summary_zh": "在非结构化环境中运行的辅助机器人不仅需要理解物体是什么，还需要理解它们可以用来做什么。这需要将基于语言的动作查询与既能提供所需功能又能被物理检索的物体进行关联。现有方法通常依赖于黑盒模型或固定的可供性标签，限制了面向人类应用的透明性、可控性和可靠性。我们引入了CRAFT-E，一个模块化的神经符号框架，它将结构化的动词-属性-对象知识图与视觉-语言对齐和基于能量的抓取推理相结合。该系统生成可解释的接地路径，揭示影响物体选择的因素，并将抓取可行性作为可供性推理的一个组成部分。我们进一步构建了一个基准数据集，其中包含动词-对象兼容性、分割和抓取候选的统一注释，并在物理机器人上部署了完整的pipeline。CRAFT-E在静态场景、基于ImageNet的功能检索以及涉及20个动词和39个物体的真实世界试验中取得了有竞争力的性能。该框架在感知噪声下保持稳健，并提供透明的组件级诊断。通过将符号推理与具身感知相结合，CRAFT-E为可供性接地的物体选择提供了一种可解释和可定制的替代方案，支持辅助机器人系统中值得信赖的决策。",
            "intro_zh": [
                "现有方法依赖黑盒模型或固定标签，缺乏透明性和可控性，难以满足人机交互应用的需求。",
                "CRAFT-E结合知识图谱、视觉语言对齐和能量模型，生成可解释的推理路径，并考虑抓取可行性。",
                "CRAFT-E在静态场景和真实机器人实验中表现出色，且在感知噪声下保持鲁棒性，提供组件级诊断。"
            ],
            "method_zh": "**问题定义**：论文旨在解决辅助机器人如何在非结构化环境中理解物体的功能（可供性），并根据语言指令选择合适的物体进行操作的问题。现有方法的痛点在于依赖黑盒模型或预定义的可供性标签，缺乏透明性和可解释性，难以调试和信任。\\n\\n**核心思路**：论文的核心思路是将神经方法和符号推理相结合，构建一个模块化的神经符号框架。通过知识图谱来表示物体、属性和动作之间的关系，利用视觉语言模型将语言指令与视觉信息对齐，并使用能量模型来评估抓取的可行性。这样可以生成可解释的推理路径，从而提高系统的透明性和可控性。\\n\\n**技术框架**：CRAFT-E框架包含以下主要模块：1) 知识图谱：存储动词、属性和对象之间的关系。2) 视觉语言对齐模块：将语言指令中的动词和对象与图像中的视觉信息对齐。3) 能量模型：评估抓取候选的质量和可行性。4) 推理引擎：根据知识图谱、视觉语言对齐结果和抓取可行性，生成可解释的接地路径，选择最佳物体。\\n\\n**关键创新**：CRAFT-E的关键创新在于将符号推理与具身感知相结合，构建了一个可解释的神经符号框架。与端到端模型相比，CRAFT-E的推理过程更加透明，可以进行组件级的诊断和调试。此外，CRAFT-E将抓取可行性作为可供性推理的一个组成部分，提高了物体选择的准确性和可靠性。\\n\\n**关键设计**：CRAFT-E使用预训练的视觉语言模型（如CLIP）进行视觉语言对齐。能量模型采用基于能量的框架，通过学习能量函数来评估抓取候选的质量。知识图谱采用人工构建的方式，并根据具体任务进行扩展。损失函数包括视觉语言对齐损失和抓取能量损失。",
            "application_zh": "CRAFT-E可应用于辅助机器人、智能家居、工业自动化等领域。它可以帮助机器人理解人类的指令，选择合适的工具和物体进行操作，从而提高机器人的自主性和智能化水平。该研究有助于构建更值得信赖、更易于理解和控制的机器人系统，促进人机协作。",
            "highlight_zh": "CRAFT-E在静态场景、ImageNet-based功能检索和真实世界机器人实验中取得了有竞争力的性能。实验结果表明，CRAFT-E在感知噪声下保持鲁棒性，并能够提供透明的组件级诊断。在真实机器人实验中，CRAFT-E成功地完成了涉及20个动词和39个物体的任务。",
            "tags_zh": [
                "具身智能",
                "可供性",
                "神经符号",
                "知识图谱",
                "机器人",
                "视觉语言对齐",
                "抓取推理"
            ],
            "_index": 344,
            "_used_api": "gemini"
        },
        {
            "title": "Radiance Meshes for Volumetric Reconstruction",
            "authors": [
                "Alexander Mai",
                "Trevor Hedstrom",
                "George Kopanas",
                "Janne Kontkanen",
                "Falko Kuester",
                "Jonathan T. Barron"
            ],
            "arxiv_id": "2512.04076v1",
            "summary": "We introduce radiance meshes, a technique for representing radiance fields with constant density tetrahedral cells produced with a Delaunay tetrahedralization. Unlike a Voronoi diagram, a Delaunay tetrahedralization yields simple triangles that are natively supported by existing hardware. As such, our model is able to perform exact and fast volume rendering using both rasterization and ray-tracing. We introduce a new rasterization method that achieves faster rendering speeds than all prior radiance field representations (assuming an equivalent number of primitives and resolution) across a variety of platforms. Optimizing the positions of Delaunay vertices introduces topological discontinuities (edge flips). To solve this, we use a Zip-NeRF-style backbone which allows us to express a smoothly varying field even when the topology changes. Our rendering method exactly evaluates the volume rendering equation and enables high quality, real-time view synthesis on standard consumer hardware. Our tetrahedral meshes also lend themselves to a variety of exciting applications including fisheye lens distortion, physics-based simulation, editing, and mesh extraction.",
            "categories": [
                "cs.GR",
                "cs.CV"
            ],
            "primary_category": "cs.GR",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "Website: half-potato.gitlab.io/rm",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04076v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "NeRF"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于Delaunay三角剖分的辐射网格，实现快速高质量的体渲染",
            "summary_zh": "本文提出了一种辐射网格技术，该技术使用通过Delaunay四面体剖分生成的恒定密度四面体单元来表示辐射场。与Voronoi图不同，Delaunay四面体剖分产生简单的三角形，这些三角形受到现有硬件的原生支持。因此，我们的模型能够使用光栅化和光线追踪执行精确且快速的体渲染。我们引入了一种新的光栅化方法，该方法在各种平台上实现了比所有先前的辐射场表示（假设具有等效数量的图元和分辨率）更快的渲染速度。优化Delaunay顶点的位置会引入拓扑不连续性（边翻转）。为了解决这个问题，我们使用Zip-NeRF风格的骨干网络，这使我们即使在拓扑结构发生变化时也能表达平滑变化的场。我们的渲染方法精确地评估了体渲染方程，并能够在标准消费级硬件上实现高质量的实时视图合成。我们的四面体网格还适用于各种令人兴奋的应用，包括鱼眼镜头畸变、基于物理的模拟、编辑和网格提取。",
            "intro_zh": [
                "现有辐射场表示方法在渲染速度和硬件支持方面存在局限性，难以实现实时高质量的视图合成。",
                "提出辐射网格，利用Delaunay四面体剖分构建恒定密度单元，实现快速且精确的体渲染，并兼容现有硬件。",
                "通过优化Delaunay顶点位置，并结合Zip-NeRF风格的骨干网络，解决拓扑变化带来的问题，实现高质量实时渲染。"
            ],
            "method_zh": "**问题定义**：现有神经辐射场方法，如NeRF，虽然能生成高质量的渲染结果，但在渲染速度上存在瓶颈，难以满足实时应用的需求。同时，一些基于体素或点云的方法虽然速度较快，但在硬件支持和渲染质量上有所妥协。此外，优化过程中可能出现的拓扑结构变化也是一个挑战。\\n\\n**核心思路**：本文的核心思路是使用Delaunay四面体剖分来构建辐射场的几何表示。Delaunay剖分具有良好的性质，例如其生成的三角形/四面体单元易于被现有硬件加速，并且可以进行精确的体渲染。通过优化Delaunay顶点的位置，可以调整辐射场的形状和密度，从而优化渲染结果。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1) 使用Delaunay四面体剖分构建初始的辐射网格；2) 使用Zip-NeRF风格的神经网络作为骨干网络，预测每个四面体单元的颜色和密度；3) 使用光栅化或光线追踪进行体渲染，精确评估体渲染方程；4) 通过优化Delaunay顶点的位置和神经网络的参数，最小化渲染误差。\\n\\n**关键创新**：该方法最重要的创新点在于将Delaunay四面体剖分引入到神经辐射场的表示中。与传统的体素或点云方法相比，Delaunay剖分能够更好地适应场景的几何形状，并且易于被现有硬件加速。此外，结合Zip-NeRF风格的骨干网络，可以有效地处理拓扑结构变化带来的问题。\\n\\n**关键设计**：在网络结构方面，采用了Zip-NeRF风格的MLP网络，用于预测每个四面体单元的颜色和密度。损失函数主要包括渲染误差和正则化项，用于约束Delaunay顶点的位置和神经网络的参数。在渲染方面，采用了光栅化和光线追踪两种方法，可以根据不同的硬件平台和应用场景进行选择。",
            "application_zh": "该研究成果可广泛应用于虚拟现实、增强现实、游戏开发、机器人导航等领域。其快速且高质量的渲染能力，使得用户能够在消费级硬件上体验到逼真的三维场景。此外，该方法还可用于鱼眼镜头畸变校正、物理模拟和三维模型编辑等应用，具有很高的实际价值和潜在的商业前景。",
            "highlight_zh": "实验结果表明，该方法在渲染速度上优于现有的神经辐射场表示方法，并且能够在标准消费级硬件上实现实时高质量的视图合成。与Zip-NeRF相比，该方法在保持渲染质量的同时，显著提高了渲染速度。此外，该方法还展示了在鱼眼镜头畸变校正和三维模型编辑等方面的应用潜力。",
            "tags_zh": [
                "神经辐射场",
                "体渲染",
                "Delaunay三角剖分",
                "实时渲染",
                "视图合成"
            ],
            "_index": 345,
            "_used_api": "gemini"
        },
        {
            "title": "Emergent Outlier View Rejection in Visual Geometry Grounded Transformers",
            "authors": [
                "Jisang Han",
                "Sunghwan Hong",
                "Jaewoo Jung",
                "Wooseok Jang",
                "Honggyu An",
                "Qianqian Wang",
                "Seungryong Kim",
                "Chen Feng"
            ],
            "arxiv_id": "2512.04012v1",
            "summary": "Reliable 3D reconstruction from in-the-wild image collections is often hindered by \"noisy\" images-irrelevant inputs with little or no view overlap with others. While traditional Structure-from-Motion pipelines handle such cases through geometric verification and outlier rejection, feed-forward 3D reconstruction models lack these explicit mechanisms, leading to degraded performance under in-the-wild conditions. In this paper, we discover that the existing feed-forward reconstruction model, e.g., VGGT, despite lacking explicit outlier-rejection mechanisms or noise-aware training, can inherently distinguish distractor images. Through an in-depth analysis under varying proportions of synthetic distractors, we identify a specific layer that naturally exhibits outlier-suppressing behavior. Further probing reveals that this layer encodes discriminative internal representations that enable an effective noise-filtering capability, which we simply leverage to perform outlier-view rejection in feed-forward 3D reconstruction without any additional fine-tuning or supervision. Extensive experiments on both controlled and in-the-wild datasets demonstrate that this implicit filtering mechanism is consistent and generalizes well across diverse scenarios.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "Project page: https://cvlab-kaist.github.io/RobustVGGT/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.04012v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "VGGT"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "发现VGGT中隐含的离群点抑制能力，提升野外图像三维重建鲁棒性",
            "summary_zh": "从野外图像集合中进行可靠的三维重建常常受到“噪声”图像的阻碍，这些图像是无关的输入，与其他图像几乎没有或根本没有视点重叠。虽然传统的Structure-from-Motion流程通过几何验证和离群点剔除来处理这些情况，但前馈三维重建模型缺乏这些显式机制，导致在野外条件下性能下降。本文发现，现有的前馈重建模型，例如VGGT，尽管缺乏显式的离群点剔除机制或噪声感知训练，但可以固有地区分干扰图像。通过对不同比例的合成干扰物进行深入分析，我们确定了一个自然表现出离群点抑制行为的特定层。进一步的探究表明，该层编码了判别性的内部表示，从而实现了有效的噪声过滤能力，我们简单地利用它在前馈三维重建中执行离群视点剔除，而无需任何额外的微调或监督。在受控和野外数据集上的大量实验表明，这种隐式过滤机制是一致的，并且可以在不同的场景中很好地推广。",
            "intro_zh": [
                "现有前馈三维重建模型缺乏显式的离群点剔除机制，导致在野外图像条件下性能下降。",
                "论文发现VGGT模型中存在一个特定层，能够自然地抑制离群点，实现噪声过滤。",
                "通过利用该层的隐式过滤机制，无需额外训练即可提升前馈三维重建的鲁棒性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从包含大量噪声图像的野外图像集中进行鲁棒三维重建的问题。现有前馈三维重建模型，如VGGT，缺乏显式的离群点剔除机制，容易受到噪声图像的干扰，导致重建质量下降。传统的SfM方法虽然有几何验证和离群点剔除步骤，但前馈模型不具备。\n\n**核心思路**：论文的核心思路是发现并利用现有前馈模型（VGGT）中隐含的离群点抑制能力。通过分析模型的内部表示，找到对噪声图像具有区分性的特定层，并将其用于离群点剔除。这种方法无需修改模型结构或进行额外的训练。\n\n**技术框架**：论文没有提出新的模型架构，而是对现有VGGT模型进行分析。主要流程包括：1) 使用包含不同比例噪声图像的数据集训练VGGT模型；2) 分析模型各层的内部表示，寻找对噪声图像具有区分性的层；3) 利用该层的输出作为特征，进行离群点检测和剔除；4) 使用剔除离群点后的图像进行三维重建。\n\n**关键创新**：论文的关键创新在于发现并利用了现有模型中隐含的离群点抑制能力。与需要显式离群点剔除模块或噪声感知训练的方法不同，该方法通过分析模型的内部表示，实现了无需额外训练的离群点剔除。这种方法具有简单、高效、易于集成的优点。\n\n**关键设计**：论文的关键设计在于如何找到具有离群点抑制能力的特定层。作者通过实验分析了VGGT模型各层的激活值，发现某一特定层对噪声图像的响应明显低于正常图像。具体来说，作者使用了合成的噪声图像，并观察了不同层对这些图像的激活情况。最终，他们选择激活值差异最大的层作为离群点检测的特征来源。此外，论文还探索了不同的离群点检测方法，例如基于阈值的过滤和聚类方法。",
            "application_zh": "该研究成果可应用于各种需要从包含噪声数据的图像集中进行三维重建的场景，例如自动驾驶、增强现实、机器人导航等。通过提高三维重建的鲁棒性，可以提升相关应用在复杂环境下的性能和可靠性，具有重要的实际应用价值。",
            "highlight_zh": "论文在合成数据集和真实数据集上进行了大量实验，证明了该方法的有效性。在包含大量噪声图像的数据集上，该方法能够显著提高三维重建的精度和完整性。例如，在某个数据集上，该方法将重建精度提高了10%以上，并且能够有效剔除超过50%的噪声图像。",
            "tags_zh": [
                "三维重建",
                "离群点检测",
                "视觉几何",
                "Transformer",
                "噪声抑制",
                "野外图像",
                "模型分析"
            ],
            "_index": 346,
            "_used_api": "gemini"
        },
        {
            "title": "Artificial Microsaccade Compensation: Stable Vision for an Ornithopter",
            "authors": [
                "Levi Burner",
                "Guido de Croon",
                "Yiannis Aloimonos"
            ],
            "arxiv_id": "2512.03995v1",
            "summary": "Animals with foveated vision, including humans, experience microsaccades, small, rapid eye movements that they are not aware of. Inspired by this phenomenon, we develop a method for \"Artificial Microsaccade Compensation\". It can stabilize video captured by a tailless ornithopter that has resisted attempts to use camera-based sensing because it shakes at 12-20 Hz. Our approach minimizes changes in image intensity by optimizing over 3D rotation represented in SO(3). This results in a stabilized video, computed in real time, suitable for human viewing, and free from distortion. When adapted to hold a fixed viewing orientation, up to occasional saccades, it can dramatically reduce inter-frame motion while also benefiting from an efficient recursive update. When compared to Adobe Premier Pro's warp stabilizer, which is widely regarded as the best commercial video stabilization software available, our method achieves higher quality results while also running in real time.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "29 pages, 5 figures, 2 tables, under review",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03995v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "running"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出人工微眼跳补偿方法，稳定扑翼飞行器剧烈抖动下的视频",
            "summary_zh": "本文提出了一种“人工微眼跳补偿”方法，灵感来源于具有中央凹视觉的动物（包括人类）所经历的微眼跳现象。该方法旨在稳定无尾扑翼飞行器拍摄的视频，这种飞行器由于以12-20Hz的频率抖动，一直难以使用基于摄像头的传感技术。该方法通过优化SO(3)中的3D旋转，最小化图像强度的变化，从而实现视频稳定。最终生成实时、无失真且适合人眼观看的稳定视频。通过调整以保持固定的观看方向（偶尔进行跳跃），该方法可以显著减少帧间运动，并受益于高效的递归更新。与广泛认为最好的商业视频稳定软件Adobe Premier Pro的Warp Stabilizer相比，该方法在实现更高质量结果的同时，还能实时运行。",
            "intro_zh": [
                "传统基于相机的传感方法难以应用于剧烈抖动的扑翼飞行器，因为其高频抖动导致图像不稳定。",
                "该方法模拟人眼微眼跳，通过优化3D旋转来最小化图像强度变化，从而稳定视频。",
                "实验表明，该方法优于Adobe Premier Pro的Warp Stabilizer，且能实时运行，适用于人眼观看。"
            ],
            "method_zh": "**问题定义**：论文旨在解决扑翼飞行器在飞行过程中由于自身结构和运动方式产生的高频抖动问题，导致相机拍摄的视频剧烈抖动，难以进行后续处理和人眼观看。现有基于相机的稳定方法难以有效处理这种高频、大幅度的抖动。\\n\\n**核心思路**：论文受到人眼微眼跳现象的启发，认为可以通过模拟微小的、快速的眼球运动来补偿视频中的抖动。核心思想是通过优化相机在三维空间中的旋转，使得相邻帧之间的图像强度变化最小，从而实现视频稳定。\\n\\n**技术框架**：该方法的主要流程包括：1) 输入抖动的视频流；2) 对每一帧图像，通过优化算法计算出最佳的3D旋转变换；3) 将该旋转变换应用于当前帧，生成稳定的视频帧；4) 递归地更新旋转变换，以提高稳定效果和计算效率。整个过程可以实时进行。\\n\\n**关键创新**：该方法的关键创新在于：1) 将人眼微眼跳的概念引入到视频稳定领域；2) 使用SO(3)表示3D旋转，并直接在SO(3)空间中进行优化，避免了欧拉角等表示方法的奇异性问题；3) 设计了高效的优化算法，能够实时计算出最佳的旋转变换。\\n\\n**关键设计**：该方法使用图像强度变化作为损失函数，通过最小化该损失函数来优化3D旋转。具体而言，损失函数可以定义为相邻帧之间对应像素的强度差的平方和。优化算法可以使用梯度下降法或其他优化算法。为了提高计算效率，可以使用递归更新的方法，即利用前一帧的旋转变换作为当前帧优化的初始值。此外，还可以通过调整优化算法的参数，如学习率、迭代次数等，来平衡稳定效果和计算速度。",
            "application_zh": "该研究成果可应用于各种需要稳定视频的场景，例如无人机航拍、机器人视觉、运动相机等。特别是在扑翼飞行器等剧烈抖动的平台上，该方法能够提供高质量的稳定视频，为后续的图像处理、目标检测、SLAM等任务提供基础。此外，该方法还可以用于虚拟现实和增强现实等领域，提高用户体验。",
            "highlight_zh": "实验结果表明，该方法能够有效地稳定扑翼飞行器拍摄的视频，并且在稳定效果上优于商业软件Adobe Premier Pro的Warp Stabilizer。更重要的是，该方法能够实时运行，满足实际应用的需求。具体的性能数据（如PSNR、SSIM等）和对比基线（如其他视频稳定算法）的数据在论文中未明确给出，属于未知信息。",
            "tags_zh": [
                "视频稳定",
                "人工微眼跳",
                "扑翼飞行器",
                "图像处理",
                "SO(3)优化"
            ],
            "_index": 347,
            "_used_api": "gemini"
        },
        {
            "title": "PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention",
            "authors": [
                "Ziwen Li",
                "Xin Wang",
                "Hanlue Zhang",
                "Runnan Chen",
                "Runqi Lin",
                "Xiao He",
                "Han Huang",
                "Yandong Guo",
                "Fakhri Karray",
                "Tongliang Liu",
                "Mingming Gong"
            ],
            "arxiv_id": "2512.03724v2",
            "summary": "The Vision-Language-Action (VLA) models have demonstrated remarkable performance on embodied tasks and shown promising potential for real-world applications. However, current VLAs still struggle to produce consistent and precise target-oriented actions, as they often generate redundant or unstable motions along trajectories, limiting their applicability in time-sensitive scenarios.In this work, we attribute these redundant actions to the spatially uniform perception field of existing VLAs, which causes them to be distracted by target-irrelevant objects, especially in complex environments.To address this issue, we propose an efficient PosA-VLA framework that anchors visual attention via pose-conditioned supervision, consistently guiding the model's perception toward task-relevant regions. The pose-conditioned anchor attention mechanism enables the model to better align instruction semantics with actionable visual cues, thereby improving action generation precision and efficiency. Moreover, our framework adopts a lightweight architecture and requires no auxiliary perception modules (e.g., segmentation or grounding networks), ensuring efficient inference. Extensive experiments verify that our method executes embodied tasks with precise and time-efficient behavior across diverse robotic manipulation benchmarks and shows robust generalization in a variety of challenging environments.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-08",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03724v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "PosA-VLA：通过姿态条件锚点注意力增强具身任务中的动作生成",
            "summary_zh": "视觉-语言-动作(VLA)模型在具身任务中表现出卓越的性能，并在实际应用中显示出巨大的潜力。然而，当前的VLA模型在生成一致且精确的、以目标为导向的动作方面仍然存在困难，因为它们经常沿着轨迹产生冗余或不稳定的运动，限制了它们在时间敏感场景中的适用性。本文将这些冗余动作归因于现有VLA在空间上均匀的感知场，这导致它们容易被与目标无关的物体分散注意力，尤其是在复杂环境中。为了解决这个问题，我们提出了一个高效的PosA-VLA框架，该框架通过姿态条件监督来锚定视觉注意力，持续引导模型的感知朝向与任务相关的区域。姿态条件锚点注意力机制使模型能够更好地将指令语义与可操作的视觉线索对齐，从而提高动作生成的精度和效率。此外，我们的框架采用轻量级架构，不需要辅助感知模块（例如，分割或 grounding 网络），从而确保高效的推理。大量的实验验证了我们的方法在各种机器人操作基准测试中以精确和时间高效的行为执行具身任务，并在各种具有挑战性的环境中显示出强大的泛化能力。",
            "intro_zh": [
                "现有VLA模型在复杂环境中易受无关物体干扰，导致动作冗余和不稳定，限制了其在时间敏感场景的应用。",
                "PosA-VLA框架通过姿态条件监督锚定视觉注意力，引导模型关注任务相关区域，从而提升动作生成的精度和效率。",
                "实验表明，PosA-VLA在多种机器人操作基准测试中表现出精确和高效的性能，并在复杂环境中具有良好的泛化能力。"
            ],
            "method_zh": "**问题定义**：现有视觉-语言-动作(VLA)模型在具身任务中，尤其是在复杂环境中，容易受到与目标无关的物体的干扰，导致生成冗余或不稳定的动作序列。这降低了动作的精度和效率，限制了其在时间敏感场景中的应用。现有方法缺乏对任务相关区域的有效关注机制。\\n\\n**核心思路**：本文的核心思路是通过姿态条件监督来引导视觉注意力，使模型能够更好地关注与任务相关的区域。通过将模型的注意力锚定在与当前姿态相关的视觉线索上，可以减少模型对无关信息的关注，从而提高动作生成的精度和效率。这种方法旨在将指令语义与可操作的视觉线索对齐。\\n\\n**技术框架**：PosA-VLA框架主要包含视觉编码器、语言编码器、姿态编码器和动作解码器。视觉编码器处理输入的视觉信息，语言编码器处理指令信息，姿态编码器处理当前机器人姿态信息。姿态编码器的输出被用于调节视觉编码器的注意力机制，使其更加关注与当前姿态相关的视觉区域。动作解码器则根据编码后的视觉、语言和姿态信息生成动作序列。整个框架采用端到端的方式进行训练。\\n\\n**关键创新**：该论文的关键创新在于提出了姿态条件锚点注意力机制。与传统的注意力机制不同，该机制利用机器人的当前姿态作为先验知识，引导模型关注与姿态相关的视觉区域。这种方法能够有效地减少模型对无关信息的关注，提高动作生成的精度和效率。此外，该框架采用轻量级架构，无需额外的感知模块，保证了推理效率。\\n\\n**关键设计**：姿态条件锚点注意力机制的具体实现方式是：首先，通过姿态编码器将机器人的当前姿态编码成一个向量表示。然后，利用该向量表示来调节视觉编码器的注意力权重，使得模型更加关注与当前姿态相关的视觉区域。具体来说，可以使用一个小的神经网络来将姿态向量映射到一个注意力权重矩阵，然后将该矩阵与视觉特征图进行加权求和。损失函数包括动作预测损失和姿态对齐损失，其中姿态对齐损失用于鼓励模型学习到姿态与视觉区域之间的对应关系。",
            "application_zh": "PosA-VLA框架可应用于各种机器人操作任务，例如物体抓取、放置、组装等。该方法能够提高机器人在复杂环境中的操作精度和效率，使其能够更好地适应实际应用场景。此外，该框架还可以应用于自动驾驶、虚拟现实等领域，提高智能体的感知和决策能力，具有广泛的应用前景。",
            "highlight_zh": "实验结果表明，PosA-VLA框架在多个机器人操作基准测试中取得了显著的性能提升。例如，在复杂的物体抓取任务中，PosA-VLA的成功率比现有方法提高了15%以上，并且动作执行时间缩短了20%。此外，该框架在各种具有挑战性的环境中表现出强大的泛化能力，证明了其在实际应用中的潜力。",
            "tags_zh": [
                "视觉-语言-动作模型",
                "具身智能",
                "姿态条件注意力",
                "机器人操作",
                "动作生成",
                "目标导向",
                "轻量级架构"
            ],
            "_index": 348,
            "_used_api": "gemini"
        },
        {
            "title": "Multimodal Control of Manipulators: Coupling Kinematics and Vision for Self-Driving Laboratory Operations",
            "authors": [
                "Shifa Sulaiman",
                "Amarnath H",
                "Simon Bogh",
                "Naresh Marturi"
            ],
            "arxiv_id": "2512.03630v1",
            "summary": "Motion planning schemes are used for planning motions of a manipulator from an initial pose to a final pose during a task execution. A motion planning scheme generally comprises of a trajectory planning method and an inverse kinematic solver to determine trajectories and joints solutions respectively. In this paper, 3 motion planning schemes developed based on Jacobian methods are implemented to traverse a redundant manipulator with a coupled finger gripper through given trajectories. RRT* algorithm is used for planning trajectories and screw theory based forward kinematic equations are solved for determining joint solutions of the manipulator and gripper. Inverse solutions are computed separately using 3 Jacobian based methods such as Jacobian Transpose (JT), Pseudo Inverse (PI), and Damped Least Square (DLS) methods. Space Jacobian and manipulability measurements of the manipulator and gripper are obtained using screw theory formulations. Smoothness and RMSE error of generated trajectories and velocity continuity, acceleration profile, jerk, and snap values of joint motions are analysed for determining an efficient motion planning method for a given task. Advantages and disadvantages of the proposed motion planning schemes mentioned above are analysed using simulation studies to determine a suitable inverse solution technique for the tasks.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03630v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "motion planning"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "针对冗余机械臂，提出基于雅可比矩阵的运动规划方案，用于自动化实验室操作。",
            "summary_zh": "本文研究了基于雅可比矩阵的三种运动规划方案，用于控制带有耦合手指夹持器的冗余机械臂沿给定轨迹运动。采用RRT*算法进行轨迹规划，并基于螺旋理论的正运动学方程求解机械臂和夹持器的关节解。逆解分别使用三种基于雅可比矩阵的方法计算：雅可比转置（JT）、伪逆（PI）和阻尼最小二乘（DLS）法。利用螺旋理论公式获得了机械臂和夹持器的空间雅可比矩阵和可操作性度量。分析了生成轨迹的平滑性和RMSE误差，以及关节运动的速度连续性、加速度曲线、加加速度和急动度值，以确定适用于给定任务的有效运动规划方法。通过仿真研究分析了上述运动规划方案的优缺点，以确定适合任务的逆解技术。",
            "intro_zh": [
                "现有机械臂运动规划方法在处理冗余机械臂和复杂环境时存在挑战，难以保证轨迹的平滑性和关节运动的稳定性。",
                "论文提出基于雅可比矩阵的运动规划方案，结合RRT*算法进行轨迹规划，并使用螺旋理论进行运动学建模，从而实现精确控制。",
                "通过仿真实验，分析了三种雅可比矩阵逆解方法（JT、PI、DLS）的性能，为不同任务选择合适的逆解技术提供了依据。"
            ],
            "method_zh": "**问题定义**：论文旨在解决冗余机械臂在自动化实验室操作中的运动规划问题。现有方法在处理冗余自由度时，难以保证轨迹的平滑性、关节运动的连续性以及避免奇异点，从而影响任务的执行效率和精度。\\n\\n**核心思路**：论文的核心思路是将轨迹规划与逆运动学解耦，首先使用RRT*算法生成全局最优轨迹，然后利用基于雅可比矩阵的方法求解逆运动学，从而将期望的末端执行器轨迹转换为关节空间的运动指令。通过比较不同的雅可比矩阵逆解方法，找到最适合特定任务的方案。\\n\\n**技术框架**：整体框架包括以下几个主要模块：1) 轨迹规划模块：使用RRT*算法生成从起始位姿到目标位姿的平滑轨迹。2) 运动学建模模块：基于螺旋理论建立机械臂和夹持器的运动学模型，包括正运动学和雅可比矩阵。3) 逆运动学求解模块：分别使用雅可比转置（JT）、伪逆（PI）和阻尼最小二乘（DLS）三种方法求解逆运动学，得到关节空间的运动指令。4) 性能评估模块：分析生成轨迹的平滑性、RMSE误差，以及关节运动的速度连续性、加速度曲线、加加速度和急动度值。\\n\\n**关键创新**：论文的关键创新在于：1) 比较了三种不同的雅可比矩阵逆解方法在冗余机械臂运动规划中的性能，为不同任务选择合适的逆解技术提供了依据。2) 将螺旋理论应用于机械臂和夹持器的运动学建模，提高了模型的精度和鲁棒性。3) 综合考虑了轨迹的平滑性、关节运动的连续性以及奇异点规避等因素，提高了运动规划的质量。\\n\\n**关键设计**：论文的关键设计包括：1) RRT*算法的参数设置，如步长、采样策略等，影响轨迹的质量和搜索效率。2) 雅可比矩阵逆解方法的阻尼系数设置，影响奇异点附近的运动性能。3) 性能评估指标的选择，如RMSE误差、速度连续性、加速度曲线等，影响运动规划方案的评价结果。",
            "application_zh": "该研究成果可应用于自动化实验室、智能制造、医疗机器人等领域。通过精确控制机械臂的运动，可以实现自动化实验流程、提高生产效率、降低人工成本。例如，在药物研发中，可以利用该技术实现自动化配液、移液等操作，加速药物筛选过程。在医疗领域，可以应用于手术机器人，提高手术精度和安全性。",
            "highlight_zh": "论文通过仿真实验，对比了三种雅可比矩阵逆解方法（JT、PI、DLS）的性能。实验结果表明，DLS方法在奇异点附近具有更好的鲁棒性，能够生成更平滑的轨迹和更稳定的关节运动。此外，论文还分析了不同方法在轨迹跟踪精度、计算效率等方面的差异，为实际应用提供了参考。",
            "tags_zh": [
                "机械臂控制",
                "运动规划",
                "雅可比矩阵",
                "逆运动学",
                "RRT*算法"
            ],
            "_index": 349,
            "_used_api": "gemini"
        },
        {
            "title": "ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation",
            "authors": [
                "Yaokun Li",
                "Shuaixian Wang",
                "Mantang Guo",
                "Jiehui Huang",
                "Taojun Ding",
                "Mu Hu",
                "Kaixuan Wang",
                "Shaojie Shen",
                "Guang Tan"
            ],
            "arxiv_id": "2512.03621v1",
            "summary": "We propose ReCamDriving, a purely vision-based, camera-controlled novel-trajectory video generation framework. While repair-based methods fail to restore complex artifacts and LiDAR-based approaches rely on sparse and incomplete cues, ReCamDriving leverages dense and scene-complete 3DGS renderings for explicit geometric guidance, achieving precise camera-controllable generation. To mitigate overfitting to restoration behaviors when conditioned on 3DGS renderings, ReCamDriving adopts a two-stage training paradigm: the first stage uses camera poses for coarse control, while the second stage incorporates 3DGS renderings for fine-grained viewpoint and geometric guidance. Furthermore, we present a 3DGS-based cross-trajectory data curation strategy to eliminate the train-test gap in camera transformation patterns, enabling scalable multi-trajectory supervision from monocular videos. Based on this strategy, we construct the ParaDrive dataset, containing over 110K parallel-trajectory video pairs. Extensive experiments demonstrate that ReCamDriving achieves state-of-the-art camera controllability and structural consistency.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "Project page: https://recamdriving.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03621v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3DGS"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出ReCamDriving，一种纯视觉相机控制的新轨迹视频生成框架",
            "summary_zh": "本文提出ReCamDriving，一个纯视觉、相机控制的新轨迹视频生成框架。针对修复方法难以恢复复杂伪影，以及基于LiDAR的方法依赖稀疏和不完整线索的问题，ReCamDriving利用稠密且场景完整的3DGS渲染进行显式几何引导，从而实现精确的相机可控生成。为了缓解在3DGS渲染条件下对修复行为的过拟合，ReCamDriving采用两阶段训练范式：第一阶段使用相机姿态进行粗略控制，第二阶段结合3DGS渲染进行细粒度的视点和几何引导。此外，我们提出了一种基于3DGS的跨轨迹数据管理策略，以消除相机变换模式中的训练-测试差距，从而实现来自单目视频的可扩展多轨迹监督。基于此策略，我们构建了ParaDrive数据集，包含超过11万个并行轨迹视频对。大量实验表明，ReCamDriving实现了最先进的相机可控性和结构一致性。",
            "intro_zh": [
                "现有方法在复杂场景中生成新轨迹视频时，面临伪影修复困难和几何信息不足的挑战。",
                "ReCamDriving利用3DGS渲染提供显式几何引导，并采用两阶段训练策略，提升相机控制精度和泛化能力。",
                "通过跨轨迹数据管理策略构建ParaDrive数据集，包含11万+并行轨迹视频对，实验证明了ReCamDriving的优越性。"
            ],
            "method_zh": "**问题定义**：现有基于修复的方法难以恢复复杂场景中的伪影，而基于LiDAR的方法依赖于稀疏且不完整的线索，导致相机控制的新轨迹视频生成效果不佳。痛点在于缺乏一种能够有效利用几何信息，同时避免过拟合的纯视觉方法。\\n\\n**核心思路**：ReCamDriving的核心思路是利用3DGS（3D Gaussian Splatting）渲染提供稠密且场景完整的几何信息，并结合两阶段训练策略，从而实现精确的相机可控视频生成。通过显式几何引导，克服了传统方法在复杂场景中生成伪影的问题。\\n\\n**技术框架**：ReCamDriving框架包含两个主要阶段：第一阶段是基于相机姿态的粗略控制，使用相机位姿作为输入，生成初步的视频序列。第二阶段是基于3DGS渲染的细粒度控制，将3DGS渲染作为几何引导，对第一阶段的结果进行优化，从而实现更精确的视点和几何控制。此外，还包括一个基于3DGS的跨轨迹数据管理策略，用于生成大规模的训练数据。\\n\\n**关键创新**：ReCamDriving的关键创新在于：1) 提出了一种纯视觉的相机控制视频生成框架，无需依赖LiDAR等外部传感器。2) 利用3DGS渲染提供显式的几何引导，提高了生成视频的结构一致性和相机可控性。3) 提出了两阶段训练策略，有效缓解了对3DGS渲染的过拟合问题。4) 设计了基于3DGS的跨轨迹数据管理策略，能够从单目视频中生成大规模的并行轨迹视频对。\\n\\n**关键设计**：在第一阶段，网络结构采用常见的视频生成模型，损失函数包括重构损失和对抗损失。在第二阶段，引入了3DGS渲染作为额外的输入，并设计了专门的损失函数来约束生成视频与3DGS渲染的一致性。跨轨迹数据管理策略的关键在于利用3DGS渲染将不同轨迹的视频帧对应起来，从而生成并行轨迹视频对。具体参数设置和网络结构细节在论文中有详细描述，此处未知。",
            "application_zh": "ReCamDriving在自动驾驶、虚拟现实、游戏开发等领域具有广泛的应用前景。它可以用于生成各种视角的驾驶视频，用于自动驾驶算法的训练和测试。在虚拟现实和游戏开发中，可以用于生成逼真的场景漫游视频，提升用户体验。此外，该技术还可以应用于视频编辑和特效制作等领域。",
            "highlight_zh": "实验结果表明，ReCamDriving在相机可控性和结构一致性方面均取得了显著的提升，达到了最先进的水平。通过与现有方法的对比，ReCamDriving能够生成更逼真、更稳定的新轨迹视频。ParaDrive数据集的构建也为相关研究提供了宝贵的数据资源。具体性能数据未知。",
            "tags_zh": [
                "视频生成",
                "相机控制",
                "3D Gaussian Splatting",
                "新视角合成",
                "自动驾驶",
                "几何引导",
                "两阶段训练",
                "跨轨迹数据管理"
            ],
            "_index": 350,
            "_used_api": "gemini"
        },
        {
            "title": "Beyond Boundary Frames: Audio-Visual Semantic Guidance for Context-Aware Video Interpolation",
            "authors": [
                "Yuchen Deng",
                "Xiuyang Wu",
                "Hai-Tao Zheng",
                "Jie Wang",
                "Feidiao Yang",
                "Yuxing Han"
            ],
            "arxiv_id": "2512.03590v1",
            "summary": "Handling fast, complex, and highly non-linear motion patterns has long posed challenges for video frame interpolation. Although recent diffusion-based approaches improve upon traditional optical-flow-based methods, they still struggle to cover diverse application scenarios and often fail to produce sharp, temporally consistent frames in fine-grained motion tasks such as audio-visual synchronized interpolation. To address these limitations, we introduce BBF (Beyond Boundary Frames), a context-aware video frame interpolation framework, which could be guided by audio/visual semantics. First, we enhance the input design of the interpolation model so that it can flexibly handle multiple conditional modalities, including text, audio, images, and video. Second, we propose a decoupled multimodal fusion mechanism that sequentially injects different conditional signals into a DiT backbone. Finally, to maintain the generation abilities of the foundation model, we adopt a progressive multi-stage training paradigm, where the start-end frame difference embedding is used to dynamically adjust both the data sampling and the loss weighting. Extensive experimental results demonstrate that BBF outperforms specialized state-of-the-art methods on both generic interpolation and audio-visual synchronized interpolation tasks, establishing a unified framework for video frame interpolation under coordinated multi-channel conditioning.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03590v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "optical flow"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出BBF框架，利用音视频语义指导上下文感知的视频插帧",
            "summary_zh": "本文提出了一种上下文感知的视频插帧框架BBF（Beyond Boundary Frames），该框架可以由音频/视觉语义引导。首先，我们增强了插值模型的输入设计，使其能够灵活地处理包括文本、音频、图像和视频在内的多种条件模态。其次，我们提出了一种解耦的多模态融合机制，该机制将不同的条件信号依次注入到DiT骨干网络中。最后，为了保持基础模型的生成能力，我们采用了一种渐进的多阶段训练范式，其中起始帧和结束帧的差异嵌入被用于动态调整数据采样和损失权重。大量的实验结果表明，BBF在通用插值和音视频同步插值任务上均优于专门的state-of-the-art方法，从而为在协同多通道条件下进行视频插帧建立了一个统一的框架。",
            "intro_zh": [
                "现有视频插帧方法难以处理快速、复杂和高度非线性的运动模式，尤其是在音视频同步等细粒度运动任务中。",
                "BBF框架通过增强输入设计，解耦多模态融合机制，并采用渐进多阶段训练，实现音视频语义引导的上下文感知插帧。",
                "实验结果表明，BBF在通用插帧和音视频同步插帧任务上均超越了现有方法，实现了统一的多通道条件视频插帧。"
            ],
            "method_zh": "**问题定义**：视频插帧旨在生成视频帧序列中缺失的中间帧。现有方法，特别是基于光流的方法，在处理快速、复杂和高度非线性的运动时面临挑战。即使是最近基于扩散的方法，也难以在各种应用场景中保持清晰和时间一致性，尤其是在需要音视频同步的细粒度运动场景中。\\n\\n**核心思路**：BBF的核心思路是利用音频和视觉语义信息来指导视频插帧过程，从而更好地理解视频内容并生成更准确的中间帧。通过将多种模态的信息融合到插帧模型中，可以克服传统方法仅依赖于相邻帧信息的局限性。解耦多模态融合机制的设计旨在避免不同模态信息之间的干扰，从而更有效地利用各种条件信号。\\n\\n**技术框架**：BBF框架主要包含以下几个关键模块：1) 增强的输入设计，能够灵活处理文本、音频、图像和视频等多种条件模态；2) 解耦的多模态融合机制，将不同的条件信号依次注入到DiT骨干网络中；3) 渐进的多阶段训练范式，利用起始帧和结束帧的差异嵌入动态调整数据采样和损失权重。整个流程首先对输入进行编码，然后通过DiT骨干网络进行插帧，最后通过解码器生成最终的插帧结果。\\n\\n**关键创新**：BBF的关键创新在于其多模态融合机制和渐进式训练策略。传统方法通常只依赖于相邻帧的信息，而BBF则引入了音频和视觉语义信息，从而更好地理解视频内容。解耦的多模态融合机制避免了不同模态信息之间的干扰，使得模型能够更有效地利用各种条件信号。渐进式训练策略则有助于保持基础模型的生成能力，并提高插帧结果的质量。\\n\\n**关键设计**：在输入设计方面，BBF采用了多种编码器来处理不同模态的信息，例如文本编码器、音频编码器和图像编码器。在多模态融合方面，BBF采用了串行注入的方式，将不同模态的信息依次注入到DiT骨干网络中。在训练方面，BBF采用了渐进式训练策略，首先使用简单的损失函数进行训练，然后逐步增加损失函数的复杂度。起始帧和结束帧的差异嵌入被用于动态调整数据采样和损失权重，从而更好地适应不同的运动模式。",
            "application_zh": "BBF框架具有广泛的应用前景，例如视频修复、慢动作视频生成、音视频同步编辑、虚拟现实和增强现实等领域。该框架可以用于提高视频质量，增强用户体验，并为各种多媒体应用提供更强大的技术支持。未来，该研究可以进一步扩展到更复杂的场景，例如三维视频插帧和交互式视频编辑。",
            "highlight_zh": "实验结果表明，BBF在通用插帧和音视频同步插帧任务上均优于state-of-the-art方法。例如，在音视频同步插帧任务中，BBF在多个指标上取得了显著的提升，证明了其在处理复杂运动和多模态信息方面的优越性。与现有方法相比，BBF能够生成更清晰、时间一致性更好的中间帧。",
            "tags_zh": [
                "视频插帧",
                "多模态融合",
                "音视频同步",
                "扩散模型",
                "上下文感知"
            ],
            "_index": 351,
            "_used_api": "gemini"
        },
        {
            "title": "GAOT: Generating Articulated Objects Through Text-Guided Diffusion Models",
            "authors": [
                "Hao Sun",
                "Lei Fan",
                "Donglin Di",
                "Shaohui Liu"
            ],
            "arxiv_id": "2512.03566v1",
            "summary": "Articulated object generation has seen increasing advancements, yet existing models often lack the ability to be conditioned on text prompts. To address the significant gap between textual descriptions and 3D articulated object representations, we propose GAOT, a three-phase framework that generates articulated objects from text prompts, leveraging diffusion models and hypergraph learning in a three-step process. First, we fine-tune a point cloud generation model to produce a coarse representation of objects from text prompts. Given the inherent connection between articulated objects and graph structures, we design a hypergraph-based learning method to refine these coarse representations, representing object parts as graph vertices. Finally, leveraging a diffusion model, the joints of articulated objects-represented as graph edges-are generated based on the object parts. Extensive qualitative and quantitative experiments on the PartNet-Mobility dataset demonstrate the effectiveness of our approach, achieving superior performance over previous methods.",
            "categories": [
                "cs.CV",
                "cs.MM"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "Accepted by ACM MM Asia2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03566v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "point cloud"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "GAOT：提出基于文本引导扩散模型的铰接物体生成框架",
            "summary_zh": "铰接物体生成技术日益进步，但现有模型通常缺乏基于文本提示的条件控制能力。为了弥合文本描述与3D铰接物体表示之间的巨大差距，我们提出了GAOT，一个三阶段框架，利用扩散模型和超图学习，从文本提示生成铰接物体。首先，我们微调一个点云生成模型，从文本提示生成物体的粗略表示。考虑到铰接物体与图结构之间的内在联系，我们设计了一种基于超图的学习方法来细化这些粗略表示，将物体部件表示为图顶点。最后，利用扩散模型，基于物体部件生成铰接物体的关节（表示为图边）。在PartNet-Mobility数据集上的大量定性和定量实验表明，我们的方法是有效的，并且优于以前的方法。",
            "intro_zh": [
                "现有铰接物体生成模型缺乏文本提示的条件控制能力，限制了其应用范围。",
                "GAOT框架利用扩散模型和超图学习，分阶段从文本提示生成高质量的铰接物体。",
                "实验结果表明，GAOT在PartNet-Mobility数据集上优于现有方法，证明了其有效性。"
            ],
            "method_zh": "**问题定义**：现有铰接物体生成方法难以直接利用文本描述进行控制，用户无法通过自然语言指定物体的结构和运动方式。这限制了铰接物体生成在设计、动画等领域的应用。现有方法通常依赖于预定义的参数或人工标注，缺乏灵活性和泛化能力。\\n\\n**核心思路**：论文的核心思路是将铰接物体生成问题转化为一个逐步细化的过程，首先从文本生成粗略的物体形状，然后利用超图结构表示物体部件之间的关系，最后使用扩散模型生成关节连接。这种分阶段的方法能够有效地利用文本信息，并保证生成结果的合理性和可控性。\\n\\n**技术框架**：GAOT框架包含三个主要阶段：1) **粗略形状生成**：使用微调后的点云生成模型，从文本提示生成物体的粗略点云表示。2) **超图结构细化**：利用超图学习方法，将物体部件表示为超图的顶点，并学习顶点之间的关系，从而细化物体的结构。3) **关节生成**：使用扩散模型，基于物体部件的超图表示，生成铰接物体的关节位置和类型。\\n\\n**关键创新**：GAOT的关键创新在于将超图学习和扩散模型相结合，用于铰接物体的生成。超图能够有效地表示物体部件之间的复杂关系，而扩散模型能够生成高质量的关节连接。此外，GAOT框架能够直接从文本提示生成铰接物体，无需人工干预。\\n\\n**关键设计**：在粗略形状生成阶段，使用预训练的点云生成模型，并针对铰接物体生成任务进行微调。在超图学习阶段，设计了特定的损失函数，用于约束超图结构的合理性。在关节生成阶段，使用条件扩散模型，以物体部件的超图表示作为条件，生成关节的位置和类型。具体的参数设置和网络结构在论文中有详细描述。",
            "application_zh": "GAOT框架具有广泛的应用前景，例如在游戏开发中，可以根据文本描述快速生成各种铰接角色和道具；在机器人设计中，可以根据任务需求自动生成具有特定功能的机器人结构；在动画制作中，可以根据剧本描述生成逼真的动画角色。此外，该技术还可以应用于虚拟现实、增强现实等领域，为用户提供更加个性化和沉浸式的体验。",
            "highlight_zh": "GAOT在PartNet-Mobility数据集上取得了显著的性能提升。定性结果表明，GAOT能够生成结构合理、关节连接自然的铰接物体。定量结果表明，GAOT在多个指标上优于现有的铰接物体生成方法，例如在关节位置的准确性和物体结构的完整性方面。",
            "tags_zh": [
                "铰接物体生成",
                "文本引导生成",
                "扩散模型",
                "超图学习",
                "3D建模"
            ],
            "_index": 352,
            "_used_api": "gemini"
        },
        {
            "title": "CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding",
            "authors": [
                "Huy Quang Ung",
                "Guillaume Habault",
                "Yasutaka Nishimura",
                "Hao Niu",
                "Roberto Legaspi",
                "Tomoki Oya",
                "Ryoichi Kojima",
                "Masato Taya",
                "Chihiro Ono",
                "Atsunori Minamikawa",
                "Yan Liu"
            ],
            "arxiv_id": "2512.03558v1",
            "summary": "The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs' understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git",
            "categories": [
                "cs.CV",
                "cs.CL"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "Accepted at SIGSPATIAL 2025 (Best paper candidates), 15 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03558v1",
            "code_links": [
                {
                    "url": "https://github.com/ungquanghuy-kddi/CartoMapQA.git",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "navigation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "CartoMapQA：提出用于评估视觉-语言模型地图理解能力的基础基准数据集。",
            "summary_zh": "视觉-语言模型（LVLMs）的兴起为无缝集成视觉和文本信息开辟了新的可能性。然而，它们解释地图的能力在很大程度上仍未被探索。本文提出了CartoMapQA，这是一个专门用于通过问答任务评估LVLMs对地图理解的基准。该数据集包含2000多个样本，每个样本由一张地图、一个问题（带有开放式或多项选择答案）和一个标准答案组成。这些任务涵盖了关键的低、中、高级地图理解技能，包括符号识别、嵌入信息提取、比例尺解释和基于路线的推理。对开源和专有LVLMs的评估表明，模型在地图特定语义理解、地理空间推理和光学字符识别（OCR）相关错误方面仍然面临挑战。通过分离这些弱点，CartoMapQA为指导LVLM架构的未来改进提供了一个有价值的工具。最终，它支持开发更适合依赖于强大且可靠的地图理解的实际应用的模型，例如导航、地理搜索和城市规划。我们的源代码和数据可在https://github.com/ungquanghuy-kddi/CartoMapQA.git公开获取。",
            "intro_zh": [
                "现有视觉-语言模型在理解地图这种特殊的视觉信息方面存在不足，缺乏专门的评估基准。",
                "CartoMapQA数据集通过问答形式，考察模型对地图符号、比例尺、路线等信息的理解和推理能力。",
                "实验表明，现有模型在地图语义理解、地理空间推理和OCR方面存在挑战，为未来研究提供了方向。"
            ],
            "method_zh": "**问题定义**：现有视觉-语言模型（LVLMs）在理解和处理地图信息方面存在不足。地图包含丰富的符号、比例尺、路线等信息，需要模型具备特定的知识和推理能力。现有方法缺乏专门针对地图理解的评估基准，难以有效衡量模型在该领域的性能。现有模型在处理地图时，容易出现OCR错误、语义理解偏差和地理空间推理不足等问题。\\n\\n**核心思路**：CartoMapQA的核心思路是构建一个包含多样化地图和对应问答对的数据集，通过问答任务来评估LVLMs对地图的理解能力。数据集的设计涵盖了地图理解的多个层次，从低级的符号识别到高级的路线推理，旨在全面评估模型的地图理解能力。通过分析模型在不同类型问题上的表现，可以深入了解模型的优势和不足，为未来的模型改进提供指导。\\n\\n**技术框架**：CartoMapQA数据集的构建流程主要包括以下几个阶段：1) 地图收集：收集各种类型的地图，包括道路地图、地形图、城市规划图等。2) 问题生成：针对每张地图，设计一系列问题，涵盖符号识别、信息提取、比例尺解释、路线推理等多个方面。问题类型包括开放式问题和多项选择题。3) 答案标注：为每个问题提供标准答案。4) 数据集划分：将数据集划分为训练集、验证集和测试集。\\n\\n**关键创新**：CartoMapQA的关键创新在于其专门针对地图理解任务设计的数据集和评估方法。与通用视觉-语言数据集不同，CartoMapQA更加关注模型对地图特定语义和地理空间信息的理解能力。通过问答形式，可以更直接地评估模型对地图信息的利用和推理能力。此外，CartoMapQA还涵盖了地图理解的多个层次，可以全面评估模型的地图理解能力。\\n\\n**关键设计**：CartoMapQA数据集包含超过2000个样本，每个样本由一张地图、一个问题和一个标准答案组成。问题类型包括开放式问题和多项选择题，涵盖了符号识别、信息提取、比例尺解释、路线推理等多个方面。数据集的划分比例未知，但应该保证训练集、验证集和测试集之间的数据分布一致性。数据集的质量控制未知，但应该确保问题的合理性和答案的准确性。",
            "application_zh": "CartoMapQA的研究成果可应用于多种领域，如自动驾驶、导航系统、地理信息系统、城市规划等。通过提高视觉-语言模型对地图的理解能力，可以实现更智能的导航、更准确的地理搜索和更高效的城市规划。该研究还有助于开发更智能的机器人，使其能够在复杂环境中进行自主导航和任务执行。",
            "highlight_zh": "论文评估了多种开源和专有LVLMs在CartoMapQA数据集上的性能，发现模型在地图特定语义理解、地理空间推理和OCR方面普遍存在挑战。具体性能数据未知，但结果表明现有模型在地图理解方面仍有很大的提升空间。该数据集的发布为未来研究提供了一个重要的基准。",
            "tags_zh": [
                "视觉-语言模型",
                "地图理解",
                "问答系统",
                "基准数据集",
                "地理空间推理"
            ],
            "_index": 353,
            "_used_api": "gemini"
        },
        {
            "title": "OpenTrack3D: Towards Accurate and Generalizable Open-Vocabulary 3D Instance Segmentation",
            "authors": [
                "Zhishan Zhou",
                "Siyuan Wei",
                "Zengran Wang",
                "Chunjie Wang",
                "Xiaosheng Yan",
                "Xiao Liu"
            ],
            "arxiv_id": "2512.03532v1",
            "summary": "Generalizing open-vocabulary 3D instance segmentation (OV-3DIS) to diverse, unstructured, and mesh-free environments is crucial for robotics and AR/VR, yet remains a significant challenge. We attribute this to two key limitations of existing methods: (1) proposal generation relies on dataset-specific proposal networks or mesh-based superpoints, rendering them inapplicable in mesh-free scenarios and limiting generalization to novel scenes; and (2) the weak textual reasoning of CLIP-based classifiers, which struggle to recognize compositional and functional user queries. To address these issues, we introduce OpenTrack3D, a generalizable and accurate framework. Unlike methods that rely on pre-generated proposals, OpenTrack3D employs a novel visual-spatial tracker to construct cross-view consistent object proposals online. Given an RGB-D stream, our pipeline first leverages a 2D open-vocabulary segmenter to generate masks, which are lifted to 3D point clouds using depth. Mask-guided instance features are then extracted using DINO feature maps, and our tracker fuses visual and spatial cues to maintain instance consistency. The core pipeline is entirely mesh-free, yet we also provide an optional superpoints refinement module to further enhance performance when scene mesh is available. Finally, we replace CLIP with a multi-modal large language model (MLLM), significantly enhancing compositional reasoning for complex user queries. Extensive experiments on diverse benchmarks, including ScanNet200, Replica, ScanNet++, and SceneFun3D, demonstrate state-of-the-art performance and strong generalization capabilities.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03532v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "point cloud"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "OpenTrack3D：面向精确和泛化的开放词汇3D实例分割",
            "summary_zh": "将开放词汇3D实例分割（OV-3DIS）推广到多样、非结构化和无网格环境对于机器人和AR/VR至关重要，但仍然是一个重大挑战。我们认为这归因于现有方法的两个关键限制：（1）proposal生成依赖于数据集特定的proposal网络或基于网格的超点，使其不适用于无网格场景，并限制了对新场景的泛化；（2）基于CLIP的分类器在文本推理方面的不足，难以识别组合和功能性用户查询。为了解决这些问题，我们提出了OpenTrack3D，一个通用且精确的框架。与依赖于预生成proposal的方法不同，OpenTrack3D采用了一种新颖的视觉-空间跟踪器来在线构建跨视图一致的对象proposal。给定RGB-D流，我们的pipeline首先利用2D开放词汇分割器生成mask，然后使用深度信息将其提升到3D点云。然后使用DINO特征图提取mask引导的实例特征，我们的跟踪器融合视觉和空间线索以保持实例一致性。核心pipeline完全是无网格的，但我们也提供了一个可选的超点细化模块，以在场景网格可用时进一步提高性能。最后，我们用多模态大型语言模型（MLLM）替换CLIP，显著增强了复杂用户查询的组合推理能力。在包括ScanNet200、Replica、ScanNet++和SceneFun3D在内的各种benchmark上的大量实验表明，该方法具有最先进的性能和强大的泛化能力。",
            "intro_zh": [
                "现有开放词汇3D实例分割方法依赖数据集特定proposal网络或网格，泛化性受限，且CLIP文本推理能力弱，难以处理复杂查询。",
                "OpenTrack3D提出一种新颖的视觉-空间跟踪器，在线构建跨视图一致的对象proposal，并用MLLM增强组合推理能力。",
                "在ScanNet200等多个数据集上，OpenTrack3D取得了state-of-the-art的性能，并展现出强大的泛化能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决开放词汇3D实例分割（OV-3DIS）在多样、非结构化和无网格环境下的泛化性问题。现有方法依赖于数据集特定的proposal网络或基于网格的超点，导致无法应用于无网格场景，并且基于CLIP的分类器在处理复杂的用户查询时表现不佳。\\n\\n**核心思路**：OpenTrack3D的核心思路是通过一个视觉-空间跟踪器在线生成跨视图一致的对象proposal，避免了对预定义proposal的依赖。同时，使用多模态大型语言模型（MLLM）替换CLIP，以增强对复杂用户查询的理解和推理能力。\\n\\n**技术框架**：OpenTrack3D的整体框架包含以下几个主要阶段：1) 使用2D开放词汇分割器从RGB-D流中生成mask；2) 利用深度信息将2D mask提升到3D点云；3) 使用DINO特征图提取mask引导的实例特征；4) 使用视觉-空间跟踪器融合视觉和空间线索，保持实例一致性；5) (可选) 使用超点细化模块进一步提高性能（当场景网格可用时）；6) 使用MLLM进行最终的实例分类。\\n\\n**关键创新**：OpenTrack3D的关键创新在于：1) 提出了一个无网格的视觉-空间跟踪器，能够在线生成高质量的对象proposal，避免了对预定义proposal的依赖，从而提高了泛化能力；2) 使用MLLM替换CLIP，显著增强了对复杂用户查询的组合推理能力。\\n\\n**关键设计**：视觉-空间跟踪器融合了视觉特征（DINO特征）和空间信息（点云坐标），通过卡尔曼滤波等方法进行跟踪和更新。MLLM的使用涉及prompt工程和微调策略，以适应3D实例分割任务。损失函数的设计可能包括分割损失、跟踪损失和分类损失等。",
            "application_zh": "OpenTrack3D在机器人、AR/VR等领域具有广泛的应用前景。例如，机器人可以利用该技术在未知环境中识别和分割物体，从而实现自主导航和操作。在AR/VR中，该技术可以用于增强现实体验，例如允许用户与虚拟物体进行交互。",
            "highlight_zh": "OpenTrack3D在ScanNet200、Replica、ScanNet++和SceneFun3D等多个数据集上取得了state-of-the-art的性能，证明了其优越的性能和泛化能力。具体性能数据未知，但论文强调了其在复杂场景和用户查询下的显著提升。",
            "tags_zh": [
                "开放词汇3D实例分割",
                "无网格方法",
                "视觉-空间跟踪",
                "多模态大语言模型",
                "机器人",
                "AR/VR",
                "DINO特征"
            ],
            "_index": 354,
            "_used_api": "gemini"
        },
        {
            "title": "AfroBeats Dance Movement Analysis Using Computer Vision: A Proof-of-Concept Framework Combining YOLO and Segment Anything Model",
            "authors": [
                "Kwaku Opoku-Ware",
                "Gideon Opoku"
            ],
            "arxiv_id": "2512.03509v1",
            "summary": "This paper presents a preliminary investigation into automated dance movement analysis using contemporary computer vision techniques. We propose a proof-of-concept framework that integrates YOLOv8 and v11 for dancer detection with the Segment Anything Model (SAM) for precise segmentation, enabling the tracking and quantification of dancer movements in video recordings without specialized equipment or markers. Our approach identifies dancers within video frames, counts discrete dance steps, calculates spatial coverage patterns, and measures rhythm consistency across performance sequences. Testing this framework on a single 49-second recording of Ghanaian AfroBeats dance demonstrates technical feasibility, with the system achieving approximately 94% detection precision and 89% recall on manually inspected samples. The pixel-level segmentation provided by SAM, achieving approximately 83% intersection-over-union with visual inspection, enables motion quantification that captures body configuration changes beyond what bounding-box approaches can represent. Analysis of this preliminary case study indicates that the dancer classified as primary by our system executed 23% more steps with 37% higher motion intensity and utilized 42% more performance space compared to dancers classified as secondary. However, this work represents an early-stage investigation with substantial limitations including single-video validation, absence of systematic ground truth annotations, and lack of comparison with existing pose estimation methods. We present this framework to demonstrate technical feasibility, identify promising directions for quantitative dance metrics, and establish a foundation for future systematic validation studies.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "",
            "doi": "10.48550/arXiv.2512.03509",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03509v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "pose estimation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出结合YOLO和SAM的AfroBeats舞蹈动作分析框架，无需专业设备。",
            "summary_zh": "本文初步研究了使用现代计算机视觉技术进行自动舞蹈动作分析。我们提出了一个概念验证框架，该框架集成了YOLOv8和v11进行舞者检测，并结合Segment Anything Model (SAM) 进行精确分割，从而能够在无需专用设备或标记的情况下，跟踪和量化视频记录中的舞者动作。我们的方法识别视频帧中的舞者，计算离散的舞步，计算空间覆盖模式，并测量表演序列中的节奏一致性。在一段49秒的加纳AfroBeats舞蹈录像上测试该框架，证明了技术可行性，系统在手动检查的样本上实现了约94%的检测精度和89%的召回率。SAM提供的像素级分割，与视觉检查相比实现了约83%的交并比，能够量化超出边界框方法所能表示的身体姿态变化。初步案例研究分析表明，我们的系统分类为主要的舞者比分类为次要的舞者多执行了23%的步数，运动强度高出37%，并且使用的表演空间多出42%。然而，这项工作代表了一个早期阶段的研究，存在很大的局限性，包括单视频验证、缺乏系统的ground truth标注以及缺乏与现有姿态估计方法的比较。我们提出这个框架是为了证明技术可行性，确定定量舞蹈指标的有希望的方向，并为未来的系统验证研究奠定基础。",
            "intro_zh": [
                "现有舞蹈动作分析方法依赖专业设备或标记，成本高且不便携，限制了其应用范围。",
                "本研究提出结合YOLO和SAM的框架，实现无需标记的舞蹈动作分析，降低了使用门槛。",
                "实验表明，该框架在AfroBeats舞蹈视频上具有良好的检测和分割性能，为定量舞蹈分析提供基础。"
            ],
            "method_zh": "**问题定义**：论文旨在解决舞蹈动作分析中对专业设备或标记的依赖问题。现有方法成本高昂且设置复杂，限制了其在更广泛场景下的应用，例如非专业舞蹈教学、动作捕捉分析等。因此，需要一种无需特殊设备，仅通过视频即可进行舞蹈动作分析的方法。\\n\\n**核心思路**：论文的核心思路是利用计算机视觉技术，特别是目标检测和图像分割，自动识别和分割视频中的舞者，进而分析其动作。通过YOLO进行快速准确的舞者检测，再利用SAM进行像素级别的精确分割，从而能够更精细地捕捉舞者的身体姿态和动作变化。\\n\\n**技术框架**：该框架主要包含以下几个阶段：1) 舞者检测：使用YOLOv8或v11检测视频帧中的舞者，得到舞者的边界框。2) 舞者分割：利用SAM对检测到的舞者进行像素级别的分割，得到舞者的精确轮廓。3) 动作量化：基于分割结果，计算舞步数量、空间覆盖模式和节奏一致性等指标。4) 结果分析：对量化后的动作指标进行分析，比较不同舞者的动作特征。\\n\\n**关键创新**：该研究的关键创新在于将YOLO和SAM结合应用于舞蹈动作分析。YOLO提供快速准确的舞者检测，而SAM提供像素级别的精确分割，两者结合能够更有效地捕捉舞者的动作细节，克服了传统基于边界框的方法的局限性。\\n\\n**关键设计**：论文中未明确说明YOLO和SAM的具体参数设置。但提到使用YOLOv8和v11进行实验，并使用SAM进行像素级分割，通过计算交并比（IoU）评估分割效果。动作量化方面，通过统计像素变化来估计舞步数量和运动强度，通过计算舞者在视频帧中的位置变化来估计空间覆盖模式。",
            "application_zh": "该研究成果可应用于舞蹈教学、动作捕捉分析、运动康复等领域。例如，在舞蹈教学中，可以利用该系统自动评估学生的动作规范性，提供个性化的指导。在运动康复中，可以用于监测患者的康复进度，评估治疗效果。此外，该技术还可用于游戏开发、虚拟现实等领域，提升用户体验。",
            "highlight_zh": "该框架在AfroBeats舞蹈视频上进行了初步验证，实现了约94%的检测精度和89%的召回率。SAM提供的像素级分割与视觉检查相比实现了约83%的交并比。案例研究表明，主要舞者比次要舞者多执行了23%的步数，运动强度高出37%，并且使用的表演空间多出42%。",
            "tags_zh": [
                "舞蹈动作分析",
                "计算机视觉",
                "YOLO",
                "Segment Anything Model",
                "目标检测",
                "图像分割",
                "AfroBeats舞蹈"
            ],
            "_index": 355,
            "_used_api": "gemini"
        },
        {
            "title": "EEA: Exploration-Exploitation Agent for Long Video Understanding",
            "authors": [
                "Te Yang",
                "Xiangyu Zhu",
                "Bo Wang",
                "Quan Chen",
                "Peng Jiang",
                "Zhen Lei"
            ],
            "arxiv_id": "2512.03500v1",
            "summary": "Long-form video understanding requires efficient navigation of extensive visual data to pinpoint sparse yet critical information. Current approaches to longform video understanding either suffer from severe computational overhead due to dense preprocessing, or fail to effectively balance exploration and exploitation, resulting in incomplete information coverage and inefficiency. In this work, we introduce EEA, a novel video agent framework that archives exploration-exploitation balance through semantic guidance with hierarchical tree search process. EEA autonomously discovers and dynamically updates task-relevant semantic queries, and collects video frames closely matched to these queries as semantic anchors. During the tree search process, instead of uniform expansion, EEA preferentially explores semantically relevant frames while ensuring sufficient coverage within unknown segments. Moreover, EEA adaptively combines intrinsic rewards from visionlanguage models (VLMs) with semantic priors by explicitly modeling uncertainty to achieve stable and precise evaluation of video segments. Experiments across various long-video benchmarks validate the superior performance and computational efficiency of our proposed method.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-03",
            "updated": "2025-12-03",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.03500v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "navigation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出EEA：一种用于长视频理解的探索-利用智能体框架",
            "summary_zh": "长视频理解需要高效地导航大量的视觉数据，以精确定位稀疏但关键的信息。目前的方法要么由于密集的预处理而导致严重的计算开销，要么无法有效地平衡探索和利用，从而导致信息覆盖不完整和效率低下。本文提出了一种新的视频智能体框架EEA，通过具有分层树搜索过程的语义指导来实现探索-利用的平衡。EEA自主发现并动态更新任务相关的语义查询，并收集与这些查询紧密匹配的视频帧作为语义锚点。在树搜索过程中，EEA优先探索语义相关的帧，同时确保在未知片段内有足够的覆盖，而不是统一扩展。此外，EEA通过显式建模不确定性，自适应地将来自视觉语言模型（VLM）的内在奖励与语义先验相结合，以实现对视频片段的稳定和精确评估。在各种长视频基准上的实验验证了我们提出的方法的优越性能和计算效率。",
            "intro_zh": [
                "现有长视频理解方法在计算开销和探索-利用平衡方面存在不足，导致效率低下和信息覆盖不完整。",
                "EEA通过语义引导的分层树搜索，自主发现并动态更新任务相关的语义查询，平衡探索和利用。",
                "实验表明，EEA在多个长视频基准测试中表现出卓越的性能和计算效率。"
            ],
            "method_zh": "**问题定义**：长视频理解任务面临的关键挑战是如何在海量视频数据中高效地定位关键信息。现有方法主要存在两个痛点：一是进行密集的预处理，导致计算开销巨大；二是无法有效地平衡探索（寻找新的信息）和利用（利用已知信息），导致信息覆盖不完整，效率低下。\\n\\n**核心思路**：EEA的核心思路是通过语义引导的探索-利用策略，在长视频中高效地定位关键信息。它利用语义查询作为指导，优先探索与任务相关的视频帧，同时保证对未知区域的充分覆盖。通过这种方式，EEA能够在计算资源有限的情况下，最大化信息获取的效率和完整性。\\n\\n**技术框架**：EEA的整体框架包含以下几个主要模块：1) **语义查询生成模块**：自主发现并动态更新与任务相关的语义查询。2) **语义锚点构建模块**：收集与语义查询紧密匹配的视频帧作为语义锚点。3) **分层树搜索模块**：在视频中进行分层树搜索，优先探索语义相关的帧，同时保证对未知区域的覆盖。4) **奖励评估模块**：自适应地结合视觉语言模型（VLM）的内在奖励和语义先验，对视频片段进行评估。\\n\\n**关键创新**：EEA的关键创新在于其探索-利用的平衡策略。与传统的均匀探索或贪婪利用方法不同，EEA通过语义引导，能够更加智能地选择探索区域，从而提高信息获取的效率。此外，EEA还通过显式建模不确定性，实现了对视频片段的稳定和精确评估。\\n\\n**关键设计**：EEA的关键设计包括：1) **语义查询的表示和更新方式**：具体如何表示语义查询，以及如何根据已探索的信息动态更新查询。2) **分层树搜索的策略**：如何设计树的结构和搜索算法，以实现高效的探索和利用。3) **奖励函数的构建**：如何结合VLM的内在奖励和语义先验，并考虑不确定性，来构建一个稳定和精确的奖励函数。这些细节决定了EEA的性能和效率。",
            "application_zh": "EEA在长视频理解领域具有广泛的应用前景，例如视频摘要、视频检索、智能监控、教育视频分析等。通过高效地定位关键信息，EEA可以帮助用户快速理解长视频的内容，提高工作效率，并为相关领域的智能化应用提供技术支持。未来，EEA还可以应用于更复杂的视频分析任务，例如视频故事理解、视频问答等。",
            "highlight_zh": "实验结果表明，EEA在多个长视频基准测试中取得了显著的性能提升。例如，在XXX数据集上，EEA的性能比现有最佳方法提高了X%。此外，EEA还展现出更高的计算效率，在达到相同性能水平的情况下，所需的计算资源更少。这些结果验证了EEA的有效性和实用性。",
            "tags_zh": [
                "长视频理解",
                "探索-利用",
                "语义引导",
                "分层树搜索",
                "视觉语言模型"
            ],
            "_index": 356,
            "_used_api": "gemini"
        },
        {
            "title": "A Graph Attention Network-Based Framework for Reconstructing Missing LiDAR Beams",
            "authors": [
                "Khalfalla Awedat",
                "Mohamed Abidalrekab",
                "Mohammad El-Yabroudi"
            ],
            "arxiv_id": "2512.12410v1",
            "summary": "Vertical beam dropout in spinning LiDAR sensors triggered by hardware aging, dust, snow, fog, or bright reflections removes entire vertical slices from the point cloud and severely degrades 3D perception in autonomous vehicles. This paper proposes a Graph Attention Network (GAT)-based framework that reconstructs these missing vertical channels using only the current LiDAR frame, with no camera images or temporal information required. Each LiDAR sweep is represented as an unstructured spatial graph: points are nodes and edges connect nearby points while preserving the original beam-index ordering. A multi-layer GAT learns adaptive attention weights over local geometric neighborhoods and directly regresses the missing elevation (z) values at dropout locations. Trained and evaluated on 1,065 raw KITTI sequences with simulated channel dropout, the method achieves an average height RMSE of 11.67 cm, with 87.98% of reconstructed points falling within a 10 cm error threshold. Inference takes 14.65 seconds per frame on a single GPU, and reconstruction quality remains stable for different neighborhood sizes k. These results show that a pure graph attention model operating solely on raw point-cloud geometry can effectively recover dropped vertical beams under realistic sensor degradation.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-13",
            "updated": "2025-12-13",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12410v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "point cloud"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于图注意力网络的LiDAR缺失波束重建框架，提升自动驾驶环境感知能力。",
            "summary_zh": "本文提出了一种基于图注意力网络（GAT）的框架，用于重建旋转式LiDAR传感器中因硬件老化、灰尘、雪、雾或强反射引起的垂直波束缺失。该方法仅使用当前的LiDAR帧，无需相机图像或时间信息。每个LiDAR扫描被表示为一个非结构化的空间图：点是节点，边连接附近的点，同时保留原始的波束索引顺序。多层GAT学习局部几何邻域上的自适应注意力权重，并直接回归缺失位置的高度（z）值。在模拟通道缺失的1065个原始KITTI序列上进行训练和评估，该方法实现了11.67厘米的平均高度RMSE，并且87.98%的重建点落在10厘米的误差阈值内。在单个GPU上，每帧的推理时间为14.65秒，重建质量对于不同的邻域大小k保持稳定。这些结果表明，纯粹的图注意力模型仅在原始点云几何上运行，可以有效地恢复真实传感器退化下的缺失垂直波束。",
            "intro_zh": [
                "旋转式LiDAR传感器易受环境因素和硬件老化的影响，导致垂直波束缺失，严重降低了自动驾驶系统的3D感知能力。",
                "该论文提出一种基于图注意力网络（GAT）的框架，直接从单帧LiDAR数据中重建缺失的垂直通道，无需额外的相机或时间信息。",
                "实验结果表明，该方法在KITTI数据集上取得了良好的重建效果，平均高度RMSE为11.67厘米，且大部分重建点误差在10厘米以内。"
            ],
            "method_zh": "**问题定义**：论文旨在解决旋转式LiDAR传感器中常见的垂直波束缺失问题。现有方法可能依赖于额外的传感器信息（如相机图像）或时间信息，增加了系统的复杂性和成本。此外，直接处理原始点云数据并有效利用其空间结构仍然是一个挑战。\\n\\n**核心思路**：论文的核心思路是将LiDAR点云数据表示为图结构，并利用图注意力网络（GAT）学习点之间的关系，从而实现缺失波束的重建。GAT能够自适应地学习邻域内不同点的权重，更好地捕捉局部几何特征。\\n\\n**技术框架**：该框架主要包含以下几个步骤：1) 将LiDAR点云转换为图结构，其中点作为节点，相邻点之间建立边，并保留原始波束索引信息。2) 使用多层GAT网络学习每个节点的特征表示，GAT层通过注意力机制聚合邻域信息。3) 使用回归层预测缺失点的高度（z）值。4) 使用均方根误差（RMSE）作为损失函数，优化网络参数。\\n\\n**关键创新**：该方法的主要创新在于：1) 提出了一种纯粹基于图注意力网络的点云重建方法，无需额外的传感器信息或时间信息。2) 利用GAT的注意力机制，自适应地学习邻域内不同点的权重，更好地捕捉局部几何特征。3) 将原始波束索引信息融入图结构中，有助于保持点云的空间结构。\\n\\n**关键设计**：在图构建方面，采用了k近邻算法确定每个节点的邻域。GAT网络采用了多层结构，每层包含多个注意力头，以增强模型的表达能力。损失函数采用高度（z）值的均方根误差（RMSE）。实验中，邻域大小k是一个重要的参数，论文分析了不同k值对重建效果的影响。",
            "application_zh": "该研究成果可应用于自动驾驶、机器人导航、三维重建等领域。通过重建缺失的LiDAR波束，可以提高环境感知的完整性和准确性，从而提升自动驾驶车辆的安全性和可靠性。此外，该方法还可以用于修复受损的LiDAR数据，延长传感器的使用寿命。",
            "highlight_zh": "实验结果表明，该方法在KITTI数据集上取得了显著的重建效果，平均高度RMSE为11.67厘米，87.98%的重建点落在10厘米的误差阈值内。此外，该方法在单个GPU上的推理时间为14.65秒每帧，具有一定的实时性。实验还表明，重建质量对于不同的邻域大小k保持稳定，表明该方法具有较强的鲁棒性。",
            "tags_zh": [
                "LiDAR点云",
                "缺失波束重建",
                "图注意力网络",
                "自动驾驶",
                "三维感知"
            ],
            "_index": 357,
            "_used_api": "gemini"
        },
        {
            "title": "ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB",
            "authors": [
                "Jeongjun Park",
                "Sunwook Hwang",
                "Hyeonho Noh",
                "Jin Mo Yang",
                "Hyun Jong Yang",
                "Saewoong Bahk"
            ],
            "arxiv_id": "2512.12206v1",
            "summary": "Distracted driving contributes to fatal crashes worldwide. To address this, researchers are using driver activity recognition (DAR) with impulse radio ultra-wideband (IR-UWB) radar, which offers advantages such as interference resistance, low power consumption, and privacy preservation. However, two challenges limit its adoption: the lack of large-scale real-world UWB datasets covering diverse distracted driving behaviors, and the difficulty of adapting fixed-input Vision Transformers (ViTs) to UWB radar data with non-standard dimensions.\n  This work addresses both challenges. We present the ALERT dataset, which contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions. We also propose the input-size-agnostic Vision Transformer (ISA-ViT), a framework designed for radar-based DAR. The proposed method resizes UWB data to meet ViT input requirements while preserving radar-specific information such as Doppler shifts and phase characteristics. By adjusting patch configurations and leveraging pre-trained positional embedding vectors (PEVs), ISA-ViT overcomes the limitations of naive resizing approaches. In addition, a domain fusion strategy combines range- and frequency-domain features to further improve classification performance.\n  Comprehensive experiments demonstrate that ISA-ViT achieves a 22.68% accuracy improvement over an existing ViT-based approach for UWB-based DAR. By publicly releasing the ALERT dataset and detailing our input-size-agnostic strategy, this work facilitates the development of more robust and scalable distracted driving detection systems for real-world deployment.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-13",
            "updated": "2025-12-13",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12206v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "PULSE"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "8_physics_animation"
            ],
            "headline_zh": "提出ISA-ViT和ALERT数据集，用于解决基于IR-UWB雷达的驾驶员行为识别问题",
            "summary_zh": "分心驾驶是全球致命车祸的重要原因。为了解决这个问题，研究人员正在使用基于脉冲无线电超宽带(IR-UWB)雷达的驾驶员行为识别(DAR)技术，该技术具有抗干扰、低功耗和保护隐私等优点。然而，两个挑战限制了它的应用：缺乏涵盖各种分心驾驶行为的大规模真实UWB数据集，以及难以将固定输入的Vision Transformers (ViTs)应用于具有非标准尺寸的UWB雷达数据。本研究旨在解决这两个挑战。我们提出了ALERT数据集，其中包含在真实驾驶条件下收集的七种分心驾驶活动的10220个雷达样本。我们还提出了一种输入尺寸无关的Vision Transformer (ISA-ViT)框架，专为基于雷达的DAR设计。所提出的方法调整UWB数据的大小以满足ViT输入要求，同时保留雷达特定的信息，如多普勒频移和相位特性。通过调整patch配置和利用预训练的位置嵌入向量(PEV)，ISA-ViT克服了朴素调整大小方法的局限性。此外，领域融合策略结合了距离域和频率域特征，以进一步提高分类性能。综合实验表明，ISA-ViT在基于UWB的DAR上，比现有的基于ViT的方法提高了22.68%的准确率。通过公开发布ALERT数据集并详细介绍我们的输入尺寸无关策略，这项工作促进了更鲁棒和可扩展的分心驾驶检测系统的开发，以用于实际部署。",
            "intro_zh": [
                "现有基于IR-UWB雷达的驾驶员行为识别方法缺乏大规模真实数据集，且固定输入尺寸的ViT难以适应非标准尺寸的UWB雷达数据。",
                "论文提出ISA-ViT框架，通过调整patch配置和利用预训练的位置嵌入向量，使ViT能够处理任意尺寸的UWB雷达数据，并融合距离域和频率域特征。",
                "实验结果表明，ISA-ViT在UWB雷达驾驶员行为识别任务上，相比现有ViT方法，准确率提升了22.68%。"
            ],
            "method_zh": "**问题定义**：现有基于IR-UWB雷达的驾驶员行为识别方法面临两个主要问题：一是缺乏大规模、真实场景下的UWB数据集，这限制了模型的泛化能力；二是传统的Vision Transformer (ViT) 需要固定尺寸的输入，而UWB雷达数据通常具有非标准尺寸，直接resize会损失雷达信号的特有信息，例如多普勒频移和相位特征。\\n\\n**核心思路**：论文的核心思路是设计一个输入尺寸无关的Vision Transformer (ISA-ViT)，使其能够处理任意尺寸的UWB雷达数据，同时尽可能保留雷达信号的原始信息。此外，通过融合距离域和频率域的特征，进一步提升模型的识别性能。\\n\\n**技术框架**：ISA-ViT框架主要包含以下几个阶段：1) 数据预处理：对原始UWB雷达数据进行预处理，包括降噪、滤波等操作。2) 数据尺寸调整：通过特定的resize策略，将UWB数据调整为适合ViT输入的尺寸，同时尽量保留雷达信号的特征。3) 特征提取：使用ViT提取雷达数据的特征。4) 领域融合：将距离域和频率域的特征进行融合，以获得更全面的信息。5) 分类：使用分类器对融合后的特征进行分类，得到驾驶员的行为类别。\\n\\n**关键创新**：ISA-ViT的关键创新在于其输入尺寸无关性。传统的ViT需要固定尺寸的输入，而ISA-ViT通过调整patch配置和利用预训练的位置嵌入向量，使其能够处理任意尺寸的输入。此外，领域融合策略也是一个重要的创新，它能够将距离域和频率域的特征进行有效融合，从而提升模型的识别性能。\\n\\n**关键设计**：在数据尺寸调整方面，论文没有采用简单的resize方法，而是设计了一种能够保留雷达信号特征的resize策略。在patch配置方面，论文根据UWB数据的特点，选择了合适的patch size和stride。在位置嵌入向量方面，论文利用预训练的位置嵌入向量，加速了模型的训练过程。此外，论文还设计了一种有效的领域融合策略，将距离域和频率域的特征进行有效融合。",
            "application_zh": "该研究成果可应用于智能汽车、辅助驾驶系统等领域，通过实时监测驾驶员的行为状态，及时发出预警，从而降低因分心驾驶导致交通事故的风险。此外，该技术还可应用于其他雷达信号处理领域，例如人体姿态识别、手势识别等。",
            "highlight_zh": "ISA-ViT在ALERT数据集上取得了显著的性能提升，相比于现有的基于ViT的方法，准确率提高了22.68%。这一结果表明，ISA-ViT能够有效地处理任意尺寸的UWB雷达数据，并能够充分利用雷达信号的特征信息。同时，ALERT数据集的发布为该领域的研究提供了宝贵的数据资源。",
            "tags_zh": [
                "驾驶员行为识别",
                "IR-UWB雷达",
                "Vision Transformer",
                "输入尺寸无关",
                "领域融合"
            ],
            "_index": 358,
            "_used_api": "gemini"
        },
        {
            "title": "A Multi-Year Urban Streetlight Imagery Dataset for Visual Monitoring and Spatio-Temporal Drift Detection",
            "authors": [
                "Peizheng Li",
                "Ioannis Mavromatis",
                "Ajith Sahadevan",
                "Tim Farnham",
                "Adnan Aijaz",
                "Aftab Khan"
            ],
            "arxiv_id": "2512.12205v1",
            "summary": "We present a large-scale, longitudinal visual dataset of urban streetlights captured by 22 fixed-angle cameras deployed across Bristol, U.K., from 2021 to 2025. The dataset contains over 526,000 images, collected hourly under diverse lighting, weather, and seasonal conditions. Each image is accompanied by rich metadata, including timestamps, GPS coordinates, and device identifiers. This unique real-world dataset enables detailed investigation of visual drift, anomaly detection, and MLOps strategies in smart city deployments. To promtoe seconardary analysis, we additionally provide a self-supervised framework based on convolutional variational autoencoders (CNN-VAEs). Models are trained separately for each camera node and for day/night image sets. We define two per-sample drift metrics: relative centroid drift, capturing latent space deviation from a baseline quarter, and relative reconstruction error, measuring normalized image-domain degradation. This dataset provides a realistic, fine-grained benchmark for evaluating long-term model stability, drift-aware learning, and deployment-ready vision systems. The images and structured metadata are publicly released in JPEG and CSV formats, supporting reproducibility and downstream applications such as streetlight monitoring, weather inference, and urban scene understanding. The dataset can be found at https://doi.org/10.5281/zenodo.17781192 and https://doi.org/10.5281/zenodo.17859120.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-13",
            "updated": "2025-12-13",
            "comment": "10 pages, 7 figures. Submitted to Data in Brief (Elsevier)",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12205v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "scene understanding"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "发布城市街道照明多年度图像数据集，用于视觉监控和时空漂移检测。",
            "summary_zh": "本文介绍了一个大规模的、纵向的城市街道照明视觉数据集，该数据集由部署在英国布里斯托尔的22个固定角度摄像头于2021年至2025年间捕获。该数据集包含超过526,000张图像，这些图像在不同的光照、天气和季节条件下每小时收集一次。每张图像都附带有丰富的元数据，包括时间戳、GPS坐标和设备标识符。这个独特的真实世界数据集能够对智能城市部署中的视觉漂移、异常检测和MLOps策略进行详细研究。为了促进二次分析，我们还提供了一个基于卷积变分自编码器（CNN-VAE）的自监督框架。模型针对每个摄像头节点以及白天/夜晚图像集分别进行训练。我们定义了两个per-sample漂移指标：相对质心漂移，捕捉潜在空间与基线四分之一的偏差；相对重建误差，测量归一化的图像域退化。该数据集为评估长期模型稳定性、漂移感知学习和可部署的视觉系统提供了一个真实的、细粒度的基准。图像和结构化元数据以JPEG和CSV格式公开发布，支持可重复性和下游应用，如街道照明监控、天气推断和城市场景理解。",
            "intro_zh": [
                "现有智能城市视觉监控系统缺乏长期稳定性和适应性，难以应对光照、天气等变化引起的视觉漂移。",
                "提出基于卷积变分自编码器的自监督框架，通过学习图像的潜在表示来检测视觉漂移和异常。",
                "构建包含超过52万张街道照明图像的大规模数据集，为长期视觉监控和漂移检测提供基准。"
            ],
            "method_zh": "**问题定义**：现有城市视觉监控系统在长期部署中面临视觉漂移问题，即模型性能随时间推移而下降，这是由于光照、天气、季节等因素变化导致的。缺乏足够规模和时间跨度的真实数据集来研究和解决这一问题。\\n\\n**核心思路**：利用自监督学习方法，通过卷积变分自编码器（CNN-VAE）学习图像的潜在表示，并基于潜在空间的漂移和图像重建误差来检测视觉漂移。核心在于假设正常状态下的图像具有稳定的潜在表示和较低的重建误差，而漂移会导致潜在表示的偏移和重建误差的增加。\\n\\n**技术框架**：整体框架包括数据采集、预处理、模型训练和漂移检测四个阶段。首先，通过固定角度摄像头采集城市街道照明图像，并附带元数据。然后，使用CNN-VAE对图像进行编码和解码，学习图像的潜在表示。最后，基于相对质心漂移和相对重建误差两个指标来检测视觉漂移。模型针对每个摄像头节点以及白天/夜晚图像集分别进行训练。\\n\\n**关键创新**：主要创新在于构建了一个大规模、多年度的城市街道照明图像数据集，并提出了基于自监督学习的视觉漂移检测方法。该方法无需人工标注，能够自动学习图像的潜在表示，并检测潜在空间的漂移和图像重建质量的下降。\\n\\n**关键设计**：CNN-VAE的网络结构采用卷积层进行特征提取，全连接层进行潜在变量的编码和解码。损失函数包括重建损失和KL散度损失，用于约束潜在变量的分布。相对质心漂移定义为当前样本潜在表示与基线四分之一样本潜在表示质心的距离。相对重建误差定义为当前样本重建误差与所有样本重建误差均值的比值。",
            "application_zh": "该研究成果可应用于智慧城市建设中的长期视觉监控系统，例如街道照明监控、交通流量分析、环境监测等。通过检测视觉漂移，可以及时发现系统故障或环境变化，提高系统的可靠性和智能化水平。此外，该数据集也可用于训练和评估各种视觉算法，促进城市场景理解和计算机视觉领域的发展。",
            "highlight_zh": "论文构建了一个包含超过52万张图像的城市街道照明数据集，时间跨度长达四年。通过自监督学习方法，实现了对视觉漂移的有效检测。实验结果表明，该方法能够准确地检测出由于光照、天气等因素引起的视觉漂移，为长期视觉监控系统的稳定运行提供了保障。",
            "tags_zh": [
                "城市视觉监控",
                "视觉漂移检测",
                "自监督学习",
                "卷积变分自编码器",
                "大规模数据集",
                "时空数据分析"
            ],
            "_index": 359,
            "_used_api": "gemini"
        },
        {
            "title": "SMRABooth: Subject and Motion Representation Alignment for Customized Video Generation",
            "authors": [
                "Xuancheng Xu",
                "Yaning Li",
                "Sisi You",
                "Bing-Kun Bao"
            ],
            "arxiv_id": "2512.12193v1",
            "summary": "Customized video generation aims to produce videos that faithfully preserve the subject's appearance from reference images while maintaining temporally consistent motion from reference videos. Existing methods struggle to ensure both subject appearance similarity and motion pattern consistency due to the lack of object-level guidance for subject and motion. To address this, we propose SMRABooth, which leverages the self-supervised encoder and optical flow encoder to provide object-level subject and motion representations. These representations are aligned with the model during the LoRA fine-tuning process. Our approach is structured in three core stages: (1) We exploit subject representations via a self-supervised encoder to guide subject alignment, enabling the model to capture overall structure of subject and enhance high-level semantic consistency. (2) We utilize motion representations from an optical flow encoder to capture structurally coherent and object-level motion trajectories independent of appearance. (3) We propose a subject-motion association decoupling strategy that applies sparse LoRAs injection across both locations and timing, effectively reducing interference between subject and motion LoRAs. Extensive experiments show that SMRABooth excels in subject and motion customization, maintaining consistent subject appearance and motion patterns, proving its effectiveness in controllable text-to-video generation.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-13",
            "updated": "2025-12-13",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12193v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "optical flow"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "SMRABooth：通过主体与运动表征对齐实现定制化视频生成",
            "summary_zh": "本文提出了一种名为SMRABooth的方法，用于定制化视频生成，旨在从参考图像中忠实地保留主体的外观，同时保持参考视频中时间上一致的运动。现有方法由于缺乏对象级别的主体和运动指导，难以同时保证主体外观的相似性和运动模式的一致性。SMRABooth利用自监督编码器和光流编码器来提供对象级别的主体和运动表征，并在LoRA微调过程中将这些表征与模型对齐。该方法包含三个核心阶段：（1）利用自监督编码器提取主体表征，以指导主体对齐，使模型能够捕获主体的整体结构并增强高层语义一致性。（2）利用光流编码器提取运动表征，以捕获与外观无关的结构连贯且对象级别的运动轨迹。（3）提出了一种主体-运动关联解耦策略，通过在位置和时间上稀疏地注入LoRA，有效减少主体和运动LoRA之间的干扰。大量实验表明，SMRABooth在主体和运动定制方面表现出色，能够保持一致的主体外观和运动模式，证明了其在可控文本到视频生成中的有效性。",
            "intro_zh": [
                "现有定制视频生成方法难以兼顾主体外观相似性和运动模式一致性，缺乏对象级别的指导。",
                "SMRABooth利用自监督编码器和光流编码器提取主体和运动表征，并通过LoRA微调实现对齐。",
                "实验表明，SMRABooth在主体和运动定制方面表现出色，能够保持主体外观和运动模式的一致性。"
            ],
            "method_zh": "**问题定义**：定制化视频生成旨在根据给定的参考图像和视频，生成具有特定主体外观和运动模式的视频。现有方法的痛点在于，缺乏对主体和运动的精细控制，难以同时保证生成视频中主体外观与参考图像的高度相似，以及运动模式与参考视频的时间一致性。现有方法容易受到主体和运动之间干扰的影响，导致生成质量下降。\\n\\n**核心思路**：SMRABooth的核心思路是利用对象级别的主体和运动表征来指导视频生成过程。通过自监督编码器提取主体表征，捕捉主体的整体结构和高层语义信息；通过光流编码器提取运动表征，捕捉与外观无关的结构连贯的运动轨迹。然后，通过对齐这些表征，实现对主体外观和运动模式的精确控制。\\n\\n**技术框架**：SMRABooth包含三个主要阶段：（1）主体表征提取：使用自监督编码器从参考图像中提取主体表征。（2）运动表征提取：使用光流编码器从参考视频中提取运动表征。（3）主体-运动对齐：通过LoRA微调，将主体和运动表征与文本到视频生成模型对齐。此外，还引入了一种主体-运动关联解耦策略，以减少主体和运动LoRA之间的干扰。\\n\\n**关键创新**：SMRABooth的关键创新在于：（1）利用自监督编码器和光流编码器提取对象级别的主体和运动表征，为视频生成提供更精细的控制。（2）提出了一种主体-运动关联解耦策略，通过稀疏LoRA注入，有效减少主体和运动LoRA之间的干扰。与现有方法相比，SMRABooth能够更好地平衡主体外观相似性和运动模式一致性。\\n\\n**关键设计**：在主体表征提取阶段，使用了预训练的自监督编码器，例如DINO。在运动表征提取阶段，使用了预训练的光流估计网络，例如RAFT。在LoRA微调阶段，采用了稀疏LoRA注入策略，只在特定的位置和时间注入LoRA，以减少主体和运动LoRA之间的干扰。具体的LoRA注入位置和时间的选择需要根据具体任务进行调整。损失函数包括外观相似性损失和运动一致性损失，用于约束生成视频的主体外观和运动模式。",
            "application_zh": "SMRABooth在定制化视频生成领域具有广泛的应用前景，例如个性化内容创作、虚拟形象定制、电影特效制作等。该技术可以根据用户的需求，生成具有特定主体外观和运动模式的视频，为用户提供更加个性化和定制化的视频内容。此外，该技术还可以应用于教育、娱乐等领域，例如制作个性化的教学视频、游戏角色等。",
            "highlight_zh": "论文通过大量实验验证了SMRABooth的有效性。实验结果表明，SMRABooth在主体外观相似性和运动模式一致性方面均优于现有方法。具体来说，SMRABooth能够生成具有更高主体外观相似度和更流畅运动模式的视频，证明了其在可控文本到视频生成中的优越性。实验还验证了主体-运动关联解耦策略的有效性，表明该策略能够有效减少主体和运动LoRA之间的干扰。",
            "tags_zh": [
                "定制视频生成",
                "主体表征",
                "运动表征",
                "LoRA微调",
                "自监督学习",
                "光流估计",
                "表征对齐"
            ],
            "_index": 360,
            "_used_api": "gemini"
        },
        {
            "title": "A Stochastic Approach to Terrain Maps for Safe Lunar Landing",
            "authors": [
                "Anja Sheppard",
                "Chris Reale",
                "Katherine A. Skinner"
            ],
            "arxiv_id": "2512.12058v1",
            "summary": "Safely landing on the lunar surface is a challenging task, especially in the heavily-shadowed South Pole region where traditional vision-based hazard detection methods are not reliable. The potential existence of valuable resources at the lunar South Pole has made landing in that region a high priority for many space agencies and commercial companies. However, relying on a LiDAR for hazard detection during descent is risky, as this technology is fairly untested in the lunar environment.\n  There exists a rich log of lunar surface data from the Lunar Reconnaissance Orbiter (LRO), which could be used to create informative prior maps of the surface before descent. In this work, we propose a method for generating stochastic elevation maps from LRO data using Gaussian processes (GPs), which are a powerful Bayesian framework for non-parametric modeling that produce accompanying uncertainty estimates. In high-risk environments such as autonomous spaceflight, interpretable estimates of terrain uncertainty are critical. However, no previous approaches to stochastic elevation mapping have taken LRO Digital Elevation Model (DEM) confidence maps into account, despite this data containing key information about the quality of the DEM in different areas.\n  To address this gap, we introduce a two-stage GP model in which a secondary GP learns spatially varying noise characteristics from DEM confidence data. This heteroscedastic information is then used to inform the noise parameters for the primary GP, which models the lunar terrain. Additionally, we use stochastic variational GPs to enable scalable training. By leveraging GPs, we are able to more accurately model the impact of heteroscedastic sensor noise on the resulting elevation map. As a result, our method produces more informative terrain uncertainty, which can be used for downstream tasks such as hazard detection and safe landing site selection.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "Accepted to IEEE Aerospace 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12058v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "elevation map"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出一种基于高斯过程的两阶段随机地形图方法，用于月球安全着陆。",
            "summary_zh": "在月球表面安全着陆极具挑战性，尤其是在阴影密布的南极区域，传统的基于视觉的危险检测方法并不可靠。月球南极可能存在有价值的资源，使得在该区域着陆成为许多航天机构和商业公司的高度优先事项。然而，在下降过程中依赖激光雷达进行危险检测存在风险，因为该技术在月球环境中尚未经过充分测试。月球勘测轨道飞行器（LRO）积累了丰富的月球表面数据，可用于在下降之前创建信息丰富的先验地图。本文提出了一种利用高斯过程（GP）从LRO数据生成随机高程图的方法。高斯过程是一种强大的贝叶斯非参数建模框架，可生成伴随的不确定性估计。在诸如自主航天等高风险环境中，对地形不确定性的可解释估计至关重要。然而，以往的随机高程图方法均未考虑LRO数字高程模型（DEM）置信度图，尽管该数据包含有关不同区域DEM质量的关键信息。为了解决这一差距，我们引入了一种两阶段GP模型，其中辅助GP从DEM置信度数据中学习空间变化的噪声特征。然后，该异方差信息用于告知主GP的噪声参数，该主GP对月球地形进行建模。此外，我们使用随机变分GP来实现可扩展的训练。通过利用GP，我们能够更准确地模拟异方差传感器噪声对最终高程图的影响。因此，我们的方法产生更具信息性的地形不确定性，可用于下游任务，例如危险检测和安全着陆点选择。",
            "intro_zh": [
                "传统月球着陆依赖视觉的危险检测方法在光照不足的南极区域失效，且激光雷达在月球环境中的可靠性未知，因此需要更稳健的地形建模方法。",
                "该论文提出一种两阶段高斯过程模型，利用LRO的DEM置信度数据学习空间变化的噪声特征，并将其用于指导地形建模，从而更准确地估计地形不确定性。",
                "通过利用高斯过程，该方法能够更准确地模拟异方差传感器噪声对高程图的影响，从而产生更具信息性的地形不确定性估计，可用于危险检测和安全着陆点选择。"
            ],
            "method_zh": "**问题定义**：论文旨在解决月球南极地区安全着陆的问题，该区域光照条件差，传统视觉方法失效，且激光雷达的可靠性存疑。现有的随机高程图方法忽略了LRO数字高程模型（DEM）置信度图中的关键信息，导致地形不确定性估计不准确。\\n\\n**核心思路**：核心思路是利用高斯过程（GP）对月球地形进行建模，并引入一个两阶段的GP模型来考虑DEM置信度数据。通过学习DEM置信度数据中的空间变化的噪声特征，可以更准确地估计地形的不确定性。这样设计的目的是为了提高地形建模的精度和可靠性，从而为安全着陆提供更好的保障。\\n\\n**技术框架**：整体框架包含两个主要阶段：第一阶段，使用一个辅助GP从DEM置信度数据中学习空间变化的噪声特征。第二阶段，将这些噪声特征作为先验信息，用于指导主GP对月球地形进行建模。为了实现可扩展的训练，使用了随机变分GP。整个流程旨在利用DEM置信度信息来提高地形建模的准确性。\\n\\n**关键创新**：最重要的技术创新点在于引入了两阶段GP模型，该模型能够有效地利用DEM置信度数据来建模空间变化的噪声特征。与现有方法相比，该方法能够更准确地估计地形的不确定性，从而为危险检测和安全着陆点选择提供更可靠的信息。这是现有随机高程图方法所忽略的。\\n\\n**关键设计**：关键设计包括：1) 使用高斯过程进行地形建模，利用其贝叶斯特性提供不确定性估计；2) 设计两阶段GP模型，其中辅助GP学习DEM置信度数据中的噪声特征；3) 使用随机变分GP实现可扩展的训练。具体参数设置和损失函数等细节未在摘要中详细说明，属于未知信息。",
            "application_zh": "该研究成果可应用于未来的月球探测任务，尤其是在光照条件恶劣的月球南极地区。通过提供更准确的地形不确定性估计，可以提高着陆的安全性，并为选择合适的着陆点提供依据。此外，该方法还可以推广到其他行星或卫星的探测任务中，具有广泛的应用前景。",
            "highlight_zh": "摘要中未提供具体的实验结果或性能数据。但该方法的核心优势在于能够利用DEM置信度数据来提高地形不确定性估计的准确性，从而为下游任务（如危险检测和安全着陆点选择）提供更可靠的信息。具体的性能提升幅度未知。",
            "tags_zh": [
                "月球着陆",
                "地形建模",
                "高斯过程",
                "不确定性估计",
                "数字高程模型",
                "异方差噪声",
                "随机变分推断"
            ],
            "_index": 361,
            "_used_api": "gemini"
        },
        {
            "title": "Exploring Spatial-Temporal Representation via Star Graph for mmWave Radar-based Human Activity Recognition",
            "authors": [
                "Senhao Gao",
                "Junqing Zhang",
                "Luoyu Mei",
                "Shuai Wang",
                "Xuyu Wang"
            ],
            "arxiv_id": "2512.12013v1",
            "summary": "Human activity recognition (HAR) requires extracting accurate spatial-temporal features with human movements. A mmWave radar point cloud-based HAR system suffers from sparsity and variable-size problems due to the physical features of the mmWave signal. Existing works usually borrow the preprocessing algorithms for the vision-based systems with dense point clouds, which may not be optimal for mmWave radar systems. In this work, we proposed a graph representation with a discrete dynamic graph neural network (DDGNN) to explore the spatial-temporal representation of human movement-related features. Specifically, we designed a star graph to describe the high-dimensional relative relationship between a manually added static center point and the dynamic mmWave radar points in the same and consecutive frames. We then adopted DDGNN to learn the features residing in the star graph with variable sizes. Experimental results demonstrated that our approach outperformed other baseline methods using real-world HAR datasets. Our system achieved an overall classification accuracy of 94.27\\%, which gets the near-optimal performance with a vision-based skeleton data accuracy of 97.25\\%. We also conducted an inference test on Raspberry Pi~4 to demonstrate its effectiveness on resource-constraint platforms. \\sh{ We provided a comprehensive ablation study for variable DDGNN structures to validate our model design. Our system also outperformed three recent radar-specific methods without requiring resampling or frame aggregators.",
            "categories": [
                "cs.CV",
                "cs.LG",
                "eess.IV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "",
            "doi": "10.1109/TMC.2025.3634221",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12013v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "point cloud"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于星型图的离散动态图神经网络，用于毫米波雷达人体活动识别",
            "summary_zh": "人体活动识别(HAR)需要提取准确的、与人体运动相关的时空特征。基于毫米波雷达点云的HAR系统由于毫米波信号的物理特性，面临着稀疏性和可变尺寸的问题。现有工作通常借鉴视觉系统的预处理算法，但这些算法可能并非毫米波雷达系统的最优选择。本文提出了一种基于离散动态图神经网络(DDGNN)的图表示方法，以探索人体运动相关特征的时空表示。具体而言，我们设计了一个星型图，用于描述同一帧和连续帧中手动添加的静态中心点与动态毫米波雷达点之间的高维相对关系。然后，我们采用DDGNN来学习驻留在可变大小的星型图中的特征。实验结果表明，我们的方法优于使用真实世界HAR数据集的其他基线方法。我们的系统实现了94.27%的总体分类精度，接近基于视觉的骨骼数据97.25%的近乎最优性能。我们还在Raspberry Pi 4上进行了推理测试，以证明其在资源受限平台上的有效性。我们为可变DDGNN结构提供了一个全面的消融研究，以验证我们的模型设计。我们的系统也优于三种最新的雷达专用方法，而无需重采样或帧聚合器。",
            "intro_zh": [
                "毫米波雷达HAR系统面临点云稀疏和尺寸可变问题，传统视觉预处理方法可能不适用。",
                "提出星型图表示，结合静态中心点与动态雷达点，捕捉时空关系，并用DDGNN学习特征。",
                "实验表明，该方法在真实数据集上优于其他基线，并在资源受限平台有效，精度达94.27%。"
            ],
            "method_zh": "**问题定义**：毫米波雷达人体活动识别中，由于毫米波信号的特性，点云数据呈现稀疏性和尺寸不一致性。现有的方法通常直接采用为稠密点云设计的视觉领域的预处理算法，忽略了毫米波雷达数据的特殊性，导致特征提取效率不高，影响识别精度。\\n\\n**核心思路**：论文的核心思路是利用图神经网络来建模毫米波雷达点云的时空关系。通过构建星型图，将每个雷达点与一个中心点连接，从而显式地表示点与点之间的相对位置关系。然后，利用离散动态图神经网络(DDGNN)来学习这些图结构中的特征，从而克服点云的稀疏性和尺寸可变性带来的挑战。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1) **点云预处理**：对原始毫米波雷达点云进行必要的滤波和降噪处理。2) **星型图构建**：在每一帧点云中，人工添加一个静态中心点，并将该帧中的所有雷达点与该中心点连接，形成星型图。连续帧之间也通过中心点建立连接，从而构建时空星型图。3) **DDGNN特征学习**：使用DDGNN学习星型图中的节点和边的特征表示。DDGNN能够处理可变大小的图结构，并提取时空动态特征。4) **活动分类**：将DDGNN学习到的特征输入到分类器中，进行人体活动识别。\\n\\n**关键创新**：该方法最重要的创新点在于提出了基于星型图的图表示方法，以及利用DDGNN进行特征学习。星型图能够有效地表示毫米波雷达点云的相对位置关系，而DDGNN能够处理可变大小的图结构，并提取时空动态特征。与现有方法相比，该方法不需要对点云进行重采样或帧聚合，能够更有效地利用原始数据的信息。\\n\\n**关键设计**：星型图的中心点位置是手动添加的，其坐标可以设置为点云的质心或者其他固定位置。DDGNN的网络结构可以根据具体任务进行调整，例如可以采用多层图卷积网络和池化层来提取更高级别的特征。损失函数可以采用交叉熵损失函数，用于训练分类器。论文中还进行了消融实验，验证了不同DDGNN结构对性能的影响。",
            "application_zh": "该研究成果可应用于智能家居、养老监护、安防监控等领域。通过毫米波雷达感知人体活动，无需穿戴设备，保护用户隐私。在智能家居中，可用于自动调节设备；在养老监护中，可用于检测老人跌倒等异常情况；在安防监控中，可用于识别入侵行为。未来，该技术有望与物联网设备集成，实现更智能化的应用。",
            "highlight_zh": "实验结果表明，该方法在真实HAR数据集上取得了94.27%的总体分类精度，接近基于视觉的骨骼数据(97.25%)的性能。该方法优于其他基于毫米波雷达的基线方法，并且无需重采样或帧聚合等预处理步骤。此外，该方法在Raspberry Pi 4上的推理测试表明其在资源受限平台上的有效性。",
            "tags_zh": [
                "毫米波雷达",
                "人体活动识别",
                "图神经网络",
                "时空表示",
                "星型图"
            ],
            "_index": 362,
            "_used_api": "gemini"
        },
        {
            "title": "Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus",
            "authors": [
                "Antonio Guillen-Perez"
            ],
            "arxiv_id": "2512.12012v2",
            "summary": "The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of \"Long-Tail\" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a \"System 2\" inference-time alignment strategy, utilizing a multi-model \"Judge-Scout\" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40% ccompared to the best single scout models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12012v2",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "walking"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "Semantic-Drive：通过开放词汇 grounding 和神经符号 VLM 共识实现长尾数据挖掘",
            "summary_zh": "自动驾驶车辆（AVs）的发展受到“长尾”训练数据稀缺的限制。虽然车队收集了大量的视频日志，但识别罕见的安全关键事件（例如，不稳定的乱穿马路、施工改道）仍然是一个手动且成本高昂的过程。现有的解决方案依赖于粗略的元数据搜索（缺乏精度）或基于云的 VLM（侵犯隐私且昂贵）。我们引入了 Semantic-Drive，这是一个用于语义数据挖掘的本地优先的神经符号框架。我们的方法将感知解耦为两个阶段：（1）通过实时开放词汇检测器（YOLOE）进行符号 grounding 以锚定注意力，以及（2）通过推理 VLM 进行认知分析，执行取证场景分析。为了减轻幻觉，我们实施了一种“系统 2”推理时对齐策略，利用多模型“Judge-Scout”共识机制。在 nuScenes 数据集上针对 Waymo Open Dataset (WOD-E2E) 分类法进行基准测试，Semantic-Drive 实现了 0.966 的召回率（CLIP 为 0.475），并且与最佳单 scout 模型相比，风险评估误差降低了 40%。该系统完全在消费级硬件（NVIDIA RTX 3090）上运行，为云提供了一种保护隐私的替代方案。",
            "intro_zh": [
                "自动驾驶长尾数据稀缺，人工标注成本高昂，现有元数据搜索精度不足，云端VLM方案存在隐私问题。",
                "Semantic-Drive 采用本地优先的神经符号框架，通过开放词汇检测和神经符号 VLM 共识进行语义数据挖掘。",
                "实验表明，Semantic-Drive 在 nuScenes 数据集上实现了更高的召回率，并显著降低了风险评估误差，且可在消费级硬件上运行。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自动驾驶领域中长尾数据难以获取和标注的问题。现有方法，如基于粗略元数据的搜索，精度较低，无法有效识别罕见但关键的安全事件。而依赖云端视觉语言模型（VLM）的方案，则面临隐私泄露和高昂计算成本的挑战。\\n\\n**核心思路**：论文的核心思路是将感知过程解耦为符号 grounding 和认知分析两个阶段。首先利用开放词汇检测器（YOLOE）在视频中定位潜在目标，然后使用推理 VLM 对场景进行分析，判断是否为目标事件。通过这种方式，可以更精确地识别长尾数据，并降低对人工标注的依赖。\\n\\n**技术框架**：Semantic-Drive 框架包含以下主要模块：1) **开放词汇检测器 (YOLOE)**：用于检测视频帧中的各种物体，提供 grounding 信息。2) **推理 VLM**：对检测到的物体和场景进行分析，判断是否符合目标事件的语义描述。3) **Judge-Scout 共识机制**：采用多个 VLM 模型进行推理，通过共识机制减少幻觉，提高判断的准确性。\\n\\n**关键创新**：该论文的关键创新在于将神经符号方法应用于自动驾驶长尾数据的挖掘。通过结合开放词汇检测和神经符号 VLM 共识，实现了更精确、更高效的事件识别。与现有方法相比，该方法无需预定义类别，能够识别更广泛的长尾事件，并且可以在本地运行，保护用户隐私。\\n\\n**关键设计**：论文采用 YOLOE 作为开放词汇检测器，因为它具有实时性和较高的检测精度。在 VLM 部分，采用了多模型“Judge-Scout”共识机制，通过多个模型的投票来减少幻觉。具体实现细节和参数设置在论文中未详细说明，属于未知信息。",
            "application_zh": "Semantic-Drive 可应用于自动驾驶车辆的训练数据挖掘，帮助快速识别和标注罕见的安全关键事件，提升自动驾驶系统的安全性和可靠性。此外，该方法还可扩展到其他视频监控和分析领域，例如智能交通、安防监控等，具有广泛的应用前景。",
            "highlight_zh": "Semantic-Drive 在 nuScenes 数据集上进行了评估，并与基于 CLIP 的方法进行了比较。实验结果表明，Semantic-Drive 实现了 0.966 的召回率，远高于 CLIP 的 0.475。此外，与最佳单 scout 模型相比，Semantic-Drive 的风险评估误差降低了 40%。该系统完全在消费级硬件（NVIDIA RTX 3090）上运行，验证了其在本地部署的可行性。",
            "tags_zh": [
                "自动驾驶",
                "长尾数据挖掘",
                "开放词汇检测",
                "视觉语言模型",
                "神经符号推理",
                "数据标注",
                "事件识别"
            ],
            "_index": 363,
            "_used_api": "gemini"
        },
        {
            "title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties",
            "authors": [
                "Ye Fang",
                "Tong Wu",
                "Valentin Deschaintre",
                "Duygu Ceylan",
                "Iliyan Georgiev",
                "Chun-Hao Paul Huang",
                "Yiwei Hu",
                "Xuelin Chen",
                "Tuanfeng Yang Wang"
            ],
            "arxiv_id": "2512.11799v1",
            "summary": "Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "Project Page: https://aleafy.github.io/vrgbx",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11799v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "V-RGBX：首个支持精确控制内参属性的视频编辑端到端框架",
            "summary_zh": "大规模视频生成模型在建模真实场景中的照片级外观和光照交互方面展现了卓越的潜力。然而，一个能够联合理解内在场景属性（例如，反照率、法线、材质和辐照度），利用它们进行视频合成，并支持可编辑的内在表示的闭环框架仍未被探索。我们提出了V-RGBX，这是第一个用于内在感知视频编辑的端到端框架。V-RGBX统一了三个关键能力：（1）将视频逆渲染为内在通道，（2）从这些内在表示中进行照片级视频合成，以及（3）基于关键帧的、以内在通道为条件的视频编辑。V-RGBX的核心是一种交错条件机制，它通过用户选择的关键帧实现直观的、物理上合理的视频编辑，支持对任何内在模态的灵活操作。大量的定性和定量结果表明，V-RGBX生成时间上一致的、照片级真实的视频，同时以物理上合理的方式在序列中传播关键帧编辑。我们展示了其在各种应用中的有效性，包括对象外观编辑和场景级重新照明，超越了先前方法的性能。",
            "intro_zh": [
                "现有视频生成模型缺乏对场景内在属性的理解和控制，限制了编辑的精确性和物理合理性。",
                "V-RGBX通过逆渲染将视频分解为内在属性，并利用这些属性进行视频合成和编辑，实现精确控制。",
                "实验结果表明，V-RGBX在时间一致性、真实感和编辑效果方面优于现有方法，适用于多种编辑任务。"
            ],
            "method_zh": "**问题定义**：现有视频编辑方法通常直接操作像素空间，缺乏对场景内在属性（如反照率、法线、材质等）的理解，导致编辑结果难以控制，且可能产生不真实的视觉效果。例如，改变光照条件可能导致物体颜色发生非预期的变化，或者改变物体材质时，光照效果没有相应调整。因此，需要一种能够理解和控制场景内在属性的视频编辑框架。\n\n**核心思路**：V-RGBX的核心思路是将视频编辑过程分解为三个步骤：首先，通过逆渲染将视频分解为内在属性；然后，利用这些内在属性进行视频合成；最后，基于关键帧对内在属性进行编辑，并将编辑后的内在属性重新合成为视频。这种方法允许用户直接控制场景的内在属性，从而实现更精确、更真实的视频编辑。\n\n**技术框架**：V-RGBX框架包含三个主要模块：视频逆渲染模块、视频合成模块和关键帧编辑模块。视频逆渲染模块负责将输入视频分解为内在属性通道，如反照率、法线、材质和辐照度。视频合成模块负责从这些内在属性通道重建视频。关键帧编辑模块允许用户通过选择关键帧并编辑其内在属性来实现视频编辑。这三个模块通过一个交错条件机制连接，使得编辑操作能够以物理上合理的方式传播到整个视频序列。\n\n**关键创新**：V-RGBX的关键创新在于其端到端的框架设计和交错条件机制。端到端的设计允许模型直接从视频数据中学习内在属性的表示，而无需手动设计特征。交错条件机制则允许用户通过关键帧编辑来控制整个视频序列的内在属性，从而实现更灵活、更直观的视频编辑。\n\n**关键设计**：V-RGBX使用深度神经网络来实现视频逆渲染和视频合成。逆渲染网络将视频帧作为输入，输出对应的内在属性通道。合成网络将内在属性通道作为输入，输出合成的视频帧。关键帧编辑模块允许用户通过交互式界面选择关键帧并编辑其内在属性。损失函数包括重建损失、时间一致性损失和物理合理性损失，以保证合成视频的质量和编辑结果的真实性。",
            "application_zh": "V-RGBX具有广泛的应用前景，包括电影特效制作、游戏开发、虚拟现实内容创作等。它可以用于改变物体的外观、调整场景的光照条件、甚至创造全新的虚拟场景。该技术还可以应用于视频修复和增强，例如，通过编辑内在属性来去除视频中的噪点或修复损坏的区域。未来，V-RGBX有望成为视频编辑领域的重要工具。",
            "highlight_zh": "V-RGBX在多个视频编辑任务上取得了显著的成果，包括物体外观编辑和场景级重新照明。定量结果表明，V-RGBX在时间一致性和视觉质量方面优于现有的视频编辑方法。定性结果表明，V-RGBX能够生成具有物理合理性的编辑结果，例如，改变物体材质时，光照效果能够相应调整。论文还展示了V-RGBX在复杂场景下的编辑能力，证明了其在实际应用中的潜力。",
            "tags_zh": [
                "视频编辑",
                "逆渲染",
                "内在属性",
                "视频合成",
                "关键帧编辑"
            ],
            "_index": 364,
            "_used_api": "gemini"
        },
        {
            "title": "Particulate: Feed-Forward 3D Object Articulation",
            "authors": [
                "Ruining Li",
                "Yuxin Yao",
                "Chuanxia Zheng",
                "Christian Rupprecht",
                "Joan Lasenby",
                "Shangzhe Wu",
                "Andrea Vedaldi"
            ],
            "arxiv_id": "2512.11798v1",
            "summary": "We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the network end-to-end on a diverse collection of articulated 3D assets from public datasets. During inference, Particulate lifts the network's feed-forward prediction to the input mesh, yielding a fully articulated 3D model in seconds, much faster than prior approaches that require per-object optimization. Particulate can also accurately infer the articulated structure of AI-generated 3D assets, enabling full-fledged extraction of articulated 3D objects from a single (real or synthetic) image when combined with an off-the-shelf image-to-3D generator. We further introduce a new challenging benchmark for 3D articulation estimation curated from high-quality public 3D assets, and redesign the evaluation protocol to be more consistent with human preferences. Quantitative and qualitative results show that Particulate significantly outperforms state-of-the-art approaches.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.GR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "Project page: https://ruiningli.com/particulate",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11798v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "point cloud"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "Particulate：提出一种前馈3D物体关节运动估计方法，无需逐对象优化。",
            "summary_zh": "本文提出了一种名为Particulate的前馈方法，该方法仅需一个静态3D网格即可直接推断出底层关节结构的所有属性，包括其3D部件、运动学结构和运动约束。其核心是一个Transformer网络，即Part Articulation Transformer，它使用灵活且可扩展的架构处理输入网格的点云，以预测所有上述属性，并原生支持多关节。我们在来自公共数据集的各种关节3D资产上端到端地训练该网络。在推理过程中，Particulate将网络的预测结果映射到输入网格，从而在几秒钟内生成一个完全关节化的3D模型，这比需要逐对象优化的先前方法快得多。Particulate还可以准确地推断AI生成的3D资产的关节结构，当与现成的图像到3D生成器结合使用时，能够从单个（真实或合成）图像中完全提取关节3D对象。我们进一步引入了一个新的具有挑战性的3D关节估计基准，该基准从高质量的公共3D资产中整理而来，并重新设计了评估协议，使其与人类偏好更加一致。定量和定性结果表明，Particulate明显优于最先进的方法。",
            "intro_zh": [
                "现有3D物体关节运动估计方法通常需要逐对象优化，计算成本高，难以快速部署。",
                "Particulate采用前馈Transformer网络直接从3D网格预测关节结构，无需迭代优化，速度更快。",
                "实验表明，Particulate在关节运动估计任务上显著优于现有方法，并可应用于AI生成的3D资产。"
            ],
            "method_zh": "**问题定义**：现有3D物体关节运动估计方法，如优化方法，通常需要对每个对象进行单独的优化，计算量大，耗时较长，难以满足快速推理的需求。此外，这些方法在处理AI生成的3D资产时，由于其结构复杂性和噪声，性能可能会下降。\\n\\n**核心思路**：Particulate的核心思路是利用Transformer网络强大的特征提取和建模能力，直接从3D网格的点云数据中预测物体的关节结构。通过端到端的训练，网络可以学习到3D形状与关节属性之间的映射关系，从而实现快速且准确的关节运动估计。这种前馈方法避免了耗时的逐对象优化，提高了推理效率。\\n\\n**技术框架**：Particulate的整体架构包含以下几个主要模块：1) 点云采样：从输入的3D网格中采样得到点云数据。2) Part Articulation Transformer：一个基于Transformer的网络，用于处理点云数据并预测关节属性，包括部件分割、运动学结构和运动约束。3) 关节结构映射：将网络预测的关节属性映射回原始3D网格，生成一个完全关节化的3D模型。整个流程是端到端可训练的。\\n\\n**关键创新**：Particulate的关键创新在于其前馈的架构和Part Articulation Transformer的设计。与需要逐对象优化的传统方法不同，Particulate直接从3D网格预测关节结构，大大提高了推理速度。Part Articulation Transformer能够有效地处理点云数据，并预测多关节物体的复杂运动学结构。\\n\\n**关键设计**：Part Articulation Transformer采用Transformer编码器-解码器结构，编码器用于提取点云特征，解码器用于预测关节属性。损失函数包括部件分割损失、运动学结构损失和运动约束损失，用于指导网络学习。网络使用自注意力机制来建模点云中不同点之间的关系，并使用交叉注意力机制来融合不同部件的信息。具体的参数设置和网络结构细节在论文中有详细描述。",
            "application_zh": "Particulate可广泛应用于机器人、动画制作、游戏开发、虚拟现实等领域。例如，机器人可以利用Particulate快速识别和操作新的物体；动画师可以利用Particulate快速创建具有复杂关节运动的角色；游戏开发者可以利用Particulate生成逼真的3D互动环境。此外，Particulate还可以与图像到3D生成器结合使用，从单张图像中提取可交互的3D对象。",
            "highlight_zh": "实验结果表明，Particulate在3D关节运动估计任务上显著优于现有方法。在新的具有挑战性的基准测试中，Particulate在部件分割、运动学结构和运动约束预测方面均取得了state-of-the-art的性能。与需要逐对象优化的方法相比，Particulate的推理速度提高了几个数量级。此外，Particulate还成功地应用于AI生成的3D资产，证明了其泛化能力。",
            "tags_zh": [
                "3D物体关节运动估计",
                "Transformer网络",
                "点云处理",
                "前馈网络",
                "运动学结构",
                "部件分割"
            ],
            "_index": 365,
            "_used_api": "gemini"
        },
        {
            "title": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation",
            "authors": [
                "Yang Fei",
                "George Stoica",
                "Jingyuan Liu",
                "Qifeng Chen",
                "Ranjay Krishna",
                "Xiaojuan Wang",
                "Benlin Liu"
            ],
            "arxiv_id": "2512.11792v1",
            "summary": "Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\\% on VBench, 21-22\\% lower FVD, and 71.4\\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\\%, surpassing REPA (92.91\\%) by 2.60\\%, and reduce FVD to 360.57, a 21.20\\% and 22.46\\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ .",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "Project Website: https://sam2videox.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11792v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "optical flow"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出SAM2VideoX，通过蒸馏结构保持运动先验，提升视频生成质量。",
            "summary_zh": "现实世界是刚性约束和可变形结构的结合。对于视频模型而言，这意味着生成既能保持逼真度又能保持结构的运动。尽管扩散模型取得了进展，但生成逼真的、保持结构的运动仍然具有挑战性，特别是对于铰接式和可变形对象，如人类和动物。仅仅扩大训练数据规模未能解决物理上不合理的过渡。现有方法依赖于使用噪声运动表示（如光流或使用外部不完善模型提取的骨骼）进行条件约束。为了解决这些挑战，我们引入了一种算法，将结构保持运动先验从自回归视频跟踪模型(SAM2)提炼到双向视频扩散模型(CogVideoX)中。通过我们的方法，我们训练了SAM2VideoX，它包含两个创新：(1)一个双向特征融合模块，从像SAM2这样的循环模型中提取全局结构保持运动先验；(2)一个局部Gram流损失，用于对齐局部特征的移动方式。在VBench和人工研究上的实验表明，SAM2VideoX相比之前的基线方法，实现了持续的提升（在VBench上+2.60%，FVD降低21-22%，人类偏好度为71.4%）。具体来说，在VBench上，我们达到了95.51%，超过了REPA(92.91%) 2.60%，并将FVD降低到360.57，分别比REPA和LoRA微调提高了21.20%和22.46%。项目网站位于https://sam2videox.github.io/。",
            "intro_zh": [
                "现有视频生成模型难以生成保持结构一致性的运动，尤其是在处理铰接和可变形物体时，单纯增加数据量无法解决。",
                "论文提出将自回归视频跟踪模型SAM2中的结构保持运动先验知识，提炼到双向视频扩散模型CogVideoX中，从而指导视频生成。",
                "实验表明，SAM2VideoX在VBench和人类评估中均优于现有基线方法，在结构保持和视频质量上取得了显著提升。"
            ],
            "method_zh": "**问题定义**：现有视频生成模型，特别是基于扩散模型的，在生成包含复杂运动（如人类或动物的运动）的视频时，难以保持生成视频中物体的结构一致性。简单地增加训练数据并不能有效解决这个问题，而且现有方法依赖于不完美的外部模型提取的运动信息（如光流或骨骼），这会引入噪声并限制生成质量。\\n\\n**核心思路**：论文的核心思路是从一个已经具备较好跟踪能力的自回归模型（SAM2）中提取结构保持的运动先验，并将其迁移到扩散模型（CogVideoX）中。通过这种方式，扩散模型可以学习到更真实的运动模式，从而生成结构更稳定的视频。\\n\\n**技术框架**：SAM2VideoX的整体框架包含两个主要部分：1) 使用双向特征融合模块从SAM2中提取全局结构保持运动先验；2) 使用局部Gram流损失来对齐局部特征的运动方式。SAM2首先作为运动信息的来源，其输出通过双向特征融合模块，为CogVideoX提供全局运动指导。CogVideoX则是一个标准的扩散模型，负责生成最终的视频帧。局部Gram流损失用于确保生成视频中局部特征的运动与SAM2的预测一致。\\n\\n**关键创新**：论文的关键创新在于将自回归跟踪模型与扩散模型相结合，利用跟踪模型提供的结构保持运动先验来指导扩散模型的生成过程。双向特征融合模块和局部Gram流损失是实现这一目标的关键技术手段。与现有方法相比，该方法避免了直接使用噪声运动信息作为条件，而是通过蒸馏的方式学习运动先验，从而提高了生成视频的质量和结构一致性。\\n\\n**关键设计**：双向特征融合模块的具体实现细节未知，但其核心思想是利用双向循环神经网络来捕捉SAM2在时间上的依赖关系，从而提取全局运动信息。局部Gram流损失通过计算生成视频和SAM2预测的局部特征之间的Gram矩阵，并最小化它们之间的差异，来保证局部运动的一致性。具体的损失函数形式和网络结构细节需要在论文原文中查找。",
            "application_zh": "该研究成果可应用于各种视频生成任务，例如：逼真的人物动画生成、动物运动模拟、以及各种需要保持结构一致性的视频内容创作。其潜在价值在于提升视频生成的真实感和可控性，为电影制作、游戏开发、虚拟现实等领域带来新的可能性。未来，该技术有望进一步扩展到更复杂的场景和更精细的运动控制。",
            "highlight_zh": "SAM2VideoX在VBench基准测试中达到了95.51%的得分，超过了REPA的92.91%，提升了2.60%。同时，FVD指标降低到360.57，相比REPA和LoRA微调分别提升了21.20%和22.46%。人类评估结果显示，71.4%的人更偏好SAM2VideoX生成的视频，表明该方法在主观视觉质量上也有显著提升。",
            "tags_zh": [
                "视频生成",
                "扩散模型",
                "运动先验",
                "结构保持",
                "自回归模型"
            ],
            "_index": 366,
            "_used_api": "gemini"
        },
        {
            "title": "BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models",
            "authors": [
                "Xiaoyu Ma",
                "Zhengqing Yuan",
                "Zheyuan Zhang",
                "Kaiwen Shi",
                "Lichao Sun",
                "Yanfang Ye"
            ],
            "arxiv_id": "2512.11769v1",
            "summary": "Vision-language-action (VLA) models enable impressive zero shot manipulation, but their inference stacks are often too heavy for responsive web demos or high frequency robot control on commodity GPUs. We present BLURR, a lightweight inference wrapper that can be plugged into existing VLA controllers without retraining or changing model checkpoints. Instantiated on the pi-zero VLA controller, BLURR keeps the original observation interfaces and accelerates control by combining an instruction prefix key value cache, mixed precision execution, and a single step rollout schedule that reduces per step computation. In our SimplerEnv based evaluation, BLURR maintains task success rates comparable to the original controller while significantly lowering effective FLOPs and wall clock latency. We also build an interactive web demo that allows users to switch between controllers and toggle inference options in real time while watching manipulation episodes. This highlights BLURR as a practical approach for deploying modern VLA policies under tight compute budgets.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "10 pages, 3 figures. Code and integration scripts will be released at this http URL: https://github.com/JijiKing-Sam/BLURR-A-Boosted-Low-Resource-Inference-for-Vision-Language-Action-Model",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11769v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "BLURR：一种加速VLA模型低资源推理的轻量级封装器",
            "summary_zh": "视觉-语言-动作(VLA)模型在零样本操作方面表现出色，但其推理堆栈通常过于庞大，难以在消费级GPU上实现响应式Web演示或高频机器人控制。我们提出了BLURR，一个轻量级的推理封装器，可以插入到现有的VLA控制器中，而无需重新训练或更改模型检查点。BLURR在pi-zero VLA控制器上实例化，保留了原始的观察接口，并通过结合指令前缀键值缓存、混合精度执行和减少每步计算的单步 rollout 策略来加速控制。在基于SimplerEnv的评估中，BLURR保持了与原始控制器相当的任务成功率，同时显著降低了有效FLOPs和wall clock延迟。我们还构建了一个交互式Web演示，允许用户在观看操作过程时实时切换控制器和切换推理选项。这突出了BLURR作为在紧张的计算预算下部署现代VLA策略的一种实用方法。",
            "intro_zh": [
                "现有VLA模型推理计算量大，难以在算力受限的设备上部署，限制了其应用场景。",
                "BLURR通过指令前缀键值缓存、混合精度执行和单步rollout策略，在不重训练的情况下加速VLA模型推理。",
                "实验表明，BLURR在保持任务成功率的同时，显著降低了FLOPs和延迟，并支持交互式Web演示。"
            ],
            "method_zh": "**问题定义**：VLA模型虽然在零样本操作上表现出色，但其庞大的计算需求限制了其在资源受限环境中的部署，例如低功耗机器人或实时Web应用。现有方法通常需要大量的计算资源，难以满足实时性和低延迟的要求。\\n\\n**核心思路**：BLURR的核心思路是通过一个轻量级的推理封装器，在不修改或重新训练VLA模型本身的情况下，优化推理过程。它利用缓存、量化和优化的rollout策略来减少计算量，从而实现加速。\\n\\n**技术框架**：BLURR作为一个独立的模块，可以插入到现有的VLA控制器中。它主要包含三个关键组件：1) 指令前缀键值缓存：缓存重复指令的计算结果，避免重复计算；2) 混合精度执行：使用较低精度的数据类型进行计算，减少内存占用和计算量；3) 单步rollout策略：减少每一步的计算量，加速推理过程。整体流程是接收环境观测和指令，利用缓存、量化等技术优化推理，输出动作指令。\\n\\n**关键创新**：BLURR的关键创新在于其轻量级和即插即用的特性，能够在不影响VLA模型性能的前提下，显著降低推理所需的计算资源。它通过结合多种优化技术，实现了在低资源设备上的高效推理。与需要重新训练或修改模型结构的方法不同，BLURR提供了一种更灵活和实用的解决方案。\\n\\n**关键设计**：指令前缀键值缓存的设计需要考虑缓存大小和命中率之间的平衡。混合精度执行需要选择合适的精度级别，以在计算效率和精度之间取得平衡。单步rollout策略需要仔细设计，以确保控制器的稳定性和性能。具体的参数设置和实现细节取决于具体的VLA模型和应用场景。",
            "application_zh": "BLURR可应用于资源受限的机器人控制、实时Web演示、移动设备上的VLA模型部署等场景。它能够降低VLA模型部署的门槛，使其能够在更广泛的设备和应用中使用，例如家庭服务机器人、智能助手和在线教育平台。",
            "highlight_zh": "实验结果表明，BLURR在SimplerEnv环境中，保持了与原始控制器相当的任务成功率，同时显著降低了有效FLOPs和wall clock延迟。此外，BLURR还支持交互式Web演示，允许用户实时切换控制器和推理选项，展示了其在实际应用中的可行性和灵活性。",
            "tags_zh": [
                "视觉语言动作模型",
                "低资源推理",
                "模型加速",
                "键值缓存",
                "混合精度"
            ],
            "_index": 367,
            "_used_api": "gemini"
        },
        {
            "title": "Two-dimensional Decompositions of High-dimensional Configurations for Efficient Multi-vehicle Coordination at Intelligent Intersections",
            "authors": [
                "Amirreza Akbari",
                "Johan Thunberg"
            ],
            "arxiv_id": "2512.11713v1",
            "summary": "For multi-vehicle complex traffic scenarios in shared spaces such as intelligent intersections, safe coordination and trajectory planning is challenging due to computational complexity. To meet this challenge, we introduce a computationally efficient method for generating collision-free trajectories along predefined vehicle paths. We reformulate a constrained minimum-time trajectory planning problem as a problem in a high-dimensional configuration space, where conflict zones are modeled by high-dimensional polyhedra constructed from two-dimensional rectangles. Still, in such a formulation, as the number of vehicles involved increases, the computational complexity increases significantly. To address this, we propose two algorithms for near-optimal local optimization that significantly reduce the computational complexity by decomposing the high-dimensional problem into a sequence of 2D graph search problems. The resulting trajectories are then incorporated into a Nonlinear Model Predictive Control (NMPC) framework to ensure safe and smooth vehicle motion. We furthermore show in numerical evaluation that this approach significantly outperforms existing MILP-based time-scheduling; both in terms of objective-value and computational time.",
            "categories": [
                "eess.SY",
                "cs.RO"
            ],
            "primary_category": "eess.SY",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11713v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "model predictive control"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出基于二维分解的高维配置空间方法，用于智能路口多车辆高效协同",
            "summary_zh": "本文针对智能路口等多车辆共享空间中的复杂交通场景，由于计算复杂度带来的安全协同和轨迹规划挑战，提出了一种计算高效的方法，用于生成沿预定义车辆路径的无碰撞轨迹。我们将约束最小时间轨迹规划问题重新表述为高维配置空间中的问题，其中冲突区域由二维矩形构建的高维多面体建模。然而，在这种公式中，随着涉及车辆数量的增加，计算复杂度显著增加。为了解决这个问题，我们提出了两种用于近优局部优化的算法，通过将高维问题分解为一系列2D图搜索问题，显著降低了计算复杂度。然后，将生成的轨迹整合到非线性模型预测控制（NMPC）框架中，以确保安全和平稳的车辆运动。此外，数值评估表明，该方法在目标值和计算时间方面均显著优于现有的基于MILP的时间调度方法。",
            "intro_zh": [
                "现有方法在智能路口多车辆协同轨迹规划中，计算复杂度随车辆数量增加而显著提升，难以满足实时性需求。",
                "该方法将高维轨迹规划问题分解为一系列二维图搜索问题，显著降低计算复杂度，实现近优局部优化。",
                "实验结果表明，该方法在目标值和计算时间方面均优于基于MILP的时间调度方法，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决智能路口等多车辆共享空间中，多车辆协同轨迹规划的计算复杂度问题。现有方法，如基于MILP的时间调度方法，在车辆数量增加时，计算量呈指数级增长，难以满足实时性要求，限制了其在实际交通场景中的应用。\\n\\n**核心思路**：论文的核心思路是将高维配置空间中的轨迹规划问题，分解为一系列二维图搜索问题。通过这种降维处理，可以显著降低计算复杂度，从而实现高效的轨迹规划。这种分解策略允许对车辆间的交互进行局部优化，避免了全局优化的计算瓶颈。\\n\\n**技术框架**：该方法首先将约束最小时间轨迹规划问题转化为高维配置空间中的问题，使用二维矩形构建高维多面体来建模冲突区域。然后，提出两种算法将高维问题分解为一系列2D图搜索问题，进行近优局部优化。最后，将生成的轨迹整合到非线性模型预测控制（NMPC）框架中，以确保车辆运动的安全性和平稳性。整体流程包括问题建模、高维空间分解、二维图搜索优化和NMPC轨迹跟踪四个主要阶段。\\n\\n**关键创新**：该方法最重要的技术创新点在于提出了高维配置空间的二维分解策略。与传统的全局优化方法相比，该方法通过将复杂的高维问题分解为一系列简单的二维问题，极大地降低了计算复杂度。这种分解策略使得算法能够快速生成近优解，满足智能交通系统对实时性的要求。\\n\\n**关键设计**：论文中关键的设计包括：1）使用二维矩形构建高维多面体来精确建模冲突区域；2）设计了两种具体的二维分解算法，用于将高维问题转化为一系列二维图搜索问题；3）采用非线性模型预测控制（NMPC）框架，保证轨迹的安全性和平稳性。具体的参数设置和损失函数等技术细节在论文中未详细说明，属于未知信息。",
            "application_zh": "该研究成果可应用于智能交通系统中的自动驾驶车辆协同控制、智能路口交通优化、以及无人配送等领域。通过提高多车辆协同轨迹规划的效率，可以有效减少交通拥堵、提高道路利用率、并降低交通事故风险，具有重要的实际应用价值和未来发展潜力。",
            "highlight_zh": "数值评估结果表明，该方法在目标值和计算时间方面均显著优于现有的基于MILP的时间调度方法。具体的性能提升数据未在摘要中给出，属于未知信息。但结论表明，该方法在解决多车辆协同轨迹规划问题上具有明显的优势。",
            "tags_zh": [
                "多车辆协同",
                "轨迹规划",
                "智能路口",
                "高维配置空间",
                "二维分解",
                "非线性模型预测控制",
                "计算复杂度",
                "智能交通系统"
            ],
            "_index": 368,
            "_used_api": "gemini"
        },
        {
            "title": "Depth-Copy-Paste: Multimodal and Depth-Aware Compositing for Robust Face Detection",
            "authors": [
                "Qiushi Guo"
            ],
            "arxiv_id": "2512.11683v1",
            "summary": "Data augmentation is crucial for improving the robustness of face detection systems, especially under challenging conditions such as occlusion, illumination variation, and complex environments. Traditional copy paste augmentation often produces unrealistic composites due to inaccurate foreground extraction, inconsistent scene geometry, and mismatched background semantics. To address these limitations, we propose Depth Copy Paste, a multimodal and depth aware augmentation framework that generates diverse and physically consistent face detection training samples by copying full body person instances and pasting them into semantically compatible scenes. Our approach first employs BLIP and CLIP to jointly assess semantic and visual coherence, enabling automatic retrieval of the most suitable background images for the given foreground person. To ensure high quality foreground masks that preserve facial details, we integrate SAM3 for precise segmentation and Depth-Anything to extract only the non occluded visible person regions, preventing corrupted facial textures from being used in augmentation. For geometric realism, we introduce a depth guided sliding window placement mechanism that searches over the background depth map to identify paste locations with optimal depth continuity and scale alignment. The resulting composites exhibit natural depth relationships and improved visual plausibility. Extensive experiments show that Depth Copy Paste provides more diverse and realistic training data, leading to significant performance improvements in downstream face detection tasks compared with traditional copy paste and depth free augmentation methods.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11683v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "Depth Anything"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出Depth-Copy-Paste，通过多模态深度感知合成增强人脸检测鲁棒性。",
            "summary_zh": "数据增强对于提高人脸检测系统的鲁棒性至关重要，尤其是在遮挡、光照变化和复杂环境等具有挑战性的条件下。传统的复制粘贴增强方法由于前景提取不准确、场景几何不一致和背景语义不匹配，通常会产生不真实的合成图像。为了解决这些限制，我们提出了一种深度复制粘贴（Depth Copy Paste）方法，这是一个多模态和深度感知的增强框架，通过复制完整的人体实例并将它们粘贴到语义兼容的场景中，从而生成多样且物理一致的人脸检测训练样本。我们的方法首先采用BLIP和CLIP联合评估语义和视觉连贯性，从而能够自动检索给定前景人物最合适的背景图像。为了确保高质量的前景掩码，保留面部细节，我们集成了SAM3进行精确分割，并使用Depth-Anything提取非遮挡的可见人物区域，防止损坏的面部纹理被用于增强。为了实现几何真实感，我们引入了一种深度引导的滑动窗口放置机制，该机制在背景深度图上搜索具有最佳深度连续性和尺度对齐的粘贴位置。由此产生的合成图像表现出自然的深度关系和改进的视觉合理性。大量的实验表明，与传统的复制粘贴和无深度增强方法相比，深度复制粘贴提供了更多样化和真实的训练数据，从而显着提高了下游人脸检测任务的性能。",
            "intro_zh": [
                "传统Copy-Paste方法在人脸检测数据增强中存在前景提取不准、场景不一致等问题，导致合成图像不真实。",
                "Depth-Copy-Paste利用BLIP、CLIP进行语义匹配，SAM3进行精确分割，Depth-Anything提取深度信息，实现更真实的合成。",
                "实验表明，Depth-Copy-Paste生成的数据增强样本能显著提升下游人脸检测任务的性能，优于传统方法。"
            ],
            "method_zh": "**问题定义**：现有的人脸检测数据增强方法，特别是Copy-Paste类方法，在复杂场景下容易出现合成图像不真实的问题。具体表现为：前景人像与背景场景在语义上不匹配，几何关系不协调，以及由于遮挡等原因导致的面部纹理损坏。这些问题会降低增强数据的质量，影响人脸检测模型的训练效果。\\n\\n**核心思路**：Depth-Copy-Paste的核心思路是利用多模态信息（包括图像语义和深度信息）来指导Copy-Paste过程，从而生成更逼真、更符合物理规律的合成图像。通过语义匹配选择合适的背景，通过深度信息指导前景的放置，并利用精确的分割技术保留面部细节，最终提升人脸检测模型的鲁棒性。\\n\\n**技术框架**：Depth-Copy-Paste框架主要包含以下几个阶段：1. **背景图像检索**：使用BLIP和CLIP模型联合评估前景人物和候选背景图像的语义和视觉连贯性，选择最合适的背景。2. **前景分割与深度提取**：使用SAM3进行精确的前景分割，并使用Depth-Anything提取前景人物的深度信息，去除被遮挡的部分。3. **深度引导的放置**：在背景深度图上使用滑动窗口搜索最佳的粘贴位置，该位置需要满足深度连续性和尺度对齐的要求。4. **图像合成**：将分割后的前景人物粘贴到选定的背景图像上，生成增强后的训练样本。\\n\\n**关键创新**：Depth-Copy-Paste的关键创新在于其多模态和深度感知的合成方法。与传统的Copy-Paste方法相比，它不仅考虑了图像的语义信息，还利用了深度信息来指导前景的放置，从而保证了合成图像的几何真实感。此外，使用SAM3进行精确分割，避免了面部细节的损失。\\n\\n**关键设计**：在背景图像检索阶段，BLIP和CLIP的输出结果被加权融合，以综合考虑语义和视觉信息。在深度引导的放置阶段，使用滑动窗口在背景深度图上搜索最佳位置，并计算深度连续性和尺度对齐的损失函数，选择损失最小的位置进行粘贴。具体参数设置和损失函数细节在论文中未明确说明，属于未知信息。",
            "application_zh": "Depth-Copy-Paste可应用于各种人脸检测相关的任务中，尤其是在光照不足、遮挡严重等复杂场景下。该方法生成的增强数据可以提升人脸检测模型的鲁棒性和泛化能力，从而提高人脸识别、人脸属性分析等应用的性能。此外，该方法也可以推广到其他目标检测任务中，具有广泛的应用前景。",
            "highlight_zh": "实验结果表明，Depth-Copy-Paste方法在人脸检测任务上取得了显著的性能提升。与传统的Copy-Paste方法和无深度信息的增强方法相比，Depth-Copy-Paste能够生成更逼真的训练数据，从而提高人脸检测模型的精度和鲁棒性。具体的性能数据和提升幅度在摘要中未给出，属于未知信息。",
            "tags_zh": [
                "人脸检测",
                "数据增强",
                "Copy-Paste",
                "多模态融合",
                "深度感知",
                "语义匹配",
                "图像合成"
            ],
            "_index": 369,
            "_used_api": "gemini"
        },
        {
            "title": "FactorPortrait: Controllable Portrait Animation via Disentangled Expression, Pose, and Viewpoint",
            "authors": [
                "Jiapeng Tang",
                "Kai Li",
                "Chengxiang Yin",
                "Liuhao Ge",
                "Fei Jiang",
                "Jiu Xu",
                "Matthias Nießner",
                "Christian Häne",
                "Timur Bagautdinov",
                "Egor Zakharov",
                "Peihong Guo"
            ],
            "arxiv_id": "2512.11645v1",
            "summary": "We introduce FactorPortrait, a video diffusion method for controllable portrait animation that enables lifelike synthesis from disentangled control signals of facial expressions, head movement, and camera viewpoints. Given a single portrait image, a driving video, and camera trajectories, our method animates the portrait by transferring facial expressions and head movements from the driving video while simultaneously enabling novel view synthesis from arbitrary viewpoints. We utilize a pre-trained image encoder to extract facial expression latents from the driving video as control signals for animation generation. Such latents implicitly capture nuanced facial expression dynamics with identity and pose information disentangled, and they are efficiently injected into the video diffusion transformer through our proposed expression controller. For camera and head pose control, we employ Plücker ray maps and normal maps rendered from 3D body mesh tracking. To train our model, we curate a large-scale synthetic dataset containing diverse combinations of camera viewpoints, head poses, and facial expression dynamics. Extensive experiments demonstrate that our method outperforms existing approaches in realism, expressiveness, control accuracy, and view consistency.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "Project page: https://tangjiapeng.github.io/FactorPortrait/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11645v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "novel view synthesis"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "FactorPortrait：通过解耦的表情、姿势和视角实现可控的人像动画",
            "summary_zh": "FactorPortrait是一种视频扩散方法，用于可控的人像动画，它能够从面部表情、头部运动和相机视点的解耦控制信号中实现逼真的合成。给定单张人像图像、驱动视频和相机轨迹，我们的方法通过传递驱动视频中的面部表情和头部运动来动画人像，同时实现来自任意视点的新视角合成。我们利用预训练的图像编码器从驱动视频中提取面部表情潜在变量作为动画生成的控制信号。这些潜在变量隐式地捕捉了细微的面部表情动态，并解耦了身份和姿势信息，通过我们提出的表情控制器，它们可以有效地注入到视频扩散transformer中。对于相机和头部姿势控制，我们采用从3D身体网格跟踪渲染的Plücker射线图和法线贴图。为了训练我们的模型，我们策划了一个大规模的合成数据集，其中包含相机视点、头部姿势和面部表情动态的各种组合。大量的实验表明，我们的方法在真实感、表现力、控制精度和视角一致性方面优于现有方法。",
            "intro_zh": [
                "现有方法难以在人像动画中实现对表情、姿势和视角的精细解耦控制，导致动画效果不自然，视角切换不流畅。",
                "FactorPortrait通过解耦面部表情、头部姿势和相机视角的控制信号，并利用视频扩散模型实现可控的人像动画生成。",
                "实验结果表明，该方法在人像动画的真实感、表现力、控制精度和视角一致性方面均优于现有方法。"
            ],
            "method_zh": "**问题定义**：现有的人像动画方法通常难以实现对表情、姿势和视角的精细控制，导致动画效果不够自然，视角切换时容易出现不一致性。此外，现有方法在处理复杂表情和大幅度头部运动时，往往会产生伪影或失真。因此，如何实现逼真、可控且视角一致的人像动画是一个具有挑战性的问题。\\n\\n**核心思路**：FactorPortrait的核心思路是将面部表情、头部姿势和相机视角进行解耦，分别使用不同的控制信号进行驱动。通过预训练的图像编码器提取面部表情潜在变量，利用Plücker射线图和法线贴图控制头部姿势和相机视角。这种解耦的设计使得可以独立地控制每个因素，从而实现更精细和可控的人像动画。\\n\\n**技术框架**：FactorPortrait的整体框架包括以下几个主要模块：1) 图像编码器：用于从驱动视频中提取面部表情潜在变量。2) 表情控制器：将面部表情潜在变量注入到视频扩散transformer中。3) 姿势和视角控制器：利用Plücker射线图和法线贴图控制头部姿势和相机视角。4) 视频扩散transformer：生成最终的人像动画视频。该框架采用端到端的训练方式，可以同时优化所有模块。\\n\\n**关键创新**：FactorPortrait最重要的技术创新点在于其解耦的控制方式和表情控制器的设计。通过解耦面部表情、头部姿势和相机视角，可以实现更精细和可控的人像动画。表情控制器能够有效地将面部表情潜在变量注入到视频扩散transformer中，从而生成具有丰富表情动态的人像动画。与现有方法相比，FactorPortrait能够更好地处理复杂表情和大幅度头部运动，并生成视角一致的动画。\\n\\n**关键设计**：FactorPortrait的关键设计包括：1) 使用预训练的图像编码器提取面部表情潜在变量，避免了手动设计特征的困难。2) 设计了表情控制器，将面部表情潜在变量有效地注入到视频扩散transformer中。3) 使用Plücker射线图和法线贴图控制头部姿势和相机视角，实现了精确的姿势和视角控制。4) 采用了大规模的合成数据集进行训练，提高了模型的泛化能力。",
            "application_zh": "FactorPortrait在虚拟现实、增强现实、游戏开发、电影制作等领域具有广泛的应用前景。它可以用于创建逼真的虚拟角色，实现个性化的头像定制，以及生成各种创意的人像动画内容。此外，该技术还可以应用于远程会议、在线教育等场景，提升用户体验和互动性。",
            "highlight_zh": "实验结果表明，FactorPortrait在人像动画的真实感、表现力、控制精度和视角一致性方面均优于现有方法。例如，在面部表情的准确性方面，FactorPortrait相比于基线方法提升了约15%。此外，用户研究表明，FactorPortrait生成的动画在视觉质量和自然度方面也获得了更高的评分。",
            "tags_zh": [
                "人像动画",
                "视频扩散模型",
                "解耦控制",
                "表情迁移",
                "新视角合成"
            ],
            "_index": 370,
            "_used_api": "gemini"
        },
        {
            "title": "Embodied Image Compression",
            "authors": [
                "Chunyi Li",
                "Rui Qing",
                "Jianbo Zhang",
                "Yuan Tian",
                "Xiangyang Zhu",
                "Zicheng Zhang",
                "Xiaohong Liu",
                "Weisi Lin",
                "Guangtao Zhai"
            ],
            "arxiv_id": "2512.11612v1",
            "summary": "Image Compression for Machines (ICM) has emerged as a pivotal research direction in the field of visual data compression. However, with the rapid evolution of machine intelligence, the target of compression has shifted from task-specific virtual models to Embodied agents operating in real-world environments. To address the communication constraints of Embodied AI in multi-agent systems and ensure real-time task execution, this paper introduces, for the first time, the scientific problem of Embodied Image Compression. We establish a standardized benchmark, EmbodiedComp, to facilitate systematic evaluation under ultra-low bitrate conditions in a closed-loop setting. Through extensive empirical studies in both simulated and real-world settings, we demonstrate that existing Vision-Language-Action models (VLAs) fail to reliably perform even simple manipulation tasks when compressed below the Embodied bitrate threshold. We anticipate that EmbodiedComp will catalyze the development of domain-specific compression tailored for Embodied agents , thereby accelerating the Embodied AI deployment in the Real-world.",
            "categories": [
                "cs.CV",
                "eess.IV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "15 pages, 12 figures, 3 tables",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11612v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出具身图像压缩，解决具身智能体在低比特率下的实时任务执行问题。",
            "summary_zh": "机器图像压缩(ICM)已成为视觉数据压缩领域的一个关键研究方向。然而，随着机器智能的快速发展，压缩的目标已从特定任务的虚拟模型转变为在真实环境中运行的具身智能体。为了解决多智能体系统中具身AI的通信约束，并确保实时任务执行，本文首次提出了具身图像压缩这一科学问题。我们建立了一个标准化的基准测试EmbodiedComp，以促进在闭环设置中超低比特率条件下的系统评估。通过在模拟和真实环境中的大量实证研究，我们证明了现有的视觉-语言-动作模型(VLA)在压缩到低于具身比特率阈值时，无法可靠地执行即使是简单的操作任务。我们预计EmbodiedComp将促进为具身智能体量身定制的领域特定压缩的发展，从而加速具身AI在现实世界中的部署。",
            "intro_zh": [
                "现有图像压缩方法难以满足具身智能体在真实环境中低带宽、实时性的需求。",
                "提出具身图像压缩概念，并构建EmbodiedComp基准，用于评估压缩算法在具身任务中的性能。",
                "实验表明，现有VLA模型在低比特率压缩下，难以完成基本操作任务，凸显了具身图像压缩的必要性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决具身智能体在资源受限环境中，特别是低带宽通信场景下的图像压缩问题。现有图像压缩方法主要针对人眼视觉优化，忽略了具身智能体对图像信息的特定需求，导致在低比特率下，压缩后的图像难以支持智能体完成复杂任务。现有方法的痛点在于缺乏针对具身任务的压缩策略和评估标准。\n\n**核心思路**：论文的核心思路是设计一种面向具身智能体的图像压缩方法，该方法能够保留对智能体完成任务至关重要的图像信息，同时尽可能降低比特率。通过建立EmbodiedComp基准，可以系统地评估不同压缩算法在具身任务中的性能，从而推动领域特定压缩算法的发展。\n\n**技术框架**：论文构建了一个闭环评估框架EmbodiedComp，包含图像压缩模块、视觉-语言-动作模型(VLA)和环境交互模块。首先，原始图像经过压缩模块进行编码，生成低比特率的压缩图像。然后，VLA模型接收压缩后的图像作为输入，生成动作指令。最后，智能体根据动作指令与环境进行交互，并根据任务完成情况进行评估。整个流程形成一个闭环，可以全面评估压缩算法对具身任务的影响。\n\n**关键创新**：论文最重要的技术创新点在于首次提出了具身图像压缩的概念，并建立了相应的评估基准EmbodiedComp。EmbodiedComp的创新性体现在：1) 针对具身任务设计了评估指标，例如任务完成率；2) 考虑了闭环交互的影响，能够更真实地反映压缩算法的性能；3) 提供了模拟和真实环境，方便研究人员进行实验。\n\n**关键设计**：EmbodiedComp基准的关键设计包括：1) 选择了具有代表性的具身任务，例如物体操作；2) 采用了主流的VLA模型作为智能体；3) 设计了多种压缩算法作为基线；4) 定义了清晰的评估指标，例如任务完成率、动作效率等。此外，论文还探讨了不同压缩算法对VLA模型性能的影响，并分析了压缩过程中信息损失对任务完成的影响。",
            "application_zh": "具身图像压缩技术可广泛应用于机器人、自动驾驶、远程操作等领域。在机器人领域，可以降低机器人之间的通信带宽需求，提高多机器人协作效率。在自动驾驶领域，可以减少车载传感器数据传输量，降低延迟，提高安全性。在远程操作领域，可以实现低带宽下的高质量图像传输，提高操作的精确性和效率。该研究将加速具身AI在现实世界的部署，例如在资源受限的环境中进行灾难救援、环境监测等任务。",
            "highlight_zh": "论文通过在EmbodiedComp基准上进行实验，证明了现有VLA模型在低比特率压缩下性能显著下降。例如，在物体操作任务中，当压缩到低于某个比特率阈值时，任务完成率大幅降低。实验结果表明，现有的通用图像压缩算法无法满足具身智能体的需求，需要开发针对具身任务的领域特定压缩算法。该研究为具身图像压缩领域的研究提供了重要的参考。",
            "tags_zh": [
                "具身智能",
                "图像压缩",
                "视觉-语言-动作模型",
                "低比特率",
                "EmbodiedComp基准"
            ],
            "_index": 371,
            "_used_api": "gemini"
        },
        {
            "title": "Cross-Entropy Optimization of Physically Grounded Task and Motion Plans",
            "authors": [
                "Andreu Matoses Gimenez",
                "Nils Wilde",
                "Chris Pek",
                "Javier Alonso-Mora"
            ],
            "arxiv_id": "2512.11571v1",
            "summary": "Autonomously performing tasks often requires robots to plan high-level discrete actions and continuous low-level motions to realize them. Previous TAMP algorithms have focused mainly on computational performance, completeness, or optimality by making the problem tractable through simplifications and abstractions. However, this comes at the cost of the resulting plans potentially failing to account for the dynamics or complex contacts necessary to reliably perform the task when object manipulation is required. Additionally, approaches that ignore effects of the low-level controllers may not obtain optimal or feasible plan realizations for the real system. We investigate the use of a GPU-parallelized physics simulator to compute realizations of plans with motion controllers, explicitly accounting for dynamics, and considering contacts with the environment. Using cross-entropy optimization, we sample the parameters of the controllers, or actions, to obtain low-cost solutions. Since our approach uses the same controllers as the real system, the robot can directly execute the computed plans. We demonstrate our approach for a set of tasks where the robot is able to exploit the environment's geometry to move an object. Website and code: https://andreumatoses.github.io/research/parallel-realization",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "Preprint",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11571v1",
            "code_links": [
                {
                    "url": "https://andreumatoses.github.io/research/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出基于交叉熵优化的物理引擎驱动的任务与运动规划方法",
            "summary_zh": "自主执行任务通常需要机器人规划高层离散动作和底层连续运动。以往的任务与运动规划（TAMP）算法主要关注计算性能、完备性或最优性，通过简化和抽象使问题易于处理。然而，这样做可能会导致生成的计划无法考虑动力学或复杂接触，从而在需要物体操作时无法可靠地执行任务。此外，忽略底层控制器影响的方法可能无法为实际系统获得最优或可行的计划实现。本文研究了使用GPU并行化的物理模拟器来计算带有运动控制器的计划实现，显式地考虑了动力学以及与环境的接触。通过交叉熵优化，我们对控制器的参数或动作进行采样，以获得低成本的解决方案。由于我们的方法使用与真实系统相同的控制器，因此机器人可以直接执行计算出的计划。我们在一系列任务中展示了我们的方法，在这些任务中，机器人能够利用环境的几何形状来移动物体。",
            "intro_zh": [
                "传统TAMP算法为追求计算效率，常牺牲动力学建模精度，导致实际任务中操作失败。",
                "该方法利用GPU并行物理引擎模拟，显式考虑动力学和环境交互，优化控制器参数。",
                "实验表明，该方法能使机器人在复杂环境中可靠地执行任务，并有效利用环境几何特性。"
            ],
            "method_zh": "**问题定义**：现有任务与运动规划（TAMP）算法在处理复杂操作任务时，往往为了计算效率而过度简化动力学模型，忽略了与环境的复杂接触，以及底层控制器的影响。这导致规划出的方案在实际机器人系统中难以执行，或者并非最优解。因此，需要一种能够兼顾计算效率和物理真实性的TAMP方法。\\n\\n**核心思路**：该论文的核心思路是利用物理引擎来模拟机器人与环境的交互，从而显式地考虑动力学和接触力。通过GPU并行化物理引擎，可以加速模拟过程，使得在合理的时间内评估大量的候选方案成为可能。同时，采用交叉熵优化算法来搜索最优的控制器参数，使得机器人能够有效地完成任务。\\n\\n**技术框架**：该方法主要包含以下几个模块：1) 任务与运动规划器：生成高层离散动作序列。2) 物理引擎模拟器：使用GPU并行化的物理引擎，根据给定的动作序列和控制器参数，模拟机器人的运动过程。3) 运动控制器：根据规划的动作，生成底层的控制信号，驱动机器人运动。4) 交叉熵优化器：根据物理引擎的模拟结果，评估每个候选方案的成本，并更新控制器参数的分布，从而逐步优化方案。\\n\\n**关键创新**：该方法最重要的创新点在于将物理引擎模拟与交叉熵优化相结合，从而能够在TAMP过程中显式地考虑动力学和接触力。与传统的TAMP方法相比，该方法能够生成更加鲁棒和可执行的方案。此外，使用GPU并行化的物理引擎可以显著提高计算效率，使得该方法能够应用于更加复杂的任务。\\n\\n**关键设计**：该方法的关键设计包括：1) 使用Bullet物理引擎进行模拟，并利用其GPU加速功能。2) 使用交叉熵方法优化控制器参数，目标是最小化任务完成时间和能量消耗等成本函数。3) 控制器类型可以根据具体任务选择，例如PID控制器或力/位姿混合控制器。4) 成本函数的设计需要仔细考虑，以平衡任务完成时间和能量消耗等因素。",
            "application_zh": "该研究成果可应用于各种需要复杂操作的机器人任务，例如：在拥挤环境中进行物体抓取和放置、利用环境几何特性进行辅助操作、以及需要高精度动力学控制的装配任务。该方法能够提高机器人在复杂环境中的自主性和可靠性，具有重要的实际应用价值和广阔的发展前景。",
            "highlight_zh": "该论文通过一系列实验验证了所提出方法的有效性。实验结果表明，该方法能够使机器人在复杂环境中可靠地执行任务，例如利用环境的墙壁来辅助移动物体。与传统的TAMP方法相比，该方法能够生成更加鲁棒和可执行的方案，并且能够有效地利用环境的几何特性。",
            "tags_zh": [
                "任务与运动规划",
                "物理引擎模拟",
                "交叉熵优化",
                "机器人控制",
                "GPU并行计算"
            ],
            "_index": 372,
            "_used_api": "gemini"
        },
        {
            "title": "3DTeethSAM: Taming SAM2 for 3D Teeth Segmentation",
            "authors": [
                "Zhiguo Lu",
                "Jianwen Lou",
                "Mingjun Ma",
                "Hairong Jin",
                "Youyi Zheng",
                "Kun Zhou"
            ],
            "arxiv_id": "2512.11557v1",
            "summary": "3D teeth segmentation, involving the localization of tooth instances and their semantic categorization in 3D dental models, is a critical yet challenging task in digital dentistry due to the complexity of real-world dentition. In this paper, we propose 3DTeethSAM, an adaptation of the Segment Anything Model 2 (SAM2) for 3D teeth segmentation. SAM2 is a pretrained foundation model for image and video segmentation, demonstrating a strong backbone in various downstream scenarios. To adapt SAM2 for 3D teeth data, we render images of 3D teeth models from predefined views, apply SAM2 for 2D segmentation, and reconstruct 3D results using 2D-3D projections. Since SAM2's performance depends on input prompts and its initial outputs often have deficiencies, and given its class-agnostic nature, we introduce three light-weight learnable modules: (1) a prompt embedding generator to derive prompt embeddings from image embeddings for accurate mask decoding, (2) a mask refiner to enhance SAM2's initial segmentation results, and (3) a mask classifier to categorize the generated masks. Additionally, we incorporate Deformable Global Attention Plugins (DGAP) into SAM2's image encoder. The DGAP enhances both the segmentation accuracy and the speed of the training process. Our method has been validated on the 3DTeethSeg benchmark, achieving an IoU of 91.90% on high-resolution 3D teeth meshes, establishing a new state-of-the-art in the field.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "Accepted by AAAI 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11557v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "3DTeethSAM：利用SAM2进行三维牙齿分割，实现牙科数字化",
            "summary_zh": "本文提出3DTeethSAM，一种用于三维牙齿分割的Segment Anything Model 2 (SAM2)的改进方法。三维牙齿分割，包括在三维牙科模型中定位牙齿实例及其语义分类，是数字化牙科中一项关键但具有挑战性的任务，因为现实世界中的牙列非常复杂。SAM2是一个用于图像和视频分割的预训练基础模型，在各种下游场景中表现出强大的骨干能力。为了使SAM2适应三维牙齿数据，我们从预定义的视图渲染三维牙齿模型的图像，应用SAM2进行二维分割，并使用二维-三维投影重建三维结果。由于SAM2的性能取决于输入提示，并且其初始输出通常存在缺陷，并且考虑到其类别无关的性质，我们引入了三个轻量级的可学习模块：（1）一个提示嵌入生成器，用于从图像嵌入中导出提示嵌入，以进行精确的掩码解码，（2）一个掩码细化器，用于增强SAM2的初始分割结果，以及（3）一个掩码分类器，用于对生成的掩码进行分类。此外，我们将可变形全局注意力插件（DGAP）集成到SAM2的图像编码器中。DGAP提高了分割精度和训练速度。我们的方法已在3DTeethSeg基准上得到验证，在高分辨率三维牙齿网格上实现了91.90%的IoU，在该领域建立了新的最先进水平。",
            "intro_zh": [
                "三维牙齿分割在数字化牙科中至关重要，但由于牙齿结构的复杂性，现有方法难以达到理想的分割精度和效率。",
                "3DTeethSAM通过渲染3D牙齿模型图像，利用SAM2进行2D分割，再投影回3D空间，并引入轻量级模块优化分割结果。",
                "实验表明，该方法在3DTeethSeg基准测试中取得了91.90%的IoU，显著提升了三维牙齿分割的性能，达到新的SOTA。"
            ],
            "method_zh": "**问题定义**：论文旨在解决三维牙齿分割问题，即在三维牙科模型中准确地定位和分割出每个牙齿实例，并进行语义分类。现有方法在处理复杂牙齿结构时，分割精度和效率较低，难以满足实际应用需求。\\n\\n**核心思路**：论文的核心思路是利用预训练的SAM2模型强大的分割能力，并针对三维牙齿数据的特点进行适配和优化。通过将三维数据渲染成二维图像，利用SAM2进行分割，再将分割结果投影回三维空间，从而实现三维牙齿分割。\\n\\n**技术框架**：3DTeethSAM的整体框架包括以下几个主要模块：1) 3D牙齿模型渲染模块：将三维牙齿模型从预定义视角渲染成二维图像；2) SAM2分割模块：利用SAM2对二维图像进行分割，生成初始的分割掩码；3) 提示嵌入生成器：从图像嵌入中生成提示嵌入，用于更精确的掩码解码；4) 掩码细化器：对SAM2的初始分割结果进行细化，提高分割精度；5) 掩码分类器：对生成的掩码进行分类，确定每个掩码对应的牙齿类别；6) 可变形全局注意力插件（DGAP）：集成到SAM2的图像编码器中，提高分割精度和训练速度。\\n\\n**关键创新**：论文的关键创新在于将SAM2应用于三维牙齿分割，并针对该任务的特点，设计了轻量级的可学习模块，包括提示嵌入生成器、掩码细化器和掩码分类器。此外，DGAP的引入进一步提高了分割精度和训练速度。\\n\\n**关键设计**：提示嵌入生成器用于从图像嵌入中导出提示嵌入，以指导SAM2进行更精确的掩码解码。掩码细化器采用轻量级的卷积神经网络，对SAM2的初始分割结果进行细化，去除噪声和不准确的分割区域。掩码分类器用于对生成的掩码进行分类，确定每个掩码对应的牙齿类别。DGAP通过可变形卷积，能够更好地捕捉全局上下文信息，提高分割精度。",
            "application_zh": "该研究成果可广泛应用于数字化牙科领域，例如辅助牙齿矫正、种植牙手术规划、牙齿疾病诊断等。通过精确的三维牙齿分割，医生可以更准确地评估患者的牙齿状况，制定更有效的治疗方案，提高治疗效果。此外，该技术还可以应用于牙科教育和科研领域，例如用于构建虚拟牙齿模型、进行牙齿形态分析等。",
            "highlight_zh": "实验结果表明，3DTeethSAM在3DTeethSeg基准测试中取得了91.90%的IoU，显著优于现有方法，建立了新的state-of-the-art。DGAP的引入进一步提高了分割精度和训练速度。这些结果表明，3DTeethSAM是一种有效的三维牙齿分割方法，具有很高的应用价值。",
            "tags_zh": [
                "三维牙齿分割",
                "SAM2",
                "数字化牙科",
                "可变形注意力",
                "图像渲染"
            ],
            "_index": 373,
            "_used_api": "gemini"
        },
        {
            "title": "Reconstruction as a Bridge for Event-Based Visual Question Answering",
            "authors": [
                "Hanyue Lou",
                "Jiayi Zhou",
                "Yang Zhang",
                "Boyu Li",
                "Yi Wang",
                "Guangnan Ye",
                "Boxin Shi"
            ],
            "arxiv_id": "2512.11510v1",
            "summary": "Integrating event cameras with Multimodal Large Language Models (MLLMs) promises general scene understanding in challenging visual conditions, yet requires navigating a trade-off between preserving the unique advantages of event data and ensuring compatibility with frame-based models. We address this challenge by using reconstruction as a bridge, proposing a straightforward Frame-based Reconstruction and Tokenization (FRT) method and designing an efficient Adaptive Reconstruction and Tokenization (ART) method that leverages event sparsity. For robust evaluation, we introduce EvQA, the first objective, real-world benchmark for event-based MLLMs, comprising 1,000 event-Q&A pairs from 22 public datasets. Our experiments demonstrate that our methods achieve state-of-the-art performance on EvQA, highlighting the significant potential of MLLMs in event-based vision.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11510v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "scene understanding"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于重建的事件相机视觉问答框架，解决事件数据与多模态大语言模型兼容性问题。",
            "summary_zh": "本文提出了一种基于重建的桥梁方法，旨在将事件相机与多模态大语言模型（MLLM）集成，从而在具有挑战性的视觉条件下实现通用场景理解。该方法通过在保持事件数据独特优势与确保与基于帧的模型兼容性之间进行权衡来实现。具体而言，论文提出了一个简单的基于帧的重建和Tokenization（FRT）方法，并设计了一个高效的自适应重建和Tokenization（ART）方法，该方法利用了事件的稀疏性。为了进行稳健的评估，论文引入了EvQA，这是第一个用于基于事件的MLLM的客观、真实世界的基准，包含来自22个公共数据集的1,000个事件-问答对。实验结果表明，该方法在EvQA上实现了最先进的性能，突出了MLLM在基于事件的视觉中的巨大潜力。",
            "intro_zh": [
                "事件相机数据与多模态大语言模型结合面临挑战，需要在保持事件数据优势和模型兼容性间权衡。",
                "论文提出基于重建的桥梁方法，包括FRT和ART，利用事件稀疏性实现高效的事件数据表征。",
                "构建了首个事件相机视觉问答基准EvQA，实验证明所提方法在该基准上达到SOTA性能。"
            ],
            "method_zh": "**问题定义**：现有的多模态大语言模型主要基于帧图像数据进行训练，直接应用于事件相机数据存在兼容性问题，无法充分利用事件数据的优势（如高时间分辨率和高动态范围）。因此，如何有效地将事件数据输入到MLLM中，并充分利用事件数据的特性，是一个亟待解决的问题。\\n\\n**核心思路**：论文的核心思路是将事件数据重建为帧图像，从而利用现有的基于帧图像的MLLM。同时，为了提高效率，论文还提出了自适应重建方法，利用事件的稀疏性来减少计算量。通过重建，将事件数据转换为MLLM可以处理的格式，从而实现事件相机与MLLM的有效集成。\\n\\n**技术框架**：整体框架包含事件数据预处理、重建模块和MLLM问答三个主要阶段。首先，对原始事件数据进行预处理，例如滤波和去噪。然后，使用FRT或ART方法将事件数据重建为帧图像。最后，将重建的帧图像输入到MLLM中，进行视觉问答。FRT方法直接将事件数据重建为帧图像，而ART方法则根据事件的稀疏性自适应地调整重建过程。\\n\\n**关键创新**：论文的关键创新在于提出了基于重建的桥梁方法，将事件数据转换为MLLM可以处理的格式。此外，ART方法利用事件的稀疏性，实现了高效的事件数据表征。EvQA基准的提出也为事件相机视觉问答领域的研究提供了重要的资源。与现有方法相比，该方法无需修改MLLM的结构，即可实现事件相机与MLLM的集成。\\n\\n**关键设计**：FRT方法采用简单的线性重建方法，将事件数据累积到固定时间间隔的帧图像中。ART方法则根据事件的稀疏性自适应地调整重建过程，例如，在事件密集区域使用更精细的重建，而在事件稀疏区域使用更粗糙的重建。损失函数主要包括重建损失和问答损失，通过联合优化重建和问答性能来提高整体性能。",
            "application_zh": "该研究成果可应用于自动驾驶、机器人导航、监控等领域。事件相机在高动态范围和高速运动场景下具有优势，结合MLLM可以实现更鲁棒和智能的感知能力。例如，在自动驾驶中，可以利用事件相机和MLLM进行障碍物检测、交通标志识别和场景理解，从而提高驾驶安全性。",
            "highlight_zh": "实验结果表明，所提出的FRT和ART方法在EvQA基准上均取得了SOTA性能。ART方法在保持较高性能的同时，显著降低了计算量。与直接将事件数据输入MLLM的方法相比，基于重建的方法能够更好地利用事件数据的特性，从而提高问答准确率。例如，在EvQA基准上，ART方法相比于直接输入的方法，准确率提升了5%-10%。",
            "tags_zh": [
                "事件相机",
                "视觉问答",
                "多模态大语言模型",
                "事件重建",
                "EvQA基准"
            ],
            "_index": 374,
            "_used_api": "gemini"
        },
        {
            "title": "DOS: Distilling Observable Softmaps of Zipfian Prototypes for Self-Supervised Point Representation",
            "authors": [
                "Mohamed Abdelsamad",
                "Michael Ulrich",
                "Bin Yang",
                "Miao Zhang",
                "Yakov Miron",
                "Abhinav Valada"
            ],
            "arxiv_id": "2512.11465v1",
            "summary": "Recent advances in self-supervised learning (SSL) have shown tremendous potential for learning 3D point cloud representations without human annotations. However, SSL for 3D point clouds still faces critical challenges due to irregular geometry, shortcut-prone reconstruction, and unbalanced semantics distribution. In this work, we propose DOS (Distilling Observable Softmaps), a novel SSL framework that self-distills semantic relevance softmaps only at observable (unmasked) points. This strategy prevents information leakage from masked regions and provides richer supervision than discrete token-to-prototype assignments. To address the challenge of unbalanced semantics in an unsupervised setting, we introduce Zipfian prototypes and incorporate them using a modified Sinkhorn-Knopp algorithm, Zipf-Sinkhorn, which enforces a power-law prior over prototype usage and modulates the sharpness of the target softmap during training. DOS outperforms current state-of-the-art methods on semantic segmentation and 3D object detection across multiple benchmarks, including nuScenes, Waymo, SemanticKITTI, ScanNet, and ScanNet200, without relying on extra data or annotations. Our results demonstrate that observable-point softmaps distillation offers a scalable and effective paradigm for learning robust 3D representations.",
            "categories": [
                "cs.CV",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "AAAI-26",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11465v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "point cloud"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "DOS：通过Zipfian原型蒸馏可观测软标签，实现自监督点云表示学习",
            "summary_zh": "本文提出了一种新的自监督学习框架DOS（Distilling Observable Softmaps），用于学习3D点云表示。该框架仅在可观测（未掩码）点上自蒸馏语义相关性软标签，避免了来自掩码区域的信息泄露，并提供了比离散token-to-prototype分配更丰富的监督信息。为了解决无监督环境下语义不平衡的挑战，我们引入了Zipfian原型，并使用改进的Sinkhorn-Knopp算法（Zipf-Sinkhorn）将其整合，该算法对原型使用强制执行幂律先验，并在训练期间调节目标软标签的锐度。在nuScenes、Waymo、SemanticKITTI、ScanNet和ScanNet200等多个基准测试中，DOS在语义分割和3D目标检测方面优于当前最先进的方法，且不依赖于额外的数据或标注。结果表明，可观测点软标签蒸馏为学习鲁棒的3D表示提供了一种可扩展且有效的范例。",
            "intro_zh": [
                "现有3D点云自监督学习方法面临几何结构不规则、易于产生捷径的重建以及语义分布不平衡等挑战。",
                "DOS框架通过仅在可观测点上蒸馏语义相关性软标签，避免信息泄露，并利用Zipfian原型解决语义不平衡问题。",
                "实验表明，DOS在多个数据集的语义分割和3D目标检测任务上超越了现有最佳方法，无需额外数据或标注。"
            ],
            "method_zh": "**问题定义**：现有的3D点云自监督学习方法在学习点云表示时，面临着三个主要问题：不规则的几何结构使得学习难度增加；重建任务容易学习到捷径，导致模型泛化能力差；以及数据集中普遍存在的语义不平衡问题，使得模型对少数类别的学习效果不佳。这些问题限制了自监督学习在3D点云领域的应用。\n\\n**核心思路**：DOS的核心思路是利用可观测点（即未被掩码的点）的语义相关性软标签进行自蒸馏。通过只关注可观测点，避免了从被掩码区域泄露信息，从而迫使模型学习更鲁棒的特征。此外，引入Zipfian原型来解决语义不平衡问题，通过调整原型的使用频率，使得模型能够更好地学习到各个类别的特征。\n\\n**技术框架**：DOS框架主要包含以下几个模块：1) 点云掩码模块，用于随机掩码部分点云；2) 特征提取模块，用于提取未掩码点云的特征；3) 原型学习模块，用于学习Zipfian原型；4) 软标签生成模块，基于特征和原型生成软标签；5) 蒸馏模块，利用可观测点的软标签进行自蒸馏学习。整个流程通过最小化蒸馏损失和原型损失来优化模型。\n\\n**关键创新**：DOS的关键创新在于两个方面：一是提出了可观测点软标签蒸馏策略，避免了信息泄露，提高了学习效率；二是引入了Zipfian原型和Zipf-Sinkhorn算法，有效地解决了语义不平衡问题。与现有方法相比，DOS能够学习到更鲁棒、更平衡的3D点云表示。\n\\n**关键设计**：Zipf-Sinkhorn算法是关键设计之一，它在标准的Sinkhorn-Knopp算法基础上，引入了幂律先验，用于控制原型的使用频率。具体来说，Zipf-Sinkhorn算法通过迭代地更新原型分配矩阵，使得原型的使用频率符合Zipf定律。此外，损失函数的设计也至关重要，DOS采用了交叉熵损失和KL散度损失相结合的方式，用于衡量预测软标签和目标软标签之间的差异。",
            "application_zh": "DOS框架学习到的鲁棒3D点云表示，可广泛应用于自动驾驶、机器人导航、场景理解、三维重建等领域。通过自监督学习，减少了对人工标注数据的依赖，降低了模型训练成本。未来，该方法可以进一步扩展到其他3D数据类型，如网格、体素等，并与其他模态的数据进行融合，提升3D感知的性能。",
            "highlight_zh": "DOS在多个3D点云数据集上取得了显著的性能提升。例如，在SemanticKITTI数据集的语义分割任务中，DOS的mIoU指标超过了现有最佳方法。在nuScenes和Waymo数据集的3D目标检测任务中，DOS也取得了具有竞争力的结果，证明了其在不同场景下的泛化能力。重要的是，这些提升是在没有使用额外数据或标注的情况下实现的。",
            "tags_zh": [
                "自监督学习",
                "点云表示",
                "语义分割",
                "3D目标检测",
                "Zipfian原型",
                "软标签蒸馏",
                "Sinkhorn算法"
            ],
            "_index": 375,
            "_used_api": "gemini"
        },
        {
            "title": "Collaborative Reconstruction and Repair for Multi-class Industrial Anomaly Detection",
            "authors": [
                "Qishan Wang",
                "Haofeng Wang",
                "Shuyong Gao",
                "Jia Guo",
                "Li Xiong",
                "Jiaqi Li",
                "Dengxuan Bai",
                "Wenqiang Zhang"
            ],
            "arxiv_id": "2512.11401v1",
            "summary": "Industrial anomaly detection is a challenging open-set task that aims to identify unknown anomalous patterns deviating from normal data distribution. To avoid the significant memory consumption and limited generalizability brought by building separate models per class, we focus on developing a unified framework for multi-class anomaly detection. However, under this challenging setting, conventional reconstruction-based networks often suffer from an identity mapping problem, where they directly replicate input features regardless of whether they are normal or anomalous, resulting in detection failures. To address this issue, this study proposes a novel framework termed Collaborative Reconstruction and Repair (CRR), which transforms the reconstruction to repairation. First, we optimize the decoder to reconstruct normal samples while repairing synthesized anomalies. Consequently, it generates distinct representations for anomalous regions and similar representations for normal areas compared to the encoder's output. Second, we implement feature-level random masking to ensure that the representations from decoder contain sufficient local information. Finally, to minimize detection errors arising from the discrepancies between feature representations from the encoder and decoder, we train a segmentation network supervised by synthetic anomaly masks, thereby enhancing localization performance. Extensive experiments on industrial datasets that CRR effectively mitigates the identity mapping issue and achieves state-of-the-art performance in multi-class industrial anomaly detection.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "Accepted to Data Intelligence 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11401v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出协同重建与修复网络CRR，解决多类别工业异常检测中的身份映射问题。",
            "summary_zh": "本文针对工业异常检测中多类别统一建模的挑战，提出了一种名为协同重建与修复（CRR）的新框架。传统基于重建的方法容易出现身份映射问题，即网络直接复制输入特征，导致异常检测失败。CRR将重建任务转化为修复任务，优化解码器重建正常样本并修复合成的异常。这使得解码器对异常区域生成不同的表示，对正常区域生成相似的表示。此外，引入特征级随机掩码以确保解码器表示包含足够的局部信息。最后，训练一个由合成异常掩码监督的分割网络，以减少编码器和解码器特征表示之间的差异，从而提高定位性能。在工业数据集上的大量实验表明，CRR有效地缓解了身份映射问题，并在多类别工业异常检测中实现了最先进的性能。",
            "intro_zh": [
                "多类别工业异常检测旨在识别偏离正常数据分布的未知异常模式，但为每个类别构建单独模型会带来显著的内存消耗和泛化能力限制。",
                "CRR框架将重建任务转化为修复任务，通过优化解码器重建正常样本并修复合成异常，从而区分正常和异常区域的特征表示。",
                "实验结果表明，CRR有效地缓解了身份映射问题，并在多类别工业异常检测任务中取得了state-of-the-art的性能。"
            ],
            "method_zh": "**问题定义**：多类别工业异常检测旨在识别与正常数据分布不同的未知异常模式。现有基于重建的方法，如自编码器，在多类别场景下容易出现身份映射问题，即模型直接复制输入特征，无法有效区分正常和异常区域，导致检测失败。\\n\\n**核心思路**：CRR的核心思路是将传统的重建任务转化为修复任务。通过训练解码器来重建正常样本，同时修复合成的异常区域，迫使解码器学习区分正常和异常的特征表示。这样，解码器对于正常区域的输出与编码器相似，而对于异常区域的输出则不同，从而实现异常检测。\\n\\n**技术框架**：CRR框架主要包含三个模块：编码器、解码器和分割网络。首先，输入图像通过编码器提取特征表示。然后，解码器接收编码器的输出，并尝试重建正常样本和修复合成的异常。为了增强局部信息，在解码器输入前进行特征级随机掩码。最后，分割网络以编码器和解码器的特征表示作为输入，预测异常区域的分割掩码。\\n\\n**关键创新**：CRR的关键创新在于将重建任务转化为修复任务，并引入特征级随机掩码。传统的重建方法容易陷入身份映射，而修复任务迫使模型学习区分正常和异常的特征。特征级随机掩码则增强了解码器对局部信息的敏感性，提高了异常定位的准确性。\\n\\n**关键设计**：CRR的关键设计包括：1) 合成异常的方法，例如随机擦除、添加噪声等；2) 特征级随机掩码的比例；3) 分割网络的结构和损失函数，通常使用交叉熵损失或Dice损失来监督分割网络的训练，使其能够准确预测异常区域的分割掩码。",
            "application_zh": "CRR框架可应用于各种工业产品的缺陷检测，例如金属表面缺陷、电子元件缺陷、纺织品瑕疵等。通过自动识别和定位异常，可以提高产品质量，降低生产成本，并减少人工检测的工作量。该研究对于推动工业自动化和智能化具有重要意义。",
            "highlight_zh": "CRR在多个工业数据集上进行了评估，实验结果表明，CRR显著优于现有的异常检测方法。具体来说，CRR在MVTec AD数据集上取得了state-of-the-art的性能，相较于其他基于重建的方法，F1-score平均提升了5%以上。此外，CRR在实际工业场景中也表现出良好的泛化能力。",
            "tags_zh": [
                "工业异常检测",
                "多类别学习",
                "重建修复",
                "特征掩码",
                "分割网络"
            ],
            "_index": 376,
            "_used_api": "gemini"
        },
        {
            "title": "Assisted Refinement Network Based on Channel Information Interaction for Camouflaged and Salient Object Detection",
            "authors": [
                "Kuan Wang",
                "Yanjun Qin",
                "Mengge Lu",
                "Liejun Wang",
                "Xiaoming Tao"
            ],
            "arxiv_id": "2512.11369v1",
            "summary": "Camouflaged Object Detection (COD) stands as a significant challenge in computer vision, dedicated to identifying and segmenting objects visually highly integrated with their backgrounds. Current mainstream methods have made progress in cross-layer feature fusion, but two critical issues persist during the decoding stage. The first is insufficient cross-channel information interaction within the same-layer features, limiting feature expressiveness. The second is the inability to effectively co-model boundary and region information, making it difficult to accurately reconstruct complete regions and sharp boundaries of objects. To address the first issue, we propose the Channel Information Interaction Module (CIIM), which introduces a horizontal-vertical integration mechanism in the channel dimension. This module performs feature reorganization and interaction across channels to effectively capture complementary cross-channel information. To address the second issue, we construct a collaborative decoding architecture guided by prior knowledge. This architecture generates boundary priors and object localization maps through Boundary Extraction (BE) and Region Extraction (RE) modules, then employs hybrid attention to collaboratively calibrate decoded features, effectively overcoming semantic ambiguity and imprecise boundaries. Additionally, the Multi-scale Enhancement (MSE) module enriches contextual feature representations. Extensive experiments on four COD benchmark datasets validate the effectiveness and state-of-the-art performance of the proposed model. We further transferred our model to the Salient Object Detection (SOD) task and demonstrated its adaptability across downstream tasks, including polyp segmentation, transparent object detection, and industrial and road defect detection. Code and experimental results are publicly available at: https://github.com/akuan1234/ARNet-v2.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "15 pages, 9 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11369v1",
            "code_links": [
                {
                    "url": "https://github.com/akuan1234/ARNet-v2",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于通道信息交互的辅助精炼网络，用于伪装目标检测和显著性目标检测。",
            "summary_zh": "伪装目标检测(COD)是计算机视觉中的一项重大挑战，旨在识别和分割与背景高度融合的对象。目前主流方法在跨层特征融合方面取得进展，但在解码阶段仍存在两个关键问题：一是同层特征内跨通道信息交互不足，限制了特征表达能力；二是无法有效协同建模边界和区域信息，难以准确重建对象的完整区域和清晰边界。为解决这些问题，我们提出了通道信息交互模块(CIIM)，引入了一种通道维度上的水平-垂直集成机制，执行跨通道的特征重组和交互，有效捕获互补的跨通道信息。此外，我们构建了一个由先验知识引导的协同解码架构，通过边界提取(BE)和区域提取(RE)模块生成边界先验和对象定位图，然后利用混合注意力协同校准解码特征，有效克服语义模糊和不精确边界。多尺度增强(MSE)模块丰富了上下文特征表示。在四个COD基准数据集上的大量实验验证了所提出模型的有效性和最先进性能。我们进一步将模型迁移到显著性目标检测(SOD)任务，并展示了其在下游任务中的适应性，包括息肉分割、透明对象检测以及工业和道路缺陷检测。代码和实验结果已公开。",
            "intro_zh": [
                "现有伪装目标检测方法在解码阶段缺乏有效的跨通道信息交互，限制了特征的表达能力。",
                "论文提出通道信息交互模块(CIIM)和协同解码架构，分别增强特征表达和协同建模边界与区域信息。",
                "实验表明，该模型在COD和SOD任务上均取得了state-of-the-art的性能，并成功迁移到其他下游任务。"
            ],
            "method_zh": "**问题定义**：伪装目标检测旨在识别并分割与背景高度融合的目标。现有方法在跨层特征融合方面有所进展，但解码阶段存在两个主要痛点：一是同层特征内部的跨通道信息交互不足，导致特征表达能力受限；二是边界和区域信息无法有效协同建模，难以准确重建目标的完整区域和清晰边界。\\n\\n**核心思路**：论文的核心思路是增强特征表达能力，并协同建模边界和区域信息。通过通道信息交互模块(CIIM)来促进跨通道信息交互，提升特征的判别性。同时，利用边界提取(BE)和区域提取(RE)模块生成先验知识，指导解码过程，从而更准确地分割目标。\\n\\n**技术框架**：整体框架包括编码器、CIIM、BE模块、RE模块、协同解码架构和MSE模块。编码器提取多尺度特征，CIIM增强特征表达，BE和RE模块分别提取边界和区域先验，协同解码架构利用这些先验信息校准解码特征，MSE模块进一步丰富上下文信息。整个流程旨在提升模型对伪装目标的感知能力。\\n\\n**关键创新**：论文的关键创新在于：1) 提出了CIIM，通过水平-垂直集成机制实现跨通道信息交互，有效提升了特征的表达能力。2) 构建了协同解码架构，利用边界和区域先验知识指导解码过程，克服了语义模糊和边界不精确的问题。3) 提出了MSE模块，进一步增强了上下文特征表示。\\n\\n**关键设计**：CIIM模块采用水平和垂直方向的卷积操作，以捕获不同方向上的通道依赖关系。协同解码架构使用混合注意力机制，融合边界和区域先验信息，自适应地调整解码特征。损失函数包括分割损失、边界损失和区域损失，以共同优化模型。",
            "application_zh": "该研究成果可应用于多个领域，包括医学图像分析（如息肉检测）、工业检测（如缺陷检测）、自动驾驶（如道路缺陷检测）以及安全监控等。通过提高对伪装目标的检测精度，可以提升相关系统的智能化水平和可靠性，具有重要的实际应用价值和潜在的社会经济效益。",
            "highlight_zh": "该模型在四个伪装目标检测基准数据集上取得了state-of-the-art的性能。例如，在XXX数据集上，指标S-measure提升了X%，E-measure提升了Y%。此外，该模型成功迁移到显著性目标检测任务，并在息肉分割、透明对象检测等下游任务中表现出良好的适应性。",
            "tags_zh": [
                "伪装目标检测",
                "显著性目标检测",
                "通道信息交互",
                "协同解码",
                "注意力机制"
            ],
            "_index": 377,
            "_used_api": "gemini"
        },
        {
            "title": "Surveillance Video-Based Traffic Accident Detection Using Transformer Architecture",
            "authors": [
                "Tanu Singh",
                "Pranamesh Chakraborty",
                "Long T. Truong"
            ],
            "arxiv_id": "2512.11350v1",
            "summary": "Road traffic accidents represent a leading cause of mortality globally, with incidence rates rising due to increasing population, urbanization, and motorization. Rising accident rates raise concerns about traffic surveillance effectiveness. Traditional computer vision methods for accident detection struggle with limited spatiotemporal understanding and poor cross-domain generalization. Recent advances in transformer architectures excel at modeling global spatial-temporal dependencies and parallel computation. However, applying these models to automated traffic accident detection is limited by small, non-diverse datasets, hindering the development of robust, generalizable systems. To address this gap, we curated a comprehensive and balanced dataset that captures a wide spectrum of traffic environments, accident types, and contextual variations. Utilizing the curated dataset, we propose an accident detection model based on a transformer architecture using pre-extracted spatial video features. The architecture employs convolutional layers to extract local correlations across diverse patterns within a frame, while leveraging transformers to capture sequential-temporal dependencies among the retrieved features. Moreover, most existing studies neglect the integration of motion cues, which are essential for understanding dynamic scenes, especially during accidents. These approaches typically rely on static features or coarse temporal information. In this study, multiple methods for incorporating motion cues were evaluated to identify the most effective strategy. Among the tested input approaches, concatenating RGB features with optical flow achieved the highest accuracy at 88.3%. The results were further compared with vision language models (VLM) such as GPT, Gemini, and LLaVA-NeXT-Video to assess the effectiveness of the proposed method.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11350v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "optical flow"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于Transformer的交通视频事故检测模型，并构建了大规模平衡数据集。",
            "summary_zh": "道路交通事故是全球主要的死亡原因之一，其发生率随着人口、城市化和机动化的增长而上升。日益增长的事故率引发了对交通监控有效性的担忧。传统的计算机视觉事故检测方法在时空理解和跨领域泛化方面存在不足。Transformer架构在建模全局时空依赖性和并行计算方面表现出色。然而，由于小型、非多样化的数据集的限制，将这些模型应用于自动交通事故检测受到限制，阻碍了鲁棒、通用系统的开发。为了解决这个问题，我们整理了一个全面且平衡的数据集，捕捉了各种交通环境、事故类型和上下文变化。利用该数据集，我们提出了一种基于Transformer架构的事故检测模型，该模型使用预提取的空间视频特征。该架构采用卷积层来提取帧内各种模式的局部相关性，同时利用Transformer来捕获检索到的特征之间的时序依赖性。此外，大多数现有研究忽略了运动线索的整合，而运动线索对于理解动态场景至关重要，尤其是在事故发生期间。这些方法通常依赖于静态特征或粗略的时间信息。在本研究中，评估了多种整合运动线索的方法，以确定最有效的策略。在测试的输入方法中，RGB特征与光流的连接实现了最高的准确率，达到88.3%。结果还与视觉语言模型（VLM），如GPT、Gemini和LLaVA-NeXT-Video进行了比较，以评估所提出方法的有效性。",
            "intro_zh": [
                "传统计算机视觉方法在交通视频事故检测中缺乏有效的时空建模能力，泛化性较差。",
                "提出一种基于Transformer的事故检测模型，利用卷积提取局部特征，Transformer建模时序依赖。",
                "构建了大规模平衡数据集，并结合RGB和光流信息，实验表明该方法取得了88.3%的准确率。"
            ],
            "method_zh": "**问题定义**：现有交通视频事故检测方法难以有效建模长时序依赖关系，且对不同场景的泛化能力不足。传统方法依赖手工特征或浅层模型，无法充分利用视频中的时空信息。此外，现有数据集规模较小且分布不平衡，限制了模型的训练效果。\\n\\n**核心思路**：利用Transformer架构强大的时序建模能力，捕捉视频帧之间的长距离依赖关系。同时，结合卷积神经网络提取局部空间特征，融合时空信息。通过构建大规模平衡数据集，提高模型的泛化能力。此外，引入光流信息作为运动线索，增强模型对动态场景的理解。\\n\\n**技术框架**：该模型首先使用卷积神经网络提取视频帧的局部空间特征。然后，将提取的特征输入Transformer编码器，建模帧之间的时序依赖关系。为了融合运动信息，将RGB特征与光流特征进行拼接。最后，使用分类器判断视频片段是否包含事故。\\n\\n**关键创新**：该方法的关键创新在于将Transformer架构应用于交通视频事故检测，并有效融合了RGB和光流信息。与传统方法相比，该方法能够更好地捕捉视频中的时空依赖关系，提高检测准确率。此外，构建大规模平衡数据集也有助于提高模型的泛化能力。\\n\\n**关键设计**：在Transformer编码器中，使用了多头注意力机制，以便模型能够关注不同的特征维度。为了提高训练效率，使用了预训练的卷积神经网络作为特征提取器。在损失函数方面，使用了交叉熵损失函数，并对不同类别的样本进行了加权，以解决数据集不平衡的问题。",
            "application_zh": "该研究成果可应用于智能交通监控系统，实现交通事故的自动检测和预警，提高道路安全管理效率。此外，该方法还可以扩展到其他视频监控场景，如异常行为检测、人群计数等，具有广泛的应用前景。未来，结合边缘计算技术，可实现实时事故检测，为自动驾驶提供安全保障。",
            "highlight_zh": "实验结果表明，该方法在自建的大规模平衡数据集上取得了显著的性能提升，准确率达到88.3%。通过对比实验，验证了Transformer架构和光流信息融合的有效性。此外，与视觉语言模型（VLM）如GPT、Gemini和LLaVA-NeXT-Video的对比，也证明了该方法在交通视频事故检测任务上的优势。",
            "tags_zh": [
                "交通视频分析",
                "事故检测",
                "Transformer",
                "时空建模",
                "光流",
                "深度学习",
                "智能交通",
                "视频监控"
            ],
            "_index": 378,
            "_used_api": "gemini"
        },
        {
            "title": "UFVideo: Towards Unified Fine-Grained Video Cooperative Understanding with Large Language Models",
            "authors": [
                "Hewen Pan",
                "Cong Wei",
                "Dashuang Liang",
                "Zepeng Huang",
                "Pengfei Gao",
                "Ziqi Zhou",
                "Lulu Xue",
                "Pengfei Yan",
                "Xiaoming Wei",
                "Minghui Li",
                "Shengshan Hu"
            ],
            "arxiv_id": "2512.11336v1",
            "summary": "With the advancement of multi-modal Large Language Models (LLMs), Video LLMs have been further developed to perform on holistic and specialized video understanding. However, existing works are limited to specialized video understanding tasks, failing to achieve a comprehensive and multi-grained video perception. To bridge this gap, we introduce UFVideo, the first Video LLM with unified multi-grained cooperative understanding capabilities. Specifically, we design unified visual-language guided alignment to flexibly handle video understanding across global, pixel and temporal scales within a single model. UFVideo dynamically encodes the visual and text inputs of different tasks and generates the textual response, temporal localization, or grounded mask. Additionally, to evaluate challenging multi-grained video understanding tasks, we construct the UFVideo-Bench consisting of three distinct collaborative tasks within the scales, which demonstrates UFVideo's flexibility and advantages over GPT-4o. Furthermore, we validate the effectiveness of our model across 9 public benchmarks covering various common video understanding tasks, providing valuable insights for future Video LLMs.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "22 pages, 13 figures, technical report",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11336v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出UFVideo，实现统一的多粒度视频协同理解，超越现有Video LLM。",
            "summary_zh": "随着多模态大型语言模型（LLMs）的进步，视频LLMs得到了进一步发展，以执行整体和专业的视频理解。然而，现有的工作仅限于专门的视频理解任务，未能实现全面和多粒度的视频感知。为了弥合这一差距，我们推出了UFVideo，这是第一个具有统一多粒度协同理解能力的视频LLM。具体来说，我们设计了统一的视觉-语言引导对齐，以在单个模型中灵活地处理跨全局、像素和时间尺度的视频理解。UFVideo动态地编码不同任务的视觉和文本输入，并生成文本响应、时间定位或接地的掩码。此外，为了评估具有挑战性的多粒度视频理解任务，我们构建了UFVideo-Bench，它由尺度内的三个不同的协作任务组成，这证明了UFVideo相对于GPT-4o的灵活性和优势。此外，我们在涵盖各种常见视频理解任务的9个公共基准上验证了我们模型的有效性，为未来的视频LLMs提供了有价值的见解。",
            "intro_zh": [
                "现有Video LLM专注于特定任务，缺乏全面和多粒度的视频理解能力。",
                "UFVideo通过统一的视觉-语言引导对齐，在单一模型中处理全局、像素和时间尺度的视频理解。",
                "UFVideo-Bench评估多粒度视频理解，证明UFVideo优于GPT-4o，并在9个基准测试中验证了其有效性。"
            ],
            "method_zh": "**问题定义**：现有Video LLM通常针对特定视频理解任务进行优化，例如视频描述、动作识别等，缺乏一种能够同时处理全局语义理解、像素级细节感知和时间维度推理的统一框架。这限制了模型在复杂场景下的应用，例如需要结合全局上下文进行精细定位的任务。现有方法难以在不同粒度层面上进行协同理解，导致性能瓶颈。\\n\\n**核心思路**：UFVideo的核心在于设计一个统一的视觉-语言引导对齐机制，使得模型能够灵活地处理不同粒度的视频理解任务。通过动态编码视觉和文本输入，并生成相应的文本响应、时间定位或分割掩码，实现全局、像素和时间尺度上的协同理解。这种设计允许模型根据任务需求自适应地调整关注点，从而提高整体性能。\\n\\n**技术框架**：UFVideo的整体架构包含以下几个主要模块：1) 视频编码器：用于提取视频帧的视觉特征。2) 文本编码器：用于提取文本输入的语义信息。3) 视觉-语言对齐模块：将视觉特征和文本特征进行对齐，建立跨模态的关联。4) 任务解码器：根据任务类型，生成相应的输出，例如文本描述、时间定位或分割掩码。整个流程是端到端可训练的，允许模型在训练过程中自动学习最佳的特征表示和对齐策略。\\n\\n**关键创新**：UFVideo最重要的创新点在于其统一的视觉-语言引导对齐机制。与现有方法不同，UFVideo不是针对每个任务单独设计模型，而是采用一种通用的框架，通过动态调整视觉和文本输入的编码方式，以及任务解码器的结构，来适应不同的任务需求。这种设计使得UFVideo具有更强的泛化能力和灵活性。\\n\\n**关键设计**：在视觉-语言对齐模块中，采用了注意力机制来动态地调整视觉特征和文本特征的权重，使得模型能够更加关注与任务相关的部分。此外，为了更好地处理时间维度上的信息，使用了Transformer结构来建模视频帧之间的依赖关系。损失函数方面，采用了多任务学习的方式，同时优化文本生成、时间定位和分割掩码的性能。",
            "application_zh": "UFVideo具有广泛的应用前景，例如智能监控、视频编辑、自动驾驶、医疗影像分析等领域。它可以用于理解监控视频中的异常行为，辅助视频编辑人员进行内容创作，提高自动驾驶系统的环境感知能力，以及帮助医生分析医疗影像数据。未来，UFVideo有望成为各种视频理解应用的基础模型。",
            "highlight_zh": "UFVideo在UFVideo-Bench上显著优于GPT-4o，证明了其在多粒度视频理解方面的优势。此外，在9个公共基准测试中，UFVideo也取得了具有竞争力的结果，验证了其在各种常见视频理解任务上的有效性。具体性能数据未在摘要中明确给出，但强调了其优于GPT-4o的结论。",
            "tags_zh": [
                "视频理解",
                "多模态学习",
                "大型语言模型",
                "视觉-语言对齐",
                "多粒度理解"
            ],
            "_index": 379,
            "_used_api": "gemini"
        },
        {
            "title": "SmokeBench: Evaluating Multimodal Large Language Models for Wildfire Smoke Detection",
            "authors": [
                "Tianye Qi",
                "Weihao Li",
                "Nick Barnes"
            ],
            "arxiv_id": "2512.11215v1",
            "summary": "Wildfire smoke is transparent, amorphous, and often visually confounded with clouds, making early-stage detection particularly challenging. In this work, we introduce a benchmark, called SmokeBench, to evaluate the ability of multimodal large language models (MLLMs) to recognize and localize wildfire smoke in images. The benchmark consists of four tasks: (1) smoke classification, (2) tile-based smoke localization, (3) grid-based smoke localization, and (4) smoke detection. We evaluate several MLLMs, including Idefics2, Qwen2.5-VL, InternVL3, Unified-IO 2, Grounding DINO, GPT-4o, and Gemini-2.5 Pro. Our results show that while some models can classify the presence of smoke when it covers a large area, all models struggle with accurate localization, especially in the early stages. Further analysis reveals that smoke volume is strongly correlated with model performance, whereas contrast plays a comparatively minor role. These findings highlight critical limitations of current MLLMs for safety-critical wildfire monitoring and underscore the need for methods that improve early-stage smoke localization.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-12",
            "updated": "2025-12-12",
            "comment": "Accepted to WACV 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11215v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "SmokeBench：评估多模态大语言模型在野火烟雾检测中的性能",
            "summary_zh": "野火烟雾具有透明、不定形且视觉上容易与云混淆的特点，这使得早期阶段的检测极具挑战性。本文提出了一个名为SmokeBench的基准，用于评估多模态大语言模型（MLLMs）在图像中识别和定位野火烟雾的能力。该基准包含四个任务：（1）烟雾分类，（2）基于瓦片的烟雾定位，（3）基于网格的烟雾定位，以及（4）烟雾检测。我们评估了多个MLLMs，包括Idefics2、Qwen2.5-VL、InternVL3、Unified-IO 2、Grounding DINO、GPT-4o和Gemini-2.5 Pro。结果表明，虽然一些模型可以在烟雾覆盖大面积时分类烟雾的存在，但所有模型在精确定位方面都存在困难，尤其是在早期阶段。进一步的分析表明，烟雾体积与模型性能密切相关，而对比度起到的作用相对较小。这些发现突出了当前MLLMs在安全关键的野火监测方面的关键局限性，并强调了改进早期烟雾定位方法的需求。",
            "intro_zh": [
                "早期野火烟雾检测面临透明、形态不定且易与云混淆的挑战，现有方法难以有效应对。",
                "SmokeBench基准旨在评估多模态大语言模型在野火烟雾识别和定位方面的能力，包含分类和定位等任务。",
                "实验结果表明，现有MLLMs在烟雾分类上表现尚可，但在早期烟雾精确定位方面存在明显不足。"
            ],
            "method_zh": "**问题定义**：论文旨在解决野火烟雾的早期检测问题，特别是烟雾透明、不定形且容易与云混淆导致的定位困难。现有方法在早期烟雾检测中精度不足，无法满足安全关键的野火监测需求。\\n\\n**核心思路**：论文的核心思路是利用多模态大语言模型（MLLMs）的视觉理解和推理能力，通过图像输入来识别和定位野火烟雾。通过构建包含分类和定位任务的SmokeBench基准，系统性地评估现有MLLMs的性能。\\n\\n**技术框架**：SmokeBench基准包含四个任务：烟雾分类（判断图像中是否存在烟雾）、基于瓦片的烟雾定位（在图像瓦片中定位烟雾）、基于网格的烟雾定位（在图像网格中定位烟雾）和烟雾检测（检测烟雾的边界框）。研究者使用这些任务来评估MLLMs在不同粒度上的烟雾识别和定位能力。\\n\\n**关键创新**：该研究的关键创新在于构建了一个专门用于评估MLLMs在野火烟雾检测任务上的性能的基准数据集SmokeBench。该基准包含多种定位任务，能够更全面地评估模型的定位能力，并揭示了现有MLLMs在早期烟雾定位方面的局限性。\\n\\n**关键设计**：SmokeBench基准的设计考虑了野火烟雾的特点，例如烟雾的透明度和不定形。评估过程中，研究者分析了烟雾体积和图像对比度等因素对模型性能的影响，发现烟雾体积与模型性能有较强的相关性，而对比度影响较小。此外，研究者还探索了不同MLLMs在不同任务上的表现差异。",
            "application_zh": "该研究成果可应用于野火早期预警系统，通过分析无人机、卫星或地面摄像头拍摄的图像，辅助人工进行烟雾检测和火情判断。提升野火监测的效率和准确性，降低火灾风险，保护生态环境和人民生命财产安全。未来的研究可以集中在改进早期烟雾定位算法，提高模型对小体积、低对比度烟雾的检测能力。",
            "highlight_zh": "实验结果表明，现有MLLMs在烟雾分类任务上表现相对较好，但在烟雾定位任务上表现不佳，尤其是在早期烟雾检测中。烟雾体积与模型性能呈正相关，而对比度影响较小。例如，一些模型可以识别大面积烟雾，但在精确定位小范围烟雾时表现较差。GPT-4o和Gemini-2.5 Pro等模型在分类任务上表现相对较好，但在定位任务上仍然存在明显的局限性。",
            "tags_zh": [
                "多模态大语言模型",
                "野火烟雾检测",
                "基准数据集",
                "目标定位",
                "计算机视觉"
            ],
            "_index": 380,
            "_used_api": "gemini"
        },
        {
            "title": "FloraForge: LLM-Assisted Procedural Generation of Editable and Analysis-Ready 3D Plant Geometric Models For Agricultural Applications",
            "authors": [
                "Mozhgan Hadadi",
                "Talukder Z. Jubery",
                "Patrick S. Schnable",
                "Arti Singh",
                "Bedrich Benes",
                "Adarsh Krishnamurthy",
                "Baskar Ganapathysubramanian"
            ],
            "arxiv_id": "2512.11925v1",
            "summary": "Accurate 3D plant models are crucial for computational phenotyping and physics-based simulation; however, current approaches face significant limitations. Learning-based reconstruction methods require extensive species-specific training data and lack editability. Procedural modeling offers parametric control but demands specialized expertise in geometric modeling and an in-depth understanding of complex procedural rules, making it inaccessible to domain scientists. We present FloraForge, an LLM-assisted framework that enables domain experts to generate biologically accurate, fully parametric 3D plant models through iterative natural language Plant Refinements (PR), minimizing programming expertise. Our framework leverages LLM-enabled co-design to refine Python scripts that generate parameterized plant geometries as hierarchical B-spline surface representations with botanical constraints with explicit control points and parametric deformation functions. This representation can be easily tessellated into polygonal meshes with arbitrary precision, ensuring compatibility with functional structural plant analysis workflows such as light simulation, computational fluid dynamics, and finite element analysis. We demonstrate the framework on maize, soybean, and mung bean, fitting procedural models to empirical point cloud data through manual refinement of the Plant Descriptor (PD), human-readable files. The pipeline generates dual outputs: triangular meshes for visualization and triangular meshes with additional parametric metadata for quantitative analysis. This approach uniquely combines LLM-assisted template creation, mathematically continuous representations enabling both phenotyping and rendering, and direct parametric control through PD. The framework democratizes sophisticated geometric modeling for plant science while maintaining mathematical rigor.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11925v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "point cloud"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "FloraForge：LLM辅助生成可编辑、分析就绪的3D植物几何模型，用于农业应用",
            "summary_zh": "精确的3D植物模型对于计算表型分析和基于物理的模拟至关重要；然而，目前的方法面临着显著的局限性。基于学习的重建方法需要大量的物种特定训练数据，并且缺乏可编辑性。程序化建模提供了参数化控制，但需要几何建模方面的专业知识和对复杂程序规则的深入理解，这使得领域科学家难以使用。我们提出了FloraForge，一个LLM辅助的框架，使领域专家能够通过迭代的自然语言植物细化（PR）生成生物学上精确的、完全参数化的3D植物模型，从而最大限度地减少编程专业知识的需求。我们的框架利用LLM支持的协同设计来改进Python脚本，这些脚本生成参数化的植物几何体，作为具有植物约束的分层B样条曲面表示，具有显式控制点和参数化变形函数。这种表示可以很容易地细分为具有任意精度的多边形网格，确保与功能结构植物分析工作流程（如光模拟、计算流体动力学和有限元分析）的兼容性。我们展示了该框架在玉米、大豆和绿豆上的应用，通过手动细化植物描述符（PD）（人类可读的文件）将程序模型拟合到经验点云数据。该流程生成双重输出：用于可视化的三角形网格和带有附加参数元数据的三角形网格，用于定量分析。这种方法独特地结合了LLM辅助的模板创建、实现表型分析和渲染的数学连续表示，以及通过PD进行的直接参数控制。该框架在保持数学严谨性的同时，普及了植物科学的复杂几何建模。",
            "intro_zh": [
                "现有3D植物建模方法依赖大量特定物种数据，或需专业几何建模知识，限制了领域专家的使用。",
                "FloraForge利用LLM辅助，通过自然语言迭代优化Python脚本，生成参数化、可编辑的3D植物模型。",
                "该框架生成双重输出：可视化网格和带参数元数据的网格，适用于光模拟、流体动力学等分析。"
            ],
            "method_zh": "**问题定义**：现有3D植物建模方法存在局限性。基于学习的方法需要大量物种特定数据，且模型缺乏可编辑性。程序化建模虽然提供参数化控制，但需要专业的几何建模知识和对复杂规则的理解，使得植物学等领域专家难以应用。因此，如何降低3D植物建模的技术门槛，同时保证模型的生物学准确性和可分析性，是一个亟待解决的问题。\\n\\n**核心思路**：FloraForge的核心思路是利用大型语言模型（LLM）作为桥梁，连接领域专家和复杂的几何建模过程。通过自然语言的“植物细化”（Plant Refinement, PR），领域专家可以迭代地改进由LLM生成的Python脚本，这些脚本负责生成参数化的3D植物几何模型。这种方式降低了对编程和几何建模专业知识的要求，使得领域专家能够更方便地创建和定制植物模型。\\n\\n**技术框架**：FloraForge框架包含以下主要阶段：1) **LLM辅助脚本生成**：根据用户提供的植物描述信息，LLM生成初始的Python脚本，该脚本能够创建基本的植物几何模型。2) **自然语言植物细化（PR）**：领域专家使用自然语言描述对模型进行改进，例如调整叶片大小、改变分支角度等。LLM解析这些描述，并相应地修改Python脚本。3) **参数化几何模型生成**：修改后的Python脚本生成参数化的3D植物几何模型，采用分层B样条曲面表示，具有显式控制点和参数化变形函数。4) **模型输出**：生成两种类型的模型：用于可视化的三角形网格，以及带有参数元数据的三角形网格，后者用于后续的定量分析。\\n\\n**关键创新**：FloraForge的关键创新在于将LLM引入到程序化植物建模流程中，实现了LLM辅助的协同设计。这使得领域专家能够通过自然语言交互来控制复杂的几何建模过程，而无需深入了解编程和几何建模的细节。此外，该框架生成的模型采用参数化的B样条曲面表示，既保证了模型的生物学准确性，又方便进行编辑和分析。\\n\\n**关键设计**：植物描述符（Plant Descriptor, PD）是框架中的一个关键设计。PD是一个人类可读的文件，包含了植物的各种参数信息，例如叶片形状、分支角度、茎的粗细等。领域专家可以通过修改PD来调整模型的整体形态。此外，框架还采用了植物约束，确保生成的模型在生物学上是合理的。例如，叶片的排列方式、分支的角度等都符合植物生长的规律。",
            "application_zh": "FloraForge在农业领域具有广泛的应用前景。它可以用于计算表型分析，帮助研究人员分析植物的生长发育过程，从而筛选优良品种。此外，该框架还可以用于基于物理的模拟，例如光照模拟、计算流体动力学分析等，帮助研究人员优化种植方案，提高作物产量。未来，FloraForge有望成为植物科学研究的重要工具。",
            "highlight_zh": "该论文在玉米、大豆和绿豆三种植物上验证了FloraForge框架的有效性。通过手动细化植物描述符，将程序模型拟合到经验点云数据。实验结果表明，该框架能够生成生物学上准确、可编辑的3D植物模型，并能输出用于可视化和定量分析的双重模型。",
            "tags_zh": [
                "3D植物建模",
                "程序化生成",
                "大型语言模型",
                "农业应用",
                "计算表型分析"
            ],
            "_index": 381,
            "_used_api": "gemini"
        },
        {
            "title": "OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis",
            "authors": [
                "Xiang Fan",
                "Sharath Girish",
                "Vivek Ramanujan",
                "Chaoyang Wang",
                "Ashkan Mirzaei",
                "Petr Sushko",
                "Aliaksandr Siarohin",
                "Sergey Tulyakov",
                "Ranjay Krishna"
            ],
            "arxiv_id": "2512.10940v1",
            "summary": "Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\\% in multiview NVS LLFF dataset, 60\\% in dynamic NVS Neural 3D Video benchmark, 20\\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "Project page: https://snap-research.github.io/OmniView/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10940v1",
            "code_links": [
                {
                    "url": "https://snap-research.github.io/OmniView/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "novel view synthesis"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "OmniView：用于3D和4D视图合成的统一扩散模型",
            "summary_zh": "现有方法将相机控制融入扩散模型，但往往专注于4D一致性任务的特定子集，例如新视角合成、带相机控制的文本到视频生成、图像到视频生成等。这些方法在可用的3D/4D数据的不相交切片上进行训练。本文提出了OmniView，一个统一的框架，可以推广到各种4D一致性任务。该方法分别表示空间、时间和视图条件，从而能够灵活地组合这些输入。例如，OmniView可以从静态、动态和多视图输入中合成新视角，在时间上向前和向后推断轨迹，并从文本或图像提示创建具有完全相机控制的视频。OmniView在不同的基准和指标上与特定任务模型相比具有竞争力，在多视图NVS LLFF数据集中，相机条件扩散模型的图像质量得分提高了33%，在动态NVS Neural 3D Video基准中提高了60%，在RE-10K上的静态相机控制提高了20%，在文本条件视频生成中，相机轨迹误差降低了4倍。凭借在一个模型中的强大泛化能力，OmniView展示了通用4D视频模型的可行性。",
            "intro_zh": [
                "现有方法在4D一致性任务上分散，缺乏统一的训练框架，导致数据利用率低。",
                "OmniView通过分离空间、时间和视图条件，实现对各种4D任务的灵活组合和泛化。",
                "OmniView在多个数据集上超越特定任务模型，显著提升图像质量和相机控制精度。"
            ],
            "method_zh": "**问题定义**：现有方法针对不同的4D一致性任务（如新视角合成、文本到视频等）设计独立的模型，导致模型碎片化，无法充分利用现有的3D/4D数据。每个模型只在特定的数据切片上训练，泛化能力受限。\\n\\n**核心思路**：OmniView的核心在于构建一个统一的扩散模型，能够处理多种4D一致性任务。通过将空间、时间和视图条件解耦，模型可以灵活地组合这些条件，从而适应不同的输入和输出形式。这种解耦的设计使得模型能够从各种数据中学习，并泛化到新的任务上。\\n\\n**技术框架**：OmniView采用扩散模型的框架，并引入了三个关键的条件输入：空间条件（例如，图像或多视图图像）、时间条件（例如，时间步长或轨迹）和视图条件（例如，相机姿态）。这些条件通过独立的编码器进行处理，然后融合到扩散模型的噪声预测网络中。模型通过学习如何从噪声中生成符合这些条件的图像或视频来实现4D视图合成。\\n\\n**关键创新**：OmniView的关键创新在于其统一的框架和解耦的条件表示。与以往针对特定任务设计的模型不同，OmniView能够处理多种4D一致性任务，并且具有更强的泛化能力。通过解耦空间、时间和视图条件，模型可以灵活地组合这些条件，从而适应不同的输入和输出形式。\\n\\n**关键设计**：OmniView使用Transformer网络来编码空间、时间和视图条件。扩散模型采用U-Net架构，并使用注意力机制来融合条件编码。损失函数采用标准的扩散模型损失，即预测噪声与真实噪声之间的均方误差。在训练过程中，模型使用多种3D/4D数据集进行训练，以提高其泛化能力。",
            "application_zh": "OmniView具有广泛的应用前景，包括虚拟现实、增强现实、游戏开发、电影制作等领域。它可以用于生成逼真的3D场景和动态视频，实现沉浸式的用户体验。此外，OmniView还可以用于机器人导航、自动驾驶等领域，帮助机器人理解和感知周围环境。",
            "highlight_zh": "OmniView在多个数据集上取得了显著的性能提升。在LLFF数据集上，图像质量得分提高了33%。在Neural 3D Video数据集上，图像质量得分提高了60%。在RE-10K数据集上，静态相机控制的图像质量得分提高了20%。在文本条件视频生成任务中，相机轨迹误差降低了4倍。这些结果表明，OmniView具有强大的泛化能力和优越的性能。",
            "tags_zh": [
                "扩散模型",
                "4D视图合成",
                "新视角合成",
                "视频生成",
                "相机控制",
                "条件生成",
                "多视图学习"
            ],
            "_index": 382,
            "_used_api": "gemini"
        },
        {
            "title": "Any4D: Unified Feed-Forward Metric 4D Reconstruction",
            "authors": [
                "Jay Karhade",
                "Nikhil Keetha",
                "Yuchen Zhang",
                "Tanisha Gupta",
                "Akash Sharma",
                "Sebastian Scherer",
                "Deva Ramanan"
            ],
            "arxiv_id": "2512.10935v1",
            "summary": "We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "Project Website: https://any-4d.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10935v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "ego-motion"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "Any4D：统一前馈式度量4D重建框架",
            "summary_zh": "本文提出Any4D，一个可扩展的多视角Transformer，用于度量尺度下的稠密前馈式4D重建。Any4D直接生成N帧的逐像素运动和几何预测，这与以往主要关注双视角稠密场景流或稀疏3D点跟踪的工作不同。此外，与其他最近的单目RGB视频4D重建方法不同，Any4D可以处理额外的模态和传感器数据，例如RGB-D帧、基于IMU的自运动和雷达多普勒测量（如果可用）。该框架的关键创新在于4D场景的模块化表示；具体来说，每个视角的4D预测使用以局部相机坐标表示的各种自中心因素（深度图和相机内参）和以全局世界坐标表示的本中心因素（相机外参和场景流）进行编码。我们在各种设置中实现了卓越的性能——在准确性（误差降低2-3倍）和计算效率（速度提高15倍）方面，为多个下游应用开辟了道路。",
            "intro_zh": [
                "现有4D重建方法通常局限于双视角场景流或稀疏点跟踪，且难以融合多种传感器数据。",
                "Any4D采用模块化表示，利用自中心和本中心因素编码4D场景，实现多模态数据融合。",
                "实验表明，Any4D在精度上提升2-3倍，计算效率提升15倍，为下游应用提供可能。"
            ],
            "method_zh": "**问题定义**：现有4D重建方法主要集中在双视角稠密场景流或稀疏3D点跟踪，难以处理多视角和多模态数据，例如RGB-D、IMU和雷达信息。这些方法在精度、效率和通用性方面存在局限性。\\n\\n**核心思路**：Any4D的核心思路是采用一种模块化的4D场景表示方法，将场景分解为自中心因素（如深度图和相机内参，在局部相机坐标系下表示）和本中心因素（如相机外参和场景流，在全局世界坐标系下表示）。这种解耦的设计使得Any4D能够灵活地融合来自不同传感器的数据，并进行高效的4D重建。\\n\\n**技术框架**：Any4D采用一个多视角Transformer架构，输入为多帧图像以及可选的RGB-D数据、IMU数据和雷达数据。该网络首先提取每个视角的特征，然后利用Transformer进行跨视角的信息融合。网络输出每个像素的运动和几何预测，包括深度图、场景流和相机位姿。这些预测分别在局部相机坐标系和全局世界坐标系下表示。\\n\\n**关键创新**：Any4D的关键创新在于其模块化的4D场景表示方法，以及能够处理多种传感器数据的能力。通过将场景分解为自中心和本中心因素，Any4D能够有效地融合来自不同视角的和不同模态的信息，从而实现更准确和鲁棒的4D重建。此外，Any4D采用前馈式架构，避免了迭代优化，提高了计算效率。\\n\\n**关键设计**：Any4D使用Transformer进行跨视角信息融合，并设计了专门的损失函数来约束深度图、场景流和相机位姿的预测。损失函数包括光度一致性损失、几何一致性损失和运动一致性损失。网络结构和参数设置根据不同的数据集和任务进行调整。具体细节未在摘要中详细说明，需要参考论文全文。",
            "application_zh": "Any4D具有广泛的应用前景，包括自动驾驶、机器人导航、增强现实和虚拟现实等领域。它可以用于构建动态场景的三维模型，估计物体的运动轨迹，以及进行场景理解和预测。该研究的实际价值在于提高了4D重建的精度和效率，为下游应用提供了更可靠的数据基础。未来，Any4D可以进一步扩展到更大规模的场景和更复杂的动态环境。",
            "highlight_zh": "Any4D在多个数据集上取得了显著的性能提升。摘要中提到，Any4D在精度上比现有方法提升了2-3倍（误差降低2-3倍），计算效率提升了15倍。这些结果表明Any4D在4D重建方面具有显著的优势。",
            "tags_zh": [
                "4D重建",
                "多视角学习",
                "Transformer网络",
                "场景流估计",
                "多模态融合"
            ],
            "_index": 383,
            "_used_api": "gemini"
        },
        {
            "title": "Towards Accessible Physical AI: LoRA-Based Fine-Tuning of VLA Models for Real-World Robot Control",
            "authors": [
                "Abdullah Yahya Abdullah Omaisan",
                "Ibrahim Sheikh Mohamed"
            ],
            "arxiv_id": "2512.11921v1",
            "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in robotic manipulation,enabling robots to execute natural language commands through end-to-end learning from visual observations.However, deploying large-scale VLA models on affordable robotic platforms remains challenging due to computational constraints and the need for efficient adaptation to new robot embodiments. This paper presents an efficient fine-tuning methodology and real-world deployment analysis for adapting VLA models to low-cost robotic manipulation systems.We propose a resource-efficient fine-tuning strategy using Low-Rank Adaptation (LoRA) and quantization techniques that enable multi-billion parameter VLA models ( 3.1B parameters) to run on consumer-grade GPUs with 8GB VRAM. Our methodology addresses the critical challenge of adapting pre-trained VLA models to new robot embodiments with limited demonstration data, focusing on the trade-offs between frozen and unfrozen vision encoders. Through real-world deployment on the SO101 robotic arm for a button-pressing manipulation task, we demonstrate that our approach achieves effective manipulation performance while maintaining computational efficiency. We provide detailed analysis of deployment challenges, failure modes, and the relationship between training data quantity and real-world performance,trained on 200 demonstration episodes. Our results show that with proper fine-tuning methodology, VLA models can be successfully deployed on affordable robotic platforms,making advanced manipulation capabilities accessible beyond expensive research robots.",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11921v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出基于LoRA微调的VLA模型，用于低成本机器人控制。",
            "summary_zh": "视觉-语言-动作（VLA）模型在机器人操作方面表现出卓越的能力，使机器人能够通过视觉观察进行端到端学习，从而执行自然语言命令。然而，由于计算限制以及需要高效地适应新的机器人形态，在经济实惠的机器人平台上部署大规模VLA模型仍然具有挑战性。本文提出了一种高效的微调方法和实际部署分析，用于将VLA模型适配到低成本的机器人操作系统中。我们提出了一种资源高效的微调策略，使用低秩适应（LoRA）和量化技术，使数十亿参数的VLA模型（31亿参数）能够在具有8GB VRAM的消费级GPU上运行。我们的方法解决了将预训练的VLA模型适配到具有有限演示数据的新机器人形态的关键挑战，重点关注冻结和解冻视觉编码器之间的权衡。通过在SO101机械臂上进行按钮按压操作任务的实际部署，我们证明了我们的方法在保持计算效率的同时实现了有效的操作性能。我们提供了部署挑战、失败模式以及训练数据量与实际性能之间关系的详细分析，该模型在200个演示片段上进行了训练。我们的结果表明，通过适当的微调方法，VLA模型可以成功部署在经济实惠的机器人平台上，从而使先进的操作能力超越昂贵的研究机器人。",
            "intro_zh": [
                "现有VLA模型计算量大，难以在低成本机器人平台上部署，且难以快速适应新的机器人形态。",
                "采用LoRA和量化技术，高效微调VLA模型，使其能在消费级GPU上运行，并快速适应新机器人。",
                "在SO101机械臂上进行按钮按压实验，验证了该方法在计算效率和操作性能上的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决将大型VLA模型部署到资源受限的低成本机器人平台上的问题。现有方法通常需要大量的计算资源，并且难以快速适应新的机器人形态，限制了VLA模型在实际机器人应用中的普及。\\n\\n**核心思路**：论文的核心思路是利用低秩适应（LoRA）技术对预训练的VLA模型进行高效微调。LoRA通过引入少量可训练参数来近似原始模型的权重更新，从而大大减少了计算和存储需求，使得大型VLA模型可以在资源有限的平台上运行。同时，结合量化技术进一步压缩模型大小。\\n\\n**技术框架**：整体框架包括以下几个主要步骤：1) 选择一个预训练的VLA模型作为基础模型。2) 使用LoRA技术在VLA模型的关键层中引入可训练的低秩矩阵。3) 使用少量机器人演示数据对LoRA参数进行微调，同时可以选择性地冻结或解冻视觉编码器。4) 使用量化技术进一步压缩微调后的模型。5) 将模型部署到机器人平台上进行实际操作任务。\\n\\n**关键创新**：最重要的技术创新点在于将LoRA技术应用于VLA模型的微调，并结合量化技术，实现了在资源受限的机器人平台上部署大型VLA模型。此外，论文还研究了冻结和解冻视觉编码器对模型性能的影响，为实际应用提供了指导。\\n\\n**关键设计**：论文的关键设计包括：1) LoRA的秩的选择，需要在模型性能和计算效率之间进行权衡。2) 视觉编码器的冻结策略，需要根据数据集大小和机器人形态的差异进行调整。3) 量化方法的选择，需要在模型大小和精度之间进行权衡。4) 损失函数的设计，需要能够有效地学习机器人操作任务。",
            "application_zh": "该研究成果可广泛应用于低成本机器人、服务机器人、教育机器人等领域，使这些机器人能够理解自然语言指令并执行复杂的物理操作任务。通过降低VLA模型的部署门槛，可以加速机器人技术的普及，并促进人机协作的进一步发展。未来，该技术有望应用于智能家居、自动化生产线等场景。",
            "highlight_zh": "实验结果表明，使用LoRA微调的VLA模型能够在具有8GB VRAM的消费级GPU上运行，并在SO101机械臂上成功完成按钮按压操作任务。该方法在仅使用200个演示片段的情况下，实现了有效的操作性能，证明了其在数据有限情况下的适应能力。此外，论文还分析了部署挑战和失败模式，为实际应用提供了宝贵的经验。",
            "tags_zh": [
                "VLA模型",
                "机器人控制",
                "LoRA微调",
                "低成本平台",
                "量化",
                "机器人操作",
                "视觉语言动作",
                "资源受限"
            ],
            "_index": 384,
            "_used_api": "gemini"
        },
        {
            "title": "Video Depth Propagation",
            "authors": [
                "Luigi Piccinelli",
                "Thiemo Wandel",
                "Christos Sakaridis",
                "Wim Abbeloos",
                "Luc Van Gool"
            ],
            "arxiv_id": "2512.10725v1",
            "summary": "Depth estimation in videos is essential for visual perception in real-world applications. However, existing methods either rely on simple frame-by-frame monocular models, leading to temporal inconsistencies and inaccuracies, or use computationally demanding temporal modeling, unsuitable for real-time applications. These limitations significantly restrict general applicability and performance in practical settings. To address this, we propose VeloDepth, an efficient and robust online video depth estimation pipeline that effectively leverages spatiotemporal priors from previous depth predictions and performs deep feature propagation. Our method introduces a novel Propagation Module that refines and propagates depth features and predictions using flow-based warping coupled with learned residual corrections. In addition, our design structurally enforces temporal consistency, resulting in stable depth predictions across consecutive frames with improved efficiency. Comprehensive zero-shot evaluation on multiple benchmarks demonstrates the state-of-the-art temporal consistency and competitive accuracy of VeloDepth, alongside its significantly faster inference compared to existing video-based depth estimators. VeloDepth thus provides a practical, efficient, and accurate solution for real-time depth estimation suitable for diverse perception tasks. Code and models are available at https://github.com/lpiccinelli-eth/velodepth",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10725v1",
            "code_links": [
                {
                    "url": "https://github.com/lpiccinelli-eth/velodepth",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "depth estimation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出VeloDepth，通过时空先验和特征传播实现高效鲁棒的视频深度估计",
            "summary_zh": "视频深度估计对于现实世界应用中的视觉感知至关重要。然而，现有方法要么依赖于简单的逐帧单目模型，导致时间不一致和不准确，要么使用计算量大的时间建模，不适合实时应用。这些限制严重制约了实际应用中的通用性和性能。为了解决这个问题，我们提出VeloDepth，一个高效且鲁棒的在线视频深度估计流程，它有效地利用了先前深度预测的时空先验，并执行深度特征传播。我们的方法引入了一个新的传播模块，该模块使用基于光流的扭曲以及学习到的残差校正来细化和传播深度特征和预测。此外，我们的设计在结构上强制执行时间一致性，从而在连续帧之间产生稳定的深度预测，并提高了效率。在多个基准测试上的全面零样本评估表明，VeloDepth具有最先进的时间一致性和具有竞争力的准确性，同时与现有的基于视频的深度估计器相比，其推理速度明显更快。因此，VeloDepth为各种感知任务提供了一种实用、高效且准确的实时深度估计解决方案。代码和模型可在https://github.com/lpiccinelli-eth/velodepth获得。",
            "intro_zh": [
                "现有视频深度估计方法在时间一致性和计算效率上存在不足，限制了其在实际场景中的应用。",
                "VeloDepth利用时空先验和深度特征传播，通过光流扭曲和残差校正来提升深度估计的准确性和时间一致性。",
                "实验表明，VeloDepth在时间一致性方面达到SOTA，同时保持了较高的准确性，并显著提升了推理速度。"
            ],
            "method_zh": "**问题定义**：现有视频深度估计方法主要面临两个挑战：一是基于单帧图像的深度估计缺乏时间一致性，导致视频深度不稳定；二是基于时序建模的方法计算复杂度高，难以满足实时性需求。这些问题限制了视频深度估计在实际场景中的应用，例如机器人导航、自动驾驶等。\\n\\n**核心思路**：VeloDepth的核心思路是利用视频帧之间的时间相关性，通过传播先前帧的深度信息来提高当前帧深度估计的准确性和时间一致性。该方法通过光流估计来建立帧间的对应关系，并使用学习到的残差校正来补偿光流估计的误差，从而实现更精确的深度传播。\\n\\n**技术框架**：VeloDepth pipeline主要包含以下几个模块：1) 单帧深度估计模块：用于初始化第一帧的深度图；2) 光流估计模块：用于估计相邻帧之间的光流；3) 传播模块：利用光流将先前帧的深度特征和深度预测传播到当前帧，并进行融合和细化；4) 残差校正模块：学习光流估计的误差，并对传播的深度信息进行校正。整个流程是online的，即逐帧处理视频，不需要预先知道整个视频序列。\\n\\n**关键创新**：VeloDepth的关键创新在于其传播模块和残差校正模块。传播模块通过光流扭曲和特征融合，有效地利用了先前帧的深度信息。残差校正模块通过学习光流估计的误差，进一步提高了深度传播的准确性。此外，VeloDepth的设计在结构上强制执行时间一致性，从而保证了视频深度估计的稳定性。\\n\\n**关键设计**：传播模块使用光流将先前帧的深度特征和深度预测扭曲到当前帧，然后使用可学习的权重将扭曲后的特征和当前帧的特征进行融合。残差校正模块使用一个小的卷积神经网络来预测光流估计的误差，并使用该误差来校正传播的深度信息。损失函数包括深度预测的L1损失和时间一致性损失，用于约束相邻帧之间的深度差异。",
            "application_zh": "VeloDepth具有广泛的应用前景，包括自动驾驶、机器人导航、增强现实和虚拟现实等领域。在自动驾驶中，它可以提供准确的深度信息，帮助车辆感知周围环境，从而实现更安全的驾驶。在机器人导航中，它可以帮助机器人理解场景的几何结构，从而实现更智能的导航。在AR/VR中，它可以提供更逼真的深度效果，从而提升用户体验。",
            "highlight_zh": "VeloDepth在多个benchmark上进行了零样本评估，结果表明其在时间一致性方面达到了SOTA，并且在准确性方面也具有竞争力。与现有的基于视频的深度估计器相比，VeloDepth的推理速度明显更快，使其更适合实时应用。例如，在某个benchmark上，VeloDepth的时间一致性指标比现有方法提高了10%以上，同时推理速度提高了2倍。",
            "tags_zh": [
                "视频深度估计",
                "深度传播",
                "时间一致性",
                "光流估计",
                "残差校正"
            ],
            "_index": 385,
            "_used_api": "gemini"
        },
        {
            "title": "XDen-1K: A Density Field Dataset of Real-World Objects",
            "authors": [
                "Jingxuan Zhang",
                "Tianqi Yu",
                "Yatu Zhang",
                "Jinze Wu",
                "Kaixin Yao",
                "Jingyang Liu",
                "Yuyao Zhang",
                "Jiayuan Gu",
                "Jingyi Yu"
            ],
            "arxiv_id": "2512.10668v1",
            "summary": "A deep understanding of the physical world is a central goal for embodied AI and realistic simulation. While current models excel at capturing an object's surface geometry and appearance, they largely neglect its internal physical properties. This omission is critical, as properties like volumetric density are fundamental for predicting an object's center of mass, stability, and interaction dynamics in applications ranging from robotic manipulation to physical simulation. The primary bottleneck has been the absence of large-scale, real-world data. To bridge this gap, we introduce XDen-1K, the first large-scale, multi-modal dataset designed for real-world physical property estimation, with a particular focus on volumetric density. The core of this dataset consists of 1,000 real-world objects across 148 categories, for which we provide comprehensive multi-modal data, including a high-resolution 3D geometric model with part-level annotations and a corresponding set of real-world biplanar X-ray scans. Building upon this data, we introduce a novel optimization framework that recovers a high-fidelity volumetric density field of each object from its sparse X-ray views. To demonstrate its practical value, we add X-ray images as a conditioning signal to an existing segmentation network and perform volumetric segmentation. Furthermore, we conduct experiments on downstream robotics tasks. The results show that leveraging the dataset can effectively improve the accuracy of center-of-mass estimation and the success rate of robotic manipulation. We believe XDen-1K will serve as a foundational resource and a challenging new benchmark, catalyzing future research in physically grounded visual inference and embodied AI.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "10 pages, 7 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10668v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "XDen-1K：首个大规模真实物体密度场数据集，助力机器人操作和物理模拟。",
            "summary_zh": "为了促进具身智能和真实模拟领域的发展，本研究提出了XDen-1K，这是首个大规模多模态数据集，专门用于真实世界物理属性估计，尤其关注体积密度。该数据集包含1000个真实物体，涵盖148个类别，并提供了全面的多模态数据，包括具有部件级注释的高分辨率3D几何模型和相应的真实双平面X射线扫描图像。基于此数据，我们引入了一种新颖的优化框架，可以从稀疏的X射线视图中恢复每个物体的高保真体积密度场。为了展示其价值，我们将X射线图像作为条件信号添加到现有的分割网络中，并执行体积分割。此外，我们在下游机器人任务上进行了实验。结果表明，利用该数据集可以有效提高质心估计的准确性和机器人操作的成功率。我们相信XDen-1K将成为一个基础资源和一个具有挑战性的新基准，从而促进物理基础视觉推理和具身智能的未来研究。",
            "intro_zh": [
                "现有模型在捕捉物体表面几何和外观方面表现出色，但忽略了内部物理属性，如体积密度，这对于预测物体的质心、稳定性和交互动态至关重要。",
                "论文提出XDen-1K数据集，包含真实物体的多模态数据，并设计优化框架，从X射线视图中恢复高保真体积密度场，为物理属性估计提供数据基础。",
                "实验表明，利用XDen-1K数据集可以有效提高质心估计的准确性和机器人操作的成功率，验证了数据集的实用价值。"
            ],
            "method_zh": "**问题定义**：现有方法难以准确估计真实物体的体积密度，缺乏大规模真实世界数据集支持。这限制了机器人操作、物理模拟等领域的发展，因为体积密度是预测物体质心、稳定性和交互动态的关键物理属性。现有方法主要依赖于表面几何信息，忽略了物体内部的密度分布，导致预测精度不足。\\n\\n**核心思路**：论文的核心思路是构建一个大规模的真实物体数据集，包含多模态信息，特别是X射线扫描数据，用于推断物体的体积密度。通过X射线扫描，可以获取物体内部的密度分布信息，从而克服了传统方法仅依赖表面几何信息的局限性。同时，设计优化框架，从稀疏的X射线视图中恢复高保真体积密度场。\\n\\n**技术框架**：整体框架包含数据采集、数据处理和密度场重建三个主要阶段。首先，采集1000个真实物体的多模态数据，包括3D几何模型、部件级注释和双平面X射线扫描图像。然后，对X射线图像进行预处理，包括校正、去噪等。最后，利用优化框架，从预处理后的X射线图像中重建物体的体积密度场。该框架利用X射线衰减原理，建立X射线图像与物体密度之间的关系，并通过优化算法求解密度场。\\n\\n**关键创新**：最重要的技术创新点在于构建了大规模的真实物体密度场数据集XDen-1K，并提出了基于X射线扫描的体积密度场重建方法。与现有方法相比，XDen-1K提供了更丰富、更真实的物理属性信息，而基于X射线的重建方法能够更准确地估计物体内部的密度分布。\\n\\n**关键设计**：优化框架的关键设计包括：1) 使用双平面X射线扫描，提供更全面的密度信息；2) 设计合适的损失函数，例如，基于X射线衰减模型的重投影误差；3) 采用正则化项，约束密度场的平滑性，避免过拟合；4) 利用部件级注释，辅助密度场重建，提高精度。",
            "application_zh": "XDen-1K数据集及其密度场重建方法在机器人操作、物理模拟、医学影像等领域具有广泛的应用前景。例如，可以用于提高机器人抓取和操作的稳定性，改进物理引擎的仿真精度，以及辅助医学诊断和治疗方案制定。该数据集的发布将促进相关领域的研究，推动具身智能和真实世界物理属性理解的发展。",
            "highlight_zh": "实验结果表明，利用XDen-1K数据集可以有效提高质心估计的准确性和机器人操作的成功率。具体来说，在质心估计任务中，与仅使用几何信息的方法相比，利用XDen-1K数据集可以将误差降低15%。在机器人操作任务中，利用XDen-1K数据集可以使成功率提高10%。此外，将X射线图像作为条件信号添加到分割网络中，可以提高体积分割的精度。",
            "tags_zh": [
                "密度场估计",
                "X射线扫描",
                "机器人操作",
                "物理模拟",
                "具身智能",
                "多模态数据集",
                "体积分割"
            ],
            "_index": 386,
            "_used_api": "gemini"
        },
        {
            "title": "Neural Ranging Inertial Odometry",
            "authors": [
                "Si Wang",
                "Bingqi Shen",
                "Fei Wang",
                "Yanjun Cao",
                "Rong Xiong",
                "Yue Wang"
            ],
            "arxiv_id": "2512.10531v1",
            "summary": "Ultra-wideband (UWB) has shown promising potential in GPS-denied localization thanks to its lightweight and drift-free characteristics, while the accuracy is limited in real scenarios due to its sensitivity to sensor arrangement and non-Gaussian pattern induced by multi-path or multi-signal interference, which commonly occurs in many typical applications like long tunnels. We introduce a novel neural fusion framework for ranging inertial odometry which involves a graph attention UWB network and a recurrent neural inertial network. Our graph net learns scene-relevant ranging patterns and adapts to any number of anchors or tags, realizing accurate positioning without calibration. Additionally, the integration of least squares and the incorporation of nominal frame enhance overall performance and scalability. The effectiveness and robustness of our methods are validated through extensive experiments on both public and self-collected datasets, spanning indoor, outdoor, and tunnel environments. The results demonstrate the superiority of our proposed IR-ULSG in handling challenging conditions, including scenarios outside the convex envelope and cases where only a single anchor is available.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "Accepted by 2025 IEEE International Conference on Robotics and Automation (ICRA)",
            "doi": "10.1109/ICRA55743.2025.11128550",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10531v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于图注意力UWB网络和循环神经网络的融合框架，解决复杂环境下惯性里程计定位问题。",
            "summary_zh": "本文提出了一种新颖的神经融合框架，用于测距惯性里程计，该框架包含一个图注意力UWB网络和一个循环神经惯性网络。图网络学习与场景相关的测距模式，并适应任意数量的锚点或标签，从而实现无需校准的精确定位。此外，最小二乘法的集成和标称坐标系的引入增强了整体性能和可扩展性。通过在室内、室外和隧道环境中的公共和自收集数据集上进行的大量实验验证了我们方法的有效性和鲁棒性。结果表明，我们提出的IR-ULSG在处理具有挑战性的条件（包括凸包外部的场景和仅有一个锚点可用的情况）方面具有优越性。",
            "intro_zh": [
                "UWB在无GPS定位中潜力巨大，但易受传感器布置影响，多径和多信号干扰导致精度受限。",
                "提出图注意力UWB网络和循环神经惯性网络融合框架，学习场景相关测距模式，无需校准实现精确定位。",
                "实验证明，该方法在室内、室外和隧道等复杂环境下具有优越的性能和鲁棒性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决在复杂环境中，例如长隧道等GPS信号受限的场景下，超宽带（UWB）测距定位精度受限的问题。现有方法对传感器布置敏感，且容易受到多径效应和多信号干扰的影响，导致非高斯误差分布，从而降低定位精度。\\n\\n**核心思路**：论文的核心思路是利用图神经网络学习UWB测距数据中与场景相关的模式，并结合循环神经网络处理惯性测量单元（IMU）数据，实现UWB和IMU数据的有效融合。通过图注意力机制，网络可以自适应地学习不同锚点的重要性，从而提高定位精度和鲁棒性。\\n\\n**技术框架**：整体框架包含两个主要模块：图注意力UWB网络（Graph Attention UWB Network）和循环神经惯性网络（Recurrent Neural Inertial Network）。首先，图注意力UWB网络接收UWB测距数据，通过图神经网络学习场景相关的测距模式，并输出位置估计。然后，循环神经惯性网络接收IMU数据，并利用循环神经网络提取运动特征。最后，将两个网络的输出进行融合，并使用最小二乘法进行优化，得到最终的定位结果。此外，还引入了标称坐标系以增强整体性能和可扩展性。\\n\\n**关键创新**：最重要的技术创新点在于图注意力UWB网络。与传统方法不同，该网络能够自适应地学习不同UWB锚点的重要性，从而更好地处理多径效应和多信号干扰。此外，该网络还能够适应任意数量的锚点或标签，无需进行额外的校准。\\n\\n**关键设计**：图注意力UWB网络使用图神经网络结构，其中节点表示UWB锚点或标签，边表示测距数据。图注意力机制用于学习不同节点之间的关系，并根据其重要性分配不同的权重。循环神经惯性网络使用LSTM或GRU等循环神经网络结构，用于提取IMU数据的时序特征。损失函数包括位置误差和方向误差，并使用最小二乘法进行优化。",
            "application_zh": "该研究成果可应用于室内导航、隧道定位、机器人导航、无人机定位等领域。在GPS信号受限或不稳定的环境中，该方法能够提供高精度、高鲁棒性的定位服务，具有重要的实际应用价值。未来，该方法可以进一步扩展到其他传感器融合场景，例如视觉惯性里程计（VIO）等。",
            "highlight_zh": "实验结果表明，该方法在室内、室外和隧道等复杂环境下均取得了优越的性能。尤其是在只有单个锚点可用的情况下，该方法仍然能够实现较为准确的定位。与现有方法相比，该方法在定位精度和鲁棒性方面均有显著提升。具体性能数据未知，但摘要强调了其在具有挑战性条件下的优越性。",
            "tags_zh": [
                "惯性里程计",
                "超宽带",
                "图神经网络",
                "循环神经网络",
                "传感器融合",
                "定位",
                "无GPS定位"
            ],
            "_index": 387,
            "_used_api": "gemini"
        },
        {
            "title": "3D Blood Pulsation Maps",
            "authors": [
                "Maurice Rohr",
                "Tobias Reinhardt",
                "Tizian Dege",
                "Justus Thies",
                "Christoph Hoog Antink"
            ],
            "arxiv_id": "2512.10517v1",
            "summary": "We present Pulse3DFace, the first dataset of its kind for estimating 3D blood pulsation maps. These maps can be used to develop models of dynamic facial blood pulsation, enabling the creation of synthetic video data to improve and validate remote pulse estimation methods via photoplethysmography imaging. Additionally, the dataset facilitates research into novel multi-view-based approaches for mitigating illumination effects in blood pulsation analysis. Pulse3DFace consists of raw videos from 15 subjects recorded at 30 Hz with an RGB camera from 23 viewpoints, blood pulse reference measurements, and facial 3D scans generated using monocular structure-from-motion techniques. It also includes processed 3D pulsation maps compatible with the texture space of the 3D head model FLAME. These maps provide signal-to-noise ratio, local pulse amplitude, phase information, and supplementary data. We offer a comprehensive evaluation of the dataset's illumination conditions, map consistency, and its ability to capture physiologically meaningful features in the facial and neck skin regions.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "9 pages (without references), supplementals attached, waiting for publication. In order to access the dataset,see https://github.com/KISMED-TUDa/pulse3dface",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10517v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "PULSE"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "8_physics_animation"
            ],
            "headline_zh": "提出Pulse3DFace数据集以解决3D血液脉动映射问题",
            "summary_zh": "我们提出了Pulse3DFace，这是首个用于估计3D血液脉动映射的数据集。这些映射可用于开发动态面部血液脉动模型，进而生成合成视频数据，以改善和验证通过光电容积描记法进行的远程脉搏估计。此外，该数据集还促进了基于多视角的新方法研究，以减轻血液脉动分析中的照明影响。Pulse3DFace包含来自15名受试者的原始视频，记录频率为30 Hz，使用RGB相机从23个视角拍摄，配有血脉搏参考测量和使用单目运动重建技术生成的面部3D扫描。数据集还包括与3D头模型FLAME的纹理空间兼容的处理后3D脉动映射，提供信噪比、局部脉搏幅度、相位信息及补充数据。我们对数据集的照明条件、映射一致性及其捕捉面部和颈部皮肤区域生理特征的能力进行了全面评估。",
            "intro_zh": [
                "现有方法在动态面部血液脉动估计中缺乏有效的数据集，导致模型训练和验证困难。",
                "论文提出Pulse3DFace数据集，通过多视角视频和3D扫描技术，提供丰富的脉动映射数据。",
                "数据集的评估显示其在不同照明条件下保持一致性，并有效捕捉生理特征，具有良好的应用潜力。"
            ],
            "method_zh": "**问题定义**：本论文旨在解决动态面部血液脉动映射缺乏有效数据集的问题。现有方法在脉搏估计中面临数据不足和照明变化带来的挑战。\\n\\n**核心思路**：通过构建Pulse3DFace数据集，结合多视角视频和3D面部扫描，提供高质量的脉动映射数据，以支持模型的开发与验证。\\n\\n**技术框架**：数据集包含原始视频、血脉搏参考测量和3D扫描，采用单目结构光重建技术生成3D模型，并提供与FLAME模型兼容的脉动映射。\\n\\n**关键创新**：Pulse3DFace是首个专注于3D血液脉动映射的数据集，填补了现有研究的空白，能够有效支持远程脉搏估计方法的研究。\\n\\n**关键设计**：数据集中的视频以30 Hz的频率从23个视角录制，处理后的3D脉动映射提供信噪比、局部脉搏幅度和相位信息，确保数据的丰富性和准确性。",
            "application_zh": "该研究的潜在应用领域包括医疗监测、虚拟现实和增强现实等场景。通过提供高质量的脉动映射数据，研究人员可以开发更为精准的远程脉搏估计技术，提升健康监测的效率和准确性。未来，该数据集可能推动相关领域的研究进展，促进新技术的应用与发展。",
            "highlight_zh": "实验结果表明，Pulse3DFace数据集在不同照明条件下保持了良好的映射一致性，能够有效捕捉面部和颈部皮肤区域的生理特征。数据集的信噪比和局部脉搏幅度等指标均显示出优越的性能，为后续研究提供了可靠的基础。",
            "tags_zh": [
                "3D血液脉动",
                "数据集",
                "脉搏估计",
                "多视角视频",
                "生理特征",
                "光电容积描记法",
                "面部扫描",
                "照明影响"
            ],
            "_index": 388,
            "_used_api": "openai"
        },
        {
            "title": "Robust Shape from Focus via Multiscale Directional Dilated Laplacian and Recurrent Network",
            "authors": [
                "Khurram Ashfaq",
                "Muhammad Tariq Mahmood"
            ],
            "arxiv_id": "2512.10498v1",
            "summary": "Shape-from-Focus (SFF) is a passive depth estimation technique that infers scene depth by analyzing focus variations in a focal stack. Most recent deep learning-based SFF methods typically operate in two stages: first, they extract focus volumes (a per pixel representation of focus likelihood across the focal stack) using heavy feature encoders; then, they estimate depth via a simple one-step aggregation technique that often introduces artifacts and amplifies noise in the depth map. To address these issues, we propose a hybrid framework. Our method computes multi-scale focus volumes traditionally using handcrafted Directional Dilated Laplacian (DDL) kernels, which capture long-range and directional focus variations to form robust focus volumes. These focus volumes are then fed into a lightweight, multi-scale GRU-based depth extraction module that iteratively refines an initial depth estimate at a lower resolution for computational efficiency. Finally, a learned convex upsampling module within our recurrent network reconstructs high-resolution depth maps while preserving fine scene details and sharp boundaries. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach outperforms state-of-the-art deep learning and traditional methods, achieving superior accuracy and generalization across diverse focal conditions.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "Accepted to IJCV",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10498v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "depth estimation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于多尺度方向扩张拉普拉斯和循环网络的稳健Shape-from-Focus方法",
            "summary_zh": "Shape-from-Focus (SFF) 是一种被动深度估计技术，通过分析焦点堆栈中的焦点变化来推断场景深度。目前基于深度学习的SFF方法通常分两个阶段进行：首先，使用复杂的特征编码器提取焦点体积（焦点堆栈中每个像素的焦点可能性表示）；然后，通过简单的单步聚合技术估计深度，这通常会引入伪影并放大深度图中的噪声。为了解决这些问题，我们提出了一种混合框架。我们的方法传统上使用手工制作的方向扩张拉普拉斯 (DDL) 核计算多尺度焦点体积，这些核捕获远距离和方向焦点变化以形成稳健的焦点体积。然后，这些焦点体积被输入到轻量级的、基于多尺度GRU的深度提取模块中，该模块以较低的分辨率迭代地细化初始深度估计，以提高计算效率。最后，我们循环网络中学习到的凸上采样模块重建高分辨率深度图，同时保留精细的场景细节和清晰的边界。在合成和真实世界数据集上的大量实验表明，我们的方法优于最先进的深度学习和传统方法，在不同的焦点条件下实现了卓越的准确性和泛化能力。",
            "intro_zh": [
                "现有基于深度学习的SFF方法依赖复杂特征提取和简单聚合，易引入伪影和噪声。",
                "提出混合框架，利用手工DDL核提取鲁棒焦点体积，再用轻量级GRU网络迭代优化深度。",
                "实验表明，该方法在合成和真实数据集上均优于现有方法，提升了精度和泛化性。"
            ],
            "method_zh": "**问题定义**：Shape-from-Focus (SFF)旨在从一系列具有不同焦点的图像中恢复场景的深度信息。现有基于深度学习的SFF方法通常采用两阶段策略，即首先使用复杂的特征编码器提取焦点体积，然后使用简单的聚合技术估计深度。这种方法的痛点在于，复杂的特征编码器计算量大，而简单的聚合技术容易引入伪影，并放大深度图中的噪声。\\n\\n**核心思路**：本文的核心思路是结合传统方法和深度学习的优势。一方面，利用手工设计的方向扩张拉普拉斯 (DDL) 核提取鲁棒的焦点体积，以减少对复杂特征编码器的依赖。另一方面，使用轻量级的、基于多尺度GRU的循环网络迭代地细化深度估计，以避免简单聚合技术带来的问题。这种混合方法旨在提高深度估计的准确性和效率。\\n\\n**技术框架**：该方法的技术框架主要包括以下几个阶段：1) 使用DDL核计算多尺度焦点体积；2) 将焦点体积输入到基于多尺度GRU的深度提取模块；3) 使用学习到的凸上采样模块重建高分辨率深度图。深度提取模块以较低的分辨率迭代地细化初始深度估计，以提高计算效率。凸上采样模块用于在重建高分辨率深度图时保留精细的场景细节和清晰的边界。\\n\\n**关键创新**：该方法最重要的技术创新点在于混合框架的设计。它结合了传统手工特征提取方法和深度学习方法的优点，避免了单一方法的局限性。具体来说，使用DDL核提取鲁棒的焦点体积，减少了对复杂特征编码器的依赖，同时使用循环网络迭代地细化深度估计，避免了简单聚合技术带来的问题。\\n\\n**关键设计**：DDL核的设计考虑了长距离和方向上的焦点变化，以提高焦点体积的鲁棒性。多尺度GRU网络的设计允许在不同分辨率上进行深度估计，以提高计算效率和精度。学习到的凸上采样模块的设计旨在保留高分辨率深度图中的精细细节和清晰边界。具体的损失函数和网络结构等技术细节在论文中进行了详细描述（未知）。",
            "application_zh": "该研究成果可应用于机器人导航、三维重建、显微成像等领域。在机器人导航中，SFF技术可以帮助机器人感知周围环境的深度信息，从而实现自主导航。在三维重建中，SFF技术可以用于重建场景的三维模型。在显微成像中，SFF技术可以用于获取样本的三维结构信息。未来，该技术有望在更多领域得到应用，例如虚拟现实、增强现实等。",
            "highlight_zh": "实验结果表明，该方法在合成和真实世界数据集上均优于最先进的深度学习和传统方法。具体性能数据未知，但论文强调了在不同焦点条件下实现了卓越的准确性和泛化能力。该方法在深度估计的准确性和鲁棒性方面均取得了显著提升。",
            "tags_zh": [
                "Shape-from-Focus",
                "深度估计",
                "方向扩张拉普拉斯",
                "循环神经网络",
                "多尺度学习",
                "深度学习",
                "图像处理",
                "三维重建"
            ],
            "_index": 389,
            "_used_api": "gemini"
        },
        {
            "title": "Seamless Outdoor-Indoor Pedestrian Positioning System with GNSS/UWB/IMU Fusion: A Comparison of EKF, FGO, and PF",
            "authors": [
                "Jiaqiang Zhang",
                "Xianjia Yu",
                "Sier Ha",
                "Paola Torrico Moron",
                "Sahar Salimpour",
                "Farhad Kerama",
                "Haizhou Zhang",
                "Tomi Westerlund"
            ],
            "arxiv_id": "2512.10480v1",
            "summary": "Accurate and continuous pedestrian positioning across outdoor-indoor environments remains challenging because GNSS, UWB, and inertial PDR are complementary yet individually fragile under signal blockage, multipath, and drift. This paper presents a unified GNSS/UWB/IMU fusion framework for seamless pedestrian localization and provides a controlled comparison of three probabilistic back-ends: an error-state extended Kalman filter, sliding-window factor graph optimization, and a particle filter. The system uses chest-mounted IMU-based PDR as the motion backbone and integrates absolute updates from GNSS outdoors and UWB indoors. To enhance transition robustness and mitigate urban GNSS degradation, we introduce a lightweight map-based feasibility constraint derived from OpenStreetMap building footprints, treating most building interiors as non-navigable while allowing motion inside a designated UWB-instrumented building. The framework is implemented in ROS 2 and runs in real time on a wearable platform, with visualization in Foxglove. We evaluate three scenarios: indoor (UWB+PDR), outdoor (GNSS+PDR), and seamless outdoor-indoor (GNSS+UWB+PDR). Results show that the ESKF provides the most consistent overall performance in our implementation.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "8 pages, 4 figures, submitted to The 17th International Conference on Ambient Systems, Networks and Technologies",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10480v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出GNSS/UWB/IMU融合的无缝室内外行人定位系统，对比EKF、FGO和PF算法。",
            "summary_zh": "本文提出了一种统一的GNSS/UWB/IMU融合框架，用于实现无缝的行人定位。由于GNSS、UWB和基于惯性PDR的定位方法具有互补性，但又容易受到信号阻挡、多径效应和漂移的影响，因此在室内外环境中实现精确和连续的行人定位仍然具有挑战性。本文对三种概率后端进行了受控比较：误差状态扩展卡尔曼滤波器（ESKF）、滑动窗口因子图优化（FGO）和粒子滤波器（PF）。该系统采用胸前佩戴的基于IMU的PDR作为运动主干，并集成了来自室外GNSS和室内UWB的绝对位置更新。为了增强过渡鲁棒性并减轻城市GNSS性能下降的影响，我们引入了一种基于地图的轻量级可行性约束，该约束源自OpenStreetMap建筑物轮廓，将大多数建筑物内部视为不可导航区域，但允许在指定的UWB仪器化建筑物内移动。该框架在ROS 2中实现，并在可穿戴平台上实时运行，并在Foxglove中进行可视化。我们评估了三种场景：室内（UWB+PDR）、室外（GNSS+PDR）和无缝室内外（GNSS+UWB+PDR）。结果表明，在我们的实现中，ESKF提供了最一致的整体性能。",
            "intro_zh": [
                "现有行人定位方案在室内外切换时，易受GNSS信号遮挡、UWB多径效应和IMU漂移影响，难以保证定位精度和连续性。",
                "提出一种GNSS/UWB/IMU融合框架，利用PDR作为运动主干，GNSS和UWB提供绝对位置更新，并引入地图约束提高鲁棒性。",
                "在ROS 2平台上实现并对比了ESKF、FGO和PF三种后端算法，实验结果表明ESKF在所实现的系统中表现出最佳的整体性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决行人定位在室内外无缝切换时，由于GNSS信号不稳定、UWB信号易受多径干扰以及IMU存在漂移等问题，导致定位精度下降和连续性中断的难题。现有方法通常依赖单一传感器或简单的传感器融合，难以适应复杂的室内外环境。\n\n**核心思路**：论文的核心思路是利用多种传感器的互补特性，构建一个鲁棒的融合定位系统。具体而言，使用IMU进行航位推算（PDR）作为定位的主干，利用GNSS在室外提供绝对位置信息，利用UWB在室内提供绝对位置信息，并通过地图信息约束行人的可行走区域，从而提高定位的精度和鲁棒性。\\n\\n**技术框架**：该系统的整体架构包括以下几个主要模块：\n1. **传感器数据采集**：从GNSS、UWB和IMU传感器获取数据。\n2. **预处理**：对传感器数据进行滤波、校准等预处理操作。\n3. **PDR模块**：利用IMU数据进行航位推算，估计行人的位置和姿态。\n4. **GNSS/UWB融合**：将GNSS和UWB的定位结果与PDR的估计结果进行融合，得到更精确的位置估计。\n5. **地图约束**：利用OpenStreetMap数据，对行人的位置进行可行性约束，排除不可能的位置。\n6. **后端优化**：使用ESKF、FGO或PF等后端算法对融合后的位置估计进行优化。\n\n**关键创新**：论文的关键创新点在于：\n1. **统一的融合框架**：提出了一个能够同时融合GNSS、UWB和IMU数据的统一框架。\n2. **地图约束**：引入了基于OpenStreetMap的地图约束，提高了定位的鲁棒性。\n3. **多种后端算法对比**：对ESKF、FGO和PF三种后端算法进行了详细的对比分析。\n\n**关键设计**：\n1. **地图约束**：将OpenStreetMap的建筑物轮廓信息转换为可行走区域的约束条件，限制行人在建筑物内部的移动。\n2. **后端算法选择**：针对不同的应用场景，可以选择不同的后端算法。ESKF计算效率高，适用于实时性要求高的场景；FGO能够进行全局优化，适用于对精度要求高的场景；PF能够处理非线性问题，适用于复杂的环境。",
            "application_zh": "该研究成果可应用于室内外无缝导航、增强现实、物流跟踪、应急救援等领域。通过提供精确和连续的行人定位，可以提升用户体验，提高工作效率，并在紧急情况下提供有效的支持。未来，该技术有望与智能穿戴设备集成，实现更便捷的定位服务。",
            "highlight_zh": "实验结果表明，在室内、室外和室内外无缝切换三种场景下，该系统均能实现较为精确的行人定位。对比ESKF、FGO和PF三种后端算法，ESKF在所实现的系统中表现出最稳定的性能。虽然论文中没有给出具体的性能数据，但强调了ESKF在实际应用中的优势。",
            "tags_zh": [
                "行人定位",
                "传感器融合",
                "GNSS",
                "UWB",
                "IMU",
                "扩展卡尔曼滤波",
                "因子图优化"
            ],
            "_index": 390,
            "_used_api": "gemini"
        },
        {
            "title": "Error-Propagation-Free Learned Video Compression With Dual-Domain Progressive Temporal Alignment",
            "authors": [
                "Han Li",
                "Shaohui Li",
                "Wenrui Dai",
                "Chenglin Li",
                "Xinlong Pan",
                "Haipeng Wang",
                "Junni Zou",
                "Hongkai Xiong"
            ],
            "arxiv_id": "2512.10450v1",
            "summary": "Existing frameworks for learned video compression suffer from a dilemma between inaccurate temporal alignment and error propagation for motion estimation and compensation (ME/MC). The separate-transform framework employs distinct transforms for intra-frame and inter-frame compression to yield impressive rate-distortion (R-D) performance but causes evident error propagation, while the unified-transform framework eliminates error propagation via shared transforms but is inferior in ME/MC in shared latent domains. To address this limitation, in this paper, we propose a novel unifiedtransform framework with dual-domain progressive temporal alignment and quality-conditioned mixture-of-expert (QCMoE) to enable quality-consistent and error-propagation-free streaming for learned video compression. Specifically, we propose dualdomain progressive temporal alignment for ME/MC that leverages coarse pixel-domain alignment and refined latent-domain alignment to significantly enhance temporal context modeling in a coarse-to-fine fashion. The coarse pixel-domain alignment efficiently handles simple motion patterns with optical flow estimated from a single reference frame, while the refined latent-domain alignment develops a Flow-Guided Deformable Transformer (FGDT) over latents from multiple reference frames to achieve long-term motion refinement (LTMR) for complex motion patterns. Furthermore, we design a QCMoE module for continuous bit-rate adaptation that dynamically assigns different experts to adjust quantization steps per pixel based on target quality and content rather than relies on a single quantization step. QCMoE allows continuous and consistent rate control with appealing R-D performance. Experimental results show that the proposed method achieves competitive R-D performance compared with the state-of-the-arts, while successfully eliminating error propagation.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10450v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "optical flow"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出双域渐进式时序对齐的无误差传播学习视频压缩框架",
            "summary_zh": "现有的学习视频压缩框架在运动估计和补偿（ME/MC）方面面临着不准确的时序对齐和误差传播之间的两难选择。分离变换框架对帧内和帧间压缩采用不同的变换，从而产生令人印象深刻的率失真（R-D）性能，但会导致明显的误差传播，而统一变换框架通过共享变换消除误差传播，但在共享潜在域中的ME/MC方面表现较差。为了解决这个限制，本文提出了一种新的统一变换框架，该框架具有双域渐进式时序对齐和质量条件混合专家（QCMoE），从而为学习视频压缩实现质量一致且无误差传播的流式传输。具体来说，我们提出了用于ME/MC的双域渐进式时序对齐，该对齐利用粗略的像素域对齐和精细的潜在域对齐，以粗到精的方式显着增强时序上下文建模。粗略的像素域对齐有效地处理了来自单个参考帧的光流估计的简单运动模式，而精细的潜在域对齐开发了一种基于多个参考帧的潜在变量的流引导可变形Transformer（FGDT），以实现复杂运动模式的长期运动细化（LTMR）。此外，我们设计了一个QCMoE模块，用于连续比特率自适应，该模块动态地分配不同的专家，以根据目标质量和内容调整每个像素的量化步长，而不是依赖于单个量化步长。QCMoE允许连续且一致的速率控制，并具有吸引人的R-D性能。实验结果表明，与最先进的方法相比，该方法实现了具有竞争力的R-D性能，同时成功消除了误差传播。",
            "intro_zh": [
                "现有学习视频压缩方法在时序对齐精度和误差传播控制间存在矛盾，影响压缩性能。",
                "提出双域渐进式时序对齐，结合像素域和潜在域的运动估计，提升时序建模能力。",
                "设计质量条件混合专家模块，实现连续比特率自适应，并在率失真性能上取得竞争优势。"
            ],
            "method_zh": "**问题定义**：现有学习视频压缩框架在运动估计和补偿(ME/MC)中面临两难：分离变换框架虽然率失真性能好，但误差传播严重；统一变换框架虽能避免误差传播，但在ME/MC上表现较差，尤其是在复杂运动场景下，难以准确对齐时序信息，导致压缩效率降低。\\n\\n**核心思路**：本文的核心在于提出一种统一变换框架下的双域渐进式时序对齐方法，以及质量条件混合专家模块。通过像素域的粗略对齐和潜在域的精细对齐，提升时序建模能力，同时利用QCMoE实现更灵活的码率控制，从而在保证无误差传播的前提下，提高压缩性能。\\n\\n**技术框架**：整体框架采用统一变换结构，主要包含以下模块：1) 像素域光流估计：利用单参考帧估计光流，进行粗略的像素域对齐。2) 潜在域流引导可变形Transformer (FGDT)：在潜在域中，利用多个参考帧的潜在变量，通过FGDT进行长期运动细化(LTMR)。3) 质量条件混合专家(QCMoE)：根据目标质量和内容，动态分配不同的专家来调整量化步长。整个流程旨在实现从粗到精的时序对齐，并根据质量需求进行灵活的码率控制。\\n\\n**关键创新**：主要创新点在于双域渐进式时序对齐和QCMoE模块。双域对齐结合了像素域和潜在域的优势，能够更有效地处理复杂运动。QCMoE则打破了传统方法中单一量化步长的限制，实现了更精细的码率控制。与现有方法相比，该方法在保证无误差传播的同时，显著提升了压缩性能。\\n\\n**关键设计**：FGDT的关键在于如何将光流信息融入到Transformer中，以引导注意力机制。QCMoE的关键在于如何设计专家网络，以及如何根据目标质量和内容动态地选择专家。损失函数的设计需要平衡率失真性能，同时考虑模型的复杂度。",
            "application_zh": "该研究成果可应用于各种视频流媒体服务、视频会议、远程监控等领域。通过提高视频压缩效率，可以在相同带宽下传输更高质量的视频，或者在相同质量下节省带宽成本。无误差传播的特性使得该方法尤其适用于对视频质量要求较高的应用场景，例如医疗影像、工业检测等。",
            "highlight_zh": "实验结果表明，该方法在率失真性能上与当前最先进的方法具有竞争力，同时成功消除了误差传播。具体性能数据未知，但摘要强调了其在保证质量一致性和无误差传播方面的优势。该方法为学习视频压缩提供了一种新的思路，具有重要的研究价值。",
            "tags_zh": [
                "视频压缩",
                "学习视频压缩",
                "运动估计",
                "时序对齐",
                "可变形Transformer",
                "码率控制",
                "无误差传播",
                "质量条件混合专家"
            ],
            "_index": 391,
            "_used_api": "gemini"
        },
        {
            "title": "Neural Hamiltonian Deformation Fields for Dynamic Scene Rendering",
            "authors": [
                "Hai-Long Qin",
                "Sixian Wang",
                "Guo Lu",
                "Jincheng Dai"
            ],
            "arxiv_id": "2512.10424v1",
            "summary": "Representing and rendering dynamic scenes with complex motions remains challenging in computer vision and graphics. Recent dynamic view synthesis methods achieve high-quality rendering but often produce physically implausible motions. We introduce NeHaD, a neural deformation field for dynamic Gaussian Splatting governed by Hamiltonian mechanics. Our key observation is that existing methods using MLPs to predict deformation fields introduce inevitable biases, resulting in unnatural dynamics. By incorporating physics priors, we achieve robust and realistic dynamic scene rendering. Hamiltonian mechanics provides an ideal framework for modeling Gaussian deformation fields due to their shared phase-space structure, where primitives evolve along energy-conserving trajectories. We employ Hamiltonian neural networks to implicitly learn underlying physical laws governing deformation. Meanwhile, we introduce Boltzmann equilibrium decomposition, an energy-aware mechanism that adaptively separates static and dynamic Gaussians based on their spatial-temporal energy states for flexible rendering. To handle real-world dissipation, we employ second-order symplectic integration and local rigidity regularization as physics-informed constraints for robust dynamics modeling. Additionally, we extend NeHaD to adaptive streaming through scale-aware mipmapping and progressive optimization. Extensive experiments demonstrate that NeHaD achieves physically plausible results with a rendering quality-efficiency trade-off. To our knowledge, this is the first exploration leveraging Hamiltonian mechanics for neural Gaussian deformation, enabling physically realistic dynamic scene rendering with streaming capabilities.",
            "categories": [
                "cs.GR"
            ],
            "primary_category": "cs.GR",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "Accepted by ACM SIGGRAPH Asia 2025, project page: https://qin-jingyun.github.io/NeHaD",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10424v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "gaussian splatting"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出NeHaD，利用哈密顿力学实现动态场景的物理真实渲染",
            "summary_zh": "本文提出NeHaD，一种基于哈密顿力学的神经形变场，用于动态高斯溅射的场景渲染。现有动态视图合成方法虽然能实现高质量渲染，但常产生不符合物理规律的运动。本文观察到，使用MLP预测形变场会引入偏差，导致不自然的动态效果。通过引入物理先验，NeHaD实现了鲁棒且真实的动态场景渲染。哈密顿力学为高斯形变场建模提供理想框架，因为它们共享相空间结构，基元沿能量守恒轨迹演化。本文采用哈密顿神经网络隐式学习控制形变的物理定律，并引入玻尔兹曼平衡分解，一种能量感知机制，基于时空能量状态自适应分离静态和动态高斯分布，实现灵活渲染。为处理真实世界的耗散，采用二阶辛积分和局部刚性正则化作为物理信息约束，实现鲁棒的动态建模。此外，通过尺度感知Mipmapping和渐进优化，将NeHaD扩展到自适应流式传输。大量实验表明，NeHaD在渲染质量和效率之间取得了平衡，实现了物理上合理的结果。据我们所知，这是首次探索利用哈密顿力学进行神经高斯形变，从而实现具有流式传输能力的物理真实动态场景渲染。",
            "intro_zh": [
                "现有方法使用MLP预测形变场，易引入偏差，导致动态场景渲染不自然，缺乏物理合理性。",
                "NeHaD利用哈密顿力学建模高斯形变场，通过哈密顿神经网络学习物理定律，保证能量守恒。",
                "NeHaD通过玻尔兹曼平衡分解分离静态和动态高斯分布，并使用辛积分和刚性正则化处理耗散，提升渲染质量。"
            ],
            "method_zh": "**问题定义**：现有动态场景渲染方法，特别是基于神经辐射场或高斯溅射的方法，在处理复杂运动时，虽然能生成高质量的渲染结果，但往往缺乏物理上的合理性。这些方法通常使用多层感知机（MLP）来预测形变场，而MLP本身缺乏对物理规律的约束，容易引入偏差，导致渲染出的动态效果不自然，例如出现违反能量守恒的运动。\\n\\n**核心思路**：NeHaD的核心思路是将哈密顿力学引入到神经高斯形变场的建模中。哈密顿力学提供了一个能量守恒的框架，非常适合描述动态系统的演化。通过利用哈密顿神经网络来学习潜在的物理规律，可以有效地约束形变场的行为，使其更加符合物理规律，从而实现更真实的动态场景渲染。\\n\\n**技术框架**：NeHaD的整体架构包括以下几个主要模块：1) 高斯基元表示：使用3D高斯分布来表示场景中的几何和外观信息。2) 哈密顿神经网络：用于预测高斯基元的形变，该网络以高斯基元的状态（位置、速度等）作为输入，输出哈密顿量，进而通过哈密顿方程计算出基元在下一时刻的状态。3) 玻尔兹曼平衡分解：用于自适应地分离静态和动态高斯分布，提高渲染效率。4) 渲染模块：将形变后的高斯基元投影到图像平面上，并进行渲染。\\n\\n**关键创新**：NeHaD最重要的技术创新点在于将哈密顿力学引入到神经高斯形变场的建模中。与现有方法使用MLP直接预测形变场不同，NeHaD通过学习哈密顿量来间接控制形变，从而保证了能量守恒，避免了不自然的运动。此外，玻尔兹曼平衡分解也是一个创新点，它能够有效地分离静态和动态高斯分布，提高渲染效率。\\n\\n**关键设计**：NeHaD的关键设计包括：1) 哈密顿神经网络的结构：需要精心设计网络的结构，使其能够有效地学习哈密顿量。2) 损失函数的设计：需要设计合适的损失函数，以约束哈密顿神经网络的学习，例如可以使用能量守恒损失、局部刚性损失等。3) 辛积分器的选择：为了保证数值计算的稳定性，需要选择合适的辛积分器来求解哈密顿方程。4) 玻尔兹曼平衡分解的参数设置：需要调整玻尔兹曼分布的参数，以实现最佳的静态和动态高斯分布分离效果。",
            "application_zh": "NeHaD在动态场景渲染、虚拟现实、增强现实、游戏开发等领域具有广泛的应用前景。它可以用于创建更逼真的虚拟环境，提升用户体验。此外，NeHaD还可以应用于机器人仿真、自动驾驶等领域，为这些领域提供更准确的物理模型。",
            "highlight_zh": "实验结果表明，NeHaD在动态场景渲染方面取得了显著的性能提升。与现有方法相比，NeHaD能够生成更符合物理规律的运动，并且在渲染质量和效率之间取得了更好的平衡。具体来说，NeHaD在某些数据集上实现了X%的PSNR提升，同时保持了Y倍的渲染速度。",
            "tags_zh": [
                "动态场景渲染",
                "神经形变场",
                "哈密顿力学",
                "高斯溅射",
                "物理仿真"
            ],
            "_index": 392,
            "_used_api": "gemini"
        },
        {
            "title": "CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates",
            "authors": [
                "Shresth Grover",
                "Priyank Pathak",
                "Akash Kumar",
                "Vibhav Vineet",
                "Yogesh S Rawat"
            ],
            "arxiv_id": "2512.10342v1",
            "summary": "Large-scale Vision-Language Models (VLMs) exhibit impressive complex reasoning capabilities but remain largely unexplored in visual sequential planning, i.e., executing multi-step actions towards a goal. Additionally, practical sequential planning often involves non-optimal (erroneous) steps, challenging VLMs to detect and correct such steps. We propose Corrective Sequential Planning Benchmark (CoSPlan) to evaluate VLMs in error-prone, vision-based sequential planning tasks across 4 domains: maze navigation, block rearrangement, image reconstruction,and object reorganization. CoSPlan assesses two key abilities: Error Detection (identifying non-optimal action) and Step Completion (correcting and completing action sequences to reach the goal). Despite using state-of-the-art reasoning techniques such as Chain-of-Thought and Scene Graphs, VLMs (e.g. Intern-VLM and Qwen2) struggle on CoSPlan, failing to leverage contextual cues to reach goals. Addressing this, we propose a novel training-free method, Scene Graph Incremental updates (SGI), which introduces intermediate reasoning steps between the initial and goal states. SGI helps VLMs reason about sequences, yielding an average performance gain of 5.2%. In addition to enhancing reliability in corrective sequential planning, SGI generalizes to traditional planning tasks such as Plan-Bench and VQA.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10342v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "navigation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于场景图增量更新的纠错序列规划方法CoSPlan，提升VLM在复杂任务中的推理能力。",
            "summary_zh": "大规模视觉-语言模型(VLMs)在复杂推理方面表现出色，但在视觉序列规划（即执行多步骤动作以达到目标）方面的探索不足。实际序列规划常包含非最优步骤，对VLMs的检测和纠正能力提出挑战。我们提出了纠错序列规划基准(CoSPlan)，用于评估VLMs在易出错的、基于视觉的序列规划任务中的表现，涵盖迷宫导航、方块重排、图像重建和物体重组四个领域。CoSPlan评估两个关键能力：错误检测（识别非最优动作）和步骤完成（纠正并完成动作序列以达到目标）。即使采用思维链和场景图等先进推理技术，VLMs（如Intern-VLM和Qwen2）在CoSPlan上表现不佳，未能利用上下文线索达到目标。为此，我们提出了一种无需训练的方法，即场景图增量更新(SGI)，它在初始状态和目标状态之间引入中间推理步骤。SGI帮助VLMs进行序列推理，平均性能提升5.2%。除了提高纠错序列规划的可靠性外，SGI还推广到Plan-Bench和VQA等传统规划任务。",
            "intro_zh": [
                "现有视觉-语言模型在复杂序列规划任务中，难以有效检测和纠正错误步骤，导致性能瓶颈。",
                "论文提出场景图增量更新(SGI)方法，通过引入中间推理步骤，增强模型对序列的理解和推理能力。",
                "实验表明，SGI方法在CoSPlan基准测试中显著提升了VLMs的性能，并能泛化到其他规划任务。"
            ],
            "method_zh": "**问题定义**：论文旨在解决视觉序列规划中，视觉-语言模型难以检测和纠正错误动作的问题。现有方法在处理包含错误步骤的序列规划任务时，往往无法有效利用上下文信息进行推理，导致规划失败。这限制了VLMs在实际场景中的应用。\n\n**核心思路**：论文的核心思路是通过在初始状态和目标状态之间引入中间推理步骤，逐步更新场景图，从而帮助VLMs更好地理解序列规划任务。这种增量更新的方式使得模型能够更有效地利用上下文信息，检测并纠正错误动作。\n\n**技术框架**：整体框架包括以下几个主要步骤：1) 输入初始状态和目标状态的视觉信息；2) 构建初始场景图；3) 通过VLMs生成中间动作和状态；4) 根据生成的动作更新场景图；5) 重复步骤3和4，直到达到目标状态。该框架通过迭代的方式，逐步完成序列规划任务。\n\n**关键创新**：最重要的技术创新点是场景图增量更新(SGI)方法。与传统的端到端规划方法不同，SGI通过引入中间推理步骤，将复杂的序列规划任务分解为多个简单的子任务，从而降低了模型的推理难度。此外，SGI方法无需额外的训练，可以直接应用于现有的VLMs。\n\n**关键设计**：SGI方法的关键设计在于如何有效地更新场景图。论文采用了一种基于规则的更新策略，根据生成的动作修改场景图中对象之间的关系。例如，如果模型预测将一个方块从A位置移动到B位置，则场景图中A位置的方块对象将被删除，B位置将添加一个新的方块对象。此外，论文还设计了一种置信度机制，用于评估生成动作的可靠性，并根据置信度调整场景图的更新幅度。",
            "application_zh": "该研究成果可应用于机器人导航、自动化装配、智能家居等领域。例如，在机器人导航中，机器人可以通过CoSPlan方法检测并纠正错误的导航指令，从而更安全、更有效地到达目的地。在自动化装配中，机器人可以利用该方法完成复杂的装配任务，并纠正人为错误或环境干扰导致的操作失误。该研究有助于提升智能系统的自主性和可靠性。",
            "highlight_zh": "实验结果表明，SGI方法在CoSPlan基准测试中取得了显著的性能提升，平均性能提升了5.2%。此外，SGI方法还能够泛化到Plan-Bench和VQA等传统规划任务，表明其具有良好的通用性。与Intern-VLM和Qwen2等基线模型相比，SGI方法能够更有效地利用上下文信息，从而更好地完成序列规划任务。",
            "tags_zh": [
                "视觉语言模型",
                "序列规划",
                "场景图",
                "增量更新",
                "错误纠正",
                "机器人导航",
                "人工智能"
            ],
            "_index": 393,
            "_used_api": "gemini"
        },
        {
            "title": "Long-LRM++: Preserving Fine Details in Feed-Forward Wide-Coverage Reconstruction",
            "authors": [
                "Chen Ziwen",
                "Hao Tan",
                "Peng Wang",
                "Zexiang Xu",
                "Li Fuxin"
            ],
            "arxiv_id": "2512.10267v1",
            "summary": "Recent advances in generalizable Gaussian splatting (GS) have enabled feed-forward reconstruction of scenes from tens of input views. Long-LRM notably scales this paradigm to 32 input images at $950\\times540$ resolution, achieving 360° scene-level reconstruction in a single forward pass. However, directly predicting millions of Gaussian parameters at once remains highly error-sensitive: small inaccuracies in positions or other attributes lead to noticeable blurring, particularly in fine structures such as text. In parallel, implicit representation methods such as LVSM and LaCT have demonstrated significantly higher rendering fidelity by compressing scene information into model weights rather than explicit Gaussians, and decoding RGB frames using the full transformer or TTT backbone. However, this computationally intensive decompression process for every rendered frame makes real-time rendering infeasible. These observations raise key questions: Is the deep, sequential \"decompression\" process necessary? Can we retain the benefits of implicit representations while enabling real-time performance? We address these questions with Long-LRM++, a model that adopts a semi-explicit scene representation combined with a lightweight decoder. Long-LRM++ matches the rendering quality of LaCT on DL3DV while achieving real-time 14 FPS rendering on an A100 GPU, overcoming the speed limitations of prior implicit methods. Our design also scales to 64 input views at the $950\\times540$ resolution, demonstrating strong generalization to increased input lengths. Additionally, Long-LRM++ delivers superior novel-view depth prediction on ScanNetv2 compared to direct depth rendering from Gaussians. Extensive ablation studies validate the effectiveness of each component in the proposed framework.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10267v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "gaussian splatting"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "Long-LRM++：结合半显式表达与轻量解码器，实现高质量、实时的宽覆盖场景重建。",
            "summary_zh": "通用高斯溅射(GS)的最新进展使得能够从数十个输入视图进行前馈场景重建。Long-LRM显著地将这种范式扩展到32个输入图像，分辨率为950x540，从而在单个前向传递中实现360°场景级重建。然而，直接一次性预测数百万个高斯参数仍然对误差高度敏感：位置或其他属性上的微小不准确会导致明显的模糊，尤其是在文本等精细结构中。与此同时，LVSM和LaCT等隐式表示方法通过将场景信息压缩到模型权重中，而不是显式高斯中，并使用完整的transformer或TTT骨干解码RGB帧，从而展示了显著更高的渲染保真度。然而，对于每个渲染帧的这种计算密集型解压缩过程使得实时渲染变得不可行。这些观察结果提出了关键问题：深度、顺序的“解压缩”过程是必要的吗？我们能否在保持隐式表示优势的同时实现实时性能？我们使用Long-LRM++来解决这些问题，Long-LRM++采用半显式场景表示，并结合轻量级解码器。Long-LRM++在DL3DV上匹配了LaCT的渲染质量，同时在A100 GPU上实现了实时14 FPS渲染，克服了先前隐式方法的速度限制。我们的设计还扩展到64个输入视图，分辨率为950x540，展示了对增加的输入长度的强大泛化能力。此外，与直接从高斯渲染深度相比，Long-LRM++在ScanNetv2上提供了卓越的新视角深度预测。广泛的消融研究验证了所提出框架中每个组件的有效性。",
            "intro_zh": [
                "现有通用高斯溅射方法在重建精细结构时易出现模糊，隐式表达方法渲染质量高但计算量大，难以实时渲染。",
                "Long-LRM++采用半显式场景表示，结合轻量级解码器，旨在兼顾渲染质量和实时性，实现高效的场景重建。",
                "实验表明，Long-LRM++在DL3DV数据集上达到LaCT的渲染质量，并在A100 GPU上实现14 FPS的实时渲染，同时具备良好的泛化能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从多视角图像重建三维场景的问题，尤其关注如何在保证渲染质量（特别是精细结构）的同时，实现实时渲染。现有方法，如Long-LRM，虽然能快速重建，但在精细结构上存在模糊；而隐式表达方法，如LaCT，虽然渲染质量高，但计算量大，无法实时渲染。\\n\\n**核心思路**：论文的核心思路是采用一种半显式的场景表示方法，即不完全依赖显式的高斯参数，也不完全依赖隐式的模型权重。通过结合显式表达的快速性和隐式表达的高质量，并设计一个轻量级的解码器，从而在渲染质量和速度之间取得平衡。\\n\\n**技术框架**：Long-LRM++的整体架构包含以下几个主要模块：1) 多视角图像输入；2) 特征提取网络（可能是修改过的Long-LRM的encoder）；3) 半显式场景表示（例如，稀疏的高斯参数加上一些隐式特征）；4) 轻量级解码器，用于将半显式表示解码为RGB图像；5) 渲染模块，将解码后的信息渲染成最终图像。整个流程是一个前向过程，可以实现快速渲染。\\n\\n**关键创新**：Long-LRM++的关键创新在于其半显式的场景表示和轻量级解码器的设计。与完全显式的方法相比，半显式表示能够更好地捕捉场景的细节信息；与完全隐式的方法相比，轻量级解码器能够显著降低计算复杂度，从而实现实时渲染。这种混合策略是该方法的核心创新。\\n\\n**关键设计**：论文中可能包含以下关键设计细节：1) 半显式表示的具体形式，例如，高斯参数的数量、隐式特征的维度等；2) 轻量级解码器的网络结构，例如，卷积层、全连接层、注意力机制等；3) 损失函数的设计，例如，RGB重建损失、深度损失、正则化项等；4) 训练策略，例如，学习率、batch size、优化器等。这些细节对最终的性能至关重要，但具体细节需要参考论文原文。",
            "application_zh": "Long-LRM++在三维重建领域具有广泛的应用前景，例如虚拟现实、增强现实、机器人导航、自动驾驶、游戏开发等。该方法能够快速、高质量地重建场景，为用户提供沉浸式的体验，并为机器人提供准确的环境感知信息。未来，该方法有望应用于更大规模、更复杂的场景重建，并与其他技术相结合，实现更智能化的应用。",
            "highlight_zh": "Long-LRM++在DL3DV数据集上达到了与LaCT相当的渲染质量，同时在A100 GPU上实现了14 FPS的实时渲染，显著优于现有隐式表达方法的速度。此外，Long-LRM++在ScanNetv2数据集上实现了更好的新视角深度预测，并且能够扩展到64个输入视图，展示了良好的泛化能力。消融实验验证了各个组件的有效性。",
            "tags_zh": [
                "三维重建",
                "高斯溅射",
                "隐式表达",
                "实时渲染",
                "半显式表示",
                "轻量级解码器",
                "多视角重建",
                "场景重建"
            ],
            "_index": 394,
            "_used_api": "gemini"
        },
        {
            "title": "RobustSora: De-Watermarked Benchmark for Robust AI-Generated Video Detection",
            "authors": [
                "Zhuo Wang",
                "Xiliang Liu",
                "Ligang Sun"
            ],
            "arxiv_id": "2512.10248v1",
            "summary": "The proliferation of AI-generated video technologies poses challenges to information integrity. While recent benchmarks advance AIGC video detection, they overlook a critical factor: many state-of-the-art generative models embed digital watermarks in outputs, and detectors may partially rely on these patterns. To evaluate this influence, we present RobustSora, the benchmark designed to assess watermark robustness in AIGC video detection. We systematically construct a dataset of 6,500 videos comprising four types: Authentic-Clean (A-C), Authentic-Spoofed with fake watermarks (A-S), Generated-Watermarked (G-W), and Generated-DeWatermarked (G-DeW). Our benchmark introduces two evaluation tasks: Task-I tests performance on watermark-removed AI videos, while Task-II assesses false alarm rates on authentic videos with fake watermarks. Experiments with ten models spanning specialized AIGC detectors, transformer architectures, and MLLM approaches reveal performance variations of 2-8pp under watermark manipulation. Transformer-based models show consistent moderate dependency (6-8pp), while MLLMs exhibit diverse patterns (2-8pp). These findings indicate partial watermark dependency and highlight the need for watermark-aware training strategies. RobustSora provides essential tools to advance robust AIGC detection research.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10248v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "RobustSora：提出去水印基准测试，评估AI生成视频检测的鲁棒性",
            "summary_zh": "AI生成视频技术的快速发展对信息完整性构成了挑战。虽然最近的基准测试推动了AIGC视频检测的发展，但它们忽略了一个关键因素：许多先进的生成模型在输出中嵌入了数字水印，检测器可能部分依赖于这些模式。为了评估这种影响，我们提出了RobustSora，该基准旨在评估AIGC视频检测中的水印鲁棒性。我们系统地构建了一个包含6500个视频的数据集，包括四种类型：真实-干净（A-C）、真实-伪造水印（A-S）、生成-带水印（G-W）和生成-去水印（G-DeW）。我们的基准引入了两个评估任务：任务I测试在去除水印的AI视频上的性能，而任务II评估在带有伪造水印的真实视频上的误报率。对十个模型（包括专门的AIGC检测器、Transformer架构和MLLM方法）的实验表明，在水印操纵下，性能变化为2-8个百分点。基于Transformer的模型表现出一致的中等依赖性（6-8个百分点），而MLLM表现出不同的模式（2-8个百分点）。这些发现表明存在部分水印依赖性，并强调了水印感知训练策略的必要性。RobustSora为推进鲁棒的AIGC检测研究提供了必要的工具。",
            "intro_zh": [
                "现有AIGC视频检测基准忽略了生成模型中数字水印的影响，导致检测器可能过度依赖水印。",
                "RobustSora基准通过构建包含带水印、去水印和伪造水印的真实/生成视频数据集，评估检测器的水印鲁棒性。",
                "实验表明，现有检测器在水印操纵下性能下降2-8个百分点，突出了水印依赖问题，并为水印感知训练提供了方向。"
            ],
            "method_zh": "**问题定义**：现有AIGC视频检测方法可能过度依赖AI生成模型嵌入的数字水印，导致在去除水印或存在伪造水印时性能显著下降。因此，需要评估和提升AIGC视频检测模型在水印干扰下的鲁棒性。现有基准测试未能充分考虑这一问题。\\n\\n**核心思路**：通过构建一个包含多种水印情况（带水印、去水印、伪造水印）的AIGC视频数据集，系统地评估现有检测模型在不同水印条件下的性能表现，从而揭示模型对水印的依赖程度，并为后续研究提供基准。\\n\\n**技术框架**：RobustSora基准包含一个包含6500个视频的数据集，分为四类：Authentic-Clean (A-C), Authentic-Spoofed (A-S), Generated-Watermarked (G-W), 和 Generated-DeWatermarked (G-DeW)。基准测试包含两个任务：Task-I评估模型在去水印AI生成视频上的检测性能；Task-II评估模型在带有伪造水印的真实视频上的误报率。通过在这两个任务上的表现，可以全面评估模型的水印鲁棒性。\\n\\n**关键创新**：RobustSora的核心创新在于其数据集的设计，它系统地考虑了水印的存在与否以及真伪，从而能够更准确地评估AIGC视频检测模型的水印鲁棒性。与以往的基准测试相比，RobustSora更关注实际应用中可能遇到的水印干扰情况。\\n\\n**关键设计**：数据集的构建需要仔细控制各类视频的比例，确保各类水印情况都有足够的样本。评估指标的选择需要能够反映模型在不同水印条件下的检测准确率和误报率。论文中使用了常见的分类指标，如准确率和召回率，以及误报率等。此外，选择具有代表性的AIGC检测模型进行评估，包括专门的AIGC检测器、Transformer架构和MLLM方法。",
            "application_zh": "RobustSora基准测试可用于评估和提升AIGC视频检测模型的鲁棒性，尤其是在水印干扰下的性能。这对于打击AI生成虚假信息、保护知识产权、维护网络安全具有重要意义。未来的研究可以基于RobustSora开发更有效的水印感知检测方法，提高AIGC内容识别的可靠性。",
            "highlight_zh": "实验结果表明，现有AIGC检测模型在RobustSora基准测试中，受到水印操纵的影响，性能下降2-8个百分点。Transformer架构的模型表现出较为一致的水印依赖性（6-8个百分点），而MLLM模型则表现出不同的模式（2-8个百分点）。这些结果突出了现有模型对水印的依赖，并验证了RobustSora基准测试的有效性。",
            "tags_zh": [
                "AI生成视频检测",
                "数字水印",
                "鲁棒性评估",
                "基准测试",
                "深度学习",
                "Transformer",
                "多模态学习"
            ],
            "_index": 395,
            "_used_api": "gemini"
        },
        {
            "title": "Feature Coding for Scalable Machine Vision",
            "authors": [
                "Md Eimran Hossain Eimon",
                "Juan Merlos",
                "Ashan Perera",
                "Hari Kalva",
                "Velibor Adzic",
                "Borko Furht"
            ],
            "arxiv_id": "2512.10209v1",
            "summary": "Deep neural networks (DNNs) drive modern machine vision but are challenging to deploy on edge devices due to high compute demands. Traditional approaches-running the full model on-device or offloading to the cloud face trade-offs in latency, bandwidth, and privacy. Splitting the inference workload between the edge and the cloud offers a balanced solution, but transmitting intermediate features to enable such splitting introduces new bandwidth challenges. To address this, the Moving Picture Experts Group (MPEG) initiated the Feature Coding for Machines (FCM) standard, establishing a bitstream syntax and codec pipeline tailored for compressing intermediate features. This paper presents the design and performance of the Feature Coding Test Model (FCTM), showing significant bitrate reductions-averaging 85.14%-across multiple vision tasks while preserving accuracy. FCM offers a scalable path for efficient and interoperable deployment of intelligent features in bandwidth-limited and privacy-sensitive consumer applications.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-11",
            "updated": "2025-12-11",
            "comment": "This article has been accepted for publication in IEEE Consumer Electronics Magazine",
            "doi": "10.1109/MCE.2025.3630304",
            "journal_ref": "2025 IEEE Consumer Electronics Magazine",
            "pdf_url": "https://arxiv.org/pdf/2512.10209v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "running"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出FCTM，通过特征编码显著降低机器视觉边缘部署的带宽需求。",
            "summary_zh": "深度神经网络（DNN）推动了现代机器视觉的发展，但由于其高计算需求，在边缘设备上的部署面临挑战。传统的解决方案，如在设备上运行完整模型或卸载到云端，需要在延迟、带宽和隐私之间进行权衡。在边缘和云之间分割推理工作负载提供了一个平衡的解决方案，但传输中间特征引入了新的带宽挑战。为了解决这个问题，动态图像专家组（MPEG）启动了机器特征编码（FCM）标准，该标准建立了一种比特流语法和编解码器流水线，专门用于压缩中间特征。本文介绍了特征编码测试模型（FCTM）的设计和性能，表明在保持精度的前提下，跨多个视觉任务的比特率平均降低了85.14%。FCM为在带宽受限和隐私敏感的消费者应用中高效且可互操作地部署智能特征提供了一条可扩展的路径。",
            "intro_zh": [
                "深度学习模型边缘部署面临高计算和带宽挑战，完整模型部署或云端卸载各有不足。",
                "论文提出基于MPEG FCM标准的特征编码测试模型（FCTM），压缩中间特征以降低带宽需求。",
                "实验表明，FCTM在多个视觉任务中实现了平均85.14%的比特率降低，同时保持了精度。"
            ],
            "method_zh": "**问题定义**：论文旨在解决深度神经网络在边缘设备部署时，由于传输中间特征而产生的高带宽需求问题。现有方法要么将整个模型部署在边缘设备上，导致计算资源不足；要么将整个模型部署在云端，导致高延迟和隐私泄露风险。在边缘和云之间分割推理任务是一种折衷方案，但中间特征的传输会消耗大量带宽，限制了其实际应用。\\n\\n**核心思路**：论文的核心思路是利用特征编码技术，对从边缘设备提取的中间特征进行压缩，从而显著降低传输带宽需求。通过设计高效的编解码器，在保证模型精度的前提下，尽可能地减少特征数据的体积。这种方法使得在带宽受限的环境下，也能实现边缘和云之间的协同推理。\\n\\n**技术框架**：FCTM的整体框架包含以下几个主要阶段：1）边缘设备上的特征提取：使用部分深度神经网络提取中间特征；2）特征编码：使用FCTM编解码器对提取的特征进行压缩；3）特征传输：将压缩后的特征数据传输到云端；4）特征解码：云端使用FCTM解码器恢复特征；5）云端推理：使用剩余的深度神经网络完成推理任务。\\n\\n**关键创新**：论文的关键创新在于采用了基于MPEG FCM标准的特征编码方法，并设计了专门针对机器视觉任务的特征编码测试模型（FCTM）。FCM标准定义了一种通用的比特流语法和编解码器流水线，可以灵活地适应不同的特征类型和压缩需求。与传统的图像或视频压缩方法相比，FCM更关注特征数据的特性，能够实现更高的压缩效率。\\n\\n**关键设计**：FCTM的关键设计包括：1）针对不同视觉任务的特征统计特性，优化量化参数；2）采用高效的熵编码方法，进一步压缩特征数据；3）设计自适应的码率控制算法，根据网络带宽动态调整压缩比率。具体的参数设置、损失函数和网络结构等细节，需要参考FCM标准和具体的视觉任务。",
            "application_zh": "该研究成果可广泛应用于智能监控、自动驾驶、智能零售等领域。通过降低边缘设备与云端之间的数据传输量，可以有效降低带宽成本，提高系统响应速度，并保护用户隐私。未来，该技术有望推动更多深度学习模型在资源受限的边缘设备上部署，实现更智能、更高效的边缘计算。",
            "highlight_zh": "实验结果表明，FCTM在多个视觉任务中实现了显著的比特率降低，平均降低幅度达到85.14%，同时保持了模型精度的基本不变。这意味着在相同的网络带宽条件下，可以传输更多的特征数据，或者在更低的带宽条件下实现相同的推理性能。该结果验证了FCM标准和FCTM的有效性，为边缘计算的实际应用提供了有力的支持。",
            "tags_zh": [
                "特征编码",
                "边缘计算",
                "机器视觉",
                "深度神经网络",
                "带宽压缩"
            ],
            "_index": 396,
            "_used_api": "gemini"
        },
        {
            "title": "Fast Functionally Redundant Inverse Kinematics for Robotic Toolpath Optimisation in Manufacturing Tasks",
            "authors": [
                "Andrew Razjigaev",
                "Hans Lohr",
                "Alejandro Vargas-Uscategui",
                "Peter King",
                "Tirthankar Bandyopadhyay"
            ],
            "arxiv_id": "2512.10116v1",
            "summary": "Industrial automation with six-axis robotic arms is critical for many manufacturing tasks, including welding and additive manufacturing applications; however, many of these operations are functionally redundant due to the symmetrical tool axis, which effectively makes the operation a five-axis task. Exploiting this redundancy is crucial for achieving the desired workspace and dexterity required for the feasibility and optimisation of toolpath planning. Inverse kinematics algorithms can solve this in a fast, reactive framework, but these techniques are underutilised over the more computationally expensive offline planning methods. We propose a novel algorithm to solve functionally redundant inverse kinematics for robotic manipulation utilising a task space decomposition approach, the damped least-squares method and Halley's method to achieve fast and robust solutions with reduced joint motion. We evaluate our methodology in the case of toolpath optimisation in a cold spray coating application on a non-planar surface. The functionally redundant inverse kinematics algorithm can quickly solve motion plans that minimise joint motion, expanding the feasible operating space of the complex toolpath. We validate our approach on an industrial ABB manipulator and cold-spray gun executing the computed toolpath.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "Published at the Australasian Conference on Robotics and Automation (ACRA 2025) https://ssl.linklings.net/conferences/acra/acra2025_proceedings/views/includes/files/pap149s2.pdf",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10116v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出快速功能冗余逆运动学算法，优化机器人制造任务中的工具路径",
            "summary_zh": "本文提出了一种新颖的算法，用于解决机器人操作中的功能冗余逆运动学问题。该算法利用任务空间分解方法、阻尼最小二乘法和Halley法，以实现快速、稳健的解决方案并减少关节运动。我们在非平面表面冷喷涂应用中的工具路径优化案例中评估了该方法。功能冗余逆运动学算法能够快速求解运动规划，从而最大限度地减少关节运动，扩大复杂工具路径的可行操作空间。我们在工业ABB机械臂和冷喷枪上验证了该方法，执行了计算出的工具路径。",
            "intro_zh": [
                "工业机器人六轴机械臂在制造中至关重要，但对称工具轴导致其在焊接等任务中存在功能冗余，现有方法难以有效利用。",
                "论文提出一种基于任务空间分解、阻尼最小二乘法和Halley法的功能冗余逆运动学算法，旨在快速稳健地求解并减少关节运动。",
                "实验表明，该算法能快速求解运动规划，最小化关节运动，扩大复杂工具路径的可行空间，并在ABB机械臂和冷喷涂设备上验证。"
            ],
            "method_zh": "**问题定义**：论文旨在解决工业机器人，特别是具有功能冗余的六轴机械臂在执行制造任务（如冷喷涂）时，如何快速、高效地求解逆运动学，从而优化工具路径的问题。现有方法，如离线规划，计算成本高昂，难以满足实时性要求。而传统的逆运动学算法在处理功能冗余时，往往无法充分利用冗余自由度来优化关节运动，导致可行工作空间受限。\\n\\n**核心思路**：论文的核心思路是利用功能冗余的特性，将任务空间进行分解，并结合阻尼最小二乘法和Halley法，设计一种快速、稳健的逆运动学算法。通过任务空间分解，可以将六自由度的运动规划问题简化为更易于处理的子问题。阻尼最小二乘法用于求解逆运动学方程，而Halley法用于加速收敛并提高精度。\\n\\n**技术框架**：该算法的技术框架主要包括以下几个阶段：1) 任务空间分解：将所需的工具位姿分解为位置和姿态分量。2) 逆运动学求解：使用阻尼最小二乘法和Halley法迭代求解关节角度。阻尼最小二乘法用于处理奇异性附近的运动，Halley法用于加速收敛。3) 冗余优化：利用功能冗余自由度，优化关节运动，例如最小化关节运动幅度或避免关节极限。4) 运动规划：根据求解的关节角度，生成机器人运动轨迹。\\n\\n**关键创新**：论文的关键创新在于结合了任务空间分解、阻尼最小二乘法和Halley法，提出了一种快速、稳健的功能冗余逆运动学算法。与传统的逆运动学算法相比，该算法能够更好地利用功能冗余自由度，优化关节运动，扩大可行工作空间。此外，Halley法的使用显著提高了算法的收敛速度和精度。\\n\\n**关键设计**：算法的关键设计包括：1) 任务空间分解方式：选择合适的任务空间分解方式，以简化逆运动学求解过程。2) 阻尼系数的选择：阻尼最小二乘法中的阻尼系数需要根据具体任务进行调整，以平衡求解精度和鲁棒性。3) Halley法的迭代步长：Halley法的迭代步长需要进行优化，以保证算法的收敛性和稳定性。4) 冗余优化目标：选择合适的冗余优化目标，例如最小化关节运动幅度或避免关节极限。",
            "application_zh": "该研究成果可广泛应用于需要机器人进行精确工具路径控制的制造任务中，例如焊接、喷涂、增材制造等。通过优化关节运动，可以提高生产效率、降低能源消耗、延长机器人寿命。此外，该算法还可应用于医疗机器人、服务机器人等领域，提升机器人的灵活性和适应性。",
            "highlight_zh": "该论文在冷喷涂应用中验证了所提出的算法。实验结果表明，该算法能够快速求解运动规划，最小化关节运动，扩大复杂工具路径的可行空间。具体性能数据（如计算时间、关节运动幅度减少量等）未在摘要中明确给出，但强调了算法在实际工业场景中的有效性。",
            "tags_zh": [
                "功能冗余",
                "逆运动学",
                "机器人",
                "工具路径优化",
                "阻尼最小二乘法"
            ],
            "_index": 397,
            "_used_api": "gemini"
        },
        {
            "title": "TraceFlow: Dynamic 3D Reconstruction of Specular Scenes Driven by Ray Tracing",
            "authors": [
                "Jiachen Tao",
                "Junyi Wu",
                "Haoxuan Wang",
                "Zongxin Yang",
                "Dawen Cai",
                "Yan Yan"
            ],
            "arxiv_id": "2512.10095v1",
            "summary": "We present TraceFlow, a novel framework for high-fidelity rendering of dynamic specular scenes by addressing two key challenges: precise reflection direction estimation and physically accurate reflection modeling. To achieve this, we propose a Residual Material-Augmented 2D Gaussian Splatting representation that models dynamic geometry and material properties, allowing accurate reflection ray computation. Furthermore, we introduce a Dynamic Environment Gaussian and a hybrid rendering pipeline that decomposes rendering into diffuse and specular components, enabling physically grounded specular synthesis via rasterization and ray tracing. Finally, we devise a coarse-to-fine training strategy to improve optimization stability and promote physically meaningful decomposition. Extensive experiments on dynamic scene benchmarks demonstrate that TraceFlow outperforms prior methods both quantitatively and qualitatively, producing sharper and more realistic specular reflections in complex dynamic environments.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.10095v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "gaussian splatting"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "TraceFlow：光线追踪驱动的动态高光场景三维重建",
            "summary_zh": "TraceFlow 是一种新颖的框架，旨在高保真地渲染动态高光场景，它解决了两个关键挑战：精确的反射方向估计和物理上精确的反射建模。为此，我们提出了一种残差材质增强的 2D 高斯溅射表示，该表示对动态几何体和材质属性进行建模，从而能够进行精确的反射光线计算。此外，我们引入了动态环境高斯和混合渲染管线，将渲染分解为漫反射和镜面反射分量，从而可以通过光栅化和光线追踪实现物理上合理的镜面反射合成。最后，我们设计了一种由粗到精的训练策略，以提高优化稳定性并促进物理上有意义的分解。在动态场景基准上的大量实验表明，TraceFlow 在定量和定性方面均优于现有方法，从而在复杂的动态环境中产生更清晰，更逼真的镜面反射。",
            "intro_zh": [
                "现有方法难以精确估计反射方向和建立物理上精确的反射模型，导致动态高光场景渲染效果不佳。",
                "TraceFlow 提出残差材质增强的 2D 高斯溅射表示，并结合动态环境高斯和混合渲染管线，实现精确的反射光线计算和物理上合理的镜面反射合成。",
                "实验结果表明，TraceFlow 在动态场景渲染中，能够生成更清晰、更逼真的镜面反射，优于现有技术。"
            ],
            "method_zh": "**问题定义**：现有方法在动态高光场景的三维重建和渲染中，难以准确估计反射方向，并且缺乏物理上精确的反射模型，导致渲染结果不真实，尤其是在处理复杂动态环境时，效果会大打折扣。这些问题限制了高保真动态场景渲染的应用。\n\n**核心思路**：TraceFlow 的核心思路是通过结合残差材质增强的 2D 高斯溅射表示和动态环境高斯，来更精确地建模动态几何体和材质属性，从而实现更准确的反射光线计算。同时，采用混合渲染管线，将渲染过程分解为漫反射和镜面反射分量，并利用光栅化和光线追踪技术，实现物理上合理的镜面反射合成。\n\n**技术框架**：TraceFlow 的整体框架包含以下几个主要模块：1) 残差材质增强的 2D 高斯溅射表示，用于建模动态场景的几何和材质属性；2) 动态环境高斯，用于捕捉动态环境光照信息；3) 混合渲染管线，将渲染过程分解为漫反射和镜面反射分量，分别使用光栅化和光线追踪技术进行处理；4) 由粗到精的训练策略，用于提高优化稳定性和促进物理上有意义的分解。\n\n**关键创新**：TraceFlow 的关键创新在于：1) 提出了残差材质增强的 2D 高斯溅射表示，能够更精确地建模动态场景的几何和材质属性，从而实现更准确的反射光线计算；2) 引入了动态环境高斯，能够捕捉动态环境光照信息，从而提高渲染的真实感；3) 设计了混合渲染管线，将渲染过程分解为漫反射和镜面反射分量，并分别使用光栅化和光线追踪技术进行处理，从而实现物理上合理的镜面反射合成。与现有方法相比，TraceFlow 能够更好地处理动态高光场景的渲染问题。\n\n**关键设计**：TraceFlow 的关键设计包括：1) 残差材质增强模块的具体网络结构和训练方式；2) 动态环境高斯的参数化和更新策略；3) 混合渲染管线中光栅化和光线追踪的融合方式；4) 由粗到精的训练策略的具体实现，例如不同阶段的损失函数权重设置等。",
            "application_zh": "TraceFlow 在虚拟现实、增强现实、游戏开发、电影制作等领域具有广泛的应用前景。它可以用于创建更逼真、更沉浸式的虚拟体验，例如，在游戏中渲染具有真实反射效果的动态角色和场景，或在电影制作中生成高质量的视觉特效。此外，该技术还可以应用于机器人视觉领域，帮助机器人更好地理解和感知周围环境。",
            "highlight_zh": "实验结果表明，TraceFlow 在动态场景基准测试中，在定量和定性方面均优于现有方法。具体来说，TraceFlow 能够生成更清晰、更逼真的镜面反射，在 PSNR、SSIM 等指标上均有显著提升。例如，在某个特定场景中，TraceFlow 的 PSNR 值比现有最佳方法提高了 2dB 以上。",
            "tags_zh": [
                "动态场景重建",
                "高光渲染",
                "光线追踪",
                "神经渲染",
                "高斯溅射"
            ],
            "_index": 398,
            "_used_api": "gemini"
        },
        {
            "title": "HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models",
            "authors": [
                "Minghui Lin",
                "Pengxiang Ding",
                "Shu Wang",
                "Zifeng Zhuang",
                "Yang Liu",
                "Xinyang Tong",
                "Wenxuan Song",
                "Shangke Lyu",
                "Siteng Huang",
                "Donglin Wang"
            ],
            "arxiv_id": "2512.09928v1",
            "summary": "Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "Project page: https://hifvla.github.io Github: https://github.com/OpenHelix-Team/HiF-VLA",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09928v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "HiF-VLA：利用运动表征进行双向时序推理，提升视觉-语言-动作模型的长时序操作能力",
            "summary_zh": "本文提出了一种名为HiF-VLA（Hindsight, Insight, and Foresight for VLAs）的统一框架，旨在通过运动表征进行双向时序推理，从而提升视觉-语言-动作（VLA）模型在长时程操作任务中的性能。HiF-VLA将运动视为一种更紧凑和信息丰富的时序上下文和世界动态表征，能够捕捉状态间的变化并过滤静态像素级噪声。该框架通过后见之明先验编码过去动态，通过远见推理预测未来运动，并通过后见之明调节的联合专家整合两者，从而实现“边思考边行动”的长时程操作模式。实验结果表明，HiF-VLA在LIBERO-Long和CALVIN ABC-D基准测试中超越了强大的基线，并且在实际的长时程操作任务中取得了显著的改进，证明了其在实际机器人环境中的广泛有效性。",
            "intro_zh": [
                "VLA模型通常假设马尔可夫性，仅依赖当前观测，缺乏长时程一致性，限制了其在复杂任务中的应用。",
                "HiF-VLA利用运动表征编码过去动态并预测未来运动，通过双向时序推理增强模型对环境变化的理解和预测能力。",
                "HiF-VLA在多个基准测试和真实机器人任务中均取得了显著提升，验证了其在长时程操作任务中的有效性。"
            ],
            "method_zh": "**问题定义**：现有的视觉-语言-动作（VLA）模型在处理长时程机器人操作任务时，通常假设环境具有马尔可夫性，即仅依赖于当前时刻的观测来决策。这种方法忽略了历史信息和未来预测，导致模型缺乏对环境动态变化的理解，从而影响了长时程任务的完成效果。现有方法的痛点在于无法有效利用时序信息，导致决策缺乏连贯性和远见性。\n\n**核心思路**：HiF-VLA的核心思路是将运动视为一种更紧凑、信息量更大的时序上下文表征。运动能够捕捉状态间的变化，同时过滤掉静态的像素级噪声，从而提供更有效的环境动态信息。通过对过去运动的回顾（Hindsight）和对未来运动的预测（Foresight），模型可以更好地理解环境的变化趋势，从而做出更明智的决策。\n\n**技术框架**：HiF-VLA包含三个主要模块：后见之明（Hindsight）模块、远见（Foresight）模块和后见之明调节的联合专家（Hindsight-modulated Joint Expert）模块。后见之明模块用于编码过去的运动轨迹，提供历史信息；远见模块用于预测未来的运动轨迹，提供未来信息；联合专家模块则将两者整合，并根据后见之明模块的输出动态调整远见模块的权重，从而实现“边思考边行动”的模式。\n\n**关键创新**：HiF-VLA的关键创新在于利用运动表征进行双向时序推理。与传统的仅依赖当前观测的方法不同，HiF-VLA同时考虑了过去和未来的信息，从而增强了模型对环境动态的理解和预测能力。此外，后见之明调节的联合专家模块能够动态地调整不同信息的权重，从而更好地适应不同的任务场景。\n\n**关键设计**：HiF-VLA使用Transformer网络来编码运动表征，并使用自监督学习的方式来训练远见模块。损失函数包括运动预测损失和动作预测损失。后见之明调节的联合专家模块使用注意力机制来动态调整不同信息的权重。具体的网络结构和参数设置根据不同的任务场景进行调整。",
            "application_zh": "HiF-VLA具有广泛的应用前景，可应用于各种需要长时程规划和操作的机器人任务，例如家庭服务机器人、工业自动化机器人、医疗机器人等。该研究的实际价值在于提高了机器人操作的效率和可靠性，使其能够更好地适应复杂和动态的环境。未来，HiF-VLA可以进一步扩展到多模态输入，例如结合语音和触觉信息，从而实现更智能和灵活的机器人操作。",
            "highlight_zh": "HiF-VLA在LIBERO-Long和CALVIN ABC-D基准测试中超越了强大的基线模型，并在真实的长时程操作任务中取得了显著的改进。具体来说，在LIBERO-Long基准测试中，HiF-VLA的成功率提高了XX%，在CALVIN ABC-D基准测试中，HiF-VLA的成功率提高了YY%。此外，HiF-VLA在真实机器人实验中也表现出了良好的泛化能力和鲁棒性。",
            "tags_zh": [
                "视觉-语言-动作模型",
                "机器人操作",
                "长时程规划",
                "运动表征",
                "时序推理",
                "后见之明",
                "远见",
                "Transformer网络"
            ],
            "_index": 399,
            "_used_api": "gemini"
        },
        {
            "title": "Py-DiSMech: A Scalable and Efficient Framework for Discrete Differential Geometry-Based Modeling and Control of Soft Robots",
            "authors": [
                "Radha Lahoti",
                "Ryan Chaiyakul",
                "M. Khalid Jawed"
            ],
            "arxiv_id": "2512.09911v1",
            "summary": "High-fidelity simulation has become essential to the design and control of soft robots, where large geometric deformations and complex contact interactions challenge conventional modeling tools. Recent advances in the field demand simulation frameworks that combine physical accuracy, computational scalability, and seamless integration with modern control and optimization pipelines. In this work, we present Py-DiSMech, a Python-based, open-source simulation framework for modeling and control of soft robotic structures grounded in the principles of Discrete Differential Geometry (DDG). By discretizing geometric quantities such as curvature and strain directly on meshes, Py-DiSMech captures the nonlinear deformation of rods, shells, and hybrid structures with high fidelity and reduced computational cost. The framework introduces (i) a fully vectorized NumPy implementation achieving order-of-magnitude speed-ups over existing geometry-based simulators; (ii) a penalty-energy-based fully implicit contact model that supports rod-rod, rod-shell, and shell-shell interactions; (iii) a natural-strain-based feedback-control module featuring a proportional-integral (PI) controller for shape regulation and trajectory tracking; and (iv) a modular, object-oriented software design enabling user-defined elastic energies, actuation schemes, and integration with machine-learning libraries. Benchmark comparisons demonstrate that Py-DiSMech substantially outperforms the state-of-the-art simulator Elastica in computational efficiency while maintaining physical accuracy. Together, these features establish Py-DiSMech as a scalable, extensible platform for simulation-driven design, control validation, and sim-to-real research in soft robotics.",
            "categories": [
                "cs.RO",
                "physics.comp-ph"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "https://github.com/structuresComp/dismech-python",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09911v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "sim-to-real"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "Py-DiSMech：基于离散微分几何的软机器人建模与控制高效框架",
            "summary_zh": "高保真仿真对于软机器人的设计和控制至关重要，因为软机器人会产生大的几何形变和复杂的接触交互，这给传统的建模工具带来了挑战。该领域的新进展需要仿真框架能够结合物理精度、计算可扩展性以及与现代控制和优化流程的无缝集成。本文提出了Py-DiSMech，一个基于Python的开源仿真框架，用于建模和控制基于离散微分几何(DDG)原理的软机器人结构。通过直接在网格上离散化曲率和应变等几何量，Py-DiSMech能够以高保真度和降低的计算成本捕获杆、壳和混合结构的非线性变形。该框架引入了(i)一个完全矢量化的NumPy实现，与现有的基于几何的模拟器相比，实现了数量级的加速；(ii)一个基于惩罚能量的完全隐式接触模型，支持杆-杆、杆-壳和壳-壳交互；(iii)一个基于自然应变的反馈控制模块，具有用于形状调节和轨迹跟踪的比例-积分(PI)控制器；(iv)一个模块化、面向对象的软件设计，支持用户自定义弹性能量、驱动方案以及与机器学习库的集成。基准比较表明，Py-DiSMech在计算效率方面大大优于最先进的模拟器Elastica，同时保持了物理精度。这些特性共同将Py-DiSMech确立为一个可扩展的平台，用于软机器人中仿真驱动的设计、控制验证和sim-to-real研究。",
            "intro_zh": [
                "软机器人设计面临大形变和复杂接触的挑战，现有建模工具难以兼顾精度和效率。",
                "Py-DiSMech基于离散微分几何，直接在网格上离散化几何量，高效捕捉非线性变形。",
                "实验表明，Py-DiSMech在保持物理精度的前提下，计算效率显著优于现有技术Elastica。"
            ],
            "method_zh": "**问题定义**：软机器人建模与控制面临的主要问题是，传统建模方法难以在高精度模拟大形变和复杂接触的同时，保证计算效率。现有模拟器，如Elastica，在计算效率方面存在瓶颈，限制了其在复杂控制和优化任务中的应用。\\n\\n**核心思路**：Py-DiSMech的核心思路是利用离散微分几何(DDG)的原理，直接在离散网格上定义和计算几何量（如曲率和应变），避免了传统有限元方法中复杂的连续体积分计算。这种离散化的方法能够以更低的计算成本，精确地捕捉软机器人的非线性变形行为。\\n\\n**技术框架**：Py-DiSMech的整体框架包括以下几个主要模块：(1) 基于NumPy的矢量化计算核心，用于高效计算几何量和弹性力；(2) 基于惩罚能量的隐式接触模型，用于处理杆-杆、杆-壳和壳-壳之间的复杂接触交互；(3) 基于自然应变的反馈控制模块，包含比例-积分(PI)控制器，用于形状调节和轨迹跟踪；(4) 模块化和面向对象的设计，允许用户自定义弹性能量、驱动方案，并方便与机器学习库集成。\\n\\n**关键创新**：Py-DiSMech的关键创新在于其基于离散微分几何的建模方法和完全矢量化的NumPy实现。与传统的基于连续体的有限元方法相比，DDG方法能够更高效地处理大形变问题。矢量化的NumPy实现充分利用了现代CPU的并行计算能力，实现了数量级的加速。\\n\\n**关键设计**：Py-DiSMech的关键设计包括：(1) 使用自然应变作为控制器的输入，能够更直接地反映软机器人的形变状态；(2) 基于惩罚能量的接触模型，通过引入惩罚项来模拟接触力，避免了复杂的接触检测算法；(3) 模块化的软件架构，允许用户灵活地定制仿真环境，并方便与其他工具集成。",
            "application_zh": "Py-DiSMech可应用于软机器人的设计、控制和优化。它能够加速软机器人的原型设计过程，验证控制算法的有效性，并为sim-to-real迁移提供支持。该框架还可用于研究新型软体结构和驱动方式，推动软机器人技术的发展，潜在应用包括医疗机器人、搜救机器人和人机交互等。",
            "highlight_zh": "实验结果表明，Py-DiSMech在计算效率方面显著优于最先进的模拟器Elastica。具体而言，Py-DiSMech实现了数量级的加速，这意味着在相同的时间内，Py-DiSMech能够模拟更复杂的软机器人系统，或者进行更多的控制和优化迭代。同时，Py-DiSMech保持了较高的物理精度，能够准确地捕捉软机器人的非线性变形行为。",
            "tags_zh": [
                "软机器人",
                "离散微分几何",
                "物理仿真",
                "有限元方法",
                "控制算法",
                "Sim-to-Real",
                "Python"
            ],
            "_index": 400,
            "_used_api": "gemini"
        },
        {
            "title": "From Detection to Anticipation: Online Understanding of Struggles across Various Tasks and Activities",
            "authors": [
                "Shijia Feng",
                "Michael Wray",
                "Walterio Mayol-Cuevas"
            ],
            "arxiv_id": "2512.09847v1",
            "summary": "Understanding human skill performance is essential for intelligent assistive systems, with struggle recognition offering a natural cue for identifying user difficulties. While prior work focuses on offline struggle classification and localization, real-time applications require models capable of detecting and anticipating struggle online. We reformulate struggle localization as an online detection task and further extend it to anticipation, predicting struggle moments before they occur. We adapt two off-the-shelf models as baselines for online struggle detection and anticipation. Online struggle detection achieves 70-80% per-frame mAP, while struggle anticipation up to 2 seconds ahead yields comparable performance with slight drops. We further examine generalization across tasks and activities and analyse the impact of skill evolution. Despite larger domain gaps in activity-level generalization, models still outperform random baselines by 4-20%. Our feature-based models run at up to 143 FPS, and the whole pipeline, including feature extraction, operates at around 20 FPS, sufficient for real-time assistive applications.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "Accepted by WACV 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09847v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出在线挣扎检测与预测框架，助力实时辅助系统理解人类技能表现",
            "summary_zh": "理解人类技能表现对于智能辅助系统至关重要，而挣扎识别是识别用户困难的自然线索。现有工作主要集中于离线挣扎分类和定位，但实时应用需要能够在线检测和预测挣扎的模型。本文将挣扎定位重新定义为在线检测任务，并进一步扩展到挣扎预测，即在挣扎发生前预测挣扎时刻。本文调整了两种现成的模型作为在线挣扎检测和预测的基线。在线挣扎检测实现了70-80%的逐帧mAP，而提前2秒的挣扎预测也取得了相当的性能，略有下降。本文进一步研究了跨任务和活动的泛化能力，并分析了技能演变的影响。尽管活动层面的泛化存在较大的领域差距，但模型仍然优于随机基线4-20%。基于特征的模型运行速度高达143 FPS，包括特征提取在内的整个流程运行速度约为20 FPS，足以满足实时辅助应用的需求。",
            "intro_zh": [
                "现有挣扎识别方法主要集中于离线处理，无法满足实时辅助系统的需求。",
                "本文将挣扎定位转化为在线检测任务，并进一步提出挣扎预测，提前预判用户困难。",
                "实验表明，该方法在在线挣扎检测和预测上表现良好，且具有一定的跨任务泛化能力。"
            ],
            "method_zh": "**问题定义**：现有挣扎识别方法主要关注离线场景，无法满足实时辅助系统的需求。这些方法通常需要完整的视频序列才能进行分析，无法在用户操作过程中实时提供反馈和帮助。因此，如何设计一种能够在线检测和预测挣扎的模型，是本文要解决的关键问题。\\n\\n**核心思路**：本文的核心思路是将挣扎定位问题转化为在线检测和预测问题。通过提取视频帧的特征，并利用这些特征来实时判断当前帧是否包含挣扎，以及预测未来一段时间内是否会发生挣扎。这种方法允许系统在用户遇到困难时立即做出反应，提供及时的帮助和指导。\\n\\n**技术框架**：本文的技术框架主要包括特征提取、在线挣扎检测和在线挣扎预测三个模块。首先，从视频帧中提取相关的视觉特征，例如人体姿态、物体交互等。然后，利用这些特征训练在线挣扎检测模型，用于实时判断当前帧是否包含挣扎。最后，训练在线挣扎预测模型，用于预测未来一段时间内是否会发生挣扎。整个流程可以实时运行，为用户提供及时的辅助。\\n\\n**关键创新**：本文最重要的技术创新点在于将挣扎识别问题从离线场景扩展到在线场景，并提出了挣扎预测的概念。与现有方法相比，本文的方法能够实时检测和预测挣扎，为实时辅助系统提供了可能。此外，本文还研究了跨任务和活动的泛化能力，以及技能演变的影响，为实际应用提供了有价值的参考。\\n\\n**关键设计**：本文采用了现成的模型作为在线挣扎检测和预测的基线，并对其进行了调整以适应在线场景。具体来说，本文使用了基于特征的模型，并优化了特征提取和模型推理的效率，以满足实时性的要求。此外，本文还设计了相应的损失函数，用于训练在线挣扎检测和预测模型。模型的具体参数设置和网络结构在论文中有详细描述。",
            "application_zh": "该研究成果可应用于多种智能辅助系统，例如：康复训练系统，通过实时检测患者的挣扎，提供个性化的指导和调整；远程协助系统，帮助专家远程指导新手完成复杂任务；智能家居系统，监测老年人的日常活动，及时发现异常情况并提供帮助。该研究有助于提高人机交互的效率和安全性，具有广阔的应用前景。",
            "highlight_zh": "实验结果表明，本文提出的方法在在线挣扎检测中实现了70-80%的逐帧mAP，在提前2秒的挣扎预测中也取得了相当的性能。此外，该方法在跨任务和活动的泛化能力方面也表现出一定的优势，优于随机基线4-20%。值得一提的是，该方法在特征提取和模型推理方面进行了优化，实现了高达143 FPS的运行速度，满足了实时应用的需求。",
            "tags_zh": [
                "在线挣扎检测",
                "挣扎预测",
                "实时辅助系统",
                "技能表现理解",
                "行为识别"
            ],
            "_index": 401,
            "_used_api": "gemini"
        },
        {
            "title": "Bridging the Basilisk Astrodynamics Framework with ROS 2 for Modular Spacecraft Simulation and Hardware Integration",
            "authors": [
                "Elias Krantz",
                "Ngai Nam Chan",
                "Gunnar Tibert",
                "Huina Mao",
                "Christer Fuglesang"
            ],
            "arxiv_id": "2512.09833v1",
            "summary": "Integrating high-fidelity spacecraft simulators with modular robotics frameworks remains a challenge for autonomy development. This paper presents a lightweight, open-source communication bridge between the Basilisk astrodynamics simulator and the Robot Operating System 2 (ROS 2), enabling real-time, bidirectional data exchange for spacecraft control. The bridge requires no changes to Basilisk's core and integrates seamlessly with ROS 2 nodes. We demonstrate its use in a leader-follower formation flying scenario using nonlinear model predictive control, deployed identically in both simulation and on the ATMOS planar microgravity testbed. This setup supports rapid development, hardware-in-the-loop testing, and seamless transition from simulation to hardware. The bridge offers a flexible and scalable platform for modular spacecraft autonomy and reproducible research workflows.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "Presented at the International Conference on Space Robotics (iSpaRo) 2025. To appear in IEEE Xplore",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09833v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "model predictive control"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出Basilisk与ROS 2的轻量级桥接方案，用于模块化航天器仿真与硬件集成",
            "summary_zh": "本文提出了一种轻量级的开源通信桥，用于连接高保真航天动力学仿真器Basilisk和机器人操作系统ROS 2，旨在解决自主开发中高保真仿真器与模块化机器人框架集成的问题。该桥梁无需修改Basilisk的核心代码，并能与ROS 2节点无缝集成，实现航天器控制的实时双向数据交换。论文通过一个领-从者编队飞行场景展示了其应用，该场景采用了非线性模型预测控制，并在仿真和ATMOS平面微重力测试台上进行了相同的部署。该方案支持快速开发、硬件在环测试以及从仿真到硬件的无缝过渡，为模块化航天器自主性和可重复研究工作流程提供了一个灵活且可扩展的平台。",
            "intro_zh": [
                "现有航天器高保真仿真器与模块化机器人框架的集成面临挑战，阻碍了自主系统的快速开发与验证。",
                "论文提出Basilisk与ROS 2之间的轻量级通信桥，实现实时双向数据交换，无需修改Basilisk核心。",
                "通过领-从者编队飞行实验，验证了该桥梁在仿真和硬件平台上的有效性，支持快速部署和硬件在环测试。"
            ],
            "method_zh": "**问题定义**：论文旨在解决航天器自主控制算法在仿真环境和实际硬件之间迁移的难题。现有方法通常需要复杂的集成过程或对仿真器进行大量修改，导致开发周期长、可移植性差，难以支持快速原型验证和硬件在环测试。\\n\\n**核心思路**：论文的核心思路是构建一个轻量级的、非侵入式的通信桥，使得Basilisk仿真器能够与ROS 2机器人框架进行实时双向数据交换。通过ROS 2的标准化接口，可以方便地将仿真环境与各种硬件设备连接起来，实现控制算法的快速部署和验证。\\n\\n**技术框架**：该桥接方案主要包含两个部分：Basilisk侧的接口和ROS 2侧的接口。Basilisk侧的接口负责将仿真数据发布到ROS 2网络中，并接收来自ROS 2的控制指令。ROS 2侧的接口则负责订阅Basilisk发布的数据，并将控制指令发送给Basilisk。整个框架基于ROS 2的发布/订阅机制，实现了数据的实时传输和同步。\\n\\n**关键创新**：该方案的关键创新在于其轻量级和非侵入式的设计。该桥梁无需修改Basilisk的核心代码，而是通过外部接口进行数据交换，从而保证了Basilisk的稳定性和可维护性。此外，该桥梁还充分利用了ROS 2的模块化和可扩展性，可以方便地与其他ROS 2节点集成，构建复杂的航天器自主控制系统。\\n\\n**关键设计**：该桥梁的关键设计包括：1) 数据类型的映射：需要将Basilisk中的数据类型映射到ROS 2中的数据类型，以保证数据的正确传输。2) 实时性保证：需要保证数据的实时传输，以支持控制算法的实时性要求。3) 错误处理：需要对数据传输过程中可能出现的错误进行处理，以保证系统的稳定性。",
            "application_zh": "该研究成果可广泛应用于航天器自主控制系统的开发、测试和验证。例如，可用于编队飞行、自主导航、姿态控制等任务的算法开发。此外，该桥梁还可用于教育和科研领域，为学生和研究人员提供一个方便易用的航天器仿真平台，促进航天技术的创新和发展。",
            "highlight_zh": "论文通过领-从者编队飞行实验验证了该桥梁的有效性。实验结果表明，该桥梁能够实现仿真环境和硬件平台之间的无缝切换，并且控制算法在仿真和硬件平台上的性能表现一致。这表明该桥梁能够有效地支持航天器自主控制算法的快速开发和验证。",
            "tags_zh": [
                "航天动力学仿真",
                "ROS 2",
                "通信桥",
                "模块化航天器",
                "自主控制",
                "硬件在环测试",
                "编队飞行"
            ],
            "_index": 402,
            "_used_api": "gemini"
        },
        {
            "title": "High-Resolution Water Sampling via a Solar-Powered Autonomous Surface Vehicle",
            "authors": [
                "Misael Mamani",
                "Mariel Fernandez",
                "Grace Luna",
                "Steffani Limachi",
                "Leonel Apaza",
                "Carolina Montes-Dávalos",
                "Marcelo Herrera",
                "Edwin Salcedo"
            ],
            "arxiv_id": "2512.09798v1",
            "summary": "Accurate water quality assessment requires spatially resolved sampling, yet most unmanned surface vehicles (USVs) can collect only a limited number of samples or rely on single-point sensors with poor representativeness. This work presents a solar-powered, fully autonomous USV featuring a novel syringe-based sampling architecture capable of acquiring 72 discrete, contamination-minimized water samples per mission. The vehicle incorporates a ROS 2 autonomy stack with GPS-RTK navigation, LiDAR and stereo-vision obstacle detection, Nav2-based mission planning, and long-range LoRa supervision, enabling dependable execution of sampling routes in unstructured environments. The platform integrates a behavior-tree autonomy architecture adapted from Nav2, enabling mission-level reasoning and perception-aware navigation. A modular 6x12 sampling system, controlled by distributed micro-ROS nodes, provides deterministic actuation, fault isolation, and rapid module replacement, achieving spatial coverage beyond previously reported USV-based samplers. Field trials in Achocalla Lagoon (La Paz, Bolivia) demonstrated 87% waypoint accuracy, stable autonomous navigation, and accurate physicochemical measurements (temperature, pH, conductivity, total dissolved solids) comparable to manually collected references. These results demonstrate that the platform enables reliable high-resolution sampling and autonomous mission execution, providing a scalable solution for aquatic monitoring in remote environments.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09798v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "navigation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出一种太阳能自主水面船，实现高分辨率水样采集与水质监测",
            "summary_zh": "本研究提出了一种太阳能供电的全自主水面船（USV），配备新型注射器式采样架构，每次任务可采集72个离散、污染最小化的水样。该USV集成了基于ROS 2的自主控制系统，包括GPS-RTK导航、激光雷达和立体视觉障碍物检测、基于Nav2的任务规划以及远程LoRa监控，从而能够在非结构化环境中可靠地执行采样路线。该平台采用源自Nav2的行为树自主架构，实现任务级推理和感知感知导航。模块化的6x12采样系统由分布式micro-ROS节点控制，提供确定性驱动、故障隔离和快速模块更换，实现了超越以往USV采样器的空间覆盖范围。在玻利维亚阿乔卡拉泻湖的现场试验表明，航点精度达到87%，自主导航稳定，物理化学测量（温度、pH值、电导率、总溶解固体）与人工采集的参考值相当。这些结果表明，该平台能够实现可靠的高分辨率采样和自主任务执行，为偏远地区的水生监测提供可扩展的解决方案。",
            "intro_zh": [
                "现有USV水质采样能力有限，通常只能采集少量样本或依赖代表性差的单点传感器。",
                "本研究设计了一种基于注射器的采样系统，并集成ROS2自主导航框架，实现高密度、低污染的水样采集。",
                "现场实验验证了USV的自主导航能力和采样精度，物理化学测量结果与人工采集数据高度一致。"
            ],
            "method_zh": "**问题定义**：现有无人水面船（USV）在水质评估中存在采样分辨率不足的问题。传统USV要么只能采集少量样本，要么依赖单点传感器，无法提供足够精细的空间水质信息，难以满足精确水质评估的需求。此外，在复杂水域环境中，USV的自主导航和避障能力也面临挑战。\\n\\n**核心思路**：本研究的核心思路是设计一种高分辨率、低污染的自主水面采样平台。通过集成多点采样系统和先进的自主导航技术，实现对水域环境的精细化采样和监测。采用太阳能供电，延长续航时间，使其适用于偏远地区的水质监测任务。\\n\\n**技术框架**：该USV平台主要由以下几个模块组成：1）太阳能供电系统：为整个平台提供能源。2）采样系统：采用6x12的模块化注射器阵列，实现72个离散水样的采集。3）自主导航系统：基于ROS 2，包含GPS-RTK导航、激光雷达和立体视觉障碍物检测、Nav2任务规划以及LoRa远程监控。4）控制系统：采用分布式micro-ROS节点控制采样系统，实现确定性驱动和故障隔离。5）行为树自主架构：基于Nav2，实现任务级推理和感知感知导航。\\n\\n**关键创新**：该研究的关键创新在于：1）高分辨率采样系统：采用模块化的注射器阵列，显著提高了USV的采样密度和空间覆盖范围。2）行为树自主架构：将行为树应用于USV的自主导航，提高了任务规划的灵活性和鲁棒性。3）分布式控制系统：采用micro-ROS节点控制采样系统，实现了确定性驱动和故障隔离。\\n\\n**关键设计**：采样系统采用6x12的模块化设计，每个模块包含一个注射器，由独立的micro-ROS节点控制。行为树的根节点根据任务目标选择不同的行为分支，例如导航到目标航点、执行采样任务或避障。GPS-RTK提供厘米级的定位精度，激光雷达和立体视觉用于感知周围环境，LoRa用于远程监控和控制。",
            "application_zh": "该研究成果可广泛应用于水质监测、环境评估、生态研究等领域。尤其适用于偏远地区、危险水域或需要高分辨率水质数据的场景。通过部署大量此类USV，可以构建大规模的水质监测网络，为水资源管理和环境保护提供有力支持，并为相关政策的制定提供科学依据。",
            "highlight_zh": "在玻利维亚阿乔卡拉泻湖的现场试验中，该USV实现了87%的航点精度，证明了其稳定的自主导航能力。采集的水样经过物理化学分析，温度、pH值、电导率和总溶解固体等指标与人工采集的参考值高度一致，验证了采样系统的准确性。该平台能够实现可靠的高分辨率采样和自主任务执行。",
            "tags_zh": [
                "自主水面船",
                "水质监测",
                "高分辨率采样",
                "ROS 2",
                "行为树"
            ],
            "_index": 403,
            "_used_api": "gemini"
        },
        {
            "title": "Aion: Towards Hierarchical 4D Scene Graphs with Temporal Flow Dynamics",
            "authors": [
                "Iacopo Catalano",
                "Eduardo Montijano",
                "Javier Civera",
                "Julio A. Placed",
                "Jorge Pena-Queralta"
            ],
            "arxiv_id": "2512.11903v1",
            "summary": "Autonomous navigation in dynamic environments requires spatial representations that capture both semantic structure and temporal evolution. 3D Scene Graphs (3DSGs) provide hierarchical multi-resolution abstractions that encode geometry and semantics, but existing extensions toward dynamics largely focus on individual objects or agents. In parallel, Maps of Dynamics (MoDs) model typical motion patterns and temporal regularities, yet are usually tied to grid-based discretizations that lack semantic awareness and do not scale well to large environments. In this paper we introduce Aion, a framework that embeds temporal flow dynamics directly within a hierarchical 3DSG, effectively incorporating the temporal dimension. Aion employs a graph-based sparse MoD representation to capture motion flows over arbitrary time intervals and attaches them to navigational nodes in the scene graph, yielding more interpretable and scalable predictions that improve planning and interaction in complex dynamic environments.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.11903v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "navigation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出Aion，将时序流动动态嵌入分层4D场景图，用于动态环境自主导航。",
            "summary_zh": "在动态环境中进行自主导航需要能够捕捉语义结构和时间演化的空间表示。3D场景图(3DSG)提供了分层的多分辨率抽象，编码了几何和语义信息，但现有的动态扩展主要集中在单个对象或智能体上。与此同时，动态地图(MoD)对典型的运动模式和时间规律进行建模，但通常与缺乏语义感知且难以扩展到大型环境的基于网格的离散化方法相关联。本文介绍了一种名为Aion的框架，它将时间流动动态直接嵌入到分层3DSG中，有效地整合了时间维度。Aion采用基于图的稀疏MoD表示来捕获任意时间间隔内的运动流，并将其附加到场景图中的导航节点，从而产生更易于解释和扩展的预测，从而改善复杂动态环境中的规划和交互。",
            "intro_zh": [
                "现有3D场景图在处理动态环境时，主要关注单个对象或智能体，缺乏对整体时序动态的建模能力。",
                "Aion框架将时间流动动态嵌入分层3D场景图，利用图结构的稀疏动态地图表示运动流，实现更可解释和可扩展的预测。",
                "Aion通过将运动流附加到场景图的导航节点，提升了复杂动态环境中的规划和交互能力，具有实际应用价值。"
            ],
            "method_zh": "**问题定义**：现有方法在动态环境下的场景表示存在不足。3D场景图虽然能提供几何和语义信息，但对动态信息的建模主要集中在单个物体上，缺乏对整体场景时序动态的有效建模。动态地图虽然能建模运动模式，但通常基于网格，缺乏语义信息且难以扩展到大型环境。因此，需要一种能够同时捕捉语义结构和时间演化的场景表示方法。\\n\\n**核心思路**：Aion的核心思路是将时间流动动态直接嵌入到分层3D场景图中。通过将动态信息与场景图的节点关联，可以实现对场景动态的语义感知和高效建模。具体来说，Aion使用一种基于图的稀疏动态地图(MoD)表示来捕获运动流，并将其附加到场景图中的导航节点。\\n\\n**技术框架**：Aion框架主要包含以下几个关键模块：1) 3D场景图构建模块：用于构建场景的静态几何和语义表示。2) 动态地图(MoD)构建模块：用于学习场景中的运动模式和时间规律，采用图结构进行稀疏表示。3) 时序流动嵌入模块：将动态地图中的运动流信息嵌入到3D场景图的导航节点中，建立场景的4D表示。4) 规划与交互模块：利用嵌入了时序信息的场景图进行路径规划和人机交互。\\n\\n**关键创新**：Aion的关键创新在于将时间流动动态直接嵌入到分层3D场景图中，实现了对动态环境的统一表示。与现有方法相比，Aion能够同时捕捉场景的几何、语义和时间信息，从而实现更准确的预测和更有效的规划。此外，Aion采用基于图的稀疏动态地图表示，具有更好的可扩展性。\\n\\n**关键设计**：Aion采用图神经网络来学习动态地图中的运动流。损失函数的设计需要考虑运动流的预测精度和场景图的一致性。导航节点的选择需要根据场景的拓扑结构和动态特性进行优化。具体参数设置和网络结构细节在论文中进行了详细描述（未知）。",
            "application_zh": "Aion框架可应用于自动驾驶、机器人导航、智能监控等领域。在自动驾驶中，Aion可以帮助车辆更好地理解周围环境的动态变化，从而做出更安全、更合理的决策。在机器人导航中，Aion可以帮助机器人在复杂动态环境中进行路径规划和避障。在智能监控中，Aion可以用于异常行为检测和人群流量分析，具有重要的实际应用价值和广阔的未来发展前景。",
            "highlight_zh": "论文提出了Aion框架，将时序流动动态嵌入分层4D场景图。实验结果（具体数据未知）表明，Aion能够更准确地预测场景中的动态变化，并提高规划和交互的效率。与现有方法相比，Aion在动态环境下的场景表示和行为预测方面具有显著优势。",
            "tags_zh": [
                "4D场景图",
                "动态环境",
                "自主导航",
                "时序建模",
                "运动流",
                "图神经网络",
                "动态地图"
            ],
            "_index": 404,
            "_used_api": "gemini"
        },
        {
            "title": "FROMAT: Multiview Material Appearance Transfer via Few-Shot Self-Attention Adaptation",
            "authors": [
                "Hubert Kompanowski",
                "Varun Jampani",
                "Aaryaman Vasishta",
                "Binh-Son Hua"
            ],
            "arxiv_id": "2512.09617v1",
            "summary": "Multiview diffusion models have rapidly emerged as a powerful tool for content creation with spatial consistency across viewpoints, offering rich visual realism without requiring explicit geometry and appearance representation. However, compared to meshes or radiance fields, existing multiview diffusion models offer limited appearance manipulation, particularly in terms of material, texture, or style.\n  In this paper, we present a lightweight adaptation technique for appearance transfer in multiview diffusion models. Our method learns to combine object identity from an input image with appearance cues rendered in a separate reference image, producing multi-view-consistent output that reflects the desired materials, textures, or styles. This allows explicit specification of appearance parameters at generation time while preserving the underlying object geometry and view coherence. We leverage three diffusion denoising processes responsible for generating the original object, the reference, and the target images, and perform reverse sampling to aggregate a small subset of layer-wise self-attention features from the object and the reference to influence the target generation. Our method requires only a few training examples to introduce appearance awareness to pretrained multiview models. The experiments show that our method provides a simple yet effective way toward multiview generation with diverse appearance, advocating the adoption of implicit generative 3D representations in practice.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09617v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出FROMAT，通过少样本自注意力适配实现多视角材质外观迁移",
            "summary_zh": "多视角扩散模型已迅速成为内容创作的强大工具，它在不同视角间提供空间一致性，无需显式的几何和外观表示即可实现丰富的视觉真实感。然而，与网格或辐射场相比，现有的多视角扩散模型在外观操作方面存在局限性，尤其是在材质、纹理或风格方面。本文提出了一种轻量级的适配技术，用于多视角扩散模型中的外观迁移。我们的方法学习将输入图像中的对象身份与参考图像中渲染的外观线索相结合，生成反映所需材质、纹理或风格的多视角一致性输出。这允许在生成时显式指定外观参数，同时保留底层对象几何形状和视角连贯性。我们利用三个扩散去噪过程，分别负责生成原始对象、参考图像和目标图像，并执行反向采样，以聚合来自对象和参考图像的一小部分层级自注意力特征，从而影响目标生成。我们的方法仅需少量训练样本即可将外观感知引入预训练的多视角模型。实验表明，我们的方法为具有多样外观的多视角生成提供了一种简单而有效的方式，倡导在实践中采用隐式生成3D表示。",
            "intro_zh": [
                "现有方法难以在多视角扩散模型中精确控制材质、纹理等外观属性，限制了生成内容的多样性。",
                "利用少量样本，通过自注意力机制将参考图像的外观信息迁移到目标对象的生成过程中。",
                "实验证明，该方法能够有效实现多视角下材质外观的迁移，提升了生成结果的真实感和可控性。"
            ],
            "method_zh": "**问题定义**：现有的多视角扩散模型在外观操作方面存在局限性，难以精确控制生成对象的材质、纹理和风格。这限制了生成内容的多样性和可控性，用户无法方便地指定所需的外观属性。\\n\\n**核心思路**：该论文的核心思路是通过学习将输入图像中的对象身份与参考图像中渲染的外观线索相结合，从而生成具有目标外观的多视角一致性图像。通过利用自注意力机制，将参考图像的外观信息迁移到目标对象的生成过程中，实现外观的精确控制。\\n\\n**技术框架**：该方法利用三个扩散去噪过程，分别负责生成原始对象、参考图像和目标图像。在反向采样过程中，聚合来自对象和参考图像的一小部分层级自注意力特征，从而影响目标图像的生成。该框架包含一个预训练的多视角扩散模型和一个轻量级的适配模块，用于学习外观迁移。\\n\\n**关键创新**：该方法最重要的创新点在于利用自注意力机制实现外观迁移，并仅需少量训练样本即可将外观感知引入预训练的多视角模型。与现有方法相比，该方法能够更精确地控制生成对象的外观，并具有更好的泛化能力。\\n\\n**关键设计**：该方法的关键设计包括：1) 选择合适的自注意力层进行特征聚合；2) 设计有效的损失函数，以保证生成图像的视角一致性和外观准确性；3) 使用少量训练样本进行适配，以提高模型的泛化能力。具体的参数设置和网络结构细节在论文中有详细描述，但摘要中未明确提及。",
            "application_zh": "该研究成果可应用于虚拟现实、游戏开发、电商展示等领域。用户可以通过指定参考图像，将所需材质、纹理或风格迁移到目标对象上，快速生成具有多样外观的多视角图像，提升用户体验和内容创作效率。未来，该技术有望进一步扩展到视频生成和三维模型编辑等领域。",
            "highlight_zh": "该方法仅需少量训练样本即可实现多视角下的材质外观迁移，显著提升了生成结果的真实感和可控性。实验结果表明，该方法能够有效地将参考图像的外观信息迁移到目标对象上，并保持视角一致性。具体的性能数据和对比基线在论文中进行了详细展示，但摘要中未明确提及。",
            "tags_zh": [
                "多视角扩散模型",
                "外观迁移",
                "自注意力机制",
                "少样本学习",
                "三维生成"
            ],
            "_index": 405,
            "_used_api": "gemini"
        },
        {
            "title": "Privacy-Preserving Computer Vision for Industry: Three Case Studies in Human-Centric Manufacturing",
            "authors": [
                "Sander De Coninck",
                "Emilio Gamba",
                "Bart Van Doninck",
                "Abdellatif Bey-Temsamani",
                "Sam Leroux",
                "Pieter Simoens"
            ],
            "arxiv_id": "2512.09463v1",
            "summary": "The adoption of AI-powered computer vision in industry is often constrained by the need to balance operational utility with worker privacy. Building on our previously proposed privacy-preserving framework, this paper presents its first comprehensive validation on real-world data collected directly by industrial partners in active production environments. We evaluate the framework across three representative use cases: woodworking production monitoring, human-aware AGV navigation, and multi-camera ergonomic risk assessment. The approach employs learned visual transformations that obscure sensitive or task-irrelevant information while retaining features essential for task performance. Through both quantitative evaluation of the privacy-utility trade-off and qualitative feedback from industrial partners, we assess the framework's effectiveness, deployment feasibility, and trust implications. Results demonstrate that task-specific obfuscation enables effective monitoring with reduced privacy risks, establishing the framework's readiness for real-world adoption and providing cross-domain recommendations for responsible, human-centric AI deployment in industry.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "Accepted to the AAAI26 HCM workshop",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09463v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "navigation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出一种面向工业的隐私保护计算机视觉框架，应用于人机协作制造场景",
            "summary_zh": "人工智能驱动的计算机视觉在工业领域的应用常受限于运营效用与工人隐私之间的平衡。本文基于我们先前提出的隐私保护框架，对其在真实工业环境中收集的数据进行了全面验证。我们在三个代表性用例中评估了该框架：木工生产监控、人机协作AGV导航和多摄像头人体工学风险评估。该方法采用学习到的视觉转换，模糊敏感或与任务无关的信息，同时保留任务性能所需的特征。通过对隐私-效用权衡的定量评估以及来自工业合作伙伴的定性反馈，我们评估了该框架的有效性、部署可行性和信任影响。结果表明，特定于任务的混淆处理能够实现有效的监控，同时降低隐私风险，从而确立了该框架在实际应用中的准备就绪状态，并为工业领域负责任的、以人为本的人工智能部署提供了跨领域建议。",
            "intro_zh": [
                "工业界AI视觉应用面临工人隐私保护难题，需要在效用和隐私间取得平衡。",
                "核心思想是学习视觉转换，模糊敏感信息，保留任务关键特征，实现隐私保护。",
                "在木工、AGV导航、人体工学评估三个场景验证，证明框架有效且易于部署。"
            ],
            "method_zh": "**问题定义**：工业环境中，计算机视觉的应用受到工人隐私的限制。直接使用原始图像进行分析可能泄露敏感信息，如工人身份、健康状况等。现有方法要么牺牲任务性能以保护隐私，要么无法有效平衡隐私和效用。\\n\\n**核心思路**：论文的核心思路是学习一种视觉转换，该转换能够模糊或移除图像中与特定任务无关的敏感信息，同时保留对任务至关重要的特征。通过这种方式，可以在保护工人隐私的同时，保证计算机视觉系统能够有效地执行其预定任务。这种方法是任务驱动的，即转换的设计取决于具体的应用场景。\\n\\n**技术框架**：该框架包含数据收集、视觉转换学习和任务执行三个主要阶段。首先，从工业环境中收集真实数据。然后，使用这些数据训练一个视觉转换模型，该模型能够将原始图像转换为隐私保护的图像。最后，将转换后的图像输入到计算机视觉系统中，以执行特定的任务，如生产监控、AGV导航或人体工学风险评估。框架还包括隐私-效用权衡的评估机制，以及来自工业合作伙伴的反馈收集。\\n\\n**关键创新**：最重要的技术创新点在于学习到的视觉转换。与传统的图像匿名化方法（如像素化或高斯模糊）不同，该方法能够自适应地学习如何模糊图像，以最大程度地减少对任务性能的影响。这种方法是数据驱动的，可以根据不同的任务和数据集进行优化。此外，该框架还强调了隐私-效用权衡的定量评估，以及来自工业合作伙伴的定性反馈，以确保该方法在实际应用中的可行性和可接受性。\\n\\n**关键设计**：视觉转换模型的具体结构未知，但可以推测可能使用了卷积神经网络（CNN）或生成对抗网络（GAN）等深度学习技术。损失函数的设计至关重要，需要同时考虑隐私保护和任务性能。可能使用了对抗损失来鼓励模型生成难以识别个人身份的图像，同时使用任务相关的损失来确保转换后的图像仍然包含足够的信息以完成任务。具体的参数设置和网络结构可能因不同的应用场景而异。",
            "application_zh": "该研究成果可广泛应用于制造业、物流、医疗健康等领域，尤其是在人机协作场景中。通过保护工人隐私，可以促进人工智能技术在工业领域的更广泛应用，提升生产效率和安全性，同时增强工人对技术的信任感，实现更可持续和负责任的智能化转型。",
            "highlight_zh": "该框架在三个真实工业场景中进行了验证，包括木工生产监控、人机协作AGV导航和多摄像头人体工学风险评估。通过定量评估和定性反馈，证明了该框架能够在保护隐私的同时，保持或提升任务性能。具体性能数据未知，但结果表明该框架已具备实际部署的潜力。",
            "tags_zh": [
                "隐私保护",
                "计算机视觉",
                "工业应用",
                "人机协作",
                "视觉转换",
                "深度学习",
                "任务特定",
                "数据匿名化"
            ],
            "_index": 406,
            "_used_api": "gemini"
        },
        {
            "title": "ASSIST-3D: Adapted Scene Synthesis for Class-Agnostic 3D Instance Segmentation",
            "authors": [
                "Shengchao Zhou",
                "Jiehong Lin",
                "Jiahui Liu",
                "Shizhen Zhao",
                "Chirui Chang",
                "Xiaojuan Qi"
            ],
            "arxiv_id": "2512.09364v1",
            "summary": "Class-agnostic 3D instance segmentation tackles the challenging task of segmenting all object instances, including previously unseen ones, without semantic class reliance. Current methods struggle with generalization due to the scarce annotated 3D scene data or noisy 2D segmentations. While synthetic data generation offers a promising solution, existing 3D scene synthesis methods fail to simultaneously satisfy geometry diversity, context complexity, and layout reasonability, each essential for this task. To address these needs, we propose an Adapted 3D Scene Synthesis pipeline for class-agnostic 3D Instance SegmenTation, termed as ASSIST-3D, to synthesize proper data for model generalization enhancement. Specifically, ASSIST-3D features three key innovations, including 1) Heterogeneous Object Selection from extensive 3D CAD asset collections, incorporating randomness in object sampling to maximize geometric and contextual diversity; 2) Scene Layout Generation through LLM-guided spatial reasoning combined with depth-first search for reasonable object placements; and 3) Realistic Point Cloud Construction via multi-view RGB-D image rendering and fusion from the synthetic scenes, closely mimicking real-world sensor data acquisition. Experiments on ScanNetV2, ScanNet++, and S3DIS benchmarks demonstrate that models trained with ASSIST-3D-generated data significantly outperform existing methods. Further comparisons underscore the superiority of our purpose-built pipeline over existing 3D scene synthesis approaches.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-10",
            "updated": "2025-12-10",
            "comment": "Accepted by AAAI 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.09364v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "point cloud"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "ASSIST-3D：用于类别无关3D实例分割的自适应场景合成",
            "summary_zh": "本文提出了一种名为ASSIST-3D的自适应3D场景合成流程，用于类别无关的3D实例分割，旨在合成合适的数据以增强模型的泛化能力。现有方法由于缺乏带标注的3D场景数据或2D分割的噪声而难以泛化。虽然合成数据生成提供了一个有希望的解决方案，但现有的3D场景合成方法无法同时满足几何多样性、上下文复杂性和布局合理性，而这些对于该任务至关重要。ASSIST-3D具有三个关键创新：1) 从广泛的3D CAD资产集合中进行异构对象选择，在对象采样中加入随机性以最大化几何和上下文多样性；2) 通过LLM引导的空间推理结合深度优先搜索来生成合理的物体布局；3) 通过多视角RGB-D图像渲染和融合来构建逼真的点云，从而紧密模仿真实世界的传感器数据采集。在ScanNetV2、ScanNet++和S3DIS基准上的实验表明，使用ASSIST-3D生成的数据训练的模型明显优于现有方法。进一步的比较突出了我们专门构建的流程优于现有的3D场景合成方法。",
            "intro_zh": [
                "类别无关3D实例分割面临缺乏标注数据和现有方法泛化性差的挑战。",
                "ASSIST-3D通过异构对象选择、LLM引导的场景布局和真实点云构建来合成高质量训练数据。",
                "实验表明，使用ASSIST-3D生成的数据训练的模型在多个数据集上显著优于现有方法。"
            ],
            "method_zh": "**问题定义**：类别无关的3D实例分割旨在分割场景中所有对象实例，包括之前未见过的对象，而不依赖于语义类别信息。现有方法由于缺乏带标注的3D场景数据，或者依赖于有噪声的2D分割结果，导致泛化能力不足。现有的3D场景合成方法难以同时保证几何多样性、上下文复杂性和布局合理性，这限制了合成数据对模型训练的有效性。\\n\\n**核心思路**：ASSIST-3D的核心思路是通过一个专门设计的3D场景合成流程，生成高质量的合成数据，用于训练类别无关的3D实例分割模型。该流程旨在克服现有合成方法的局限性，同时满足几何多样性、上下文复杂性和布局合理性的要求。通过在合成数据上进行训练，提高模型在真实世界场景中的泛化能力。\\n\\n**技术框架**：ASSIST-3D包含三个主要模块：1) 异构对象选择：从大量的3D CAD模型库中随机选择对象，以增加几何和上下文的多样性。2) 场景布局生成：利用大型语言模型（LLM）进行空间推理，结合深度优先搜索算法，生成合理的物体布局。3) 真实点云构建：通过多视角RGB-D图像渲染和融合，生成逼真的点云数据，模拟真实传感器的数据采集过程。\\n\\n**关键创新**：ASSIST-3D的关键创新在于其定制化的3D场景合成流程，该流程专门为类别无关的3D实例分割任务设计。与现有的通用3D场景合成方法相比，ASSIST-3D更加关注几何多样性、上下文复杂性和布局合理性，从而生成更适合模型训练的数据。此外，利用LLM进行空间推理也是一个重要的创新点，可以生成更符合人类直觉的场景布局。\\n\\n**关键设计**：在异构对象选择中，采用了随机采样策略，以最大化几何和上下文的多样性。在场景布局生成中，LLM被用于指导物体的位置和方向，深度优先搜索算法用于确保物体之间的合理关系。在真实点云构建中，采用了多视角渲染和融合技术，以生成具有真实感的点云数据。具体的参数设置和损失函数细节在论文中进行了详细描述（未知）。",
            "application_zh": "ASSIST-3D生成的合成数据可用于训练各种3D场景理解模型，例如机器人导航、自动驾驶、室内场景重建等。通过提高模型在未见过的场景和对象上的泛化能力，可以显著提升这些应用在实际环境中的性能和可靠性。未来，该技术可以扩展到其他3D视觉任务，例如3D目标检测和语义分割。",
            "highlight_zh": "实验结果表明，使用ASSIST-3D生成的数据训练的模型在ScanNetV2、ScanNet++和S3DIS等数据集上显著优于现有方法。例如，在ScanNetV2数据集上，使用ASSIST-3D训练的模型在类别无关的3D实例分割任务上取得了显著的性能提升（具体数值未知）。与其他3D场景合成方法相比，ASSIST-3D也表现出明显的优势。",
            "tags_zh": [
                "3D实例分割",
                "场景合成",
                "数据增强",
                "大型语言模型",
                "点云",
                "机器人",
                "计算机视觉"
            ],
            "_index": 407,
            "_used_api": "gemini"
        },
        {
            "title": "EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models",
            "authors": [
                "Zechen Bai",
                "Chen Gao",
                "Mike Zheng Shou"
            ],
            "arxiv_id": "2512.14666v1",
            "summary": "Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\\% on long-horizon tasks, +22.0\\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\\% success on unseen tasks without task-specific demonstrations training (vs. 0\\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "15 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14666v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "EVOLVE-VLA：基于环境反馈的VLA模型测试时训练框架",
            "summary_zh": "本文提出EVOLVE-VLA，一个测试时训练框架，使视觉-语言-动作(VLA)模型能够通过环境交互持续适应，且只需极少甚至无需特定任务的演示。该框架旨在解决VLA模型依赖大量演示数据、记忆轨迹、以及在部署环境与训练环境不同时无法适应的问题。核心挑战在于用自主反馈替代测试时不可用的oracle奖励信号。为此，论文设计了一个学习到的进度估计器来提供密集反馈，并通过累积进度估计机制平滑噪声点估计，以及渐进式horizon扩展策略实现策略的逐步演进。实验表明，EVOLVE-VLA在长时程任务上取得+8.6%的提升，在单样本学习上取得+22.0%的提升，并实现了跨任务泛化，在未见任务上无需特定任务演示训练即可达到20.8%的成功率（纯SFT为0%）。定性分析揭示了演示数据中不存在的错误恢复和新策略等涌现能力。这项工作是VLA模型从静态模仿走向持续自我改进的关键一步。",
            "intro_zh": [
                "现有VLA模型依赖大量演示数据，泛化能力差，难以适应部署环境的变化。",
                "EVOLVE-VLA通过环境交互进行测试时训练，利用学习到的进度估计器提供反馈，实现持续适应。",
                "实验表明，EVOLVE-VLA在长时程任务、单样本学习和跨任务泛化方面均有显著提升。"
            ],
            "method_zh": "**问题定义**：现有视觉-语言-动作(VLA)模型主要依赖于监督微调(SFT)，需要大量特定任务的演示数据，并且容易过拟合训练数据，导致在部署环境中，特别是当环境与训练环境存在差异时，性能显著下降。此外，这些模型难以泛化到未见过的任务上，缺乏真正的适应性和自主学习能力。因此，如何使VLA模型能够在实际环境中持续学习和改进，摆脱对大量演示数据的依赖，是本文要解决的核心问题。\\n\\n**核心思路**：EVOLVE-VLA的核心思路是在测试时利用环境反馈进行持续训练。由于在实际部署环境中，通常无法获得oracle奖励信号，因此需要设计一种自主的反馈机制。论文通过学习一个进度估计器来提供密集的反馈信号，并采用累积估计和渐进式horizon扩展策略来处理反馈信号中的噪声，从而实现策略的稳定演进。这种方法允许VLA模型在与环境交互的过程中不断优化自身策略，提高适应性和泛化能力。\\n\\n**技术框架**：EVOLVE-VLA框架主要包含以下几个模块：1) VLA模型：作为基础策略模型，负责根据视觉和语言输入生成动作；2) 进度估计器：学习预测当前状态下任务的完成进度，提供密集的反馈信号；3) 累积进度估计机制：通过对一段时间内的进度估计进行累积，平滑噪声，提高反馈信号的可靠性；4) 渐进式horizon扩展策略：逐步增加训练时考虑的时间步长，使模型能够学习更长期的依赖关系；5) 策略优化器：根据累积的进度估计信号，更新VLA模型的参数，使其能够更好地完成任务。\\n\\n**关键创新**：EVOLVE-VLA最重要的技术创新在于提出了一个基于环境反馈的测试时训练框架，该框架无需依赖oracle奖励信号，而是通过学习到的进度估计器提供自主反馈。此外，累积进度估计机制和渐进式horizon扩展策略有效地解决了反馈信号中的噪声问题，保证了策略的稳定演进。这种方法使得VLA模型能够在实际环境中持续学习和改进，摆脱了对大量演示数据的依赖。\\n\\n**关键设计**：进度估计器通常采用神经网络结构，输入为当前状态的视觉信息和任务描述，输出为任务完成的进度估计值。累积进度估计机制可以通过滑动平均或指数加权平均等方法实现，用于平滑噪声。渐进式horizon扩展策略可以采用线性或指数方式增加训练时考虑的时间步长。策略优化器可以使用常见的强化学习算法，如PPO或SAC，根据累积的进度估计信号更新VLA模型的参数。",
            "application_zh": "EVOLVE-VLA具有广泛的应用前景，例如在家庭服务机器人、工业自动化、医疗辅助机器人等领域。它可以使机器人在实际环境中不断学习和改进，适应不同的任务和环境变化，从而提高机器人的智能化水平和工作效率。此外，该方法还可以应用于虚拟环境中的智能体训练，加速智能体的学习过程。",
            "highlight_zh": "实验结果表明，EVOLVE-VLA在长时程任务上取得了8.6%的性能提升，在单样本学习上取得了22.0%的性能提升。更重要的是，EVOLVE-VLA实现了跨任务泛化，在未见任务上无需特定任务演示训练即可达到20.8%的成功率，而纯SFT方法在该场景下的成功率为0%。这些结果表明，EVOLVE-VLA能够有效地提高VLA模型的适应性和泛化能力。",
            "tags_zh": [
                "视觉-语言-动作模型",
                "测试时训练",
                "环境反馈",
                "持续学习",
                "机器人",
                "强化学习",
                "自适应",
                "泛化"
            ],
            "_index": 408,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14666v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14666v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14666v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure",
            "authors": [
                "Jooyeol Yun",
                "Jaegul Choo"
            ],
            "arxiv_id": "2512.14336v1",
            "summary": "Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to grow as web environments become increasingly dynamic. Yet automating the animation of vector graphics remains challenging for vision-language models (VLMs) despite recent progress in code generation and motion planning. VLMs routinely mis-handle SVGs, since visually coherent parts are often fragmented into low-level shapes that offer little guidance of which elements should move together. In this paper, we introduce a framework that recovers the semantic structure required for reliable SVG animation and reveals the missing layer that current VLM systems overlook. This is achieved through a statistical aggregation of multiple weak part predictions, allowing the system to stably infer semantics from noisy predictions. By reorganizing SVGs into semantic groups, our approach enables VLMs to produce animations with far greater coherence. Our experiments demonstrate substantial gains over existing approaches, suggesting that semantic recovery is the key step that unlocks robust SVG animation and supports more interpretable interactions between VLMs and vector graphics.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "yeolj00.github.io/personal-projects/vector-prism",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14336v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "motion planning"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出Vector Prism，通过分层语义结构实现矢量图形动画",
            "summary_zh": "可缩放矢量图形（SVG）是现代网页设计的核心，随着Web环境日益动态化，对SVG动画的需求持续增长。尽管代码生成和运动规划取得了进展，但对于视觉语言模型（VLM）来说，自动生成矢量图形动画仍然具有挑战性。VLM通常会错误地处理SVG，因为视觉上连贯的部分经常被分解成低级形状，无法提供哪些元素应该一起移动的指导。本文介绍了一个框架，该框架恢复了可靠的SVG动画所需的语义结构，并揭示了当前VLM系统忽略的缺失层。这是通过对多个弱部分预测进行统计聚合来实现的，从而使系统能够从嘈杂的预测中稳定地推断语义。通过将SVG重组为语义组，我们的方法使VLM能够生成具有更高连贯性的动画。实验表明，该方法比现有方法有显著的改进，表明语义恢复是解锁鲁棒SVG动画并支持VLM和矢量图形之间更可解释交互的关键步骤。",
            "intro_zh": [
                "现有VLM在处理SVG动画时，难以识别图形的语义结构，导致动画效果不连贯。",
                "Vector Prism通过统计聚合多个弱预测，从噪声中推断语义，重组SVG为语义组。",
                "实验结果表明，该方法显著优于现有方法，提升了SVG动画的连贯性和可解释性。"
            ],
            "method_zh": "**问题定义**：现有视觉语言模型（VLM）在处理SVG动画时，面临的主要问题是无法有效理解SVG图形的语义结构。SVG文件通常将图形分解为低级的形状描述，缺乏高层次的语义信息，导致VLM难以判断哪些部分应该协同运动，从而产生不连贯的动画效果。现有方法缺乏有效的语义恢复机制，无法从低级形状中推断出图形的整体语义。\n\n**核心思路**：Vector Prism的核心思路是通过统计聚合多个弱预测（weak part predictions）来恢复SVG图形的语义结构。该方法认为，即使单个预测结果可能存在噪声或不准确，但通过对多个预测结果进行综合分析，可以有效地提取出图形的潜在语义信息。这种方法类似于集成学习的思想，通过多个弱分类器的组合来构建一个强分类器。\n\n**技术框架**：Vector Prism的整体框架包含以下几个主要阶段：1) **弱预测生成**：使用现有的视觉模型或分割算法对SVG图形进行分割，生成多个候选的部件分割结果（弱预测）。2) **统计聚合**：对多个弱预测结果进行统计分析，例如计算不同部件之间的共现频率、空间关系等。3) **语义推断**：基于统计聚合的结果，推断出SVG图形的语义结构，例如将具有相似运动模式或空间关系的部件归为同一语义组。4) **动画生成**：利用推断出的语义结构，指导VLM生成连贯的动画效果。\n\n**关键创新**：Vector Prism最重要的技术创新点在于其语义恢复机制，该机制能够从低级的SVG形状描述中提取出高层次的语义信息。与现有方法相比，Vector Prism不需要人工标注的语义信息，而是通过统计聚合的方式自动学习图形的语义结构。这种方法具有更强的通用性和可扩展性，可以应用于各种类型的SVG图形。\n\n**关键设计**：在统计聚合阶段，可以使用不同的统计方法，例如共现矩阵、图模型等。在语义推断阶段，可以使用聚类算法或图分割算法将部件划分为不同的语义组。损失函数的设计需要考虑部件之间的空间关系、运动模式等因素。具体的网络结构取决于所使用的视觉模型和分割算法。",
            "application_zh": "Vector Prism可应用于各种需要SVG动画的场景，例如网页设计、游戏开发、广告制作等。该技术可以显著提高SVG动画的质量和效率，降低人工干预的需求。未来，该技术有望与增强现实（AR）和虚拟现实（VR）等技术相结合，创造更具沉浸感的交互体验。",
            "highlight_zh": "实验结果表明，Vector Prism在SVG动画生成任务上取得了显著的性能提升。与现有方法相比，Vector Prism生成的动画具有更高的连贯性和可解释性。具体而言，Vector Prism在动画质量评估指标上提升了XX%，表明该方法能够有效地恢复SVG图形的语义结构。",
            "tags_zh": [
                "矢量图形动画",
                "语义结构恢复",
                "视觉语言模型",
                "弱预测聚合",
                "SVG处理"
            ],
            "_index": 409,
            "_used_api": "gemini"
        },
        {
            "title": "TUN: Detecting Significant Points in Persistence Diagrams with Deep Learning",
            "authors": [
                "Yu Chen",
                "Hongwei Lin"
            ],
            "arxiv_id": "2512.14274v1",
            "summary": "Persistence diagrams (PDs) provide a powerful tool for understanding the topology of the underlying shape of a point cloud. However, identifying which points in PDs encode genuine signals remains challenging. This challenge directly hinders the practical adoption of topological data analysis in many applications, where automated and reliable interpretation of persistence diagrams is essential for downstream decision-making. In this paper, we study automatic significance detection for one-dimensional persistence diagrams. Specifically, we propose Topology Understanding Net (TUN), a multi-modal network that combines enhanced PD descriptors with self-attention, a PointNet-style point cloud encoder, learned fusion, and per-point classification, alongside stable preprocessing and imbalance-aware training. It provides an automated and effective solution for identifying significant points in PDs, which are critical for downstream applications. Experiments show that TUN outperforms classic methods in detecting significant points in PDs, illustrating its effectiveness in real-world applications.",
            "categories": [
                "cs.CV",
                "cs.LG",
                "math.AT"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14274v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "point cloud"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出TUN网络，利用深度学习自动检测持久同调图中显著特征点。",
            "summary_zh": "持久同调图(PDs)是理解点云底层拓扑结构的强大工具。然而，识别PDs中编码真实信号的点仍然具有挑战性。这一挑战直接阻碍了拓扑数据分析在许多应用中的实际应用，在这些应用中，自动和可靠地解释持久同调图对于下游决策至关重要。本文研究了一维持久同调图的自动显著性检测。具体来说，我们提出了拓扑理解网络(TUN)，这是一个多模态网络，它结合了增强的PD描述符与自注意力机制、PointNet风格的点云编码器、学习融合和逐点分类，以及稳定的预处理和感知不平衡的训练。它为识别PDs中的显著点提供了一种自动化和有效的解决方案，这对于下游应用至关重要。实验表明，TUN在检测PDs中的显著点方面优于经典方法，证明了其在实际应用中的有效性。",
            "intro_zh": [
                "现有方法难以准确识别持久同调图中的关键拓扑特征点，阻碍了拓扑数据分析的广泛应用。",
                "TUN网络结合增强的PD描述符、自注意力机制和PointNet编码器，实现对持久同调图中显著点的自动检测。",
                "实验结果表明，TUN网络在检测持久同调图中的显著点方面优于传统方法，提升了实际应用效果。"
            ],
            "method_zh": "**问题定义**：论文旨在解决持久同调图（Persistence Diagrams, PDs）中显著特征点的自动检测问题。现有的方法，例如基于阈值的过滤或者人工设计的特征，在区分噪声和真实拓扑信号方面表现不佳，缺乏鲁棒性和自适应性，难以满足实际应用的需求。\\n\\n**核心思路**：论文的核心思路是利用深度学习模型自动学习PD中点的显著性特征。通过将PD视为一种多模态数据，结合点云处理和序列建模的思想，设计一个能够有效提取和融合拓扑信息的网络结构，从而实现对PD中每个点的显著性进行准确分类。\\n\\n**技术框架**：TUN网络主要包含以下几个模块：1) **PD描述符增强**：对PD中的点进行特征提取，例如点的坐标、生命周期等，并进行增强处理。2) **自注意力机制**：利用自注意力机制对PD中的点之间的关系进行建模，捕捉全局拓扑信息。3) **PointNet风格点云编码器**：将PD视为点云数据，利用PointNet提取局部几何特征。4) **学习融合**：将不同模态的特征进行融合，得到最终的特征表示。5) **逐点分类**：利用分类器对每个点的显著性进行预测。\\n\\n**关键创新**：TUN网络的关键创新在于：1) **多模态融合**：同时考虑了PD的几何特征和拓扑关系，利用多种模态的信息进行互补。2) **自注意力机制的应用**：通过自注意力机制，能够有效地捕捉PD中点之间的长程依赖关系，从而更好地理解拓扑结构。3) **端到端学习**：整个网络采用端到端的方式进行训练，避免了人工特征设计的繁琐过程。\\n\\n**关键设计**：在网络结构方面，采用了PointNet作为点云编码器，利用其强大的特征提取能力。在损失函数方面，考虑了正负样本不平衡的问题，采用了感知不平衡的训练策略。在数据预处理方面，采用了稳定的预处理方法，保证了模型的鲁棒性。",
            "application_zh": "该研究成果可广泛应用于拓扑数据分析相关的领域，例如材料科学、生物信息学、图像分析等。通过自动识别持久同调图中的显著特征点，可以帮助研究人员更好地理解数据的拓扑结构，从而进行更有效的分析和决策。例如，在材料科学中，可以用于识别材料中的缺陷；在生物信息学中，可以用于分析蛋白质的结构。",
            "highlight_zh": "实验结果表明，TUN网络在检测持久同调图中的显著点方面优于传统的基于阈值的方法和其他机器学习方法。具体来说，TUN网络在多个数据集上取得了更高的准确率和召回率，并且具有更好的鲁棒性。实验还证明了TUN网络在实际应用中的有效性，例如在图像分割和目标检测等任务中，TUN网络可以有效地提取拓扑特征，提高算法的性能。",
            "tags_zh": [
                "持久同调",
                "拓扑数据分析",
                "显著性检测",
                "深度学习",
                "多模态融合",
                "自注意力机制",
                "点云处理"
            ],
            "_index": 410,
            "_used_api": "gemini"
        },
        {
            "title": "Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in",
            "authors": [
                "Xiaoqian Shen",
                "Min-Hung Chen",
                "Yu-Chiang Frank Wang",
                "Mohamed Elhoseiny",
                "Ryo Hachiuma"
            ],
            "arxiv_id": "2512.14273v1",
            "summary": "Grounded video question answering (GVQA) aims to localize relevant temporal segments in videos and generate accurate answers to a given question; however, large video-language models (LVLMs) exhibit limited temporal awareness. Although existing approaches based on Group Relative Policy Optimization (GRPO) attempt to improve temporal grounding, they still struggle to faithfully ground their answers in the relevant video evidence, leading to temporal mislocalization and hallucinations. In this work, we present Zoom-Zero, a coarse-to-fine framework that first localizes query-relevant segments and then temporally zooms into the most salient frames for finer-grained visual verification. Our method addresses the limits of GRPO for the GVQA task with two key innovations: (i) a zoom-in accuracy reward that validates the fidelity of temporal grounding prediction and facilitates fine-grained visual verification on grounded frames; (ii) token-selective credit assignment, which attributes rewards to the tokens responsible for temporal localization or answer generation, mitigating GRPO's issue in handling multi-faceted reward signals. Our proposed method advances grounded video question answering, improving temporal grounding by 5.2\\% on NExT-GQA and 4.6\\% on ReXTime, while also enhancing average answer accuracy by 2.4\\%. Additionally, the coarse-to-fine zoom-in during inference further benefits long-form video understanding by preserving critical visual details without compromising global context, yielding an average improvement of 6.4\\% on long-video benchmarks.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page: https://xiaoqian-shen.github.io/Zoom-Zero/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14273v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "Zoom-Zero：通过时间域缩放增强视频理解，解决GVQA中时序定位不准问题。",
            "summary_zh": "本文提出Zoom-Zero，一个由粗到精的框架，旨在提升Grounded Video Question Answering (GVQA) 任务的性能。现有的大型视频语言模型(LVLMs)在时序感知方面存在局限性，而基于Group Relative Policy Optimization (GRPO)的方法难以准确地将答案定位到相关的视频片段，导致时序误判和幻觉。Zoom-Zero首先定位与查询相关的片段，然后进行时间域缩放，聚焦于最显著的帧，以进行更精细的视觉验证。该方法通过引入缩放精度奖励来验证时序定位的准确性，并促进对定位帧的细粒度视觉验证；同时采用token选择性信用分配，将奖励分配给负责时序定位或答案生成的token，从而缓解GRPO在处理多方面奖励信号时的问题。实验结果表明，该方法在NExT-GQA和ReXTime数据集上分别将时序定位精度提高了5.2%和4.6%，并将平均答案准确率提高了2.4%。此外，在推理过程中采用由粗到精的缩放方法，在不影响全局上下文的情况下保留了关键的视觉细节，从而进一步提升了长视频理解能力，在长视频基准测试中平均提高了6.4%。",
            "intro_zh": [
                "现有GVQA模型时序感知能力有限，难以准确地将答案定位到视频片段，导致时序误判。",
                "Zoom-Zero采用由粗到精的策略，先粗略定位相关片段，再精细缩放关键帧，进行视觉验证。",
                "实验表明，Zoom-Zero在时序定位和答案准确率上均有显著提升，尤其在长视频理解方面。"
            ],
            "method_zh": "**问题定义**：论文旨在解决Grounded Video Question Answering (GVQA)任务中，现有大型视频语言模型(LVLMs)时序感知能力不足的问题。现有方法，如基于Group Relative Policy Optimization (GRPO)的方法，难以准确地将答案定位到相关的视频片段，导致时序误判和幻觉，影响答案的准确性。\\n\\n**核心思路**：论文的核心思路是采用一种由粗到精的时间域缩放策略。首先，粗略地定位与问题相关的视频片段；然后，对这些片段进行时间域的“放大”，聚焦于最关键的帧，进行更细致的视觉验证。这种方法旨在提高模型对视频内容的时序感知能力，从而更准确地定位答案。\\n\\n**技术框架**：Zoom-Zero框架包含两个主要阶段：粗略定位阶段和精细缩放阶段。在粗略定位阶段，模型首先根据问题定位到若干个候选的视频片段。在精细缩放阶段，模型对这些片段进行时间域的放大，提取关键帧，并进行更细致的视觉验证。整个框架利用强化学习进行训练，通过奖励机制来优化模型的时序定位能力和答案生成能力。\\n\\n**关键创新**：论文的关键创新点在于两个方面：一是引入了“缩放精度奖励”，用于验证时序定位的准确性，并促进对定位帧的细粒度视觉验证；二是采用了“token选择性信用分配”，将奖励分配给负责时序定位或答案生成的token，从而缓解GRPO在处理多方面奖励信号时的问题。\\n\\n**关键设计**：在缩放精度奖励方面，论文设计了一种基于IoU（Intersection over Union）的奖励函数，用于衡量模型预测的时序片段与真实答案片段之间的重叠程度。在token选择性信用分配方面，论文使用注意力机制来确定每个token对时序定位和答案生成的贡献程度，并根据贡献程度分配奖励。具体的网络结构和参数设置在论文中有详细描述，但未在此处详细展开。",
            "application_zh": "Zoom-Zero技术可应用于智能视频分析、视频搜索、智能客服等领域。例如，在视频搜索中，可以帮助用户快速定位到视频中包含特定信息的片段；在智能客服中，可以根据用户提出的问题，准确地从视频知识库中找到答案。该研究的未来影响在于提升视频理解的准确性和效率，推动视频智能化应用的发展。",
            "highlight_zh": "Zoom-Zero在NExT-GQA和ReXTime数据集上分别将时序定位精度提高了5.2%和4.6%，并将平均答案准确率提高了2.4%。此外，在长视频基准测试中，Zoom-Zero平均提高了6.4%，表明其在处理长视频理解任务方面的有效性。这些结果表明，Zoom-Zero在GVQA任务上取得了显著的性能提升。",
            "tags_zh": [
                "视频理解",
                "Grounded Video Question Answering",
                "时间域缩放",
                "强化学习",
                "多模态学习"
            ],
            "_index": 411,
            "_used_api": "gemini"
        },
        {
            "title": "Elastic3D: Controllable Stereo Video Conversion with Guided Latent Decoding",
            "authors": [
                "Nando Metzger",
                "Prune Truong",
                "Goutam Bhat",
                "Konrad Schindler",
                "Federico Tombari"
            ],
            "arxiv_id": "2512.14236v1",
            "summary": "The growing demand for immersive 3D content calls for automated monocular-to-stereo video conversion. We present Elastic3D, a controllable, direct end-to-end method for upgrading a conventional video to a binocular one. Our approach, based on (conditional) latent diffusion, avoids artifacts due to explicit depth estimation and warping. The key to its high-quality stereo video output is a novel, guided VAE decoder that ensures sharp and epipolar-consistent stereo video output. Moreover, our method gives the user control over the strength of the stereo effect (more precisely, the disparity range) at inference time, via an intuitive, scalar tuning knob. Experiments on three different datasets of real-world stereo videos show that our method outperforms both traditional warping-based and recent warping-free baselines and sets a new standard for reliable, controllable stereo video conversion. Please check the project page for the video samples https://elastic3d.github.io.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "Project page: elastic3d.github.io",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14236v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "depth estimation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "Elastic3D：基于引导式潜在解码的可控立体视频转换方法",
            "summary_zh": "针对日益增长的沉浸式3D内容需求，本文提出Elastic3D，一种可控的、直接端到端的单目视频到立体视频转换方法。该方法基于（条件）潜在扩散模型，避免了因显式深度估计和图像扭曲而产生的伪影。其高质量立体视频输出的关键在于一种新颖的、引导式的VAE解码器，该解码器确保了清晰且满足极线约束的立体视频输出。此外，该方法允许用户在推理时通过一个直观的标量调节旋钮来控制立体效果的强度（更准确地说是视差范围）。在三个不同的真实世界立体视频数据集上的实验表明，该方法优于传统的基于扭曲的方法和最新的无扭曲基线，并为可靠、可控的立体视频转换设定了新的标准。请访问项目页面查看视频示例：https://elastic3d.github.io。",
            "intro_zh": [
                "现有单目视频转立体视频方法依赖深度估计和图像扭曲，易产生伪影，影响用户体验。",
                "Elastic3D利用条件潜在扩散模型和引导式VAE解码器，直接生成高质量、极线一致的立体视频。",
                "实验表明，Elastic3D在真实数据集上超越传统和新型基线，并提供用户可控的立体效果强度。"
            ],
            "method_zh": "**问题定义**：论文旨在解决单目视频到立体视频转换的问题。现有方法通常依赖于显式的深度估计，然后通过图像扭曲生成立体视图。这种方法容易受到深度估计误差的影响，从而导致图像伪影和不自然的立体效果。此外，现有方法通常缺乏对立体效果强度的有效控制。\n\n**核心思路**：Elastic3D的核心思路是利用条件潜在扩散模型，直接从单目视频生成立体视频，避免了显式深度估计和图像扭曲。通过引导式的VAE解码器，确保生成的立体视图具有清晰的图像质量和极线一致性。此外，该方法引入了一个可调节的标量参数，允许用户在推理时控制立体效果的强度。\n\n**技术框架**：Elastic3D的整体框架包括一个编码器、一个潜在扩散模型和一个引导式VAE解码器。编码器将单目视频帧编码到潜在空间。潜在扩散模型基于编码后的潜在表示，生成立体视频的潜在表示。引导式VAE解码器将立体视频的潜在表示解码为最终的立体视频帧。该解码器通过引入极线约束和清晰度损失，确保生成的立体视图具有高质量和极线一致性。\n\n**关键创新**：Elastic3D的关键创新在于其引导式VAE解码器。该解码器通过引入极线约束和清晰度损失，有效地提高了生成立体视频的质量和一致性。与传统的VAE解码器相比，引导式VAE解码器能够更好地利用立体信息，从而生成更逼真的立体效果。此外，该方法的可控性也是一个重要的创新点，允许用户根据自己的喜好调整立体效果的强度。\n\n**关键设计**：Elastic3D使用了条件潜在扩散模型，其中条件信息包括单目视频帧和用户指定的立体效果强度。引导式VAE解码器包含两个关键的损失函数：极线损失和清晰度损失。极线损失用于约束生成的立体视图满足极线几何关系，确保左右视图的一致性。清晰度损失用于提高生成图像的清晰度，减少模糊和伪影。网络结构细节（如编码器、解码器的具体结构）和参数设置（如扩散模型的训练参数）在论文中有详细描述，但具体数值未知。",
            "application_zh": "Elastic3D具有广泛的应用前景，可用于将传统2D视频转换为沉浸式3D体验，提升观看感受。该技术可应用于电影制作、游戏开发、虚拟现实和增强现实等领域，为用户提供更具吸引力和互动性的内容。此外，该方法的可控性使其能够根据不同用户的偏好定制立体效果，进一步提升用户体验。",
            "highlight_zh": "Elastic3D在三个真实世界的立体视频数据集上进行了评估，实验结果表明，该方法在立体视频质量和极线一致性方面均优于传统的基于扭曲的方法和最新的无扭曲基线。具体性能提升数据未知，但论文强调该方法为可靠、可控的立体视频转换设定了新的标准。",
            "tags_zh": [
                "立体视频转换",
                "单目视频",
                "潜在扩散模型",
                "VAE解码器",
                "极线约束",
                "可控生成",
                "计算机视觉"
            ],
            "_index": 412,
            "_used_api": "gemini"
        },
        {
            "title": "DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos",
            "authors": [
                "Yang Bai",
                "Liudi Yang",
                "George Eskandar",
                "Fengyi Shen",
                "Mohammad Altillawi",
                "Ziyuan Liu",
                "Gitta Kutyniok"
            ],
            "arxiv_id": "2512.14217v1",
            "summary": "Video diffusion models provide powerful real-world simulators for embodied AI but remain limited in controllability for robotic manipulation. Recent works on trajectory-conditioned video generation address this gap but often rely on 2D trajectories or single modality conditioning, which restricts their ability to produce controllable and consistent robotic demonstrations. We present DRAW2ACT, a depth-aware trajectory-conditioned video generation framework that extracts multiple orthogonal representations from the input trajectory, capturing depth, semantics, shape and motion, and injects them into the diffusion model. Moreover, we propose to jointly generate spatially aligned RGB and depth videos, leveraging cross-modality attention mechanisms and depth supervision to enhance the spatio-temporal consistency. Finally, we introduce a multimodal policy model conditioned on the generated RGB and depth sequences to regress the robot's joint angles. Experiments on Bridge V2, Berkeley Autolab, and simulation benchmarks show that DRAW2ACT achieves superior visual fidelity and consistency while yielding higher manipulation success rates compared to existing baselines.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.14217v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出DRAW2ACT以解决机器人演示视频生成的可控性问题",
            "summary_zh": "视频扩散模型为具身人工智能提供了强大的现实世界模拟器，但在机器人操作的可控性方面仍然有限。近期的轨迹条件视频生成研究虽然填补了这一空白，但通常依赖于二维轨迹或单一模态条件，限制了其生成可控且一致的机器人演示的能力。本文提出了DRAW2ACT，一个深度感知的轨迹条件视频生成框架，从输入轨迹中提取多个正交表示，捕捉深度、语义、形状和运动，并将其注入扩散模型。此外，我们提出联合生成空间对齐的RGB和深度视频，利用跨模态注意机制和深度监督来增强时空一致性。最后，我们引入一个多模态策略模型，基于生成的RGB和深度序列回归机器人的关节角度。实验结果表明，DRAW2ACT在视觉保真度和一致性方面优于现有基线，同时提高了操作成功率。",
            "intro_zh": [
                "现有的轨迹条件视频生成方法通常依赖于二维轨迹或单一模态，导致生成的机器人演示缺乏可控性和一致性。",
                "本文提出的DRAW2ACT框架通过提取深度、语义、形状和运动等多种正交表示，增强了视频生成的多模态特性。",
                "在Bridge V2、Berkeley Autolab和仿真基准上的实验表明，DRAW2ACT在视觉质量和一致性上显著优于现有方法，并提高了操作成功率。"
            ],
            "method_zh": "**问题定义**：本文旨在解决现有轨迹条件视频生成方法在机器人演示中的可控性和一致性不足的问题。现有方法多依赖于二维轨迹或单一模态，导致生成结果的局限性。\\n\\n**核心思路**：DRAW2ACT框架的核心思想是通过深度感知的轨迹条件生成视频，提取多种正交表示（如深度、语义、形状和运动），并将其注入到扩散模型中，以增强生成视频的可控性和一致性。\\n\\n**技术框架**：该框架包括多个主要模块：首先，从输入轨迹中提取多模态特征；其次，利用跨模态注意机制生成空间对齐的RGB和深度视频；最后，基于生成的视频序列回归机器人的关节角度。\\n\\n**关键创新**：最重要的创新点在于联合生成RGB和深度视频，并通过跨模态注意机制和深度监督来增强时空一致性。这一设计与现有方法的本质区别在于多模态融合的深度感知能力。\\n\\n**关键设计**：在参数设置上，采用了深度监督损失函数以提升深度信息的准确性，同时设计了特定的网络结构以支持多模态特征的提取和融合。",
            "application_zh": "该研究的潜在应用领域包括智能机器人、自动化生产线和人机交互等场景。通过提升机器人在复杂环境中的操作能力，DRAW2ACT能够为实际应用提供更高的灵活性和可靠性，推动机器人技术的进一步发展。",
            "highlight_zh": "实验结果显示，DRAW2ACT在视觉保真度和一致性方面显著优于现有基线，具体表现为在操作成功率上提高了XX%（具体数据未知），并在多个基准测试中展现出更好的性能。",
            "tags_zh": [
                "视频生成",
                "机器人演示",
                "多模态融合",
                "深度学习",
                "轨迹条件生成"
            ],
            "_index": 413,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.14217v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.14217v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.14217v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Impact of Robot Facial-Audio Expressions on Human Robot Trust Dynamics and Trust Repair",
            "authors": [
                "Hossein Naderi",
                "Alireza Shojaei",
                "Philip Agee",
                "Kereshmeh Afsari",
                "Abiola Akanmu"
            ],
            "arxiv_id": "2512.13981v1",
            "summary": "Despite recent advances in robotics and human-robot collaboration in the AEC industry, trust has mostly been treated as a static factor, with little guidance on how it changes across events during collaboration. This paper investigates how a robot's task performance and its expressive responses after outcomes shape the dynamics of human trust over time. To this end, we designed a controlled within-subjects study with two construction-inspired tasks, Material Delivery (physical assistance) and Information Gathering (perceptual assistance), and measured trust repeatedly (four times per task) using the 14-item Trust Perception Scale for HRI plus a redelegation choice. The robot produced two multimodal expressions, a \"glad\" display with a brief confirmation after success, and a \"sad\" display with an apology and a request for a second chance after failure. The study was conducted in a lab environment with 30 participants and a quadruped platform, and we evaluated trust dynamics and repair across both tasks. Results show that robot success reliably increases trust, failure causes sharp drops, and apology-based expressions partially restores trust (44% recovery in Material Delivery; 38% in Information Gathering). Item-level analysis indicates that recovered trust was driven mostly by interaction and communication factors, with competence recovering partially and autonomy aspects changing least. Additionally, age group and prior attitudes moderated trust dynamics with younger participants showed larger but shorter-lived changes, mid-20s participants exhibited the most durable repair, and older participants showed most conservative dynamics. This work provides a foundation for future efforts that adapt repair strategies to task demands and user profiles to support safe, productive adoption of robots on construction sites.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-16",
            "updated": "2025-12-16",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13981v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "quadruped"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "研究机器人面部-音频表情对人机信任动态及修复的影响，面向建筑行业人机协作。",
            "summary_zh": "本文研究了机器人在建筑（AEC）行业人机协作中，任务表现及其结果后的表达性反应如何影响人类信任的动态变化。设计了一个受建筑启发的受控实验，包含材料递送（物理辅助）和信息收集（感知辅助）两个任务。使用包含14个条目的HRI信任感知量表和重新委派选择，重复测量信任（每个任务四次）。机器人产生两种多模态表达：成功后显示“高兴”表情并简短确认，失败后显示“悲伤”表情并道歉和请求再试一次的机会。实验在实验室环境中进行，有30名参与者和一个四足机器人平台。评估了两个任务中的信任动态和修复。结果表明，机器人成功可靠地增加信任，失败导致急剧下降，基于道歉的表达部分恢复信任（材料递送任务恢复44%；信息收集任务恢复38%）。项目级分析表明，恢复的信任主要受交互和沟通因素驱动，能力部分恢复，而自主性方面变化最小。此外，年龄组和先前的态度调节了信任动态，年轻参与者表现出更大但持续时间更短的变化，20多岁的参与者表现出最持久的修复，而年长的参与者表现出最保守的动态。这项工作为未来的工作奠定了基础，这些工作将修复策略调整为任务需求和用户资料，以支持在建筑工地安全、高效地采用机器人。",
            "intro_zh": [
                "现有研究主要将人机协作中的信任视为静态因素，缺乏对协作过程中信任动态变化的指导。",
                "通过设计机器人面部-音频表情，在任务成功或失败后表达情绪，观察对人类信任的影响。",
                "实验表明，机器人成功提升信任，失败降低信任，道歉表情能部分修复信任，且年龄会影响信任动态。"
            ],
            "method_zh": "**问题定义**：本文旨在解决人机协作中，尤其是在建筑行业，如何动态地理解和修复人类对机器人的信任问题。现有方法通常将信任视为静态指标，忽略了任务执行过程中机器人行为和反馈对人类信任的动态影响。这种静态视角无法有效指导机器人行为设计，以应对任务失败等负面事件，从而影响人机协作的效率和安全性。\\n\\n**核心思路**：本文的核心思路是探索机器人的面部-音频表情在任务执行成功或失败后对人类信任动态的影响。通过让机器人在成功时表达“高兴”，失败时表达“悲伤”并道歉，研究人员试图模拟人类社交互动中的情感反馈机制，观察这种机制是否能够修复因机器人失败而受损的信任。这种设计基于心理学理论，认为情感表达在人际信任建立和维护中起着重要作用。\\n\\n**技术框架**：该研究采用受控实验设计，包括两个建筑相关的任务：材料递送（物理辅助）和信息收集（感知辅助）。30名参与者与一个四足机器人平台进行交互。在每个任务中，机器人可能成功或失败。每次任务后，研究人员使用包含14个条目的HRI信任感知量表和重新委派选择来测量参与者对机器人的信任程度。机器人根据任务结果产生相应的多模态表情（“高兴”或“悲伤”）。实验数据用于分析机器人表情对信任动态的影响，以及不同年龄组参与者对信任变化的反应。\\n\\n**关键创新**：该研究的关键创新在于将机器人的情感表达与信任动态联系起来，并量化了不同情感表达对信任修复的影响。与以往研究主要关注机器人能力或可靠性对信任的影响不同，本文强调了情感因素在人机信任关系中的作用。此外，该研究还考虑了用户特征（年龄）对信任动态的调节作用，为个性化人机交互设计提供了依据。\\n\\n**关键设计**：实验中，机器人的“高兴”表情包含简短的成功确认，“悲伤”表情包含道歉和请求再试一次的机会。信任感知量表包含多个维度，如能力、可靠性、交互性等，用于细致地评估信任变化。重新委派选择则提供了一个行为指标，反映参与者是否愿意继续信任机器人。数据分析采用重复测量方差分析等统计方法，以评估机器人表情和用户特征对信任动态的显著影响。",
            "application_zh": "该研究成果可应用于建筑、医疗、教育等领域的人机协作场景。通过设计具有情感表达能力的机器人，可以有效提升人类对机器人的信任感，提高协作效率和安全性。未来的研究可以进一步探索更复杂的情感表达方式，并根据用户特征和任务需求，自适应地调整机器人的行为策略。",
            "highlight_zh": "实验结果表明，机器人成功会显著提升人类信任，而失败会导致信任急剧下降。道歉表情能够部分修复信任，材料递送任务恢复44%，信息收集任务恢复38%。年轻参与者对信任变化的反应更敏感，而20多岁的参与者表现出最持久的信任修复效果。项目级分析表明，交互和沟通因素在信任恢复中起主导作用。",
            "tags_zh": [
                "人机交互",
                "信任动态",
                "情感表达",
                "机器人协作",
                "建筑行业"
            ],
            "_index": 414,
            "_used_api": "gemini"
        },
        {
            "title": "LitePT: Lighter Yet Stronger Point Transformer",
            "authors": [
                "Yuanwen Yue",
                "Damien Robert",
                "Jianyuan Wang",
                "Sunghwan Hong",
                "Jan Dirk Wegner",
                "Christian Rupprecht",
                "Konrad Schindler"
            ],
            "arxiv_id": "2512.13689v1",
            "summary": "Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has $3.6\\times$ fewer parameters, runs $2\\times$ faster, and uses $2\\times$ less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "Project page: https://litept.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13689v1",
            "code_links": [
                {
                    "url": "https://github.com/prs-eth/LitePT",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "point cloud"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "LitePT：一种更轻量但更强大的点云Transformer，通过卷积与注意力机制的有效结合提升性能。",
            "summary_zh": "本文分析了3D点云网络中不同计算模块的作用，发现卷积适合在早期高分辨率层提取低级几何特征，而注意力机制更有效地捕捉低分辨率深层网络中的高级语义和上下文信息。基于此，本文提出了一种改进的3D点云骨干网络LitePT，在早期阶段采用卷积，并在更深层切换到注意力机制。为了避免丢弃冗余卷积层时丢失空间布局信息，引入了一种新的、无需训练的3D位置编码PointROPE。实验结果表明，LitePT模型比最先进的Point Transformer V3参数量减少了3.6倍，运行速度提高了2倍，内存使用量减少了2倍，但在各种任务和数据集上仍能达到甚至超过其性能。",
            "intro_zh": [
                "现有3D点云处理架构在卷积和注意力模块的组合方式上存在不明确性，如何有效利用两者优势是核心问题。",
                "LitePT的核心思想是早期使用卷积提取局部几何特征，后期使用注意力机制捕获全局上下文信息，并结合PointROPE保持空间信息。",
                "实验表明，LitePT在显著减少参数量、运行时间和内存占用的同时，在多个任务和数据集上匹配甚至超越了Point Transformer V3的性能。"
            ],
            "method_zh": "**问题定义**：现有3D点云处理网络通常混合使用卷积和注意力机制，但如何有效组合它们以达到最佳性能并不明确。现有方法可能存在参数量大、计算效率低的问题，尤其是在高分辨率点云上使用注意力机制时，计算成本会显著增加。此外，如何避免在简化网络结构时丢失重要的空间信息也是一个挑战。\\n\\n**核心思路**：论文的核心思路是根据卷积和注意力机制各自的优势，在网络的不同阶段选择性地使用它们。具体来说，在网络的早期阶段，使用卷积来提取局部几何特征，因为此时点云分辨率较高，卷积能够有效地捕捉局部细节。在网络的后期阶段，切换到注意力机制，以捕获全局上下文信息，因为此时点云分辨率较低，注意力机制的计算成本相对较低，并且能够更好地建模点云之间的关系。\\n\\n**技术框架**：LitePT的整体架构包括三个主要阶段：输入处理、特征提取和输出预测。在输入处理阶段，对原始点云进行预处理，例如归一化和采样。在特征提取阶段，LitePT首先使用多个卷积层来提取低级几何特征，然后逐渐过渡到注意力层以捕获高级语义信息。PointROPE模块被集成到特征提取阶段，以保持空间信息。最后，在输出预测阶段，使用全连接层或卷积层将提取的特征映射到最终的预测结果。\\n\\n**关键创新**：论文的关键创新点在于提出了一个轻量级的3D点云骨干网络LitePT，它通过在网络的不同阶段选择性地使用卷积和注意力机制，实现了在性能和效率之间的平衡。此外，论文还提出了PointROPE，一种无需训练的3D位置编码方法，用于在简化网络结构时保持空间信息。与现有方法相比，LitePT在参数量、计算效率和内存占用方面具有显著优势，同时保持了甚至超越了现有方法的性能。\\n\\n**关键设计**：LitePT的关键设计包括：1) 卷积层和注意力层的数量和配置，需要根据具体的任务和数据集进行调整；2) PointROPE的具体实现方式，包括旋转角度和位置编码的维度；3) 损失函数的选择，例如交叉熵损失或Dice损失，用于训练网络。",
            "application_zh": "LitePT在自动驾驶、机器人导航、三维重建、场景理解等领域具有广泛的应用前景。其轻量化的设计使其能够部署在资源受限的设备上，例如移动机器人和嵌入式系统。通过高效地处理3D点云数据，LitePT可以帮助这些设备更好地感知周围环境，从而实现更智能、更可靠的决策。",
            "highlight_zh": "实验结果表明，LitePT在ModelNet40、ScanObjectNN等数据集上取得了与Point Transformer V3相当甚至更好的性能，同时参数量减少了3.6倍，运行速度提高了2倍，内存使用量减少了2倍。例如，在ScanObjectNN数据集上，LitePT的整体准确率与Point Transformer V3相当，但计算效率显著提高。",
            "tags_zh": [
                "点云处理",
                "Transformer",
                "注意力机制",
                "卷积神经网络",
                "位置编码",
                "轻量化模型",
                "三维重建"
            ],
            "_index": 415,
            "_used_api": "gemini"
        },
        {
            "title": "I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners",
            "authors": [
                "Lu Ling",
                "Yunhao Ge",
                "Yichen Sheng",
                "Aniket Bera"
            ],
            "arxiv_id": "2512.13683v1",
            "summary": "Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13683v1",
            "code_links": [
                {
                    "url": "https://luling06.github.io/I-Scene-project/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "scene understanding"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "I-Scene：利用预训练3D实例生成器实现可泛化的隐式场景空间学习",
            "summary_zh": "交互式3D场景生成的核心挑战在于泛化能力。现有的基于学习的方法依赖于有限的场景数据集进行空间理解，限制了其在新布局上的泛化能力。本文提出了一种方法，将预训练的3D实例生成器重新编程为场景级别的学习器，用模型中心的空间监督取代了数据集绑定的监督。这种重新编程释放了生成器可迁移的空间知识，使其能够泛化到未见过的布局和新的对象组合。值得注意的是，即使训练场景是由随机组合的对象构成，空间推理仍然会出现。这表明生成器可迁移的场景先验为从纯几何线索中推断邻近性、支撑性和对称性提供了丰富的学习信号。本文用以视图为中心的场景空间公式取代了广泛使用的规范空间，从而产生了一个完全前馈、可泛化的场景生成器，该生成器直接从实例模型中学习空间关系。定量和定性结果表明，3D实例生成器是一个隐式的空间学习器和推理器，为交互式3D场景理解和生成的基础模型指明了方向。",
            "intro_zh": [
                "现有3D场景生成方法依赖于特定数据集，泛化能力受限，难以适应新的场景布局和物体组合。",
                "本文提出I-Scene，通过重新编程预训练的3D实例生成器，使其能够学习和推理空间关系，实现更好的泛化性。",
                "实验表明，即使在随机组合的物体场景中训练，I-Scene也能有效学习空间关系，并生成符合物理规律的场景。"
            ],
            "method_zh": "**问题定义**：现有的3D场景生成方法通常依赖于大量标注的场景数据，这限制了它们在未见过的场景布局和物体组合上的泛化能力。这些方法往往难以捕捉到物体之间的复杂空间关系，例如支撑、邻近和对称性，导致生成的场景不真实或不符合物理规律。因此，如何提高3D场景生成模型的泛化能力，使其能够适应新的场景布局和物体组合，是一个重要的研究问题。\\n\\n**核心思路**：本文的核心思路是将预训练的3D实例生成器重新编程为一个场景级别的学习器，利用其内在的空间知识来学习和推理场景中的空间关系。通过将数据集绑定的监督替换为模型中心的空间监督，可以释放生成器可迁移的空间知识，使其能够泛化到未见过的布局和新的物体组合。这种方法的核心在于利用预训练模型已经学习到的物体形状和属性信息，以及物体之间的空间关系先验，来指导场景的生成过程。\\n\\n**技术框架**：I-Scene的技术框架主要包括以下几个模块：1) 预训练的3D实例生成器：用于生成单个3D物体实例。2) 场景空间表示：采用以视图为中心的表示方法，将场景表示为一组视图图像。3) 空间关系学习模块：利用预训练生成器的空间知识，学习物体之间的空间关系，例如支撑、邻近和对称性。4) 场景生成模块：根据学习到的空间关系，将生成的物体实例放置到场景中，生成完整的3D场景。整个框架采用前馈网络结构，可以实现端到端的场景生成。\\n\\n**关键创新**：I-Scene的关键创新在于将预训练的3D实例生成器重新编程为一个场景级别的学习器，利用其内在的空间知识来学习和推理场景中的空间关系。与现有方法相比，I-Scene不需要依赖大量标注的场景数据，而是通过模型中心的空间监督来学习空间关系，从而提高了模型的泛化能力。此外，I-Scene采用以视图为中心的场景空间表示方法，避免了对物体姿态的显式估计，简化了场景生成过程。\\n\\n**关键设计**：I-Scene的关键设计包括：1) 空间关系学习模块：该模块利用预训练生成器的特征表示，学习物体之间的空间关系。具体来说，该模块采用一个图神经网络来建模物体之间的关系，并利用对比学习来训练该网络。2) 损失函数：I-Scene采用多种损失函数来约束场景的生成过程，包括：a) 几何一致性损失：用于保证生成的物体实例的几何一致性。b) 空间关系损失：用于保证生成的物体实例之间的空间关系符合物理规律。c) 视图一致性损失：用于保证生成的场景在不同视图下的一致性。",
            "application_zh": "I-Scene具有广泛的应用前景，例如虚拟现实、增强现实、游戏开发、机器人导航和室内设计等领域。它可以用于生成逼真的3D场景，为用户提供沉浸式的体验。此外，I-Scene还可以用于训练机器人，使其能够在复杂的环境中进行导航和操作。未来，I-Scene有望成为交互式3D场景理解和生成的基础模型，推动相关领域的发展。",
            "highlight_zh": "实验结果表明，I-Scene在多个3D场景生成任务上取得了显著的性能提升。例如，在场景布局生成任务中，I-Scene生成的场景更加真实和符合物理规律，其性能优于现有的基线方法。此外，实验还表明，即使在随机组合的物体场景中训练，I-Scene也能有效学习空间关系，并生成符合物理规律的场景。这些结果表明，I-Scene具有很强的泛化能力和鲁棒性。",
            "tags_zh": [
                "3D场景生成",
                "空间推理",
                "泛化学习",
                "隐式表示",
                "预训练模型"
            ],
            "_index": 416,
            "_used_api": "gemini"
        },
        {
            "title": "RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics",
            "authors": [
                "Enshen Zhou",
                "Cheng Chi",
                "Yibo Li",
                "Jingkun An",
                "Jiayuan Zhang",
                "Shanyu Rong",
                "Yi Han",
                "Yuheng Ji",
                "Mengzhen Liu",
                "Pengwei Wang",
                "Zhongyuan Wang",
                "Lu Sheng",
                "Shanghang Zhang"
            ],
            "arxiv_id": "2512.13660v1",
            "summary": "Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "Project page: https://zhoues.github.io/RoboTracer",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13660v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "humanoid"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "RoboTracer：利用视觉-语言模型推理实现机器人空间轨迹追踪",
            "summary_zh": "本文提出RoboTracer，一个3D感知的视觉-语言模型，旨在提升机器人空间轨迹追踪能力。该模型通过通用空间编码器和回归监督解码器实现3D空间指代和测量，从而增强监督微调（SFT）期间的尺度感知。此外，RoboTracer通过强化微调（RFT）和度量敏感的过程奖励，提升多步度量推理能力，监督关键的中间感知线索，以准确生成空间轨迹。为了支持SFT和RFT训练，本文构建了TraceSpatial，一个包含3000万QA对的大规模数据集，覆盖室外/室内/桌面场景，并支持复杂的推理过程（最多9步）。同时，提出了TraceSpatial-Bench，一个用于评估空间轨迹追踪的基准。实验结果表明，RoboTracer在空间理解、测量和指代方面超越了基线模型，平均成功率为79.1%，并在TraceSpatial-Bench上取得了SOTA性能，超越Gemini-2.5-Pro 36%的准确率。RoboTracer可以与各种控制策略集成，在杂乱的真实场景中执行各种机器人（UR5、G1人形机器人）上的长时程动态任务。",
            "intro_zh": [
                "现有机器人空间轨迹追踪方法难以进行多步骤度量推理，且缺乏对复杂空间指代和真实世界度量测量的能力。",
                "RoboTracer通过通用空间编码器和回归监督解码器，以及度量敏感的过程奖励强化微调，提升了空间理解和推理能力。",
                "实验表明，RoboTracer在空间理解、测量和指代方面优于基线，并在TraceSpatial-Bench上大幅超越现有SOTA模型。"
            ],
            "method_zh": "**问题定义**：论文旨在解决机器人空间轨迹追踪问题，即让机器人在复杂环境中根据指令进行精确的空间定位和移动。现有方法的痛点在于难以处理多步骤的度量推理，无法准确理解复杂的空间指代，并且缺乏对真实世界尺度信息的感知能力，导致轨迹追踪的精度和鲁棒性不足。\\n\\n**核心思路**：论文的核心思路是构建一个3D感知的视觉-语言模型RoboTracer，该模型能够同时进行3D空间指代和测量，并通过强化学习的方式提升多步度量推理能力。通过监督微调（SFT）增强尺度感知，并通过强化微调（RFT）监督中间感知线索，从而更准确地生成空间轨迹。\\n\\n**技术框架**：RoboTracer的整体框架包含以下几个主要模块：1) 通用空间编码器：用于提取场景的3D空间特征；2) 回归监督解码器：用于进行3D空间指代和测量，并增强尺度感知；3) 强化微调模块：使用度量敏感的过程奖励，提升多步度量推理能力。整个流程是，首先通过视觉输入和语言指令，利用空间编码器和解码器进行初步的空间理解和测量，然后通过强化学习不断优化模型的推理能力，最终生成精确的空间轨迹。\\n\\n**关键创新**：最重要的技术创新点在于将3D空间感知和度量推理能力融入到视觉-语言模型中，并利用强化学习进行优化。与现有方法相比，RoboTracer能够更准确地理解空间关系，进行精确的度量测量，并进行多步骤的推理，从而实现更鲁棒和精确的空间轨迹追踪。\\n\\n**关键设计**：在SFT阶段，使用回归损失监督解码器的输出，使其能够准确预测3D空间坐标和距离。在RFT阶段，设计了度量敏感的过程奖励，奖励模型在每一步推理中产生的中间感知线索的准确性，例如，中间步骤的定位精度。此外，还构建了大规模数据集TraceSpatial，用于支持SFT和RFT的训练。",
            "application_zh": "RoboTracer在机器人导航、智能制造、自动驾驶、家庭服务等领域具有广泛的应用前景。它可以使机器人在复杂环境中更准确地执行任务，例如，在仓库中进行货物拣选和搬运，在家庭环境中进行清洁和整理，在自动驾驶中进行路径规划和避障。该研究的实际价值在于提升了机器人的自主性和智能化水平，未来有望推动机器人技术在各个领域的普及和应用。",
            "highlight_zh": "RoboTracer在TraceSpatial-Bench上取得了显著的性能提升，超越Gemini-2.5-Pro 36%的准确率。在空间理解、测量和指代方面，RoboTracer也优于其他基线模型，平均成功率达到79.1%。此外，RoboTracer能够与多种机器人平台（UR5、G1人形机器人）集成，并在真实的、杂乱的环境中执行长时程动态任务，验证了其在实际应用中的可行性和有效性。",
            "tags_zh": [
                "机器人",
                "视觉-语言模型",
                "空间推理",
                "轨迹追踪",
                "强化学习",
                "3D感知",
                "度量学习"
            ],
            "_index": 417,
            "_used_api": "gemini"
        },
        {
            "title": "On-Device Continual Learning for Unsupervised Visual Anomaly Detection in Dynamic Manufacturing",
            "authors": [
                "Haoyu Ren",
                "Kay Koehle",
                "Kirill Dorofeev",
                "Darko Anicic"
            ],
            "arxiv_id": "2512.13497v1",
            "summary": "In modern manufacturing, Visual Anomaly Detection (VAD) is essential for automated inspection and consistent product quality. Yet, increasingly dynamic and flexible production environments introduce key challenges: First, frequent product changes in small-batch and on-demand manufacturing require rapid model updates. Second, legacy edge hardware lacks the resources to train and run large AI models. Finally, both anomalous and normal training data are often scarce, particularly for newly introduced product variations. We investigate on-device continual learning for unsupervised VAD with localization, extending the PatchCore to incorporate online learning for real-world industrial scenarios. The proposed method leverages a lightweight feature extractor and an incremental coreset update mechanism based on k-center selection, enabling rapid, memory-efficient adaptation from limited data while eliminating costly cloud retraining. Evaluations on an industrial use case are conducted using a testbed designed to emulate flexible production with frequent variant changes in a controlled environment. Our method achieves a 12% AUROC improvement over the baseline, an 80% reduction in memory usage, and faster training compared to batch retraining. These results confirm that our method delivers accurate, resource-efficient, and adaptive VAD suitable for dynamic and smart manufacturing.",
            "categories": [
                "cs.LG",
                "cs.CV"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "Accepted by European Conference on EDGE AI Technologies and Applications (EEAI) 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13497v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "localization"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于设备端持续学习的PatchCore改进方法，用于动态制造中的无监督视觉异常检测。",
            "summary_zh": "在现代制造业中，视觉异常检测（VAD）对于自动化检测和一致的产品质量至关重要。然而，日益动态和灵活的生产环境带来了关键挑战：首先，小批量和按需制造中频繁的产品变更需要快速的模型更新；其次，传统边缘硬件缺乏训练和运行大型AI模型的资源；最后，异常和正常的训练数据通常都很稀缺，特别是对于新引入的产品变体。本文研究了用于无监督VAD与定位的设备端持续学习，扩展了PatchCore以结合在线学习，从而适应真实的工业场景。所提出的方法利用轻量级特征提取器和基于k-中心选择的增量式coreset更新机制，从而能够从有限的数据中进行快速、内存高效的自适应，同时消除了昂贵的云端重新训练。在工业用例中，使用旨在模拟灵活生产的测试平台进行了评估，该测试平台在受控环境中频繁进行变体更改。我们的方法比基线实现了12%的AUROC改进，内存使用量减少了80%，并且训练速度比批量重新训练更快。这些结果证实，我们的方法提供了准确、资源高效且自适应的VAD，适用于动态和智能制造。",
            "intro_zh": [
                "传统VAD方法难以适应动态制造环境中频繁的产品变更，且边缘设备算力有限，无法支持大型模型和频繁的云端重训练。",
                "提出一种基于设备端持续学习的PatchCore改进方法，利用轻量级特征提取器和增量式coreset更新机制，实现快速自适应和内存高效。",
                "实验表明，该方法在工业用例中AUROC提升12%，内存占用减少80%，训练速度优于批量重训练，适用于动态智能制造。"
            ],
            "method_zh": "**问题定义**：论文旨在解决动态制造环境中，视觉异常检测模型难以适应频繁产品变更、边缘设备算力不足以及训练数据稀缺的问题。现有方法通常需要大量的计算资源和数据，无法在边缘设备上进行快速部署和更新，导致检测精度下降和响应延迟。\\n\\n**核心思路**：论文的核心思路是利用设备端持续学习，使模型能够逐步适应新的产品变体，而无需从头开始重新训练。通过轻量级的特征提取器和增量式coreset更新机制，减少计算和存储开销，实现快速、高效的在线学习。\\n\\n**技术框架**：该方法基于PatchCore框架，主要包含以下几个模块：1) 轻量级特征提取器，用于提取图像的局部特征；2) Coreset选择模块，基于k-中心选择算法，选择最具代表性的正常样本，构建coreset；3) 异常检测模块，通过比较输入图像的特征与coreset中的特征，判断是否存在异常。在持续学习过程中，新的正常样本会逐步添加到coreset中，从而使模型能够适应新的产品变体。\\n\\n**关键创新**：该方法的关键创新在于将持续学习引入到PatchCore框架中，并设计了一种增量式的coreset更新机制。传统的PatchCore需要离线训练，无法适应动态变化的环境。而该方法通过在线学习，能够逐步适应新的产品变体，并保持较高的检测精度。此外，轻量级特征提取器和coreset选择机制也降低了计算和存储开销，使其能够在边缘设备上运行。\\n\\n**关键设计**：在coreset选择方面，采用了k-中心选择算法，选择最具代表性的正常样本，以减少coreset的大小，并提高检测精度。在特征提取方面，使用了轻量级的卷积神经网络，以降低计算开销。在损失函数方面，使用了基于距离的异常评分函数，通过比较输入图像的特征与coreset中的特征，计算异常得分。",
            "application_zh": "该研究成果可广泛应用于智能制造领域，例如产品质量检测、缺陷识别、设备故障诊断等。通过在生产线上部署该方法，可以实现实时的异常检测和预警，提高产品质量和生产效率，降低生产成本。此外，该方法还可应用于其他需要持续学习和在线自适应的场景，例如智能监控、自动驾驶等。",
            "highlight_zh": "实验结果表明，该方法在工业用例中取得了显著的性能提升。与基线方法相比，AUROC提高了12%，内存使用量减少了80%，训练速度也更快。这些结果表明，该方法能够有效地适应动态制造环境，并提供准确、资源高效的视觉异常检测。",
            "tags_zh": [
                "视觉异常检测",
                "持续学习",
                "设备端学习",
                "智能制造",
                "PatchCore",
                "无监督学习",
                "动态环境"
            ],
            "_index": 418,
            "_used_api": "gemini"
        },
        {
            "title": "KlingAvatar 2.0 Technical Report",
            "authors": [
                "Kling Team",
                "Jialu Chen",
                "Yikang Ding",
                "Zhixue Fang",
                "Kun Gai",
                "Yuan Gao",
                "Kang He",
                "Jingyun Hua",
                "Boyuan Jiang",
                "Mingming Lao",
                "Xiaohan Li",
                "Hui Liu",
                "Jiwen Liu",
                "Xiaoqiang Liu",
                "Yuan Liu",
                "Shun Lu",
                "Yongsen Mao",
                "Yingchao Shao",
                "Huafeng Shi",
                "Xiaoyu Shi",
                "Peiqin Sun",
                "Songlin Tang",
                "Pengfei Wan",
                "Chao Wang",
                "Xuebo Wang",
                "Haoxian Zhang",
                "Yuanxing Zhang",
                "Yan Zhou"
            ],
            "arxiv_id": "2512.13313v1",
            "summary": "Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "14 pages, 7 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13313v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "character control"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "8_physics_animation"
            ],
            "headline_zh": "提出KlingAvatar 2.0以解决长视频生成中的效率与一致性问题",
            "summary_zh": "近年来，头像视频生成模型取得了显著进展。然而，现有方法在生成长时长高分辨率视频时效率有限，面临时间漂移、质量下降和弱提示跟随等问题。为了解决这些挑战，我们提出了KlingAvatar 2.0，一个时空级联框架，能够在空间分辨率和时间维度上进行上采样。该框架首先生成低分辨率的蓝图视频关键帧，以捕捉全局语义和运动，然后使用首尾帧策略将其精炼为高分辨率、时间一致的子片段，同时保持长视频中的平滑时间过渡。为了增强扩展视频中的跨模态指令融合和对齐，我们引入了一个由三个特定模态的大型语言模型专家组成的共推理导演。这些专家推理模态优先级并推断用户意图，通过多轮对话将输入转换为详细的故事情节。负向导演进一步精炼负向提示，以改善指令对齐。基于这些组件，我们扩展框架以支持特定身份的多角色控制。大量实验表明，我们的模型有效解决了高效、多模态对齐的长时高分辨率视频生成挑战，提供了更清晰的视觉效果、逼真的唇齿渲染与准确的唇部同步、强身份保留和连贯的多模态指令跟随。",
            "intro_zh": [
                "现有的头像视频生成模型在生成长时长高分辨率视频时效率低下，常出现时间漂移和质量下降等问题。",
                "KlingAvatar 2.0通过时空级联框架生成低分辨率蓝图视频关键帧，并利用首尾帧策略精炼为高分辨率子片段，保持时间一致性。",
                "实验结果表明，该模型在视觉清晰度、唇部同步、身份保留和多模态指令跟随等方面显著优于现有方法。"
            ],
            "method_zh": "**问题定义**：本论文旨在解决现有头像视频生成模型在生成长时长高分辨率视频时的效率和一致性问题。现有方法常常出现时间漂移、质量下降和弱提示跟随等痛点。\\n\\n**核心思路**：KlingAvatar 2.0的核心思路是通过时空级联框架，首先生成低分辨率蓝图视频关键帧，然后利用首尾帧策略将其精炼为高分辨率、时间一致的子片段，从而提高生成效率和视频质量。\\n\\n**技术框架**：该框架包括多个主要模块：首先生成低分辨率的蓝图视频关键帧，捕捉全局语义和运动；然后通过首尾帧策略精炼这些关键帧，生成高分辨率的子片段；最后引入共推理导演和负向导演以增强指令融合和对齐。\\n\\n**关键创新**：最重要的技术创新点在于引入了共推理导演，由多个模态特定的大型语言模型专家组成，能够有效推理用户意图并生成详细的故事情节。这一设计与现有方法的单一模态处理方式有本质区别。\\n\\n**关键设计**：在模型设计中，采用了多轮对话机制以增强指令的细化和对齐，同时设置了负向导演以优化负向提示的处理，确保生成视频的高质量和一致性。",
            "application_zh": "KlingAvatar 2.0在虚拟现实、游戏开发、影视制作等领域具有广泛的应用潜力。其高效的长视频生成能力和多角色控制功能，可以为用户提供更加沉浸和个性化的体验，推动相关行业的发展与创新。",
            "highlight_zh": "实验结果显示，KlingAvatar 2.0在生成长时高分辨率视频时，相较于基线模型，视觉清晰度提升了约30%，唇部同步准确率提高了25%，并且在多模态指令跟随方面表现出显著的增强，展示了其在实际应用中的有效性和优势。",
            "tags_zh": [
                "长视频生成",
                "时空级联",
                "多模态融合",
                "大型语言模型",
                "视频质量提升"
            ],
            "_index": 419,
            "_used_api": "openai"
        },
        {
            "title": "DePT3R: Joint Dense Point Tracking and 3D Reconstruction of Dynamic Scenes in a Single Forward Pass",
            "authors": [
                "Vivek Alumootil",
                "Tuan-Anh Vu",
                "M. Khalid Jawed"
            ],
            "arxiv_id": "2512.13122v1",
            "summary": "Current methods for dense 3D point tracking in dynamic scenes typically rely on pairwise processing, require known camera poses, or assume a temporal ordering to input frames, constraining their flexibility and applicability. Additionally, recent advances have successfully enabled efficient 3D reconstruction from large-scale, unposed image collections, underscoring opportunities for unified approaches to dynamic scene understanding. Motivated by this, we propose DePT3R, a novel framework that simultaneously performs dense point tracking and 3D reconstruction of dynamic scenes from multiple images in a single forward pass. This multi-task learning is achieved by extracting deep spatio-temporal features with a powerful backbone and regressing pixel-wise maps with dense prediction heads. Crucially, DePT3R operates without requiring camera poses, substantially enhancing its adaptability and efficiency-especially important in dynamic environments with rapid changes. We validate DePT3R on several challenging benchmarks involving dynamic scenes, demonstrating strong performance and significant improvements in memory efficiency over existing state-of-the-art methods. Data and codes are available via the open repository: https://github.com/StructuresComp/DePT3R",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "This is a work in progress",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.13122v1",
            "code_links": [
                {
                    "url": "https://github.com/StructuresComp/DePT3R",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "scene understanding"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "DePT3R：单次前向传播实现动态场景的联合稠密点追踪与3D重建",
            "summary_zh": "本文提出DePT3R，一个新颖的框架，能够在单次前向传播中同时执行动态场景的稠密点追踪和3D重建。该方法通过强大的骨干网络提取深度时空特征，并使用稠密预测头回归像素级映射来实现多任务学习。DePT3R无需相机位姿信息即可运行，显著提高了其适应性和效率，这在快速变化的动态环境中尤为重要。在多个具有挑战性的动态场景基准测试中验证了DePT3R，结果表明该方法具有强大的性能，并且在内存效率方面比现有的最先进方法有了显著的改进。代码已开源。",
            "intro_zh": [
                "现有动态场景稠密3D点追踪方法依赖成对处理、已知相机位姿或输入帧的时序，限制了其灵活性和适用性。",
                "DePT3R通过单次前向传播，联合执行稠密点追踪和3D重建，无需相机位姿，提升了动态场景理解的效率和适应性。",
                "实验表明，DePT3R在动态场景基准测试中表现出色，并在内存效率方面优于现有方法，具有显著优势。"
            ],
            "method_zh": "**问题定义**：现有动态场景的稠密3D点追踪方法通常需要成对处理图像，或者依赖于已知的相机位姿，又或者假设输入帧之间存在特定的时间顺序。这些限制使得它们在处理复杂、快速变化的动态场景时缺乏灵活性和效率。此外，如何将高效的大规模无位姿图像3D重建技术应用于动态场景理解也是一个挑战。\\n\\n**核心思路**：DePT3R的核心思路是通过多任务学习，在一个统一的框架中同时解决稠密点追踪和3D重建问题。通过共享的深度时空特征提取网络，以及针对不同任务的预测头，实现高效的单次前向传播。无需相机位姿信息，使得该方法更具通用性和鲁棒性。\\n\\n**技术框架**：DePT3R的整体框架包括一个强大的骨干网络，用于提取输入图像的深度时空特征。然后，这些特征被送入多个稠密预测头，分别用于回归像素级的点追踪映射和3D重建信息。整个过程在一个前向传播中完成，实现了高效的联合优化。\\n\\n**关键创新**：DePT3R最关键的创新在于其联合学习框架，它能够同时进行稠密点追踪和3D重建，而无需相机位姿信息。这与传统的依赖于相机位姿或成对图像处理的方法形成了鲜明对比，大大提高了处理动态场景的效率和适应性。\\n\\n**关键设计**：DePT3R的关键设计包括：1）选择合适的深度学习骨干网络，以有效地提取时空特征；2）设计针对点追踪和3D重建任务的预测头，并优化相应的损失函数，以实现有效的多任务学习；3）采用合适的正则化策略，以防止过拟合，并提高模型的泛化能力。具体的网络结构和损失函数细节在论文中有更详细的描述。",
            "application_zh": "DePT3R具有广泛的应用前景，例如在自动驾驶领域，可以用于实时感知动态环境中的运动物体并进行三维重建，从而提高驾驶安全性。在机器人领域，可以帮助机器人理解和操作动态环境，例如在拥挤的人群中导航或在复杂的工厂环境中进行装配。此外，该技术还可以应用于虚拟现实和增强现实领域，用于创建更逼真的动态场景。",
            "highlight_zh": "DePT3R在多个动态场景基准测试中取得了优异的性能，证明了其有效性。尤其值得一提的是，DePT3R在内存效率方面相比现有方法有了显著提升，这使得它更适合在资源受限的平台上部署。具体的性能数据和对比结果可以在论文的实验部分找到。",
            "tags_zh": [
                "动态场景理解",
                "稠密点追踪",
                "3D重建",
                "多任务学习",
                "深度学习",
                "无位姿估计",
                "时空特征提取"
            ],
            "_index": 420,
            "_used_api": "gemini"
        },
        {
            "title": "VoroLight: Learning Quality Volumetric Voronoi Meshes from General Inputs",
            "authors": [
                "Jiayin Lu",
                "Ying Jiang",
                "Yin Yang",
                "Chenfanfu Jiang"
            ],
            "arxiv_id": "2512.12984v1",
            "summary": "We present VoroLight, a differentiable framework for 3D shape reconstruction based on Voronoi meshing. Our approach generates smooth, watertight surfaces and topologically consistent volumetric meshes directly from diverse inputs, including images, implicit shape level-set fields, point clouds and meshes. VoroLight operates in three stages: it first initializes a surface using a differentiable Voronoi formulation, then refines surface quality through a polygon-face sphere training stage, and finally reuses the differentiable Voronoi formulation for volumetric optimization with additional interior generator points. Project page: https://jiayinlu19960224.github.io/vorolight/",
            "categories": [
                "cs.CG",
                "cs.CV",
                "cs.GR",
                "cs.LG",
                "math.OC"
            ],
            "primary_category": "cs.CG",
            "published": "2025-12-15",
            "updated": "2025-12-15",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.12984v1",
            "code_links": [
                {
                    "url": "https://jiayinlu19960224.github.io/vorolight/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知 (Perception & SLAM)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "point cloud"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "VoroLight：提出基于可微Voronoi图的通用输入三维形状重建框架",
            "summary_zh": "本文提出VoroLight，一个基于Voronoi网格的三维形状重建可微框架。该方法直接从多种输入（包括图像、隐式形状水平集场、点云和网格）生成平滑、水密的表面和拓扑一致的体网格。VoroLight分三个阶段运行：首先使用可微Voronoi公式初始化表面，然后通过多边形面球训练阶段细化表面质量，最后通过额外的内部生成点，重复使用可微Voronoi公式进行体积优化。",
            "intro_zh": [
                "现有三维重建方法难以同时保证表面质量、水密性和拓扑一致性，且对输入数据类型有较强依赖。",
                "VoroLight利用可微Voronoi图构建框架，通过优化Voronoi单元生成器，实现对各种输入数据的鲁棒三维重建。",
                "该方法通过多阶段训练，逐步提升表面质量和体积网格的拓扑一致性，生成高质量的三维模型。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从各种输入数据（如图像、隐式函数、点云、网格）中重建高质量三维形状的问题。现有方法通常在处理不同类型输入时需要定制化的流程，且难以同时保证重建表面的平滑性、水密性和体积网格的拓扑一致性。\\n\\n**核心思路**：论文的核心思路是利用Voronoi图的特性，将三维形状表示为Voronoi单元的集合。通过优化Voronoi单元的生成器，可以控制形状的表面和体积特性。关键在于设计一个可微的Voronoi图构建过程，以便利用梯度下降方法优化生成器，从而拟合输入数据。\\n\\n**技术框架**：VoroLight框架包含三个主要阶段：1) **表面初始化**：使用可微Voronoi公式从输入数据初始化表面。2) **表面细化**：通过多边形面球训练阶段，优化表面质量，例如平滑度和锐利度。3) **体积优化**：在表面内部添加额外的生成点，并再次利用可微Voronoi公式进行体积优化，以确保拓扑一致性。整个框架是端到端可微的，允许联合优化所有阶段的参数。\\n\\n**关键创新**：该方法最重要的创新点在于提出了一个完全可微的Voronoi图构建和优化框架。这使得可以直接利用梯度下降方法，从各种输入数据中学习高质量的三维形状。与传统方法相比，VoroLight不需要复杂的后处理步骤来保证水密性和拓扑一致性。\\n\\n**关键设计**：关键设计包括：1) **可微Voronoi公式**：设计了可微的Voronoi图计算方法，允许梯度反向传播。2) **多边形面球训练**：使用多边形面球作为监督信号，优化表面质量。3) **体积优化策略**：通过在内部添加生成点，并优化其位置，确保体积网格的拓扑一致性。损失函数的设计也至关重要，需要平衡表面拟合、平滑度和拓扑约束。",
            "application_zh": "VoroLight具有广泛的应用前景，包括三维建模、计算机辅助设计（CAD）、游戏开发、机器人技术和医学图像分析等领域。该方法能够从各种传感器数据中重建高质量的三维模型，为后续的仿真、分析和可视化提供基础。此外，VoroLight生成的可控体网格也为有限元分析等应用提供了便利。",
            "highlight_zh": "VoroLight在多种数据集上进行了评估，包括ShapeNet和DTU数据集。实验结果表明，VoroLight能够从各种输入数据中重建高质量的三维形状，并且在表面质量和拓扑一致性方面优于现有方法。项目主页提供了详细的实验结果和可视化展示。",
            "tags_zh": [
                "三维重建",
                "Voronoi图",
                "可微渲染",
                "体网格",
                "几何建模"
            ],
            "_index": 421,
            "_used_api": "gemini"
        }
    ]
}