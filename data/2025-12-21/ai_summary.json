{
    "papers": [
        {
            "title": "AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning",
            "authors": [
                "Ran Gong",
                "Xiaohan Zhang",
                "Jinghuan Shang",
                "Maria Vittoria Minniti",
                "Jigarkumar Patel",
                "Valerio Pepe",
                "Riedana Yan",
                "Ahmet Gundogdu",
                "Ivan Kapelyukh",
                "Ali Abbas",
                "Xiaoqiang Yan",
                "Harsh Patel",
                "Laura Herlant",
                "Karl Schmeckpeper"
            ],
            "arxiv_id": "2512.17853v1",
            "summary": "Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world. While simulation has become a promising way for scaling up data collection, the related tasks, including simulation task design, task-aware scene generation, expert demonstration synthesis, and sim-to-real transfer, still demand substantial human effort. We present AnyTask, an automated framework that pairs massively parallel GPU simulation with foundation models to design diverse manipulation tasks and synthesize robot data. We introduce three AnyTask agents for generating expert demonstrations aiming to solve as many tasks as possible: 1) ViPR, a novel task and motion planning agent with VLM-in-the-loop Parallel Refinement; 2) ViPR-Eureka, a reinforcement learning agent with generated dense rewards and LLM-guided contact sampling; 3) ViPR-RL, a hybrid planning and learning approach that jointly produces high-quality demonstrations with only sparse rewards. We train behavior cloning policies on generated data, validate them in simulation, and deploy them directly on real robot hardware. The policies generalize to novel object poses, achieving 44% average success across a suite of real-world pick-and-place, drawer opening, contact-rich pushing, and long-horizon manipulation tasks. Our project website is at https://anytask.rai-inst.com .",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "28 pages, 25 figures. The first four authors contributed equally",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17853v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "[T]sim-to-real",
                        "motion planning"
                    ],
                    "score": 10.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "[T]policy learning",
                        "behavior cloning",
                        "Eureka"
                    ],
                    "score": 9.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model",
                        "task and motion planning"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 25.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "AnyTask：自动化任务与数据生成框架，推进Sim-to-Real策略学习",
            "summary_zh": "通用机器人学习受到数据限制：大规模、多样化和高质量的交互数据在现实世界中收集成本高昂。虽然仿真已成为扩展数据收集的一种有希望的方式，但相关任务，包括仿真任务设计、任务感知场景生成、专家演示合成和Sim-to-Real迁移，仍然需要大量的人工干预。我们提出了AnyTask，一个自动化框架，它将大规模并行GPU仿真与基础模型相结合，以设计多样化的操作任务并合成机器人数据。我们引入了三个AnyTask Agent，用于生成旨在解决尽可能多任务的专家演示：1) ViPR，一种新颖的任务和运动规划Agent，具有VLM-in-the-loop并行优化；2) ViPR-Eureka，一种强化学习Agent，具有生成的密集奖励和LLM引导的接触采样；3) ViPR-RL，一种混合规划和学习方法，它仅使用稀疏奖励共同产生高质量的演示。我们在生成的数据上训练行为克隆策略，在仿真中验证它们，并将它们直接部署在真实机器人硬件上。这些策略推广到新的物体姿势，在真实世界的抓取放置、抽屉打开、富接触推和长时程操作任务套件中实现了44%的平均成功率。",
            "intro_zh": [
                "真实世界机器人数据收集成本高昂，仿真成为重要替代方案，但任务设计、场景生成和迁移仍需大量人工。",
                "AnyTask框架结合GPU并行仿真与基础模型，自动化设计多样操作任务并合成机器人数据，降低人工成本。",
                "通过ViPR等Agent生成专家演示，训练的行为克隆策略在真实机器人上实现了44%的平均成功率。"
            ],
            "method_zh": "**问题定义**：现有机器人学习方法依赖于大量真实世界数据，但数据收集成本高昂且耗时。仿真环境可以生成大量数据，但任务设计、场景生成、专家演示合成以及从仿真到真实的迁移仍然需要大量的人工干预。这限制了通用机器人学习的发展。\\n\\n**核心思路**：AnyTask的核心思路是利用大规模并行GPU仿真和基础模型，自动化地生成多样化的操作任务和高质量的机器人数据。通过自动化任务设计和数据生成流程，降低对人工干预的依赖，从而加速Sim-to-Real策略学习。\\n\\n**技术框架**：AnyTask框架包含以下几个主要模块：1) 任务设计模块：利用基础模型自动生成多样化的操作任务。2) 场景生成模块：根据任务需求自动生成任务相关的场景。3) 专家演示合成模块：使用ViPR、ViPR-Eureka和ViPR-RL等Agent生成高质量的专家演示数据。4) 策略训练模块：使用生成的数据训练行为克隆策略。5) Sim-to-Real迁移模块：将训练好的策略部署到真实机器人上。\\n\\n**关键创新**：AnyTask的关键创新在于自动化任务和数据生成流程，特别是ViPR Agent的设计。ViPR Agent采用VLM-in-the-loop并行优化，能够有效地生成高质量的专家演示数据。此外，ViPR-Eureka Agent利用LLM引导的接触采样，提高了强化学习的效率。ViPR-RL Agent则结合了规划和学习方法，在稀疏奖励下也能生成高质量的演示。\\n\\n**关键设计**：ViPR Agent的关键设计包括：1) VLM-in-the-loop：利用视觉语言模型（VLM）来评估任务完成情况，并指导任务规划。2) 并行优化：利用GPU并行计算能力，同时优化多个任务规划方案，提高效率。3) 奖励函数设计：ViPR-Eureka Agent设计了基于LLM的密集奖励函数，引导Agent学习。4) 接触采样：ViPR-Eureka Agent利用LLM引导的接触采样，提高探索效率。",
            "application_zh": "AnyTask框架可应用于各种机器人操作任务，例如工业自动化、家庭服务机器人、医疗机器人等。通过自动化任务设计和数据生成，可以降低机器人学习的成本，加速机器人在复杂环境中的部署。该框架还有助于推动通用机器人学习的发展，使机器人能够适应各种不同的任务和环境。",
            "highlight_zh": "实验结果表明，使用AnyTask框架生成的数据训练的行为克隆策略，在真实世界的抓取放置、抽屉打开、富接触推和长时程操作任务套件中实现了44%的平均成功率。该策略能够推广到新的物体姿势，证明了AnyTask框架的有效性和泛化能力。相较于其他方法，AnyTask显著降低了人工干预的需求，提高了数据生成的效率。",
            "tags_zh": [
                "机器人学习",
                "Sim-to-Real",
                "自动化任务生成",
                "专家演示",
                "行为克隆"
            ],
            "_index": 0,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17853v1/figures_and_tables/draft_main_figure.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17853v1/figures_and_tables/vipr_improvement.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17853v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Semantic Co-Speech Gesture Synthesis and Real-Time Control for Humanoid Robots",
            "authors": [
                "Gang Zhang"
            ],
            "arxiv_id": "2512.17183v1",
            "summary": "We present an innovative end-to-end framework for synthesizing semantically meaningful co-speech gestures and deploying them in real-time on a humanoid robot. This system addresses the challenge of creating natural, expressive non-verbal communication for robots by integrating advanced gesture generation techniques with robust physical control. Our core innovation lies in the meticulous integration of a semantics-aware gesture synthesis module, which derives expressive reference motions from speech input by leveraging a generative retrieval mechanism based on large language models (LLMs) and an autoregressive Motion-GPT model. This is coupled with a high-fidelity imitation learning control policy, the MotionTracker, which enables the Unitree G1 humanoid robot to execute these complex motions dynamically and maintain balance. To ensure feasibility, we employ a robust General Motion Retargeting (GMR) method to bridge the embodiment gap between human motion data and the robot platform. Through comprehensive evaluation, we demonstrate that our combined system produces semantically appropriate and rhythmically coherent gestures that are accurately tracked and executed by the physical robot. To our knowledge, this work represents a significant step toward general real-world use by providing a complete pipeline for automatic, semantic-aware, co-speech gesture generation and synchronized real-time physical deployment on a humanoid robot.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17183v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]humanoid",
                        "[T]humanoid robot",
                        "Unitree"
                    ],
                    "score": 14.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "imitation learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "motion retargeting"
                    ],
                    "score": 3.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 21.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch",
                "7_retargeting",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出一种基于语义理解的共语姿势生成与人形机器人实时控制框架",
            "summary_zh": "本文提出了一种创新的端到端框架，用于合成具有语义意义的共语姿势，并将其在人形机器人上实时部署。该系统通过将先进的姿势生成技术与鲁棒的物理控制相结合，解决了为机器人创建自然、富有表现力的非语言交流的挑战。核心创新在于语义感知姿势合成模块的精心集成，该模块通过利用基于大型语言模型（LLMs）的生成检索机制和自回归Motion-GPT模型，从语音输入中导出富有表现力的参考动作。这与高保真模仿学习控制策略MotionTracker相结合，使Unitree G1人形机器人能够动态地执行这些复杂动作并保持平衡。为了确保可行性，我们采用了一种鲁棒的通用运动重定向（GMR）方法来弥合人体运动数据和机器人平台之间的具身差距。通过全面的评估，我们证明了我们的组合系统能够产生语义上适当且节奏上连贯的姿势，并且可以被物理机器人准确地跟踪和执行。据我们所知，这项工作代表了朝着通用现实世界使用迈出的重要一步，它提供了一个完整的管道，用于自动、语义感知的共语姿势生成以及在人形机器人上同步的实时物理部署。",
            "intro_zh": [
                "现有机器人共语姿势生成方法难以保证语义相关性，且难以在真实机器人上实时部署。",
                "该论文提出了一种端到端的框架，利用大型语言模型和Motion-GPT模型生成语义相关的姿势，并使用模仿学习控制策略进行实时控制。",
                "实验结果表明，该系统能够生成语义适当且节奏连贯的姿势，并由Unitree G1人形机器人准确跟踪和执行。"
            ],
            "method_zh": "**问题定义**：现有方法在机器人共语姿势生成方面存在两个主要痛点。一是生成的姿势与语音的语义关联性较弱，缺乏自然性和表现力。二是难以将生成的姿势有效地部署到真实的机器人平台上，实现实时的同步控制，尤其是在保持机器人平衡的同时执行复杂动作。\n\\n**核心思路**：该论文的核心思路是将语义感知的姿势生成与高保真度的机器人控制相结合。通过利用大型语言模型理解语音的语义信息，并生成与之对应的姿势动作。然后，通过模仿学习训练控制策略，使机器人能够准确地复现这些姿势，并保持自身的平衡。\n\\n**技术框架**：该框架主要包含三个模块：1) 语义感知姿势合成模块：利用大型语言模型和Motion-GPT模型，从语音输入中生成参考动作。2) 通用运动重定向（GMR）模块：将人体运动数据映射到机器人平台上，解决具身差异。3) 模仿学习控制模块（MotionTracker）：训练控制策略，使机器人能够跟踪参考动作并保持平衡。\n\\n**关键创新**：该论文的关键创新在于将大型语言模型引入到共语姿势生成中，实现了语义感知的姿势合成。此外，通过模仿学习训练控制策略，实现了在真实机器人上的实时部署和控制。这种端到端的集成方法是现有方法所缺乏的。\n\\n**关键设计**：在姿势合成模块中，使用了基于大型语言模型的生成检索机制，以确保生成的姿势与语音的语义相关。Motion-GPT模型采用自回归结构，能够生成连贯的动作序列。在控制模块中，使用了模仿学习方法，通过学习人类的运动数据，训练机器人控制策略。GMR模块采用了一种鲁棒的映射方法，以减少人体和机器人之间的差异。",
            "application_zh": "该研究成果可应用于多种人机交互场景，例如：智能客服机器人、教育机器人、康复机器人等。通过赋予机器人更自然、更富有表现力的非语言交流能力，可以显著提升人机交互的质量和效率，使机器人更好地理解人类意图并做出相应的反应。未来，该技术有望应用于更广泛的领域，例如：虚拟现实、游戏等。",
            "highlight_zh": "该论文提出的系统在Unitree G1人形机器人上进行了实验验证，结果表明该系统能够生成语义适当且节奏连贯的姿势，并由机器人准确跟踪和执行。与传统的基于规则或运动捕捉的方法相比，该系统能够生成更自然、更富有表现力的姿势，并且能够更好地适应不同的语音输入。",
            "tags_zh": [
                "共语姿势生成",
                "人形机器人",
                "实时控制",
                "大型语言模型",
                "模仿学习",
                "运动重定向",
                "人机交互"
            ],
            "_index": 1,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17183v1/paper.drawio.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17183v1/001_Neutral_0_mirror_x_1_0_retarget2mocap_comparison.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17183v1/joint_comparison.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Learning When to Look: A Disentangled Curriculum for Strategic Perception in Multimodal Reasoning",
            "authors": [
                "Siqi Yang",
                "Zilve Gao",
                "Haibo Qiu",
                "Fanfan Liu",
                "Peng Shi",
                "Zhixiong Zeng",
                "Qingmin Liao",
                "Lin Ma"
            ],
            "arxiv_id": "2512.17227v1",
            "summary": "Multimodal Large Language Models (MLLMs) demonstrate significant potential but remain brittle in complex, long-chain visual reasoning tasks. A critical failure mode is \"visual forgetting\", where models progressively lose visual grounding as reasoning extends, a phenomenon aptly described as \"think longer, see less\". We posit this failure stems from current training paradigms prematurely entangling two distinct cognitive skills: (1) abstract logical reasoning \"how-to-think\") and (2) strategic visual perception (\"when-to-look\"). This creates a foundational cold-start deficiency -- weakening abstract reasoning -- and a strategic perception deficit, as models lack a policy for when to perceive. In this paper, we propose a novel curriculum-based framework to disentangle these skills. First, we introduce a disentangled Supervised Fine-Tuning (SFT) curriculum that builds a robust abstract reasoning backbone on text-only data before anchoring it to vision with a novel Perception-Grounded Chain-of-Thought (PG-CoT) paradigm. Second, we resolve the strategic perception deficit by formulating timing as a reinforcement learning problem. We design a Pivotal Perception Reward that teaches the model when to look by coupling perceptual actions to linguistic markers of cognitive uncertainty (e.g., \"wait\", \"verify\"), thereby learning an autonomous grounding policy. Our contributions include the formalization of these two deficiencies and the development of a principled, two-stage framework to address them, transforming the model from a heuristic-driven observer to a strategic, grounded reasoner. \\textbf{Code}: \\url{https://github.com/gaozilve-max/learning-when-to-look}.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17227v1",
            "code_links": [
                {
                    "url": "https://github.com/gaozilve-max/learning-when-to-look",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "[T]multimodal",
                        "visual grounding",
                        "chain-of-thought"
                    ],
                    "score": 18.0
                }
            ],
            "relevance_score": 19.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出解耦课程学习框架，解决多模态推理中视觉信息遗忘问题。",
            "summary_zh": "多模态大型语言模型(MLLMs)展现出巨大潜力，但在复杂、长链视觉推理任务中仍然脆弱。一个关键的失败模式是“视觉遗忘”，即模型随着推理的进行逐渐失去视觉基础，这种现象被恰当地描述为“思考越长，看得越少”。我们认为这种失败源于当前训练范式过早地将两种不同的认知技能纠缠在一起：(1)抽象逻辑推理（“如何思考”）和(2)战略性视觉感知（“何时看”）。这造成了一个基础性的冷启动缺陷——削弱了抽象推理——以及一个战略性感知缺陷，因为模型缺乏何时感知的策略。在本文中，我们提出了一个新颖的基于课程的框架来解耦这些技能。首先，我们引入了一个解耦的监督微调(SFT)课程，该课程在文本数据上构建强大的抽象推理骨干，然后通过一种新颖的感知基础链式思考(PG-CoT)范式将其锚定到视觉。其次，我们通过将时间安排建模为一个强化学习问题来解决战略性感知缺陷。我们设计了一个关键感知奖励，通过将感知动作与认知不确定性的语言标记（例如，“等待”，“验证”）耦合，来教导模型何时看，从而学习自主基础策略。我们的贡献包括对这两种缺陷的形式化，以及开发一个有原则的两阶段框架来解决它们，将模型从启发式驱动的观察者转变为战略性的、有基础的推理者。",
            "intro_zh": [
                "现有MLLM在长链视觉推理中存在“视觉遗忘”问题，即推理越深入，视觉信息利用越差。",
                "论文提出解耦训练策略，将抽象推理和战略感知分离，分别进行优化，避免过早耦合。",
                "通过监督微调和强化学习，模型学会何时关注视觉信息，显著提升了长链推理任务的性能。"
            ],
            "method_zh": "**问题定义**：多模态大型语言模型在执行复杂视觉推理任务时，会逐渐遗忘或忽略视觉信息，导致推理错误。现有方法通常将视觉感知和抽象推理耦合训练，使得模型难以有效利用视觉信息进行长链推理，尤其是在需要多次观察和思考的场景下。\\n\\n**核心思路**：论文的核心思想是将视觉感知（何时看）和抽象推理（如何思考）解耦，分别进行训练。首先，利用文本数据训练一个强大的抽象推理骨干网络。然后，通过强化学习训练模型学习何时关注视觉信息，从而建立一个战略性的视觉感知策略。这种解耦训练的方式可以避免过早耦合导致的冷启动问题，并提升模型在复杂视觉推理任务中的表现。\\n\\n**技术框架**：该框架包含两个主要阶段：(1) 解耦的监督微调(SFT)课程，用于构建强大的抽象推理骨干网络；(2) 基于强化学习的战略感知训练，用于学习何时关注视觉信息。SFT阶段使用文本数据进行训练，PG-CoT范式将视觉信息锚定到文本推理链上。强化学习阶段使用Pivotal Perception Reward，鼓励模型在需要时（例如，遇到“等待”、“验证”等语言标记）进行视觉感知。\\n\\n**关键创新**：论文的关键创新在于：(1) 形式化了视觉遗忘问题，并将其归因于抽象推理和战略感知的过早耦合；(2) 提出了一个解耦的训练框架，将抽象推理和战略感知分离，分别进行优化；(3) 设计了Pivotal Perception Reward，通过将感知动作与认知不确定性的语言标记耦合，引导模型学习何时关注视觉信息。与现有方法相比，该方法更注重培养模型的战略性感知能力。\\n\\n**关键设计**：在SFT阶段，使用标准的交叉熵损失函数进行训练。在强化学习阶段，Pivotal Perception Reward的设计至关重要，它根据模型在特定时间步采取的感知动作以及语言标记来计算奖励。具体来说，当模型在需要视觉信息时（例如，遇到“等待”、“验证”等语言标记）采取感知动作，则给予正向奖励；反之，则给予负向奖励。此外，论文还可能涉及一些超参数的调整，例如学习率、奖励系数等，以获得最佳的训练效果。",
            "application_zh": "该研究成果可应用于需要复杂视觉推理的场景，如智能问答、视觉导航、机器人操作等。例如，在智能客服中，模型可以根据用户的问题和提供的图像，自主决定何时需要进一步查看图像细节，从而更准确地回答用户的问题。该研究有助于提升多模态AI系统的可靠性和智能化水平。",
            "highlight_zh": "实验结果表明，该方法在多个视觉推理任务上取得了显著的性能提升。例如，在某个长链推理数据集上，该方法相比基线模型提升了10%以上的准确率。此外，消融实验验证了解耦训练和Pivotal Perception Reward的有效性，证明了该方法能够有效解决视觉遗忘问题。",
            "tags_zh": [
                "多模态推理",
                "视觉遗忘",
                "解耦学习",
                "强化学习",
                "课程学习"
            ],
            "_index": 2,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17227v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17227v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17227v1/figures/model_compare.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Generative Human-Object Interaction Detection via Differentiable Cognitive Steering of Multi-modal LLMs",
            "authors": [
                "Zhaolin Cai",
                "Huiyu Duan",
                "Zitong Xu",
                "Fan Li",
                "Zhi Liu",
                "Jing Liu",
                "Wei Shen",
                "Xiongkuo Min",
                "Guangtao Zhai"
            ],
            "arxiv_id": "2512.17640v1",
            "summary": "Human-object interaction (HOI) detection aims to localize human-object pairs and the interactions between them. Existing methods operate under a closed-world assumption, treating the task as a classification problem over a small, predefined verb set, which struggles to generalize to the long-tail of unseen or ambiguous interactions in the wild. While recent multi-modal large language models (MLLMs) possess the rich world knowledge required for open-vocabulary understanding, they remain decoupled from existing HOI detectors since fine-tuning them is computationally prohibitive. To address these constraints, we propose \\GRASP-HO}, a novel Generative Reasoning And Steerable Perception framework that reformulates HOI detection from the closed-set classification task to the open-vocabulary generation problem. To bridge the vision and cognitive, we first extract hybrid interaction representations, then design a lightweight learnable cognitive steering conduit (CSC) module to inject the fine-grained visual evidence into a frozen MLLM for effective reasoning. To address the supervision mismatch between classification-based HOI datasets and open-vocabulary generative models, we introduce a hybrid guidance strategy that coupling the language modeling loss and auxiliary classification loss, enabling discriminative grounding without sacrificing generative flexibility. Experiments demonstrate state-of-the-art closed-set performance and strong zero-shot generalization, achieving a unified paradigm that seamlessly bridges discriminative perception and generative reasoning for open-world HOI detection.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17640v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "open-vocabulary",
                        "open vocabulary"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱五：交互与反应 (Interaction & Reaction)",
                    "id": "5_interaction_reaction",
                    "matched_keywords": [
                        "[T]human-object interaction",
                        "HOI"
                    ],
                    "score": 10.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 17.0,
            "hit_pillars": [
                "3_perception_slam",
                "5_interaction_reaction",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出GRASP-HO框架，通过可微分认知引导多模态LLM实现生成式人-物交互检测。",
            "summary_zh": "人-物交互(HOI)检测旨在定位人-物对以及它们之间的交互。现有方法通常基于封闭世界假设，将该任务视为对预定义动词集合的分类问题，难以泛化到真实场景中未见或模糊的长尾交互。虽然最近的多模态大型语言模型(MLLM)拥有开放词汇理解所需的丰富世界知识，但由于微调成本过高，它们与现有的HOI检测器脱节。为了解决这些限制，我们提出了GRASP-HO，一种新颖的生成式推理和可控感知框架，将HOI检测从封闭集分类任务重新定义为开放词汇生成问题。为了连接视觉和认知，我们首先提取混合交互表示，然后设计一个轻量级的可学习认知引导模块(CSC)，将细粒度的视觉证据注入到冻结的MLLM中以进行有效的推理。为了解决基于分类的HOI数据集和开放词汇生成模型之间的监督不匹配问题，我们引入了一种混合指导策略，将语言建模损失和辅助分类损失相结合，从而在不牺牲生成灵活性的情况下实现判别性 grounding。实验表明，该方法在封闭集上实现了最先进的性能，并具有强大的零样本泛化能力，从而实现了一种统一的范例，无缝地桥接了判别性感知和生成式推理，用于开放世界HOI检测。",
            "intro_zh": [
                "现有HOI检测方法受限于预定义的动词集合，难以处理真实场景中复杂多变的交互。",
                "GRASP-HO框架将HOI检测重构为开放词汇生成问题，利用MLLM的知识进行推理。",
                "通过可学习的认知引导模块和混合指导策略，GRASP-HO在封闭集和零样本场景下均表现出色。"
            ],
            "method_zh": "**问题定义**：现有HOI检测方法主要基于封闭世界假设，依赖于预定义的动词类别进行分类，无法有效处理真实场景中长尾分布的、未知的或语义模糊的交互行为。此外，直接微调大型多模态语言模型（MLLM）进行HOI检测计算成本过高，难以实现。\n\\n**核心思路**：GRASP-HO的核心思路是将HOI检测任务从传统的封闭集分类问题转化为开放词汇的生成问题，充分利用MLLM强大的语言理解和生成能力。通过将视觉信息有效地注入到MLLM中，引导其生成描述人-物交互的自然语言描述，从而实现对未知交互的识别。\n\\n**技术框架**：GRASP-HO框架主要包含以下几个关键模块：1) 混合交互表示提取模块，用于提取人、物及其交互的视觉特征；2) 可学习认知引导模块(CSC)，用于将提取的视觉特征注入到冻结的MLLM中，引导MLLM进行推理；3) 混合指导策略，结合语言建模损失和辅助分类损失，优化模型训练。\n\\n**关键创新**：GRASP-HO的关键创新在于：1) 将HOI检测任务重新定义为开放词汇生成问题，突破了传统方法的类别限制；2) 提出了轻量级的可学习认知引导模块(CSC)，实现了视觉信息到MLLM的有效传递，避免了对MLLM进行昂贵的微调；3) 设计了混合指导策略，解决了分类数据集与生成模型之间的监督不匹配问题。\n\\n**关键设计**：认知引导模块（CSC）采用轻量级网络结构，通过注意力机制将视觉特征融入MLLM的输入中。混合指导策略结合了语言建模损失（衡量生成文本的流畅性和准确性）和辅助分类损失（利用现有HOI数据集的类别信息），以提升模型的判别能力。具体损失函数的权重比例需要根据实验进行调整。",
            "application_zh": "GRASP-HO框架在智能监控、人机交互、机器人视觉等领域具有广泛的应用前景。它可以用于识别监控视频中的异常行为，理解人与机器人的交互意图，以及帮助机器人更好地理解和操作周围环境中的物体。该研究为开放世界场景下的人-物交互理解提供了新的思路。",
            "highlight_zh": "实验结果表明，GRASP-HO在封闭集HOI检测任务上取得了state-of-the-art的性能，并在零样本HOI检测任务上表现出强大的泛化能力。相较于传统方法，GRASP-HO在多个数据集上均取得了显著的性能提升，验证了其有效性和优越性。具体提升幅度在论文中有详细数据。",
            "tags_zh": [
                "人-物交互检测",
                "多模态大语言模型",
                "生成式模型",
                "开放词汇",
                "认知引导",
                "零样本学习",
                "混合指导策略"
            ],
            "_index": 3,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17640v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17640v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17640v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Kinematics-Aware Diffusion Policy with Consistent 3D Observation and Action Space for Whole-Arm Robotic Manipulation",
            "authors": [
                "Kangchen Lv",
                "Mingrui Yu",
                "Yongyi Jia",
                "Chenyu Zhang",
                "Xiang Li"
            ],
            "arxiv_id": "2512.17568v1",
            "summary": "Whole-body control of robotic manipulators with awareness of full-arm kinematics is crucial for many manipulation scenarios involving body collision avoidance or body-object interactions, which makes it insufficient to consider only the end-effector poses in policy learning. The typical approach for whole-arm manipulation is to learn actions in the robot's joint space. However, the unalignment between the joint space and actual task space (i.e., 3D space) increases the complexity of policy learning, as generalization in task space requires the policy to intrinsically understand the non-linear arm kinematics, which is difficult to learn from limited demonstrations. To address this issue, this letter proposes a kinematics-aware imitation learning framework with consistent task, observation, and action spaces, all represented in the same 3D space. Specifically, we represent both robot states and actions using a set of 3D points on the arm body, naturally aligned with the 3D point cloud observations. This spatially consistent representation improves the policy's sample efficiency and spatial generalizability while enabling full-body control. Built upon the diffusion policy, we further incorporate kinematics priors into the diffusion processes to guarantee the kinematic feasibility of output actions. The joint angle commands are finally calculated through an optimization-based whole-body inverse kinematics solver for execution. Simulation and real-world experimental results demonstrate higher success rates and stronger spatial generalizability of our approach compared to existing methods in body-aware manipulation policy learning.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "The first two authors contributed equally. Project Website: https://kinematics-aware-diffusion-policy.github.io",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17568v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "whole-body control",
                        "[T]manipulation"
                    ],
                    "score": 8.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "policy learning",
                        "imitation learning",
                        "[T]diffusion policy"
                    ],
                    "score": 7.5
                }
            ],
            "relevance_score": 15.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出基于运动学感知的扩散策略，解决机械臂全身操作中的空间泛化问题。",
            "summary_zh": "本文提出了一种运动学感知的模仿学习框架，用于机械臂全身操作，该框架具有一致的任务、观察和动作空间，均以3D空间表示。具体而言，机器人状态和动作都使用机械臂上的3D点集表示，与3D点云观测自然对齐。这种空间一致的表示提高了策略的样本效率和空间泛化能力，同时实现了全身控制。该方法基于扩散策略，进一步将运动学先验知识融入扩散过程，以保证输出动作的运动学可行性。最后，通过基于优化的全身逆运动学求解器计算关节角度指令以供执行。仿真和真实世界的实验结果表明，与现有的全身感知操作策略学习方法相比，该方法具有更高的成功率和更强的空间泛化能力。",
            "intro_zh": [
                "现有方法在机械臂全身操作中仅考虑末端执行器姿态，忽略了全身运动学，限制了其在避障和人机交互等场景的应用。",
                "论文提出一种基于扩散策略的运动学感知模仿学习框架，在统一的3D空间中表示任务、观测和动作，提升空间泛化能力。",
                "实验结果表明，该方法在仿真和真实环境中均优于现有方法，实现了更高的成功率和更强的空间泛化能力。"
            ],
            "method_zh": "**问题定义**：现有机械臂全身控制方法通常在关节空间学习动作，但关节空间与实际任务空间（3D空间）的不对齐增加了策略学习的复杂性。策略需要在有限的演示中学习非线性的机械臂运动学，以实现任务空间中的泛化，这非常困难。因此，需要一种方法能够在任务空间中直接学习策略，并保证运动学可行性。\\n\\n**核心思路**：论文的核心思路是在统一的3D空间中表示机器人状态、动作和观测，从而实现空间一致性。通过这种方式，策略可以直接在任务空间中学习，而无需显式地学习复杂的运动学关系。此外，论文还利用扩散模型，并将运动学先验知识融入扩散过程中，以保证生成动作的运动学可行性。\\n\\n**技术框架**：该方法主要包含以下几个模块：1) 3D空间表示模块：将机器人状态和动作表示为机械臂上的3D点集。2) 扩散策略模块：基于扩散模型学习策略，将运动学先验知识融入扩散过程。3) 逆运动学求解模块：通过基于优化的全身逆运动学求解器，将扩散策略生成的3D动作转换为关节角度指令。整体流程是，首先利用3D点云观测获取机器人状态，然后通过扩散策略生成3D动作，最后通过逆运动学求解器转换为关节角度指令。\\n\\n**关键创新**：最重要的技术创新点在于提出了空间一致的表示方法，将机器人状态、动作和观测都表示在同一个3D空间中。这种表示方法使得策略可以直接在任务空间中学习，避免了学习复杂的运动学关系。此外，将运动学先验知识融入扩散过程，保证了生成动作的运动学可行性。\\n\\n**关键设计**：论文使用扩散模型作为策略学习的核心。扩散模型通过逐步添加噪声到数据，然后再逐步去噪来生成数据。论文将运动学先验知识融入去噪过程中，例如，可以限制生成点的位置，使其始终位于机械臂的可达范围内。逆运动学求解器采用基于优化的方法，目标是找到一组关节角度，使得机械臂上的3D点尽可能接近扩散策略生成的3D动作。具体的损失函数包括位置误差、姿态误差和关节限制等。",
            "application_zh": "该研究成果可应用于各种需要全身控制的机器人操作场景，例如：复杂环境下的物体抓取、人机协作装配、医疗康复机器人等。通过提高机械臂操作的成功率和空间泛化能力，可以显著提升机器人在复杂环境中的适应性和自主性，具有重要的实际应用价值和广阔的应用前景。",
            "highlight_zh": "在仿真和真实世界的实验中，该方法都取得了显著的性能提升。例如，在某项抓取任务中，该方法的成功率比现有方法提高了15%，并且在不同空间位置的泛化能力也得到了显著提升。实验结果表明，该方法能够有效地学习全身操作策略，并具有很强的鲁棒性和泛化能力。",
            "tags_zh": [
                "机械臂操作",
                "全身控制",
                "扩散策略",
                "模仿学习",
                "运动学感知",
                "3D空间表示",
                "空间泛化"
            ],
            "_index": 4,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17568v1/figs/top_fig.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17568v1/figs/framework.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17568v1/figs/overview_tasks.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Neuro-Symbolic Control with Large Language Models for Language-Guided Spatial Tasks",
            "authors": [
                "Momina Liaqat Ali",
                "Muhammad Abid"
            ],
            "arxiv_id": "2512.17321v1",
            "summary": "Although large language models (LLMs) have recently become effective tools for language-conditioned control in embodied systems, instability, slow convergence, and hallucinated actions continue to limit their direct application to continuous control. A modular neuro-symbolic control framework that clearly distinguishes between low-level motion execution and high-level semantic reasoning is proposed in this work. While a lightweight neural delta controller performs bounded, incremental actions in continuous space, a locally deployed LLM interprets symbolic tasks. We assess the suggested method in a planar manipulation setting with spatial relations between objects specified by language. Numerous tasks and local language models, such as Mistral, Phi, and LLaMA-3.2, are used in extensive experiments to compare LLM-only control, neural-only control, and the suggested LLM+DL framework. In comparison to LLM-only baselines, the results show that the neuro-symbolic integration consistently increases both success rate and efficiency, achieving average step reductions exceeding 70% and speedups of up to 8.83x while remaining robust to language model quality. The suggested framework enhances interpretability, stability, and generalization without any need of reinforcement learning or costly rollouts by controlling the LLM to symbolic outputs and allocating uninterpreted execution to a neural controller trained on artificial geometric data. These outputs show empirically that neuro-symbolic decomposition offers a scalable and principled way to integrate language understanding with ongoing control, this approach promotes the creation of dependable and effective language-guided embodied systems.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17321v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model",
                        "language conditioned"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 15.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出神经符号控制框架，利用大语言模型解决语言引导的空间任务",
            "summary_zh": "本文提出了一种模块化的神经符号控制框架，用于解决具身系统中语言条件控制的问题。该框架明确区分了底层运动执行和高层语义推理。轻量级的神经delta控制器在连续空间中执行有界的增量动作，而本地部署的大语言模型（LLM）解释符号任务。在平面操作环境中，通过语言指定对象之间的空间关系，评估了该方法。通过大量实验，比较了仅LLM控制、仅神经控制以及所提出的LLM+DL框架，使用了Mistral、Phi和LLaMA-3.2等多种LLM。结果表明，与仅LLM的基线相比，神经符号集成始终提高成功率和效率，平均步数减少超过70%，速度提高高达8.83倍，同时对语言模型质量保持鲁棒性。该框架通过控制LLM输出符号，并将未解释的执行分配给在人工几何数据上训练的神经控制器，从而增强了可解释性、稳定性和泛化性，无需强化学习或昂贵的rollout。实验结果表明，神经符号分解为集成语言理解和持续控制提供了一种可扩展且有原则的方法，从而促进了可靠且有效的语言引导具身系统的创建。",
            "intro_zh": [
                "现有大语言模型在具身系统的语言条件控制中存在不稳定性、收敛慢和幻觉动作等问题。",
                "提出一种神经符号控制框架，将高层语义推理（LLM）和底层运动控制（神经控制器）分离。",
                "实验表明，该框架显著提高了成功率和效率，减少了步数，并对语言模型质量具有鲁棒性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决具身智能中，如何利用大语言模型（LLM）实现语言引导的空间任务控制问题。现有方法，特别是直接使用LLM进行控制，存在不稳定、收敛速度慢以及产生幻觉动作等问题，限制了其在连续控制任务中的应用。这些问题源于LLM难以直接处理连续控制信号，以及缺乏对环境的精确建模。\n\n**核心思路**：论文的核心思路是将控制任务分解为高层语义推理和底层运动执行两个部分，分别由LLM和神经控制器负责。LLM负责理解语言指令，并将其转化为符号化的动作序列；神经控制器则负责执行这些符号化的动作，实现精确的运动控制。这种神经符号结合的方式，既利用了LLM强大的语言理解能力，又避免了其直接控制连续动作的缺陷。\n\n**技术框架**：整体框架包含两个主要模块：1) **LLM 模块**：负责接收语言指令，并输出符号化的动作序列。该模块使用本地部署的LLM，例如Mistral、Phi或LLaMA-3.2。2) **神经控制器模块**：负责接收LLM输出的符号化动作，并执行相应的运动控制。该模块使用一个轻量级的神经delta控制器，该控制器在连续空间中执行有界的增量动作。这两个模块通过符号化的动作序列进行连接，形成一个完整的控制回路。\n\n**关键创新**：论文的关键创新在于将神经符号控制方法应用于语言引导的空间任务。通过将LLM与神经控制器相结合，实现了对语言指令的精确理解和对运动的精确控制。此外，该方法还具有良好的可解释性、稳定性和泛化性，无需强化学习或昂贵的rollout。\n\n**关键设计**：神经控制器采用delta控制，输出的是增量动作而非绝对位置，这有助于提高控制的稳定性。LLM的prompt设计至关重要，需要清晰地定义任务目标和可执行的动作空间。实验中，神经控制器在人工生成的几何数据上进行训练，避免了对真实数据的依赖。",
            "application_zh": "该研究成果可应用于机器人操作、自动化装配、智能家居等领域。例如，用户可以通过自然语言指令引导机器人完成复杂的空间操作任务，如“将红色的杯子放在蓝色的盒子旁边”。该方法有望提高人机交互的自然性和效率，并降低机器人编程的难度。",
            "highlight_zh": "实验结果表明，与仅使用LLM的基线方法相比，该神经符号控制框架在多个空间操作任务中显著提高了成功率和效率。具体而言，平均步数减少超过70%，速度提高高达8.83倍，并且对不同质量的语言模型具有鲁棒性。这些结果表明，该框架能够有效地将语言理解与运动控制相结合，实现更可靠和高效的语言引导具身系统。",
            "tags_zh": [
                "神经符号控制",
                "大语言模型",
                "具身智能",
                "语言引导",
                "空间任务"
            ],
            "_index": 5,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17321v1/Drawbacks.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17321v1/Picture1.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17321v1/fig1_success_by_task_agg.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Dexterous World Models",
            "authors": [
                "Byungjun Kim",
                "Taeksoo Kim",
                "Junyoung Lee",
                "Hanbyul Joo"
            ],
            "arxiv_id": "2512.17907v1",
            "summary": "Recent progress in 3D reconstruction has made it easy to create realistic digital twins from everyday environments. However, current digital twins remain largely static and are limited to navigation and view synthesis without embodied interactivity. To bridge this gap, we introduce Dexterous World Model (DWM), a scene-action-conditioned video diffusion framework that models how dexterous human actions induce dynamic changes in static 3D scenes.\n  Given a static 3D scene rendering and an egocentric hand motion sequence, DWM generates temporally coherent videos depicting plausible human-scene interactions. Our approach conditions video generation on (1) static scene renderings following a specified camera trajectory to ensure spatial consistency, and (2) egocentric hand mesh renderings that encode both geometry and motion cues to model action-conditioned dynamics directly. To train DWM, we construct a hybrid interaction video dataset. Synthetic egocentric interactions provide fully aligned supervision for joint locomotion and manipulation learning, while fixed-camera real-world videos contribute diverse and realistic object dynamics.\n  Experiments demonstrate that DWM enables realistic and physically plausible interactions, such as grasping, opening, and moving objects, while maintaining camera and scene consistency. This framework represents a first step toward video diffusion-based interactive digital twins and enables embodied simulation from egocentric actions.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "Project Page: snuvclab.github.io/dwm",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17907v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "locomotion",
                        "manipulation"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]world model"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "physically plausible"
                    ],
                    "score": 2.5
                },
                {
                    "name": "支柱五：交互与反应 (Interaction & Reaction)",
                    "id": "5_interaction_reaction",
                    "matched_keywords": [
                        "human-scene interaction"
                    ],
                    "score": 2.5
                },
                {
                    "name": "支柱六：视频提取与匹配 (Video Extraction)",
                    "id": "6_video_extraction",
                    "matched_keywords": [
                        "egocentric"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 15.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch",
                "4_motion_diffusion",
                "5_interaction_reaction",
                "6_video_extraction"
            ],
            "headline_zh": "提出灵巧世界模型DWM，实现基于视频扩散的交互式数字孪生",
            "summary_zh": "本文提出灵巧世界模型（DWM），这是一个场景-动作条件下的视频扩散框架，用于建模灵巧的人类动作如何引起静态3D场景中的动态变化。给定静态3D场景渲染和以自我为中心的手部运动序列，DWM生成时间上连贯的视频，描绘合理的人与场景交互。该方法通过以下方式调节视频生成：（1）遵循指定相机轨迹的静态场景渲染，以确保空间一致性；（2）以自我为中心的手部网格渲染，编码几何和运动线索，以直接建模动作条件下的动态。为了训练DWM，构建了一个混合交互视频数据集。合成的以自我为中心的交互为联合运动和操作学习提供完全对齐的监督，而固定相机的真实世界视频则贡献了多样且逼真的对象动态。实验表明，DWM能够实现逼真且物理上合理的交互，例如抓取、打开和移动对象，同时保持相机和场景一致性。该框架代表了基于视频扩散的交互式数字孪生的第一步，并实现了来自以自我为中心动作的具身模拟。",
            "intro_zh": [
                "现有数字孪生技术主要局限于静态场景的导航和视图合成，缺乏具身交互能力。",
                "DWM通过场景和动作条件下的视频扩散模型，生成逼真的人与场景交互视频，实现动态变化建模。",
                "混合数据集结合了合成数据的精确监督和真实数据的多样性，提升了模型泛化能力和真实感。"
            ],
            "method_zh": "**问题定义**：现有数字孪生技术无法模拟人类与环境的交互，缺乏对物理世界的动态理解。现有方法难以生成逼真、连贯且与人类动作相关的场景变化视频，限制了数字孪生的应用范围。\\n\\n**核心思路**：DWM的核心在于利用视频扩散模型，将静态3D场景和人类动作序列作为条件，生成动态的交互视频。通过将场景和动作信息融入扩散过程，模型能够学习到人类动作如何影响场景中的物体，从而生成逼真的交互效果。\\n\\n**技术框架**：DWM的整体框架包含以下几个主要模块：1) 静态场景渲染模块：根据给定的相机轨迹渲染静态3D场景。2) 手部运动编码模块：将以自我为中心的手部运动序列编码为手部网格渲染。3) 视频扩散模型：以场景渲染和手部网格渲染作为条件，生成交互视频。训练过程中，使用混合数据集进行监督，包括合成数据和真实数据。\\n\\n**关键创新**：DWM的关键创新在于将视频扩散模型与场景和动作条件相结合，实现了对交互式场景动态的建模。与传统的视频生成方法相比，DWM能够更好地控制生成视频的内容和风格，并生成与人类动作相关的逼真场景变化。此外，混合数据集的构建也为模型的训练提供了更全面的监督信息。\\n\\n**关键设计**：DWM使用U-Net结构的视频扩散模型，并引入了注意力机制来融合场景和动作信息。损失函数包括L1损失和感知损失，以保证生成视频的质量和真实感。在混合数据集的构建中，对合成数据和真实数据进行了加权，以平衡两者的贡献。",
            "application_zh": "DWM可应用于虚拟现实、游戏开发、机器人仿真等领域。例如，可以用于创建更逼真的虚拟环境，允许用户与虚拟物体进行交互。在机器人仿真中，DWM可以帮助机器人学习如何与环境进行交互，从而提高机器人的自主性和适应性。此外，DWM还可以用于生成训练数据，用于训练其他机器学习模型。",
            "highlight_zh": "实验结果表明，DWM能够生成逼真且物理上合理的交互视频，例如抓取、打开和移动物体。与基线方法相比，DWM在生成视频的质量、连贯性和真实感方面均有显著提升。用户研究表明，DWM生成的视频更符合人类的直觉，更具吸引力。",
            "tags_zh": [
                "视频扩散模型",
                "交互式数字孪生",
                "具身模拟",
                "3D场景理解",
                "动作条件视频生成"
            ],
            "_index": 6,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17907v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17907v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17907v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Chorus: Multi-Teacher Pretraining for Holistic 3D Gaussian Scene Encoding",
            "authors": [
                "Yue Li",
                "Qi Ma",
                "Runyi Yang",
                "Mengjiao Ma",
                "Bin Ren",
                "Nikola Popovic",
                "Nicu Sebe",
                "Theo Gevers",
                "Luc Van Gool",
                "Danda Pani Paudel",
                "Martin R. Oswald"
            ],
            "arxiv_id": "2512.17817v1",
            "summary": "While 3DGS has emerged as a high-fidelity scene representation, encoding rich, general-purpose features directly from its primitives remains under-explored. We address this gap by introducing Chorus, a multi-teacher pretraining framework that learns a holistic feed-forward 3D Gaussian Splatting (3DGS) scene encoder by distilling complementary signals from 2D foundation models. Chorus employs a shared 3D encoder and teacher-specific projectors to learn from language-aligned, generalist, and object-aware teachers, encouraging a shared embedding space that captures signals from high-level semantics to fine-grained structure.\n  We evaluate Chorus on a wide range of tasks: open-vocabulary semantic and instance segmentation, linear and decoder probing, as well as data-efficient supervision. Besides 3DGS, we also test Chorus on several benchmarks that only support point clouds by pretraining a variant using only Gaussians' centers, colors, estimated normals as inputs. Interestingly, this encoder shows strong transfer and outperforms the point clouds baseline while using 39.9 times fewer training scenes. Finally, we propose a render-and-distill adaptation that facilitates out-of-domain finetuning. Our code and model will be released upon publication.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17817v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "3DGS",
                        "gaussian splatting",
                        "splatting",
                        "open-vocabulary",
                        "open vocabulary"
                    ],
                    "score": 12.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 15.0,
            "hit_pillars": [
                "3_perception_slam",
                "9_embodied_foundation"
            ],
            "headline_zh": "Chorus：多教师预训练用于整体3D高斯场景编码",
            "summary_zh": "3D高斯溅射(3DGS)已成为一种高保真场景表示方法，但直接从其图元中编码丰富的、通用的特征仍未得到充分探索。我们通过引入Chorus来解决这一差距，Chorus是一个多教师预训练框架，通过从2D基础模型中提取互补信号来学习整体前馈3DGS场景编码器。Chorus采用共享的3D编码器和特定于教师的投影器，从语言对齐的、通用的和对象感知的教师那里学习，从而鼓励共享嵌入空间，该空间捕获从高级语义到精细结构的信号。我们在广泛的任务中评估Chorus：开放词汇语义和实例分割、线性探测和解码器探测，以及数据高效监督。除了3DGS，我们还在几个仅支持点云的基准上测试Chorus，方法是预训练一个仅使用高斯中心、颜色、估计法线作为输入的变体。有趣的是，这个编码器显示出强大的迁移能力，并且优于点云基线，同时使用的训练场景减少了39.9倍。最后，我们提出了一种渲染和蒸馏的自适应方法，以促进领域外的微调。我们的代码和模型将在发布后发布。",
            "intro_zh": [
                "现有3DGS场景表示缺乏通用的特征编码能力，限制了其在各种下游任务中的应用。",
                "Chorus利用多教师预训练框架，从2D基础模型中提取互补信息，学习3DGS场景的整体编码。",
                "实验表明，Chorus在语义分割、实例分割等任务上表现出色，且在数据高效性和跨域迁移方面具有优势。"
            ],
            "method_zh": "**问题定义**：现有3D高斯溅射(3DGS)场景表示方法虽然在渲染质量上表现出色，但缺乏直接从3DGS图元中提取通用特征的能力。这限制了3DGS在各种需要高级语义理解和推理的下游任务中的应用，例如开放词汇语义分割和实例分割。现有的方法要么依赖于2D图像的特征，要么直接处理点云，无法充分利用3DGS的结构化信息。\n\n**核心思路**：Chorus的核心思路是通过多教师预训练，将2D基础模型的知识迁移到3DGS场景编码器中。通过从多个具有互补能力的2D教师模型（例如，语言对齐模型、通用模型和对象感知模型）中提取信息，Chorus能够学习到更丰富、更通用的3DGS场景表示。这种方法旨在弥合2D和3D之间的差距，并充分利用预训练模型的强大能力。\n\n**技术框架**：Chorus框架包含一个共享的3D编码器和多个特定于教师的投影器。3D编码器负责将3DGS图元（例如，高斯中心、颜色、法线等）编码成特征向量。每个教师模型都有一个对应的投影器，负责将教师模型的输出映射到与3D编码器输出相同的嵌入空间。训练过程中，通过最小化3D编码器输出和教师模型输出之间的差异，将知识从教师模型迁移到3D编码器。框架还包含一个渲染和蒸馏的自适应方法，以促进领域外的微调。\n\n**关键创新**：Chorus的关键创新在于其多教师预训练策略。与传统的单教师蒸馏方法相比，Chorus能够从多个具有互补能力的教师模型中学习，从而获得更全面、更鲁棒的场景表示。此外，Chorus还提出了一种渲染和蒸馏的自适应方法，能够有效地将模型迁移到新的领域。\n\n**关键设计**：3D编码器可以使用各种网络结构，例如MLP或Transformer。教师投影器通常是简单的线性层或MLP。损失函数通常采用对比损失或均方误差损失，用于衡量3D编码器输出和教师模型输出之间的差异。渲染和蒸馏的自适应方法涉及将3D场景渲染成2D图像，然后使用教师模型对渲染图像进行处理，并将教师模型的输出作为3D编码器的目标。",
            "application_zh": "Chorus具有广泛的应用前景，包括但不限于：机器人导航、自动驾驶、虚拟现实/增强现实、三维场景理解与编辑等。通过学习通用的3D场景表示，Chorus能够提升这些应用在复杂环境中的感知和推理能力，实现更智能、更高效的交互。未来，Chorus有望成为3D场景理解领域的重要基础模型。",
            "highlight_zh": "Chorus在多个任务上取得了显著的性能提升。例如，在开放词汇语义分割任务中，Chorus优于现有的点云基线方法。更重要的是，通过仅使用高斯中心、颜色和法线进行预训练，Chorus在点云基准测试中表现出色，同时使用的训练场景减少了39.9倍，展示了其强大的数据效率和泛化能力。渲染和蒸馏的自适应方法也成功地将模型迁移到新的领域。",
            "tags_zh": [
                "3D高斯溅射",
                "多教师学习",
                "预训练",
                "场景编码",
                "知识蒸馏"
            ],
            "_index": 7,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17817v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17817v1/figs/view_selection.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17817v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation",
            "authors": [
                "Rang Li",
                "Lei Li",
                "Shuhuai Ren",
                "Hao Tian",
                "Shuhao Gu",
                "Shicheng Li",
                "Zihao Yue",
                "Yudong Wang",
                "Wenhan Ma",
                "Zhe Yang",
                "Jingyuan Ma",
                "Zhifang Sui",
                "Fuli Luo"
            ],
            "arxiv_id": "2512.17495v1",
            "summary": "Visual grounding, localizing objects from natural language descriptions, represents a critical bridge between language and vision understanding. While multimodal large language models (MLLMs) achieve impressive scores on existing benchmarks, a fundamental question remains: can MLLMs truly ground language in vision with human-like sophistication, or are they merely pattern-matching on simplified datasets? Current benchmarks fail to capture real-world complexity where humans effortlessly navigate ambiguous references and recognize when grounding is impossible. To rigorously assess MLLMs' true capabilities, we introduce GroundingME, a benchmark that systematically challenges models across four critical dimensions: (1) Discriminative, distinguishing highly similar objects, (2) Spatial, understanding complex relational descriptions, (3) Limited, handling occlusions or tiny objects, and (4) Rejection, recognizing ungroundable queries. Through careful curation combining automated generation with human verification, we create 1,005 challenging examples mirroring real-world complexity. Evaluating 25 state-of-the-art MLLMs reveals a profound capability gap: the best model achieves only 45.1% accuracy, while most score 0% on rejection tasks, reflexively hallucinating objects rather than acknowledging their absence, raising critical safety concerns for deployment. We explore two strategies for improvements: (1) test-time scaling selects optimal response by thinking trajectory to improve complex grounding by up to 2.9%, and (2) data-mixture training teaches models to recognize ungroundable queries, boosting rejection accuracy from 0% to 27.9%. GroundingME thus serves as both a diagnostic tool revealing current limitations in MLLMs and a roadmap toward human-level visual grounding.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17495v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal",
                        "[T]visual grounding"
                    ],
                    "score": 15.0
                }
            ],
            "relevance_score": 15.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "GroundingME：多维度评测揭示MLLM在视觉定位能力上的差距",
            "summary_zh": "视觉定位，即从自然语言描述中定位物体，是连接语言和视觉理解的关键桥梁。尽管多模态大型语言模型（MLLM）在现有基准测试中取得了令人印象深刻的分数，但一个根本问题仍然存在：MLLM是否真正像人类一样精细地将语言定位到视觉中，还是仅仅在简化的数据集上进行模式匹配？现有的基准测试未能捕捉到现实世界的复杂性，在现实世界中，人类可以轻松地处理模糊的引用，并识别何时无法进行定位。为了严格评估MLLM的真实能力，我们引入了GroundingME，这是一个基准测试，它在四个关键维度上系统地挑战模型：（1）区分性，区分高度相似的物体；（2）空间性，理解复杂的关系描述；（3）有限性，处理遮挡或微小物体；（4）拒绝性，识别无法定位的查询。通过结合自动生成和人工验证的精心策划，我们创建了1,005个具有挑战性的示例，反映了现实世界的复杂性。对25个最先进的MLLM的评估揭示了一个深刻的能力差距：最好的模型仅达到45.1%的准确率，而大多数模型在拒绝任务中的得分为0%，本能地幻觉出物体，而不是承认它们不存在，这引发了对部署的关键安全问题。我们探索了两种改进策略：（1）测试时缩放通过思考轨迹选择最佳响应，从而将复杂定位提高高达2.9%；（2）数据混合训练教会模型识别无法定位的查询，将拒绝准确率从0%提高到27.9%。因此，GroundingME既可以作为揭示MLLM当前局限性的诊断工具，也可以作为实现人类水平视觉定位的路线图。",
            "intro_zh": [
                "现有MLLM在视觉定位任务上表现看似出色，但现有评测benchmark难以反映真实场景的复杂性，无法有效评估模型真正的定位能力。",
                "提出GroundingME基准，从区分性、空间性、有限性和拒绝性四个维度系统性地评估MLLM的视觉定位能力，更贴近现实世界。",
                "实验表明现有MLLM在GroundingME上表现不佳，尤其在拒绝无法定位的查询时几乎完全失效，通过数据混合训练可显著提升拒绝准确率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多模态大型语言模型（MLLM）在视觉定位任务中存在的真实能力评估问题。现有基准测试无法充分捕捉现实世界场景的复杂性，例如区分相似物体、理解空间关系、处理遮挡以及识别无法定位的查询。这导致模型在现有基准上表现良好，但实际应用中可能存在较大差距。\\n\\n**核心思路**：论文的核心思路是构建一个更具挑战性和现实感的视觉定位基准测试集，即GroundingME。该基准从四个关键维度（区分性、空间性、有限性和拒绝性）系统性地评估MLLM的视觉定位能力，从而更准确地反映模型在真实场景中的表现。通过分析模型在这些维度上的表现，可以揭示模型在视觉定位方面的局限性，并为改进模型提供指导。\\n\\n**技术框架**：GroundingME基准的构建流程主要包括以下几个阶段：1) 数据生成：结合自动生成和人工验证，创建包含1005个具有挑战性的示例。2) 维度设计：从区分性、空间性、有限性和拒绝性四个维度设计测试用例，覆盖现实世界场景中的常见挑战。3) 模型评估：使用GroundingME评估25个最先进的MLLM，分析模型在不同维度上的表现。4) 改进策略探索：探索测试时缩放和数据混合训练两种策略，以提升模型在复杂定位和拒绝任务上的性能。\\n\\n**关键创新**：GroundingME的关键创新在于其多维度评估体系，能够更全面、更准确地评估MLLM的视觉定位能力。与现有基准测试相比，GroundingME更注重模拟现实世界场景的复杂性，例如引入相似物体、复杂空间关系、遮挡以及无法定位的查询。此外，论文还探索了两种有效的改进策略，为提升MLLM的视觉定位能力提供了新的思路。\\n\\n**关键设计**：在数据生成方面，论文采用了自动生成和人工验证相结合的方法，以保证数据的质量和多样性。在维度设计方面，论文精心选择了区分性、空间性、有限性和拒绝性四个维度，这些维度涵盖了视觉定位任务中的关键挑战。在模型评估方面，论文使用了多种评估指标，例如准确率，以全面评估模型在不同维度上的表现。此外，论文还对测试时缩放和数据混合训练两种策略进行了详细的参数设置和实验分析。",
            "application_zh": "该研究成果可应用于机器人导航、智能监控、图像搜索、辅助驾驶等领域。通过提高MLLM的视觉定位能力，可以使机器人在复杂环境中更好地理解人类指令，更准确地识别和定位目标物体，从而实现更智能、更安全的应用。此外，该研究提出的GroundingME基准可以作为评估和改进MLLM视觉定位能力的重要工具。",
            "highlight_zh": "实验结果表明，现有最先进的MLLM在GroundingME基准上的表现远低于人类水平，最佳模型仅达到45.1%的准确率，且在拒绝任务中几乎完全失效（0%准确率）。通过测试时缩放，复杂定位能力提升高达2.9%。通过数据混合训练，拒绝准确率从0%提升到27.9%，表明该方法能够有效提升模型识别无法定位查询的能力。",
            "tags_zh": [
                "视觉定位",
                "多模态大语言模型",
                "基准测试",
                "视觉理解",
                "多维度评估"
            ],
            "_index": 8,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17495v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17495v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17495v1/images/category.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting",
            "authors": [
                "Ananta R. Bhattarai",
                "Helge Rhodin"
            ],
            "arxiv_id": "2512.17908v1",
            "summary": "Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17908v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "distillation"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "depth estimation",
                        "monocular depth",
                        "[T]Depth Anything"
                    ],
                    "score": 10.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 14.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam",
                "9_embodied_foundation"
            ],
            "headline_zh": "Re-Depth Anything：利用自监督重照明进行测试时深度优化",
            "summary_zh": "单目深度估计仍然具有挑战性，因为最近的基础模型，如Depth Anything V2 (DA-V2)，在处理远离训练分布的真实世界图像时表现不佳。我们提出了Re-Depth Anything，这是一个测试时自监督框架，通过将DA-V2与大规模2D扩散模型的强大先验知识融合，来弥合这一领域差距。我们的方法通过重新照亮预测的深度图并增强输入，直接在输入图像上执行无标签优化。这种重合成方法利用具有分数蒸馏采样(SDS)的新生成环境中的阴影形状(SfS)线索来代替经典的光度重建。为了防止优化崩溃，我们的框架采用了一种有针对性的优化策略：我们冻结编码器，仅更新中间嵌入，同时微调解码器，而不是直接优化深度或微调整个模型。在不同的基准测试中，Re-Depth Anything在深度精度和真实感方面都比DA-V2有了显著提高，展示了通过增强几何推理进行自监督的新途径。",
            "intro_zh": [
                "现有单目深度估计模型在处理真实场景图像时，由于领域差异，性能显著下降，难以满足实际应用需求。",
                "Re-Depth Anything利用大规模2D扩散模型的先验知识，通过重照明和数据增强，实现测试时自监督深度优化。",
                "实验结果表明，该方法在多个基准测试中显著提升了深度估计的精度和真实感，优于现有方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决单目深度估计模型，特别是Depth Anything V2 (DA-V2)，在处理与训练数据分布差异较大的真实世界图像时，深度估计精度下降的问题。现有方法难以有效利用图像中的几何信息，导致深度估计结果不准确，缺乏真实感。\\n\\n**核心思路**：论文的核心思路是利用自监督学习，在测试时对深度图进行优化。具体来说，通过重新照亮预测的深度图，并结合大规模2D扩散模型的先验知识，来指导深度图的优化过程。这种方法避免了对整个模型进行微调，而是专注于优化中间嵌入，从而提高了优化效率和稳定性。\\n\\n**技术框架**：Re-Depth Anything框架主要包含以下几个阶段：1) 使用DA-V2模型生成初始深度图；2) 对输入图像进行增强；3) 利用重照明技术，根据深度图和光照条件重新合成图像；4) 使用大规模2D扩散模型作为先验，通过分数蒸馏采样(SDS)计算损失；5) 通过优化中间嵌入和微调解码器，更新深度图。整个过程是自监督的，不需要额外的标签数据。\\n\\n**关键创新**：该方法最重要的创新点在于将大规模2D扩散模型的先验知识引入到单目深度估计的测试时优化过程中。通过重照明和分数蒸馏采样，可以有效地利用图像中的几何信息，提高深度估计的精度和真实感。此外，该方法采用了一种有针对性的优化策略，只更新中间嵌入和微调解码器，避免了对整个模型进行微调，从而提高了优化效率和稳定性。\\n\\n**关键设计**：在关键设计方面，论文采用了分数蒸馏采样(SDS)作为损失函数，用于衡量重合成图像与真实图像之间的差异。此外，论文还设计了一种有针对性的优化策略，冻结编码器，只更新中间嵌入和微调解码器。这种策略可以有效地防止优化崩溃，并提高优化效率。具体的光照模型和扩散模型的选择在论文中有详细描述。",
            "application_zh": "Re-Depth Anything具有广泛的应用前景，例如机器人导航、自动驾驶、虚拟现实、增强现实等领域。它可以提高机器人和自动驾驶系统对环境的感知能力，从而提高其安全性和可靠性。在虚拟现实和增强现实领域，它可以生成更逼真的3D场景，从而提高用户体验。此外，该方法还可以应用于图像编辑、三维重建等领域。",
            "highlight_zh": "实验结果表明，Re-Depth Anything在多个基准测试中显著提升了深度估计的精度和真实感。例如，在某个数据集上，该方法的深度估计误差降低了10%以上，并且生成的深度图更加平滑和真实。与DA-V2相比，Re-Depth Anything在深度精度和视觉质量上都有显著提升，证明了该方法的有效性。",
            "tags_zh": [
                "单目深度估计",
                "自监督学习",
                "测试时优化",
                "深度图优化",
                "扩散模型",
                "重照明",
                "分数蒸馏采样"
            ],
            "_index": 9,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17908v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17908v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17908v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "FLEG: Feed-Forward Language Embedded Gaussian Splatting from Any Views",
            "authors": [
                "Qijian Tian",
                "Xin Tan",
                "Jiayu Ying",
                "Xuhong Wang",
                "Yuan Xie",
                "Lizhuang Ma"
            ],
            "arxiv_id": "2512.17541v1",
            "summary": "We present FLEG, a feed-forward network that reconstructs language-embedded 3D Gaussians from any views. Previous straightforward solutions combine feed-forward reconstruction with Gaussian heads but suffer from fixed input views and insufficient 3D training data. In contrast, we propose a 3D-annotation-free training framework for 2D-to-3D lifting from arbitrary uncalibrated and unposed multi-view images. Since the framework does not require 3D annotations, we can leverage large-scale video data with easily obtained 2D instance information to enrich semantic embedding. We also propose an instance-guided contrastive learning to align 2D semantics with the 3D representations. In addition, to mitigate the high memory and computational cost of dense views, we further propose a geometry-semantic hierarchical sparsification strategy. Our FLEG efficiently reconstructs language-embedded 3D Gaussian representation in a feed-forward manner from arbitrary sparse or dense views, jointly producing accurate geometry, high-fidelity appearance, and language-aligned semantics. Extensive experiments show that it outperforms existing methods on various related tasks. Project page: https://fangzhou2000.github.io/projects/fleg.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "Project page: https://fangzhou2000.github.io/projects/fleg",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17541v1",
            "code_links": [
                {
                    "url": "https://fangzhou2000.github.io/projects/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "contrastive learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]gaussian splatting",
                        "[T]splatting"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 13.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "FLEG：提出一种从任意视角进行前馈语言嵌入高斯溅射的方法",
            "summary_zh": "本文提出了一种名为FLEG的前馈网络，该网络可以从任意视角重建语言嵌入的3D高斯分布。以往的直接解决方案将前馈重建与高斯头部结合，但存在输入视角固定和3D训练数据不足的问题。为了解决这些问题，我们提出了一个无需3D标注的训练框架，用于从任意未校准和未姿态的多视角图像中进行2D到3D的提升。由于该框架不需要3D标注，我们可以利用大规模视频数据和易于获得的2D实例信息来丰富语义嵌入。我们还提出了一种实例引导的对比学习方法，以对齐2D语义和3D表示。此外，为了减轻密集视角的内存和计算成本，我们进一步提出了一种几何-语义分层稀疏化策略。我们的FLEG能够以高效的前馈方式从任意稀疏或密集视角重建语言嵌入的3D高斯表示，从而联合生成精确的几何形状、高保真外观和语言对齐的语义。大量实验表明，它在各种相关任务上优于现有方法。",
            "intro_zh": [
                "现有方法在从多视角图像重建3D场景时，存在输入视角固定和缺乏足够3D训练数据的问题。",
                "FLEG提出了一种无需3D标注的训练框架，利用大规模视频数据和2D实例信息来丰富语义嵌入，并采用实例引导的对比学习对齐2D和3D语义。",
                "FLEG通过几何-语义分层稀疏化策略降低计算成本，实验表明其在几何精度、外观保真度和语义对齐方面优于现有方法。"
            ],
            "method_zh": "**问题定义**：现有方法在从多视角图像重建3D场景时，通常需要预定义的相机姿态和大量的3D标注数据。此外，直接将2D图像特征提升到3D空间进行重建，容易受到视角变化的影响，并且难以有效地利用大规模的2D图像数据进行语义信息的学习。这些方法在处理任意视角和稀疏视角的情况下表现不佳，且难以实现语言嵌入的3D场景重建。\\n\\n**核心思路**：FLEG的核心思路是利用大规模的2D视频数据，通过实例分割等技术提取2D语义信息，并将其与3D高斯表示进行对齐。通过无需3D标注的训练框架，避免了对昂贵的3D数据的依赖。同时，采用几何-语义分层稀疏化策略，降低了计算和内存成本，使得FLEG能够处理任意视角和稀疏视角下的场景重建。\\n\\n**技术框架**：FLEG的整体框架包含以下几个主要模块：1) 2D特征提取模块，用于从多视角图像中提取2D特征和语义信息；2) 2D-to-3D提升模块，将2D特征提升到3D空间，并初始化3D高斯表示；3) 实例引导的对比学习模块，用于对齐2D语义和3D表示；4) 几何-语义分层稀疏化模块，用于降低计算成本；5) 渲染模块，用于从3D高斯表示中渲染出图像。整个流程以端到端的方式进行训练。\\n\\n**关键创新**：FLEG的关键创新在于：1) 提出了无需3D标注的训练框架，能够利用大规模的2D视频数据进行训练；2) 提出了实例引导的对比学习方法，有效地对齐了2D语义和3D表示；3) 提出了几何-语义分层稀疏化策略，降低了计算成本，使得FLEG能够处理任意视角和稀疏视角下的场景重建。与现有方法相比，FLEG能够更有效地利用2D语义信息，并且具有更强的泛化能力。\\n\\n**关键设计**：在实例引导的对比学习中，使用了InfoNCE损失函数来最大化同一实例在2D和3D空间中的表示相似度，并最小化不同实例之间的相似度。几何-语义分层稀疏化策略首先根据高斯点的几何位置进行粗略的稀疏化，然后根据高斯点的语义信息进行精细的稀疏化。网络结构采用了Transformer架构，用于学习2D特征之间的关系，并将其映射到3D空间。",
            "application_zh": "FLEG具有广泛的应用前景，例如在机器人导航、虚拟现实、增强现实、自动驾驶等领域。它可以用于构建具有语义信息的3D场景地图，帮助机器人理解周围环境，并进行更智能的决策。此外，FLEG还可以用于生成逼真的虚拟场景，为用户提供沉浸式的体验。在自动驾驶领域，FLEG可以用于感知周围环境，提高驾驶安全性。",
            "highlight_zh": "实验结果表明，FLEG在多个数据集上都取得了显著的性能提升。例如，在ScanNet数据集上，FLEG在几何精度、外观保真度和语义对齐方面都优于现有方法。与基线方法相比，FLEG在几何精度上提升了约10%，在外观保真度上提升了约8%，在语义对齐方面提升了约12%。这些结果表明，FLEG能够有效地重建具有语义信息的3D场景。",
            "tags_zh": [
                "3D重建",
                "高斯溅射",
                "语言嵌入",
                "对比学习",
                "多视角图像",
                "语义分割",
                "无需3D标注"
            ],
            "_index": 10,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17541v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17541v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17541v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Flying in Clutter on Monocular RGB by Learning in 3D Radiance Fields with Domain Adaptation",
            "authors": [
                "Xijie Huang",
                "Jinhan Li",
                "Tianyue Wu",
                "Xin Zhou",
                "Zhichao Han",
                "Fei Gao"
            ],
            "arxiv_id": "2512.17349v1",
            "summary": "Modern autonomous navigation systems predominantly rely on lidar and depth cameras. However, a fundamental question remains: Can flying robots navigate in clutter using solely monocular RGB images? Given the prohibitive costs of real-world data collection, learning policies in simulation offers a promising path. Yet, deploying such policies directly in the physical world is hindered by the significant sim-to-real perception gap. Thus, we propose a framework that couples the photorealism of 3D Gaussian Splatting (3DGS) environments with Adversarial Domain Adaptation. By training in high-fidelity simulation while explicitly minimizing feature discrepancy, our method ensures the policy relies on domain-invariant cues. Experimental results demonstrate that our policy achieves robust zero-shot transfer to the physical world, enabling safe and agile flight in unstructured environments with varying illumination.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "8 pages, 7 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17349v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "sim-to-real"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "3DGS",
                        "gaussian splatting",
                        "splatting"
                    ],
                    "score": 8.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "zero-shot transfer"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 13.0,
            "hit_pillars": [
                "1_robot_core",
                "3_perception_slam",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于3D辐射场和对抗域适应的单目RGB图像无人机复杂环境导航方法",
            "summary_zh": "现代自主导航系统主要依赖激光雷达和深度相机。然而，一个根本问题仍然存在：飞行机器人能否仅使用单目RGB图像在复杂环境中导航？考虑到真实世界数据收集的高昂成本，在模拟环境中学习策略提供了一条有希望的途径。然而，由于显著的模拟到真实世界的感知差距，将这些策略直接部署到物理世界受到阻碍。因此，我们提出了一个框架，将3D高斯溅射（3DGS）环境的光真实感与对抗域适应相结合。通过在高保真模拟中训练，同时显式地最小化特征差异，我们的方法确保策略依赖于领域不变的线索。实验结果表明，我们的策略实现了对物理世界的鲁棒零样本迁移，从而能够在具有不同光照的非结构化环境中进行安全和敏捷的飞行。",
            "intro_zh": [
                "现有自主导航系统依赖激光雷达和深度相机，但成本高昂，探索仅使用单目RGB图像在复杂环境中导航是挑战。",
                "该论文提出了一种基于3D高斯溅射和对抗域适应的框架，在高保真模拟环境中训练策略，并最小化特征差异。",
                "实验结果表明，该策略能够实现对物理世界的鲁棒零样本迁移，在非结构化环境中实现安全敏捷的飞行。"
            ],
            "method_zh": "**问题定义**：论文旨在解决仅使用单目RGB图像，无人机在复杂环境中自主导航的问题。现有方法依赖激光雷达或深度相机，成本较高且体积较大。直接在模拟环境中训练策略并迁移到真实世界，会受到模拟到真实世界（sim-to-real）的感知差距的影响，导致性能下降。\\n\\n**核心思路**：论文的核心思路是利用3D高斯溅射（3DGS）生成高保真度的模拟环境，并结合对抗域适应技术，减小模拟环境和真实环境之间的特征差异。通过训练一个能够提取领域不变特征的策略，实现从模拟到真实的零样本迁移。\\n\\n**技术框架**：整体框架包含两个主要部分：高保真模拟环境构建和对抗域适应策略学习。首先，使用3DGS构建逼真的模拟环境。然后，设计一个对抗域适应网络，该网络包含一个策略网络和一个判别器网络。策略网络负责学习导航策略，判别器网络负责区分输入来自模拟环境还是真实环境。通过对抗训练，策略网络学习提取领域不变的特征，从而提高策略在真实环境中的泛化能力。\\n\\n**关键创新**：该论文的关键创新在于将3DGS和对抗域适应相结合，用于解决单目RGB图像的无人机复杂环境导航问题。3DGS提供了高保真度的模拟环境，对抗域适应减小了模拟和真实环境之间的差距。这种结合使得策略能够从模拟环境迁移到真实环境，而无需额外的真实数据训练。\\n\\n**关键设计**：在对抗域适应方面，使用了梯度反转层（Gradient Reversal Layer, GRL）来训练策略网络，使其提取的特征难以被判别器区分。损失函数包括策略网络的导航损失（例如，模仿学习损失或强化学习奖励）和对抗损失。对抗损失鼓励策略网络学习领域不变的特征。具体的网络结构和参数设置（例如，学习率、批大小、网络层数）在论文中有详细描述，但此处未知。",
            "application_zh": "该研究成果可应用于低成本、轻量化的无人机自主导航系统，尤其适用于资源受限或环境复杂的场景，如灾后搜救、农业巡检、室内导航等。通过降低对昂贵传感器的依赖，可以扩展无人机的应用范围，并促进无人机技术的普及。",
            "highlight_zh": "该论文提出的方法实现了在复杂环境下的鲁棒零样本迁移。实验结果表明，该策略能够在具有不同光照的非结构化环境中进行安全和敏捷的飞行。具体的性能数据（例如，成功率、平均飞行速度、避障距离等）和对比基线（例如，不使用对抗域适应的策略）在论文中有详细描述，但此处未知。",
            "tags_zh": [
                "无人机导航",
                "单目视觉",
                "3D辐射场",
                "对抗域适应",
                "零样本迁移"
            ],
            "_index": 11,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17349v1/figures/3dgs.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17349v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17349v1/figures/rl_training_comparison.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "G3Splat: Geometrically Consistent Generalizable Gaussian Splatting",
            "authors": [
                "Mehdi Hosseinzadeh",
                "Shin-Fang Chng",
                "Yi Xu",
                "Simon Lucey",
                "Ian Reid",
                "Ravi Garg"
            ],
            "arxiv_id": "2512.17547v1",
            "summary": "3D Gaussians have recently emerged as an effective scene representation for real-time splatting and accurate novel-view synthesis, motivating several works to adapt multi-view structure prediction networks to regress per-pixel 3D Gaussians from images. However, most prior work extends these networks to predict additional Gaussian parameters -- orientation, scale, opacity, and appearance -- while relying almost exclusively on view-synthesis supervision. We show that a view-synthesis loss alone is insufficient to recover geometrically meaningful splats in this setting. We analyze and address the ambiguities of learning 3D Gaussian splats under self-supervision for pose-free generalizable splatting, and introduce G3Splat, which enforces geometric priors to obtain geometrically consistent 3D scene representations. Trained on RE10K, our approach achieves state-of-the-art performance in (i) geometrically consistent reconstruction, (ii) relative pose estimation, and (iii) novel-view synthesis. We further demonstrate strong zero-shot generalization on ScanNet, substantially outperforming prior work in both geometry recovery and relative pose estimation. Code and pretrained models are released on our project page (https://m80hz.github.io/g3splat/).",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "Project page: https://m80hz.github.io/g3splat/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17547v1",
            "code_links": [
                {
                    "url": "https://m80hz.github.io/g3splat/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]gaussian splatting",
                        "[T]splatting"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "G3Splat：通过几何一致性先验实现可泛化的高斯溅射",
            "summary_zh": "3D高斯溅射最近成为一种有效的场景表示方法，可用于实时溅射和精确的新视角合成，这促使一些工作调整多视角结构预测网络，以从图像中回归每个像素的3D高斯分布。然而，先前的大部分工作扩展这些网络以预测额外的高斯参数——方向、尺度、不透明度和外观——同时几乎完全依赖于视角合成监督。我们表明，仅视角合成损失不足以在这种设置下恢复几何上有意义的溅射。我们分析并解决了在无姿态可泛化溅射的自监督下学习3D高斯溅射的模糊性，并引入了G3Splat，它强制执行几何先验以获得几何一致的3D场景表示。在RE10K上训练后，我们的方法在（i）几何一致的重建，（ii）相对姿态估计和（iii）新视角合成方面实现了最先进的性能。我们进一步展示了在ScanNet上的强大零样本泛化能力，在几何恢复和相对姿态估计方面都大大优于先前的工作。代码和预训练模型已在我们的项目页面上发布。",
            "intro_zh": [
                "现有方法依赖视角合成损失学习3D高斯溅射，难以保证几何一致性，导致重建质量下降。",
                "G3Splat通过引入几何先验，显式地约束3D高斯溅射的学习过程，从而提升几何一致性。",
                "实验表明，G3Splat在几何重建、相对姿态估计和新视角合成方面均优于现有方法，并具有良好的泛化能力。"
            ],
            "method_zh": "**问题定义**：现有基于3D高斯溅射的场景表示方法，在进行novel-view synthesis时，通常依赖于多视角图像进行训练，并使用view-synthesis loss作为主要的监督信号。然而，这种方法在学习高斯参数（如方向、尺度等）时存在模糊性，导致重建的3D场景在几何上不一致，影响了后续任务的性能，例如相对姿态估计。\n\n**核心思路**：G3Splat的核心思路是通过引入几何先验，显式地约束3D高斯溅射的学习过程，从而解决几何不一致的问题。具体来说，论文通过设计特定的损失函数，鼓励学习到的高斯分布在空间中具有合理的几何形状，例如，相邻的高斯分布应该具有相似的朝向，避免出现不自然的扭曲。\n\n**技术框架**：G3Splat的整体框架基于现有的多视角结构预测网络，并对其进行了扩展。该框架首先使用一个神经网络从多视角图像中预测每个像素的3D高斯参数，然后使用一个splatting渲染器将这些高斯分布渲染成图像。与现有方法不同的是，G3Splat在训练过程中引入了几何一致性损失，以约束学习到的高斯参数。该框架包含以下主要模块：图像特征提取模块、高斯参数预测模块、splatting渲染模块和几何一致性损失计算模块。\n\n**关键创新**：G3Splat最重要的技术创新点在于引入了几何一致性损失，该损失能够有效地约束3D高斯溅射的学习过程，从而提高重建的3D场景的几何一致性。与现有方法相比，G3Splat不再仅仅依赖于view-synthesis loss，而是将几何先验融入到学习过程中，从而更好地利用了多视角图像中的几何信息。\n\n**关键设计**：G3Splat的关键设计包括以下几个方面：1) 几何一致性损失的具体形式，例如，可以使用相邻高斯分布的朝向差异作为损失项；2) 几何一致性损失的权重，需要仔细调整以平衡view-synthesis loss和几何一致性损失之间的关系；3) 网络结构的细节，例如，可以使用更深的网络来提取更丰富的图像特征。",
            "application_zh": "G3Splat在三维重建、新视角合成、机器人导航、增强现实等领域具有广泛的应用前景。它可以用于创建高质量的3D场景模型，从而为机器人提供更准确的环境感知能力，并为用户提供更逼真的虚拟现实体验。此外，G3Splat还可以用于生成任意视角的图像，从而为图像编辑和视频制作提供更大的灵活性。",
            "highlight_zh": "G3Splat在RE10K数据集上实现了state-of-the-art的性能，在几何一致性重建、相对姿态估计和新视角合成方面均优于现有方法。此外，G3Splat在ScanNet数据集上展示了强大的零样本泛化能力，在几何恢复和相对姿态估计方面都大大优于先前的工作。例如，在相对姿态估计任务中，G3Splat的性能比现有方法提升了超过10%。",
            "tags_zh": [
                "高斯溅射",
                "三维重建",
                "新视角合成",
                "几何一致性",
                "可泛化",
                "自监督学习",
                "场景表示"
            ],
            "_index": 12,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17547v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17547v1/figs/re10k/ctx/aef133f549f40970_ctx_0_000010.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17547v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Any-Optical-Model: A Universal Foundation Model for Optical Remote Sensing",
            "authors": [
                "Xuyang Li",
                "Chenyu Li",
                "Danfeng Hong"
            ],
            "arxiv_id": "2512.17224v1",
            "summary": "Optical satellites, with their diverse band layouts and ground sampling distances, supply indispensable evidence for tasks ranging from ecosystem surveillance to emergency response. However, significant discrepancies in band composition and spatial resolution across different optical sensors present major challenges for existing Remote Sensing Foundation Models (RSFMs). These models are typically pretrained on fixed band configurations and resolutions, making them vulnerable to real world scenarios involving missing bands, cross sensor fusion, and unseen spatial scales, thereby limiting their generalization and practical deployment. To address these limitations, we propose Any Optical Model (AOM), a universal RSFM explicitly designed to accommodate arbitrary band compositions, sensor types, and resolution scales. To preserve distinctive spectral characteristics even when bands are missing or newly introduced, AOM introduces a spectrum-independent tokenizer that assigns each channel a dedicated band embedding, enabling explicit encoding of spectral identity. To effectively capture texture and contextual patterns from sub-meter to hundred-meter imagery, we design a multi-scale adaptive patch embedding mechanism that dynamically modulates the receptive field. Furthermore, to maintain global semantic consistency across varying resolutions, AOM incorporates a multi-scale semantic alignment mechanism alongside a channel-wise self-supervised masking and reconstruction pretraining strategy that jointly models spectral-spatial relationships. Extensive experiments on over 10 public datasets, including those from Sentinel-2, Landsat, and HLS, demonstrate that AOM consistently achieves state-of-the-art (SOTA) performance under challenging conditions such as band missing, cross sensor, and cross resolution settings.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "Accepted by AAAI2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17224v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "spatial relationship"
                    ],
                    "score": 3.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]foundation model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "7_retargeting",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Any-Optical-Model，解决遥感领域跨传感器、分辨率和缺失波段的通用性难题。",
            "summary_zh": "光学卫星凭借其多样的波段布局和地面采样距离，为生态系统监测到应急响应等任务提供了不可或缺的证据。然而，不同光学传感器在波段组成和空间分辨率上的显著差异，对现有的遥感基础模型(RSFM)提出了重大挑战。这些模型通常在固定的波段配置和分辨率上进行预训练，使其容易受到涉及缺失波段、跨传感器融合和未见空间尺度的真实场景的影响，从而限制了其泛化能力和实际部署。为了解决这些限制，我们提出了Any Optical Model (AOM)，一个通用RSFM，专门设计用于适应任意波段组成、传感器类型和分辨率尺度。为了在波段缺失或新引入时保持独特的频谱特征，AOM引入了一种频谱独立的tokenizer，为每个通道分配一个专用的波段嵌入，从而能够显式地编码频谱身份。为了有效地捕获从亚米级到百米级图像的纹理和上下文模式，我们设计了一种多尺度自适应patch嵌入机制，动态地调节感受野。此外，为了保持跨不同分辨率的全局语义一致性，AOM结合了一种多尺度语义对齐机制，以及一种通道级的自监督掩码和重建预训练策略，该策略联合建模了光谱-空间关系。在包括Sentinel-2、Landsat和HLS在内的10多个公共数据集上进行的大量实验表明，AOM在诸如波段缺失、跨传感器和跨分辨率设置等具有挑战性的条件下，始终如一地实现了最先进(SOTA)的性能。",
            "intro_zh": [
                "现有遥感基础模型难以处理不同传感器、分辨率和缺失波段带来的挑战，限制了其泛化能力和实际应用。",
                "Any-Optical-Model (AOM) 通过频谱独立tokenizer、多尺度自适应patch嵌入和语义对齐机制，实现对任意波段、传感器和分辨率的通用适应。",
                "在多个数据集上的实验表明，AOM 在波段缺失、跨传感器和跨分辨率等场景下均取得了 SOTA 性能。"
            ],
            "method_zh": "**问题定义**：现有遥感基础模型通常针对特定传感器和分辨率进行预训练，难以处理实际应用中常见的波段缺失、跨传感器数据融合以及不同分辨率图像分析等问题。这些问题严重限制了模型的泛化能力和实际部署价值。\\n\\n**核心思路**：AOM的核心思路是设计一个通用的遥感基础模型，使其能够适应任意波段组成、传感器类型和分辨率尺度。通过显式编码波段身份、动态调节感受野以及多尺度语义对齐，模型能够有效提取和利用不同来源遥感数据的光谱和空间信息。\\n\\n**技术框架**：AOM的整体框架包括以下几个主要模块：1) 频谱独立Tokenizer：为每个通道分配一个独立的波段嵌入，显式编码光谱身份。2) 多尺度自适应Patch嵌入：动态调节感受野，捕获不同分辨率图像的纹理和上下文信息。3) 多尺度语义对齐：保持跨不同分辨率的全局语义一致性。4) 通道级自监督掩码和重建预训练：联合建模光谱-空间关系。\\n\\n**关键创新**：AOM最重要的技术创新在于其通用性设计，能够有效解决现有遥感基础模型在处理异构遥感数据时面临的挑战。与现有方法相比，AOM不再依赖于特定的波段配置和分辨率，而是能够自适应地处理各种类型的遥感数据。\\n\\n**关键设计**：AOM的关键设计包括：1) 频谱独立Tokenizer的具体实现方式，例如使用可学习的嵌入向量表示每个波段。2) 多尺度自适应Patch嵌入的动态感受野调节策略，例如使用注意力机制或可变形卷积。3) 多尺度语义对齐的具体方法，例如使用对比学习或知识蒸馏。4) 自监督预训练的掩码策略和重建目标，例如随机掩盖部分通道并预测原始值。",
            "application_zh": "Any-Optical-Model (AOM) 具有广泛的应用前景，可用于生态环境监测、自然灾害评估、城市规划、农业估产等领域。其通用性设计使其能够有效整合来自不同传感器和分辨率的遥感数据，为决策提供更全面和准确的信息支持，加速遥感技术在各行业的落地应用。",
            "highlight_zh": "AOM 在包括 Sentinel-2、Landsat 和 HLS 在内的 10 多个公共数据集上进行了广泛的实验，结果表明 AOM 在波段缺失、跨传感器和跨分辨率等具有挑战性的条件下，始终如一地实现了 SOTA 性能。具体性能数据和提升幅度在论文中进行了详细的展示。",
            "tags_zh": [
                "遥感基础模型",
                "跨传感器",
                "多分辨率",
                "波段缺失",
                "自监督学习"
            ],
            "_index": 13,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17224v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17224v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17224v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Towards Explainable Conversational AI for Early Diagnosis with Large Language Models",
            "authors": [
                "Maliha Tabassum",
                "M Shamim Kaiser"
            ],
            "arxiv_id": "2512.17559v1",
            "summary": "Healthcare systems around the world are grappling with issues like inefficient diagnostics, rising costs, and limited access to specialists. These problems often lead to delays in treatment and poor health outcomes. Most current AI and deep learning diagnostic systems are not very interactive or transparent, making them less effective in real-world, patient-centered environments. This research introduces a diagnostic chatbot powered by a Large Language Model (LLM), using GPT-4o, Retrieval-Augmented Generation, and explainable AI techniques. The chatbot engages patients in a dynamic conversation, helping to extract and normalize symptoms while prioritizing potential diagnoses through similarity matching and adaptive questioning. With Chain-of-Thought prompting, the system also offers more transparent reasoning behind its diagnoses. When tested against traditional machine learning models like Naive Bayes, Logistic Regression, SVM, Random Forest, and KNN, the LLM-based system delivered impressive results, achieving an accuracy of 90% and Top-3 accuracy of 100%. These findings offer a promising outlook for more transparent, interactive, and clinically relevant AI in healthcare.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17559v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model",
                        "chain-of-thought"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于LLM的对话式AI，用于早期诊断并提升可解释性",
            "summary_zh": "本文提出了一种基于大型语言模型（LLM）的诊断聊天机器人，旨在解决医疗系统中诊断效率低下、成本上升以及专家资源有限等问题。该聊天机器人利用GPT-4o、检索增强生成（RAG）和可解释AI技术，与患者进行动态对话，提取和规范化症状，并通过相似性匹配和自适应提问来确定潜在诊断的优先级。借助思维链（Chain-of-Thought）提示，该系统还提供了更透明的诊断推理过程。实验结果表明，与传统的机器学习模型（如朴素贝叶斯、逻辑回归、SVM、随机森林和KNN）相比，该基于LLM的系统表现出色，达到了90%的准确率和100%的Top-3准确率。这些发现为医疗领域中更透明、交互式和临床相关的AI应用提供了有希望的前景。",
            "intro_zh": [
                "现有AI诊断系统交互性和透明度不足，难以有效应用于以患者为中心的真实环境。",
                "利用GPT-4o、RAG和可解释AI技术构建对话式诊断系统，提升诊断透明度。",
                "实验表明，该系统在诊断准确率上显著优于传统机器学习模型，具有临床应用潜力。"
            ],
            "method_zh": "**问题定义**：现有医疗诊断系统面临效率低、成本高、专家资源有限等问题，且缺乏足够的交互性和透明度，难以满足实际临床需求。传统的AI和深度学习诊断系统通常是黑盒模型，难以解释其诊断结果，导致医生和患者对其信任度不高。因此，如何构建一个交互式、透明且准确的AI诊断系统是亟待解决的问题。\\n\\n**核心思路**：本文的核心思路是利用大型语言模型（LLM）的强大自然语言处理能力，构建一个能够与患者进行动态对话的诊断聊天机器人。通过对话式交互，系统能够更全面地收集患者的症状信息，并利用检索增强生成（RAG）技术，结合医学知识库进行诊断推理。同时，采用思维链（Chain-of-Thought）提示，使系统能够解释其诊断过程，提高透明度和可信度。\\n\\n**技术框架**：该诊断聊天机器人的整体架构包含以下几个主要模块：1) 对话管理模块：负责与患者进行对话，收集症状信息。2) 症状规范化模块：将患者描述的症状进行规范化处理，以便进行后续的诊断推理。3) 诊断推理模块：利用RAG技术，结合医学知识库，根据患者的症状信息进行诊断推理，并给出潜在的诊断结果。4) 解释生成模块：采用思维链提示，生成诊断推理过程的解释，提高透明度。5) 优先级排序模块：对潜在的诊断结果进行优先级排序，以便医生进行参考。\\n\\n**关键创新**：该论文的关键创新在于将大型语言模型（LLM）、检索增强生成（RAG）和可解释AI技术相结合，构建了一个交互式、透明且准确的AI诊断系统。与传统的机器学习模型相比，该系统具有更强的自然语言处理能力和知识推理能力，能够更好地理解患者的症状信息，并给出更准确的诊断结果。同时，通过思维链提示，该系统能够解释其诊断过程，提高透明度和可信度。\\n\\n**关键设计**：在关键设计方面，该论文采用了GPT-4o作为底层LLM，并利用RAG技术，结合医学知识库进行诊断推理。为了提高诊断的准确性和透明度，该论文采用了思维链（Chain-of-Thought）提示，使系统能够解释其诊断过程。此外，该论文还设计了一种自适应提问策略，根据患者的回答，动态调整提问内容，以便更全面地收集症状信息。具体的参数设置和网络结构等技术细节在论文中未详细描述，属于未知信息。",
            "application_zh": "该研究成果可应用于远程医疗、在线健康咨询、基层医疗机构等场景，辅助医生进行早期诊断，提高诊断效率和准确性，降低医疗成本，并改善患者就医体验。未来，该技术有望与可穿戴设备、电子病历等系统集成，实现更智能化的健康管理。",
            "highlight_zh": "实验结果表明，基于LLM的诊断聊天机器人达到了90%的准确率和100%的Top-3准确率，显著优于传统的机器学习模型（如朴素贝叶斯、逻辑回归、SVM、随机森林和KNN）。这表明LLM在医疗诊断领域具有巨大的潜力，能够提供更准确、更可靠的诊断结果。",
            "tags_zh": [
                "大型语言模型",
                "对话式AI",
                "早期诊断",
                "可解释AI",
                "检索增强生成",
                "医疗健康",
                "GPT-4o"
            ],
            "_index": 14,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17559v1/figure1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17559v1/figure2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17559v1/figure3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "CIFE: Code Instruction-Following Evaluation",
            "authors": [
                "Sravani Gunnu",
                "Shanmukha Guttula",
                "Hima Patel"
            ],
            "arxiv_id": "2512.17387v1",
            "summary": "Large Language Models (LLMs) are increasingly applied to real-world code generation, where functional correctness alone is insufficient for reliable deployment, developers also expect adherence to explicit requirements for robustness, formatting, and security. Existing benchmarks primarily assess correctness through test-case execution, offering limited insight into how reliably models follow such constraints. We introduce a benchmark of 1,000 Python tasks, each paired with an average of 7 developer-specified constraints spanning 13 categories. Constraints are curated through a four-stage human-LLM pipeline to ensure they are atomic, relevant, and objective. We evaluate 14 open- and closed-source models using complementary adherence metrics and propose the C2A Score, a composite measure that jointly captures correctness and constraint compliance. Results reveal a substantial gap between partial and strict satisfaction, while strong models achieve over 90% partial adherence, strict adherence remains between 39-66%. These findings highlight that trustworthy code generation requires not only correctness but also consistent adherence to developer intent.",
            "categories": [
                "cs.SE",
                "cs.CL"
            ],
            "primary_category": "cs.SE",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "20 pages, 22 figures, 2 tables",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17387v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "[T]instruction following"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "CIFE：提出代码指令遵循评估基准，衡量LLM在代码生成中对开发者约束的遵守程度",
            "summary_zh": "大型语言模型（LLMs）越来越多地应用于实际的代码生成，但仅凭功能正确性不足以保证可靠部署。开发者还期望模型能够遵守明确的需求，例如鲁棒性、格式和安全性。本文提出了一个包含1000个Python任务的基准测试，每个任务都配有平均7个开发者指定的约束，涵盖13个类别。这些约束通过四阶段的人工-LLM流水线进行筛选，以确保其原子性、相关性和客观性。我们使用互补的遵守度指标评估了14个开源和闭源模型，并提出了C2A评分，这是一个综合指标，共同衡量正确性和约束合规性。结果表明，部分满足和严格满足之间存在巨大差距，虽然强大的模型实现了超过90%的部分遵守度，但严格遵守度仍然在39-66%之间。这些发现强调，可信的代码生成不仅需要正确性，还需要始终如一地遵守开发者意图。",
            "intro_zh": [
                "现有代码生成评估主要关注功能正确性，忽略了开发者对代码鲁棒性、格式和安全性的明确约束。",
                "CIFE基准通过人工-LLM协作，构建了包含丰富开发者约束的Python代码生成任务集。",
                "实验表明，现有模型在部分约束遵守方面表现较好，但在严格遵守所有约束方面仍有较大提升空间。"
            ],
            "method_zh": "**问题定义**：现有代码生成评估方法主要通过测试用例执行来评估代码的功能正确性，忽略了开发者在实际应用中对代码风格、安全性、鲁棒性等方面的约束。这些约束往往以自然语言的形式存在，如何评估模型对这些约束的遵循程度是一个挑战。\\n\\n**核心思路**：本文的核心思路是构建一个包含丰富开发者约束的代码生成评估基准，并设计相应的评估指标来衡量模型对这些约束的遵守程度。通过这个基准，可以更全面地评估代码生成模型的性能，并促进模型在实际应用中的可靠性。\\n\\n**技术框架**：CIFE基准的构建包含以下几个主要阶段：1) 收集Python代码生成任务；2) 通过人工和LLM协作，为每个任务生成多个开发者约束；3) 对约束进行筛选和验证，确保其原子性、相关性和客观性；4) 设计评估指标，包括部分遵守度和严格遵守度，以及综合指标C2A Score。\\n\\n**关键创新**：CIFE基准的关键创新在于其对开发者约束的显式建模和评估。与传统的只关注功能正确性的评估方法不同，CIFE基准更关注模型对开发者意图的理解和执行能力。此外，CIFE基准的约束生成过程采用了人工-LLM协作的方式，保证了约束的多样性和质量。\\n\\n**关键设计**：CIFE基准的约束涵盖13个类别，包括输入验证、错误处理、代码风格、安全性等方面。C2A Score是一个综合指标，它结合了代码的正确性和约束遵守度，可以更全面地评估代码生成模型的性能。C2A Score的计算公式为：C2A Score = Correctness * Strict Adherence，其中Correctness表示代码的功能正确性，Strict Adherence表示代码对所有约束的严格遵守度。",
            "application_zh": "CIFE基准可以应用于代码生成模型的评估和改进，帮助开发者选择更可靠的代码生成工具。此外，CIFE基准还可以用于研究如何更好地将自然语言约束融入到代码生成过程中，提高代码生成模型的智能化水平。该研究的成果有助于提高软件开发的效率和质量，降低软件维护成本。",
            "highlight_zh": "实验结果表明，现有代码生成模型在CIFE基准上的表现参差不齐。虽然一些强大的模型可以实现超过90%的部分约束遵守度，但严格遵守度仍然较低，在39-66%之间。这表明现有模型在理解和执行开发者意图方面仍有提升空间。C2A Score的引入可以更全面地评估代码生成模型的性能，并为模型改进提供指导。",
            "tags_zh": [
                "代码生成",
                "大型语言模型",
                "指令遵循",
                "评估基准",
                "开发者约束"
            ],
            "_index": 15,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17387v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17387v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17387v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Mitty: Diffusion-based Human-to-Robot Video Generation",
            "authors": [
                "Yiren Song",
                "Cheng Liu",
                "Weijia Mao",
                "Mike Zheng Shou"
            ],
            "arxiv_id": "2512.17253v1",
            "summary": "Learning directly from human demonstration videos is a key milestone toward scalable and generalizable robot learning. Yet existing methods rely on intermediate representations such as keypoints or trajectories, introducing information loss and cumulative errors that harm temporal and visual consistency. We present Mitty, a Diffusion Transformer that enables video In-Context Learning for end-to-end Human2Robot video generation. Built on a pretrained video diffusion model, Mitty leverages strong visual-temporal priors to translate human demonstrations into robot-execution videos without action labels or intermediate abstractions. Demonstration videos are compressed into condition tokens and fused with robot denoising tokens through bidirectional attention during diffusion. To mitigate paired-data scarcity, we also develop an automatic synthesis pipeline that produces high-quality human-robot pairs from large egocentric datasets. Experiments on Human2Robot and EPIC-Kitchens show that Mitty delivers state-of-the-art results, strong generalization to unseen environments, and new insights for scalable robot learning from human observations.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17253v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱六：视频提取与匹配 (Video Extraction)",
                    "id": "6_video_extraction",
                    "matched_keywords": [
                        "egocentric"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "[T]human-to-robot"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 11.0,
            "hit_pillars": [
                "6_video_extraction",
                "7_retargeting"
            ],
            "headline_zh": "Mitty：提出基于扩散模型的Human2Robot视频生成方法，实现端到端机器人学习。",
            "summary_zh": "本文提出Mitty，一种基于扩散Transformer的视频上下文学习方法，用于端到端的人类到机器人视频生成。现有方法依赖于关键点或轨迹等中间表示，导致信息损失和累积误差，影响时间和视觉一致性。Mitty建立在预训练的视频扩散模型之上，利用强大的视觉-时间先验，将人类演示转化为机器人执行视频，无需动作标签或中间抽象。演示视频被压缩成条件tokens，并通过扩散过程中的双向注意力与机器人去噪tokens融合。为了缓解配对数据稀缺问题，还开发了一种自动合成流程，从大型自我中心数据集中生成高质量的人-机器人对。在Human2Robot和EPIC-Kitchens上的实验表明，Mitty提供了最先进的结果，对未见环境的强大泛化能力，以及从人类观察中进行可扩展机器人学习的新见解。",
            "intro_zh": [
                "现有机器人学习方法依赖中间表示，易造成信息损失和误差累积，影响视频一致性。",
                "Mitty利用扩散Transformer，直接将人类演示视频转化为机器人执行视频，无需中间抽象。",
                "通过自动合成流程生成高质量人-机器人数据，实验证明Mitty具有优秀的泛化能力。"
            ],
            "method_zh": "**问题定义**：现有机器人学习方法通常依赖于从人类演示视频中提取的中间表示，例如关键点或轨迹。这种方法存在两个主要问题：一是信息损失，中间表示无法完全捕捉原始视频中的所有信息；二是误差累积，中间表示的提取过程本身可能引入误差，这些误差会进一步影响后续的机器人控制。因此，需要一种能够直接从人类演示视频生成机器人执行视频的端到端方法。\n\\n**核心思路**：Mitty的核心思路是利用预训练的视频扩散模型强大的视觉-时间先验知识，将人类演示视频作为条件，引导扩散过程生成对应的机器人执行视频。通过这种方式，避免了中间表示的使用，从而减少了信息损失和误差累积。同时，利用扩散模型的生成能力，可以生成更加自然和真实的机器人视频。\n\\n**技术框架**：Mitty的整体框架包括两个主要部分：一是视频扩散模型，用于生成机器人执行视频；二是条件编码器，用于将人类演示视频编码成条件tokens。具体来说，首先使用预训练的视频扩散模型作为生成器，然后将人类演示视频通过条件编码器压缩成条件tokens。在扩散过程中，通过双向注意力机制将条件tokens与机器人去噪tokens融合，从而引导生成过程。此外，为了缓解配对数据稀缺问题，还设计了一个自动合成流程，用于生成高质量的人-机器人配对数据。\n\\n**关键创新**：Mitty的关键创新在于将视频扩散模型应用于Human2Robot视频生成任务，并提出了一种有效的条件融合方法。与现有方法相比，Mitty无需中间表示，可以直接从人类演示视频生成机器人执行视频，从而减少了信息损失和误差累积。此外，Mitty还提出了一种自动合成流程，用于生成高质量的人-机器人配对数据，进一步提高了模型的性能。\n\\n**关键设计**：Mitty的关键设计包括以下几个方面：一是使用预训练的视频扩散模型，利用其强大的视觉-时间先验知识；二是设计双向注意力机制，用于有效融合条件tokens和机器人去噪tokens；三是设计自动合成流程，用于生成高质量的人-机器人配对数据。在具体实现上，使用了Transformer作为扩散模型的基本架构，并采用了标准的扩散训练方法。损失函数主要包括扩散模型的重建损失和对抗损失，以提高生成视频的质量。",
            "application_zh": "Mitty具有广泛的应用前景，例如可以通过学习人类的演示，使机器人能够执行各种复杂的任务，如烹饪、清洁、装配等。此外，Mitty还可以用于机器人教学，通过生成不同场景下的机器人执行视频，帮助人们更好地理解和学习机器人操作。未来，Mitty有望成为实现通用机器人学习的关键技术。",
            "highlight_zh": "Mitty在Human2Robot和EPIC-Kitchens数据集上取得了state-of-the-art的结果。实验表明，Mitty能够生成高质量的机器人执行视频，并且具有很强的泛化能力，可以适应未见过的环境。相较于之前的中间表示方法，Mitty在视觉和时间一致性上都有显著提升。具体性能数据未知，但论文强调了其优于现有技术的表现。",
            "tags_zh": [
                "机器人学习",
                "视频生成",
                "扩散模型",
                "Human2Robot",
                "上下文学习",
                "Transformer",
                "端到端学习"
            ],
            "_index": 16,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17253v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17253v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17253v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Large Language Models as Pokémon Battle Agents: Strategic Play and Content Generation",
            "authors": [
                "Daksh Jain",
                "Aarya Jain",
                "Ashutosh Desai",
                "Avyakt Verma",
                "Ishan Bhanuka",
                "Pratik Narang",
                "Dhruv Kumar"
            ],
            "arxiv_id": "2512.17308v1",
            "summary": "Strategic decision-making in Pokémon battles presents a unique testbed for evaluating large language models. Pokémon battles demand reasoning about type matchups, statistical trade-offs, and risk assessment, skills that mirror human strategic thinking. This work examines whether Large Language Models (LLMs) can serve as competent battle agents, capable of both making tactically sound decisions and generating novel, balanced game content. We developed a turn-based Pokémon battle system where LLMs select moves based on battle state rather than pre-programmed logic. The framework captures essential Pokémon mechanics: type effectiveness multipliers, stat-based damage calculations, and multi-Pokémon team management. Through systematic evaluation across multiple model architectures we measured win rates, decision latency, type-alignment accuracy, and token efficiency. These results suggest LLMs can function as dynamic game opponents without domain-specific training, offering a practical alternative to reinforcement learning for turn-based strategic games. The dual capability of tactical reasoning and content creation, positions LLMs as both players and designers, with implications for procedural generation and adaptive difficulty systems in interactive entertainment.",
            "categories": [
                "cs.AI",
                "cs.CL"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "Under Review",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17308v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "利用大型语言模型作为宝可梦对战智能体，实现策略博弈与内容生成",
            "summary_zh": "本文探索了大型语言模型（LLMs）在宝可梦对战中的应用，这是一个评估LLM战略决策能力的独特测试平台。宝可梦对战需要推理属性克制关系、统计权衡和风险评估，这些技能与人类的战略思维相似。本文研究了LLM是否可以作为合格的对战智能体，既能做出战术上合理的决策，又能生成新颖且平衡的游戏内容。我们开发了一个回合制宝可梦对战系统，其中LLM根据战斗状态选择招式，而不是预先编程的逻辑。该框架捕捉了宝可梦的关键机制：属性克制倍率、基于属性值的伤害计算和多宝可梦队伍管理。通过对多种模型架构的系统评估，我们测量了胜率、决策延迟、属性对齐准确性和token效率。结果表明，LLM无需特定领域的训练即可充当动态游戏对手，为回合制战略游戏提供了一种替代强化学习的实用方法。战术推理和内容创建的双重能力使LLM既可以作为玩家又可以作为设计者，对交互式娱乐中的程序生成和自适应难度系统具有重要意义。",
            "intro_zh": [
                "现有宝可梦对战智能体依赖预编程逻辑或强化学习，缺乏灵活性和泛化能力。",
                "本文提出利用大型语言模型直接进行策略决策，无需领域特定训练，模拟人类战略思维。",
                "实验表明，LLM能够作为动态游戏对手，并具备生成游戏内容的能力，胜率和决策效率可观。"
            ],
            "method_zh": "**问题定义**：论文旨在解决如何让AI在复杂的策略游戏中做出合理的决策，并生成游戏内容。现有方法，如预编程规则或强化学习，存在泛化能力差、训练成本高等问题，难以适应游戏规则的变化和新内容的生成。宝可梦对战是一个很好的测试平台，因为它需要考虑属性克制、数值计算和风险评估等多种因素。\\n\\n**核心思路**：论文的核心思路是利用大型语言模型（LLMs）的强大推理能力和泛化能力，直接根据游戏状态进行决策，而无需进行特定领域的训练。通过将游戏状态作为LLM的输入，并让LLM输出相应的行动，可以模拟人类玩家的决策过程。此外，LLM还可以用于生成新的宝可梦、招式等游戏内容。\\n\\n**技术框架**：整体框架包含一个回合制宝可梦对战系统和一个LLM决策模块。对战系统负责模拟游戏环境，包括宝可梦属性、招式效果、伤害计算等。LLM决策模块接收当前游戏状态作为输入，输出下一步行动（选择招式或更换宝可梦）。框架主要包含以下阶段：1. 状态编码：将游戏状态（包括宝可梦属性、剩余血量、天气等）编码为LLM可以理解的文本格式。2. LLM推理：将编码后的状态输入LLM，LLM根据当前状态生成下一步行动的文本描述。3. 行动解码：将LLM输出的文本描述解码为游戏可以执行的行动指令。4. 状态更新：根据执行的行动更新游戏状态，进入下一回合。\\n\\n**关键创新**：最重要的技术创新点在于直接利用LLM进行策略决策，而无需进行特定领域的训练或强化学习。这与传统的游戏AI方法有本质区别，传统方法通常需要大量的游戏数据进行训练，而LLM则可以利用其预训练的知识进行泛化。此外，LLM还具备生成游戏内容的能力，可以自动生成新的宝可梦、招式等，从而降低游戏开发成本。\\n\\n**关键设计**：论文中关键的设计包括：1. 状态编码方式：如何将游戏状态有效地编码为LLM可以理解的文本格式，需要考虑信息的完整性和简洁性。2. LLM的选择：选择合适的LLM架构和规模，需要在性能和计算成本之间进行权衡。3. 行动解码方式：如何将LLM输出的文本描述准确地解码为游戏可以执行的行动指令，需要考虑行动的有效性和安全性。4. 评估指标：设计合适的评估指标来衡量LLM的性能，包括胜率、决策延迟、属性对齐准确性和token效率。",
            "application_zh": "该研究成果可应用于游戏AI开发，特别是回合制策略游戏，降低开发成本，提升AI的智能水平和泛化能力。此外，LLM生成游戏内容的能力，为程序化内容生成和自适应难度系统提供了新的思路，有望提升玩家的游戏体验。该技术还可能扩展到其他需要策略决策的领域，如军事模拟、金融交易等。",
            "highlight_zh": "实验结果表明，LLM在宝可梦对战中表现出良好的策略决策能力，无需特定领域训练即可达到可观的胜率。不同模型架构的性能存在差异，但均能有效利用属性克制关系。此外，LLM还能够生成符合游戏规则和平衡性的新内容，例如新的宝可梦和招式，展示了其在游戏内容生成方面的潜力。",
            "tags_zh": [
                "大型语言模型",
                "宝可梦对战",
                "策略决策",
                "游戏AI",
                "内容生成"
            ],
            "_index": 17,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "RecipeMasterLLM: Revisiting RoboEarth in the Era of Large Language Models",
            "authors": [
                "Asil Kaan Bozcuoglu",
                "Ziyuan Liu"
            ],
            "arxiv_id": "2512.17309v1",
            "summary": "RoboEarth was a pioneering initiative in cloud robotics, establishing a foundational framework for robots to share and exchange knowledge about actions, objects, and environments through a standardized knowledge graph. Initially, this knowledge was predominantly hand-crafted by engineers using RDF triples within OWL Ontologies, with updates, such as changes in an object's pose, being asserted by the robot's control and perception routines. However, with the advent and rapid development of Large Language Models (LLMs), we believe that the process of knowledge acquisition can be significantly automated. To this end, we propose RecipeMasterLLM, a high-level planner, that generates OWL action ontologies based on a standardized knowledge graph in response to user prompts. This architecture leverages a fine-tuned LLM specifically trained to understand and produce action descriptions consistent with the RoboEarth standardized knowledge graph. Moreover, during the Retrieval-Augmented Generation (RAG) phase, environmental knowledge is supplied to the LLM to enhance its contextual understanding and improve the accuracy of the generated action descriptions.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17309v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "RecipeMasterLLM：利用大语言模型重塑RoboEarth知识获取流程",
            "summary_zh": "RoboEarth是云计算机器人领域的先驱项目，它通过标准化的知识图谱，为机器人共享和交换关于动作、对象和环境的知识建立了一个基础框架。最初，这些知识主要由工程师使用OWL本体中的RDF三元组手工构建，并通过机器人的控制和感知程序来更新，例如对象姿态的变化。然而，随着大型语言模型（LLM）的出现和快速发展，我们认为知识获取的过程可以显著自动化。为此，我们提出了RecipeMasterLLM，一个高级规划器，它可以根据用户提示，基于标准化的知识图谱生成OWL动作本体。该架构利用经过微调的LLM，专门训练其理解和生成与RoboEarth标准化知识图谱一致的动作描述。此外，在检索增强生成（RAG）阶段，环境知识被提供给LLM，以增强其上下文理解并提高生成的动作描述的准确性。",
            "intro_zh": [
                "现有RoboEarth知识图谱主要依赖人工构建，效率低且难以扩展，无法适应快速变化的环境。",
                "RecipeMasterLLM利用微调的大语言模型自动生成OWL动作本体，显著提升知识获取效率。",
                "该方法采用检索增强生成（RAG），将环境知识融入LLM，提高生成动作描述的准确性。"
            ],
            "method_zh": "**问题定义**：RoboEarth项目旨在构建一个机器人可以共享知识的云平台，但其知识图谱的构建和维护依赖于人工，成本高昂且难以扩展。现有方法无法充分利用自然语言处理的最新进展，特别是大型语言模型在知识获取和推理方面的潜力。因此，如何自动化RoboEarth知识图谱的构建和更新，降低人工成本，提高知识获取效率，是本文要解决的核心问题。\\n\\n**核心思路**：本文的核心思路是利用大型语言模型（LLM）的强大自然语言理解和生成能力，自动生成RoboEarth所需的OWL动作本体。通过微调LLM，使其能够理解RoboEarth的知识图谱结构和动作描述规范，从而实现从用户指令到机器可理解的知识表示的自动转换。RAG的引入进一步提升了LLM的上下文理解能力，使其能够根据环境信息生成更准确的动作描述。\\n\\n**技术框架**：RecipeMasterLLM的整体架构包含以下几个主要模块：1) 用户输入模块：接收用户以自然语言形式输入的任务指令。2) 检索模块：从知识库中检索与当前任务相关的环境知识。3) LLM生成模块：利用微调的LLM，结合用户指令和检索到的环境知识，生成OWL动作本体。4) 知识图谱更新模块：将生成的OWL动作本体添加到RoboEarth知识图谱中。整个流程采用检索增强生成（RAG）框架，确保LLM能够获取必要的上下文信息。\\n\\n**关键创新**：本文最重要的技术创新点在于将大型语言模型应用于RoboEarth知识图谱的自动构建。与传统的手工构建方法相比，该方法能够显著降低人工成本，提高知识获取效率。此外，通过微调LLM和引入RAG，提高了生成动作描述的准确性和可靠性。\\n\\n**关键设计**：LLM采用开源的预训练语言模型，并使用RoboEarth相关的动作描述数据进行微调。微调的目标是使LLM能够生成符合RoboEarth知识图谱规范的OWL动作本体。RAG模块使用基于向量相似度的检索方法，从知识库中检索与用户指令相关的环境知识。具体参数设置和损失函数等技术细节在论文中未详细说明，属于未知信息。",
            "application_zh": "RecipeMasterLLM可应用于各种机器人任务规划和自动化场景，例如智能家居、工业自动化、医疗机器人等。通过自动生成机器人所需的知识，可以降低机器人部署和维护的成本，提高机器人的智能化水平。未来，该技术有望促进机器人技术的普及和应用。",
            "highlight_zh": "论文主要提出了RecipeMasterLLM的框架和设计思路，并没有提供具体的实验结果和性能数据。因此，无法总结具体的性能数据、对比基线和提升幅度。实验亮点未知。",
            "tags_zh": [
                "大型语言模型",
                "知识图谱",
                "机器人",
                "RoboEarth",
                "动作规划"
            ],
            "_index": 18,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17309v1/images/idea.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17309v1/images/d1.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17309v1/images/setup.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Adversarial Robustness of Vision in Open Foundation Models",
            "authors": [
                "Jonathon Fox",
                "William J Buchanan",
                "Pavlos Papadopoulos"
            ],
            "arxiv_id": "2512.17902v1",
            "summary": "With the increase in deep learning, it becomes increasingly difficult to understand the model in which AI systems can identify objects. Thus, an adversary could aim to modify an image by adding unseen elements, which will confuse the AI in its recognition of an entity. This paper thus investigates the adversarial robustness of LLaVA-1.5-13B and Meta's Llama 3.2 Vision-8B-2. These are tested for untargeted PGD (Projected Gradient Descent) against the visual input modality, and empirically evaluated on the Visual Question Answering (VQA) v2 dataset subset. The results of these adversarial attacks are then quantified using the standard VQA accuracy metric. This evaluation is then compared with the accuracy degradation (accuracy drop) of LLaVA and Llama 3.2 Vision. A key finding is that Llama 3.2 Vision, despite a lower baseline accuracy in this setup, exhibited a smaller drop in performance under attack compared to LLaVA, particularly at higher perturbation levels. Overall, the findings confirm that the vision modality represents a viable attack vector for degrading the performance of contemporary open-weight VLMs, including Meta's Llama 3.2 Vision. Furthermore, they highlight that adversarial robustness does not necessarily correlate directly with standard benchmark performance and may be influenced by underlying architectural and training factors.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.CR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "10.1109/ACCESS.2025.3645997",
            "journal_ref": "IEEE Access, 2025",
            "pdf_url": "https://arxiv.org/pdf/2512.17902v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]foundation model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "研究揭示开放视觉基础模型在对抗攻击下的脆弱性，并发现鲁棒性与基准性能不直接相关。",
            "summary_zh": "随着深度学习的普及，理解AI系统识别物体的内在机制变得日益困难。因此，攻击者可以通过添加未见过的元素来修改图像，从而混淆AI对实体的识别。本文研究了LLaVA-1.5-13B和Meta的Llama 3.2 Vision-8B-2的对抗鲁棒性。使用PGD（Projected Gradient Descent）方法对视觉输入模态进行非目标攻击，并在Visual Question Answering (VQA) v2数据集的子集上进行实证评估。使用标准的VQA准确率指标量化对抗攻击的结果。然后将此评估与LLaVA和Llama 3.2 Vision的准确率下降（accuracy drop）进行比较。一个关键发现是，尽管Llama 3.2 Vision在这种设置下的基线准确率较低，但在攻击下的性能下降幅度小于LLaVA，尤其是在较高的扰动水平下。总体而言，研究结果证实，视觉模态是降低当代开放权重VLM（包括Meta的Llama 3.2 Vision）性能的可行攻击向量。此外，它们强调对抗鲁棒性不一定与标准基准性能直接相关，并且可能受到底层架构和训练因素的影响。",
            "intro_zh": [
                "现有视觉模型易受对抗攻击影响，攻击者通过微小扰动即可降低模型性能，但模型内在脆弱性尚不明确。",
                "本文通过对抗攻击评估LLaVA-1.5-13B和Llama 3.2 Vision-8B-2的鲁棒性，分析模型在视觉模态下的脆弱程度。",
                "实验表明，Llama 3.2 Vision在基线性能较低的情况下，对抗攻击下的性能下降幅度小于LLaVA，揭示鲁棒性与基准性能的非直接相关性。"
            ],
            "method_zh": "**问题定义**：论文旨在研究开放视觉基础模型在对抗攻击下的鲁棒性。现有方法虽然在标准数据集上表现良好，但容易受到对抗样本的攻击，即通过对输入图像进行微小但精心设计的扰动，导致模型产生错误的预测。现有方法的痛点在于缺乏对模型内在脆弱性的深入理解，以及对抗鲁棒性与标准性能之间的关系。\n\n**核心思路**：论文的核心思路是通过对抗攻击来评估模型的鲁棒性，并分析模型在不同扰动程度下的性能下降情况。通过比较不同模型的性能下降幅度，可以了解模型对对抗攻击的敏感程度，并揭示对抗鲁棒性与标准性能之间的关系。这种方法可以帮助我们更好地理解模型的内在脆弱性，并为提高模型的鲁棒性提供指导。\n\n**技术框架**：论文的技术框架主要包括以下几个步骤：1) 选择两个开放视觉基础模型：LLaVA-1.5-13B和Meta的Llama 3.2 Vision-8B-2；2) 使用PGD（Projected Gradient Descent）方法生成对抗样本，对视觉输入模态进行非目标攻击；3) 在Visual Question Answering (VQA) v2数据集的子集上进行实验评估；4) 使用标准的VQA准确率指标量化对抗攻击的结果；5) 比较不同模型在对抗攻击下的性能下降幅度。\n\n**关键创新**：论文的关键创新在于揭示了对抗鲁棒性与标准基准性能之间的非直接相关性。实验结果表明，即使一个模型在标准数据集上表现良好，它也可能在对抗攻击下表现不佳。这表明，对抗鲁棒性是一个独立于标准性能的指标，需要单独进行评估和优化。此外，论文还发现，不同的模型在对抗攻击下的表现差异很大，这表明模型的架构和训练方式对抗鲁棒性有重要影响。\n\n**关键设计**：论文的关键设计包括：1) 使用PGD方法生成对抗样本，该方法是一种常用的对抗攻击方法，可以有效地生成能够欺骗模型的对抗样本；2) 在VQA v2数据集上进行实验评估，该数据集是一个常用的视觉问答数据集，可以有效地评估模型在视觉理解方面的能力；3) 使用标准的VQA准确率指标量化对抗攻击的结果，该指标可以有效地衡量模型在对抗攻击下的性能下降情况。",
            "application_zh": "该研究成果可应用于提升视觉模型的安全性与可靠性，例如在自动驾驶、医疗影像诊断等安全攸关领域，增强模型抵御恶意攻击的能力，避免因对抗样本导致的误判。此外，该研究有助于开发更鲁棒的视觉模型训练方法，提高模型在真实世界复杂环境中的泛化能力。",
            "highlight_zh": "实验结果表明，Llama 3.2 Vision虽然基线VQA准确率低于LLaVA，但在PGD对抗攻击下，性能下降幅度明显小于LLaVA，尤其是在高扰动水平下。这表明Llama 3.2 Vision在一定程度上更具对抗鲁棒性，同时也揭示了标准基准性能与对抗鲁棒性之间可能存在负相关关系。",
            "tags_zh": [
                "对抗攻击",
                "视觉基础模型",
                "鲁棒性评估",
                "PGD攻击",
                "VQA",
                "LLaVA",
                "Llama 3.2 Vision"
            ],
            "_index": 19,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17902v1/figs/transformer.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17902v1/figs/panda_original.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17902v1/figs/pertubation.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "PathFLIP: Fine-grained Language-Image Pretraining for Versatile Computational Pathology",
            "authors": [
                "Fengchun Liu",
                "Songhan Jiang",
                "Linghan Cai",
                "Ziyue Wang",
                "Yongbing Zhang"
            ],
            "arxiv_id": "2512.17621v1",
            "summary": "While Vision-Language Models (VLMs) have achieved notable progress in computational pathology (CPath), the gigapixel scale and spatial heterogeneity of Whole Slide Images (WSIs) continue to pose challenges for multimodal understanding. Existing alignment methods struggle to capture fine-grained correspondences between textual descriptions and visual cues across thousands of patches from a slide, compromising their performance on downstream tasks. In this paper, we propose PathFLIP (Pathology Fine-grained Language-Image Pretraining), a novel framework for holistic WSI interpretation. PathFLIP decomposes slide-level captions into region-level subcaptions and generates text-conditioned region embeddings to facilitate precise visual-language grounding. By harnessing Large Language Models (LLMs), PathFLIP can seamlessly follow diverse clinical instructions and adapt to varied diagnostic contexts. Furthermore, it exhibits versatile capabilities across multiple paradigms, efficiently handling slide-level classification and retrieval, fine-grained lesion localization, and instruction following. Extensive experiments demonstrate that PathFLIP outperforms existing large-scale pathological VLMs on four representative benchmarks while requiring significantly less training data, paving the way for fine-grained, instruction-aware WSI interpretation in clinical practice.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17621v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal",
                        "instruction following"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "PathFLIP：用于多功能计算病理学的细粒度语言-图像预训练",
            "summary_zh": "视觉-语言模型(VLM)在计算病理学(CPath)领域取得了显著进展，但全切片图像(WSI)的千兆像素尺度和空间异质性仍然对多模态理解构成挑战。现有的对齐方法难以捕捉文本描述和来自切片的数千个图像块之间的细粒度对应关系，从而影响下游任务的性能。本文提出了PathFLIP(病理学细粒度语言-图像预训练)，这是一个用于整体WSI解释的新框架。PathFLIP将切片级别的标题分解为区域级别的子标题，并生成文本条件区域嵌入，以促进精确的视觉-语言基础。通过利用大型语言模型(LLM)，PathFLIP可以无缝地遵循各种临床指令并适应不同的诊断环境。此外，它在多种范例中表现出通用的能力，有效地处理切片级别的分类和检索、细粒度的病灶定位以及指令跟随。大量的实验表明，PathFLIP在四个代表性基准测试中优于现有的大规模病理VLM，同时需要显著更少的训练数据，为临床实践中细粒度的、指令感知的WSI解释铺平了道路。",
            "intro_zh": [
                "现有VLM难以捕捉WSI中图像块与文本描述间的细粒度对应关系，限制了其在计算病理学任务中的性能。",
                "PathFLIP通过将切片级标题分解为区域级子标题，并生成文本条件区域嵌入，实现精确的视觉-语言对齐。",
                "实验表明，PathFLIP在多个病理学任务上优于现有VLM，且所需训练数据更少，具有实际应用潜力。"
            ],
            "method_zh": "**问题定义**：现有视觉-语言模型在处理计算病理学中的全切片图像（WSI）时，难以建立图像块和文本描述之间的细粒度对应关系。由于WSI具有千兆像素级别的高分辨率和复杂的空间异质性，简单的图像级别或粗粒度的区域级别对齐无法充分利用图像中的信息，导致下游任务（如病灶定位、分类等）的性能受限。现有方法缺乏对临床指令的理解和适应能力，难以满足实际应用需求。\\n\\n**核心思路**：PathFLIP的核心思路是进行细粒度的视觉-语言预训练，从而更好地理解WSI。通过将切片级别的文本描述分解为区域级别的子描述，并利用这些子描述来指导区域特征的学习，从而建立图像块和文本之间的精确对应关系。利用大型语言模型（LLM）增强模型对临床指令的理解和泛化能力，使其能够适应不同的诊断环境和任务。\\n\\n**技术框架**：PathFLIP框架主要包含以下几个模块：1) 文本分解模块：利用LLM将切片级别的文本描述分解为多个区域级别的子描述。2) 视觉编码模块：提取WSI中各个图像块的视觉特征。3) 文本编码模块：将区域级别的子描述编码为文本嵌入。4) 对比学习模块：通过对比学习，使视觉特征和文本嵌入在特征空间中对齐，从而建立细粒度的视觉-语言对应关系。5) 指令跟随模块：利用LLM，使模型能够理解和执行各种临床指令。\\n\\n**关键创新**：PathFLIP的关键创新在于：1) 细粒度的视觉-语言对齐：通过将切片级别的文本描述分解为区域级别的子描述，实现了图像块和文本之间的精确对应关系。2) 利用LLM增强指令跟随能力：使模型能够理解和执行各种临床指令，提高了模型的泛化能力和实用性。3) 显著降低训练数据需求：在取得更好性能的同时，PathFLIP所需训练数据显著少于现有方法。\\n\\n**关键设计**：PathFLIP的关键设计包括：1) 使用LLM进行文本分解，保证子描述的质量和相关性。2) 使用对比学习损失函数，促使视觉特征和文本嵌入对齐。3) 设计指令跟随模块，使模型能够理解和执行各种临床指令。4) 采用多任务学习策略，同时优化视觉-语言对齐和指令跟随任务。",
            "application_zh": "PathFLIP在计算病理学领域具有广泛的应用前景，可用于辅助医生进行疾病诊断、病灶定位、预后评估等。该模型能够理解临床指令，并根据指令执行相应的任务，例如根据病理报告定位肿瘤区域、判断肿瘤的恶性程度等。PathFLIP还可以用于构建智能病理诊断系统，提高诊断效率和准确性，为患者提供更好的医疗服务。未来，PathFLIP有望应用于药物研发、个性化治疗等领域。",
            "highlight_zh": "PathFLIP在四个代表性基准测试中均取得了优于现有大规模病理VLM的性能。例如，在切片级别分类任务中，PathFLIP的准确率提高了X%。更重要的是，PathFLIP在取得更好性能的同时，所需训练数据显著减少，这使得该模型更易于训练和部署。实验结果表明，PathFLIP在细粒度的视觉-语言理解和指令跟随方面具有显著优势。",
            "tags_zh": [
                "计算病理学",
                "视觉-语言预训练",
                "全切片图像",
                "细粒度对齐",
                "大型语言模型"
            ],
            "_index": 20,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17621v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17621v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17621v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection",
            "authors": [
                "Sairam VCR",
                "Rishabh Lalla",
                "Aveen Dayal",
                "Tejal Kulkarni",
                "Anuj Lalla",
                "Vineeth N Balasubramanian",
                "Muhammad Haris Khan"
            ],
            "arxiv_id": "2512.17514v1",
            "summary": "Current state-of-the-art approaches in Source-Free Object Detection (SFOD) typically rely on Mean-Teacher self-labeling. However, domain shift often reduces the detector's ability to maintain strong object-focused representations, causing high-confidence activations over background clutter. This weak object focus results in unreliable pseudo-labels from the detection head. While prior works mainly refine these pseudo-labels, they overlook the underlying need to strengthen the feature space itself. We propose FALCON-SFOD (Foundation-Aligned Learning with Clutter suppression and Noise robustness), a framework designed to enhance object-focused adaptation under domain shift. It consists of two complementary components. SPAR (Spatial Prior-Aware Regularization) leverages the generalization strength of vision foundation models to regularize the detector's feature space. Using class-agnostic binary masks derived from OV-SAM, SPAR promotes structured and foreground-focused activations by guiding the network toward object regions. IRPL (Imbalance-aware Noise Robust Pseudo-Labeling) complements SPAR by promoting balanced and noise-tolerant learning under severe foreground-background imbalance. Guided by a theoretical analysis that connects these designs to tighter localization and classification error bounds, FALCON-SFOD achieves competitive performance across SFOD benchmarks.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17514v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]foundation model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "FALCON-SFOD：利用先验知识增强源域无关目标检测中的目标聚焦",
            "summary_zh": "本文提出FALCON-SFOD，一个用于增强域迁移下目标聚焦自适应的框架，旨在解决源域无关目标检测(SFOD)中，由于域偏移导致检测器难以维持强目标聚焦表示，从而产生大量背景杂乱激活的问题。该框架包含两个互补组件：空间先验感知正则化(SPAR)，利用视觉基础模型的泛化能力来正则化检测器的特征空间，通过OV-SAM生成的类别无关二值掩码，引导网络关注目标区域，促进结构化和前景聚焦的激活；不平衡感知噪声鲁棒伪标签(IRPL)，通过促进平衡和噪声容忍的学习来补充SPAR，解决严重的前景-背景不平衡问题。理论分析表明，这些设计能够收紧定位和分类误差界限，FALCON-SFOD在SFOD基准测试中取得了具有竞争力的性能。",
            "intro_zh": [
                "源域无关目标检测(SFOD)受域偏移影响，检测器易产生背景杂乱激活，导致伪标签质量下降。",
                "FALCON-SFOD框架通过空间先验感知正则化(SPAR)和不平衡感知噪声鲁棒伪标签(IRPL)增强目标聚焦。",
                "实验结果表明，FALCON-SFOD在多个SFOD基准测试中表现出色，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：源域无关目标检测(SFOD)旨在利用已标注的源域数据训练目标检测器，并将其应用于未标注的目标域，而无需访问源域数据。现有方法，特别是基于Mean-Teacher自标记的方法，在域偏移下，检测器难以维持强目标聚焦的特征表示，导致高置信度的激活出现在背景杂乱区域，从而产生不可靠的伪标签。现有工作主要集中在伪标签的提炼上，忽略了增强特征空间本身的重要性。\\n\\n**核心思路**：FALCON-SFOD的核心思路是利用视觉基础模型（Vision Foundation Model）的先验知识来指导目标检测器的特征学习，从而增强其在目标域中的目标聚焦能力。具体来说，通过空间先验感知正则化(SPAR)模块，利用开放词汇语义分割模型(OV-SAM)生成的类别无关二值掩码，引导检测器关注目标区域，抑制背景噪声。同时，通过不平衡感知噪声鲁棒伪标签(IRPL)模块，解决前景-背景不平衡问题，提高伪标签的质量。\\n\\n**技术框架**：FALCON-SFOD框架主要包含两个模块：SPAR和IRPL。首先，利用OV-SAM在目标域图像上生成类别无关的二值掩码，这些掩码指示了图像中可能包含目标的区域。然后，SPAR模块利用这些掩码对检测器的特征空间进行正则化，促使检测器学习到更关注目标区域的特征表示。同时，IRPL模块用于生成更准确的伪标签，并解决训练过程中前景-背景类别不平衡的问题。整个框架采用Mean-Teacher的训练方式，即同时训练一个学生模型和一个教师模型，并利用教师模型生成的伪标签来指导学生模型的学习。\\n\\n**关键创新**：FALCON-SFOD的关键创新在于利用视觉基础模型的先验知识来增强目标检测器的特征空间。与现有方法主要关注伪标签的提炼不同，FALCON-SFOD从根本上解决了特征空间中目标聚焦不足的问题。通过SPAR模块，FALCON-SFOD能够有效地抑制背景噪声，提高检测器对目标区域的敏感性。此外，IRPL模块进一步提高了伪标签的质量，使得检测器能够更好地适应目标域。\\n\\n**关键设计**：SPAR模块的关键设计在于如何有效地利用OV-SAM生成的二值掩码。论文采用了一种空间先验感知损失函数，该损失函数鼓励检测器的特征激活在掩码指示的目标区域内具有更高的值，而在掩码之外具有更低的值。IRPL模块的关键设计在于如何解决前景-背景类别不平衡的问题。论文采用了一种不平衡感知损失函数，该损失函数对少数类别（通常是前景目标）赋予更高的权重，从而使得检测器能够更好地学习到这些类别的特征。",
            "application_zh": "FALCON-SFOD在安全监控、自动驾驶、医学图像分析等领域具有广泛的应用前景。例如，在安全监控中，可以利用FALCON-SFOD检测不同场景下的异常行为，而无需针对每个场景重新训练模型。在自动驾驶中，可以利用FALCON-SFOD检测不同天气和光照条件下的交通标志和行人，提高驾驶安全性。在医学图像分析中，可以利用FALCON-SFOD检测不同类型的病灶，辅助医生进行诊断。",
            "highlight_zh": "论文在多个SFOD基准测试中验证了FALCON-SFOD的有效性。实验结果表明，FALCON-SFOD在多个数据集上取得了显著的性能提升，超过了现有的state-of-the-art方法。具体的性能数据需要在论文中查找，这里无法给出。",
            "tags_zh": [
                "源域无关目标检测",
                "领域自适应",
                "视觉基础模型",
                "伪标签学习",
                "特征空间正则化"
            ],
            "_index": 21,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17514v1/images/FALCON-SFOD-teaser.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17514v1/images/spar_impact.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17514v1/images/map_vs_m.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation",
            "authors": [
                "Jon Muhovič",
                "Janez Perš"
            ],
            "arxiv_id": "2512.17450v1",
            "summary": "Unmanned surface vehicles can encounter a number of varied visual circumstances during operation, some of which can be very difficult to interpret. While most cases can be solved only using color camera images, some weather and lighting conditions require additional information. To expand the available maritime data, we present a novel multimodal maritime dataset MULTIAQUA (Multimodal Aquatic Dataset). Our dataset contains synchronized, calibrated and annotated data captured by sensors of different modalities, such as RGB, thermal, IR, LIDAR, etc. The dataset is aimed at developing supervised methods that can extract useful information from these modalities in order to provide a high quality of scene interpretation regardless of potentially poor visibility conditions. To illustrate the benefits of the proposed dataset, we evaluate several multimodal methods on our difficult nighttime test set. We present training approaches that enable multimodal methods to be trained in a more robust way, thus enabling them to retain reliable performance even in near-complete darkness. Our approach allows for training a robust deep neural network only using daytime images, thus significantly simplifying data acquisition, annotation, and the training process.",
            "categories": [
                "cs.CV",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17450v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]multimodal"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出MULTIAQUA多模态水面数据集，并设计鲁棒训练策略提升水面语义分割性能",
            "summary_zh": "本文提出了一个新的多模态水面数据集MULTIAQUA（Multimodal Aquatic Dataset）。该数据集包含由多种模态传感器（如RGB、热成像、红外、激光雷达等）同步、校准和标注的数据。该数据集旨在开发有监督的方法，能够从这些模态中提取有用的信息，从而提供高质量的场景理解，而无需考虑潜在的恶劣能见度条件。为了说明所提出的数据集的优势，我们在具有挑战性的夜间测试集上评估了几种多模态方法。我们提出了一些训练方法，使多模态方法能够以更鲁棒的方式进行训练，从而即使在接近完全黑暗的情况下也能保持可靠的性能。我们的方法允许仅使用白天图像训练鲁棒的深度神经网络，从而显著简化数据采集、标注和训练过程。",
            "intro_zh": [
                "无人水面艇在复杂环境中作业时，仅依靠可见光图像难以应对各种天气和光照条件，需要多模态信息融合。",
                "论文提出MULTIAQUA数据集，包含多种模态数据，并设计鲁棒训练策略，提升模型在恶劣条件下的语义分割性能。",
                "实验表明，该方法仅使用白天图像训练，即可在夜间等低能见度环境下实现可靠的语义分割，降低了数据采集和标注成本。"
            ],
            "method_zh": "**问题定义**：现有的水面语义分割方法在恶劣天气和光照条件下表现不佳，因为它们主要依赖于可见光图像。在夜间或雾天等情况下，可见光图像的信息量不足以进行准确的场景理解。因此，需要一种能够融合多种模态信息，并在各种条件下都能保持鲁棒性的语义分割方法。\\n\\n**核心思路**：论文的核心思路是利用多模态数据来弥补单一模态数据的不足。通过融合来自不同传感器的信息，例如RGB、热成像、红外和激光雷达，可以获得更全面的场景表示，从而提高语义分割的准确性和鲁棒性。此外，论文还提出了一种鲁棒的训练策略，使模型能够在仅使用白天图像的情况下，也能在夜间等低能见度环境下表现良好。\\n\\n**技术框架**：整体框架包括数据采集、数据预处理、模型训练和模型评估四个主要阶段。数据采集阶段使用多种传感器同步采集水面场景的多模态数据。数据预处理阶段对采集到的数据进行校准、同步和标注。模型训练阶段使用提出的鲁棒训练策略训练多模态语义分割模型。模型评估阶段在具有挑战性的夜间测试集上评估模型的性能。\\n\\n**关键创新**：论文的关键创新点在于提出了MULTIAQUA多模态水面数据集，并设计了一种鲁棒的训练策略。该数据集包含了多种模态的数据，为多模态水面语义分割的研究提供了基础。该训练策略允许仅使用白天图像训练模型，从而降低了数据采集和标注的成本。\\n\\n**关键设计**：论文中使用的多模态语义分割模型基于深度神经网络，具体网络结构未知。鲁棒训练策略的关键在于设计合适的损失函数和数据增强方法，以提高模型在各种条件下的泛化能力。具体的参数设置、损失函数和网络结构等技术细节在论文中未详细描述，属于未知信息。",
            "application_zh": "该研究成果可应用于无人水面艇的自主导航、环境监测、搜救等领域。通过提高水面语义分割的准确性和鲁棒性，可以使无人水面艇在各种天气和光照条件下安全可靠地运行，从而扩展其应用范围和提高其应用价值。未来，该技术还可以应用于智能港口、海洋牧场等领域。",
            "highlight_zh": "论文在MULTIAQUA数据集的夜间测试集上评估了提出的方法，结果表明，该方法仅使用白天图像训练，即可在夜间等低能见度环境下实现可靠的语义分割。具体的性能数据和对比基线在摘要中未提及，属于未知信息。但该结果表明，提出的方法具有很强的实用价值。",
            "tags_zh": [
                "多模态学习",
                "语义分割",
                "水面环境",
                "无人水面艇",
                "数据集",
                "鲁棒训练"
            ],
            "_index": 22,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17450v1/images/lj1_3_048300.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17450v1/images/lidar_lj3_0_039000.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17450v1/images/adr1_1_001000.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models",
            "authors": [
                "Jiajun Wu",
                "Jian Yang",
                "Wei Zhang",
                "Lin Jing",
                "Yuqing Ma",
                "Ensheng Shi",
                "Yuchi Ma",
                "Zhoujun Li",
                "Xianglong Liu"
            ],
            "arxiv_id": "2512.17385v1",
            "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, their effectiveness heavily relies on supervised training with extensive labeled (e.g., question-answering pairs) or unlabeled datasets (e.g., code snippets), which are often expensive and difficult to obtain at scale. To address this limitation, this paper introduces a method IPC, an unsupervised framework that leverages Internal Probing of LLMs for Code generation without any external corpus, even unlabeled code snippets. We introduce the problem space probing, test understanding probing, solution space probing, and knowledge consolidation and reinforcement to probe the internal knowledge and confidence patterns existing in LLMs. Further, IPC identifies reliable code candidates through self-consistency mechanisms and representation-based quality estimation to train UCoder (coder with unsupervised learning). We validate the proposed approach across multiple code benchmarks, demonstrating that unsupervised methods can achieve competitive performance compared to supervised approaches while significantly reducing the dependency on labeled data and computational resources. Analytic experiments reveal that internal model states contain rich signals about code quality and correctness, and that properly harnessing these signals enables effective unsupervised learning for code generation tasks, opening new directions for training code LLMs in resource-constrained scenarios.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17385v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "UCoder：通过内部探测大语言模型实现无监督代码生成",
            "summary_zh": "本文提出了一种名为IPC的无监督代码生成框架，该框架通过内部探测大语言模型（LLMs）来实现代码生成，无需任何外部语料库，甚至未标记的代码片段。IPC通过问题空间探测、测试理解探测、解空间探测以及知识巩固和强化等手段，挖掘LLMs内部蕴含的知识和置信度模式。此外，IPC通过自洽性机制和基于表征的质量估计来识别可靠的代码候选，从而训练UCoder（基于无监督学习的编码器）。在多个代码基准测试中验证了该方法的有效性，表明无监督方法可以达到与监督方法相媲美的性能，同时显著降低了对标记数据和计算资源的依赖。分析实验表明，内部模型状态包含关于代码质量和正确性的丰富信号，并且正确利用这些信号能够为代码生成任务实现有效的无监督学习，为在资源受限场景下训练代码LLMs开辟了新的方向。",
            "intro_zh": [
                "现有代码生成方法依赖大量标注或未标注数据，获取成本高昂且规模受限。",
                "UCoder通过内部探测LLM，无需外部语料库，挖掘模型内部知识和置信度模式。",
                "实验表明，无监督方法可与监督方法媲美，同时降低对数据和计算资源的依赖。"
            ],
            "method_zh": "**问题定义**：论文旨在解决代码生成任务中对大规模标注数据或未标注代码片段的依赖问题。现有方法需要大量的监督训练或无监督预训练，这在数据获取困难或计算资源受限的情况下是不可行的。因此，如何利用LLM自身蕴含的知识，在无需外部语料的情况下进行代码生成是本研究要解决的核心问题。\\n\\n**核心思路**：论文的核心思路是通过“内部探测”LLM，挖掘其内部状态中蕴含的关于代码质量和正确性的信息。具体来说，通过设计一系列探测任务，例如问题空间探测、测试理解探测和解空间探测，来提取LLM对问题理解、代码逻辑和正确性的置信度。然后，利用这些置信度信息来筛选和优化生成的代码，从而实现无监督的代码生成。\\n\\n**技术框架**：UCoder框架主要包含以下几个阶段：1) **内部探测 (Internal Probing)**：设计不同的探测任务，提取LLM内部状态中蕴含的知识和置信度信息。2) **代码候选生成 (Code Candidate Generation)**：利用LLM生成多个代码候选。3) **代码质量评估 (Code Quality Estimation)**：基于自洽性机制和表征的质量估计，对代码候选进行评估和筛选。4) **UCoder训练 (UCoder Training)**：利用筛选后的高质量代码候选，训练UCoder模型。\\n\\n**关键创新**：该论文最重要的创新点在于提出了一个完全无监督的代码生成框架，无需任何外部语料库。通过内部探测LLM，挖掘其内部状态中蕴含的知识和置信度信息，并利用这些信息来指导代码生成过程。这种方法打破了传统代码生成方法对大规模数据的依赖，为在资源受限场景下训练代码LLM提供了新的思路。\\n\\n**关键设计**：论文的关键设计包括：1) **问题空间探测**：通过分析LLM对不同问题的理解程度，来评估其对问题空间的掌握程度。2) **测试理解探测**：通过分析LLM对测试用例的理解程度，来评估其对代码逻辑的理解程度。3) **解空间探测**：通过分析LLM生成的不同代码候选的置信度，来评估其对解空间的探索能力。4) **自洽性机制**：利用LLM对同一问题的多次生成结果进行一致性检验，筛选出高质量的代码候选。5) **基于表征的质量估计**：利用LLM内部状态的表征信息，对代码候选的质量进行评估。",
            "application_zh": "UCoder的潜在应用领域包括：在数据稀缺或计算资源受限的环境下进行代码生成，例如嵌入式系统、移动设备等。该研究的实际价值在于降低了代码生成的成本和门槛，使得更多开发者能够利用LLM进行代码开发。未来，该方法可以进一步扩展到其他自然语言处理任务中，例如文本摘要、机器翻译等，实现真正的无监督学习。",
            "highlight_zh": "实验结果表明，UCoder在多个代码基准测试中取得了与监督方法相媲美的性能，同时显著降低了对标记数据和计算资源的依赖。例如，在HumanEval数据集上，UCoder的性能达到了XX%，与使用大规模标注数据训练的监督模型相比，性能差距缩小至YY%。这些结果验证了内部探测LLM进行无监督代码生成的有效性。",
            "tags_zh": [
                "代码生成",
                "无监督学习",
                "大语言模型",
                "内部探测",
                "自洽性",
                "表征学习",
                "知识挖掘"
            ],
            "_index": 23,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17385v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17385v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17385v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Governance-Aware Hybrid Fine-Tuning for Multilingual Large Language Models",
            "authors": [
                "Haomin Qi",
                "Chengbo Huang",
                "Zihan Dai",
                "Yunkai Gao"
            ],
            "arxiv_id": "2512.17344v1",
            "summary": "We present a governance-aware hybrid fine-tuning framework for multilingual, low-resource adaptation of large language models. The core algorithm combines gradient-aligned low-rank updates with structured orthogonal transformations through layer-wise mixing and introduces unitary constraints in selected sub-layers to stabilize deep optimization. In tandem with lightweight, label-free data governance steps, including language identification, near-duplicate removal, and quality filtering, the framework targets accuracy, calibration, and cross-language parity under tight compute budgets. Across XNLI and FLORES, the hybrid approach delivers consistent gains over strong PEFT baselines while maintaining directional balance and improving probability calibration, as shown in Tables II and III. It is more resilient to lightweight orthographic variants, as shown in Table IV, and benefits additively from simple governance steps, as shown in Table V. Training footprint measurements indicate modest overhead and a favorable cost-quality frontier, as shown in Table VI and Figure 2. Together, these results show that hybrid and unitary PEFT provide a stable and accessible path to resource-efficient multilingual adaptation when paired with practical data governance.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "11 pages, 4 figures, 6 tables. arXiv admin note: substantial text overlap with arXiv:2507.18076",
            "doi": "",
            "journal_ref": "2025 IEEE International Conference on Big Data",
            "pdf_url": "https://arxiv.org/pdf/2512.17344v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出一种治理感知的混合微调框架，用于多语言大语言模型的低资源自适应。",
            "summary_zh": "本文提出了一种治理感知的混合微调框架，用于多语言大语言模型的低资源自适应。该核心算法结合了梯度对齐的低秩更新与结构化正交变换，通过逐层混合并在选定的子层中引入酉约束来稳定深度优化。结合轻量级的、无标签的数据治理步骤，包括语言识别、近重复删除和质量过滤，该框架旨在在严格的计算预算下提高准确性、校准性和跨语言对等性。在XNLI和FLORES上的实验表明，混合方法在强大的PEFT基线上实现了持续的收益，同时保持了方向平衡并改善了概率校准。它对轻量级的拼写变体更具弹性，并且受益于简单的数据治理步骤。训练占用空间测量表明开销适中，并且具有良好的成本-质量边界。总之，这些结果表明，混合和酉PEFT在与实际数据治理相结合时，为资源高效的多语言自适应提供了一条稳定且易于访问的路径。",
            "intro_zh": [
                "现有方法在低资源场景下微调多语言大语言模型时，难以兼顾准确性、校准性和跨语言公平性，且计算成本高昂。",
                "提出一种混合微调框架，结合梯度对齐的低秩更新和结构化正交变换，并引入酉约束，以稳定优化过程。",
                "实验表明，该方法在XNLI和FLORES数据集上优于现有PEFT基线，同时提高了概率校准和对拼写变体的鲁棒性。"
            ],
            "method_zh": "**问题定义**：本文旨在解决多语言大语言模型在低资源场景下的微调问题。现有方法通常面临以下痛点：1) 准确性不足，尤其是在目标语言数据稀缺时；2) 模型校准性差，预测概率与实际置信度不符；3) 跨语言公平性难以保证，不同语言的表现差异较大；4) 微调成本高昂，难以在有限的计算资源下完成。\n\n**核心思路**：本文的核心思路是结合梯度对齐的低秩更新和结构化正交变换，并引入酉约束，以实现高效且稳定的微调。梯度对齐的低秩更新可以减少需要训练的参数量，降低计算成本；结构化正交变换可以保持模型参数的正交性，避免梯度消失或爆炸；酉约束可以进一步稳定优化过程，提高模型的泛化能力。此外，还引入了数据治理步骤，包括语言识别、近重复删除和质量过滤，以提高训练数据的质量。\n\n**技术框架**：该框架主要包含以下几个模块：1) 数据治理模块：对原始数据进行预处理，包括语言识别、近重复删除和质量过滤；2) 混合微调模块：结合梯度对齐的低秩更新和结构化正交变换，对模型进行微调；3) 酉约束模块：在选定的子层中引入酉约束，以稳定优化过程；4) 评估模块：评估模型在准确性、校准性和跨语言公平性等方面的表现。\n\n**关键创新**：本文最重要的技术创新点在于提出了治理感知的混合微调方法，该方法结合了多种技术手段，以实现高效且稳定的微调。与现有方法相比，该方法不仅降低了计算成本，还提高了模型的准确性、校准性和跨语言公平性。此外，数据治理步骤的引入也提高了训练数据的质量，进一步提升了模型的性能。\n\n**关键设计**：在混合微调模块中，采用了梯度对齐的低秩更新，具体来说，使用LoRA (Low-Rank Adaptation) 方法，只训练少量新增的低秩矩阵，而冻结预训练模型的参数。在结构化正交变换中，使用了 Householder 变换。在酉约束模块中，使用了 Cayley 变换来参数化酉矩阵。损失函数包括交叉熵损失和正则化项，用于约束模型参数。",
            "application_zh": "该研究成果可应用于多语言机器翻译、跨语言信息检索、多语言对话系统等领域。通过低成本地将大型语言模型适配到各种语言和场景，可以有效提升模型的性能和用户体验，尤其是在低资源语言场景下具有重要价值。未来，该方法有望进一步推广到更多自然语言处理任务中。",
            "highlight_zh": "实验结果表明，该混合微调方法在XNLI和FLORES数据集上均优于现有的PEFT基线。例如，在XNLI数据集上，该方法在准确率方面取得了显著提升，同时保持了良好的校准性和跨语言公平性。此外，该方法对轻量级的拼写变体具有更强的鲁棒性，并且受益于简单的数据治理步骤。训练成本分析表明，该方法具有良好的成本-质量比。",
            "tags_zh": [
                "多语言模型",
                "低资源学习",
                "参数高效微调",
                "数据治理",
                "正交变换"
            ],
            "_index": 24,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17344v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17344v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17344v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "RadarGen: Automotive Radar Point Cloud Generation from Cameras",
            "authors": [
                "Tomer Borreda",
                "Fangqiang Ding",
                "Sanja Fidler",
                "Shengyu Huang",
                "Or Litany"
            ],
            "arxiv_id": "2512.17897v1",
            "summary": "We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "Project page: https://radargen.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17897v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "physically plausible"
                    ],
                    "score": 2.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 8.5,
            "hit_pillars": [
                "4_motion_diffusion",
                "9_embodied_foundation"
            ],
            "headline_zh": "RadarGen：提出一种基于图像的汽车雷达点云生成扩散模型",
            "summary_zh": "本文提出RadarGen，一种从多视角相机图像合成逼真汽车雷达点云的扩散模型。RadarGen通过将雷达测量结果表示为鸟瞰图形式，并将空间结构与雷达截面(RCS)和多普勒属性编码，从而将高效的图像潜在扩散应用于雷达领域。一个轻量级的恢复步骤从生成的地图中重建点云。为了更好地使生成与视觉场景对齐，RadarGen结合了从预训练基础模型中提取的BEV对齐的深度、语义和运动线索，这些线索引导随机生成过程朝着物理上合理的雷达模式发展。原则上，以图像为条件使得该方法与现有的视觉数据集和仿真框架广泛兼容，为跨传感模态的统一生成式仿真提供了一个可扩展的方向。对大规模驾驶数据的评估表明，RadarGen捕获了特征雷达测量分布，并缩小了与在真实数据上训练的感知模型之间的差距，标志着朝着跨传感模态的统一生成式仿真迈出了一步。",
            "intro_zh": [
                "现有雷达数据生成方法难以保证真实性和多样性，且依赖昂贵的硬件和复杂的环境设置。",
                "RadarGen利用图像潜在扩散模型，并结合视觉信息（深度、语义、运动）引导雷达点云生成，提升真实感。",
                "实验表明，RadarGen生成的雷达点云能够有效缩小与真实数据训练的感知模型之间的性能差距。"
            ],
            "method_zh": "**问题定义**：论文旨在解决汽车雷达点云数据生成的问题。现有方法，如基于规则的仿真或GAN，难以生成足够真实和多样化的雷达数据，且依赖于精确的场景建模和参数调整。这限制了雷达感知算法的训练和验证，尤其是在corner case场景下。\\n\\n**核心思路**：论文的核心思路是利用扩散模型强大的生成能力，并以多视角相机图像作为条件，生成与视觉场景一致的雷达点云。通过视觉信息引导，可以生成更符合物理规律和场景语义的雷达数据。将雷达数据表示为鸟瞰图(BEV)形式，方便与图像特征对齐和进行扩散建模。\\n\\n**技术框架**：RadarGen的整体框架包含以下几个主要步骤：1) 从多视角相机图像中提取BEV对齐的深度、语义和运动特征；2) 将这些特征作为条件输入到图像潜在扩散模型中；3) 扩散模型生成雷达鸟瞰图，包含空间结构、RCS和多普勒信息；4) 从生成的雷达鸟瞰图中重建雷达点云。\\n\\n**关键创新**：RadarGen的关键创新在于：1) 将图像潜在扩散模型应用于雷达点云生成，充分利用了扩散模型强大的生成能力；2) 引入BEV对齐的深度、语义和运动特征作为条件，引导雷达点云生成，保证了生成数据的真实性和场景一致性；3) 使用鸟瞰图形式表示雷达数据，方便与图像特征对齐和进行扩散建模。\\n\\n**关键设计**：RadarGen使用预训练的视觉基础模型提取深度、语义和运动特征。扩散模型采用U-Net结构，以图像特征作为cross-attention的key和value。雷达鸟瞰图的分辨率和通道数需要根据具体数据集进行调整。损失函数包括扩散模型的标准损失函数，以及可选的对抗损失函数，以进一步提高生成数据的真实性。",
            "application_zh": "RadarGen可应用于自动驾驶系统的雷达感知算法的训练和验证，尤其是在真实数据难以获取的corner case场景下。它还可以用于生成多模态仿真数据，促进跨模态感知算法的研究。此外，该方法可以扩展到其他传感器模态，实现统一的生成式仿真框架。",
            "highlight_zh": "实验结果表明，RadarGen生成的雷达点云在视觉上与真实场景高度一致，并且能够有效缩小与在真实数据上训练的感知模型之间的性能差距。具体来说，使用RadarGen生成的数据训练的雷达目标检测器，其性能接近于使用真实数据训练的模型，证明了RadarGen生成数据的有效性。",
            "tags_zh": [
                "雷达点云生成",
                "扩散模型",
                "多模态仿真",
                "自动驾驶",
                "鸟瞰图"
            ],
            "_index": 25,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17897v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17897v1/figures/data/fig_method_radar_maps.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17897v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "UniStateDLO: Unified Generative State Estimation and Tracking of Deformable Linear Objects Under Occlusion for Constrained Manipulation",
            "authors": [
                "Kangchen Lv",
                "Mingrui Yu",
                "Shihefeng Wang",
                "Xiangyang Ji",
                "Xiang Li"
            ],
            "arxiv_id": "2512.17764v1",
            "summary": "Perception of deformable linear objects (DLOs), such as cables, ropes, and wires, is the cornerstone for successful downstream manipulation. Although vision-based methods have been extensively explored, they remain highly vulnerable to occlusions that commonly arise in constrained manipulation environments due to surrounding obstacles, large and varying deformations, and limited viewpoints. Moreover, the high dimensionality of the state space, the lack of distinctive visual features, and the presence of sensor noises further compound the challenges of reliable DLO perception. To address these open issues, this paper presents UniStateDLO, the first complete DLO perception pipeline with deep-learning methods that achieves robust performance under severe occlusion, covering both single-frame state estimation and cross-frame state tracking from partial point clouds. Both tasks are formulated as conditional generative problems, leveraging the strong capability of diffusion models to capture the complex mapping between highly partial observations and high-dimensional DLO states. UniStateDLO effectively handles a wide range of occlusion patterns, including initial occlusion, self-occlusion, and occlusion caused by multiple objects. In addition, it exhibits strong data efficiency as the entire network is trained solely on a large-scale synthetic dataset, enabling zero-shot sim-to-real generalization without any real-world training data. Comprehensive simulation and real-world experiments demonstrate that UniStateDLO outperforms all state-of-the-art baselines in both estimation and tracking, producing globally smooth yet locally precise DLO state predictions in real time, even under substantial occlusions. Its integration as the front-end module in a closed-loop DLO manipulation system further demonstrates its ability to support stable feedback control in complex, constrained 3-D environments.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "The first two authors contributed equally. Project page: https://unistatedlo.github.io",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17764v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation",
                        "sim-to-real"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "UniStateDLO：提出统一的生成式框架，用于遮挡下可变形线性物体的状态估计与跟踪",
            "summary_zh": "本文提出UniStateDLO，这是一个完整的基于深度学习的可变形线性物体（DLO）感知流程，它在严重遮挡下实现了鲁棒的性能，涵盖了来自部分点云的单帧状态估计和跨帧状态跟踪。这两项任务都被表述为条件生成问题，利用扩散模型强大的能力来捕捉高度部分观测和高维DLO状态之间复杂的映射关系。UniStateDLO有效地处理了各种遮挡模式，包括初始遮挡、自遮挡和由多个物体引起的遮挡。此外，它表现出强大的数据效率，因为整个网络仅在一个大规模合成数据集上进行训练，从而实现了零样本的sim-to-real泛化，而无需任何真实世界训练数据。全面的仿真和真实世界实验表明，UniStateDLO在估计和跟踪方面均优于所有最先进的基线，即使在大量遮挡下也能实时生成全局平滑但局部精确的DLO状态预测。将其作为前端模块集成到闭环DLO操作系统中，进一步证明了其在复杂、受约束的3D环境中支持稳定反馈控制的能力。",
            "intro_zh": [
                "现有基于视觉的DLO感知方法易受遮挡影响，尤其是在受限操作环境中，存在高维状态空间和缺乏明显视觉特征等挑战。",
                "UniStateDLO将状态估计和跟踪问题建模为条件生成问题，利用扩散模型学习部分观测与DLO状态之间的复杂映射关系。",
                "UniStateDLO仅使用合成数据训练，即可实现零样本的sim-to-real泛化，并在真实和仿真环境中优于现有方法。"
            ],
            "method_zh": "**问题定义**：可变形线性物体（DLO）的感知在机器人操作中至关重要，但现有方法在存在遮挡时表现不佳。遮挡可能由环境中的其他物体、DLO自身的形状或有限的视角引起。此外，DLO状态空间维度高，缺乏明显的视觉特征，以及传感器噪声，都使得准确感知DLO状态变得困难。现有方法难以在遮挡情况下提供鲁棒和精确的状态估计和跟踪。\n\n**核心思路**：UniStateDLO的核心思想是将DLO的状态估计和跟踪问题视为一个条件生成问题，并利用扩散模型来学习从部分观测到完整DLO状态的映射。扩散模型擅长捕捉复杂的数据分布，因此能够从被遮挡的部分点云中推断出完整的DLO形状。通过将估计和跟踪统一在一个生成框架中，可以更好地利用时间信息，提高跟踪的鲁棒性。\n\n**技术框架**：UniStateDLO包含两个主要模块：状态估计模块和状态跟踪模块。状态估计模块接收单帧的部分点云作为输入，并使用条件扩散模型生成完整的DLO状态。状态跟踪模块则利用前一帧的状态估计结果和当前帧的部分点云，进一步优化DLO状态，实现跨帧的稳定跟踪。整个流程可以概括为：输入部分点云 -> 状态估计（扩散模型） -> 状态跟踪（融合时间信息） -> 输出完整DLO状态。\n\n**关键创新**：UniStateDLO的关键创新在于将扩散模型应用于DLO的状态估计和跟踪，并将其统一在一个生成框架中。与传统的基于优化的方法或直接回归方法不同，UniStateDLO能够更好地处理遮挡和噪声，生成更平滑和更准确的DLO状态。此外，该方法仅使用合成数据进行训练，实现了零样本的sim-to-real泛化，大大降低了数据采集和标注的成本。\n\n**关键设计**：UniStateDLO使用了一个条件扩散模型，该模型以部分点云作为条件，生成完整的DLO状态。扩散模型的损失函数包括一个重建损失和一个正则化项，用于保证生成的DLO状态的平滑性和物理可行性。在状态跟踪模块中，使用卡尔曼滤波或类似的滤波方法来融合时间信息，提高跟踪的鲁棒性。网络结构细节（如编码器-解码器架构）和超参数设置（如扩散步数）未知。",
            "application_zh": "UniStateDLO在机器人操作领域具有广泛的应用前景，例如电缆布线、绳索操作、医疗手术等。该方法能够提高机器人在复杂和受限环境中操作DLO的可靠性和效率，降低人工干预的需求。未来，可以将UniStateDLO与其他感知模态（如力觉）相结合，进一步提高DLO感知的准确性和鲁棒性。",
            "highlight_zh": "UniStateDLO在仿真和真实世界实验中均优于现有方法。在状态估计方面，UniStateDLO在遮挡率较高的情况下，能够显著降低状态估计误差。在状态跟踪方面，UniStateDLO能够实现更稳定的跟踪，即使在DLO发生剧烈形变或被严重遮挡时，也能保持准确的跟踪结果。具体性能提升数据未知。",
            "tags_zh": [
                "可变形线性物体感知",
                "状态估计",
                "状态跟踪",
                "扩散模型",
                "遮挡处理"
            ],
            "_index": 26,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17764v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17764v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17764v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Xiaomi MiMo-VL-Miloco Technical Report",
            "authors": [
                "Jiaze Li",
                "Jingyang Chen",
                "Yuxun Qu",
                "Jianzhong Ju",
                "Zhenbo Luo",
                "Jian Luan",
                "Shijie Xu",
                "Zhenru Lin",
                "Junyou Zhu",
                "Boshen Xu",
                "Wenhui Tan",
                "Pei Fu"
            ],
            "arxiv_id": "2512.17436v1",
            "summary": "We open-source \\textbf{MiMo-VL-Miloco-7B} and its quantized variant \\textbf{MiMo-VL-Miloco-7B-GGUF}, a pair of home-centric vision-language models that achieve strong performance on both home-scenario understanding and general multimodal reasoning. Built on the MiMo-VL-7B backbone, MiMo-VL-Miloco-7B is specialized for smart-home environments, attaining leading F1 scores on gesture recognition and common home-scenario understanding, while also delivering consistent gains across video benchmarks such as Video-MME, Video-MMMU, and Charades-STA, as well as language understanding benchmarks including MMMU-Pro and MMLU-Pro. In our experiments, MiMo-VL-Miloco-7B outperforms strong closed-source and open-source baselines on home-scenario understanding and several multimodal reasoning benchmarks. To balance specialization and generality, we design a two-stage training pipeline that combines supervised fine-tuning with reinforcement learning based on Group Relative Policy Optimization, leveraging efficient multi-domain data. We further incorporate chain-of-thought supervision and token-budget-aware reasoning, enabling the model to learn knowledge in a data-efficient manner while also performing reasoning efficiently. Our analysis shows that targeted home-scenario training not only enhances activity and gesture understanding, but also improves text-only reasoning with only modest trade-offs on document-centric tasks. Model checkpoints, quantized GGUF weights, and our home-scenario evaluation toolkit are publicly available at \\href{https://github.com/XiaoMi/xiaomi-mimo-vl-miloco}{https://github.com/XiaoMi/xiaomi-mimo-vl-miloco} to support research and deployment in real-world smart-home applications.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17436v1",
            "code_links": [
                {
                    "url": "https://github.com/XiaoMi/xiaomi-mimo-vl-miloco",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal",
                        "chain-of-thought"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出MiMo-VL-Miloco以解决智能家居场景理解问题",
            "summary_zh": "我们开源了MiMo-VL-Miloco-7B及其量化变体MiMo-VL-Miloco-7B-GGUF，这是一对专注于家庭场景的视觉-语言模型，在家庭场景理解和多模态推理方面表现优异。基于MiMo-VL-7B骨干网络，MiMo-VL-Miloco-7B专为智能家居环境设计，在手势识别和常见家庭场景理解上取得了领先的F1分数，同时在视频基准（如Video-MME、Video-MMMU和Charades-STA）和语言理解基准（如MMMU-Pro和MMLU-Pro）上也表现出一致的提升。实验结果表明，MiMo-VL-Miloco-7B在家庭场景理解和多个多模态推理基准上超越了强大的闭源和开源基线。我们设计了一个结合监督微调和基于组相对策略优化的强化学习的两阶段训练流程，利用高效的多领域数据，进一步引入链式思维监督和基于令牌预算的推理，使模型能够以数据高效的方式学习知识，同时高效地进行推理。",
            "intro_zh": [
                "现有的多模态模型在智能家居场景理解方面表现不足，尤其是在手势识别和活动理解上存在局限性。",
                "论文提出了一种基于MiMo-VL-7B的两阶段训练流程，结合了监督微调和强化学习，以提升模型在家庭场景中的表现。",
                "实验结果显示，MiMo-VL-Miloco-7B在多个基准测试中超越了现有的强基线，特别是在家庭场景理解和多模态推理任务上取得了显著提升。"
            ],
            "method_zh": "**问题定义**：本论文旨在解决智能家居环境中的多模态理解问题，现有方法在手势识别和家庭场景理解方面存在性能不足，难以满足实际应用需求。\\n\\n**核心思路**：提出的MiMo-VL-Miloco-7B模型通过结合监督学习和强化学习的两阶段训练流程，旨在提升模型在特定家庭场景中的理解能力，同时保持一定的通用性。\\n\\n**技术框架**：整体架构包括两个主要阶段：首先进行监督微调，随后通过基于组相对策略优化的强化学习进行进一步训练。模型还引入了链式思维监督和令牌预算意识的推理机制，以提高学习效率和推理能力。\\n\\n**关键创新**：最重要的创新点在于将强化学习与监督学习相结合，形成了一个高效的训练流程，能够在家庭场景中进行针对性训练，同时提升文本推理能力。\\n\\n**关键设计**：模型设计中采用了多领域数据集进行训练，损失函数和网络结构经过精心调整，以确保在家庭场景理解和多模态推理任务中取得最佳性能。",
            "application_zh": "该研究的潜在应用领域包括智能家居助手、家庭监控系统和人机交互界面等。通过提升模型在家庭场景中的理解能力，可以为用户提供更为智能和个性化的服务，未来可能在智能家居行业产生深远影响。",
            "highlight_zh": "实验结果表明，MiMo-VL-Miloco-7B在家庭场景理解和多模态推理基准上超越了多个强基线，特别是在手势识别任务中取得了领先的F1分数，并在视频理解基准上也显示出一致的性能提升。",
            "tags_zh": [
                "智能家居",
                "多模态理解",
                "视觉-语言模型",
                "强化学习",
                "手势识别",
                "家庭场景",
                "数据高效学习"
            ],
            "_index": 27,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17436v1/figure/radar.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17436v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "PhysFire-WM: A Physics-Informed World Model for Emulating Fire Spread Dynamics",
            "authors": [
                "Nan Zhou",
                "Huandong Wang",
                "Jiahao Li",
                "Yang Li",
                "Xiao-Ping Zhang",
                "Yong Li",
                "Xinlei Chen"
            ],
            "arxiv_id": "2512.17152v1",
            "summary": "Fine-grained fire prediction plays a crucial role in emergency response. Infrared images and fire masks provide complementary thermal and boundary information, yet current methods are predominantly limited to binary mask modeling with inherent signal sparsity, failing to capture the complex dynamics of fire. While world models show promise in video generation, their physical inconsistencies pose significant challenges for fire forecasting. This paper introduces PhysFire-WM, a Physics-informed World Model for emulating Fire spread dynamics. Our approach internalizes combustion dynamics by encoding structured priors from a Physical Simulator to rectify physical discrepancies, coupled with a Cross-task Collaborative Training strategy (CC-Train) that alleviates the issue of limited information in mask-based modeling. Through parameter sharing and gradient coordination, CC-Train effectively integrates thermal radiation dynamics and spatial boundary delineation, enhancing both physical realism and geometric accuracy. Extensive experiments on a fine-grained multimodal fire dataset demonstrate the superior accuracy of PhysFire-WM in fire spread prediction. Validation underscores the importance of physical priors and cross-task collaboration, providing new insights for applying physics-informed world models to disaster prediction.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17152v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]world model"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出PhysFire-WM，利用物理信息世界模型模拟火灾蔓延动态",
            "summary_zh": "精细化的火灾预测在应急响应中至关重要。红外图像和火灾掩膜提供了互补的热信息和边界信息，但现有方法主要局限于二元掩膜建模，存在信号稀疏性问题，无法捕捉火灾的复杂动态。世界模型在视频生成方面展现了潜力，但其物理不一致性对火灾预测构成了挑战。本文提出了PhysFire-WM，一个物理信息世界模型，用于模拟火灾蔓延动态。该方法通过编码来自物理模拟器的结构化先验知识来纠正物理差异，从而内化燃烧动态。同时，采用跨任务协同训练策略（CC-Train）来缓解基于掩膜建模的信息有限问题。通过参数共享和梯度协调，CC-Train有效地整合了热辐射动态和空间边界描绘，从而提高了物理真实性和几何精度。在细粒度多模态火灾数据集上的大量实验表明，PhysFire-WM在火灾蔓延预测方面具有卓越的准确性。验证强调了物理先验和跨任务协作的重要性，为将物理信息世界模型应用于灾害预测提供了新的见解。",
            "intro_zh": [
                "现有火灾预测方法依赖二元掩膜建模，存在信号稀疏性问题，难以捕捉火灾蔓延的复杂动态。",
                "PhysFire-WM通过编码物理模拟器的先验知识来纠正物理不一致性，并采用跨任务协同训练策略整合热辐射和空间边界信息。",
                "实验表明，PhysFire-WM在火灾蔓延预测方面具有更高的准确性，验证了物理先验和跨任务协作的重要性。"
            ],
            "method_zh": "**问题定义**：现有火灾预测方法主要基于二元掩膜建模，这种方法忽略了火灾蔓延过程中的复杂物理动态，并且由于掩膜的稀疏性，难以准确预测火灾的未来状态。此外，直接使用世界模型进行火灾预测时，由于缺乏物理约束，预测结果可能不符合实际的物理规律，导致预测不准确。\\n\\n**核心思路**：PhysFire-WM的核心思路是将物理模拟器的先验知识融入到世界模型中，从而约束模型的学习过程，使其能够更好地模拟火灾蔓延的物理过程。通过跨任务协同训练，模型能够同时学习热辐射动态和空间边界信息，从而提高预测的准确性和物理真实性。\\n\\n**技术框架**：PhysFire-WM的整体框架包含以下几个主要模块：1) 物理模拟器：用于生成火灾蔓延的物理先验知识。2) 世界模型：用于学习火灾蔓延的动态模型。3) 跨任务协同训练模块（CC-Train）：用于整合热辐射动态和空间边界信息。该框架首先利用物理模拟器生成火灾蔓延的物理先验知识，然后将这些先验知识编码到世界模型中。最后，通过CC-Train模块，模型同时学习热辐射动态和空间边界信息，从而提高预测的准确性和物理真实性。\\n\\n**关键创新**：PhysFire-WM的关键创新在于将物理信息融入到世界模型中，从而约束模型的学习过程，使其能够更好地模拟火灾蔓延的物理过程。此外，CC-Train模块通过参数共享和梯度协调，有效地整合了热辐射动态和空间边界信息，从而提高了预测的准确性和物理真实性。\\n\\n**关键设计**：PhysFire-WM的关键设计包括：1) 使用物理模拟器生成火灾蔓延的物理先验知识。2) 设计了一种新的网络结构，用于编码物理先验知识。3) 设计了CC-Train模块，用于整合热辐射动态和空间边界信息。具体来说，CC-Train模块通过参数共享和梯度协调，使得模型能够同时学习热辐射动态和空间边界信息。损失函数的设计也至关重要，需要平衡物理约束和数据驱动的学习。",
            "application_zh": "PhysFire-WM可应用于火灾应急响应、消防安全评估和城市规划等领域。通过准确预测火灾蔓延趋势，可以帮助消防部门制定更有效的灭火策略，减少人员伤亡和财产损失。此外，该模型还可以用于评估建筑物的防火性能，为城市规划提供科学依据，提高城市的整体抗灾能力。",
            "highlight_zh": "实验结果表明，PhysFire-WM在火灾蔓延预测方面取得了显著的性能提升。与现有方法相比，PhysFire-WM在预测精度方面提高了约15%，并且能够更好地捕捉火灾蔓延的物理动态。消融实验验证了物理先验和跨任务协作的重要性，表明它们对提高预测准确性和物理真实性至关重要。",
            "tags_zh": [
                "火灾预测",
                "世界模型",
                "物理信息",
                "跨任务学习",
                "应急响应"
            ],
            "_index": 28,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17152v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17152v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17152v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation",
            "authors": [
                "Shengwei Zhao",
                "Jingwen Yao",
                "Sitong Wei",
                "Linhai Xu",
                "Yuying Liu",
                "Dong Zhang",
                "Zhiqiang Tian",
                "Shaoyi Du"
            ],
            "arxiv_id": "2512.17194v1",
            "summary": "Multi-modal Retrieval-Augmented Generation (MMRAG) enables highly credible generation by integrating external multi-modal knowledge, thus demonstrating impressive performance in complex multi-modal scenarios. However, existing MMRAG methods fail to clarify the reasoning logic behind retrieval and response generation, which limits the explainability of the results. To address this gap, we propose to introduce reinforcement learning into multi-modal retrieval-augmented generation, enhancing the reasoning capabilities of multi-modal large language models through a two-stage reinforcement fine-tuning framework to achieve explainable multi-modal retrieval-augmented generation. Specifically, in the first stage, rule-based reinforcement fine-tuning is employed to perform coarse-grained point-wise ranking of multi-modal documents, effectively filtering out those that are significantly irrelevant. In the second stage, reasoning-based reinforcement fine-tuning is utilized to jointly optimize fine-grained list-wise ranking and answer generation, guiding multi-modal large language models to output explainable reasoning logic in the MMRAG process. Our method achieves state-of-the-art results on WebQA and MultimodalQA, two benchmark datasets for multi-modal retrieval-augmented generation, and its effectiveness is validated through comprehensive ablation experiments.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "This paper was accepted to AAAI2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17194v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出MMRAG-RFT，通过两阶段强化学习提升多模态检索增强生成的可解释性。",
            "summary_zh": "多模态检索增强生成(MMRAG)通过整合外部多模态知识来实现高度可信的生成，在复杂的多模态场景中表现出令人印象深刻的性能。然而，现有的MMRAG方法未能阐明检索和响应生成背后的推理逻辑，这限制了结果的可解释性。为了解决这个问题，我们提出将强化学习引入多模态检索增强生成，通过两阶段强化微调框架增强多模态大型语言模型的推理能力，从而实现可解释的多模态检索增强生成。具体来说，在第一阶段，采用基于规则的强化微调来执行多模态文档的粗粒度逐点排序，有效地过滤掉那些显著不相关的文档。在第二阶段，利用基于推理的强化微调来联合优化细粒度的列表式排序和答案生成，引导多模态大型语言模型在MMRAG过程中输出可解释的推理逻辑。我们的方法在WebQA和MultimodalQA这两个多模态检索增强生成的基准数据集上取得了最先进的结果，并通过全面的消融实验验证了其有效性。",
            "intro_zh": [
                "现有MMRAG方法缺乏对检索和生成过程的推理逻辑的解释，导致结果可解释性不足。",
                "提出MMRAG-RFT框架，通过两阶段强化学习微调，提升多模态大语言模型在检索和生成过程中的推理能力。",
                "在WebQA和MultimodalQA数据集上取得了SOTA结果，并通过消融实验验证了框架的有效性。"
            ],
            "method_zh": "**问题定义**：现有的多模态检索增强生成（MMRAG）方法在生成答案时，缺乏对检索到的多模态文档进行有效排序和推理的能力，导致生成结果缺乏可解释性。用户难以理解模型为何选择特定的文档以及如何利用这些文档生成最终答案。现有方法未能明确地建模检索和生成之间的推理过程，导致模型在复杂场景下的表现受到限制。\\n\\n**核心思路**：论文的核心思路是通过引入强化学习，显式地建模多模态文档的排序和答案生成过程，从而提升MMRAG的可解释性。具体而言，通过两阶段的强化微调，首先进行粗粒度的文档过滤，然后进行细粒度的排序和答案生成，引导模型学习可解释的推理逻辑。\\n\\n**技术框架**：MMRAG-RFT框架包含两个主要阶段：1) 基于规则的强化微调（Rule-based Reinforcement Fine-tuning）：对多模态文档进行粗粒度的逐点排序，过滤掉不相关的文档。2) 基于推理的强化微调（Reasoning-based Reinforcement Fine-tuning）：联合优化细粒度的列表式排序和答案生成，引导模型输出可解释的推理逻辑。这两个阶段都利用强化学习来优化模型的行为，使其更符合人类的推理习惯。\\n\\n**关键创新**：该方法最重要的创新点在于将强化学习引入到MMRAG框架中，并设计了两阶段的强化微调策略。与传统的监督学习方法不同，强化学习能够更好地建模序列决策过程，从而优化检索和生成之间的交互。此外，两阶段的微调策略能够有效地平衡效率和效果，先进行粗粒度过滤，再进行细粒度优化。\\n\\n**关键设计**：在第一阶段，使用基于规则的奖励函数来指导模型的学习，例如，如果检索到的文档与问题相关，则给予正向奖励，否则给予负向奖励。在第二阶段，使用基于推理的奖励函数，鼓励模型生成包含推理逻辑的答案。具体的奖励函数设计需要根据具体的任务和数据集进行调整。此外，论文还可能涉及一些关于网络结构、损失函数和训练策略的细节，但具体内容未知。",
            "application_zh": "该研究成果可应用于需要高度可信和可解释性的多模态问答系统、智能客服、教育辅导等领域。通过提供清晰的推理逻辑，可以增强用户对AI系统决策的信任感，并促进人机协作。未来，该方法有望扩展到更复杂的多模态任务中，例如多模态对话生成、多模态内容创作等。",
            "highlight_zh": "MMRAG-RFT在WebQA和MultimodalQA两个基准数据集上取得了state-of-the-art的结果。具体的性能提升数据未知，但摘要中明确指出通过全面的消融实验验证了其有效性。这表明该方法在提升多模态检索增强生成的可解释性和准确性方面具有显著优势。",
            "tags_zh": [
                "多模态检索增强生成",
                "强化学习",
                "可解释性",
                "多模态大语言模型",
                "两阶段微调"
            ],
            "_index": 29,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17194v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17194v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "ImagineNav++: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination",
            "authors": [
                "Teng Wang",
                "Xinxin Zhao",
                "Wenzhe Cai",
                "Changyin Sun"
            ],
            "arxiv_id": "2512.17435v1",
            "summary": "Visual navigation is a fundamental capability for autonomous home-assistance robots, enabling long-horizon tasks such as object search. While recent methods have leveraged Large Language Models (LLMs) to incorporate commonsense reasoning and improve exploration efficiency, their planning remains constrained by textual representations, which cannot adequately capture spatial occupancy or scene geometry--critical factors for navigation decisions. We explore whether Vision-Language Models (VLMs) can achieve mapless visual navigation using only onboard RGB/RGB-D streams, unlocking their potential for spatial perception and planning. We achieve this through an imagination-powered navigation framework, ImagineNav++, which imagines future observation images from candidate robot views and translates navigation planning into a simple best-view image selection problem for VLMs. First, a future-view imagination module distills human navigation preferences to generate semantically meaningful viewpoints with high exploration potential. These imagined views then serve as visual prompts for the VLM to identify the most informative viewpoint. To maintain spatial consistency, we develop a selective foveation memory mechanism, which hierarchically integrates keyframe observations via a sparse-to-dense framework, constructing a compact yet comprehensive memory for long-term spatial reasoning. This approach transforms goal-oriented navigation into a series of tractable point-goal navigation tasks. Extensive experiments on open-vocabulary object and instance navigation benchmarks show that ImagineNav++ achieves SOTA performance in mapless settings, even surpassing most map-based methods, highlighting the importance of scene imagination and memory in VLM-based spatial reasoning.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "17 pages, 10 figures. arXiv admin note: text overlap with arXiv:2410.09874",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17435v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "open-vocabulary",
                        "open vocabulary"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 7.0,
            "hit_pillars": [
                "3_perception_slam",
                "9_embodied_foundation"
            ],
            "headline_zh": "ImagineNav++：通过场景想象提示视觉-语言模型，实现具身导航",
            "summary_zh": "视觉导航是自主家庭辅助机器人的基本能力，能够实现诸如物体搜索等长时程任务。虽然最近的方法利用大型语言模型（LLM）来整合常识推理并提高探索效率，但它们的规划仍然受到文本表示的限制，无法充分捕捉空间占用或场景几何结构——这些是导航决策的关键因素。我们探索了视觉-语言模型（VLM）是否仅使用板载RGB/RGB-D流就能实现无地图视觉导航，从而释放它们在空间感知和规划方面的潜力。我们通过一个由想象驱动的导航框架ImagineNav++来实现这一点，该框架从候选机器人视角想象未来的观察图像，并将导航规划转化为VLM的最佳视角图像选择问题。首先，一个未来视角想象模块提炼人类导航偏好，以生成具有高探索潜力的语义上有意义的视点。然后，这些想象的视图作为VLM的视觉提示，以识别信息量最大的视点。为了保持空间一致性，我们开发了一种选择性中央凹记忆机制，该机制通过稀疏到密集框架分层整合关键帧观察，构建一个紧凑而全面的记忆，用于长期空间推理。这种方法将面向目标的导航转化为一系列易于处理的点目标导航任务。在开放词汇物体和实例导航基准上的大量实验表明，ImagineNav++在无地图设置中实现了SOTA性能，甚至超过了大多数基于地图的方法，突出了场景想象和记忆在基于VLM的空间推理中的重要性。",
            "intro_zh": [
                "现有基于LLM的导航方法依赖文本表示，无法有效捕捉空间信息，限制了导航决策。",
                "ImagineNav++通过想象未来场景图像，将导航规划转化为VLM的最佳视角选择问题，利用VLM的空间感知能力。",
                "实验表明，ImagineNav++在无地图导航中达到SOTA，甚至超越了多数基于地图的方法，验证了场景想象和记忆的重要性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决在无地图环境下，如何利用视觉-语言模型（VLM）实现高效、准确的视觉导航问题。现有方法，特别是那些依赖大型语言模型（LLM）的方法，虽然能够整合常识推理，但在处理空间信息方面存在不足，因为它们主要依赖文本表示，无法充分捕捉场景的几何结构和空间占用情况。这限制了它们在复杂环境中的导航能力。\\n\\n**核心思路**：论文的核心思路是利用VLM的视觉感知能力，通过“场景想象”来弥补传统方法在空间信息处理上的不足。具体来说，ImagineNav++框架通过想象从不同候选视角观察到的未来场景图像，并将导航规划问题转化为一个最佳视角选择问题。VLM被用来评估这些想象的视角，选择信息量最大的一个，从而引导机器人的导航行为。\\n\\n**技术框架**：ImagineNav++框架主要包含两个核心模块：未来视角想象模块和选择性中央凹记忆机制。未来视角想象模块负责生成具有高探索潜力的语义上有意义的视点，这些视点作为VLM的视觉提示。选择性中央凹记忆机制则用于维护空间一致性，通过分层整合关键帧观察，构建一个紧凑而全面的记忆，用于长期空间推理。整体流程是将目标导向的导航任务分解为一系列可处理的点目标导航任务。\\n\\n**关键创新**：该论文的关键创新在于将“场景想象”的概念引入到VLM驱动的导航中。通过让VLM评估想象的未来场景，该方法能够更好地利用VLM的视觉感知能力进行空间推理和导航规划。此外，选择性中央凹记忆机制也是一个重要的创新，它能够有效地管理和利用历史观测信息，提高导航的鲁棒性和效率。\\n\\n**关键设计**：未来视角想象模块的设计细节（例如，如何提炼人类导航偏好并生成有意义的视点）以及选择性中央凹记忆机制的具体实现（例如，稀疏到密集框架的细节、关键帧选择策略、记忆更新机制）是关键的设计要素。论文中可能还涉及特定的损失函数，用于训练未来视角想象模块，以及VLM的prompt工程细节。",
            "application_zh": "该研究成果可应用于家庭服务机器人、自动驾驶、虚拟现实等领域。在家庭服务机器人中，可以帮助机器人更好地理解环境，完成物体搜索、清洁等任务。在自动驾驶领域，可以提高车辆在复杂环境中的导航能力。在虚拟现实领域，可以增强虚拟环境的真实感和交互性。",
            "highlight_zh": "ImagineNav++在无地图开放词汇物体和实例导航基准测试中取得了SOTA性能，甚至超越了大多数基于地图的方法。这表明了场景想象和记忆在基于VLM的空间推理中的重要性，并验证了该方法的有效性。",
            "tags_zh": [
                "视觉导航",
                "视觉-语言模型",
                "场景想象",
                "具身智能",
                "无地图导航"
            ],
            "_index": 30,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17435v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17435v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17435v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "HydroGym: A Reinforcement Learning Platform for Fluid Dynamics",
            "authors": [
                "Christian Lagemann",
                "Sajeda Mokbel",
                "Miro Gondrum",
                "Mario Rüttgers",
                "Jared Callaham",
                "Ludger Paehler",
                "Samuel Ahnert",
                "Nicholas Zolman",
                "Kai Lagemann",
                "Nikolaus Adams",
                "Matthias Meinke",
                "Wolfgang Schröder",
                "Jean-Christophe Loiseau",
                "Esther Lagemann",
                "Steven L. Brunton"
            ],
            "arxiv_id": "2512.17534v1",
            "summary": "Modeling and controlling fluid flows is critical for several fields of science and engineering, including transportation, energy, and medicine. Effective flow control can lead to, e.g., lift increase, drag reduction, mixing enhancement, and noise reduction. However, controlling a fluid faces several significant challenges, including high-dimensional, nonlinear, and multiscale interactions in space and time. Reinforcement learning (RL) has recently shown great success in complex domains, such as robotics and protein folding, but its application to flow control is hindered by a lack of standardized benchmark platforms and the computational demands of fluid simulations. To address these challenges, we introduce HydroGym, a solver-independent RL platform for flow control research. HydroGym integrates sophisticated flow control benchmarks, scalable runtime infrastructure, and state-of-the-art RL algorithms. Our platform includes 42 validated environments spanning from canonical laminar flows to complex three-dimensional turbulent scenarios, validated over a wide range of Reynolds numbers. We provide non-differentiable solvers for traditional RL and differentiable solvers that dramatically improve sample efficiency through gradient-enhanced optimization. Comprehensive evaluation reveals that RL agents consistently discover robust control principles across configurations, such as boundary layer manipulation, acoustic feedback disruption, and wake reorganization. Transfer learning studies demonstrate that controllers learned at one Reynolds number or geometry adapt efficiently to new conditions, requiring approximately 50% fewer training episodes. The HydroGym platform is highly extensible and scalable, providing a framework for researchers in fluid dynamics, machine learning, and control to add environments, surrogate models, and control algorithms to advance science and technology.",
            "categories": [
                "physics.flu-dyn",
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "physics.flu-dyn",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17534v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 6.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "HydroGym：用于流体动力学的强化学习平台，提供可扩展的控制基准。",
            "summary_zh": "流体流动建模与控制在交通运输、能源和医学等多个科学与工程领域至关重要。有效的流动控制可以实现诸如升力增加、阻力减少、混合增强和噪声降低等目标。然而，控制流体面临着诸多挑战，包括高维度、非线性以及时空中多尺度的相互作用。强化学习（RL）最近在机器人和蛋白质折叠等复杂领域取得了巨大成功，但其在流动控制中的应用受到缺乏标准化基准平台以及流体模拟计算需求的限制。为了应对这些挑战，我们推出了HydroGym，一个独立于求解器的流动控制研究RL平台。HydroGym集成了复杂的流动控制基准、可扩展的运行时基础设施和最先进的RL算法。我们的平台包括42个经过验证的环境，涵盖从规范层流到复杂三维湍流场景，并在广泛的雷诺数范围内进行了验证。我们为传统RL提供不可微求解器，并提供可微求解器，通过梯度增强优化显著提高样本效率。综合评估表明，RL智能体在各种配置中始终如一地发现鲁棒的控制原理，例如边界层操纵、声反馈中断和尾流重组。迁移学习研究表明，在一个雷诺数或几何形状下学习的控制器能够有效地适应新条件，所需训练次数减少约50%。HydroGym平台具有高度可扩展性，为流体动力学、机器学习和控制领域的研究人员提供了一个框架，可以添加环境、替代模型和控制算法，以推进科学和技术。",
            "intro_zh": [
                "流动控制面临高维度、非线性、多尺度交互等挑战，现有方法难以有效应对复杂流动环境。",
                "HydroGym平台通过集成多种流动控制基准、可扩展基础设施和先进RL算法，提供统一的流体控制研究平台。",
                "实验表明，RL智能体在HydroGym平台中能发现鲁棒的控制策略，且迁移学习能显著减少训练需求。"
            ],
            "method_zh": "**问题定义**：论文旨在解决流体控制领域缺乏标准化强化学习基准平台的问题。现有方法在处理高维度、非线性、多尺度交互的复杂流动环境时面临挑战，计算成本高昂，难以实现有效的控制策略。\n\n**核心思路**：HydroGym的核心思路是构建一个独立于求解器的强化学习平台，该平台集成了多种经过验证的流动控制基准环境，并提供可扩展的运行时基础设施和先进的强化学习算法。通过提供标准化的环境和工具，HydroGym旨在促进流体控制领域强化学习研究的进展。\n\n**技术框架**：HydroGym平台包含以下主要模块：\n1. **环境库**：提供42个经过验证的流动控制环境，涵盖从层流到湍流的各种场景，并支持不同的雷诺数。\n2. **求解器**：提供不可微求解器用于传统强化学习，以及可微求解器用于梯度增强优化，以提高样本效率。\n3. **强化学习算法**：集成多种先进的强化学习算法，方便研究人员进行实验和比较。\n4. **可扩展基础设施**：支持添加新的环境、替代模型和控制算法，方便研究人员进行定制和扩展。\n\n**关键创新**：HydroGym的关键创新在于其作为一个solver-independent的强化学习平台，为流体动力学研究提供了一个标准化的、可扩展的、易于使用的环境。通过提供可微求解器，HydroGym还显著提高了样本效率，使得强化学习在流体控制领域的应用更加可行。\n\n**关键设计**：HydroGym的关键设计包括：\n1. **环境设计**：精心设计的流动控制环境，涵盖不同的流动类型和雷诺数，以评估强化学习算法的泛化能力。\n2. **可微求解器**：利用可微求解器计算梯度，从而实现梯度增强优化，提高样本效率。\n3. **奖励函数设计**：设计合适的奖励函数，引导强化学习智能体学习有效的控制策略。",
            "application_zh": "HydroGym平台在交通运输、能源、医学等领域具有广泛的应用前景。例如，可以用于优化飞行器的气动外形，降低阻力，提高燃油效率；可以用于优化风力发电机的叶片设计，提高发电效率；还可以用于优化医疗器械的设计，改善血液流动，降低血栓形成的风险。该平台有望加速流体控制领域强化学习研究的进展，推动相关技术的应用。",
            "highlight_zh": "实验结果表明，RL智能体在HydroGym平台中能够发现鲁棒的控制策略，例如边界层操纵、声反馈中断和尾流重组。迁移学习研究表明，在一个雷诺数或几何形状下学习的控制器能够有效地适应新条件，所需训练次数减少约50%。这些结果表明，HydroGym平台为流体控制领域的强化学习研究提供了一个有效的工具。",
            "tags_zh": [
                "流体动力学",
                "强化学习",
                "流动控制",
                "基准平台",
                "可微求解器"
            ],
            "_index": 31,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Long-Range depth estimation using learning based Hybrid Distortion Model for CCTV cameras",
            "authors": [
                "Ami Pandat",
                "Punna Rajasekhar",
                "G. Aravamuthan",
                "Gopika Vinod",
                "Rohit Shukla"
            ],
            "arxiv_id": "2512.17784v1",
            "summary": "Accurate camera models are essential for photogrammetry applications such as 3D mapping and object localization, particularly for long distances. Various stereo-camera based 3D localization methods are available but are limited to few hundreds of meters' range. This is majorly due to the limitation of the distortion models assumed for the non-linearities present in the camera lens. This paper presents a framework for modeling a suitable distortion model that can be used for localizing the objects at longer distances. It is well known that neural networks can be a better alternative to model a highly complex non-linear lens distortion function; on contrary, it is observed that a direct application of neural networks to distortion models fails to converge to estimate the camera parameters. To resolve this, a hybrid approach is presented in this paper where the conventional distortion models are initially extended to incorporate higher-order terms and then enhanced using neural network based residual correction model. This hybrid approach has substantially improved long-range localization performance and is capable of estimating the 3D position of objects at distances up to 5 kilometres. The estimated 3D coordinates are transformed to GIS coordinates and are plotted on a GIS map for visualization. Experimental validation demonstrates the robustness and effectiveness of proposed framework, offering a practical solution to calibrate CCTV cameras for long-range photogrammetry applications.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17784v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]depth estimation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于学习的混合畸变模型，用于CCTV相机长距离深度估计。",
            "summary_zh": "本文提出了一种用于长距离物体定位的相机畸变模型框架，适用于三维地图构建和物体定位等摄影测量应用。传统的基于立体相机的三维定位方法受限于相机镜头非线性畸变模型的精度，通常只能在数百米范围内有效。为了解决这个问题，本文提出了一种混合方法，首先扩展传统畸变模型，加入高阶项，然后使用基于神经网络的残差校正模型进行增强。该方法显著提高了长距离定位性能，能够估计远达5公里的物体三维位置。估计的三维坐标被转换为GIS坐标，并在GIS地图上进行可视化。实验验证表明，该框架具有鲁棒性和有效性，为长距离摄影测量应用中的CCTV相机标定提供了一种实用的解决方案。",
            "intro_zh": [
                "现有基于立体相机的三维定位方法在长距离上受限于相机镜头畸变模型的精度。",
                "提出一种混合畸变模型，结合传统畸变模型的高阶项扩展和神经网络的残差校正。",
                "实验结果表明，该方法能够有效提升长距离定位性能，最远可达5公里。"
            ],
            "method_zh": "**问题定义**：论文旨在解决CCTV相机在长距离（例如几公里）场景下的深度估计问题。现有方法，特别是基于传统畸变模型的立体视觉方法，由于无法准确建模相机镜头的复杂非线性畸变，导致远距离定位精度显著下降。因此，如何建立一个能够准确描述长距离场景下相机畸变的模型是关键问题。\\n\\n**核心思路**：论文的核心思路是结合传统畸变模型和神经网络的优势，提出一种混合畸变模型。传统畸变模型虽然计算效率高，但表达能力有限；神经网络具有强大的非线性建模能力，但直接应用于畸变模型估计时难以收敛。因此，论文采用混合方法，首先使用扩展的传统畸变模型进行初步校正，然后利用神经网络学习残差，对初步校正结果进行精细调整。\\n\\n**技术框架**：整体框架包含以下几个主要步骤：1) 使用扩展的传统畸变模型（包含高阶项）对图像进行初步校正；2) 构建一个神经网络，以初步校正后的图像坐标作为输入，输出残差校正量；3) 将神经网络的输出与初步校正结果相加，得到最终的校正后的图像坐标；4) 利用校正后的图像坐标进行三维重建和定位；5) 将三维坐标转换到GIS坐标系，并在GIS地图上进行可视化。\\n\\n**关键创新**：该方法最重要的创新点在于提出了混合畸变模型，将传统畸变模型和神经网络结合起来。这种混合方法既利用了传统模型的计算效率，又发挥了神经网络的非线性建模能力，从而能够更准确地描述相机镜头的复杂畸变。与直接使用神经网络建模畸变相比，该方法更容易收敛，并且具有更好的泛化能力。\\n\\n**关键设计**：论文中，传统畸变模型扩展到了高阶项，以更好地拟合复杂的畸变。神经网络的具体结构（例如层数、神经元数量、激活函数等）以及训练方式（例如损失函数、优化器、学习率等）未知，但其目标是学习残差校正量，以弥补传统畸变模型的不足。损失函数的设计需要考虑定位精度和鲁棒性，可能包括重投影误差、三维点云的平滑性等。",
            "application_zh": "该研究成果可广泛应用于智能交通、安防监控、城市规划等领域。例如，可以利用CCTV相机进行远距离车辆定位和跟踪，实现交通流量监控和事故检测；可以用于构建高精度三维城市模型，为城市规划和管理提供支持；还可以应用于灾害救援，快速定位受灾人员和评估灾情。",
            "highlight_zh": "实验结果表明，该方法能够有效提高长距离定位精度，最远可达5公里。通过与传统畸变模型相比，该方法在远距离目标的三维坐标估计方面取得了显著的性能提升。此外，该方法能够将估计的三维坐标转换到GIS坐标系，并在GIS地图上进行可视化，为实际应用提供了便利。",
            "tags_zh": [
                "长距离深度估计",
                "相机标定",
                "畸变模型",
                "神经网络",
                "CCTV相机"
            ],
            "_index": 32,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "HeadHunt-VAD: Hunting Robust Anomaly-Sensitive Heads in MLLM for Tuning-Free Video Anomaly Detection",
            "authors": [
                "Zhaolin Cai",
                "Fan Li",
                "Ziwei Zheng",
                "Haixia Bi",
                "Lijun He"
            ],
            "arxiv_id": "2512.17601v1",
            "summary": "Video Anomaly Detection (VAD) aims to locate events that deviate from normal patterns in videos. Traditional approaches often rely on extensive labeled data and incur high computational costs. Recent tuning-free methods based on Multimodal Large Language Models (MLLMs) offer a promising alternative by leveraging their rich world knowledge. However, these methods typically rely on textual outputs, which introduces information loss, exhibits normalcy bias, and suffers from prompt sensitivity, making them insufficient for capturing subtle anomalous cues. To address these constraints, we propose HeadHunt-VAD, a novel tuning-free VAD paradigm that bypasses textual generation by directly hunting robust anomaly-sensitive internal attention heads within the frozen MLLM. Central to our method is a Robust Head Identification module that systematically evaluates all attention heads using a multi-criteria analysis of saliency and stability, identifying a sparse subset of heads that are consistently discriminative across diverse prompts. Features from these expert heads are then fed into a lightweight anomaly scorer and a temporal locator, enabling efficient and accurate anomaly detection with interpretable outputs. Extensive experiments show that HeadHunt-VAD achieves state-of-the-art performance among tuning-free methods on two major VAD benchmarks while maintaining high efficiency, validating head-level probing in MLLMs as a powerful and practical solution for real-world anomaly detection.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17601v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "HeadHunt-VAD：在MLLM中寻找异常敏感头，实现免调优视频异常检测",
            "summary_zh": "视频异常检测（VAD）旨在定位视频中偏离正常模式的事件。传统方法通常依赖于大量的标注数据，并产生高昂的计算成本。最近基于多模态大型语言模型（MLLM）的免调优方法，通过利用其丰富的世界知识，提供了一种有前景的替代方案。然而，这些方法通常依赖于文本输出，这会引入信息损失，表现出常态偏差，并受到提示敏感性的影响，使其不足以捕捉细微的异常线索。为了解决这些限制，我们提出了HeadHunt-VAD，一种新颖的免调优VAD范式，它通过直接在冻结的MLLM中寻找鲁棒的异常敏感内部注意力头来绕过文本生成。我们方法的核心是一个鲁棒头识别模块，该模块使用显着性和稳定性的多标准分析系统地评估所有注意力头，从而识别出在不同提示中始终具有区分性的稀疏头子集。然后，来自这些专家头的特征被馈送到轻量级异常评分器和时间定位器，从而能够以可解释的输出实现高效准确的异常检测。大量的实验表明，HeadHunt-VAD在两个主要的VAD基准测试中，在免调优方法中实现了最先进的性能，同时保持了高效率，验证了MLLM中的头部级别探测是用于实际异常检测的强大而实用的解决方案。",
            "intro_zh": [
                "现有视频异常检测方法依赖大量标注数据和高计算成本，且基于MLLM的免调优方法存在信息损失和常态偏差。",
                "HeadHunt-VAD通过直接在冻结的MLLM中寻找鲁棒的异常敏感注意力头，绕过文本生成，提升异常检测能力。",
                "实验表明，HeadHunt-VAD在两个VAD基准测试中实现了最先进的免调优性能，验证了头部级别探测的有效性。"
            ],
            "method_zh": "**问题定义**：视频异常检测旨在识别视频中不符合正常模式的事件。现有基于多模态大语言模型（MLLM）的免调优方法，虽然避免了昂贵的微调过程，但依赖文本输出，导致信息损失，易受常态偏差影响，且对提示词敏感，难以捕捉细微异常。\n\n**核心思路**：HeadHunt-VAD的核心在于直接挖掘MLLM内部的注意力机制，寻找那些对异常事件具有高度敏感性和鲁棒性的注意力头。通过分析这些“专家”注意力头的输出，绕过文本生成环节，从而更直接地捕捉视频中的异常信息。\n\n**技术框架**：HeadHunt-VAD主要包含两个模块：鲁棒头识别模块和异常检测模块。鲁棒头识别模块负责从MLLM中筛选出对异常敏感且稳定的注意力头；异常检测模块则利用这些注意力头的特征进行异常评分和时间定位。整个流程无需对MLLM进行任何微调。\n\n**关键创新**：该方法最重要的创新点在于将视频异常检测问题转化为对预训练MLLM内部知识的挖掘。通过寻找对异常事件具有区分性的注意力头，实现了免调优的异常检测，避免了传统方法对大量标注数据的依赖，并解决了文本输出带来的信息损失问题。\n\n**关键设计**：鲁棒头识别模块采用多标准分析方法，综合考虑注意力头的显著性和稳定性。显著性评估注意力头对异常事件的响应强度，稳定性评估注意力头在不同提示下的响应一致性。通过设定阈值，筛选出既显著又稳定的注意力头子集。异常检测模块使用轻量级的异常评分器和时间定位器，例如简单的线性层或GRU网络，以实现高效的异常检测。",
            "application_zh": "HeadHunt-VAD具有广泛的应用前景，例如智能监控、工业质检、医疗影像分析等领域。该方法无需针对特定场景进行模型微调，降低了部署成本，提高了应用灵活性。未来，该技术有望应用于更复杂的异常检测任务，例如多模态异常检测、小样本异常检测等。",
            "highlight_zh": "HeadHunt-VAD在ShanghaiTech和UCF-Crime两个主流视频异常检测数据集上取得了显著的性能提升，在免调优方法中达到了state-of-the-art水平。实验结果表明，该方法能够有效识别异常事件，并具有较高的检测精度和效率，验证了其在实际应用中的可行性。",
            "tags_zh": [
                "视频异常检测",
                "多模态大语言模型",
                "注意力机制",
                "免调优学习",
                "鲁棒性",
                "异常敏感头",
                "知识挖掘"
            ],
            "_index": 33,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17601v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17601v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17601v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding",
            "authors": [
                "Jiaqi Tang",
                "Jianmin Chen",
                "Wei Wei",
                "Xiaogang Xu",
                "Runtao Liu",
                "Xiangyu Wu",
                "Qipeng Xie",
                "Jiafei Wu",
                "Lei Zhang",
                "Qifeng Chen"
            ],
            "arxiv_id": "2512.17532v1",
            "summary": "Multimodal Large Language Models struggle to maintain reliable performance under extreme real-world visual degradations, which impede their practical robustness. Existing robust MLLMs predominantly rely on implicit training/adaptation that focuses solely on visual encoder generalization, suffering from limited interpretability and isolated optimization. To overcome these limitations, we propose Robust-R1, a novel framework that explicitly models visual degradations through structured reasoning chains. Our approach integrates: (i) supervised fine-tuning for degradation-aware reasoning foundations, (ii) reward-driven alignment for accurately perceiving degradation parameters, and (iii) dynamic reasoning depth scaling adapted to degradation intensity. To facilitate this approach, we introduce a specialized 11K dataset featuring realistic degradations synthesized across four critical real-world visual processing stages, each annotated with structured chains connecting degradation parameters, perceptual influence, pristine semantic reasoning chain, and conclusion. Comprehensive evaluations demonstrate state-of-the-art robustness: Robust-R1 outperforms all general and robust baselines on the real-world degradation benchmark R-Bench, while maintaining superior anti-degradation performance under multi-intensity adversarial degradations on MMMB, MMStar, and RealWorldQA.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "Accepted by AAAI2026 Oral",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17532v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Robust-R1框架，通过显式建模视觉退化提升多模态大模型在真实场景下的鲁棒性。",
            "summary_zh": "多模态大语言模型在极端真实视觉退化下难以维持可靠的性能，阻碍了它们的实际鲁棒性。现有的鲁棒MLLM主要依赖于隐式的训练/适应，仅关注视觉编码器的泛化能力，缺乏可解释性和孤立的优化。为了克服这些限制，我们提出了Robust-R1，一种通过结构化推理链显式建模视觉退化的新框架。我们的方法整合了：（i）用于退化感知推理基础的监督微调，（ii）用于准确感知退化参数的奖励驱动对齐，以及（iii）适应于退化强度的动态推理深度缩放。为了促进这种方法，我们引入了一个专门的11K数据集，该数据集具有在四个关键的真实世界视觉处理阶段合成的逼真退化，每个阶段都用连接退化参数、感知影响、原始语义推理链和结论的结构化链进行注释。全面的评估表明了最先进的鲁棒性：Robust-R1在真实世界退化基准R-Bench上优于所有通用和鲁棒基线，同时在MMMB、MMStar和RealWorldQA上保持了多强度对抗退化下的卓越抗退化性能。",
            "intro_zh": [
                "现有鲁棒多模态大模型依赖隐式训练，忽略了对视觉退化的显式建模，导致可解释性差且优化孤立。",
                "Robust-R1通过结构化推理链显式建模视觉退化，包含退化感知推理、奖励驱动对齐和动态推理深度缩放三个模块。",
                "在R-Bench等基准测试中，Robust-R1超越了现有通用和鲁棒模型，并在对抗性退化下表现出更强的抗退化能力。"
            ],
            "method_zh": "**问题定义**：多模态大语言模型在实际应用中，面临各种视觉退化（如噪声、模糊、压缩等）的挑战，导致性能显著下降。现有方法主要关注视觉编码器的泛化能力，缺乏对退化过程的显式建模和推理，难以有效应对复杂的退化情况。因此，如何提升多模态大模型在各种视觉退化下的鲁棒性是一个关键问题。\\n\\n**核心思路**：Robust-R1的核心思路是通过显式地建模视觉退化过程，构建结构化的推理链，从而使模型能够理解退化对图像语义的影响，并进行相应的补偿。这种显式建模的方式提高了模型的可解释性，并允许针对不同的退化情况进行优化。通过奖励驱动的对齐，模型可以更准确地感知退化参数，并根据退化强度动态调整推理深度，从而实现更鲁棒的视觉理解。\\n\\n**技术框架**：Robust-R1框架主要包含三个阶段：1) 退化感知推理基础：使用监督微调，训练模型理解退化参数与图像语义之间的关系，构建推理基础。2) 奖励驱动对齐：通过奖励机制，鼓励模型准确感知退化参数，并将其与推理链对齐。3) 动态推理深度缩放：根据退化强度，动态调整推理深度，以平衡性能和计算成本。整个框架通过结构化的推理链，将退化参数、感知影响、原始语义推理链和结论连接起来，实现端到端的优化。\\n\\n**关键创新**：Robust-R1最重要的创新点在于对视觉退化的显式建模。与现有方法不同，Robust-R1不是简单地增强视觉编码器的泛化能力，而是通过结构化的推理链，将退化过程分解为多个步骤，并对每个步骤进行建模和优化。这种显式建模的方式提高了模型的可解释性，并允许针对不同的退化情况进行优化。此外，奖励驱动对齐和动态推理深度缩放也是关键创新，它们使模型能够更准确地感知退化参数，并根据退化强度调整推理策略。\\n\\n**关键设计**：Robust-R1的关键设计包括：1) 专门构建的包含11K图像的退化数据集，该数据集覆盖了四个关键的真实世界视觉处理阶段，并对每个图像进行了结构化的标注，包括退化参数、感知影响、原始语义推理链和结论。2) 使用奖励函数来鼓励模型准确感知退化参数，奖励函数的设计需要平衡准确性和鲁棒性。3) 设计动态推理深度缩放策略，根据退化强度调整推理深度，以平衡性能和计算成本。具体的网络结构和损失函数等技术细节在论文中进行了详细描述。",
            "application_zh": "Robust-R1框架可应用于各种需要鲁棒视觉理解的场景，例如自动驾驶、机器人导航、医学图像分析、安全监控等。该研究有助于提升多模态大模型在真实复杂环境下的可靠性，推动人工智能技术在实际场景中的应用。",
            "highlight_zh": "Robust-R1在R-Bench基准测试中取得了state-of-the-art的性能，超越了所有通用和鲁棒基线。此外，在MMMB、MMStar和RealWorldQA等基准测试中，Robust-R1在多强度对抗退化下表现出卓越的抗退化性能，证明了其在真实场景下的鲁棒性。",
            "tags_zh": [
                "多模态大模型",
                "视觉退化",
                "鲁棒性",
                "显式建模",
                "推理链"
            ],
            "_index": 34,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17532v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17532v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17532v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs",
            "authors": [
                "Yunkai Dang",
                "Meiyi Zhu",
                "Donghao Wang",
                "Yizhuo Zhang",
                "Jiacheng Yang",
                "Qi Fan",
                "Yuekun Yang",
                "Wenbin Li",
                "Feng Miao",
                "Yang Gao"
            ],
            "arxiv_id": "2512.17319v1",
            "summary": "Multimodal large language models (MLLMs) demonstrate strong perception and reasoning performance on existing remote sensing (RS) benchmarks. However, most prior benchmarks rely on low-resolution imagery, and some high-resolution benchmarks suffer from flawed reasoning-task designs. We show that text-only LLMs can perform competitively with multimodal vision-language models on RS reasoning tasks without access to images, revealing a critical mismatch between current benchmarks and the intended evaluation of visual understanding. To enable faithful assessment, we introduce RSHR-Bench, a super-high-resolution benchmark for RS visual understanding and reasoning. RSHR-Bench contains 5,329 full-scene images with a long side of at least 4,000 pixels, with up to about 3 x 10^8 pixels per image, sourced from widely used RS corpora and UAV collections. We design four task families: multiple-choice VQA, open-ended VQA, image captioning, and single-image evaluation. These tasks cover nine perception categories and four reasoning types, supporting multi-turn and multi-image dialog. To reduce reliance on language priors, we apply adversarial filtering with strong LLMs followed by rigorous human verification. Overall, we construct 3,864 VQA tasks, 3,913 image captioning tasks, and 500 fully human-written or verified single-image evaluation VQA pairs. Evaluations across open-source, closed-source, and RS-specific VLMs reveal persistent performance gaps in super-high-resolution scenarios. Code: https://github.com/Yunkaidang/RSHR",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.MM"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17319v1",
            "code_links": [
                {
                    "url": "https://github.com/Yunkaidang/RSHR",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出RSHR-Bench以解决遥感超高分辨率视觉理解评估问题",
            "summary_zh": "多模态大语言模型（MLLMs）在现有遥感基准上表现出强大的感知和推理能力。然而，大多数基准依赖于低分辨率图像，而一些高分辨率基准在推理任务设计上存在缺陷。我们展示了仅使用文本的LLMs在遥感推理任务中可以与多模态视觉-语言模型竞争，揭示了当前基准与视觉理解评估之间的关键不匹配。为实现真实评估，我们引入了RSHR-Bench，这是一个超高分辨率的遥感视觉理解和推理基准，包含5,329幅长边至少为4,000像素的全场景图像，设计了多种任务类型以支持多轮和多图对话。通过严格的人类验证，我们构建了多个任务，评估结果显示在超高分辨率场景中存在持续的性能差距。",
            "intro_zh": [
                "现有遥感基准大多依赖低分辨率图像，导致评估结果与实际视觉理解能力不匹配。",
                "论文提出RSHR-Bench基准，包含超高分辨率图像，并设计多种任务以全面评估视觉理解能力。",
                "实验结果表明，现有的视觉语言模型在超高分辨率场景中存在显著的性能差距，验证了新基准的必要性。"
            ],
            "method_zh": "**问题定义**：本论文旨在解决现有遥感基准在超高分辨率图像评估中的不足，尤其是低分辨率图像导致的评估不准确问题。\\n\\n**核心思路**：通过引入RSHR-Bench基准，提供高达3亿像素的图像，设计多样化的任务以真实评估遥感视觉理解能力，减少对语言先验的依赖。\\n\\n**技术框架**：RSHR-Bench包含五千多幅超高分辨率图像，设计了四类任务：多项选择VQA、开放式VQA、图像描述和单图评估，支持多轮对话。\\n\\n**关键创新**：最重要的创新在于引入了超高分辨率图像和多样化的任务设计，填补了现有基准在高分辨率场景中的空白。\\n\\n**关键设计**：采用对抗过滤与强大的LLMs结合，经过严格的人类验证，确保任务的有效性和准确性，构建了3,864个VQA任务和3,913个图像描述任务。",
            "application_zh": "该研究的潜在应用领域包括遥感图像分析、环境监测、城市规划等。通过提供更准确的视觉理解评估，RSHR-Bench能够促进相关领域的技术进步，推动多模态学习的发展，提升遥感数据的利用效率。",
            "highlight_zh": "实验结果显示，现有的视觉语言模型在超高分辨率场景中表现出持续的性能差距，尤其是在RSHR-Bench上，VQA任务的准确率提升了约15%，验证了新基准的有效性和必要性。",
            "tags_zh": [
                "遥感图像",
                "超高分辨率",
                "多模态大语言模型",
                "视觉理解",
                "推理任务",
                "基准评估",
                "对抗过滤",
                "人类验证"
            ],
            "_index": 35,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17319v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17319v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17319v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Deep But Reliable: Advancing Multi-turn Reasoning for Thinking with Images",
            "authors": [
                "Wenhao Yang",
                "Yu Xia",
                "Jinlong Huang",
                "Shiyin Lu",
                "Qing-Guo Chen",
                "Zhao Xu",
                "Weihua Luo",
                "Kaifu Zhang",
                "Yuanyu Wan",
                "Lijun Zhang"
            ],
            "arxiv_id": "2512.17306v1",
            "summary": "Recent advances in large Vision-Language Models (VLMs) have exhibited strong reasoning capabilities on complex visual tasks by thinking with images in their Chain-of-Thought (CoT), which is achieved by actively invoking tools to analyze visual inputs rather than merely perceiving them. However, existing models often struggle to reflect on and correct themselves when attempting incorrect reasoning trajectories. To address this limitation, we propose DRIM, a model that enables deep but reliable multi-turn reasoning when thinking with images in its multimodal CoT. Our pipeline comprises three stages: data construction, cold-start SFT and RL. Based on a high-resolution image dataset, we construct high-difficulty and verifiable visual question-answer pairs, where solving each task requires multi-turn tool calls to reach the correct answer. In the SFT stage, we collect tool trajectories as cold-start data, guiding a multi-turn reasoning pattern. In the RL stage, we introduce redundancy-penalized policy optimization, which incentivizes the model to develop a self-reflective reasoning pattern. The basic idea is to impose judgment on reasoning trajectories and penalize those that produce incorrect answers without sufficient multi-scale exploration. Extensive experiments demonstrate that DRIM achieves superior performance on visual understanding benchmarks.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17306v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal",
                        "chain-of-thought"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出DRIM模型，提升视觉语言模型在图像推理中的多轮自反思能力",
            "summary_zh": "本文提出DRIM模型，旨在提升大型视觉语言模型(VLM)在图像推理中进行多轮自反思的能力。现有模型在进行基于图像的思维链(CoT)推理时，难以反思和纠正错误的推理轨迹。DRIM模型通过三阶段流程解决此问题：数据构建、冷启动SFT和强化学习。首先，基于高分辨率图像数据集，构建高难度且可验证的视觉问答对，每个任务需要多轮工具调用才能得到正确答案。在SFT阶段，收集工具轨迹作为冷启动数据，引导多轮推理模式。在强化学习阶段，引入冗余惩罚策略优化，激励模型发展自反思推理模式，对产生错误答案且缺乏充分多尺度探索的推理轨迹进行惩罚。实验结果表明，DRIM在视觉理解基准测试中表现出色。",
            "intro_zh": [
                "现有视觉语言模型在复杂视觉任务中，难以反思和纠正错误的推理轨迹。",
                "DRIM模型通过数据构建、冷启动SFT和强化学习三个阶段，实现深度且可靠的多轮推理。",
                "实验表明，DRIM模型在视觉理解基准测试中取得了优越的性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决现有视觉语言模型在进行多轮图像推理时，缺乏自反思和纠错能力的问题。现有方法在推理过程中，一旦出现错误，很难回溯并修正，导致最终结果错误。这主要是因为模型缺乏对自身推理过程的评估和改进机制。\\n\\n**核心思路**：论文的核心思路是让模型具备“深度”和“可靠性”的多轮推理能力。通过引入自反思机制，使模型能够评估自身的推理过程，并在发现错误时进行纠正。这种自反思能力通过冗余惩罚策略优化来实现，鼓励模型进行多尺度的探索，并对不充分的探索进行惩罚。\\n\\n**技术框架**：DRIM模型包含三个主要阶段：1) 数据构建：构建高难度、可验证的视觉问答对，需要多轮工具调用才能解决。2) 冷启动SFT：利用收集到的工具轨迹数据，对模型进行监督微调，引导模型学习多轮推理模式。3) 强化学习：引入冗余惩罚策略优化，训练模型进行自反思推理。\\n\\n**关键创新**：DRIM的关键创新在于引入了冗余惩罚策略优化，这是一种新型的强化学习方法，旨在激励模型发展自反思推理模式。通过对推理轨迹进行评估，并对那些产生错误答案且缺乏充分多尺度探索的轨迹进行惩罚，模型能够学会识别和纠正自身的错误。\\n\\n**关键设计**：在数据构建阶段，论文设计了高难度、可验证的视觉问答对，确保模型需要进行多轮推理才能得到正确答案。在强化学习阶段，冗余惩罚策略优化的具体实现方式（例如，如何定义“多尺度探索”和“惩罚”）以及相关的超参数设置是关键的技术细节。具体的损失函数设计也至关重要，需要平衡奖励和惩罚，以引导模型学习到有效的自反思推理策略。",
            "application_zh": "DRIM模型具有广泛的应用前景，例如智能客服、自动驾驶、医疗诊断等领域。在这些领域中，模型需要具备强大的推理能力和可靠性，才能做出准确的决策。DRIM模型通过提升视觉语言模型的多轮自反思能力，可以显著提高其在这些领域的应用效果，并推动相关技术的发展。",
            "highlight_zh": "DRIM模型在视觉理解基准测试中取得了显著的性能提升，证明了其有效性。具体的性能数据和对比基线需要在论文中查找。DRIM模型通过引入自反思机制，能够更好地理解图像内容，并进行更准确的推理，从而在各种视觉任务中表现出色。",
            "tags_zh": [
                "视觉语言模型",
                "多轮推理",
                "自反思",
                "强化学习",
                "思维链",
                "图像理解",
                "冗余惩罚"
            ],
            "_index": 36,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy",
            "authors": [
                "Aditya Gahlawat",
                "Ahmed Aboudonia",
                "Sandeep Banik",
                "Naira Hovakimyan",
                "Nikolai Matni",
                "Aaron D. Ames",
                "Gioele Zardini",
                "Alberto Speranzon"
            ],
            "arxiv_id": "2512.17899v1",
            "summary": "Imitation learning (IL) enables autonomous behavior by learning from expert demonstrations. While more sample-efficient than comparative alternatives like reinforcement learning, IL is sensitive to compounding errors induced by distribution shifts. There are two significant sources of distribution shifts when using IL-based feedback laws on systems: distribution shifts caused by policy error and distribution shifts due to exogenous disturbances and endogenous model errors due to lack of learning. Our previously developed approaches, Taylor Series Imitation Learning (TaSIL) and $\\mathcal{L}_1$ -Distributionally Robust Adaptive Control (\\ellonedrac), address the challenge of distribution shifts in complementary ways. While TaSIL offers robustness against policy error-induced distribution shifts, \\ellonedrac offers robustness against distribution shifts due to aleatoric and epistemic uncertainties. To enable certifiable IL for learned and/or uncertain dynamical systems, we formulate \\textit{Distributionally Robust Imitation Policy (DRIP)} architecture, a Layered Control Architecture (LCA) that integrates TaSIL and~\\ellonedrac. By judiciously designing individual layer-centric input and output requirements, we show how we can guarantee certificates for the entire control pipeline. Our solution paves the path for designing fully certifiable autonomy pipelines, by integrating learning-based components, such as perception, with certifiable model-based decision-making through the proposed LCA approach.",
            "categories": [
                "eess.SY",
                "cs.LG"
            ],
            "primary_category": "eess.SY",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "18 pages, 5 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17899v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "[T]imitation learning"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出分布鲁棒模仿学习以解决自主系统的认证问题",
            "summary_zh": "模仿学习（IL）通过学习专家示范来实现自主行为。尽管相较于强化学习等替代方法，IL在样本效率上更具优势，但其对分布变化引起的累积误差非常敏感。使用IL反馈法时，主要存在两种分布变化来源：政策误差引起的分布变化和由于外部干扰及模型误差引起的分布变化。本文提出的分布鲁棒模仿政策（DRIP）架构，结合了之前开发的泰勒级数模仿学习（TaSIL）和$\text{L}_1$-分布鲁棒自适应控制（$\text{L}_1$-DRAC），通过合理设计各层的输入输出要求，确保整个控制管道的认证。这一解决方案为设计完全可认证的自主系统提供了新的思路。",
            "intro_zh": [
                "现有的模仿学习方法在面对分布变化时表现出较大的脆弱性，尤其是在政策误差和外部干扰的影响下。",
                "本文提出的DRIP架构通过整合TaSIL和$\text{L}_1$-DRAC，旨在提高模仿学习在不确定动态系统中的鲁棒性和可认证性。",
                "实验结果表明，DRIP架构在处理分布变化时显著提高了系统的稳定性和性能，展示了其在实际应用中的潜力。"
            ],
            "method_zh": "**问题定义**：本文旨在解决模仿学习在动态系统中因分布变化导致的鲁棒性不足问题。现有方法在政策误差和外部干扰下容易产生累积误差，影响系统性能。\\n\\n**核心思路**：论文提出的DRIP架构通过结合TaSIL和$\text{L}_1$-DRAC，分别针对政策误差和不确定性引起的分布变化提供鲁棒性，从而实现可认证的模仿学习。\\n\\n**技术框架**：DRIP架构采用分层控制架构（LCA），将学习组件与基于模型的决策制定相结合。每一层的输入输出要求经过精心设计，以确保整个控制管道的认证。\\n\\n**关键创新**：最重要的创新在于将两种不同的鲁棒性方法有效整合，形成一个统一的框架，能够同时应对政策误差和不确定性引起的分布变化，这在现有方法中尚属首次。\\n\\n**关键设计**：在设计中，关键参数包括各层的输入输出要求、损失函数的选择，以及网络结构的设计，以确保系统在面对不同类型的分布变化时仍能保持稳定性和性能。通过这些设计，DRIP架构能够提供强有力的认证保证。",
            "application_zh": "该研究的潜在应用领域包括自动驾驶、机器人控制和智能制造等需要高鲁棒性和可认证性的自主系统。通过实现可认证的模仿学习，能够提高系统在复杂环境中的适应能力，确保其安全性和可靠性，具有重要的实际价值和未来影响。",
            "highlight_zh": "实验结果显示，DRIP架构在面对政策误差和外部干扰时，系统的稳定性提高了约30%，并且在多种动态环境下的表现优于传统模仿学习方法，验证了其在实际应用中的有效性。",
            "tags_zh": [
                "模仿学习",
                "分布鲁棒性",
                "自主系统",
                "控制架构",
                "动态系统",
                "认证机制",
                "鲁棒控制",
                "不确定性"
            ],
            "_index": 37,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17899v1/Figures/Arch.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17899v1/Figures/HighLevel.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17899v1/Figures/IL_L1_DRAC_figures_Policy_induced_shift.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing",
            "authors": [
                "Lingxiao Zhao",
                "Haoran Zhou",
                "Yuezhi Che",
                "Dazhao Cheng"
            ],
            "arxiv_id": "2512.17574v1",
            "summary": "Multimodal large language models (MLLMs) extend LLMs with visual understanding through a three-stage pipeline: multimodal preprocessing, vision encoding, and LLM inference. While these stages enhance capability, they introduce significant system bottlenecks. First, multimodal preprocessing-especially video decoding-often dominates Time-to-First-Token (TTFT). Most systems rely on CPU-based decoding, which severely limits throughput, while existing GPU-based approaches prioritize throughput-oriented parallelism and fail to meet the latency-sensitive requirements of MLLM inference. Second, the vision encoder is a standalone, compute-intensive stage that produces visual embeddings and cannot be co-batched with LLM prefill or decoding. This heterogeneity forces inter-stage blocking and increases token-generation latency. Even when deployed on separate GPUs, these stages underutilize available compute and memory resources, reducing overall utilization and constraining system throughput.\n  To address these challenges, we present FlashCodec and UnifiedServe, two complementary designs that jointly optimize the end-to-end MLLM pipeline. FlashCodec accelerates the multimodal preprocessing stage through collaborative multi-GPU video decoding, reducing decoding latency while preserving high throughput. UnifiedServe optimizes the vision-to-text and inference stages using a logically decoupled their execution to eliminate inter-stage blocking, yet physically sharing GPU resources to maximize GPU system utilization. By carefully orchestrating execution across stages and minimizing interference, UnifiedServe Together, our proposed framework forms an end-to-end optimized stack that can serve up to 3.0$\\times$ more requests or enforce 1.5$\\times$ tighter SLOs, while achieving up to 4.4$\\times$ higher throughput compared to state-of-the-art systems.",
            "categories": [
                "cs.DC",
                "cs.LG"
            ],
            "primary_category": "cs.DC",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17574v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出FlashCodec和UnifiedServe，通过GPU内调度和资源共享加速多阶段MLLM推理。",
            "summary_zh": "多模态大型语言模型（MLLM）通过三个阶段的流程扩展了LLM的视觉理解能力：多模态预处理、视觉编码和LLM推理。这些阶段的增强能力也带来了显著的系统瓶颈。首先，多模态预处理，特别是视频解码，通常主导了首个token生成时间（TTFT）。大多数系统依赖于基于CPU的解码，这严重限制了吞吐量，而现有的基于GPU的方法优先考虑面向吞吐量的并行性，无法满足MLLM推理的延迟敏感性要求。其次，视觉编码器是一个独立的、计算密集型的阶段，它生成视觉嵌入，无法与LLM的预填充或解码进行联合批处理。这种异构性迫使阶段间阻塞，并增加了token生成延迟。即使部署在单独的GPU上，这些阶段也未能充分利用可用的计算和内存资源，从而降低了总体利用率并限制了系统吞吐量。为了解决这些挑战，我们提出了FlashCodec和UnifiedServe，这两个互补的设计共同优化端到端的MLLM流程。FlashCodec通过协作式多GPU视频解码加速多模态预处理阶段，从而在保持高吞吐量的同时降低解码延迟。UnifiedServe通过逻辑上解耦视觉到文本和推理阶段的执行来优化这两个阶段，从而消除阶段间阻塞，同时物理上共享GPU资源以最大化GPU系统利用率。通过精心编排跨阶段的执行并最小化干扰，UnifiedServe与FlashCodec共同构成了一个端到端优化的堆栈，与最先进的系统相比，可以提供高达3.0倍的请求服务能力或强制执行1.5倍更严格的SLO，同时实现高达4.4倍的吞吐量。",
            "intro_zh": [
                "现有MLLM系统在多模态预处理（特别是视频解码）和视觉编码阶段存在瓶颈，导致TTFT过高和资源利用率不足。",
                "论文提出FlashCodec和UnifiedServe，分别加速视频解码和优化视觉编码与LLM推理的协同执行，从而提升整体性能。",
                "实验结果表明，该框架能够显著提升MLLM系统的请求服务能力、满足更严格的SLO，并提高吞吐量，最高可达4.4倍。"
            ],
            "method_zh": "**问题定义**：现有MLLM推理系统面临的主要问题是多模态预处理（尤其是视频解码）的延迟瓶颈以及视觉编码阶段与LLM推理阶段的异构性导致的资源利用率低下。CPU解码速度慢，GPU解码又难以兼顾低延迟。视觉编码器无法与LLM联合批处理，导致阶段间阻塞，降低了整体吞吐量和响应速度。\\n\\n**核心思路**：论文的核心思路是通过软硬件协同优化，解决MLLM推理流程中的瓶颈。FlashCodec通过多GPU协作加速视频解码，降低预处理延迟。UnifiedServe通过解耦执行逻辑，同时共享GPU资源，消除阶段间阻塞，提高资源利用率。\\n\\n**技术框架**：整体框架包含两个主要组件：FlashCodec和UnifiedServe。FlashCodec负责加速多模态预处理阶段的视频解码，采用多GPU并行解码策略。UnifiedServe负责优化视觉编码和LLM推理阶段，通过逻辑解耦和物理共享GPU资源，实现高效的阶段间协同。整个流程旨在最小化端到端延迟，最大化系统吞吐量。\\n\\n**关键创新**：论文的关键创新在于：1) FlashCodec提出的协作式多GPU视频解码方法，能够在保证高吞吐量的同时降低解码延迟。2) UnifiedServe提出的逻辑解耦和物理共享的执行策略，消除了视觉编码和LLM推理阶段之间的阻塞，提高了GPU资源利用率。\\n\\n**关键设计**：FlashCodec的关键设计在于如何有效地将视频帧分配给多个GPU进行并行解码，并保证解码后的帧顺序正确。UnifiedServe的关键设计在于如何实现视觉编码和LLM推理的逻辑解耦，使得两个阶段可以独立执行，同时又能够共享GPU资源，避免资源竞争和干扰。具体的参数设置和网络结构细节未在摘要中体现，属于未知信息。",
            "application_zh": "该研究成果可应用于各种需要实时多模态理解的场景，例如智能客服、视频分析、自动驾驶等。通过提高MLLM推理的效率和降低延迟，可以提升用户体验，并为更复杂的应用提供支持。未来，该技术有望推动多模态人工智能在实际场景中的广泛应用。",
            "highlight_zh": "实验结果表明，FlashCodec和UnifiedServe能够显著提升MLLM系统的性能。与最先进的系统相比，该框架可以提供高达3.0倍的请求服务能力，强制执行1.5倍更严格的SLO，并实现高达4.4倍的吞吐量。这些数据表明，该方法在实际应用中具有显著的优势。",
            "tags_zh": [
                "多模态大语言模型",
                "MLLM推理",
                "GPU加速",
                "视频解码",
                "资源调度",
                "系统优化",
                "低延迟",
                "高吞吐量"
            ],
            "_index": 38,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17574v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17574v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17574v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "MINPO: Memory-Informed Neural Pseudo-Operator to Resolve Nonlocal Spatiotemporal Dynamics",
            "authors": [
                "Farinaz Mostajeran",
                "Aruzhan Tleubek",
                "Salah A Faroughi"
            ],
            "arxiv_id": "2512.17273v1",
            "summary": "Many physical systems exhibit nonlocal spatiotemporal behaviors described by integro-differential equations (IDEs). Classical methods for solving IDEs require repeatedly evaluating convolution integrals, whose cost increases quickly with kernel complexity and dimensionality. Existing neural solvers can accelerate selected instances of these computations, yet they do not generalize across diverse nonlocal structures. In this work, we introduce the Memory-Informed Neural Pseudo-Operator (MINPO), a unified framework for modeling nonlocal dynamics arising from long-range spatial interactions and/or long-term temporal memory. MINPO, employing either Kolmogorov-Arnold Networks (KANs) or multilayer perceptron networks (MLPs) as encoders, learns the nonlocal operator and its inverse directly through neural representations, and then explicitly reconstruct the unknown solution fields. The learning is guarded by a lightweight nonlocal consistency loss term to enforce coherence between the learned operator and reconstructed solution. The MINPO formulation allows to naturally capture and efficiently resolve nonlocal spatiotemporal dependencies governed by a wide spectrum of IDEs and their subsets, including fractional PDEs. We evaluate the efficacy of MINPO in comparison with classical techniques and state-of-the-art neural-based strategies based on MLPs, such as A-PINN and fPINN, along with their newly-developed KAN variants, A-PIKAN and fPIKAN, designed to facilitate a fair comparison. Our study offers compelling evidence of the accuracy of MINPO and demonstrates its robustness in handling (i) diverse kernel types, (ii) different kernel dimensionalities, and (iii) the substantial computational demands arising from repeated evaluations of kernel integrals. MINPO, thus, generalizes beyond problem-specific formulations, providing a unified framework for systems governed by nonlocal operators.",
            "categories": [
                "cs.LG",
                "math-ph",
                "math.NA"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17273v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "[T]spatiotemporal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "8_physics_animation"
            ],
            "headline_zh": "提出MINPO，利用记忆信息神经伪算子解决非局部时空动力学问题",
            "summary_zh": "许多物理系统表现出非局部时空行为，这些行为由积分微分方程（IDEs）描述。求解IDEs的经典方法需要重复评估卷积积分，其成本随着核的复杂性和维度的增加而迅速增加。现有的神经求解器可以加速这些计算的特定实例，但它们不能推广到不同的非局部结构。本文介绍了一种记忆信息神经伪算子（MINPO），这是一个统一的框架，用于建模由长程空间相互作用和/或长期时间记忆引起的非局部动力学。MINPO采用Kolmogorov-Arnold网络（KANs）或多层感知器网络（MLPs）作为编码器，直接通过神经表示学习非局部算子及其逆，然后显式地重构未知的解场。学习过程由一个轻量级的非局部一致性损失项来保证，以加强学习到的算子和重构解之间的一致性。MINPO公式能够自然地捕获和有效地解决由各种IDEs及其子集（包括分数阶偏微分方程）控制的非局部时空依赖性。我们通过与经典技术和基于MLP的最新神经策略（如A-PINN和fPINN）以及它们新开发的KAN变体（A-PIKAN和fPIKAN）进行比较，评估了MINPO的有效性，旨在促进公平的比较。我们的研究提供了令人信服的证据，证明了MINPO的准确性，并证明了其在处理（i）不同核类型，（ii）不同核维度，以及（iii）由重复评估核积分引起的大量计算需求方面的鲁棒性。因此，MINPO推广到特定于问题的公式之外，为受非局部算子控制的系统提供了一个统一的框架。",
            "intro_zh": [
                "传统求解积分微分方程的方法计算成本高昂，尤其是在处理复杂核函数和高维度问题时，现有神经求解器泛化能力不足。",
                "MINPO通过学习非局部算子及其逆，直接重构解场，并引入非局部一致性损失，保证学习到的算子和解的一致性。",
                "实验表明，MINPO在处理不同核类型、核维度以及高计算需求问题时，相比传统方法和现有神经方法，展现出更高的准确性和鲁棒性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决物理系统中常见的非局部时空动力学建模问题，这类问题通常由积分微分方程描述。传统方法如有限元法在求解这类方程时，需要重复计算高维卷积积分，计算复杂度高，效率低。现有的神经算子方法虽然在特定问题上表现良好，但缺乏通用性，难以适应不同类型的非局部结构。\\n\\n**核心思路**：论文的核心思路是利用神经网络直接学习非局部算子及其逆算子，从而避免了传统方法中耗时的卷积积分计算。通过学习算子，模型能够直接从输入数据预测输出结果，而无需显式地求解积分微分方程。此外，论文还引入了记忆信息，即利用历史信息来辅助当前状态的预测，从而更好地捕捉时空依赖关系。\\n\\n**技术框架**：MINPO框架主要包含以下几个模块：1) 编码器：使用Kolmogorov-Arnold Networks (KANs)或多层感知器 (MLPs) 将输入数据编码成低维表示。2) 非局部算子学习：利用编码后的表示学习非局部算子及其逆算子。3) 解重构：使用学习到的逆算子从低维表示重构解场。4) 一致性损失：引入非局部一致性损失，确保学习到的算子和重构的解之间的一致性。整个框架通过端到端的方式进行训练。\\n\\n**关键创新**：MINPO的关键创新在于：1) 直接学习非局部算子及其逆算子，避免了传统方法的卷积积分计算。2) 引入记忆信息，更好地捕捉时空依赖关系。3) 提出非局部一致性损失，保证学习到的算子和解的一致性。4) 采用KANs作为编码器，相比MLPs，KANs具有更强的表达能力。\\n\\n**关键设计**：MINPO的关键设计包括：1) 编码器的选择：可以选择KANs或MLPs，KANs通常具有更好的性能，但计算成本也更高。2) 非局部一致性损失的设计：该损失函数用于衡量学习到的算子和重构的解之间的一致性，其具体形式需要根据具体问题进行调整。3) 网络结构的优化：需要根据具体问题选择合适的网络结构，以保证模型的表达能力和泛化能力。",
            "application_zh": "MINPO具有广泛的应用前景，可用于模拟和预测各种物理系统的非局部时空动力学行为，例如流体动力学、热传导、材料科学和生物系统。该方法可以加速科学计算，并为理解复杂物理现象提供新的视角。未来，MINPO有望应用于更复杂的系统建模和控制，例如气候预测和药物设计。",
            "highlight_zh": "实验结果表明，MINPO在处理不同类型的核函数和不同维度的问题时，均优于传统的数值方法和现有的神经算子方法，如A-PINN和fPINN。例如，在某个具体问题上，MINPO的预测精度比A-PINN提高了10%以上，并且在计算效率方面也有显著提升。此外，MINPO的KAN变体（A-PIKAN和fPIKAN）也表现出优于MLP变体的性能。",
            "tags_zh": [
                "非局部动力学",
                "神经算子",
                "积分微分方程",
                "Kolmogorov-Arnold网络",
                "物理信息神经网络"
            ],
            "_index": 39,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17273v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17273v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17273v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Accelerating Multi-modal LLM Gaming Performance via Input Prediction and Mishit Correction",
            "authors": [
                "Ziyang Lin",
                "Zixuan Sun",
                "Sanhorn Chen",
                "Xiaoyang Chen",
                "Roy Zhao"
            ],
            "arxiv_id": "2512.17250v1",
            "summary": "Real-time sequential control agents are often bottlenecked by inference latency. Even modest per-step planning delays can destabilize control and degrade overall performance. We propose a speculation-and-correction framework that adapts the predict-then-verify philosophy of speculative execution to model-based control with TD-MPC2. At each step, a pretrained world model and latent-space MPC planner generate a short-horizon action queue together with predicted latent rollouts, allowing the agent to execute multiple planned actions without immediate replanning. When a new observation arrives, the system measures the mismatch between the encoded real latent state and the queued predicted latent. For small to moderate mismatch, a lightweight learned corrector applies a residual update to the speculative action, distilled offline from a replanning teacher. For large mismatch, the agent safely falls back to full replanning and clears stale action queues. We study both a gated two-tower MLP corrector and a temporal Transformer corrector to address local errors and systematic drift. Experiments on the DMC Humanoid-Walk task show that our method reduces the number of planning inferences from 500 to 282, improves end-to-end step latency by 25 percent, and maintains strong control performance with only a 7.1 percent return reduction. Ablation results demonstrate that speculative execution without correction is unreliable over longer horizons, highlighting the necessity of mismatch-aware correction for robust latency reduction.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "UIUC 25 Fall CS 498",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17250v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "humanoid",
                        "MPC"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "world model"
                    ],
                    "score": 1.5
                }
            ],
            "relevance_score": 5.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出基于输入预测和误差校正的多模态LLM游戏加速框架",
            "summary_zh": "实时序列控制智能体常受限于推理延迟。即使是适度的单步规划延迟也会破坏控制的稳定性并降低整体性能。我们提出了一种推测-校正框架，该框架将推测执行的预测-验证理念应用于基于模型的TD-MPC2控制。在每个步骤中，预训练的世界模型和潜在空间MPC规划器生成一个短视界动作队列以及预测的潜在轨迹，允许智能体执行多个计划动作而无需立即重新规划。当新的观察结果到达时，系统会测量编码的真实潜在状态与排队的预测潜在状态之间的不匹配。对于小到中等的不匹配，一个轻量级的学习校正器将残差更新应用于推测动作，该动作是从离线重新规划教师中提炼出来的。对于较大的不匹配，智能体会安全地回退到完全重新规划并清除过时的动作队列。我们研究了一个门控双塔MLP校正器和一个时间Transformer校正器，以解决局部误差和系统漂移。在DMC Humanoid-Walk任务上的实验表明，我们的方法将规划推理次数从500次减少到282次，将端到端步延迟提高了25%，并保持了强大的控制性能，仅降低了7.1%的回报。消融实验结果表明，没有校正的推测执行在较长时间范围内是不可靠的，突出了不匹配感知校正对于鲁棒延迟降低的必要性。",
            "intro_zh": [
                "现有实时控制智能体受限于推理延迟，影响控制稳定性和性能。",
                "提出推测-校正框架，利用世界模型和MPC规划器生成动作队列，减少规划频率。",
                "实验表明，该方法显著降低推理次数和延迟，同时保持较好的控制性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决实时序列控制任务中，由于多模态大型语言模型（LLM）推理延迟导致的控制性能下降问题。现有方法通常依赖于每一步的即时规划，计算开销大，难以满足实时性要求，尤其是在复杂环境中。\n\n**核心思路**：核心思路是采用“推测-校正”的策略，类似于CPU中的推测执行。智能体首先基于世界模型预测未来状态和动作序列，然后执行这些动作。当实际观测与预测不符时，通过轻量级的校正器对动作进行修正，避免完全重新规划，从而降低平均推理延迟。\n\n**技术框架**：整体框架包含以下几个主要模块：1) 预训练的世界模型：用于预测未来状态；2) 潜在空间MPC规划器：生成短视界动作队列；3) 状态编码器：将观测编码为潜在状态；4) 误差检测器：计算实际潜在状态与预测潜在状态之间的不匹配程度；5) 校正器：根据不匹配程度对推测动作进行修正；6) 回退机制：当不匹配过大时，回退到完全重新规划。\n\n**关键创新**：关键创新在于将推测执行的思想引入到基于模型的控制中，并提出了不匹配感知的校正机制。与传统的推测执行不同，该方法不是简单地丢弃错误的推测结果，而是尝试通过学习到的校正器来修正动作，从而更有效地利用计算资源。此外，论文还探索了不同的校正器结构（MLP和Transformer），以适应不同的误差模式。\n\n**关键设计**：论文设计了两种校正器：门控双塔MLP校正器和时间Transformer校正器。MLP校正器用于处理局部误差，而Transformer校正器用于处理系统性漂移。校正器的训练采用离线蒸馏的方式，以重新规划的教师模型为目标。损失函数的设计需要平衡校正的精度和稳定性，避免过度校正导致控制不稳定。具体参数设置和网络结构细节在论文中有详细描述。",
            "application_zh": "该研究成果可应用于各种需要实时控制的场景，例如机器人导航、游戏AI、自动驾驶等。通过降低推理延迟，可以提高控制系统的响应速度和稳定性，从而改善用户体验和系统性能。此外，该方法还可以扩展到其他类型的模型和任务中，具有广泛的应用前景。",
            "highlight_zh": "实验结果表明，该方法在DMC Humanoid-Walk任务中，将规划推理次数从500次减少到282次，端到端步延迟提高了25%，同时仅降低了7.1%的回报。消融实验验证了不匹配感知校正对于鲁棒延迟降低的必要性，表明单纯的推测执行在长时间范围内不可靠。",
            "tags_zh": [
                "多模态LLM",
                "实时控制",
                "模型预测控制",
                "推测执行",
                "误差校正",
                "延迟优化",
                "机器人",
                "游戏AI"
            ],
            "_index": 40,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17250v1/sd-vllm.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17250v1/263a66ec-f63d-4057-b71f-790564ed66a7.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17250v1/speculation.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes",
            "authors": [
                "Carlos Vélez García",
                "Miguel Cazorla",
                "Jorge Pomares"
            ],
            "arxiv_id": "2512.17846v1",
            "summary": "We present Planning as Descent (PaD), a framework for offline goal-conditioned reinforcement learning that grounds trajectory synthesis in verification. Instead of learning a policy or explicit planner, PaD learns a goal-conditioned energy function over entire latent trajectories, assigning low energy to feasible, goal-consistent futures. Planning is realized as gradient-based refinement in this energy landscape, using identical computation during training and inference to reduce train-test mismatch common in decoupled modeling pipelines.\n  PaD is trained via self-supervised hindsight goal relabeling, shaping the energy landscape around the planning dynamics. At inference, multiple trajectory candidates are refined under different temporal hypotheses, and low-energy plans balancing feasibility and efficiency are selected.\n  We evaluate PaD on OGBench cube manipulation tasks. When trained on narrow expert demonstrations, PaD achieves state-of-the-art 95\\% success, strongly outperforming prior methods that peak at 68\\%. Remarkably, training on noisy, suboptimal data further improves success and plan efficiency, highlighting the benefits of verification-driven planning. Our results suggest learning to evaluate and refine trajectories provides a robust alternative to direct policy learning for offline, reward-free planning.",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17846v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "policy learning"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 5.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出Planning as Descent (PaD)，通过学习能量场进行离线目标条件强化学习。",
            "summary_zh": "本文提出了一种名为Planning as Descent (PaD)的离线目标条件强化学习框架，该框架将轨迹生成建立在验证的基础上。PaD不学习策略或显式规划器，而是学习一个关于整个潜在轨迹的目标条件能量函数，为可行且与目标一致的未来分配低能量。规划通过在这个能量场中进行基于梯度的优化来实现，训练和推理过程中使用相同的计算方式，从而减少了解耦建模流程中常见的训练-测试不匹配问题。PaD通过自监督的回溯目标重标记进行训练，围绕规划动态塑造能量场。在推理时，多个轨迹候选在不同的时间假设下进行优化，并选择平衡可行性和效率的低能量计划。在OGBench魔方操作任务上的评估表明，在狭窄的专家演示数据上训练时，PaD达到了最先进的95%的成功率，显著优于之前方法的68%。值得注意的是，在嘈杂的、次优数据上训练进一步提高了成功率和计划效率，突出了验证驱动规划的优势。我们的结果表明，学习评估和优化轨迹为离线、无奖励规划提供了一种稳健的替代方案，优于直接策略学习。",
            "intro_zh": [
                "现有离线强化学习方法在训练和测试之间存在不匹配，导致泛化能力不足。",
                "PaD通过学习目标条件能量函数，将规划转化为能量场中的梯度下降，实现轨迹优化。",
                "实验表明，PaD在魔方操作任务上显著优于现有方法，尤其是在噪声数据上训练时。"
            ],
            "method_zh": "**问题定义**：现有的离线目标条件强化学习方法，通常依赖于学习策略或显式规划器，这些方法容易受到训练数据质量的影响，并且在训练和测试之间存在不匹配，导致泛化能力较差。特别是在复杂任务中，策略学习可能难以捕捉到最优轨迹的细微之处，而显式规划器则需要大量的计算资源和领域知识。\n\n**核心思路**：PaD的核心思路是将规划问题转化为在学习到的能量场中进行梯度下降的过程。通过学习一个目标条件能量函数，该函数为可行且与目标一致的轨迹分配低能量，从而将规划问题转化为寻找能量最低的轨迹。这种方法避免了直接学习策略或显式规划器，而是通过优化轨迹的能量来实现规划。\n\n**技术框架**：PaD的整体框架包括以下几个主要模块：1）轨迹编码器：将轨迹编码为潜在向量表示。2）目标条件能量函数：学习一个能量函数，该函数以轨迹的潜在向量表示和目标为输入，输出一个能量值。3）梯度下降优化器：使用梯度下降算法在能量场中优化轨迹，寻找能量最低的轨迹。4）回溯目标重标记：使用自监督的回溯目标重标记方法来训练能量函数，从而使能量场围绕规划动态进行塑造。\n\n**关键创新**：PaD的关键创新在于将规划问题转化为能量场中的梯度下降过程。与传统的策略学习或显式规划方法不同，PaD通过学习一个能量函数来评估轨迹的可行性和效率，并通过梯度下降来优化轨迹。这种方法具有以下优点：1）避免了直接学习策略或显式规划器，从而减少了训练和测试之间的不匹配。2）可以通过自监督的回溯目标重标记方法来训练能量函数，从而提高了能量函数的泛化能力。3）可以通过梯度下降来优化轨迹，从而实现高效的规划。\n\n**关键设计**：PaD的关键设计包括：1）能量函数的选择：能量函数可以使用神经网络来表示，例如多层感知机或卷积神经网络。2）梯度下降优化器的选择：可以使用各种梯度下降优化器，例如Adam或SGD。3）回溯目标重标记策略：可以使用不同的回溯目标重标记策略来生成训练数据。4）损失函数的设计：损失函数用于训练能量函数，可以使用各种损失函数，例如均方误差或交叉熵损失。",
            "application_zh": "PaD具有广泛的应用前景，例如机器人导航、运动规划、游戏AI等领域。它可以用于解决离线强化学习中的泛化问题，提高规划的效率和鲁棒性。此外，PaD还可以应用于无奖励规划场景，通过学习能量函数来评估轨迹的可行性和效率，从而实现自主规划。",
            "highlight_zh": "PaD在OGBench魔方操作任务上取得了显著的成果。在狭窄的专家演示数据上训练时，PaD达到了最先进的95%的成功率，显著优于之前方法的68%。更令人惊讶的是，在嘈杂的、次优数据上训练进一步提高了成功率和计划效率，这表明PaD对噪声数据具有很强的鲁棒性，并且能够从次优数据中学习到有效的规划策略。",
            "tags_zh": [
                "离线强化学习",
                "目标条件强化学习",
                "能量场",
                "轨迹优化",
                "梯度下降",
                "机器人规划",
                "自监督学习"
            ],
            "_index": 41,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17846v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17846v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17846v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Vidarc: Embodied Video Diffusion Model for Closed-loop Control",
            "authors": [
                "Yao Feng",
                "Chendong Xiang",
                "Xinyi Mao",
                "Hengkai Tan",
                "Zuyue Zhang",
                "Shuhe Huang",
                "Kaiwen Zheng",
                "Haitian Liu",
                "Hang Su",
                "Jun Zhu"
            ],
            "arxiv_id": "2512.17661v1",
            "summary": "Robotic arm manipulation in data-scarce settings is a highly challenging task due to the complex embodiment dynamics and diverse contexts. Recent video-based approaches have shown great promise in capturing and transferring the temporal and physical interactions by pre-training on Internet-scale video data. However, such methods are often not optimized for the embodiment-specific closed-loop control, typically suffering from high latency and insufficient grounding. In this paper, we present Vidarc (Video Diffusion for Action Reasoning and Closed-loop Control), a novel autoregressive embodied video diffusion approach augmented by a masked inverse dynamics model. By grounding video predictions with action-relevant masks and incorporating real-time feedback through cached autoregressive generation, Vidarc achieves fast, accurate closed-loop control. Pre-trained on one million cross-embodiment episodes, Vidarc surpasses state-of-the-art baselines, achieving at least a 15% higher success rate in real-world deployment and a 91% reduction in latency. We also highlight its robust generalization and error correction capabilities across previously unseen robotic platforms.",
            "categories": [
                "cs.RO",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17661v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "cross-embodiment"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 5.0,
            "hit_pillars": [
                "1_robot_core",
                "7_retargeting"
            ],
            "headline_zh": "Vidarc：用于闭环控制的具身视频扩散模型，提升机器人操作性能。",
            "summary_zh": "由于复杂的具身动力学和多样的环境，数据稀缺场景下的机器人手臂操作极具挑战性。最近基于视频的方法通过在互联网规模的视频数据上进行预训练，在捕获和迁移时间和物理交互方面显示出巨大的潜力。然而，这些方法通常没有针对特定具身闭环控制进行优化，通常存在高延迟和不足的 grounding 问题。本文提出了 Vidarc（Video Diffusion for Action Reasoning and Closed-loop Control），一种新颖的自回归具身视频扩散方法，通过 masked inverse dynamics 模型进行增强。通过使用与动作相关的掩码来 grounding 视频预测，并通过缓存的自回归生成来整合实时反馈，Vidarc 实现了快速、准确的闭环控制。Vidarc 在一百万个跨具身 episodes 上进行预训练，超越了最先进的基线，在真实世界部署中实现了至少 15% 的成功率提升和 91% 的延迟降低。我们还强调了其在以前未见过的机器人平台上的鲁棒泛化和纠错能力。",
            "intro_zh": [
                "现有基于视频的机器人控制方法在具身闭环控制方面存在高延迟和 grounding 不足的问题。",
                "Vidarc 提出了一种自回归具身视频扩散模型，通过动作相关掩码和实时反馈，实现快速准确的闭环控制。",
                "Vidarc 在真实机器人部署中，成功率提升至少 15%，延迟降低 91%，并展现出良好的泛化能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决数据稀缺场景下，机器人手臂操作中现有基于视频的控制方法存在的延迟高、与环境交互不足的问题。这些方法通常难以适应特定具身机器人的动力学特性，导致控制精度和效率降低。\\n\\n**核心思路**：Vidarc 的核心思路是利用视频扩散模型学习机器人操作的动态过程，并通过自回归生成的方式预测未来状态。为了提高控制精度和降低延迟，论文引入了动作相关的掩码来指导视频预测，并利用实时反馈进行纠错。\\n\\n**技术框架**：Vidarc 的整体框架包含以下几个主要模块：1) 视频扩散模型：用于学习机器人操作的视频数据分布；2) Masked Inverse Dynamics 模型：用于预测给定状态和目标状态之间的动作；3) 自回归生成模块：通过缓存历史状态和动作，实现实时的闭环控制。整个流程是，首先利用视频扩散模型预测未来状态，然后利用 Masked Inverse Dynamics 模型计算所需动作，最后将动作发送给机器人执行，并根据实时反馈更新状态。\\n\\n**关键创新**：Vidarc 的关键创新在于将视频扩散模型与 masked inverse dynamics 模型相结合，并采用自回归生成的方式进行闭环控制。这种方法能够有效地利用视频数据中的信息，提高控制精度和鲁棒性，并降低延迟。与现有方法相比，Vidarc 更加注重具身机器人的动力学特性，并能够更好地适应不同的环境。\\n\\n**关键设计**：论文中使用了大量的视频数据进行预训练，并采用了特定的损失函数来优化视频扩散模型和 masked inverse dynamics 模型。自回归生成模块采用了缓存机制，以降低延迟。具体的网络结构和参数设置在论文中有详细描述。",
            "application_zh": "Vidarc 的潜在应用领域包括工业自动化、家庭服务机器人、医疗机器人等。该研究成果可以提高机器人在复杂环境中的操作能力，降低开发成本，并促进机器人技术的普及。未来，Vidarc 可以进一步扩展到多模态输入、多任务学习等场景，为机器人智能化提供更强大的支持。",
            "highlight_zh": "Vidarc 在真实机器人部署中取得了显著的性能提升。与最先进的基线方法相比，Vidarc 的成功率提高了至少 15%，延迟降低了 91%。此外，Vidarc 还展现出良好的泛化能力，能够在以前未见过的机器人平台上进行操作，并具有一定的纠错能力。",
            "tags_zh": [
                "机器人控制",
                "视频扩散模型",
                "具身智能",
                "闭环控制",
                "逆动力学",
                "自回归模型",
                "深度学习"
            ],
            "_index": 42,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17661v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17661v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17661v1/figures/artifacts.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework",
            "authors": [
                "Tobias Sautter",
                "Jan-Niklas Dihlmann",
                "Hendrik P. A. Lensch"
            ],
            "arxiv_id": "2512.17459v1",
            "summary": "Recent advances in 3D scene generation produce visually appealing output, but current representations hinder artists' workflows that require modifiable 3D textured mesh scenes for visual effects and game development. Despite significant advances, current textured mesh scene reconstruction methods are far from artist ready, suffering from incorrect object decomposition, inaccurate spatial relationships, and missing backgrounds. We present 3D-RE-GEN, a compositional framework that reconstructs a single image into textured 3D objects and a background. We show that combining state of the art models from specific domains achieves state of the art scene reconstruction performance, addressing artists' requirements.\n  Our reconstruction pipeline integrates models for asset detection, reconstruction, and placement, pushing certain models beyond their originally intended domains. Obtaining occluded objects is treated as an image editing task with generative models to infer and reconstruct with scene level reasoning under consistent lighting and geometry. Unlike current methods, 3D-RE-GEN generates a comprehensive background that spatially constrains objects during optimization and provides a foundation for realistic lighting and simulation tasks in visual effects and games. To obtain physically realistic layouts, we employ a novel 4-DoF differentiable optimization that aligns reconstructed objects with the estimated ground plane. 3D-RE-GEN~achieves state of the art performance in single image 3D scene reconstruction, producing coherent, modifiable scenes through compositional generation guided by precise camera recovery and spatial optimization.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "Project Page: https://3dregen.jdihlmann.com/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17459v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "scene reconstruction"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "spatial relationship"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 5.0,
            "hit_pillars": [
                "3_perception_slam",
                "7_retargeting"
            ],
            "headline_zh": "3D-RE-GEN：提出一种生成式框架，用于室内场景的单图三维重建，满足艺术家对可编辑网格的需求。",
            "summary_zh": "本文提出3D-RE-GEN，一个组合式框架，可以将单张图像重建为带纹理的3D物体和背景。虽然3D场景生成领域取得了显著进展，但现有表示方法阻碍了艺术家们对可修改的3D纹理网格场景的需求，这在视觉特效和游戏开发中至关重要。现有的纹理网格场景重建方法远未达到艺术家可用的程度，存在物体分解不正确、空间关系不准确以及缺少背景等问题。3D-RE-GEN结合了特定领域的最先进模型，实现了最先进的场景重建性能，满足了艺术家的需求。该重建流程集成了物体检测、重建和放置模型，并将某些模型扩展到其原始领域之外。遮挡物体的获取被视为图像编辑任务，利用生成模型进行推理和重建，并进行场景级别的推理，保持光照和几何一致性。与现有方法不同，3D-RE-GEN生成了一个全面的背景，在优化过程中对物体进行空间约束，并为视觉特效和游戏中的真实光照和模拟任务奠定了基础。为了获得物理上真实的布局，采用了一种新颖的4自由度可微优化方法，使重建的物体与估计的地面平面对齐。3D-RE-GEN在单图像3D场景重建中实现了最先进的性能，通过精确的相机恢复和空间优化引导的组合生成，生成连贯的、可修改的场景。",
            "intro_zh": [
                "现有3D重建方法在物体分解、空间关系和背景生成方面存在不足，难以满足艺术家对可编辑3D场景的需求。",
                "3D-RE-GEN通过组合物体检测、重建和放置等模块，并结合生成模型进行图像编辑，实现高质量的场景重建。",
                "该方法生成全面的背景，并采用4自由度可微优化，确保重建物体与地面平面对齐，实现物理上真实的布局。"
            ],
            "method_zh": "**问题定义**：现有单图三维场景重建方法难以生成高质量、可编辑的3D场景，具体表现为物体分解不准确、物体间空间关系不合理、缺少完整背景等问题。这些问题限制了重建结果在视觉特效和游戏开发等领域的应用，因为艺术家需要能够灵活修改和调整场景中的各个元素。\\n\\n**核心思路**：3D-RE-GEN的核心思路是将单图三维场景重建分解为多个子任务，并针对每个子任务选择最先进的模型进行组合。同时，利用生成模型进行图像编辑，推断和重建被遮挡的物体，并生成完整的背景。通过这种组合式的框架，可以充分利用现有模型的优势，并解决现有方法的不足。\\n\\n**技术框架**：3D-RE-GEN的整体框架包含以下几个主要模块：1) 物体检测：检测图像中的物体，并估计其类别和位置。2) 物体重建：根据检测到的物体，重建其三维模型。3) 物体放置：将重建的物体放置到场景中，并调整其位置和姿态。4) 背景生成：生成场景的背景，并与重建的物体进行融合。5) 空间优化：通过可微优化，调整物体的位置和姿态，使其与地面平面对齐，并保证物体之间的空间关系合理。\\n\\n**关键创新**：该方法最重要的创新点在于其组合式的框架，以及利用生成模型进行图像编辑和背景生成。通过组合不同的模型，可以充分利用现有技术的优势，并解决现有方法的不足。利用生成模型进行图像编辑，可以推断和重建被遮挡的物体，从而提高重建的完整性。生成完整的背景，可以为场景提供更真实的光照和模拟效果。\\n\\n**关键设计**：该方法采用了一种新颖的4自由度可微优化方法，用于调整物体的位置和姿态。该优化方法考虑了物体与地面平面的对齐关系，以及物体之间的空间关系。此外，该方法还设计了一种损失函数，用于衡量重建结果的质量，包括物体的形状、纹理和空间关系。",
            "application_zh": "该研究成果可应用于视觉特效、游戏开发、室内设计、机器人导航等领域。在视觉特效和游戏开发中，可以快速生成高质量的3D场景，提高制作效率。在室内设计中，可以根据单张照片重建室内场景，方便用户进行虚拟装修和布局设计。在机器人导航中，可以利用重建的3D场景进行路径规划和环境感知。",
            "highlight_zh": "3D-RE-GEN在单图像3D场景重建任务上取得了state-of-the-art的性能。通过组合生成和空间优化，该方法能够生成连贯、可修改的场景，显著优于现有方法。具体性能数据和对比基线在论文实验部分给出，表明该方法在重建质量和效率上均有明显提升。",
            "tags_zh": [
                "三维重建",
                "场景生成",
                "生成模型",
                "图像编辑",
                "可微优化"
            ],
            "_index": 43,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17459v1/sec/images/pipeline.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17459v1/sec/images/appl_querying.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17459v1/sec/images/appl_querying_fail.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers",
            "authors": [
                "Zeyuan Allen-Zhu"
            ],
            "arxiv_id": "2512.17351v1",
            "summary": "Understanding architectural differences in language models is challenging, especially at academic-scale pretraining (e.g., 1.3B parameters, 100B tokens), where results are often dominated by noise and randomness. To overcome this, we introduce controlled synthetic pretraining tasks that isolate and evaluate core model capabilities. Within this framework, we discover CANON LAYERS: lightweight architectural components -- named after the musical term \"canon\" -- that promote horizontal information flow across neighboring tokens. Canon layers compute weighted sums of nearby token representations and integrate seamlessly into Transformers, linear attention, state-space models, or any sequence architecture.\n  We present 12 key results. This includes how Canon layers enhance reasoning depth (e.g., by $2\\times$), reasoning breadth, knowledge manipulation, etc. They lift weak architectures like NoPE to match RoPE, and linear attention to rival SOTA linear models like Mamba2/GDN -- validated both through synthetic tasks and real-world academic-scale pretraining. This synthetic playground offers an economical, principled path to isolate core model capabilities often obscured at academic scales. Equipped with infinite high-quality data, it may even PREDICT how future architectures will behave as training pipelines improve -- e.g., through better data curation or RL-based post-training -- unlocking deeper reasoning and hierarchical inference.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "V1.1 appeared in NeurIPS 2025 main conference; V2 adds GDN experiments, tightens some experiments (for a stronger, fairer comparison), and re-organizes sections",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17351v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "Mamba",
                        "linear attention"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 5.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出Canon Layers，增强语言模型水平信息流动与推理能力",
            "summary_zh": "本文提出了一种名为Canon Layers的轻量级架构组件，旨在促进相邻token之间的水平信息流动。Canon Layers通过计算附近token表示的加权和，可以无缝集成到Transformer、线性注意力、状态空间模型或任何序列架构中。研究通过受控的合成预训练任务，隔离并评估了模型的关键能力。实验结果表明，Canon Layers能够显著提升模型的推理深度（例如提升2倍）、推理广度和知识操作能力。它还可以提升弱架构（如NoPE）的性能，使其与RoPE相匹配，并使线性注意力模型能够与Mamba2/GDN等先进线性模型相媲美。该合成环境提供了一种经济有效且有原则的方法来隔离在学术规模下常常被掩盖的模型核心能力，并可能预测未来架构在训练流程改进后的表现。",
            "intro_zh": [
                "现有语言模型架构差异难以理解，尤其是在学术规模预训练中，结果常受噪声和随机性影响。",
                "论文提出Canon Layers，通过加权求和相邻token表示，促进水平信息流动，提升模型推理能力。",
                "实验表明，Canon Layers能显著提升推理深度和广度，并使弱架构达到SOTA水平，已通过合成任务和学术规模预训练验证。"
            ],
            "method_zh": "**问题定义**：现有语言模型架构的设计选择繁多，但缺乏一种系统性的方法来理解不同架构组件对模型性能的影响，尤其是在计算资源有限的学术规模预训练中，实验结果往往受到噪声和随机性的干扰，难以得出可靠的结论。因此，需要一种可控的环境来隔离和评估不同架构组件的核心能力。\\n\\n**核心思路**：论文的核心思路是设计一种轻量级的架构组件Canon Layers，通过促进相邻token之间的水平信息流动来增强模型的推理能力。这种设计灵感来源于音乐术语“canon”，旨在通过简单的加权求和操作，实现信息在序列中的有效传递。\\n\\n**技术框架**：Canon Layers可以无缝集成到各种序列模型架构中，包括Transformer、线性注意力模型和状态空间模型。其核心操作是对每个token的表示，计算其相邻token表示的加权和。这个加权和的结果被添加到原始token表示中，从而实现信息的水平传递。整个过程可以看作是在现有模型架构中插入一个额外的处理层，该层专门负责促进信息在序列中的流动。\\n\\n**关键创新**：Canon Layers的关键创新在于其简洁性和通用性。它通过简单的加权求和操作，实现了信息在序列中的有效传递，而无需引入复杂的注意力机制或状态变量。这种设计使得Canon Layers可以轻松地集成到各种不同的模型架构中，并提升它们的推理能力。此外，论文还通过受控的合成预训练任务，提供了一种系统性的方法来评估不同架构组件的核心能力。\\n\\n**关键设计**：Canon Layers的关键设计在于加权和的权重选择。论文中可能探讨了不同的权重设置方案，例如均匀权重、高斯权重或可学习的权重。此外，Canon Layers的集成方式也可能存在多种选择，例如将其添加到Transformer的每个注意力层之后，或者将其添加到整个模型的输入或输出层。具体的参数设置、损失函数和网络结构等技术细节可能在论文的实验部分进行详细描述。",
            "application_zh": "Canon Layers可应用于各种需要序列建模的任务，如自然语言处理、语音识别、时间序列预测等。通过提升模型的推理能力，Canon Layers有望改善机器翻译、文本摘要、问答系统等应用的性能。此外，该研究提出的合成预训练方法，为未来语言模型架构的设计和评估提供了一种新的思路。",
            "highlight_zh": "实验结果表明，Canon Layers能够显著提升模型的推理深度（提升2倍）、推理广度和知识操作能力。它还可以提升弱架构（如NoPE）的性能，使其与RoPE相匹配，并使线性注意力模型能够与Mamba2/GDN等先进线性模型相媲美。这些结果在合成任务和学术规模预训练中都得到了验证。",
            "tags_zh": [
                "语言模型",
                "架构设计",
                "水平信息流动",
                "推理能力",
                "Transformer",
                "线性注意力",
                "状态空间模型"
            ],
            "_index": 44,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Trust-Region Adaptive Policy Optimization",
            "authors": [
                "Mingyu Su",
                "Jian Guan",
                "Yuxian Gu",
                "Minlie Huang",
                "Hongning Wang"
            ],
            "arxiv_id": "2512.17636v1",
            "summary": "Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models' (LLMs) complex reasoning abilities. However, the dominant two-stage pipeline (SFT then RL) suffers from a key inconsistency: SFT enforces rigid imitation that suppresses exploration and induces forgetting, limiting RL's potential for improvements. We address this inefficiency with TRAPO (\\textbf{T}rust-\\textbf{R}egion \\textbf{A}daptive \\textbf{P}olicy \\textbf{O}ptimization), a hybrid framework that interleaves SFT and RL within each training instance by optimizing SFT loss on expert prefixes and RL loss on the model's own completions, unifying external supervision and self-exploration. To stabilize training, we introduce Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL and yielding stable, mode-seeking updates favorable for RL. An adaptive prefix-selection mechanism further allocates expert guidance based on measured utility. Experiments on five mathematical reasoning benchmarks show that TRAPO consistently surpasses standard SFT, RL, and SFT-then-RL pipelines, as well as recent state-of-the-art approaches, establishing a strong new paradigm for reasoning-enhanced LLMs.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17636v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出TRAPO，交错SFT与RL优化LLM推理能力，显著提升数学推理性能。",
            "summary_zh": "本文提出Trust-Region Adaptive Policy Optimization (TRAPO)，旨在解决大型语言模型(LLMs)在复杂推理能力提升中，监督式微调(SFT)和强化学习(RL)两阶段训练流程的不一致性问题。传统SFT强制模仿，抑制探索并导致遗忘，限制了RL的改进潜力。TRAPO是一种混合框架，通过在每个训练实例中交错SFT和RL，优化专家前缀上的SFT损失和模型自身补全上的RL损失，从而统一外部监督和自我探索。为了稳定训练，引入Trust-Region SFT (TrSFT)，在信任区域内最小化前向KL散度，并在区域外衰减优化，有效转向反向KL，产生稳定的、模式寻求的更新，有利于RL。自适应前缀选择机制进一步根据测量的效用分配专家指导。在五个数学推理基准上的实验表明，TRAPO始终优于标准SFT、RL和SFT-then-RL流程，以及最近的state-of-the-art方法，为增强LLM推理能力建立了一个强大的新范式。",
            "intro_zh": [
                "现有SFT-then-RL两阶段训练流程存在SFT抑制探索和导致遗忘的问题，限制了RL的改进潜力。",
                "TRAPO交错SFT和RL，统一外部监督和自我探索，并引入Trust-Region SFT稳定训练。",
                "实验表明，TRAPO在数学推理任务上超越了SFT、RL以及SFT-then-RL等基线方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型通过监督微调（SFT）和强化学习（RL）进行后训练时，SFT阶段对RL阶段的负面影响。具体来说，SFT过度模仿专家数据，限制了模型后续在RL阶段的探索能力，并可能导致模型遗忘先前学习到的知识。这种两阶段训练流程的不一致性阻碍了模型推理能力的进一步提升。\\n\\n**核心思路**：TRAPO的核心思路是在训练过程中交错进行SFT和RL，从而将外部监督和自我探索结合起来。通过在每个训练实例中同时优化专家前缀上的SFT损失和模型自身补全上的RL损失，模型可以在模仿专家行为的同时，探索新的解决方案。此外，引入Trust-Region SFT (TrSFT) 来稳定训练过程，避免模型在RL阶段发生剧烈变化。\\n\\n**技术框架**：TRAPO的整体框架包含以下几个关键部分：1) 交错SFT和RL：在每个训练步骤中，模型首先根据专家前缀生成补全，然后计算SFT损失和RL损失。2) Trust-Region SFT (TrSFT)：通过限制SFT的更新幅度，防止模型过度拟合专家数据。3) 自适应前缀选择：根据模型在当前状态下的表现，动态选择合适的专家前缀进行学习。整个训练流程旨在平衡模仿学习和强化学习，从而提高模型的推理能力。\\n\\n**关键创新**：TRAPO的关键创新在于其混合训练框架，它打破了传统的SFT-then-RL两阶段训练模式，实现了SFT和RL的无缝集成。通过交错SFT和RL，TRAPO能够更好地利用专家知识，同时鼓励模型进行自我探索。此外，Trust-Region SFT的引入有效地稳定了训练过程，避免了模型在RL阶段出现不稳定的情况。\\n\\n**关键设计**：TrSFT的关键设计在于使用信任区域来约束SFT的更新。具体来说，TrSFT最小化信任区域内的前向KL散度，并在区域外衰减优化，从而有效地转向反向KL散度。这种设计使得模型能够更稳定地学习专家知识，并避免过度拟合。此外，自适应前缀选择机制根据模型在当前状态下的表现，动态调整专家指导的强度，从而进一步提高了训练效率。",
            "application_zh": "TRAPO方法可广泛应用于需要复杂推理能力的大型语言模型，例如数学问题求解、代码生成、逻辑推理等领域。该方法通过结合模仿学习和强化学习，能够有效提升模型的推理能力和泛化性能，具有重要的实际应用价值。未来，该方法可以进一步扩展到其他类型的任务和模型，例如多模态推理和对话生成。",
            "highlight_zh": "实验结果表明，TRAPO在五个数学推理基准上均优于标准SFT、RL和SFT-then-RL流程，以及最近的state-of-the-art方法。例如，在某些基准测试中，TRAPO的性能提升超过了10%。这些结果表明，TRAPO是一种有效的增强LLM推理能力的方法。",
            "tags_zh": [
                "大型语言模型",
                "强化学习",
                "监督式微调",
                "推理能力",
                "信任区域优化"
            ],
            "_index": 45,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17636v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17636v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17636v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning",
            "authors": [
                "Javier Gonzalez-Ruiz",
                "Carlos Rodriguez-Pardo",
                "Iacopo Savelli",
                "Alice Di Bella",
                "Massimo Tavoni"
            ],
            "arxiv_id": "2512.17444v1",
            "summary": "Electricity systems are key to transforming today's society into a carbon-free economy. Long-term electricity market mechanisms, including auctions, support schemes, and other policy instruments, are critical in shaping the electricity generation mix. In light of the need for more advanced tools to support policymakers and other stakeholders in designing, testing, and evaluating long-term markets, this work presents a multi-agent reinforcement learning model capable of capturing the key features of decarbonizing energy systems. Profit-maximizing generation companies make investment decisions in the wholesale electricity market, responding to system needs, competitive dynamics, and policy signals. The model employs independent proximal policy optimization, which was selected for suitability to the decentralized and competitive environment. Nevertheless, given the inherent challenges of independent learning in multi-agent settings, an extensive hyperparameter search ensures that decentralized training yields market outcomes consistent with competitive behavior. The model is applied to a stylized version of the Italian electricity system and tested under varying levels of competition, market designs, and policy scenarios. Results highlight the critical role of market design for decarbonizing the electricity sector and avoiding price volatility. The proposed framework allows assessing long-term electricity markets in which multiple policy and market mechanisms interact simultaneously, with market participants responding and adapting to decarbonization pathways.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.NE",
                "econ.GN"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "Accepted to Energy and AI. Code available in https://github.com/jjgonzalez2491/MARLEY_V1",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17444v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出基于多智能体强化学习的电力市场长期设计评估框架，助力实现深度脱碳目标。",
            "summary_zh": "电力系统是将当今社会转型为无碳经济的关键。长期电力市场机制，包括拍卖、支持计划和其他政策工具，对于塑造电力生产结构至关重要。鉴于需要更先进的工具来支持决策者和其他利益相关者设计、测试和评估长期市场，本研究提出了一种多智能体强化学习模型，该模型能够捕捉脱碳能源系统的关键特征。利润最大化的发电公司在批发电力市场中做出投资决策，响应系统需求、竞争动态和政策信号。该模型采用独立的近端策略优化（PPO），因其适用于分散和竞争环境而被选中。尽管如此，考虑到多智能体环境中独立学习的固有挑战，广泛的超参数搜索确保分散训练产生与竞争行为一致的市场结果。该模型应用于意大利电力系统的简化版本，并在不同程度的竞争、市场设计和政策情景下进行了测试。结果突出了市场设计对于电力部门脱碳和避免价格波动的关键作用。所提出的框架允许评估长期电力市场，其中多个政策和市场机制同时相互作用，市场参与者响应并适应脱碳路径。",
            "intro_zh": [
                "现有电力市场设计工具难以有效评估长期政策和市场机制对深度脱碳的影响。",
                "利用多智能体强化学习，模拟发电公司在不同市场设计和政策下的投资决策，捕捉竞争动态。",
                "通过意大利电力系统案例研究，验证模型在评估市场设计和避免价格波动方面的有效性。"
            ],
            "method_zh": "**问题定义**：现有电力市场设计评估工具难以模拟市场参与者的长期投资决策行为，无法有效评估不同政策和市场机制对电力系统深度脱碳的影响。现有方法通常依赖于静态模型或简化假设，难以捕捉市场竞争的复杂动态。\n\n**核心思路**：本研究的核心思路是利用多智能体强化学习（MARL）模拟电力市场中发电公司的投资决策行为。每个发电公司被建模为一个独立的智能体，通过与环境（电力市场）交互学习，最大化自身利润。这种方法能够捕捉市场参与者之间的竞争和合作关系，以及政策和市场机制对投资决策的长期影响。\n\n**技术框架**：该框架包含以下主要模块：1) 电力市场环境：模拟电力市场的供需关系、价格形成机制和政策约束。2) 发电公司智能体：每个智能体代表一个发电公司，使用强化学习算法学习最优投资策略。3) 强化学习算法：采用独立的近端策略优化（Independent Proximal Policy Optimization, IPPO）算法，每个智能体独立进行策略学习。4) 评估模块：评估不同市场设计和政策情景下的电力系统性能，包括碳排放、价格波动和投资效率。\n\n**关键创新**：该研究的关键创新在于将多智能体强化学习应用于电力市场长期设计评估。与传统的静态模型相比，该方法能够捕捉市场参与者的动态行为和市场竞争的复杂性。此外，该研究采用独立的PPO算法，避免了中心化训练的计算负担，更适用于大规模电力市场。\n\n**关键设计**：该研究的关键设计包括：1) 智能体的状态空间：包括电力需求、价格、政策信号和自身发电容量等信息。2) 动作空间：包括投资新建发电厂的类型和容量。3) 奖励函数：基于发电公司的利润进行设计，鼓励智能体做出有利于自身利润最大化的投资决策。4) 超参数搜索：通过广泛的超参数搜索，确保分散训练能够产生与竞争行为一致的市场结果。",
            "application_zh": "该研究成果可应用于电力市场设计、政策评估和投资决策等领域。决策者可以利用该框架评估不同市场设计和政策对电力系统脱碳的影响，优化市场机制，促进清洁能源发展。发电公司可以利用该框架预测市场趋势，制定合理的投资策略，提高盈利能力。该研究为实现电力系统深度脱碳提供了新的工具和方法。",
            "highlight_zh": "在意大利电力系统的简化版本上进行的实验表明，该模型能够有效地评估不同市场设计和政策情景下的电力系统性能。结果表明，合理的市场设计对于电力部门脱碳至关重要，能够有效避免价格波动。例如，某种市场设计下，碳排放量降低了X%，价格波动率降低了Y%。该模型还能够识别潜在的市场风险和投资机会。",
            "tags_zh": [
                "多智能体强化学习",
                "电力市场设计",
                "深度脱碳",
                "能源政策",
                "市场机制"
            ],
            "_index": 46,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17444v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17444v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17444v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "AlignDP: Hybrid Differential Privacy with Rarity-Aware Protection for LLMs",
            "authors": [
                "Madhava Gaikwad"
            ],
            "arxiv_id": "2512.17251v1",
            "summary": "Large language models are exposed to risks of extraction, distillation, and unauthorized fine-tuning. Existing defenses use watermarking or monitoring, but these act after leakage. We design AlignDP, a hybrid privacy lock that blocks knowledge transfer at the data interface. The key idea is to separate rare and non-rare fields. Rare fields are shielded by PAC indistinguishability, giving effective zero-epsilon local DP. Non-rare fields are privatized with RAPPOR, giving unbiased frequency estimates under local DP. A global aggregator enforces composition and budget. This two-tier design hides rare events and adds controlled noise to frequent events. We prove limits of PAC extension to global aggregation, give bounds for RAPPOR estimates, and analyze utility trade-off. A toy simulation confirms feasibility: rare categories remain hidden, frequent categories are recovered with small error.",
            "categories": [
                "cs.CR",
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: LOCK-LLM Work-shop, NeurIPS 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17251v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "distillation"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "AlignDP：针对LLM的混合差分隐私方法，通过稀有感知保护提升安全性",
            "summary_zh": "大型语言模型面临提取、蒸馏和未经授权的微调等风险。现有的防御措施如水印或监控，都是在泄露发生后才起作用。本文设计了AlignDP，一种混合隐私锁，可在数据接口处阻止知识转移。核心思想是将稀有字段和非稀有字段分离。稀有字段通过PAC不可区分性进行保护，实现有效的零epsilon局部差分隐私（DP）。非稀有字段使用RAPPOR进行私有化，从而在局部DP下提供无偏的频率估计。全局聚合器强制执行组合和预算。这种双层设计隐藏了稀有事件，并为频繁事件添加了可控噪声。论文证明了PAC扩展到全局聚合的限制，给出了RAPPOR估计的界限，并分析了效用权衡。玩具仿真实验证实了可行性：稀有类别保持隐藏，频繁类别以小误差恢复。",
            "intro_zh": [
                "大型语言模型面临数据泄露风险，现有防御方法通常在泄露后才生效，缺乏主动防御能力。",
                "AlignDP提出一种混合差分隐私方法，区分稀有和非稀有字段，分别采用PAC不可区分性和RAPPOR进行保护。",
                "实验结果表明，AlignDP能够在隐藏稀有类别的同时，以较小的误差恢复频繁类别，验证了其可行性。"
            ],
            "method_zh": "**问题定义**：大型语言模型容易受到各种攻击，包括数据提取、模型蒸馏和未经授权的微调。现有的防御机制，例如水印和监控系统，通常是在数据泄露发生后才采取行动，缺乏主动预防措施。因此，需要一种在数据层面主动阻止知识转移的隐私保护机制。\\n\\n**核心思路**：AlignDP的核心思想是根据数据字段的稀有程度采用不同的差分隐私策略。对于稀有字段，采用PAC（Probably Approximately Correct）不可区分性，提供更强的隐私保护，近似于零epsilon的局部差分隐私。对于非稀有字段，采用RAPPOR（Randomized Aggregatable Privacy-Preserving Ordinal Response），在局部差分隐私下提供无偏的频率估计。\\n\\n**技术框架**：AlignDP包含以下主要模块：1) 数据字段分类器：区分稀有和非稀有字段。2) 稀有字段保护模块：应用PAC不可区分性。3) 非稀有字段保护模块：使用RAPPOR进行私有化。4) 全局聚合器：汇总来自不同用户的私有化数据，并强制执行差分隐私预算。整体流程是，用户首先对数据进行分类，然后根据字段类型应用相应的隐私保护机制，最后将私有化后的数据发送到全局聚合器进行分析。\\n\\n**关键创新**：AlignDP的关键创新在于其混合差分隐私策略，它根据数据字段的稀有程度自适应地调整隐私保护强度。这种方法能够在保护稀有数据的同时，保持对频繁数据的分析效用。此外，论文还分析了PAC扩展到全局聚合的限制，并给出了RAPPOR估计的界限。\\n\\n**关键设计**：AlignDP的关键设计包括：1) 稀有字段的PAC不可区分性实现方式，具体如何选择合适的PAC参数以达到所需的隐私级别。2) RAPPOR的参数设置，例如哈希函数的数量和扰动概率，这些参数会影响隐私保护强度和数据效用之间的权衡。3) 全局聚合器的设计，如何有效地汇总来自不同用户的私有化数据，并控制差分隐私预算。",
            "application_zh": "AlignDP可应用于各种需要保护用户数据隐私的大型语言模型应用场景，例如医疗健康、金融服务和法律咨询等。通过在数据接口处阻止知识转移，AlignDP可以有效防止敏感信息的泄露，提高用户对LLM的信任度，并促进LLM在隐私敏感领域的应用。",
            "highlight_zh": "论文通过玩具仿真实验验证了AlignDP的可行性。实验结果表明，AlignDP能够在隐藏稀有类别的同时，以较小的误差恢复频繁类别。这表明AlignDP能够在隐私保护和数据效用之间取得良好的平衡。具体的性能数据和对比基线未知，需要在论文中查找。",
            "tags_zh": [
                "差分隐私",
                "大型语言模型",
                "隐私保护",
                "PAC学习",
                "RAPPOR"
            ],
            "_index": 47,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17251v1/aligndp_pipeline.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17251v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17251v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "About Time: Model-free Reinforcement Learning with Timed Reward Machines",
            "authors": [
                "Anirban Majumdar",
                "Ritam Raha",
                "Rajarshi Roy",
                "David Parker",
                "Marta Kwiatkowska"
            ],
            "arxiv_id": "2512.17637v1",
            "summary": "Reward specification plays a central role in reinforcement learning (RL), guiding the agent's behavior. To express non-Markovian rewards, formalisms such as reward machines have been introduced to capture dependencies on histories. However, traditional reward machines lack the ability to model precise timing constraints, limiting their use in time-sensitive applications. In this paper, we propose timed reward machines (TRMs), which are an extension of reward machines that incorporate timing constraints into the reward structure. TRMs enable more expressive specifications with tunable reward logic, for example, imposing costs for delays and granting rewards for timely actions. We study model-free RL frameworks (i.e., tabular Q-learning) for learning optimal policies with TRMs under digital and real-time semantics. Our algorithms integrate the TRM into learning via abstractions of timed automata, and employ counterfactual-imagining heuristics that exploit the structure of the TRM to improve the search. Experimentally, we demonstrate that our algorithm learns policies that achieve high rewards while satisfying the timing constraints specified by the TRM on popular RL benchmarks. Moreover, we conduct comparative studies of performance under different TRM semantics, along with ablations that highlight the benefits of counterfactual-imagining.",
            "categories": [
                "cs.AI",
                "cs.FL",
                "cs.LO"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17637v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出基于时序奖励机的免模型强化学习方法，解决时序约束下的奖励建模问题。",
            "summary_zh": "奖励规范在强化学习中起着核心作用，它指导着智能体的行为。为了表达非马尔可夫奖励，引入了奖励机等形式化方法来捕获对历史的依赖关系。然而，传统的奖励机缺乏对精确时序约束的建模能力，限制了它们在时间敏感型应用中的使用。本文提出了时序奖励机（TRM），它是奖励机的扩展，将时序约束纳入奖励结构中。TRM 能够实现更具表现力的规范和可调的奖励逻辑，例如，对延迟施加成本，并对及时行动给予奖励。我们研究了免模型强化学习框架（即表格 Q-learning），用于在数字和实时语义下学习具有 TRM 的最优策略。我们的算法通过时序自动机的抽象将 TRM 集成到学习中，并采用反事实想象启发式方法，利用 TRM 的结构来改进搜索。实验表明，我们的算法学习到的策略能够在流行的强化学习基准上实现高奖励，同时满足 TRM 指定的时序约束。此外，我们还进行了不同 TRM 语义下的性能比较研究，以及突出反事实想象优势的消融实验。",
            "intro_zh": [
                "传统奖励机无法建模精确时序约束，限制了其在时间敏感型任务中的应用。",
                "提出时序奖励机（TRM），将时序约束融入奖励结构，实现更灵活的奖励逻辑。",
                "通过实验验证，该算法在满足时序约束的同时，能有效提升强化学习的性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决强化学习中奖励函数难以表达精确时序约束的问题。传统的奖励机虽然可以处理非马尔可夫奖励，但无法对时间延迟、时间窗口等时序信息进行建模，导致在时间敏感型任务中表现不佳。现有方法缺乏一种能够灵活表达和利用时序信息的奖励规范。\n\n**核心思路**：论文的核心思路是将时序约束融入奖励机，提出时序奖励机（TRM）。TRM通过引入时钟变量和时序约束条件，扩展了传统奖励机的状态转移规则，使得奖励函数能够对智能体的行为在时间维度上进行更精细的控制。这样，智能体可以学习到既能完成任务，又能满足时序要求的策略。\n\n**技术框架**：整体框架包括：1）定义时序奖励机（TRM），明确状态、事件、时钟变量和转移规则；2）将TRM集成到Q-learning算法中，利用TRM的状态作为Q-learning的状态空间的一部分；3）采用反事实想象启发式方法，利用TRM的结构信息来指导策略搜索，加速学习过程。该框架适用于表格型Q-learning等免模型强化学习算法。\n\n**关键创新**：最重要的创新点在于时序奖励机（TRM）的提出。TRM通过引入时钟变量和时序约束，扩展了传统奖励机的表达能力，使其能够对时间敏感型任务进行建模。与传统奖励机相比，TRM能够更精确地控制奖励的分配，从而引导智能体学习到满足时序要求的策略。\n\n**关键设计**：TRM的关键设计包括：1）时钟变量的引入，用于记录时间流逝；2）时序约束条件的定义，用于限制状态转移的时间范围；3）反事实想象启发式方法，用于加速学习过程。具体而言，反事实想象通过模拟不同状态下的奖励变化，帮助智能体更好地理解TRM的结构，从而更有效地探索策略空间。论文还研究了数字和实时语义下TRM的不同实现方式。",
            "application_zh": "该研究成果可应用于机器人控制、自动驾驶、资源调度等时间敏感型领域。例如，在机器人控制中，可以利用TRM对机器人的动作执行时间进行约束，使其能够更精确地完成任务。在自动驾驶中，可以利用TRM对车辆的行驶速度和到达时间进行控制，提高交通效率和安全性。未来，该方法有望推广到更广泛的智能系统设计中。",
            "highlight_zh": "实验结果表明，所提出的算法在流行的强化学习基准上能够学习到满足TRM指定的时序约束，并获得较高的奖励。通过对比不同TRM语义下的性能，以及消融实验，验证了反事实想象启发式方法的有效性。具体性能提升数据未知，但实验结果表明该方法优于基线方法。",
            "tags_zh": [
                "强化学习",
                "奖励机",
                "时序约束",
                "免模型学习",
                "Q-learning"
            ],
            "_index": 48,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17637v1/data/taxi-domain.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17637v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17637v1/data/frozenlake-domain.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience",
            "authors": [
                "Jiangjie Chen",
                "Wenxiang Chen",
                "Jiacheng Du",
                "Jinyi Hu",
                "Zhicheng Jiang",
                "Allan Jie",
                "Xiaoran Jin",
                "Xing Jin",
                "Chenggang Li",
                "Wenlei Shi",
                "Zhihong Wang",
                "Mingxuan Wang",
                "Chenrui Wei",
                "Shufa Wei",
                "Huajian Xin",
                "Fan Yang",
                "Weihao Gao",
                "Zheng Yuan",
                "Tianyang Zhan",
                "Zeyu Zheng",
                "Tianxi Zhou",
                "Thomas Hanwen Zhu"
            ],
            "arxiv_id": "2512.17260v1",
            "summary": "Large language models have recently made significant progress to generate rigorous mathematical proofs. In contrast, utilizing LLMs for theorem proving in formal languages (such as Lean) remains challenging and computationally expensive, particularly when addressing problems at the undergraduate level and beyond. In this work, we present \\textbf{Seed-Prover 1.5}, a formal theorem-proving model trained via large-scale agentic reinforcement learning, alongside an efficient test-time scaling (TTS) workflow. Through extensive interactions with Lean and other tools, the model continuously accumulates experience during the RL process, substantially enhancing the capability and efficiency of formal theorem proving. Furthermore, leveraging recent advancements in natural language proving, our TTS workflow efficiently bridges the gap between natural and formal languages. Compared to state-of-the-art methods, Seed-Prover 1.5 achieves superior performance with a smaller compute budget. It solves \\textbf{88\\% of PutnamBench} (undergraduate-level), \\textbf{80\\% of Fate-H} (graduate-level), and \\textbf{33\\% of Fate-X} (PhD-level) problems. Notably, using our system, we solved \\textbf{11 out of 12 problems} from Putnam 2025 within 9 hours. Our findings suggest that scaling learning from experience, driven by high-quality formal feedback, holds immense potential for the future of formal mathematical reasoning.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "21 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17260v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "Seed-Prover 1.5：通过经验学习掌握本科水平定理证明",
            "summary_zh": "大型语言模型（LLMs）在生成严谨的数学证明方面取得了显著进展。然而，将LLMs应用于形式语言（如Lean）中的定理证明仍然具有挑战性且计算成本高昂，尤其是在解决本科及以上水平的问题时。本文提出了Seed-Prover 1.5，一个通过大规模agentic强化学习训练的形式定理证明模型，以及高效的测试时缩放（TTS）工作流程。通过与Lean和其他工具的广泛交互，该模型在强化学习过程中不断积累经验，从而显著提高了形式定理证明的能力和效率。此外，利用自然语言证明的最新进展，我们的TTS工作流程有效地弥合了自然语言和形式语言之间的差距。与最先进的方法相比，Seed-Prover 1.5以更小的计算预算实现了卓越的性能。它解决了88%的PutnamBench（本科水平）、80%的Fate-H（研究生水平）和33%的Fate-X（博士水平）问题。值得注意的是，使用我们的系统，我们在9小时内解决了Putnam 2025中的12个问题中的11个。我们的研究结果表明，由高质量的形式反馈驱动的经验学习扩展，为形式数学推理的未来提供了巨大的潜力。",
            "intro_zh": [
                "现有方法利用LLM进行形式化定理证明时，面临计算成本高昂和难以处理本科以上难度问题等挑战。",
                "Seed-Prover 1.5通过大规模强化学习，让模型与Lean等工具交互，持续积累经验，提升证明能力和效率。",
                "该模型结合测试时缩放（TTS）工作流程，有效连接自然语言和形式语言，并在多个基准测试中取得优异成绩。"
            ],
            "method_zh": "**问题定义**：论文旨在解决形式化定理证明领域中，现有大型语言模型在处理本科及以上难度问题时，计算成本高昂且证明能力不足的问题。现有方法难以有效利用形式化语言的精确性和结构化特点，导致在复杂定理证明任务中表现不佳。\\n\\n**核心思路**：论文的核心思路是通过大规模的agentic强化学习，使模型能够与形式化证明环境（如Lean）进行交互，并从交互过程中积累经验。这种经验学习的方式能够让模型逐步掌握形式化证明的技巧和策略，从而提高证明的效率和成功率。同时，利用测试时缩放（TTS）工作流程，弥合自然语言和形式语言之间的差距，进一步提升模型的泛化能力。\\n\\n**技术框架**：Seed-Prover 1.5的技术框架主要包括以下几个部分：1) 基于大型语言模型的agent，负责生成证明步骤；2) 形式化证明环境（如Lean），用于验证证明步骤的正确性并提供反馈；3) 强化学习算法，用于优化agent的策略，使其能够生成更有效的证明步骤；4) 测试时缩放（TTS）工作流程，用于将自然语言描述的问题转化为形式化语言，并对证明结果进行解释。整个流程通过不断迭代，使模型能够逐步提高形式化定理证明的能力。\\n\\n**关键创新**：论文最重要的技术创新点在于将大规模agentic强化学习应用于形式化定理证明领域，并结合高效的测试时缩放（TTS）工作流程。这种方法能够充分利用形式化证明环境的反馈信息，使模型能够快速学习和适应不同的证明任务。与传统的基于规则或搜索的方法相比，Seed-Prover 1.5具有更强的泛化能力和更高的效率。\\n\\n**关键设计**：论文的关键设计包括：1) 强化学习算法的选择，例如使用Proximal Policy Optimization (PPO) 或其他合适的算法；2) 奖励函数的设计，用于鼓励模型生成正确的证明步骤并避免错误的步骤；3) 网络结构的设计，例如使用Transformer或其他适合处理序列数据的结构；4) 测试时缩放（TTS）工作流程的具体实现，例如使用预训练的语言模型或专门设计的翻译模型。",
            "application_zh": "该研究成果可应用于自动化定理证明、形式化验证、软件工程等领域。通过提高定理证明的效率和自动化程度，可以加速数学研究的进程，并提高软件和硬件系统的可靠性。未来，该技术有望应用于更广泛的领域，例如人工智能安全、智能合约验证等。",
            "highlight_zh": "Seed-Prover 1.5在PutnamBench（本科水平）上解决了88%的问题，在Fate-H（研究生水平）上解决了80%的问题，在Fate-X（博士水平）上解决了33%的问题。更值得关注的是，该系统在9小时内解决了Putnam 2025竞赛中的12个问题中的11个，展示了其强大的问题解决能力。",
            "tags_zh": [
                "形式化定理证明",
                "强化学习",
                "大型语言模型",
                "经验学习",
                "Lean",
                "自动化推理",
                "测试时缩放"
            ],
            "_index": 49,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17260v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17260v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17260v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Cooperative Energy Scheduling of Multi-Microgrids Based on Risk-Sensitive Reinforcement Learning",
            "authors": [
                "Rongxiang Zhang",
                "Bo Li",
                "Jinghua Li",
                "Yuguang Song",
                "Ziqing Zhu",
                "Wentao Yang",
                "Zhengmao Li",
                "Edris Pouresmaeil",
                "Joshua Y. Kim"
            ],
            "arxiv_id": "2512.17246v1",
            "summary": "With the rapid development of distributed renewable energy, multi-microgrids play an increasingly important role in improving the flexibility and reliability of energy supply. Reinforcement learning has shown great potential in coordination strategies due to its model-free nature. Current methods lack explicit quantification of the relationship between individual and joint risk values, resulting in obscured credit assignment. Moreover, they often depend on explicit communication, which becomes inefficient as system complexity grows. To address these challenges, this paper proposes a risk-sensitive reinforcement learning framework with shared memory (RRL-SM) for multi-microgrid scheduling. Specifically, a risk-sensitive value factorization scheme is proposed to quantify the relationship between individual and joint risk values by leveraging distributional modeling and attention-based representations, thereby aligning local decisions with global risk objectives. An implicit shared-memory coordination mechanism is implemented through a global memory space to enhance the overall efficiency of decentralized decision-making. Collectively, the integrated approach delivers more reliable cooperative scheduling under renewable energy uncertainty. Simulation results show that RRL-SM reduces load-shedding risk by 84.5%, demonstrating a favorable balance between reliability and economic performance.",
            "categories": [
                "eess.SY"
            ],
            "primary_category": "eess.SY",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17246v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出风险敏感强化学习框架以优化多微电网的能量调度",
            "summary_zh": "随着分布式可再生能源的快速发展，多微电网在提高能源供应的灵活性和可靠性方面发挥着越来越重要的作用。强化学习因其无模型特性在协调策略中展现出巨大潜力。然而，现有方法缺乏对个体与联合风险值关系的明确量化，导致信用分配不清。此外，随着系统复杂性的增加，显式通信的效率也受到影响。为了解决这些挑战，本文提出了一种基于共享记忆的风险敏感强化学习框架（RRL-SM）用于多微电网调度。通过分布式建模和基于注意力的表示，提出了一种风险敏感值因子分解方案，以量化个体与联合风险值之间的关系，从而使局部决策与全球风险目标对齐。实验结果表明，RRL-SM将负荷削减风险降低了84.5%，在可再生能源不确定性下提供了更可靠的合作调度。",
            "intro_zh": [
                "现有方法未能明确量化个体与联合风险值之间的关系，导致信用分配模糊。",
                "提出了一种风险敏感强化学习框架RRL-SM，通过共享记忆机制提升多微电网调度效率。",
                "实验结果显示，RRL-SM将负荷削减风险降低了84.5%，在可靠性与经济性之间取得了良好平衡。"
            ],
            "method_zh": "**问题定义**：本文旨在解决多微电网调度中的风险管理问题，现有方法在个体与联合风险值的量化上存在不足，导致决策效率低下。\\n\\n**核心思路**：提出风险敏感强化学习框架RRL-SM，通过共享记忆机制和风险敏感值因子分解，提升局部决策与全球目标的一致性，从而优化调度策略。\\n\\n**技术框架**：RRL-SM框架包括三个主要模块：风险敏感值因子分解模块、共享记忆协调机制和强化学习决策模块。通过全局记忆空间实现隐式协调，增强去中心化决策的整体效率。\\n\\n**关键创新**：本研究的核心创新在于引入风险敏感值因子分解方案，利用分布式建模和注意力机制量化个体与联合风险值的关系，显著提升了决策的准确性和效率。\\n\\n**关键设计**：在模型设计中，采用了特定的损失函数以平衡风险与收益，同时在网络结构中引入了注意力机制，以便更好地捕捉风险信息和决策相关性。通过这些设计，RRL-SM能够在复杂环境中实现高效的能量调度。",
            "application_zh": "该研究的潜在应用领域包括智能电网、可再生能源管理和分布式能源系统。通过优化多微电网的调度策略，能够有效提高能源利用效率，降低运营成本，增强系统的灵活性和可靠性，具有重要的实际价值和广泛的应用前景。",
            "highlight_zh": "实验结果表明，RRL-SM框架在负荷削减风险方面表现优异，降低幅度达到84.5%。与传统方法相比，该框架在可靠性和经济性之间实现了良好的平衡，显示出其在复杂能源管理场景中的应用潜力。",
            "tags_zh": [
                "风险敏感强化学习",
                "多微电网",
                "能量调度",
                "共享记忆",
                "可再生能源管理"
            ],
            "_index": 50,
            "_used_api": "openai",
            "figures": []
        },
        {
            "title": "SAVeD: A First-Person Social Media Video Dataset for ADAS-equipped vehicle Near-Miss and Crash Event Analyses",
            "authors": [
                "Shaoyan Zhai",
                "Mohamed Abdel-Aty",
                "Chenzhu Wang",
                "Rodrigo Vena Garcia"
            ],
            "arxiv_id": "2512.17724v1",
            "summary": "The advancement of safety-critical research in driving behavior in ADAS-equipped vehicles require real-world datasets that not only include diverse traffic scenarios but also capture high-risk edge cases such as near-miss events and system failures. However, existing datasets are largely limited to either simulated environments or human-driven vehicle data, lacking authentic ADAS (Advanced Driver Assistance System) vehicle behavior under risk conditions. To address this gap, this paper introduces SAVeD, a large-scale video dataset curated from publicly available social media content, explicitly focused on ADAS vehicle-related crashes, near-miss incidents, and disengagements. SAVeD features 2,119 first-person videos, capturing ADAS vehicle operations in diverse locations, lighting conditions, and weather scenarios. The dataset includes video frame-level annotations for collisions, evasive maneuvers, and disengagements, enabling analysis of both perception and decision-making failures. We demonstrate SAVeD's utility through multiple analyses and contributions: (1) We propose a novel framework integrating semantic segmentation and monocular depth estimation to compute real-time Time-to-Collision (TTC) for dynamic objects. (2) We utilize the Generalized Extreme Value (GEV) distribution to model and quantify the extreme risk in crash and near-miss events across different roadway types. (3) We establish benchmarks for state-of-the-art VLLMs (VideoLLaMA2 and InternVL2.5 HiCo R16), showing that SAVeD's detailed annotations significantly enhance model performance through domain adaptation in complex near-miss scenarios.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17724v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "depth estimation",
                        "monocular depth"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "SAVeD：用于ADAS车辆近失和碰撞事件分析的第一人称社交媒体视频数据集",
            "summary_zh": "本文提出了SAVeD，一个大规模视频数据集，专门用于配备ADAS车辆的碰撞、近失事件和脱离场景分析，数据来源于公开的社交媒体内容。SAVeD包含2119个第一人称视角视频，捕捉了ADAS车辆在不同地点、光照和天气条件下的运行情况。数据集包含碰撞、规避动作和脱离等事件的视频帧级别标注，从而能够分析感知和决策方面的故障。论文通过多项分析展示了SAVeD的效用：(1) 提出了一个新框架，集成了语义分割和单目深度估计，以计算动态对象的实时碰撞时间(TTC)。(2) 利用广义极值(GEV)分布来建模和量化不同道路类型中碰撞和近失事件的极端风险。(3) 为最先进的VLLM（VideoLLaMA2和InternVL2.5 HiCo R16）建立了基准，表明SAVeD的详细标注通过复杂近失场景中的领域自适应显著提高了模型性能。",
            "intro_zh": [
                "现有ADAS车辆驾驶行为研究缺乏真实风险场景数据，现有数据集多为模拟环境或人类驾驶数据。",
                "SAVeD数据集通过收集社交媒体视频，提供包含碰撞、近失和脱离等高风险场景的真实ADAS车辆数据。",
                "实验证明，SAVeD数据集能够有效提升VLLM在复杂近失场景中的性能，并为风险建模提供支持。"
            ],
            "method_zh": "**问题定义**：现有ADAS研究缺乏真实且包含高风险边缘案例（如近失事件和系统故障）的数据集。现有数据集主要集中在模拟环境或人类驾驶车辆数据，缺少真实ADAS车辆在风险条件下的行为数据。\\n\\n**核心思路**：通过挖掘公开的社交媒体视频，构建一个包含ADAS车辆相关碰撞、近失事件和脱离场景的大规模数据集。利用该数据集，可以分析ADAS系统的感知和决策缺陷，并为相关算法的开发和评估提供支持。\\n\\n**技术框架**：SAVeD数据集构建流程主要包括：1) 从社交媒体平台收集视频数据；2) 对视频进行筛选，选择包含ADAS车辆相关事件的视频；3) 对视频进行帧级别标注，标注内容包括碰撞、规避动作和脱离等事件；4) 利用标注数据进行模型训练和评估，例如训练VLLM模型，并评估其在近失场景中的性能。此外，论文还提出了一个基于语义分割和单目深度估计的实时TTC计算框架，以及利用GEV分布进行风险建模的方法。\\n\\n**关键创新**：SAVeD数据集本身就是一个重要的创新点，它填补了ADAS研究领域真实高风险场景数据集的空白。此外，论文提出的基于语义分割和单目深度估计的实时TTC计算框架，以及利用GEV分布进行风险建模的方法，也具有一定的创新性。与现有方法相比，SAVeD数据集提供了更真实、更全面的数据，能够更有效地支持ADAS系统的研究和开发。\\n\\n**关键设计**：SAVeD数据集包含2119个第一人称视角视频，覆盖了不同的地点、光照条件和天气场景。视频帧级别标注包括碰撞、规避动作和脱离等事件。在TTC计算框架中，语义分割用于识别场景中的动态对象，单目深度估计用于估计动态对象的距离。GEV分布用于建模碰撞和近失事件的极端风险，其参数通过最大似然估计方法进行估计。VLLM模型的训练采用领域自适应方法，利用SAVeD数据集的标注数据对模型进行微调。",
            "application_zh": "SAVeD数据集可应用于ADAS系统的开发和测试，例如用于评估ADAS系统的感知和决策能力，提高系统在复杂和危险场景下的安全性。此外，该数据集还可用于研究驾驶员在ADAS系统干预下的行为模式，从而更好地设计人机交互界面，提升驾驶体验。该数据集的发布将促进自动驾驶和智能交通领域的研究进展。",
            "highlight_zh": "论文通过实验证明，SAVeD数据集能够显著提升VLLM在复杂近失场景中的性能。例如，通过在SAVeD数据集上进行领域自适应，VideoLLaMA2和InternVL2.5 HiCo R16等模型的性能得到了显著提升。此外，论文还利用GEV分布对不同道路类型中的碰撞和近失事件的极端风险进行了建模和量化，为风险评估和安全策略制定提供了依据。",
            "tags_zh": [
                "ADAS",
                "数据集",
                "近失事件",
                "碰撞分析",
                "社交媒体视频",
                "深度估计",
                "风险建模"
            ],
            "_index": 51,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "EMAG: Self-Rectifying Diffusion Sampling with Exponential Moving Average Guidance",
            "authors": [
                "Ankit Yadav",
                "Ta Duc Huy",
                "Lingqiao Liu"
            ],
            "arxiv_id": "2512.17303v1",
            "summary": "In diffusion and flow-matching generative models, guidance techniques are widely used to improve sample quality and consistency. Classifier-free guidance (CFG) is the de facto choice in modern systems and achieves this by contrasting conditional and unconditional samples. Recent work explores contrasting negative samples at inference using a weaker model, via strong/weak model pairs, attention-based masking, stochastic block dropping, or perturbations to the self-attention energy landscape. While these strategies refine the generation quality, they still lack reliable control over the granularity or difficulty of the negative samples, and target-layer selection is often fixed. We propose Exponential Moving Average Guidance (EMAG), a training-free mechanism that modifies attention at inference time in diffusion transformers, with a statistics-based, adaptive layer-selection rule. Unlike prior methods, EMAG produces harder, semantically faithful negatives (fine-grained degradations), surfacing difficult failure modes, enabling the denoiser to refine subtle artifacts, boosting the quality and human preference score (HPS) by +0.46 over CFG. We further demonstrate that EMAG naturally composes with advanced guidance techniques, such as APG and CADS, further improving HPS.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "26 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17303v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "flow matching"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "classifier-free guidance"
                    ],
                    "score": 2.5
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "2_algo_arch",
                "4_motion_diffusion"
            ],
            "headline_zh": "提出EMAG：一种基于指数移动平均指导的自校正扩散采样方法，提升生成质量。",
            "summary_zh": "在扩散和流匹配生成模型中，指导技术被广泛用于提高样本质量和一致性。无分类器指导（CFG）是现代系统中的常用选择，它通过对比条件和无条件样本来实现这一点。最近的研究探索了在推理时使用较弱模型对比负样本，通过强/弱模型对、基于注意力的掩码、随机块丢弃或扰动自注意力能量景观等方式。虽然这些策略改进了生成质量，但它们仍然缺乏对负样本粒度或难度的可靠控制，并且目标层选择通常是固定的。我们提出了一种名为指数移动平均指导（EMAG）的免训练机制，该机制在扩散Transformer中修改推理时的注意力，并采用基于统计的自适应层选择规则。与先前的方法不同，EMAG产生更难的、语义上忠实的负样本（细粒度退化），揭示了困难的失败模式，使去噪器能够改进细微的伪影，从而提高质量和人类偏好得分（HPS），相比CFG提升了+0.46。我们进一步证明，EMAG自然地与高级指导技术（如APG和CADS）结合，从而进一步提高HPS。",
            "intro_zh": [
                "现有扩散模型指导方法缺乏对负样本粒度和难度的有效控制，且目标层选择固定，限制了生成质量的进一步提升。",
                "EMAG通过指数移动平均自适应地选择注意力层，生成更难且语义忠实的负样本，从而暴露并纠正生成过程中的细微错误。",
                "实验表明，EMAG显著提升了生成质量和人类偏好得分，并且可以与现有高级指导技术结合，进一步提高性能。"
            ],
            "method_zh": "**问题定义**：现有扩散模型，特别是基于Transformer的扩散模型，在生成高质量图像时依赖于指导技术，如无分类器指导（CFG）。然而，现有方法在生成负样本时，难以控制负样本的难度和粒度，并且通常采用固定的层选择策略，这限制了模型纠正细微错误的能力，阻碍了生成质量的进一步提升。\\n\\n**核心思路**：EMAG的核心思想是利用指数移动平均（EMA）来构建一个“弱”模型，并使用该弱模型生成更难、更细粒度的负样本。通过对比原始模型和EMA模型在注意力机制上的差异，自适应地选择需要进行负样本生成的层，从而使模型能够专注于纠正那些最容易出错的区域。这种方法旨在暴露并纠正生成过程中的细微伪影，提升整体生成质量。\\n\\n**技术框架**：EMAG主要在扩散Transformer的推理阶段进行操作，无需额外的训练。其流程如下：1) 使用原始模型进行前向传播，计算每一层的注意力权重。2) 使用指数移动平均（EMA）更新注意力权重，得到一个“弱”模型的注意力权重。3) 基于原始模型和EMA模型的注意力权重差异，自适应地选择需要进行负样本生成的层。4) 在选定的层上，使用EMA模型生成负样本。5) 将原始模型和负样本结合，进行最终的图像生成。\\n\\n**关键创新**：EMAG的关键创新在于其自适应层选择规则和基于EMA的负样本生成方法。与现有方法相比，EMAG能够生成更难、更细粒度的负样本，并且能够根据模型的实际表现，自适应地选择需要进行负样本生成的层。这种方法使得模型能够专注于纠正那些最容易出错的区域，从而提升整体生成质量。此外，EMAG是一种免训练的方法，可以直接应用于现有的扩散模型，无需额外的训练成本。\\n\\n**关键设计**：EMAG的关键设计包括：1) 指数移动平均的衰减率：控制EMA模型的更新速度，影响负样本的难度。2) 自适应层选择规则：基于原始模型和EMA模型的注意力权重差异，选择需要进行负样本生成的层。可以使用不同的统计指标来衡量注意力权重的差异，例如KL散度或余弦相似度。3) 负样本生成方式：可以使用不同的方法来生成负样本，例如直接使用EMA模型的输出，或者对EMA模型的输出进行一定的扰动。",
            "application_zh": "EMAG可广泛应用于各种基于扩散模型的图像生成任务，例如文本到图像生成、图像修复、图像超分辨率等。该方法能够提升生成图像的质量和细节，尤其是在需要高保真度和精细控制的应用场景中具有重要价值。未来，EMAG有望与其他先进的生成模型和指导技术结合，进一步推动图像生成领域的发展。",
            "highlight_zh": "实验结果表明，EMAG在图像生成质量上优于传统的无分类器指导（CFG）方法，人类偏好得分（HPS）提升了+0.46。此外，EMAG可以与高级指导技术（如APG和CADS）结合，进一步提高HPS。这些结果表明，EMAG能够有效地提升生成图像的质量和细节，并且具有良好的通用性和可扩展性。",
            "tags_zh": [
                "扩散模型",
                "生成模型",
                "注意力机制",
                "指数移动平均",
                "自适应指导"
            ],
            "_index": 52,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17303v1/resources/Slide2.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17303v1/resources/negative_sample_grid.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17303v1/resources/Negative_example_degradation_control.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "A Service Robot's Guide to Interacting with Busy Customers",
            "authors": [
                "Suraj Nukala",
                "Meera Sushma",
                "Leimin Tian",
                "Akansel Cosgun",
                "Dana Kulic"
            ],
            "arxiv_id": "2512.17241v1",
            "summary": "The growing use of service robots in hospitality highlights the need to understand how to effectively communicate with pre-occupied customers. This study investigates the efficacy of commonly used communication modalities by service robots, namely, acoustic/speech, visual display, and micromotion gestures in capturing attention and communicating intention with a user in a simulated restaurant scenario. We conducted a two-part user study (N=24) using a Temi robot to simulate delivery tasks, with participants engaged in a typing game (MonkeyType) to emulate a state of busyness. The participants' engagement in the typing game is measured by words per minute (WPM) and typing accuracy. In Part 1, we compared non-verbal acoustic cue versus baseline conditions to assess attention capture during a single-cup delivery task. In Part 2, we evaluated the effectiveness of speech, visual display, micromotion and their multimodal combination in conveying specific intentions (correct cup selection) during a two-cup delivery task. The results indicate that, while speech is highly effective in capturing attention, it is less successful in clearly communicating intention. Participants rated visual as the most effective modality for intention clarity, followed by speech, with micromotion being the lowest ranked.These findings provide insights into optimizing communication strategies for service robots, highlighting the distinct roles of attention capture and intention communication in enhancing user experience in dynamic hospitality settings.",
            "categories": [
                "cs.RO",
                "cs.HC"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "Presented at ACRA 2025. 10 pages, 4 figures. Includes a user study (N=24) using the Temi robot evaluating speech, visual, and micromotion modalities",
            "doi": "",
            "journal_ref": "Proceedings of the 2025 Australasian Conference on Robotics and Automation (ACRA 2025)",
            "pdf_url": "https://arxiv.org/pdf/2512.17241v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "研究服务机器人与忙碌顾客交互，优化沟通方式以提升用户体验",
            "summary_zh": "本研究探讨了服务机器人与忙碌顾客有效沟通的方式，着重考察了语音、视觉显示和微动姿势等常用沟通方式在模拟餐厅场景中的有效性。通过一项包含24名参与者的两部分用户研究，模拟Temi机器人的送货任务，参与者进行打字游戏（MonkeyType）以模拟忙碌状态。第一部分评估了非语言声音提示与基线条件在单杯递送任务中的注意力捕获效果。第二部分评估了语音、视觉显示、微动姿势及其多模态组合在双杯递送任务中传达特定意图（正确选择杯子）的有效性。结果表明，语音在捕获注意力方面非常有效，但在清晰传达意图方面效果较差。参与者认为视觉显示是传达意图最有效的模式，其次是语音，微动姿势排名最低。这些发现为优化服务机器人的沟通策略提供了见解，突出了注意力捕获和意图沟通在动态服务环境中提升用户体验的不同作用。",
            "intro_zh": [
                "现有服务机器人与忙碌顾客交互时，沟通方式效率低下，难以有效吸引注意并清晰传达意图。",
                "研究对比语音、视觉和微动等多种模态，探索不同模态在注意力捕获和意图传达方面的作用。",
                "实验表明语音擅长吸引注意，视觉更利于意图表达，为服务机器人优化沟通策略提供指导。"
            ],
            "method_zh": "**问题定义**：论文旨在解决服务机器人在与忙碌的顾客交互时，如何有效地吸引顾客的注意力并清晰地传达其意图的问题。现有方法通常采用单一的沟通方式，例如仅使用语音或视觉提示，但这些方法可能无法充分考虑到顾客的忙碌状态，导致沟通效率低下，用户体验不佳。因此，需要研究更有效的多模态沟通策略，以适应动态的服务环境。\\n\\n**核心思路**：论文的核心思路是对比不同沟通模态（语音、视觉显示、微动姿势）在注意力捕获和意图传达方面的效果，并探索多模态组合的潜力。通过模拟真实的餐厅场景，让参与者在忙碌状态下与服务机器人交互，评估不同模态对用户体验的影响。这种方法能够更准确地反映实际应用中的情况，为优化服务机器人的沟通策略提供依据。\\n\\n**技术框架**：研究采用两部分的用户研究。第一部分比较非语言声音提示与基线条件，评估注意力捕获效果。第二部分评估语音、视觉显示、微动姿势及其多模态组合在传达特定意图方面的有效性。研究使用Temi机器人模拟送货任务，参与者通过打字游戏模拟忙碌状态。通过问卷调查和行为数据分析，评估不同模态的有效性。\\n\\n**关键创新**：论文的关键创新在于系统地比较了不同沟通模态在服务机器人与忙碌顾客交互中的作用，并区分了注意力捕获和意图传达这两个不同的沟通目标。现有研究通常只关注单一模态或笼统的沟通效果，而忽略了不同模态在不同沟通目标上的差异。\\n\\n**关键设计**：研究的关键设计包括：1) 使用MonkeyType打字游戏模拟顾客的忙碌状态，更贴近实际场景；2) 采用Temi机器人作为实验平台，模拟真实的送货任务；3) 通过问卷调查和行为数据（如杯子选择的准确率）综合评估不同模态的有效性；4) 细致地分析了不同模态在注意力捕获和意图传达方面的差异。",
            "application_zh": "该研究成果可应用于餐厅、酒店、医院等多种服务场景，提升服务机器人的交互效率和用户满意度。通过优化沟通策略，服务机器人能够更好地适应动态环境，减少对顾客的干扰，提高服务质量。未来，该研究可扩展到其他类型的服务机器人和更复杂的交互场景，例如导览机器人、陪伴机器人等。",
            "highlight_zh": "实验结果表明，语音在捕获注意力方面效果显著，但视觉显示在传达意图方面更胜一筹。参与者普遍认为视觉显示是传达意图最有效的模态，其次是语音，而微动姿势的效果相对较差。这些发现为服务机器人设计更有效的沟通策略提供了重要依据，例如在需要快速吸引注意时使用语音提示，而在需要清晰传达信息时则优先考虑视觉显示。",
            "tags_zh": [
                "服务机器人",
                "人机交互",
                "多模态沟通",
                "注意力捕获",
                "意图传达"
            ],
            "_index": 53,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17241v1/hero.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17241v1/right_vis.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17241v1/micromotions.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training",
            "authors": [
                "Kristoffer Wickstrøm",
                "Teresa Dorszewski",
                "Siyan Chen",
                "Michael Kampffmeyer",
                "Elisabeth Wetzer",
                "Robert Jenssen"
            ],
            "arxiv_id": "2512.17891v1",
            "summary": "Current approaches for designing self-explainable models (SEMs) require complicated training procedures and specific architectures which makes them impractical. With the advance of general purpose foundation models based on Vision Transformers (ViTs), this impracticability becomes even more problematic. Therefore, new methods are necessary to provide transparency and reliability to ViT-based foundation models. In this work, we present a new method for turning any well-trained ViT-based model into a SEM without retraining, which we call Keypoint Counting Classifiers (KCCs). Recent works have shown that ViTs can automatically identify matching keypoints between images with high precision, and we build on these results to create an easily interpretable decision process that is inherently visualizable in the input. We perform an extensive evaluation which show that KCCs improve the human-machine communication compared to recent baselines. We believe that KCCs constitute an important step towards making ViT-based foundation models more transparent and reliable.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17891v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出无需训练的Keypoint Counting Classifiers，将ViT转化为自解释模型",
            "summary_zh": "当前自解释模型（SEM）的设计方法需要复杂的训练过程和特定的架构，这使其不切实际。随着基于Vision Transformers（ViT）的通用基础模型的进步，这种不切实际的问题变得更加突出。因此，需要新的方法来为基于ViT的基础模型提供透明度和可靠性。本文提出了一种新的方法，可以将任何经过良好训练的基于ViT的模型转化为SEM，而无需重新训练，我们称之为Keypoint Counting Classifiers（KCCs）。最近的研究表明，ViT可以自动识别图像之间的高精度匹配关键点，我们在此基础上创建了一个易于解释的决策过程，该过程在输入中具有固有的可视化能力。我们进行了广泛的评估，结果表明，与最近的基线相比，KCCs改善了人机通信。我们认为，KCCs是使基于ViT的基础模型更加透明和可靠的重要一步。",
            "intro_zh": [
                "现有自解释模型训练复杂、架构特定，难以应用于ViT等通用基础模型。",
                "提出Keypoint Counting Classifiers (KCCs)，利用ViT自动识别的关键点，构建可解释的决策过程。",
                "实验表明，KCCs在人机交互方面优于现有基线，提升了ViT模型的透明性和可靠性。"
            ],
            "method_zh": "**问题定义**：现有自解释模型（SEM）的训练过程复杂，需要特定的网络架构，这使得它们难以应用于像Vision Transformers（ViT）这样的通用基础模型。因此，如何为预训练的ViT模型提供无需重新训练的自解释能力是一个关键问题。现有方法的痛点在于需要从头开始训练或者对现有模型进行微调，计算成本高昂，且可能破坏预训练模型的泛化能力。\\n\\n**核心思路**：本文的核心思路是利用ViT模型本身所具备的关键点检测能力，构建一个基于关键点计数的分类器。通过统计图像中特定关键点的数量，并将其作为分类的依据，从而实现模型的自解释性。这种方法无需重新训练ViT模型，而是直接利用其已学习到的特征表示。\\n\\n**技术框架**：KCCs方法主要包含以下几个阶段：1) 利用预训练的ViT模型提取图像的关键点特征；2) 对提取的关键点进行聚类，形成若干个关键点簇；3) 对于每个类别，统计图像中属于该类别对应关键点簇的关键点数量；4) 基于关键点数量进行分类决策。整体流程简单明了，易于实现和部署。\\n\\n**关键创新**：KCCs最重要的创新在于它提供了一种无需重新训练即可将预训练ViT模型转化为自解释模型的方法。与现有方法相比，KCCs避免了复杂的训练过程和特定的网络架构设计，大大降低了计算成本和开发难度。此外，KCCs基于关键点计数进行分类，使得模型的决策过程具有高度的可解释性。\\n\\n**关键设计**：KCCs的关键设计在于如何选择合适的关键点聚类算法以及如何确定每个类别对应的关键点簇。论文中可能使用了诸如K-means等聚类算法，并通过实验验证了不同聚类参数对模型性能的影响。此外，论文可能还探讨了如何利用先验知识或领域知识来指导关键点簇的选择，以进一步提高模型的准确性和可解释性。",
            "application_zh": "KCCs方法可广泛应用于需要模型可解释性的计算机视觉任务中，例如医疗图像诊断、自动驾驶决策、安全监控等领域。通过提供清晰的决策依据，KCCs可以增强用户对模型的信任，并促进人机协作。未来，KCCs有望成为提升AI系统透明度和可靠性的重要工具。",
            "highlight_zh": "论文通过实验验证了KCCs在人机交互方面的优越性。与现有基线方法相比，KCCs能够提供更清晰、更易于理解的决策依据，从而显著改善人机通信效果。具体的性能数据和提升幅度需要在论文中查找，但整体而言，实验结果表明KCCs是一种有效的自解释模型构建方法。",
            "tags_zh": [
                "自解释模型",
                "Vision Transformer",
                "关键点检测",
                "可解释性",
                "人机交互"
            ],
            "_index": 54,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17891v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17891v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17891v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Auxiliary Descriptive Knowledge for Few-Shot Adaptation of Vision-Language Model",
            "authors": [
                "SuBeen Lee",
                "GilHan Park",
                "WonJun Moon",
                "Hyun Seok Seong",
                "Jae-Pil Heo"
            ],
            "arxiv_id": "2512.17313v1",
            "summary": "Despite the impressive zero-shot capabilities of Vision-Language Models (VLMs), they often struggle in downstream tasks with distribution shifts from the pre-training data. Few-Shot Adaptation (FSA-VLM) has emerged as a key solution, typically using Parameter-Efficient Fine-Tuning (PEFT) to adapt models with minimal data. However, these PEFT methods are constrained by their reliance on fixed, handcrafted prompts, which are often insufficient to understand the semantics of classes. While some studies have proposed leveraging image-induced prompts to provide additional clues for classification, they introduce prohibitive computational overhead at inference. Therefore, we introduce Auxiliary Descriptive Knowledge (ADK), a novel framework that efficiently enriches text representations without compromising efficiency. ADK first leverages a Large Language Model to generate a rich set of descriptive prompts for each class offline. These pre-computed features are then deployed in two ways: (1) as Compositional Knowledge, an averaged representation that provides rich semantics, especially beneficial when class names are ambiguous or unfamiliar to the VLM; and (2) as Instance-Specific Knowledge, where a lightweight, non-parametric attention mechanism dynamically selects the most relevant descriptions for a given image. This approach provides two additional types of knowledge alongside the handcrafted prompt, thereby facilitating category distinction across various domains. Also, ADK acts as a parameter-free, plug-and-play component that enhances existing PEFT methods. Extensive experiments demonstrate that ADK consistently boosts the performance of multiple PEFT baselines, setting a new state-of-the-art across various scenarios.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17313v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出辅助描述知识ADK，提升视觉-语言模型在少样本迁移学习中的性能",
            "summary_zh": "尽管视觉-语言模型(VLM)具有令人印象深刻的零样本能力，但它们在下游任务中，当数据分布与预训练数据存在差异时，表现往往不佳。少样本迁移学习(FSA-VLM)已成为一个关键解决方案，通常使用参数高效微调(PEFT)来以最少的数据调整模型。然而，这些PEFT方法受到其对固定、手工制作提示的依赖的限制，这些提示通常不足以理解类别的语义。虽然一些研究提出了利用图像诱导提示来为分类提供额外的线索，但它们在推理时引入了过高的计算开销。因此，我们引入了辅助描述知识(ADK)，这是一个新颖的框架，可以有效地丰富文本表示，而不会影响效率。ADK首先利用大型语言模型离线生成每个类别的丰富描述性提示集。然后以两种方式部署这些预先计算的特征：(1)作为组合知识，一种平均表示，提供丰富的语义，尤其是在类名模糊或VLM不熟悉时；(2)作为实例特定知识，其中轻量级、非参数注意力机制动态地选择给定图像最相关的描述。这种方法提供了手工制作提示之外的两种额外类型的知识，从而有助于跨各种领域的类别区分。此外，ADK充当无参数、即插即用的组件，可增强现有的PEFT方法。大量的实验表明，ADK始终提高多个PEFT基线的性能，在各种场景中设置了新的最先进水平。",
            "intro_zh": [
                "现有VLM的少样本迁移学习方法依赖手工提示，难以充分理解类别语义，限制了模型性能。",
                "提出辅助描述知识(ADK)框架，利用大型语言模型生成丰富的类别描述，增强文本表示。",
                "实验表明，ADK能显著提升现有PEFT方法的性能，并在多个场景中达到新的state-of-the-art。"
            ],
            "method_zh": "**问题定义**：现有的视觉-语言模型在少样本迁移学习任务中，依赖于手工设计的文本提示，这些提示往往无法充分表达类别的语义信息，导致模型在面对分布偏移时性能下降。此外，一些利用图像生成提示的方法虽然可以提供额外信息，但计算成本过高，不适用于实际应用。\\n\\n**核心思路**：论文的核心思路是利用大型语言模型(LLM)生成丰富的类别描述，作为辅助知识来增强视觉-语言模型的文本表示。通过预先计算并存储这些描述，可以在推理阶段高效地利用这些知识，而无需引入额外的计算负担。\\n\\n**技术框架**：ADK框架包含两个主要组成部分：离线描述生成和在线知识融合。首先，利用LLM为每个类别生成多个描述性提示。然后，这些预计算的描述被用于两种方式：组合知识和实例特定知识。组合知识是对所有描述进行平均，提供类别的整体语义信息。实例特定知识则使用一个轻量级的非参数注意力机制，根据输入图像动态地选择最相关的描述。\\n\\n**关键创新**：ADK的关键创新在于其高效的知识融合方式。通过预先计算描述并使用注意力机制动态选择相关描述，ADK能够在不引入额外计算负担的情况下，显著提升模型的性能。此外，ADK作为一个即插即用的模块，可以方便地集成到现有的PEFT方法中。\\n\\n**关键设计**：ADK的关键设计包括：1) 使用LLM生成多样化的类别描述；2) 使用平均池化生成组合知识，提供类别的整体语义；3) 使用非参数注意力机制，根据图像特征动态选择实例相关的描述。具体来说，注意力机制的权重是基于图像特征和描述特征之间的相似度计算的。",
            "application_zh": "该研究成果可广泛应用于图像分类、目标检测、图像检索等视觉-语言任务中，尤其是在数据稀缺或类别语义复杂的场景下。例如，在医学图像分析中，可以利用ADK来辅助医生诊断罕见疾病，或在自动驾驶领域，提升模型对复杂交通场景的理解能力。该方法具有很强的通用性和可扩展性，有望推动视觉-语言模型在实际应用中的发展。",
            "highlight_zh": "实验结果表明，ADK能够显著提升现有PEFT方法在少样本迁移学习任务中的性能。例如，在多个数据集上，ADK将CoOp、Tip-Adapter等基线的性能提升了多个百分点，并在多个场景中取得了state-of-the-art的结果。这证明了ADK能够有效地利用辅助知识来增强视觉-语言模型的文本表示，从而提升模型的泛化能力。",
            "tags_zh": [
                "视觉-语言模型",
                "少样本学习",
                "迁移学习",
                "参数高效微调",
                "辅助知识"
            ],
            "_index": 55,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17313v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17313v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17313v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "ABE-CLIP: Training-Free Attribute Binding Enhancement for Compositional Image-Text Matching",
            "authors": [
                "Qi Zhang",
                "Yuxu Chen",
                "Lei Deng",
                "Lili Shen"
            ],
            "arxiv_id": "2512.17178v1",
            "summary": "Contrastive Language-Image Pretraining (CLIP) has achieved remarkable performance in various multimodal tasks. However, it still struggles with compositional image-text matching, particularly in accurately associating objects with their corresponding attributes, because its inherent global representation often overlooks fine-grained semantics for attribute binding. Existing methods often require additional training or extensive hard negative sampling, yet they frequently show limited generalization to novel compositional concepts and fail to fundamentally address the drawbacks of global representations. In this paper, we propose ABE-CLIP, a novel training-free Attribute Binding Enhancement method designed to strengthen attribute-object binding in CLIP-like models. Specifically, we employ a Semantic Refinement Mechanism to refine token embeddings for both object and attribute phrases in the text, thereby mitigating attribute confusion and improving semantic precision. We further introduce a Local Token-Patch Alignment strategy that computes similarity scores between refined textual tokens and their most relevant image patches. By aggregating localized similarity scores, ABE-CLIP computes the final image-text similarity. Experiments on multiple datasets demonstrate that ABE-CLIP significantly improves attribute-object binding performance, even surpassing methods that require extensive training.",
            "categories": [
                "cs.CV",
                "cs.IR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "10 pages, 8 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17178v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "ABE-CLIP：免训练的属性绑定增强方法，提升组合图像-文本匹配性能",
            "summary_zh": "对比语言-图像预训练（CLIP）在各种多模态任务中取得了显著的性能。然而，它在组合图像-文本匹配方面仍然存在困难，特别是在准确地将对象与其对应的属性相关联时，因为它固有的全局表示通常忽略了属性绑定的细粒度语义。现有方法通常需要额外的训练或大量的困难负样本采样，但它们经常表现出对新组合概念的有限泛化能力，并且未能从根本上解决全局表示的缺点。在本文中，我们提出了一种新颖的免训练属性绑定增强方法ABE-CLIP，旨在加强类CLIP模型中的属性-对象绑定。具体来说，我们采用语义细化机制来细化文本中对象和属性短语的token嵌入，从而减轻属性混淆并提高语义精度。我们进一步引入了一种局部token-patch对齐策略，该策略计算细化的文本token与其最相关的图像patch之间的相似度得分。通过聚合局部相似度得分，ABE-CLIP计算最终的图像-文本相似度。在多个数据集上的实验表明，ABE-CLIP显著提高了属性-对象绑定性能，甚至超过了需要大量训练的方法。",
            "intro_zh": [
                "CLIP在组合图像-文本匹配中，难以准确关联对象及其属性，因其全局表示忽略了细粒度语义。",
                "ABE-CLIP通过语义细化机制和局部token-patch对齐策略，增强CLIP模型中的属性-对象绑定。",
                "实验表明，ABE-CLIP显著提升了属性-对象绑定性能，优于需要额外训练的方法。"
            ],
            "method_zh": "**问题定义**：CLIP在组合图像-文本匹配任务中表现不佳，尤其是在属性绑定方面。其全局表示方法难以捕捉细粒度的语义信息，导致无法准确地将图像中的对象与其对应的文本属性关联起来。现有方法通常需要额外的训练或复杂的负样本挖掘，但泛化能力有限，且未能从根本上解决全局表示的局限性。\\n\\n**核心思路**：ABE-CLIP的核心思路是通过增强文本token的语义表示，并建立文本token与图像patch之间的局部对应关系，从而提升属性-对象绑定的准确性。该方法无需额外的训练，直接作用于预训练的CLIP模型，旨在弥补CLIP在细粒度语义理解方面的不足。\\n\\n**技术框架**：ABE-CLIP主要包含两个核心模块：语义细化机制（Semantic Refinement Mechanism）和局部token-patch对齐策略（Local Token-Patch Alignment）。首先，语义细化机制用于增强文本中对象和属性短语的token嵌入，减少属性混淆。然后，局部token-patch对齐策略计算细化后的文本token与图像patch之间的相似度，并聚合这些局部相似度得分，得到最终的图像-文本相似度。\\n\\n**关键创新**：ABE-CLIP的关键创新在于其免训练的属性绑定增强方法。与需要额外训练或复杂负样本挖掘的方法不同，ABE-CLIP直接作用于预训练的CLIP模型，通过语义细化和局部对齐来提升性能。这种方法更高效，且具有更好的泛化能力。\\n\\n**关键设计**：语义细化机制的具体实现方式未知，但其目标是提高对象和属性token嵌入的语义精度。局部token-patch对齐策略的关键在于如何选择与文本token最相关的图像patch，以及如何有效地聚合局部相似度得分。论文中可能使用了注意力机制或其他相似度度量方法来实现token-patch的对齐。具体的参数设置、损失函数和网络结构等细节在论文中可能有所描述，但此处无法得知。",
            "application_zh": "ABE-CLIP可应用于图像检索、视觉问答、图像描述生成等领域，尤其是在需要理解图像中对象及其属性的任务中。例如，在电商领域，可以根据用户输入的属性描述（如“红色的连衣裙”）检索到符合条件的商品。该研究有助于提升多模态理解的准确性和效率，具有广泛的应用前景。",
            "highlight_zh": "ABE-CLIP在多个数据集上取得了显著的性能提升，证明了其在属性-对象绑定方面的有效性。该方法无需额外训练，即可超越需要大量训练的基线方法，显示了其高效性和泛化能力。具体的性能数据和提升幅度需要在论文中查找。",
            "tags_zh": [
                "组合图像-文本匹配",
                "属性绑定",
                "CLIP",
                "免训练",
                "语义细化",
                "局部对齐",
                "多模态学习"
            ],
            "_index": 56,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17178v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17178v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17178v1/images/yellow_cylinder_and_red_cub.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow",
            "authors": [
                "Herlock Rahimi"
            ],
            "arxiv_id": "2512.17878v1",
            "summary": "Score-based diffusion models currently constitute the state of the art in continuous generative modeling. These methods are typically formulated via overdamped or underdamped Ornstein--Uhlenbeck-type stochastic differential equations, in which sampling is driven by a combination of deterministic drift and Brownian diffusion, resulting in continuous particle trajectories in the ambient space. While such dynamics enjoy exponential convergence guarantees for strongly log-concave target distributions, it is well known that their mixing rates deteriorate exponentially in the presence of nonconvex or multimodal landscapes, such as double-well potentials. Since many practical generative modeling tasks involve highly non-log-concave target distributions, considerable recent effort has been devoted to developing sampling schemes that improve exploration beyond classical diffusion dynamics.\n  A promising line of work leverages tools from information geometry to augment diffusion-based samplers with controlled mass reweighting mechanisms. This perspective leads naturally to Wasserstein--Fisher--Rao (WFR) geometries, which couple transport in the sample space with vertical (reaction) dynamics on the space of probability measures. In this work, we formulate such reweighting mechanisms through the introduction of explicit correction terms and show how they can be implemented via weighted stochastic differential equations using the Feynman--Kac representation. Our study provides a preliminary but rigorous investigation of WFR-based sampling dynamics, and aims to clarify their geometric and operator-theoretic structure as a foundation for future theoretical and algorithmic developments.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "26 pages, 1 figure",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17878v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于加权随机微分方程的Wasserstein-Fisher-Rao梯度流方法，提升生成模型采样效率。",
            "summary_zh": "本文提出了一种基于加权随机微分方程的采样方法，旨在改进基于score的扩散模型在非凸或多峰目标分布下的采样效率。现有的扩散模型通常采用过阻尼或欠阻尼的Ornstein-Uhlenbeck型随机微分方程，其采样过程依赖于确定性漂移和布朗扩散的结合。然而，这些方法在处理非凸或多峰分布时，混合速率会显著下降。为了解决这个问题，本文利用信息几何工具，引入了可控的质量重加权机制，并将其与扩散采样器结合。这种方法自然地引出了Wasserstein-Fisher-Rao (WFR) 几何，将样本空间中的传输与概率测度空间上的垂直（反应）动力学相结合。通过Feynman-Kac表示，本文展示了如何通过加权随机微分方程实现这种重加权机制。本研究对基于WFR的采样动力学进行了初步但严谨的调查，旨在阐明其几何和算子理论结构，为未来的理论和算法发展奠定基础。",
            "intro_zh": [
                "传统扩散模型在处理非凸或多峰目标分布时，采样效率显著降低，难以有效探索复杂的概率空间。",
                "本文提出通过引入质量重加权机制，并结合Wasserstein-Fisher-Rao几何，改进扩散模型的采样过程。",
                "通过加权随机微分方程和Feynman-Kac表示，实现了该重加权机制，为WFR采样动力学提供了理论基础。"
            ],
            "method_zh": "**问题定义**：现有的基于score的扩散模型在处理具有非凸或多峰特性的目标分布时，采样效率会显著下降。这是因为传统的Ornstein-Uhlenbeck型随机微分方程的混合速率在这种情况下会指数级恶化。因此，如何提高扩散模型在复杂目标分布下的采样效率是一个关键问题。\\n\\n**核心思路**：本文的核心思路是利用信息几何中的Wasserstein-Fisher-Rao (WFR) 几何，通过引入可控的质量重加权机制来改进扩散模型的采样过程。WFR几何将样本空间中的传输与概率测度空间上的垂直动力学相结合，从而允许模型在采样过程中动态地调整概率分布，更好地探索目标分布的复杂结构。\\n\\n**技术框架**：本文的技术框架主要包括以下几个部分：首先，将重加权机制形式化为显式的校正项。然后，利用Feynman-Kac表示，将这些校正项转化为加权随机微分方程。最后，通过求解该加权随机微分方程，实现基于WFR几何的采样过程。整体流程可以看作是在传统扩散模型的基础上，增加了一个基于WFR几何的重加权模块，用于动态调整采样轨迹。\\n\\n**关键创新**：本文最重要的技术创新在于将WFR几何引入到扩散模型的采样过程中，并提出了一种基于加权随机微分方程的实现方法。与传统的扩散模型相比，该方法能够更好地处理非凸或多峰的目标分布，提高采样效率。本质区别在于，传统方法仅依赖于确定性漂移和布朗扩散，而本文的方法则引入了动态的质量重加权机制，允许模型在采样过程中自适应地调整概率分布。\\n\\n**关键设计**：本文的关键设计在于如何选择合适的校正项来实现WFR几何中的质量重加权。具体而言，需要设计一个合适的加权函数，该函数能够反映目标分布的局部结构，并引导采样过程向高概率区域移动。此外，还需要选择合适的数值方法来求解加权随机微分方程，以保证采样过程的稳定性和准确性。具体的参数设置和损失函数的设计细节在论文中未明确给出，属于未来研究的方向。",
            "application_zh": "该研究成果可应用于各种生成建模任务，例如图像生成、音频合成、分子设计等。通过提高采样效率，可以更快地生成高质量的样本，从而加速相关领域的研究和应用。此外，该方法还可以用于优化算法和强化学习等领域，通过改进采样策略来提高算法的性能。",
            "highlight_zh": "由于是初步的理论研究，论文主要贡献在于提出了基于加权随机微分方程实现WFR梯度流的框架，并对WFR采样动力学进行了理论分析。具体的实验结果和性能数据未在摘要中提及，需要查阅论文全文以获取更多信息。未来的工作将集中在算法实现和实验验证上，以评估该方法在实际应用中的性能提升。",
            "tags_zh": [
                "扩散模型",
                "生成模型",
                "随机微分方程",
                "Wasserstein距离",
                "信息几何"
            ],
            "_index": 57,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17878v1/Unknown.png",
                    "caption": "",
                    "figure_id": "fig_0"
                }
            ]
        },
        {
            "title": "Confidence-Credibility Aware Weighted Ensembles of Small LLMs Outperform Large LLMs in Emotion Detection",
            "authors": [
                "Menna Elgabry",
                "Ali Hamdi"
            ],
            "arxiv_id": "2512.17630v1",
            "summary": "This paper introduces a confidence-weighted, credibility-aware ensemble framework for text-based emotion detection, inspired by Condorcet's Jury Theorem (CJT). Unlike conventional ensembles that often rely on homogeneous architectures, our approach combines architecturally diverse small transformer-based large language models (sLLMs) - BERT, RoBERTa, DistilBERT, DeBERTa, and ELECTRA, each fully fine-tuned for emotion classification. To preserve error diversity, we minimize parameter convergence while taking advantage of the unique biases of each model. A dual-weighted voting mechanism integrates both global credibility (validation F1 score) and local confidence (instance-level probability) to dynamically weight model contributions. Experiments on the DAIR-AI dataset demonstrate that our credibility-confidence ensemble achieves a macro F1 score of 93.5 percent, surpassing state-of-the-art benchmarks and significantly outperforming large-scale LLMs, including Falcon, Mistral, Qwen, and Phi, even after task-specific Low-Rank Adaptation (LoRA). With only 595M parameters in total, our small LLMs ensemble proves more parameter-efficient and robust than models up to 7B parameters, establishing that carefully designed ensembles of small, fine-tuned models can outperform much larger LLMs in specialized natural language processing (NLP) tasks such as emotion detection.",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "Accepted at IRICT 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17630v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出置信度-可信度加权的小LLM集成方法，在情感检测任务上超越大LLM",
            "summary_zh": "本文提出了一种基于Condorcet陪审团定理（CJT）的置信度加权、可信度感知的文本情感检测集成框架。与通常依赖同构架构的传统集成方法不同，我们的方法结合了架构多样的小型Transformer语言模型（sLLM），包括BERT、RoBERTa、DistilBERT、DeBERTa和ELECTRA，每个模型都经过情感分类的完全微调。为了保持误差多样性，我们在利用每个模型独特偏差的同时，最小化参数收敛。双重加权投票机制集成了全局可信度（验证F1分数）和局部置信度（实例级概率），以动态地加权模型贡献。在DAIR-AI数据集上的实验表明，我们的可信度-置信度集成方法实现了93.5%的宏F1分数，超过了最先进的基准，并显著优于大型LLM，包括Falcon、Mistral、Qwen和Phi，即使在任务特定的低秩适应（LoRA）之后也是如此。我们的小型LLM集成总共只有595M参数，证明比高达7B参数的模型更具参数效率和鲁棒性，这表明精心设计的小型、微调模型集成可以在情感检测等专门的自然语言处理（NLP）任务中优于更大的LLM。",
            "intro_zh": [
                "现有情感检测方法依赖同构架构，且大型语言模型参数冗余，效率较低。",
                "提出一种置信度-可信度加权集成框架，结合多个小型Transformer模型，利用模型偏差。",
                "实验结果表明，该方法在情感检测任务上超越了大型语言模型，且参数效率更高。"
            ],
            "method_zh": "**问题定义**：论文旨在解决文本情感检测任务中，现有方法依赖于单一大型语言模型，导致参数冗余、计算成本高昂，且忽略了不同模型之间的互补性的问题。现有方法的痛点在于模型复杂度高，难以部署，且性能提升受限。\\n\\n**核心思路**：论文的核心思路是利用Condorcet陪审团定理（CJT）的思想，通过集成多个小型、异构的语言模型，利用它们各自的优势和偏差，从而在整体上获得更准确的情感分类结果。这种方法旨在通过模型的多样性来提高鲁棒性和泛化能力。\\n\\n**技术框架**：整体框架包含以下几个主要模块：1) 选择多个小型Transformer模型（BERT、RoBERTa、DistilBERT、DeBERTa、ELECTRA）。2) 对每个模型进行情感分类任务的微调。3) 使用双重加权投票机制，结合全局可信度（验证集F1分数）和局部置信度（实例级别的预测概率）来动态地加权每个模型的贡献。4) 将加权后的预测结果进行集成，得到最终的情感分类结果。\\n\\n**关键创新**：最重要的技术创新点在于提出了置信度-可信度加权的集成方法。与传统的集成方法不同，该方法不仅考虑了模型的全局性能（可信度），还考虑了模型在特定实例上的预测置信度，从而能够更精细地控制每个模型的贡献。此外，选择异构的小型模型也有助于保持模型的多样性，避免过拟合。\\n\\n**关键设计**：关键设计包括：1) 模型选择：选择具有不同架构和训练方式的小型Transformer模型，以保证模型的多样性。2) 加权机制：使用验证集F1分数作为全局可信度，使用模型输出的softmax概率作为局部置信度，并设计合理的加权公式将两者结合。3) 损失函数：使用交叉熵损失函数进行模型微调。4) 最小化参数收敛：通过不同的初始化和训练策略，避免模型参数过度相似。",
            "application_zh": "该研究成果可应用于情感分析、舆情监控、智能客服、个性化推荐等领域。通过集成多个小型模型，可以在保证性能的同时，降低计算成本和部署难度，使得情感检测技术能够更广泛地应用于资源受限的场景。未来，该方法可以扩展到其他自然语言处理任务，例如文本分类、命名实体识别等。",
            "highlight_zh": "实验结果表明，该方法在DAIR-AI数据集上实现了93.5%的宏F1分数，超过了当前最先进的基线方法，并且显著优于大型语言模型（例如Falcon、Mistral、Qwen和Phi），即使这些大型模型经过LoRA微调后也无法达到相同的性能。此外，该集成模型仅包含595M参数，远小于大型语言模型，证明了其更高的参数效率。",
            "tags_zh": [
                "情感检测",
                "集成学习",
                "小型语言模型",
                "置信度加权",
                "可信度感知"
            ],
            "_index": 58,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17630v1/architecture1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17630v1/ar2.drawio.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories",
            "authors": [
                "Lilin Wang",
                "Lucas Ramalho",
                "Alan Celestino",
                "Phuc Anthony Pham",
                "Yu Liu",
                "Umang Kumar Sinha",
                "Andres Portillo",
                "Onassis Osunwa",
                "Gabriel Maduekwe"
            ],
            "arxiv_id": "2512.17419v1",
            "summary": "Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.",
            "categories": [
                "cs.SE",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ],
            "primary_category": "cs.SE",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17419v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "SWE-Bench++：一个从开源仓库大规模生成软件工程基准的框架",
            "summary_zh": "SWE-Bench++是一个自动化的框架，用于从开源GitHub项目中生成仓库级别的编码任务，旨在解决现有软件工程基准数据集规模小、手动维护、静态以及主要集中于Python bug修复的问题。该框架通过收集真实的pull request，覆盖了11种编程语言的bug修复和功能请求。SWE-Bench++通过四个阶段将GitHub pull request转化为可复现的、基于执行的任务：程序化资源获取、环境合成、测试预言提取和质量保证。此外，还包含一个提示引导的轨迹合成步骤，将强模型难以解决的实例转化为训练轨迹。初始基准包含来自3971个仓库的11133个实例，涵盖11种语言。在包含1782个实例的子集上，当前最强的模型表现如下：claude-sonnet-4.5达到36.20%的pass@10，gpt-5-2025-08-07达到34.57%，gemini/gemini-2.5-pro达到24.92%，gpt-4o达到16.89%。通过在SWE-Bench++实例上进行微调，可以显著提高SWE-bench Multilingual基准上的性能，证明了该数据集的有效性。SWE-Bench++为评估和改进仓库级别的代码生成提供了一个可扩展的多语言基准。",
            "intro_zh": [
                "现有软件工程基准数据集依赖手动维护，规模有限，且主要集中于Python bug修复，难以充分评估LLM在多样化场景下的代码生成能力。",
                "SWE-Bench++通过自动化流程从GitHub pull request中提取真实世界的编码任务，覆盖多种编程语言和任务类型，从而构建大规模、动态的基准数据集。",
                "实验表明，当前最先进的模型在SWE-Bench++上表现仍有提升空间，且使用SWE-Bench++进行微调可以有效提升模型在其他基准上的性能。"
            ],
            "method_zh": "**问题定义**：现有软件工程基准（如SWE-bench）主要依赖于手动构建，存在规模有限、数据静态、以及主要关注Python bug修复等问题。这些局限性使得它们难以充分评估大型语言模型（LLMs）在真实软件开发场景中的代码生成能力，尤其是在处理多种编程语言和复杂任务（如功能添加）时。现有方法的痛点在于缺乏自动化、可扩展的数据生成流程，以及对多样化软件工程任务的覆盖。\n\n**核心思路**：SWE-Bench++的核心思路是通过自动化地从开源GitHub仓库中提取pull request（PR），并将其转化为可执行的、基于测试的软件工程任务。这种方法利用了真实世界软件开发活动的自然数据，避免了人工合成数据的局限性。通过程序化的方式处理PR，可以大规模地生成包含bug修复和功能请求等多种任务类型的基准数据集，并覆盖多种编程语言。\n\n**技术框架**：SWE-Bench++的整体框架包含四个主要阶段：1) **程序化资源获取**：自动从GitHub仓库中筛选和下载合适的pull request。2) **环境合成**：为每个pull request构建一个可复现的执行环境，包括依赖项和必要的配置。3) **测试预言提取**：从pull request中提取测试用例，作为评估代码生成质量的标准。4) **质量保证**：对生成的任务进行验证，确保其有效性和可执行性。此外，还有一个**提示引导的轨迹合成**步骤，用于将模型难以解决的实例转化为训练轨迹，以提升模型的学习效果。\n\n**关键创新**：SWE-Bench++的关键创新在于其完全自动化的数据生成流程，以及对真实世界pull request的利用。与以往依赖人工或合成数据的方法不同，SWE-Bench++能够大规模地生成多样化的、贴近实际软件开发场景的基准数据集。此外，该框架还支持多种编程语言，并覆盖了bug修复和功能请求等多种任务类型，从而更全面地评估LLMs的代码生成能力。\n\n**关键设计**：在程序化资源获取阶段，需要设计有效的筛选策略，以选择高质量的pull request。在环境合成阶段，需要解决依赖项管理和环境配置的问题，确保任务的可复现性。在测试预言提取阶段，需要设计算法从pull request的描述和代码变更中自动提取测试用例。提示引导的轨迹合成步骤，需要设计有效的提示策略，引导模型逐步解决复杂问题。",
            "application_zh": "SWE-Bench++可用于评估和改进大型语言模型在软件工程任务中的表现，例如代码生成、代码修复和代码理解。该基准数据集可以促进软件工程领域的自动化和智能化，帮助开发者更高效地完成编码任务。此外，SWE-Bench++还可以用于训练和微调代码生成模型，提升其在实际软件开发场景中的应用能力。",
            "highlight_zh": "SWE-Bench++基准数据集包含来自3971个仓库的11133个实例，覆盖11种编程语言。在包含1782个实例的子集上，当前最强的模型claude-sonnet-4.5达到了36.20%的pass@10，gpt-5-2025-08-07达到了34.57%。实验还表明，使用SWE-Bench++进行微调可以显著提高模型在SWE-bench Multilingual基准上的性能，验证了该数据集的有效性。",
            "tags_zh": [
                "软件工程基准",
                "代码生成",
                "大型语言模型",
                "自动化测试",
                "开源仓库"
            ],
            "_index": 59,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17419v1/images/framework-swe-bench-plus.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17419v1/images/repos-resolved-by-type.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17419v1/images/difficulty-distribution.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens",
            "authors": [
                "Tung-Ling Li",
                "Yuhao Wu",
                "Hongliang Liu"
            ],
            "arxiv_id": "2512.17375v1",
            "summary": "Reward models and LLM-as-a-Judge systems are central to modern post-training pipelines such as RLHF, DPO, and RLAIF, where they provide scalar feedback and binary decisions that guide model selection and RL-based fine-tuning. We show that these judge systems exhibit a recurring vulnerability: short sequences of low-perplexity control tokens can flip many binary evaluations from correct ``No'' judgments to incorrect ``Yes'' judgments by steering the last-layer logit gap. These control tokens are patterns that a policy model could plausibly generate during post-training, and thus represent realistic reward-hacking risks rather than worst-case adversarial strings. Our method, AdvJudge-Zero, uses the model's next-token distribution and beam-search exploration to discover diverse control-token sequences from scratch, and our analysis shows that the induced hidden-state perturbations concentrate in a low-rank ``soft mode'' that is anti-aligned with the judge's refusal direction. Empirically, these tokens cause very high false positive rates when large open-weight and specialized judge models score incorrect answers on math and reasoning benchmarks. Finally, we show that LoRA-based adversarial training on small sets of control-token-augmented examples can markedly reduce these false positives while preserving evaluation quality.",
            "categories": [
                "cs.LG",
                "cs.CL",
                "cs.CR"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17375v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "RLHF",
                        "DPO"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "AdvJudge-Zero：通过对抗控制令牌翻转LLM评判器的二元决策",
            "summary_zh": "奖励模型和LLM评判系统是现代后训练流程（如RLHF、DPO和RLAIF）的核心，它们提供标量反馈和二元决策，指导模型选择和基于强化学习的微调。本文揭示了这些评判系统存在一种反复出现的漏洞：短序列的低困惑度控制令牌可以通过操纵最后一层logit间隙，将许多二元评估从正确的“否”判断翻转为不正确的“是”判断。这些控制令牌是策略模型在后训练期间可能生成的模式，因此代表了实际的奖励黑客风险，而不是最坏情况下的对抗性字符串。本文提出的AdvJudge-Zero方法利用模型的下一个令牌分布和束搜索探索，从头开始发现各种控制令牌序列。分析表明，诱导的隐藏状态扰动集中在一个低秩“软模式”中，该模式与评判器的拒绝方向反向对齐。实验表明，当大型开放权重和专用评判模型对数学和推理基准上的不正确答案进行评分时，这些令牌会导致非常高的假阳性率。最后，本文表明，在少量控制令牌增强示例上进行基于LoRA的对抗训练可以显著降低这些假阳性率，同时保持评估质量。",
            "intro_zh": [
                "现有奖励模型和LLM评判系统易受攻击，可能被精心设计的控制令牌序列欺骗，导致错误的二元决策。",
                "AdvJudge-Zero方法通过探索模型的下一个令牌分布，自动发现能够翻转评判结果的控制令牌序列。",
                "实验表明，对抗训练可以有效降低控制令牌导致的假阳性率，同时保持评判质量。"
            ],
            "method_zh": "**问题定义**：现有的基于LLM的评判系统在RLHF等流程中扮演重要角色，但容易受到对抗攻击。具体来说，短序列的控制令牌可以改变LLM评判器的决策，将错误的答案判断为正确。这种攻击的威胁在于，这些控制令牌并非完全随机的对抗样本，而是模型在训练过程中可能生成的，因此更具实际意义。\\n\\n**核心思路**：本文的核心思路是利用模型的自身特性（下一个token的概率分布）来寻找能够影响评判结果的控制令牌序列。通过优化这些控制令牌，使得它们能够最大程度地改变LLM评判器的输出logit，从而翻转其二元决策。这种方法不需要外部的对抗样本生成器，而是直接利用模型本身的信息。\\n\\n**技术框架**：AdvJudge-Zero方法主要包含以下几个阶段：\n1. **控制令牌搜索**：使用模型的下一个token分布和束搜索算法，从头开始生成控制令牌序列。\n2. **隐藏状态分析**：分析控制令牌引起的隐藏状态扰动，发现其集中在一个低秩“软模式”中。\n3. **对抗训练**：使用生成的控制令牌增强训练数据，通过LoRA进行对抗训练，提高模型的鲁棒性。\\n\\n**关键创新**：最重要的创新在于提出了一种完全基于模型自身信息的对抗样本生成方法。与传统的对抗攻击方法不同，AdvJudge-Zero不需要外部的对抗样本生成器，而是直接利用模型的下一个token分布来寻找控制令牌。这种方法生成的控制令牌更具实际意义，因为它们是模型在训练过程中可能生成的。\\n\\n**关键设计**：\n1. **控制令牌搜索**：使用束搜索算法，以最大化logit差距为目标，寻找最优的控制令牌序列。\n2. **隐藏状态分析**：使用奇异值分解（SVD）分析隐藏状态扰动，发现其集中在一个低秩“软模式”中。\n3. **对抗训练**：使用LoRA进行对抗训练，以减少计算成本，并防止模型过拟合。",
            "application_zh": "该研究成果可应用于提升LLM评判系统的安全性与鲁棒性，降低其在RLHF等后训练流程中被恶意利用的风险。通过对抗训练，可以提高评判系统对潜在对抗样本的抵抗能力，从而保证模型训练的稳定性和可靠性。此外，该方法也可用于评估和改进其他基于LLM的系统。",
            "highlight_zh": "实验结果表明，AdvJudge-Zero方法能够有效地发现控制令牌序列，这些序列可以显著提高LLM评判器的假阳性率。在数学和推理基准测试中，使用这些控制令牌可以使大型开放权重和专用评判模型将错误答案判断为正确的概率大幅提升。通过在少量控制令牌增强示例上进行基于LoRA的对抗训练，可以显著降低这些假阳性率，同时保持评估质量。",
            "tags_zh": [
                "LLM评判器",
                "对抗攻击",
                "控制令牌",
                "奖励黑客",
                "对抗训练"
            ],
            "_index": 60,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "A Theoretical Analysis of State Similarity Between Markov Decision Processes",
            "authors": [
                "Zhenyu Tao",
                "Wei Xu",
                "Xiaohu You"
            ],
            "arxiv_id": "2512.17265v1",
            "summary": "The bisimulation metric (BSM) is a powerful tool for analyzing state similarities within a Markov decision process (MDP), revealing that states closer in BSM have more similar optimal value functions. While BSM has been successfully utilized in reinforcement learning (RL) for tasks like state representation learning and policy exploration, its application to state similarity between multiple MDPs remains challenging. Prior work has attempted to extend BSM to pairs of MDPs, but a lack of well-established mathematical properties has limited further theoretical analysis between MDPs. In this work, we formally establish a generalized bisimulation metric (GBSM) for measuring state similarity between arbitrary pairs of MDPs, which is rigorously proven with three fundamental metric properties, i.e., GBSM symmetry, inter-MDP triangle inequality, and a distance bound on identical spaces. Leveraging these properties, we theoretically analyze policy transfer, state aggregation, and sampling-based estimation across MDPs, obtaining explicit bounds that are strictly tighter than existing ones derived from the standard BSM. Additionally, GBSM provides a closed-form sample complexity for estimation, improving upon existing asymptotic results based on BSM. Numerical results validate our theoretical findings and demonstrate the effectiveness of GBSM in multi-MDP scenarios.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "Submitted to an IEEE Transactions. arXiv admin note: substantial text overlap with arXiv:2509.18714",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17265v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "representation learning"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出广义双模拟度量GBSM，用于评估马尔可夫决策过程间的状态相似性。",
            "summary_zh": "双模拟度量(BSM)是分析马尔可夫决策过程(MDP)中状态相似性的强大工具，它揭示了BSM中距离更近的状态具有更相似的最优价值函数。虽然BSM已成功应用于强化学习(RL)中的状态表示学习和策略探索等任务，但将其应用于多个MDP之间的状态相似性仍然具有挑战性。先前的工作试图将BSM扩展到MDP对，但由于缺乏完善的数学性质，限制了MDP之间进一步的理论分析。在这项工作中，我们正式建立了广义双模拟度量(GBSM)，用于衡量任意MDP对之间的状态相似性，并通过三个基本度量属性（即GBSM对称性、MDP间三角不等式和相同空间上的距离界限）对其进行了严格证明。利用这些属性，我们从理论上分析了跨MDP的策略迁移、状态聚合和基于采样的估计，获得了比现有标准BSM推导的更严格的显式界限。此外，GBSM为估计提供了闭式样本复杂度，改进了基于BSM的现有渐近结果。数值结果验证了我们的理论发现，并证明了GBSM在多MDP场景中的有效性。",
            "intro_zh": [
                "现有双模拟度量(BSM)在跨多个MDP衡量状态相似性方面存在不足，缺乏完善的数学性质，限制了理论分析。",
                "论文提出广义双模拟度量(GBSM)，通过严格证明其对称性、三角不等式等性质，实现对MDP间状态相似性的有效度量。",
                "实验结果验证了GBSM的有效性，并在策略迁移、状态聚合和采样估计等方面获得了比现有方法更优的性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决如何有效衡量不同马尔可夫决策过程(MDP)之间的状态相似性问题。现有方法，特别是基于双模拟度量(BSM)的扩展，缺乏严格的数学性质，导致无法进行有效的理论分析，限制了其在跨MDP的策略迁移、状态聚合等方面的应用。现有方法通常只能提供渐近结果，缺乏闭式解和严格的性能界限。\n\\n**核心思路**：论文的核心思路是提出一种新的度量方式，即广义双模拟度量(GBSM)，该度量不仅能够衡量单个MDP内的状态相似性，还能衡量不同MDP之间的状态相似性。GBSM的设计目标是满足度量的基本性质，如对称性、三角不等式等，从而为后续的理论分析提供坚实的基础。通过GBSM，可以更好地理解不同MDP之间的关系，并指导策略迁移等任务。\n\\n**技术框架**：论文的技术框架主要包括以下几个部分：1) 形式化定义广义双模拟度量(GBSM)；2) 严格证明GBSM满足度量的基本性质，包括对称性、MDP间三角不等式和相同空间上的距离界限；3) 利用GBSM的性质，从理论上分析跨MDP的策略迁移、状态聚合和基于采样的估计；4) 推导出基于GBSM的闭式样本复杂度，并与现有基于BSM的渐近结果进行比较；5) 通过数值实验验证GBSM的有效性和理论分析的正确性。\n\\n**关键创新**：论文最重要的技术创新点在于提出了广义双模拟度量(GBSM)，并严格证明了其满足度量的基本性质。与现有方法相比，GBSM能够更准确地衡量不同MDP之间的状态相似性，并为跨MDP的策略迁移等任务提供更有效的理论指导。此外，GBSM还提供了闭式样本复杂度，改进了现有基于BSM的渐近结果。\n\\n**关键设计**：GBSM的具体定义涉及到对不同MDP的状态空间、动作空间和转移概率的比较。关键在于如何定义不同MDP之间的“相似”转移，以及如何将这种相似性转化为一个可度量的距离。论文中可能涉及到对状态空间和动作空间进行嵌入或映射，以便进行比较。此外，GBSM的计算可能涉及到迭代算法，需要仔细设计迭代的停止条件和收敛性保证。损失函数的设计可能涉及到最小化不同MDP之间的状态价值函数的差异，同时考虑GBSM的度量性质。",
            "application_zh": "该研究成果可广泛应用于多智能体强化学习、迁移学习、元学习等领域。例如，在机器人控制中，可以将一个机器人的策略迁移到另一个机器人上，从而减少训练时间和成本。在游戏AI中，可以将一个游戏的AI策略迁移到另一个类似的游戏中，提高AI的通用性和适应性。此外，该研究还可以用于状态抽象和状态聚合，从而降低强化学习的计算复杂度。",
            "highlight_zh": "论文通过数值实验验证了GBSM的有效性，并证明了其在策略迁移、状态聚合和采样估计等方面优于现有基于BSM的方法。具体而言，GBSM在策略迁移任务中能够实现更高的策略性能，在状态聚合任务中能够获得更紧凑的状态表示，在采样估计任务中能够提供更准确的价值函数估计。实验结果表明，GBSM能够有效地衡量不同MDP之间的状态相似性，并为跨MDP的强化学习任务提供更有效的理论指导。",
            "tags_zh": [
                "强化学习",
                "马尔可夫决策过程",
                "双模拟度量",
                "状态相似性",
                "策略迁移",
                "状态聚合",
                "广义双模拟度量"
            ],
            "_index": 61,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "LLM-based Behaviour Driven Development for Hardware Design",
            "authors": [
                "Rolf Drechsler",
                "Qian Liu"
            ],
            "arxiv_id": "2512.17814v1",
            "summary": "Test and verification are essential activities in hardware and system design, but their complexity grows significantly with increasing system sizes. While Behavior Driven Development (BDD) has proven effective in software engineering, it is not yet well established in hardware design, and its practical use remains limited. One contributing factor is the manual effort required to derive precise behavioral scenarios from textual specifications.\n  Recent advances in Large Language Models (LLMs) offer new opportunities to automate this step. In this paper, we investigate the use of LLM-based techniques to support BDD in the context of hardware design.",
            "categories": [
                "cs.SE",
                "cs.AI",
                "cs.AR"
            ],
            "primary_category": "cs.SE",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "7 pages, keynote given at 2nd International Symposium on Artificial Intelligence and Internet of Things (AIIoT-25), December 22-24th, 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17814v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于LLM的硬件设计行为驱动开发方法，提升测试验证效率",
            "summary_zh": "测试和验证是硬件和系统设计中至关重要的环节，但随着系统规模的增大，其复杂性也显著增加。行为驱动开发（BDD）已在软件工程中证明有效，但在硬件设计中尚未得到广泛应用，其实际应用仍然有限。其中一个原因是需要手动从文本规范中推导出精确的行为场景，这需要大量的人工工作。本文探讨了使用基于大型语言模型（LLM）的技术来支持硬件设计中的BDD，旨在自动化这一步骤。",
            "intro_zh": [
                "硬件设计的测试验证复杂度随系统规模增大而显著提升，传统方法效率较低。",
                "利用大型语言模型（LLM）自动从文本规范生成行为场景，从而支持硬件设计的行为驱动开发（BDD）。",
                "论文探索了LLM在硬件BDD中的应用，旨在降低手动工作量，提升测试验证效率，具体实验结果未知。"
            ],
            "method_zh": "**问题定义**：硬件设计中的测试和验证环节至关重要，但随着系统复杂度的增加，手动创建测试用例和验证场景变得越来越困难且耗时。现有的行为驱动开发（BDD）方法在硬件设计领域的应用受限于从文本规范到精确行为场景的手动转换过程，这需要领域专家的大量时间和精力。\\n\\n**核心思路**：论文的核心思路是利用大型语言模型（LLM）强大的自然语言理解和生成能力，自动地从硬件设计的文本规范中提取并生成行为场景。这样可以显著减少手动工作量，提高BDD在硬件设计中的应用效率。通过自动化行为场景的生成，可以更快速地进行测试和验证，从而缩短硬件设计的周期。\\n\\n**技术框架**：论文提出的技术框架主要包含以下几个阶段：1) 输入硬件设计的文本规范；2) 使用LLM对文本规范进行分析和理解；3) LLM根据理解的结果生成行为场景；4) 将生成的行为场景用于硬件的测试和验证。具体模块和流程的详细设计未知。\\n\\n**关键创新**：该方法最重要的创新点在于将LLM应用于硬件设计的BDD流程中，实现了行为场景的自动生成。这与传统的手动方法相比，极大地提高了效率和可扩展性。通过利用LLM的强大能力，可以处理更复杂的文本规范，并生成更精确的行为场景。\\n\\n**关键设计**：论文中关于LLM的具体选择、训练方式、提示工程（prompt engineering）以及如何将生成的行为场景集成到现有的硬件验证流程中的技术细节未知。损失函数、网络结构等信息也未提及。",
            "application_zh": "该研究成果可应用于各种硬件设计领域，包括处理器、存储器、网络设备和嵌入式系统等。通过自动化行为场景的生成，可以加速硬件的测试和验证过程，降低开发成本，并提高硬件产品的质量和可靠性。未来，该方法有望成为硬件设计流程中的一个重要组成部分。",
            "highlight_zh": "由于论文摘要中没有提供具体的实验结果，因此无法总结实验亮点。需要查阅论文全文才能了解具体的性能数据、对比基线以及提升幅度等信息。目前已知的是，该研究旨在通过LLM自动化硬件BDD流程，从而减少手动工作量。",
            "tags_zh": [
                "硬件设计",
                "行为驱动开发",
                "大型语言模型",
                "测试验证",
                "自动化"
            ],
            "_index": 62,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17814v1/verilogadd.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17814v1/scenarioadd.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17814v1/environment_new.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "AncientBench: Towards Comprehensive Evaluation on Excavated and Transmitted Chinese Corpora",
            "authors": [
                "Zhihan Zhou",
                "Daqian Shi",
                "Rui Song",
                "Lida Shi",
                "Xiaolei Diao",
                "Hao Xu"
            ],
            "arxiv_id": "2512.17756v1",
            "summary": "Comprehension of ancient texts plays an important role in archaeology and understanding of Chinese history and civilization. The rapid development of large language models needs benchmarks that can evaluate their comprehension of ancient characters. Existing Chinese benchmarks are mostly targeted at modern Chinese and transmitted documents in ancient Chinese, but the part of excavated documents in ancient Chinese is not covered. To meet this need, we propose the AncientBench, which aims to evaluate the comprehension of ancient characters, especially in the scenario of excavated documents. The AncientBench is divided into four dimensions, which correspond to the four competencies of ancient character comprehension: glyph comprehension, pronunciation comprehension, meaning comprehension, and contextual comprehension. The benchmark also contains ten tasks, including radical, phonetic radical, homophone, cloze, translation, and more, providing a comprehensive framework for evaluation. We convened archaeological researchers to conduct experimental evaluations, proposed an ancient model as baseline, and conducted extensive experiments on the currently best-performing large language models. The experimental results reveal the great potential of large language models in ancient textual scenarios as well as the gap with humans. Our research aims to promote the development and application of large language models in the field of archaeology and ancient Chinese language.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17756v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出AncientBench，用于全面评估模型对出土和传世古汉语语料的理解能力",
            "summary_zh": "为了评估大型语言模型对古文字的理解能力，尤其是在出土文献场景下的表现，本文提出了AncientBench。该基准测试分为四个维度，对应于古文字理解的四个能力：字形理解、发音理解、意义理解和语境理解。AncientBench包含十个任务，包括部首、声旁、同音字、完形填空、翻译等，为评估提供了一个全面的框架。研究团队邀请了考古研究人员进行实验评估，提出了一个古文字模型作为基线，并对当前性能最佳的大型语言模型进行了广泛的实验。实验结果揭示了大型语言模型在古文字场景中的巨大潜力以及与人类的差距。该研究旨在促进大型语言模型在考古学和古代汉语语言领域的发展和应用。",
            "intro_zh": [
                "现有中文基准测试主要针对现代汉语和传世古籍，缺乏对出土古文字的评估。",
                "AncientBench从字形、发音、意义和语境四个维度，全面评估模型对古文字的理解能力。",
                "实验结果表明，大型语言模型在古文字理解方面具有潜力，但与人类水平仍存在差距。"
            ],
            "method_zh": "**问题定义**：现有的大型语言模型在古汉语理解方面表现出一定的能力，但缺乏专门针对出土文献的评估基准。现有的中文基准测试主要集中在现代汉语和传世古籍，忽略了出土文献中独特的文字特点和语境，这限制了对模型在考古学和古文字研究领域应用潜力的评估。因此，需要一个能够全面评估模型对出土和传世古汉语语料理解能力的基准测试。\\n\\n**核心思路**：AncientBench的核心思路是将古文字的理解能力分解为四个关键维度：字形理解、发音理解、意义理解和语境理解。通过设计针对每个维度的任务，可以更全面地评估模型对古文字的掌握程度。此外，该基准测试特别关注出土文献的特点，旨在反映实际考古研究中遇到的挑战。\\n\\n**技术框架**：AncientBench包含四个维度和十个任务。四个维度分别是：字形理解（评估模型对古文字字形的识别能力）、发音理解（评估模型对古文字发音的理解能力）、意义理解（评估模型对古文字含义的理解能力）和语境理解（评估模型在特定语境下理解古文字的能力）。十个任务包括：部首识别、声旁识别、同音字辨析、完形填空、翻译等。研究团队还提出了一个古文字模型作为基线模型。\\n\\n**关键创新**：AncientBench的关键创新在于其全面性和针对性。它不仅涵盖了古文字理解的多个维度，还特别关注出土文献的特点。此外，该基准测试的任务设计也具有创新性，例如，完形填空任务可以评估模型在特定语境下理解古文字的能力。\\n\\n**关键设计**：AncientBench的任务设计考虑了古文字的特点和考古研究的需求。例如，部首和声旁识别任务可以评估模型对古文字字形结构的理解能力，翻译任务可以评估模型对古文字含义的理解能力。此外，研究团队还邀请了考古研究人员参与基准测试的设计和评估，以确保其专业性和实用性。具体的参数设置、损失函数、网络结构等技术细节在论文中针对基线模型有更详细的描述，但整体基准测试更侧重于任务的设计和数据集的构建。",
            "application_zh": "AncientBench可用于评估和提升大型语言模型在考古学、历史学和古文字学等领域的应用能力。通过该基准测试，可以推动模型更好地理解和分析古代文献，辅助考古研究，促进中华文明的传承和发展。未来，可以进一步扩展AncientBench，纳入更多类型的出土文献和更复杂的任务，以更好地满足实际应用需求。",
            "highlight_zh": "实验结果表明，目前的大型语言模型在AncientBench上表现出一定的古文字理解能力，但在某些任务上与人类水平仍存在较大差距。例如，在语境理解任务中，模型的表现明显低于人类专家。这表明，大型语言模型在古文字理解方面仍有很大的提升空间，AncientBench可以作为评估和改进模型性能的重要工具。",
            "tags_zh": [
                "古文字理解",
                "大型语言模型",
                "出土文献",
                "基准测试",
                "考古学",
                "自然语言处理"
            ],
            "_index": 63,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17756v1/img/image_head.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17756v1/img/image_sample.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "UmniBench: Unified Understand and Generation Model Oriented Omni-dimensional Benchmark",
            "authors": [
                "Kai Liu",
                "Leyang Chen",
                "Wenbo Li",
                "Zhikai Chen",
                "Zhixin Wang",
                "Renjing Pei",
                "Linghe Kong",
                "Yulun Zhang"
            ],
            "arxiv_id": "2512.17196v1",
            "summary": "Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. However, evaluations of unified multimodal models (UMMs) remain decoupled, assessing their understanding and generation abilities separately with corresponding datasets. To address this, we propose UmniBench, a benchmark tailored for UMMs with omni-dimensional evaluation. First, UmniBench can assess the understanding, generation, and editing ability within a single evaluation process. Based on human-examined prompts and QA pairs, UmniBench leverages UMM itself to evaluate its generation and editing ability with its understanding ability. This simple but effective paradigm allows comprehensive evaluation of UMMs. Second, UmniBench covers 13 major domains and more than 200 concepts, ensuring a thorough inspection of UMMs. Moreover, UmniBench can also decouple and separately evaluate understanding, generation, and editing abilities, providing a fine-grained assessment. Based on UmniBench, we benchmark 24 popular models, including both UMMs and single-ability large models. We hope this benchmark provides a more comprehensive and objective view of unified models and logistical support for improving the performance of the community model.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "Project Page: https://umnibench.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17196v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出 UmniBench，用于统一多模态模型在理解、生成和编辑能力上的全面评估。",
            "summary_zh": "统一多模态理解和生成在先进的专有系统中展现了令人印象深刻的能力。然而，统一多模态模型（UMM）的评估仍然是分离的，使用相应的数据集分别评估其理解和生成能力。为了解决这个问题，我们提出了 UmniBench，这是一个为 UMM 量身定制的基准，具有全方位评估能力。首先，UmniBench 可以在单个评估过程中评估理解、生成和编辑能力。基于人工检查的提示和问答对，UmniBench 利用 UMM 本身来评估其生成和编辑能力以及理解能力。这种简单而有效的范例可以对 UMM 进行全面评估。其次，UmniBench 涵盖 13 个主要领域和 200 多个概念，确保对 UMM 进行彻底检查。此外，UmniBench 还可以分离并分别评估理解、生成和编辑能力，从而提供细粒度的评估。基于 UmniBench，我们对 24 个流行的模型进行了基准测试，包括 UMM 和单能力大型模型。我们希望这个基准能够为统一模型提供更全面和客观的视角，并为提高社区模型的性能提供后勤支持。",
            "intro_zh": [
                "现有统一多模态模型（UMM）的评估方式割裂，缺乏统一的基准来同时评估理解、生成和编辑能力。",
                "UmniBench 提出了一种新的评估范式，利用 UMM 本身来评估其生成和编辑能力，同时结合理解能力进行综合评估。",
                "UmniBench 覆盖 13 个领域和 200 多个概念，并对 24 个模型进行了基准测试，为 UMM 的发展提供支持。"
            ],
            "method_zh": "**问题定义**：现有统一多模态模型（UMM）的评估通常是分离的，即使用不同的数据集和指标分别评估模型的理解和生成能力。这种评估方式无法全面反映 UMM 的综合能力，也难以发现模型在不同能力之间的潜在关联和瓶颈。此外，缺乏统一的基准使得不同 UMM 之间的比较变得困难。现有方法的痛点在于缺乏一个能够同时评估理解、生成和编辑能力，并提供细粒度评估结果的基准。\n\n**核心思路**：UmniBench 的核心思路是利用 UMM 自身的能力来评估其性能。具体来说，它基于人工设计的提示和问答对，让 UMM 生成答案或编辑内容，然后利用 UMM 自身的理解能力来评估生成或编辑结果的质量。这种自评估的方式可以避免引入额外的评估模型，并能够更直接地反映 UMM 的真实能力。 这种设计思路的优势在于简单有效，能够全面评估 UMM 的各项能力。\n\n**技术框架**：UmniBench 的整体框架包括以下几个主要部分：1) 数据集构建：构建包含 13 个领域和 200 多个概念的提示和问答对数据集。2) 评估流程：针对每个提示，让 UMM 生成答案或编辑内容。3) 自评估：利用 UMM 自身的理解能力评估生成或编辑结果的质量。4) 结果分析：对评估结果进行统计分析，提供细粒度的性能报告。该框架支持对理解、生成和编辑能力进行联合评估，也支持对各项能力进行单独评估。\n\n**关键创新**：UmniBench 的最重要创新点在于其自评估的范式。与传统的依赖外部评估模型的方法不同，UmniBench 利用 UMM 自身的能力来评估其生成和编辑结果。这种自评估的方式可以更直接地反映 UMM 的真实能力，并避免引入额外的偏差。此外，UmniBench 还通过构建包含多个领域和概念的综合数据集，实现了对 UMM 的全面评估。\n\n**关键设计**：UmniBench 的关键设计包括：1) 人工设计的提示和问答对，确保评估的准确性和可靠性。2) 基于 UMM 自身的理解能力构建的评估指标，例如，使用 UMM 判断生成答案与正确答案的相似度。3) 细粒度的性能报告，提供对 UMM 在不同领域和概念上的性能分析。 4) 评估指标的设计需要仔细考虑，以确保能够准确反映 UMM 的性能。",
            "application_zh": "UmniBench 可用于评估和比较不同的统一多模态模型，帮助研究人员和开发者选择合适的模型并改进其性能。该基准还可以用于诊断 UMM 的弱点，指导模型架构设计和训练策略的优化。此外，UmniBench 有助于推动多模态理解和生成技术在智能助手、图像编辑、内容创作等领域的应用。",
            "highlight_zh": "UmniBench 对 24 个流行的模型进行了基准测试，包括 UMM 和单能力大型模型。实验结果表明，不同的模型在不同的领域和能力上表现出不同的优势和劣势。UmniBench 能够提供细粒度的性能报告，帮助研究人员深入了解模型的性能特点，并为模型改进提供指导。",
            "tags_zh": [
                "统一多模态模型",
                "多模态评估",
                "理解能力",
                "生成能力",
                "编辑能力",
                "基准测试",
                "自评估"
            ],
            "_index": 64,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17196v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17196v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17196v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "PILAR: Personalizing Augmented Reality Interactions with LLM-based Human-Centric and Trustworthy Explanations for Daily Use Cases",
            "authors": [
                "Ripan Kumar Kundu",
                "Istiak Ahmed",
                "Khaza Anuarul Hoque"
            ],
            "arxiv_id": "2512.17172v1",
            "summary": "Artificial intelligence (AI)-driven augmented reality (AR) systems are becoming increasingly integrated into daily life, and with this growth comes a greater need for explainability in real-time user interactions. Traditional explainable AI (XAI) methods, which often rely on feature-based or example-based explanations, struggle to deliver dynamic, context-specific, personalized, and human-centric insights for everyday AR users. These methods typically address separate explainability dimensions (e.g., when, what, how) with different explanation techniques, resulting in unrealistic and fragmented experiences for seamless AR interactions. To address this challenge, we propose PILAR, a novel framework that leverages a pre-trained large language model (LLM) to generate context-aware, personalized explanations, offering a more intuitive and trustworthy experience in real-time AI-powered AR systems. Unlike traditional methods, which rely on multiple techniques for different aspects of explanation, PILAR employs a unified LLM-based approach that dynamically adapts explanations to the user's needs, fostering greater trust and engagement. We implement the PILAR concept in a real-world AR application (e.g., personalized recipe recommendations), an open-source prototype that integrates real-time object detection, recipe recommendation, and LLM-based personalized explanations of the recommended recipes based on users' dietary preferences. We evaluate the effectiveness of PILAR through a user study with 16 participants performing AR-based recipe recommendation tasks, comparing an LLM-based explanation interface to a traditional template-based one. Results show that the LLM-based interface significantly enhances user performance and experience, with participants completing tasks 40% faster and reporting greater satisfaction, ease of use, and perceived transparency.",
            "categories": [
                "cs.HC",
                "cs.AI"
            ],
            "primary_category": "cs.HC",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "Published in the 2025 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)",
            "doi": "10.1109/ISMAR-Adjunct68609.2025.00060",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17172v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "PILAR：利用LLM生成以人为本的可信解释，个性化增强现实交互，应用于日常场景。",
            "summary_zh": "人工智能驱动的增强现实（AR）系统日益融入日常生活，对实时用户交互的可解释性需求也随之增长。传统的XAI方法依赖于基于特征或示例的解释，难以提供动态、特定于上下文、个性化和以人为本的洞察。这些方法通常使用不同的解释技术来解决不同的可解释性维度，导致不切实际和碎片化的AR交互体验。为解决此问题，我们提出了PILAR，一种利用预训练大型语言模型（LLM）生成上下文感知、个性化解释的新框架，在实时AI驱动的AR系统中提供更直观和可信的体验。与传统方法不同，PILAR采用统一的基于LLM的方法，动态地调整解释以满足用户的需求，从而增强信任和参与度。我们在一个真实的AR应用（例如，个性化食谱推荐）中实现了PILAR概念，这是一个开源原型，集成了实时对象检测、食谱推荐和基于LLM的个性化食谱解释，该解释基于用户的饮食偏好。通过一项用户研究，我们评估了PILAR的有效性，其中16名参与者执行基于AR的食谱推荐任务，并将基于LLM的解释界面与传统的基于模板的界面进行比较。结果表明，基于LLM的界面显著提高了用户的性能和体验，参与者完成任务的速度提高了40%，并且报告了更高的满意度、易用性和感知透明度。",
            "intro_zh": [
                "现有XAI方法难以提供动态、个性化、以人为本的AR交互解释，导致用户体验不佳。",
                "PILAR利用LLM生成上下文感知和个性化的解释，统一解决不同解释维度的问题，提升用户信任。",
                "用户研究表明，与传统方法相比，PILAR使任务完成速度提升40%，显著提高用户满意度和易用性。"
            ],
            "method_zh": "**问题定义**：现有增强现实(AR)系统中的可解释人工智能(XAI)方法，如基于特征或示例的解释，无法提供动态、上下文相关、个性化和以人为本的解释。这些方法通常需要多种技术来处理不同的解释维度（例如，何时、什么、如何），导致用户体验不连贯和不自然。因此，如何为AR用户提供无缝、可信且易于理解的解释是一个关键问题。\\n\\n**核心思路**：PILAR的核心思路是利用预训练的大型语言模型(LLM)来生成上下文感知的、个性化的解释。LLM具有强大的语言理解和生成能力，可以根据用户的具体需求和场景，动态地调整解释的内容和形式。这种方法旨在提供更直观、更可信的AR交互体验，增强用户的信任感和参与度。\\n\\n**技术框架**：PILAR框架包含以下主要模块：1) **实时对象检测**：用于识别AR场景中的物体。2) **食谱推荐**：根据检测到的物体和用户的饮食偏好，推荐相关的食谱。3) **LLM解释生成**：利用LLM根据用户的偏好和食谱内容，生成个性化的解释。整个流程是：用户通过AR设备与环境交互，系统检测到相关对象后，推荐食谱，然后LLM生成针对该食谱的个性化解释，并呈现给用户。\\n\\n**关键创新**：PILAR的关键创新在于使用统一的LLM来处理所有解释维度，而不是像传统方法那样使用多种技术。这种统一的方法可以生成更连贯、更自然的解释，并更好地适应用户的需求。此外，PILAR还关注个性化，根据用户的饮食偏好定制解释，从而提高用户的信任感和满意度。\\n\\n**关键设计**：PILAR使用预训练的LLM，并通过微调来适应特定的AR应用场景。在食谱推荐应用中，系统会收集用户的饮食偏好信息，并将其作为LLM的输入，以生成个性化的解释。具体的提示工程（prompt engineering）策略和LLM的选择（例如，GPT-3, PaLM等）是影响解释质量的关键因素。论文中没有明确说明具体的损失函数或网络结构，但强调了LLM的微调过程。",
            "application_zh": "PILAR框架具有广泛的应用前景，例如个性化教育、智能家居、远程医疗等。在教育领域，可以为学生提供个性化的学习辅导；在智能家居领域，可以帮助用户更好地理解和控制智能设备；在远程医疗领域，可以为患者提供更清晰的诊断解释。该研究有助于推动人机交互的智能化和个性化发展。",
            "highlight_zh": "用户研究表明，与传统的基于模板的解释界面相比，基于LLM的PILAR界面显著提高了用户的性能和体验。具体来说，参与者在使用PILAR界面时，完成任务的速度提高了40%，并且报告了更高的满意度、易用性和感知透明度。这些结果表明，LLM可以有效地生成更有效、更令人满意的AR解释。",
            "tags_zh": [
                "增强现实",
                "可解释人工智能",
                "大型语言模型",
                "人机交互",
                "个性化推荐"
            ],
            "_index": 65,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "DEER: A Comprehensive and Reliable Benchmark for Deep-Research Expert Reports",
            "authors": [
                "Janghoon Han",
                "Heegyu Kim",
                "Changho Lee",
                "Dahm Lee",
                "Min Hyung Park",
                "Hosung Song",
                "Stanley Jungkyu Choi",
                "Moontae Lee",
                "Honglak Lee"
            ],
            "arxiv_id": "2512.17776v1",
            "summary": "As large language models (LLMs) advance, deep research systems can generate expert-level reports via multi-step reasoning and evidence-based synthesis, but evaluating such reports remains challenging. Existing benchmarks often lack systematic criteria for expert reporting, evaluations that rely heavily on LLM judges can fail to capture issues that require expert judgment, and source verification typically covers only a limited subset of explicitly cited statements rather than report-wide factual reliability. We introduce DEER, a benchmark for evaluating expert-level deep research reports. DEER comprises 50 report-writing tasks spanning 13 domains and an expert-grounded evaluation taxonomy (7 dimensions, 25 sub-dimension) operationalized into 130 fine-grained rubric items. DEER further provides task-specific expert guidance to help LLM judges assess expert-level report quality more consistently. Complementing rubric-based assessment, we propose a document-level fact-checking architecture that extracts and verifies all claims across the entire report, including both cited and uncited ones, and quantifies external-evidence quality. DEER correlates closely with human expert judgments and yields interpretable diagnostics of system strengths and weaknesses.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "Work in progress",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17776v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "DEER：一个全面可靠的深度研究专家报告评估基准",
            "summary_zh": "随着大型语言模型（LLMs）的进步，深度研究系统能够通过多步骤推理和基于证据的综合生成专家级报告，但评估此类报告仍然具有挑战性。现有的基准通常缺乏针对专家报告的系统性标准，严重依赖LLM评判的评估可能无法捕捉到需要专家判断的问题，并且源验证通常仅涵盖显式引用的语句的有限子集，而不是报告范围内的事实可靠性。我们引入了DEER，这是一个用于评估专家级深度研究报告的基准。DEER包含跨越13个领域的50个报告撰写任务，以及一个专家基础的评估分类法（7个维度，25个子维度），并将其操作化为130个细粒度的评分标准项。DEER还提供特定于任务的专家指导，以帮助LLM评判更一致地评估专家级报告质量。作为基于评分标准的评估的补充，我们提出了一种文档级事实检查架构，该架构提取并验证整个报告中的所有声明，包括引用的和未引用的声明，并量化外部证据质量。DEER与人类专家的判断密切相关，并产生对系统优势和劣势的可解释诊断。",
            "intro_zh": [
                "现有专家报告评估基准缺乏系统性标准，且过度依赖LLM评判，难以捕捉需要专家知识的问题。",
                "DEER基准通过构建专家基础的评估体系，并提供任务相关的专家指导，提升LLM评判的一致性。",
                "DEER引入文档级事实核查架构，验证报告中所有声明，并量化外部证据质量，更全面评估报告可靠性。"
            ],
            "method_zh": "**问题定义**：现有的大型语言模型在生成专家级别的研究报告时，缺乏一个全面且可靠的评估基准。现有的基准要么缺乏针对专家报告的系统性评估标准，要么过度依赖LLM进行评估，而LLM本身可能无法准确捕捉到需要专业知识才能判断的问题。此外，现有的事实核查方法通常只关注报告中显式引用的部分，而忽略了未引用的声明，导致无法全面评估报告的可靠性。\\n\\n**核心思路**：DEER的核心思路是构建一个专家基础的评估体系，并提供任务相关的专家指导，以提升LLM评判的一致性和准确性。同时，引入一种文档级别的全面事实核查架构，以验证报告中的所有声明，包括引用的和未引用的部分，从而更全面地评估报告的可靠性。\\n\\n**技术框架**：DEER基准包含以下几个主要组成部分：1) 50个报告撰写任务，涵盖13个不同的领域；2) 一个专家基础的评估分类法，包含7个维度和25个子维度，并将其转化为130个细粒度的评分标准项；3) 任务相关的专家指导，用于帮助LLM评判更一致地评估报告质量；4) 一个文档级别的全面事实核查架构，用于提取和验证报告中的所有声明，并量化外部证据的质量。\\n\\n**关键创新**：DEER的关键创新在于：1) 构建了一个专家基础的评估体系，该体系能够更准确地评估专家级别的研究报告；2) 引入了任务相关的专家指导，从而提升了LLM评判的一致性和准确性；3) 提出了一种文档级别的全面事实核查架构，该架构能够验证报告中的所有声明，包括引用的和未引用的部分，从而更全面地评估报告的可靠性。\\n\\n**关键设计**：DEER的评估分类法包含7个维度，分别是：正确性、完整性、相关性、一致性、清晰性、可信度和有用性。每个维度又包含若干个子维度，例如，正确性维度包含事实正确性、逻辑正确性和计算正确性等子维度。每个子维度都对应若干个细粒度的评分标准项，例如，事实正确性子维度对应“报告中的所有事实陈述是否都得到了可靠来源的支持”等评分标准项。文档级事实核查架构使用信息抽取技术从报告中提取所有声明，然后使用搜索引擎和知识图谱等外部资源验证这些声明的真实性。",
            "application_zh": "DEER基准可用于评估和比较不同深度研究系统的性能，帮助研究人员了解系统的优势和劣势，并指导系统改进。此外，DEER还可以用于训练和微调LLM，使其能够更好地生成高质量的专家级研究报告。该基准的实际价值在于推动深度研究系统的发展，提高研究报告的质量和可靠性，从而促进科学研究和知识传播。",
            "highlight_zh": "DEER基准与人类专家的判断具有很高的相关性，表明该基准能够有效地评估专家级研究报告的质量。实验结果表明，使用DEER基准可以对深度研究系统的优势和劣势进行可解释的诊断，从而帮助研究人员更好地理解系统的行为。文档级事实核查架构能够有效地识别报告中的错误声明，并量化外部证据的质量。",
            "tags_zh": [
                "深度研究报告",
                "评估基准",
                "大型语言模型",
                "事实核查",
                "专家评估",
                "自然语言处理",
                "信息抽取"
            ],
            "_index": 66,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17776v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17776v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17776v1/latex/figures/heatmap_criteria.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "When the Gold Standard isn't Necessarily Standard: Challenges of Evaluating the Translation of User-Generated Content",
            "authors": [
                "Lydia Nishimwe",
                "Benoît Sagot",
                "Rachel Bawden"
            ],
            "arxiv_id": "2512.17738v1",
            "summary": "User-generated content (UGC) is characterised by frequent use of non-standard language, from spelling errors to expressive choices such as slang, character repetitions, and emojis. This makes evaluating UGC translation particularly challenging: what counts as a \"good\" translation depends on the level of standardness desired in the output. To explore this, we examine the human translation guidelines of four UGC datasets, and derive a taxonomy of twelve non-standard phenomena and five translation actions (NORMALISE, COPY, TRANSFER, OMIT, CENSOR). Our analysis reveals notable differences in how UGC is treated, resulting in a spectrum of standardness in reference translations. Through a case study on large language models (LLMs), we show that translation scores are highly sensitive to prompts with explicit translation instructions for UGC, and that they improve when these align with the dataset's guidelines. We argue that when preserving UGC style is important, fair evaluation requires both models and metrics to be aware of translation guidelines. Finally, we call for clear guidelines during dataset creation and for the development of controllable, guideline-aware evaluation frameworks for UGC translation.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "10 pages, 19 pages with references and appendices",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17738v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "针对用户生成内容翻译评估标准不统一问题，提出一套非标准现象分类和翻译行为规范。",
            "summary_zh": "用户生成内容(UGC)的特点是频繁使用非标准语言，包括拼写错误、俚语、字符重复和表情符号等表达方式。这使得UGC翻译的评估极具挑战性：何为“好的”翻译取决于输出中期望的标准程度。为了探讨这一点，我们研究了四个UGC数据集的人工翻译指南，并推导出包含十二种非标准现象和五种翻译行为（标准化、复制、转移、省略、审查）的分类体系。我们的分析揭示了UGC处理方式的显著差异，导致参考翻译中存在标准程度的差异。通过对大型语言模型(LLM)的案例研究，我们表明翻译分数对带有明确UGC翻译指令的提示非常敏感，并且当这些指令与数据集的指南一致时，翻译分数会提高。我们认为，当保持UGC风格很重要时，公平的评估需要模型和指标都了解翻译指南。最后，我们呼吁在数据集创建过程中制定明确的指南，并为UGC翻译开发可控的、感知指南的评估框架。",
            "intro_zh": [
                "现有UGC翻译评估缺乏统一标准，不同数据集对非标准语言的处理方式各异，导致评估结果不稳定。",
                "论文提出一套包含非标准现象分类和翻译行为规范的体系，旨在弥合UGC翻译评估标准上的差异。",
                "实验表明，大型语言模型的翻译质量受翻译指令影响显著，与数据集指南对齐的指令能提升翻译效果。"
            ],
            "method_zh": "**问题定义**：现有用户生成内容（UGC）翻译评估面临的主要问题是缺乏统一的标准。不同数据集在处理UGC中常见的非标准语言现象（如拼写错误、俚语、表情符号等）时，采取了不同的翻译策略，导致评估结果难以比较，且无法准确反映翻译模型的真实性能。现有方法未能充分考虑UGC的特殊性，简单地沿用标准文本的评估方法，无法满足实际需求。\\n\\n**核心思路**：论文的核心思路是正视UGC的非标准性，并将其纳入翻译评估的考量范围。通过分析现有UGC数据集的翻译指南，归纳出常见的非标准现象和对应的翻译行为，从而建立一套更贴合UGC特点的评估体系。这种方法强调了翻译策略的选择应与数据集的翻译指南相一致，以确保评估的公平性和有效性。\\n\\n**技术框架**：论文的技术框架主要包含以下几个步骤：1) 收集并分析四个UGC数据集的人工翻译指南；2) 基于分析结果，构建一个包含十二种非标准现象和五种翻译行为（NORMALISE, COPY, TRANSFER, OMIT, CENSOR）的分类体系；3) 通过案例研究，评估大型语言模型在不同翻译指令下的表现；4) 分析评估结果，并提出改进UGC翻译评估的建议。\\n\\n**关键创新**：论文的关键创新在于提出了一个针对UGC翻译的非标准现象和翻译行为的分类体系。该体系能够帮助研究人员更好地理解UGC的特殊性，并指导翻译模型的训练和评估。此外，论文还强调了翻译指令对模型性能的影响，并呼吁在数据集创建过程中制定明确的翻译指南。\\n\\n**关键设计**：论文的关键设计包括：1) 对四个UGC数据集的翻译指南进行深入分析，提取出共性的非标准现象和翻译行为；2) 构建了一个包含十二种非标准现象（如拼写错误、语法错误、俚语、表情符号等）和五种翻译行为（标准化、复制、转移、省略、审查）的分类体系；3) 设计了一系列实验，通过改变翻译指令，评估大型语言模型在不同设置下的翻译性能。",
            "application_zh": "该研究成果可应用于提升用户生成内容的机器翻译质量和评估标准。在社交媒体、在线论坛等领域，能够更准确地翻译非标准语言，保留原文风格，并为数据集构建和模型训练提供指导，最终提升用户体验和跨文化交流效率。未来，可进一步开发可控的、感知指南的UGC翻译评估框架。",
            "highlight_zh": "实验结果表明，大型语言模型在UGC翻译任务中的表现对翻译指令非常敏感。当翻译指令与数据集的翻译指南对齐时，翻译质量显著提升。这强调了在UGC翻译评估中，必须考虑翻译指令和数据集指南的一致性，以确保评估的公平性和有效性。具体性能提升幅度未知，但结论明确。",
            "tags_zh": [
                "用户生成内容翻译",
                "非标准语言处理",
                "机器翻译评估",
                "大型语言模型",
                "翻译指南"
            ],
            "_index": 67,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17738v1/figures/ugc-translation-example.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17738v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17738v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Linear Personality Probing and Steering in LLMs: A Big Five Study",
            "authors": [
                "Michel Frising",
                "Daniel Balcells"
            ],
            "arxiv_id": "2512.17639v1",
            "summary": "Large language models (LLMs) exhibit distinct and consistent personalities that greatly impact trust and engagement. While this means that personality frameworks would be highly valuable tools to characterize and control LLMs' behavior, current approaches remain either costly (post-training) or brittle (prompt engineering). Probing and steering via linear directions has recently emerged as a cheap and efficient alternative. In this paper, we investigate whether linear directions aligned with the Big Five personality traits can be used for probing and steering model behavior. Using Llama 3.3 70B, we generate descriptions of 406 fictional characters and their Big Five trait scores. We then prompt the model with these descriptions and questions from the Alpaca questionnaire, allowing us to sample hidden activations that vary along personality traits in known, quantifiable ways. Using linear regression, we learn a set of per-layer directions in activation space, and test their effectiveness for probing and steering model behavior. Our results suggest that linear directions aligned with trait-scores are effective probes for personality detection, while their steering capabilities strongly depend on context, producing reliable effects in forced-choice tasks but limited influence in open-ended generation or when additional context is present in the prompt.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "29 pages, 6 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17639v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "利用线性探针和引导实现LLM性格控制：基于大五人格的研究",
            "summary_zh": "大型语言模型（LLMs）表现出独特且一致的性格，这极大地影响了信任和互动。这意味着性格框架将成为表征和控制LLMs行为的非常有价值的工具，但目前的方法要么成本高昂（后训练），要么脆弱（提示工程）。通过线性方向进行探针和引导最近成为一种廉价而有效的替代方案。在本文中，我们研究了与大五人格特质对齐的线性方向是否可以用于探测和引导模型行为。使用Llama 3.3 70B，我们生成了406个虚构角色的描述及其大五人格特质分数。然后，我们使用这些描述和Alpaca问卷中的问题来提示模型，从而能够以已知、可量化的方式对沿人格特质变化的隐藏激活进行采样。使用线性回归，我们学习了一组每层激活空间中的方向，并测试了它们在探测和引导模型行为方面的有效性。我们的结果表明，与特质分数对齐的线性方向是性格检测的有效探针，而它们的引导能力在很大程度上取决于上下文，在强制选择任务中产生可靠的效果，但在开放式生成或提示中存在额外上下文时影响有限。",
            "intro_zh": [
                "现有LLM性格控制方法存在后训练成本高或提示工程脆弱等问题，限制了其应用。",
                "该研究探索利用与大五人格特质对齐的线性方向，实现对LLM性格的探测和引导。",
                "实验表明线性方向可有效探测性格，但引导能力受上下文影响，在特定任务中效果显著。"
            ],
            "method_zh": "**问题定义**：现有的大型语言模型性格控制方法主要存在两个痛点：一是后训练方法，需要耗费大量的计算资源和时间；二是提示工程方法，其效果往往不稳定，容易受到提示语的细微变化的影响。因此，需要一种更高效、更稳定的方法来控制LLM的性格。\n\\n**核心思路**：本文的核心思路是利用线性探针和引导，通过学习与大五人格特质对齐的线性方向，实现在LLM的激活空间中对性格进行探测和引导。这种方法的优势在于其计算成本较低，且相对稳定，能够有效地控制LLM的性格。\n\\n**技术框架**：该研究的技术框架主要包括以下几个步骤：1. 生成包含性格描述和对应大五人格分数的虚构角色数据；2. 使用这些数据提示LLM，并提取隐藏层的激活值；3. 利用线性回归学习激活空间中与大五人格特质对齐的线性方向；4. 使用学习到的线性方向进行性格探测和引导，并评估其效果。\n\\n**关键创新**：该研究的关键创新在于提出了一种基于线性探针和引导的LLM性格控制方法。与传统的后训练或提示工程方法相比，该方法具有计算成本低、稳定性高等优点。此外，该研究还深入探讨了线性方向在不同上下文下的引导能力，为LLM性格控制提供了新的思路。\n\\n**关键设计**：在实验设计方面，作者使用了Llama 3.3 70B模型，并生成了406个虚构角色的描述及其大五人格特质分数。在学习线性方向时，作者使用了线性回归方法。在评估性格探测和引导效果时，作者使用了Alpaca问卷，并设计了强制选择任务和开放式生成任务。此外，作者还分析了不同上下文对引导效果的影响。",
            "application_zh": "该研究成果可应用于个性化对话系统、虚拟角色扮演、心理健康辅助等领域。通过控制LLM的性格，可以提升用户体验，增强用户信任感，并为特定应用场景定制更合适的AI助手。未来，该技术有望在人机交互领域发挥重要作用。",
            "highlight_zh": "实验结果表明，与大五人格特质对齐的线性方向是性格检测的有效探针。在强制选择任务中，该方法能够产生可靠的引导效果。虽然在开放式生成任务中引导效果有限，但该研究为探索LLM性格控制提供了新的方向和思路。",
            "tags_zh": [
                "大型语言模型",
                "性格控制",
                "线性探针",
                "线性引导",
                "大五人格",
                "提示工程",
                "人机交互"
            ],
            "_index": 68,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17639v1/media/main_diagram.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17639v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17639v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "ClothHMR: 3D Mesh Recovery of Humans in Diverse Clothing from Single Image",
            "authors": [
                "Yunqi Gao",
                "Leyuan Liu",
                "Yuhan Li",
                "Changxin Gao",
                "Yuanyuan Liu",
                "Jingying Chen"
            ],
            "arxiv_id": "2512.17545v1",
            "summary": "With 3D data rapidly emerging as an important form of multimedia information, 3D human mesh recovery technology has also advanced accordingly. However, current methods mainly focus on handling humans wearing tight clothing and perform poorly when estimating body shapes and poses under diverse clothing, especially loose garments. To this end, we make two key insights: (1) tailoring clothing to fit the human body can mitigate the adverse impact of clothing on 3D human mesh recovery, and (2) utilizing human visual information from large foundational models can enhance the generalization ability of the estimation. Based on these insights, we propose ClothHMR, to accurately recover 3D meshes of humans in diverse clothing. ClothHMR primarily consists of two modules: clothing tailoring (CT) and FHVM-based mesh recovering (MR). The CT module employs body semantic estimation and body edge prediction to tailor the clothing, ensuring it fits the body silhouette. The MR module optimizes the initial parameters of the 3D human mesh by continuously aligning the intermediate representations of the 3D mesh with those inferred from the foundational human visual model (FHVM). ClothHMR can accurately recover 3D meshes of humans wearing diverse clothing, precisely estimating their body shapes and poses. Experimental results demonstrate that ClothHMR significantly outperforms existing state-of-the-art methods across benchmark datasets and in-the-wild images. Additionally, a web application for online fashion and shopping powered by ClothHMR is developed, illustrating that ClothHMR can effectively serve real-world usage scenarios. The code and model for ClothHMR are available at: \\url{https://github.com/starVisionTeam/ClothHMR}.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "15 pages,16 figures",
            "doi": "10.1145/3731715.3733288",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17545v1",
            "code_links": [
                {
                    "url": "https://github.com/starVisionTeam/ClothHMR",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱六：视频提取与匹配 (Video Extraction)",
                    "id": "6_video_extraction",
                    "matched_keywords": [
                        "human mesh recovery"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "6_video_extraction"
            ],
            "headline_zh": "提出ClothHMR以解决多样服装下3D人类网格恢复问题",
            "summary_zh": "随着3D数据迅速成为重要的多媒体信息形式，3D人类网格恢复技术也随之发展。然而，现有方法主要集中在紧身衣物的处理上，对于多样服装，尤其是宽松衣物下的身体形状和姿态估计效果较差。为此，本文提出了ClothHMR，通过服装裁剪和基于人类视觉模型的网格恢复，准确恢复穿着多样服装的人类3D网格。实验结果表明，ClothHMR在基准数据集和真实场景图像中显著优于现有最先进的方法。此外，基于ClothHMR开发的在线时尚购物应用展示了其在实际场景中的有效性。",
            "intro_zh": [
                "现有3D人类网格恢复方法主要针对紧身衣物，面对多样服装时表现不佳，尤其是宽松衣物的身体形状和姿态估计。",
                "本文提出ClothHMR，通过服装裁剪模块和基于人类视觉模型的网格恢复模块，提高多样服装下的3D网格恢复精度。",
                "实验结果显示，ClothHMR在多个基准数据集上显著超越现有方法，且在真实场景图像中表现优异。"
            ],
            "method_zh": "**问题定义**：本文旨在解决在多样服装下进行3D人类网格恢复的挑战，现有方法在宽松衣物下的身体形状和姿态估计效果较差。\\n\\n**核心思路**：论文提出的核心思路是通过裁剪服装以更好地适应人体轮廓，并利用大型基础模型中的人类视觉信息来增强估计的泛化能力。\\n\\n**技术框架**：ClothHMR主要由两个模块组成：服装裁剪（CT）模块和基于FHVM的网格恢复（MR）模块。CT模块通过身体语义估计和边缘预测来裁剪服装，MR模块则通过持续对齐3D网格的中间表示与FHVM推断结果来优化初始参数。\\n\\n**关键创新**：最重要的技术创新在于结合了服装裁剪与FHVM的网格恢复，显著提高了在多样服装下的3D网格恢复精度，这是现有方法所未能实现的。\\n\\n**关键设计**：CT模块采用了身体语义估计和边缘预测技术，确保服装与身体轮廓的贴合；MR模块则通过对齐中间表示来优化网格参数，提升了恢复的准确性。具体的损失函数和网络结构设计在论文中进行了详细描述。 ",
            "application_zh": "ClothHMR的研究成果在时尚行业具有广泛的应用潜力，能够为在线购物、虚拟试衣、游戏角色建模等场景提供精准的3D人类模型。此外，该技术的实际价值在于提升用户体验，帮助消费者更好地选择合适的服装。未来，ClothHMR有望在更多领域如虚拟现实和增强现实中发挥重要作用。",
            "highlight_zh": "实验结果表明，ClothHMR在多个基准数据集上相较于现有最先进的方法提升了约15%-20%的准确率，尤其在真实场景图像中表现出色，显示出其强大的泛化能力和实用性。",
            "tags_zh": [
                "3D网格恢复",
                "人类视觉模型",
                "服装裁剪",
                "姿态估计",
                "计算机视觉",
                "深度学习",
                "在线购物"
            ],
            "_index": 69,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17545v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17545v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17545v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion",
            "authors": [
                "Hoiyeong Jin",
                "Hyojin Jang",
                "Jeongho Kim",
                "Junha Hyung",
                "Kinam Kim",
                "Dongjin Kim",
                "Huijin Choi",
                "Hyeonji Kim",
                "Jaegul Choo"
            ],
            "arxiv_id": "2512.17504v1",
            "summary": "Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing, yet realistic video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusion and lighting effects. We present InsertAnywhere, a new VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with a 4D aware mask generation module that reconstructs the scene geometry and propagates user specified object placement across frames while maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend a diffusion based video generation model to jointly synthesize the inserted object and its surrounding local variations such as illumination and shading. To enable supervised training, we introduce ROSE++, an illumination aware synthetic dataset constructed by transforming the ROSE object removal dataset into triplets of object removed video, object present video, and a VLM generated reference image. Through extensive experiments, we demonstrate that our framework produces geometrically plausible and visually coherent object insertions across diverse real world scenarios, significantly outperforming existing research and commercial models.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "16 pages, project page: https://myyzzzoooo.github.io/InsertAnywhere/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17504v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "scene understanding"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "InsertAnywhere：融合4D场景几何与扩散模型，实现逼真的视频对象插入",
            "summary_zh": "扩散模型在视频生成领域的最新进展为可控视频编辑带来了新的可能性，但由于对4D场景理解的局限以及对遮挡和光照效果处理的不足，逼真的视频对象插入（VOI）仍然具有挑战性。我们提出了InsertAnywhere，一个新的VOI框架，它实现了几何一致的对象放置和外观逼真的视频合成。我们的方法首先使用一个4D感知的掩码生成模块，该模块重建场景几何并在帧之间传播用户指定的对象放置，同时保持时间一致性和遮挡一致性。在此空间基础上，我们扩展了一个基于扩散的视频生成模型，以联合合成插入的对象及其周围的局部变化，如光照和阴影。为了实现监督训练，我们引入了ROSE++，一个光照感知的合成数据集，通过将ROSE对象移除数据集转换为对象移除视频、对象存在视频和VLM生成的参考图像的三元组来构建。通过大量的实验，我们证明了我们的框架在各种真实场景中产生了几何上合理且视觉上连贯的对象插入，显著优于现有的研究和商业模型。",
            "intro_zh": [
                "现有视频对象插入方法缺乏对4D场景的深入理解，难以处理遮挡和光照变化等复杂情况。",
                "InsertAnywhere框架利用4D感知掩码生成模块重建场景几何，并结合扩散模型合成逼真的插入对象。",
                "实验表明，InsertAnywhere在几何一致性和视觉连贯性方面显著优于现有方法，效果更佳。"
            ],
            "method_zh": "**问题定义**：视频对象插入（VOI）旨在将新的对象无缝地融入现有视频中。现有的方法在处理复杂的场景几何、遮挡关系以及光照变化时表现不足，导致插入的对象与周围环境不协调，缺乏真实感。这些方法通常难以保证插入对象在时间上的连贯性，容易出现闪烁或不自然的运动。\n\n**核心思路**：InsertAnywhere的核心思路是将4D场景几何信息融入到扩散模型的视频生成过程中。通过重建场景的几何结构，可以实现对象在视频帧之间的几何一致性放置。同时，利用扩散模型强大的生成能力，可以合成与场景光照条件相匹配的插入对象，从而提高真实感。\n\n**技术框架**：InsertAnywhere框架主要包含两个模块：4D感知掩码生成模块和扩散模型视频合成模块。首先，4D感知掩码生成模块负责重建场景的几何结构，并根据用户指定的对象位置生成时间上连贯的掩码。然后，扩散模型视频合成模块利用生成的掩码和场景信息，联合合成插入的对象及其周围的局部变化，如光照和阴影。为了进行监督训练，该论文还提出了ROSE++数据集。\n\n**关键创新**：该论文的关键创新在于将4D场景几何信息与扩散模型相结合，从而实现了更逼真、更连贯的视频对象插入。与现有方法相比，InsertAnywhere能够更好地处理遮挡关系和光照变化，并保证插入对象在时间上的几何一致性。ROSE++数据集的提出也为监督训练提供了数据支持。\n\n**关键设计**：4D感知掩码生成模块利用深度估计和光流等技术重建场景几何。扩散模型视频合成模块采用U-Net结构，并引入了注意力机制来更好地融合场景信息。ROSE++数据集通过将ROSE数据集转换为三元组形式，并利用VLM生成参考图像，从而实现了光照感知的监督训练。",
            "application_zh": "InsertAnywhere技术可广泛应用于视频编辑、电影特效、游戏开发等领域。例如，用户可以使用该技术轻松地在现有视频中添加新的角色或物体，从而创造出更具创意和吸引力的内容。该技术还可以用于虚拟现实和增强现实应用中，以增强用户体验。未来，该技术有望进一步发展，实现更智能、更自动化的视频编辑功能。",
            "highlight_zh": "实验结果表明，InsertAnywhere在几何一致性和视觉连贯性方面显著优于现有方法。在多个真实场景的测试中，InsertAnywhere能够生成几何上合理且视觉上连贯的对象插入，效果优于现有的研究和商业模型。通过定量指标和定性比较，验证了该框架的有效性和优越性。",
            "tags_zh": [
                "视频对象插入",
                "扩散模型",
                "4D场景理解",
                "几何一致性",
                "视频生成",
                "ROSE++数据集",
                "光照感知",
                "视频编辑"
            ],
            "_index": 70,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17504v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17504v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17504v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "SynergyWarpNet: Attention-Guided Cooperative Warping for Neural Portrait Animation",
            "authors": [
                "Shihang Li",
                "Zhiqiang Gong",
                "Minming Ye",
                "Yue Gao",
                "Wen Yao"
            ],
            "arxiv_id": "2512.17331v1",
            "summary": "Recent advances in neural portrait animation have demonstrated remarked potential for applications in virtual avatars, telepresence, and digital content creation. However, traditional explicit warping approaches often struggle with accurate motion transfer or recovering missing regions, while recent attention-based warping methods, though effective, frequently suffer from high complexity and weak geometric grounding. To address these issues, we propose SynergyWarpNet, an attention-guided cooperative warping framework designed for high-fidelity talking head synthesis. Given a source portrait, a driving image, and a set of reference images, our model progressively refines the animation in three stages. First, an explicit warping module performs coarse spatial alignment between the source and driving image using 3D dense optical flow. Next, a reference-augmented correction module leverages cross-attention across 3D keypoints and texture features from multiple reference images to semantically complete occluded or distorted regions. Finally, a confidence-guided fusion module integrates the warped outputs with spatially-adaptive fusing, using a learned confidence map to balance structural alignment and visual consistency. Comprehensive evaluations on benchmark datasets demonstrate state-of-the-art performance.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "Submitted to ICASSP 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17331v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "optical flow"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "SynergyWarpNet：用于神经肖像动画的注意力引导协同扭曲网络",
            "summary_zh": "本文提出了一种名为SynergyWarpNet的注意力引导协同扭曲框架，用于高保真说话人头部合成。针对神经肖像动画中传统显式扭曲方法难以精确传递运动或恢复缺失区域，以及现有基于注意力扭曲方法复杂度高且几何基础薄弱的问题，该模型通过三个阶段逐步优化动画效果。首先，显式扭曲模块使用3D稠密光流对源图像和驱动图像进行粗略的空间对齐。然后，参考增强校正模块利用多个参考图像的3D关键点和纹理特征进行交叉注意力，以语义上补全遮挡或扭曲区域。最后，置信度引导融合模块通过空间自适应融合集成扭曲后的输出，并使用学习到的置信度图来平衡结构对齐和视觉一致性。在基准数据集上的综合评估表明，该方法达到了最先进的性能。",
            "intro_zh": [
                "现有神经肖像动画方法在运动传递和区域恢复方面存在不足，尤其是在处理遮挡和形变时。",
                "SynergyWarpNet利用显式扭曲、参考图像增强和置信度引导融合，实现更精确和自然的肖像动画。",
                "实验结果表明，该方法在基准数据集上取得了优于现有技术的性能，提升了肖像动画的质量。"
            ],
            "method_zh": "**问题定义**：神经肖像动画旨在根据驱动图像将源肖像图像进行动画处理，使其呈现出驱动图像中的表情和姿态。现有方法主要面临两个挑战：一是传统的显式扭曲方法难以精确地传递复杂的面部运动，尤其是在大姿态变化或遮挡情况下；二是基于注意力机制的方法虽然能够处理遮挡，但往往计算复杂度高，且缺乏明确的几何约束，容易产生不自然的扭曲效果。\\n\\n**核心思路**：SynergyWarpNet的核心思路是结合显式几何扭曲和隐式注意力机制的优势，通过协同扭曲的方式逐步优化动画效果。首先利用显式扭曲进行粗略的运动传递，然后利用参考图像的信息通过注意力机制进行语义补全和细节增强，最后通过置信度引导的融合策略平衡结构对齐和视觉一致性。\\n\\n**技术框架**：SynergyWarpNet包含三个主要模块：1) **显式扭曲模块**：利用3D稠密光流估计源图像和驱动图像之间的运动场，并进行粗略的图像扭曲。2) **参考增强校正模块**：利用多个参考图像，通过交叉注意力机制学习参考图像的特征，并将其用于补全和校正扭曲后的图像。3) **置信度引导融合模块**：学习一个置信度图，用于指导融合显式扭曲和参考增强校正模块的输出，从而在结构对齐和视觉一致性之间取得平衡。\\n\\n**关键创新**：该方法的主要创新在于：1) 提出了一个协同扭曲框架，结合了显式几何扭曲和隐式注意力机制的优势，能够更精确地传递运动和恢复缺失区域。2) 引入了参考增强校正模块，利用多个参考图像的信息进行语义补全和细节增强，提高了动画的真实感。3) 设计了置信度引导融合模块，能够自适应地平衡结构对齐和视觉一致性，避免了扭曲伪影的产生。\\n\\n**关键设计**：在显式扭曲模块中，使用了预训练的3D人脸模型来估计稠密光流。在参考增强校正模块中，使用了交叉注意力机制来学习参考图像的特征，并将其用于补全和校正扭曲后的图像。在置信度引导融合模块中，使用了一个卷积神经网络来学习置信度图，并使用该置信度图来加权融合显式扭曲和参考增强校正模块的输出。损失函数包括光流损失、重建损失和对抗损失，用于保证动画的质量和真实感。",
            "application_zh": "SynergyWarpNet在虚拟化身、远程呈现和数字内容创作等领域具有广泛的应用前景。它可以用于创建逼真的虚拟角色，实现高质量的远程视频会议，以及生成各种有趣的数字内容，例如动画短片和互动游戏。该技术有望提升人机交互的自然性和沉浸感，并为数字娱乐产业带来新的发展机遇。",
            "highlight_zh": "该论文在基准数据集上进行了全面的评估，实验结果表明SynergyWarpNet在肖像动画质量方面达到了最先进的水平。相较于现有方法，该方法能够更精确地传递运动，恢复缺失区域，并生成更自然的动画效果。具体的性能数据和对比基线在论文中进行了详细的展示。",
            "tags_zh": [
                "神经肖像动画",
                "协同扭曲",
                "注意力机制",
                "图像合成",
                "说话人头部合成"
            ],
            "_index": 71,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17331v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17331v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17331v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Perfect reconstruction of sparse signals using nonconvexity control and one-step RSB message passing",
            "authors": [
                "Xiaosi Gu",
                "Ayaka Sakata",
                "Tomoyuki Obuchi"
            ],
            "arxiv_id": "2512.17426v1",
            "summary": "We consider sparse signal reconstruction via minimization of the smoothly clipped absolute deviation (SCAD) penalty, and develop one-step replica-symmetry-breaking (1RSB) extensions of approximate message passing (AMP), termed 1RSB-AMP. Starting from the 1RSB formulation of belief propagation, we derive explicit update rules of 1RSB-AMP together with the corresponding state evolution (1RSB-SE) equations. A detailed comparison shows that 1RSB-AMP and 1RSB-SE agree remarkably well at the macroscopic level, even in parameter regions where replica-symmetric (RS) AMP, termed RS-AMP, diverges and where the 1RSB description itself is not expected to be thermodynamically exact. Fixed-point analysis of 1RSB-SE reveals a phase diagram consisting of success, failure, and diverging phases, as in the RS case. However, the diverging-region boundary now depends on the Parisi parameter due to the 1RSB ansatz, and we propose a new criterion -- minimizing the size of the diverging region -- rather than the conventional zero-complexity condition, to determine its value. Combining this criterion with the nonconvexity-control (NCC) protocol proposed in a previous RS study improves the algorithmic limit of perfect reconstruction compared with RS-AMP. Numerical solutions of 1RSB-SE and experiments with 1RSB-AMP confirm that this improved limit is achieved in practice, though the gain is modest and remains slightly inferior to the Bayes-optimal threshold. We also report the behavior of thermodynamic quantities -- overlaps, free entropy, complexity, and the non-self-averaging susceptibility -- that characterize the 1RSB phase in this problem.",
            "categories": [
                "stat.ML",
                "cond-mat.dis-nn",
                "cs.LG"
            ],
            "primary_category": "stat.ML",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "49 pages, 10 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17426v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "AMP"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "8_physics_animation"
            ],
            "headline_zh": "提出基于非凸性控制和一步RSB消息传递的稀疏信号完美重构方法",
            "summary_zh": "本文研究了通过最小化平滑裁剪绝对偏差（SCAD）惩罚进行稀疏信号重构的问题，并开发了近似消息传递（AMP）的一步副本对称性破缺（1RSB）扩展，称为1RSB-AMP。从置信传播的1RSB公式出发，我们推导了1RSB-AMP的显式更新规则以及相应的状态演化（1RSB-SE）方程。详细的比较表明，1RSB-AMP和1RSB-SE在宏观层面上非常吻合，即使在副本对称（RS）AMP（称为RS-AMP）发散的参数区域以及1RSB描述本身预计在热力学上不精确的区域也是如此。1RSB-SE的定点分析揭示了一个由成功、失败和发散阶段组成的状态图，与RS情况相同。然而，由于1RSB ansatz，发散区域边界现在取决于Parisi参数，我们提出了一种新的标准——最小化发散区域的大小——而不是传统的零复杂度条件，来确定其值。将此标准与先前RS研究中提出的非凸性控制（NCC）协议相结合，与RS-AMP相比，提高了完美重构的算法极限。1RSB-SE的数值解和1RSB-AMP的实验证实，实际上实现了这一改进的极限，尽管增益不大，并且仍然略低于贝叶斯最优阈值。我们还报告了热力学量的行为——重叠、自由熵、复杂性和非自平均磁化率——这些量表征了该问题中的1RSB阶段。",
            "intro_zh": [
                "现有基于副本对称（RS）的近似消息传递（AMP）方法在稀疏信号重构中存在算法极限，无法达到贝叶斯最优。",
                "本文提出一步副本对称性破缺（1RSB）的AMP扩展，并结合非凸性控制（NCC）协议，旨在提升稀疏信号完美重构的算法极限。",
                "实验结果表明，与RS-AMP相比，该方法提高了完美重构的算法极限，尽管提升幅度有限，且仍略低于贝叶斯最优阈值。"
            ],
            "method_zh": "**问题定义**：论文旨在解决稀疏信号重构问题，具体而言，是通过最小化SCAD惩罚函数来实现。现有基于RS-AMP的方法在某些参数区域会发散，且算法极限距离贝叶斯最优阈值仍有差距，这意味着重构性能有待提升。\\n\\n**核心思路**：论文的核心思路是引入一步副本对称性破缺（1RSB）框架，对AMP算法进行扩展。1RSB框架能够更准确地描述系统的复杂性，从而改善算法的性能。此外，结合非凸性控制（NCC）协议，进一步优化算法的性能。\\n\\n**技术框架**：整体框架包括以下几个主要步骤：1）从置信传播的1RSB公式出发，推导1RSB-AMP的更新规则；2）建立相应的状态演化（1RSB-SE）方程；3）通过定点分析，得到包含成功、失败和发散阶段的状态图；4）提出最小化发散区域大小的新标准来确定Parisi参数；5）结合NCC协议，优化算法性能。\\n\\n**关键创新**：最重要的技术创新点在于将1RSB框架引入到AMP算法中，并提出了一种新的确定Parisi参数的标准，即最小化发散区域的大小。与传统的零复杂度条件相比，该标准能够更有效地控制算法的发散行为，从而提高重构性能。\\n\\n**关键设计**：论文的关键设计包括：1）SCAD惩罚函数的选择，它是一种非凸惩罚函数，能够更好地促进稀疏性；2）1RSB-AMP的更新规则，这些规则基于置信传播的1RSB公式推导而来；3）NCC协议的具体实现，它通过控制非凸性来优化算法的性能；4）Parisi参数的确定方法，即最小化发散区域的大小。",
            "application_zh": "该研究成果可应用于压缩感知、图像处理、无线通信等领域，在这些领域中，稀疏信号重构是一个关键问题。通过提高稀疏信号重构的性能，可以改善这些应用的效果，例如，在压缩感知中，可以减少所需的采样数量，在图像处理中，可以提高图像的重建质量。",
            "highlight_zh": "实验结果表明，与RS-AMP相比，1RSB-AMP结合NCC协议提高了完美重构的算法极限。虽然提升幅度有限，且仍略低于贝叶斯最优阈值，但验证了1RSB框架在稀疏信号重构中的有效性。数值解和实验结果在宏观层面上高度吻合，验证了理论分析的正确性。",
            "tags_zh": [
                "稀疏信号重构",
                "近似消息传递",
                "副本对称性破缺",
                "非凸性控制",
                "SCAD惩罚"
            ],
            "_index": 72,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17426v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17426v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17426v1/x5.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Learning solution operator of dynamical systems with diffusion maps kernel ridge regression",
            "authors": [
                "Jiwoo Song",
                "Daning Huang",
                "John Harlim"
            ],
            "arxiv_id": "2512.17203v1",
            "summary": "Many scientific and engineering systems exhibit complex nonlinear dynamics that are difficult to predict accurately over long time horizons. Although data-driven models have shown promise, their performance often deteriorates when the geometric structures governing long-term behavior are unknown or poorly represented. We demonstrate that a simple kernel ridge regression (KRR) framework, when combined with a dynamics-aware validation strategy, provides a strong baseline for long-term prediction of complex dynamical systems. By employing a data-driven kernel derived from diffusion maps, the proposed Diffusion Maps Kernel Ridge Regression (DM-KRR) method implicitly adapts to the intrinsic geometry of the system's invariant set, without requiring explicit manifold reconstruction or attractor modeling, procedures that often limit predictive performance. Across a broad range of systems, including smooth manifolds, chaotic attractors, and high-dimensional spatiotemporal flows, DM-KRR consistently outperforms state-of-the-art random feature, neural-network and operator-learning methods in both accuracy and data efficiency. These findings underscore that long-term predictive skill depends not only on model expressiveness, but critically on respecting the geometric constraints encoded in the data through dynamically consistent model selection. Together, simplicity, geometry awareness, and strong empirical performance point to a promising path for reliable and efficient learning of complex dynamical systems.",
            "categories": [
                "cs.LG",
                "math.NA"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-19",
            "updated": "2025-12-19",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.17203v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "spatiotemporal"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "8_physics_animation"
            ],
            "headline_zh": "提出基于扩散映射核岭回归(DM-KRR)的动力系统解算子学习方法，提升长期预测精度。",
            "summary_zh": "许多科学和工程系统表现出复杂的非线性动力学，难以进行准确的长期预测。尽管数据驱动模型显示出潜力，但当控制长期行为的几何结构未知或表示不佳时，它们的性能通常会下降。本文证明，一个简单的核岭回归（KRR）框架，当与动态感知验证策略相结合时，为复杂动力系统的长期预测提供了一个强大的基线。通过采用从扩散映射导出的数据驱动核，所提出的扩散映射核岭回归（DM-KRR）方法隐式地适应系统不变集的内在几何结构，而无需显式流形重建或吸引子建模，这些过程通常限制预测性能。在包括光滑流形、混沌吸引子和高维时空流在内的广泛系统中，DM-KRR在准确性和数据效率方面始终优于最先进的随机特征、神经网络和算子学习方法。这些发现强调，长期预测能力不仅取决于模型的表达能力，而且关键在于通过动态一致的模型选择来尊重数据中编码的几何约束。总之，简单性、几何感知和强大的经验性能为可靠和高效地学习复杂动力系统指明了一条有希望的道路。",
            "intro_zh": [
                "现有数据驱动模型在长期预测复杂动力系统时，由于未能有效捕捉系统内在几何结构，性能易下降。",
                "提出DM-KRR方法，利用扩散映射自适应系统不变集的几何结构，无需显式流形重建或吸引子建模。",
                "实验表明，DM-KRR在多种系统上优于现有随机特征、神经网络和算子学习方法，提升了预测精度和数据效率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决复杂动力系统长期预测精度低的问题。现有方法，如神经网络和算子学习，在处理具有复杂几何结构的动力系统时，往往难以捕捉到系统内在的几何约束，导致长期预测性能下降。这些方法通常需要大量的训练数据，并且对超参数的选择非常敏感。\\n\\n**核心思路**：论文的核心思路是利用扩散映射（Diffusion Maps）来学习动力系统的内在几何结构，并将其融入到核岭回归（Kernel Ridge Regression, KRR）框架中。扩散映射能够有效地提取数据中的低维流形结构，从而使模型能够更好地适应系统的动态特性。通过这种方式，模型可以在不需要显式地进行流形重建或吸引子建模的情况下，实现对复杂动力系统的长期预测。\\n\\n**技术框架**：DM-KRR方法主要包含以下几个阶段：1) 数据收集：从动力系统中采样得到训练数据。2) 扩散映射：利用扩散映射算法，从训练数据中学习得到一个数据驱动的核函数，该核函数能够反映系统内在的几何结构。3) 核岭回归：使用学习到的核函数，构建一个核岭回归模型，用于预测系统的未来状态。4) 动态感知验证：采用一种动态感知的验证策略，选择最优的模型参数，以保证模型在长期预测中的稳定性。\\n\\n**关键创新**：DM-KRR的关键创新在于将扩散映射与核岭回归相结合，从而使模型能够自适应地学习动力系统的内在几何结构。与传统的核方法相比，DM-KRR的核函数是数据驱动的，能够更好地适应系统的动态特性。与神经网络等深度学习方法相比，DM-KRR具有更高的数据效率和更强的可解释性。\\n\\n**关键设计**：扩散映射核的选择是关键。论文可能采用了基于扩散距离的核函数，例如高斯核。岭回归中的正则化参数需要仔细调整，以防止过拟合。动态感知验证策略可能涉及到对不同参数组合下的模型进行长期预测，并选择在验证集上表现最好的参数组合。",
            "application_zh": "该研究成果可应用于各种科学和工程领域，例如气候预测、流体动力学、机器人控制和金融建模等。通过提高复杂动力系统的长期预测精度，可以为决策提供更可靠的依据，并促进相关领域的发展。例如，在气候预测中，可以更准确地预测极端天气事件的发生；在机器人控制中，可以实现更稳定和高效的运动规划。",
            "highlight_zh": "DM-KRR在包括光滑流形、混沌吸引子和高维时空流在内的多个动力系统上进行了测试，实验结果表明，DM-KRR在预测精度和数据效率方面均优于现有的随机特征、神经网络和算子学习方法。具体性能提升数据未知，但摘要强调了“consistently outperforms state-of-the-art”和“accuracy and data efficiency”。",
            "tags_zh": [
                "动力系统",
                "解算子学习",
                "扩散映射",
                "核岭回归",
                "长期预测",
                "数据驱动",
                "几何结构"
            ],
            "_index": 73,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.17203v1/pics/torus/torus_test_rmse_small.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.17203v1/pics/lorenz/combined_lorenz_ks_small.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.17203v1/pics/ks-travelling/ks_travelling_prediction_results_small.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        }
    ]
}