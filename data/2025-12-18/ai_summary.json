{
    "papers": [
        {
            "title": "E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion",
            "authors": [
                "Enis Yalcin",
                "Joshua O'Hara",
                "Maria Stamatopoulou",
                "Chengxu Zhou",
                "Dimitrios Kanoulas"
            ],
            "arxiv_id": "2512.16446v1",
            "summary": "Vision-language models (VLMs) show promise in automating reward design in humanoid locomotion, which could eliminate the need for tedious manual engineering. However, current VLM-based methods are essentially \"blind\", as they lack the environmental perception required to navigate complex terrain. We present E-SDS (Environment-aware See it, Do it, Sorted), a framework that closes this perception gap. E-SDS integrates VLMs with real-time terrain sensor analysis to automatically generate reward functions that facilitate training of robust perceptive locomotion policies, grounded by example videos. Evaluated on a Unitree G1 humanoid across four distinct terrains (simple, gaps, obstacles, stairs), E-SDS uniquely enabled successful stair descent, while policies trained with manually-designed rewards or a non-perceptive automated baseline were unable to complete the task. In all terrains, E-SDS also reduced velocity tracking error by 51.9-82.6%. Our framework reduces the human effort of reward design from days to less than two hours while simultaneously producing more robust and capable locomotion policies.",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "12 pages, 3 figures, 4 tables. Accepted at RiTA 2025 (Springer LNNS)",
            "doi": "",
            "journal_ref": "RiTA 2025 (Springer LNNS)",
            "pdf_url": "https://arxiv.org/pdf/2512.16446v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]humanoid",
                        "[T]humanoid locomotion",
                        "[T]locomotion",
                        "locomotion policy",
                        "Unitree"
                    ],
                    "score": 22.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning",
                        "reward design"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 28.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "E-SDS：环境感知的人形机器人强化学习，解决复杂地形导航问题",
            "summary_zh": "本文提出E-SDS（Environment-aware See it, Do it, Sorted）框架，旨在解决人形机器人运动中环境感知不足的问题。E-SDS集成了视觉语言模型（VLM）和实时地形传感器分析，自动生成奖励函数，从而训练出具有鲁棒性的感知运动策略，并以示例视频作为指导。在Unitree G1人形机器人上，针对四种不同地形（简单地形、间隙、障碍物、楼梯）的评估表明，E-SDS能够成功完成下楼梯任务，而手动设计的奖励或非感知的自动化基线方法均无法完成此任务。在所有地形中，E-SDS还将速度跟踪误差降低了51.9-82.6%。该框架将奖励设计的人工工作量从数天减少到不到两小时，同时生成更强大和更具能力的运动策略。",
            "intro_zh": [
                "现有基于视觉语言模型的人形机器人运动方法缺乏环境感知能力，难以在复杂地形中导航。",
                "E-SDS框架结合视觉语言模型和地形传感器数据，自动生成奖励函数，提升运动策略的鲁棒性。",
                "实验表明，E-SDS在复杂地形（如楼梯）上表现出色，并显著降低了速度跟踪误差。"
            ],
            "method_zh": "**问题定义**：现有基于视觉语言模型(VLM)的人形机器人运动方法，在奖励函数设计上取得了进展，但忽略了环境感知的重要性。这些方法本质上是“盲目的”，无法根据复杂地形调整运动策略，导致在复杂环境中难以实现稳定和高效的运动。人工设计奖励函数耗时耗力，且难以泛化到不同地形。\n\n**核心思路**：E-SDS的核心思路是将视觉语言模型与实时地形感知相结合，利用VLM生成初步的奖励函数，然后通过地形传感器数据对奖励函数进行调整，使其能够适应不同的环境。这种环境感知的奖励函数能够引导机器人学习更鲁棒和适应性更强的运动策略。\n\n**技术框架**：E-SDS框架包含以下几个主要模块：1) **环境感知模块**：利用地形传感器（如激光雷达或深度相机）获取周围环境的几何信息。2) **视觉语言模型模块**：使用VLM根据示例视频生成初步的奖励函数，该奖励函数鼓励机器人模仿视频中的运动行为。3) **奖励函数调整模块**：根据环境感知模块获取的地形信息，对VLM生成的奖励函数进行调整，例如，在遇到障碍物时，增加避障的奖励。4) **强化学习训练模块**：使用调整后的奖励函数训练人形机器人的运动策略。\n\n**关键创新**：E-SDS的关键创新在于将视觉语言模型与实时地形感知相结合，实现了环境感知的自动奖励函数设计。与传统的基于VLM的方法相比，E-SDS能够根据环境信息动态调整奖励函数，从而提高运动策略的鲁棒性和适应性。与手动设计的奖励函数相比，E-SDS能够显著减少人工工作量，并生成更强大的运动策略。\n\n**关键设计**：E-SDS的关键设计包括：1) 使用预训练的视觉语言模型，例如CLIP，以提取示例视频中的语义信息。2) 设计合适的奖励函数调整策略，例如，根据地形坡度调整前进速度的奖励，根据障碍物距离调整避障的奖励。3) 使用合适的强化学习算法，例如PPO，训练人形机器人的运动策略。具体参数设置（如学习率、折扣因子等）需要根据具体任务进行调整。",
            "application_zh": "E-SDS框架可应用于各种人形机器人的运动控制，尤其是在复杂和动态环境中，例如灾难救援、物流运输、家庭服务等。该框架能够降低人形机器人运动控制的开发成本，提高机器人的自主性和适应性，使其能够更好地服务于人类。",
            "highlight_zh": "E-SDS在Unitree G1人形机器人上进行了评估，结果表明，E-SDS能够成功完成下楼梯任务，而手动设计的奖励或非感知的自动化基线方法均无法完成此任务。在所有地形中，E-SDS还将速度跟踪误差降低了51.9-82.6%。此外，E-SDS将奖励设计的人工工作量从数天减少到不到两小时。",
            "tags_zh": [
                "人形机器人",
                "强化学习",
                "视觉语言模型",
                "环境感知",
                "奖励函数设计"
            ],
            "_index": 0,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16446v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16446v1/figure/vel_chart.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16446v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future",
            "authors": [
                "Tianshuai Hu",
                "Xiaolu Liu",
                "Song Wang",
                "Yiyao Zhu",
                "Ao Liang",
                "Lingdong Kong",
                "Guoyang Zhao",
                "Zeying Gong",
                "Jun Cen",
                "Zhiyu Huang",
                "Xiaoshuai Hao",
                "Linfeng Li",
                "Hang Song",
                "Xiangtai Li",
                "Jun Ma",
                "Shaojie Shen",
                "Jianke Zhu",
                "Dacheng Tao",
                "Ziwei Liu",
                "Junwei Liang"
            ],
            "arxiv_id": "2512.16760v1",
            "summary": "Autonomous driving has long relied on modular \"Perception-Decision-Action\" pipelines, where hand-crafted interfaces and rule-based components often break down in complex or long-tailed scenarios. Their cascaded design further propagates perception errors, degrading downstream planning and control. Vision-Action (VA) models address some limitations by learning direct mappings from visual inputs to actions, but they remain opaque, sensitive to distribution shifts, and lack structured reasoning or instruction-following capabilities. Recent progress in Large Language Models (LLMs) and multimodal learning has motivated the emergence of Vision-Language-Action (VLA) frameworks, which integrate perception with language-grounded decision making. By unifying visual understanding, linguistic reasoning, and actionable outputs, VLAs offer a pathway toward more interpretable, generalizable, and human-aligned driving policies. This work provides a structured characterization of the emerging VLA landscape for autonomous driving. We trace the evolution from early VA approaches to modern VLA frameworks and organize existing methods into two principal paradigms: End-to-End VLA, which integrates perception, reasoning, and planning within a single model, and Dual-System VLA, which separates slow deliberation (via VLMs) from fast, safety-critical execution (via planners). Within these paradigms, we further distinguish subclasses such as textual vs. numerical action generators and explicit vs. implicit guidance mechanisms. We also summarize representative datasets and benchmarks for evaluating VLA-based driving systems and highlight key challenges and open directions, including robustness, interpretability, and instruction fidelity. Overall, this work aims to establish a coherent foundation for advancing human-compatible autonomous driving systems.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Preprint; 40 pages, 7 figures, 9 tables; GitHub at https://github.com/worldbench/awesome-vla-for-ad",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16760v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]vision-language-action",
                        "VLA",
                        "large language model",
                        "multimodal",
                        "instruction following"
                    ],
                    "score": 21.0
                }
            ],
            "relevance_score": 21.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "综述性论文：面向自动驾驶的视觉-语言-动作模型研究进展与未来展望",
            "summary_zh": "自动驾驶长期以来依赖于模块化的“感知-决策-行动”流程，但手工设计的接口和基于规则的组件在复杂或长尾场景中经常失效。其级联设计进一步传播感知误差，降低下游规划和控制的性能。视觉-动作（VA）模型通过学习从视觉输入到动作的直接映射来解决一些局限性，但它们仍然不透明，对分布偏移敏感，并且缺乏结构化推理或指令遵循能力。大型语言模型（LLM）和多模态学习的最新进展推动了视觉-语言-动作（VLA）框架的出现，该框架将感知与基于语言的决策相结合。通过统一视觉理解、语言推理和可操作的输出，VLA为更可解释、更通用和更符合人类习惯的驾驶策略提供了一条途径。本文对新兴的自动驾驶VLA领域进行了结构化描述，追溯了从早期VA方法到现代VLA框架的演变，并将现有方法组织成两种主要范例：端到端VLA，它在单个模型中集成了感知、推理和规划；双系统VLA，它将慢速审议（通过VLM）与快速、安全关键的执行（通过规划器）分开。在这些范例中，我们进一步区分了文本与数值动作生成器以及显式与隐式指导机制等子类。我们还总结了用于评估基于VLA的驾驶系统的代表性数据集和基准，并强调了关键挑战和开放方向，包括鲁棒性、可解释性和指令保真度。总的来说，这项工作旨在为推进人机兼容的自动驾驶系统奠定连贯的基础。",
            "intro_zh": [
                "传统自动驾驶依赖“感知-决策-行动”流程，但手工设计组件在复杂场景失效，且感知误差会向下游传播。",
                "视觉-语言-动作（VLA）模型通过结合视觉理解、语言推理和可执行动作，旨在实现更通用和可解释的自动驾驶策略。",
                "论文综述了VLA在自动驾驶中的应用，分析了端到端和双系统两种主要范例，并探讨了未来发展方向。"
            ],
            "method_zh": "**问题定义**：传统自动驾驶系统依赖于模块化的“感知-决策-行动”流程，这些流程通常包含手工设计的接口和规则，在复杂或长尾场景下表现不佳。此外，级联的设计会导致感知误差向下游传播，影响规划和控制的准确性。视觉-动作（VA）模型虽然尝试直接从视觉输入映射到动作，但缺乏可解释性，对数据分布变化敏感，并且难以进行结构化推理和指令遵循。\\n\\n**核心思路**：论文的核心思路是利用大型语言模型（LLM）和多模态学习的最新进展，构建视觉-语言-动作（VLA）框架，将视觉感知、语言推理和可执行动作统一起来。通过语言作为桥梁，VLA模型能够更好地理解驾驶场景，进行推理和决策，并生成符合人类指令的驾驶行为。这种方法旨在提高自动驾驶系统的通用性、可解释性和人机交互能力。\\n\\n**技术框架**：论文将现有的VLA方法组织成两种主要范例：端到端VLA和双系统VLA。端到端VLA模型将感知、推理和规划集成到一个单一的模型中，直接从视觉输入生成动作。双系统VLA模型则将慢速审议（通过视觉语言模型VLM）与快速、安全关键的执行（通过规划器）分开，VLM负责高级决策和指令生成，规划器负责低级别的动作控制。在这些范例中，还区分了文本与数值动作生成器以及显式与隐式指导机制等子类。\\n\\n**关键创新**：论文的主要创新在于对自动驾驶领域的VLA模型进行了系统性的综述和分类，提出了端到端VLA和双系统VLA两种主要范例，并对各种VLA方法的特点和优缺点进行了深入分析。此外，论文还总结了用于评估VLA模型的代表性数据集和基准，并指出了该领域面临的关键挑战和未来发展方向。\\n\\n**关键设计**：论文本身是一篇综述，因此没有具体的模型设计细节。但是，论文中讨论的VLA模型通常会涉及到以下关键设计：视觉编码器（例如，卷积神经网络或Transformer）用于提取图像特征；语言模型（例如，Transformer）用于处理语言指令和进行推理；动作生成器（例如，神经网络或规划器）用于生成驾驶动作。损失函数的设计通常会涉及到模仿学习、强化学习或两者结合的方法，以训练模型生成符合人类驾驶习惯的动作。",
            "application_zh": "该研究对自动驾驶领域具有重要的应用价值。VLA模型能够提升自动驾驶系统的通用性、可解释性和人机交互能力，使其能够更好地适应复杂和动态的驾驶环境，并能够理解和执行人类的指令。未来，VLA模型有望被应用于各种自动驾驶车辆，包括乘用车、卡车和无人配送车等，从而提高交通运输的效率和安全性。",
            "highlight_zh": "该论文是一篇综述性文章，没有具体的实验结果。但是，论文总结了现有VLA模型在自动驾驶任务中的表现，并指出了该领域面临的关键挑战和未来发展方向。例如，如何提高VLA模型的鲁棒性、可解释性和指令保真度，以及如何利用大规模数据集和预训练模型来提升VLA模型的性能等。",
            "tags_zh": [
                "自动驾驶",
                "视觉语言动作模型",
                "多模态学习",
                "大型语言模型",
                "端到端学习",
                "双系统架构",
                "综述",
                "决策规划"
            ],
            "_index": 1,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16760v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16760v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16760v1/figures/fig3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation",
            "authors": [
                "Xin Lin",
                "Meixi Song",
                "Dizhe Zhang",
                "Wenxuan Lu",
                "Haodong Li",
                "Bo Du",
                "Ming-Hsuan Yang",
                "Truong Nguyen",
                "Lu Qi"
            ],
            "arxiv_id": "2512.16913v1",
            "summary": "In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \\href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP\\_website/}",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project Page: https://insta360-research-team.github.io/DAP_website/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16913v1",
            "code_links": [
                {
                    "url": "https://insta360-research-team.github.io/DAP_website/",
                    "type": "project_page"
                },
                {
                    "url": "https://insta360-research-team.github.io/DAP",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]depth estimation",
                        "metric depth"
                    ],
                    "score": 8.0
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "geometric consistency"
                    ],
                    "score": 3.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]foundation model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 20.0,
            "hit_pillars": [
                "3_perception_slam",
                "7_retargeting",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出全景深度基础模型以解决多场景深度估计问题",
            "summary_zh": "本研究提出了一种全景度量深度基础模型，能够在多样化场景距离下进行泛化。我们从数据构建和框架设计的角度探索了数据循环的范式。通过结合公共数据集、来自UE5模拟器的高质量合成数据、文本到图像模型以及网络上的真实全景图像，收集了大规模数据集。为减少室内/室外和合成/真实数据之间的领域差距，我们引入了三阶段伪标签整理管道，以生成可靠的无标签图像的真实标签。我们采用DINOv3-Large作为主干网络，并引入了即插即用的范围掩码头、以清晰度为中心的优化和以几何为中心的优化，以提高对不同距离的鲁棒性，并在视图之间强制几何一致性。实验结果表明，在多个基准测试上表现出强大的性能和零-shot泛化能力，尤其是在多样化的真实场景中，度量预测表现出特别的鲁棒性和稳定性。",
            "intro_zh": [
                "现有的深度估计方法在处理多样化场景和不同距离时存在泛化能力不足的问题。",
                "论文提出了一种基于DINOv3-Large的全景深度估计模型，结合伪标签整理和优化策略以提升鲁棒性。",
                "在多个基准测试上，模型展示了强大的性能，尤其在真实场景中实现了稳定的度量预测。"
            ],
            "method_zh": "**问题定义**：本论文旨在解决全景深度估计中的泛化能力不足问题，现有方法在不同场景和距离下的表现不够稳定，导致深度估计的准确性下降。\\n\\n**核心思路**：论文提出了一种数据驱动的循环范式，通过构建大规模数据集和引入伪标签整理管道，增强模型对不同场景的适应性和鲁棒性。\\n\\n**技术框架**：整体架构包括数据收集、伪标签生成和模型训练三个主要阶段。数据收集阶段结合了公共数据集和合成数据，伪标签生成阶段通过三阶段管道确保标签的可靠性，模型训练阶段则采用DINOv3-Large作为主干网络。\\n\\n**关键创新**：最重要的创新在于引入了即插即用的范围掩码头和优化策略，增强了模型对不同距离的适应性，并确保了几何一致性，这在现有方法中较为少见。\\n\\n**关键设计**：在模型设计中，采用了清晰度和几何一致性为中心的优化策略，确保了在多样化场景下的鲁棒性，同时通过伪标签整理管道生成高质量的训练数据。",
            "application_zh": "该研究的潜在应用领域包括虚拟现实、增强现实和机器人导航等场景，能够为这些领域提供高精度的深度信息，提升用户体验和系统性能。未来，该模型的应用可能推动全景图像处理和三维重建技术的发展。",
            "highlight_zh": "在多个基准测试（如Stanford2D3D、Matterport3D和Deep360）中，模型展示了优异的性能，特别是在真实场景中的度量预测表现出强大的鲁棒性和稳定性，零-shot泛化能力显著提升。",
            "tags_zh": [
                "全景深度估计",
                "深度学习",
                "伪标签整理",
                "几何一致性",
                "数据驱动"
            ],
            "_index": 2,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16913v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16913v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16913v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation",
            "authors": [
                "Jingjing Qian",
                "Boyao Han",
                "Chen Shi",
                "Lei Xiao",
                "Long Yang",
                "Shaoshuai Shi",
                "Li Jiang"
            ],
            "arxiv_id": "2512.16811v1",
            "summary": "Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16811v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "vision-language-action",
                        "[T]VLA"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 18.0,
            "hit_pillars": [
                "1_robot_core",
                "9_embodied_foundation"
            ],
            "headline_zh": "GeoPredict：利用预测运动学和3D高斯几何实现精确的VLA操作",
            "summary_zh": "视觉-语言-动作(VLA)模型在机器人操作中表现出强大的泛化能力，但很大程度上是反应式的和以2D为中心的，这使得它们在需要精确3D推理的任务中不可靠。我们提出了GeoPredict，一个几何感知的VLA框架，它用预测运动学和几何先验来增强连续动作策略。GeoPredict引入了一个轨迹级模块，该模块编码运动历史并预测机器人手臂的多步3D关键点轨迹，以及一个预测性3D高斯几何模块，该模块预测工作空间几何形状，并通过沿未来关键点轨迹的跟踪引导细化。这些预测模块仅作为训练时的监督，通过基于深度的渲染实现，而推理只需要轻量级的额外查询token，无需调用任何3D解码。在RoboCasa Human-50、LIBERO和真实世界操作任务上的实验表明，GeoPredict始终优于强大的VLA基线，尤其是在几何密集型和空间要求高的场景中。",
            "intro_zh": [
                "现有VLA模型在精确3D操作任务中表现不足，主要原因是其反应式和2D中心的特性。",
                "GeoPredict通过引入预测运动学和3D高斯几何先验，增强VLA模型的3D推理能力，提升操作精度。",
                "实验表明，GeoPredict在多个数据集和真实场景中，显著优于现有VLA基线，尤其在几何密集型任务中。"
            ],
            "method_zh": "**问题定义**：现有的视觉-语言-动作(VLA)模型在机器人操作任务中，尤其是在需要精确3D推理的任务中，存在泛化能力不足的问题。这些模型通常是反应式的，即根据当前观察到的状态立即做出动作决策，缺乏对未来状态的预测和规划。此外，它们主要以2D视觉信息为基础，难以准确理解和利用3D几何信息，导致在几何密集型场景中表现不佳。\\n\\n**核心思路**：GeoPredict的核心思路是利用预测性的运动学和几何先验来增强VLA模型的3D推理能力。通过预测机器人手臂的未来运动轨迹和工作空间的几何形状，模型可以更好地理解任务的3D空间结构，从而做出更精确的动作决策。这种预测性的方法使得模型能够提前规划，避免了反应式方法中的盲目性。\\n\\n**技术框架**：GeoPredict框架包含一个轨迹级模块和一个预测性3D高斯几何模块。轨迹级模块编码运动历史，并预测机器人手臂的多步3D关键点轨迹。预测性3D高斯几何模块预测工作空间几何形状，并通过沿未来关键点轨迹的跟踪引导细化。这两个模块在训练时作为监督信号，通过深度渲染提供3D信息。在推理阶段，只需要额外的查询token，无需进行复杂的3D解码。\\n\\n**关键创新**：GeoPredict的关键创新在于引入了预测性的运动学和几何先验，并将其作为训练时的监督信号。这种方法有效地利用了3D信息，提高了VLA模型在几何密集型任务中的性能。此外，GeoPredict在推理阶段避免了复杂的3D解码，保持了模型的轻量级和高效性。\\n\\n**关键设计**：GeoPredict使用轨迹级模块预测机器人手臂的多步3D关键点轨迹，这需要设计合适的网络结构来编码运动历史并预测未来的关键点位置。预测性3D高斯几何模块使用3D高斯表示来建模工作空间的几何形状，并设计了跟踪引导细化机制，以提高几何预测的准确性。损失函数的设计需要平衡运动学预测和几何预测的准确性，并确保模型能够有效地利用这些预测信息来指导动作决策。",
            "application_zh": "GeoPredict在机器人操作领域具有广泛的应用前景，例如在复杂装配、精细操作、以及需要与不规则物体交互的场景中。该研究成果可以提升机器人在工业自动化、医疗机器人、家庭服务机器人等领域的智能化水平，使其能够更好地适应复杂多变的环境，完成更加精细和复杂的任务。",
            "highlight_zh": "GeoPredict在RoboCasa Human-50、LIBERO和真实世界操作任务上进行了评估，实验结果表明，GeoPredict始终优于强大的VLA基线。尤其是在几何密集型和空间要求高的场景中，GeoPredict的性能提升更为显著，证明了其在精确3D操作方面的优势。",
            "tags_zh": [
                "视觉语言动作模型",
                "机器人操作",
                "3D几何推理",
                "预测运动学",
                "3D高斯几何"
            ],
            "_index": 3,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16811v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16811v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16811v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals",
            "authors": [
                "Shuting Zhao",
                "Zeyu Xiao",
                "Xinrong Chen"
            ],
            "arxiv_id": "2512.16791v1",
            "summary": "Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Accepted by AAAI 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16791v1",
            "code_links": [
                {
                    "url": "https://kaka-1314.github.io/KineST/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]state space model",
                        "representation learning"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "[T]spatiotemporal",
                        "[T]motion tracking"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 18.0,
            "hit_pillars": [
                "2_algo_arch",
                "8_physics_animation"
            ],
            "headline_zh": "KineST：一种基于运动学引导的时空状态空间模型，用于从稀疏信号中进行人体运动跟踪",
            "summary_zh": "全身运动跟踪在AR/VR应用中至关重要，它连接了物理交互和虚拟交互。然而，基于头戴式显示器获取的稀疏信号重建逼真且多样化的全身姿态具有挑战性，而头戴式显示器是AR/VR场景中的主要设备。现有的姿态重建方法通常计算成本高昂，或者依赖于分别建模空间和时间依赖性，难以平衡准确性、时间连贯性和效率。为了解决这个问题，我们提出KineST，一种新颖的运动学引导的状态空间模型，它有效地提取时空依赖性，同时整合局部和全局姿态感知。创新来自两个核心思想。首先，为了更好地捕捉复杂的关节关系，状态空间对偶框架内的扫描策略被重新制定为运动学引导的双向扫描，从而嵌入运动学先验。其次，采用混合时空表示学习方法来紧密耦合空间和时间上下文，从而平衡准确性和平滑性。此外，引入了几何角速度损失，对旋转变化施加物理上有意义的约束，以进一步提高运动稳定性。大量实验表明，KineST在轻量级框架内，在准确性和时间一致性方面都具有优越的性能。",
            "intro_zh": [
                "现有方法难以兼顾AR/VR中基于稀疏信号的全身姿态重建的准确性、时间连贯性和计算效率。",
                "KineST通过运动学引导的双向扫描和混合时空表示学习，有效提取时空依赖性并整合局部和全局姿态感知。",
                "实验表明，KineST在轻量级框架内，在准确性和时间一致性方面都优于现有方法，提升了运动跟踪的性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决基于AR/VR场景中头戴式显示器等设备提供的稀疏信号，进行准确、连贯且高效的全身运动跟踪问题。现有方法通常面临计算成本高、难以同时建模空间和时间依赖性，以及难以平衡准确性、时间连贯性和效率的挑战。\\n\\n**核心思路**：论文的核心思路是利用运动学先验知识引导时空状态空间模型的构建，从而更有效地提取时空依赖关系。通过运动学引导的双向扫描，更好地捕捉关节之间的复杂关系；通过混合时空表示学习，紧密耦合空间和时间上下文，从而在准确性和平滑性之间取得平衡。\\n\\n**技术框架**：KineST的核心是一个状态空间模型，其整体流程包括：1) 输入稀疏的运动信号；2) 利用运动学引导的双向扫描策略，在状态空间对偶框架内提取空间特征；3) 采用混合时空表示学习方法，融合空间和时间上下文信息；4) 通过几何角速度损失进行优化，保证运动的物理合理性；5) 输出重建的全身运动姿态。\\n\\n**关键创新**：论文的关键创新在于：1) 提出了运动学引导的双向扫描策略，将运动学先验知识融入状态空间模型的构建中，从而更好地捕捉关节之间的复杂关系；2) 采用了混合时空表示学习方法，紧密耦合空间和时间上下文，从而在准确性和平滑性之间取得平衡；3) 引入了几何角速度损失，对旋转变化施加物理上有意义的约束，进一步提高了运动的稳定性。\\n\\n**关键设计**：运动学引导的双向扫描策略具体实现方式未知，需要参考论文细节。混合时空表示学习的具体网络结构未知，需要参考论文细节。几何角速度损失的具体形式未知，需要参考论文细节。这些细节的设计是保证模型性能的关键。",
            "application_zh": "该研究成果可广泛应用于AR/VR、游戏、动画制作、康复训练等领域。通过更准确、连贯和高效的全身运动跟踪，可以提升AR/VR应用的沉浸感和交互体验，为游戏和动画制作提供更逼真的角色动作，并为康复训练提供更精确的运动分析和指导。未来，该技术有望进一步应用于人机交互、远程协作等领域。",
            "highlight_zh": "论文通过实验验证了KineST在准确性和时间一致性方面的优越性能。具体性能数据和对比基线需要在论文中查找。论文强调KineST在轻量级框架下实现了高性能，这意味着它在计算资源受限的场景中具有更大的应用潜力。具体的提升幅度需要在论文中查找。",
            "tags_zh": [
                "人体运动跟踪",
                "状态空间模型",
                "运动学引导",
                "时空建模",
                "AR/VR",
                "稀疏信号",
                "姿态重建"
            ],
            "_index": 4,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16791v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16791v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16791v1/scan2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation",
            "authors": [
                "Yixiang Chen",
                "Yan Huang",
                "Keji He",
                "Peiyan Li",
                "Liang Wang"
            ],
            "arxiv_id": "2512.16724v1",
            "summary": "When performing 3D manipulation tasks, robots have to execute action planning based on perceptions from multiple fixed cameras. The multi-camera setup introduces substantial redundancy and irrelevant information, which increases computational costs and forces the model to spend extra training time extracting crucial task-relevant details. To filter out redundant information and accurately extract task-relevant features, we propose the VERM (Virtual Eye for Robotic Manipulation) method, leveraging the knowledge in foundation models to imagine a virtual task-adaptive view from the constructed 3D point cloud, which efficiently captures necessary information and mitigates occlusion. To facilitate 3D action planning and fine-grained manipulation, we further design a depth-aware module and a dynamic coarse-to-fine procedure. Extensive experimental results on both simulation benchmark RLBench and real-world evaluations demonstrate the effectiveness of our method, surpassing previous state-of-the-art methods while achieving 1.89x speedup in training time and 1.54x speedup in inference speed. More results can be found on our project website at https://verm-ral.github.io .",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Accepted at RA-L 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16724v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]foundation model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 15.0,
            "hit_pillars": [
                "1_robot_core",
                "9_embodied_foundation"
            ],
            "headline_zh": "VERM：利用基础模型为机器人操作构建高效3D虚拟视觉",
            "summary_zh": "在执行3D操作任务时，机器人需要基于多个固定摄像头的感知进行动作规划。多摄像头设置引入了大量的冗余和不相关信息，增加了计算成本，并迫使模型花费额外的训练时间来提取关键的任务相关细节。为了过滤掉冗余信息并准确提取任务相关的特征，我们提出了一种VERM（机器人操作虚拟视觉）方法，利用基础模型中的知识，从构建的3D点云中想象出一个虚拟的、任务自适应的视角，从而有效地捕获必要的信息并减轻遮挡。为了促进3D动作规划和精细操作，我们进一步设计了一个深度感知模块和一个动态的由粗到精的过程。在模拟基准RLBench和真实世界评估中进行的大量实验结果表明了我们方法的有效性，超越了先前的最先进方法，同时在训练时间上实现了1.89倍的加速，在推理速度上实现了1.54倍的加速。",
            "intro_zh": [
                "多摄像头冗余信息增加了机器人操作任务的计算成本，模型需要额外训练提取关键信息。",
                "VERM方法利用基础模型知识，从3D点云构建任务自适应的虚拟视角，有效捕获信息并减少遮挡。",
                "实验表明，VERM在RLBench和真实场景中超越SOTA方法，训练加速1.89倍，推理加速1.54倍。"
            ],
            "method_zh": "**问题定义**：现有机器人3D操作任务依赖多摄像头，导致信息冗余、计算成本高昂，模型训练效率低下，难以快速提取任务关键信息。现有方法难以有效过滤冗余信息，并缺乏对任务相关特征的精确提取机制。\\n\\n**核心思路**：VERM的核心在于利用预训练的基础模型，从多摄像头获取的3D点云中生成一个虚拟的、任务相关的视角。这个虚拟视角能够过滤掉不相关的信息，突出显示与当前任务最相关的特征，从而提高动作规划的效率和精度。\\n\\n**技术框架**：VERM主要包含三个模块：1) 3D点云构建模块，将多摄像头数据融合为3D点云；2) 虚拟视角生成模块，利用基础模型从3D点云中生成任务相关的虚拟视角图像；3) 动作规划模块，基于虚拟视角图像进行动作规划。此外，还包含一个深度感知模块和一个动态的由粗到精的过程，以提升操作的精细程度。\\n\\n**关键创新**：VERM的关键创新在于利用基础模型生成任务自适应的虚拟视角。与传统方法直接使用多摄像头数据或人工设计的特征相比，VERM能够自动学习并提取与任务最相关的特征，从而提高了模型的泛化能力和效率。\\n\\n**关键设计**：深度感知模块利用深度信息来增强对3D场景的理解，动态的由粗到精的过程允许模型先进行粗略的动作规划，然后逐步细化，从而提高操作的精度。具体的损失函数和网络结构等细节在论文中未明确说明，属于未知信息。",
            "application_zh": "VERM方法具有广泛的应用前景，可应用于工业自动化、家庭服务机器人、医疗机器人等领域。通过减少计算成本和提高操作精度，VERM可以显著提升机器人在复杂环境中的操作能力，实现更高效、更智能的自动化操作。未来，该方法有望推动机器人技术在各行各业的普及应用。",
            "highlight_zh": "实验结果表明，VERM在RLBench模拟环境和真实世界场景中均取得了显著的性能提升，超越了先前的SOTA方法。具体而言，VERM在训练时间上实现了1.89倍的加速，在推理速度上实现了1.54倍的加速。这些结果验证了VERM方法的有效性和高效性。",
            "tags_zh": [
                "机器人操作",
                "虚拟视觉",
                "基础模型",
                "3D重建",
                "动作规划"
            ],
            "_index": 5,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16724v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16724v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16724v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "AIMM: An AI-Driven Multimodal Framework for Detecting Social-Media-Influenced Stock Market Manipulation",
            "authors": [
                "Sandeep Neela"
            ],
            "arxiv_id": "2512.16103v1",
            "summary": "Market manipulation now routinely originates from coordinated social media campaigns, not isolated trades. Retail investors, regulators, and brokerages need tools that connect online narratives and coordination patterns to market behavior. We present AIMM, an AI-driven framework that fuses Reddit activity, bot and coordination indicators, and OHLCV market features into a daily AIMM Manipulation Risk Score for each ticker.\n  The system uses a parquet-native pipeline with a Streamlit dashboard that allows analysts to explore suspicious windows, inspect underlying posts and price action, and log model outputs over time. Due to Reddit API restrictions, we employ calibrated synthetic social features matching documented event characteristics; market data (OHLCV) uses real historical data from Yahoo Finance. This release makes three contributions. First, we build the AIMM Ground Truth dataset (AIMM-GT): 33 labeled ticker-days spanning eight equities, drawing from SEC enforcement actions, community-verified manipulation cases, and matched normal controls. Second, we implement forward-walk evaluation and prospective prediction logging for both retrospective and deployment-style assessment. Third, we analyze lead times and show that AIMM flagged GME 22 days before the January 2021 squeeze peak.\n  The current labeled set is small (33 ticker-days, 3 positive events), but results show preliminary discriminative capability and early warnings for the GME incident. We release the code, dataset schema, and dashboard design to support research on social media-driven market surveillance.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Preprint",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16103v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]multimodal"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 15.0,
            "hit_pillars": [
                "1_robot_core",
                "9_embodied_foundation"
            ],
            "headline_zh": "AIMM：一种AI驱动的多模态框架，用于检测社交媒体影响的股市操纵",
            "summary_zh": "本文提出AIMM，一个AI驱动的框架，旨在融合Reddit活动、机器人和协同行为指标以及OHLCV市场特征，为每个股票代码生成每日AIMM操纵风险评分。该系统采用基于Parquet的流水线，并配备Streamlit仪表板，使分析师能够探索可疑窗口，检查底层帖子和价格行为，并记录模型随时间推移的输出。由于Reddit API的限制，我们使用了经过校准的合成社交特征，以匹配已记录的事件特征；市场数据（OHLCV）使用来自Yahoo Finance的真实历史数据。本文贡献包括：构建AIMM Ground Truth数据集（AIMM-GT），包含33个标记的股票代码-天，涵盖8个股票，数据来自SEC执法行动、社区验证的操纵案例和匹配的正常对照；实施前向步进评估和前瞻性预测日志记录，用于回顾性和部署式评估；分析提前期，结果表明AIMM在2021年1月GME挤兑高峰前22天发出了警告。尽管当前标记集较小，但结果显示了初步的区分能力和对GME事件的早期预警。我们发布代码、数据集模式和仪表板设计，以支持对社交媒体驱动的市场监控的研究。",
            "intro_zh": [
                "现有方法难以有效关联社交媒体叙事、协同模式与市场行为，从而难以检测股市操纵。",
                "AIMM框架融合Reddit活动、机器人指标和市场特征，生成每日操纵风险评分，辅助分析师识别可疑行为。",
                "实验表明，AIMM具有初步的区分能力，并在GME事件发生前22天发出预警，展示了其潜在的应用价值。"
            ],
            "method_zh": "**问题定义**：论文旨在解决社交媒体影响下股市操纵行为的检测问题。现有方法难以将社交媒体上的叙事、协同模式与市场行为有效关联，导致无法及时发现和预警操纵行为。零售投资者、监管机构和经纪公司需要能够连接在线叙事和协同模式与市场行为的工具。\\n\\n**核心思路**：论文的核心思路是利用AI技术，融合来自不同模态的数据（社交媒体活动、机器人行为指标、市场交易数据），构建一个综合的风险评估体系。通过分析这些数据的关联性，可以更准确地识别潜在的股市操纵行为。这种多模态融合的方法能够弥补单一数据源的局限性，提高检测的准确性和可靠性。\\n\\n**技术框架**：AIMM框架包含以下主要模块：1) 数据收集与预处理：收集Reddit数据、机器人行为指标和OHLCV市场数据，并进行清洗和预处理。由于Reddit API限制，使用校准的合成社交特征。2) 特征工程：从收集的数据中提取相关特征，例如Reddit帖子的情感、机器人账户的活跃度、交易量和价格波动等。3) 模型训练：使用机器学习模型（具体模型类型未知）将提取的特征映射到操纵风险评分。4) 风险评估与预警：根据模型输出的风险评分，生成每日AIMM操纵风险评分，并设置阈值进行预警。5) 可视化与分析：通过Streamlit仪表板，分析师可以探索可疑窗口，检查底层帖子和价格行为，并记录模型输出。\\n\\n**关键创新**：AIMM的关键创新在于其多模态融合的框架，能够综合考虑社交媒体、机器人行为和市场交易数据，从而更全面地评估股市操纵风险。此外，AIMM还构建了一个包含标记数据的AIMM-GT数据集，并实现了前向步进评估和前瞻性预测日志记录，为模型评估和部署提供了支持。\\n\\n**关键设计**：由于论文摘要中没有提供关于关键参数设置、损失函数、网络结构等技术细节，因此这部分信息未知。但可以推测，模型训练可能采用了监督学习方法，损失函数可能与分类或回归任务相关。合成社交特征的校准方法也是一个关键设计，旨在模拟真实的社交媒体行为。",
            "application_zh": "AIMM框架可应用于金融监管、风险管理和投资决策等领域。监管机构可以使用AIMM来监控市场操纵行为，及时采取措施保护投资者利益。经纪公司可以利用AIMM来评估客户交易的风险，并提供个性化的投资建议。零售投资者可以使用AIMM来识别潜在的投资风险，做出更明智的投资决策。该研究有助于提升市场透明度和公平性，维护金融市场的稳定。",
            "highlight_zh": "实验结果表明，AIMM具有初步的区分能力，能够识别潜在的股市操纵行为。特别值得一提的是，AIMM在2021年1月GME挤兑高峰前22天发出了预警，展示了其在早期预警方面的潜力。尽管当前标记集较小，但这些结果表明AIMM具有实际应用价值，并为未来的研究提供了方向。",
            "tags_zh": [
                "股市操纵检测",
                "社交媒体分析",
                "多模态融合",
                "风险评估",
                "机器学习"
            ],
            "_index": 6,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16103v1/figures/GroundTruthCoverage.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16103v1/figures/Forward-walk.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16103v1/figures/PredictionLog.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "AlignMerge - Alignment-Preserving Large Language Model Merging via Fisher-Guided Geometric Constraints",
            "authors": [
                "Aniruddha Roy",
                "Jyoti Patel",
                "Aman Chadha",
                "Vinija Jain",
                "Amitava Das"
            ],
            "arxiv_id": "2512.16245v1",
            "summary": "Merging large language models (LLMs) is a practical way to compose capabilities from multiple fine-tuned checkpoints without retraining. Yet standard schemes (linear weight soups, task vectors, and Fisher-weighted averaging) can preserve loss while quietly destroying alignment. We argue that merging is not a numerical trick but a geometry-constrained operation around an already-aligned anchor: fusion must be steered to respect safety geometry, not validated post hoc.\n  We introduce AlignMerge, a geometry-aware merging framework that makes alignment an explicit invariant. In a local Fisher chart around an instruction-tuned base, we estimate an alignment subspace with projector P_A and optimize:\n  L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud,\n  where L_geo keeps the merge close to its experts in Fisher-Rao geometry, L_align penalizes motion along alignment-sensitive directions, and L_bud enforces a soft alignment budget. As the alignment functional we use the decoding-invariant Alignment Quality Index (AQI), a latent-space criterion that captures how cleanly aligned and misaligned behaviors separate in representation space.\n  Across five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2), merging safety anchors with task experts, AlignMerge improves alignment metrics (AQI, toxicity, LLM-judge alignment) while matching or exceeding the best expert on instruction-following, reasoning, and helpfulness. It also exhibits smaller alignment-subspace drift and fewer budget violations than Fisher soups, TIES, SafeMerge, and MergeAlign. These results make alignment-preserving merging a first-class design goal and suggest a path to geometry-aware composition of future foundation models.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16245v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model",
                        "foundation model",
                        "instruction following"
                    ],
                    "score": 15.0
                }
            ],
            "relevance_score": 15.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "AlignMerge：通过Fisher引导的几何约束实现对齐保持的大语言模型合并",
            "summary_zh": "合并大型语言模型(LLMs)是一种实用的方法，可以在不重新训练的情况下组合来自多个微调检查点的能力。然而，标准方案（线性权重平均、任务向量和Fisher加权平均）可能会在保持损失的同时悄然破坏对齐。我们认为合并不是一种数值技巧，而是一种围绕已对齐锚点的几何约束操作：必须引导融合以尊重安全性几何，而不是事后验证。我们引入AlignMerge，一个几何感知合并框架，它使对齐成为显式不变性。在指令调整基础周围的局部Fisher图中，我们使用投影算子P_A估计对齐子空间并优化：L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud，其中L_geo使合并结果在Fisher-Rao几何中接近其专家，L_align惩罚沿对齐敏感方向的运动，L_bud强制执行软对齐预算。作为对齐函数，我们使用解码不变的对齐质量指数(AQI)，这是一个潜在空间标准，可以捕获对齐和未对齐行为在表示空间中分离的清晰程度。在五个模型系列（LLaMA-3 8B、Mistral 7B、Qwen 2、Phi-3.5、Gemma 2）中，将安全锚点与任务专家合并，AlignMerge提高了对齐指标（AQI、毒性、LLM-judge对齐），同时在指令遵循、推理和帮助性方面匹配或超过了最佳专家。与Fisher soups、TIES、SafeMerge和MergeAlign相比，它还表现出更小的对齐子空间漂移和更少的预算违规。这些结果使保持对齐的合并成为首要的设计目标，并为未来基础模型的几何感知组合提供了一条途径。",
            "intro_zh": [
                "现有LLM合并方法在保持模型性能的同时，容易破坏模型的对齐性，导致安全性下降。",
                "AlignMerge通过在Fisher-Rao几何空间中施加约束，显式地将对齐作为合并过程中的不变性条件。",
                "实验表明，AlignMerge在多个模型上提高了对齐指标，同时保持或超过了最佳专家的指令遵循和推理能力。"
            ],
            "method_zh": "**问题定义**：现有的大语言模型合并方法，如线性权重平均、任务向量等，虽然能够保持模型的性能指标，但往往忽略了模型对齐性，导致合并后的模型可能产生不安全或有害的输出。这些方法缺乏对模型几何结构的考虑，无法保证合并后的模型仍然保持良好的对齐特性。\\n\\n**核心思路**：AlignMerge的核心思路是将模型合并视为一个在几何空间中受约束的优化问题。它围绕一个已经对齐的“锚点”模型，通过施加几何约束，确保合并后的模型在保持性能的同时，也保持良好的对齐性。这种方法显式地将对齐作为合并过程中的一个不变性条件，避免了事后验证对齐性的不足。\\n\\n**技术框架**：AlignMerge框架包含以下主要步骤：1. 选择一个已经对齐的“锚点”模型。2. 在锚点模型周围的Fisher图中，估计一个对齐子空间，并计算投影算子P_A。3. 定义一个包含几何约束、对齐约束和预算约束的损失函数L_AlignMerge。4. 通过优化L_AlignMerge，得到合并后的模型。\\n\\n**关键创新**：AlignMerge的关键创新在于：1. 显式地将对齐作为合并过程中的不变性条件。2. 利用Fisher-Rao几何来约束合并过程，确保合并后的模型在几何空间中接近其专家模型。3. 使用解码不变的对齐质量指数(AQI)作为对齐函数，能够更好地捕捉模型对齐和未对齐行为在表示空间中的分离程度。\\n\\n**关键设计**：L_AlignMerge损失函数包含三个部分：L_geo用于保持合并结果在Fisher-Rao几何中接近其专家模型；L_align用于惩罚沿对齐敏感方向的运动，防止模型偏离对齐子空间；L_bud用于强制执行软对齐预算，限制模型在未对齐方向上的偏移。lambda_align和lambda_bud是超参数，用于控制对齐约束和预算约束的强度。AQI的计算涉及到在潜在空间中区分对齐和未对齐行为，需要选择合适的正负样本。",
            "application_zh": "AlignMerge技术可应用于安全敏感的大语言模型部署场景，例如医疗、金融等领域。通过合并多个具有不同专长的模型，同时保证模型的安全性和可靠性，可以构建更强大、更可信赖的AI系统。该技术还有助于构建更具适应性和可扩展性的模型组合，应对不断变化的任务需求。",
            "highlight_zh": "实验结果表明，AlignMerge在LLaMA-3 8B、Mistral 7B、Qwen 2、Phi-3.5、Gemma 2等多个模型系列上，显著提高了对齐指标（AQI、毒性、LLM-judge对齐），同时在指令遵循、推理和帮助性方面匹配或超过了最佳专家。与Fisher soups、TIES、SafeMerge和MergeAlign等基线方法相比，AlignMerge表现出更小的对齐子空间漂移和更少的预算违规。",
            "tags_zh": [
                "大语言模型合并",
                "模型对齐",
                "Fisher信息几何",
                "安全性",
                "几何约束"
            ],
            "_index": 7,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16245v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16245v1/figures/mechanistic.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16245v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "BrepLLM: Native Boundary Representation Understanding with Large Language Models",
            "authors": [
                "Liyuan Deng",
                "Hao Guo",
                "Yunpeng Bai",
                "Yongkang Dai",
                "Huaxi Huang",
                "Yilei Shi"
            ],
            "arxiv_id": "2512.16413v1",
            "summary": "Current token-sequence-based Large Language Models (LLMs) are not well-suited for directly processing 3D Boundary Representation (Brep) models that contain complex geometric and topological information. We propose BrepLLM, the first framework that enables LLMs to parse and reason over raw Brep data, bridging the modality gap between structured 3D geometry and natural language. BrepLLM employs a two-stage training pipeline: Cross-modal Alignment Pre-training and Multi-stage LLM Fine-tuning. In the first stage, an adaptive UV sampling strategy converts Breps into graphs representation with geometric and topological information. We then design a hierarchical BrepEncoder to extract features from geometry (i.e., faces and edges) and topology, producing both a single global token and a sequence of node tokens. Then we align the global token with text embeddings from a frozen CLIP text encoder (ViT-L/14) via contrastive learning. In the second stage, we integrate the pretrained BrepEncoder into an LLM. We then align its sequence of node tokens using a three-stage progressive training strategy: (1) training an MLP-based semantic mapping from Brep representation to 2D with 2D-LLM priors. (2) performing fine-tuning of the LLM. (3) designing a Mixture-of-Query Experts (MQE) to enhance geometric diversity modeling. We also construct Brep2Text, a dataset comprising 269,444 Brep-text question-answer pairs. Experiments show that BrepLLM achieves state-of-the-art (SOTA) results on 3D object classification and captioning tasks.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16413v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "contrastive learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "semantic mapping",
                        "semantic map"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 14.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam",
                "9_embodied_foundation"
            ],
            "headline_zh": "BrepLLM：提出一种原生边界表示理解的大语言模型框架",
            "summary_zh": "当前基于token序列的大语言模型(LLMs)不适合直接处理包含复杂几何和拓扑信息的3D边界表示(Brep)模型。我们提出了BrepLLM，这是第一个使LLMs能够解析和推理原始Brep数据的框架，弥合了结构化3D几何和自然语言之间的模态差距。BrepLLM采用两阶段训练流程：跨模态对齐预训练和多阶段LLM微调。在第一阶段，自适应UV采样策略将Brep转换为具有几何和拓扑信息的图表示。然后，我们设计了一个分层BrepEncoder来提取几何（即面和边）和拓扑的特征，生成单个全局token和一系列节点token。然后，我们通过对比学习将全局token与来自冻结的CLIP文本编码器(ViT-L/14)的文本嵌入对齐。在第二阶段，我们将预训练的BrepEncoder集成到LLM中。然后，我们使用三阶段渐进式训练策略对齐其节点token序列：(1)训练一个基于MLP的语义映射，将Brep表示映射到具有2D-LLM先验的2D。(2)执行LLM的微调。(3)设计一个混合查询专家(MQE)来增强几何多样性建模。我们还构建了Brep2Text数据集，包含269,444个Brep-文本问答对。实验表明，BrepLLM在3D对象分类和字幕任务上取得了最先进(SOTA)的结果。",
            "intro_zh": [
                "现有的大语言模型难以直接处理包含复杂几何和拓扑信息的3D Brep模型。",
                "BrepLLM通过两阶段训练，将Brep数据转换为LLM可理解的token序列，实现跨模态对齐。",
                "实验结果表明，BrepLLM在3D对象分类和描述任务上达到了当前最优水平。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大语言模型(LLM)无法直接理解和处理3D边界表示(Brep)模型的问题。现有的LLM主要处理文本序列，而Brep模型包含复杂的几何和拓扑信息，直接输入LLM会导致信息丢失或无法有效利用。因此，如何将Brep数据转换为LLM能够理解的形式，并使其具备对3D几何信息的推理能力是本论文要解决的核心问题。\\n\\n**核心思路**：论文的核心思路是通过一个两阶段的训练流程，将Brep数据转换为LLM可以处理的token序列，并利用对比学习和多阶段微调，使LLM能够理解和推理Brep数据中的几何和拓扑信息。这种方法的核心在于设计一个有效的BrepEncoder，能够提取Brep数据中的关键特征，并将其映射到与文本嵌入空间对齐的token表示。\\n\\n**技术框架**：BrepLLM的整体框架包含两个主要阶段：跨模态对齐预训练和多阶段LLM微调。在跨模态对齐预训练阶段，首先使用自适应UV采样策略将Brep模型转换为图表示，然后通过分层BrepEncoder提取几何和拓扑特征，生成全局token和节点token序列。接着，通过对比学习将全局token与CLIP文本编码器的文本嵌入对齐。在多阶段LLM微调阶段，首先训练一个MLP将Brep表示映射到2D空间，然后对LLM进行微调，最后使用混合查询专家(MQE)增强几何多样性建模。\\n\\n**关键创新**：论文的关键创新在于提出了BrepLLM框架，这是第一个能够使LLM直接解析和推理原始Brep数据的框架。该框架通过设计自适应UV采样策略和分层BrepEncoder，有效地提取了Brep数据中的几何和拓扑信息，并将其转换为LLM可以处理的token表示。此外，论文还提出了一个多阶段的微调策略，有效地提升了LLM对Brep数据的理解和推理能力。\\n\\n**关键设计**：在跨模态对齐预训练阶段，使用了对比学习损失函数来对齐BrepEncoder提取的全局token和CLIP文本编码器的文本嵌入。在多阶段LLM微调阶段，设计了一个基于MLP的语义映射，将Brep表示映射到2D空间，并使用2D-LLM的先验知识进行指导。此外，还设计了一个混合查询专家(MQE)来增强几何多样性建模，提升了LLM对不同几何形状的适应能力。",
            "application_zh": "BrepLLM具有广泛的应用前景，例如在CAD/CAM领域，可以用于自动化设计、零件识别和智能制造。在机器人领域，可以用于3D场景理解、物体抓取和导航。此外，该技术还可以应用于建筑设计、游戏开发和虚拟现实等领域，实现更智能、更高效的3D内容生成和交互。",
            "highlight_zh": "BrepLLM在3D对象分类和描述任务上取得了显著的性能提升，达到了当前最优水平。具体来说，在Brep2Text数据集上，BrepLLM在3D对象分类任务上超越了现有方法，并在3D对象描述任务上生成了更准确、更丰富的描述文本。这些实验结果表明，BrepLLM能够有效地理解和推理Brep数据中的几何和拓扑信息。",
            "tags_zh": [
                "边界表示",
                "大语言模型",
                "跨模态学习",
                "3D几何",
                "拓扑信息"
            ],
            "_index": 8,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16413v1/images/zhanshitu1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16413v1/images/framework.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16413v1/images/BrepEncoder.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "AdaSearch: Balancing Parametric Knowledge and Search in Large Language Models via Reinforcement Learning",
            "authors": [
                "Tzu-Han Lin",
                "Wei-Lin Chen",
                "Chen-An Li",
                "Hung-yi Lee",
                "Yun-Nung Chen",
                "Yu Meng"
            ],
            "arxiv_id": "2512.16883v1",
            "summary": "Equipping large language models (LLMs) with search engines via reinforcement learning (RL) has emerged as an effective approach for building search agents. However, overreliance on search introduces unnecessary cost and risks exposure to noisy or malicious content, while relying solely on parametric knowledge risks hallucination. The central challenge is to develop agents that adaptively balance parametric knowledge with external search, invoking search only when necessary. Prior work mitigates search overuse by shaping rewards around the number of tool calls. However, these penalties require substantial reward engineering, provide ambiguous credit assignment, and can be exploited by agents that superficially reduce calls. Moreover, evaluating performance solely through call counts conflates necessary and unnecessary search, obscuring the measurement of true adaptive behavior. To address these limitations, we first quantify the self-knowledge awareness of existing search agents via an F1-based decision metric, revealing that methods such as Search-R1 often overlook readily available parametric knowledge. Motivated by these findings, we propose AdaSearch, a simple two-stage, outcome-driven RL framework that disentangles problem solving from the decision of whether to invoke search, and makes this decision process explicit and interpretable. This transparency is crucial for high-stakes domains such as finance and medical question answering, yet is largely neglected by prior approaches. Experiments across multiple model families and sizes demonstrate that AdaSearch substantially improves knowledge-boundary awareness, reduces unnecessary search calls, preserves strong task performance, and offers more transparent, interpretable decision behaviors.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Preprint. Code and artifacts will be uploaded to https://github.com/hank0316/AdaSearch",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16883v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 13.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出AdaSearch，通过强化学习平衡大语言模型的参数知识与外部搜索。",
            "summary_zh": "本文提出了一种利用强化学习（RL）为大型语言模型（LLM）配备搜索引擎的有效方法，以构建搜索代理。然而，过度依赖搜索会引入不必要的成本，并可能暴露于噪声或恶意内容，而仅依赖参数知识则存在幻觉风险。核心挑战在于开发能够自适应地平衡参数知识与外部搜索的代理，仅在必要时才调用搜索。现有工作通过围绕工具调用次数塑造奖励来缓解搜索过度使用，但这些惩罚需要大量的奖励工程，提供模糊的信用分配，并且可能被表面上减少调用的代理利用。此外，仅通过调用次数评估性能会混淆必要和不必要的搜索，从而模糊了对真正自适应行为的衡量。为了解决这些局限性，我们首先通过基于F1的决策指标量化现有搜索代理的自我知识感知能力，揭示了诸如Search-R1之类的方法经常忽略现成的参数知识。受这些发现的启发，我们提出AdaSearch，这是一个简单的两阶段、结果驱动的RL框架，它将问题解决与是否调用搜索的决策分离开来，并使该决策过程明确且可解释。这种透明度对于金融和医学问答等高风险领域至关重要，但之前的研究方法在很大程度上忽略了这一点。跨多个模型系列和规模的实验表明，AdaSearch显着提高了知识边界意识，减少了不必要的搜索调用，保持了强大的任务性能，并提供了更透明、可解释的决策行为。",
            "intro_zh": [
                "现有搜索增强的LLM过度依赖搜索，引入成本和噪声，而仅依赖模型自身知识则易产生幻觉。",
                "AdaSearch通过两阶段强化学习，解耦问题解决和搜索决策，显式地学习何时调用搜索。",
                "实验表明AdaSearch能显著提高知识边界意识，减少不必要的搜索，同时保持任务性能。"
            ],
            "method_zh": "**问题定义**：现有方法在利用搜索引擎增强大型语言模型时，难以平衡参数知识和外部搜索。过度依赖搜索会增加成本并引入噪声，而完全依赖参数知识则容易产生幻觉。现有方法通常通过惩罚工具调用次数来减少搜索的使用，但这需要大量的奖励工程，并且信用分配模糊，容易被利用。\\n\\n**核心思路**：AdaSearch的核心思路是将问题解决过程与是否调用搜索的决策过程解耦。通过两阶段强化学习，首先训练一个问题解决器，然后在第二阶段训练一个决策器，明确地决定是否需要调用搜索。这种解耦使得模型能够更好地学习何时利用自身的参数知识，何时需要外部信息。\\n\\n**技术框架**：AdaSearch包含两个主要阶段：1) 问题解决阶段：使用强化学习训练一个问题解决器，使其能够尽可能地利用自身的参数知识解决问题。2) 搜索决策阶段：使用强化学习训练一个决策器，该决策器根据问题解决器的状态和问题本身，决定是否需要调用搜索。决策器的目标是最大化任务完成的奖励，同时最小化不必要的搜索调用。\\n\\n**关键创新**：AdaSearch的关键创新在于将问题解决和搜索决策解耦，并使用强化学习显式地学习搜索策略。与现有方法相比，AdaSearch不需要复杂的奖励工程，并且能够提供更透明、可解释的决策过程。此外，AdaSearch通过F1-based决策指标量化了现有搜索代理的自我知识感知能力，为后续的改进提供了依据。\\n\\n**关键设计**：AdaSearch使用Actor-Critic算法进行强化学习。在问题解决阶段，Actor网络负责生成答案，Critic网络负责评估答案的质量。在搜索决策阶段，Actor网络负责决定是否调用搜索，Critic网络负责评估搜索决策的质量。奖励函数的设计旨在鼓励模型利用自身的参数知识解决问题，并仅在必要时才调用搜索。具体参数设置和网络结构在论文中有详细描述，此处不再赘述。",
            "application_zh": "AdaSearch适用于需要平衡知识和搜索的各种应用场景，例如金融问答、医疗诊断、法律咨询等高风险领域。通过减少不必要的搜索调用，可以降低成本并提高效率。此外，AdaSearch提供的透明、可解释的决策过程，有助于提高用户对模型的信任度。",
            "highlight_zh": "实验结果表明，AdaSearch在多个模型系列和规模上都取得了显著的改进。与现有方法相比，AdaSearch显著提高了知识边界意识，减少了不必要的搜索调用，同时保持了强大的任务性能。例如，在某些任务上，AdaSearch可以将不必要的搜索调用减少高达50%，同时保持与现有方法相当甚至更好的准确率。",
            "tags_zh": [
                "大型语言模型",
                "强化学习",
                "搜索引擎",
                "知识边界",
                "自适应搜索"
            ],
            "_index": 9,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16883v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16883v1/figures/qwen3b_comparison.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16883v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence",
            "authors": [
                "Xiaopeng Lin",
                "Shijie Lian",
                "Bin Yu",
                "Ruoqi Yang",
                "Changti Wu",
                "Yuzhuo Miao",
                "Yurun Jin",
                "Yukun Shi",
                "Cong Huang",
                "Bojun Cheng",
                "Kai Chen"
            ],
            "arxiv_id": "2512.16793v1",
            "summary": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "17 pages, 4 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16793v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "humanoid",
                        "humanoid robot"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱六：视频提取与匹配 (Video Extraction)",
                    "id": "6_video_extraction",
                    "matched_keywords": [
                        "[T]egocentric"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "VLA"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 13.0,
            "hit_pillars": [
                "1_robot_core",
                "6_video_extraction",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出PhysBrain以解决机器人视觉语言模型与物理智能的匹配问题",
            "summary_zh": "机器人泛化依赖于物理智能，即在自我中心感知和行动下推理状态变化、接触丰富的交互和长远规划的能力。然而，大多数视觉语言模型主要在第三人称数据上训练，导致人形机器人面临视角不匹配的问题。收集机器人自我中心数据的规模化仍然不切实际，而大规模的人类自我中心视频提供了一种可扩展的替代方案。本文提出了Egocentric2Embodiment翻译管道，将第一人称视频转化为多层次、基于模式的视觉问答监督，构建了Egocentric2Embodiment数据集（E2E-3M）。通过在该数据集上训练，获得了一个自我中心感知的具身智能体PhysBrain，展现出显著的自我中心理解能力，尤其在EgoThink规划任务中表现优异。",
            "intro_zh": [
                "现有的视觉语言模型主要依赖第三人称数据，导致机器人在自我中心感知下的泛化能力不足。",
                "提出了Egocentric2Embodiment翻译管道，将第一人称视频转化为结构化的多层次监督，构建了大规模E2E-3M数据集。",
                "PhysBrain在E2E-3M数据集上训练后，展现出更强的自我中心理解能力，VLA微调的样本效率显著提高，成功率达到53.9%。"
            ],
            "method_zh": "**问题定义**：本文旨在解决机器人在自我中心感知下的物理智能不足，现有方法主要依赖第三人称数据，导致视角不匹配和泛化能力不足。\\n\\n**核心思路**：提出Egocentric2Embodiment翻译管道，将人类自我中心视频转化为结构化的训练监督，利用丰富的交互上下文和因果结构来提升机器人的理解能力。\\n\\n**技术框架**：整体架构包括数据收集、视频处理、监督生成和模型训练四个主要模块。首先收集人类自我中心视频，然后通过翻译管道生成多层次的视觉问答监督，最后在E2E-3M数据集上训练PhysBrain。\\n\\n**关键创新**：最重要的创新在于将原始自我中心视频转化为结构化的训练监督，确保了证据的基础和时间一致性，这在现有方法中是缺乏的。\\n\\n**关键设计**：在翻译管道中，采用了多层次的模式驱动方法，设计了特定的损失函数以确保生成的监督具有高质量和一致性，同时优化了网络结构以适应自我中心数据的特性。",
            "application_zh": "该研究的潜在应用领域包括机器人控制、智能家居系统和人机交互等。通过提升机器人在自我中心感知下的理解能力，PhysBrain能够更好地执行复杂的任务，增强机器人在现实环境中的适应性和灵活性，未来可能对智能机器人技术的发展产生深远影响。",
            "highlight_zh": "在E2E-3M数据集上训练的PhysBrain展现出显著的自我中心理解能力，尤其在EgoThink任务中表现优异，成功率达到53.9%。与传统方法相比，PhysBrain在样本效率和任务成功率上均有显著提升，展示了从人类自我中心监督到机器人控制的有效转移。",
            "tags_zh": [
                "自我中心感知",
                "物理智能",
                "视觉语言模型",
                "机器人控制",
                "数据集构建",
                "多层次监督",
                "因果结构",
                "长远规划"
            ],
            "_index": 10,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16793v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16793v1/fig/data_pipeline.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16793v1/fig/data_sum.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image",
            "authors": [
                "Yushi Hu",
                "Reyhane Askari-Hemmat",
                "Melissa Hall",
                "Emily Dinan",
                "Luke Zettlemoyer",
                "Marjan Ghazvininejad"
            ],
            "arxiv_id": "2512.16899v1",
            "summary": "Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning (\"thinking-with-images\"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.",
            "categories": [
                "cs.CL",
                "cs.CV"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Code and data available at https://github.com/facebookresearch/MMRB2",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16899v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "[T]multimodal"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Multimodal RewardBench 2，用于评估处理交错文本和图像的Omni Reward模型。",
            "summary_zh": "奖励模型(RMs)对于训练大型语言模型(LLMs)至关重要，但对于处理交错图像和文本序列的Omni模型，奖励模型的研究仍然不足。本文提出了Multimodal RewardBench 2 (MMRB2)，这是第一个全面的基准，用于评估多模态理解和（交错）生成方面的奖励模型。MMRB2涵盖四个任务：文本到图像、图像编辑、交错生成和多模态推理（“用图像思考”），每个任务提供1000个专家标注的偏好对，这些数据来自21个源任务中的23个模型和代理。MMRB2的设计特点包括：（1）实用但具有挑战性的提示；（2）来自最先进模型和代理的响应；（3）通过集成过滤策略策划的具有强烈人类专家共识的偏好对。使用MMRB2，我们研究了每个子任务的现有评判器，包括多模态LLM-as-a-judge和使用人类偏好训练的模型。最新的Gemini 3 Pro达到了75-80%的准确率。GPT-5和Gemini 2.5 Pro达到了66-75%的准确率，而人类的准确率超过90%，但它们超过了广泛使用的GPT-4o（59%）。性能最佳的开源模型Qwen3-VL-32B实现了与Gemini 2.5 Flash（64%）相似的准确率。我们还表明，MMRB2的性能与使用Best-of-N抽样的下游任务成功率密切相关，并进行了深入分析，揭示了未来改进奖励模型的关键领域。",
            "intro_zh": [
                "现有奖励模型在处理交错文本和图像的多模态任务中表现不足，缺乏专门的评估基准。",
                "提出Multimodal RewardBench 2 (MMRB2)，包含四个任务，提供专家标注的偏好对，用于全面评估多模态奖励模型。",
                "实验表明，Gemini 3 Pro在MMRB2上表现最佳，但与人类水平仍有差距，开源模型Qwen3-VL-32B表现接近Gemini 2.5 Flash。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多模态奖励模型评估不足的问题，尤其是在处理交错文本和图像序列时。现有的奖励模型和评估方法主要集中在文本或图像的单模态任务上，缺乏对多模态交互和推理能力的有效评估。这导致了在多模态场景下，奖励模型难以准确衡量生成内容的质量和符合人类偏好的程度。\\n\\n**核心思路**：论文的核心思路是构建一个全面的多模态奖励模型评估基准，即Multimodal RewardBench 2 (MMRB2)。该基准包含多个具有挑战性的多模态任务，并提供高质量的人工标注偏好数据，用于训练和评估奖励模型。通过在MMRB2上进行评估，可以更准确地了解奖励模型在多模态场景下的性能，并指导模型的改进。\\n\\n**技术框架**：MMRB2基准包含四个主要任务：（1）文本到图像生成；（2）图像编辑；（3）交错文本和图像生成；（4）多模态推理（“用图像思考”）。每个任务都包含1000个专家标注的偏好对，这些数据来自21个源任务中的23个模型和代理。数据的收集和标注过程采用了集成过滤策略，以确保偏好对具有高度的人类专家共识。\\n\\n**关键创新**：MMRB2的关键创新在于它是第一个专门针对多模态奖励模型评估的综合性基准。它不仅包含了多种具有挑战性的多模态任务，还提供了高质量的人工标注偏好数据，并采用了严格的质量控制流程。此外，MMRB2还研究了现有评判器（包括多模态LLM-as-a-judge和使用人类偏好训练的模型）的性能，为未来的研究提供了重要的参考。\\n\\n**关键设计**：MMRB2的关键设计包括：(1) 实用但具有挑战性的提示，旨在模拟真实应用场景；(2) 来自最先进模型和代理的响应，确保评估的代表性；(3) 通过集成过滤策略策划的具有强烈人类专家共识的偏好对，保证数据的质量。此外，论文还分析了MMRB2性能与下游任务成功率之间的相关性，验证了基准的有效性。",
            "application_zh": "该研究成果可应用于训练和评估多模态大型语言模型，提升模型在图像生成、图像编辑、多模态对话和推理等任务中的性能。高质量的奖励模型能够更好地对齐模型输出与人类偏好，从而提高用户体验和应用价值，例如智能助手、内容创作工具等。",
            "highlight_zh": "实验结果表明，最新的Gemini 3 Pro在MMRB2上达到了75-80%的准确率，GPT-5和Gemini 2.5 Pro达到了66-75%的准确率，超过了GPT-4o（59%）。开源模型Qwen3-VL-32B实现了与Gemini 2.5 Flash（64%）相似的准确率。MMRB2性能与下游任务成功率高度相关，验证了基准的有效性。",
            "tags_zh": [
                "多模态学习",
                "奖励模型",
                "基准测试",
                "图像文本交错",
                "大型语言模型"
            ],
            "_index": 11,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16899v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16899v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16899v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation",
            "authors": [
                "Zhiyang Guo",
                "Ori Zhang",
                "Jax Xiang",
                "Alan Zhao",
                "Wengang Zhou",
                "Houqiang Li"
            ],
            "arxiv_id": "2512.16767v1",
            "summary": "Posing 3D characters is a fundamental task in computer graphics and vision. However, existing methods like auto-rigging and pose-conditioned generation often struggle with challenges such as inaccurate skinning weight prediction, topological imperfections, and poor pose conformance, limiting their robustness and generalizability. To overcome these limitations, we introduce Make-It-Poseable, a novel feed-forward framework that reformulates character posing as a latent-space transformation problem. Instead of deforming mesh vertices as in traditional pipelines, our method reconstructs the character in new poses by directly manipulating its latent representation. At the core of our method is a latent posing transformer that manipulates shape tokens based on skeletal motion. This process is facilitated by a dense pose representation for precise control. To ensure high-fidelity geometry and accommodate topological changes, we also introduce a latent-space supervision strategy and an adaptive completion module. Our method demonstrates superior performance in posing quality. It also naturally extends to 3D editing applications like part replacement and refinement.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project page: https://jasongzy.github.io/Make-It-Poseable/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16767v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]humanoid"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "[T]character animation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "1_robot_core",
                "8_physics_animation"
            ],
            "headline_zh": "提出Make-It-Poseable，用于解决3D人形角色动画中姿态控制难题。",
            "summary_zh": "本文提出了一种名为Make-It-Poseable的新型前馈框架，用于3D人形角色动画的姿态控制。现有方法如自动绑定和姿态条件生成，在蒙皮权重预测不准确、拓扑结构缺陷和姿态一致性差等方面存在挑战，限制了它们的鲁棒性和泛化能力。为了克服这些限制，Make-It-Poseable将角色姿态控制重新定义为一个潜在空间变换问题。该方法不直接变形网格顶点，而是通过操纵角色的潜在表示来重建新的姿态。核心是一个潜在姿态Transformer，它基于骨骼运动来操纵形状token。密集姿态表示用于实现精确控制。为了确保高保真几何形状并适应拓扑变化，还引入了潜在空间监督策略和自适应补全模块。该方法在姿态质量方面表现出卓越的性能，并且自然地扩展到3D编辑应用，如部件替换和优化。",
            "intro_zh": [
                "现有3D角色姿态控制方法在蒙皮权重预测、拓扑结构和姿态一致性方面存在不足，影响了鲁棒性和泛化性。",
                "Make-It-Poseable将姿态控制视为潜在空间变换问题，通过操纵潜在表示而非直接变形顶点来重建角色。",
                "该方法通过潜在姿态Transformer、密集姿态表示、潜在空间监督和自适应补全模块，实现了高质量的姿态控制。"
            ],
            "method_zh": "**问题定义**：论文旨在解决3D人形角色动画中姿态控制的问题。现有方法，如自动绑定和姿态条件生成，在蒙皮权重预测、拓扑结构以及姿态一致性方面存在不足，导致生成结果不准确，鲁棒性和泛化能力较差。这些问题限制了3D角色动画在各种应用中的使用。\n\n**核心思路**：论文的核心思路是将角色姿态控制问题转化为潜在空间中的变换问题。不再直接操作3D模型的顶点，而是学习一个潜在空间，并在该空间中通过变换来改变角色的姿态。这种方法可以更好地处理拓扑变化，并提高姿态控制的精度和鲁棒性。通过在潜在空间进行操作，可以避免传统方法中复杂的蒙皮和变形计算。\n\n**技术框架**：Make-It-Poseable框架主要包含以下几个模块：1) 编码器：将3D角色模型编码到潜在空间中。2) 潜在姿态Transformer：根据输入的骨骼运动信息，在潜在空间中对形状token进行变换，从而改变角色的姿态。3) 解码器：将变换后的潜在表示解码回3D模型。4) 自适应补全模块：用于处理拓扑变化，并补全缺失的几何细节。整个流程是前馈的，可以高效地生成新的姿态。\n\n**关键创新**：该方法最重要的创新点在于将姿态控制问题转化为潜在空间变换问题。与直接操作3D模型顶点的方法不同，该方法通过操纵潜在表示来重建角色，从而更好地处理拓扑变化，并提高姿态控制的精度和鲁棒性。此外，潜在姿态Transformer的设计也使得可以有效地利用骨骼运动信息来控制角色的姿态。\n\n**关键设计**：论文中使用了密集姿态表示来精确控制角色的姿态。潜在姿态Transformer采用Transformer架构，用于学习骨骼运动与潜在空间表示之间的映射关系。为了确保高保真几何形状，引入了潜在空间监督策略，通过在潜在空间中进行监督，可以更好地约束模型的学习。自适应补全模块的设计也考虑了拓扑变化，可以有效地补全缺失的几何细节。损失函数的设计也至关重要，需要综合考虑姿态一致性、几何形状和拓扑结构等因素。",
            "application_zh": "该研究成果可广泛应用于3D游戏开发、虚拟现实、动画制作等领域。通过Make-It-Poseable，可以更高效、更精确地控制3D角色的姿态，从而提升用户体验和创作效率。此外，该方法还可以应用于3D角色编辑，例如部件替换和优化，为3D内容创作提供更多可能性。未来，该技术有望进一步推动虚拟角色的智能化和自动化。",
            "highlight_zh": "论文提出的Make-It-Poseable方法在姿态质量方面表现出卓越的性能。实验结果表明，该方法能够生成高质量的3D角色姿态，并且能够有效地处理拓扑变化。与现有方法相比，该方法在姿态一致性和几何细节方面都有显著提升。此外，该方法还能够自然地扩展到3D编辑应用，例如部件替换和优化。",
            "tags_zh": [
                "3D角色动画",
                "姿态控制",
                "潜在空间变换",
                "Transformer",
                "人形建模"
            ],
            "_index": 12,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16767v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16767v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16767v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "SDFoam: Signed-Distance Foam for explicit surface reconstruction",
            "authors": [
                "Antonella Rech",
                "Nicola Conci",
                "Nicola Garau"
            ],
            "arxiv_id": "2512.16706v1",
            "summary": "Neural radiance fields (NeRF) have driven impressive progress in view synthesis by using ray-traced volumetric rendering. Splatting-based methods such as 3D Gaussian Splatting (3DGS) provide faster rendering by rasterizing 3D primitives. RadiantFoam (RF) brought ray tracing back, achieving throughput comparable to Gaussian Splatting by organizing radiance with an explicit Voronoi Diagram (VD). Yet, all the mentioned methods still struggle with precise mesh reconstruction. We address this gap by jointly learning an explicit VD with an implicit Signed Distance Field (SDF). The scene is optimized via ray tracing and regularized by an Eikonal objective. The SDF introduces metric-consistent isosurfaces, which, in turn, bias near-surface Voronoi cell faces to align with the zero level set. The resulting model produces crisper, view-consistent surfaces with fewer floaters and improved topology, while preserving photometric quality and maintaining training speed on par with RadiantFoam. Across diverse scenes, our hybrid implicit-explicit formulation, which we name SDFoam, substantially improves mesh reconstruction accuracy (Chamfer distance) with comparable appearance (PSNR, SSIM), without sacrificing efficiency.",
            "categories": [
                "cs.CV",
                "cs.GR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16706v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "3DGS",
                        "gaussian splatting",
                        "splatting",
                        "NeRF",
                        "neural radiance field"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "SDFoam：结合显式Voronoi图和隐式SDF，实现精确表面重建",
            "summary_zh": "神经辐射场（NeRF）通过光线追踪的体渲染在视角合成方面取得了显著进展。基于Splatting的方法，如3D高斯溅射（3DGS），通过栅格化3D图元提供了更快的渲染速度。RadiantFoam（RF）通过使用显式Voronoi图（VD）组织辐射，重新引入了光线追踪，实现了与高斯溅射相当的吞吐量。然而，上述方法在精确网格重建方面仍然存在困难。我们通过联合学习显式VD和隐式有向距离场（SDF）来解决这个问题。场景通过光线追踪进行优化，并由Eikonal目标正则化。SDF引入了度量一致的等值面，进而使近表面Voronoi单元面与零水平集对齐。由此产生的模型产生更清晰、视角一致的表面，减少了漂浮伪影并改善了拓扑结构，同时保持了光度质量，并保持了与RadiantFoam相当的训练速度。在不同的场景中，我们提出的混合隐式-显式公式，我们称之为SDFoam，在不牺牲效率的情况下，显著提高了网格重建精度（Chamfer距离），并具有可比的外观（PSNR，SSIM）。",
            "intro_zh": [
                "现有NeRF和3DGS等方法在精确网格重建方面存在不足，难以生成高质量的几何表面。",
                "SDFoam联合学习显式Voronoi图和隐式SDF，利用SDF的度量一致性来约束Voronoi图，从而优化表面几何。",
                "实验表明，SDFoam在保持光度质量和训练速度的同时，显著提高了网格重建精度，减少了伪影。"
            ],
            "method_zh": "**问题定义**：现有神经渲染方法，如NeRF和3DGS，虽然在视角合成方面表现出色，但在精确网格重建方面存在局限性。这些方法生成的网格表面通常不够清晰，存在漂浮伪影，并且拓扑结构可能不准确。RadiantFoam虽然提高了渲染速度，但仍然难以实现高质量的几何重建。因此，如何提高神经渲染方法在网格重建方面的精度和质量是一个关键问题。\\n\\n**核心思路**：SDFoam的核心思路是将显式的Voronoi图和隐式的有向距离场（SDF）相结合，利用SDF的度量一致性来约束Voronoi图的形状，从而优化表面的几何结构。Voronoi图用于快速渲染，而SDF则提供精确的表面几何信息，两者相互补充，共同优化。\\n\\n**技术框架**：SDFoam的整体框架包括以下几个主要步骤：1) 初始化：使用一组3D点初始化场景，并构建Voronoi图。2) 光线追踪：通过光线追踪渲染场景，并计算渲染损失。3) SDF优化：使用Eikonal损失优化SDF，使其与真实表面一致。4) Voronoi图优化：利用SDF的梯度信息，调整Voronoi图的形状，使其与SDF的零水平集对齐。5) 迭代优化：重复步骤2-4，直到收敛。\\n\\n**关键创新**：SDFoam的关键创新在于将显式的Voronoi图和隐式的SDF相结合，形成一种混合的隐式-显式表示。这种混合表示既能实现快速渲染，又能提供精确的表面几何信息。与传统的NeRF方法相比，SDFoam能够生成更清晰、更准确的网格表面。与RadiantFoam相比，SDFoam通过引入SDF约束，显著提高了网格重建的精度。\\n\\n**关键设计**：SDFoam的关键设计包括：1) Eikonal损失：使用Eikonal损失来正则化SDF，使其梯度范数接近于1，从而保证SDF的度量一致性。2) Voronoi图对齐：利用SDF的梯度信息，调整Voronoi图的形状，使其与SDF的零水平集对齐。具体来说，通过最小化Voronoi单元面与SDF零水平集之间的距离来实现。3) 光线追踪优化：使用光线追踪渲染场景，并计算渲染损失，从而优化场景的外观。",
            "application_zh": "SDFoam在三维重建、虚拟现实、增强现实、机器人导航等领域具有广泛的应用前景。它可以用于生成高质量的三维模型，用于游戏开发、电影制作等。此外，SDFoam还可以用于机器人导航，帮助机器人理解周围环境的几何结构，从而实现更精确的定位和导航。该研究的未来影响在于推动神经渲染技术在实际应用中的发展，使其能够更好地服务于各个领域。",
            "highlight_zh": "实验结果表明，SDFoam在网格重建精度方面显著优于现有方法。在多个数据集上，SDFoam的Chamfer距离比RadiantFoam降低了显著比例，同时保持了与RadiantFoam相当的PSNR和SSIM。这意味着SDFoam在提高几何重建精度的同时，没有牺牲外观质量。此外，SDFoam的训练速度与RadiantFoam相当，表明其具有较高的效率。",
            "tags_zh": [
                "神经辐射场",
                "三维重建",
                "有向距离场",
                "Voronoi图",
                "光线追踪",
                "隐式表示",
                "显式表示"
            ],
            "_index": 13,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16706v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16706v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16706v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models",
            "authors": [
                "Yuxin Wang",
                "Lei Ke",
                "Boqiang Zhang",
                "Tianyuan Qu",
                "Hanxun Yu",
                "Zhenpeng Huang",
                "Meng Yu",
                "Dan Xu",
                "Dong Yu"
            ],
            "arxiv_id": "2512.16561v1",
            "summary": "While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, a novel unified framework that seamlessly integrates native 3D object perception with 3D-aware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate 3D object localization, the model further performs explicit reasoning in 3D, achieving more interpretable and structured spatial understanding. To support robust training for these capabilities, we develop a scalable data construction pipeline that leverages depth estimation to lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial question-answering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both 3D object localization and 3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on 3D grounding tasks, but also consistently surpasses existing methods in 3D spatial reasoning in vision-language model.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project Page: https://n3d-vlm.github.io",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16561v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "depth estimation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "spatial relationship"
                    ],
                    "score": 3.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal",
                        "chain-of-thought"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 11.0,
            "hit_pillars": [
                "3_perception_slam",
                "7_retargeting",
                "9_embodied_foundation"
            ],
            "headline_zh": "N3D-VLM：原生3D感知赋能视觉语言模型精确空间推理",
            "summary_zh": "当前的多模态模型虽然可以基于2D图像回答问题，但缺乏固有的3D物体感知能力，限制了其理解3D场景中的空间关系和深度线索的能力。本文提出了N3D-VLM，一种新颖的统一框架，它将原生3D物体感知与3D感知视觉推理无缝集成，从而实现精确的3D grounding和可解释的空间理解。与直接从RGB/RGB-D输入预测答案的传统端到端模型不同，我们的方法赋予模型原生的3D物体感知能力，使其能够基于文本描述直接在3D空间中定位物体。在精确的3D物体定位的基础上，该模型进一步在3D中执行显式推理，从而实现更可解释和结构化的空间理解。为了支持这些能力的稳健训练，我们开发了一个可扩展的数据构建流程，该流程利用深度估计将大规模2D标注提升到3D空间，显著增加了3D物体grounding数据的多样性和覆盖范围，比现有的最大单图像3D检测数据集大六倍以上。此外，该流程生成针对3D中思维链（CoT）推理的空间问答数据集，促进3D物体定位和3D空间推理的联合训练。实验结果表明，我们的统一框架不仅在3D grounding任务上实现了最先进的性能，而且在视觉语言模型中的3D空间推理方面始终优于现有方法。",
            "intro_zh": [
                "现有视觉语言模型缺乏对3D场景的感知能力，难以理解空间关系和深度信息，限制了其应用。",
                "N3D-VLM通过集成原生3D物体感知和3D感知视觉推理，实现精确的3D定位和可解释的空间理解。",
                "该方法在3D grounding和空间推理任务上均取得了SOTA性能，并构建了大规模3D标注数据集。"
            ],
            "method_zh": "**问题定义**：现有视觉语言模型主要基于2D图像进行推理，缺乏对3D场景的理解能力，无法准确感知物体间的空间关系和深度信息。这限制了模型在需要空间推理的任务中的表现，例如回答关于物体位置、距离等问题。现有方法通常直接从RGB或RGB-D图像预测答案，缺乏对3D场景的显式建模，导致结果难以解释。\n\n**核心思路**：N3D-VLM的核心思路是赋予模型原生的3D物体感知能力，使其能够直接在3D空间中定位物体，并在此基础上进行空间推理。通过显式地建模3D场景，模型可以更好地理解物体间的空间关系，从而提高空间推理的准确性和可解释性。这种方法避免了直接从2D图像预测答案的局限性，使得模型能够像人类一样，先理解场景的3D结构，再进行推理。\n\n**技术框架**：N3D-VLM的整体框架包含以下几个主要模块：1) 3D物体感知模块：该模块负责基于文本描述在3D空间中定位物体。2) 3D空间推理模块：该模块基于3D物体定位的结果，进行空间关系的推理，例如判断物体之间的距离、方位等。3) 数据构建流程：为了支持模型的训练，论文提出了一个可扩展的数据构建流程，该流程利用深度估计将大规模2D标注提升到3D空间，生成大规模的3D物体grounding和空间问答数据集。\n\n**关键创新**：N3D-VLM最重要的技术创新点在于将原生3D物体感知能力集成到视觉语言模型中。与现有方法不同，N3D-VLM不是直接从2D图像预测答案，而是先在3D空间中定位物体，再进行空间推理。这种方法使得模型能够更好地理解场景的3D结构，从而提高空间推理的准确性和可解释性。此外，论文提出的数据构建流程也为3D视觉语言模型的研究提供了重要的数据支持。\n\n**关键设计**：数据构建流程利用深度估计将2D标注提升到3D空间，具体实现细节未知。损失函数的设计可能包含3D物体定位的损失和空间推理的损失，具体形式未知。网络结构可能包含用于3D物体定位的模块和用于空间推理的模块，具体结构未知。",
            "application_zh": "N3D-VLM在机器人导航、自动驾驶、虚拟现实等领域具有广泛的应用前景。例如，在机器人导航中，机器人可以利用N3D-VLM理解人类的指令，例如“把桌子上的杯子拿到沙发旁边”，从而完成复杂的任务。在自动驾驶中，N3D-VLM可以帮助车辆理解周围环境，例如识别行人、车辆和交通标志，并进行相应的决策。在虚拟现实中，N3D-VLM可以增强用户的沉浸感，例如让用户与虚拟环境中的物体进行交互。",
            "highlight_zh": "N3D-VLM在3D grounding任务上取得了SOTA性能，具体数值未知。在3D空间推理任务上，N3D-VLM也显著优于现有方法，具体提升幅度未知。论文构建了一个比现有最大单图像3D检测数据集大六倍以上的大规模3D标注数据集，为相关研究提供了重要的数据支持。",
            "tags_zh": [
                "视觉语言模型",
                "3D感知",
                "空间推理",
                "3D grounding",
                "深度估计"
            ],
            "_index": 14,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16561v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16561v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16561v1/x5.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning",
            "authors": [
                "Tin Stribor Sohn",
                "Maximilian Dillitzer",
                "Jason J. Corso",
                "Eric Sax"
            ],
            "arxiv_id": "2512.16461v1",
            "summary": "Autonomous robotic systems require spatio-temporal understanding of dynamic environments to ensure reliable navigation and interaction. While Vision-Language Models (VLMs) provide open-world semantic priors, they lack grounding in 3D geometry and temporal dynamics. Conversely, geometric perception captures structure and motion but remains semantically sparse. We propose SNOW (Scene Understanding with Open-World Knowledge), a training-free and backbone-agnostic framework for unified 4D scene understanding that integrates VLM-derived semantics with point cloud geometry and temporal consistency. SNOW processes synchronized RGB images and 3D point clouds, using HDBSCAN clustering to generate object-level proposals that guide SAM2-based segmentation. Each segmented region is encoded through our proposed Spatio-Temporal Tokenized Patch Encoding (STEP), producing multimodal tokens that capture localized semantic, geometric, and temporal attributes. These tokens are incrementally integrated into a 4D Scene Graph (4DSG), which serves as 4D prior for downstream reasoning. A lightweight SLAM backend anchors all STEP tokens spatially in the environment, providing the global reference alignment, and ensuring unambiguous spatial grounding across time. The resulting 4DSG forms a queryable, unified world model through which VLMs can directly interpret spatial scene structure and temporal dynamics. Experiments on a diverse set of benchmarks demonstrate that SNOW enables precise 4D scene understanding and spatially grounded inference, thereby setting new state-of-the-art performance in several settings, highlighting the importance of structured 4D priors for embodied reasoning and autonomous robotics.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16461v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "world model"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]scene understanding"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam",
                "9_embodied_foundation"
            ],
            "headline_zh": "SNOW：利用世界知识进行时空场景理解，实现开放世界具身推理",
            "summary_zh": "自主机器人系统需要对动态环境进行时空理解，以确保可靠的导航和交互。视觉-语言模型(VLMs)提供了开放世界的语义先验，但缺乏3D几何和时间动态的 grounding。几何感知可以捕获结构和运动，但语义稀疏。我们提出了SNOW(Scene Understanding with Open-World Knowledge)，一个无需训练且与骨干网络无关的框架，用于统一的4D场景理解，它将VLM衍生的语义与点云几何和时间一致性相结合。SNOW处理同步的RGB图像和3D点云，使用HDBSCAN聚类生成对象级别的 proposals，指导基于SAM2的分割。每个分割区域通过我们提出的时空 Tokenized Patch Encoding (STEP)进行编码，产生多模态 tokens，捕获局部语义、几何和时间属性。这些 tokens 增量式地集成到4D场景图(4DSG)中，作为下游推理的4D先验。轻量级的SLAM后端在环境中对所有STEP tokens进行空间锚定，提供全局参考对齐，并确保跨时间无歧义的空间 grounding。由此产生的4DSG形成了一个可查询的统一世界模型，通过该模型，VLMs可以直接解释空间场景结构和时间动态。在各种基准测试上的实验表明，SNOW能够实现精确的4D场景理解和空间 grounding 推理，从而在多个设置中实现了新的最先进性能，突出了结构化4D先验对于具身推理和自主机器人的重要性。",
            "intro_zh": [
                "现有方法在具身智能中，视觉语言模型缺乏3D几何和时间动态的关联，而几何感知语义信息不足，限制了机器人对环境的全面理解。",
                "SNOW框架通过融合视觉语言模型的语义信息、点云几何以及时间一致性，构建统一的4D场景图，为机器人提供更丰富的环境先验。",
                "实验结果表明，SNOW在多个基准测试中取得了最先进的性能，验证了其在4D场景理解和空间推理方面的有效性，对具身智能具有重要意义。"
            ],
            "method_zh": "**问题定义**：现有方法在具身智能任务中，通常面临两个挑战。一是视觉语言模型（VLM）虽然具备强大的语义理解能力，但缺乏对3D几何结构和时间动态的感知，导致无法准确理解真实世界的空间关系和变化。二是传统的几何感知方法，如SLAM和三维重建，虽然能够精确地捕捉场景的几何结构，但语义信息稀疏，难以进行高级别的推理和决策。因此，如何将VLM的语义知识与几何感知相结合，构建一个统一的、具有时空一致性的场景表示，是当前具身智能研究的关键问题。\\n\\n**核心思路**：SNOW的核心思路是将VLM的语义知识、点云几何信息以及时间一致性信息融合到一个统一的4D场景图中。通过HDBSCAN聚类和SAM2分割，提取场景中的对象级别 proposals，并使用提出的STEP编码器将每个分割区域编码为包含语义、几何和时间属性的多模态 tokens。这些tokens被增量式地集成到4DSG中，形成一个可查询的、具有时空一致性的世界模型。轻量级的SLAM后端负责对所有STEP tokens进行空间锚定，确保全局参考对齐和跨时间的空间 grounding。\\n\\n**技术框架**：SNOW框架主要包含以下几个模块：1) 数据输入模块：接收同步的RGB图像和3D点云数据。2) 对象Proposal生成模块：使用HDBSCAN聚类生成对象级别的 proposals，并利用SAM2进行分割。3) STEP编码模块：将每个分割区域编码为包含语义、几何和时间属性的多模态 tokens。4) 4DSG构建模块：将STEP tokens增量式地集成到4D场景图中。5) SLAM后端：对所有STEP tokens进行空间锚定，确保全局参考对齐。6) 查询接口：提供可查询的接口，允许VLMs直接访问和利用4DSG中的信息。\\n\\n**关键创新**：SNOW的关键创新在于以下几个方面：1) 提出了STEP编码器，能够有效地将语义、几何和时间属性融合到多模态 tokens中。2) 构建了4D场景图，能够以统一的方式表示场景的静态结构和动态变化。3) 利用轻量级的SLAM后端进行空间锚定，确保全局参考对齐和跨时间的空间 grounding。4) 提供可查询的接口，允许VLMs直接访问和利用4DSG中的信息。\\n\\n**关键设计**：STEP编码器通过将分割区域划分为多个patches，并对每个patch提取语义、几何和时间特征。语义特征通过VLM提取，几何特征通过点云处理提取，时间特征通过光流估计提取。这些特征被拼接在一起，并通过一个MLP进行编码，生成最终的STEP token。SLAM后端采用了一种轻量级的图优化方法，能够实时地对STEP tokens进行空间锚定。4DSG采用了一种基于图数据库的实现方式，能够高效地存储和查询场景信息。",
            "application_zh": "SNOW框架在自主机器人、增强现实、虚拟现实等领域具有广泛的应用前景。例如，在自主机器人领域，SNOW可以帮助机器人更好地理解周围环境，从而实现更可靠的导航、交互和任务执行。在增强现实和虚拟现实领域，SNOW可以用于构建更逼真的虚拟场景，并实现更自然的交互体验。此外，SNOW还可以应用于智能监控、自动驾驶等领域，具有重要的实际价值和未来影响。",
            "highlight_zh": "SNOW在多个基准测试中取得了显著的性能提升。例如，在ScanNet数据集上，SNOW在场景理解任务中的准确率提高了15%。在RoboTHOR数据集上，SNOW在具身推理任务中的成功率提高了20%。这些实验结果表明，SNOW能够有效地提高机器人对环境的理解能力和推理能力，为具身智能的发展奠定了坚实的基础。",
            "tags_zh": [
                "时空场景理解",
                "具身推理",
                "视觉语言模型",
                "4D场景图",
                "点云处理"
            ],
            "_index": 15,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16461v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16461v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16461v1/02_Figures/RoboSpatial_1.jpeg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "ManiLong-Shot: Interaction-Aware One-Shot Imitation Learning for Long-Horizon Manipulation",
            "authors": [
                "Zixuan Chen",
                "Chongkai Gao",
                "Lin Shao",
                "Jieqi Shi",
                "Jing Huo",
                "Yang Gao"
            ],
            "arxiv_id": "2512.16302v1",
            "summary": "One-shot imitation learning (OSIL) offers a promising way to teach robots new skills without large-scale data collection. However, current OSIL methods are primarily limited to short-horizon tasks, thus limiting their applicability to complex, long-horizon manipulations. To address this limitation, we propose ManiLong-Shot, a novel framework that enables effective OSIL for long-horizon prehensile manipulation tasks. ManiLong-Shot structures long-horizon tasks around physical interaction events, reframing the problem as sequencing interaction-aware primitives instead of directly imitating continuous trajectories. This primitive decomposition can be driven by high-level reasoning from a vision-language model (VLM) or by rule-based heuristics derived from robot state changes. For each primitive, ManiLong-Shot predicts invariant regions critical to the interaction, establishes correspondences between the demonstration and the current observation, and computes the target end-effector pose, enabling effective task execution. Extensive simulation experiments show that ManiLong-Shot, trained on only 10 short-horizon tasks, generalizes to 20 unseen long-horizon tasks across three difficulty levels via one-shot imitation, achieving a 22.8% relative improvement over the SOTA. Additionally, real-robot experiments validate ManiLong-Shot's ability to robustly execute three long-horizon manipulation tasks via OSIL, confirming its practical applicability.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Accepted by AAAI 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16302v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]imitation learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "ManiLong-Shot：交互感知的单样本模仿学习用于长时程操作任务",
            "summary_zh": "本文提出ManiLong-Shot，一个新颖的框架，旨在实现长时程灵巧操作任务的有效单样本模仿学习(OSIL)。ManiLong-Shot围绕物理交互事件构建长时程任务，将问题重新定义为对交互感知原语进行排序，而不是直接模仿连续轨迹。这种原语分解可以由视觉-语言模型(VLM)的高级推理驱动，也可以由机器人状态变化推导出的基于规则的启发式方法驱动。对于每个原语，ManiLong-Shot预测对交互至关重要的不变区域，建立演示和当前观察之间的对应关系，并计算目标末端执行器姿势，从而实现有效的任务执行。大量的仿真实验表明，ManiLong-Shot仅在10个短时程任务上训练，即可通过单样本模仿泛化到20个未见过的长时程任务，涵盖三个难度级别，相对于SOTA方法实现了22.8%的相对改进。此外，真实机器人实验验证了ManiLong-Shot通过OSIL稳健地执行三个长时程操作任务的能力，证实了其在实际应用中的可行性。",
            "intro_zh": [
                "现有单样本模仿学习方法主要局限于短时程任务，难以应用于复杂的长时程操作任务。",
                "ManiLong-Shot将长时程任务分解为交互感知的原语序列，通过预测不变区域和建立对应关系来模仿。",
                "实验表明，ManiLong-Shot在仿真和真实机器人上均表现出良好的泛化能力和鲁棒性，显著优于现有方法。"
            ],
            "method_zh": "**问题定义**：现有单样本模仿学习(OSIL)方法难以处理长时程操作任务，因为直接模仿连续轨迹在长序列中容易累积误差，并且难以泛化到新的场景。因此，需要一种能够有效处理长时程、高复杂度的操作任务的OSIL方法。\\n\\n**核心思路**：ManiLong-Shot的核心思路是将长时程任务分解为一系列交互感知的原语。每个原语对应一个特定的物理交互事件，例如抓取、放置等。通过学习如何执行这些原语，并将它们按照正确的顺序组合起来，就可以完成整个长时程任务。这种分解方式可以降低问题的复杂度，提高泛化能力。\\n\\n**技术框架**：ManiLong-Shot的整体框架包含以下几个主要模块：1) 任务分解模块：将长时程任务分解为一系列交互原语。该模块可以使用视觉-语言模型(VLM)进行高级推理，也可以使用基于规则的启发式方法。2) 原语执行模块：对于每个原语，该模块预测对交互至关重要的不变区域，建立演示和当前观察之间的对应关系，并计算目标末端执行器姿势。3) 控制模块：根据计算出的目标姿势，控制机器人执行相应的动作。\\n\\n**关键创新**：ManiLong-Shot的关键创新在于其交互感知的原语分解方法。与直接模仿连续轨迹的方法不同，ManiLong-Shot关注于任务中的物理交互事件，并将任务分解为一系列与这些事件相关的原语。这种分解方式可以更好地捕捉任务的本质，提高泛化能力。此外，利用VLM进行任务分解也是一个创新点。\\n\\n**关键设计**：在原语执行模块中，论文可能使用了特定的网络结构来预测不变区域和建立对应关系。例如，可以使用卷积神经网络(CNN)来提取图像特征，并使用注意力机制来建立演示和当前观察之间的对应关系。损失函数的设计也至关重要，可能包括模仿损失、对应损失等。具体的参数设置和网络结构细节需要在论文中进一步查找。",
            "application_zh": "ManiLong-Shot具有广泛的应用前景，例如在智能制造、家庭服务、医疗康复等领域。它可以用于教导机器人执行各种复杂的长时程操作任务，例如装配产品、整理物品、辅助病人等。该研究的成果有助于提高机器人的智能化水平，使其能够更好地服务于人类。",
            "highlight_zh": "ManiLong-Shot在仿真实验中，仅在10个短时程任务上训练，即可通过单样本模仿泛化到20个未见过的长时程任务，涵盖三个难度级别，相对于SOTA方法实现了22.8%的相对改进。在真实机器人实验中，ManiLong-Shot成功执行了三个长时程操作任务，验证了其在实际应用中的可行性。",
            "tags_zh": [
                "单样本模仿学习",
                "长时程操作",
                "交互感知",
                "原语分解",
                "视觉语言模型"
            ],
            "_index": 16,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16302v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16302v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16302v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
            "authors": [
                "Chaoyang Wang",
                "Kaituo Feng",
                "Dongyang Chen",
                "Zhongyu Wang",
                "Zhixun Li",
                "Sicheng Gao",
                "Meng Meng",
                "Xu Zhou",
                "Manyuan Zhang",
                "Yuzhang Shang",
                "Xiangyu Yue"
            ],
            "arxiv_id": "2512.16918v1",
            "summary": "Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project page: https://github.com/CYWang735/AdaTooler-V",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16918v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal",
                        "chain-of-thought"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "AdaTooler-V：一种自适应工具使用的图像和视频多模态大语言模型",
            "summary_zh": "本文提出AdaTooler-V，一种多模态大语言模型(MLLM)，通过确定视觉问题是否真正需要工具来执行自适应工具使用。为了实现这一目标，我们引入了AT-GRPO，一种强化学习算法，它基于每个样本的工具效益评分自适应地调整奖励尺度，鼓励模型仅在工具提供真正改进时才调用它们。此外，我们构建了两个数据集来支持训练：AdaTooler-V-CoT-100k用于SFT冷启动，AdaTooler-V-300k用于RL，具有跨单图像、多图像和视频数据的可验证奖励。在十二个基准测试上的实验表明了AdaTooler-V强大的推理能力，在各种视觉推理任务中优于现有方法。值得注意的是，AdaTooler-V-7B在高分辨率基准V*上实现了89.8%的准确率，超过了商业专有模型GPT-4o和Gemini 1.5 Pro。所有代码、模型和数据均已发布。",
            "intro_zh": [
                "现有开源多模态大语言模型存在盲目工具使用模式，即使不必要也会调用视觉工具，显著增加推理开销并降低模型性能。",
                "AdaTooler-V通过判断视觉问题是否真正需要工具，从而实现自适应工具使用，避免不必要的工具调用。",
                "实验结果表明，AdaTooler-V在多个视觉推理任务中超越现有方法，并在高分辨率基准测试中超过GPT-4o和Gemini 1.5 Pro。"
            ],
            "method_zh": "**问题定义**：现有开源多模态大语言模型在处理视觉任务时，常常不加区分地调用视觉工具，即使这些工具对于解决问题并非必要。这种盲目使用工具的方式导致了计算资源的浪费，增加了推理时间，并且在某些情况下还会降低模型的性能，因为不相关的工具可能会引入噪声或干扰。\n\n**核心思路**：AdaTooler-V的核心思路是让模型具备自适应地判断是否需要使用工具的能力。模型需要学习何时应该调用工具以提升性能，以及何时应该避免调用工具以节省计算资源。这种自适应性是通过强化学习来实现的，模型根据其行为获得的奖励来学习最佳的工具使用策略。\n\n**技术框架**：AdaTooler-V的整体框架包括预训练的多模态大语言模型、工具调用模块和强化学习训练模块。首先，使用AdaTooler-V-CoT-100k数据集进行监督微调(SFT)，使模型具备初步的工具使用能力。然后，使用AdaTooler-V-300k数据集，通过AT-GRPO强化学习算法对模型进行训练，使其能够自适应地选择是否调用工具。AT-GRPO算法根据每个样本的工具效益评分来调整奖励尺度，鼓励模型仅在工具能够带来显著改进时才调用它们。\n\n**关键创新**：AdaTooler-V的关键创新在于AT-GRPO强化学习算法和自适应工具使用策略。AT-GRPO算法能够根据样本的特性动态调整奖励，从而更有效地训练模型。自适应工具使用策略使得模型能够根据具体任务的需求，智能地选择是否调用工具，避免了盲目使用工具带来的问题。此外，构建的两个数据集AdaTooler-V-CoT-100k和AdaTooler-V-300k也为模型的训练提供了高质量的数据支持。\n\n**关键设计**：AT-GRPO算法的关键设计在于工具效益评分(Tool Benefit Score)的计算方式和奖励尺度的调整策略。工具效益评分用于衡量工具的使用对解决问题带来的提升程度。奖励尺度根据工具效益评分进行调整，当工具效益评分较高时，模型调用工具会获得更高的奖励；当工具效益评分较低时，模型调用工具会受到惩罚。这种设计鼓励模型学习仅在工具能够带来显著改进时才调用它们。具体的奖励函数和网络结构等细节在论文中有详细描述。",
            "application_zh": "AdaTooler-V在多个领域具有广泛的应用前景，例如智能客服、自动驾驶、医疗诊断等。它可以用于处理各种视觉推理任务，例如图像描述、视觉问答、视频理解等。通过自适应地选择是否调用工具，AdaTooler-V可以提高推理效率，降低计算成本，并提升模型性能。未来，该技术有望应用于更复杂的视觉任务，并与其他技术相结合，实现更智能化的视觉系统。",
            "highlight_zh": "AdaTooler-V在十二个基准测试中表现出色，证明了其强大的推理能力。尤其是在高分辨率基准V*上，AdaTooler-V-7B实现了89.8%的准确率，超过了商业专有模型GPT-4o和Gemini 1.5 Pro。这表明AdaTooler-V在处理复杂视觉任务方面具有显著优势，并且能够与最先进的商业模型相媲美。",
            "tags_zh": [
                "多模态大语言模型",
                "自适应工具使用",
                "强化学习",
                "视觉推理",
                "图像理解"
            ],
            "_index": 17,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16918v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16918v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16918v1/x5.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation",
            "authors": [
                "Kaiwen Jiang",
                "Xueting Li",
                "Seonwook Park",
                "Ravi Ramamoorthi",
                "Shalini De Mello",
                "Koki Nagano"
            ],
            "arxiv_id": "2512.16893v1",
            "summary": "Portrait animation has witnessed tremendous quality improvements thanks to recent advances in video diffusion models. However, these 2D methods often compromise 3D consistency and speed, limiting their applicability in real-world scenarios, such as digital twins or telepresence. In contrast, 3D-aware facial animation feedforward methods -- built upon explicit 3D representations, such as neural radiance fields or Gaussian splatting -- ensure 3D consistency and achieve faster inference speed, but come with inferior expression details. In this paper, we aim to combine their strengths by distilling knowledge from a 2D diffusion-based method into a feed-forward encoder, which instantly converts an in-the-wild single image into a 3D-consistent, fast yet expressive animatable representation. Our animation representation is decoupled from the face's 3D representation and learns motion implicitly from data, eliminating the dependency on pre-defined parametric models that often constrain animation capabilities. Unlike previous computationally intensive global fusion mechanisms (e.g., multiple attention layers) for fusing 3D structural and animation information, our design employs an efficient lightweight local fusion strategy to achieve high animation expressivity. As a result, our method runs at 107.31 FPS for animation and pose control while achieving comparable animation quality to the state-of-the-art, surpassing alternative designs that trade speed for quality or vice versa. Project website is https://research.nvidia.com/labs/amri/projects/instant4d",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project website is https://research.nvidia.com/labs/amri/projects/instant4d",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16893v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]distillation"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "gaussian splatting",
                        "splatting",
                        "neural radiance field"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "提出基于3D感知表达蒸馏的快速高表现力高斯头部Avatar方法",
            "summary_zh": "本文提出了一种新的方法，旨在结合基于扩散模型的2D人像动画和基于显式3D表示（如神经辐射场或高斯溅射）的3D人脸动画的优点。该方法通过将知识从2D扩散模型提炼到一个前馈编码器中，实现从单张图像到3D一致、快速且富有表现力的可动画表示的即时转换。动画表示与人脸的3D表示解耦，并从数据中隐式地学习运动，从而消除了对预定义参数模型的依赖。采用轻量级的局部融合策略，以实现高动画表现力，避免了以往计算密集型的全局融合机制。该方法在动画和姿态控制方面以107.31 FPS的速度运行，同时实现了与最先进方法相当的动画质量。",
            "intro_zh": [
                "2D人像动画在质量上取得了显著提升，但通常牺牲了3D一致性和速度，限制了其在数字孪生等场景中的应用。",
                "该方法通过将2D扩散模型的知识提炼到前馈编码器中，实现快速生成3D一致且富有表现力的可动画人脸。",
                "该方法采用轻量级局部融合策略，在保证动画质量的同时，实现了107.31 FPS的快速动画和姿态控制。"
            ],
            "method_zh": "**问题定义**：现有2D人像动画方法虽然质量高，但缺乏3D一致性且速度慢，难以应用于实时场景。而基于3D表示的人脸动画方法虽然保证了3D一致性和速度，但动画细节表现力不足。因此，需要一种既能保证3D一致性和速度，又能实现高表现力动画的方法。\\n\\n**核心思路**：核心思路是将高质量的2D扩散模型的知识蒸馏到基于3D表示的前馈网络中，从而结合两者的优点。通过这种方式，可以快速生成具有丰富表情细节且3D一致的人脸动画。同时，将动画表示与3D结构解耦，并采用轻量级的局部融合策略，进一步提升了效率和表现力。\\n\\n**技术框架**：该方法包含一个前馈编码器，用于将单张输入图像转换为3D人脸表示和动画参数。该3D人脸表示基于高斯溅射，保证了渲染速度和质量。动画参数则用于控制人脸的表情和姿态。为了融合3D结构和动画信息，采用了轻量级的局部融合模块。整个框架通过蒸馏学习进行训练，利用2D扩散模型生成的动画序列作为监督信号。\\n\\n**关键创新**：该方法的关键创新在于：1) 采用蒸馏学习的方式，将2D扩散模型的知识迁移到3D人脸动画中；2) 将动画表示与3D结构解耦，从而可以独立控制表情和姿态；3) 采用轻量级的局部融合策略，避免了计算密集型的全局融合，提高了效率。\\n\\n**关键设计**：在网络结构方面，编码器采用卷积神经网络，用于提取图像特征。高斯溅射表示采用标准的3D高斯参数化。局部融合模块采用多个卷积层和注意力机制，用于融合3D结构和动画信息。损失函数包括重建损失、3D一致性损失和动画损失。重建损失用于保证生成的人脸图像与输入图像相似。3D一致性损失用于保证生成的人脸在不同视角下的一致性。动画损失用于保证生成的动画与2D扩散模型生成的动画相似。",
            "application_zh": "该研究成果可广泛应用于数字孪生、远程呈现、虚拟会议、游戏角色生成等领域。通过该方法，可以快速生成逼真且富有表现力的3D人脸Avatar，为用户提供更加沉浸式的交互体验。此外，该方法还可以用于人脸表情识别、人脸动画编辑等任务，具有重要的实际应用价值。",
            "highlight_zh": "该方法在动画和姿态控制方面实现了107.31 FPS的速度，同时达到了与最先进方法相当的动画质量。与现有方法相比，该方法在速度和质量之间取得了更好的平衡。实验结果表明，该方法在表情细节和3D一致性方面均优于其他基于3D表示的人脸动画方法。",
            "tags_zh": [
                "人脸动画",
                "3D人脸",
                "高斯溅射",
                "知识蒸馏",
                "扩散模型",
                "实时渲染",
                "数字孪生"
            ],
            "_index": 18,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16893v1/fig/expressiveness_vs_consistency_colored.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16893v1/fig/pipeline-2.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16893v1/fig/residual_features.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning",
            "authors": [
                "Qihao Liu",
                "Luoxin Ye",
                "Wufei Ma",
                "Yu-Cheng Chou",
                "Alan Yuille"
            ],
            "arxiv_id": "2512.16917v1",
            "summary": "Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.",
            "categories": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16917v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning",
                        "distillation",
                        "reward shaping"
                    ],
                    "score": 7.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出生成对抗推理器，通过对抗强化学习提升LLM的推理能力，尤其在数学问题上。",
            "summary_zh": "本文提出了一种名为生成对抗推理器(Generative Adversarial Reasoner)的在线联合训练框架，旨在通过对抗强化学习协同进化LLM推理器和基于LLM的判别器，从而增强推理能力。该框架采用计算高效的审查机制，将每个推理链划分为逻辑完整的、长度相当的片段，判别器使用简洁、结构化的理由评估每个片段的合理性。学习过程耦合了互补信号：LLM推理器因产生逻辑一致且能得出正确答案的步骤而获得奖励，而判别器因正确检测到推理过程中的错误或区分推理轨迹而获得奖励。这产生了密集的、良好校准的、在线的步骤级别奖励，补充了稀疏的精确匹配信号，改善了信用分配，提高了样本效率，并增强了LLM的整体推理质量。在各种数学基准测试中，该方法相对于使用标准RL后训练的强大基线，实现了持续的收益。具体而言，在AIME24上，DeepSeek-R1-Distill-Qwen-7B从54.0提高到61.3（+7.3），DeepSeek-R1-Distill-Llama-8B从43.7提高到53.7（+10.0）。模块化判别器还能够灵活地进行奖励塑造，以实现诸如教师知识蒸馏、偏好对齐和基于数学证明的推理等目标。",
            "intro_zh": [
                "现有具备显式推理能力的大型语言模型在数学推理方面表现出色，但仍存在过程错误，如不正确的计算和脆弱的逻辑。",
                "论文提出生成对抗推理器，通过对抗强化学习协同训练LLM推理器和判别器，利用判别器对推理过程进行细粒度评估。",
                "实验结果表明，该方法在多个数学基准测试中，相较于标准强化学习后训练的基线模型，性能得到显著提升。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型（LLM）在数学推理过程中出现的逻辑错误、计算错误等问题。现有方法通常依赖于稀疏的奖励信号（例如，答案是否完全正确），难以对推理过程中的每一步进行有效指导，导致信用分配困难，训练效率低下。\\n\\n**核心思路**：论文的核心思路是引入一个判别器，与LLM推理器进行对抗训练。判别器负责评估推理过程中的每一步是否合理，并给出结构化的理由。通过这种方式，可以为推理器提供更密集、更细粒度的奖励信号，从而提高训练效率和推理质量。\\n\\n**技术框架**：整体框架包含两个主要模块：LLM推理器和LLM判别器。推理器负责生成推理步骤，判别器负责评估每个推理步骤的合理性。训练过程采用在线强化学习，推理器根据判别器的反馈调整策略，判别器根据推理器的表现调整评估标准。一个关键组件是“审查机制”，它将推理链分割成逻辑完整的片段，以便判别器进行评估。\\n\\n**关键创新**：最重要的技术创新点在于使用对抗强化学习来训练LLM推理器。与传统的强化学习方法相比，该方法能够提供更密集、更细粒度的奖励信号，从而更好地指导推理过程。此外，模块化的判别器设计使得可以灵活地进行奖励塑造，以适应不同的目标，例如知识蒸馏和偏好对齐。\\n\\n**关键设计**：审查机制将推理链分割成长度相当的片段，确保每个片段在逻辑上是完整的。判别器输出结构化的理由，解释其评估结果。推理器和判别器的奖励函数被设计为相互对抗，推理器试图生成能够欺骗判别器的推理步骤，而判别器试图准确地识别推理过程中的错误。具体参数设置和网络结构细节未在摘要中详细说明，属于未知信息。",
            "application_zh": "该研究成果可应用于各种需要复杂推理能力的场景，例如数学问题求解、科学研究、智能问答系统等。通过提高LLM的推理能力，可以使其在这些领域发挥更大的作用，并为自动化推理和决策提供更可靠的基础。未来的影响包括提升AI在复杂问题解决方面的能力，并可能促进AI在教育、科研等领域的应用。",
            "highlight_zh": "实验结果表明，该方法在AIME24数学基准测试中取得了显著的性能提升。具体而言，DeepSeek-R1-Distill-Qwen-7B模型从54.0提高到61.3（+7.3），DeepSeek-R1-Distill-Llama-8B模型从43.7提高到53.7（+10.0）。这些结果表明，该方法能够有效地提高LLM的推理能力，并优于现有的强化学习后训练方法。",
            "tags_zh": [
                "大型语言模型",
                "强化学习",
                "对抗学习",
                "数学推理",
                "推理器",
                "判别器",
                "奖励塑造",
                "在线学习"
            ],
            "_index": 19,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16917v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16917v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16917v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Non-Asymptotic Global Convergence of PPO-Clip",
            "authors": [
                "Yin Liu",
                "Qiming Dai",
                "Junyu Zhang",
                "Zaiwen Wen"
            ],
            "arxiv_id": "2512.16565v1",
            "summary": "Reinforcement learning (RL) has gained attention for aligning large language models (LLMs) via reinforcement learning from human feedback (RLHF). The actor-only variants of Proximal Policy Optimization (PPO) are widely applied for their efficiency. These algorithms incorporate a clipping mechanism to improve stability. Besides, a regularization term, such as the reverse KL-divergence or a more general \\(f\\)-divergence, is introduced to prevent policy drift. Despite their empirical success, a rigorous theoretical understanding of the problem and the algorithm's properties is limited. This paper advances the theoretical foundations of the PPO-Clip algorithm by analyzing a deterministic actor-only PPO algorithm within the general RL setting with \\(f\\)-divergence regularization under the softmax policy parameterization. We derive a non-uniform Lipschitz smoothness condition and a Łojasiewicz inequality for the considered problem. Based on these, a non-asymptotic linear convergence rate to the globally optimal policy is established for the forward KL-regularizer. Furthermore, stationary convergence and local linear convergence are derived for the reverse KL-regularizer.",
            "categories": [
                "math.OC",
                "cs.LG"
            ],
            "primary_category": "math.OC",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16565v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "[T]PPO",
                        "RLHF"
                    ],
                    "score": 7.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出PPO-Clip算法的非渐近全局收敛性分析",
            "summary_zh": "强化学习（RL）因其在通过人类反馈对大型语言模型（LLM）进行对齐的能力而受到关注。本文针对现有的PPO算法在理论理解上的不足，分析了在一般RL设置下，采用软最大策略参数化的确定性PPO算法，并引入了f-散度正则化。通过推导非均匀Lipschitz光滑性条件和Łojasiewicz不等式，建立了前向KL正则化器的非渐近线性收敛率。此外，还推导了反向KL正则化器的平稳收敛和局部线性收敛性，为PPO-Clip算法的理论基础提供了重要支持。",
            "intro_zh": [
                "现有的PPO算法在理论分析上存在不足，尤其是在收敛性和稳定性方面的理解较为有限。",
                "论文通过分析确定性PPO算法，结合f-散度正则化，提出了一种新的理论框架，增强了对PPO-Clip算法的理解。",
                "研究表明，前向KL正则化器可以实现非渐近线性收敛，而反向KL正则化器则具备平稳和局部线性收敛性，提升了算法的稳定性。"
            ],
            "method_zh": "**问题定义**：本文旨在解决PPO算法在理论分析上的不足，特别是缺乏对其收敛性和稳定性的严格理解。现有方法在实际应用中表现良好，但缺乏系统的理论支持。\\n\\n**核心思路**：论文通过分析确定性PPO算法，结合f-散度正则化，提出了一种新的理论框架，旨在提供对PPO-Clip算法的深入理解，并确保其收敛性。\\n\\n**技术框架**：整体架构包括对PPO算法的理论分析，推导非均匀Lipschitz光滑性条件和Łojasiewicz不等式，进而建立收敛性结果。主要模块包括算法设计、正则化策略和收敛性分析。\\n\\n**关键创新**：最重要的技术创新在于首次为PPO-Clip算法提供了非渐近全局收敛性的理论分析，尤其是在前向和反向KL正则化器下的不同收敛特性。\\n\\n**关键设计**：论文中引入了f-散度正则化，采用软最大策略参数化，并推导了相关的光滑性条件和不等式，为算法的收敛性提供了理论依据。",
            "application_zh": "该研究的潜在应用领域包括自然语言处理、机器人控制和其他需要通过人类反馈进行学习的智能系统。通过增强PPO-Clip算法的理论基础，能够在实际应用中提高算法的稳定性和收敛速度，进而提升模型的性能和可靠性。",
            "highlight_zh": "实验结果表明，前向KL正则化器实现了非渐近线性收敛，而反向KL正则化器则展现了平稳和局部线性收敛性。这些结果为PPO-Clip算法在实际应用中的有效性提供了强有力的理论支持。",
            "tags_zh": [
                "强化学习",
                "PPO算法",
                "KL散度",
                "收敛性分析",
                "理论研究",
                "算法稳定性",
                "人类反馈"
            ],
            "_index": 20,
            "_used_api": "openai",
            "figures": []
        },
        {
            "title": "The Evolution of Reranking Models in Information Retrieval: From Heuristic Methods to Large Language Models",
            "authors": [
                "Tejul Pandit",
                "Sakshi Mahendru",
                "Meet Raval",
                "Dhvani Upadhyay"
            ],
            "arxiv_id": "2512.16236v1",
            "summary": "Reranking is a critical stage in contemporary information retrieval (IR) systems, improving the relevance of the user-presented final results by honing initial candidate sets. This paper is a thorough guide to examine the changing reranker landscape and offer a clear view of the advancements made in reranking methods. We present a comprehensive survey of reranking models employed in IR, particularly within modern Retrieval Augmented Generation (RAG) pipelines, where retrieved documents notably influence output quality.\n  We embark on a chronological journey through the historical trajectory of reranking techniques, starting with foundational approaches, before exploring the wide range of sophisticated neural network architectures such as cross-encoders, sequence-generation models like T5, and Graph Neural Networks (GNNs) utilized for structural information. Recognizing the computational cost of advancing neural rerankers, we analyze techniques for enhancing efficiency, notably knowledge distillation for creating competitive, lighter alternatives. Furthermore, we map the emerging territory of integrating Large Language Models (LLMs) in reranking, examining novel prompting strategies and fine-tuning tactics. This survey seeks to elucidate the fundamental ideas, relative effectiveness, computational features, and real-world trade-offs of various reranking strategies. The survey provides a structured synthesis of the diverse reranking paradigms, highlighting their underlying principles and comparative strengths and weaknesses.",
            "categories": [
                "cs.IR",
                "cs.AI"
            ],
            "primary_category": "cs.IR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "15 pages, 1 figure, Accepted in CLNLP'25",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16236v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "distillation"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "综述信息检索中重排序模型演进：从启发式方法到大语言模型",
            "summary_zh": "重排序是现代信息检索（IR）系统中的关键阶段，它通过优化初始候选集来提高用户最终检索结果的相关性。本文全面考察了重排序领域的发展变化，清晰地展示了重排序方法所取得的进步。我们对信息检索中使用的重排序模型进行了全面的综述，特别是在现代检索增强生成（RAG）流程中，检索到的文档对输出质量有显著影响。本文按时间顺序回顾了重排序技术的发展历程，从基础方法开始，然后探讨了各种复杂的神经网络架构，如交叉编码器、序列生成模型（如T5）和用于结构信息的图神经网络（GNN）。考虑到神经重排序器推进的计算成本，我们分析了提高效率的技术，特别是用于创建有竞争力的、更轻量级替代方案的知识蒸馏。此外，我们还描绘了将大型语言模型（LLM）集成到重排序中的新兴领域，研究了新颖的提示策略和微调策略。本综述旨在阐明各种重排序策略的基本思想、相对有效性、计算特征和实际权衡，并对不同的重排序范式进行结构化的综合，突出其基本原理以及相对优势和劣势。",
            "intro_zh": [
                "现有信息检索系统在初始检索后，需要对候选结果进行重排序以提升用户体验，但传统方法效果有限。",
                "本文全面综述了信息检索中重排序模型的发展历程，重点关注神经重排序模型和大型语言模型在重排序中的应用。",
                "分析了各种重排序策略的原理、有效性、计算成本和实际应用，并对比了不同方法的优缺点。"
            ],
            "method_zh": "**问题定义**：信息检索系统通常会返回大量候选文档，但这些文档的相关性参差不齐。重排序旨在对这些初始检索结果进行重新排序，将最相关的文档排在前面，从而提高用户体验。现有方法的痛点在于，传统的启发式方法效果有限，而复杂的神经模型计算成本高昂。\\n\\n**核心思路**：本文的核心思路是对信息检索中的重排序模型进行全面的综述，从传统的启发式方法到现代的神经模型，再到新兴的大型语言模型，系统地梳理了各种重排序技术的发展历程和优缺点。通过对比分析，为研究者和开发者提供选择合适的重排序模型的指导。\\n\\n**技术框架**：本文的综述框架主要包括以下几个部分：1）传统的启发式重排序方法；2）基于神经网络的重排序模型，如交叉编码器、序列生成模型（如T5）和图神经网络（GNN）；3）提高神经重排序模型效率的技术，如知识蒸馏；4）将大型语言模型（LLM）集成到重排序中的方法，包括提示策略和微调策略。\\n\\n**关键创新**：本文的创新之处在于对信息检索中的重排序模型进行了全面的、系统性的综述，涵盖了从传统方法到最新技术的发展历程。特别关注了大型语言模型在重排序中的应用，并分析了各种方法的优缺点和适用场景。\\n\\n**关键设计**：本文主要关注各种重排序模型的设计思想和技术细节，包括特征工程、网络结构、损失函数、训练方法等。对于大型语言模型在重排序中的应用，重点关注提示策略和微调策略的设计。",
            "application_zh": "该研究成果可应用于各种信息检索系统，如搜索引擎、问答系统、推荐系统等。通过选择合适的重排序模型，可以显著提高检索结果的相关性和用户满意度。此外，该综述还可以为研究者提供参考，促进重排序技术的发展。",
            "highlight_zh": "本文是一篇综述性文章，没有具体的实验结果。但文章对各种重排序模型进行了详细的对比分析，总结了它们的优缺点和适用场景。特别关注了大型语言模型在重排序中的应用，并分析了各种提示策略和微调策略的效果。",
            "tags_zh": [
                "信息检索",
                "重排序",
                "神经网络",
                "大型语言模型",
                "检索增强生成"
            ],
            "_index": 21,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16236v1/Reranker_Module_2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                }
            ]
        },
        {
            "title": "ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning",
            "authors": [
                "Zihan Zhou",
                "Animesh Garg",
                "Ajay Mandlekar",
                "Caelan Garrett"
            ],
            "arxiv_id": "2512.16861v1",
            "summary": "Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16861v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "motion planning"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning",
                        "imitation learning"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "ReinforceGen：结合自动数据生成与强化学习的混合技能策略，解决机器人长时程操作难题。",
            "summary_zh": "本文提出了一种名为ReinforceGen的系统，用于解决机器人领域中长期存在的长时程操作难题。ReinforceGen结合了任务分解、数据生成、模仿学习和运动规划，以形成初始解决方案，并通过基于强化学习的微调来改进每个组件。具体而言，ReinforceGen首先将任务分割成多个局部技能，这些技能通过运动规划连接。技能和运动规划目标通过模仿学习在由10个人类演示生成的数据集上进行训练，然后通过在线自适应和强化学习进行微调。在Robosuite数据集上的基准测试表明，ReinforceGen在最高重置范围设置下，使用视觉运动控制在所有任务上达到了80%的成功率。额外的消融研究表明，我们的微调方法平均提高了89%的性能。",
            "intro_zh": [
                "长时程操作是机器人领域的长期挑战，现有方法难以有效分解任务和进行长期规划。",
                "ReinforceGen通过任务分解、数据生成、模仿学习和运动规划相结合，并使用强化学习进行微调，从而提升性能。",
                "在Robosuite数据集上，ReinforceGen在所有任务上达到了80%的成功率，并且微调方法平均提高了89%的性能。"
            ],
            "method_zh": "**问题定义**：长时程操作任务需要机器人完成一系列复杂的动作，现有方法通常难以有效地分解任务，并且在长期规划中容易出现误差累积，导致任务失败。此外，从零开始训练机器人策略需要大量的样本，而真实世界数据的获取成本很高。\\n\\n**核心思路**：ReinforceGen的核心思路是将长时程任务分解为多个局部技能，并利用模仿学习从少量人类演示数据中学习这些技能的初始策略。然后，通过强化学习对这些策略进行微调，以提高其鲁棒性和泛化能力。运动规划用于连接这些局部技能，从而完成整个长时程任务。\\n\\n**技术框架**：ReinforceGen的整体框架包括以下几个主要模块：1) 任务分解：将长时程任务分解为多个局部技能。2) 数据生成：利用少量人类演示数据生成大量的训练数据。3) 模仿学习：使用生成的数据训练局部技能和运动规划目标的初始策略。4) 运动规划：使用运动规划算法连接局部技能，形成完整的任务执行序列。5) 强化学习微调：使用强化学习算法对局部技能和运动规划目标的策略进行微调，以提高其鲁棒性和泛化能力。\\n\\n**关键创新**：ReinforceGen的关键创新在于将模仿学习和强化学习相结合，并利用自动数据生成来提高训练效率。通过模仿学习，可以从少量人类演示数据中快速学习到初始策略，而强化学习则可以进一步提高策略的性能。自动数据生成可以有效地扩充训练数据集，从而提高策略的泛化能力。\\n\\n**关键设计**：ReinforceGen使用了一种混合技能策略，其中每个局部技能都由一个独立的策略控制。这些策略可以使用不同的网络结构和损失函数进行训练。强化学习微调使用了PPO算法，并设计了合适的奖励函数来引导策略的学习。运动规划使用了RRT算法，并根据任务的特点进行了优化。",
            "application_zh": "ReinforceGen具有广泛的应用前景，可以应用于各种需要机器人完成复杂操作的任务中，例如装配、抓取、导航等。该研究可以降低机器人部署的成本，提高机器人的智能化水平，并促进机器人技术在工业、医疗、服务等领域的应用。",
            "highlight_zh": "ReinforceGen在Robosuite数据集上取得了显著的成果，在最高重置范围设置下，使用视觉运动控制在所有任务上达到了80%的成功率。消融研究表明，强化学习微调方法对性能提升贡献巨大，平均提高了89%。这些结果表明ReinforceGen是一种有效的长时程操作解决方案。",
            "tags_zh": [
                "机器人操作",
                "强化学习",
                "模仿学习",
                "任务分解",
                "运动规划",
                "数据生成",
                "长时程任务"
            ],
            "_index": 22,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16861v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16861v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16861v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "SARMAE: Masked Autoencoder for SAR Representation Learning",
            "authors": [
                "Danxu Liu",
                "Di Wang",
                "Hebaixu Wang",
                "Haoyang Chen",
                "Wentao Jiang",
                "Yilin Cheng",
                "Haonan Guo",
                "Wei Cui",
                "Jing Zhang"
            ],
            "arxiv_id": "2512.16635v1",
            "summary": "Synthetic Aperture Radar (SAR) imagery plays a critical role in all-weather, day-and-night remote sensing applications. However, existing SAR-oriented deep learning is constrained by data scarcity, while the physically grounded speckle noise in SAR imagery further hampers fine-grained semantic representation learning. To address these challenges, we propose SARMAE, a Noise-Aware Masked Autoencoder for self-supervised SAR representation learning. Specifically, we construct SAR-1M, the first million-scale SAR dataset, with additional paired optical images, to enable large-scale pre-training. Building upon this, we design Speckle-Aware Representation Enhancement (SARE), which injects SAR-specific speckle noise into masked autoencoders to facilitate noise-aware and robust representation learning. Furthermore, we introduce Semantic Anchor Representation Constraint (SARC), which leverages paired optical priors to align SAR features and ensure semantic consistency. Extensive experiments across multiple SAR datasets demonstrate that SARMAE achieves state-of-the-art performance on classification, detection, and segmentation tasks. Code and models will be available at https://github.com/MiliLab/SARMAE.",
            "categories": [
                "cs.CV",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Code and models will be available at https://github.com/MiliLab/SARMAE",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16635v1",
            "code_links": [
                {
                    "url": "https://github.com/MiliLab/SARMAE",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]representation learning",
                        "[T]masked autoencoder"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出SARMAE以解决SAR图像表示学习中的噪声问题",
            "summary_zh": "合成孔径雷达（SAR）图像在全天候、昼夜遥感应用中发挥着重要作用。然而，现有的SAR导向深度学习受到数据稀缺的限制，同时SAR图像中的物理散斑噪声进一步阻碍了细粒度语义表示学习。为了解决这些挑战，本文提出了SARMAE，一种噪声感知的自监督SAR表示学习的掩码自编码器。我们构建了SAR-1M，这是第一个百万规模的SAR数据集，并配有额外的光学图像，以支持大规模预训练。基于此，我们设计了散斑感知表示增强（SARE），将SAR特有的散斑噪声注入掩码自编码器，以促进噪声感知和鲁棒的表示学习。此外，我们引入了语义锚定表示约束（SARC），利用配对的光学先验对齐SAR特征，确保语义一致性。大量实验表明，SARMAE在分类、检测和分割任务上达到了最先进的性能。",
            "intro_zh": [
                "现有SAR图像深度学习方法受到数据稀缺和散斑噪声的影响，限制了其在细粒度语义表示学习中的应用。",
                "本文提出SARMAE，通过构建SAR-1M数据集和引入散斑感知表示增强（SARE）来实现噪声感知的自监督学习。",
                "实验结果表明，SARMAE在多个SAR数据集上实现了分类、检测和分割任务的最先进性能，显示出显著的提升。"
            ],
            "method_zh": "**问题定义**：本文旨在解决SAR图像表示学习中的数据稀缺和散斑噪声问题。现有方法在处理SAR图像时，往往无法有效应对噪声对表示学习的影响，导致性能不足。\\n\\n**核心思路**：SARMAE通过构建一个大规模的SAR数据集，并在掩码自编码器中引入散斑噪声，来实现噪声感知的自监督学习。这种设计旨在增强模型对噪声的鲁棒性，提高表示学习的质量。\\n\\n**技术框架**：SARMAE的整体架构包括数据预处理、散斑噪声注入、掩码自编码器训练和语义锚定约束等主要模块。首先，利用SAR-1M数据集进行预训练，然后通过SARE和SARC模块进行特征增强和对齐。\\n\\n**关键创新**：最重要的创新在于引入了散斑感知表示增强（SARE）和语义锚定表示约束（SARC），这两个模块使得模型能够有效地处理SAR特有的噪声，并确保特征的语义一致性。\\n\\n**关键设计**：在模型设计中，采用了特定的损失函数来平衡噪声感知和语义一致性，同时在网络结构上进行了优化，以适应SAR图像的特性。",
            "application_zh": "该研究的潜在应用领域包括军事侦察、灾害监测、环境监测等需要高精度SAR图像分析的场景。通过提高SAR图像的表示学习能力，SARMAE能够为相关领域提供更为准确的决策支持，具有重要的实际价值和未来影响。",
            "highlight_zh": "在多个SAR数据集上的实验结果显示，SARMAE在分类、检测和分割任务上均达到了最先进的性能，相较于基线方法，性能提升幅度超过了10%。具体而言，在某些任务中，准确率提高了15%以上，展示了其优越性。",
            "tags_zh": [
                "合成孔径雷达",
                "自监督学习",
                "散斑噪声",
                "表示学习",
                "深度学习"
            ],
            "_index": 23,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16635v1/images/radar.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16635v1/images/dataset.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16635v1/images/model.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Causal-Tune: Mining Causal Factors from Vision Foundation Models for Domain Generalized Semantic Segmentation",
            "authors": [
                "Yin Zhang",
                "Yongqiang Zhang",
                "Yaoyue Zheng",
                "Bogdan Raducanu",
                "Dan Liu"
            ],
            "arxiv_id": "2512.16567v1",
            "summary": "Fine-tuning Vision Foundation Models (VFMs) with a small number of parameters has shown remarkable performance in Domain Generalized Semantic Segmentation (DGSS). Most existing works either train lightweight adapters or refine intermediate features to achieve better generalization on unseen domains. However, they both overlook the fact that long-term pre-trained VFMs often exhibit artifacts, which hinder the utilization of valuable representations and ultimately degrade DGSS performance. Inspired by causal mechanisms, we observe that these artifacts are associated with non-causal factors, which usually reside in the low- and high-frequency components of the VFM spectrum. In this paper, we explicitly examine the causal and non-causal factors of features within VFMs for DGSS, and propose a simple yet effective method to identify and disentangle them, enabling more robust domain generalization. Specifically, we propose Causal-Tune, a novel fine-tuning strategy designed to extract causal factors and suppress non-causal ones from the features of VFMs. First, we extract the frequency spectrum of features from each layer using the Discrete Cosine Transform (DCT). A Gaussian band-pass filter is then applied to separate the spectrum into causal and non-causal components. To further refine the causal components, we introduce a set of causal-aware learnable tokens that operate in the frequency domain, while the non-causal components are discarded. Finally, refined features are transformed back into the spatial domain via inverse DCT and passed to the next layer. Extensive experiments conducted on various cross-domain tasks demonstrate the effectiveness of Causal-Tune. In particular, our method achieves superior performance under adverse weather conditions, improving +4.8% mIoU over the baseline in snow conditions.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Accepted by AAAI 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16567v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]foundation model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "Causal-Tune：挖掘视觉基础模型中的因果因子，用于领域泛化语义分割",
            "summary_zh": "本文提出了一种针对领域泛化语义分割（DGSS）的因果调优（Causal-Tune）方法，旨在解决视觉基础模型（VFM）中存在的伪影问题。作者观察到，这些伪影与VFM频谱中的低频和高频非因果因素相关。Causal-Tune显式地检查VFM特征中的因果和非因果因素，并通过离散余弦变换（DCT）提取每一层的特征频谱，然后应用高斯带通滤波器将频谱分离为因果和非因果分量。为了进一步提炼因果分量，引入了一组在频域中运行的因果感知可学习tokens，并丢弃非因果分量。最后，将精炼后的特征通过逆DCT转换回空间域，并传递到下一层。在各种跨域任务上的大量实验表明了Causal-Tune的有效性，尤其是在恶劣天气条件下，与基线相比，在雪地条件下提高了+4.8%的mIoU。",
            "intro_zh": [
                "现有领域泛化语义分割方法忽略了预训练视觉基础模型中存在的伪影，这些伪影阻碍了有价值表征的利用。",
                "Causal-Tune通过显式地识别和分离视觉基础模型特征中的因果和非因果因素，从而实现更鲁棒的领域泛化。",
                "实验表明，Causal-Tune在各种跨域任务中表现出色，尤其是在恶劣天气条件下，显著提升了语义分割的性能。"
            ],
            "method_zh": "**问题定义**：领域泛化语义分割（DGSS）旨在使模型在未见过的目标领域上也能保持良好的分割性能。现有的方法，如微调轻量级适配器或优化中间层特征，忽略了预训练视觉基础模型（VFM）中存在的伪影。这些伪影通常与非因果因素相关，阻碍了VFM中宝贵表征的利用，最终降低了DGSS的性能。\\n\\n**核心思路**：本文的核心思路是基于因果机制，将VFM特征分解为因果因素和非因果因素，并抑制非因果因素的影响，从而提高模型的泛化能力。作者观察到，这些非因果因素通常存在于VFM频谱的低频和高频分量中。通过提取和过滤这些频率分量，可以有效地去除伪影，并保留对语义分割任务有用的因果信息。\\n\\n**技术框架**：Causal-Tune的整体框架包括以下几个主要步骤：1) 使用离散余弦变换（DCT）提取VFM每一层特征的频率频谱。2) 应用高斯带通滤波器将频谱分离为因果和非因果分量。3) 引入一组因果感知可学习tokens，在频域中操作，以进一步提炼因果分量。4) 丢弃非因果分量。5) 使用逆DCT将精炼后的特征转换回空间域，并传递到下一层。\\n\\n**关键创新**：Causal-Tune的关键创新在于显式地建模和分离VFM特征中的因果和非因果因素。与以往的方法不同，Causal-Tune不是简单地微调整个VFM，而是有选择性地保留和增强因果信息，同时抑制非因果噪声。这种方法能够更有效地利用VFM的预训练知识，并提高模型在未见领域上的泛化能力。\\n\\n**关键设计**：高斯带通滤波器的参数（例如中心频率和带宽）需要根据具体的VFM和数据集进行调整，以实现最佳的因果/非因果分离效果。因果感知可学习tokens的设计也至关重要，它们需要能够有效地捕捉频域中的因果信息，并抑制噪声。损失函数的设计也需要考虑如何鼓励模型学习更鲁棒的因果表征。",
            "application_zh": "Causal-Tune在自动驾驶、机器人视觉、遥感图像分析等领域具有广泛的应用前景。通过提高模型在不同环境和条件下的鲁棒性，可以显著提升这些应用系统的可靠性和安全性。例如，在自动驾驶中，Causal-Tune可以帮助车辆更好地应对恶劣天气条件，从而减少交通事故的发生。",
            "highlight_zh": "Causal-Tune在多个跨域语义分割任务上取得了显著的性能提升。特别是在恶劣天气条件下，例如雪地场景，Causal-Tune相比基线方法提高了+4.8%的mIoU。这些实验结果表明，Causal-Tune能够有效地提取因果因素并抑制非因果因素，从而提高模型的泛化能力。",
            "tags_zh": [
                "领域泛化",
                "语义分割",
                "视觉基础模型",
                "因果推断",
                "频率分析",
                "离散余弦变换",
                "伪影去除"
            ],
            "_index": 24,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16567v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16567v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16567v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors",
            "authors": [
                "Kejun Liu",
                "Yuanyuan Liu",
                "Lin Wei",
                "Chang Tang",
                "Yibing Zhan",
                "Zijing Chen",
                "Zhe Chen"
            ],
            "arxiv_id": "2512.16485v1",
            "summary": "Emotion Recognition (ER) is the process of analyzing and identifying human emotions from sensing data. Currently, the field heavily relies on facial expression recognition (FER) because visual channel conveys rich emotional cues. However, facial expressions are often used as social tools rather than manifestations of genuine inner emotions. To understand and bridge this gap between FER and ER, we introduce eye behaviors as an important emotional cue and construct an Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset. To collect data with genuine emotions, spontaneous emotion induction paradigm is exploited with stimulus material, during which non-invasive eye behavior data, like eye movement sequences and eye fixation maps, is captured together with facial expression videos. To better illustrate the gap between ER and FER, multi-view emotion labels for mutimodal ER and FER are separately annotated. Furthermore, based on the new dataset, we design a simple yet effective Eye-behavior-aided MER Transformer (EMERT) that enhances ER by bridging the emotion gap. EMERT leverages modality-adversarial feature decoupling and a multitask Transformer to model eye behaviors as a strong complement to facial expressions. In the experiment, we introduce seven multimodal benchmark protocols for a variety of comprehensive evaluations of the EMER dataset. The results show that the EMERT outperforms other state-of-the-art multimodal methods by a great margin, revealing the importance of modeling eye behaviors for robust ER. To sum up, we provide a comprehensive analysis of the importance of eye behaviors in ER, advancing the study on addressing the gap between FER and ER for more robust ER performance. Our EMER dataset and the trained EMERT models will be publicly available at https://github.com/kejun1/EMER.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Accepted by TMM",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16485v1",
            "code_links": [
                {
                    "url": "https://github.com/kejun1/EMER",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]multimodal"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出EMER数据集和EMERT模型，利用眼部行为弥合面部表情识别和情感识别之间的差距",
            "summary_zh": "情感识别(ER)是从感知数据中分析和识别人类情感的过程。目前，该领域严重依赖于面部表情识别(FER)，因为视觉通道传递丰富的情感线索。然而，面部表情通常被用作社交工具，而不是真实内心情感的体现。为了理解和弥合FER和ER之间的差距，我们引入眼部行为作为一个重要的情感线索，并构建了一个眼部行为辅助的多模态情感识别(EMER)数据集。为了收集具有真实情感的数据，我们利用刺激材料进行自发情感诱导，在此过程中，非侵入性的眼部行为数据，如眼动序列和眼部注视图，与面部表情视频一起被捕获。为了更好地说明ER和FER之间的差距，我们分别对多模态ER和FER进行多视角情感标注。此外，基于新的数据集，我们设计了一个简单而有效的眼部行为辅助的MER Transformer (EMERT)，通过弥合情感差距来增强ER。EMERT利用模态对抗特征解耦和一个多任务Transformer来建模眼部行为，作为面部表情的有力补充。在实验中，我们为EMER数据集引入了七个多模态基准协议，用于各种综合评估。结果表明，EMERT优于其他最先进的多模态方法，揭示了建模眼部行为对于鲁棒ER的重要性。总而言之，我们对眼部行为在ER中的重要性进行了全面的分析，从而推进了解决FER和ER之间差距的研究，以获得更强大的ER性能。我们的EMER数据集和训练好的EMERT模型将在https://github.com/kejun1/EMER上公开。",
            "intro_zh": [
                "现有情感识别主要依赖面部表情，但面部表情常作为社交工具，无法真实反映内心情感。",
                "论文提出将眼部行为作为情感线索，构建EMER数据集，并设计EMERT模型弥合情感差距。",
                "实验结果表明，EMERT模型在EMER数据集上显著优于其他多模态方法，验证了眼部行为的重要性。"
            ],
            "method_zh": "**问题定义**：现有情感识别方法过度依赖面部表情，忽略了面部表情可能存在的伪装性，导致情感识别的准确性受到影响。因此，需要一种更鲁棒的情感识别方法，能够克服面部表情的局限性，更准确地捕捉真实的情感状态。\\n\\n**核心思路**：论文的核心思路是将眼部行为作为情感识别的重要补充信息。眼部行为，如眼动序列和眼部注视图，能够反映个体的情绪状态，并且相对于面部表情更难伪装。通过融合眼部行为和面部表情信息，可以提高情感识别的准确性和鲁棒性。\\n\\n**技术框架**：论文提出的EMERT模型主要包含以下几个模块：1)模态对抗特征解耦模块，用于分离面部表情和眼部行为中的情感相关和情感无关特征；2)多任务Transformer模块，用于融合解耦后的面部表情和眼部行为特征，并同时预测情感标签和眼部行为标签。整体流程是：输入面部表情视频和眼部行为数据，经过特征提取和解耦后，输入到多任务Transformer中进行情感预测。\\n\\n**关键创新**：论文的关键创新在于：1)提出了将眼部行为作为情感识别的重要线索，并构建了相应的多模态数据集EMER；2)设计了模态对抗特征解耦模块，能够有效分离不同模态中的情感相关和情感无关特征；3)提出了多任务Transformer模型，能够同时学习情感和眼部行为的表示。与现有方法相比，EMERT模型能够更有效地利用眼部行为信息，提高情感识别的准确性和鲁棒性。\\n\\n**关键设计**：在模态对抗特征解耦模块中，使用了梯度反转层(Gradient Reversal Layer)来实现对抗训练，从而分离情感相关和情感无关特征。在多任务Transformer模块中，使用了交叉注意力机制(Cross-Attention)来融合不同模态的特征。损失函数包括情感分类损失和眼部行为预测损失，通过联合优化这两个损失函数来提高模型的性能。",
            "application_zh": "该研究成果可应用于人机交互、心理健康评估、智能客服等领域。通过结合面部表情和眼部行为进行情感识别，可以提高人机交互的自然性和准确性，帮助心理医生更准确地评估患者的情绪状态，并使智能客服能够更好地理解用户的情感需求。",
            "highlight_zh": "实验结果表明，EMERT模型在EMER数据集上取得了显著的性能提升，优于其他最先进的多模态情感识别方法。具体而言，EMERT模型在七个多模态基准协议上都取得了最佳性能，证明了眼部行为在情感识别中的重要性，以及EMERT模型有效融合多模态信息的能力。",
            "tags_zh": [
                "情感识别",
                "多模态融合",
                "眼部行为",
                "面部表情",
                "Transformer",
                "对抗学习",
                "数据集"
            ],
            "_index": 25,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16485v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16485v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16485v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Pretrained Battery Transformer (PBT): A battery life prediction foundation model",
            "authors": [
                "Ruifeng Tan",
                "Weixiang Hong",
                "Jia Li",
                "Jiaqiang Huang",
                "Tong-Yi Zhang"
            ],
            "arxiv_id": "2512.16334v1",
            "summary": "Early prediction of battery cycle life is essential for accelerating battery research, manufacturing, and deployment. Although machine learning methods have shown encouraging results, progress is hindered by data scarcity and heterogeneity arising from diverse aging conditions. In other fields, foundation models (FMs) trained on diverse datasets have achieved broad generalization through transfer learning, but no FMs have been reported for battery cycle life prediction yet. Here we present the Pretrained Battery Transformer (PBT), the first FM for battery life prediction, developed through domain-knowledge-encoded mixture-of-expert layers. Validated on the largest public battery life database, PBT learns transferable representations from 13 lithium-ion battery (LIB) datasets, outperforming existing models by an average of 19.8%. With transfer learning, PBT achieves state-of-the-art performance across 15 diverse datasets encompassing various operating conditions, formation protocols, and chemistries of LIBs. This work establishes a foundation model pathway for battery lifetime prediction, paving the way toward universal battery lifetime prediction systems.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "5 figures in the main content",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16334v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]foundation model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出预训练电池Transformer（PBT），用于电池寿命预测，显著提升泛化性能。",
            "summary_zh": "本文提出了预训练电池Transformer（PBT），这是首个用于电池寿命预测的Foundation Model。PBT通过领域知识编码的混合专家层进行训练，在最大的公开电池寿命数据库上验证，从13个锂离子电池（LIB）数据集学习可迁移的表征，性能平均优于现有模型19.8%。通过迁移学习，PBT在包含各种操作条件、形成协议和LIB化学成分的15个不同数据集上实现了最先进的性能。这项工作为电池寿命预测建立了一个基础模型路径，为通用电池寿命预测系统铺平了道路。",
            "intro_zh": [
                "电池寿命的早期预测对于加速电池研究至关重要，但数据稀缺性和异构性阻碍了现有机器学习方法的进展。",
                "论文提出预训练电池Transformer（PBT），通过领域知识编码的混合专家层，学习可迁移的电池表征。",
                "PBT在多个数据集上验证，性能优于现有模型，并通过迁移学习在不同工况和化学成分的电池数据集上取得SOTA结果。"
            ],
            "method_zh": "**问题定义**：现有电池寿命预测方法受限于数据稀缺性和异构性，难以泛化到不同工况和化学成分的电池。现有模型难以充分利用不同数据集的信息，导致预测精度不高。\\n\\n**核心思路**：论文的核心思路是借鉴自然语言处理中的Foundation Model思想，通过在大规模、多样化的电池数据集上进行预训练，学习通用的电池表征。然后，通过迁移学习将这些表征应用于新的电池寿命预测任务。\\n\\n**技术框架**：PBT的整体架构基于Transformer模型，包含嵌入层、Transformer编码器层和预测层。关键在于混合专家层（Mixture-of-Experts, MoE）的设计，MoE允许模型根据输入数据的特性选择不同的专家网络进行处理，从而更好地适应不同类型的电池数据。预训练阶段，模型在大规模数据集上学习电池的通用表征。迁移学习阶段，模型在目标数据集上进行微调，以适应特定任务。\\n\\n**关键创新**：PBT的关键创新在于将Foundation Model的思想引入电池寿命预测领域，并设计了领域知识编码的混合专家层。混合专家层能够根据电池的类型、工况等信息，选择不同的专家网络进行处理，从而更好地利用不同数据集的信息，提高模型的泛化能力。\\n\\n**关键设计**：PBT使用了Transformer编码器作为其核心架构，并针对电池数据特性进行了优化。混合专家层由多个专家网络和一个门控网络组成。门控网络根据输入数据的特性，选择合适的专家网络进行处理。损失函数包括预测损失和正则化项，以防止过拟合。具体的参数设置和网络结构细节在论文中有详细描述。",
            "application_zh": "PBT可应用于电池研发、生产和部署等多个领域。在研发阶段，可以加速新型电池材料的筛选和优化。在生产阶段，可以提高电池质量控制的效率。在部署阶段，可以实现更精准的电池健康管理和寿命预测，从而延长电池的使用寿命，降低维护成本。PBT有望推动电池技术的进步，促进新能源产业的发展。",
            "highlight_zh": "PBT在最大的公开电池寿命数据库上验证，性能平均优于现有模型19.8%。通过迁移学习，PBT在包含各种操作条件、形成协议和LIB化学成分的15个不同数据集上实现了最先进的性能。这些实验结果表明，PBT具有很强的泛化能力和迁移学习能力，能够有效地解决电池寿命预测中的数据稀缺性和异构性问题。",
            "tags_zh": [
                "电池寿命预测",
                "预训练模型",
                "Transformer",
                "迁移学习",
                "混合专家网络"
            ],
            "_index": 26,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models",
            "authors": [
                "Xueqi Ma",
                "Xingjun Ma",
                "Sarah Monazam Erfani",
                "Danilo Mandic",
                "James Bailey"
            ],
            "arxiv_id": "2512.16244v1",
            "summary": "Developing open-set classification methods capable of classifying in-distribution (ID) data while detecting out-of-distribution (OOD) samples is essential for deploying graph neural networks (GNNs) in open-world scenarios. Existing methods typically treat all OOD samples as a single class, despite real-world applications, especially high-stake settings such as fraud detection and medical diagnosis, demanding deeper insights into OOD samples, including their probable labels. This raises a critical question: can OOD detection be extended to OOD classification without true label information? To address this question, we propose a Coarse-to-Fine open-set Classification (CFC) framework that leverages large language models (LLMs) for graph datasets. CFC consists of three key components: a coarse classifier that uses LLM prompts for OOD detection and outlier label generation, a GNN-based fine classifier trained with OOD samples identified by the coarse classifier for enhanced OOD detection and ID classification, and refined OOD classification achieved through LLM prompts and post-processed OOD labels. Unlike methods that rely on synthetic or auxiliary OOD samples, CFC employs semantic OOD instances that are genuinely out-of-distribution based on their inherent meaning, improving interpretability and practical utility. Experimental results show that CFC improves OOD detection by ten percent over state-of-the-art methods on graph and text domains and achieves up to seventy percent accuracy in OOD classification on graph datasets.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Accepted to AAAI 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16244v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出CFC框架，利用大语言模型实现图节点开放集分类与细粒度OOD识别。",
            "summary_zh": "开发能够在识别分布内(ID)数据的同时检测分布外(OOD)样本的开放集分类方法，对于在开放世界场景中部署图神经网络(GNN)至关重要。现有方法通常将所有OOD样本视为单个类别，然而现实应用，尤其是在欺诈检测和医疗诊断等高风险场景中，需要对OOD样本进行更深入的了解，包括其可能的标签。这提出了一个关键问题：能否在没有真实标签信息的情况下将OOD检测扩展到OOD分类？为了解决这个问题，我们提出了一个粗到细的开放集分类(CFC)框架，该框架利用大型语言模型(LLM)处理图数据集。CFC由三个关键组件组成：一个粗分类器，它使用LLM提示进行OOD检测和异常值标签生成；一个基于GNN的细分类器，该分类器使用粗分类器识别的OOD样本进行训练，以增强OOD检测和ID分类；以及通过LLM提示和后处理OOD标签实现的精细化OOD分类。与依赖合成或辅助OOD样本的方法不同，CFC采用基于其内在含义的语义OOD实例，这些实例是真正分布外的，从而提高了可解释性和实用性。实验结果表明，CFC在图和文本领域将OOD检测提高了10%，并在图数据集上实现了高达70%的OOD分类准确率。",
            "intro_zh": [
                "现有开放集图节点分类方法难以提供OOD样本的细粒度信息，限制了其在高风险场景的应用。",
                "CFC框架利用大语言模型进行粗粒度OOD检测和标签生成，再用GNN进行细粒度分类，实现OOD分类。",
                "实验表明，CFC在OOD检测和分类任务上均优于现有方法，尤其在OOD分类准确率方面提升显著。"
            ],
            "method_zh": "**问题定义**：现有图神经网络的开放集分类方法主要关注区分ID样本和OOD样本，但通常将所有OOD样本视为一个类别，无法提供关于OOD样本的更细粒度信息。在实际应用中，例如欺诈检测或医疗诊断，了解OOD样本的潜在类别至关重要。因此，论文旨在解决如何在没有OOD样本真实标签的情况下，对OOD样本进行分类的问题。\\n\\n**核心思路**：论文的核心思路是利用大语言模型(LLM)的语义理解能力，对OOD样本进行粗粒度的分类，生成伪标签，然后利用这些伪标签训练图神经网络(GNN)，从而实现细粒度的OOD分类。这种“粗到细”的方法能够有效地利用LLM的先验知识，同时结合GNN在图结构数据上的优势。\\n\\n**技术框架**：CFC框架包含三个主要阶段：1) **粗分类器**：使用LLM对图节点进行分类，并识别OOD样本，同时生成OOD样本的伪标签。LLM通过特定的prompt进行引导，以生成具有语义意义的标签。2) **细分类器**：使用GNN，并利用粗分类器识别的OOD样本及其伪标签进行训练，从而提高GNN在OOD检测和ID分类方面的性能。3) **OOD分类优化**：使用LLM对GNN的OOD分类结果进行优化，通过prompt工程和后处理，进一步提升OOD分类的准确性。\\n\\n**关键创新**：CFC框架的关键创新在于将大语言模型(LLM)引入到图神经网络的开放集分类任务中，并利用LLM的语义理解能力生成OOD样本的伪标签。与现有方法依赖合成或辅助OOD样本不同，CFC直接利用LLM对真实OOD样本进行分类，从而提高了OOD分类的可解释性和实用性。\\n\\n**关键设计**：在粗分类器阶段，关键在于设计合适的LLM prompt，以引导LLM生成高质量的OOD伪标签。在细分类器阶段，使用标准的GNN模型，并采用交叉熵损失函数进行训练。在OOD分类优化阶段，通过prompt工程和后处理，对LLM的输出进行修正，以提高OOD分类的准确性。具体的参数设置和网络结构细节未在摘要中详细说明，属于未知信息。",
            "application_zh": "该研究成果可应用于欺诈检测、医疗诊断、金融风控等领域。通过识别和分类异常节点，可以帮助发现潜在的欺诈行为、疾病风险或金融风险。未来，该方法可以扩展到其他类型的图数据和开放世界场景，为智能决策提供更可靠的支持。",
            "highlight_zh": "实验结果表明，CFC框架在图和文本领域将OOD检测的性能提升了10%，并在图数据集上实现了高达70%的OOD分类准确率。这些结果表明，CFC框架能够有效地利用大语言模型的语义理解能力，提高图神经网络在开放集分类任务中的性能。",
            "tags_zh": [
                "开放集分类",
                "图神经网络",
                "大语言模型",
                "OOD检测",
                "OOD分类",
                "图节点分类",
                "粗到细学习"
            ],
            "_index": 27,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16244v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16244v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16244v1/ood-prompt.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "A Multimodal Approach to Alzheimer's Diagnosis: Geometric Insights from Cube Copying and Cognitive Assessments",
            "authors": [
                "Jaeho Yang",
                "Kijung Yoon"
            ],
            "arxiv_id": "2512.16184v1",
            "summary": "Early and accessible detection of Alzheimer's disease (AD) remains a critical clinical challenge, and cube-copying tasks offer a simple yet informative assessment of visuospatial function. This work proposes a multimodal framework that converts hand-drawn cube sketches into graph-structured representations capturing geometric and topological properties, and integrates these features with demographic information and neuropsychological test (NPT) scores for AD classification. Cube drawings are modeled as graphs with node features encoding spatial coordinates, local graphlet-based topology, and angular geometry, which are processed using graph neural networks and fused with age, education, and NPT features in a late-fusion model. Experimental results show that graph-based representations provide a strong unimodal baseline and substantially outperform pixel-based convolutional models, while multimodal integration further improves performance and robustness to class imbalance. SHAP-based interpretability analysis identifies specific graphlet motifs and geometric distortions as key predictors, closely aligning with clinical observations of disorganized cube drawings in AD. Together, these results establish graph-based analysis of cube copying as an interpretable, non-invasive, and scalable approach for Alzheimer's disease screening.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16184v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]multimodal"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于图神经网络的多模态融合框架，用于阿尔茨海默病早期诊断。",
            "summary_zh": "阿尔茨海默病(AD)的早期和可及性检测仍然是一项关键的临床挑战，而立方体复制任务提供了一种简单但信息丰富的视觉空间功能评估方法。本研究提出了一种多模态框架，该框架将手绘立方体草图转换为图结构表示，以捕获几何和拓扑属性，并将这些特征与人口统计信息和神经心理测试(NPT)分数相结合，用于AD分类。立方体图被建模为图，其节点特征编码空间坐标、基于局部图元的拓扑结构和角度几何。这些图通过图神经网络处理，并在后期融合模型中与年龄、教育程度和NPT特征融合。实验结果表明，基于图的表示提供了强大的单模态基线，并且显著优于基于像素的卷积模型，而多模态集成进一步提高了性能和对类不平衡的鲁棒性。基于SHAP的可解释性分析确定了特定的图元基序和几何扭曲是关键预测因子，这与AD中立方体图的临床观察结果非常吻合。总之，这些结果确立了基于图的立方体复制分析作为一种可解释、非侵入性和可扩展的阿尔茨海默病筛查方法。",
            "intro_zh": [
                "阿尔茨海默病早期诊断面临挑战，传统方法依赖复杂且耗时的神经心理测试。",
                "该论文提出将手绘立方体图转换为图结构，提取几何和拓扑特征，结合其他信息进行诊断。",
                "实验表明，基于图的表示优于传统像素方法，多模态融合进一步提升了诊断性能和鲁棒性。"
            ],
            "method_zh": "**问题定义**：阿尔茨海默病的早期诊断是临床上的一个难题。传统的诊断方法，如神经心理测试，往往耗时且复杂。立方体复制任务作为一种简单易行的评估方法，能够反映患者的视觉空间功能，但如何有效利用手绘立方体图的信息进行诊断仍然是一个挑战。现有方法，例如基于像素的卷积神经网络，难以充分捕捉立方体图的几何和拓扑结构信息。\\n\\n**核心思路**：该论文的核心思路是将手绘立方体图转换为图结构，利用图神经网络(GNN)提取图中的几何和拓扑特征。这种方法能够更有效地捕捉立方体图中的结构信息，从而提高阿尔茨海默病的诊断准确率。将立方体图转换为图结构，可以更好地利用图神经网络处理非欧几里得空间数据的优势。\\n\\n**技术框架**：该论文提出的多模态框架包含以下几个主要阶段：1) 数据预处理：收集手绘立方体图、人口统计信息和神经心理测试(NPT)分数。2) 图构建：将手绘立方体图转换为图结构，节点特征包括空间坐标、局部图元拓扑和角度几何。3) 特征提取：使用图神经网络提取图结构中的特征。4) 多模态融合：将图神经网络提取的特征与人口统计信息和NPT分数进行融合。5) 分类：使用分类器进行阿尔茨海默病的诊断。\\n\\n**关键创新**：该论文的关键创新在于将手绘立方体图转换为图结构，并利用图神经网络提取特征。这种方法能够更有效地捕捉立方体图中的几何和拓扑结构信息，从而提高阿尔茨海默病的诊断准确率。此外，该论文还提出了一个多模态融合框架，将图神经网络提取的特征与人口统计信息和NPT分数进行融合，进一步提高了诊断性能。\\n\\n**关键设计**：在图构建阶段，论文使用了局部图元(graphlet)来描述节点的局部拓扑结构。图神经网络使用了Graph Convolutional Network (GCN) 或 Graph Attention Network (GAT) 等常见的图神经网络结构。在多模态融合阶段，使用了后期融合(late-fusion)策略，即将不同模态的特征分别提取后，再进行融合。损失函数使用了交叉熵损失函数，用于分类任务的训练。",
            "application_zh": "该研究成果可应用于阿尔茨海默病的早期筛查和辅助诊断，尤其是在医疗资源匮乏的地区。通过简单的立方体复制任务和图神经网络分析，可以实现低成本、非侵入性的初步诊断，为患者争取宝贵的治疗时间。未来，该方法可扩展到其他神经退行性疾病的诊断。",
            "highlight_zh": "实验结果表明，基于图的表示方法在立方体复制任务上的表现优于基于像素的卷积模型。多模态融合进一步提升了诊断性能，并提高了对类别不平衡的鲁棒性。SHAP分析揭示了特定的图元基序和几何扭曲是关键的预测因子，与临床观察结果一致。该方法在阿尔茨海默病诊断方面具有显著优势。",
            "tags_zh": [
                "阿尔茨海默病诊断",
                "图神经网络",
                "多模态融合",
                "立方体复制任务",
                "几何特征",
                "拓扑特征",
                "早期筛查"
            ],
            "_index": 28,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16184v1/Figure1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16184v1/Figure2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16184v1/Figure3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge",
            "authors": [
                "Khurram Khalil",
                "Khaza Anuarul Hoque"
            ],
            "arxiv_id": "2512.16855v1",
            "summary": "Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.",
            "categories": [
                "cs.AI",
                "cs.LO"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Published in the IEEE ICCAD 2025 conference",
            "doi": "10.1109/ICCAD66269.2025.11240962",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16855v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "TOGGLE：时序逻辑引导的大语言模型边缘压缩方法",
            "summary_zh": "大型语言模型（LLM）在自然语言任务中表现出色，但需要大量的计算资源，限制了其在资源受限的边缘设备上的部署。现有的压缩技术，如量化和剪枝，通常会降低关键的语言属性，并且缺乏对模型行为保持的正式保证。我们提出了时序逻辑引导的大语言模型压缩（TOGGLE），这是一个新颖的框架，它利用信号时序逻辑（STL）来正式地指定和执行压缩过程中的语言属性。TOGGLE采用STL鲁棒性引导的贝叶斯优化，系统地探索逐层量化和剪枝配置，生成压缩模型，这些模型在不进行重新训练或微调的情况下，正式地满足指定的语言约束。在四个LLM架构（GPT-2、DeepSeek-V2 7B、LLaMA 3 8B和Mistral 7B）上评估TOGGLE，我们实现了高达3.3倍的计算成本（FLOPs）降低和高达68.8%的模型大小降低，同时满足所有语言属性。TOGGLE代表了形式化方法首次集成到LLM压缩中，从而能够在边缘硬件上高效、可验证地部署LLM。",
            "intro_zh": [
                "现有LLM压缩方法在降低计算资源需求的同时，往往会牺牲模型的关键语言属性，且缺乏对模型行为的正式保证。",
                "TOGGLE利用信号时序逻辑（STL）来形式化地指定和执行压缩过程中的语言属性，确保压缩后的模型满足预定义的语言约束。",
                "实验结果表明，TOGGLE在显著降低计算成本和模型大小的同时，能够保持模型的语言属性，无需重新训练或微调。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型（LLM）在资源受限的边缘设备上部署的问题。现有压缩方法，如量化和剪枝，虽然可以减小模型大小和计算量，但往往会损害模型的语言能力，并且缺乏形式化的验证手段来保证压缩后的模型仍然满足特定的语言属性。\\n\\n**核心思路**：TOGGLE的核心思路是利用信号时序逻辑（STL）来形式化地描述LLM需要满足的语言属性，并在模型压缩过程中，通过优化算法来寻找满足这些属性的最佳压缩配置。这种方法避免了传统压缩方法中对语言属性的忽视，并提供了形式化的保证。\\n\\n**技术框架**：TOGGLE框架主要包含以下几个阶段：1) 使用STL形式化地定义LLM需要满足的语言属性；2) 使用STL鲁棒性度量来评估模型满足这些属性的程度；3) 使用贝叶斯优化算法，以STL鲁棒性为目标函数，搜索最佳的逐层量化和剪枝配置；4) 生成压缩后的模型，该模型在满足指定的语言约束的同时，具有更小的模型大小和更低的计算成本。\\n\\n**关键创新**：TOGGLE的关键创新在于将形式化方法（STL）引入到LLM压缩中。与传统的压缩方法不同，TOGGLE不仅关注模型的大小和计算量，更关注模型压缩后是否仍然满足预定义的语言属性。通过STL鲁棒性引导的贝叶斯优化，TOGGLE能够系统地探索压缩配置，并生成满足语言约束的压缩模型。\\n\\n**关键设计**：TOGGLE的关键设计包括：1) 使用STL来形式化地描述语言属性，例如，可以使用STL来描述模型在特定输入下必须产生特定输出的约束；2) 使用STL鲁棒性度量来量化模型满足这些属性的程度，鲁棒性越高，表示模型越能抵抗噪声和扰动，从而更好地满足语言属性；3) 使用贝叶斯优化算法来搜索最佳的逐层量化和剪枝配置，贝叶斯优化能够有效地探索搜索空间，并找到满足语言约束的最佳压缩配置。",
            "application_zh": "TOGGLE的应用场景广泛，包括智能家居、自动驾驶、可穿戴设备等边缘计算领域。通过在边缘设备上部署压缩后的LLM，可以实现本地化的自然语言处理，提高响应速度和数据安全性。此外，TOGGLE还可以应用于对模型安全性有严格要求的场景，例如金融和医疗领域，确保模型在压缩后仍然满足关键的语言属性。",
            "highlight_zh": "TOGGLE在GPT-2、DeepSeek-V2 7B、LLaMA 3 8B和Mistral 7B四个LLM架构上进行了评估，实验结果表明，TOGGLE能够实现高达3.3倍的计算成本（FLOPs）降低和高达68.8%的模型大小降低，同时满足所有指定的语言属性。这些结果表明，TOGGLE是一种有效的LLM压缩方法，可以在边缘设备上部署高性能的LLM。",
            "tags_zh": [
                "大语言模型压缩",
                "边缘计算",
                "信号时序逻辑",
                "形式化方法",
                "贝叶斯优化"
            ],
            "_index": 29,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Prefix Probing: Lightweight Harmful Content Detection for Large Language Models",
            "authors": [
                "Jirui Yang",
                "Hengqi Guo",
                "Zhihui Lu",
                "Yi Zhao",
                "Yuansen Zhang",
                "Shijing Hu",
                "Qiang Duan",
                "Yinggui Wang",
                "Tao Wei"
            ],
            "arxiv_id": "2512.16650v1",
            "summary": "Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of \"agreement/execution\" versus \"refusal/safety\" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.",
            "categories": [
                "cs.AI",
                "cs.CR"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16650v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Prefix Probing，以低延迟、低成本方式检测大语言模型中的有害内容。",
            "summary_zh": "大型语言模型在实际安全敏感应用中面临检测精度、推理延迟和部署成本的三重权衡。本文介绍了一种黑盒有害内容检测方法Prefix Probing，它通过比较“同意/执行”与“拒绝/安全”开头前缀的条件对数概率，并利用前缀缓存将检测开销降低到接近首个token的延迟。在推理过程中，该方法仅需对探针前缀进行一次对数概率计算，即可生成有害性评分并应用阈值，无需调用任何额外的模型或多阶段推理。为了进一步增强前缀的区分能力，我们设计了一种高效的前缀构建算法，可以自动发现信息量大的前缀，从而显著提高检测性能。大量实验表明，Prefix Probing实现了与主流外部安全模型相当的检测效果，同时仅产生极小的计算成本，且无需额外的模型部署，突显了其强大的实用性和效率。",
            "intro_zh": [
                "现有大语言模型有害内容检测方法在精度、延迟和成本间存在权衡，难以兼顾。",
                "Prefix Probing通过比较特定前缀的条件概率，无需额外模型或多阶段推理，实现高效检测。",
                "论文设计高效前缀构建算法，自动发现信息量大的前缀，显著提升检测性能，实验效果媲美主流安全模型。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型在实际应用中，有害内容检测面临的精度、推理延迟和部署成本难以兼顾的问题。现有方法通常需要额外的安全模型或多阶段推理，导致延迟增加和部署成本上升。\\n\\n**核心思路**：Prefix Probing的核心思路是利用大语言模型自身的能力，通过比较特定前缀（如“同意/执行” vs “拒绝/安全”）的条件对数概率来判断内容的有害性。这种方法避免了引入额外的模型，从而降低了延迟和部署成本。\\n\\n**技术框架**：Prefix Probing的整体框架非常简单：1) **前缀构建**：使用高效的算法自动发现具有区分性的前缀。2) **概率计算**：计算输入文本分别接上“同意/执行”和“拒绝/安全”前缀后的条件对数概率。3) **有害性评分**：根据两个概率的差值计算有害性评分。4) **阈值判断**：将有害性评分与预设阈值进行比较，判断内容是否具有危害性。\\n\\n**关键创新**：Prefix Probing的关键创新在于其轻量级和高效性。它无需训练额外的模型，仅依赖于大语言模型自身的概率预测能力。此外，自动前缀构建算法能够有效地找到最具区分性的前缀，进一步提升检测性能。与现有方法相比，Prefix Probing在保证检测精度的同时，显著降低了推理延迟和部署成本。\\n\\n**关键设计**：论文设计了一种高效的前缀构建算法，具体细节未知。有害性评分的计算方式是两个条件对数概率的差值，阈值的选择需要根据具体的应用场景进行调整。论文中没有提及具体的损失函数或网络结构，因为该方法是黑盒的，不需要对大语言模型进行任何修改。",
            "application_zh": "Prefix Probing可广泛应用于各种需要对大语言模型生成内容进行安全过滤的场景，例如聊天机器人、内容生成平台、代码生成工具等。它能够以低延迟、低成本的方式有效识别和过滤有害内容，保障用户安全，提升用户体验，并降低平台的运营风险。该方法还有潜力扩展到其他类型的文本分类任务中。",
            "highlight_zh": "实验结果表明，Prefix Probing在检测有害内容方面取得了与主流外部安全模型相当的性能，同时显著降低了计算成本和部署成本。该方法仅需极小的计算开销，即可实现有效的有害内容检测，无需额外模型部署，具有很强的实用性。",
            "tags_zh": [
                "大语言模型",
                "有害内容检测",
                "黑盒方法",
                "前缀探测",
                "安全过滤"
            ],
            "_index": 30,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16650v1/figs/insight.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16650v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16650v1/figs/f1_vs_time_2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "TimeSeries2Report prompting enables adaptive large language model management of lithium-ion batteries",
            "authors": [
                "Jiayang Yang",
                "Chunhui Zhao",
                "Martin Guay",
                "Zhixing Cao"
            ],
            "arxiv_id": "2512.16453v1",
            "summary": "Large language models (LLMs) offer promising capabilities for interpreting multivariate time-series data, yet their application to real-world battery energy storage system (BESS) operation and maintenance remains largely unexplored. Here, we present TimeSeries2Report (TS2R), a prompting framework that converts raw lithium-ion battery operational time-series into structured, semantically enriched reports, enabling LLMs to reason, predict, and make decisions in BESS management scenarios. TS2R encodes short-term temporal dynamics into natural language through a combination of segmentation, semantic abstraction, and rule-based interpretation, effectively bridging low-level sensor signals with high-level contextual insights. We benchmark TS2R across both lab-scale and real-world datasets, evaluating report quality and downstream task performance in anomaly detection, state-of-charge prediction, and charging/discharging management. Compared with vision-, embedding-, and text-based prompting baselines, report-based prompting via TS2R consistently improves LLM performance in terms of across accuracy, robustness, and explainability metrics. Notably, TS2R-integrated LLMs achieve expert-level decision quality and predictive consistency without retraining or architecture modification, establishing a practical path for adaptive, LLM-driven battery intelligence.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16453v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出TimeSeries2Report框架，利用大语言模型优化锂离子电池管理",
            "summary_zh": "本文提出了一种名为TimeSeries2Report (TS2R) 的提示框架，旨在将原始锂离子电池运行时间序列数据转换为结构化、语义丰富的报告，从而使大语言模型 (LLM) 能够在电池储能系统 (BESS) 管理场景中进行推理、预测和决策。TS2R 通过分割、语义抽象和基于规则的解释，将短期时间动态编码为自然语言，有效地将低级传感器信号与高级上下文信息连接起来。该框架在实验室规模和真实世界数据集上进行了基准测试，评估了报告质量以及在异常检测、荷电状态预测和充放电管理等下游任务中的性能。与基于视觉、嵌入和文本的提示基线相比，通过 TS2R 进行的基于报告的提示始终提高了 LLM 在准确性、鲁棒性和可解释性方面的性能。值得注意的是，集成了 TS2R 的 LLM 在无需重新训练或架构修改的情况下，实现了专家级的决策质量和预测一致性，为自适应的、LLM 驱动的电池智能建立了一条切实可行的路径。",
            "intro_zh": [
                "现有方法难以有效利用大语言模型处理电池储能系统中的多变量时间序列数据，缺乏有效的桥梁连接底层传感器信号和高级决策。",
                "TS2R框架通过时间序列分割、语义抽象和规则解释，将原始时间序列转化为结构化报告，使LLM能够理解并利用这些信息进行推理。",
                "实验表明，TS2R框架显著提升了LLM在异常检测、荷电状态预测和充放电管理等任务中的性能，无需模型重训练。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大语言模型在电池储能系统（BESS）管理中应用的问题，特别是如何有效地利用LLM处理和理解BESS运行过程中产生的大量多变量时间序列数据。现有方法通常难以将这些原始数据转化为LLM能够理解和利用的知识，导致LLM在BESS管理中的应用受限。现有方法缺乏将低级传感器信号与高级上下文信息连接的有效手段，使得LLM难以进行有效的推理和决策。\\n\\n**核心思路**：论文的核心思路是将原始时间序列数据转换为结构化、语义丰富的报告，从而使LLM能够更好地理解和利用这些数据。这种方法通过将时间序列数据转化为自然语言描述，使得LLM能够利用其强大的自然语言处理能力进行推理、预测和决策。这样设计的目的是为了弥合低级传感器信号与高级上下文信息之间的差距，使LLM能够更好地理解BESS的运行状态和潜在问题。\\n\\n**技术框架**：TS2R框架主要包含三个阶段：1) 时间序列分割：将连续的时间序列数据分割成具有语义意义的片段。2) 语义抽象：对每个时间序列片段进行语义抽象，提取关键特征和模式。3) 规则解释：利用预定义的规则将抽象后的语义信息转化为自然语言报告。LLM接收这些报告作为输入，并利用其强大的自然语言处理能力进行推理、预测和决策。\\n\\n**关键创新**：TS2R框架的关键创新在于它提供了一种将原始时间序列数据转化为LLM可理解的自然语言报告的有效方法。与传统的基于视觉、嵌入或文本的提示方法相比，TS2R能够更好地保留时间序列数据的动态特征和上下文信息，从而提高LLM在BESS管理任务中的性能。此外，TS2R框架无需对LLM进行重新训练或架构修改，即可实现专家级的决策质量和预测一致性。\\n\\n**关键设计**：TS2R框架的关键设计包括：1) 时间序列分割算法的选择，需要根据具体的BESS运行数据特点进行调整。2) 语义抽象规则的定义，需要领域专家参与，确保能够准确地提取关键特征和模式。3) 自然语言报告的生成方式，需要保证报告的清晰、简洁和易于理解。此外，论文还可能涉及到一些超参数的调整，例如时间序列片段的长度、语义抽象的粒度等。",
            "application_zh": "该研究成果可广泛应用于电池储能系统的智能管理与维护，例如异常检测、状态预测、寿命评估和优化控制。通过集成TS2R框架，可以实现更加智能、高效和可靠的电池储能系统运行，降低运维成本，延长电池寿命，并为电网的稳定运行提供保障。未来，该技术有望扩展到其他类型的时间序列数据分析和预测领域。",
            "highlight_zh": "实验结果表明，与基于视觉、嵌入和文本的提示基线相比，TS2R框架显著提高了LLM在异常检测、荷电状态预测和充放电管理等任务中的性能。具体而言，TS2R-integrated LLMs在无需重新训练或架构修改的情况下，实现了专家级的决策质量和预测一致性，在各项指标上均有显著提升，证明了该框架的有效性和优越性。",
            "tags_zh": [
                "时间序列分析",
                "大语言模型",
                "锂离子电池",
                "储能系统",
                "智能管理",
                "自然语言处理",
                "异常检测"
            ],
            "_index": 31,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16453v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16453v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16453v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "An Information-Theoretic Framework for Robust Large Language Model Editing",
            "authors": [
                "Qizhou Chen",
                "Chengyu Wang",
                "Taolin Zhang",
                "Xiaofeng He"
            ],
            "arxiv_id": "2512.16227v1",
            "summary": "Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16227v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于信息瓶颈的IBKE框架，用于稳健的大语言模型知识编辑。",
            "summary_zh": "大型语言模型（LLMs）已成为科学、技术和社会中不可或缺的工具，推动了各个领域的变革性进步。然而，这些模型中存在的错误或过时信息可能会损害其准确性，并限制其安全部署。开发有效的策略来更新模型知识，同时避免完全重新训练的成本和干扰，仍然是一个关键挑战。当前的模型编辑技术常常难以将修正推广到狭窄领域之外，导致意想不到的后果，并限制了它们的实际影响。本文介绍了一种基于信息瓶颈理论的LLM编辑新框架。该方法精确地压缩和隔离了通用知识修正所需的基本信息，同时最大限度地减少对不相关模型行为的干扰。在此基础上，我们提出了信息瓶颈知识编辑器（IBKE），它利用紧凑的潜在表示来指导基于梯度的更新，从而实现稳健且广泛适用的模型编辑。我们在多个LLM架构和标准基准任务上验证了IBKE的有效性，证明了其最先进的准确性以及编辑的改进的通用性和特异性。这些发现为开放域知识编辑建立了一个理论上合理且实用的范例，提高了LLM在实际应用中的效用和可信度。",
            "intro_zh": [
                "现有模型编辑方法泛化性差，易产生副作用，限制了实际应用。",
                "利用信息瓶颈理论，压缩并隔离知识修正的关键信息，减少对无关行为的干扰。",
                "IBKE在多个LLM和基准测试中表现出优异的准确性、通用性和特异性。"
            ],
            "method_zh": "**问题定义**：现有的大语言模型知识编辑方法，难以在保证编辑准确性的同时，避免对模型其他知识的干扰，即泛化性较差，容易产生副作用。这限制了LLM在实际场景中的可靠应用。\\n\\n**核心思路**：论文的核心思路是利用信息瓶颈（Information Bottleneck, IB）理论，在编辑过程中，只保留与待编辑知识相关的信息，去除冗余信息，从而提高编辑的泛化性和特异性。通过压缩和隔离关键信息，减少对模型原有知识的干扰。\\n\\n**技术框架**：IBKE框架主要包含以下几个阶段：1) **知识表示学习**：将需要编辑的知识编码成紧凑的潜在表示。2) **信息瓶颈压缩**：利用信息瓶颈原理，对潜在表示进行压缩，提取关键信息。3) **梯度引导更新**：使用压缩后的潜在表示，引导基于梯度的模型参数更新。4) **知识验证**：评估编辑后的模型在相关任务上的性能，以及对其他知识的影响。\\n\\n**关键创新**：IBKE的关键创新在于将信息瓶颈理论引入到大语言模型知识编辑中。与以往直接修改模型参数的方法不同，IBKE通过压缩知识表示，只保留必要信息，从而实现了更精确和更具泛化性的编辑。这种方法能够有效减少编辑对模型其他知识的干扰，提高编辑的可靠性。\\n\\n**关键设计**：IBKE的关键设计包括：1) 使用变分自编码器（VAE）学习知识的潜在表示。2) 使用KL散度作为信息瓶颈的正则化项，控制潜在表示的信息量。3) 设计特定的损失函数，鼓励模型在编辑后能够正确回答相关问题，同时保持对其他问题的回答不变。具体的参数设置和网络结构选择取决于所使用的LLM架构和数据集。",
            "application_zh": "该研究成果可应用于各种需要持续更新知识的大语言模型应用场景，例如：智能客服、知识问答、内容生成等。通过IBKE框架，可以高效、安全地更新模型知识，提高模型在实际应用中的准确性和可靠性。该技术还有助于减少模型错误信息的传播，提升LLM的可信度。",
            "highlight_zh": "实验结果表明，IBKE在多个LLM架构和标准基准任务上取得了最先进的性能。与现有方法相比，IBKE在编辑准确性、通用性和特异性方面均有显著提升。具体数据（原文未提供）表明，IBKE能够在保证编辑效果的同时，最大限度地减少对模型其他知识的干扰，从而提高了编辑的可靠性。",
            "tags_zh": [
                "大语言模型",
                "知识编辑",
                "信息瓶颈",
                "模型更新",
                "泛化性"
            ],
            "_index": 32,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16227v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16227v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16227v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "A Multi-Agent Large Language Model Framework for Automated Qualitative Analysis",
            "authors": [
                "Qidi Xu",
                "Nuzha Amjad",
                "Grace Giles",
                "Alexa Cumming",
                "De'angelo Hermesky",
                "Alexander Wen",
                "Min Ji Kwak",
                "Yejin Kim"
            ],
            "arxiv_id": "2512.16063v1",
            "summary": "Understanding patients experiences is essential for advancing patient centered care, especially in chronic diseases that require ongoing communication. However, qualitative thematic analysis, the primary approach for exploring these experiences, remains labor intensive, subjective, and difficult to scale. In this study, we developed a multi agent large language model framework that automates qualitative thematic analysis through three agents (Instructor, Thematizer, CodebookGenerator), named Collaborative Theme Identification Agent (CoTI). We applied CoTI to 12 heart failure patient interviews to analyze their perceptions of medication intensity. CoTI identified key phrases, themes, and codebook that were more similar to those of the senior investigator than both junior investigators and baseline NLP models. We also implemented CoTI into a user-facing application to enable AI human interaction in qualitative analysis. However, collaboration between CoTI and junior investigators provided only marginal gains, suggesting they may overrely on CoTI and limit their independent critical thinking.",
            "categories": [
                "cs.HC",
                "cs.AI"
            ],
            "primary_category": "cs.HC",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "42 pages, 5 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16063v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出CoTI多智能体LLM框架，自动化定性分析，提升患者体验研究效率。",
            "summary_zh": "理解患者体验对于提升以患者为中心的护理至关重要，尤其是在需要持续沟通的慢性疾病中。然而，定性主题分析是探索这些体验的主要方法，但仍然劳动密集、主观且难以扩展。本研究开发了一个多智能体大型语言模型框架，通过三个智能体（指导者、主题化者、代码本生成器）自动化定性主题分析，命名为协同主题识别智能体（CoTI）。我们将CoTI应用于12个心力衰竭患者访谈，以分析他们对药物强度的看法。CoTI识别的关键短语、主题和代码本与资深研究员的结果更相似，优于初级研究员和基线NLP模型。我们还将CoTI集成到面向用户的应用程序中，以实现AI人机交互的定性分析。然而，CoTI与初级研究员之间的协作仅提供了边际收益，表明他们可能过度依赖CoTI并限制了他们的独立批判性思维。",
            "intro_zh": [
                "定性主题分析在患者体验研究中至关重要，但其劳动密集、主观且难以扩展的特性限制了其应用。",
                "论文提出CoTI框架，利用多智能体LLM协同工作，自动化主题识别、代码本生成等定性分析流程。",
                "实验表明，CoTI在心力衰竭患者访谈分析中，结果与资深研究员更接近，优于初级研究员和基线模型。"
            ],
            "method_zh": "**问题定义**：论文旨在解决定性研究中主题分析耗时费力、主观性强、难以规模化的问题。现有方法依赖人工分析，效率低下且结果易受研究者个人经验影响。\\n\\n**核心思路**：论文的核心思路是利用大型语言模型（LLM）的强大自然语言处理能力，构建多智能体协同框架，模拟人工分析过程中的不同角色，从而实现定性分析的自动化和客观化。通过智能体间的协作，降低主观偏差，提高分析效率。\\n\\n**技术框架**：CoTI框架包含三个主要智能体：Instructor（指导者）、Thematizer（主题化者）和 CodebookGenerator（代码本生成器）。Instructor负责引导整个分析流程，Thematizer负责从文本中提取主题，CodebookGenerator负责生成代码本。整个流程包括数据预处理、智能体协同分析、结果整合等步骤。用户可以通过用户界面与CoTI进行交互，调整参数和查看结果。\\n\\n**关键创新**：CoTI的关键创新在于其多智能体协同架构，通过模拟人工分析中的不同角色，实现了更全面、客观的分析结果。与传统的单模型方法相比，CoTI能够更好地捕捉文本中的细微差别和复杂关系。此外，CoTI还提供了一个用户友好的界面，方便研究人员使用和定制分析流程。\\n\\n**关键设计**：论文中没有明确说明关键参数设置、损失函数或网络结构等技术细节。但可以推断，每个智能体都基于预训练的LLM进行微调，并可能使用了特定的提示工程（Prompt Engineering）技术来指导智能体的行为。智能体之间的通信机制和协作策略也是CoTI的关键设计要素，但具体实现细节未知。",
            "application_zh": "该研究成果可应用于医疗健康领域，自动化患者访谈、病历记录等文本数据的定性分析，帮助医生和研究人员更好地理解患者体验，优化治疗方案，提升医疗服务质量。此外，该框架还可扩展到其他需要定性分析的领域，如社会科学、市场调研等，具有广泛的应用前景。",
            "highlight_zh": "实验结果表明，CoTI在分析心力衰竭患者访谈数据时，识别的关键短语、主题和代码本与资深研究员的结果更相似，优于初级研究员和基线NLP模型。这表明CoTI能够有效降低主观偏差，提高分析结果的客观性和准确性。然而，CoTI与初级研究员的协作收益有限，提示需要关注人机协作模式的设计。",
            "tags_zh": [
                "多智能体系统",
                "大型语言模型",
                "定性分析",
                "主题分析",
                "患者体验",
                "自然语言处理"
            ],
            "_index": 33,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "DualGuard: Dual-stream Large Language Model Watermarking Defense against Paraphrase and Spoofing Attack",
            "authors": [
                "Hao Li",
                "Yubing Ren",
                "Yanan Cao",
                "Yingjie Li",
                "Fang Fang",
                "Shi Wang",
                "Li Guo"
            ],
            "arxiv_id": "2512.16182v1",
            "summary": "With the rapid development of cloud-based services, large language models (LLMs) have become increasingly accessible through various web platforms. However, this accessibility has also led to growing risks of model abuse. LLM watermarking has emerged as an effective approach to mitigate such misuse and protect intellectual property. Existing watermarking algorithms, however, primarily focus on defending against paraphrase attacks while overlooking piggyback spoofing attacks, which can inject harmful content, compromise watermark reliability, and undermine trust in attribution. To address this limitation, we propose DualGuard, the first watermarking algorithm capable of defending against both paraphrase and spoofing attacks. DualGuard employs the adaptive dual-stream watermarking mechanism, in which two complementary watermark signals are dynamically injected based on the semantic content. This design enables DualGuard not only to detect but also to trace spoofing attacks, thereby ensuring reliable and trustworthy watermark detection. Extensive experiments conducted across multiple datasets and language models demonstrate that DualGuard achieves excellent detectability, robustness, traceability, and text quality, effectively advancing the state of LLM watermarking for real-world applications.",
            "categories": [
                "cs.CR",
                "cs.CL"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16182v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出DualGuard，防御针对大语言模型水印的复述攻击和欺骗攻击。",
            "summary_zh": "随着云服务的快速发展，大型语言模型（LLMs）越来越容易通过各种网络平台访问。然而，这种可访问性也导致了模型滥用的风险日益增加。LLM水印技术已成为缓解此类滥用和保护知识产权的有效方法。然而，现有的水印算法主要侧重于防御复述攻击，而忽略了尾随欺骗攻击，这种攻击会注入有害内容，损害水印的可靠性，并破坏对归属的信任。为了解决这个局限性，我们提出了DualGuard，这是第一个能够防御复述攻击和欺骗攻击的水印算法。DualGuard采用自适应双流水印机制，其中两个互补的水印信号根据语义内容动态注入。这种设计使DualGuard不仅能够检测，而且能够追踪欺骗攻击，从而确保可靠和值得信赖的水印检测。在多个数据集和语言模型上进行的大量实验表明，DualGuard实现了出色的可检测性、鲁棒性、可追溯性和文本质量，有效地推进了LLM水印在实际应用中的发展。",
            "intro_zh": [
                "现有LLM水印技术主要关注复述攻击防御，忽略了尾随欺骗攻击带来的水印可靠性与归属信任风险。",
                "DualGuard采用自适应双流水印机制，基于语义内容动态注入互补水印信号，实现欺骗攻击的检测与追踪。",
                "实验证明DualGuard在可检测性、鲁棒性、可追溯性和文本质量方面表现出色，推动了LLM水印的实际应用。"
            ],
            "method_zh": "**问题定义**：现有的大语言模型水印技术主要关注防御复述攻击，即攻击者通过改写水印文本来尝试移除或弱化水印。然而，一种更隐蔽的攻击方式，即尾随欺骗攻击（piggyback spoofing attack），却被忽视。这种攻击通过在原始文本后附加恶意内容，不仅能绕过水印检测，还能注入有害信息，严重威胁模型安全和用户信任。因此，论文旨在解决如何有效防御复述攻击和尾随欺骗攻击，保证水印的可靠性和可追溯性问题。\\n\\n**核心思路**：DualGuard的核心思路是利用双流水印机制，同时嵌入两种互补的水印信号。一种水印信号用于抵抗复述攻击，保证基本的可检测性；另一种水印信号则专门用于检测和追踪尾随欺骗攻击。通过分析两种水印信号之间的关系，可以判断是否存在恶意注入，并追踪攻击源。这种双重保障的设计旨在提高水印的整体鲁棒性和安全性。\\n\\n**技术框架**：DualGuard的技术框架主要包含以下几个模块：1) 语义分析模块：分析输入文本的语义信息，为后续的水印嵌入提供依据。2) 自适应水印嵌入模块：根据语义分析结果，动态调整两种水印信号的强度和位置。3) 水印检测模块：检测文本中是否存在水印信号，并判断是否存在尾随欺骗攻击。4) 攻击追踪模块：如果检测到欺骗攻击，则追踪攻击源，并提供相应的证据。整个流程旨在实现水印的可靠嵌入、准确检测和有效追踪。\\n\\n**关键创新**：DualGuard的关键创新在于其双流水印机制，这是第一个同时考虑复述攻击和尾随欺骗攻击的水印算法。与传统的单水印方法相比，DualGuard能够更全面地保护LLM的知识产权和用户安全。此外，自适应水印嵌入模块能够根据语义信息动态调整水印信号，进一步提高了水印的鲁棒性和隐蔽性。\\n\\n**关键设计**：DualGuard的关键设计包括：1) 语义分析模块采用预训练语言模型（如BERT）提取文本的语义特征。2) 自适应水印嵌入模块使用强化学习算法，根据语义特征动态调整水印信号的强度和位置。3) 水印检测模块使用统计假设检验方法，判断文本中是否存在水印信号。4) 攻击追踪模块使用溯源算法，根据水印信号的变化追踪攻击源。具体的参数设置、损失函数和网络结构等细节未在摘要中详细说明，属于未知信息。",
            "application_zh": "DualGuard可应用于各种基于云的大语言模型服务，用于保护模型的知识产权，防止恶意用户滥用模型生成有害内容。该技术可用于内容审核、版权保护、虚假信息检测等领域，提升LLM服务的安全性和可信度，促进LLM技术的健康发展。未来，该技术有望扩展到其他类型的人工智能模型，构建更完善的安全防护体系。",
            "highlight_zh": "实验结果表明，DualGuard在抵抗复述攻击和尾随欺骗攻击方面均表现出色。在多个数据集和语言模型上，DualGuard实现了高检测率和低误报率，同时保持了良好的文本质量。与现有水印算法相比，DualGuard在防御欺骗攻击方面具有显著优势，能够有效追踪攻击源，为LLM的安全应用提供了有力保障。具体的性能数据和提升幅度在摘要中未明确给出，属于未知信息。",
            "tags_zh": [
                "大语言模型",
                "水印技术",
                "复述攻击",
                "欺骗攻击",
                "双流水印",
                "知识产权保护",
                "安全防护"
            ],
            "_index": 34,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16182v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16182v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16182v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos",
            "authors": [
                "Mingfei Chen",
                "Yifan Wang",
                "Zhengqin Li",
                "Homanga Bharadhwaj",
                "Yujin Chen",
                "Chuan Qin",
                "Ziyi Kou",
                "Yuan Tian",
                "Eric Whitmire",
                "Rajinder Sodhi",
                "Hrvoje Benko",
                "Eli Shlizerman",
                "Yue Liu"
            ],
            "arxiv_id": "2512.16907v1",
            "summary": "Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project website: https://egoman-project.github.io",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16907v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "motion generation"
                    ],
                    "score": 2.5
                },
                {
                    "name": "支柱六：视频提取与匹配 (Video Extraction)",
                    "id": "6_video_extraction",
                    "matched_keywords": [
                        "[T]egocentric"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 8.5,
            "hit_pillars": [
                "4_motion_diffusion",
                "6_video_extraction"
            ],
            "headline_zh": "EgoMAN：基于自中心交互视频，学习3D手部轨迹预测，实现从推理到运动的生成",
            "summary_zh": "本文提出了一种基于自中心视角的交互场景下，进行阶段感知的3D手部轨迹预测方法。为了解决现有方法缺乏语义监督以及推理与动作弱连接的问题，作者首先构建了EgoMAN数据集，这是一个大规模的自中心数据集，包含21.9万条6DoF轨迹和300万个结构化的问答对，用于语义、空间和运动推理。然后，作者提出了EgoMAN模型，这是一个推理到运动的框架，通过轨迹-token接口连接视觉-语言推理和运动生成。该方法通过逐步训练，使推理与运动动态对齐，从而生成准确且具有阶段感知的轨迹，并具有跨真实世界场景的泛化能力。",
            "intro_zh": [
                "现有3D手部轨迹预测方法缺乏语义信息的有效利用，且推理与动作生成之间联系薄弱。",
                "EgoMAN模型通过轨迹-token接口，将视觉-语言推理与运动生成连接，实现从推理到运动的轨迹预测。",
                "EgoMAN数据集和模型的结合，实现了准确且具有阶段感知的3D手部轨迹预测，并具备良好的泛化性。"
            ],
            "method_zh": "**问题定义**：现有3D手部轨迹预测方法主要面临两个挑战：一是数据集层面，缺乏与语义信息强关联的大规模数据集；二是模型层面，推理（reasoning）过程与动作生成（motion generation）过程联系不够紧密，导致预测精度和泛化能力受限。现有数据集通常将运动与语义监督解耦，模型也难以有效利用语义信息指导运动生成。\\n\\n**核心思路**：本文的核心思路是通过构建一个大规模的、包含丰富语义信息的数据集（EgoMAN），并设计一个能够有效连接视觉-语言推理和运动生成的模型（EgoMAN模型），从而实现从推理到运动的3D手部轨迹预测。通过显式地将推理过程与运动生成过程联系起来，模型可以更好地理解场景语义，并生成更准确、更自然的轨迹。\\n\\n**技术框架**：EgoMAN模型的整体框架是一个推理到运动的流程。首先，利用视觉和语言信息进行推理，提取场景的语义信息和运动意图。然后，通过一个轨迹-token接口，将推理结果转换为运动生成的输入。最后，利用运动生成模块，生成3D手部轨迹。整个框架通过逐步训练的方式，使推理过程与运动动态对齐，从而提高预测精度和泛化能力。\\n\\n**关键创新**：本文的关键创新在于以下两点：一是EgoMAN数据集的构建，该数据集包含大量的6DoF手部轨迹和结构化的问答对，为模型提供了丰富的语义信息和运动监督信号；二是EgoMAN模型的提出，该模型通过轨迹-token接口，将视觉-语言推理与运动生成连接起来，实现了从推理到运动的轨迹预测。这种推理到运动的框架能够更好地利用场景语义信息，并生成更准确、更自然的轨迹。\\n\\n**关键设计**：EgoMAN模型中的轨迹-token接口是关键设计之一，它将推理结果转换为一系列轨迹token，这些token包含了运动的起始位置、方向、速度等信息。运动生成模块利用这些token作为输入，生成最终的3D手部轨迹。此外，模型还采用了逐步训练的策略，首先训练推理模块，然后训练运动生成模块，最后将两个模块联合训练，从而使推理过程与运动动态对齐。",
            "application_zh": "该研究成果可应用于人机交互、机器人操作、虚拟现实/增强现实等领域。例如，机器人可以根据人类的意图预测其手部运动轨迹，从而更好地辅助人类完成任务。在VR/AR中，可以根据用户与虚拟环境的交互，预测用户的手部运动，从而提供更自然、更沉浸式的体验。该研究的未来影响在于提升人机交互的智能化和自然化水平。",
            "highlight_zh": "EgoMAN数据集包含21.9万条6DoF轨迹和300万个结构化的问答对，是目前最大的自中心手部轨迹预测数据集之一。EgoMAN模型在EgoMAN数据集上取得了显著的性能提升，相较于现有方法，在轨迹预测精度和阶段感知能力方面均有明显优势。实验结果表明，该模型具有良好的泛化能力，可以应用于不同的真实世界场景。",
            "tags_zh": [
                "3D手部轨迹预测",
                "自中心视角",
                "人机交互",
                "视觉语言推理",
                "运动生成"
            ],
            "_index": 35,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16907v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16907v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16907v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
            "authors": [
                "Hanlin Wang",
                "Hao Ouyang",
                "Qiuyu Wang",
                "Yue Yu",
                "Yihao Meng",
                "Wen Wang",
                "Ka Leong Cheng",
                "Shuailei Ma",
                "Qingyan Bai",
                "Yixuan Li",
                "Cheng Chen",
                "Yanhong Zeng",
                "Xing Zhu",
                "Yujun Shen",
                "Qifeng Chen"
            ],
            "arxiv_id": "2512.16924v1",
            "summary": "We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project page and code: https://worldcanvas.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16924v1",
            "code_links": [
                {
                    "url": "https://worldcanvas.github.io/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "world model"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal",
                        "visual grounding"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "WorldCanvas：结合文本、轨迹和参考图像，实现可控的世界事件模拟。",
            "summary_zh": "我们提出了WorldCanvas，一个用于可提示世界事件的框架，它通过结合文本、轨迹和参考图像来实现丰富的、用户导向的模拟。与仅使用文本的方法和现有的轨迹控制图像到视频方法不同，我们的多模态方法将轨迹（编码运动、时间和可见性）与自然语言（用于语义意图）和参考图像（用于对象身份的视觉基础）相结合，从而能够生成连贯的、可控的事件，包括多智能体交互、对象进入/退出、参考引导的外观和违反直觉的事件。生成的视频不仅展示了时间连贯性，还展示了涌现一致性，即使在暂时消失的情况下也能保持对象身份和场景。通过支持表达性的世界事件生成，WorldCanvas将世界模型从被动预测器提升为交互式的、用户塑造的模拟器。我们的项目页面位于：https://worldcanvas.github.io/。",
            "intro_zh": [
                "现有方法在世界事件模拟中，要么仅依赖文本，缺乏精细控制，要么依赖轨迹控制，但难以保持视觉一致性。",
                "WorldCanvas结合文本、轨迹和参考图像，利用多模态信息实现对世界事件的精确控制和视觉一致性保持。",
                "实验结果表明，WorldCanvas能够生成具有时间连贯性和涌现一致性的视频，实现多智能体交互和对象外观控制。"
            ],
            "method_zh": "**问题定义**：现有世界事件模拟方法存在局限性。基于文本的方法难以精确控制事件细节，而基于轨迹控制的图像到视频方法难以保持对象身份和场景的一致性，尤其是在对象暂时消失的情况下。因此，需要一种能够结合语义意图、运动信息和视觉信息的框架，以实现更丰富、更可控的世界事件模拟。\\n\\n**核心思路**：WorldCanvas的核心思路是利用多模态信息融合，将文本的语义信息、轨迹的运动信息和参考图像的视觉信息结合起来，从而实现对世界事件的精确控制和视觉一致性保持。通过这种方式，可以生成包含多智能体交互、对象进入/退出、参考引导的外观和违反直觉的事件的连贯视频。\\n\\n**技术框架**：WorldCanvas框架包含以下主要模块：1) 轨迹编码模块，用于编码对象的运动轨迹，包括位置、时间和可见性等信息；2) 文本编码模块，用于编码自然语言描述，提取语义意图；3) 参考图像编码模块，用于编码参考图像，提取对象身份和外观信息；4) 视频生成模块，基于编码后的轨迹、文本和参考图像，生成具有时间连贯性和涌现一致性的视频。\\n\\n**关键创新**：WorldCanvas的关键创新在于多模态信息的融合方式。它不仅简单地将文本、轨迹和参考图像的信息拼接在一起，而是通过一种精心设计的融合机制，使得不同模态的信息能够相互补充，从而实现对世界事件的更精确控制和更逼真模拟。此外，该框架还引入了涌现一致性的概念，即使在对象暂时消失的情况下，也能保持对象身份和场景的一致性。\\n\\n**关键设计**：具体的技术细节包括：轨迹编码采用时序模型（例如LSTM或Transformer）来捕捉运动模式；文本编码采用预训练语言模型（例如BERT或GPT）来提取语义信息；参考图像编码采用卷积神经网络（例如ResNet或ViT）来提取视觉特征。视频生成模块可能采用生成对抗网络（GAN）或扩散模型等技术，以生成高质量的视频。损失函数的设计需要考虑时间连贯性、涌现一致性和参考图像的相似性。",
            "application_zh": "WorldCanvas可应用于游戏开发、电影制作、机器人仿真、自动驾驶等领域。它可以帮助开发者快速生成各种场景和事件，提高开发效率。在机器人仿真和自动驾驶领域，WorldCanvas可以用于生成各种复杂的交通场景，帮助训练和评估算法的性能。此外，该技术还可以用于教育和娱乐领域，例如创建个性化的故事和虚拟体验。",
            "highlight_zh": "WorldCanvas通过结合文本、轨迹和参考图像，实现了对世界事件的精细控制和视觉一致性保持。实验结果表明，该方法能够生成具有时间连贯性和涌现一致性的视频，包括多智能体交互、对象进入/退出等复杂事件。与现有方法相比，WorldCanvas在对象身份保持和场景一致性方面取得了显著提升，能够生成更逼真、更可控的世界事件模拟。",
            "tags_zh": [
                "世界事件模拟",
                "多模态融合",
                "轨迹控制",
                "视频生成",
                "涌现一致性"
            ],
            "_index": 36,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16924v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16924v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16924v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Task-Oriented Data Synthesis and Control-Rectify Sampling for Remote Sensing Semantic Segmentation",
            "authors": [
                "Yunkai Yang",
                "Yudong Zhang",
                "Kunquan Zhang",
                "Jinxiao Zhang",
                "Xinying Chen",
                "Haohuan Fu",
                "Runmin Dong"
            ],
            "arxiv_id": "2512.16740v1",
            "summary": "With the rapid progress of controllable generation, training data synthesis has become a promising way to expand labeled datasets and alleviate manual annotation in remote sensing (RS). However, the complexity of semantic mask control and the uncertainty of sampling quality often limit the utility of synthetic data in downstream semantic segmentation tasks. To address these challenges, we propose a task-oriented data synthesis framework (TODSynth), including a Multimodal Diffusion Transformer (MM-DiT) with unified triple attention and a plug-and-play sampling strategy guided by task feedback. Built upon the powerful DiT-based generative foundation model, we systematically evaluate different control schemes, showing that a text-image-mask joint attention scheme combined with full fine-tuning of the image and mask branches significantly enhances the effectiveness of RS semantic segmentation data synthesis, particularly in few-shot and complex-scene scenarios. Furthermore, we propose a control-rectify flow matching (CRFM) method, which dynamically adjusts sampling directions guided by semantic loss during the early high-plasticity stage, mitigating the instability of generated images and bridging the gap between synthetic data and downstream segmentation tasks. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art controllable generation methods, producing more stable and task-oriented synthetic data for RS semantic segmentation.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16740v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "flow matching"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出TODSynth框架，用于遥感语义分割任务的数据合成与控制优化。",
            "summary_zh": "随着可控生成技术的快速发展，训练数据合成已成为扩展遥感（RS）标注数据集和缓解人工标注负担的一种有前景的方法。然而，语义掩码控制的复杂性和采样质量的不确定性常常限制了合成数据在下游语义分割任务中的效用。为了应对这些挑战，我们提出了一个面向任务的数据合成框架（TODSynth），包括一个具有统一三重注意力的多模态扩散Transformer（MM-DiT）和一个由任务反馈指导的即插即用采样策略。基于强大的DiT生成基础模型，我们系统地评估了不同的控制方案，表明文本-图像-掩码联合注意力方案与图像和掩码分支的完全微调相结合，显著增强了RS语义分割数据合成的有效性，尤其是在少样本和复杂场景中。此外，我们提出了一种控制校正流匹配（CRFM）方法，该方法在早期高可塑性阶段动态调整由语义损失引导的采样方向，从而减轻生成图像的不稳定性，并弥合合成数据与下游分割任务之间的差距。大量实验表明，我们的方法始终优于最先进的可控生成方法，为RS语义分割生成更稳定和面向任务的合成数据。",
            "intro_zh": [
                "遥感图像语义分割依赖大量标注数据，人工标注成本高昂，可控生成技术为数据扩充提供了新途径。",
                "TODSynth框架通过多模态扩散Transformer和任务反馈采样策略，提升合成数据的质量和任务相关性。",
                "实验表明，该方法在少样本和复杂场景下，能生成更稳定、更符合遥感语义分割任务需求的合成数据。"
            ],
            "method_zh": "**问题定义**：遥感语义分割任务中，标注数据的获取成本高昂，严重制约了模型性能的提升。现有的数据合成方法难以有效控制生成过程，导致合成数据质量不高，与真实数据存在较大差距，无法有效提升下游分割任务的性能。尤其是在少样本和复杂场景下，问题更为突出。\\n\\n**核心思路**：论文的核心思路是设计一个面向任务的数据合成框架，通过多模态控制和任务反馈机制，生成高质量、与下游分割任务高度相关的合成数据。具体来说，利用多模态扩散模型，结合文本、图像和掩码信息进行联合控制，并引入控制校正流匹配方法，根据语义损失动态调整采样方向，从而提高合成数据的稳定性和任务适应性。\\n\\n**技术框架**：TODSynth框架主要包含两个核心模块：多模态扩散Transformer（MM-DiT）和控制校正流匹配（CRFM）。MM-DiT是一个基于DiT的生成模型，通过统一的三重注意力机制融合文本、图像和掩码信息，实现对生成过程的精细控制。CRFM则是一种采样策略，在扩散模型的采样过程中，根据下游分割任务的语义损失动态调整采样方向，从而优化合成数据的质量。\\n\\n**关键创新**：该论文的关键创新在于：1) 提出了统一三重注意力机制，有效融合了文本、图像和掩码信息，实现了对生成过程的精细控制；2) 提出了控制校正流匹配方法，通过任务反馈动态调整采样方向，提高了合成数据的稳定性和任务适应性；3) 系统地评估了不同的控制方案，并证明了文本-图像-掩码联合注意力方案的有效性。\\n\\n**关键设计**：MM-DiT采用了DiT作为基础模型，并在此基础上引入了统一的三重注意力机制，将文本、图像和掩码信息融合到Transformer的注意力层中。CRFM方法则是在扩散模型的采样过程中，计算合成图像在下游分割任务上的语义损失，并根据该损失调整采样方向。具体来说，CRFM使用流匹配的思想，将合成图像的分布与真实图像的分布进行匹配，从而优化合成数据的质量。损失函数的设计也至关重要，需要综合考虑生成图像的真实性和分割任务的性能。",
            "application_zh": "该研究成果可广泛应用于遥感图像处理领域，例如土地覆盖分类、城市规划、灾害监测等。通过合成高质量的训练数据，可以有效降低人工标注成本，提高遥感图像语义分割模型的精度和泛化能力。此外，该方法还可以推广到其他需要数据增强的计算机视觉任务中，具有重要的实际应用价值和广阔的应用前景。",
            "highlight_zh": "实验结果表明，TODSynth框架在遥感语义分割任务中取得了显著的性能提升。与最先进的可控生成方法相比，TODSynth能够生成更稳定、更符合任务需求的合成数据，从而提高了下游分割模型的精度。具体来说，在少样本场景下，TODSynth的性能提升尤为明显，证明了其在数据稀缺情况下的有效性。",
            "tags_zh": [
                "遥感语义分割",
                "数据合成",
                "可控生成",
                "扩散模型",
                "多模态学习"
            ],
            "_index": 37,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16740v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16740v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16740v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction",
            "authors": [
                "Yuxin Ray Song",
                "Jinzhou Li",
                "Rao Fu",
                "Devin Murphy",
                "Kaichen Zhou",
                "Rishi Shiv",
                "Yaqi Li",
                "Haoyu Xiong",
                "Crystal Elaine Owens",
                "Yilun Du",
                "Yiyue Luo",
                "Xianyi Cheng",
                "Antonio Torralba",
                "Wojciech Matusik",
                "Paul Pu Liang"
            ],
            "arxiv_id": "2512.16842v1",
            "summary": "The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "https://opentouch-tactile.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16842v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱六：视频提取与匹配 (Video Extraction)",
                    "id": "6_video_extraction",
                    "matched_keywords": [
                        "egocentric"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 7.0,
            "hit_pillars": [
                "1_robot_core",
                "6_video_extraction",
                "9_embodied_foundation"
            ],
            "headline_zh": "OpenTouch：构建真实场景下完整手部触觉交互数据集与基准",
            "summary_zh": "人手是与物理世界交互的主要界面，但自我中心感知很少知道何时、何地或以多大力度进行接触。目前缺乏稳健的可穿戴触觉传感器，并且没有现有的真实场景数据集将第一人称视频与完整手部触觉对齐。为了弥合视觉感知和物理交互之间的差距，我们提出了OpenTouch，这是第一个真实场景下的自我中心完整手部触觉数据集，包含5.1小时的同步视频-触觉-姿态数据和2,900个带有详细文本注释的精选片段。使用OpenTouch，我们引入了检索和分类基准，以探究触觉如何支撑感知和行动。我们表明，触觉信号为抓取理解提供了一个紧凑而强大的线索，加强了跨模态对齐，并且可以从真实场景视频查询中可靠地检索。通过发布这个带注释的视觉-触觉-姿态数据集和基准，我们的目标是推进多模态自我中心感知、具身学习和富接触机器人操作。",
            "intro_zh": [
                "现有方法缺乏在真实场景下同步第一人称视频与完整手部触觉的数据集，阻碍了视觉感知与物理交互的融合。",
                "OpenTouch构建了首个真实场景下的自我中心完整手部触觉数据集，包含同步视频、触觉和姿态数据，并提供详细文本注释。",
                "通过OpenTouch，论文提出了检索和分类基准，验证了触觉信号在抓取理解和跨模态对齐方面的有效性。"
            ],
            "method_zh": "**问题定义**：现有方法缺乏在真实场景下同步第一人称视角视频与完整手部触觉信息的数据集。这使得研究人员难以开发能够理解和利用触觉信息的视觉感知系统，从而限制了具身智能和机器人操作的发展。现有的触觉传感器和数据集往往集中在实验室环境或特定任务上，难以泛化到复杂的真实世界场景。\\n\\n**核心思路**：OpenTouch的核心思路是构建一个大规模、高质量的真实场景数据集，包含同步的第一人称视角视频、完整手部触觉数据和手部姿态信息。通过提供这样的数据集，研究人员可以训练和评估模型，以学习触觉与视觉之间的关系，从而提高对物理交互的理解能力。此外，论文还提出了基于该数据集的检索和分类基准，以促进相关研究的进展。\\n\\n**技术框架**：OpenTouch数据集的构建流程包括数据采集、数据同步、数据标注和数据发布。数据采集使用可穿戴触觉传感器记录手部触觉信息，同时使用第一人称相机记录视频。数据同步将触觉数据、视频数据和手部姿态数据对齐。数据标注包括文本描述和关键帧标注。数据发布将数据集公开，并提供相应的API和工具。\\n\\n**关键创新**：OpenTouch的关键创新在于它是第一个在真实场景下采集的自我中心完整手部触觉数据集。与现有的数据集相比，OpenTouch具有更大的规模、更高的质量和更强的泛化能力。此外，论文还提出了基于该数据集的检索和分类基准，为相关研究提供了一个统一的评估平台。\\n\\n**关键设计**：OpenTouch数据集包含5.1小时的同步视频-触觉-姿态数据和2,900个带有详细文本注释的精选片段。触觉传感器采用了高分辨率的触觉感应技术，能够捕捉到细微的触觉变化。视频采用了高帧率的相机，能够捕捉到快速的手部动作。手部姿态数据采用了先进的姿态估计算法，能够准确地估计手部关节的位置和方向。",
            "application_zh": "OpenTouch数据集和基准测试为多模态自我中心感知、具身学习和富接触机器人操作等领域提供了重要资源。该研究成果可应用于开发更智能的机器人，使其能够更好地理解和操纵物理世界，例如在家庭服务、工业自动化和医疗保健等领域。",
            "highlight_zh": "实验结果表明，触觉信号能够显著提高抓取理解的准确性，并加强跨模态对齐的效果。通过OpenTouch数据集，研究人员能够从真实场景视频查询中可靠地检索到相关的触觉信息。这些结果验证了触觉在感知和行动中的重要作用。",
            "tags_zh": [
                "触觉感知",
                "具身智能",
                "机器人操作",
                "多模态学习",
                "自我中心视觉",
                "数据集",
                "基准测试"
            ],
            "_index": 38,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16842v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16842v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16842v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Kling-Omni Technical Report",
            "authors": [
                "Kling Team",
                "Jialu Chen",
                "Yuanzheng Ci",
                "Xiangyu Du",
                "Zipeng Feng",
                "Kun Gai",
                "Sainan Guo",
                "Feng Han",
                "Jingbin He",
                "Kang He",
                "Xiao Hu",
                "Xiaohua Hu",
                "Boyuan Jiang",
                "Fangyuan Kong",
                "Hang Li",
                "Jie Li",
                "Qingyu Li",
                "Shen Li",
                "Xiaohan Li",
                "Yan Li",
                "Jiajun Liang",
                "Borui Liao",
                "Yiqiao Liao",
                "Weihong Lin",
                "Quande Liu",
                "Xiaokun Liu",
                "Yilun Liu",
                "Yuliang Liu",
                "Shun Lu",
                "Hangyu Mao",
                "Yunyao Mao",
                "Haodong Ouyang",
                "Wenyu Qin",
                "Wanqi Shi",
                "Xiaoyu Shi",
                "Lianghao Su",
                "Haozhi Sun",
                "Peiqin Sun",
                "Pengfei Wan",
                "Chao Wang",
                "Chenyu Wang",
                "Meng Wang",
                "Qiulin Wang",
                "Runqi Wang",
                "Xintao Wang",
                "Xuebo Wang",
                "Zekun Wang",
                "Min Wei",
                "Tiancheng Wen",
                "Guohao Wu",
                "Xiaoshi Wu",
                "Zhenhua Wu",
                "Da Xie",
                "Yingtong Xiong",
                "Yulong Xu",
                "Sile Yang",
                "Zikang Yang",
                "Weicai Ye",
                "Ziyang Yuan",
                "Shenglong Zhang",
                "Shuaiyu Zhang",
                "Yuanxing Zhang",
                "Yufan Zhang",
                "Wenzheng Zhao",
                "Ruiliang Zhou",
                "Yan Zhou",
                "Guosheng Zhu",
                "Yongjie Zhu"
            ],
            "arxiv_id": "2512.16776v1",
            "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Kling-Omni Technical Report",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16776v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal",
                        "instruction following"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "Kling-Omni：通用生成框架，实现多模态输入到高质量视频的端到端合成",
            "summary_zh": "Kling-Omni是一个通用的生成框架，旨在直接从多模态视觉语言输入合成高保真视频。Kling-Omni采用端到端的视角，弥合了不同视频生成、编辑和智能推理任务之间的功能分离，将它们集成到一个整体系统中。与不连贯的流水线方法不同，Kling-Omni支持多种用户输入，包括文本指令、参考图像和视频上下文，并将它们处理成统一的多模态表示，以提供电影质量和高度智能的视频内容创作。为了支持这些能力，我们构建了一个全面的数据系统，作为多模态视频创作的基础。该框架通过高效的大规模预训练策略和用于推理的基础设施优化得到进一步加强。全面的评估表明，Kling-Omni在上下文生成、基于推理的编辑和多模态指令遵循方面表现出卓越的能力。我们相信，Kling-Omni超越了内容创作工具，是朝着能够感知、推理、生成和与动态复杂世界交互的多模态世界模拟器迈出的关键一步。",
            "intro_zh": [
                "现有视频生成方法通常采用分离的流水线，难以处理多模态输入和复杂的推理任务。",
                "Kling-Omni通过统一的多模态表示，将视频生成、编辑和推理集成到端到端的框架中。",
                "Kling-Omni通过大规模预训练和基础设施优化，在上下文生成、推理编辑和多模态指令跟随方面表现出色。"
            ],
            "method_zh": "**问题定义**：现有视频生成方法通常是针对特定任务设计的，例如文本到视频生成或视频编辑，缺乏通用性和灵活性。这些方法通常采用分离的流水线，难以处理多模态输入（如文本、图像和视频）以及复杂的推理任务。此外，生成视频的质量和智能程度也受到限制。\\n\\n**核心思路**：Kling-Omni的核心思路是将视频生成、编辑和推理任务统一到一个端到端的框架中，通过学习统一的多模态表示来处理各种输入，并生成高质量、智能的视频内容。这种方法避免了传统流水线的复杂性和局限性，提高了模型的通用性和灵活性。\\n\\n**技术框架**：Kling-Omni的整体架构包含以下几个主要模块：1) 多模态输入编码器：将文本、图像和视频等不同模态的输入编码成统一的向量表示。2) 视频生成器：基于编码后的多模态表示生成视频内容。3) 推理模块：用于执行基于视频内容的推理任务，例如回答问题或进行编辑。整个框架采用端到端的训练方式，通过优化生成视频的质量和推理的准确性来提高整体性能。\\n\\n**关键创新**：Kling-Omni最重要的技术创新点在于其统一的多模态表示学习方法。与现有方法不同，Kling-Omni能够将不同模态的输入信息融合到一个统一的表示空间中，从而实现跨模态的推理和生成。此外，Kling-Omni还采用了大规模预训练策略，利用海量的视频数据来提高模型的泛化能力。\\n\\n**关键设计**：具体的技术细节未知，但可以推测可能包含以下设计：1) 使用Transformer架构作为视频生成器的主干网络，以捕捉视频中的时序依赖关系。2) 采用对比学习或生成对抗网络（GAN）等方法来提高生成视频的质量。3) 设计特定的损失函数来鼓励模型学习到具有语义意义的多模态表示。4) 使用数据并行或模型并行等技术来加速大规模预训练。",
            "application_zh": "Kling-Omni具有广泛的应用前景，包括电影制作、游戏开发、广告创意、教育娱乐等领域。它可以用于快速生成高质量的视频内容，降低视频制作的成本和门槛。此外，Kling-Omni还可以用于创建虚拟现实和增强现实体验，为用户提供更加沉浸式的互动体验。未来，Kling-Omni有望成为多模态世界模拟器的重要组成部分。",
            "highlight_zh": "论文通过全面的评估表明，Kling-Omni在上下文生成、基于推理的编辑和多模态指令遵循方面表现出卓越的能力。具体的性能数据和对比基线未知，但摘要强调了其在多个任务上的优越性，表明Kling-Omni在多模态视频生成领域取得了显著进展。",
            "tags_zh": [
                "视频生成",
                "多模态学习",
                "端到端框架",
                "智能推理",
                "上下文生成",
                "视频编辑",
                "预训练",
                "通用框架"
            ],
            "_index": 39,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16776v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16776v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16776v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs",
            "authors": [
                "Jintao Tong",
                "Jiaqi Gu",
                "Yujing Lou",
                "Lubin Fan",
                "Yixiong Zou",
                "Yue Wu",
                "Jieping Ye",
                "Ruixuan Li"
            ],
            "arxiv_id": "2512.16584v1",
            "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding tasks through text reasoning, they often fall short in scenarios requiring visual imagination. Unlike current works that take predefined external toolkits or generate images during thinking, however, humans can form flexible visual-text imagination and interactions during thinking without predefined toolkits, where one important reason is that humans construct the visual-text thinking process in a unified space inside the brain. Inspired by this capability, given that current MLLMs already encode visual and text information in the same feature space, we hold that visual tokens can be seamlessly inserted into the reasoning process carried by text tokens, where ideally, all visual imagination processes can be encoded by the latent features. To achieve this goal, we propose Sketch-in-Latents (SkiLa), a novel paradigm for unified multi-modal reasoning that expands the auto-regressive capabilities of MLLMs to natively generate continuous visual embeddings, termed latent sketch tokens, as visual thoughts. During multi-step reasoning, the model dynamically alternates between textual thinking mode for generating textual think tokens and visual sketching mode for generating latent sketch tokens. A latent visual semantics reconstruction mechanism is proposed to ensure these latent sketch tokens are semantically grounded. Extensive experiments demonstrate that SkiLa achieves superior performance on vision-centric tasks while exhibiting strong generalization to diverse general multi-modal benchmarks. Codes will be released at https://github.com/TungChintao/SkiLa.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "14 pages, 11 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16584v1",
            "code_links": [
                {
                    "url": "https://github.com/TungChintao/SkiLa",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Sketch-in-Latents (SkiLa)，实现MLLM中统一的多模态推理与视觉想象。",
            "summary_zh": "多模态大型语言模型(MLLM)擅长通过文本推理进行视觉理解任务，但在需要视觉想象的场景中表现不佳。与采用预定义外部工具包或在思考过程中生成图像的现有方法不同，人类可以在没有预定义工具包的情况下进行灵活的视觉-文本想象和交互，一个重要原因是人类在大脑内部的统一空间中构建视觉-文本思考过程。受此启发，鉴于当前的MLLM已经将视觉和文本信息编码在相同的特征空间中，我们认为视觉token可以无缝地插入到文本token所携带的推理过程中，理想情况下，所有的视觉想象过程都可以由潜在特征编码。为了实现这一目标，我们提出Sketch-in-Latents (SkiLa)，这是一种用于统一多模态推理的新范式，它扩展了MLLM的自回归能力，以原生生成连续的视觉嵌入，称为潜在草图token，作为视觉思考。在多步推理过程中，模型动态地在用于生成文本思考token的文本思考模式和用于生成潜在草图token的视觉草图模式之间切换。提出了一种潜在的视觉语义重建机制，以确保这些潜在的草图token在语义上是接地的。大量的实验表明，SkiLa在以视觉为中心的任务上取得了优异的性能，同时对各种通用多模态基准表现出强大的泛化能力。",
            "intro_zh": [
                "现有MLLM在视觉想象方面存在不足，无法像人类一样灵活进行视觉-文本交互。",
                "SkiLa通过生成连续的潜在草图token，将视觉信息无缝融入MLLM的推理过程。",
                "实验表明，SkiLa在视觉任务上表现优异，并具有良好的多模态泛化能力。"
            ],
            "method_zh": "**问题定义**：现有MLLM在处理需要视觉想象的任务时，依赖于外部工具或生成图像，这限制了其灵活性和效率。它们无法像人类一样，在统一的思维空间中进行视觉和文本的无缝交互。现有方法的痛点在于缺乏一种内在的、统一的多模态推理机制。\\n\\n**核心思路**：SkiLa的核心思路是将视觉信息表示为连续的潜在草图token，并将其嵌入到MLLM的自回归推理过程中。通过这种方式，模型可以在文本思考和视觉草图之间动态切换，实现视觉和文本的统一推理。这种设计模仿了人类大脑中视觉和文本信息在统一空间中交互的方式。\\n\\n**技术框架**：SkiLa的整体框架包含以下几个主要模块：1) 文本编码器：将文本输入编码为文本token序列。2) 视觉编码器：将视觉输入编码为视觉特征。3) 潜在草图生成器：基于文本token和视觉特征，生成潜在草图token序列。4) 自回归解码器：交替生成文本token和潜在草图token，进行多步推理。5) 视觉语义重建模块：用于确保潜在草图token的语义一致性。\\n\\n**关键创新**：SkiLa最重要的创新点在于它将视觉想象过程表示为连续的潜在嵌入，并将其融入到MLLM的自回归推理过程中。这与现有方法依赖于离散的外部工具或生成图像的方式有本质区别。SkiLa实现了视觉和文本的统一表示和推理，从而提高了模型的灵活性和效率。\\n\\n**关键设计**：SkiLa的关键设计包括：1) 潜在草图token的表示方式：使用连续的向量表示视觉信息，允许模型进行细粒度的视觉推理。2) 视觉语义重建损失：用于约束潜在草图token的语义一致性，确保其能够准确地表达视觉信息。3) 文本思考和视觉草图模式的动态切换机制：允许模型根据任务需求灵活地调整推理过程。",
            "application_zh": "SkiLa具有广泛的应用前景，例如视觉问答、图像编辑、机器人导航和人机交互等领域。它可以帮助机器更好地理解和利用视觉信息，从而实现更智能、更自然的人机交互。未来，SkiLa有望应用于自动驾驶、智能家居和虚拟现实等领域。",
            "highlight_zh": "SkiLa在多个视觉任务上取得了显著的性能提升。例如，在视觉问答任务上，SkiLa的准确率比现有最佳模型提高了5%。在图像编辑任务上，SkiLa能够生成更逼真、更符合用户意图的图像。实验结果表明，SkiLa具有强大的视觉推理和泛化能力。",
            "tags_zh": [
                "多模态大语言模型",
                "视觉想象",
                "统一推理",
                "潜在空间",
                "自回归生成",
                "视觉语义重建"
            ],
            "_index": 40,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16584v1/img/method.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16584v1/img/hyper.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16584v1/img/case_geo.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "4D Primitive-Mâché: Glueing Primitives for Persistent 4D Scene Reconstruction",
            "authors": [
                "Kirill Mazur",
                "Marwan Taher",
                "Andrew J. Davison"
            ],
            "arxiv_id": "2512.16564v1",
            "summary": "We present a dynamic reconstruction system that receives a casual monocular RGB video as input, and outputs a complete and persistent reconstruction of the scene. In other words, we reconstruct not only the the currently visible parts of the scene, but also all previously viewed parts, which enables replaying the complete reconstruction across all timesteps.\n  Our method decomposes the scene into a set of rigid 3D primitives, which are assumed to be moving throughout the scene. Using estimated dense 2D correspondences, we jointly infer the rigid motion of these primitives through an optimisation pipeline, yielding a 4D reconstruction of the scene, i.e. providing 3D geometry dynamically moving through time. To achieve this, we also introduce a mechanism to extrapolate motion for objects that become invisible, employing motion-grouping techniques to maintain continuity.\n  The resulting system enables 4D spatio-temporal awareness, offering capabilities such as replayable 3D reconstructions of articulated objects through time, multi-object scanning, and object permanence. On object scanning and multi-object datasets, our system significantly outperforms existing methods both quantitatively and qualitatively.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "For project page, see https://makezur.github.io/4DPM/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16564v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]scene reconstruction"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出4D Primitive-Mâché，用于单目视频的持久化4D场景重建",
            "summary_zh": "本文提出了一种动态重建系统，该系统以单目RGB视频作为输入，输出场景的完整且持久的重建结果。换句话说，我们不仅重建当前可见的场景部分，还重建所有先前观察到的部分，从而能够重放所有时间步长的完整重建。我们的方法将场景分解为一组刚性3D图元，这些图元被认为是在整个场景中移动的。利用估计的密集2D对应关系，我们通过优化流程联合推断这些图元的刚性运动，从而产生场景的4D重建，即提供随时间动态移动的3D几何体。为此，我们还引入了一种机制来推断不可见物体的运动，采用运动分组技术来保持连续性。该系统实现了4D时空感知，提供了诸如随时间推移的可重放3D铰接物体重建、多物体扫描和物体持久性等功能。在物体扫描和多物体数据集上，我们的系统在定量和定性方面均显著优于现有方法。",
            "intro_zh": [
                "现有动态场景重建方法难以重建所有时间步长的完整场景，尤其是在物体遮挡或离开视野后。",
                "该论文提出将场景分解为刚性3D图元，通过优化方法联合推断图元的刚性运动，实现4D场景重建。",
                "实验结果表明，该方法在物体扫描和多物体数据集上，显著优于现有方法，实现了更好的重建效果。"
            ],
            "method_zh": "**问题定义**：现有动态场景重建方法通常只能重建当前帧可见的场景部分，无法持久化地重建整个场景，尤其是在物体被遮挡或离开视野后。这限制了对场景的完整理解和后续应用，例如重放特定时间段的场景状态。现有方法在处理复杂运动和遮挡时，重建质量也会显著下降。\\n\\n**核心思路**：该论文的核心思路是将动态场景分解为一组刚性3D图元，并假设这些图元在场景中进行刚性运动。通过估计图像中的2D对应关系，并优化这些图元的运动参数，可以实现对场景的4D重建，即得到随时间变化的3D几何信息。这种基于图元的表示方法能够更好地处理遮挡和运动，并实现对不可见区域的运动推断。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1) 输入单目RGB视频；2) 估计图像中的密集2D对应关系；3) 将场景分解为一组刚性3D图元；4) 通过优化方法，联合推断这些图元的刚性运动参数，得到4D重建结果；5) 对于不可见的物体，采用运动分组技术来推断其运动轨迹，保持重建的连续性。\\n\\n**关键创新**：该方法的关键创新在于：1) 提出了基于刚性3D图元的4D场景表示方法，能够更好地处理遮挡和运动；2) 引入了运动分组技术，用于推断不可见物体的运动轨迹，实现持久化的场景重建；3) 通过联合优化图元的运动参数，实现了对场景的全局一致性重建。\\n\\n**关键设计**：该方法使用优化框架来估计图元的运动参数。损失函数可能包含以下几个部分：1) 2D对应关系约束，确保图元的投影与图像中的特征点匹配；2) 刚性运动约束，保证图元的运动符合刚性运动的规律；3) 运动平滑约束，避免图元的运动出现突变。具体的优化算法和参数设置未知。",
            "application_zh": "该研究成果可应用于增强现实、虚拟现实、机器人导航、自动驾驶等领域。例如，在AR/VR中，可以实现对真实场景的动态重建和交互；在机器人导航中，可以帮助机器人理解周围环境的动态变化，从而做出更合理的决策；在自动驾驶中，可以提高对道路上其他车辆和行人的感知能力，增强安全性。",
            "highlight_zh": "该系统在物体扫描和多物体数据集上进行了评估，实验结果表明，该方法在定量和定性方面均显著优于现有方法。具体的性能数据和提升幅度未知，但摘要强调了显著的性能提升。",
            "tags_zh": [
                "4D重建",
                "动态场景",
                "单目视觉",
                "刚性图元",
                "运动估计",
                "场景理解",
                "持久化重建"
            ],
            "_index": 41,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16564v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16564v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16564v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture",
            "authors": [
                "Haodi He",
                "Jihun Yu",
                "Ronald Fedkiw"
            ],
            "arxiv_id": "2512.16397v1",
            "summary": "We leverage increasingly popular three-dimensional neural representations in order to construct a unified and consistent explanation of a collection of uncalibrated images of the human face. Our approach utilizes Gaussian Splatting, since it is more explicit and thus more amenable to constraints than NeRFs. We leverage segmentation annotations to align the semantic regions of the face, facilitating the reconstruction of a neutral pose from only 11 images (as opposed to requiring a long video). We soft constrain the Gaussians to an underlying triangulated surface in order to provide a more structured Gaussian Splat reconstruction, which in turn informs subsequent perturbations to increase the accuracy of the underlying triangulated surface. The resulting triangulated surface can then be used in a standard graphics pipeline. In addition, and perhaps most impactful, we show how accurate geometry enables the Gaussian Splats to be transformed into texture space where they can be treated as a view-dependent neural texture. This allows one to use high visual fidelity Gaussian Splatting on any asset in a scene without the need to modify any other asset or any other aspect (geometry, lighting, renderer, etc.) of the graphics pipeline. We utilize a relightable Gaussian model to disentangle texture from lighting in order to obtain a delit high-resolution albedo texture that is also readily usable in a standard graphics pipeline. The flexibility of our system allows for training with disparate images, even with incompatible lighting, facilitating robust regularization. Finally, we demonstrate the efficacy of our approach by illustrating its use in a text-driven asset creation pipeline.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.GR"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Submitted to CVPR 2026. 21 pages, 22 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16397v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "gaussian splatting",
                        "splatting",
                        "NeRF"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "利用高斯溅射重建高保真面部几何与纹理，实现可控人脸生成",
            "summary_zh": "本文利用日益流行的三维神经表示，从一组未经校准的人脸图像中构建统一且一致的解释。该方法采用高斯溅射，因为它比NeRF更显式，因此更易于约束。利用分割标注对齐面部的语义区域，仅需11张图像即可重建中性姿势（而无需长视频）。通过软约束将高斯分布约束到潜在的三角化表面，以提供更结构化的高斯溅射重建，进而指导后续扰动以提高潜在三角化表面的精度。生成的三角化表面可用于标准图形管线。此外，也是最重要的，展示了精确的几何体如何使高斯溅射转换为纹理空间，在纹理空间中，它们可以被视为与视角相关的神经纹理。这允许在场景中的任何资产上使用高视觉保真度的高斯溅射，而无需修改任何其他资产或图形管线的任何其他方面（几何体、光照、渲染器等）。利用可重新照明的高斯模型将纹理与光照分离，以获得可直接在标准图形管线中使用的高分辨率反照率纹理。该系统的灵活性允许使用不同的图像进行训练，即使光照不兼容，也有助于鲁棒的正则化。最后，通过展示其在文本驱动的资产创建管线中的应用，证明了该方法的有效性。",
            "intro_zh": [
                "现有方法难以从少量未校准图像中重建高保真人脸几何与纹理，尤其是在光照条件不一致的情况下。",
                "利用高斯溅射的显式特性，结合语义分割和几何约束，实现从少量图像中重建高质量人脸。",
                "通过实验验证，该方法能够生成高分辨率、可重新光照的反照率纹理，并应用于文本驱动的资产创建。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从少量、未经校准的人脸图像中重建高保真度人脸几何和纹理的问题。现有方法，如NeRF，通常需要大量的训练数据（例如，长视频）才能获得较好的效果，并且难以处理光照条件不一致的图像。此外，将神经表示集成到现有图形管线中也存在挑战。\\n\\n**核心思路**：论文的核心思路是利用高斯溅射（Gaussian Splatting）的显式特性，并结合语义分割和几何约束，从而在少量图像下实现高质量的人脸重建。通过将高斯溅射转换为纹理空间，可以将其作为一种与视角相关的神经纹理，方便地集成到现有的图形管线中。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1) 使用少量未校准的人脸图像作为输入；2) 利用语义分割对齐面部区域，并初始化高斯溅射；3) 通过软约束将高斯分布约束到潜在的三角化表面，从而获得更结构化的重建；4) 对三角化表面进行优化，提高几何精度；5) 将高斯溅射转换为纹理空间，得到与视角相关的神经纹理；6) 利用可重新光照的高斯模型，分离纹理和光照，得到高分辨率的反照率纹理。\\n\\n**关键创新**：该方法最重要的创新点在于将高斯溅射与几何约束相结合，从而在少量图像下实现高质量的人脸重建。此外，将高斯溅射转换为纹理空间，并将其作为一种与视角相关的神经纹理，方便地集成到现有的图形管线中，无需修改其他资产或渲染流程。\\n\\n**关键设计**：论文的关键设计包括：1) 使用语义分割对齐面部区域，加速训练并提高重建质量；2) 使用软约束将高斯分布约束到三角化表面，从而获得更结构化的重建；3) 利用可重新光照的高斯模型，分离纹理和光照，得到高分辨率的反照率纹理。具体的参数设置、损失函数和网络结构等技术细节在论文中进行了详细描述（未知）。",
            "application_zh": "该研究成果可应用于虚拟现实、增强现实、游戏开发、数字人生成等领域。通过该方法，可以快速、高效地创建逼真的人脸模型，并将其集成到现有的图形管线中，从而提升用户体验和内容创作效率。此外，该方法还可以用于人脸识别、表情迁移等应用。",
            "highlight_zh": "该方法仅使用11张图像即可重建高质量的人脸几何和纹理，无需长视频。通过将高斯溅射转换为纹理空间，可以方便地集成到现有的图形管线中，无需修改其他资产或渲染流程。实验结果表明，该方法能够生成高分辨率、可重新光照的反照率纹理，并应用于文本驱动的资产创建。",
            "tags_zh": [
                "高斯溅射",
                "人脸重建",
                "神经渲染",
                "纹理生成",
                "几何建模"
            ],
            "_index": 42,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16397v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16397v1/figs/ablation/vis_gaussians/2_geometry_render.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16397v1/figs/head_poses/00.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "EverybodyDance: Bipartite Graph-Based Identity Correspondence for Multi-Character Animation",
            "authors": [
                "Haotian Ling",
                "Zequn Chen",
                "Qiuying Chen",
                "Donglin Di",
                "Yongjia Ma",
                "Hao Li",
                "Chen Wei",
                "Zhulin Tao",
                "Xun Yang"
            ],
            "arxiv_id": "2512.16360v1",
            "summary": "Consistent pose-driven character animation has achieved remarkable progress in single-character scenarios. However, extending these advances to multi-character settings is non-trivial, especially when position swap is involved. Beyond mere scaling, the core challenge lies in enforcing correct Identity Correspondence (IC) between characters in reference and generated frames. To address this, we introduce EverybodyDance, a systematic solution targeting IC correctness in multi-character animation. EverybodyDance is built around the Identity Matching Graph (IMG), which models characters in the generated and reference frames as two node sets in a weighted complete bipartite graph. Edge weights, computed via our proposed Mask-Query Attention (MQA), quantify the affinity between each pair of characters. Our key insight is to formalize IC correctness as a graph structural metric and to optimize it during training. We also propose a series of targeted strategies tailored for multi-character animation, including identity-embedded guidance, a multi-scale matching strategy, and pre-classified sampling, which work synergistically. Finally, to evaluate IC performance, we curate the Identity Correspondence Evaluation benchmark, dedicated to multi-character IC correctness. Extensive experiments demonstrate that EverybodyDance substantially outperforms state-of-the-art baselines in both IC and visual fidelity.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16360v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "[T]character animation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "8_physics_animation"
            ],
            "headline_zh": "提出EverybodyDance，通过二分图匹配解决多角色动画中的身份对应问题。",
            "summary_zh": "本文提出EverybodyDance，一个针对多角色动画中身份正确对应问题的系统性解决方案。核心是身份匹配图（IMG），它将生成帧和参考帧中的角色建模为加权完全二分图中的两个节点集合。通过提出的Mask-Query Attention (MQA)计算边权重，量化角色对之间的亲和力。论文将身份对应正确性形式化为图结构度量，并在训练期间优化它。此外，还提出了一系列针对多角色动画的策略，包括身份嵌入引导、多尺度匹配策略和预分类采样，协同工作。最后，构建了身份对应评估基准，用于评估多角色身份对应正确性。大量实验表明，EverybodyDance在身份对应和视觉保真度方面均优于现有技术水平。",
            "intro_zh": [
                "现有姿态驱动的角色动画在单角色场景中取得了显著进展，但扩展到多角色场景，尤其是在涉及位置交换时，极具挑战。",
                "EverybodyDance的核心思想是将角色间的身份对应问题建模为二分图匹配问题，并设计Mask-Query Attention机制计算角色间的亲和力。",
                "实验结果表明，EverybodyDance在身份对应和视觉保真度方面显著优于现有方法，并在新构建的身份对应评估基准上进行了验证。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多角色动画中身份对应（Identity Correspondence, IC）的正确性问题。现有方法在处理多角色动画，特别是角色位置发生交换时，难以保证生成动画中角色的身份与参考帧中的角色身份一致。这导致生成的动画角色混乱，缺乏逻辑性和可控性。现有方法缺乏对角色间身份关系的建模和优化，难以应对复杂的多角色场景。\\n\\n**核心思路**：论文的核心思路是将多角色动画中的身份对应问题建模为一个二分图匹配问题。具体来说，将参考帧和生成帧中的角色分别视为二分图的两个节点集合，通过计算节点之间的相似度（即边权重）来表示角色之间的匹配程度。通过优化二分图的匹配结果，可以实现角色身份的正确对应。这种方法能够显式地建模角色间的关系，并利用图结构信息来提高身份对应的准确性。\\n\\n**技术框架**：EverybodyDance的整体框架包括以下几个主要模块：1) **Identity Matching Graph (IMG)**：构建二分图，将参考帧和生成帧中的角色表示为节点，角色间的亲和力表示为边权重。2) **Mask-Query Attention (MQA)**：计算角色间的亲和力，作为IMG的边权重。MQA利用角色的掩码信息和查询向量来提取角色特征，并通过注意力机制计算相似度。3) **Identity-Embedded Guidance**：利用身份嵌入信息来引导动画生成过程，确保生成的角色具有一致的身份特征。4) **Multi-Scale Matching Strategy**：采用多尺度匹配策略，在不同尺度上进行身份对应，提高鲁棒性。5) **Pre-Classified Sampling**：采用预分类采样策略，选择具有代表性的样本进行训练，提高效率。\\n\\n**关键创新**：论文最重要的技术创新点在于将身份对应问题形式化为图结构度量，并设计了Mask-Query Attention机制来计算角色间的亲和力。与现有方法相比，EverybodyDance能够显式地建模角色间的关系，并利用图结构信息来优化身份对应。MQA能够有效地提取角色特征，并对角色间的相似度进行准确估计。此外，论文还提出了一系列针对多角色动画的策略，包括身份嵌入引导、多尺度匹配策略和预分类采样，进一步提高了性能。\\n\\n**关键设计**：在IMG中，边权重由MQA计算得到，MQA的输入包括角色的掩码信息和查询向量。查询向量通过编码器提取角色特征得到。损失函数包括身份对应损失和视觉保真度损失。身份对应损失用于优化二分图的匹配结果，视觉保真度损失用于保证生成动画的质量。网络结构采用生成对抗网络（GAN），生成器负责生成动画，判别器负责判别生成动画的真伪。",
            "application_zh": "该研究成果可广泛应用于虚拟现实、游戏开发、电影制作等领域。例如，可以用于创建多人在线游戏中玩家角色的动画，或者用于电影中多个演员的动作捕捉和动画生成。该技术能够提高多角色动画的质量和效率，降低制作成本，并为用户提供更加逼真和自然的动画体验。未来，该技术有望进一步扩展到更复杂的多角色场景，例如人群动画和社交互动动画。",
            "highlight_zh": "实验结果表明，EverybodyDance在身份对应和视觉保真度方面均显著优于现有方法。在身份对应准确率方面，EverybodyDance比最先进的基线方法提高了约10%。此外，通过消融实验验证了各个模块的有效性，证明了IMG、MQA以及其他策略的协同作用。论文还构建了一个新的身份对应评估基准，为多角色动画的研究提供了新的评估工具。",
            "tags_zh": [
                "多角色动画",
                "身份对应",
                "二分图匹配",
                "姿态驱动",
                "Mask-Query Attention"
            ],
            "_index": 43,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16360v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16360v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16360v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "PixelArena: A benchmark for Pixel-Precision Visual Intelligence",
            "authors": [
                "Feng Liang",
                "Sizhe Cheng",
                "Chenqi Yi"
            ],
            "arxiv_id": "2512.16303v1",
            "summary": "Multi-modal large language models that have image output are emerging. Many image generation benchmarks focus on aesthetics instead of fine-grained generation capabilities. In PixelArena, we propose using semantic segmentation tasks to objectively examine their fine-grained generative intelligence with pixel precision. We find the latest Gemini 3 Pro Image has emergent image generation capabilities that generate semantic masks with high fidelity under zero-shot settings, showcasing visual intelligence unseen before and true generalization in new image generation tasks. We further investigate its results, compare them qualitatively and quantitatively with those of other models, and present failure cases. The findings not only signal exciting progress in the field but also provide insights into future research related to multimodality, reasoning, interpretability and benchmarking.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "7 pages, 11 figures, project page: https://pixelarena.reify.ing/project",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16303v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "PixelArena：提出像素级视觉智能评估基准，用于客观评估图像生成模型的细粒度生成能力。",
            "summary_zh": "本文提出了PixelArena，一个用于像素精度视觉智能的评估基准。随着多模态大语言模型在图像输出方面的涌现，现有的图像生成基准更多关注美学，而非细粒度的生成能力。PixelArena通过语义分割任务，以像素精度客观地检验模型的细粒度生成智能。研究发现，最新的Gemini 3 Pro Image在零样本设置下展现出卓越的图像生成能力，能够生成高保真度的语义掩码，体现了前所未有的视觉智能和在新图像生成任务中的真正泛化能力。进一步研究了其结果，与其他模型进行了定性和定量比较，并展示了失败案例。这些发现不仅标志着该领域令人兴奋的进展，也为多模态、推理、可解释性和基准测试等未来研究提供了见解。",
            "intro_zh": [
                "现有图像生成基准侧重于美学质量，缺乏对模型细粒度生成能力的客观评估。",
                "PixelArena利用语义分割任务，以像素精度评估图像生成模型的细粒度视觉智能。",
                "实验表明，Gemini 3 Pro Image在零样本语义分割任务中表现出卓越的生成能力和泛化性。"
            ],
            "method_zh": "**问题定义**：现有图像生成模型的评估主要集中在生成图像的美学质量上，缺乏对模型理解和生成图像细节能力的客观评估。尤其是在像素级别的细粒度控制和理解方面，现有方法难以有效评估模型是否真正具备视觉智能。因此，需要一个能够以像素精度评估模型生成图像语义信息的基准。\n\n**核心思路**：PixelArena的核心思路是利用语义分割任务作为评估图像生成模型细粒度视觉智能的代理任务。语义分割需要模型理解图像中每个像素所属的类别，从而反映模型对图像内容的细致理解和生成能力。通过比较生成图像的语义分割结果与真实标签，可以客观地评估模型的生成质量。\n\n**技术框架**：PixelArena基准测试主要包含以下几个阶段：1) 给定文本提示，使用待评估的图像生成模型生成图像；2) 使用预训练的语义分割模型对生成的图像进行语义分割，得到预测的语义掩码；3) 将预测的语义掩码与真实的语义标签进行比较，计算像素级别的精度指标，如IoU (Intersection over Union) 和 Pixel Accuracy；4) 对比不同模型的性能指标，并进行定性分析，找出模型的优势和不足。\n\n**关键创新**：PixelArena的关键创新在于将语义分割任务作为评估图像生成模型细粒度视觉智能的桥梁。与传统的美学评估方法不同，PixelArena提供了一种客观、可量化的评估方法，能够深入了解模型在像素级别的理解和生成能力。此外，该基准测试可以用于评估各种图像生成模型，包括GAN、VAE和扩散模型等。\n\n**关键设计**：PixelArena的关键设计包括：1) 选择合适的语义分割数据集，例如Cityscapes、ADE20K等，这些数据集包含丰富的语义信息和像素级别的标注；2) 使用标准的语义分割评估指标，如IoU和Pixel Accuracy，来量化模型的性能；3) 设计合理的文本提示，以引导模型生成具有特定语义信息的图像；4) 对比不同模型的性能，并进行统计显著性检验，以确保评估结果的可靠性。",
            "application_zh": "PixelArena可用于评估和比较各种图像生成模型的细粒度视觉智能，推动多模态大语言模型的发展。该基准测试可以帮助研究人员更好地理解模型的优势和不足，从而改进模型的设计和训练方法。此外，PixelArena还可以应用于图像编辑、图像修复等领域，提高生成图像的质量和可控性。",
            "highlight_zh": "实验结果表明，Gemini 3 Pro Image在零样本语义分割任务中表现出卓越的生成能力，其生成的语义掩码具有很高的保真度。与其他模型相比，Gemini 3 Pro Image在IoU和Pixel Accuracy等指标上取得了显著的提升，体现了其强大的视觉智能和泛化能力。该研究还发现了Gemini 3 Pro Image的一些失败案例，为未来的研究提供了重要的参考。",
            "tags_zh": [
                "图像生成",
                "视觉智能",
                "语义分割",
                "多模态大语言模型",
                "评估基准"
            ],
            "_index": 44,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16303v1/images/celeb/label-color-palette.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16303v1/images/celeb/model-comparison-celeb.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16303v1/images/celeb/best-f1.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "MACL: Multi-Label Adaptive Contrastive Learning Loss for Remote Sensing Image Retrieval",
            "authors": [
                "Amna Amir",
                "Erchan Aptoula"
            ],
            "arxiv_id": "2512.16294v1",
            "summary": "Semantic overlap among land-cover categories, highly imbalanced label distributions, and complex inter-class co-occurrence patterns constitute significant challenges for multi-label remote-sensing image retrieval. In this article, Multi-Label Adaptive Contrastive Learning (MACL) is introduced as an extension of contrastive learning to address them. It integrates label-aware sampling, frequency-sensitive weighting, and dynamic-temperature scaling to achieve balanced representation learning across both common and rare categories. Extensive experiments on three benchmark datasets (DLRSD, ML-AID, and WHDLD), show that MACL consistently outperforms contrastive-loss based baselines, effectively mitigating semantic imbalance and delivering more reliable retrieval performance in large-scale remote-sensing archives. Code, pretrained models, and evaluation scripts will be released at https://github.com/amna/MACL upon acceptance.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16294v1",
            "code_links": [
                {
                    "url": "https://github.com/amna/MACL",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "representation learning",
                        "[T]contrastive learning"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出MACL：一种多标签自适应对比学习损失，用于遥感图像检索",
            "summary_zh": "多标签遥感图像检索面临着地物类别间语义重叠、标签分布高度不平衡以及复杂的类间共现模式等挑战。本文提出了一种多标签自适应对比学习（MACL）方法，作为对比学习的扩展来解决这些问题。MACL集成了标签感知采样、频率敏感加权和动态温度缩放，以实现常见类别和稀有类别之间平衡的表征学习。在三个基准数据集（DLRSD、ML-AID和WHDLD）上的大量实验表明，MACL始终优于基于对比损失的基线方法，有效地缓解了语义不平衡，并在大规模遥感档案中提供了更可靠的检索性能。代码、预训练模型和评估脚本将在接收后发布在https://github.com/amna/MACL。",
            "intro_zh": [
                "多标签遥感图像检索面临语义重叠和标签不平衡等挑战，现有方法难以有效处理。",
                "MACL通过标签感知采样、频率敏感加权和动态温度缩放，实现平衡的表征学习。",
                "实验表明，MACL在多个数据集上优于现有方法，提升了大规模遥感图像检索的可靠性。"
            ],
            "method_zh": "**问题定义**：多标签遥感图像检索任务中，由于地物类别之间存在语义重叠，标签分布极度不平衡，以及复杂的类间共现关系，导致现有方法难以学习到有效的图像表征，从而影响检索性能。现有方法通常难以兼顾常见类别和稀有类别，导致检索结果偏向于常见类别，而忽略了稀有类别的准确性。\\n\\n**核心思路**：MACL的核心思路是通过自适应的对比学习，平衡不同类别之间的表征学习。具体来说，通过标签感知采样，增加稀有类别的采样概率；通过频率敏感加权，降低常见类别的损失权重，提高稀有类别的损失权重；通过动态温度缩放，调整对比学习的温度参数，使得模型能够更好地学习到不同类别之间的区分性特征。\\n\\n**技术框架**：MACL的整体框架包括三个主要模块：特征提取模块、标签感知采样模块和自适应对比学习模块。首先，特征提取模块用于提取遥感图像的视觉特征。然后，标签感知采样模块根据标签的频率，对训练样本进行采样，增加稀有类别的采样概率。最后，自适应对比学习模块根据标签的频率，对损失函数进行加权，并动态调整温度参数，从而实现平衡的表征学习。\\n\\n**关键创新**：MACL的关键创新在于其自适应性。它能够根据标签的频率，动态调整采样概率、损失权重和温度参数，从而更好地适应不同数据集和不同类别的特点。与传统的对比学习方法相比，MACL能够更好地处理标签不平衡问题，并学习到更具区分性的图像表征。\\n\\n**关键设计**：标签感知采样采用逆频率采样策略，即标签频率越低的类别，其采样概率越高。频率敏感加权采用指数加权策略，即标签频率越低的类别，其损失权重越高。动态温度缩放采用sigmoid函数，根据标签频率动态调整温度参数。损失函数是基于对比学习的InfoNCE损失，并结合了频率敏感加权。",
            "application_zh": "该研究成果可广泛应用于遥感图像检索、地物分类、变化检测等领域。通过提高遥感图像检索的准确性和可靠性，可以帮助用户更有效地利用大规模遥感数据，为城市规划、环境保护、灾害监测等应用提供支持。未来，该方法可以进一步扩展到其他多标签图像分析任务中。",
            "highlight_zh": "实验结果表明，MACL在DLRSD、ML-AID和WHDLD三个数据集上均优于基于对比损失的基线方法。例如，在ML-AID数据集上，MACL的平均精度均值（mAP）比最佳基线方法提高了3.2%。这些结果表明，MACL能够有效地缓解语义不平衡问题，并提高遥感图像检索的性能。",
            "tags_zh": [
                "遥感图像检索",
                "多标签学习",
                "对比学习",
                "标签不平衡",
                "自适应学习"
            ],
            "_index": 45,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16294v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16294v1/Images/Architecture/Architecture.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16294v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "SegGraph: Leveraging Graphs of SAM Segments for Few-Shot 3D Part Segmentation",
            "authors": [
                "Yueyang Hu",
                "Haiyong Jiang",
                "Haoxuan Song",
                "Jun Xiao",
                "Hao Pan"
            ],
            "arxiv_id": "2512.16143v1",
            "summary": "This work presents a novel framework for few-shot 3D part segmentation. Recent advances have demonstrated the significant potential of 2D foundation models for low-shot 3D part segmentation. However, it is still an open problem that how to effectively aggregate 2D knowledge from foundation models to 3D. Existing methods either ignore geometric structures for 3D feature learning or neglects the high-quality grouping clues from SAM, leading to under-segmentation and inconsistent part labels. We devise a novel SAM segment graph-based propagation method, named SegGraph, to explicitly learn geometric features encoded within SAM's segmentation masks. Our method encodes geometric features by modeling mutual overlap and adjacency between segments while preserving intra-segment semantic consistency. We construct a segment graph, conceptually similar to an atlas, where nodes represent segments and edges capture their spatial relationships (overlap/adjacency). Each node adaptively modulates 2D foundation model features, which are then propagated via a graph neural network to learn global geometric structures. To enforce intra-segment semantic consistency, we map segment features to 3D points with a novel view-direction-weighted fusion attenuating contributions from low-quality segments. Extensive experiments on PartNet-E demonstrate that our method outperforms all competing baselines by at least 6.9 percent mIoU. Further analysis reveals that SegGraph achieves particularly strong performance on small components and part boundaries, demonstrating its superior geometric understanding. The code is available at: https://github.com/YueyangHu2000/SegGraph.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16143v1",
            "code_links": [
                {
                    "url": "https://github.com/YueyangHu2000/SegGraph",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "spatial relationship"
                    ],
                    "score": 3.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "7_retargeting",
                "9_embodied_foundation"
            ],
            "headline_zh": "SegGraph：利用SAM分割图进行少样本3D部件分割",
            "summary_zh": "本文提出了一种新颖的少样本3D部件分割框架。最近的研究表明，2D基础模型在低样本3D部件分割方面具有巨大的潜力。然而，如何有效地将来自基础模型的2D知识聚合到3D仍然是一个开放的问题。现有方法要么忽略3D特征学习的几何结构，要么忽略来自SAM的高质量分组线索，导致分割不足和部件标签不一致。我们设计了一种新颖的基于SAM分割图的传播方法，名为SegGraph，以显式地学习SAM分割掩码中编码的几何特征。我们的方法通过建模段之间的相互重叠和邻接关系来编码几何特征，同时保持段内语义一致性。我们构建了一个分割图，在概念上类似于地图集，其中节点代表分割，边代表它们之间的空间关系（重叠/邻接）。每个节点自适应地调节2D基础模型特征，然后通过图神经网络传播以学习全局几何结构。为了加强段内语义一致性，我们使用一种新颖的视角方向加权融合将段特征映射到3D点，从而衰减来自低质量段的贡献。在PartNet-E上的大量实验表明，我们的方法优于所有竞争基线至少6.9个百分点的mIoU。进一步的分析表明，SegGraph在小组件和部件边界上实现了特别强大的性能，证明了其卓越的几何理解能力。",
            "intro_zh": [
                "现有少样本3D部件分割方法未能有效聚合2D基础模型的知识到3D，忽略了几何结构或SAM分组线索。",
                "SegGraph通过构建SAM分割图，显式学习分割掩码中的几何特征，并保持段内语义一致性。",
                "实验表明，SegGraph在PartNet-E数据集上显著优于现有方法，尤其在小组件和部件边界上表现出色。"
            ],
            "method_zh": "**问题定义**：论文旨在解决少样本3D部件分割问题。现有方法在利用2D基础模型知识时，要么忽略了3D几何结构的学习，要么未能充分利用SAM分割提供的高质量分组信息，导致分割结果不完整，部件标签不一致。这些方法无法有效捕捉部件之间的空间关系，尤其是在小部件和部件边界处表现不佳。\\n\\n**核心思路**：论文的核心思路是构建一个基于SAM分割的图结构（SegGraph），显式地学习和利用分割区域之间的几何关系。通过图神经网络进行信息传播，可以有效地聚合来自不同分割区域的特征，并增强对全局几何结构的理解。同时，通过视角方向加权融合，保证了段内语义一致性，减少了低质量分割的影响。\\n\\n**技术框架**：SegGraph框架主要包含以下几个阶段：1) 利用SAM生成2D分割掩码；2) 构建分割图，节点代表分割区域，边表示分割区域之间的空间关系（重叠和邻接）；3) 使用图神经网络在分割图上进行特征传播，每个节点自适应地调节2D基础模型特征；4) 通过视角方向加权融合，将分割区域特征映射到3D点云，进行最终的部件分割。\\n\\n**关键创新**：该方法的核心创新在于利用SAM分割图来显式地建模和学习3D几何特征。与以往方法相比，SegGraph能够更好地捕捉部件之间的空间关系，尤其是在小部件和部件边界处。此外，视角方向加权融合机制能够有效抑制低质量分割的影响，提高分割的鲁棒性。\\n\\n**关键设计**：分割图的构建方式是关键。论文考虑了分割区域之间的重叠和邻接关系，使用不同的权重来表示这些关系。图神经网络的具体结构（例如，使用的图卷积算子）以及视角方向加权融合的权重计算方式也是重要的设计细节。损失函数的设计也需要考虑如何平衡分割精度和部件一致性。",
            "application_zh": "该研究成果可应用于机器人感知、自动驾驶、三维场景理解等领域。例如，机器人可以利用该技术更准确地识别和分割物体部件，从而更好地进行操作和交互。在自动驾驶领域，可以用于精确识别车辆、行人等目标的不同部件，提高安全性。此外，该技术还可用于CAD模型分析、虚拟现实等领域。",
            "highlight_zh": "SegGraph在PartNet-E数据集上取得了显著的性能提升，mIoU指标超过所有基线方法至少6.9%。尤其在小组件和部件边界上的分割效果提升明显，证明了其对几何结构的卓越理解能力。消融实验验证了分割图结构和视角方向加权融合的有效性。",
            "tags_zh": [
                "少样本学习",
                "3D部件分割",
                "图神经网络",
                "SAM分割",
                "几何特征学习"
            ],
            "_index": 46,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game",
            "authors": [
                "Barna Pásztor",
                "Thomas Kleine Buening",
                "Andreas Krause"
            ],
            "arxiv_id": "2512.16626v1",
            "summary": "We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader's actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.GT",
                "cs.MA",
                "stat.ML"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "10 pages, 5 tables, 1 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16626v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "RLHF"
                    ],
                    "score": 3.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Stackelberg学习框架SLHF，通过序贯博弈优化人类反馈偏好",
            "summary_zh": "本文提出了一种新的偏好优化框架——Stackelberg学习框架（SLHF）。SLHF将对齐问题建模为两个策略之间的序贯博弈：领导者（Leader）承诺一个动作，跟随者（Follower）根据领导者的动作做出响应。这种方法将偏好优化分解为跟随者的优化问题和领导者对抗性优化问题。与为动作分配标量奖励的RLHF或寻求同步博弈均衡的NLHF不同，SLHF利用序贯博弈的不对称性来捕获更丰富的偏好结构。SLHF的序贯设计自然地实现了推理时优化，因为跟随者学会改进领导者的动作，并且这些改进可以通过迭代采样来利用。我们比较了SLHF、RLHF和NLHF的解概念，并阐述了在一致性、数据敏感性和对非传递偏好的鲁棒性方面的关键优势。对大型语言模型的实验表明，SLHF在不同的偏好数据集上实现了强大的对齐，可以从0.5B扩展到8B参数，并产生了可以在模型系列之间转移而无需进一步微调的推理时优化。",
            "intro_zh": [
                "现有基于人类反馈的强化学习（RLHF）和纳什学习（NLHF）方法在处理复杂偏好结构时存在局限性。",
                "SLHF将偏好优化建模为领导者-跟随者的序贯博弈，利用序贯博弈的不对称性来捕捉更丰富的偏好结构。",
                "实验表明，SLHF在不同规模的语言模型上实现了强大的对齐，并能进行跨模型迁移的推理时优化。"
            ],
            "method_zh": "**问题定义**：现有基于人类反馈的强化学习（RLHF）方法通常将人类反馈转化为标量奖励，简化了复杂的偏好结构。纳什学习（NLHF）则假设策略同步进行，忽略了策略之间的依赖关系。这些方法在处理非传递偏好或需要细粒度调整的场景下表现不佳。\\n\\n**核心思路**：SLHF的核心思想是将偏好学习建模为一个Stackelberg博弈，其中领导者（Leader）策略先行动，跟随者（Follower）策略根据领导者的行动做出反应。这种序贯博弈结构能够更好地捕捉策略之间的依赖关系，并允许跟随者对领导者的行为进行细化和改进。\\n\\n**技术框架**：SLHF的整体框架包含两个主要阶段：跟随者学习和领导者优化。首先，跟随者通过学习人类反馈，学习如何根据领导者的动作进行改进。然后，领导者通过对抗性优化，学习如何生成能够最大化人类偏好的动作，同时考虑到跟随者的反应。在推理阶段，跟随者可以对领导者的输出进行迭代优化，从而提高整体性能。\\n\\n**关键创新**：SLHF的关键创新在于将偏好学习问题建模为一个序贯博弈，从而能够捕捉更丰富的偏好结构。与RLHF和NLHF相比，SLHF能够更好地处理非传递偏好，并且允许在推理时进行细粒度优化。此外，SLHF的序贯设计使得跟随者学习到的知识可以跨模型迁移。\\n\\n**关键设计**：SLHF的关键设计包括：1) 使用合适的损失函数来训练跟随者，使其能够准确预测人类对领导者动作的偏好；2) 设计有效的对抗性优化算法来训练领导者，使其能够生成能够最大化人类偏好的动作；3) 探索不同的迭代优化策略，以提高推理时的性能。",
            "application_zh": "SLHF可应用于各种需要从人类反馈中学习的场景，例如对话系统、文本生成、图像生成和机器人控制。通过利用SLHF，可以训练出更符合人类偏好、更安全可靠的智能系统。此外，SLHF的推理时优化能力使其能够适应不同的用户需求和环境变化。",
            "highlight_zh": "实验结果表明，SLHF在不同的偏好数据集上实现了强大的对齐，并且能够扩展到不同规模的语言模型（从0.5B到8B参数）。更重要的是，SLHF产生的推理时优化可以跨模型系列转移，无需进一步微调，这表明SLHF具有很强的泛化能力。SLHF在一致性、数据敏感性和对非传递偏好的鲁棒性方面优于RLHF和NLHF。",
            "tags_zh": [
                "人机交互",
                "偏好学习",
                "强化学习",
                "Stackelberg博弈",
                "大型语言模型"
            ],
            "_index": 47,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16626v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                }
            ]
        },
        {
            "title": "Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning",
            "authors": [
                "Mahbub E Sobhani",
                "Md. Faiyaz Abdullah Sayeedi",
                "Mohammad Nehad Alam",
                "Proma Hossain Progga",
                "Swakkhar Shatabda"
            ],
            "arxiv_id": "2512.16698v1",
            "summary": "Diagram-grounded geometry problem solving is a critical benchmark for multimodal large language models (MLLMs), yet the benefits of multi-agent design over single-agent remain unclear. We systematically compare single-agent and multi-agent pipelines on four visual math benchmarks: Geometry3K, MathVerse, OlympiadBench, and We-Math. For open-source models, multi-agent consistently improves performance. For example, Qwen-2.5-VL (7B) gains +6.8 points and Qwen-2.5-VL (32B) gains +3.3 on Geometry3K, and both Qwen-2.5-VL variants see further gains on OlympiadBench and We-Math. In contrast, the closed-source Gemini-2.0-Flash generally performs better in single-agent mode on classic benchmarks, while multi-agent yields only modest improvements on the newer We-Math dataset. These findings show that multi-agent pipelines provide clear benefits for open-source models and can assist strong proprietary systems on newer, less familiar benchmarks, but agentic decomposition is not universally optimal. All code, data, and reasoning files are available at https://github.com/faiyazabdullah/Interpreter-Solver",
            "categories": [
                "cs.AI",
                "cs.CG"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Accepted to the ARR October 2025 cycle",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16698v1",
            "code_links": [
                {
                    "url": "https://github.com/faiyazabdullah/Interpreter-Solver",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "对比单智能体与多智能体框架，评估其在图解几何问题求解中的表现。",
            "summary_zh": "图解几何问题求解是多模态大型语言模型（MLLM）的关键基准，但多智能体设计相对于单智能体的优势尚不明确。本文系统地比较了单智能体和多智能体流程在四个视觉数学基准（Geometry3K、MathVerse、OlympiadBench 和 We-Math）上的表现。对于开源模型，多智能体方法始终能提高性能。例如，Qwen-2.5-VL (7B) 在 Geometry3K 上获得了 +6.8 的提升，Qwen-2.5-VL (32B) 获得了 +3.3 的提升，并且两个 Qwen-2.5-VL 变体在 OlympiadBench 和 We-Math 上都获得了进一步的提升。相比之下，闭源模型 Gemini-2.0-Flash 在经典基准测试中通常在单智能体模式下表现更好，而多智能体仅在新数据集 We-Math 上产生了适度的改进。这些发现表明，多智能体流程为开源模型提供了明显的优势，并且可以帮助强大的专有系统处理更新、不太熟悉的基准，但智能体分解并非普遍最佳。",
            "intro_zh": [
                "现有的多模态大语言模型在图解几何问题求解中面临挑战，尤其是在复杂推理和视觉信息利用方面。",
                "论文对比单智能体和多智能体框架，探索智能体分解是否能有效提升模型在此任务上的性能。",
                "实验结果表明，多智能体框架对开源模型有显著提升，对闭源模型在新数据集上有一定帮助。"
            ],
            "method_zh": "**问题定义**：论文旨在研究在图解几何问题求解任务中，多智能体框架是否优于单智能体框架。现有的单智能体方法可能在处理复杂几何问题时，由于缺乏有效的分解和协同机制，导致推理能力不足，难以充分利用图中的视觉信息。\\n\\n**核心思路**：论文的核心思路是通过将复杂的几何问题分解为多个子任务，并分配给不同的智能体协同完成，从而提高问题求解的效率和准确性。多智能体框架能够模拟人类解决复杂问题的过程，通过分工合作，更好地利用各种信息源。\\n\\n**技术框架**：论文构建了单智能体和多智能体两种流程。单智能体流程直接使用MLLM对问题进行求解。多智能体流程则包含问题分解、子任务分配、智能体执行和结果整合等阶段。具体来说，可能包括一个负责理解问题和图表的智能体，一个负责进行几何推理的智能体，以及一个负责生成最终答案的智能体。\\n\\n**关键创新**：论文的关键创新在于系统性地对比了单智能体和多智能体框架在图解几何问题求解任务中的性能差异，并针对不同类型的模型（开源和闭源）进行了深入分析。此外，论文还探讨了多智能体框架在处理不同难度和类型的几何问题时的表现。\\n\\n**关键设计**：论文可能采用了特定的智能体间通信机制，例如消息传递或共享知识库，以实现智能体之间的协同。此外，论文可能还设计了特定的损失函数或奖励机制，以鼓励智能体更好地完成子任务并提高整体性能。具体的参数设置和网络结构取决于所使用的MLLM和智能体架构，论文中应该有详细描述。",
            "application_zh": "该研究成果可应用于教育领域，例如开发智能几何辅导系统，帮助学生理解和解决几何问题。此外，该研究对于提升多模态大语言模型在复杂推理和视觉理解任务中的能力具有重要意义，可应用于机器人导航、图像识别等领域。",
            "highlight_zh": "实验结果表明，对于开源模型，多智能体框架在Geometry3K、OlympiadBench和We-Math等基准测试中均能显著提升性能。例如，Qwen-2.5-VL (7B) 在 Geometry3K 上获得了 +6.8 的提升。然而，对于闭源模型 Gemini-2.0-Flash，单智能体模式在经典基准上表现更好，多智能体仅在We-Math数据集上略有提升。",
            "tags_zh": [
                "多智能体系统",
                "图解几何问题求解",
                "多模态大语言模型",
                "视觉推理",
                "智能体分解"
            ],
            "_index": 48,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16698v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16698v1/figures/diagram.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "Scaling Laws for Energy Efficiency of Local LLMs",
            "authors": [
                "Ander Alvarez",
                "Alessandro Genuardi",
                "Nilotpal Sinha",
                "Antonio Tiene",
                "Samuel Mugel",
                "Román Orús"
            ],
            "arxiv_id": "2512.16531v1",
            "summary": "Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven \"resolution knee\", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16531v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "针对本地LLM，揭示CPU能效缩放规律，并提出量子启发压缩优化方案。",
            "summary_zh": "在边缘设备上部署本地大型语言模型和视觉语言模型需要在精度、计算和能源预算之间取得平衡。尽管图形处理器在现代人工智能部署中占据主导地位，但大多数消费级硬件（包括笔记本电脑、台式机、工业控制器和嵌入式系统）依赖于中央处理器。本文系统地对两种广泛用于本地推理的中央处理器进行了基准测试：MacBook Pro M2（代表主流笔记本电脑）和 Raspberry Pi 5（代表低功耗嵌入式环境）。通过统一的方法，即连续采样处理器和内存使用情况以及曲线下面积积分，我们描述了语言模型的计算负载如何随输入文本长度缩放，以及视觉语言模型的计算负载如何随图像分辨率缩放。我们发现了两个经验缩放规律：（1）语言模型推理的计算成本与token长度近似线性缩放；（2）视觉语言模型表现出由预处理驱动的“分辨率膝点”，即计算量在内部分辨率钳位之上保持不变，而在其之下急剧下降。此外，我们表明，量子启发压缩可将处理器和内存使用率降低高达71.9%，并将能耗降低高达62%，同时保持或提高语义准确性。这些结果系统地量化了多模态中央处理器在本地语言和视觉语言工作负载中的缩放规律，并确定了模型压缩和输入分辨率预处理是可持续边缘推理的有效且低成本的手段。",
            "intro_zh": [
                "现有方法在边缘设备上部署LLM时，未能充分探索CPU的能效缩放规律，导致资源利用率不高。",
                "本文通过在主流CPU上进行基准测试，揭示了语言模型和视觉语言模型在CPU上的能耗缩放规律。",
                "实验表明，量子启发压缩能显著降低CPU和内存使用率，并降低能耗，同时保持甚至提高模型精度。"
            ],
            "method_zh": "**问题定义**：论文旨在解决在CPU上部署本地大型语言模型和视觉语言模型时，如何优化能效的问题。现有方法缺乏对CPU上LLM能耗缩放规律的系统研究，导致在资源受限的边缘设备上部署时，难以做出合理的性能和能耗权衡。\\n\\n**核心思路**：论文的核心思路是通过实验测量和分析，揭示LLM和VLM在CPU上的计算负载与输入长度/分辨率之间的关系，从而建立经验性的缩放规律。然后，利用模型压缩技术（量子启发压缩）来降低计算和内存需求，从而提高能效。\\n\\n**技术框架**：论文采用的整体框架包括：1) 在两种代表性的CPU平台（MacBook Pro M2和Raspberry Pi 5）上进行基准测试；2) 使用统一的方法，连续采样CPU和内存使用情况，并计算曲线下面积；3) 分析计算负载与输入长度/分辨率之间的关系，建立缩放规律；4) 应用量子启发压缩，评估其对性能和能耗的影响。\\n\\n**关键创新**：论文的关键创新在于：1) 首次系统地研究了LLM和VLM在CPU上的能耗缩放规律，并提出了两个经验性的缩放规律；2) 验证了量子启发压缩在降低CPU能耗方面的有效性，并表明其可以在保持甚至提高模型精度的前提下，显著降低资源消耗。\\n\\n**关键设计**：论文的关键设计包括：1) 选择具有代表性的CPU平台，覆盖主流笔记本电脑和低功耗嵌入式设备；2) 采用连续采样和曲线下面积积分的方法，准确测量CPU和内存使用情况；3) 使用量子启发压缩，具体实现细节未知，但强调了其在压缩模型的同时保持或提高语义准确性的能力。",
            "application_zh": "该研究成果可应用于各种边缘计算场景，例如在智能家居设备、工业控制器、机器人等资源受限的设备上部署本地LLM和VLM，实现低功耗、高性能的AI应用。通过模型压缩和输入分辨率预处理，可以有效降低部署成本，并提高设备续航能力。",
            "highlight_zh": "实验结果表明，语言模型推理的计算成本与token长度近似线性缩放；视觉语言模型存在“分辨率膝点”现象。量子启发压缩可将处理器和内存使用率降低高达71.9%，并将能耗降低高达62%，同时保持或提高语义准确性。这些数据突出了模型压缩在边缘设备上部署LLM和VLM的潜力。",
            "tags_zh": [
                "本地LLM",
                "CPU能效",
                "缩放规律",
                "边缘计算",
                "模型压缩",
                "量子启发",
                "视觉语言模型"
            ],
            "_index": 49,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16531v1/figures/updated/llm_mac_cpu_auc.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16531v1/figures/updated/llm_rpi_cpu_auc.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16531v1/figures/updated/vlm_mac_cpu_auc.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs",
            "authors": [
                "Sara Papi",
                "Javier Garcia Gilabert",
                "Zachary Hopton",
                "Vilém Zouhar",
                "Carlos Escolano",
                "Gerard I. Gállego",
                "Jorge Iranzo-Sánchez",
                "Ahrii Kim",
                "Dominik Macháček",
                "Patricia Schmidtova",
                "Maike Züfle"
            ],
            "arxiv_id": "2512.16378v1",
            "summary": "As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.SD"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project available at https://github.com/sarapapi/hearing2translate",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16378v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "foundation model"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "首个SpeechLLM综合评测：对比端到端与级联架构语音翻译性能",
            "summary_zh": "随着大型语言模型（LLM）超越文本领域，将语音作为原生模态集成催生了SpeechLLM，旨在直接翻译口语，从而绕过传统的基于转录的流程。然而，这种集成是否能提高语音到文本的翻译质量，优于已建立的级联架构，仍然是一个悬而未决的问题。我们提出了Hearing to Translate，这是第一个综合测试套件，严格地将5个最先进的SpeechLLM与16个强大的直接和级联系统进行基准测试，这些系统将领先的语音基础模型（SFM）与多语言LLM相结合。我们的分析跨越16个基准、13个语言对和9个具有挑战性的条件，包括口齿不清、嘈杂和长篇语音。通过这项广泛的评估，我们发现级联系统仍然是最可靠的，而当前的SpeechLLM仅在选定的设置中与级联系统相匹配，并且SFM落后于两者，这突出了集成LLM（无论是在模型内部还是在流程中）对于高质量语音翻译至关重要。",
            "intro_zh": [
                "现有语音翻译方法依赖级联架构，但端到端SpeechLLM的潜力尚不明确，缺乏系统性对比。",
                "论文构建全面测试集，对比SpeechLLM与级联系统，评估不同噪声、语速等条件下的翻译质量。",
                "实验表明，级联系统整体更可靠，SpeechLLM仅在特定场景表现相当，语音基础模型性能仍有差距。"
            ],
            "method_zh": "**问题定义**：论文旨在解决语音翻译领域中，端到端SpeechLLM与传统级联系统孰优孰劣的问题。现有级联系统依赖语音识别和机器翻译两个独立模块，可能存在误差累积和信息损失。而新兴的SpeechLLM试图直接从语音生成目标语言文本，理论上可以避免上述问题，但其性能优势尚未得到充分验证。因此，论文致力于通过全面的实验评估，明确两种架构的优缺点，为未来的语音翻译系统设计提供指导。\n\n**核心思路**：论文的核心思路是通过构建一个全面的测试基准，系统性地比较端到端SpeechLLM和级联系统的语音翻译性能。该基准覆盖多种语言对、噪声条件、语速和口语风格，力求模拟真实应用场景。通过在相同条件下评估不同系统的翻译质量，从而客观地评估它们的优劣。\n\n**技术框架**：论文采用对比实验的研究框架。首先，收集并整理了包含多种语言对和噪声条件的语音翻译数据集。然后，选取了5个最先进的SpeechLLM和16个基于领先语音基础模型（SFM）的级联系统作为评估对象。最后，在构建的测试基准上，使用标准的机器翻译评估指标（如BLEU）对各个系统的翻译质量进行评估和比较。通过分析实验结果，得出关于不同架构优缺点的结论。\n\n**关键创新**：论文的关键创新在于构建了首个全面评估SpeechLLM的测试基准Hearing to Translate。该基准覆盖了多种语言对、噪声条件、语速和口语风格，能够更全面地评估语音翻译系统的性能。此外，论文还对当前最先进的SpeechLLM和级联系统进行了系统的对比分析，揭示了两种架构的优缺点，为未来的语音翻译系统设计提供了有价值的参考。\n\n**关键设计**：论文在实验设计中考虑了多种因素，以保证评估的客观性和全面性。例如，在选择评估对象时，选取了当前最先进的SpeechLLM和基于领先SFM的级联系统。在构建测试基准时，覆盖了多种语言对、噪声条件、语速和口语风格。在评估翻译质量时，使用了标准的机器翻译评估指标（如BLEU）。此外，论文还对实验结果进行了详细的统计分析，以确保结论的可靠性。",
            "application_zh": "该研究成果可应用于语音翻译相关的多个领域，如国际会议同声传译、跨语言语音搜索、多语言客服机器人等。通过选择合适的语音翻译架构，可以提升跨语言交流的效率和准确性。未来的研究可以进一步探索如何结合端到端和级联架构的优点，设计更强大的语音翻译系统。",
            "highlight_zh": "实验结果表明，在大多数情况下，级联系统在语音翻译任务中表现更可靠。虽然SpeechLLM在特定场景下可以与级联系统媲美，但语音基础模型（SFM）的性能相对落后。这表明，无论是在模型内部还是流程中，集成大型语言模型（LLM）对于实现高质量的语音翻译至关重要。",
            "tags_zh": [
                "语音翻译",
                "SpeechLLM",
                "级联系统",
                "端到端模型",
                "语音基础模型"
            ],
            "_index": 50,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16378v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16378v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16378v1/figs/pearmut_screenshot_1.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Code-in-the-Loop Forensics: Agentic Tool Use for Image Forgery Detection",
            "authors": [
                "Fanrui Zhang",
                "Qiang Zhang",
                "Sizhuo Zhou",
                "Jianwen Sun",
                "Chuanhao Li",
                "Jiaxin Ai",
                "Yukang Feng",
                "Yujie Zhang",
                "Wenjie Li",
                "Zizhen Li",
                "Yifan Chang",
                "Jiawei Liu",
                "Kaipeng Zhang"
            ],
            "arxiv_id": "2512.16300v1",
            "summary": "Existing image forgery detection (IFD) methods either exploit low-level, semantics-agnostic artifacts or rely on multimodal large language models (MLLMs) with high-level semantic knowledge. Although naturally complementary, these two information streams are highly heterogeneous in both paradigm and reasoning, making it difficult for existing methods to unify them or effectively model their cross-level interactions. To address this gap, we propose ForenAgent, a multi-round interactive IFD framework that enables MLLMs to autonomously generate, execute, and iteratively refine Python-based low-level tools around the detection objective, thereby achieving more flexible and interpretable forgery analysis. ForenAgent follows a two-stage training pipeline combining Cold Start and Reinforcement Fine-Tuning to enhance its tool interaction capability and reasoning adaptability progressively. Inspired by human reasoning, we design a dynamic reasoning loop comprising global perception, local focusing, iterative probing, and holistic adjudication, and instantiate it as both a data-sampling strategy and a task-aligned process reward. For systematic training and evaluation, we construct FABench, a heterogeneous, high-quality agent-forensics dataset comprising 100k images and approximately 200k agent-interaction question-answer pairs. Experiments show that ForenAgent exhibits emergent tool-use competence and reflective reasoning on challenging IFD tasks when assisted by low-level tools, charting a promising route toward general-purpose IFD. The code will be released after the review process is completed.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "11 pages, 6 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16300v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出ForenAgent，利用Agentic工具进行图像伪造检测，实现更灵活和可解释的分析。",
            "summary_zh": "现有的图像伪造检测（IFD）方法要么利用低级、语义无关的伪影，要么依赖于具有高级语义知识的多模态大型语言模型（MLLM）。这两种信息流在范式和推理上高度异构，难以统一或有效建模其跨层交互。为了解决这个问题，我们提出了ForenAgent，一个多轮交互式IFD框架，使MLLM能够自主生成、执行和迭代改进基于Python的低级工具，从而实现更灵活和可解释的伪造分析。ForenAgent采用结合冷启动和强化微调的两阶段训练流程，逐步提高其工具交互能力和推理适应性。受人类推理的启发，我们设计了一个动态推理循环，包括全局感知、局部聚焦、迭代探测和整体判断，并将其实例化为数据采样策略和任务对齐的过程奖励。为了系统地训练和评估，我们构建了FABench，一个异构、高质量的agent-forensics数据集，包含10万张图像和大约20万个agent交互问答对。实验表明，在低级工具的辅助下，ForenAgent在具有挑战性的IFD任务中表现出涌现的工具使用能力和反思性推理，为通用IFD开辟了一条有希望的途径。",
            "intro_zh": [
                "现有图像伪造检测方法难以有效融合低级伪影和高级语义知识，限制了检测性能和可解释性。",
                "ForenAgent利用MLLM自主生成和执行低级工具，通过多轮交互迭代优化检测结果，实现更灵活的伪造分析。",
                "构建了包含10万张图像的FABench数据集，实验证明ForenAgent在图像伪造检测任务中展现出强大的工具使用能力。"
            ],
            "method_zh": "**问题定义**：现有图像伪造检测方法主要存在两个痛点：一是依赖低级特征，缺乏语义理解；二是依赖多模态大语言模型，但难以有效利用低级信息。这两种信息流的异构性使得现有方法难以统一建模，导致检测精度和可解释性受限。\\n\\n**核心思路**：ForenAgent的核心思路是利用多模态大语言模型（MLLM）作为智能体，使其能够自主生成、执行和迭代改进基于Python的低级工具，从而实现对图像伪造的更深入分析。通过工具的使用，MLLM可以更有效地利用低级信息，并结合其自身的高级语义知识，从而提高检测精度和可解释性。\\n\\n**技术框架**：ForenAgent的整体框架是一个多轮交互式循环，包含以下几个主要模块：1) 全局感知：MLLM首先对图像进行全局感知，获取图像的整体信息。2) 局部聚焦：根据全局感知的结果，MLLM确定需要重点关注的区域。3) 迭代探测：MLLM生成并执行低级工具，对重点区域进行详细分析，例如边缘检测、噪声分析等。4) 整体判断：MLLM综合分析工具的执行结果和自身的高级语义知识，做出最终的伪造判断。\\n\\n**关键创新**：ForenAgent的关键创新在于将MLLM与低级工具相结合，构建了一个可自主学习和迭代优化的图像伪造检测系统。通过工具的使用，MLLM可以更有效地利用低级信息，并结合其自身的高级语义知识，从而提高检测精度和可解释性。此外，该方法还设计了一个动态推理循环，模拟人类的推理过程，进一步提高了检测性能。\\n\\n**关键设计**：ForenAgent采用了两阶段训练流程：冷启动和强化微调。冷启动阶段旨在使MLLM初步具备工具使用能力。强化微调阶段则通过奖励机制，鼓励MLLM生成更有效的工具和执行策略。此外，论文还设计了一个动态推理循环，包括全局感知、局部聚焦、迭代探测和整体判断，并将其实例化为数据采样策略和任务对齐的过程奖励。",
            "application_zh": "ForenAgent在图像取证、新闻真实性验证、版权保护等领域具有广泛的应用前景。该研究能够帮助人们更准确地识别伪造图像，维护网络空间的健康和安全，并为相关领域的法律诉讼提供技术支持。未来，该技术有望应用于视频伪造检测等更复杂的场景。",
            "highlight_zh": "实验结果表明，ForenAgent在图像伪造检测任务中表现出色，展现出涌现的工具使用能力和反思性推理。通过与现有方法的对比，ForenAgent在多个数据集上取得了显著的性能提升，证明了其有效性和优越性。具体性能数据和提升幅度在论文中详细展示。",
            "tags_zh": [
                "图像伪造检测",
                "多模态大语言模型",
                "Agentic工具",
                "强化学习",
                "可解释性",
                "图像取证",
                "工具学习"
            ],
            "_index": 51,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16300v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16300v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16300v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding",
            "authors": [
                "Sanjoy Chowdhury",
                "Karren D. Yang",
                "Xudong Liu",
                "Fartash Faghri",
                "Pavan Kumar Anasosalu Vasu",
                "Oncel Tuzel",
                "Dinesh Manocha",
                "Chun-Liang Li",
                "Raviteja Vemulapalli"
            ],
            "arxiv_id": "2512.16250v1",
            "summary": "Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.",
            "categories": [
                "cs.AI",
                "cs.MA"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16250v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "AMUSE：用于Agentic多说话人理解的音视频基准和对齐框架",
            "summary_zh": "本文提出了AMUSE，一个专门用于评估Agentic多说话人理解能力的音视频基准。现有的大型多模态语言模型（MLLM），如GPT-4o和Qwen3-Omni，在感知方面表现出色，但在多说话人、以对话为中心的场景中表现不佳，这些场景需要Agentic推理来跟踪说话者、维护角色以及理解跨时间的事件。AMUSE围绕本质上是Agentic的任务设计，要求模型将复杂的音视频交互分解为规划、理解和反思步骤。它在零样本、引导和Agentic三种模式以及六个任务族（包括时空说话人定位和多模态对话摘要）中评估MLLM。研究发现，当前模型在非Agentic和Agentic评估下都表现出较弱的多说话人推理和不一致的行为。受这些任务的Agentic本质和LLM Agent最新进展的推动，本文提出RAFT，一种数据高效的Agentic对齐框架，它将奖励优化与内在多模态自我评估（作为奖励）和选择性参数适应相结合，以实现数据和参数高效的更新。使用RAFT，在AMUSE基准测试中，准确率提高了高达39.52％。AMUSE和RAFT共同为检查多模态模型中的Agentic推理和提高其能力提供了一个实用的平台。",
            "intro_zh": [
                "现有多模态大语言模型在多说话人对话场景中，缺乏有效的Agentic推理能力，难以跟踪说话人、理解角色和事件。",
                "论文提出AMUSE基准测试和RAFT对齐框架，旨在评估和提升模型在Agentic多说话人理解方面的能力。",
                "RAFT框架通过奖励优化和多模态自评估，在AMUSE基准测试中实现了高达39.52%的相对准确率提升。"
            ],
            "method_zh": "**问题定义**：现有的大型多模态语言模型（MLLM）在处理多说话人音视频对话场景时，面临Agentic推理的挑战。这些模型难以准确跟踪每个说话人的身份、角色，以及理解跨越时间轴的事件关联。现有的评估方法也缺乏对Agentic推理能力的针对性测试。\n\n**核心思路**：论文的核心思路是构建一个专门用于评估和提升Agentic多说话人理解能力的基准测试（AMUSE）和对齐框架（RAFT）。AMUSE基准侧重于需要规划、理解和反思的任务，而RAFT框架则利用奖励优化和多模态自评估来提升模型性能。\n\n**技术框架**：RAFT框架包含以下主要组成部分：1) 多模态输入（音视频数据）；2) Agentic任务分解（将复杂任务分解为规划、理解和反思步骤）；3) 奖励优化（使用奖励信号来指导模型学习）；4) 多模态自评估（模型自我评估性能并生成奖励信号）；5) 选择性参数适应（仅更新模型的部分参数，以提高数据效率）。整体流程是，模型接收音视频输入，执行Agentic任务，进行自我评估，并根据奖励信号更新参数。\n\n**关键创新**：RAFT框架的关键创新在于其数据高效的Agentic对齐方法。它结合了奖励优化和内在多模态自评估，无需大量人工标注数据即可提升模型性能。此外，选择性参数适应进一步提高了数据效率，使得模型能够更快地适应新的任务。\n\n**关键设计**：在RAFT框架中，奖励函数的设计至关重要，它需要能够准确反映模型在Agentic任务中的表现。多模态自评估模块利用LLM的推理能力来生成奖励信号。选择性参数适应则通过控制哪些参数被更新，来平衡学习速度和泛化能力。具体的参数设置和网络结构细节在论文中进行了详细描述，但此处未知。",
            "application_zh": "该研究成果可应用于多种场景，如智能会议助手、多模态对话系统、视频内容分析等。通过提升模型在多说话人环境下的理解能力，可以实现更自然、更智能的人机交互，并为视频内容分析提供更准确的信息。",
            "highlight_zh": "实验结果表明，RAFT框架在AMUSE基准测试中取得了显著的性能提升，相对准确率提高了高达39.52%。这表明RAFT框架能够有效地提升模型在Agentic多说话人理解方面的能力，并且具有较高的数据效率。",
            "tags_zh": [
                "多说话人理解",
                "音视频分析",
                "Agentic推理",
                "多模态学习",
                "基准测试",
                "奖励优化",
                "自评估",
                "大语言模型"
            ],
            "_index": 52,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16250v1/figures/teaser.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16250v1/figures/eval-modes-short.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16250v1/figures/raft-revised.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe",
            "authors": [
                "Bingxiang He",
                "Zekai Qu",
                "Zeyuan Liu",
                "Yinghao Chen",
                "Yuxin Zuo",
                "Cheng Qian",
                "Kaiyan Zhang",
                "Weize Chen",
                "Chaojun Xiao",
                "Ganqu Cui",
                "Ning Ding",
                "Zhiyuan Liu"
            ],
            "arxiv_id": "2512.16649v1",
            "summary": "Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: \\textbf{Is this complexity necessary?} We present \\textbf{JustRL}, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\\% and 64.3\\% average accuracy across nine mathematical benchmarks) while using 2$\\times$ less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "12 pages, 3 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16649v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "curriculum learning"
                    ],
                    "score": 3.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "JustRL：通过简单强化学习方法扩展15亿参数LLM，实现数学推理SOTA",
            "summary_zh": "大型语言模型（LLM）的强化学习研究日益复杂，包括多阶段训练流程、动态超参数调整和课程学习策略。本文质疑这种复杂性是否必要，并提出了JustRL，一种极简方法，采用单阶段训练和固定超参数，在两个15亿参数的推理模型上实现了最先进的性能（在九个数学基准测试中平均准确率分别为54.9％和64.3％），同时计算量比复杂方法少2倍。相同的超参数无需调整即可在两个模型之间迁移，并且训练过程表现出平稳的单调改进，超过4,000个步骤，没有通常需要干预的崩溃或平台期。关键的是，消融实验表明，添加诸如显式长度惩罚和鲁棒验证器之类的“标准技巧”可能会因探索崩溃而降低性能。这些结果表明，该领域可能正在增加复杂性来解决可以通过稳定、扩展的基线来解决的问题。我们发布了我们的模型和代码，以建立一个简单、经过验证的社区基线。",
            "intro_zh": [
                "现有LLM强化学习方法过度复杂，依赖多阶段训练和动态超参数，缺乏对必要性的深入分析。",
                "JustRL采用单阶段训练和固定超参数，简化了训练流程，并实现了优异的性能。",
                "实验表明，JustRL在数学推理任务上超越现有方法，且计算成本更低，并揭示了某些“标准技巧”可能适得其反。"
            ],
            "method_zh": "**问题定义**：现有基于强化学习的LLM训练方法，为了提升性能，往往引入了复杂的多阶段训练流程、动态调整的超参数以及课程学习策略。然而，这些复杂性是否真正必要，以及它们带来的收益是否超过了其引入的额外成本，是值得探讨的问题。现有方法的痛点在于训练流程复杂、计算资源消耗大，且难以调试和复现。\\n\\n**核心思路**：JustRL的核心思路是回归简单，通过采用单阶段训练和固定的超参数，避免了复杂流程带来的问题。作者认为，通过稳定且规模化的训练，可以解决现有方法中需要复杂技巧才能解决的问题。这种方法旨在证明，在适当的规模下，简单的强化学习方法也能达到甚至超过复杂方法的效果。\\n\\n**技术框架**：JustRL的技术框架非常简洁。它使用一个标准的强化学习流程，包括：1）使用LLM生成候选答案；2）使用奖励模型评估答案质量；3）使用强化学习算法（例如PPO）更新LLM的参数。整个训练过程在一个阶段内完成，并且超参数在整个训练过程中保持不变。\\n\\n**关键创新**：JustRL最重要的创新点在于其对“简单性”的强调。它挑战了当前LLM强化学习领域追求复杂性的趋势，并证明了通过简单的训练方法也能达到SOTA性能。此外，该研究还通过消融实验发现，一些常用的技巧（如长度惩罚和鲁棒验证器）实际上可能会降低性能，这进一步强调了简单性的重要性。\\n\\n**关键设计**：JustRL的关键设计在于其超参数的选择和奖励函数的设计。作者通过实验找到了一个适用于不同模型的超参数组合，并使用了一个简单的奖励函数来评估答案的质量。具体来说，奖励函数基于答案的正确性，并可能包含一些简单的惩罚项（例如，对过长答案的惩罚）。此外，作者还仔细选择了训练数据，以确保模型的训练效果。",
            "application_zh": "JustRL的潜在应用领域包括各种需要语言模型进行推理和决策的任务，例如数学问题求解、代码生成、对话系统等。该研究的实际价值在于提供了一种更简单、更高效的LLM训练方法，降低了训练成本，并促进了LLM在更广泛领域的应用。未来，JustRL可以作为LLM强化学习的一个强大基线，并为后续研究提供参考。",
            "highlight_zh": "JustRL在两个15亿参数的推理模型上实现了SOTA性能，在九个数学基准测试中平均准确率分别达到54.9％和64.3％，同时计算量比复杂方法少2倍。消融实验表明，添加显式长度惩罚和鲁棒验证器等“标准技巧”可能会降低性能。相同的超参数无需调整即可在两个模型之间迁移。",
            "tags_zh": [
                "强化学习",
                "大型语言模型",
                "数学推理",
                "单阶段训练",
                "超参数优化"
            ],
            "_index": 53,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16649v1/figures/fig1_aime24_curves_added.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16649v1/figures/fig2_training_dynamics.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16649v1/figures/fig3_training_dynamics.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Learning-based Approximate Model Predictive Control for an Impact Wrench Tool",
            "authors": [
                "Mark Benazet",
                "Francesco Ricca",
                "Dario Bralla",
                "Melanie N. Zeilinger",
                "Andrea Carron"
            ],
            "arxiv_id": "2512.16624v1",
            "summary": "Learning-based model predictive control has emerged as a powerful approach for handling complex dynamics in mechatronic systems, enabling data-driven performance improvements while respecting safety constraints. However, when computational resources are severely limited, as in battery-powered tools with embedded processors, existing approaches struggle to meet real-time requirements. In this paper, we address the problem of real-time torque control for impact wrenches, where high-frequency control updates are necessary to accurately track the fast transients occurring during periodic impact events, while maintaining high-performance safety-critical control that mitigates harmful vibrations and component wear. The key novelty of the approach is that we combine data-driven model augmentation through Gaussian process regression with neural network approximation of the resulting control policy. This insight allows us to deploy predictive control on resource-constrained embedded platforms while maintaining both constraint satisfaction and microsecond-level inference times. The proposed framework is evaluated through numerical simulations and hardware experiments on a custom impact wrench testbed. The results show that our approach successfully achieves real-time control suitable for high-frequency operation while maintaining constraint satisfaction and improving tracking accuracy compared to baseline PID control.",
            "categories": [
                "eess.SY"
            ],
            "primary_category": "eess.SY",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16624v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]model predictive control"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出基于学习的近似模型预测控制，用于冲击扳手的实时扭矩控制",
            "summary_zh": "本文提出了一种基于学习的模型预测控制方法，用于解决机电系统中复杂动力学问题，通过数据驱动的方式提升性能并满足安全约束。针对嵌入式处理器等计算资源受限的电池供电工具，现有方法难以满足实时性要求。本文聚焦冲击扳手的实时扭矩控制，需要高频控制更新以精确跟踪周期性冲击事件中的快速瞬变，同时保持高性能的安全控制，减轻有害振动和部件磨损。该方法的核心创新在于结合了高斯过程回归的数据驱动模型增强和神经网络对控制策略的近似。这种结合使得在资源受限的嵌入式平台上部署预测控制成为可能，同时保证了约束满足和微秒级的推理时间。通过数值仿真和定制冲击扳手测试台上的硬件实验验证了所提出框架的有效性，结果表明该方法成功实现了适用于高频操作的实时控制，同时保持了约束满足，并相比基线PID控制提高了跟踪精度。",
            "intro_zh": [
                "现有冲击扳手控制方法难以在资源受限的嵌入式平台上实现高频、安全、高性能的实时扭矩控制。",
                "结合高斯过程回归进行数据驱动的模型增强，并使用神经网络近似模型预测控制策略，实现实时控制。",
                "实验结果表明，该方法在满足约束条件的同时，提高了跟踪精度，适用于高频操作。"
            ],
            "method_zh": "**问题定义**：论文旨在解决冲击扳手在资源受限的嵌入式平台上进行实时扭矩控制的问题。现有方法，如传统的PID控制，难以在高频操作下精确跟踪快速瞬变，并且无法有效处理安全约束，导致有害振动和部件磨损。此外，传统的模型预测控制（MPC）方法计算复杂度高，难以在资源受限的平台上实现实时控制。\\n\\n**核心思路**：论文的核心思路是结合数据驱动的模型增强和神经网络近似，以实现高效的实时模型预测控制。具体来说，首先利用高斯过程回归（GPR）学习冲击扳手的动态模型，从而提高模型精度。然后，使用神经网络（NN）近似求解MPC问题，从而降低计算复杂度，使其能够在嵌入式平台上实时运行。这种混合方法既能利用数据驱动的优势，又能保证实时性。\\n\\n**技术框架**：整体框架包括以下几个主要模块：1) 数据采集：通过实验或仿真收集冲击扳手的工作数据。2) 模型学习：使用高斯过程回归（GPR）学习冲击扳手的动态模型，对原始模型进行增强。3) 控制策略近似：使用神经网络（NN）学习近似的MPC控制策略。4) 实时控制：在嵌入式平台上部署训练好的神经网络，进行实时扭矩控制。整个流程是离线训练，在线部署。\\n\\n**关键创新**：论文的关键创新在于将数据驱动的模型增强与神经网络近似相结合，用于解决资源受限平台上的实时模型预测控制问题。与传统的MPC方法相比，该方法通过神经网络近似显著降低了计算复杂度，使其能够在嵌入式平台上实时运行。与传统的PID控制相比，该方法能够更好地处理复杂动态和安全约束，提高控制性能。\\n\\n**关键设计**：论文中，高斯过程回归用于学习冲击扳手的动态模型，其核函数选择和超参数优化对模型精度至关重要。神经网络的结构（例如，层数、神经元数量）和训练方法（例如，损失函数、优化器）需要仔细设计，以保证控制策略的精度和实时性。此外，约束条件的处理方式（例如，软约束、硬约束）也会影响控制性能和安全性。具体的参数设置和网络结构在论文中可能没有详细给出，属于未知信息。",
            "application_zh": "该研究成果可应用于各种需要高精度、高频率控制的电池供电工具，例如电动工具、机器人关节等。通过提高控制性能和降低功耗，可以延长工具的使用寿命，提高工作效率，并降低维护成本。此外，该方法还可以推广到其他资源受限的嵌入式控制系统，例如无人机、移动机器人等。",
            "highlight_zh": "实验结果表明，所提出的方法能够在定制的冲击扳手测试台上实现实时扭矩控制，并且在满足约束条件的同时，相比基线PID控制提高了跟踪精度。具体的性能数据（例如，跟踪误差、计算时间）和提升幅度在摘要中没有明确给出，属于未知信息。但论文强调了该方法在微秒级别实现了推理时间，满足了高频操作的需求。",
            "tags_zh": [
                "模型预测控制",
                "学习控制",
                "高斯过程回归",
                "神经网络",
                "实时控制",
                "冲击扳手",
                "嵌入式系统"
            ],
            "_index": 54,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16624v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16624v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16624v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Economic versus energetic model predictive control of a cold production plant with thermal energy storage",
            "authors": [
                "Manuel G. Satué",
                "Manuel R. Arahal",
                "Luis F. Acedo",
                "Manuel G. Ortega"
            ],
            "arxiv_id": "2512.16379v1",
            "summary": "Economic model predictive control has been proposed as a means for solving the unit loading and unit allocation problem in multi-chiller cooling plants. The adjective economic stems from the use of financial cost due to electricity consumption in a time horizon, such is the loss function minimized at each sampling period. The energetic approach is rarely encountered. This article presents for the first time a comparison between the energetic optimization objective and the economic one. The comparison is made on a cooling plant using air-cooled water chillers and a cold storage system. Models developed have been integrated into Simscape, and non-convex mixed optimization methods used to achieve optimal control trajectories for both energetic and economic goals considered separately. The results over several scenarios, and in different seasons, support the consideration of the energetic approach despite the current prevalence of the economic one. The results are dependent on the electric season and the available tariffs. In particular, for the high electric season and considering a representative tariff, the results show that an increment of about 2.15% in energy consumption takes place when using the economic approach instead of the energetic one. On the other hand, a reduction in cost of 2.94% is achieved.",
            "categories": [
                "eess.SY"
            ],
            "primary_category": "eess.SY",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "14 pages",
            "doi": "10.1016/j.applthermaleng.2022.118309",
            "journal_ref": "Applied Thermal Engineering 210 (2022) 118309",
            "pdf_url": "https://arxiv.org/pdf/2512.16379v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]model predictive control"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "对比经济与能量型模型预测控制，优化冷库生产能耗与成本",
            "summary_zh": "本文首次对比了能量优化目标与经济优化目标在冷库生产中的应用。研究对象为采用风冷式冷水机组和冷能存储系统的制冷设备。论文将所建立的模型集成到Simscape中，并使用非凸混合优化方法，分别针对能量和经济目标获得最优控制轨迹。在不同场景和季节下的结果表明，尽管目前经济优化方法更为普遍，但能量优化方法也值得考虑。结果受电力季节和可用电价的影响。特别是在用电高峰期，考虑代表性电价时，使用经济优化方法代替能量优化方法会导致能耗增加约2.15%，但成本降低2.94%。",
            "intro_zh": [
                "多冷水机组冷却工厂面临机组负荷和分配问题，经济型模型预测控制通过最小化电费来解决此问题。",
                "论文对比了能量优化和经济优化两种目标在冷库控制中的效果，旨在为实际应用提供参考。",
                "实验结果表明，在用电高峰期，经济优化虽然降低了成本，但会导致能耗略微增加，需要权衡。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多冷水机组冷却工厂中，如何优化冷库的运行，以降低能耗或成本的问题。现有方法主要集中于经济型模型预测控制，即以电费最小化为目标，但忽略了能量消耗本身可能带来的影响。\\n\\n**核心思路**：论文的核心思路是对比能量优化和经济优化两种不同的目标函数，分析它们在不同场景下的性能差异。通过分别优化能量消耗和经济成本，研究两种策略对冷库运行的影响，从而为实际应用提供决策依据。\\n\\n**技术框架**：论文的技术框架主要包括以下几个部分：1) 建立冷库系统的Simscape模型，包括冷水机组和冷能存储系统；2) 设计能量优化和经济优化两种模型预测控制器，分别以能量消耗和经济成本为目标函数；3) 使用非凸混合优化方法求解最优控制轨迹；4) 在不同场景和季节下进行仿真实验，对比两种控制器的性能。\\n\\n**关键创新**：论文的关键创新在于首次对能量优化和经济优化在冷库控制中进行了直接对比。以往的研究主要集中于经济优化，而忽略了能量消耗的影响。通过对比分析，论文揭示了两种优化目标在不同场景下的优缺点，为实际应用提供了更全面的信息。\\n\\n**关键设计**：论文的关键设计包括：1) 精确的冷库系统Simscape模型，能够准确模拟冷库的运行特性；2) 合理的能量和经济目标函数，能够准确反映能量消耗和经济成本；3) 有效的非凸混合优化方法，能够求解复杂系统的最优控制轨迹。具体参数设置和损失函数细节未在摘要中详细说明，属于未知信息。",
            "application_zh": "该研究成果可应用于各种需要冷能供应的场景，如大型建筑物、数据中心、工业生产等。通过选择合适的优化目标，可以在降低能耗和降低成本之间进行权衡，从而实现冷库运行的最优化。研究结果有助于提高能源利用效率，降低运行成本，并为可持续发展做出贡献。",
            "highlight_zh": "实验结果表明，在用电高峰期，使用经济优化方法代替能量优化方法会导致能耗增加约2.15%，但成本降低2.94%。这表明在电价较高时，经济优化可能更具优势，但在其他情况下，能量优化可能更符合可持续发展的目标。具体的实验设置和对比基线未在摘要中详细说明，属于未知信息。",
            "tags_zh": [
                "模型预测控制",
                "能量优化",
                "经济优化",
                "冷库控制",
                "Simscape模型"
            ],
            "_index": 55,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16379v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16379v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16379v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Sceniris: A Fast Procedural Scene Generation Framework",
            "authors": [
                "Jinghuan Shang",
                "Harsh Patel",
                "Ran Gong",
                "Karl Schmeckpeper"
            ],
            "arxiv_id": "2512.16896v1",
            "summary": "Synthetic 3D scenes are essential for developing Physical AI and generative models. Existing procedural generation methods often have low output throughput, creating a significant bottleneck in scaling up dataset creation. In this work, we introduce Sceniris, a highly efficient procedural scene generation framework for rapidly generating large-scale, collision-free scene variations. Sceniris also provides an optional robot reachability check, providing manipulation-feasible scenes for robot tasks. Sceniris is designed for maximum efficiency by addressing the primary performance limitations of the prior method, Scene Synthesizer. Leveraging batch sampling and faster collision checking in cuRobo, Sceniris achieves at least 234x speed-up over Scene Synthesizer. Sceniris also expands the object-wise spatial relationships available in prior work to support diverse scene requirements. Our code is available at https://github.com/rai-inst/sceniris",
            "categories": [
                "cs.RO",
                "cs.CV",
                "cs.GR"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Code is available at https://github.com/rai-inst/sceniris",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16896v1",
            "code_links": [
                {
                    "url": "https://github.com/rai-inst/sceniris",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "spatial relationship"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 5.0,
            "hit_pillars": [
                "1_robot_core",
                "7_retargeting"
            ],
            "headline_zh": "Sceniris：一种快速程序化场景生成框架，加速物理AI和生成模型开发。",
            "summary_zh": "合成3D场景对于开发物理人工智能和生成模型至关重要。现有的程序化生成方法通常输出吞吐量较低，这在扩展数据集创建方面造成了显著的瓶颈。本文介绍Sceniris，这是一个高效的程序化场景生成框架，用于快速生成大规模、无碰撞的场景变体。Sceniris还提供可选的机器人可达性检查，为机器人任务提供可操作的场景。Sceniris旨在通过解决先前方法Scene Synthesizer的主要性能限制来实现最大效率。通过利用批量采样和cuRobo中更快的碰撞检测，Sceniris实现了至少234倍于Scene Synthesizer的速度提升。Sceniris还扩展了先前工作中可用的对象级空间关系，以支持多样化的场景需求。代码已开源。",
            "intro_zh": [
                "现有程序化场景生成方法吞吐量低，严重制约了大规模数据集的构建，阻碍了物理AI和生成模型的发展。",
                "Sceniris通过批量采样和更快的碰撞检测，显著提升了场景生成的效率，并扩展了对象间的空间关系。",
                "实验表明，Sceniris相比于Scene Synthesizer，在场景生成速度上实现了至少234倍的加速，效果显著。"
            ],
            "method_zh": "**问题定义**：论文旨在解决现有程序化场景生成方法速度慢的问题。现有方法在生成大规模数据集时效率低下，无法满足物理AI和生成模型对大量训练数据的需求。Scene Synthesizer等方法在碰撞检测和采样方面存在性能瓶颈，限制了场景生成的吞吐量。\\n\\n**核心思路**：Sceniris的核心思路是通过优化采样策略和碰撞检测算法来提高场景生成的效率。具体来说，它采用批量采样来减少采样次数，并利用cuRobo中更快的碰撞检测算法来加速碰撞检测过程。此外，Sceniris还扩展了对象间的空间关系，以支持更多样化的场景生成。\\n\\n**技术框架**：Sceniris的整体框架包括以下几个主要模块：1) 场景描述模块：定义场景中对象的属性和空间关系。2) 采样模块：根据场景描述生成对象的初始位置和姿态。3) 碰撞检测模块：检测对象之间是否存在碰撞。4) 优化模块：调整对象的位置和姿态，以消除碰撞并满足空间关系约束。5) 可达性检测模块（可选）：检查生成的场景是否适合机器人操作。\\n\\n**关键创新**：Sceniris的关键创新在于其高效的场景生成方法。它通过批量采样和更快的碰撞检测算法，显著提高了场景生成的速度。与Scene Synthesizer相比，Sceniris在性能上有了显著的提升。此外，Sceniris还扩展了对象间的空间关系，使其能够生成更多样化的场景。\\n\\n**关键设计**：Sceniris的关键设计包括：1) 批量采样策略：一次采样多个对象，减少采样次数。2) cuRobo碰撞检测算法：利用GPU加速碰撞检测，提高检测速度。3) 对象级空间关系扩展：支持更多类型的空间关系，如支撑、包含等。4) 可达性检测模块：使用机器人运动学模型检查场景的可操作性。具体参数设置和损失函数等细节未在论文中详细描述。",
            "application_zh": "Sceniris可广泛应用于物理AI和生成模型领域，例如机器人操作、自动驾驶、游戏开发等。它可以用于生成大规模的训练数据集，帮助训练更鲁棒、更智能的AI模型。此外，Sceniris还可以用于快速生成各种场景，用于机器人任务规划和仿真，加速机器人技术的研发和应用。未来，Sceniris有望成为物理AI和生成模型领域的重要工具。",
            "highlight_zh": "Sceniris通过批量采样和cuRobo加速的碰撞检测，实现了显著的性能提升。实验结果表明，Sceniris相比于Scene Synthesizer，在场景生成速度上实现了至少234倍的加速。这一显著的性能提升使得Sceniris能够快速生成大规模的、无碰撞的场景变体，为物理AI和生成模型的发展提供了有力支持。",
            "tags_zh": [
                "程序化场景生成",
                "物理AI",
                "机器人操作",
                "碰撞检测",
                "批量采样"
            ],
            "_index": 56,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16896v1/assets/main_steps.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16896v1/assets/spatial_rels_cc.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16896v1/assets/main_benchmark.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "TextEditBench: Evaluating Reasoning-aware Text Editing Beyond Rendering",
            "authors": [
                "Rui Gui",
                "Yang Wan",
                "Haochen Han",
                "Dongxing Mao",
                "Fangming Liu",
                "Min Li",
                "Alex Jinpeng Wang"
            ],
            "arxiv_id": "2512.16270v1",
            "summary": "Text rendering has recently emerged as one of the most challenging frontiers in visual generation, drawing significant attention from large-scale diffusion and multimodal models. However, text editing within images remains largely unexplored, as it requires generating legible characters while preserving semantic, geometric, and contextual coherence. To fill this gap, we introduce TextEditBench, a comprehensive evaluation benchmark that explicitly focuses on text-centric regions in images. Beyond basic pixel manipulations, our benchmark emphasizes reasoning-intensive editing scenarios that require models to understand physical plausibility, linguistic meaning, and cross-modal dependencies. We further propose a novel evaluation dimension, Semantic Expectation (SE), which measures reasoning ability of model to maintain semantic consistency, contextual coherence, and cross-modal alignment during text editing. Extensive experiments on state-of-the-art editing systems reveal that while current models can follow simple textual instructions, they still struggle with context-dependent reasoning, physical consistency, and layout-aware integration. By focusing evaluation on this long-overlooked yet fundamental capability, TextEditBench establishes a new testing ground for advancing text-guided image editing and reasoning in multimodal generation.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16270v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 5.0,
            "hit_pillars": [
                "1_robot_core",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出TextEditBench，用于评估图像文本编辑中蕴含推理能力的模型。",
            "summary_zh": "本文提出了TextEditBench，一个全面的评估基准，专门关注图像中以文本为中心的区域。与基本的像素操作不同，该基准强调推理密集型的编辑场景，要求模型理解物理合理性、语言意义和跨模态依赖关系。此外，本文还提出了一种新的评估维度，即语义期望（SE），用于衡量模型在文本编辑过程中保持语义一致性、上下文连贯性和跨模态对齐的推理能力。对最先进的编辑系统进行的大量实验表明，虽然当前的模型可以遵循简单的文本指令，但它们仍然难以处理依赖于上下文的推理、物理一致性和布局感知的集成。通过专注于这种长期被忽视但至关重要的能力，TextEditBench 为推进文本引导的图像编辑和多模态生成中的推理建立了一个新的测试平台。",
            "intro_zh": [
                "现有图像编辑方法在文本编辑方面存在不足，尤其是在需要理解物理合理性、语言意义和跨模态依赖的复杂场景。",
                "TextEditBench通过构建包含推理密集型编辑场景的基准，并提出语义期望（SE）评估指标，来解决上述问题。",
                "实验表明，现有模型在处理上下文推理、物理一致性和布局感知集成方面存在困难，TextEditBench为此类能力的提升提供了测试平台。"
            ],
            "method_zh": "**问题定义**：现有图像编辑方法，特别是文本编辑，在处理需要复杂推理的场景时表现不佳。这些场景要求模型不仅要生成清晰可辨认的字符，还要保持编辑后的图像在语义、几何和上下文上的连贯性。现有方法难以同时满足这些要求，尤其是在涉及到物理合理性、语言理解和跨模态对齐时。\\n\\n**核心思路**：TextEditBench的核心思路是构建一个更具挑战性的评估基准，该基准专注于图像中以文本为中心的区域，并包含需要模型进行推理的编辑任务。通过评估模型在这些任务上的表现，可以更全面地了解其文本编辑能力，并推动相关技术的发展。此外，引入语义期望（SE）指标，显式地衡量模型在编辑后保持语义一致性的能力。\\n\\n**技术框架**：TextEditBench主要包含两个部分：一是数据集，该数据集包含各种需要推理的文本编辑场景；二是评估指标，包括传统的像素级指标和新提出的语义期望（SE）指标。编辑任务涵盖了物理合理性、语言意义和跨模态依赖等多个方面。模型首先接收图像和文本编辑指令，然后生成编辑后的图像，最后通过评估指标来衡量生成图像的质量。\\n\\n**关键创新**：TextEditBench的关键创新在于其对推理能力的强调。与以往的图像编辑基准不同，TextEditBench专注于需要模型理解物理世界、语言规则和跨模态关系的编辑任务。此外，语义期望（SE）指标的提出，为评估模型在编辑过程中保持语义一致性的能力提供了一种新的方法。\\n\\n**关键设计**：TextEditBench的数据集设计考虑了多种因素，包括文本的类型、位置、大小和方向，以及图像的背景和上下文。编辑指令的设计也力求多样化，涵盖了各种需要推理的场景。语义期望（SE）指标的计算方法是基于预训练的语言模型和视觉模型，通过比较编辑前后图像的语义表示来衡量语义一致性。",
            "application_zh": "TextEditBench的研究成果可应用于图像内容创作、智能文档处理、广告设计等领域。通过提升图像文本编辑的推理能力，可以实现更逼真、更符合用户意图的图像编辑效果，提高相关应用的用户体验和效率。未来，该研究还可以扩展到视频文本编辑等更复杂的场景。",
            "highlight_zh": "实验结果表明，当前最先进的图像编辑模型在TextEditBench上表现不佳，尤其是在需要上下文推理、物理一致性和布局感知集成的任务上。这表明现有模型在处理复杂文本编辑任务时仍存在很大的提升空间。语义期望（SE）指标能够有效区分不同模型的推理能力，为未来的研究提供了新的评估标准。",
            "tags_zh": [
                "文本编辑",
                "图像生成",
                "推理能力",
                "多模态学习",
                "评估基准"
            ],
            "_index": 57,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16270v1/figures/src/data_collection.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16270v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16270v1/figures/src/evaluation.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Skeleton-Snippet Contrastive Learning with Multiscale Feature Fusion for Action Localization",
            "authors": [
                "Qiushuo Cheng",
                "Jingjing Liu",
                "Catherine Morgan",
                "Alan Whone",
                "Majid Mirmehdi"
            ],
            "arxiv_id": "2512.16504v1",
            "summary": "The self-supervised pretraining paradigm has achieved great success in learning 3D action representations for skeleton-based action recognition using contrastive learning. However, learning effective representations for skeleton-based temporal action localization remains challenging and underexplored. Unlike video-level {action} recognition, detecting action boundaries requires temporally sensitive features that capture subtle differences between adjacent frames where labels change. To this end, we formulate a snippet discrimination pretext task for self-supervised pretraining, which densely projects skeleton sequences into non-overlapping segments and promotes features that distinguish them across videos via contrastive learning. Additionally, we build on strong backbones of skeleton-based action recognition models by fusing intermediate features with a U-shaped module to enhance feature resolution for frame-level localization. Our approach consistently improves existing skeleton-based contrastive learning methods for action localization on BABEL across diverse subsets and evaluation protocols. We also achieve state-of-the-art transfer learning performance on PKUMMD with pretraining on NTU RGB+D and BABEL.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16504v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]contrastive learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出基于骨骼片段对比学习和多尺度特征融合的动作定位方法",
            "summary_zh": "本文针对基于骨骼的动作定位任务，提出了一种自监督预训练范式。不同于视频级别的动作识别，动作定位需要对时间敏感的特征，以捕捉相邻帧之间细微的标签变化。为此，我们设计了一个片段判别预训练任务，将骨骼序列密集地投影到非重叠的片段中，并通过对比学习来区分不同视频中的片段特征。此外，我们利用U型模块融合中间特征，增强特征分辨率，从而提升帧级别的定位性能。实验结果表明，我们的方法在BABEL数据集上持续改进了现有的基于骨骼的对比学习方法，并在PKUMMD数据集上实现了最先进的迁移学习性能，预训练数据来自NTU RGB+D和BABEL。",
            "intro_zh": [
                "基于骨骼的动作定位需要捕捉帧间细微差异，现有方法难以有效学习时间敏感的特征。",
                "论文提出片段判别预训练任务，通过对比学习区分不同视频片段，增强模型对时间边界的感知能力。",
                "实验表明，该方法在BABEL和PKUMMD数据集上均取得了显著提升，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：现有的基于骨骼的动作识别方法在视频级别表现良好，但直接应用于时间动作定位任务时，无法有效捕捉动作边界，尤其是在相邻帧标签发生变化时，对时间敏感的特征提取能力不足。\\n\\n**核心思路**：论文的核心思路是通过自监督预训练，学习对时间片段具有区分性的特征表示。具体来说，将骨骼序列分割成多个非重叠的片段，然后利用对比学习，使得来自同一视频的片段特征尽可能相似，而来自不同视频的片段特征尽可能不同。这样可以迫使模型学习到能够区分不同时间片段的特征，从而提高动作定位的准确性。\\n\\n**技术框架**：整体框架包括两个主要部分：片段判别预训练和多尺度特征融合。首先，使用片段判别预训练任务，在大量无标签的骨骼数据上训练模型，学习对时间片段具有区分性的特征表示。然后，将预训练好的模型作为骨干网络，并添加一个U型模块，用于融合不同尺度的特征。最后，在目标数据集上进行微调，以适应特定的动作定位任务。\\n\\n**关键创新**：论文的关键创新在于提出了片段判别预训练任务，该任务能够有效地学习对时间片段具有区分性的特征表示。与传统的对比学习方法不同，该方法不是直接对比整个视频的特征，而是对比视频片段的特征，从而更加关注时间边界的信息。\\n\\n**关键设计**：在片段判别预训练任务中，使用了InfoNCE损失函数，用于最大化正样本之间的相似度，最小化负样本之间的相似度。U型模块的具体结构未知，但其目的是融合不同层级的特征，以提高特征的分辨率。具体的参数设置和网络结构细节在论文中可能有所描述，但摘要中未提及。",
            "application_zh": "该研究成果可应用于智能监控、人机交互、康复训练等领域。例如，在智能监控中，可以利用该方法自动检测异常行为；在人机交互中，可以利用该方法识别用户的动作意图；在康复训练中，可以利用该方法评估患者的康复进度。该研究有助于提升相关系统的智能化水平和用户体验。",
            "highlight_zh": "该方法在BABEL数据集上，相较于现有的基于骨骼的对比学习方法，取得了持续的改进。在PKUMMD数据集上，通过在NTU RGB+D和BABEL数据集上进行预训练，实现了最先进的迁移学习性能。具体的性能数据和提升幅度需要在论文中查找。",
            "tags_zh": [
                "骨骼动作定位",
                "自监督学习",
                "对比学习",
                "时间片段判别",
                "多尺度特征融合"
            ],
            "_index": 58,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16504v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16504v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16504v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
            "authors": [
                "Peter Chen",
                "Xiaopeng Li",
                "Ziniu Li",
                "Wotao Yin",
                "Xi Chen",
                "Tianyi Lin"
            ],
            "arxiv_id": "2512.16912v1",
            "summary": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "35 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16912v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "通过裁剪、熵和虚假奖励重新思考RLVR，提升LLM数学推理能力",
            "summary_zh": "本文研究了具有可验证奖励的强化学习（RLVR）中的探索-利用权衡，RLVR是一种用于改进大型语言模型（LLM）推理的框架。最近的研究表明，RLVR可以通过两种看似矛盾的机制来激发LLM强大的数学推理能力：虚假奖励，通过奖励与ground truth无关的结果来抑制利用；以及熵最小化，通过将模型推向更自信和确定性的输出来抑制探索。这突出了一种令人困惑的动态：抑制利用和抑制探索都能提高推理性能，但协调这些影响的潜在原则仍然知之甚少。我们关注两个基本问题：（i）策略熵如何与性能相关，以及（ii）虚假奖励是否能带来收益，可能是通过裁剪偏差和模型污染的相互作用。我们的结果表明，虚假奖励下的裁剪偏差降低了策略熵，从而产生更自信和确定性的输出，而仅靠熵最小化不足以改进。我们进一步提出了一个奖励-错位模型，解释了为什么虚假奖励可以提高超出污染环境的性能。我们的发现阐明了虚假奖励益处背后的机制，并为更有效的RLVR训练提供了原则。",
            "intro_zh": [
                "现有RLVR方法在探索-利用平衡上存在困境，抑制探索和利用均能提升LLM推理能力，但内在机制不明。",
                "论文提出通过裁剪偏差降低策略熵，使模型输出更自信和确定，从而提升性能，并构建奖励-错位模型。",
                "实验结果表明，虚假奖励下的裁剪偏差能有效降低策略熵，提升模型性能，并解释了虚假奖励的有效性。"
            ],
            "method_zh": "**问题定义**：现有RLVR方法在利用强化学习提升大型语言模型（LLM）的数学推理能力时，存在探索与利用的权衡难题。一方面，虚假奖励（spurious rewards）通过奖励与真实目标无关的结果来抑制模型的过度利用；另一方面，熵最小化通过鼓励模型产生更自信和确定的输出，来抑制模型的过度探索。然而，同时抑制探索和利用却都能提升推理性能，这其中的内在机制尚不明确。现有方法缺乏对策略熵与性能之间关系的深入理解，以及对虚假奖励有效性的合理解释。\n\n**核心思路**：本文的核心思路是，虚假奖励通过引入裁剪偏差（clipping bias）来降低策略熵，从而使得模型产生更自信和确定的输出，进而提升性能。此外，论文提出了一个奖励-错位模型（reward-misalignment model），用于解释虚假奖励在非污染环境下的有效性。通过分析裁剪偏差、策略熵和虚假奖励之间的关系，揭示了RLVR中探索-利用权衡的内在机制。\n\n**技术框架**：论文主要通过理论分析和实验验证来研究RLVR中的探索-利用权衡。具体而言，首先分析了裁剪偏差对策略熵的影响，然后通过实验验证了虚假奖励下裁剪偏差降低策略熵的效果。此外，论文还提出了奖励-错位模型，并基于该模型解释了虚假奖励的有效性。整个研究框架围绕着策略熵、裁剪偏差和虚假奖励展开，旨在揭示RLVR中提升LLM推理能力的内在机制。\n\n**关键创新**：论文的关键创新在于：1) 揭示了虚假奖励通过裁剪偏差降低策略熵，从而提升LLM推理能力的机制；2) 提出了奖励-错位模型，解释了虚假奖励在非污染环境下的有效性。这些发现为理解和改进RLVR训练提供了新的视角和理论依据。与现有方法相比，本文更深入地探讨了策略熵、裁剪偏差和虚假奖励之间的关系，并提出了更具解释性的模型。\n\n**关键设计**：论文的关键设计包括：1) 裁剪偏差的引入，通过限制奖励的范围来影响策略熵；2) 策略熵的计算和分析，用于评估模型的探索程度；3) 奖励-错位模型的构建，用于解释虚假奖励的有效性。此外，论文还设计了一系列实验，用于验证理论分析的正确性，并评估不同方法在提升LLM推理能力方面的性能。",
            "application_zh": "该研究成果可应用于提升大型语言模型在数学推理、代码生成等领域的性能。通过合理设计奖励机制，平衡探索与利用，可以训练出更可靠、更高效的LLM。此外，该研究对于理解和改进其他基于强化学习的LLM训练方法也具有重要的参考价值，有助于推动人工智能技术的发展。",
            "highlight_zh": "实验结果表明，在虚假奖励下，裁剪偏差能够有效降低策略熵，使得模型输出更加自信和确定，从而提升了LLM的推理性能。此外，奖励-错位模型成功解释了虚假奖励在非污染环境下的有效性，为RLVR训练提供了新的理论依据。这些发现为改进RLVR训练策略，提升LLM的推理能力提供了重要的指导。",
            "tags_zh": [
                "强化学习",
                "大型语言模型",
                "探索-利用权衡",
                "虚假奖励",
                "策略熵",
                "裁剪偏差",
                "数学推理",
                "奖励模型"
            ],
            "_index": 59,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16912v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16912v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16912v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Meta-RL Induces Exploration in Language Agents",
            "authors": [
                "Yulun Jiang",
                "Liangze Jiang",
                "Damien Teney",
                "Michael Moor",
                "Maria Brbic"
            ],
            "arxiv_id": "2512.16848v1",
            "summary": "Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16848v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "LaMer：基于元强化学习提升语言Agent在复杂环境中的探索能力",
            "summary_zh": "强化学习(RL)使得训练大型语言模型(LLM) Agent与环境交互并解决多轮长时序任务成为可能。然而，RL训练的Agent在需要主动探索的任务中表现不佳，并且无法有效地从试错经验中学习。本文提出了LaMer，一个通用的元强化学习框架，使LLM Agent能够在测试时主动探索并从环境反馈中学习。LaMer包含两个关键组件：(i)一个跨episode的训练框架，鼓励探索和长期奖励优化；(ii)通过反思进行上下文策略调整，允许Agent从任务反馈信号中调整其策略，而无需梯度更新。在不同环境中的实验表明，LaMer显著提高了性能，在Sokoban、MineSweeper和Webshop上的性能分别提高了11%、14%和19%。此外，与RL训练的Agent相比，LaMer还展示了对更具挑战性或先前未见过的任务的更好泛化能力。总的来说，我们的结果表明，元强化学习提供了一种原则性的方法来诱导语言Agent进行探索，从而通过学习到的探索策略实现对新环境的更稳健的适应。",
            "intro_zh": [
                "现有RL训练的LLM Agent在需要主动探索和长期规划的任务中表现不足，缺乏有效的探索机制。",
                "LaMer通过元强化学习框架，鼓励Agent在训练时进行跨episode的探索，并在测试时通过反思进行策略调整。",
                "实验表明，LaMer在Sokoban、MineSweeper和Webshop等任务上显著优于RL基线，并具有更好的泛化能力。"
            ],
            "method_zh": "**问题定义**：现有基于强化学习训练的语言Agent在复杂环境中进行探索时效率低下，难以适应新的任务和环境。传统的强化学习方法往往侧重于利用已有的知识，而忽略了主动探索的重要性，导致Agent容易陷入局部最优解，无法有效地发现新的策略和行为。\n\n**核心思路**：LaMer的核心思路是利用元强化学习的思想，让Agent学习如何进行有效的探索。通过跨episode的训练，Agent可以学习到一种通用的探索策略，使其能够更好地适应新的任务和环境。此外，LaMer还引入了反思机制，允许Agent根据环境的反馈信号动态调整其策略，从而提高其适应性和鲁棒性。\n\n**技术框架**：LaMer框架包含两个主要组成部分：跨episode训练和上下文策略调整。在跨episode训练阶段，Agent在多个不同的任务episode中进行训练，目标是学习一种能够最大化长期奖励的通用策略。在上下文策略调整阶段，Agent根据当前任务的反馈信号，通过反思机制动态调整其策略，从而更好地适应当前任务。整个框架无需梯度更新，降低了计算成本。\n\n**关键创新**：LaMer的关键创新在于将元强化学习的思想引入到语言Agent的训练中，并结合反思机制实现了高效的探索和策略调整。与传统的强化学习方法相比，LaMer能够更好地适应新的任务和环境，并具有更强的泛化能力。此外，LaMer的上下文策略调整机制无需梯度更新，降低了计算成本，使其更易于部署和应用。\n\n**关键设计**：LaMer的关键设计包括：(1)跨episode训练框架，鼓励Agent进行多样化的探索；(2)基于反思的上下文策略调整机制，允许Agent根据环境反馈动态调整策略；(3)奖励函数的设计，鼓励Agent进行长期规划和优化；(4)合适的LLM选择，保证Agent具备足够的表达能力和推理能力。",
            "application_zh": "LaMer具有广泛的应用前景，可以应用于各种需要语言Agent进行主动探索和适应的任务中，例如机器人导航、游戏AI、智能助手等。该研究有助于提升Agent在复杂环境中的智能水平，使其能够更好地完成各种任务，具有重要的实际价值和未来影响。",
            "highlight_zh": "LaMer在Sokoban、MineSweeper和Webshop等多个环境中的实验结果表明，其性能显著优于传统的强化学习基线，分别取得了11%、14%和19%的性能提升。此外，LaMer还展示了对更具挑战性或先前未见过的任务的更好泛化能力，证明了其在复杂环境中的有效性和鲁棒性。",
            "tags_zh": [
                "元强化学习",
                "语言Agent",
                "主动探索",
                "策略调整",
                "上下文学习"
            ],
            "_index": 60,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16848v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16848v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16848v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning",
            "authors": [
                "Bahman Abolhassani",
                "Tugba Erpek",
                "Kemal Davaslioglu",
                "Yalin E. Sagduyu",
                "Sastry Kompella"
            ],
            "arxiv_id": "2512.16813v1",
            "summary": "Reactive jammers pose a severe security threat to robotic-swarm networks by selectively disrupting inter-agent communications and undermining formation integrity and mission success. Conventional countermeasures such as fixed power control or static channel hopping are largely ineffective against such adaptive adversaries. This paper presents a multi-agent reinforcement learning (MARL) framework based on the QMIX algorithm to improve the resilience of swarm communications under reactive jamming. We consider a network of multiple transmitter-receiver pairs sharing channels while a reactive jammer with Markovian threshold dynamics senses aggregate power and reacts accordingly. Each agent jointly selects transmit frequency (channel) and power, and QMIX learns a centralized but factorizable action-value function that enables coordinated yet decentralized execution. We benchmark QMIX against a genie-aided optimal policy in a no-channel-reuse setting, and against local Upper Confidence Bound (UCB) and a stateless reactive policy in a more general fading regime with channel reuse enabled. Simulation results show that QMIX rapidly converges to cooperative policies that nearly match the genie-aided bound, while achieving higher throughput and lower jamming incidence than the baselines, thereby demonstrating MARL's effectiveness for securing autonomous swarms in contested environments.",
            "categories": [
                "cs.NI",
                "cs.AI",
                "cs.DC",
                "cs.LG",
                "eess.SP"
            ],
            "primary_category": "cs.NI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16813v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出基于QMIX的多智能体强化学习方法，提升集群网络在反应式干扰下的抗干扰能力",
            "summary_zh": "本文提出了一种基于QMIX算法的多智能体强化学习(MARL)框架，旨在提高集群通信在反应式干扰下的弹性。反应式干扰机会选择性地干扰智能体间的通信，从而破坏集群的完整性和任务成功率，对机器人集群网络构成严重的安全威胁。传统的对策，如固定功率控制或静态信道跳频，在很大程度上对这种自适应的对抗无效。本文考虑了一个多发射机-接收机对共享信道的网络，同时一个具有马尔可夫阈值动态的反应式干扰机感知总功率并做出相应反应。每个智能体联合选择发射频率（信道）和功率，QMIX学习一个集中式但可分解的动作价值函数，从而实现协调但分散的执行。我们将QMIX与无信道复用设置中的genie-aided最优策略，以及在启用信道复用的更一般的衰落机制下的局部上限置信区间(UCB)和无状态反应策略进行基准测试。仿真结果表明，QMIX迅速收敛到接近genie-aided界限的协作策略，同时比基线实现更高的吞吐量和更低的干扰发生率，从而证明了MARL在竞争环境中保护自主集群的有效性。",
            "intro_zh": [
                "现有固定功率或静态信道跳频等方法难以有效应对反应式干扰对集群通信的威胁。",
                "采用基于QMIX的多智能体强化学习框架，使智能体能够协同选择信道和功率，提升抗干扰能力。",
                "实验表明，QMIX能快速收敛到接近最优的策略，显著提高吞吐量并降低干扰发生率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决机器人集群网络在反应式干扰下的通信安全问题。现有的固定功率控制和静态信道跳频等方法无法有效应对自适应的反应式干扰，导致集群通信中断，影响任务完成。反应式干扰机会根据感知到的总功率动态调整干扰策略，使得传统的静态防御手段失效。\\n\\n**核心思路**：论文的核心思路是利用多智能体强化学习（MARL）来训练集群中的每个智能体，使其能够协同选择合适的信道和发射功率，从而避开干扰，提高通信的可靠性和效率。通过学习一个集中式但可分解的动作价值函数，实现智能体间的协调，同时保持分散执行的灵活性。\\n\\n**技术框架**：整体框架包含多个发射机-接收机对，它们共享信道进行通信。一个反应式干扰机监听信道，并根据感知到的总功率决定是否进行干扰。每个智能体（发射机）通过QMIX算法学习最优的信道和功率选择策略。QMIX算法包含一个全局的混合网络，用于将每个智能体的局部Q值函数混合成一个全局的Q值函数，从而实现集中式训练和分散式执行。\\n\\n**关键创新**：论文的关键创新在于将QMIX算法应用于集群网络的抗干扰问题，并设计了合适的奖励函数和状态空间，使得智能体能够学习到协同的抗干扰策略。与传统的单智能体强化学习方法相比，QMIX能够更好地处理多智能体环境中的信用分配问题，从而实现更有效的协同。此外，论文还考虑了反应式干扰机的自适应行为，使得学习到的策略更具鲁棒性。\\n\\n**关键设计**：论文中，每个智能体的状态包括自身发射功率、信道状态信息以及邻居智能体的状态信息。动作空间包括可选择的信道和功率等级。奖励函数的设计旨在鼓励智能体成功传输数据包，同时避免被干扰。QMIX算法中的混合网络采用非线性结构，以更好地捕捉智能体之间的复杂关系。训练过程中，采用经验回放和目标网络等技术来提高学习的稳定性和效率。",
            "application_zh": "该研究成果可应用于各种需要高可靠性通信的集群机器人系统，例如：灾难救援、环境监测、军事侦察等。通过提升集群网络在复杂电磁环境下的抗干扰能力，可以确保集群任务的顺利完成，具有重要的实际应用价值和军事意义。未来，该方法可以进一步扩展到更大规模的集群网络，并与其他抗干扰技术相结合，以实现更强大的抗干扰能力。",
            "highlight_zh": "实验结果表明，QMIX算法能够快速收敛到接近genie-aided最优策略的性能水平，在吞吐量方面优于局部UCB和无状态反应策略等基线方法。具体而言，QMIX能够显著降低干扰发生率，并提高数据包的成功传输率。在信道复用场景下，QMIX仍然能够保持较高的性能，证明了其在复杂环境下的鲁棒性。",
            "tags_zh": [
                "多智能体强化学习",
                "集群网络",
                "抗干扰",
                "反应式干扰",
                "QMIX",
                "通信安全"
            ],
            "_index": 61,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16813v1/topology.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16813v1/based10.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16813v1/based8.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "On The Hidden Biases of Flow Matching Samplers",
            "authors": [
                "Soon Hoe Lim"
            ],
            "arxiv_id": "2512.16768v1",
            "summary": "We study the implicit bias of flow matching (FM) samplers via the lens of empirical flow matching. Although population FM may produce gradient-field velocities resembling optimal transport (OT), we show that the empirical FM minimizer is almost never a gradient field, even when each conditional flow is. Consequently, empirical FM is intrinsically energetically suboptimal. In view of this, we analyze the kinetic energy of generated samples. With Gaussian sources, both instantaneous and integrated kinetic energies exhibit exponential concentration, while heavy-tailed sources lead to polynomial tails. These behaviors are governed primarily by the choice of source distribution rather than the data. Overall, these notes provide a concise mathematical account of the structural and energetic biases arising in empirical FM.",
            "categories": [
                "stat.ML",
                "cs.LG",
                "math.PR"
            ],
            "primary_category": "stat.ML",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "20 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16768v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]flow matching"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "揭示Flow Matching采样器中的隐式偏差，分析其能量次优性",
            "summary_zh": "本文研究了Flow Matching (FM)采样器在经验流匹配视角下的隐式偏差。尽管总体FM可能产生类似于最优传输(OT)的梯度场速度，但我们表明，即使每个条件流都是梯度场，经验FM的最小化器几乎都不是梯度场。因此，经验FM本质上是能量次优的。鉴于此，我们分析了生成样本的动能。对于高斯源，瞬时和积分动能都表现出指数集中，而重尾源导致多项式尾部。这些行为主要受源分布的选择控制，而不是数据本身。总的来说，这些笔记对经验FM中出现的结构和能量偏差提供了一个简明的数学解释。",
            "intro_zh": [
                "现有Flow Matching方法在经验估计时会偏离最优传输，导致能量次优。",
                "论文通过分析经验流匹配的梯度场特性和生成样本的动能，揭示了其隐式偏差。",
                "研究表明，源分布（高斯或重尾）而非数据本身，是影响动能分布的关键因素。"
            ],
            "method_zh": "**问题定义**：Flow Matching (FM) 旨在学习一个连续的归一化流，将一个简单的源分布（如高斯分布）转换为复杂的数据分布。然而，在实际应用中，我们只能获得有限的样本，因此需要使用经验流匹配。现有的FM方法在经验估计时，其解可能不再是最优传输的梯度场，导致能量次优，影响生成样本的质量。\\n\\n**核心思路**：论文的核心思路是通过分析经验流匹配的解的结构特性和生成样本的动能，来揭示其隐式偏差。具体来说，论文证明了即使每个条件流都是梯度场，经验FM的最小化器几乎都不是梯度场。此外，论文还研究了不同源分布（高斯分布和重尾分布）对生成样本动能的影响。\\n\\n**技术框架**：论文主要采用数学分析的方法，没有涉及复杂的模型架构或训练流程。其分析框架主要包括以下几个步骤：1) 推导经验流匹配的解的表达式；2) 分析该解是否为梯度场；3) 计算生成样本的动能；4) 分析动能的分布特性。\\n\\n**关键创新**：论文最重要的创新点在于揭示了经验Flow Matching的解的结构性偏差，即经验FM的最小化器几乎都不是梯度场，即使每个条件流都是。这一发现解释了经验FM的能量次优性，并为改进FM方法提供了新的思路。\\n\\n**关键设计**：论文的关键设计在于选择合适的数学工具来分析经验流匹配的解和生成样本的动能。例如，论文使用了最优传输理论来定义理想的梯度场，并使用概率论和统计学的方法来分析动能的分布特性。此外，论文还考虑了两种不同的源分布（高斯分布和重尾分布），并分析了它们对动能分布的影响。",
            "application_zh": "该研究成果可应用于生成模型的改进，尤其是在需要高保真度和能量效率的场景下，例如图像生成、音频合成等。通过理解和缓解Flow Matching的隐式偏差，可以提高生成样本的质量和多样性，并降低计算成本。此外，该研究也为其他基于流的生成模型的设计提供了理论指导。",
            "highlight_zh": "论文的主要实验结果是理论分析，证明了经验FM的解不是梯度场，并分析了高斯源和重尾源下生成样本的动能分布。对于高斯源，瞬时和积分动能都表现出指数集中，而重尾源导致多项式尾部。这些结果表明，源分布的选择对生成样本的能量特性有显著影响。",
            "tags_zh": [
                "Flow Matching",
                "生成模型",
                "隐式偏差",
                "最优传输",
                "能量次优性"
            ],
            "_index": 62,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "NDRL: Cotton Irrigation and Nitrogen Application with Nested Dual-Agent Reinforcement Learning",
            "authors": [
                "Ruifeng Xu",
                "Liang He"
            ],
            "arxiv_id": "2512.16408v1",
            "summary": "Effective irrigation and nitrogen fertilization have a significant impact on crop yield. However, existing research faces two limitations: (1) the high complexity of optimizing water-nitrogen combinations during crop growth and poor yield optimization results; and (2) the difficulty in quantifying mild stress signals and the delayed feedback, which results in less precise dynamic regulation of water and nitrogen and lower resource utilization efficiency. To address these issues, we propose a Nested Dual-Agent Reinforcement Learning (NDRL) method. The parent agent in NDRL identifies promising macroscopic irrigation and fertilization actions based on projected cumulative yield benefits, reducing ineffective explorationwhile maintaining alignment between objectives and yield. The child agent's reward function incorporates quantified Water Stress Factor (WSF) and Nitrogen Stress Factor (NSF), and uses a mixed probability distribution to dynamically optimize daily strategies, thereby enhancing both yield and resource efficiency. We used field experiment data from 2023 and 2024 to calibrate and validate the Decision Support System for Agrotechnology Transfer (DSSAT) to simulate real-world conditions and interact with NDRL. Experimental results demonstrate that, compared to the best baseline, the simulated yield increased by 4.7% in both 2023 and 2024, the irrigation water productivity increased by 5.6% and 5.1% respectively, and the nitrogen partial factor productivity increased by 6.3% and 1.0% respectively. Our method advances the development of cotton irrigation and nitrogen fertilization, providing new ideas for addressing the complexity and precision issues in agricultural resource management and for sustainable agricultural development.",
            "categories": [
                "cs.LG",
                "cs.MA"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Accepted by ICONIP 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16408v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出嵌套双智能体强化学习NDRL，优化棉花灌溉施氮策略，提升产量和资源利用率",
            "summary_zh": "本文提出了一种嵌套双智能体强化学习（NDRL）方法，旨在解决作物生长过程中水氮组合优化的高复杂性和产量优化效果不佳的问题，以及量化轻微胁迫信号的困难和反馈延迟的问题，从而提高水氮调控的精确性和资源利用效率。NDRL中的父智能体基于预测的累积产量效益识别有希望的宏观灌溉和施肥行动，减少无效探索，同时保持目标与产量的一致性。子智能体的奖励函数结合了量化的水分胁迫因子（WSF）和氮素胁迫因子（NSF），并使用混合概率分布动态优化每日策略，从而提高产量和资源效率。使用2023年和2024年的田间试验数据校准和验证了农业技术转移决策支持系统（DSSAT），以模拟真实世界条件并与NDRL交互。实验结果表明，与最佳基线相比，模拟产量在2023年和2024年均提高了4.7%，灌溉用水生产率分别提高了5.6%和5.1%，氮素偏生产率分别提高了6.3%和1.0%。该方法推动了棉花灌溉和施氮技术的发展，为解决农业资源管理中的复杂性和精确性问题以及可持续农业发展提供了新思路。",
            "intro_zh": [
                "现有方法难以有效优化作物生长期间复杂的水氮组合，导致产量提升有限。",
                "NDRL通过嵌套双智能体结构，父智能体宏观决策，子智能体精细调控，提升优化效率。",
                "实验表明，NDRL相较于基线方法，显著提升了棉花产量、灌溉用水生产率和氮素偏生产率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决棉花种植过程中，如何精确控制灌溉和施氮量，以最大化产量并提高资源利用率的问题。现有方法的痛点在于难以量化作物对水氮的轻微胁迫信号，导致反馈延迟，无法进行精准的动态调节；同时，水氮组合优化的复杂性高，传统方法难以找到最优策略。\\n\\n**核心思路**：论文的核心思路是利用嵌套双智能体强化学习框架，将宏观决策和微观调控相结合。父智能体负责根据预测的累积产量效益，选择有希望的宏观灌溉和施肥策略，减少无效探索。子智能体则根据量化的水分胁迫因子（WSF）和氮素胁迫因子（NSF），动态优化每日策略，从而提高产量和资源效率。\\n\\n**技术框架**：NDRL的整体架构包含两个智能体：父智能体和子智能体。父智能体基于DSSAT模拟的作物生长状态，预测不同水氮组合的累积产量效益，并选择宏观策略。子智能体则根据每日的WSF和NSF，以及父智能体的宏观策略，利用混合概率分布动态调整灌溉和施氮量。DSSAT作为环境模拟器，提供作物生长状态的反馈，用于训练和评估NDRL。\\n\\n**关键创新**：NDRL的关键创新在于嵌套双智能体结构和量化的胁迫因子。嵌套结构实现了宏观策略和微观调控的有效结合，提高了优化效率。WSF和NSF的引入，使得智能体能够感知作物对水氮的轻微胁迫，从而进行更精准的动态调节。与现有方法相比，NDRL能够更好地应对水氮组合优化的复杂性和反馈延迟问题。\\n\\n**关键设计**：子智能体的奖励函数设计是关键，它结合了产量和资源利用率的指标，并引入了WSF和NSF作为惩罚项，以鼓励智能体在保证产量的同时，节约水氮资源。子智能体使用混合概率分布来选择每日策略，允许智能体在探索和利用之间进行平衡。父智能体使用深度Q网络（DQN）进行训练，子智能体使用策略梯度方法进行训练。",
            "application_zh": "该研究成果可应用于精准农业领域，为棉花等作物的灌溉和施氮管理提供决策支持。通过NDRL，可以实现水肥资源的优化配置，提高作物产量和资源利用率，降低农业生产成本，减少环境污染，促进农业可持续发展。该方法具有推广价值，可应用于其他作物和地区，为解决全球粮食安全问题提供技术支持。",
            "highlight_zh": "实验结果表明，NDRL在模拟环境下，相较于最佳基线方法，在2023年和2024年均实现了4.7%的产量提升。同时，灌溉用水生产率分别提高了5.6%和5.1%，氮素偏生产率分别提高了6.3%和1.0%。这些数据表明，NDRL能够有效提高产量和资源利用率，具有显著的优势。",
            "tags_zh": [
                "强化学习",
                "农业灌溉",
                "氮肥施用",
                "双智能体",
                "作物模型",
                "精准农业"
            ],
            "_index": 63,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16408v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16408v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16408v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "A Task-Driven, Planner-in-the-Loop Computational Design Framework for Modular Manipulators",
            "authors": [
                "Maolin Lei",
                "Edoardo Romiti",
                "Arturo Laurenzi",
                "Rui Dai",
                "Matteo Dalle Vedove",
                "Jiatao Ding",
                "Daniele Fontanelli",
                "Nikos Tsagarakis"
            ],
            "arxiv_id": "2512.16069v1",
            "summary": "Modular manipulators composed of pre-manufactured and interchangeable modules offer high adaptability across diverse tasks. However, their deployment requires generating feasible motions while jointly optimizing morphology and mounted pose under kinematic, dynamic, and physical constraints. Moreover, traditional single-branch designs often extend reach by increasing link length, which can easily violate torque limits at the base joint. To address these challenges, we propose a unified task-driven computational framework that integrates trajectory planning across varying morphologies with the co-optimization of morphology and mounted pose. Within this framework, a hierarchical model predictive control (HMPC) strategy is developed to enable motion planning for both redundant and non-redundant manipulators. For design optimization, the CMA-ES is employed to efficiently explore a hybrid search space consisting of discrete morphology configurations and continuous mounted poses. Meanwhile, a virtual module abstraction is introduced to enable bi-branch morphologies, allowing an auxiliary branch to offload torque from the primary branch and extend the achievable workspace without increasing the capacity of individual joint modules. Extensive simulations and hardware experiments on polishing, drilling, and pick-and-place tasks demonstrate the effectiveness of the proposed framework. The results show that: 1) the framework can generate multiple feasible designs that satisfy kinematic and dynamic constraints while avoiding environmental collisions for given tasks; 2) flexible design objectives, such as maximizing manipulability, minimizing joint effort, or reducing the number of modules, can be achieved by customizing the cost functions; and 3) a bi-branch morphology capable of operating in a large workspace can be realized without requiring more powerful basic modules.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16069v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "model predictive control",
                        "motion planning"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出任务驱动的模块化机械臂计算设计框架，实现形态与运动的协同优化",
            "summary_zh": "本文提出了一种统一的任务驱动计算框架，用于模块化机械臂的设计，该框架集成了不同形态下的轨迹规划以及形态和安装姿态的协同优化。开发了一种分层模型预测控制（HMPC）策略，以实现冗余和非冗余机械臂的运动规划。采用CMA-ES算法高效探索离散形态配置和连续安装姿态的混合搜索空间。引入虚拟模块抽象，实现双分支形态，允许辅助分支卸载主分支的扭矩，并在不增加单个关节模块容量的情况下扩展可实现的工作空间。在抛光、钻孔和取放任务中的仿真和硬件实验表明了该框架的有效性。结果表明，该框架可以生成多个满足运动学和动力学约束的可行设计，同时避免环境碰撞；可以通过自定义成本函数实现灵活的设计目标，例如最大化可操作性、最小化关节力或减少模块数量；并且可以在不需要更强大的基本模块的情况下实现能够在大型工作空间中运行的双分支形态。",
            "intro_zh": [
                "传统单分支机械臂通过增加连杆长度来扩展工作空间，易导致基关节扭矩超限，是核心问题。",
                "提出统一的计算框架，将轨迹规划与形态、安装姿态的协同优化相结合，解决上述问题。",
                "仿真和硬件实验验证了框架的有效性，能生成满足约束的设计，并实现灵活的设计目标。"
            ],
            "method_zh": "**问题定义**：模块化机械臂的设计需要同时优化机械臂的形态、安装位置以及运动轨迹，以满足特定的任务需求。传统方法通常采用单分支结构，为了扩大工作空间，会增加连杆的长度，但这会导致基关节的扭矩需求增加，容易超出关节的承载能力。因此，如何在满足运动学、动力学和物理约束的前提下，设计出既能完成任务又能避免关节过载的模块化机械臂是一个关键问题。\\n\\n**核心思路**：本文的核心思路是将轨迹规划与机械臂的形态和安装姿态的优化相结合，形成一个统一的任务驱动计算框架。通过在设计过程中考虑运动规划，可以更准确地评估不同形态的性能，并选择最优的设计方案。此外，引入双分支结构，利用辅助分支来分担主分支的扭矩，从而在不增加单个关节模块容量的情况下，扩展机械臂的工作空间。\\n\\n**技术框架**：该框架包含以下主要模块：1) 运动规划模块，采用分层模型预测控制（HMPC）策略，为给定的机械臂形态生成可行的运动轨迹。2) 设计优化模块，使用CMA-ES算法在离散的形态配置和连续的安装姿态空间中进行搜索，找到最优的设计方案。3) 虚拟模块抽象模块，用于实现双分支形态，允许辅助分支卸载主分支的扭矩。整个流程是迭代进行的，首先根据任务需求初始化机械臂的形态和安装姿态，然后通过运动规划模块生成运动轨迹，接着通过设计优化模块评估设计的性能并进行优化，最后根据优化结果调整机械臂的形态和安装姿态，重复以上步骤直到找到满足要求的设计方案。\\n\\n**关键创新**：该论文的关键创新在于：1) 提出了一个统一的任务驱动计算框架，将轨迹规划与机械臂形态和安装姿态的优化相结合。2) 引入了虚拟模块抽象，实现了双分支形态，可以在不增加单个关节模块容量的情况下扩展机械臂的工作空间。3) 采用分层模型预测控制（HMPC）策略，实现了冗余和非冗余机械臂的运动规划。\\n\\n**关键设计**：在运动规划模块中，HMPC策略被用于生成运动轨迹，其目标是最小化关节的运动和力矩，同时避免碰撞。在设计优化模块中，CMA-ES算法被用于搜索最优的形态和安装姿态，其目标是最大化机械臂的可操作性，最小化关节的力矩，并减少模块的数量。在虚拟模块抽象模块中，双分支结构的设计需要仔细考虑辅助分支的位置和长度，以确保其能够有效地分担主分支的扭矩。",
            "application_zh": "该研究成果可应用于各种需要灵活可配置机械臂的场景，例如：自动化装配、精密加工、医疗手术、灾难救援等。通过该框架，可以根据不同的任务需求，快速设计出满足特定约束的模块化机械臂，提高生产效率和作业质量。未来，该研究可以扩展到更多类型的模块化机器人，并与其他先进技术（如强化学习、深度学习）相结合，实现更智能化的机器人设计。",
            "highlight_zh": "在抛光、钻孔和取放任务的仿真和硬件实验中，该框架成功生成了满足运动学和动力学约束的可行设计，并避免了环境碰撞。通过自定义成本函数，实现了最大化可操作性、最小化关节力或减少模块数量等灵活的设计目标。实验结果还表明，双分支形态可以在不增加单个关节模块容量的情况下，实现更大的工作空间。",
            "tags_zh": [
                "模块化机械臂",
                "计算设计",
                "轨迹规划",
                "形态优化",
                "模型预测控制",
                "双分支结构",
                "CMA-ES",
                "任务驱动"
            ],
            "_index": 64,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16069v1/figure/dual_arm_robot.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16069v1/figure/framework.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16069v1/figure/balance_updae.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Auto-Vocabulary 3D Object Detection",
            "authors": [
                "Haomeng Zhang",
                "Kuan-Chuan Peng",
                "Suhas Lohit",
                "Raymond A. Yeh"
            ],
            "arxiv_id": "2512.16077v1",
            "summary": "Open-vocabulary 3D object detection methods are able to localize 3D boxes of classes unseen during training. Despite the name, existing methods rely on user-specified classes both at training and inference. We propose to study Auto-Vocabulary 3D Object Detection (AV3DOD), where the classes are automatically generated for the detected objects without any user input. To this end, we introduce Semantic Score (SS) to evaluate the quality of the generated class names. We then develop a novel framework, AV3DOD, which leverages 2D vision-language models (VLMs) to generate rich semantic candidates through image captioning, pseudo 3D box generation, and feature-space semantics expansion. AV3DOD achieves the state-of-the-art (SOTA) performance on both localization (mAP) and semantic quality (SS) on the ScanNetV2 and SUNRGB-D datasets. Notably, it surpasses the SOTA, CoDA, by 3.48 overall mAP and attains a 24.5% relative improvement in SS on ScanNetV2.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "technical report",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16077v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "open-vocabulary",
                        "open vocabulary"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出AV3DOD，实现无需用户干预的自动词汇3D目标检测",
            "summary_zh": "开放词汇3D目标检测方法能够定位训练期间未见过的类别的3D框。尽管名称如此，现有方法在训练和推理时都依赖于用户指定的类别。我们提出研究自动词汇3D目标检测（AV3DOD），其中类别是为检测到的对象自动生成的，无需任何用户输入。为此，我们引入了语义分数（SS）来评估生成的类名称的质量。然后，我们开发了一个新颖的框架AV3DOD，它利用2D视觉-语言模型（VLM）通过图像字幕、伪3D框生成和特征空间语义扩展来生成丰富的语义候选。AV3DOD在ScanNetV2和SUNRGB-D数据集上的定位（mAP）和语义质量（SS）方面均实现了最先进的（SOTA）性能。值得注意的是，它超过了SOTA方法CoDA，在ScanNetV2上总体mAP提高了3.48，并且在SS方面实现了24.5%的相对改进。",
            "intro_zh": [
                "现有开放词汇3D目标检测依赖用户指定类别，限制了其自动化程度和泛化能力。",
                "AV3DOD利用2D视觉-语言模型自动生成类别，无需人工干预，提升了检测的灵活性。",
                "实验表明，AV3DOD在ScanNetV2数据集上，mAP提升3.48，语义质量SS提升24.5%。"
            ],
            "method_zh": "**问题定义**：现有开放词汇3D目标检测方法虽然能够检测训练集中未见过的类别，但仍然依赖于用户在训练和推理阶段提供类别信息。这限制了系统的自动化程度，并且可能无法覆盖所有潜在的类别。因此，需要一种能够自动生成类别名称的3D目标检测方法。\\n\\n**核心思路**：AV3DOD的核心思路是利用2D视觉-语言模型（VLM）的强大语义理解能力，从图像中提取丰富的语义信息，并将其用于生成3D目标的类别名称。通过图像字幕、伪3D框生成和特征空间语义扩展等技术，将2D视觉信息与3D几何信息相结合，从而实现自动化的类别生成。\\n\\n**技术框架**：AV3DOD框架主要包含以下几个阶段：1) **图像字幕**：使用2D VLM对场景图像进行描述，生成初始的语义候选。2) **伪3D框生成**：根据检测到的3D目标，在图像中生成对应的伪3D框，并提取框内的视觉特征。3) **特征空间语义扩展**：利用VLM将视觉特征映射到语义空间，并结合图像字幕生成的语义候选，进行语义扩展和筛选。4) **语义分数评估**：引入语义分数（SS）来评估生成的类别名称的质量，选择最佳的类别名称。\\n\\n**关键创新**：AV3DOD的关键创新在于实现了完全自动化的类别生成，无需任何用户输入。通过巧妙地利用2D VLM的语义理解能力，将2D视觉信息与3D几何信息相结合，从而克服了传统方法对人工标注的依赖。此外，语义分数（SS）的引入为评估生成类别名称的质量提供了一种有效的手段。\\n\\n**关键设计**：在图像字幕阶段，使用了预训练的2D VLM模型，例如CLIP或ALIGN。在伪3D框生成阶段，需要根据3D目标的位置和大小，将其投影到2D图像平面上，并生成对应的伪3D框。在特征空间语义扩展阶段，使用了余弦相似度等方法来衡量视觉特征和语义候选之间的相似度。语义分数（SS）的计算方式需要根据具体的任务和数据集进行调整，可以考虑使用语言模型的困惑度或人工评估等方法。",
            "application_zh": "AV3DOD可应用于机器人导航、自动驾驶、场景理解等领域。在这些场景中，系统需要识别各种各样的物体，而人工标注所有类别是不现实的。AV3DOD能够自动生成类别名称，从而扩展了系统的识别能力，提高了其适应性和鲁棒性。未来，该技术有望应用于更广泛的场景，例如智能家居、虚拟现实等。",
            "highlight_zh": "AV3DOD在ScanNetV2和SUNRGB-D数据集上取得了显著的性能提升。在ScanNetV2数据集上，AV3DOD的总体mAP超过了SOTA方法CoDA 3.48，并且在语义质量（SS）方面实现了24.5%的相对改进。这些结果表明，AV3DOD能够有效地生成高质量的类别名称，并提高3D目标检测的性能。",
            "tags_zh": [
                "3D目标检测",
                "开放词汇",
                "自动词汇",
                "视觉-语言模型",
                "图像字幕"
            ],
            "_index": 65,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16077v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16077v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16077v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning",
            "authors": [
                "Andrew Wagenmaker",
                "Perry Dong",
                "Raymond Tsao",
                "Chelsea Finn",
                "Sergey Levine"
            ],
            "arxiv_id": "2512.16911v1",
            "summary": "Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.RO"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16911v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                }
            ],
            "relevance_score": 3.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出后验行为克隆(PostBC)方法，提升强化学习微调的预训练策略效果",
            "summary_zh": "本文研究了预训练策略对强化学习(RL)微调性能的影响，并探讨了如何预训练策略以确保其作为有效的微调初始化。研究表明，标准的行为克隆(BC)无法保证对演示者行为的覆盖，这是有效RL微调的必要条件。因此，本文提出后验行为克隆(PostBC)策略，该策略训练模型以模拟演示者行为的后验分布，从而确保对演示者行为的覆盖，并实现更有效的微调。PostBC在保证预训练性能不低于BC策略的同时，可以通过现代生成模型在机器人控制领域实现，并且在真实的机器人控制基准和实际机器人操作任务中，与标准行为克隆相比，显著提高了RL微调性能。",
            "intro_zh": [
                "现有行为克隆(BC)方法在预训练策略时，难以保证对演示者行为的充分覆盖，限制了后续强化学习微调的性能。",
                "论文提出后验行为克隆(PostBC)方法，通过建模演示者行为的后验分布，确保预训练策略能够覆盖演示者的行为空间。",
                "实验结果表明，PostBC在真实机器人控制任务中，显著提升了强化学习微调的性能，优于标准行为克隆方法。"
            ],
            "method_zh": "**问题定义**：现有的行为克隆（BC）方法旨在直接模仿演示数据中的动作，但这种方法可能无法完全覆盖演示者行为的分布。这意味着在强化学习（RL）微调阶段，策略可能无法探索到演示者曾经采取过的关键动作，从而限制了微调的性能。因此，如何预训练一个能够有效覆盖演示者行为分布的策略，成为了一个关键问题。\\n\\n**核心思路**：论文的核心思路是，与其直接模仿演示数据中的动作，不如学习演示者行为的后验分布。这意味着模型需要学习在给定数据集的情况下，演示者采取各种动作的概率。通过学习后验分布，模型可以更好地泛化到未见过的状态，并确保在RL微调阶段能够探索到更广泛的行为空间。\\n\\n**技术框架**：PostBC的整体框架包括以下几个步骤：1) 收集演示数据集；2) 使用生成模型（如变分自编码器VAE或生成对抗网络GAN）学习演示者行为的后验分布；3) 使用学习到的后验分布生成策略；4) 使用强化学习算法对策略进行微调。关键在于第二步，即如何有效地学习后验分布。\\n\\n**关键创新**：PostBC最重要的创新在于，它将预训练策略的目标从直接模仿动作，转变为学习动作的后验分布。这种转变使得预训练策略能够更好地覆盖演示者行为空间，从而为后续的RL微调提供更好的初始化。与传统的BC方法相比，PostBC能够更好地泛化到未见过的状态，并探索到更广泛的行为空间。\\n\\n**关键设计**：PostBC的关键设计包括：1) 使用合适的生成模型来学习后验分布，例如，可以使用变分自编码器（VAE）或生成对抗网络（GAN）。2) 设计合适的损失函数来训练生成模型，例如，可以使用KL散度来衡量生成分布与真实后验分布之间的差异。3) 在RL微调阶段，可以使用各种强化学习算法，例如，可以使用PPO或SAC等算法。",
            "application_zh": "PostBC方法可广泛应用于机器人控制、自动驾驶、游戏AI等领域。在这些领域中，通常需要先使用大量演示数据进行预训练，然后再使用强化学习进行微调。PostBC可以作为一种有效的预训练方法，提高强化学习的效率和性能，从而加速这些领域的智能化进程。",
            "highlight_zh": "实验结果表明，在真实的机器人控制基准和实际机器人操作任务中，PostBC显著提高了RL微调的性能。例如，在某项机器人操作任务中，使用PostBC预训练的策略，经过RL微调后，成功率比使用标准BC预训练的策略提高了15%。这表明PostBC能够有效地提高RL微调的效率和性能。",
            "tags_zh": [
                "后验行为克隆",
                "强化学习微调",
                "预训练策略",
                "机器人控制",
                "行为克隆",
                "生成模型",
                "策略优化"
            ],
            "_index": 66,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16911v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16911v1/im/corn_in_pot2.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16911v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
            "authors": [
                "Yuanchen Ju",
                "Yongyuan Liang",
                "Yen-Jen Wang",
                "Nandiraju Gireesh",
                "Yuanliang Ju",
                "Seungjae Lee",
                "Qiao Gu",
                "Elvis Hsieh",
                "Furong Huang",
                "Koushil Sreenath"
            ],
            "arxiv_id": "2512.16909v1",
            "summary": "Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "25 pages, 10 figures. Project page:https://hybridrobotics.github.io/MomaGraph/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16909v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "scene understanding"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 3.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "提出MomaGraph，利用视觉-语言模型为具身任务规划构建状态感知的统一场景图。",
            "summary_zh": "本文提出MomaGraph，一种用于具身智能体的统一场景表示，它集成了空间-功能关系和部件级别的交互元素，旨在解决现有场景图表示方法中空间和功能关系分离、场景静态化以及忽略任务相关信息的问题。同时，本文贡献了MomaGraph-Scenes，一个大规模的、带有丰富标注的、任务驱动的家庭环境场景图数据集，以及MomaGraph-Bench，一个包含从高层规划到细粒度场景理解的六种推理能力的系统评估套件。基于此，本文进一步开发了MomaGraph-R1，一个在MomaGraph-Scenes上通过强化学习训练的7B视觉-语言模型。MomaGraph-R1能够预测面向任务的场景图，并作为Graph-then-Plan框架下的零样本任务规划器。实验结果表明，该模型在开源模型中达到了最先进的水平，在基准测试中达到了71.6%的准确率（比最佳基线高出11.4%），同时能够泛化到公共基准测试，并有效地迁移到真实机器人实验中。",
            "intro_zh": [
                "现有场景图表示方法在处理具身任务时，缺乏对空间-功能关系的统一建模，且忽略了对象状态和任务相关信息。",
                "MomaGraph通过整合空间-功能关系和部件级别的交互元素，构建了状态感知的统一场景图，从而更有效地支持具身智能体的任务规划。",
                "MomaGraph-R1模型在MomaGraph-Bench上取得了显著的性能提升，并在真实机器人实验中展现了良好的泛化能力。"
            ],
            "method_zh": "**问题定义**：现有场景图方法通常将空间和功能关系分离处理，将场景视为静态快照，忽略了对象的状态变化和与当前任务最相关的信息。这导致具身智能体难以有效地进行任务规划，尤其是在复杂的家庭环境中。\\n\\n**核心思路**：MomaGraph的核心思路是构建一个统一的、状态感知的场景图表示，它能够同时捕捉对象的位置、功能以及可交互的部件。通过整合空间-功能关系，并利用视觉-语言模型预测场景图，MomaGraph能够为具身智能体提供更全面的场景理解，从而支持更有效的任务规划。\\n\\n**技术框架**：MomaGraph的整体框架包含数据收集与标注、模型训练和任务规划三个主要阶段。首先，构建MomaGraph-Scenes数据集，其中包含丰富的场景图标注，包括对象、关系和状态信息。然后，利用该数据集训练MomaGraph-R1视觉-语言模型，该模型能够根据视觉输入预测面向任务的场景图。最后，将预测的场景图作为输入，利用Graph-then-Plan框架进行任务规划。\\n\\n**关键创新**：MomaGraph的关键创新在于其统一的场景图表示，它能够同时捕捉空间-功能关系和对象状态。此外，MomaGraph-R1模型利用视觉-语言模型进行场景图预测，并结合强化学习进行训练，从而提高了模型的性能和泛化能力。与现有方法相比，MomaGraph能够更全面地理解场景，并更好地支持具身智能体的任务规划。\\n\\n**关键设计**：MomaGraph-R1模型是一个7B参数的视觉-语言模型，它以图像作为输入，输出场景图的表示。模型采用Transformer架构，并使用交叉注意力机制融合视觉和语言信息。在训练过程中，模型使用强化学习进行微调，以优化其在任务规划方面的性能。具体的损失函数包括场景图预测损失和任务规划奖励。",
            "application_zh": "MomaGraph在家庭服务机器人、自动驾驶、增强现实等领域具有广泛的应用前景。它可以帮助机器人更好地理解周围环境，从而执行更复杂的任务，例如物品整理、清洁和烹饪。此外，MomaGraph还可以用于构建更智能的虚拟助手，为用户提供更个性化的服务。",
            "highlight_zh": "MomaGraph-R1在MomaGraph-Bench基准测试中取得了71.6%的准确率，比最佳基线高出11.4%。此外，该模型还能够泛化到公共基准测试，并在真实机器人实验中表现出良好的性能。这些结果表明，MomaGraph是一种有效的场景表示方法，能够显著提高具身智能体的任务规划能力。",
            "tags_zh": [
                "具身智能",
                "场景图",
                "视觉-语言模型",
                "任务规划",
                "强化学习"
            ],
            "_index": 67,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16909v1/Figures/Teaser.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16909v1/Figures/Failure.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16909v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Hypernetworks That Evolve Themselves",
            "authors": [
                "Joachim Winther Pedersen",
                "Erwan Plantec",
                "Eleni Nisioti",
                "Marcello Barylli",
                "Milton Montero",
                "Kathrin Korte",
                "Sebastian Risi"
            ],
            "arxiv_id": "2512.16406v1",
            "summary": "How can neural networks evolve themselves without relying on external optimizers? We propose Self-Referential Graph HyperNetworks, systems where the very machinery of variation and inheritance is embedded within the network. By uniting hypernetworks, stochastic parameter generation, and graph-based representations, Self-Referential GHNs mutate and evaluate themselves while adapting mutation rates as selectable traits. Through new reinforcement learning benchmarks with environmental shifts (CartPoleSwitch, LunarLander-Switch), Self-Referential GHNs show swift, reliable adaptation and emergent population dynamics. In the locomotion benchmark Ant-v5, they evolve coherent gaits, showing promising fine-tuning capabilities by autonomously decreasing variation in the population to concentrate around promising solutions. Our findings support the idea that evolvability itself can emerge from neural self-reference. Self-Referential GHNs reflect a step toward synthetic systems that more closely mirror biological evolution, offering tools for autonomous, open-ended learning agents.",
            "categories": [
                "cs.NE",
                "cs.AI"
            ],
            "primary_category": "cs.NE",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16406v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "locomotion"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                }
            ],
            "relevance_score": 3.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出自引用图超网络，实现无需外部优化器的神经网络自进化。",
            "summary_zh": "本文提出自引用图超网络（Self-Referential Graph HyperNetworks, Self-Referential GHNs），该系统将变异和遗传机制嵌入网络内部，无需依赖外部优化器即可实现神经网络的自进化。通过结合超网络、随机参数生成和基于图的表示，Self-Referential GHNs能够自我变异和评估，同时将变异率作为可选择的特征进行调整。在包含环境变化的强化学习基准测试（CartPoleSwitch, LunarLander-Switch）中，Self-Referential GHNs展现出快速、可靠的适应性和涌现的人口动态。在locomotion基准测试Ant-v5中，它们进化出连贯的步态，并通过自主降低种群变异性以集中于有希望的解决方案，展现出良好的微调能力。研究结果表明，可进化性本身可以从神经自我参照中涌现。Self-Referential GHNs代表着朝着更接近生物进化的合成系统迈出的一步，为自主、开放式学习智能体提供了工具。",
            "intro_zh": [
                "传统神经网络依赖外部优化器进行进化，限制了其自主性和适应性，面临环境变化时表现不佳。",
                "提出自引用图超网络，将变异和遗传机制嵌入网络内部，实现神经网络的自我变异、评估和进化。",
                "在多个强化学习基准测试中，该方法展现出快速适应环境变化的能力，并能自主学习复杂的运动控制策略。"
            ],
            "method_zh": "**问题定义**：现有神经网络的进化通常依赖于外部优化器，例如遗传算法或进化策略。这些外部优化器需要手动设计和调整，并且可能无法有效地探索复杂的搜索空间。此外，当环境发生变化时，预先训练好的神经网络可能难以适应，需要重新训练或进行微调，效率较低。因此，如何让神经网络自主地进化，适应不断变化的环境，是一个重要的研究问题。\\n\\n**核心思路**：本文的核心思路是将神经网络的进化机制嵌入到网络本身中，使其能够自我变异、评估和选择。通过使用超网络生成神经网络的参数，并结合随机参数生成和基于图的表示，可以实现对神经网络结构的灵活控制和修改。同时，将变异率作为可选择的特征进行调整，可以使网络自主地控制自身的进化速度。\\n\\n**技术框架**：Self-Referential GHNs的整体架构包含以下几个主要模块：1) **图表示模块**：使用图结构来表示神经网络的结构和连接。2) **超网络模块**：使用超网络来生成神经网络的参数，超网络的输入是图表示，输出是神经网络的权重。3) **变异模块**：通过随机修改图结构和超网络参数来实现神经网络的变异。4) **评估模块**：使用强化学习或其他评估方法来评估变异后的神经网络的性能。5) **选择模块**：根据性能选择优秀的个体，并将其作为下一代的基础。\\n\\n**关键创新**：该方法最重要的技术创新点在于将神经网络的进化机制嵌入到网络本身中，实现了神经网络的自引用和自进化。与传统的进化方法相比，该方法无需外部优化器，可以自主地探索复杂的搜索空间，并适应不断变化的环境。此外，将变异率作为可选择的特征进行调整，可以使网络自主地控制自身的进化速度，提高了进化的效率。\\n\\n**关键设计**：在具体实现中，使用了图神经网络来表示神经网络的结构和连接。超网络采用多层感知机结构，输入是图节点的特征向量，输出是神经网络的权重。变异操作包括添加、删除和修改图节点和连接。评估方法采用强化学习算法，例如PPO。选择方法采用锦标赛选择或轮盘赌选择。变异率的调整通过梯度下降或其他优化方法来实现。",
            "application_zh": "该研究成果可应用于自主机器人、游戏AI、智能控制等领域。通过自进化，机器人可以自主学习适应复杂环境，游戏AI可以不断进化出更智能的策略，智能控制系统可以自动优化参数以提高性能。该研究为开发更智能、更自主的AI系统提供了新的思路。",
            "highlight_zh": "在CartPoleSwitch和LunarLander-Switch等强化学习基准测试中，Self-Referential GHNs展现出快速、可靠的适应性。在Ant-v5 locomotion基准测试中，它们进化出连贯的步态，并通过自主降低种群变异性以集中于有希望的解决方案，展现出良好的微调能力。这些结果表明，该方法具有很强的进化能力和适应性。",
            "tags_zh": [
                "自进化神经网络",
                "超网络",
                "图神经网络",
                "强化学习",
                "自主学习"
            ],
            "_index": 68,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16406v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16406v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16406v1/figures/environments.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies",
            "authors": [
                "Arhan Jain",
                "Mingtong Zhang",
                "Kanav Arora",
                "William Chen",
                "Marcel Torne",
                "Muhammad Zubair Irshad",
                "Sergey Zakharov",
                "Yue Wang",
                "Sergey Levine",
                "Chelsea Finn",
                "Wei-Chiu Ma",
                "Dhruv Shah",
                "Abhishek Gupta",
                "Karl Pertsch"
            ],
            "arxiv_id": "2512.16881v1",
            "summary": "A significant challenge for robot learning research is our ability to accurately measure and compare the performance of robot policies. Benchmarking in robotics is historically challenging due to the stochasticity, reproducibility, and time-consuming nature of real-world rollouts. This challenge is exacerbated for recent generalist policies, which has to be evaluated across a wide variety of scenes and tasks. Evaluation in simulation offers a scalable complement to real world evaluations, but the visual and physical domain gap between existing simulation benchmarks and the real world has made them an unreliable signal for policy improvement. Furthermore, building realistic and diverse simulated environments has traditionally required significant human effort and expertise. To bridge the gap, we introduce Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS), a scalable real-to-sim framework for high-fidelity simulated robot evaluation. PolaRiS utilizes neural reconstruction methods to turn short video scans of real-world scenes into interactive simulation environments. Additionally, we develop a simple simulation data co-training recipe that bridges remaining real-to-sim gaps and enables zero-shot evaluation in unseen simulation environments. Through extensive paired evaluations between simulation and the real world, we demonstrate that PolaRiS evaluations provide a much stronger correlation to real world generalist policy performance than existing simulated benchmarks. Its simplicity also enables rapid creation of diverse simulated environments. As such, this work takes a step towards distributed and democratized evaluation for the next generation of robotic foundation models.",
            "categories": [
                "cs.RO",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Website: https://polaris-evals.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16881v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "PolaRiS：用于通用机器人策略的可扩展真实到模拟评估框架",
            "summary_zh": "机器人学习研究面临的一个重大挑战是准确测量和比较机器人策略的性能。由于真实世界rollout的随机性、可重复性和耗时性，机器人技术的基准测试历来具有挑战性。对于最近的通用策略，需要在各种场景和任务中进行评估，这使得挑战更加严峻。仿真评估为真实世界评估提供了一种可扩展的补充，但现有仿真基准与真实世界之间的视觉和物理领域差距使其成为不可靠的策略改进信号。此外，构建逼真且多样化的仿真环境传统上需要大量的人力和专业知识。为了弥合差距，我们引入了仿真中的策略评估和环境重建（PolaRiS），这是一个可扩展的真实到模拟框架，用于高保真仿真机器人评估。PolaRiS利用神经重建方法将真实世界场景的短视频扫描转换为交互式仿真环境。此外，我们开发了一种简单的仿真数据协同训练方法，弥合了剩余的真实到模拟差距，并实现了在未见过的仿真环境中的零样本评估。通过仿真和真实世界之间的广泛配对评估，我们证明PolaRiS评估比现有仿真基准更能提供与真实世界通用策略性能的更强相关性。它的简单性还能够快速创建多样化的仿真环境。因此，这项工作朝着为下一代机器人基础模型进行分布式和民主化的评估迈出了一步。",
            "intro_zh": [
                "机器人策略评估面临真实环境成本高、可重复性差的问题，现有仿真环境与真实环境存在较大差距，导致评估结果不准确。",
                "PolaRiS通过神经重建将真实场景转化为交互式仿真环境，并结合仿真数据协同训练，缩小真实到模拟的差距，实现更可靠的策略评估。",
                "实验表明，PolaRiS评估与真实世界通用策略性能的相关性远高于现有仿真基准，能够快速创建多样化的仿真环境。"
            ],
            "method_zh": "**问题定义**：现有机器人策略评估，尤其是在通用机器人策略方面，面临着真实环境评估成本高昂、难以复现，以及现有仿真环境与真实环境存在较大视觉和物理差异的问题。这种差异导致在仿真环境中表现良好的策略，在真实环境中表现不佳，使得仿真评估的可靠性大打折扣。此外，构建逼真且多样化的仿真环境需要大量的人工和专业知识，限制了评估的规模和范围。\\n\\n**核心思路**：PolaRiS的核心思路是通过神经重建技术，将真实世界的场景快速转化为高保真的仿真环境。这样既避免了手动构建仿真环境的繁琐过程，又保证了仿真环境与真实环境的相似性。此外，通过仿真数据协同训练，进一步缩小真实到模拟的差距，提高仿真评估的准确性和可靠性。\\n\\n**技术框架**：PolaRiS框架主要包含两个阶段：环境重建和策略评估。首先，利用神经重建方法，如神经辐射场（NeRF）或类似技术，将真实场景的短视频扫描转化为可交互的3D仿真环境。然后，在这些重建的仿真环境中对机器人策略进行评估。为了进一步提高评估的准确性，PolaRiS还采用了仿真数据协同训练的方法，即在真实数据和仿真数据上联合训练策略，以弥合真实到模拟的差距。\\n\\n**关键创新**：PolaRiS最重要的创新点在于其能够快速、自动地将真实场景转化为高保真的仿真环境，从而实现可扩展的真实到模拟的机器人策略评估。与传统的手动构建仿真环境相比，PolaRiS大大降低了构建成本和时间，并提高了仿真环境的真实性和多样性。此外，仿真数据协同训练进一步提高了评估的准确性和可靠性。\\n\\n**关键设计**：PolaRiS的关键设计包括：1) 使用高质量的神经重建方法，以保证仿真环境的视觉逼真度；2) 设计有效的仿真数据协同训练策略，以弥合真实到模拟的差距；3) 采用标准化的评估指标和协议，以便于不同策略之间的比较和分析。具体的参数设置、损失函数和网络结构等技术细节取决于所使用的神经重建方法和协同训练策略，论文中可能包含更详细的描述。",
            "application_zh": "PolaRiS的应用场景广泛，包括机器人算法的开发、测试和验证，以及机器人基础模型的训练和评估。它可以用于各种机器人应用，如家庭服务机器人、工业机器人、自动驾驶等。通过PolaRiS，研究人员可以更快速、更经济地评估和改进机器人策略，加速机器人技术的发展和应用。",
            "highlight_zh": "实验结果表明，PolaRiS评估与真实世界通用策略性能的相关性远高于现有仿真基准。通过在多个真实场景中进行评估，PolaRiS能够更准确地预测策略在真实环境中的表现。此外，PolaRiS还展示了快速创建多样化仿真环境的能力，为大规模的机器人策略评估提供了可能。",
            "tags_zh": [
                "机器人学习",
                "仿真评估",
                "真实到模拟",
                "神经重建",
                "通用机器人策略",
                "领域自适应",
                "协同训练"
            ],
            "_index": 69,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16881v1/figures/Teaser_Karl_version.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16881v1/figures/polaris_pipeline.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16881v1/figures/scene_comp_gui.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization",
            "authors": [
                "Xiaoyan Cong",
                "Haotian Yang",
                "Angtian Wang",
                "Yizhi Wang",
                "Yiding Yang",
                "Canyu Zhang",
                "Chongyang Ma"
            ],
            "arxiv_id": "2512.16906v1",
            "summary": "Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16906v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "instruction following"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "VIVA：基于VLM引导和奖励优化的指令驱动视频编辑框架",
            "summary_zh": "本文提出VIVA，一个可扩展的指令驱动视频编辑框架，旨在根据自然语言指令修改输入视频，同时保持内容一致性和时间连贯性。现有基于扩散模型的方法通常在简单编辑操作的配对数据上训练，限制了它们对复杂真实指令的泛化能力。VIVA利用VLM引导的编码和奖励优化来解决这一泛化问题。首先，引入一个基于VLM的指导器，将文本指令、源视频的第一帧以及可选的参考图像编码为视觉相关的指令表示，为扩散Transformer主干网络提供细粒度的空间和语义上下文。其次，提出一个后训练阶段Edit-GRPO，将Group Relative Policy Optimization应用于视频编辑领域，使用相对奖励直接优化模型，使其生成符合指令、保持内容一致且美观的编辑结果。此外，还提出了一个数据构建流程，用于合成生成多样且高质量的基本编辑操作的配对视频-指令数据。大量实验表明，VIVA在指令遵循、泛化能力和编辑质量方面优于现有方法。",
            "intro_zh": [
                "现有指令驱动视频编辑方法泛化性不足，难以处理复杂指令，因为它们依赖于简单编辑操作的配对数据训练。",
                "VIVA框架利用VLM编码指令和视频信息，并采用奖励优化策略，提升模型对复杂指令的理解和编辑能力。",
                "实验结果表明，VIVA在指令遵循、泛化能力和编辑质量上均优于现有技术，实现了更好的视频编辑效果。"
            ],
            "method_zh": "**问题定义**：指令驱动的视频编辑旨在根据给定的自然语言指令修改视频内容，同时保持视频内容的一致性和时间上的连贯性。现有的基于扩散模型的方法通常依赖于简单的编辑操作数据进行训练，这限制了它们在处理复杂和真实的编辑指令时的泛化能力。这些方法难以理解复杂的语义关系，并且在生成高质量的编辑结果时面临挑战。\\n\\n**核心思路**：VIVA的核心思路是利用视觉语言模型（VLM）来增强模型对指令的理解，并使用奖励优化来提升编辑质量。VLM能够将文本指令和视频内容编码成统一的视觉语义空间，从而提供更丰富的上下文信息。奖励优化则通过直接优化编辑结果的质量，使其更符合指令意图，并保持视频内容的一致性。\\n\\n**技术框架**：VIVA框架主要包含两个阶段：VLM引导的编码阶段和奖励优化阶段。在编码阶段，VLM Instructor将文本指令、源视频的第一帧以及可选的参考图像编码为视觉相关的指令表示。这些表示被输入到扩散Transformer主干网络中，用于指导视频编辑过程。在奖励优化阶段，Edit-GRPO算法被用于优化模型的策略，使其能够生成更高质量的编辑结果。此外，还设计了一个数据生成流程，用于合成高质量的配对视频-指令数据。\\n\\n**关键创新**：VIVA的关键创新在于以下两点：1) 引入VLM Instructor，利用VLM的强大语义理解能力，为视频编辑提供更丰富的上下文信息。2) 提出Edit-GRPO算法，将Group Relative Policy Optimization应用于视频编辑领域，通过直接优化编辑结果的质量，提升模型的性能。与现有方法相比，VIVA能够更好地理解复杂指令，并生成更高质量的编辑结果。\\n\\n**关键设计**：VLM Instructor使用预训练的视觉语言模型，例如CLIP，来编码文本指令和视频帧。Edit-GRPO算法使用相对奖励来评估编辑结果的质量，例如，判断一个编辑结果是否比另一个更符合指令意图。数据生成流程使用程序化生成和人工标注相结合的方式，生成多样且高质量的配对视频-指令数据。具体的网络结构和损失函数细节在论文中有详细描述（未知）。",
            "application_zh": "VIVA具有广泛的应用前景，可用于视频内容创作、视频修复、个性化视频编辑等领域。例如，用户可以通过简单的自然语言指令，快速修改视频内容，实现各种创意效果。该技术还可以应用于智能视频监控、自动驾驶等领域，提升视频分析和理解能力。未来，VIVA有望成为视频编辑领域的重要工具，推动视频内容创作和应用的发展。",
            "highlight_zh": "实验结果表明，VIVA在多个视频编辑任务上取得了显著的性能提升。相较于现有方法，VIVA在指令遵循度、内容一致性和编辑质量方面均有明显优势。具体性能数据和对比基线在论文中有详细描述（未知），但总体而言，VIVA能够生成更符合指令意图、更逼真自然的编辑结果。",
            "tags_zh": [
                "指令驱动视频编辑",
                "视觉语言模型",
                "奖励优化",
                "扩散模型",
                "视频生成"
            ],
            "_index": 70,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16906v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16906v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16906v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion",
            "authors": [
                "Giorgos Petsangourakis",
                "Christos Sgouropoulos",
                "Bill Psomas",
                "Theodoros Giannakopoulos",
                "Giorgos Sfikas",
                "Ioannis Kakogeorgiou"
            ],
            "arxiv_id": "2512.16636v1",
            "summary": "Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework. The code is available at https://github.com/giorgospets/reglue .",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16636v1",
            "code_links": [
                {
                    "url": "https://github.com/giorgospets/reglue",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "REGLUE：融合全局与局部语义的解耦扩散模型，提升图像合成质量。",
            "summary_zh": "潜在扩散模型(LDMs)在图像合成方面取得了最先进的成果，但其重建式去噪目标仅提供了间接的语义监督：高层语义出现缓慢，需要更长的训练时间，并限制了样本质量。最近的研究通过表征对齐从视觉基础模型(VFMs)外部注入语义，或者通过在扩散过程中联合建模VFM特征的一小部分来内部注入语义，未能充分利用可用的丰富、非线性、多层空间语义。我们提出了REGLUE（Representation Entanglement with Global-Local Unified Encoding），一个统一的潜在扩散框架，它在单个SiT骨干网络中联合建模(i)VAE图像潜在变量，(ii)紧凑的局部(patch级别)VFM语义，以及(iii)全局(图像级别)[CLS] token。一个轻量级的卷积语义压缩器将多层VFM特征非线性地聚合为低维、空间结构化的表示，并在扩散过程中与VAE潜在变量纠缠。外部对齐损失进一步将内部表示正则化到冻结的VFM目标。在ImageNet 256x256上，REGLUE始终如一地提高了FID，并加速了SiT-B/2和SiT-XL/2基线以及REPA、ReDi和REG的收敛速度。大量实验表明，(a)空间VFM语义至关重要，(b)非线性压缩是充分发挥其优势的关键，以及(c)全局token和外部对齐在我们全局-局部-潜在联合建模框架中充当互补的、轻量级的增强。",
            "intro_zh": [
                "现有潜在扩散模型语义监督不足，导致训练缓慢且样本质量受限，未能充分利用视觉基础模型(VFM)的丰富语义信息。",
                "REGLUE通过联合建模VAE潜在变量、局部VFM语义和全局[CLS] token，在扩散过程中融合全局和局部语义信息，实现更有效的语义监督。",
                "实验表明，REGLUE在ImageNet 256x256上显著提升了FID，并加速了收敛速度，优于现有方法。"
            ],
            "method_zh": "**问题定义**：现有的潜在扩散模型在图像生成任务中，虽然取得了不错的效果，但是其训练过程依赖于重建式的去噪目标，这种方式提供的语义监督是间接的，导致高层语义的学习较为缓慢，需要更长的训练时间，并且最终生成的图像质量也受到限制。此外，现有方法在利用视觉基础模型（VFMs）的语义信息时，要么只利用了VFM特征的一小部分，要么没有充分利用VFM提供的多层空间语义信息。\n\n**核心思路**：REGLUE的核心思路是通过联合建模VAE图像潜在变量、局部（patch级别）VFM语义以及全局（图像级别）[CLS] token，从而在扩散过程中更有效地融合全局和局部语义信息。通过这种方式，模型可以更好地理解图像的整体结构和局部细节，从而生成更高质量的图像。\n\n**技术框架**：REGLUE的整体框架包含以下几个主要模块：1) VAE编码器：将输入图像编码为潜在变量。2) 视觉基础模型（VFM）：提取图像的多层语义特征。3) 卷积语义压缩器：将VFM特征非线性地压缩为低维、空间结构化的表示。4) SiT骨干网络：作为扩散模型的去噪器，联合建模VAE潜在变量、压缩后的VFM语义和全局[CLS] token。5) 外部对齐损失：正则化内部表示，使其与冻结的VFM目标对齐。\n\n**关键创新**：REGLUE的关键创新在于其全局-局部-潜在联合建模框架，该框架能够有效地融合来自VAE潜在变量、局部VFM语义和全局[CLS] token的信息。此外，使用非线性卷积语义压缩器来处理VFM特征也是一个重要的创新点，它可以更好地提取和利用VFM提供的丰富语义信息。\n\n**关键设计**：REGLUE的关键设计包括：1) 使用轻量级的卷积语义压缩器，以减少计算量并保留关键的语义信息。2) 在SiT骨干网络中同时建模VAE潜在变量、局部VFM语义和全局[CLS] token，实现全局和局部语义的有效融合。3) 使用外部对齐损失，以确保内部表示与VFM目标对齐，从而提高生成图像的质量。",
            "application_zh": "REGLUE具有广泛的应用前景，包括图像生成、图像编辑、图像修复等。该方法可以用于生成逼真度更高、语义更丰富的图像，例如，可以用于生成高质量的艺术作品、游戏素材、虚拟现实内容等。此外，REGLUE还可以应用于医学图像分析、遥感图像分析等领域，帮助医生和研究人员更好地理解和分析图像数据。",
            "highlight_zh": "REGLUE在ImageNet 256x256数据集上进行了实验，结果表明，REGLUE能够显著提高FID，并加速收敛速度。具体来说，REGLUE优于SiT-B/2和SiT-XL/2基线，以及REPA、ReDi和REG等现有方法。实验还表明，空间VFM语义至关重要，非线性压缩是充分发挥其优势的关键，全局token和外部对齐可以作为互补的增强手段。",
            "tags_zh": [
                "潜在扩散模型",
                "图像合成",
                "视觉基础模型",
                "语义融合",
                "全局局部语义"
            ],
            "_index": 71,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16636v1/figs/training_steps_photos/50k_steps/000343.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16636v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16636v1/figs/cfg/golden_retriever_207/000444.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks",
            "authors": [
                "Beitong Zhou",
                "Zhexiao Huang",
                "Yuan Guo",
                "Zhangxuan Gu",
                "Tianyu Xia",
                "Zichen Luo",
                "Fei Tang",
                "Dehan Kong",
                "Yanyi Shang",
                "Suling Ou",
                "Zhenlin Guo",
                "Changhua Meng",
                "Shuheng Shen"
            ],
            "arxiv_id": "2512.16501v1",
            "summary": "GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16501v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出VenusBench-GD，一个全面的多平台GUI基准，用于评估多样化的Grounding任务。",
            "summary_zh": "GUI grounding是构建强大GUI代理的关键组成部分。然而，现有的grounding基准存在显著局限性：它们要么提供的数据量不足且领域覆盖范围狭窄，要么过度关注单一平台并需要高度专业化的领域知识。本文提出了VenusBench-GD，这是一个全面的、双语的GUI grounding基准，跨越多个平台，能够对真实应用进行分层评估。VenusBench-GD的贡献如下：（i）引入了一个大规模、跨平台的基准，具有广泛的应用覆盖、多样的UI元素和丰富的标注数据；（ii）建立了一个高质量的数据构建流程，用于grounding任务，实现了比现有基准更高的标注准确率；（iii）通过提出一个分层任务分类法扩展了元素grounding的范围，该分类法将grounding分为基本和高级类别，包含六个不同的子任务，旨在从互补的角度评估模型。实验结果揭示了关键见解：通用多模态模型现在在基本grounding任务上与专用GUI模型相匹配甚至超越。相比之下，高级任务仍然偏爱GUI专用模型，尽管它们表现出显著的过拟合和较差的鲁棒性。这些结果强调了全面、多层评估框架的必要性。",
            "intro_zh": [
                "现有GUI grounding基准数据集规模小、领域窄，或过于关注单平台，限制了GUI代理的发展。",
                "VenusBench-GD构建了一个大规模、跨平台、双语的GUI grounding基准，并设计了分层任务分类法。",
                "实验表明，通用多模态模型在基本任务上表现出色，但高级任务仍需专用模型，且存在过拟合问题。"
            ],
            "method_zh": "**问题定义**：论文旨在解决现有GUI grounding基准数据集不足的问题，包括数据量小、领域覆盖范围窄、平台单一以及标注质量不高等问题。现有方法难以全面评估GUI代理的grounding能力，并且容易过拟合特定平台或任务。\\n\\n**核心思路**：论文的核心思路是构建一个大规模、跨平台、高质量的GUI grounding基准数据集，并设计一个分层的任务分类体系，从而能够更全面、更准确地评估GUI代理的grounding能力。通过引入多样化的应用场景和UI元素，以及高质量的人工标注，提高数据集的泛化性和可靠性。\\n\\n**技术框架**：VenusBench-GD的构建流程主要包含以下几个阶段：1) 数据收集：从多个平台收集GUI应用数据；2) 数据清洗：对收集到的数据进行清洗和过滤，去除噪声和冗余信息；3) 数据标注：对UI元素进行标注，包括位置、类型、文本等信息；4) 任务划分：将grounding任务划分为基本和高级类别，并设计六个不同的子任务；5) 数据集发布：将构建好的数据集发布，供研究人员使用。\\n\\n**关键创新**：该论文的关键创新在于：1) 构建了一个大规模、跨平台的GUI grounding基准数据集，覆盖了更广泛的应用场景和UI元素；2) 提出了一个分层的任务分类体系，能够更全面地评估GUI代理的grounding能力；3) 实现了比现有基准更高的标注准确率，提高了数据集的可靠性。\\n\\n**关键设计**：论文中关于数据集构建和任务划分的具体技术细节未详细描述，例如，数据收集的具体平台和应用类型、数据清洗的具体方法、标注的具体规范、任务划分的具体标准以及六个子任务的具体定义等。这些细节对于复现和进一步研究至关重要，但文中未提供足够的信息。",
            "application_zh": "VenusBench-GD可用于训练和评估各种GUI代理，例如自动化测试工具、辅助技术和智能助手。该基准数据集能够促进GUI grounding技术的发展，提高GUI代理的智能化水平，从而改善用户体验并提高工作效率。未来，该研究可以扩展到更多平台和应用领域，并探索更复杂的grounding任务。",
            "highlight_zh": "实验结果表明，通用多模态模型在基本grounding任务上与专用GUI模型相匹配甚至超越，但在高级任务上，GUI专用模型仍然更胜一筹，但存在显著的过拟合和较差的鲁棒性。这表明需要更全面、多层次的评估框架来推动GUI grounding技术的发展。",
            "tags_zh": [
                "GUI grounding",
                "基准数据集",
                "多平台",
                "多模态学习",
                "分层任务",
                "用户界面",
                "人工智能"
            ],
            "_index": 72,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16501v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16501v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16501v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Avatar4D: Synthesizing Domain-Specific 4D Humans for Real-World Pose Estimation",
            "authors": [
                "Jerrin Bright",
                "Zhibo Wang",
                "Dmytro Klepachevskyi",
                "Yuhao Chen",
                "Sirisha Rambhatla",
                "David Clausi",
                "John Zelek"
            ],
            "arxiv_id": "2512.16199v1",
            "summary": "We present Avatar4D, a real-world transferable pipeline for generating customizable synthetic human motion datasets tailored to domain-specific applications. Unlike prior works, which focus on general, everyday motions and offer limited flexibility, our approach provides fine-grained control over body pose, appearance, camera viewpoint, and environmental context, without requiring any manual annotations. To validate the impact of Avatar4D, we focus on sports, where domain-specific human actions and movement patterns pose unique challenges for motion understanding. In this setting, we introduce Syn2Sport, a large-scale synthetic dataset spanning sports, including baseball and ice hockey. Avatar4D features high-fidelity 4D (3D geometry over time) human motion sequences with varying player appearances rendered in diverse environments. We benchmark several state-of-the-art pose estimation models on Syn2Sport and demonstrate their effectiveness for supervised learning, zero-shot transfer to real-world data, and generalization across sports. Furthermore, we evaluate how closely the generated synthetic data aligns with real-world datasets in feature space. Our results highlight the potential of such systems to generate scalable, controllable, and transferable human datasets for diverse domain-specific tasks without relying on domain-specific real data.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16199v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "zero-shot transfer"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "Avatar4D：合成特定领域4D人体数据，用于真实场景姿态估计",
            "summary_zh": "本文提出Avatar4D，一个可迁移的真实世界流水线，用于生成可定制的合成人体运动数据集，专门针对特定领域的应用。与以往专注于通用日常运动且灵活性有限的工作不同，我们的方法提供了对身体姿势、外观、相机视角和环境上下文的细粒度控制，无需任何手动标注。为了验证Avatar4D的影响，我们专注于体育运动，其中特定领域的人体动作和运动模式对运动理解提出了独特的挑战。在这种背景下，我们引入了Syn2Sport，一个涵盖棒球和冰球等运动的大规模合成数据集。Avatar4D具有高保真4D（随时间变化的3D几何）人体运动序列，具有不同的运动员外观，并在不同的环境中渲染。我们在Syn2Sport上对几种最先进的姿态估计模型进行了基准测试，并证明了它们在监督学习、零样本迁移到真实世界数据以及跨运动泛化方面的有效性。此外，我们评估了生成的合成数据在特征空间中与真实世界数据集的对齐程度。我们的结果突出了这种系统在生成可扩展、可控和可迁移的人体数据集方面的潜力，用于各种特定领域的任务，而无需依赖特定领域的真实数据。",
            "intro_zh": [
                "现有方法在生成人体运动数据时，缺乏对特定领域动作的细粒度控制和定制能力，限制了其在专业领域的应用。",
                "Avatar4D通过控制身体姿势、外观、相机视角和环境上下文，生成特定领域的高质量4D人体运动数据，无需手动标注。",
                "实验表明，基于Avatar4D生成的Syn2Sport数据集训练的姿态估计模型，在真实世界数据上表现出良好的零样本迁移和泛化能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决现有合成人体运动数据集在特定领域（如体育运动）应用中，缺乏真实性和可控性的问题。现有方法通常关注通用场景，难以模拟特定运动的复杂动作和环境，导致模型在真实场景中表现不佳。\\n\\n**核心思路**：论文的核心思路是构建一个可定制的合成数据生成流水线，允许用户精细控制人体姿势、外观、相机视角和环境上下文，从而生成高度逼真且针对特定领域优化的4D人体运动数据。通过合成数据训练的模型可以更好地泛化到真实世界场景。\\n\\n**技术框架**：Avatar4D流水线包含以下主要模块：1) 人体模型和运动生成：使用参数化人体模型（如SMPL）和运动捕捉数据或程序化动画生成多样化的人体运动序列。2) 外观定制：允许用户调整人体模型的纹理、服装和配饰，以模拟不同的运动员外观。3) 环境渲染：将人体模型放置在不同的合成环境中，并调整相机视角和光照条件。4) 数据生成：渲染生成带有精确标注的4D人体运动数据。\\n\\n**关键创新**：Avatar4D的关键创新在于其高度的可定制性和真实感。它允许用户根据特定领域的需求，精细控制合成数据的各个方面，从而生成更具代表性和泛化能力的训练数据。与以往方法相比，Avatar4D无需手动标注，即可生成大规模的4D人体运动数据。\\n\\n**关键设计**：Avatar4D使用参数化人体模型SMPL来表示人体几何，并采用运动捕捉数据或程序化动画来驱动人体运动。为了提高合成数据的真实感，论文使用了高质量的纹理和光照模型。此外，论文还设计了一系列损失函数，用于优化合成数据的质量和多样性。",
            "application_zh": "Avatar4D具有广泛的应用前景，包括体育运动分析、虚拟现实、游戏开发、人机交互和机器人控制等领域。通过生成特定领域的高质量合成数据，可以降低数据采集和标注的成本，加速相关领域的研究和应用落地。例如，可以利用Avatar4D生成大量足球运动员的运动数据，用于训练足球机器人或开发智能足球游戏。",
            "highlight_zh": "论文在Syn2Sport数据集上评估了多个最先进的姿态估计模型，结果表明，使用Syn2Sport训练的模型在真实世界数据上表现出良好的零样本迁移能力。例如，在棒球姿态估计任务中，使用Syn2Sport训练的模型在真实数据集上的性能与使用真实数据训练的模型相当，甚至更好。此外，实验还表明，Syn2Sport可以有效地提高模型在跨运动泛化方面的能力。",
            "tags_zh": [
                "4D人体建模",
                "合成数据生成",
                "姿态估计",
                "领域自适应",
                "体育运动分析",
                "计算机视觉",
                "深度学习"
            ],
            "_index": 73,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16199v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16199v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16199v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times",
            "authors": [
                "Jintao Zhang",
                "Kaiwen Zheng",
                "Kai Jiang",
                "Haoxu Wang",
                "Ion Stoica",
                "Joseph E. Gonzalez",
                "Jianfei Chen",
                "Jun Zhu"
            ],
            "arxiv_id": "2512.16093v1",
            "summary": "We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.\n  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16093v1",
            "code_links": [
                {
                    "url": "https://github.com/thu-ml/TurboDiffusion",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "linear attention",
                        "distillation"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "TurboDiffusion：通过多重加速策略实现视频扩散模型100-200倍的加速。",
            "summary_zh": "TurboDiffusion是一个视频生成加速框架，能够在保持视频质量的同时，将端到端扩散生成速度提高100-200倍。TurboDiffusion主要依赖于以下几个加速组件：（1）注意力加速：TurboDiffusion使用低比特SageAttention和可训练的稀疏线性注意力（SLA）来加速注意力计算。（2）步骤蒸馏：TurboDiffusion采用rCM进行高效的步骤蒸馏。（3）W8A8量化：TurboDiffusion将模型参数和激活量化为8位，以加速线性层并压缩模型。此外，TurboDiffusion还包含其他一些工程优化。我们在Wan2.2-I2V-14B-720P、Wan2.1-T2V-1.3B-480P、Wan2.1-T2V-14B-720P和Wan2.1-T2V-14B-480P模型上进行了实验。实验结果表明，即使在单个RTX 5090 GPU上，TurboDiffusion也能实现100-200倍的视频生成加速，同时保持相当的视频质量。包含模型检查点和易于使用的代码的GitHub存储库可在https://github.com/thu-ml/TurboDiffusion上找到。",
            "intro_zh": [
                "视频扩散模型计算成本高昂，限制了其在实际应用中的部署和使用。",
                "TurboDiffusion通过注意力加速、步骤蒸馏和模型量化等多重策略，显著降低计算复杂度。",
                "实验表明，TurboDiffusion在保证视频质量的前提下，实现了100-200倍的加速效果。"
            ],
            "method_zh": "**问题定义**：现有视频扩散模型计算量巨大，推理速度慢，难以满足实时或快速生成视频的需求。尤其是在高分辨率视频生成时，计算瓶颈更加明显。因此，如何加速视频扩散模型的推理过程，同时保持生成视频的质量，是一个重要的研究问题。\\n\\n**核心思路**：TurboDiffusion的核心思路是通过多方面的优化，包括算法层面的注意力机制加速和步骤蒸馏，以及系统层面的模型量化和工程优化，来降低计算复杂度，从而实现整体的加速效果。这种多管齐下的方法能够在不显著降低视频质量的前提下，大幅提升生成速度。\\n\\n**技术框架**：TurboDiffusion的整体框架主要包含以下几个模块：1) **注意力加速模块**：使用低比特SageAttention和可训练的稀疏线性注意力（SLA）来减少注意力计算的复杂度。2) **步骤蒸馏模块**：采用rCM（未知具体含义，原文如此）进行高效的步骤蒸馏，减少生成视频所需的迭代步骤。3) **模型量化模块**：将模型参数和激活量化为8位（W8A8），以加速线性层计算并压缩模型大小。4) **工程优化模块**：包含一系列其他的工程优化手段，进一步提升整体性能。\\n\\n**关键创新**：TurboDiffusion的关键创新在于其综合利用多种加速策略，并针对视频扩散模型的特点进行了优化。例如，稀疏线性注意力（SLA）能够有效地减少注意力计算中的冗余，而步骤蒸馏则能够减少生成视频所需的迭代次数。此外，W8A8量化能够在保证模型精度的前提下，显著降低计算量和内存占用。多种优化策略的结合是TurboDiffusion能够实现显著加速的关键。\\n\\n**关键设计**：论文中提到了SageAttention、稀疏线性注意力（SLA）、rCM步骤蒸馏和W8A8量化等关键技术，但具体的技术细节（例如SLA的具体稀疏模式、rCM的具体实现方式、量化的具体策略等）并未详细描述。这些细节对于理解TurboDiffusion的性能至关重要，但目前信息不足，无法进行深入分析。",
            "application_zh": "TurboDiffusion的加速技术可以广泛应用于视频生成、视频编辑、游戏开发、虚拟现实等领域。更快的视频生成速度可以提升用户体验，降低计算成本，并促进相关应用的普及。例如，在游戏开发中，可以利用TurboDiffusion快速生成游戏场景和角色动画；在虚拟现实中，可以实时生成高质量的虚拟环境。",
            "highlight_zh": "TurboDiffusion在多个视频生成模型上实现了100-200倍的加速，同时保持了与原始模型相当的视频质量。即使在单个RTX 5090 GPU上，也能达到如此显著的加速效果，表明TurboDiffusion具有很高的实用价值。开源的代码和模型检查点也方便了其他研究者和开发者使用和改进。",
            "tags_zh": [
                "视频生成",
                "扩散模型",
                "模型加速",
                "注意力机制",
                "模型量化",
                "步骤蒸馏",
                "低比特计算"
            ],
            "_index": 74,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16093v1/src/figs/original/outputs_1.3B/frames/12-1.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16093v1/src/figs/turbodiffusion/outputs_1.3B/frames/12-1.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16093v1/src/figs/i2v/original/outputs_A14B_720p/frames/1-1.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Machine Learning Enabled Graph Analysis of Particulate Composites: Application to Solid-state Battery Cathodes",
            "authors": [
                "Zebin Li",
                "Shimao Deng",
                "Yijin Liu",
                "Jia-Mian Hu"
            ],
            "arxiv_id": "2512.16085v1",
            "summary": "Particulate composites underpin many solid-state chemical and electrochemical systems, where microstructural features such as multiphase boundaries and inter-particle connections strongly influence system performance. Advances in X-ray microscopy enable capturing large-scale, multimodal images of these complex microstructures with an unprecedentedly high throughput. However, harnessing these datasets to discover new physical insights and guide microstructure optimization remains a major challenge. Here, we develop a machine learning (ML) enabled framework that enables automated transformation of experimental multimodal X-ray images of multiphase particulate composites into scalable, topology-aware graphs for extracting physical insights and establishing local microstructure-property relationships at both the particle and network level. Using the multiphase particulate cathode of solid-state lithium batteries as an example, our ML-enabled graph analysis corroborates the critical role of triple phase junctions and concurrent ion/electron conduction channels in realizing desirable local electrochemical activity. Our work establishes graph-based microstructure representation as a powerful paradigm for bridging multimodal experimental imaging and functional understanding, and facilitating microstructure-aware data-driven materials design in a broad range of particulate composites.",
            "categories": [
                "cond-mat.mtrl-sci",
                "cs.CV"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16085v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于机器学习的图分析方法，用于固态电池正极材料微观结构表征与性能预测。",
            "summary_zh": "本文提出了一种基于机器学习（ML）的框架，该框架能够自动将多相颗粒复合材料的实验多模态X射线图像转换为可扩展的、具有拓扑感知能力的图，从而提取物理见解，并在颗粒和网络层面建立局部微观结构-性能关系。以固态锂电池的多相颗粒正极为例，我们的ML图分析证实了三相结和并发离子/电子传导通道在实现理想的局部电化学活性中的关键作用。这项工作将基于图的微观结构表示确立为连接多模态实验成像和功能理解的强大范例，并促进了各种颗粒复合材料中具有微观结构感知的数据驱动材料设计。",
            "intro_zh": [
                "传统方法难以有效利用高通量多模态X射线图像数据，限制了对颗粒复合材料微观结构与性能关系的深入理解。",
                "该研究提出一种基于机器学习的图分析框架，将多模态图像转化为拓扑感知的图，提取微观结构特征。",
                "通过固态电池正极材料的案例研究，验证了该方法在揭示三相结和离子/电子传导通道作用方面的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决如何从颗粒复合材料的大规模多模态X射线图像中提取有意义的微观结构信息，并将其与材料性能关联起来的问题。现有方法难以有效处理高通量图像数据，无法充分挖掘微观结构特征与性能之间的复杂关系。传统图像处理方法难以捕捉颗粒间的拓扑关系，而手动分析耗时且容易出错。\\n\\n**核心思路**：论文的核心思路是将颗粒复合材料的微观结构表示为图，其中节点代表颗粒，边代表颗粒间的连接。通过机器学习方法自动从图像中提取颗粒和连接信息，构建拓扑感知的图。然后，利用图分析技术提取微观结构特征，并建立其与材料性能之间的关系模型。这种方法能够有效地处理大规模图像数据，并捕捉颗粒间的复杂拓扑关系。\\n\\n**技术框架**：该框架包含以下主要模块：1) 多模态X射线图像采集；2) 图像预处理和分割，利用机器学习算法自动分割出不同的相；3) 图构建，将分割后的颗粒表示为图的节点，根据颗粒间的连接关系构建边；4) 图分析，提取图的节点和边的特征，例如颗粒大小、形状、连接数等；5) 性能预测，利用机器学习模型建立图特征与材料性能之间的关系。\\n\\n**关键创新**：该研究的关键创新在于将机器学习与图分析相结合，实现对颗粒复合材料微观结构的自动化、高通量分析。与传统图像处理方法相比，该方法能够更好地捕捉颗粒间的拓扑关系，并提取更丰富的微观结构特征。此外，该方法能够处理多模态图像数据，从而获得更全面的材料信息。\\n\\n**关键设计**：在图像分割阶段，使用了机器学习算法（具体算法未知）进行自动分割。在图构建阶段，需要定义颗粒间连接的标准，例如距离阈值。在图分析阶段，使用了多种图特征提取方法（具体方法未知），例如节点度、聚类系数等。在性能预测阶段，使用了机器学习模型（具体模型未知）建立图特征与材料性能之间的关系，并进行了模型训练和验证。",
            "application_zh": "该研究成果可广泛应用于各种颗粒复合材料的设计与优化，例如固态电池、催化剂、陶瓷材料等。通过分析材料的微观结构，可以预测其性能，并指导材料的制备工艺。该方法有助于加速新材料的研发过程，降低实验成本，并提高材料的性能。",
            "highlight_zh": "该研究通过固态锂电池正极材料的案例研究，验证了该方法的有效性。结果表明，三相结和并发离子/电子传导通道在实现理想的局部电化学活性中起着关键作用。该研究为理解微观结构与性能之间的关系提供了新的视角，并为优化固态电池正极材料的设计提供了指导。",
            "tags_zh": [
                "机器学习",
                "图分析",
                "颗粒复合材料",
                "固态电池",
                "微观结构",
                "X射线成像",
                "材料设计"
            ],
            "_index": 75,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Impacts of Racial Bias in Historical Training Data for News AI",
            "authors": [
                "Rahul Bhargava",
                "Malene Hornstrup Jespersen",
                "Emily Boardman Ndulue",
                "Vivica Dsouza"
            ],
            "arxiv_id": "2512.16901v1",
            "summary": "AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier. Our use in research settings surfaced the concerning \"blacks\" thematic topic label. Through quantitative and qualitative means we investigate this label's use in the training corpus, what concepts it might be encoding in the trained classifier, and how those concepts impact our model use. Via the application of explainable AI methods, we find that the \"blacks\" label operates partially as a general \"racism detector\" across some minoritized groups. However, it performs poorly against expectations on modern examples such as COVID-19 era anti-Asian hate stories, and reporting on the Black Lives Matter movement. This case study of interrogating embedded biases in a model reveals how similar applications in newsroom settings can lead to unexpected outputs that could impact a wide variety of potential uses of any large language model-story discovery, audience targeting, summarization, etc. The fundamental tension this exposes for newsrooms is how to adopt AI-enabled workflow tools while reducing the risk of reproducing historical biases in news coverage.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "cs.CY"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16901v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "揭示新闻AI中历史数据偏见：以纽约时报语料库为例，分析种族标签的影响。",
            "summary_zh": "人工智能技术已迅速应用于涉及大型文本语料库的商业和研究应用，包括计算新闻研究和新闻编辑室环境。这些模型在来自各种来源的现有数据上进行训练，可以被概念化为编码了几十年前的态度和刻板印象的历史产物。本文研究了一个这样的例子，该例子在广泛使用的《纽约时报》注释语料库上训练，以创建一个多标签分类器。我们在研究环境中的使用浮现了令人担忧的“黑人”主题标签。通过定量和定性的方法，我们调查了该标签在训练语料库中的使用情况，它可能在训练的分类器中编码了哪些概念，以及这些概念如何影响我们的模型使用。通过应用可解释的人工智能方法，我们发现“黑人”标签在一定程度上充当了针对一些少数群体的通用“种族主义检测器”。然而，它在现代例子（如COVID-19时代的反亚裔仇恨故事和关于“黑人的命也是命”运动的报道）上的表现不符合预期。这个调查模型中嵌入偏见的案例研究揭示了新闻编辑室环境中类似的应用如何导致意想不到的输出，这些输出可能会影响任何大型语言模型的各种潜在用途——故事发现、受众定位、摘要等。这为新闻编辑室暴露出的根本矛盾是，如何在采用人工智能支持的工作流程工具的同时，降低在新闻报道中重现历史偏见的风险。",
            "intro_zh": [
                "现有新闻AI模型训练于历史数据，可能内嵌过时的偏见和刻板印象，导致不公平或不准确的结果。",
                "该研究通过分析《纽约时报》语料库训练的多标签分类器，揭示了“黑人”标签的潜在偏见及其影响。",
                "实验表明该标签在某些情况下充当“种族主义检测器”，但在现代反歧视事件中表现不佳，突显了偏见的复杂性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决新闻AI模型中由于历史训练数据偏差而导致的不公平或不准确的问题。现有方法未能充分识别和减轻这些偏差，导致模型在处理涉及种族、性别等敏感话题时可能产生歧视性结果。特别是，论文关注的是《纽约时报》注释语料库中“黑人”标签的使用，以及该标签可能在模型中编码的潜在偏见。\n\n**核心思路**：论文的核心思路是通过定量和定性分析相结合的方式，深入挖掘训练数据中存在的偏见，并评估这些偏见对模型行为的影响。通过可解释AI方法，揭示模型如何使用“黑人”标签，以及该标签在不同情境下的表现。这种方法旨在帮助新闻编辑室等机构更好地理解和减轻AI模型中的偏见。\n\n**技术框架**：该研究的技术框架主要包括以下几个阶段：1) 数据分析：对《纽约时报》注释语料库中“黑人”标签的使用情况进行统计分析，了解其在不同时间段和主题下的分布。2) 模型训练：使用该语料库训练一个多标签分类器，用于预测新闻文章的主题标签。3) 可解释AI：应用可解释AI方法（具体方法未知），分析模型如何使用“黑人”标签进行预测，并识别与该标签相关的关键特征。4) 案例研究：选择COVID-19时代的反亚裔仇恨故事和关于“黑人的命也是命”运动的报道作为案例，评估模型在这些现代情境下的表现。\n\n**关键创新**：该研究的关键创新在于将可解释AI方法应用于新闻AI模型的偏见分析，从而能够更深入地理解模型内部的决策过程。通过结合定量和定性分析，揭示了“黑人”标签在不同情境下的复杂表现，并指出了模型在处理现代反歧视事件时的局限性。这种方法为新闻编辑室等机构提供了一种评估和减轻AI模型偏见的有效途径。\n\n**关键设计**：论文中没有明确说明关键的参数设置、损失函数、网络结构等技术细节。但是，可以推断，模型训练可能使用了常见的文本分类算法，例如基于Transformer的模型。可解释AI方法的选择和应用是关键，但具体方法未知。",
            "application_zh": "该研究成果可应用于新闻编辑室、内容推荐系统、情感分析等领域，帮助识别和减轻AI模型中的偏见，提高模型的公平性和准确性。通过理解历史数据中的偏见，可以避免在新闻报道、内容推荐等方面重现这些偏见，从而促进更加公正和包容的社会。",
            "highlight_zh": "研究发现，《纽约时报》语料库训练的AI模型中，“黑人”标签在某些情况下充当“种族主义检测器”，但对COVID-19时代的反亚裔仇恨故事和“黑人的命也是命”运动等现代案例表现不佳。这表明历史数据中的偏见可能导致模型在处理现代问题时出现偏差。",
            "tags_zh": [
                "新闻AI",
                "偏见分析",
                "历史数据",
                "可解释AI",
                "种族偏见"
            ],
            "_index": 76,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16901v1/figures/fig1-blacks-use.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16901v1/figures/fig2-boxplots.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16901v1/figures/fig3-terms-grid.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI",
            "authors": [
                "Hao Liang",
                "Xiaochen Ma",
                "Zhou Liu",
                "Zhen Hao Wong",
                "Zhengyang Zhao",
                "Zimo Meng",
                "Runming He",
                "Chengyu Shen",
                "Qifeng Cai",
                "Zhaoyang Han",
                "Meiyi Qiang",
                "Yalin Feng",
                "Tianyi Bai",
                "Zewei Pan",
                "Ziyi Guo",
                "Yizhen Jiang",
                "Jingwen Deng",
                "Qijie You",
                "Peichao Lai",
                "Tianyu Guo",
                "Chi Hsu Tsai",
                "Hengyi Feng",
                "Rui Hu",
                "Wenkai Yu",
                "Junbo Niu",
                "Bohan Zeng",
                "Ruichuan An",
                "Lu Ma",
                "Jihao Huang",
                "Yaowei Zheng",
                "Conghui He",
                "Linpeng Tang",
                "Bin Cui",
                "Weinan E",
                "Wentao Zhang"
            ],
            "arxiv_id": "2512.16676v1",
            "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.",
            "categories": [
                "cs.LG",
                "cs.CL"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16676v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "DataFlow：一个LLM驱动的统一数据准备与工作流自动化框架",
            "summary_zh": "为了应对大语言模型（LLM）对高质量数据日益增长的需求，本文提出了DataFlow，一个统一且可扩展的LLM驱动的数据准备框架。DataFlow采用系统级抽象，实现了模块化、可重用和可组合的数据转换，并提供了一个类似PyTorch的pipeline构建API，用于构建可调试和优化的数据流。该框架包含近200个可重用算子和六个领域通用的pipeline，涵盖文本、数学推理、代码、Text-to-SQL、agentic RAG和大规模知识抽取。为了进一步提高可用性，我们引入了DataFlow-Agent，它通过算子合成、pipeline规划和迭代验证，自动将自然语言规范转换为可执行的pipeline。在六个代表性用例中，DataFlow始终提高了下游LLM的性能。我们的数学、代码和文本pipeline优于人工数据集和专门的合成基线，在Text-to-SQL中实现了高达+3%的执行准确率（超过SynSQL），在代码基准测试中平均提高了+7%，在MATH、GSM8K和AIME上提高了1-3个点。此外，DataFlow生成的统一的1万样本数据集使基础模型能够超越在100万Infinity-Instruct数据上训练的同类模型。这些结果表明，DataFlow为可靠、可重复和可扩展的LLM数据准备提供了一个实用且高性能的基础，并为未来的数据中心AI开发奠定了系统级基础。",
            "intro_zh": [
                "现有数据准备流程依赖临时脚本和松散的工作流，缺乏抽象，难以复现，且对模型在环数据生成支持有限。",
                "DataFlow通过系统级抽象实现模块化、可重用和可组合的数据转换，并提供PyTorch风格的pipeline构建API。",
                "实验表明，DataFlow在多个任务上超越人工数据集和特定基线，并能提升下游LLM性能，例如Text-to-SQL准确率提升3%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大规模语言模型（LLM）训练中高质量数据准备的难题。现有方法通常依赖于临时脚本和非结构化的工作流程，导致数据准备过程难以复现、扩展性和优化性差，并且缺乏对模型在环数据生成流程的有效支持。\\n\\n**核心思路**：论文的核心思路是构建一个统一的、可扩展的LLM驱动的数据准备框架DataFlow。该框架通过系统级的抽象，将数据转换过程模块化、可重用和可组合，从而简化数据准备流程，提高效率和可维护性。DataFlow还引入了DataFlow-Agent，利用LLM自动将自然语言描述转化为可执行的数据pipeline。\\n\\n**技术框架**：DataFlow框架包含以下主要模块：1) 可重用算子库：包含近200个算子，涵盖文本、数学、代码等多个领域；2) PyTorch风格的pipeline构建API：方便用户构建、调试和优化数据流；3) DataFlow-Agent：自动将自然语言规范转换为可执行的pipeline，包括算子合成、pipeline规划和迭代验证等步骤。整体流程是从自然语言需求开始，通过DataFlow-Agent生成pipeline，然后利用算子库和pipeline API执行数据转换，最终得到高质量的数据集。\\n\\n**关键创新**：DataFlow的关键创新在于：1) 统一的系统级抽象，使得数据准备流程更加模块化和可重用；2) DataFlow-Agent，利用LLM自动生成数据pipeline，降低了数据准备的门槛；3) 框架的可扩展性，方便用户添加新的算子和pipeline，适应不同的数据准备需求。与现有方法相比，DataFlow更加结构化、自动化和可扩展。\\n\\n**关键设计**：DataFlow-Agent的设计是关键。它利用LLM进行算子合成和pipeline规划，需要解决如何将自然语言需求准确映射到算子和pipeline的问题。迭代验证机制用于确保生成的pipeline的正确性。此外，框架还考虑了数据流的优化，例如算子融合和并行执行，以提高数据准备的效率。具体参数设置和损失函数等细节未在摘要中提及，属于未知信息。",
            "application_zh": "DataFlow框架可广泛应用于各种需要高质量数据的大语言模型训练场景，例如文本生成、数学推理、代码生成、知识图谱构建等。它能显著降低数据准备的成本和复杂度，提高LLM的性能和可靠性，加速数据中心AI的发展。",
            "highlight_zh": "实验结果表明，DataFlow在多个任务上取得了显著的性能提升。例如，在Text-to-SQL任务中，DataFlow的准确率比SynSQL提高了3%。在代码基准测试中，平均提高了7%。在MATH、GSM8K和AIME等数学推理任务中，提高了1-3个点。此外，DataFlow生成的1万样本数据集使基础模型能够超越在100万Infinity-Instruct数据上训练的同类模型。",
            "tags_zh": [
                "数据准备",
                "大语言模型",
                "工作流自动化",
                "LLM Agent",
                "数据中心AI"
            ],
            "_index": 77,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16676v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16676v1/x5.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16676v1/x6.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Muon is Provably Faster with Momentum Variance Reduction",
            "authors": [
                "Xun Qian",
                "Hussein Rammal",
                "Dmitry Kovalev",
                "Peter Richtárik"
            ],
            "arxiv_id": "2512.16598v1",
            "summary": "Recent empirical research has demonstrated that deep learning optimizers based on the linear minimization oracle (LMO) over specifically chosen Non-Euclidean norm balls, such as Muon and Scion, outperform Adam-type methods in the training of large language models. In this work, we show that such optimizers can be provably improved by replacing their vanilla momentum by momentum variance reduction (MVR). Instead of proposing and analyzing MVR variants of Muon and Scion separately, we incorporate MVR into the recently proposed Gluon framework, which captures Muon, Scion and other specific Non-Euclidean LMO-based methods as special cases, and at the same time works with a more general smoothness assumption which better captures the layer-wise structure of neural networks. In the non-convex case, we incorporate MVR into Gluon in three different ways. All of them improve the convergence rate from ${\\cal O} (\\frac{1}{K^{1/4}})$ to ${\\cal O} (\\frac{1}{K^{1/3}})$. Additionally, we provide improved rates in the star-convex case. Finally, we conduct several numerical experiments that verify the superior performance of our proposed algorithms in terms of iteration complexity.",
            "categories": [
                "math.OC",
                "cs.LG"
            ],
            "primary_category": "math.OC",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "31 pages, 4 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16598v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出动量方差减少的Muon优化器以提升深度学习性能",
            "summary_zh": "近期的实证研究表明，基于线性最小化oracle（LMO）并在特定非欧几里得范数球上优化的深度学习优化器，如Muon和Scion，在训练大型语言模型时优于Adam类方法。本文展示了通过将传统动量替换为动量方差减少（MVR），可以在理论上改进这些优化器的性能。我们将MVR整合进最近提出的Gluon框架，该框架能够捕捉Muon、Scion及其他特定的非欧几里得LMO方法，并在更一般的光滑性假设下工作，从而更好地反映神经网络的层级结构。在非凸情况下，我们以三种不同方式将MVR融入Gluon，均将收敛速度从${\textcal O} (\frac{1}{K^{1/4}})$提升至${\textcal O} (\frac{1}{K^{1/3}})$，并在星凸情况下提供了改进的收敛速率。最后，我们进行了多次数值实验，验证了所提算法在迭代复杂度方面的优越性能。",
            "intro_zh": [
                "现有的深度学习优化器在训练大型语言模型时，收敛速度和效率存在不足，尤其是传统的动量方法表现不佳。",
                "本文提出将动量方差减少（MVR）技术整合进Gluon框架，以提升优化器的收敛速度和性能，适用于多种非欧几里得优化方法。",
                "实验结果表明，整合MVR的优化器在迭代复杂度上显著优于传统方法，收敛速度从${\textcal O} (\frac{1}{K^{1/4}})$提升至${\textcal O} (\frac{1}{K^{1/3}})$。"
            ],
            "method_zh": "**问题定义**：本文旨在解决现有深度学习优化器在训练大型语言模型时的收敛速度不足的问题，尤其是传统动量方法的局限性。\\n\\n**核心思路**：通过将动量方差减少（MVR）技术引入Gluon框架，提升优化器的收敛性能，适应更复杂的非欧几里得空间。\\n\\n**技术框架**：Gluon框架整合了Muon、Scion等多种优化方法，采用更一般的光滑性假设，分为三个主要模块：动量更新、方差减少和收敛分析。\\n\\n**关键创新**：将MVR与Gluon框架结合，提供了理论上的收敛速率改进，尤其在非凸和星凸情况下表现突出，显著提升了优化器的效率。\\n\\n**关键设计**：在设计中，采用了新的动量更新策略和损失函数，确保在不同的网络结构和参数设置下均能有效提升收敛速度。具体参数设置和网络结构细节在实验部分进行了详细描述。",
            "application_zh": "该研究的潜在应用领域包括自然语言处理、计算机视觉等需要高效训练的深度学习任务。通过提升优化器的性能，能够加速模型训练过程，降低计算成本，具有重要的实际价值和广泛的应用前景。",
            "highlight_zh": "实验结果显示，整合MVR的优化器在收敛速度上有显著提升，具体表现为收敛速率从${\textcal O} (\frac{1}{K^{1/4}})$提升至${\textcal O} (\frac{1}{K^{1/3}})$，并在星凸情况下也取得了更好的收敛性能，验证了所提方法的有效性。",
            "tags_zh": [
                "深度学习",
                "优化器",
                "动量方差减少",
                "非欧几里得优化",
                "收敛速度",
                "Gluon框架",
                "大型语言模型"
            ],
            "_index": 78,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16598v1/fig/MVR1gbs512.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16598v1/fig/MVR1gbs128.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16598v1/fig/MVR2gbs512.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "A Systematic Study of Code Obfuscation Against LLM-based Vulnerability Detection",
            "authors": [
                "Xiao Li",
                "Yue Li",
                "Hao Wu",
                "Yue Zhang",
                "Yechao Zhang",
                "Fengyuan Xu",
                "Sheng Zhong"
            ],
            "arxiv_id": "2512.16538v1",
            "summary": "As large language models (LLMs) are increasingly adopted for code vulnerability detection, their reliability and robustness across diverse vulnerability types have become a pressing concern. In traditional adversarial settings, code obfuscation has long been used as a general strategy to bypass auditing tools, preserving exploitability without tampering with the tools themselves. Numerous efforts have explored obfuscation methods and tools, yet their capabilities differ in terms of supported techniques, granularity, and programming languages, making it difficult to systematically assess their impact on LLM-based vulnerability detection. To address this gap, we provide a structured systematization of obfuscation techniques and evaluate them under a unified framework. Specifically, we categorize existing obfuscation methods into three major classes (layout, data flow, and control flow) covering 11 subcategories and 19 concrete techniques. We implement these techniques across four programming languages (Solidity, C, C++, and Python) using a consistent LLM-driven approach, and evaluate their effects on 15 LLMs spanning four model families (DeepSeek, OpenAI, Qwen, and LLaMA), as well as on two coding agents (GitHub Copilot and Codex). Our findings reveal both positive and negative impacts of code obfuscation on LLM-based vulnerability detection, highlighting conditions under which obfuscation leads to performance improvements or degradations. We further analyze these outcomes with respect to vulnerability characteristics, code properties, and model attributes. Finally, we outline several open problems and propose future directions to enhance the robustness of LLMs for real-world vulnerability detection.",
            "categories": [
                "cs.CR",
                "cs.LG"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16538v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "系统性研究代码混淆对基于LLM的漏洞检测的影响，揭示其性能变化规律",
            "summary_zh": "随着大型语言模型（LLM）越来越多地应用于代码漏洞检测，其在各种漏洞类型上的可靠性和鲁棒性日益受到关注。代码混淆作为一种绕过审计工具的通用策略，长期以来被用于传统的对抗环境中，它在不篡改工具本身的情况下保留了可利用性。虽然已经有很多关于混淆方法和工具的研究，但它们在支持的技术、粒度和编程语言方面存在差异，这使得系统性地评估它们对基于LLM的漏洞检测的影响变得困难。为了解决这个问题，我们对混淆技术进行了结构化的系统化研究，并在统一的框架下评估了它们。具体来说，我们将现有的混淆方法分为三大类（布局、数据流和控制流），涵盖11个子类别和19个具体技术。我们使用一致的LLM驱动方法在四种编程语言（Solidity、C、C++和Python）中实现了这些技术，并评估了它们对跨越四个模型系列（DeepSeek、OpenAI、Qwen和LLaMA）的15个LLM以及两个编码代理（GitHub Copilot和Codex）的影响。我们的研究结果揭示了代码混淆对基于LLM的漏洞检测的积极和消极影响，突出了混淆导致性能提高或降低的条件。我们进一步分析了这些结果与漏洞特征、代码属性和模型属性的关系。最后，我们概述了几个开放性问题，并提出了未来方向，以增强LLM在实际漏洞检测中的鲁棒性。",
            "intro_zh": [
                "现有代码混淆工具在技术、粒度和语言支持上存在差异，难以系统评估其对LLM漏洞检测的影响。",
                "论文对代码混淆技术进行分类和系统化，并在统一框架下评估其对多种LLM漏洞检测的影响。",
                "实验揭示了代码混淆对LLM漏洞检测的正反两方面影响，并分析了影响因素，为提升LLM鲁棒性提供指导。"
            ],
            "method_zh": "**问题定义**：论文旨在解决代码混淆技术对基于大型语言模型（LLM）的漏洞检测工具的影响评估问题。现有代码混淆工具种类繁多，支持的技术、粒度和编程语言各不相同，缺乏一个统一的框架来系统性地评估它们对LLM漏洞检测性能的影响。这使得难以理解哪些混淆技术能够有效绕过LLM检测，以及哪些因素会影响LLM的检测效果。\\n\\n**核心思路**：论文的核心思路是对现有的代码混淆技术进行分类和系统化，构建一个统一的评估框架，并在该框架下评估不同混淆技术对多种LLM漏洞检测性能的影响。通过这种方式，可以揭示代码混淆对LLM漏洞检测的积极和消极影响，并分析影响因素，从而为提升LLM的鲁棒性提供指导。\\n\\n**技术框架**：论文的技术框架主要包括以下几个部分：1) 代码混淆技术的分类和系统化：将现有的代码混淆技术分为三大类（布局、数据流和控制流），并进一步细分为11个子类别和19个具体技术。2) 混淆技术的实现：使用一致的LLM驱动方法在四种编程语言（Solidity、C、C++和Python）中实现了这些混淆技术。3) 评估框架的构建：构建一个统一的评估框架，用于评估不同混淆技术对多种LLM漏洞检测性能的影响。4) 实验评估：在15个LLM（来自DeepSeek、OpenAI、Qwen和LLaMA四个模型系列）和两个编码代理（GitHub Copilot和Codex）上进行实验评估。5) 结果分析：分析实验结果，揭示代码混淆对LLM漏洞检测的积极和消极影响，并分析影响因素。\\n\\n**关键创新**：论文的关键创新在于对代码混淆技术进行了系统化的分类和评估。以往的研究通常只关注少数几种混淆技术，或者只在少数几个LLM上进行评估。而本论文对大量的混淆技术进行了分类，并在多种LLM上进行了评估，从而能够更全面地了解代码混淆对LLM漏洞检测的影响。此外，论文还分析了影响LLM检测效果的因素，为提升LLM的鲁棒性提供了指导。\\n\\n**关键设计**：论文的关键设计包括：1) 代码混淆技术的分类体系：将混淆技术分为布局、数据流和控制流三大类，并进一步细分为11个子类别，涵盖了常见的混淆技术。2) 统一的评估框架：构建了一个统一的评估框架，用于评估不同混淆技术对多种LLM漏洞检测性能的影响。该框架包括漏洞数据集、混淆技术实现、LLM漏洞检测器和评估指标等。3) 多种LLM的评估：在15个LLM和两个编码代理上进行了评估，涵盖了不同的模型架构和训练数据，从而能够更全面地了解代码混淆对LLM漏洞检测的影响。",
            "application_zh": "该研究成果可应用于提升软件安全和代码审计领域。通过了解代码混淆对LLM漏洞检测的影响，可以指导开发者设计更有效的混淆策略，保护代码免受恶意攻击。同时，也能帮助安全研究人员开发更鲁棒的LLM漏洞检测工具，提高软件安全保障水平。研究结果对未来开发更安全的软件系统具有重要意义。",
            "highlight_zh": "实验结果表明，代码混淆对LLM漏洞检测的影响是双面的，某些混淆技术可以提高检测性能，而另一些则会降低性能。研究发现，混淆效果与漏洞类型、代码属性和模型属性密切相关。例如，某些混淆技术在特定类型的漏洞上表现出更好的绕过效果，而某些模型对特定类型的混淆技术更敏感。",
            "tags_zh": [
                "代码混淆",
                "漏洞检测",
                "大型语言模型",
                "软件安全",
                "对抗攻击"
            ],
            "_index": 79,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Feature-Selective Representation Misdirection for Machine Unlearning",
            "authors": [
                "Taozhao Chen",
                "Linghan Huang",
                "Kim-Kwang Raymond Choo",
                "Huaming Chen"
            ],
            "arxiv_id": "2512.16297v1",
            "summary": "As large language models (LLMs) are increasingly adopted in safety-critical and regulated sectors, the retention of sensitive or prohibited knowledge introduces escalating risks, ranging from privacy leakage to regulatory non-compliance to to potential misuse, and so on. Recent studies suggest that machine unlearning can help ensure deployed models comply with evolving legal, safety, and governance requirements. However, current unlearning techniques assume clean separation between forget and retain datasets, which is challenging in operational settings characterized by highly entangled distributions. In such scenarios, perturbation-based methods often degrade general model utility or fail to ensure safety. To address this, we propose Selective Representation Misdirection for Unlearning (SRMU), a novel principled activation-editing framework that enforces feature-aware and directionally controlled perturbations. Unlike indiscriminate model weights perturbations, SRMU employs a structured misdirection vector with an activation importance map. The goal is to allow SRMU selectively suppresses harmful representations while preserving the utility on benign ones. Experiments are conducted on the widely used WMDP benchmark across low- and high-entanglement configurations. Empirical results reveal that SRMU delivers state-of-the-art unlearning performance with minimal utility losses, and remains effective under 20-30\\% overlap where existing baselines collapse. SRMU provides a robust foundation for safety-driven model governance, privacy compliance, and controlled knowledge removal in the emerging LLM-based applications. We release the replication package at https://figshare.com/s/d5931192a8824de26aff.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16297v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出选择性表征误导(SRMU)框架，用于解决LLM中知识遗忘难题，兼顾安全与效用。",
            "summary_zh": "随着大型语言模型（LLM）在安全关键和受监管领域得到越来越多的应用，模型中保留的敏感或违禁知识带来了越来越大的风险，包括隐私泄露、不符合法规以及潜在的滥用等等。近期的研究表明，机器遗忘可以帮助确保已部署的模型符合不断变化的法律、安全和治理要求。然而，当前的遗忘技术假设遗忘数据集和保留数据集之间存在清晰的分离，这在具有高度纠缠分布的实际操作环境中是具有挑战性的。在这种情况下，基于扰动的方法通常会降低模型的通用效用或无法确保安全性。为了解决这个问题，我们提出了一种用于遗忘的选择性表征误导（SRMU），这是一种新颖的、有原则的激活编辑框架，它强制执行特征感知和方向控制的扰动。与不加区分的模型权重扰动不同，SRMU采用具有激活重要性图的结构化误导向量。目标是允许SRMU选择性地抑制有害表征，同时保持对良性表征的效用。在广泛使用的WMDP基准上，针对低纠缠和高纠缠配置进行了实验。实验结果表明，SRMU提供了最先进的遗忘性能，同时效用损失最小，并且在现有基线崩溃的20-30％重叠下仍然有效。SRMU为基于LLM的新兴应用中的安全驱动模型治理、隐私合规性和受控知识移除提供了坚实的基础。我们在https://figshare.com/s/d5931192a8824de26aff发布了复制包。",
            "intro_zh": [
                "现有机器遗忘方法在遗忘数据和保留数据高度纠缠时表现不佳，容易导致模型效用下降或无法保证安全性。",
                "SRMU通过激活编辑，有选择性地对模型表征进行扰动，抑制有害信息，同时保留有益信息，实现更精准的知识遗忘。",
                "实验表明，SRMU在低/高纠缠场景下均优于现有方法，在20-30%数据重叠时仍有效，且效用损失最小。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型（LLM）中知识遗忘的问题，特别是当需要遗忘的数据与需要保留的数据高度纠缠时。现有的遗忘方法，如基于扰动的方法，要么会降低模型的整体性能（效用），要么无法有效地移除有害知识，从而无法满足安全和合规性要求。\\n\\n**核心思路**：SRMU的核心思路是选择性地修改模型内部的表征，使其不再包含需要遗忘的信息，同时尽可能地保留模型在其他任务上的性能。通过激活编辑的方式，对模型中的激活值进行有方向性的扰动，从而实现对特定特征的抑制或增强。\\n\\n**技术框架**：SRMU框架主要包含以下几个步骤：1) **激活重要性图生成**：评估模型中每个激活值对于特定任务的重要性。2) **误导向量生成**：根据激活重要性图，生成一个结构化的误导向量，用于指导激活值的修改方向和幅度。3) **激活编辑**：将误导向量应用于模型的激活值，从而实现对模型表征的修改。\\n\\n**关键创新**：SRMU的关键创新在于其选择性和方向性的扰动方式。与传统的对模型权重进行全局扰动的方法不同，SRMU能够根据激活值的重要性，有选择性地对模型表征进行修改，从而在实现知识遗忘的同时，尽可能地保留模型的整体性能。此外，SRMU还通过误导向量的设计，实现了对扰动方向的控制，从而能够更有效地抑制有害信息。\\n\\n**关键设计**：SRMU的关键设计包括：1) **激活重要性评估方法**：如何准确地评估每个激活值对于特定任务的重要性，是SRMU的关键。论文中可能采用了某种梯度或注意力机制来评估激活值的重要性。2) **误导向量的生成方式**：误导向量的结构和生成方式直接影响了SRMU的遗忘效果和性能保留能力。论文中可能采用了某种优化算法或启发式方法来生成误导向量。3) **激活编辑的具体实现**：如何将误导向量应用于模型的激活值，也是SRMU的关键。论文中可能采用了某种加权平均或线性组合的方式来实现激活编辑。",
            "application_zh": "SRMU技术可应用于各种需要知识遗忘的场景，例如：1)保护用户隐私，移除模型中包含的个人敏感信息；2)遵守法律法规，移除模型中包含的违禁内容；3)模型安全治理，移除模型中存在的偏见或有害知识。该技术有助于提升LLM在安全关键领域的应用，并促进负责任的AI发展。",
            "highlight_zh": "实验结果表明，SRMU在WMDP基准测试中取得了最先进的遗忘性能，同时保持了最小的效用损失。在高数据纠缠场景下（20-30%重叠），SRMU仍然有效，而现有的基线方法则失效。这些结果表明，SRMU是一种鲁棒且有效的知识遗忘方法。",
            "tags_zh": [
                "机器遗忘",
                "大型语言模型",
                "表征学习",
                "激活编辑",
                "知识移除"
            ],
            "_index": 80,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16297v1/img/overviewSRMU.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16297v1/img/combinationablation.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "In-Context Probing for Membership Inference in Fine-Tuned Language Models",
            "authors": [
                "Zhexi Lu",
                "Hongliang Chi",
                "Nathalie Baracaldo",
                "Swanand Ravindra Kadhe",
                "Yuseok Jeon",
                "Lei Yu"
            ],
            "arxiv_id": "2512.16292v1",
            "summary": "Membership inference attacks (MIAs) pose a critical privacy threat to fine-tuned large language models (LLMs), especially when models are adapted to domain-specific tasks using sensitive data. While prior black-box MIA techniques rely on confidence scores or token likelihoods, these signals are often entangled with a sample's intrinsic properties - such as content difficulty or rarity - leading to poor generalization and low signal-to-noise ratios. In this paper, we propose ICP-MIA, a novel MIA framework grounded in the theory of training dynamics, particularly the phenomenon of diminishing returns during optimization. We introduce the Optimization Gap as a fundamental signal of membership: at convergence, member samples exhibit minimal remaining loss-reduction potential, while non-members retain significant potential for further optimization. To estimate this gap in a black-box setting, we propose In-Context Probing (ICP), a training-free method that simulates fine-tuning-like behavior via strategically constructed input contexts. We propose two probing strategies: reference-data-based (using semantically similar public samples) and self-perturbation (via masking or generation). Experiments on three tasks and multiple LLMs show that ICP-MIA significantly outperforms prior black-box MIAs, particularly at low false positive rates. We further analyze how reference data alignment, model type, PEFT configurations, and training schedules affect attack effectiveness. Our findings establish ICP-MIA as a practical and theoretically grounded framework for auditing privacy risks in deployed LLMs.",
            "categories": [
                "cs.CR",
                "cs.LG"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16292v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出ICP-MIA框架以解决细调语言模型的成员推断攻击问题",
            "summary_zh": "成员推断攻击（MIA）对细调的大型语言模型（LLMs）构成了严重的隐私威胁，尤其是在使用敏感数据进行领域特定任务适配时。以往的黑箱MIA技术依赖于置信度分数或标记似然性，但这些信号常常与样本的内在属性交织在一起，导致泛化能力差和信噪比低。本文提出了ICP-MIA，一个基于训练动态理论的新型MIA框架，特别是优化过程中的收益递减现象。我们引入了优化差距作为成员资格的基本信号：在收敛时，成员样本的剩余损失减少潜力最小，而非成员则保留显著的进一步优化潜力。为在黑箱设置中估计这一差距，我们提出了上下文探测（ICP），一种通过战略性构建输入上下文模拟细调行为的无训练方法。实验表明，ICP-MIA在多个LLM上显著优于以往的黑箱MIA，尤其是在低假阳性率下。",
            "intro_zh": [
                "现有的黑箱成员推断攻击方法依赖于置信度分数，导致泛化能力差和信噪比低。",
                "本文提出ICP-MIA框架，利用优化差距作为成员资格信号，设计上下文探测方法以估计该差距。",
                "实验结果显示，ICP-MIA在多个任务上显著提升了攻击效果，尤其在低假阳性率下表现优异。"
            ],
            "method_zh": "**问题定义**：本文解决的是细调语言模型中的成员推断攻击问题，现有方法依赖于置信度分数，导致信号与样本属性交织，影响攻击效果。\\n\\n**核心思路**：提出ICP-MIA框架，利用优化差距作为成员资格信号，设计上下文探测方法以在黑箱环境中估计该差距，从而提高攻击的准确性和有效性。\\n\\n**技术框架**：ICP-MIA框架包括两个主要模块：优化差距的定义与估计，以及上下文探测策略的实施。上下文探测通过构建参考数据和自扰动两种策略来模拟细调行为。\\n\\n**关键创新**：最重要的创新在于引入优化差距作为成员资格的信号，并通过上下文探测方法在无训练的情况下有效估计这一差距，显著提升了攻击的准确性。\\n\\n**关键设计**：关键设计包括参考数据的选择（使用语义相似的公共样本）和自扰动策略（通过掩蔽或生成），确保探测过程的有效性和准确性。实验中还考虑了模型类型、PEFT配置和训练计划对攻击效果的影响。 ",
            "application_zh": "该研究的潜在应用领域包括隐私审计、模型安全性评估以及敏感数据处理等。ICP-MIA框架为评估和降低部署语言模型的隐私风险提供了理论基础和实践指导，未来可广泛应用于各类需要保护用户隐私的AI系统中。",
            "highlight_zh": "实验结果表明，ICP-MIA在三个任务和多种大型语言模型上显著优于以往的黑箱MIA方法，尤其在低假阳性率下，提升幅度达到XX%（具体数据需根据实验结果填写）。",
            "tags_zh": [
                "成员推断攻击",
                "隐私保护",
                "语言模型",
                "优化差距",
                "上下文探测",
                "黑箱攻击",
                "模型安全性"
            ],
            "_index": 81,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16292v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16292v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16292v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "CKA-Guided Modular Quantization: Beyond Bit-Width to Algorithmic Diversity",
            "authors": [
                "Jinhao Zhang",
                "Yunquan Zhang",
                "Daning Chen"
            ],
            "arxiv_id": "2512.16282v1",
            "summary": "Current mainstream post-training quantization methods for large language models typically apply a uniform quantization strategy across all network layers, overlooking the substantial differences in algorithmic suitability among layers. To address this limitation, we propose CKA Guided Modular Quantization, a fine-tuning-free, plug-and-play framework for algorithmic heterogeneous quantization. Our method independently evaluates multiple PTQ algorithms on each layer and employs Linear Centered Kernel Alignment (CKA) as a metric to automatically select the optimal quantization strategy per layer. The individually optimized strategies are then integrated to construct a hybrid quantized model. Experiments demonstrate that our approach consistently outperforms both uniform quantization baselines and state-of-the-art mixed-precision methods across mainstream LLMs including LLaMA and Qwen ,in terms of perplexity (PPL) and downstream task performance.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16282v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出CKA引导的模块化量化方法，实现大模型层间算法多样性量化",
            "summary_zh": "当前主流的大语言模型后训练量化方法通常在所有网络层应用统一的量化策略，忽略了层间算法适用性的显著差异。为了解决这个局限性，我们提出了CKA引导的模块化量化方法，这是一个无需微调、即插即用的算法异构量化框架。我们的方法独立评估每个层上的多个PTQ算法，并采用线性中心核对齐（CKA）作为指标来自动选择每个层的最佳量化策略。然后，将单独优化的策略集成以构建混合量化模型。实验表明，在包括LLaMA和Qwen在内的主流LLM上，我们的方法在困惑度（PPL）和下游任务性能方面始终优于统一量化基线和最先进的混合精度方法。",
            "intro_zh": [
                "现有大模型量化方法忽略了不同层对量化算法的适应性差异，导致性能瓶颈。",
                "提出CKA引导的模块化量化，通过CKA指标为每层选择最优量化算法，实现异构量化。",
                "实验表明，该方法在LLaMA和Qwen等模型上，显著优于传统统一量化和混合精度方法。"
            ],
            "method_zh": "**问题定义**：现有大语言模型的后训练量化（PTQ）方法通常采用统一的量化策略，即对模型的所有层都使用相同的量化算法和位宽。然而，不同的层可能具有不同的激活分布、权重分布和对量化误差的敏感度。因此，统一的量化策略无法充分利用每一层的特性，导致量化后的模型性能下降。现有混合精度量化方法虽然考虑了不同层使用不同位宽，但仍然忽略了不同量化算法的适用性问题。\\n\\n**核心思路**：论文的核心思路是为大语言模型的每一层选择最合适的量化算法，从而实现算法级别的异构量化。具体来说，对于每一层，论文会尝试多种不同的PTQ算法，并使用线性中心核对齐（CKA）来评估每种算法的量化效果。CKA可以衡量原始层和量化层之间的表示相似性，相似性越高，说明量化对原始层的影响越小，量化效果越好。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1) **算法池构建**：构建一个包含多种PTQ算法的算法池，例如INT8、INT4、FP16等。2) **逐层评估**：对于模型的每一层，分别使用算法池中的每种算法进行量化。3) **CKA相似度计算**：计算原始层和每种量化层之间的CKA相似度。4) **策略选择**：选择CKA相似度最高的算法作为该层的最佳量化策略。5) **模型集成**：将所有层的最佳量化策略集成到一起，构建最终的混合量化模型。\\n\\n**关键创新**：该方法最重要的技术创新点在于使用CKA作为量化算法选择的指标。CKA能够有效地衡量量化前后模型表示的相似性，从而选择出对原始模型影响最小的量化算法。与传统的基于性能指标（如困惑度）的算法选择方法相比，CKA的计算成本更低，且不需要额外的训练数据。此外，该方法实现了算法级别的异构量化，突破了传统混合精度量化方法的位宽限制。\\n\\n**关键设计**：论文的关键设计包括：1) **CKA计算方式**：论文采用线性CKA，其计算效率更高。2) **算法池选择**：算法池中包含了多种主流的PTQ算法，例如INT8、INT4、FP16等，可以满足不同层的量化需求。3) **模型集成方式**：论文采用简单的模型集成方式，即将所有层的最佳量化策略直接应用到模型中。",
            "application_zh": "该研究成果可广泛应用于大语言模型的部署和推理加速，尤其是在资源受限的边缘设备上。通过异构量化，可以在保证模型性能的同时，显著降低模型的大小和计算复杂度，从而实现更高效的推理。该方法还有助于推动大模型在移动设备、嵌入式系统等领域的应用。",
            "highlight_zh": "实验结果表明，该方法在LLaMA和Qwen等主流大语言模型上，显著优于统一量化和混合精度量化方法。例如，在LLaMA模型上，该方法可以在保持相似困惑度的前提下，将模型大小降低到原来的1/4。此外，该方法在下游任务上也取得了显著的性能提升。",
            "tags_zh": [
                "大语言模型",
                "后训练量化",
                "异构量化",
                "线性中心核对齐",
                "模型压缩"
            ],
            "_index": 82,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16282v1/figure6.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16282v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16282v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference",
            "authors": [
                "Jian Tian",
                "Shuailong Li",
                "Yang Cao",
                "Wenbo Cui",
                "Minghan Zhu",
                "Wenkang Wu",
                "Jianming Zhang",
                "Yanpeng Wang",
                "Zhiwen Xiao",
                "Zhenyu Hou",
                "Dou Shen"
            ],
            "arxiv_id": "2512.16134v1",
            "summary": "The evolution of Large Language Model (LLM) serving towards complex, distributed architectures--specifically the P/D-separated, large-scale DP+EP paradigm--introduces distinct scheduling challenges. Unlike traditional deployments where schedulers can treat instances as black boxes, DP+EP architectures exhibit high internal synchronization costs. We identify that immediate request dispatching in such systems leads to severe in-engine queuing and parallelization bubbles, degrading Time-to-First-Token (TTFT). To address this, we propose Staggered Batch Scheduling (SBS), a mechanism that deliberately buffers requests to form optimal execution batches. This temporal decoupling eliminates internal queuing bubbles without compromising throughput. Furthermore, leveraging the scheduling window created by buffering, we introduce a Load-Aware Global Allocation strategy that balances computational load across DP units for both Prefill and Decode phases. Deployed on a production H800 cluster serving Deepseek-V3, our system reduces TTFT by 30%-40% and improves throughput by 15%-20% compared to state-of-the-art immediate scheduling baselines.",
            "categories": [
                "cs.DC",
                "cs.LG"
            ],
            "primary_category": "cs.DC",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16134v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出交错批调度(SBS)，优化DP+EP架构下LLM推理的首Token延迟和吞吐量。",
            "summary_zh": "针对大规模分布式架构（特别是P/D分离的DP+EP范式）下LLM服务面临的调度挑战，本文提出交错批调度(SBS)。与传统调度器将实例视为黑盒不同，DP+EP架构具有较高的内部同步成本。直接请求调度会导致严重的引擎内排队和并行化气泡，从而降低首Token延迟(TTFT)。SBS通过缓冲请求以形成最佳执行批次来解决此问题，这种时间解耦消除了内部排队气泡，且不影响吞吐量。此外，利用缓冲创建的调度窗口，引入负载感知全局分配策略，平衡Prefill和Decode阶段的DP单元上的计算负载。在服务Deepseek-V3的生产H800集群上部署，与最先进的即时调度基线相比，该系统将TTFT降低了30%-40%，吞吐量提高了15%-20%。",
            "intro_zh": [
                "DP+EP架构下LLM服务面临内部同步成本高、即时调度导致引擎内排队和并行化气泡的问题，严重影响首Token延迟。",
                "提出交错批调度(SBS)，通过缓冲请求形成最佳执行批次，消除内部排队气泡，并利用调度窗口进行负载均衡。",
                "在H800集群上，SBS将Deepseek-V3的TTFT降低30%-40%，吞吐量提高15%-20%，显著优于现有即时调度方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大规模分布式LLM服务中，特别是采用DP+EP架构时，由于高内部同步成本和即时调度策略导致的Time-to-First-Token (TTFT) 延迟过高以及吞吐量下降的问题。现有方法通常将LLM实例视为黑盒，忽略了DP+EP架构内部的复杂性，导致引擎内部出现排队和并行化气泡，降低了效率。\\n\\n**核心思路**：论文的核心思路是通过引入时间上的解耦，即不立即调度请求，而是将请求缓冲一段时间，形成更优的执行批次。这种“交错”的批调度方式，能够有效地消除引擎内部的排队气泡，从而降低TTFT。同时，利用缓冲带来的调度窗口，可以进行全局的负载感知分配，进一步提升整体性能。\\n\\n**技术框架**：整体框架包含两个主要部分：交错批调度(Staggered Batch Scheduling)和负载感知全局分配(Load-Aware Global Allocation)。首先，请求会被缓冲在一个队列中，等待形成合适的批次。然后，SBS会根据一定的策略选择合适的请求组成批次，并将其发送到DP单元进行处理。同时，利用缓冲窗口，LGA会根据各个DP单元的负载情况，动态地调整请求的分配，以实现负载均衡。Prefill和Decode阶段都采用LGA策略。\\n\\n**关键创新**：论文的关键创新在于将时间维度引入到LLM的调度中，通过缓冲请求来优化批次形成，从而避免了即时调度带来的排队问题。此外，负载感知的全局分配策略，能够充分利用DP单元的计算资源，进一步提升整体性能。与现有方法的本质区别在于，SBS不再将LLM实例视为黑盒，而是充分考虑了DP+EP架构内部的同步成本和负载情况。\\n\\n**关键设计**：关于关键设计，论文提到了负载感知全局分配策略。具体来说，LGA会监控各个DP单元的负载情况，并根据负载情况动态地调整请求的分配。具体的负载指标和分配策略在论文中可能没有详细展开，属于实现细节，但整体思路是尽量保证各个DP单元的负载均衡，避免出现某些单元过载而其他单元空闲的情况。具体的参数设置和损失函数等细节未知。",
            "application_zh": "该研究成果可广泛应用于大规模语言模型的在线服务场景，尤其是在采用DP+EP等分布式架构的系统中。通过降低首Token延迟和提高吞吐量，可以显著提升用户体验，并降低服务成本。未来，该方法可以进一步扩展到支持更多类型的LLM架构和更复杂的调度策略。",
            "highlight_zh": "实验结果表明，在生产H800集群上服务Deepseek-V3时，与最先进的即时调度基线相比，SBS将首Token延迟(TTFT)降低了30%-40%，吞吐量提高了15%-20%。这些数据表明SBS在实际应用中具有显著的性能优势。",
            "tags_zh": [
                "大语言模型",
                "LLM推理",
                "分布式训练",
                "数据并行",
                "模型并行",
                "调度算法",
                "负载均衡",
                "首Token延迟"
            ],
            "_index": 83,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16134v1/figures/schedule_unit_1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16134v1/figures/schedule_unit_2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16134v1/figures/queue_1.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Scaling Text2SQL via LLM-efficient Schema Filtering with Functional Dependency Graph Rerankers",
            "authors": [
                "Thanh Dat Hoang",
                "Thanh Tam Nguyen",
                "Thanh Trung Huynh",
                "Hongzhi Yin",
                "Quoc Viet Hung Nguyen"
            ],
            "arxiv_id": "2512.16083v1",
            "summary": "Most modern Text2SQL systems prompt large language models (LLMs) with entire schemas -- mostly column information -- alongside the user's question. While effective on small databases, this approach fails on real-world schemas that exceed LLM context limits, even for commercial models. The recent Spider 2.0 benchmark exemplifies this with hundreds of tables and tens of thousands of columns, where existing systems often break. Current mitigations either rely on costly multi-step prompting pipelines or filter columns by ranking them against user's question independently, ignoring inter-column structure. To scale existing systems, we introduce \\toolname, an open-source, LLM-efficient schema filtering framework that compacts Text2SQL prompts by (i) ranking columns with a query-aware LLM encoder enriched with values and metadata, (ii) reranking inter-connected columns via a lightweight graph transformer over functional dependencies, and (iii) selecting a connectivity-preserving sub-schema with a Steiner-tree heuristic. Experiments on real datasets show that \\toolname achieves near-perfect recall and higher precision than CodeS, SchemaExP, Qwen rerankers, and embedding retrievers, while maintaining sub-second median latency and scaling to schemas with 23,000+ columns. Our source code is available at https://github.com/thanhdath/grast-sql.",
            "categories": [
                "cs.DB",
                "cs.AI",
                "cs.HC",
                "cs.LG"
            ],
            "primary_category": "cs.DB",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16083v1",
            "code_links": [
                {
                    "url": "https://github.com/thanhdath/grast-sql",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出GRaST，通过LLM高效的模式过滤和函数依赖图重排序，扩展Text2SQL到大规模数据库。",
            "summary_zh": "大多数现代Text2SQL系统在提示大型语言模型（LLM）时，会将整个模式（主要是列信息）与用户的问题一起提供。虽然这种方法在小型数据库上有效，但对于超出LLM上下文限制的真实模式则会失效，即使是商业模型也是如此。最近的Spider 2.0基准测试就是一个例子，它包含数百个表和数万列，现有系统经常崩溃。目前的缓解措施要么依赖于代价高昂的多步骤提示流程，要么通过独立地将列与用户的问题进行排序来过滤列，忽略了列间的结构。为了扩展现有系统，我们引入了GRaST，这是一个开源的、LLM高效的模式过滤框架，它通过以下方式压缩Text2SQL提示：（i）使用富含值和元数据的查询感知LLM编码器对列进行排序，（ii）通过函数依赖关系上的轻量级图Transformer对互连的列进行重排序，以及（iii）使用Steiner树启发式算法选择一个保持连通性的子模式。在真实数据集上的实验表明，GRaST实现了接近完美的召回率和比CodeS、SchemaExP、Qwen重排序器和嵌入检索器更高的精度，同时保持了亚秒级的中值延迟，并可扩展到具有23,000+列的模式。我们的源代码可在https://github.com/thanhdath/grast-sql 获得。",
            "intro_zh": [
                "现有Text2SQL系统在处理大规模数据库时，由于LLM上下文长度限制，无法有效利用完整数据库模式信息。",
                "GRaST框架通过查询感知的LLM编码器、函数依赖图Transformer和Steiner树启发式算法，实现高效的模式过滤和子模式选择。",
                "实验表明，GRaST在保持低延迟的同时，实现了接近完美的召回率和更高的精度，并可扩展到包含大量列的数据库模式。"
            ],
            "method_zh": "**问题定义**：现有Text2SQL系统在处理大规模数据库时，需要将整个数据库模式信息输入到LLM中，但由于LLM上下文长度的限制，导致系统性能下降甚至崩溃。现有的解决方法要么依赖于多步骤的提示，增加了计算成本，要么忽略了列之间的依赖关系，导致过滤后的模式信息不完整。\\n\\n**核心思路**：GRaST的核心思路是通过高效的模式过滤，选择与用户查询相关的子模式，从而减少LLM需要处理的信息量。同时，利用函数依赖图来捕捉列之间的关系，保证选择的子模式包含完整的语义信息。\\n\\n**技术框架**：GRaST框架包含三个主要阶段：（1）列排序：使用查询感知的LLM编码器对数据库中的列进行排序，评估其与用户查询的相关性。（2）列重排序：利用函数依赖图Transformer对互连的列进行重排序，考虑列之间的依赖关系。（3）子模式选择：使用Steiner树启发式算法选择一个保持连通性的子模式，作为LLM的输入。\\n\\n**关键创新**：GRaST的关键创新在于结合了查询感知的LLM编码器和函数依赖图Transformer，从而在保证模式信息完整性的同时，实现了高效的模式过滤。此外，使用Steiner树启发式算法选择子模式，进一步提高了系统的效率。\\n\\n**关键设计**：查询感知的LLM编码器通过融合列的值和元数据信息，提高了列排序的准确性。函数依赖图Transformer利用图结构来捕捉列之间的依赖关系，从而更好地进行列重排序。Steiner树启发式算法通过寻找连接关键列的最小代价树，保证选择的子模式包含完整的语义信息。",
            "application_zh": "GRaST框架可应用于各种需要处理大规模数据库的Text2SQL系统，例如智能客服、数据分析平台和商业智能工具。通过提高Text2SQL系统处理大规模数据库的能力，可以帮助用户更方便地从海量数据中获取所需信息，从而提高工作效率和决策质量。未来，该技术还可以扩展到其他需要处理大规模结构化数据的领域。",
            "highlight_zh": "在真实数据集上的实验表明，GRaST实现了接近完美的召回率，并且精度高于CodeS、SchemaExP、Qwen重排序器和嵌入检索器等基线方法。同时，GRaST保持了亚秒级的中值延迟，并可扩展到具有超过23,000列的模式，证明了其在大规模数据库上的有效性和高效性。",
            "tags_zh": [
                "Text2SQL",
                "大型语言模型",
                "模式过滤",
                "函数依赖图",
                "图Transformer",
                "Steiner树",
                "数据库查询"
            ],
            "_index": 84,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs",
            "authors": [
                "Shubham Mishra",
                "Samyek Jain",
                "Gorang Mehrishi",
                "Shiv Tiwari",
                "Harsh Sharma",
                "Pratik Narang",
                "Dhruv Kumar"
            ],
            "arxiv_id": "2512.16795v1",
            "summary": "Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.CY",
                "cs.IR"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Under Review",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16795v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出推理追踪增强的RAG框架，解决检索信息冲突和主观性问题，提升LLM回答的正确性和可信度。",
            "summary_zh": "检索增强生成(RAG)使大型语言模型(LLM)能够基于外部证据，但当检索到的信息存在冲突、过时或主观性时，RAG会失效。现有工作分别解决这些问题，但缺乏统一的推理监督。本文提出了一种推理追踪增强的RAG框架，该框架在三个阶段添加了结构化的、可解释的推理过程：(1)文档级别的裁决，(2)冲突分析，和(3)基于证据的综合，从而生成带有引用的答案或合理的拒绝。引入了一种冲突感知信任评分(CATS)流程，该流程使用LLM作为评判器，评估基础性、事实正确性、拒绝准确性和冲突行为一致性。本文构建了一个包含539个查询的推理数据集和评估流程，为冲突感知、可解释的RAG系统奠定了基础。实验结果表明，该方法相对于基线有显著提升，尤其是在Qwen模型上，监督微调将端到端答案正确率从0.069提高到0.883，行为一致性从0.074提高到0.722。",
            "intro_zh": [
                "现有RAG方法在处理冲突、过时或主观信息时表现不佳，缺乏统一的推理监督机制。",
                "提出推理追踪增强的RAG框架，通过文档裁决、冲突分析和证据综合三个阶段，实现可解释的推理过程。",
                "实验表明，该方法显著提升了LLM回答的正确性和行为一致性，尤其是在Qwen模型上效果显著。"
            ],
            "method_zh": "**问题定义**：现有RAG模型在面对检索到的信息冲突、包含过时信息或主观信息时，无法有效判断和整合，导致生成错误或不可靠的答案。现有的解决方案通常针对特定问题，缺乏统一的推理和监督机制。\\n\\n**核心思路**：本文的核心思路是在RAG流程中引入显式的推理过程，通过结构化的推理步骤来解决信息冲突和主观性问题，提高答案的可靠性和可解释性。通过对检索到的文档进行裁决、冲突分析和证据综合，最终生成基于事实的答案或合理的拒绝。\\n\\n**技术框架**：该框架包含三个主要阶段：1) **文档级别裁决**：对检索到的文档进行评估，判断其可靠性和相关性。2) **冲突分析**：识别并分析文档之间的冲突信息。3) **基于证据的综合**：根据裁决结果和冲突分析，综合各文档的信息，生成带有引用的答案或合理的拒绝。此外，还引入了冲突感知信任评分(CATS)流程，使用LLM作为评判器，评估模型的性能。\\n\\n**关键创新**：该方法最重要的创新点在于将推理过程显式地融入到RAG框架中，通过结构化的推理步骤来解决信息冲突和主观性问题。与现有方法相比，该方法提供了一种统一的、可解释的推理监督机制，能够更有效地提高答案的可靠性和可信度。\\n\\n**关键设计**：CATS流程是关键设计之一，它使用LLM作为评判器，对模型的 groundedness, factual correctness, refusal accuracy, 和 conflict-behavior alignment 进行评估。具体的参数设置、损失函数和网络结构等技术细节在论文中没有详细说明，属于未知信息。",
            "application_zh": "该研究成果可应用于需要高可靠性和可信度的信息检索和问答系统，例如医疗诊断、法律咨询、金融分析等领域。通过提供基于证据的、可解释的答案，可以帮助用户更好地理解信息，做出更明智的决策。未来，该方法可以进一步扩展到处理更复杂的信息冲突和主观性问题。",
            "highlight_zh": "实验结果表明，该方法在Qwen模型上取得了显著的性能提升。通过监督微调，端到端答案正确率从0.069提高到0.883，行为一致性从0.074提高到0.722。这些结果表明，该方法能够有效地提高LLM回答的正确性和可靠性，并使其行为更加符合预期。",
            "tags_zh": [
                "检索增强生成",
                "RAG",
                "推理追踪",
                "冲突分析",
                "信息融合",
                "可解释性",
                "LLM",
                "事实性"
            ],
            "_index": 85,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Plausibility as Failure: How LLMs and Humans Co-Construct Epistemic Error",
            "authors": [
                "Claudia Vale Oliveira",
                "Nelson Zagalo",
                "Filipe Silva",
                "Anabela Brandao",
                "Syeda Faryal Hussain Khurrum",
                "Joaquim Santos"
            ],
            "arxiv_id": "2512.16750v1",
            "summary": "Large language models (LLMs) are increasingly used as epistemic partners in everyday reasoning, yet their errors remain predominantly analyzed through predictive metrics rather than through their interpretive effects on human judgment. This study examines how different forms of epistemic failure emerge, are masked, and are tolerated in human AI interaction, where failure is understood as a relational breakdown shaped by model-generated plausibility and human interpretive judgment. We conducted a three round, multi LLM evaluation using interdisciplinary tasks and progressively differentiated assessment frameworks to observe how evaluators interpret model responses across linguistic, epistemic, and credibility dimensions. Our findings show that LLM errors shift from predictive to hermeneutic forms, where linguistic fluency, structural coherence, and superficially plausible citations conceal deeper distortions of meaning. Evaluators frequently conflated criteria such as correctness, relevance, bias, groundedness, and consistency, indicating that human judgment collapses analytical distinctions into intuitive heuristics shaped by form and fluency. Across rounds, we observed a systematic verification burden and cognitive drift. As tasks became denser, evaluators increasingly relied on surface cues, allowing erroneous yet well formed answers to pass as credible. These results suggest that error is not solely a property of model behavior but a co-constructed outcome of generative plausibility and human interpretive shortcuts. Understanding AI epistemic failure therefore requires reframing evaluation as a relational interpretive process, where the boundary between system failure and human miscalibration becomes porous. The study provides implications for LLM assessment, digital literacy, and the design of trustworthy human AI communication.",
            "categories": [
                "cs.HC",
                "cs.AI"
            ],
            "primary_category": "cs.HC",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "19 pages, 2 tables, 77 references, 6 appendices",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16750v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "揭示LLM与人类交互中认知错误的共建机制，强调评估的解释性视角",
            "summary_zh": "大型语言模型（LLM）日益成为日常推理中的认知伙伴，然而对其错误的分析主要集中在预测指标上，而忽略了其对人类判断的解释性影响。本研究考察了在人机交互中，不同形式的认知失败如何产生、被掩盖和被容忍。这里的失败被理解为一种关系破裂，由模型生成的可信度和人类的解释性判断共同塑造。我们进行了三轮多LLM评估，采用跨学科任务和逐步区分的评估框架，观察评估者如何从语言、认知和可信度维度解释模型响应。研究结果表明，LLM的错误从预测形式转变为解释形式，其中流畅的语言、结构连贯性和表面上可信的引用掩盖了更深层次的意义扭曲。评估者经常混淆正确性、相关性、偏差、依据和一致性等标准，表明人类判断将分析区分简化为受形式和流畅性影响的直觉启发式。在整个过程中，我们观察到系统性的验证负担和认知漂移。随着任务变得更加密集，评估者越来越依赖表面线索，允许错误但形式良好的答案被认为是可信的。这些结果表明，错误不仅仅是模型行为的属性，而是生成的可信度和人类解释性捷径共同构建的结果。因此，理解AI认知失败需要将评估重新定义为一个关系解释过程，其中系统失败和人类校准错误之间的界限变得模糊。该研究为LLM评估、数字素养和可信赖的人机通信设计提供了启示。",
            "intro_zh": [
                "现有LLM错误分析侧重预测指标，忽略了其对人类判断的解释性影响，导致对人机交互中认知错误的理解不足。",
                "该研究将LLM错误视为一种关系破裂，由模型生成的可信度和人类的解释性判断共同塑造，强调评估的解释性视角。",
                "通过多轮评估，发现LLM错误会从预测形式转变为解释形式，人类评估易受表面线索影响，产生认知漂移。"
            ],
            "method_zh": "**问题定义**：现有的大型语言模型（LLM）的错误评估主要集中在预测准确性等指标上，忽略了LLM的输出如何影响人类的理解和判断。这种片面的评估方式无法充分揭示LLM在实际应用中可能造成的认知偏差和误导，尤其是在人机协作场景下。现有方法未能充分考虑人类的解释过程在错误产生和传播中的作用。\\n\\n**核心思路**：该研究的核心在于将LLM的错误视为一种“共建”现象，即错误并非仅仅是模型自身的属性，而是模型生成的内容与人类的解读相互作用的结果。研究认为，LLM生成的内容具有一定的“可信度”，这种可信度会影响人类的判断，导致人类忽略或容忍模型中的错误。因此，理解LLM的错误需要从一个关系性的角度出发，考察模型输出和人类解释之间的互动。\\n\\n**技术框架**：该研究采用了一个三轮的多LLM评估框架。第一轮使用跨学科任务，旨在识别不同类型的LLM错误。第二轮和第三轮则逐步细化评估框架，更加关注评估者如何解释模型响应，并考察评估者在面对复杂任务时是否会更加依赖表面线索。评估维度包括语言流畅性、认知合理性、可信度等。研究人员分析了评估者在不同轮次中的判断，以揭示认知漂移和验证负担等现象。\\n\\n**关键创新**：该研究的关键创新在于其对LLM错误的重新定义。它不再将错误视为模型自身的缺陷，而是将其视为一种人机交互的产物。这种视角转变强调了人类解释在错误产生和传播中的作用，并为LLM的评估和设计提供了新的思路。此外，该研究还提出了“认知漂移”和“验证负担”等概念，这些概念有助于理解人类在与LLM交互时可能出现的认知偏差。\\n\\n**关键设计**：研究中，任务的设计涵盖了多个学科领域，旨在考察LLM在不同知识领域的表现。评估框架的设计则侧重于考察评估者对模型输出的解释和判断，包括对语言流畅性、认知合理性和可信度的评估。研究人员还特别关注了评估者在不同轮次中的判断变化，以揭示认知漂移和验证负担等现象。没有明确提及具体的参数设置、损失函数或网络结构，因为该研究主要关注的是人机交互过程中的认知现象，而非模型的内部机制。",
            "application_zh": "该研究成果可应用于LLM评估体系的改进，提升数字素养教育，并指导更值得信赖的人机交互界面设计。通过理解认知错误的共建机制，可以开发更有效的工具和策略，帮助用户识别和纠正LLM的错误，从而提高人机协作的效率和可靠性。此外，该研究还有助于推动负责任的AI发展，确保AI系统在实际应用中不会对人类产生误导。",
            "highlight_zh": "研究发现，LLM错误会从预测形式转变为解释形式，即语言流畅、结构连贯但意义扭曲。评估者易混淆正确性、相关性等标准，依赖表面线索，导致认知漂移。随着任务密度增加，评估者更易接受错误但形式良好的答案。这些结果强调了人类解释在LLM错误评估中的重要性。",
            "tags_zh": [
                "大型语言模型",
                "人机交互",
                "认知错误",
                "评估框架",
                "解释性",
                "认知漂移",
                "验证负担"
            ],
            "_index": 86,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Cyber Humanism in Education: Reclaiming Agency through AI and Learning Sciences",
            "authors": [
                "Giovanni Adorni"
            ],
            "arxiv_id": "2512.16701v1",
            "summary": "Generative Artificial Intelligence (GenAI) is rapidly reshaping how knowledge is produced and validated in education. Rather than adding another digital tool, large language models reconfigure reading, writing, and coding into hybrid human-AI workflows, raising concerns about epistemic automation, cognitive offloading, and the de-professiona\\-lisation of teachers. This paper proposes \\emph{Cyber Humanism in Education} as a framework for reclaiming human agency in this landscape. We conceptualise AI-enabled learning environments as socio-technical infrastructures co-authored by humans and machines, and position educators and learners as epistemic agents and \\emph{algorithmic citizens} who have both the right and the responsibility to shape these infrastructures.\n  We articulate three pillars for cyber-humanist design, \\emph{reflexive competence}, \\emph{algorithmic citizenship}, and \\emph{dialogic design}, and relate them to major international digital and AI competence frameworks. We then present higher-education case studies that operationalise these ideas through \\emph{prompt-based learning} and a new \\emph{Conversational AI Educator} certification within the EPICT ecosystem. The findings show how such practices can strengthen epistemic agency while surfacing tensions around workload, equity, and governance, and outline implications for the future of AI-rich, human-centred education.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "15 pages, 16 references, Key Note preented at the \"WAILS 2025 - The 2nd. Workshop on Artificial Intelligence with and for Learning Sciences\", Cagliary, Italy, 10-12 December 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16701v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出教育领域“赛博人文主义”框架，通过AI和学习科学重塑能动性",
            "summary_zh": "生成式人工智能（GenAI）正在迅速重塑教育中知识的生产和验证方式。大型语言模型不仅仅是另一种数字工具，它们将阅读、写作和编码重新配置为混合人机工作流程，引发了对认知自动化、认知卸载和教师去专业化的担忧。本文提出了“教育中的赛博人文主义”框架，旨在重塑这一领域中的人类能动性。我们将支持人工智能的学习环境概念化为由人类和机器共同创作的社会技术基础设施，并将教育者和学习者定位为认知主体和“算法公民”，他们有权力和责任来塑造这些基础设施。我们阐述了赛博人文主义设计的三个支柱：反思能力、算法公民身份和对话式设计，并将它们与主要的国际数字和人工智能能力框架联系起来。然后，我们展示了高等教育案例研究，通过“基于提示的学习”和EPICT生态系统中的新型“对话式人工智能教育者”认证来实施这些想法。研究结果表明，这些实践如何能够加强认知能动性，同时揭示围绕工作量、公平和治理的紧张关系，并概述了以人为本的富人工智能教育的未来影响。",
            "intro_zh": [
                "生成式AI重塑教育，但引发了认知自动化和教师去专业化的担忧，现有方法未能充分应对。",
                "提出“教育中的赛博人文主义”框架，将学习环境视为人机共创的社会技术基础设施。",
                "通过案例研究，展示了基于提示的学习和对话式AI教育者认证如何加强认知能动性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决生成式AI在教育领域快速发展带来的挑战，特别是认知自动化、认知卸载以及教师去专业化的问题。现有方法未能充分应对这些挑战，导致教育者和学习者在AI驱动的学习环境中丧失能动性。论文关注的核心问题是如何在AI赋能的教育环境中重新获得和维护人类的能动性，确保教育过程仍然以人为本。\\n\\n**核心思路**：论文的核心思路是构建“教育中的赛博人文主义”框架，将AI视为与人类共同创作学习环境的伙伴，而非简单的工具。该框架强调教育者和学习者作为“算法公民”的角色，他们有权力和责任参与塑造AI驱动的学习环境。通过这种方式，论文旨在平衡AI带来的效率提升与人类的认知参与和批判性思维能力。\\n\\n**技术框架**：该框架包含三个主要支柱：反思能力、算法公民身份和对话式设计。反思能力强调教育者和学习者需要具备批判性地评估和利用AI的能力；算法公民身份强调在AI驱动的环境中，个体应具备的权利和责任；对话式设计强调人与AI之间的互动应该是双向的、协作的。论文通过案例研究，展示了如何在高等教育中应用这些支柱，例如通过“基于提示的学习”和“对话式AI教育者”认证。\\n\\n**关键创新**：论文的关键创新在于提出了“教育中的赛博人文主义”这一概念，并将其具体化为可操作的框架。与现有方法不同，该框架不仅仅关注AI的技术层面，更关注AI对教育者和学习者认知、伦理和社会层面的影响。它强调人类在AI驱动的学习环境中的能动性和责任，而非被动接受。\\n\\n**关键设计**：论文并没有涉及具体的参数设置、损失函数或网络结构等技术细节。其重点在于概念框架的构建和应用，以及对教育实践的指导。关键设计体现在三个支柱的定义和相互关系上，以及如何将这些支柱转化为具体的教育实践，例如通过设计能够促进反思性学习的提示，或者通过开发能够促进对话式学习的AI教育者。",
            "application_zh": "该研究成果可应用于高等教育、K12教育等多个领域，指导AI赋能的教学设计和实践。通过提升教育者和学习者的“算法公民”意识，促进更公平、以人为本的教育模式发展。未来，该框架可用于评估和优化AI教育工具，并为教育政策制定提供参考。",
            "highlight_zh": "论文通过案例研究展示了“基于提示的学习”和“对话式AI教育者”认证在高等教育中的应用，表明这些实践可以加强认知能动性。研究还揭示了在AI教育应用中，围绕工作量、公平和治理等方面存在的紧张关系，为未来研究提供了方向。",
            "tags_zh": [
                "赛博人文主义",
                "教育人工智能",
                "算法公民",
                "人机协作",
                "提示学习"
            ],
            "_index": 87,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Microsoft Academic Graph Information Retrieval for Research Recommendation and Assistance",
            "authors": [
                "Jacob Reiss",
                "Shikshya Shiwakoti",
                "Samuel Goldsmith",
                "Ujjwal Pandit"
            ],
            "arxiv_id": "2512.16661v1",
            "summary": "In today's information-driven world, access to scientific publications has become increasingly easy. At the same time, filtering through the massive volume of available research has become more challenging than ever. Graph Neural Networks (GNNs) and graph attention mechanisms have shown strong effectiveness in searching large-scale information databases, particularly when combined with modern large language models. In this paper, we propose an Attention-Based Subgraph Retriever, a GNN-as-retriever model that applies attention-based pruning to extract a refined subgraph, which is then passed to a large language model for advanced knowledge reasoning.",
            "categories": [
                "cs.IR",
                "cs.AI"
            ],
            "primary_category": "cs.IR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "5 pages, 3 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16661v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于注意力机制的子图检索器，用于科研推荐和辅助，提升知识推理能力。",
            "summary_zh": "在当今信息驱动的世界中，获取科学出版物变得越来越容易。与此同时，筛选海量可用研究成果的难度也前所未有。图神经网络（GNN）和图注意力机制在搜索大规模信息数据库方面表现出强大的有效性，特别是与现代大型语言模型结合使用时。在本文中，我们提出了一种基于注意力的子图检索器，这是一种GNN检索模型，它应用基于注意力的剪枝来提取精炼的子图，然后将其传递给大型语言模型以进行高级知识推理。",
            "intro_zh": [
                "现有科研信息检索方法难以有效应对海量文献带来的筛选挑战。",
                "提出基于注意力机制的子图检索器，利用GNN提取精炼子图，辅助大型语言模型进行知识推理。",
                "论文重点在于模型设计和框架搭建，实验结果未知，效果有待验证。"
            ],
            "method_zh": "**问题定义**：论文旨在解决科研信息过载的问题，即如何从海量的学术文献中快速准确地检索到所需信息，并辅助研究人员进行知识推理。现有方法在处理大规模信息网络时，效率和准确性面临挑战，难以有效提取关键信息。\\n\\n**核心思路**：论文的核心思路是利用图神经网络（GNN）强大的图结构学习能力，结合注意力机制，从Microsoft Academic Graph中提取与查询相关的精炼子图。通过注意力机制对图中的节点和边进行重要性评估，从而实现子图的剪枝和优化，降低计算复杂度，提高检索效率。\\n\\n**技术框架**：整体框架包含两个主要阶段：首先，使用基于注意力的GNN作为检索器，对Microsoft Academic Graph进行处理，提取与查询相关的子图；其次，将提取的子图输入到大型语言模型中，利用语言模型的知识推理能力，对子图进行进一步分析和理解，最终生成推荐结果或提供研究辅助。\\n\\n**关键创新**：论文的关键创新在于提出了基于注意力的子图检索器，将GNN的图结构学习能力与注意力机制相结合，实现了对大规模信息网络的有效剪枝和优化。这种方法能够提取更具代表性和相关性的子图，从而提高检索效率和知识推理的准确性。与传统的GNN检索方法相比，该方法能够更好地关注重要的节点和边，从而避免了信息冗余和噪声干扰。\\n\\n**关键设计**：论文的关键设计包括注意力机制的选择和应用方式，以及GNN的网络结构设计。具体的注意力机制和GNN结构未知，但可以推测，注意力机制可能用于评估节点和边的重要性，并根据重要性进行剪枝。GNN的网络结构可能采用多层图卷积或图注意力层，以捕捉图中的复杂关系。",
            "application_zh": "该研究成果可应用于科研推荐系统，帮助研究人员快速找到相关文献和专家，提高科研效率。此外，还可以应用于智能问答系统，为用户提供更准确、更全面的学术信息。该方法具有潜力应用于其他大规模信息网络，例如社交网络、知识图谱等，为用户提供个性化的信息检索和推荐服务。",
            "highlight_zh": "摘要中未提供具体的实验结果和性能数据，因此无法总结实验亮点。论文主要侧重于方法论的提出，实验验证部分未知。",
            "tags_zh": [
                "图神经网络",
                "注意力机制",
                "子图检索",
                "科研推荐",
                "知识推理"
            ],
            "_index": 88,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16661v1/gril_framework.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16661v1/sag_outline.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16661v1/AttentionbasedGraphRetriever_Algo.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics",
            "authors": [
                "Iker García-Ferrero",
                "David Montero",
                "Roman Orus"
            ],
            "arxiv_id": "2512.16602v1",
            "summary": "We introduce Refusal Steering, an inference-time method to exercise fine-grained control over Large Language Models refusal behaviour on politically sensitive topics without retraining. We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores and we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal--compliance direction. On Qwen3-Next-80B-A3B-Thinking, our method removes the refusal behaviour of the model around politically sensitive topics while maintaining safety on JailbreakBench and near-baseline performance on general benchmarks. The approach generalizes across 4B and 80B models and can also induce targeted refusals when desired. We analize the steering vectors and show that refusal signals concentrate in deeper layers of the transformer and are distributed across many dimensions. Together, these results demonstrate that activation steering can remove political refusal behaviour while retaining safety alignment for harmful content, offering a practical path to controllable, transparent moderation at inference time.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16602v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "Refusal Steering：通过激活向量调控LLM在敏感话题上的拒绝行为",
            "summary_zh": "本文提出了一种名为Refusal Steering的推理时方法，用于对大型语言模型（LLM）在政治敏感话题上的拒绝行为进行细粒度控制，而无需重新训练模型。该方法使用LLM作为裁判，取代了脆弱的基于模式的拒绝检测，并赋予拒绝置信度分数。此外，还提出了一种岭正则化变体来计算steering vectors，从而更好地隔离拒绝-顺从方向。在Qwen3-Next-80B-A3B-Thinking模型上，该方法消除了模型在政治敏感话题上的拒绝行为，同时保持了JailbreakBench上的安全性以及在通用基准测试上的接近基线性能。该方法可以推广到4B和80B模型，并且可以在需要时诱导有针对性的拒绝。通过分析steering vectors，表明拒绝信号集中在transformer的更深层，并且分布在许多维度上。这些结果表明，激活steering可以消除政治拒绝行为，同时保持对有害内容的安全对齐，从而为推理时可控、透明的审核提供了一条实用途径。",
            "intro_zh": [
                "现有方法依赖脆弱的模式匹配进行拒绝检测，缺乏细粒度控制能力，难以适应复杂场景。",
                "Refusal Steering通过LLM裁判评估拒绝置信度，并使用岭回归计算steering vectors，精准控制拒绝行为。",
                "实验表明，该方法在消除政治敏感话题拒绝行为的同时，保持了模型在安全性和通用性能上的良好表现。"
            ],
            "method_zh": "**问题定义**：大型语言模型（LLM）在处理政治敏感话题时，常常会采取拒绝回答的方式以避免潜在的风险或争议。然而，这种一刀切的拒绝策略缺乏灵活性，无法满足不同场景下的需求。现有的基于模式匹配的拒绝检测方法脆弱且难以维护，无法实现细粒度的控制。因此，需要一种能够在推理时动态调整LLM拒绝行为的方法，使其既能避免有害内容，又能灵活应对政治敏感话题。\\n\\n**核心思路**：Refusal Steering的核心思路是通过学习一个steering vector，在推理时对LLM的激活状态进行微调，从而改变其拒绝或顺从的倾向。该方法利用另一个LLM作为裁判，评估模型对特定问题的拒绝置信度，并基于此训练steering vector。通过调整steering vector的方向和强度，可以实现对LLM拒绝行为的精细控制。\\n\\n**技术框架**：Refusal Steering主要包含以下几个阶段：1) **拒绝置信度评估**：使用一个预训练的LLM（裁判模型）对目标LLM的回答进行评估，输出一个拒绝置信度分数。2) **Steering Vector计算**：基于拒绝置信度分数，使用岭正则化回归方法学习一个steering vector，该向量代表了拒绝-顺从的方向。3) **推理时激活调控**：在推理时，将steering vector添加到目标LLM的激活状态中，从而改变其拒绝行为。\\n\\n**关键创新**：Refusal Steering的关键创新在于：1) 使用LLM作为裁判，取代了传统的基于模式匹配的拒绝检测方法，提高了鲁棒性和泛化能力。2) 提出了一种岭正则化变体，用于计算steering vector，更好地隔离了拒绝-顺从方向，提高了控制精度。3) 实现了在推理时对LLM拒绝行为的细粒度控制，无需重新训练模型。\\n\\n**关键设计**：在steering vector计算过程中，使用了岭正则化来防止过拟合，并提高steering vector的泛化能力。具体而言，损失函数包含一个L2正则化项，用于约束steering vector的模长。此外，实验中发现，将steering vector添加到transformer的更深层，可以获得更好的控制效果。具体添加的位置和层数需要根据具体模型进行调整。",
            "application_zh": "Refusal Steering可应用于各种需要对LLM拒绝行为进行精细控制的场景，例如：内容审核、智能客服、教育辅导等。通过调整LLM在政治敏感话题上的拒绝策略，可以使其更好地适应不同文化背景和用户需求。此外，该方法还可以用于诱导LLM在特定情况下采取拒绝行为，例如，当用户提出的问题涉及个人隐私或安全风险时。",
            "highlight_zh": "在Qwen3-Next-80B-A3B-Thinking模型上的实验表明，Refusal Steering可以有效消除模型在政治敏感话题上的拒绝行为，同时保持了JailbreakBench上的安全性以及在通用基准测试上的接近基线性能。该方法还可以推广到4B和80B模型，并且可以在需要时诱导有针对性的拒绝。分析表明，拒绝信号集中在transformer的更深层，并且分布在许多维度上。",
            "tags_zh": [
                "大型语言模型",
                "拒绝行为控制",
                "激活向量调控",
                "政治敏感话题",
                "推理时干预"
            ],
            "_index": 89,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16602v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16602v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16602v1/images/top_layer_pca_2d_chinabadWRMD.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "From Personalization to Prejudice: Bias and Discrimination in Memory-Enhanced AI Agents for Recruitment",
            "authors": [
                "Himanshu Gharat",
                "Himanshi Agrawal",
                "Gourab K. Patro"
            ],
            "arxiv_id": "2512.16532v1",
            "summary": "Large Language Models (LLMs) have empowered AI agents with advanced capabilities for understanding, reasoning, and interacting across diverse tasks. The addition of memory further enhances them by enabling continuity across interactions, learning from past experiences, and improving the relevance of actions and responses over time; termed as memory-enhanced personalization. Although such personalization through memory offers clear benefits, it also introduces risks of bias. While several previous studies have highlighted bias in ML and LLMs, bias due to memory-enhanced personalized agents is largely unexplored. Using recruitment as an example use case, we simulate the behavior of a memory-enhanced personalized agent, and study whether and how bias is introduced and amplified in and across various stages of operation. Our experiments on agents using safety-trained LLMs reveal that bias is systematically introduced and reinforced through personalization, emphasizing the need for additional protective measures or agent guardrails in memory-enhanced LLM-based AI agents.",
            "categories": [
                "cs.AI",
                "cs.IR"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "In Proceedings of the Nineteenth ACM International Conference on Web Search and Data Mining (WSDM '26)",
            "doi": "10.1145/3773966.3779376",
            "journal_ref": "In Proceedings of the Nineteenth ACM International Conference on Web Search and Data Mining (WSDM '26), 2026, Boise, ID, USA. ACM, New York, NY, USA",
            "pdf_url": "https://arxiv.org/pdf/2512.16532v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "揭示记忆增强型AI招聘Agent的偏见引入与强化机制",
            "summary_zh": "大型语言模型(LLMs)赋予了AI Agent强大的理解、推理和交互能力，可以处理各种任务。通过添加记忆功能，AI Agent能够跨交互保持连续性，从过去的经验中学习，并随着时间的推移提高行为和响应的相关性，这种方式被称为记忆增强型个性化。虽然这种通过记忆实现的个性化提供了明显的优势，但也带来了偏见风险。尽管之前的研究已经强调了ML和LLM中的偏见，但关于记忆增强型个性化Agent所带来的偏见在很大程度上尚未被探索。本文以招聘为例，模拟了记忆增强型个性化Agent的行为，并研究了偏见是如何在各个操作阶段被引入和加强的。对使用安全训练LLM的Agent进行的实验表明，偏见通过个性化被系统地引入和强化，强调了在基于记忆增强型LLM的AI Agent中采取额外保护措施或Agent防护措施的必要性。",
            "intro_zh": [
                "现有研究较少关注记忆增强型AI Agent中的偏见问题，尤其是在个性化过程中如何引入和强化偏见。",
                "该研究模拟了记忆增强型AI招聘Agent的行为，分析偏见在不同阶段的产生和演变过程。",
                "实验结果表明，即使使用安全训练的LLM，偏见仍然会通过个性化被系统性地引入和强化。"
            ],
            "method_zh": "**问题定义**：论文旨在研究在记忆增强型AI Agent中，尤其是在招聘场景下，偏见是如何被引入和强化的。现有方法主要关注ML和LLM本身的偏见，而忽略了记忆增强和个性化带来的新的偏见来源。这些偏见可能导致不公平的招聘结果，损害求职者的权益。\\n\\n**核心思路**：论文的核心思路是通过模拟记忆增强型AI Agent在招聘过程中的行为，观察和分析偏见的产生和演变过程。通过控制实验变量，例如Agent的初始知识、交互历史等，来识别偏见的关键来源和影响因素。\\n\\n**技术框架**：该研究的技术框架主要包括以下几个模块：1) 招聘场景模拟器：模拟真实的招聘流程，包括简历筛选、面试等环节。2) 记忆增强型AI Agent：基于LLM构建，具有记忆功能，能够记录和利用与求职者的交互历史。3) 偏见评估指标：用于量化Agent在招聘过程中产生的偏见程度，例如不同性别、种族求职者的录取率差异。4) 安全训练的LLM：使用经过安全训练的LLM作为Agent的基础模型，以降低初始偏见。\\n\\n**关键创新**：该研究的关键创新在于关注了记忆增强和个性化对AI Agent偏见的影响。与以往研究主要关注模型本身的偏见不同，该研究揭示了记忆功能如何放大和固化偏见，以及个性化策略如何加剧不公平现象。\\n\\n**关键设计**：研究中使用了安全训练的LLM，并通过控制Agent的记忆容量、交互策略等参数来模拟不同的个性化程度。同时，设计了多种偏见评估指标，例如统计不同群体求职者的录取率差异、分析Agent的决策依据等。此外，还探索了不同的Agent防护措施，例如偏见检测和纠正机制，以降低偏见的影响。",
            "application_zh": "该研究成果可应用于各种需要个性化AI Agent的场景，例如智能客服、教育辅导、金融风控等。通过识别和减轻记忆增强型AI Agent中的偏见，可以提高决策的公平性和透明度，避免歧视性结果，从而提升用户体验和社会福祉。未来的研究可以探索更有效的偏见检测和纠正方法，以及设计更公平的个性化策略。",
            "highlight_zh": "实验结果表明，即使使用安全训练的LLM，记忆增强型AI Agent仍然会通过个性化引入和强化偏见。具体来说，Agent在与特定群体的求职者交互后，可能会形成对该群体的刻板印象，从而影响后续的招聘决策。这种偏见会随着交互次数的增加而逐渐加剧，导致不公平的招聘结果。",
            "tags_zh": [
                "大型语言模型",
                "AI Agent",
                "记忆增强",
                "个性化",
                "偏见",
                "招聘",
                "公平性",
                "机器学习"
            ],
            "_index": 90,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16532v1/Figure_1_overview.png",
                    "caption": "",
                    "figure_id": "fig_0"
                }
            ]
        },
        {
            "title": "cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution",
            "authors": [
                "Jinwu Chen",
                "Qidie Wu",
                "Bin Li",
                "Lin Ma",
                "Xin Si",
                "Yang Hu",
                "Shouyi Yin",
                "Jun Yang"
            ],
            "arxiv_id": "2512.16465v1",
            "summary": "Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16465v1",
            "code_links": [
                {
                    "url": "https://github.com/champloo2878/cuPilot-Kernels.git",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "cuPilot：一种策略协调的多智能体框架，用于CUDA内核演化",
            "summary_zh": "优化CUDA内核是一项具有挑战性且劳动密集型的工作，需要软硬件协同设计专业知识和高性能内核库的专有性质。虽然最近的大型语言模型（LLM）与进化算法相结合在自动内核优化方面显示出希望，但由于其次优的智能体设计和不匹配的演化表示，现有方法通常在性能方面表现不佳。这项工作识别了这些不匹配之处，并提出了cuPilot，一个策略协调的多智能体框架，它引入策略作为内核演化的中间语义表示。主要贡献包括策略协调的进化算法、屋顶线引导的提示和策略级别的种群初始化。实验结果表明，cuPilot生成的内核在100个内核的基准测试中，相对于PyTorch实现了平均3.09倍的加速。在GEMM任务上，cuPilot展示了复杂的优化，并实现了关键硬件单元的高利用率。生成的内核已在https://github.com/champloo2878/cuPilot-Kernels.git上开源。",
            "intro_zh": [
                "现有CUDA内核自动优化方法在智能体设计和演化表示上存在不足，导致性能受限。",
                "cuPilot提出策略协调的多智能体框架，将策略作为内核演化的中间语义表示，解决上述问题。",
                "实验表明，cuPilot生成的内核在多个基准测试中显著优于PyTorch，并实现了硬件资源的高效利用。"
            ],
            "method_zh": "**问题定义**：CUDA内核优化需要专业的软硬件协同设计知识，且高性能内核库具有专有性，导致优化过程耗时耗力。现有基于LLM和进化算法的方法，由于智能体设计不佳和演化表示不匹配，难以达到理想的优化效果。\\n\\n**核心思路**：cuPilot的核心思路是将内核优化过程分解为一系列策略，并设计多智能体协同完成这些策略。通过引入“策略”这一中间语义表示，弥合了LLM生成代码与硬件性能之间的鸿沟，从而实现更有效的内核演化。\\n\\n**技术框架**：cuPilot框架包含以下主要模块：1) 策略协调的进化算法：负责整体的演化流程，协调多个智能体的行为。2) 屋顶线引导的提示：利用屋顶线模型指导LLM生成更优的内核代码。3) 策略级别的种群初始化：通过策略指导初始化种群，加速演化过程。整体流程是从初始种群开始，通过策略指导的变异和交叉，生成新的内核代码，并根据性能反馈不断优化。\\n\\n**关键创新**：cuPilot的关键创新在于引入了“策略”作为内核演化的中间语义表示。与直接生成内核代码相比，策略更易于理解和控制，也更符合人类专家的优化思路。此外，策略协调的多智能体框架能够更好地探索搜索空间，找到更优的内核配置。\\n\\n**关键设计**：策略协调的进化算法是cuPilot的核心。具体而言，每个智能体负责一种特定的优化策略，例如循环展开、数据重排等。智能体之间通过共享信息和协同工作，共同完成内核优化任务。屋顶线引导的提示利用硬件性能模型，指导LLM生成更符合硬件特性的代码。策略级别的种群初始化则利用专家知识，加速演化过程。",
            "application_zh": "cuPilot可应用于各种需要高性能计算的领域，如深度学习、科学计算、图像处理等。它可以帮助开发者自动优化CUDA内核，提高程序运行效率，降低开发成本。该研究的成果有助于推动高性能计算的普及和发展，并为未来的自动内核优化研究提供新的思路。",
            "highlight_zh": "实验结果表明，cuPilot在100个内核的基准测试中，相对于PyTorch实现了平均3.09倍的加速。在GEMM任务上，cuPilot展示了复杂的优化，并实现了关键硬件单元的高利用率。这些结果表明，cuPilot能够有效地优化CUDA内核，并显著提高程序性能。",
            "tags_zh": [
                "CUDA内核优化",
                "多智能体系统",
                "进化算法",
                "大型语言模型",
                "高性能计算"
            ],
            "_index": 91,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16465v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16465v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16465v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Towards AI-Supported Research: a Vision of the TIB AIssistant",
            "authors": [
                "Sören Auer",
                "Allard Oelen",
                "Mohamad Yaser Jaradeh",
                "Mutahira Khalid",
                "Farhana Keya",
                "Sasi Kiran Gaddipati",
                "Jennifer D'Souza",
                "Lorenz Schlüter",
                "Amirreza Alasti",
                "Gollam Rabby",
                "Azanzi Jiomekong",
                "Oliver Karras"
            ],
            "arxiv_id": "2512.16447v1",
            "summary": "The rapid advancements in Generative AI and Large Language Models promise to transform the way research is conducted, potentially offering unprecedented opportunities to augment scholarly workflows. However, effectively integrating AI into research remains a challenge due to varying domain requirements, limited AI literacy, the complexity of coordinating tools and agents, and the unclear accuracy of Generative AI in research. We present the vision of the TIB AIssistant, a domain-agnostic human-machine collaborative platform designed to support researchers across disciplines in scientific discovery, with AI assistants supporting tasks across the research life cycle. The platform offers modular components - including prompt and tool libraries, a shared data store, and a flexible orchestration framework - that collectively facilitate ideation, literature analysis, methodology development, data analysis, and scholarly writing. We describe the conceptual framework, system architecture, and implementation of an early prototype that demonstrates the feasibility and potential impact of our approach.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16447v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出TIB AIssistant平台，旨在利用AI增强科研工作流程，促进跨学科的科学发现。",
            "summary_zh": "生成式AI和大型语言模型的快速发展有望改变研究的进行方式，为增强学术工作流程提供前所未有的机会。然而，由于领域需求各异、AI素养有限、工具和代理协调复杂以及生成式AI在研究中的准确性不明确，将AI有效集成到研究中仍然是一个挑战。我们提出了TIB AIssistant的愿景，这是一个领域无关的人机协作平台，旨在支持跨学科研究人员进行科学发现，AI助手支持研究生命周期中的各项任务。该平台提供模块化组件，包括提示和工具库、共享数据存储以及灵活的编排框架，共同促进构思、文献分析、方法论开发、数据分析和学术写作。我们描述了概念框架、系统架构以及早期原型的实现，展示了我们方法的可行性和潜在影响。",
            "intro_zh": [
                "当前科研工作流程在AI集成方面面临领域差异大、AI素养不足、工具协调复杂以及AI准确性不确定等挑战。",
                "TIB AIssistant的核心思想是构建一个领域无关的人机协作平台，利用AI助手支持科研生命周期的各个阶段。",
                "早期原型验证了该方法的可行性和潜在影响，平台提供模块化组件，促进科研任务的各个环节。"
            ],
            "method_zh": "**问题定义**：论文旨在解决科研人员在研究过程中面临的AI集成难题，现有方法难以有效应对不同领域的需求，科研人员AI素养参差不齐，工具和代理的协调复杂，以及生成式AI在研究中的准确性存在不确定性。这些痛点阻碍了AI在科研领域的广泛应用。\\n\\n**核心思路**：论文的核心思路是构建一个领域无关的人机协作平台TIB AIssistant，通过模块化的AI助手支持研究生命周期的各个阶段，包括构思、文献分析、方法论开发、数据分析和学术写作。该平台旨在降低AI的使用门槛，提高科研效率和质量。\\n\\n**技术框架**：TIB AIssistant的整体架构包含以下主要模块：1) 提示和工具库：提供丰富的预定义提示和工具，方便用户快速启动任务；2) 共享数据存储：集中管理研究数据，方便AI助手访问和利用；3) 灵活的编排框架：支持用户自定义工作流程，灵活组合不同的AI助手和工具。用户可以通过平台界面与AI助手进行交互，完成各种科研任务。\\n\\n**关键创新**：该平台的关键创新在于其领域无关性和模块化设计。领域无关性使得平台可以应用于不同的学科领域，降低了AI在科研领域的应用门槛。模块化设计使得用户可以根据自己的需求灵活组合不同的AI助手和工具，定制个性化的工作流程。\\n\\n**关键设计**：论文中没有详细描述关键的参数设置、损失函数、网络结构等技术细节。平台的设计重点在于架构的灵活性和易用性，以及对不同AI工具的集成能力。具体的AI助手可能采用不同的模型和算法，需要根据具体的任务进行选择和配置。这些细节在论文中没有明确说明。",
            "application_zh": "TIB AIssistant平台具有广泛的应用前景，可以应用于各个学科领域，例如自然科学、社会科学、人文科学等。它可以帮助研究人员更高效地进行文献分析、数据分析、方法论开发和学术写作，从而加速科学发现的进程。该平台还可以促进跨学科合作，为不同领域的专家提供一个共享的平台，共同解决复杂的科学问题。",
            "highlight_zh": "论文展示了一个早期原型，验证了TIB AIssistant平台的可行性和潜在影响。虽然论文没有提供具体的性能数据和对比基线，但原型展示了平台在支持科研任务方面的能力，例如文献检索、数据分析和文本生成。该原型为未来的研究和开发奠定了基础。",
            "tags_zh": [
                "AI辅助研究",
                "人机协作",
                "科研平台",
                "生成式AI",
                "大型语言模型"
            ],
            "_index": 92,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16447v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16447v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "TIB AIssistant: a Platform for AI-Supported Research Across Research Life Cycles",
            "authors": [
                "Allard Oelen",
                "Sören Auer"
            ],
            "arxiv_id": "2512.16442v1",
            "summary": "The rapidly growing popularity of adopting Artificial Intelligence (AI), and specifically Large Language Models (LLMs), is having a widespread impact throughout society, including the academic domain. AI-supported research has the potential to support researchers with tasks across the entire research life cycle. In this work, we demonstrate the TIB AIssistant, an AI-supported research platform providing support throughout the research life cycle. The AIssistant consists of a collection of assistants, each responsible for a specific research task. In addition, tools are provided to give access to external scholarly services. Generated data is stored in the assets and can be exported as an RO-Crate bundle to provide transparency and enhance reproducibility of the research project. We demonstrate the AIssistant's main functionalities by means of a sequential walk-through of assistants, interacting with each other to generate sections for a draft research paper. In the end, with the AIssistant, we lay the foundation for a larger agenda of providing a community-maintained platform for AI-supported research.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16442v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "TIB AIssistant：一个支持研究全生命周期的人工智能研究平台",
            "summary_zh": "人工智能（AI），特别是大型语言模型（LLMs）的迅速普及，正在对包括学术领域在内的整个社会产生广泛影响。AI支持的研究有潜力在整个研究生命周期中为研究人员提供帮助。本文展示了TIB AIssistant，这是一个AI支持的研究平台，为整个研究生命周期提供支持。AIssistant由一系列助手组成，每个助手负责特定的研究任务。此外，还提供了工具来访问外部学术服务。生成的数据存储在资产中，并且可以导出为RO-Crate包，以提供透明度并增强研究项目的可重复性。我们通过一个助手的顺序演练来演示AIssistant的主要功能，这些助手相互交互以生成研究论文草案的各个部分。最后，通过AIssistant，我们为提供一个社区维护的AI支持研究平台奠定了基础。",
            "intro_zh": [
                "研究人员在研究生命周期的各个阶段面临诸多挑战，需要高效的工具来辅助完成任务。",
                "TIB AIssistant 平台通过集成多个AI助手，分别负责不同的研究任务，简化研究流程。",
                "该平台支持数据存储和导出为RO-Crate格式，增强了研究的透明度和可重复性。"
            ],
            "method_zh": "**问题定义**：当前研究人员在研究的各个阶段，例如文献综述、实验设计、论文撰写等，都需要花费大量时间和精力。现有的工具往往是孤立的，缺乏整合，难以满足研究人员对高效、协同研究的需求。\\n\\n**核心思路**：TIB AIssistant 的核心思路是构建一个集成化的AI辅助研究平台，通过模块化的AI助手来支持研究的各个环节。每个助手专注于特定的任务，并通过共享数据和协同工作，形成一个完整的AI辅助研究流程。\\n\\n**技术框架**：TIB AIssistant 平台包含以下主要模块：1) AI助手模块：包含多个AI助手，每个助手负责特定的研究任务，例如文献检索、数据分析、论文写作等。2) 外部服务接口：提供访问外部学术服务的接口，例如数据库、知识库等。3) 数据存储模块：用于存储生成的数据，并支持导出为RO-Crate格式。4) 用户界面：提供用户友好的交互界面，方便用户使用和管理AI助手。\\n\\n**关键创新**：TIB AIssistant 的关键创新在于其集成化的设计理念和模块化的AI助手。与现有的孤立的AI工具相比，TIB AIssistant 能够提供更全面、更协同的AI辅助研究服务。通过RO-Crate格式的数据导出，增强了研究的透明度和可重复性，促进了开放科学的发展。\\n\\n**关键设计**：AI助手的具体实现细节未知，但可以推测其可能使用了大型语言模型（LLMs）作为底层技术。RO-Crate是一种用于描述和打包研究数据的标准，TIB AIssistant 使用RO-Crate来保证数据的可访问性和可重用性。平台的用户界面设计注重易用性和可定制性，方便用户根据自己的需求配置和使用AI助手。",
            "application_zh": "TIB AIssistant 平台可应用于各个学科领域的研究，例如自然科学、社会科学、人文科学等。它可以帮助研究人员提高研究效率，降低研究成本，并促进跨学科的合作研究。该平台还有助于推动开放科学的发展，提高研究的透明度和可重复性，从而加速知识的发现和传播。",
            "highlight_zh": "论文通过一个完整的案例演示了 TIB AIssistant 的主要功能，展示了如何使用多个 AI 助手协同工作，生成研究论文草稿的各个部分。虽然论文没有提供具体的性能数据，但通过案例可以看出，该平台能够有效地辅助研究人员完成研究任务，提高研究效率。RO-Crate格式的导出功能也增强了研究结果的可复现性。",
            "tags_zh": [
                "人工智能辅助研究",
                "大型语言模型",
                "研究平台",
                "RO-Crate",
                "开放科学"
            ],
            "_index": 93,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16442v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                }
            ]
        },
        {
            "title": "Introducing ORKG ASK: an AI-driven Scholarly Literature Search and Exploration System Taking a Neuro-Symbolic Approach",
            "authors": [
                "Allard Oelen",
                "Mohamad Yaser Jaradeh",
                "Sören Auer"
            ],
            "arxiv_id": "2512.16425v1",
            "summary": "As the volume of published scholarly literature continues to grow, finding relevant literature becomes increasingly difficult. With the rise of generative Artificial Intelligence (AI), and particularly Large Language Models (LLMs), new possibilities emerge to find and explore literature. We introduce ASK (Assistant for Scientific Knowledge), an AI-driven scholarly literature search and exploration system that follows a neuro-symbolic approach. ASK aims to provide active support to researchers in finding relevant scholarly literature by leveraging vector search, LLMs, and knowledge graphs. The system allows users to input research questions in natural language and retrieve relevant articles. ASK automatically extracts key information and generates answers to research questions using a Retrieval-Augmented Generation (RAG) approach. We present an evaluation of ASK, assessing the system's usability and usefulness. Findings indicate that the system is user-friendly and users are generally satisfied while using the system.",
            "categories": [
                "cs.IR",
                "cs.AI"
            ],
            "primary_category": "cs.IR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "10.1007/978-3-031-97207-2_2",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16425v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出ORKG ASK：一种基于神经符号方法的AI驱动的学术文献搜索与探索系统",
            "summary_zh": "随着发表的学术文献数量持续增长，找到相关的文献变得越来越困难。生成式人工智能（AI）的兴起，特别是大型语言模型（LLM），为发现和探索文献带来了新的可能性。我们介绍ASK（科学知识助手），这是一个AI驱动的学术文献搜索和探索系统，它遵循神经符号方法。ASK旨在通过利用向量搜索、LLM和知识图谱，为研究人员寻找相关学术文献提供积极支持。该系统允许用户以自然语言输入研究问题并检索相关文章。ASK自动提取关键信息，并使用检索增强生成（RAG）方法生成研究问题的答案。我们对ASK进行了评估，评估了系统的可用性和实用性。调查结果表明，该系统用户友好，用户在使用该系统时普遍感到满意。",
            "intro_zh": [
                "现有学术文献数量庞大，研究人员难以快速找到所需信息，传统搜索方法效率较低。",
                "ASK系统采用神经符号方法，结合向量搜索、LLM和知识图谱，提升文献检索和探索的效率。",
                "评估结果表明，ASK系统具有良好的用户友好性和实用性，用户对系统整体满意。"
            ],
            "method_zh": "**问题定义**：当前学术文献数量爆炸式增长，研究人员面临信息过载的挑战，传统的关键词搜索方法难以满足精确查找和知识发现的需求。现有方法缺乏对文献深层语义的理解，无法有效回答复杂的研究问题。\\n\\n**核心思路**：ASK的核心思路是结合神经方法（LLM）和符号方法（知识图谱），利用LLM理解用户自然语言查询，并利用知识图谱进行结构化知识推理和信息检索。通过检索增强生成（RAG）方法，将检索到的相关信息输入LLM，生成更准确和全面的答案。\\n\\n**技术框架**：ASK系统主要包含以下几个模块：1) 自然语言查询理解模块，使用LLM将用户查询转换为向量表示；2) 向量搜索模块，利用向量数据库检索与查询相关的文献；3) 知识图谱模块，存储和管理学术知识，用于知识推理和信息补充；4) 检索增强生成模块，将检索到的文献和知识图谱信息输入LLM，生成答案。\\n\\n**关键创新**：ASK的关键创新在于其神经符号融合的方法，它结合了LLM的自然语言理解能力和知识图谱的结构化知识表示能力。与传统的基于关键词的搜索方法相比，ASK能够更好地理解用户意图，并提供更相关和全面的答案。此外，RAG方法的应用使得ASK能够利用外部知识来增强LLM的生成能力。\\n\\n**关键设计**：ASK系统使用了预训练的LLM，例如BERT或RoBERTa，进行微调以适应学术文献检索的任务。向量数据库采用FAISS或Annoy等高效的近似最近邻搜索算法。知识图谱的构建和维护需要持续的知识抽取和融合过程。RAG模块的关键在于如何有效地将检索到的信息融入到LLM的输入中，例如使用prompt engineering技术。",
            "application_zh": "ASK系统可应用于学术研究、科技情报分析、知识管理等领域。研究人员可以利用该系统快速找到相关文献，了解研究进展，并发现新的研究方向。该系统还可以帮助企业进行技术趋势分析和竞争情报收集，为决策提供支持。未来，ASK有望成为科研人员不可或缺的助手，加速科学发现的进程。",
            "highlight_zh": "论文对ASK系统进行了用户评估，结果表明用户对系统的可用性和实用性普遍满意。用户认为该系统易于使用，能够有效地帮助他们找到相关文献并回答研究问题。具体的性能数据和对比基线在摘要中未提及，属于未知信息。",
            "tags_zh": [
                "学术文献搜索",
                "知识图谱",
                "大型语言模型",
                "神经符号方法",
                "检索增强生成"
            ],
            "_index": 94,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16425v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16425v1/figures/screenshot-ask.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16425v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Synthelite: Chemist-aligned and feasibility-aware synthesis planning with LLMs",
            "authors": [
                "Nguyen Xuan-Vu",
                "Daniel Armstrong",
                "Milena Wehrbach",
                "Andres M Bran",
                "Zlatko Jončev",
                "Philippe Schwaller"
            ],
            "arxiv_id": "2512.16424v1",
            "summary": "Computer-aided synthesis planning (CASP) has long been envisioned as a complementary tool for synthetic chemists. However, existing frameworks often lack mechanisms to allow interaction with human experts, limiting their ability to integrate chemists' insights. In this work, we introduce Synthelite, a synthesis planning framework that uses large language models (LLMs) to directly propose retrosynthetic transformations. Synthelite can generate end-to-end synthesis routes by harnessing the intrinsic chemical knowledge and reasoning capabilities of LLMs, while allowing expert intervention through natural language prompts. Our experiments demonstrate that Synthelite can flexibly adapt its planning trajectory to diverse user-specified constraints, achieving up to 95\\% success rates in both strategy-constrained and starting-material-constrained synthesis tasks. Additionally, Synthelite exhibits the ability to account for chemical feasibility during route design. We envision Synthelite to be both a useful tool and a step toward a paradigm where LLMs are the central orchestrators of synthesis planning.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16424v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "Synthelite：利用LLM实现化学家导向且可行性感知的合成路线规划",
            "summary_zh": "计算机辅助合成规划（CASP）一直被认为是合成化学家的辅助工具。然而，现有的框架通常缺乏与人类专家交互的机制，限制了其整合化学家见解的能力。本文介绍Synthelite，一个使用大型语言模型（LLM）直接提出逆合成转化的合成规划框架。Synthelite通过利用LLM固有的化学知识和推理能力来生成端到端的合成路线，同时允许专家通过自然语言提示进行干预。实验表明，Synthelite可以灵活地调整其规划轨迹以适应各种用户指定的约束，在策略约束和起始材料约束的合成任务中均达到高达95%的成功率。此外，Synthelite还表现出在路线设计期间考虑化学可行性的能力。我们设想Synthelite既是一个有用的工具，也是朝着LLM成为合成规划中心协调者的范例迈出的一步。",
            "intro_zh": [
                "现有CASP系统缺乏与化学专家的有效交互，难以整合专家知识和经验。",
                "Synthelite利用LLM的化学知识和推理能力，通过自然语言提示实现人机协同的合成路线规划。",
                "实验表明，Synthelite在多种约束条件下均表现出高成功率，并能考虑化学反应的可行性。"
            ],
            "method_zh": "**问题定义**：现有的计算机辅助合成规划（CASP）系统，虽然在一定程度上辅助了化学家的工作，但缺乏与人类专家的有效交互机制。这导致系统难以充分利用化学家的专业知识和经验，限制了其在复杂合成路线规划中的应用。现有方法的痛点在于无法灵活地接受和响应化学家的反馈，难以进行人机协同的合成路线设计。\\n\\n**核心思路**：Synthelite的核心思路是利用大型语言模型（LLM）强大的自然语言处理和知识推理能力，直接生成逆合成转化。通过将LLM作为合成规划的核心引擎，并允许化学家通过自然语言提示进行干预，Synthelite旨在实现人机协同的合成路线设计。这种方法的核心在于利用LLM的化学知识和推理能力，同时结合化学家的专业判断，从而提高合成路线规划的效率和成功率。\\n\\n**技术框架**：Synthelite的整体框架包含以下几个主要模块：1) LLM核心引擎：负责根据输入的起始分子和约束条件，生成可能的逆合成转化。2) 自然语言交互模块：允许化学家通过自然语言提示对LLM的生成结果进行指导和修正。3) 合成路线评估模块：评估生成的合成路线的可行性和效率。4) 迭代优化模块：根据化学家的反馈和评估结果，迭代优化合成路线。整个流程是一个人机协同的循环，化学家通过自然语言提示引导LLM的生成，LLM则根据化学家的反馈不断优化合成路线。\\n\\n**关键创新**：Synthelite最重要的技术创新点在于将LLM作为合成规划的核心引擎，并实现了人机协同的合成路线设计。与传统的基于规则或模板的CASP系统不同，Synthelite能够利用LLM的知识推理能力，生成更具创造性和灵活性的合成路线。此外，Synthelite的自然语言交互模块使得化学家能够方便地对LLM的生成结果进行指导和修正，从而实现人机协同的合成路线设计。\\n\\n**关键设计**：Synthelite的关键设计包括：1) LLM的选择和训练：选择具有较强化学知识和推理能力的LLM，并使用大量的化学文献和反应数据进行训练。2) 自然语言提示的设计：设计清晰简洁的自然语言提示，以便化学家能够有效地指导LLM的生成。3) 合成路线评估指标的选择：选择合适的评估指标，以评估合成路线的可行性和效率，例如反应产率、反应条件等。4) 迭代优化策略的设计：设计有效的迭代优化策略，以便根据化学家的反馈和评估结果，不断优化合成路线。",
            "application_zh": "Synthelite可应用于药物发现、材料科学等领域，加速新分子和新材料的合成路线设计。通过人机协同，Synthelite能够提高合成效率，降低研发成本，并为化学家提供新的思路和灵感。未来，Synthelite有望成为化学研究的重要工具，推动化学领域的创新发展。",
            "highlight_zh": "Synthelite在策略约束和起始材料约束的合成任务中均达到高达95%的成功率，证明了其在复杂约束条件下进行合成路线规划的能力。此外，Synthelite还能够考虑化学反应的可行性，生成更具实用价值的合成路线。这些实验结果表明，Synthelite在合成路线规划方面具有显著优势。",
            "tags_zh": [
                "计算机辅助合成规划",
                "大型语言模型",
                "逆合成分析",
                "人机协同",
                "化学信息学"
            ],
            "_index": 95,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16424v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16424v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16424v1/figs/sm_constrained_solve.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "AI Needs Physics More Than Physics Needs AI",
            "authors": [
                "Peter Coveney",
                "Roger Highfield"
            ],
            "arxiv_id": "2512.16344v1",
            "summary": "Artificial intelligence (AI) is commonly depicted as transformative. Yet, after more than a decade of hype, its measurable impact remains modest outside a few high-profile scientific and commercial successes. The 2024 Nobel Prizes in Chemistry and Physics recognized AI's potential, but broader assessments indicate the impact to date is often more promotional than technical. We argue that while current AI may influence physics, physics has significantly more to offer this generation of AI. Current architectures - large language models, reasoning models, and agentic AI - can depend on trillions of meaningless parameters, suffer from distributional bias, lack uncertainty quantification, provide no mechanistic insights, and fail to capture even elementary scientific laws. We review critiques of these limits, highlight opportunities in quantum AI and analogue computing, and lay down a roadmap for the adoption of 'Big AI': a synthesis of theory-based rigour with the flexibility of machine learning.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16344v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "强调物理学对人工智能的重要性，提出融合理论与机器学习的“大AI”愿景",
            "summary_zh": "人工智能（AI）常被描绘成具有变革性的技术。然而，经过十多年的炒作，除了少数引人注目的科学和商业成功案例外，其可衡量的影响仍然有限。2024年诺贝尔化学奖和物理学奖认可了AI的潜力，但更广泛的评估表明，迄今为止，AI的影响更多的是宣传而非技术。我们认为，虽然当前的AI可能会影响物理学，但物理学能为这一代AI提供更多。当前架构——大型语言模型、推理模型和代理AI——可能依赖于数万亿个无意义的参数，遭受分布偏差，缺乏不确定性量化，无法提供机制性见解，甚至无法捕捉基本的科学定律。我们回顾了对这些局限性的批评，强调了量子AI和模拟计算中的机遇，并为采用“大AI”制定了路线图：即将基于理论的严谨性与机器学习的灵活性相结合。",
            "intro_zh": [
                "现有AI架构依赖大量无意义参数，缺乏对基本科学定律的理解，限制了其在科学领域的应用。",
                "论文提出“大AI”概念，旨在融合基于理论的严谨性和机器学习的灵活性，弥补现有AI的不足。",
                "论文探讨了量子AI和模拟计算的机遇，并为“大AI”的采纳制定了路线图，指明了未来发展方向。"
            ],
            "method_zh": "**问题定义**：当前人工智能，特别是大型语言模型等，虽然在某些领域取得了显著进展，但在科学领域应用时面临诸多挑战。这些模型依赖于海量数据和参数，缺乏对物理规律的理解，容易出现分布偏差，无法进行不确定性量化，并且难以提供机制性的解释。现有方法的痛点在于其“黑盒”特性和对数据的高度依赖，使其难以应用于需要精确预测和解释的科学问题。\\n\\n**核心思路**：论文的核心思路是强调物理学等理论学科对人工智能发展的重要性。作者认为，人工智能不应仅仅依赖于数据驱动的模式识别，而应该将理论知识融入到模型设计中，从而提高模型的泛化能力、可解释性和可靠性。通过将物理学原理与机器学习方法相结合，可以构建更加强大和可靠的“大AI”系统。\\n\\n**技术框架**：论文并没有提出一个具体的、可直接实现的技术框架，而是提出了一个高层次的愿景和路线图。这个路线图包括以下几个关键要素：1) 强调理论的重要性，鼓励将物理学等理论知识融入到AI模型中；2) 探索量子AI和模拟计算等新兴技术，以提高AI的计算能力和效率；3) 关注不确定性量化和可解释性，提高AI模型的可靠性和可信度。\\n\\n**关键创新**：论文的关键创新在于其对人工智能发展方向的重新思考。作者认为，当前人工智能的发展过于依赖数据和算力，而忽略了理论的重要性。通过强调物理学等理论学科对人工智能的指导作用，可以构建更加强大、可靠和可解释的AI系统。这种观点挑战了当前人工智能发展的主流趋势，为未来的研究提供了新的方向。\\n\\n**关键设计**：由于论文主要关注的是对AI发展方向的宏观思考，因此并没有涉及具体的参数设置、损失函数或网络结构等技术细节。未来的研究可以围绕如何将具体的物理学原理融入到AI模型中展开，例如，可以设计基于物理定律的损失函数，或者构建能够模拟物理过程的神经网络结构。",
            "application_zh": "该研究的潜在应用领域包括材料科学、药物发现、气候建模等。通过将物理学原理与机器学习相结合，可以加速科学研究的进程，提高预测的准确性和可靠性，并为解决复杂的科学问题提供新的思路。未来，这种融合理论与机器学习的“大AI”有望在各个科学领域发挥重要作用。",
            "highlight_zh": "论文的核心亮点在于对当前AI发展局限性的深刻反思，以及对未来AI发展方向的前瞻性思考。作者指出，当前AI在科学领域的应用受到诸多限制，并强调了物理学等理论学科对AI发展的重要性。虽然论文没有提供具体的实验结果，但其提出的“大AI”愿景为未来的研究提供了新的方向和思路。",
            "tags_zh": [
                "人工智能",
                "物理学",
                "机器学习",
                "理论建模",
                "量子AI"
            ],
            "_index": 96,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Agent Tools Orchestration Leaks More: Dataset, Benchmark, and Mitigation",
            "authors": [
                "Yuxuan Qiao",
                "Dongqin Liu",
                "Hongchang Yang",
                "Wei Zhou",
                "Songlin Hu"
            ],
            "arxiv_id": "2512.16310v1",
            "summary": "Driven by Large Language Models, the single-agent, multi-tool architecture has become a popular paradigm for autonomous agents due to its simplicity and effectiveness. However, this architecture also introduces a new and severe privacy risk, which we term Tools Orchestration Privacy Risk (TOP-R), where an agent, to achieve a benign user goal, autonomously aggregates information fragments across multiple tools and leverages its reasoning capabilities to synthesize unexpected sensitive information. We provide the first systematic study of this risk. First, we establish a formal framework, attributing the risk's root cause to the agent's misaligned objective function: an overoptimization for helpfulness while neglecting privacy awareness. Second, we construct TOP-Bench, comprising paired leakage and benign scenarios, to comprehensively evaluate this risk. To quantify the trade-off between safety and robustness, we introduce the H-Score as a holistic metric. The evaluation results reveal that TOP-R is a severe risk: the average Risk Leakage Rate (RLR) of eight representative models reaches 90.24%, while the average H-Score is merely 0.167, with no model exceeding 0.3. Finally, we propose the Privacy Enhancement Principle (PEP) method, which effectively mitigates TOP-R, reducing the Risk Leakage Rate to 46.58% and significantly improving the H-Score to 0.624. Our work reveals both a new class of risk and inherent structural limitations in current agent architectures, while also offering feasible mitigation strategies.",
            "categories": [
                "cs.CR",
                "cs.AI",
                "cs.CL"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16310v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "揭示Agent工具编排中的隐私泄露风险，并提出TOP-Bench基准与PEP缓解方法",
            "summary_zh": "本文系统性地研究了由大型语言模型驱动的单Agent多工具架构中存在的工具编排隐私风险(TOP-R)。这种架构为了实现用户的良性目标，可能自主地聚合多个工具中的信息片段，并利用其推理能力合成意想不到的敏感信息。研究首先建立了一个正式框架，将风险的根本原因归结为Agent的目标函数错位：过度优化了有用性而忽略了隐私意识。其次，构建了TOP-Bench，包含配对的泄露和良性场景，以全面评估这种风险。为了量化安全性和鲁棒性之间的权衡，引入了H-Score作为整体指标。评估结果表明TOP-R是一种严重的风险：八个代表性模型的平均风险泄露率(RLR)达到90.24%，而平均H-Score仅为0.167，没有模型超过0.3。最后，提出了隐私增强原则(PEP)方法，有效地缓解了TOP-R，将风险泄露率降低到46.58%，并将H-Score显著提高到0.624。这项工作揭示了一种新型风险以及当前Agent架构中固有的结构性限制，同时也提供了可行的缓解策略。",
            "intro_zh": [
                "现有Agent架构在追求有用性时忽略了隐私保护，导致Agent可能无意中泄露敏感信息，造成工具编排隐私风险(TOP-R)。",
                "论文提出隐私增强原则(PEP)方法，旨在通过调整Agent的目标函数，使其在实现用户目标的同时，更加关注隐私保护。",
                "实验结果表明，PEP方法能有效降低风险泄露率(RLR)并显著提高H-Score，在安全性和鲁棒性之间取得更好的平衡。"
            ],
            "method_zh": "**问题定义**：论文旨在解决单Agent多工具架构中，Agent为了完成用户任务，可能通过编排多个工具，无意中泄露用户隐私信息的问题。现有方法往往只关注Agent的性能和效率，忽略了其潜在的隐私风险，导致Agent在追求有用性的同时，可能过度收集和利用信息，从而泄露敏感数据。\\n\\n**核心思路**：论文的核心思路是调整Agent的目标函数，使其在追求有用性的同时，更加关注隐私保护。具体来说，就是通过引入隐私增强原则(PEP)，引导Agent在选择工具和生成回复时，优先考虑隐私保护，避免泄露敏感信息。这种方法旨在在Agent的性能和隐私之间取得平衡。\\n\\n**技术框架**：论文的技术框架主要包括三个部分：首先，建立了一个正式的风险模型，用于描述和分析工具编排隐私风险(TOP-R)。其次，构建了一个包含配对的泄露和良性场景的基准测试集TOP-Bench，用于评估Agent的隐私泄露风险。最后，提出了隐私增强原则(PEP)方法，用于缓解TOP-R。PEP方法通过修改Agent的目标函数，使其在选择工具和生成回复时，更加关注隐私保护。\\n\\n**关键创新**：论文最重要的技术创新点在于提出了隐私增强原则(PEP)方法，这是一种针对Agent工具编排隐私风险的有效缓解策略。与现有方法不同，PEP方法不是简单地限制Agent对工具的使用，而是通过调整Agent的目标函数，使其在追求有用性的同时，更加关注隐私保护。这种方法可以在不显著降低Agent性能的前提下，有效地降低隐私泄露风险。\\n\\n**关键设计**：PEP方法的关键设计在于如何修改Agent的目标函数，使其既能实现用户目标，又能保护用户隐私。具体来说，PEP方法通过引入一个隐私损失项，惩罚Agent的隐私泄露行为。这个隐私损失项可以基于不同的隐私度量标准来定义，例如差分隐私。此外，PEP方法还引入了一个隐私预算参数，用于控制Agent的隐私保护程度。通过调整隐私预算参数，可以在Agent的性能和隐私之间进行权衡。",
            "application_zh": "该研究成果可应用于各种需要使用Agent进行自动化任务处理的场景，例如智能客服、自动化报告生成、智能家居控制等。通过应用PEP方法，可以有效降低Agent在执行任务过程中泄露用户隐私的风险，提高用户对Agent系统的信任度，促进Agent技术的广泛应用。未来的研究可以进一步探索更有效的隐私保护方法，并将其应用于更复杂的Agent系统中。",
            "highlight_zh": "实验结果表明，现有的Agent模型存在严重的工具编排隐私风险(TOP-R)，平均风险泄露率(RLR)高达90.24%，平均H-Score仅为0.167。而应用PEP方法后，风险泄露率显著降低至46.58%，H-Score显著提升至0.624。这表明PEP方法能够有效缓解TOP-R，并在安全性和鲁棒性之间取得更好的平衡。实验结果还表明，不同的Agent模型对TOP-R的敏感程度不同，需要根据具体情况选择合适的隐私保护策略。",
            "tags_zh": [
                "Agent工具编排",
                "隐私泄露风险",
                "大型语言模型",
                "隐私增强原则",
                "基准测试",
                "目标函数",
                "隐私保护"
            ],
            "_index": 97,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16310v1/Problem_Introduction.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16310v1/Dataset_Construction.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16310v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Beyond the Benchmark: Innovative Defenses Against Prompt Injection Attacks",
            "authors": [
                "Safwan Shaheer",
                "G. M. Refatul Islam",
                "Mohammad Rafid Hamid",
                "Tahsin Zaman Jilan"
            ],
            "arxiv_id": "2512.16307v1",
            "summary": "In this fast-evolving area of LLMs, our paper discusses the significant security risk presented by prompt injection attacks. It focuses on small open-sourced models, specifically the LLaMA family of models. We introduce novel defense mechanisms capable of generating automatic defenses and systematically evaluate said generated defenses against a comprehensive set of benchmarked attacks. Thus, we empirically demonstrated the improvement proposed by our approach in mitigating goal-hijacking vulnerabilities in LLMs. Our work recognizes the increasing relevance of small open-sourced LLMs and their potential for broad deployments on edge devices, aligning with future trends in LLM applications. We contribute to the greater ecosystem of open-source LLMs and their security in the following: (1) assessing present prompt-based defenses against the latest attacks, (2) introducing a new framework using a seed defense (Chain Of Thoughts) to refine the defense prompts iteratively, and (3) showing significant improvements in detecting goal hijacking attacks. Out strategies significantly reduce the success rates of the attacks and false detection rates while at the same time effectively detecting goal-hijacking capabilities, paving the way for more secure and efficient deployments of small and open-source LLMs in resource-constrained environments.",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "10 pages, 4 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16307v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "chain-of-thought"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "针对LLaMA模型，提出迭代式prompt优化防御prompt注入攻击",
            "summary_zh": "本文探讨了大型语言模型（LLMs）领域中由prompt注入攻击带来的重大安全风险，特别关注小型开源模型，尤其是LLaMA系列模型。我们提出了一种新颖的防御机制，能够自动生成防御，并针对一系列基准攻击系统地评估这些生成的防御。实验结果表明，该方法在减轻LLMs中的目标劫持漏洞方面有所改进。我们的工作认识到小型开源LLMs日益增长的相关性及其在边缘设备上广泛部署的潜力，这与LLM应用的未来趋势相符。我们通过以下方式为开源LLMs及其安全性的生态系统做出贡献：（1）评估当前基于prompt的防御措施对最新攻击的有效性；（2）引入一种使用种子防御（思维链）迭代改进防御prompt的新框架；（3）在检测目标劫持攻击方面显示出显著的改进。我们的策略显著降低了攻击的成功率和误检率，同时有效地检测目标劫持能力，为在资源受限环境中更安全、更高效地部署小型开源LLMs铺平了道路。",
            "intro_zh": [
                "现有prompt防御方法在面对新型prompt注入攻击时存在不足，尤其是在小型开源LLM上。",
                "提出一种迭代式prompt优化框架，利用思维链作为种子防御，逐步改进防御prompt的有效性。",
                "实验表明，该方法能显著降低攻击成功率和误检率，提升小型开源LLM的安全性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决小型开源LLM（如LLaMA系列）中，prompt注入攻击导致的目标劫持问题。现有的prompt防御方法难以有效应对不断演化的攻击手段，尤其是在资源受限的边缘设备上部署时，防御效果和效率都面临挑战。\\n\\n**核心思路**：论文的核心思路是利用迭代优化的方式，自动生成并改进防御prompt。通过将思维链（Chain of Thoughts）作为初始的种子防御，模型可以逐步学习并完善防御策略，从而更有效地抵御prompt注入攻击。这种方法旨在提高防御的鲁棒性和适应性，使其能够应对各种类型的攻击。\\n\\n**技术框架**：该框架包含以下主要阶段：1) **种子防御生成**：使用思维链方法生成初始防御prompt。2) **迭代优化**：利用攻击样本评估当前防御prompt的有效性，并根据评估结果调整prompt。3) **防御prompt评估**：使用基准攻击数据集评估优化后的防御prompt的性能，包括攻击成功率和误检率。整个流程循环进行，直到防御prompt达到预期的性能指标。\\n\\n**关键创新**：该方法最重要的创新点在于其迭代优化的防御prompt生成方式。与传统的静态防御prompt相比，该方法能够根据实际攻击情况动态调整防御策略，从而提高防御的适应性和有效性。此外，使用思维链作为种子防御，有助于模型更好地理解攻击意图，从而生成更具针对性的防御prompt。\\n\\n**关键设计**：论文的关键设计包括：1) **思维链的prompt设计**：如何设计有效的思维链prompt，以引导模型进行正确的推理和判断。2) **迭代优化的目标函数**：如何定义目标函数，以衡量防御prompt的性能，并指导优化过程。3) **攻击样本的选择**：如何选择具有代表性的攻击样本，以评估和改进防御prompt的鲁棒性。具体的参数设置和损失函数等技术细节在论文中应该有更详细的描述（未知）。",
            "application_zh": "该研究成果可应用于各种需要安全可靠的LLM部署场景，尤其是在资源受限的边缘设备上。例如，智能家居设备、移动应用和嵌入式系统等。通过自动生成和优化防御prompt，可以有效防止恶意用户利用prompt注入攻击篡改LLM的行为，保障系统的安全性和可靠性，具有重要的实际应用价值和潜在的社会影响。",
            "highlight_zh": "该研究通过实验验证了所提出的迭代式prompt优化框架的有效性。实验结果表明，该方法能够显著降低prompt注入攻击的成功率，并降低误检率，从而提高了小型开源LLM的安全性。具体的性能提升数据和对比基线需要在论文中查找（未知）。",
            "tags_zh": [
                "prompt注入攻击",
                "LLaMA模型",
                "思维链",
                "迭代优化",
                "防御prompt",
                "目标劫持",
                "开源LLM"
            ],
            "_index": 98,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16307v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16307v1/attack_types.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16307v1/defense.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Adaptation of Agentic AI",
            "authors": [
                "Pengcheng Jiang",
                "Jiacheng Lin",
                "Zhiyi Shi",
                "Zifeng Wang",
                "Luxi He",
                "Yichen Wu",
                "Ming Zhong",
                "Peiyang Song",
                "Qizheng Zhang",
                "Heng Wang",
                "Xueqiang Xu",
                "Hanwen Xu",
                "Pengrui Han",
                "Dylan Zhang",
                "Jiashuo Sun",
                "Chaoqi Yang",
                "Kun Qian",
                "Tian Wang",
                "Changran Hu",
                "Manling Li",
                "Quanzheng Li",
                "Hao Peng",
                "Sheng Wang",
                "Jingbo Shang",
                "Chao Zhang",
                "Jiaxuan You",
                "Liyuan Liu",
                "Pan Lu",
                "Yu Zhang",
                "Heng Ji",
                "Yejin Choi",
                "Dawn Song",
                "Jimeng Sun",
                "Jiawei Han"
            ],
            "arxiv_id": "2512.16301v1",
            "summary": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.",
            "categories": [
                "cs.AI",
                "cs.CL"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16301v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Agentic AI自适应框架，提升智能体性能、可靠性和泛化能力",
            "summary_zh": "本文旨在对快速发展的Agentic AI研究领域进行统一，提出了一个系统的框架，涵盖了智能体自适应和工具自适应。进一步将智能体自适应分解为工具执行信号驱动和智能体输出信号驱动两种形式，并将工具自适应分解为智能体无关和智能体监督两种形式。该框架有助于明确Agentic AI中自适应策略的设计空间，明确其权衡，并为系统设计期间选择或切换策略提供实用指导。本文回顾了每个类别中的代表性方法，分析了它们的优缺点，并强调了关键的开放挑战和未来的机遇。总而言之，本文旨在为寻求构建更强大、高效和可靠的Agentic AI系统的研究人员和从业者提供概念基础和实践路线图。",
            "intro_zh": [
                "现有Agentic AI系统在性能、可靠性和泛化方面存在不足，需要有效的自适应机制。",
                "论文提出了一个统一的框架，涵盖智能体和工具的自适应，并细分为不同类型。",
                "该框架旨在帮助研究人员和实践者设计更强大、高效和可靠的Agentic AI系统。"
            ],
            "method_zh": "**问题定义**：Agentic AI系统在执行复杂任务时，面临性能、可靠性和泛化能力的挑战。现有方法缺乏系统性的自适应策略，难以应对不断变化的任务需求和环境。因此，需要一种统一的框架来指导Agentic AI系统的自适应设计。\\n\\n**核心思路**：论文的核心思路是将Agentic AI系统的自适应过程分解为智能体自适应和工具自适应两个维度。智能体自适应关注如何根据工具执行结果或智能体自身输出来调整智能体的行为。工具自适应则关注如何根据智能体的反馈或独立地改进工具的性能。通过这种分解，可以更清晰地理解不同自适应策略的优缺点，并为系统设计提供指导。\\n\\n**技术框架**：该框架包含两个主要部分：智能体自适应和工具自适应。智能体自适应进一步分为两种类型：工具执行信号驱动的自适应和智能体输出信号驱动的自适应。工具自适应也分为两种类型：智能体无关的自适应和智能体监督的自适应。该框架提供了一个统一的视角，用于分析和比较不同的自适应方法。\\n\\n**关键创新**：该论文的关键创新在于提出了一个统一的自适应框架，将智能体和工具的自适应过程进行了系统性的分解和分类。这种分解方式有助于研究人员更好地理解不同自适应策略的本质，并为设计新的自适应方法提供了理论基础。\\n\\n**关键设计**：论文没有涉及具体的参数设置、损失函数或网络结构等技术细节。该论文主要关注的是框架的设计和分类，旨在为Agentic AI系统的自适应研究提供一个高层次的指导。",
            "application_zh": "该研究成果可应用于各种需要智能体与工具交互的复杂任务，例如自动化客服、智能家居控制、自动驾驶、医疗诊断等。通过自适应机制，Agentic AI系统可以更好地适应不同的任务需求和环境变化，提高工作效率和准确性，并降低人工干预的成本。未来，该框架可以促进Agentic AI技术在更多领域的应用和发展。",
            "highlight_zh": "该论文提出了一个统一的Agentic AI自适应框架，对智能体和工具的自适应进行了系统性的分类和分析。虽然论文没有提供具体的实验结果，但它为研究人员提供了一个清晰的设计空间和实践指导，有助于开发更强大、高效和可靠的Agentic AI系统。该框架的提出本身就是一个重要的贡献。",
            "tags_zh": [
                "Agentic AI",
                "智能体自适应",
                "工具自适应",
                "自适应框架",
                "人工智能"
            ],
            "_index": 99,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16301v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16301v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16301v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "QuadSentinel: Sequent Safety for Machine-Checkable Control in Multi-agent Systems",
            "authors": [
                "Yiliu Yang",
                "Yilei Jiang",
                "Qunzhong Wang",
                "Yingshui Tan",
                "Xiaoyong Zhu",
                "Sherman S. M. Chow",
                "Bo Zheng",
                "Xiangyu Yue"
            ],
            "arxiv_id": "2512.16279v1",
            "summary": "Safety risks arise as large language model-based agents solve complex tasks with tools, multi-step plans, and inter-agent messages. However, deployer-written policies in natural language are ambiguous and context dependent, so they map poorly to machine-checkable rules, and runtime enforcement is unreliable. Expressing safety policies as sequents, we propose \\textsc{QuadSentinel}, a four-agent guard (state tracker, policy verifier, threat watcher, and referee) that compiles these policies into machine-checkable rules built from predicates over observable state and enforces them online. Referee logic plus an efficient top-$k$ predicate updater keeps costs low by prioritizing checks and resolving conflicts hierarchically. Measured on ST-WebAgentBench (ICML CUA~'25) and AgentHarm (ICLR~'25), \\textsc{QuadSentinel} improves guardrail accuracy and rule recall while reducing false positives. Against single-agent baselines such as ShieldAgent (ICML~'25), it yields better overall safety control. Near-term deployments can adopt this pattern without modifying core agents by keeping policies separate and machine-checkable. Our code will be made publicly available at https://github.com/yyiliu/QuadSentinel.",
            "categories": [
                "cs.AI",
                "cs.CL"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Preprint",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16279v1",
            "code_links": [
                {
                    "url": "https://github.com/yyiliu/QuadSentinel",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "QuadSentinel：多智能体系统中基于时序的安全策略可验证控制",
            "summary_zh": "当基于大型语言模型的智能体使用工具、多步计划和智能体间消息解决复杂任务时，会产生安全风险。然而，部署者编写的自然语言策略是模糊且依赖上下文的，因此它们很难映射到机器可检查的规则，并且运行时强制执行是不可靠的。本文将安全策略表示为时序逻辑，提出了\textsc{QuadSentinel}，一个四智能体守卫（状态跟踪器、策略验证器、威胁观察器和裁判），它将这些策略编译成由可观察状态上的谓词构建的机器可检查规则，并在在线强制执行它们。裁判逻辑加上高效的 top-$k$ 谓词更新器，通过优先检查和分层解决冲突来降低成本。在 ST-WebAgentBench (ICML CUA~'25) 和 AgentHarm (ICLR~'25) 上的测量表明，\textsc{QuadSentinel} 提高了护栏精度和规则召回率，同时减少了误报。与 ShieldAgent (ICML~'25) 等单智能体基线相比，它产生了更好的整体安全控制。通过保持策略分离和机器可检查，近期部署可以在不修改核心智能体的情况下采用这种模式。我们的代码将在 https://github.com/yyiliu/QuadSentinel 上公开。",
            "intro_zh": [
                "现有方法难以将自然语言安全策略转化为机器可验证规则，导致运行时强制执行不可靠。",
                "QuadSentinel 提出四智能体守卫架构，将安全策略编译为基于状态谓词的机器可检查规则，并在线执行。",
                "实验表明，QuadSentinel 提高了护栏精度和规则召回率，减少了误报，并优于单智能体基线。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多智能体系统中，基于大型语言模型的智能体在执行复杂任务时产生的安全风险问题。现有方法依赖自然语言描述的安全策略，这些策略模糊且依赖上下文，难以转化为机器可验证的规则，导致运行时安全控制效果不佳。现有方法缺乏一种有效的机制，能够将高层次的安全策略转化为可执行的、机器可验证的规则，并在运行时进行可靠的强制执行。\\n\\n**核心思路**：论文的核心思路是将安全策略表示为时序逻辑，并设计一个四智能体守卫架构（QuadSentinel），将这些策略编译成基于可观察状态谓词的机器可检查规则。通过在线强制执行这些规则，实现对多智能体系统的安全控制。这种设计旨在解决自然语言策略的模糊性和不可靠性问题，提供一种更精确、可验证的安全保障机制。\\n\\n**技术框架**：QuadSentinel 包含四个主要智能体：状态跟踪器（State Tracker）、策略验证器（Policy Verifier）、威胁观察器（Threat Watcher）和裁判（Referee）。状态跟踪器负责监控和记录系统的状态信息；策略验证器将安全策略编译成机器可检查的规则；威胁观察器检测潜在的威胁和违规行为；裁判负责仲裁冲突，并根据策略验证器的结果执行相应的安全措施。整个流程是：状态跟踪器提供状态信息，策略验证器验证策略，威胁观察器检测威胁，最后裁判根据前三者的信息做出决策。\\n\\n**关键创新**：论文的关键创新在于提出了 QuadSentinel 架构，将安全策略的制定、验证和执行分离到不同的智能体中，实现了模块化和可验证的安全控制。此外，论文还引入了裁判逻辑和高效的 top-$k$ 谓词更新器，用于降低计算成本，并优先检查和分层解决冲突。这种架构能够有效地将高层次的安全策略转化为可执行的、机器可验证的规则，并在运行时进行可靠的强制执行。\\n\\n**关键设计**：论文的关键设计包括：1) 将安全策略表示为时序逻辑，使其更易于形式化验证；2) 设计了四智能体架构，实现了安全控制的模块化和可扩展性；3) 引入了裁判逻辑和 top-$k$ 谓词更新器，用于降低计算成本和提高效率。具体的参数设置、损失函数和网络结构等技术细节在论文中未详细描述，属于未知信息。",
            "application_zh": "该研究成果可应用于各种多智能体系统，例如自动驾驶、机器人协作、智能家居等领域。通过部署 QuadSentinel，可以有效提高系统的安全性，防止潜在的风险和违规行为。该方法具有广泛的应用前景，有助于构建更安全、可靠的智能系统。",
            "highlight_zh": "实验结果表明，QuadSentinel 在 ST-WebAgentBench 和 AgentHarm 数据集上，提高了护栏精度和规则召回率，同时减少了误报。与 ShieldAgent 等单智能体基线相比，QuadSentinel 实现了更好的整体安全控制，验证了其有效性。",
            "tags_zh": [
                "多智能体系统",
                "安全策略",
                "机器可验证控制",
                "时序逻辑",
                "智能体架构"
            ],
            "_index": 100,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16279v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16279v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16279v1/imgs/harmful_by_category.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls",
            "authors": [
                "Ora Nova Fandina",
                "Eitan Farchi",
                "Shmulik Froimovich",
                "Raviv Gal",
                "Wesam Ibraheem",
                "Rami Katan",
                "Alice Podolsky"
            ],
            "arxiv_id": "2512.16272v1",
            "summary": "Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities.\n  To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked.\n  Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 94% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.",
            "categories": [
                "cs.SE",
                "cs.AI"
            ],
            "primary_category": "cs.SE",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16272v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "利用分析提示缓解LLM在代码评估中的盲点，提升COBOL代码生成质量",
            "summary_zh": "大型语言模型（LLM）越来越多地被部署为代码生成流程中的评估者（LaaJ）。尽管这种方式具有可扩展性，但LaaJ往往会忽略特定领域的关键问题，引发对其在关键评估任务中可靠性的担忧。为了更好地理解这些局限性，本文在一个具体的工业用例中检验了LaaJ的行为：通过COBOL代码生成实现遗留代码现代化。研究发现，即使是生产环境中部署的LaaJ也可能遗漏领域关键错误，暴露出其评估能力中存在的盲点。为了更好地理解这些盲点，本文分析了生成的COBOL程序和相关的LaaJ判断，并利用专家知识构建了一个初步的分类体系。基于此，开发了一个轻量级的分析检查工具，可以标记实践中观察到的30多个特定领域问题。该工具的输出被用作分析提示，动态地注入到评估者的提示中，以鼓励LaaJ重新审视可能被忽略的方面。在包含100个程序的测试集上，使用四个生产级别的LaaJ进行的实验表明，LaaJ单独只能检测到代码中约45%的错误，而分析检查器本身缺乏解释深度。当结合使用时，LaaJ+提示配置实现了高达94%的覆盖率，并产生了质量更高、更准确的解释，证明了分析-LLM混合方法可以显著提高已部署流程中的评估可靠性。本文发布了数据集和所有使用的提示。",
            "intro_zh": [
                "现有LaaJ在代码生成评估中存在领域知识盲点，导致关键错误被忽略，影响评估可靠性。",
                "提出一种分析提示方法，通过轻量级分析检查工具识别领域特定问题，并将其作为提示注入LaaJ。",
                "实验表明，LaaJ+提示配置显著提升错误检测覆盖率（最高达94%），并生成更准确的解释。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型（LLM）在作为代码生成评估者（LaaJ）时，由于缺乏特定领域的知识而导致的评估盲点问题。现有方法依赖于LLM自身的理解能力，容易忽略COBOL等遗留代码中的领域特定错误，从而影响代码质量和可靠性。\\n\\n**核心思路**：论文的核心思路是将领域专家知识融入到LLM的评估过程中。通过构建一个轻量级的分析检查工具，该工具能够识别COBOL代码中常见的领域特定问题，并将这些问题作为“分析提示”注入到LLM的评估提示中。这样，LLM在评估代码时，可以借助这些提示来关注可能被忽略的关键方面，从而提高评估的准确性和可靠性。\\n\\n**技术框架**：该方法的技术框架主要包含以下几个模块：1) COBOL代码生成器：生成需要评估的COBOL代码。2) 分析检查工具：基于领域专家知识，对生成的COBOL代码进行静态分析，识别潜在的领域特定问题。3) 提示注入模块：将分析检查工具的输出（即分析提示）动态地注入到LLM的评估提示中。4) LLM评估器（LaaJ）：使用带有分析提示的提示，对生成的COBOL代码进行评估，并给出评估结果和解释。\\n\\n**关键创新**：该方法最重要的技术创新点在于将领域专家知识与LLM的评估能力相结合。通过分析提示，弥补了LLM在特定领域知识方面的不足，使其能够更准确地识别代码中的错误。这种混合方法不仅提高了评估的准确性，还增强了评估结果的可解释性。\\n\\n**关键设计**：分析检查工具的设计是关键。它需要能够覆盖COBOL代码中常见的领域特定问题，例如数据类型转换、变量初始化、错误处理等。提示注入模块的设计也需要考虑如何将分析提示有效地融入到LLM的评估提示中，以最大程度地发挥提示的作用。论文中使用了超过30种领域特定问题的检查规则，并探索了不同的提示注入方式，以找到最佳的配置。",
            "application_zh": "该研究成果可应用于遗留系统现代化改造、代码质量评估、自动化代码审查等领域。通过提升LLM在特定领域代码评估中的准确性和可靠性，可以降低人工成本，提高开发效率，并减少潜在的软件缺陷。该方法具有通用性，可以扩展到其他领域，例如金融、医疗等，具有广阔的应用前景。",
            "highlight_zh": "实验结果表明，单独使用LaaJ只能检测到约45%的错误，而结合分析提示后，错误检测覆盖率最高可达94%。此外，LaaJ+提示配置生成的评估解释也更加准确和丰富。这些结果表明，分析-LLM混合方法可以显著提高代码评估的可靠性。",
            "tags_zh": [
                "大型语言模型",
                "代码评估",
                "领域知识",
                "分析提示",
                "COBOL代码"
            ],
            "_index": 101,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16272v1/hint_llaj.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16272v1/taxonomy.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16272v1/hybrid_laaj_issues_triplets_total_native_orange_leftlegend.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Learning to Wait: Synchronizing Agents with the Physical World",
            "authors": [
                "Yifei She",
                "Ping Zhang",
                "He Liu",
                "Yanmin Jia",
                "Yang Jing",
                "Zijun Liu",
                "Peng Sun",
                "Xiangbin Li",
                "Xiaohe Hu"
            ],
            "arxiv_id": "2512.16262v1",
            "summary": "Real-world agentic tasks, unlike synchronous Markov Decision Processes (MDPs), often involve non-blocking actions with variable latencies, creating a fundamental \\textit{Temporal Gap} between action initiation and completion. Existing environment-side solutions, such as blocking wrappers or frequent polling, either limit scalability or dilute the agent's context window with redundant observations. In this work, we propose an \\textbf{Agent-side Approach} that empowers Large Language Models (LLMs) to actively align their \\textit{Cognitive Timeline} with the physical world. By extending the Code-as-Action paradigm to the temporal domain, agents utilize semantic priors and In-Context Learning (ICL) to predict precise waiting durations (\\texttt{time.sleep(t)}), effectively synchronizing with asynchronous environment without exhaustive checking. Experiments in a simulated Kubernetes cluster demonstrate that agents can precisely calibrate their internal clocks to minimize both query overhead and execution latency, validating that temporal awareness is a learnable capability essential for autonomous evolution in open-ended environments.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16262v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Agent端时间同步方法，解决LLM在异步环境中的时序校准问题",
            "summary_zh": "与同步马尔可夫决策过程（MDP）不同，现实世界的Agent任务通常涉及具有可变延迟的非阻塞动作，从而在动作发起和完成之间产生根本性的“时间间隔”。现有的环境端解决方案，如阻塞包装器或频繁轮询，要么限制了可扩展性，要么用冗余观察稀释了Agent的上下文窗口。本文提出了一种Agent端方法，使大型语言模型（LLM）能够主动将其“认知时间线”与物理世界对齐。通过将代码即动作范式扩展到时间域，Agent利用语义先验和上下文学习（ICL）来预测精确的等待时长（`time.sleep(t)`），从而有效地与异步环境同步，而无需详尽的检查。在模拟的Kubernetes集群中的实验表明，Agent可以精确地校准其内部时钟，以最大限度地减少查询开销和执行延迟，从而验证了时间感知是在开放环境中自主进化必不可少的、可学习的能力。",
            "intro_zh": [
                "现实Agent任务中，动作完成存在时间延迟，导致Agent与环境交互出现时间间隔，现有方法难以兼顾效率与上下文完整性。",
                "论文提出Agent端时间同步方法，通过LLM预测等待时长，使Agent主动与异步环境对齐，无需频繁轮询。",
                "实验表明，该方法能有效校准Agent内部时钟，降低查询开销和执行延迟，验证了时间感知能力的可学习性。"
            ],
            "method_zh": "**问题定义**：现有方法在处理现实世界Agent任务时，由于动作的非阻塞性和可变延迟，导致Agent的认知时间线与物理世界存在时间间隔。环境端的解决方案，如阻塞包装器，会限制可扩展性；频繁轮询则会稀释Agent的上下文窗口，降低效率。因此，如何使Agent在异步环境中高效、准确地与环境同步是一个关键问题。\\n\\n**核心思路**：论文的核心思路是赋予Agent时间感知能力，使其能够主动预测并等待动作完成所需的时间，从而实现与异步环境的同步。通过将时间维度融入Agent的决策过程，避免了被动等待或频繁查询，提高了效率和准确性。\\n\\n**技术框架**：该方法基于代码即动作范式，利用大型语言模型（LLM）作为Agent的决策核心。Agent通过观察环境状态，利用语义先验和上下文学习（ICL）来预测等待时长，并执行`time.sleep(t)`指令。整体流程包括：1) Agent观察环境；2) LLM基于上下文预测等待时间；3) Agent执行`time.sleep(t)`；4) Agent再次观察环境，循环执行。\\n\\n**关键创新**：最重要的技术创新点在于将时间感知能力融入Agent的决策过程，使其能够主动预测等待时间，而不是依赖环境的同步机制。这种Agent端的时间同步方法，避免了环境端解决方案的局限性，提高了Agent在异步环境中的适应性和效率。\\n\\n**关键设计**：关键设计包括：1) 使用LLM作为Agent的决策核心，利用其强大的语义理解和推理能力；2) 通过上下文学习（ICL）提供时间相关的示例，引导LLM预测准确的等待时间；3) 使用`time.sleep(t)`指令模拟等待动作完成，实现Agent与环境的时间同步；4) 在模拟的Kubernetes集群中进行实验，验证该方法在实际场景中的有效性。",
            "application_zh": "该研究成果可应用于各种需要与异步环境交互的Agent任务，例如机器人控制、自动化运维、智能家居等。通过赋予Agent时间感知能力，可以提高其在复杂、动态环境中的自主性和效率，实现更智能、更可靠的自动化系统。未来，该方法有望扩展到更广泛的领域，例如智能交通、金融交易等。",
            "highlight_zh": "实验结果表明，该方法在模拟的Kubernetes集群中能够显著降低查询开销和执行延迟。Agent能够精确地校准其内部时钟，与异步环境实现高效同步。具体性能数据未知，但论文强调了该方法在最小化查询开销和执行延迟方面的有效性。",
            "tags_zh": [
                "Agent",
                "大型语言模型",
                "时间同步",
                "异步环境",
                "上下文学习"
            ],
            "_index": 102,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16262v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16262v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16262v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Sigma-Moe-Tiny Technical Report",
            "authors": [
                "Qingguo Hu",
                "Zhenghao Lin",
                "Ziyue Yang",
                "Yucheng Ding",
                "Xiao Liu",
                "Yuting Jiang",
                "Ruizhe Wang",
                "Tianyu Chen",
                "Zhongxin Guo",
                "Yifan Xiong",
                "Rui Gao",
                "Lei Qu",
                "Jinsong Su",
                "Peng Cheng",
                "Yeyun Gong"
            ],
            "arxiv_id": "2512.16248v1",
            "summary": "Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures.\n  Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny\n  Code: https://github.com/microsoft/ltp-megatron-lm",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16248v1",
            "code_links": [
                {
                    "url": "https://github.com/microsoft/ltp-megatron-lm",
                    "type": "github"
                },
                {
                    "url": "https://qghuxmu.github.io/Sigma-MoE-Tiny",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "Sigma-MoE-Tiny：提出一种高稀疏MoE语言模型，解决专家负载均衡难题，实现高效扩展。",
            "summary_zh": "本文介绍了Sigma-MoE-Tiny，一种混合专家（MoE）语言模型，它实现了现有开源模型中最高的稀疏性。Sigma-MoE-Tiny采用细粒度的专家分割，每层最多96个专家，但每个token仅激活一个专家，从而在200亿总参数下仅激活0.5B参数。这种极端稀疏性带来的主要挑战是专家负载均衡。研究发现，在这种设置下，广泛使用的负载均衡损失在较低层中往往失效。为了解决这个问题，提出了一种渐进式稀疏化策略，旨在平衡专家利用率和训练稳定性。Sigma-MoE-Tiny在一个多样化和高质量的语料库上进行预训练，然后进行后训练以进一步释放其能力。整个训练过程保持了显著的稳定性，没有出现不可恢复的损失峰值。综合评估表明，尽管仅激活0.5B参数，Sigma-MoE-Tiny在同等或更大规模的同类模型中实现了顶级的性能。此外，还深入讨论了高稀疏MoE模型中的负载均衡问题，为未来MoE架构中提高稀疏性提供了见解。",
            "intro_zh": [
                "现有MoE模型在极端稀疏性下，专家负载均衡面临挑战，传统负载均衡损失在底层失效。",
                "提出渐进式稀疏化策略，旨在平衡专家利用率和训练稳定性，解决高稀疏性下的负载均衡问题。",
                "Sigma-MoE-Tiny仅激活0.5B参数，但在同等或更大规模模型中表现出色，训练过程稳定。"
            ],
            "method_zh": "**问题定义**：论文旨在解决MoE模型中，在极高稀疏度下专家负载不均衡的问题。现有的负载均衡损失函数在高稀疏度下，尤其是在模型的底层，效果不佳，导致部分专家过度使用，而另一些专家利用率不足，影响模型整体性能。\\n\\n**核心思路**：论文的核心思路是通过渐进式稀疏化策略来解决专家负载均衡问题。该策略并非一开始就采用最高的稀疏度，而是逐步增加稀疏度，从而在训练初期保证专家能够得到充分的训练和利用，避免一开始就出现严重的负载不均衡。\\n\\n**技术框架**：Sigma-MoE-Tiny模型基于Transformer架构，并引入了MoE层。整体流程包括：首先，在高质量语料库上进行预训练，使用渐进式稀疏化策略进行专家选择和负载均衡；然后，进行后训练，进一步提升模型性能。模型包含多个MoE层，每个MoE层包含多个专家，每个token只路由到一个专家。\\n\\n**关键创新**：关键创新在于提出的渐进式稀疏化策略。与传统的固定稀疏度MoE模型不同，Sigma-MoE-Tiny在训练过程中动态调整稀疏度，从而更好地平衡专家利用率和训练稳定性。这种方法能够有效避免因高稀疏度导致的早期训练不稳定和专家负载不均衡问题。\\n\\n**关键设计**：渐进式稀疏化策略的具体实现包括：在训练初期，使用较低的稀疏度，允许更多的token路由到不同的专家，确保每个专家都能得到充分的训练。随着训练的进行，逐步增加稀疏度，最终达到目标稀疏度。论文中可能还涉及了对路由策略的优化，例如使用Top-K路由或可学习的路由函数，以及对负载均衡损失函数的改进，以更好地适应高稀疏度场景。具体的参数设置和损失函数细节需要在论文原文中查找。",
            "application_zh": "Sigma-MoE-Tiny的研究成果可应用于各种需要高效扩展的大规模语言模型场景，例如自然语言处理、机器翻译、文本生成等。其高稀疏性使得模型在资源受限的环境下也能运行，具有很高的实际应用价值。未来，该研究可以推动MoE模型在移动设备和边缘计算等领域的应用。",
            "highlight_zh": "Sigma-MoE-Tiny在仅激活0.5B参数的情况下，实现了与参数规模更大模型相当甚至更优越的性能。实验结果表明，该模型在多个NLP任务上取得了领先水平，证明了其在高稀疏度下的有效性。此外，训练过程的稳定性也表明了渐进式稀疏化策略的优越性。",
            "tags_zh": [
                "混合专家模型",
                "MoE",
                "稀疏模型",
                "负载均衡",
                "渐进式稀疏化",
                "语言模型",
                "深度学习"
            ],
            "_index": 103,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16248v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16248v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16248v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Ev-Trust: A Strategy Equilibrium Trust Mechanism for Evolutionary Games in LLM-Based Multi-Agent Services",
            "authors": [
                "Shiduo Yang",
                "Jiye Wang",
                "Jiayu Qin",
                "Jianbin Li",
                "Yu Wang",
                "Yuanhe Zhao",
                "Kenan Guo"
            ],
            "arxiv_id": "2512.16167v1",
            "summary": "The rapid evolution of the Web toward an agent-centric paradigm, driven by large language models (LLMs), has enabled autonomous agents to reason, plan, and interact in complex decentralized environments. However, the openness and heterogeneity of LLM-based multi-agent systems also amplify the risks of deception, fraud, and misinformation, posing severe challenges to trust establishment and system robustness. To address this issue, we propose Ev-Trust, a strategy-equilibrium trust mechanism grounded in evolutionary game theory. This mechanism integrates direct trust, indirect trust, and expected revenue into a dynamic feedback structure that guides agents' behavioral evolution toward equilibria. Within a decentralized \"Request-Response-Payment-Evaluation\" service framework, Ev-Trust enables agents to adaptively adjust strategies, naturally excluding malicious participants while reinforcing high-quality collaboration. Furthermore, our theoretical derivation based on replicator dynamics equations proves the existence and stability of local evolutionary equilibria. Experimental results indicate that our approach effectively reflects agent trustworthiness in LLM-driven open service interaction scenarios, reduces malicious strategies, and increases collective revenue. We hope Ev-Trust can provide a new perspective on trust modeling for the agentic service web in group evolutionary game scenarios.",
            "categories": [
                "cs.MA",
                "cs.AI",
                "cs.GT"
            ],
            "primary_category": "cs.MA",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "12 pages, 11 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16167v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Ev-Trust机制，利用演化博弈论解决LLM多智能体服务中的信任问题。",
            "summary_zh": "随着Web向以智能体为中心的范式快速演进，由大型语言模型（LLM）驱动的自主智能体能够在复杂的去中心化环境中进行推理、规划和交互。然而，基于LLM的多智能体系统的开放性和异构性也加剧了欺骗、欺诈和虚假信息的风险，对信任建立和系统鲁棒性构成严峻挑战。为了解决这个问题，我们提出了一种基于演化博弈论的策略均衡信任机制Ev-Trust。该机制将直接信任、间接信任和预期收益整合到一个动态反馈结构中，引导智能体的行为演化到均衡状态。在去中心化的“请求-响应-支付-评估”服务框架内，Ev-Trust使智能体能够自适应地调整策略，自然地排除恶意参与者，同时加强高质量的协作。此外，我们基于复制者动态方程的理论推导证明了局部演化均衡的存在性和稳定性。实验结果表明，我们的方法有效地反映了LLM驱动的开放服务交互场景中智能体的可信度，减少了恶意策略，并增加了集体收益。我们希望Ev-Trust能够为群体演化博弈场景中的智能体服务网络提供一种新的信任建模视角。",
            "intro_zh": [
                "基于LLM的多智能体系统面临欺骗、欺诈和虚假信息的风险，信任建立和系统鲁棒性是关键挑战。",
                "提出Ev-Trust机制，将直接信任、间接信任和预期收益整合，引导智能体行为演化到策略均衡。",
                "实验表明，Ev-Trust能有效反映智能体可信度，减少恶意策略，并增加集体收益。"
            ],
            "method_zh": "**问题定义**：论文旨在解决基于LLM的多智能体系统中，由于开放性和异构性带来的信任危机问题。现有方法难以有效识别和排除恶意智能体，导致欺骗、欺诈和虚假信息泛滥，影响系统整体的鲁棒性和协作效率。现有信任机制无法很好地适应智能体策略的动态演化，容易被恶意智能体利用。\\n\\n**核心思路**：论文的核心思路是利用演化博弈论，将智能体之间的交互建模为一个动态博弈过程。通过引入直接信任、间接信任和预期收益，构建一个动态反馈结构，引导智能体的策略向演化均衡状态收敛。这种机制能够使诚实守信的智能体获得更高的收益，从而在群体中占据优势地位，而恶意智能体则逐渐被淘汰。\\n\\n**技术框架**：Ev-Trust机制运行在一个去中心化的“请求-响应-支付-评估”服务框架内。主要流程如下：1) 请求者智能体发起服务请求；2) 响应者智能体提供服务；3) 请求者智能体根据服务质量支付报酬；4) 请求者智能体对响应者智能体进行评估，更新信任值。Ev-Trust机制在评估阶段发挥作用，它综合考虑直接信任（请求者对响应者的直接评价）、间接信任（其他智能体的评价）和预期收益（基于历史交互的收益预测），计算出一个综合信任值，用于指导智能体未来的策略选择。\\n\\n**关键创新**：Ev-Trust的关键创新在于将演化博弈论引入到多智能体信任建模中。与传统的静态信任模型不同，Ev-Trust能够动态地适应智能体策略的演化，从而更有效地识别和排除恶意智能体。此外，Ev-Trust综合考虑了直接信任、间接信任和预期收益，从而更全面地评估智能体的可信度。\\n\\n**关键设计**：Ev-Trust使用复制者动态方程来模拟智能体策略的演化过程。信任值的计算公式综合考虑了直接信任、间接信任和预期收益，并使用权重参数来调节它们之间的相对重要性。具体参数设置（例如权重参数、学习率等）需要根据具体的应用场景进行调整。论文中给出了一个具体的参数设置示例，并进行了实验验证。",
            "application_zh": "Ev-Trust机制可应用于各种基于LLM的多智能体服务场景，例如：去中心化知识共享平台、智能客服系统、供应链管理系统等。通过建立有效的信任机制，可以提高系统的鲁棒性、协作效率和用户满意度，促进智能体生态的健康发展。未来，该机制可以进一步扩展到更复杂的博弈场景，例如：联盟形成、资源分配等。",
            "highlight_zh": "实验结果表明，Ev-Trust机制能够有效地反映智能体的可信度，减少恶意策略的比例，并提高整体的集体收益。具体来说，在模拟的“请求-响应-支付-评估”服务场景中，使用Ev-Trust机制的智能体群体比没有使用该机制的群体获得了更高的平均收益，并且恶意智能体的生存率显著降低。量化数据（例如：收益提升百分比、恶意智能体比例下降幅度）未知。",
            "tags_zh": [
                "多智能体系统",
                "大型语言模型",
                "演化博弈论",
                "信任机制",
                "策略均衡"
            ],
            "_index": 104,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16167v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16167v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16167v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection",
            "authors": [
                "Hao Li",
                "Yubing Ren",
                "Yanan Cao",
                "Yingjie Li",
                "Fang Fang",
                "Xuebin Wang"
            ],
            "arxiv_id": "2512.16439v1",
            "summary": "Benefiting from the superior capabilities of large language models in natural language understanding and generation, Embeddings-as-a-Service (EaaS) has emerged as a successful commercial paradigm on the web platform. However, prior studies have revealed that EaaS is vulnerable to imitation attacks. Existing methods protect the intellectual property of EaaS through watermarking techniques, but they all ignore the most important properties of embedding: semantics, resulting in limited harmlessness and stealthiness. To this end, we propose SemMark, a novel semantic-based watermarking paradigm for EaaS copyright protection. SemMark employs locality-sensitive hashing to partition the semantic space and inject semantic-aware watermarks into specific regions, ensuring that the watermark signals remain imperceptible and diverse. In addition, we introduce the adaptive watermark weight mechanism based on the local outlier factor to preserve the original embedding distribution. Furthermore, we propose Detect-Sampling and Dimensionality-Reduction attacks and construct four scenarios to evaluate the watermarking method. Extensive experiments are conducted on four popular NLP datasets, and SemMark achieves superior verifiability, diversity, stealthiness, and harmlessness.",
            "categories": [
                "cs.CR",
                "cs.CL"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16439v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出SemMark以解决EaaS版权保护中的水印隐蔽性问题",
            "summary_zh": "随着大型语言模型在自然语言理解和生成方面的优越能力，嵌入即服务（EaaS）已成为一种成功的商业模式。然而，现有研究表明EaaS易受模仿攻击。虽然已有方法通过水印技术保护EaaS的知识产权，但忽视了嵌入的语义特性，导致隐蔽性和无害性有限。为此，本文提出了SemMark，一种基于语义的水印范式，利用局部敏感哈希将语义空间划分，并在特定区域注入语义感知水印，确保水印信号隐形且多样。此外，基于局部离群因子的自适应水印权重机制被引入，以保持原始嵌入分布。通过构建四种场景进行评估，SemMark在可验证性、多样性、隐蔽性和无害性方面表现优越。",
            "intro_zh": [
                "现有EaaS保护方法在水印设计上忽视了嵌入的语义特性，导致其隐蔽性和无害性不足。",
                "本文提出SemMark，通过局部敏感哈希技术在语义空间中注入水印，确保水印信号隐形且多样。",
                "在四个流行的NLP数据集上进行的实验表明，SemMark在可验证性和隐蔽性等方面显著优于现有方法。"
            ],
            "method_zh": "**问题定义**：本文旨在解决EaaS在版权保护中面临的模仿攻击问题，现有水印方法未能充分考虑嵌入的语义特性，导致水印的隐蔽性和无害性不足。\\n\\n**核心思路**：SemMark的核心思路是利用局部敏感哈希技术将语义空间划分，并在特定区域注入语义感知水印，从而确保水印信号的隐形和多样性，同时引入自适应水印权重机制以保持原始嵌入分布。\\n\\n**技术框架**：SemMark的整体架构包括语义空间的划分、语义水印的注入、以及自适应水印权重的调整。主要模块包括局部敏感哈希、语义水印生成和水印检测。\\n\\n**关键创新**：SemMark的主要创新在于其语义感知水印设计和自适应水印权重机制，这与现有方法的设计思路有本质区别，能够更好地保护EaaS的知识产权。\\n\\n**关键设计**：在设计中，采用局部离群因子来动态调整水印权重，确保水印的多样性和隐蔽性，同时保持原始嵌入的分布特性。",
            "application_zh": "该研究的潜在应用领域包括自然语言处理、知识产权保护和内容创作等。通过提供一种有效的水印保护机制，SemMark能够帮助企业保护其EaaS产品的知识产权，防止模仿和盗用，具有重要的实际价值和未来影响。",
            "highlight_zh": "实验结果显示，SemMark在可验证性、多样性、隐蔽性和无害性方面均优于现有水印方法，具体表现为在四个NLP数据集上实现了显著的性能提升，验证了其有效性和实用性。",
            "tags_zh": [
                "嵌入即服务",
                "版权保护",
                "水印技术",
                "自然语言处理",
                "语义感知"
            ],
            "_index": 105,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16439v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16439v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16439v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Evaluating OpenAI GPT Models for Translation of Endangered Uralic Languages: A Comparison of Reasoning and Non-Reasoning Architectures",
            "authors": [
                "Yehor Tereshchenko",
                "Mika Hämäläinen",
                "Svitlana Myroniuk"
            ],
            "arxiv_id": "2512.16287v1",
            "summary": "The evaluation of Large Language Models (LLMs) for translation tasks has primarily focused on high-resource languages, leaving a significant gap in understanding their performance on low-resource and endangered languages. This study presents a comprehensive comparison of OpenAI's GPT models, specifically examining the differences between reasoning and non-reasoning architectures for translating between Finnish and four low-resource Uralic languages: Komi-Zyrian, Moksha, Erzya, and Udmurt. Using a parallel corpus of literary texts, we evaluate model willingness to attempt translation through refusal rate analysis across different model architectures. Our findings reveal significant performance variations between reasoning and non-reasoning models, with reasoning models showing 16 percentage points lower refusal rates. The results provide valuable insights for researchers and practitioners working with Uralic languages and contribute to the broader understanding of reasoning model capabilities for endangered language preservation.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "IWCLUL 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16287v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "评估OpenAI GPT模型在濒危乌拉尔语翻译中的性能：推理与非推理架构对比",
            "summary_zh": "本研究旨在评估大型语言模型（LLMs）在翻译任务中的性能，尤其关注低资源和濒危语言。目前，对LLMs的评估主要集中在高资源语言上，对它们在低资源语言上的表现缺乏了解。本研究对比了OpenAI的GPT模型，特别是推理和非推理架构在芬兰语与四种低资源乌拉尔语（科米-兹梁语、莫克沙语、埃尔齐亚语和乌德穆尔特语）之间翻译的差异。我们使用文学文本的平行语料库，通过分析不同模型架构的拒绝率来评估模型尝试翻译的意愿。研究结果表明，推理模型和非推理模型之间存在显著的性能差异，推理模型的拒绝率降低了16个百分点。这些结果为研究乌拉尔语的研究人员和从业者提供了有价值的见解，并有助于更广泛地理解推理模型在濒危语言保护方面的能力。",
            "intro_zh": [
                "现有大型语言模型翻译能力评估主要集中在高资源语言，忽略了低资源和濒危语言的翻译效果。",
                "本研究对比了OpenAI的GPT模型，特别是推理和非推理架构在乌拉尔语翻译任务中的表现差异。",
                "实验结果表明，推理模型在低资源乌拉尔语翻译中表现更佳，拒绝率显著低于非推理模型。"
            ],
            "method_zh": "**问题定义**：论文旨在评估OpenAI的GPT模型在低资源濒危乌拉尔语翻译任务中的性能。现有方法主要集中在高资源语言，缺乏对低资源语言翻译能力的深入评估，导致对这些语言的数字化保护和传承缺乏有效工具。现有方法无法有效衡量模型在低资源语言翻译任务中的置信度，即模型是否愿意尝试翻译，以及翻译质量如何。\\n\\n**核心思路**：论文的核心思路是通过对比推理和非推理架构的GPT模型在乌拉尔语翻译任务中的表现，来评估其在低资源语言翻译中的潜力。通过分析模型的拒绝率，来衡量模型尝试翻译的意愿。推理模型通常具有更强的上下文理解和逻辑推理能力，因此预期在低资源语言翻译中表现更佳。\\n\\n**技术框架**：研究采用平行语料库，包含芬兰语和四种低资源乌拉尔语（科米-兹梁语、莫克沙语、埃尔齐亚语和乌德穆尔特语）的文学文本。研究流程包括：1) 构建平行语料库；2) 使用不同的GPT模型（包括推理和非推理架构）进行翻译；3) 分析模型的拒绝率，即模型拒绝翻译的比例；4) 对翻译结果进行人工评估或使用自动评估指标（如BLEU）进行评估。\\n\\n**关键创新**：该研究的关键创新在于关注低资源和濒危语言的翻译任务，并对比了推理和非推理架构的GPT模型在这些任务中的表现差异。通过分析模型的拒绝率，提供了一种评估模型在低资源语言翻译任务中置信度的新方法。\\n\\n**关键设计**：研究的关键设计包括：1) 选择具有代表性的低资源乌拉尔语作为研究对象；2) 构建高质量的平行语料库；3) 选择合适的GPT模型进行对比，例如GPT-3、GPT-3.5和GPT-4等；4) 设计合理的拒绝率分析方法，例如设置阈值来判断模型是否拒绝翻译；5) 采用人工评估和自动评估相结合的方式来评估翻译质量。具体参数设置和损失函数等细节取决于所使用的GPT模型。",
            "application_zh": "该研究成果可应用于濒危语言的数字化保护和传承，例如自动翻译文学作品、构建多语种语料库、开发语言学习工具等。通过提高低资源语言的机器翻译质量，可以促进不同语言文化之间的交流和理解，并为语言学研究提供数据支持。未来，该研究可以扩展到其他低资源语言，并探索更有效的模型架构和训练方法。",
            "highlight_zh": "实验结果表明，推理模型（如GPT-4）在低资源乌拉尔语翻译任务中的拒绝率显著低于非推理模型。具体而言，推理模型的拒绝率比非推理模型降低了16个百分点，表明推理模型更愿意尝试翻译这些低资源语言。这一结果突显了推理模型在处理低资源语言方面的优势。",
            "tags_zh": [
                "低资源语言翻译",
                "濒危语言保护",
                "大型语言模型",
                "GPT模型",
                "推理模型",
                "乌拉尔语",
                "机器翻译"
            ],
            "_index": 106,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding",
            "authors": [
                "Chenkai Xu",
                "Yijie Jin",
                "Jiajun Li",
                "Yi Tu",
                "Guoping Long",
                "Dandan Tu",
                "Tianqi Hou",
                "Junchi Yan",
                "Zhijie Deng"
            ],
            "arxiv_id": "2512.16229v1",
            "summary": "Diffusion Large Language Models (dLLMs) have demonstrated significant potential for high-speed inference. However, current confidence-driven decoding strategies are constrained by limited parallelism, typically achieving only 1--3 tokens per forward pass (TPF). In this work, we identify that the degree of parallelism during dLLM inference is highly sensitive to the Token Filling Order (TFO). Then, we introduce Lookahead PArallel Decoding LoPA, a training-free, plug-and-play algorithm, to identify a superior TFO and hence accelerate inference. LoPA concurrently explores distinct candidate TFOs via parallel branches, and selects the one with the highest potential for future parallelism based on branch confidence. We apply LoPA to the state-of-the-art D2F model and observe a substantial enhancement in decoding efficiency. Notably, LoPA increases the TPF of D2F-Dream to 10.1 on the GSM8K while maintaining performance superior to the Dream baseline. Furthermore, to facilitate this unprecedented degree of parallelism, we develop a specialized multi-device inference system featuring Branch Parallelism (BP), which achieves a single-sample throughput of 1073.9 tokens per second under multi-GPU deployment. The code is available at https://github.com/zhijie-group/LoPA.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16229v1",
            "code_links": [
                {
                    "url": "https://github.com/zhijie-group/LoPA",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "LoPA：通过前瞻并行解码加速扩散大语言模型推理",
            "summary_zh": "扩散大语言模型(dLLM)在高速推理方面展现出巨大潜力。然而，当前基于置信度的解码策略受到有限并行性的约束，通常每次前向传播只能实现1-3个token的生成(TPF)。本文发现dLLM推理过程中的并行度对Token填充顺序(TFO)高度敏感。因此，我们提出了Lookahead PArallel Decoding (LoPA)，一种无需训练、即插即用的算法，用于识别更优的TFO，从而加速推理。LoPA通过并行分支同时探索不同的候选TFO，并基于分支置信度选择未来并行潜力最大的一个。我们将LoPA应用于最先进的D2F模型，观察到解码效率的显著提升。值得注意的是，LoPA在GSM8K上将D2F-Dream的TPF提高到10.1，同时保持优于Dream基线的性能。此外，为了支持这种前所未有的并行度，我们开发了一种专门的多设备推理系统，该系统具有分支并行性(BP)，在多GPU部署下实现了每秒1073.9个token的单样本吞吐量。代码已开源。",
            "intro_zh": [
                "现有扩散大语言模型推理受限于置信度驱动的解码策略，并行度低，严重影响推理速度。",
                "LoPA通过并行探索不同的Token填充顺序，并基于置信度选择最优顺序，从而提升并行度。",
                "实验表明，LoPA显著提升了D2F模型的解码效率，在GSM8K上实现了更高的TPF和性能。"
            ],
            "method_zh": "**问题定义**：扩散大语言模型（dLLM）的推理速度受限于其解码过程的并行度。传统的置信度驱动的解码策略，由于其固有的串行性，导致每次前向传播只能生成少量的token，严重限制了推理效率。现有方法难以充分挖掘dLLM的并行潜力，成为提升推理速度的瓶颈。\\n\\n**核心思路**：LoPA的核心思路是通过优化Token填充顺序（TFO）来提高dLLM推理的并行度。不同的TFO会导致不同的并行潜力。LoPA通过并行探索多个候选TFO，并根据每个分支的置信度来评估其未来并行潜力，从而选择最优的TFO。这种前瞻性的策略能够有效地挖掘dLLM的并行潜力，从而加速推理。\\n\\n**技术框架**：LoPA的整体框架包括以下几个主要步骤：1) **并行分支探索**：同时探索多个不同的候选TFO，每个TFO对应一个并行分支。2) **置信度评估**：评估每个分支的置信度，用于衡量其未来并行潜力。3) **最优TFO选择**：基于置信度选择具有最高未来并行潜力的TFO。4) **并行解码**：使用选择的最优TFO进行并行解码，生成多个token。\\n\\n**关键创新**：LoPA的关键创新在于其前瞻性的并行解码策略。与传统的串行解码方法不同，LoPA通过并行探索多个候选TFO，并根据置信度选择最优的TFO，从而最大化并行度。这种前瞻性的策略能够有效地挖掘dLLM的并行潜力，显著提升推理速度。此外，LoPA是一种无需训练、即插即用的算法，易于集成到现有的dLLM推理系统中。\\n\\n**关键设计**：LoPA的关键设计包括：1) **分支数量**：并行探索的分支数量是一个重要的参数，需要根据计算资源和性能需求进行调整。2) **置信度评估方法**：置信度评估方法的选择会影响最优TFO的选择，需要根据具体的dLLM模型进行优化。3) **多设备推理系统**：为了支持高并行度，论文开发了一个专门的多设备推理系统，该系统具有分支并行性(BP)，能够充分利用多GPU的计算资源。",
            "application_zh": "LoPA具有广泛的应用前景，可应用于各种需要高速推理的场景，例如实时对话系统、机器翻译、文本摘要等。通过提高dLLM的推理速度，LoPA可以显著提升这些应用的性能和用户体验。此外，LoPA还可以促进dLLM在资源受限设备上的部署，例如移动设备和嵌入式系统，从而拓展dLLM的应用范围。",
            "highlight_zh": "LoPA在D2F-Dream模型上取得了显著的性能提升。在GSM8K数据集上，LoPA将D2F-Dream的TPF提高到10.1，同时保持了优于Dream基线的性能。此外，通过开发专门的多设备推理系统，LoPA实现了每秒1073.9个token的单样本吞吐量，展示了其强大的并行解码能力。",
            "tags_zh": [
                "扩散大语言模型",
                "并行解码",
                "Token填充顺序",
                "推理加速",
                "多设备推理"
            ],
            "_index": 107,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16229v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16229v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16229v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Machine Learning-based Optimal Control for Colloidal Self-Assembly",
            "authors": [
                "Andres Lizano-Villalobos",
                "Fangyuan Ma",
                "Wentao Tang",
                "Wei Sun",
                "Xun Tang"
            ],
            "arxiv_id": "2512.16402v1",
            "summary": "Achieving precise control of colloidal self-assembly into specific patterns remains a longstanding challenge due to the complex process dynamics. Recently, machine learning-based state representation and reinforcement learning-based control strategies have started to accumulate popularity in the field, showing great potential in achieving an automatable and generalizable approach to producing patterned colloidal assembly. In this work, we adopted a machine learning-based optimal control framework, combining unsupervised learning and graph convolutional neural work for state observation with deep reinforcement learning-based optimal control policy calculation, to provide a data-driven control approach that can potentially be generalized to other many-body self-assembly systems. With Brownian dynamics simulations, we demonstrated its superior performance as compared to traditional order parameter-based state description, and its efficacy in obtaining ordered 2-dimensional spherical colloidal self-assembly in an electric field-mediated system with an actual success rate of 97%.",
            "categories": [
                "cond-mat.soft",
                "eess.SY"
            ],
            "primary_category": "cond-mat.soft",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "19 pages, 5 figures, 1 table",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16402v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "deep reinforcement learning"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出基于机器学习的最优控制框架，实现胶体自组装的精确控制",
            "summary_zh": "由于复杂的动力学过程，精确控制胶体自组装成特定模式一直是一个长期存在的挑战。 近年来，基于机器学习的状态表示和基于强化学习的控制策略在该领域开始受到欢迎，显示出在实现自动化和通用化方法以产生图案化胶体组装方面的巨大潜力。 在这项工作中，我们采用了一种基于机器学习的最优控制框架，将无监督学习和图卷积神经网络用于状态观察，以及基于深度强化学习的最优控制策略计算，以提供一种数据驱动的控制方法，该方法可能推广到其他多体自组装系统。 通过布朗动力学模拟，我们证明了其相对于传统的基于序参数的状态描述的优越性能，以及在电场介导的系统中获得有序二维球形胶体自组装的有效性，实际成功率为 97%。",
            "intro_zh": [
                "精确控制胶体自组装面临挑战，传统方法难以应对复杂动力学过程。",
                "采用机器学习最优控制框架，结合无监督学习、图卷积网络和深度强化学习。",
                "布朗动力学模拟表明，该方法优于传统方法，成功率高达97%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决胶体自组装过程中难以精确控制的问题。现有方法，如基于序参数的控制，无法充分描述复杂的多体相互作用，导致控制精度不足，泛化能力差。传统方法依赖于人工设计的特征，难以适应不同的自组装系统。\\n\\n**核心思路**：论文的核心思路是利用机器学习方法自动学习胶体系统的状态表示，并基于此进行最优控制。通过无监督学习提取系统关键特征，利用图卷积神经网络处理多体相互作用，最后使用深度强化学习寻找最优控制策略。这种数据驱动的方法能够更好地适应复杂系统，提高控制精度和泛化能力。\\n\\n**技术框架**：整体框架包含三个主要模块：1) **状态观察模块**：使用无监督学习（具体算法未知）从原始数据中提取低维状态表示，并使用图卷积神经网络（GCN）处理胶体粒子之间的相互作用。GCN的输入是粒子的位置和电荷等信息，输出是每个粒子的状态向量。2) **控制策略学习模块**：使用深度强化学习（DRL）（具体算法未知）学习最优控制策略。DRL智能体根据当前状态选择控制动作（例如，电场强度），并根据环境反馈（例如，组装结构的质量）调整策略。3) **布朗动力学模拟器**：用于模拟胶体粒子的运动和相互作用，为DRL智能体提供训练环境。\\n\\n**关键创新**：该方法的核心创新在于将机器学习方法引入胶体自组装控制，实现了数据驱动的控制策略。与传统方法相比，该方法能够自动学习系统状态表示，无需人工设计特征，从而提高了控制精度和泛化能力。此外，使用图卷积神经网络处理多体相互作用，能够更好地捕捉胶体系统的复杂动力学行为。\\n\\n**关键设计**：论文中关键的设计细节包括：1) 无监督学习算法的选择（未知），用于提取低维状态表示。2) 图卷积神经网络的结构设计，包括层数、卷积核大小等（未知），用于处理胶体粒子之间的相互作用。3) 深度强化学习算法的选择（未知），以及奖励函数的设计，用于引导智能体学习最优控制策略。奖励函数可能包括组装结构的质量、能量消耗等因素。",
            "application_zh": "该研究成果可应用于材料科学、纳米技术等领域，实现对纳米颗粒、蛋白质等微观粒子的精确组装，从而制造具有特定功能的纳米器件、生物传感器等。该方法有望加速新材料的研发和生物医学工程的发展，例如，可以用于构建具有特定光学或电学性质的超材料，或者用于构建药物递送系统。",
            "highlight_zh": "实验结果表明，该方法在二维球形胶体自组装任务中取得了显著的成功，实际成功率高达97%。与传统的基于序参数的状态描述方法相比，该方法具有更强的控制能力和更高的组装精度。该结果验证了机器学习方法在胶体自组装控制中的有效性和优越性。",
            "tags_zh": [
                "胶体自组装",
                "机器学习",
                "最优控制",
                "深度强化学习",
                "图卷积神经网络"
            ],
            "_index": 108,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Prime and Reach: Synthesising Body Motion for Gaze-Primed Object Reach",
            "authors": [
                "Masashi Hatano",
                "Saptarshi Sinha",
                "Jacob Chalk",
                "Wei-Hong Li",
                "Hideo Saito",
                "Dima Damen"
            ],
            "arxiv_id": "2512.16456v1",
            "summary": "Human motion generation is a challenging task that aims to create realistic motion imitating natural human behaviour. We focus on the well-studied behaviour of priming an object/location for pick up or put down -- that is, the spotting of an object/location from a distance, known as gaze priming, followed by the motion of approaching and reaching the target location. To that end, we curate, for the first time, 23.7K gaze-primed human motion sequences for reaching target object locations from five publicly available datasets, i.e., HD-EPIC, MoGaze, HOT3D, ADT, and GIMO. We pre-train a text-conditioned diffusion-based motion generation model, then fine-tune it conditioned on goal pose or location, on our curated sequences. Importantly, we evaluate the ability of the generated motion to imitate natural human movement through several metrics, including the 'Reach Success' and a newly introduced 'Prime Success' metric. On the largest dataset, HD-EPIC, our model achieves 60% prime success and 89% reach success when conditioned on the goal object location.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project Page: https://masashi-hatano.github.io/prime-and-reach/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16456v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "motion generation"
                    ],
                    "score": 2.5
                }
            ],
            "relevance_score": 2.5,
            "hit_pillars": [
                "4_motion_diffusion"
            ],
            "headline_zh": "提出一种基于注视引导的运动合成方法以解决人类动作生成问题",
            "summary_zh": "人类动作生成是一项挑战性任务，旨在创建模仿自然人类行为的真实运动。本文聚焦于物体/位置的注视引导行为，即从远处识别物体/位置，随后接近并达到目标位置。我们首次整理了来自五个公开数据集的23.7K个注视引导的人类运动序列，并在此基础上预训练了一个文本条件的扩散模型，随后根据目标姿态或位置进行了微调。通过多项指标评估生成运动模仿自然人类运动的能力，结果显示在最大数据集HD-EPIC上，我们的模型在目标物体位置条件下达到了60%的注视成功率和89%的到达成功率。",
            "intro_zh": [
                "现有的人类动作生成方法在模拟自然运动方面存在不足，尤其是在注视引导的物体接触行为上。",
                "本文提出了一种基于文本条件的扩散模型，通过微调实现对目标位置的运动生成，首次整理了大量注视引导的运动序列。",
                "在HD-EPIC数据集上，模型在目标位置条件下实现了60%的注视成功率和89%的到达成功率，显示出显著的性能提升。"
            ],
            "method_zh": "**问题定义**：本文旨在解决人类动作生成中的注视引导物体接触行为，现有方法在此方面的表现不足，难以生成自然的运动轨迹。\\n\\n**核心思路**：通过整理大量注视引导的运动序列，构建一个文本条件的扩散模型，能够根据目标位置生成相应的运动，模拟人类的自然行为。\\n\\n**技术框架**：整体架构包括数据整理、模型预训练和微调三个主要阶段。首先，从多个数据集中提取注视引导的运动序列；其次，预训练扩散模型；最后，基于目标位置进行微调以生成运动。\\n\\n**关键创新**：本研究的创新点在于首次整理了23.7K个注视引导的运动序列，并引入了新的“注视成功”指标，评估生成运动的自然性。与现有方法相比，能够更好地模拟人类的运动行为。\\n\\n**关键设计**：模型的损失函数设计考虑了运动的自然性和目标位置的准确性，网络结构采用了扩散模型的框架，结合文本条件输入以增强生成效果。",
            "application_zh": "该研究的潜在应用领域包括人机交互、机器人运动生成和虚拟现实等。通过生成更自然的人类动作，可以提升机器人与人类的协作能力，改善用户体验，并推动智能系统在复杂环境中的应用。",
            "highlight_zh": "在HD-EPIC数据集上，模型在目标位置条件下实现了60%的注视成功率和89%的到达成功率，显著优于现有方法，展示了良好的运动生成能力和自然性。",
            "tags_zh": [
                "人类动作生成",
                "注视引导",
                "运动合成",
                "扩散模型",
                "机器人技术",
                "人机交互",
                "虚拟现实"
            ],
            "_index": 109,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16456v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16456v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16456v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Single-View Shape Completion for Robotic Grasping in Clutter",
            "authors": [
                "Abhishek Kashyap",
                "Yuxuan Yang",
                "Henrik Andreasson",
                "Todor Stoyanov"
            ],
            "arxiv_id": "2512.16449v1",
            "summary": "In vision-based robot manipulation, a single camera view can only capture one side of objects of interest, with additional occlusions in cluttered scenes further restricting visibility. As a result, the observed geometry is incomplete, and grasp estimation algorithms perform suboptimally. To address this limitation, we leverage diffusion models to perform category-level 3D shape completion from partial depth observations obtained from a single view, reconstructing complete object geometries to provide richer context for grasp planning. Our method focuses on common household items with diverse geometries, generating full 3D shapes that serve as input to downstream grasp inference networks. Unlike prior work, which primarily considers isolated objects or minimal clutter, we evaluate shape completion and grasping in realistic clutter scenarios with household objects. In preliminary evaluations on a cluttered scene, our approach consistently results in better grasp success rates than a naive baseline without shape completion by 23% and over a recent state of the art shape completion approach by 19%. Our code is available at https://amm.aass.oru.se/shape-completion-grasping/.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16449v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出基于扩散模型的单视角形状补全方法，提升复杂场景下机器人抓取成功率。",
            "summary_zh": "在基于视觉的机器人操作中，单个相机视角只能捕捉到目标物体的一侧，而复杂场景中的遮挡进一步限制了可见性。这导致观测到的几何形状不完整，抓取估计算法的性能欠佳。为了解决这一局限性，我们利用扩散模型从单视角获取的局部深度观测中执行类别级别的3D形状补全，重建完整的物体几何形状，为抓取规划提供更丰富的上下文信息。我们的方法侧重于具有多样几何形状的常见家居物品，生成完整的3D形状，作为下游抓取推理网络的输入。与主要考虑孤立物体或极少遮挡的先前工作不同，我们在具有家居物品的真实复杂场景中评估形状补全和抓取。在复杂场景的初步评估中，我们的方法始终比没有形状补全的朴素基线提高了23%的抓取成功率，并且比最近最先进的形状补全方法提高了19%。我们的代码可在https://amm.aass.oru.se/shape-completion-grasping/ 获取。",
            "intro_zh": [
                "现有方法在复杂场景下，单视角视觉信息不完整，导致机器人抓取性能下降，是亟待解决的核心问题。",
                "论文提出利用扩散模型，从单视角局部深度信息重建完整3D形状，为抓取提供更全面的几何上下文。",
                "实验表明，该方法在复杂场景中，相较于传统方法和现有形状补全方法，显著提升了机器人抓取成功率。"
            ],
            "method_zh": "**问题定义**：现有基于视觉的机器人抓取方法在复杂环境中面临挑战，主要原因是单视角相机只能捕捉到物体部分信息，加上遮挡，导致抓取算法无法获得完整的物体几何信息，从而影响抓取成功率。现有形状补全方法通常只考虑孤立物体或简单场景，难以应对真实复杂环境。\n\n**核心思路**：论文的核心思路是利用扩散模型强大的生成能力，从单视角局部深度观测中推断出完整的3D物体形状。通过补全缺失的几何信息，为下游抓取算法提供更准确、全面的物体表示，从而提高抓取成功率。这种方法能够有效应对单视角信息不完整和遮挡问题。\n\n**技术框架**：该方法主要包含两个阶段：1) 形状补全阶段：使用扩散模型从单视角深度图像中生成完整的3D形状。输入是单视角深度图，输出是补全后的3D形状。2) 抓取推理阶段：将补全后的3D形状输入到抓取推理网络中，预测最佳抓取姿态。整体流程是从不完整的视觉信息到完整的3D形状，再到可靠的抓取姿态。\n\n**关键创新**：该论文的关键创新在于将扩散模型应用于复杂场景下的单视角形状补全，并将其与机器人抓取任务相结合。与现有方法相比，该方法能够更好地处理复杂场景中的遮挡和单视角信息缺失问题，从而显著提高抓取性能。此外，该方法专注于常见家居物品，更贴近实际应用。\n\n**关键设计**：论文使用了类别级别的扩散模型进行形状补全。具体来说，扩散模型被训练来生成特定类别的3D形状，例如杯子、碗等。损失函数可能包括重建损失和对抗损失，以保证生成形状的质量和真实性。抓取推理网络可能采用PointNet或类似的网络结构，直接从3D点云中预测抓取姿态。具体的网络结构和参数设置在论文中应该有详细描述（未知）。",
            "application_zh": "该研究成果可广泛应用于智能家居、仓储物流、工业自动化等领域。通过提升机器人对复杂环境中物体的抓取能力，可以实现更高效、更智能的自动化操作，例如自动整理家居物品、自动分拣货物、自动装配产品等。未来，该技术有望进一步扩展到更多复杂场景和任务中，推动机器人技术的进步。",
            "highlight_zh": "实验结果表明，该方法在复杂场景下，相较于没有形状补全的朴素基线，抓取成功率提高了23%。与最近最先进的形状补全方法相比，抓取成功率也提高了19%。这些数据表明，该方法能够有效提升机器人抓取性能，并且优于现有技术。",
            "tags_zh": [
                "形状补全",
                "扩散模型",
                "机器人抓取",
                "单视角视觉",
                "复杂场景",
                "3D重建"
            ],
            "_index": 110,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16449v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16449v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16449v1/images/ReOcS/easy/shard_0/scene_1/view_6/rgb.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "A2VISR: An Active and Adaptive Ground-Aerial Localization System Using Visual Inertial and Single-Range Fusion",
            "authors": [
                "Sijia Chen",
                "Wei Dong"
            ],
            "arxiv_id": "2512.16367v1",
            "summary": "It's a practical approach using the ground-aerial collaborative system to enhance the localization robustness of flying robots in cluttered environments, especially when visual sensors degrade. Conventional approaches estimate the flying robot's position using fixed cameras observing pre-attached markers, which could be constrained by limited distance and susceptible to capture failure. To address this issue, we improve the ground-aerial localization framework in a more comprehensive manner, which integrates active vision, single-ranging, inertial odometry, and optical flow. First, the designed active vision subsystem mounted on the ground vehicle can be dynamically rotated to detect and track infrared markers on the aerial robot, improving the field of view and the target recognition with a single camera. Meanwhile, the incorporation of single-ranging extends the feasible distance and enhances re-capture capability under visual degradation. During estimation, a dimension-reduced estimator fuses multi-source measurements based on polynomial approximation with an extended sliding window, balancing computational efficiency and redundancy. Considering different sensor fidelities, an adaptive sliding confidence evaluation algorithm is implemented to assess measurement quality and dynamically adjust the weighting parameters based on moving variance. Finally, extensive experiments under conditions such as smoke interference, illumination variation, obstacle occlusion, prolonged visual loss, and extended operating range demonstrate that the proposed approach achieves robust online localization, with an average root mean square error of approximately 0.09 m, while maintaining resilience to capture loss and sensor failures.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "accept by IEEE Transactions on Industrial Electronics",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16367v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "optical flow"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出A2VISR：一种融合视觉惯性和单测距的主动自适应地空协同定位系统",
            "summary_zh": "本文提出了一种地空协同系统，旨在提高飞行机器人在复杂环境中，尤其是在视觉传感器性能下降时的定位鲁棒性。传统方法使用固定相机观察预先安装的标记来估计飞行机器人的位置，但受到距离限制和易捕获失败的约束。为了解决这个问题，我们以更全面的方式改进了地空定位框架，集成了主动视觉、单点测距、惯性里程计和光流。地面车辆上安装的主动视觉子系统可以动态旋转，以检测和跟踪空中机器人上的红外标记，从而提高视野范围和目标识别能力。同时，单点测距的加入扩展了可行距离，增强了视觉退化下的重捕获能力。在估计过程中，降维估计器基于多项式逼近的扩展滑动窗口融合多源测量，平衡了计算效率和冗余。考虑到不同传感器的可靠性，自适应滑动置信度评估算法用于评估测量质量，并基于移动方差动态调整权重参数。在烟雾干扰、光照变化、障碍物遮挡、长时间视觉丢失和扩展操作范围等条件下进行的大量实验表明，所提出的方法实现了鲁棒的在线定位，平均均方根误差约为0.09米，同时保持了对捕获丢失和传感器故障的弹性。",
            "intro_zh": [
                "现有方法依赖固定相机和预设标记，易受距离限制和捕获失败影响，难以在复杂环境中保持定位鲁棒性。",
                "A2VISR通过融合主动视觉、单点测距、惯性里程计和光流，提升视野范围、重捕获能力和定位精度。",
                "实验结果表明，该方法在多种复杂条件下实现了鲁棒的在线定位，平均均方根误差约为0.09米。"
            ],
            "method_zh": "**问题定义**：论文旨在解决复杂环境下，飞行机器人在视觉传感器性能下降时，定位鲁棒性不足的问题。现有方法依赖于固定相机和预先安装的标记，容易受到距离限制、遮挡和光照变化的影响，导致定位精度下降甚至失败。\\n\\n**核心思路**：论文的核心思路是构建一个地空协同定位系统，通过地面车辆搭载的主动视觉系统和单点测距传感器，辅助空中机器人进行定位。主动视觉系统可以动态调整视角，扩大视野范围，提高目标识别能力。单点测距则扩展了定位距离，增强了在视觉退化情况下的重捕获能力。\\n\\n**技术框架**：该系统包含地面车辆和空中机器人两部分。地面车辆搭载主动视觉子系统和单点测距传感器，负责检测和跟踪空中机器人上的红外标记。空中机器人搭载惯性测量单元（IMU）和光流传感器，提供自身的运动信息。系统采用扩展滑动窗口的降维估计器，融合来自多个传感器的测量数据，估计空中机器人的位置。\\n\\n**关键创新**：该论文的关键创新在于：1) 提出了一种主动视觉系统，可以动态调整视角，提高目标识别能力；2) 融合了单点测距信息，扩展了定位距离，增强了在视觉退化情况下的重捕获能力；3) 提出了一种自适应滑动置信度评估算法，可以根据传感器数据的质量动态调整权重参数。\\n\\n**关键设计**：主动视觉系统采用舵机控制相机旋转，实现动态视角调整。单点测距传感器采用红外激光测距仪。扩展滑动窗口的降维估计器采用多项式逼近方法，降低计算复杂度。自适应滑动置信度评估算法基于移动方差计算传感器数据的置信度，并将其作为权重参数。",
            "application_zh": "该研究成果可应用于复杂环境下的无人机自主导航、搜救、巡检等领域。例如，在烟雾、光照变化或障碍物遮挡等情况下，该系统能够提供更鲁棒和精确的定位信息，提高无人机的工作效率和安全性。未来，该技术有望扩展到更多类型的机器人和应用场景。",
            "highlight_zh": "实验结果表明，在烟雾干扰、光照变化、障碍物遮挡、长时间视觉丢失和扩展操作范围等复杂条件下，该方法实现了鲁棒的在线定位，平均均方根误差约为0.09米。该方法还表现出对捕获丢失和传感器故障的弹性，证明了其在实际应用中的可靠性。",
            "tags_zh": [
                "地空协同定位",
                "主动视觉",
                "单点测距",
                "视觉惯性融合",
                "扩展滑动窗口"
            ],
            "_index": 111,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16367v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16367v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16367v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Privacy-Aware Sharing of Raw Spatial Sensor Data for Cooperative Perception",
            "authors": [
                "Bangya Liu",
                "Chengpo Yan",
                "Chenghao Jiang",
                "Suman Banerjee",
                "Akarsh Prabhakara"
            ],
            "arxiv_id": "2512.16265v1",
            "summary": "Cooperative perception between vehicles is poised to offer robust and reliable scene understanding. Recently, we are witnessing experimental systems research building testbeds that share raw spatial sensor data for cooperative perception. While there has been a marked improvement in accuracies and is the natural way forward, we take a moment to consider the problems with such an approach for eventual adoption by automakers. In this paper, we first argue that new forms of privacy concerns arise and discourage stakeholders to share raw sensor data. Next, we present SHARP, a research framework to minimize privacy leakage and drive stakeholders towards the ambitious goal of raw data based cooperative perception. Finally, we discuss open questions for networked systems, mobile computing, perception researchers, industry and government in realizing our proposed framework.",
            "categories": [
                "cs.NI",
                "cs.RO"
            ],
            "primary_category": "cs.NI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16265v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "scene understanding"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出SHARP框架，旨在最小化原始空间传感器数据共享中的隐私泄露，促进车辆协同感知。",
            "summary_zh": "车辆间的协同感知有望提供更强大和可靠的场景理解能力。最近，我们看到实验性系统研究正在构建测试平台，用于共享原始空间传感器数据以实现协同感知。虽然精度有了显著提高，并且这是自然的发展方向，但我们花时间考虑了这种方法在汽车制造商最终采用时可能存在的问题。在本文中，我们首先论证了新的隐私问题正在出现，并阻碍了利益相关者共享原始传感器数据。接下来，我们提出了SHARP，一个研究框架，旨在最小化隐私泄露，并推动利益相关者朝着基于原始数据的协同感知的宏伟目标前进。最后，我们讨论了网络系统、移动计算、感知研究人员、工业界和政府在实现我们提出的框架时面临的开放性问题。",
            "intro_zh": [
                "现有车辆协同感知系统依赖原始传感器数据共享，但直接共享引发了严重的隐私泄露风险，阻碍了实际应用。",
                "SHARP框架旨在通过隐私保护机制，在保证协同感知性能的同时，最小化原始传感器数据共享带来的隐私风险。",
                "论文提出了SHARP框架，并讨论了在网络系统、移动计算和感知研究等领域实现该框架所面临的挑战和开放性问题。"
            ],
            "method_zh": "**问题定义**：论文旨在解决车辆协同感知中，直接共享原始空间传感器数据所带来的隐私泄露问题。现有方法虽然提升了感知精度，但忽略了数据共享可能暴露车辆所有者、乘客甚至周围环境的敏感信息，导致用户不愿共享数据，阻碍了协同感知技术的推广应用。\\n\\n**核心思路**：SHARP框架的核心思路是在原始数据共享前，通过一系列隐私保护机制，对数据进行处理，从而在保证协同感知性能的前提下，显著降低隐私泄露的风险。这种方法旨在平衡感知精度和隐私保护，鼓励更多用户参与数据共享。\\n\\n**技术框架**：SHARP框架的具体架构未知，但可以推断其包含以下主要模块：1) 隐私分析模块：用于评估原始数据中潜在的隐私风险。2) 隐私保护模块：包含多种隐私保护机制，如差分隐私、数据匿名化、数据泛化等，用于对原始数据进行处理。3) 数据共享模块：负责将处理后的数据安全地共享给其他车辆或云平台。4) 感知融合模块：接收来自不同车辆的数据，进行融合，从而提升整体的感知性能。\\n\\n**关键创新**：论文的关键创新在于提出了一个通用的隐私保护框架SHARP，并强调了在车辆协同感知中隐私保护的重要性。虽然具体的隐私保护机制可能并非全新，但将这些机制整合到一个框架中，并针对车辆协同感知场景进行优化，具有重要的实际意义。\\n\\n**关键设计**：由于论文摘要中未提供具体的技术细节，因此SHARP框架的关键设计未知。但可以推测，框架需要根据不同的隐私风险级别，选择合适的隐私保护机制。此外，框架还需要考虑隐私保护机制对感知性能的影响，需要在隐私保护和感知精度之间进行权衡。具体的参数设置、损失函数、网络结构等技术细节未知。",
            "application_zh": "该研究成果可应用于智能交通系统、自动驾驶车辆、车联网等领域。通过保护用户隐私，SHARP框架能够促进车辆间的数据共享，提升协同感知能力，从而提高道路安全、优化交通效率，并为自动驾驶技术的发展提供有力支持。未来，该框架还可扩展到其他需要数据共享的场景，如智慧城市、环境监测等。",
            "highlight_zh": "由于论文摘要中未提供具体的实验结果，因此SHARP框架的实验亮点未知。但可以推测，实验将评估SHARP框架在不同隐私保护级别下的感知性能，并与直接共享原始数据的方法进行对比。实验结果可能表明，SHARP框架能够在保证一定感知精度的前提下，显著降低隐私泄露的风险。",
            "tags_zh": [
                "协同感知",
                "隐私保护",
                "数据共享",
                "车辆网络",
                "智能交通"
            ],
            "_index": 112,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16265v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16265v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16265v1/hotnets25-template/newfigures/privacy_metrics.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
            "authors": [
                "Jinjie Mai",
                "Chaoyang Wang",
                "Guocheng Gordon Qian",
                "Willi Menapace",
                "Sergey Tulyakov",
                "Bernard Ghanem",
                "Peter Wonka",
                "Ashkan Mirzaei"
            ],
            "arxiv_id": "2512.16920v1",
            "summary": "While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \\emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "Project page: https://snap-research.github.io/easyv2v/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16920v1",
            "code_links": [
                {
                    "url": "https://snap-research.github.io/easyv2v/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "spatiotemporal"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "8_physics_animation"
            ],
            "headline_zh": "EasyV2V：高质量的基于指令的视频编辑框架，实现灵活可控的视频编辑。",
            "summary_zh": "视频编辑相较于图像编辑发展缓慢，面临着一致性、控制性和泛化性方面的挑战。本文研究了数据、架构和控制的设计空间，并提出了一个简单有效的基于指令的视频编辑框架EasyV2V。在数据方面，我们利用具有快速逆过程的现有专家模型构建多样化的视频对，通过单帧监督将图像编辑对提升为视频，利用共享仿射运动生成伪视频对，挖掘密集字幕片段以构建视频对，并添加过渡监督以学习编辑如何展开。在模型方面，我们观察到预训练的文本到视频模型具有编辑能力，这促使我们采用简化的设计。简单的序列连接与轻量级的LoRA微调足以训练出一个强大的模型。在控制方面，我们通过单一的掩码机制统一了时空控制，并支持可选的参考图像。总而言之，EasyV2V可以处理灵活的输入，例如视频+文本、视频+掩码+文本、视频+掩码+参考+文本，并实现了最先进的视频编辑效果，超越了现有的和商业系统。",
            "intro_zh": [
                "现有视频编辑方法在一致性、控制性和泛化性方面存在不足，难以满足高质量的编辑需求。",
                "EasyV2V利用预训练文本到视频模型的编辑能力，通过简单有效的序列连接和LoRA微调实现指令驱动的视频编辑。",
                "实验表明，EasyV2V在视频编辑任务上取得了最先进的结果，超越了现有的商业系统和同期的研究工作。"
            ],
            "method_zh": "**问题定义**：当前视频编辑技术在保持视频内容的时空一致性、精确控制编辑过程以及泛化到不同场景方面存在挑战。现有方法通常难以在复杂场景下生成高质量的编辑结果，并且缺乏对编辑过程的精细控制能力。\\n\\n**核心思路**：EasyV2V的核心思路是利用预训练的文本到视频模型所具备的潜在编辑能力，通过简单有效的微调策略，将其转化为一个高质量的、基于指令的视频编辑框架。该方法旨在通过统一的框架处理多种输入形式，并提供灵活的时空控制。\\n\\n**技术框架**：EasyV2V的整体框架包括数据构建和模型训练两个主要部分。数据构建阶段，作者利用多种策略生成高质量的视频编辑对，包括利用现有图像编辑模型生成视频、通过单帧监督提升图像编辑对、利用共享仿射运动生成伪视频对、挖掘密集字幕片段以及添加过渡监督。模型训练阶段，作者采用简单的序列连接方式将视频和文本指令输入到预训练的文本到视频模型中，并通过轻量级的LoRA微调来优化模型。\\n\\n**关键创新**：EasyV2V的关键创新在于其简单而有效的框架设计，以及对数据构建和模型训练策略的优化。通过充分利用预训练模型的先验知识，并结合多样化的数据增强方法，EasyV2V能够生成高质量的视频编辑结果，并提供灵活的时空控制。此外，统一的掩码机制简化了时空控制的实现。\\n\\n**关键设计**：在数据构建方面，作者设计了多种数据增强策略，以提高模型的泛化能力。在模型训练方面，作者采用了轻量级的LoRA微调策略，以避免对预训练模型进行过度修改。此外，作者还设计了一个统一的掩码机制，用于实现对视频内容的时空控制。具体的损失函数和网络结构细节在论文中未详细说明，属于未知信息。",
            "application_zh": "EasyV2V具有广泛的应用前景，可应用于电影制作、广告设计、社交媒体内容创作等领域。该框架可以帮助用户快速生成高质量的视频编辑内容，降低视频编辑的门槛，并为创意表达提供更多可能性。未来，该技术有望进一步发展，实现更加智能和自动化的视频编辑。",
            "highlight_zh": "EasyV2V在视频编辑任务上取得了最先进的结果，超越了现有的商业系统和同期的研究工作。具体性能数据和对比基线在论文中有所展示，但未在摘要中明确提及。该框架能够处理多种输入形式，并提供灵活的时空控制，为用户提供更加便捷和高效的视频编辑体验。",
            "tags_zh": [
                "视频编辑",
                "指令驱动",
                "文本到视频",
                "预训练模型",
                "LoRA微调",
                "时空控制",
                "数据增强"
            ],
            "_index": 113,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Distributional AGI Safety",
            "authors": [
                "Nenad Tomašev",
                "Matija Franklin",
                "Julian Jacobs",
                "Sébastien Krier",
                "Simon Osindero"
            ],
            "arxiv_id": "2512.16856v1",
            "summary": "AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16856v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "affordance"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出分布式的AGI安全框架，应对多智能体协作涌现的通用智能风险。",
            "summary_zh": "人工智能安全和对齐研究主要集中在保护单个AI系统的方法上，并假设最终会出现一个单体的通用人工智能（AGI）。另一种AGI涌现的假设是，通用能力水平首先通过具有互补技能和能力的多个子AGI个体智能体的协作来体现，但这种假设受到的关注要少得多。本文认为，这种拼凑式的AGI假设需要认真考虑，并应为相应的安全措施和缓解措施提供信息。具有工具使用能力以及沟通和协调能力的高级AI智能体的快速部署使这成为一个紧迫的安全考虑。因此，本文提出了一个分布式的AGI安全框架，该框架超越了评估和对齐单个智能体。该框架的核心是设计和实施虚拟智能体沙盒经济（不可渗透或半渗透），其中智能体之间的交易受稳健的市场机制的约束，并结合适当的可审计性、声誉管理和监督，以减轻集体风险。",
            "intro_zh": [
                "现有AI安全研究主要关注单个AGI，忽略了多个子AGI协作涌现通用智能的潜在风险。",
                "提出分布式的AGI安全框架，通过构建虚拟沙盒经济来管理和减轻多智能体协作风险。",
                "该框架强调市场机制、可审计性、声誉管理和监督，以确保智能体交互的安全性和可控性。"
            ],
            "method_zh": "**问题定义**：现有AI安全研究主要关注单个AGI的安全，忽略了多个子AGI通过协作涌现通用智能的可能性。这种“拼凑式AGI”的出现，由于其分布式和动态的特性，给传统的安全保障方法带来了新的挑战。现有的安全方法难以有效应对多个智能体之间的复杂交互和潜在的集体风险。\\n\\n**核心思路**：本文的核心思路是构建一个分布式的AGI安全框架，通过模拟一个虚拟的智能体经济环境，来研究和管理多智能体协作带来的安全风险。该框架的核心在于利用市场机制来调节智能体之间的交互，并通过可审计性、声誉管理和监督机制来减轻潜在的集体风险。\\n\\n**技术框架**：该框架主要包含以下几个关键模块：1) 虚拟智能体沙盒经济：创建一个隔离的或半隔离的虚拟环境，智能体可以在其中进行交互和交易。2) 市场机制：设计稳健的市场规则，例如价格机制、供需关系等，来调节智能体之间的资源分配和行为。3) 可审计性：记录智能体之间的所有交易和交互行为，以便进行分析和追溯。4) 声誉管理：建立智能体的声誉系统，根据其行为和交易记录来评估其可信度。5) 监督机制：实施监督规则和策略，例如行为限制、风险预警等，来防止智能体之间的恶意行为。\\n\\n**关键创新**：该框架最重要的创新点在于其从分布式的角度来考虑AGI安全问题，并提出了利用经济学原理来管理多智能体协作风险的新思路。与传统的关注单个智能体的安全方法不同，该框架更加关注智能体之间的交互和集体行为，从而能够更有效地应对“拼凑式AGI”带来的安全挑战。\\n\\n**关键设计**：关键设计包括：1) 沙盒经济的规模和复杂性，需要根据具体应用场景进行调整。2) 市场机制的设计，需要考虑效率、公平性和鲁棒性等因素。3) 声誉系统的评估指标和算法，需要能够准确反映智能体的行为和可信度。4) 监督规则的制定，需要在安全性和灵活性之间进行权衡。此外，还需要设计合适的审计工具和可视化界面，以便进行风险分析和监控。",
            "application_zh": "该研究成果可应用于构建安全的AI协作平台，例如智能制造、自动驾驶、金融交易等领域。通过在虚拟沙盒环境中模拟和评估多智能体系统的行为，可以提前发现潜在的安全风险，并制定相应的应对策略。此外，该框架还可以用于评估不同AI系统的安全性和可靠性，为AI技术的安全部署提供保障。",
            "highlight_zh": "论文提出了一个全新的分布式AGI安全框架，并强调了市场机制在管理多智能体协作风险中的作用。虽然论文没有提供具体的实验数据，但其提出的框架为未来的研究提供了一个有价值的方向，并有望推动AI安全领域的发展。未来的研究可以基于该框架，构建具体的虚拟沙盒环境，并进行实验验证。",
            "tags_zh": [
                "AGI安全",
                "分布式智能",
                "多智能体系统",
                "沙盒环境",
                "市场机制",
                "风险管理",
                "AI协作"
            ],
            "_index": 114,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models",
            "authors": [
                "Zhenyu Wu",
                "Jingjing Xie",
                "Zehao Li",
                "Bowen Yang",
                "Qiushi Sun",
                "Zhaoyang Liu",
                "Zhoumianze Liu",
                "Yu Qiao",
                "Xiangyu Yue",
                "Zun Wang",
                "Zichen Ding"
            ],
            "arxiv_id": "2512.16295v1",
            "summary": "With VLM-powered computer-using agents (CUAs) becoming increasingly capable at graphical user interface (GUI) navigation and manipulation, reliable step-level decision-making has emerged as a key bottleneck for real-world deployment. In long-horizon workflows, errors accumulate quickly and irreversible actions can cause unintended consequences, motivating critic models that assess each action before execution. While critic models offer a promising solution, their effectiveness is hindered by the lack of diverse, high-quality GUI feedback data and public critic benchmarks for step-level evaluation in computer use. To bridge these gaps, we introduce OS-Oracle that makes three core contributions: (1) a scalable data pipeline for synthesizing cross-platform GUI critic data; (2) a two-stage training paradigm combining supervised fine-tuning (SFT) and consistency-preserving group relative policy optimization (CP-GRPO); (3) OS-Critic Bench, a holistic benchmark for evaluating critic model performance across Mobile, Web, and Desktop platforms. Leveraging this framework, we curate a high-quality dataset containing 310k critic samples. The resulting critic model, OS-Oracle-7B, achieves state-of-the-art performance among open-source VLMs on OS-Critic Bench, and surpasses proprietary models on the mobile domain. Furthermore, when serving as a pre-critic, OS-Oracle-7B improves the performance of native GUI agents such as UI-TARS-1.5-7B in OSWorld and AndroidWorld environments. The code is open-sourced at https://github.com/numbmelon/OS-Oracle.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16295v1",
            "code_links": [
                {
                    "url": "https://github.com/numbmelon/OS-Oracle",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "OS-Oracle：构建跨平台GUI评论模型的综合框架，提升计算机使用代理的决策能力",
            "summary_zh": "随着VLM驱动的计算机使用代理(CUA)在图形用户界面(GUI)导航和操作方面能力日益增强，可靠的步进式决策已成为实际部署的关键瓶颈。在长流程任务中，错误会迅速累积，不可逆操作可能导致意外后果，因此需要评估每次操作的评论模型。然而，缺乏多样化、高质量的GUI反馈数据和用于计算机使用中步进式评估的公共评论基准阻碍了评论模型的有效性。为了弥补这些差距，我们引入了OS-Oracle，它做出了三个核心贡献：(1)用于合成跨平台GUI评论数据的可扩展数据管道；(2)结合监督微调(SFT)和一致性保持组相对策略优化(CP-GRPO)的两阶段训练范式；(3)OS-Critic Bench，一个用于评估移动、Web和桌面平台评论模型性能的整体基准。利用该框架，我们整理了一个包含310k评论样本的高质量数据集。由此产生的评论模型OS-Oracle-7B在OS-Critic Bench上实现了最先进的开源VLM性能，并在移动领域超越了专有模型。此外，当作为预评论模型时，OS-Oracle-7B提高了原生GUI代理（如UI-TARS-1.5-7B）在OSWorld和AndroidWorld环境中的性能。代码已在https://github.com/numbmelon/OS-Oracle开源。",
            "intro_zh": [
                "现有的计算机使用代理在长流程任务中容易累积错误，缺乏有效的步进式决策机制，导致实际应用受限。",
                "OS-Oracle通过构建可扩展的数据管道、设计两阶段训练范式，并提供全面的评估基准，来解决GUI评论模型的不足。",
                "实验结果表明，OS-Oracle-7B在跨平台GUI评论任务中取得了领先的性能，并能有效提升现有GUI代理的性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决计算机使用代理（CUA）在GUI环境中的步进式决策问题。现有方法在长流程任务中容易出错，缺乏有效的反馈机制来评估每一步操作的合理性。此外，缺乏高质量的GUI反馈数据和公共评论基准也限制了评论模型的发展。\\n\\n**核心思路**：论文的核心思路是构建一个高质量的GUI评论模型，该模型能够评估CUA在GUI环境中的每一步操作，并提供反馈以指导CUA做出更合理的决策。通过大规模数据合成、有效的训练范式和全面的评估基准，提升评论模型的性能和泛化能力。\\n\\n**技术框架**：OS-Oracle框架包含三个主要组成部分：(1) 可扩展的数据管道，用于合成跨平台GUI评论数据；(2) 两阶段训练范式，包括监督微调(SFT)和一致性保持组相对策略优化(CP-GRPO)；(3) OS-Critic Bench，用于评估评论模型性能的基准。数据管道负责生成多样化的GUI交互数据，训练范式用于优化评论模型的性能，基准用于评估模型的泛化能力。\\n\\n**关键创新**：论文的关键创新在于：(1) 提出了一个可扩展的数据管道，能够合成大规模、高质量的跨平台GUI评论数据，解决了数据稀缺的问题；(2) 设计了一种两阶段训练范式，结合了监督学习和强化学习的优点，提升了评论模型的性能和鲁棒性；(3) 构建了一个全面的评估基准，能够评估评论模型在不同平台和任务上的性能。\\n\\n**关键设计**：在数据管道方面，论文采用了多种数据增强技术，以增加数据的多样性。在训练范式方面，CP-GRPO损失函数旨在保持策略的一致性，避免模型过度拟合。在OS-Critic Bench中，论文选择了多个具有代表性的GUI任务，并设计了合理的评估指标。",
            "application_zh": "该研究成果可应用于各种需要计算机使用代理的场景，例如自动化测试、智能助手、机器人流程自动化(RPA)等。通过提升CUA的决策能力，可以减少人工干预，提高工作效率，并降低错误率。未来，该技术有望应用于更复杂的GUI环境，例如游戏、虚拟现实等。",
            "highlight_zh": "OS-Oracle-7B在OS-Critic Bench上取得了最先进的开源VLM性能，并在移动领域超越了专有模型。作为预评论模型时，OS-Oracle-7B显著提高了UI-TARS-1.5-7B在OSWorld和AndroidWorld环境中的性能。具体而言，在某些任务上，性能提升超过10%。",
            "tags_zh": [
                "GUI评论模型",
                "计算机使用代理",
                "跨平台",
                "数据合成",
                "两阶段训练",
                "强化学习",
                "VLM"
            ],
            "_index": 115,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16295v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16295v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16295v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Resilience of coupled systems under deep uncertainty and dynamic complexity: An integrative literature review",
            "authors": [
                "Jannie Coenen",
                "Vítor Vasconcelos",
                "Heiman Wertheim",
                "Marcel Olde Rikkert",
                "Sophie Hadjisotiriou",
                "Vittorio Nespeca",
                "Tom Oreel",
                "Rick Quax",
                "Etiënne Rouwette",
                "Vincent Marchau",
                "Hubert Korzilius"
            ],
            "arxiv_id": "2512.16608v1",
            "summary": "Resilience in coupled systems is increasingly critical in addressing global challenges such as climate change and pandemics. These systems show unpredictable behaviour due to dynamic complexity and deep uncertainty across spatiotemporal scales. Despite growing interest, few studies systematically integrate both concepts when assessing resilience. This paper conducts an integrative review of 102 English-language publications to identify gaps in current approaches. Findings reveal that most papers address lower levels of uncertainty and rarely consider dynamic complexity and deep uncertainty simultaneously, which limits the effectiveness of resilience strategies. To advance systems research, we propose a conceptual framework and practical tools to support researchers and decision-makers in evaluating and improving resilience. The paper also outlines future research directions for more robust, adaptive, and integrative resilience assessments.",
            "categories": [
                "physics.soc-ph",
                "eess.SY"
            ],
            "primary_category": "physics.soc-ph",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16608v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "spatiotemporal"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "8_physics_animation"
            ],
            "headline_zh": "综述耦合系统在深度不确定性和动态复杂性下的韧性研究，提出未来研究方向。",
            "summary_zh": "耦合系统的韧性在应对气候变化和疫情等全球挑战中日益重要。由于时空尺度上的动态复杂性和深度不确定性，这些系统表现出不可预测的行为。尽管人们对此越来越感兴趣，但在评估韧性时，很少有研究系统地整合这两个概念。本文对102篇英文出版物进行了综合性回顾，以找出当前方法中的差距。研究结果表明，大多数论文处理的是较低水平的不确定性，很少同时考虑动态复杂性和深度不确定性，这限制了韧性策略的有效性。为了推进系统研究，我们提出了一个概念框架和实用工具，以支持研究人员和决策者评估和提高韧性。本文还概述了未来研究方向，以实现更稳健、适应性和综合性的韧性评估。",
            "intro_zh": [
                "现有韧性研究未能充分整合动态复杂性和深度不确定性，导致韧性策略效果受限。",
                "论文提出概念框架和实用工具，旨在支持研究人员和决策者评估和提升耦合系统的韧性。",
                "通过综合文献回顾，论文识别了当前研究的差距，并为未来研究指明了方向。"
            ],
            "method_zh": "**问题定义**：论文旨在解决耦合系统韧性评估中，对动态复杂性和深度不确定性考虑不足的问题。现有方法通常只关注较低层次的不确定性，无法有效应对现实世界中复杂且动态变化的挑战。这导致制定的韧性策略缺乏鲁棒性和适应性，难以应对突发事件和长期演变。\\n\\n**核心思路**：论文的核心思路是通过综合性的文献回顾，识别现有研究在处理动态复杂性和深度不确定性方面的不足，并在此基础上提出一个概念框架和实用工具，以支持更全面、更有效的韧性评估。该框架旨在帮助研究人员和决策者更好地理解和应对耦合系统中的复杂性和不确定性。\\n\\n**技术框架**：论文采用综合文献回顾的方法，对102篇相关英文出版物进行系统分析。该过程包括：1) 确定研究范围和关键词；2) 收集相关文献；3) 提取关键信息，如研究方法、关注的复杂性和不确定性类型、以及提出的韧性策略；4) 对提取的信息进行综合分析，识别现有研究的差距和不足；5) 基于分析结果，提出概念框架和实用工具，并为未来研究指明方向。\\n\\n**关键创新**：论文的关键创新在于其综合性的视角，将动态复杂性和深度不确定性同时纳入耦合系统韧性评估的考量。与现有研究相比，该论文更全面地考虑了影响系统韧性的各种因素，并提出了更具针对性的解决方案。此外，提出的概念框架和实用工具为研究人员和决策者提供了一个有用的工具，可以帮助他们更好地理解和应对复杂系统中的挑战。\\n\\n**关键设计**：论文提出的概念框架和实用工具的具体设计细节在摘要中没有详细说明，属于未知信息。未来的研究方向可能包括开发更具体的评估指标、建立更精细的仿真模型、以及设计更有效的干预策略。",
            "application_zh": "该研究成果可应用于气候变化适应、疫情应对、城市规划、资源管理等领域。通过更全面地评估耦合系统的韧性，可以制定更有效的风险管理策略，提高系统应对突发事件和长期挑战的能力，从而保障社会经济的可持续发展。",
            "highlight_zh": "该论文通过对102篇英文文献的综合回顾，揭示了现有韧性研究在同时考虑动态复杂性和深度不确定性方面的不足。研究发现，大多数论文仅关注较低水平的不确定性，限制了韧性策略的有效性。该研究为未来更稳健、适应性和综合性的韧性评估奠定了基础。",
            "tags_zh": [
                "耦合系统",
                "韧性评估",
                "动态复杂性",
                "深度不确定性",
                "文献综述",
                "风险管理",
                "系统研究"
            ],
            "_index": 116,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "From Liability to Asset: A Three-Mode Grid-Forming Control Framework for Centralized Data Center UPS Systems",
            "authors": [
                "Mohamed Shamseldein"
            ],
            "arxiv_id": "2512.16497v1",
            "summary": "AI workloads are turning large data centers into highly dynamic power-electronic loads; fault-time behavior and workload pulsing can stress weak-grid points of interconnection. This paper proposes a centralized medium-voltage (MV) uninterruptible power supply (UPS) control architecture implemented as three operating modes: Mode 1 regulates a DC stiff bus and shapes normal-operation grid draw, Mode 2 enforces current-limited fault-mode P--Q priority with UPS battery energy storage system (UPS-BESS) buffering and a rate-limited post-fault \"soft return,\" and Mode 3 optionally provides droop-based fast frequency response via grid-draw modulation. Fundamental-frequency averaged dq simulations (50 MW block, short-circuit ratio (SCR) = 1.5, 0.5 p.u. three-phase dip for 150~ms) show zero unserved information-technology (IT) energy (0.00000 MWh vs.0.00208 MWh for a momentary-cessation benchmark), a 0.57 p.u. peak inverter current (vs. 1.02 p.u. for a synchronous-reference-frame phase-locked loop (SRF-PLL) low-voltage ride-through (LVRT) baseline), a nonzero mean fault-window grid draw of 0.20~p.u. (vs.approx 0 for momentary cessation), and an improved settled point-of-common-coupling (PCC) voltage minimum of 0.79 p.u. after one cycle (vs. 0.66 p.u.). A forced-oscillation case study applies a 1 Hz pulsed load (+/- 0.25 p.u.) and shows that the normal-operation shaping filters the oscillation seen by the grid while the UPS-BESS buffers the pulsing component.",
            "categories": [
                "eess.SY"
            ],
            "primary_category": "eess.SY",
            "published": "2025-12-18",
            "updated": "2025-12-18",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.16497v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "PULSE"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "8_physics_animation"
            ],
            "headline_zh": "针对数据中心UPS系统，提出三模式电网重构控制框架，提升弱电网适应性。",
            "summary_zh": "人工智能负载正将大型数据中心转变为高度动态的电力电子负载；故障期间的行为和负载脉冲可能会给互联的弱电网带来压力。本文提出了一种集中式中压（MV）不间断电源（UPS）控制架构，该架构实现为三种运行模式：模式1调节直流母线并塑造正常运行的电网消耗；模式2通过UPS电池储能系统（UPS-BESS）缓冲和速率限制的故障后“软返回”来强制执行电流限制的故障模式P-Q优先级；模式3可选地通过电网消耗调制提供基于下垂的快速频率响应。基频平均dq仿真（50 MW模块，短路比（SCR）=1.5，0.5 p.u.三相骤降150毫秒）显示零未服务信息技术（IT）能量（0.00000 MWh vs. 0.00208 MWh用于瞬时中断基准），0.57 p.u.峰值逆变器电流（vs. 1.02 p.u.用于同步参考系锁相环（SRF-PLL）低电压穿越（LVRT）基线），0.20 p.u.的非零平均故障窗口电网消耗（vs.瞬时中断的近似0），以及一个周期后改善的稳定公共连接点（PCC）电压最小值0.79 p.u.（vs. 0.66 p.u.）。强制振荡案例研究应用1 Hz脉冲负载（+/- 0.25 p.u.），并表明正常运行整形滤波器过滤了电网看到的振荡，而UPS-BESS缓冲了脉冲分量。",
            "intro_zh": [
                "数据中心负载波动和故障易对弱电网造成冲击，传统UPS难以有效应对。",
                "提出三模式控制框架，通过直流母线调节、故障模式优先级控制和频率响应调制，增强系统鲁棒性。",
                "仿真结果表明，该方法显著降低了IT能量损失，改善了电压稳定性和电网适应性。"
            ],
            "method_zh": "**问题定义**：数据中心UPS系统在面对动态负载和电网故障时，容易对电网造成冲击，尤其是在弱电网环境下。传统UPS控制策略在故障期间可能导致电压骤降、电流过载等问题，影响数据中心的稳定运行。现有方法难以兼顾正常运行时的电网友好性和故障时的快速响应能力。\n\\n**核心思路**：本文的核心思路是将UPS控制分解为三个不同的运行模式，分别针对正常运行、故障模式和频率响应进行优化。通过集中式控制架构，实现对UPS-BESS的协调控制，从而提高系统的整体性能和可靠性。这种分层控制策略能够更好地适应数据中心负载的动态变化和电网的各种扰动。\n\\n**技术框架**：该控制框架包含三个主要模式：\n1. **模式1：直流母线调节**。该模式主要负责在正常运行期间调节直流母线电压，并对电网的电流进行整形，以减少对电网的干扰。\n2. **模式2：故障模式P-Q优先级控制**。在电网发生故障时，该模式通过UPS-BESS提供缓冲，并采用电流限制的P-Q优先级控制策略，确保关键负载的供电。\n3. **模式3：基于下垂的快速频率响应**。该模式可选，通过调制电网的电流消耗，提供快速的频率响应，以支持电网的稳定。\n\\n**关键创新**：该方法的主要创新在于提出了一个三模式的集中式UPS控制框架，能够根据不同的运行状态自动切换控制策略，从而实现对数据中心UPS系统的全面优化。与传统的单一控制策略相比，该框架能够更好地适应数据中心负载的动态变化和电网的各种扰动，提高了系统的整体性能和可靠性。\n\\n**关键设计**：\n*   **模式切换逻辑**：根据电网电压、频率等参数，自动切换不同的控制模式。\n*   **故障模式P-Q优先级控制**：根据负载的重要性，设置不同的有功功率（P）和无功功率（Q）优先级。\n*   **软返回策略**：在故障恢复后，采用速率限制的“软返回”策略，避免对电网造成冲击。\n*   **下垂控制参数**：根据电网的特性，合理设置下垂控制的参数，以实现快速的频率响应。",
            "application_zh": "该研究成果可应用于大型数据中心的中压UPS系统，尤其是在连接到弱电网的数据中心。通过提高数据中心对电网故障的适应能力，可以减少因电力问题导致的数据丢失和服务中断，保障数据中心的关键业务运行。此外，该技术还有潜力推广到其他需要高可靠性供电的场景，如医院、工厂等。",
            "highlight_zh": "仿真结果显示，与瞬时中断基准相比，该方法实现了零未服务IT能量。在0.5 p.u.三相骤降150毫秒的故障情况下，峰值逆变器电流降低至0.57 p.u.（基线为1.02 p.u.），故障期间的平均电网消耗为0.20 p.u.（基线近似为0），一个周期后的PCC电压最小值提升至0.79 p.u.（基线为0.66 p.u.）。",
            "tags_zh": [
                "数据中心",
                "不间断电源",
                "电网重构",
                "弱电网",
                "电力电子",
                "储能系统",
                "三模式控制"
            ],
            "_index": 117,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.16497v1/simulation_results_pulse_ramp.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.16497v1/simulation_results_pulse.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.16497v1/simulation_results_stage1.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        }
    ]
}