from typing import List, Tuple, Dict, Optional
from datetime import datetime, timedelta, timezone
from zoneinfo import ZoneInfo
import re
import json
import arxiv
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

US_EASTERN = ZoneInfo("US/Eastern")

def load_tags(tags_file: str) -> List[str]:
    with open(tags_file, 'r', encoding='utf-8') as f:
        tags = json.load(f)
    return tags['tags']


def load_interests(interests_file: str) -> Dict:
    """åŠ è½½ç”¨æˆ·æ„Ÿå…´è¶£çš„é¢†åŸŸé…ç½®"""
    try:
        with open(interests_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return None


def _keyword_match(keyword: str, text: str) -> bool:
    """
    æ™ºèƒ½å…³é”®è¯åŒ¹é…ï¼š
    - çŸ­å…³é”®è¯ï¼ˆ<=4å­—ç¬¦ï¼‰ä½¿ç”¨è¯è¾¹ç•ŒåŒ¹é…ï¼Œé¿å…è¯¯åŒ¹é…
    - é•¿å…³é”®è¯ä½¿ç”¨å­ä¸²åŒ¹é…
    - æ”¯æŒè¿å­—ç¬¦å’Œç©ºæ ¼çš„å˜ä½“åŒ¹é…
    """
    kw = keyword.lower()
    
    # ç”Ÿæˆå˜ä½“ï¼šè¿å­—ç¬¦ <-> ç©ºæ ¼
    variants = [kw]
    if '-' in kw:
        variants.append(kw.replace('-', ' '))
        variants.append(kw.replace('-', ''))
    if ' ' in kw:
        variants.append(kw.replace(' ', '-'))
        variants.append(kw.replace(' ', ''))
    
    for variant in variants:
        # çŸ­å…³é”®è¯ä½¿ç”¨è¯è¾¹ç•ŒåŒ¹é…ï¼ˆé¿å… "VO" åŒ¹é… "evolution"ï¼‰
        if len(variant) <= 4:
            pattern = r'\b' + re.escape(variant) + r'\b'
            if re.search(pattern, text, re.IGNORECASE):
                return True
        else:
            # é•¿å…³é”®è¯ä½¿ç”¨å­ä¸²åŒ¹é…
            if variant in text:
                return True
    
    return False


def _check_negative_keywords(title: str, abstract: str, negative_keywords: List[str]) -> Tuple[bool, List[str]]:
    """
    æ£€æŸ¥è®ºæ–‡æ˜¯å¦åŒ¹é…è´Ÿé¢å…³é”®è¯ï¼ˆä¸€ç¥¨å¦å†³ï¼‰
    è¿”å›: (æ˜¯å¦æ’é™¤, åŒ¹é…åˆ°çš„è´Ÿé¢å…³é”®è¯åˆ—è¡¨)
    """
    if not negative_keywords:
        return False, []
    
    text = f"{title} {abstract}".lower()
    matched_negatives = []
    
    for kw in negative_keywords:
        if _keyword_match(kw, text):
            matched_negatives.append(kw)
    
    return len(matched_negatives) > 0, matched_negatives


def _calculate_combination_bonus(title: str, abstract: str, combination_bonuses: List[Dict]) -> float:
    """
    è®¡ç®—ç»„åˆåŠ åˆ†
    å¦‚æœè®ºæ–‡åŒæ—¶åŒ¹é…å¤šä¸ªç›¸å…³æ¡ä»¶ï¼Œç»™äºˆé¢å¤–åŠ åˆ†
    """
    if not combination_bonuses:
        return 0.0
    
    text = f"{title} {abstract}".lower()
    total_bonus = 0.0
    
    for combo in combination_bonuses:
        conditions = combo.get("conditions", [])
        bonus = combo.get("bonus", 0.0)
        
        # æ£€æŸ¥æ‰€æœ‰æ¡ä»¶ç»„æ˜¯å¦éƒ½è‡³å°‘åŒ¹é…ä¸€ä¸ªå…³é”®è¯
        all_matched = True
        for condition_group in conditions:
            group_matched = False
            for kw in condition_group:
                if _keyword_match(kw, text):
                    group_matched = True
                    break
            if not group_matched:
                all_matched = False
                break
        
        if all_matched:
            total_bonus += bonus
    
    return total_bonus


def match_interests(paper: Dict, interests_config: Dict) -> Dict:
    """
    åŠ æƒæ‰“åˆ†åŒ¹é…ç³»ç»Ÿ
    
    ç‰¹ç‚¹ï¼š
    1. æ ‡é¢˜åŒ¹é…æƒé‡é«˜äºæ‘˜è¦
    2. ä¸åŒå…´è¶£é¢†åŸŸæœ‰ä¸åŒæƒé‡
    3. ç»„åˆå…³é”®è¯å¯è·å¾—é¢å¤–åŠ åˆ†
    4. è´Ÿé¢å…³é”®è¯ä¸€ç¥¨å¦å†³ï¼ˆé™¤éæ­£é¢åˆ†æ•°å¾ˆé«˜ï¼‰
    """
    if not interests_config:
        return {
            "matched_interests": [], 
            "relevance_score": 0.0, 
            "excluded": False, 
            "exclusion_keywords": [],
            "combination_bonus": 0.0
        }
    
    title = paper.get("title", "").lower()
    abstract = paper.get("summary", "").lower()
    
    # è·å–é…ç½®å‚æ•°
    scoring_config = interests_config.get("scoring", {})
    title_multiplier = scoring_config.get("title_multiplier", 3.0)
    abstract_multiplier = scoring_config.get("abstract_multiplier", 1.0)
    min_threshold = scoring_config.get("min_score_threshold", 2.0)
    
    interests = interests_config.get("interests", [])
    negative_keywords = interests_config.get("negative_keywords", [])
    combination_bonuses = interests_config.get("combination_bonuses", [])
    
    # 1. æ£€æŸ¥è´Ÿé¢å…³é”®è¯
    is_negative, matched_negatives = _check_negative_keywords(title, abstract, negative_keywords)
    
    # 2. è®¡ç®—å„å…´è¶£é¢†åŸŸçš„åŠ æƒå¾—åˆ†
    matched = []
    total_score = 0.0
    
    for interest in interests:
        if not interest.get("enabled", True):
            continue
        
        name = interest.get("name", "")
        keywords = interest.get("keywords", [])
        weight = interest.get("weight", 1.0)
        
        category_score = 0.0
        matched_keywords = []
        title_matches = []
        abstract_matches = []
        
        for kw in keywords:
            # æ ‡é¢˜åŒ¹é…ï¼šé«˜æƒé‡
            if _keyword_match(kw, title):
                category_score += title_multiplier * weight
                title_matches.append(kw)
                matched_keywords.append(f"[T]{kw}")
            # æ‘˜è¦åŒ¹é…ï¼šåŸºç¡€æƒé‡
            elif _keyword_match(kw, abstract):
                category_score += abstract_multiplier * weight
                abstract_matches.append(kw)
                matched_keywords.append(kw)
        
        if category_score > 0:
            matched.append({
                "name": name,
                "matched_keywords": matched_keywords,
                "title_matches": title_matches,
                "abstract_matches": abstract_matches,
                "score": round(category_score, 2),
                "weight": weight
            })
            total_score += category_score
    
    # 3. è®¡ç®—ç»„åˆåŠ åˆ†
    combo_bonus = _calculate_combination_bonus(title, abstract, combination_bonuses)
    total_score += combo_bonus
    
    # 4. åˆ¤æ–­æ˜¯å¦æ’é™¤
    # è´Ÿé¢å…³é”®è¯å¦å†³é€»è¾‘ï¼š
    # - å¦‚æœæœ‰æ­£é¢åŒ¹é…ä¸”åˆ†æ•° >= 3å€é˜ˆå€¼ï¼ˆ6.0ï¼‰ï¼Œæ­£é¢åŒ¹é…è¦†ç›–è´Ÿé¢
    # - å¦‚æœæ²¡æœ‰æ­£é¢åŒ¹é…æˆ–åˆ†æ•°å¤ªä½ï¼Œè´Ÿé¢å…³é”®è¯ç”Ÿæ•ˆ
    should_exclude = False
    if is_negative:
        # åªæœ‰å½“æ­£é¢åˆ†æ•°è¶³å¤Ÿé«˜ï¼ˆ>= 3å€é˜ˆå€¼ï¼‰æ—¶æ‰èƒ½è¦†ç›–è´Ÿé¢å…³é”®è¯
        if total_score < min_threshold * 3:
            should_exclude = True
    
    return {
        "matched_interests": matched,
        "relevance_score": round(total_score, 2),
        "excluded": should_exclude,
        "exclusion_keywords": matched_negatives,
        "combination_bonus": round(combo_bonus, 2)
    }


def filter_by_interests(papers: List[Dict], interests_file: str = "interests.json") -> List[Dict]:
    """
    æ ¹æ®åŠ æƒæ‰“åˆ†ç³»ç»Ÿç­›é€‰è®ºæ–‡
    
    ç‰¹ç‚¹ï¼š
    1. æ ‡é¢˜åŒ¹é…æƒé‡ 3xï¼Œæ‘˜è¦åŒ¹é…æƒé‡ 1x
    2. ä¸åŒå…´è¶£é¢†åŸŸæœ‰ä¸åŒæƒé‡ï¼ˆ1.0-2.0ï¼‰
    3. ç»„åˆåŒ¹é…å¯è·å¾—é¢å¤–åŠ åˆ†
    4. è´Ÿé¢å…³é”®è¯ä¸€ç¥¨å¦å†³ï¼ˆé™¤éæ­£é¢åˆ†æ•°å¾ˆé«˜ï¼‰
    """
    interests_config = load_interests(interests_file)
    
    if not interests_config:
        print("[INFO] æœªæ‰¾åˆ° interests.jsonï¼Œè·³è¿‡å…´è¶£ç­›é€‰")
        return papers
    
    # è·å–é˜ˆå€¼é…ç½®
    scoring_config = interests_config.get("scoring", {})
    min_threshold = scoring_config.get("min_score_threshold", 2.0)
    
    filtered = []
    excluded_count = 0
    below_threshold_count = 0
    
    for paper in papers:
        match_info = match_interests(paper, interests_config)
        paper["matched_interests"] = match_info["matched_interests"]
        paper["relevance_score"] = match_info["relevance_score"]
        paper["combination_bonus"] = match_info.get("combination_bonus", 0)
        
        # æ£€æŸ¥æ˜¯å¦è¢«è´Ÿé¢å…³é”®è¯æ’é™¤
        if match_info.get("excluded", False):
            excluded_count += 1
            continue
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°åˆ†æ•°é˜ˆå€¼
        if match_info["relevance_score"] >= min_threshold:
            filtered.append(paper)
        else:
            below_threshold_count += 1
    
    # æŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åº
    filtered.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\n{'='*50}")
    print(f"ğŸ“Š ç­›é€‰ç»Ÿè®¡")
    print(f"{'='*50}")
    print(f"   åŸå§‹è®ºæ–‡æ•°: {len(papers)}")
    print(f"   âœ… é€šè¿‡ç­›é€‰: {len(filtered)} ç¯‡ (åˆ†æ•° â‰¥ {min_threshold})")
    print(f"   âŒ è´Ÿé¢æ’é™¤: {excluded_count} ç¯‡")
    print(f"   âšª æœªè¾¾é˜ˆå€¼: {below_threshold_count} ç¯‡")
    
    # æ˜¾ç¤ºå„é¢†åŸŸåŒ¹é…ç»Ÿè®¡
    interest_counts = {}
    interest_scores = {}
    for p in filtered:
        for m in p.get("matched_interests", []):
            name = m["name"]
            interest_counts[name] = interest_counts.get(name, 0) + 1
            interest_scores[name] = interest_scores.get(name, 0) + m.get("score", 0)
    
    if interest_counts:
        print(f"\nğŸ“ˆ å„é¢†åŸŸå‘½ä¸­ç»Ÿè®¡:")
        for name, count in sorted(interest_counts.items(), key=lambda x: -x[1]):
            avg_score = interest_scores[name] / count if count > 0 else 0
            print(f"   {name}: {count} ç¯‡ (å¹³å‡åˆ†: {avg_score:.1f})")
    
    # æ˜¾ç¤º Top 5 é«˜åˆ†è®ºæ–‡
    if filtered:
        print(f"\nğŸ† Top 5 é«˜åˆ†è®ºæ–‡:")
        for i, p in enumerate(filtered[:5], 1):
            title = p.get("title", "")[:50]
            score = p.get("relevance_score", 0)
            bonus = p.get("combination_bonus", 0)
            interests = [m["name"].split("(")[0].strip() for m in p.get("matched_interests", [])[:2]]
            bonus_str = f" (+{bonus}ç»„åˆ)" if bonus > 0 else ""
            print(f"   {i}. [{score:.1f}åˆ†{bonus_str}] {title}...")
            print(f"      é¢†åŸŸ: {', '.join(interests)}")
    
    print(f"{'='*50}\n")
    
    return filtered

def get_UTC_range() -> Tuple[str, str, str]:
    fmt = "%Y%m%d%H%M"
    
    now_utc = datetime.now(timezone.utc)
    now_et = now_utc.astimezone(US_EASTERN)
    today_et = now_et.date()
    t2000_et = datetime(today_et.year, today_et.month, today_et.day, 20, 0, 0, tzinfo=US_EASTERN)
    
    if now_et < t2000_et:
        end_et = t2000_et - timedelta(days=1, minutes=1)
    else:
        end_et = t2000_et
    if end_et.weekday() in (4, 5):  # Friday or Saturday
        end_et -= timedelta(days=end_et.weekday() - 3, minutes=1)  # Move to Thursday
    
    if end_et.weekday() == 6:
        start_et = end_et - timedelta(days=3, minutes=-1)
    else:
        start_et = end_et - timedelta(days=1, minutes=-1)
    
    return (start_et.astimezone(timezone.utc).strftime(fmt),
            end_et.astimezone(timezone.utc).strftime(fmt),
            end_et.strftime("%Y-%m-%d"))


def extract_code_links(text: str) -> List[Dict[str, str]]:
    """ä»æ–‡æœ¬ä¸­æå–ä»£ç ä»“åº“é“¾æ¥ï¼ˆGitHub, GitLab, Hugging Face ç­‰ï¼‰"""
    patterns = [
        # GitHub
        (r'https?://github\.com/[\w\-\.]+/[\w\-\.]+(?:/[\w\-\.]*)?', 'github'),
        # GitLab
        (r'https?://gitlab\.com/[\w\-\.]+/[\w\-\.]+(?:/[\w\-\.]*)?', 'gitlab'),
        # Hugging Face
        (r'https?://huggingface\.co/[\w\-\.]+(?:/[\w\-\.]+)?', 'huggingface'),
        # é¡¹ç›®ä¸»é¡µï¼ˆå¸¸è§æ¨¡å¼ï¼‰
        (r'https?://[\w\-\.]+\.github\.io/[\w\-\.]+/?', 'project_page'),
    ]
    
    links = []
    seen = set()
    for pattern, link_type in patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        for url in matches:
            # æ¸…ç† URLï¼ˆç§»é™¤æœ«å°¾çš„æ ‡ç‚¹ï¼‰
            url = re.sub(r'[.,;:!?)}\]]+$', '', url)
            if url not in seen:
                seen.add(url)
                links.append({"url": url, "type": link_type})
    return links


def fetch_arxiv_thumbnail(arxiv_id: str, timeout: int = 10) -> Optional[str]:
    """
    ä» arXiv é¡µé¢æŠ“å–è®ºæ–‡é¢„è§ˆç¼©ç•¥å›¾
    è¿”å›å›¾ç‰‡ URL æˆ– None
    """
    base_id = arxiv_id.split('v')[0]
    abs_url = f"https://arxiv.org/abs/{base_id}"
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (compatible; ArxivDailyBot/1.0)'
        }
        resp = requests.get(abs_url, headers=headers, timeout=timeout)
        if resp.status_code != 200:
            return None
        
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # æ–¹å¼1: æŸ¥æ‰¾ og:image meta æ ‡ç­¾
        og_image = soup.find('meta', property='og:image')
        if og_image and og_image.get('content'):
            return og_image['content']
        
        # æ–¹å¼2: æŸ¥æ‰¾è®ºæ–‡ç¼©ç•¥å›¾ (é€šå¸¸åœ¨ arXiv é¡µé¢çš„ç‰¹å®šä½ç½®)
        # arXiv ä½¿ç”¨ https://arxiv.org/html/{id}/extracted/figure1.png æ ¼å¼
        # æˆ–è€… https://static.arxiv.org/static/browse/0.3.4/images/...
        
        # æŸ¥æ‰¾é¡µé¢ä¸­çš„ç¬¬ä¸€å¼ ç›¸å…³å›¾ç‰‡
        for img in soup.find_all('img'):
            src = img.get('src', '')
            # æ’é™¤å›¾æ ‡å’Œè£…é¥°å›¾ç‰‡
            if any(x in src.lower() for x in ['icon', 'logo', 'button', 'arrow', 'social']):
                continue
            # æ‰¾åˆ°æœ‰æ„ä¹‰çš„å›¾ç‰‡
            if 'arxiv' in src or src.startswith('/'):
                if src.startswith('/'):
                    src = f"https://arxiv.org{src}"
                return src
        
        return None
    except Exception:
        return None


def fetch_thumbnails_batch(papers: List[Dict], max_workers: int = 10) -> List[Dict]:
    """æ‰¹é‡æŠ“å–è®ºæ–‡ç¼©ç•¥å›¾"""
    print(f"[INFO] æ­£åœ¨æŠ“å– {len(papers)} ç¯‡è®ºæ–‡çš„é¢„è§ˆå›¾...")
    
    def fetch_one(paper: Dict) -> Tuple[str, Optional[str]]:
        arxiv_id = paper.get('arxiv_id', '')
        thumb = fetch_arxiv_thumbnail(arxiv_id)
        return arxiv_id, thumb
    
    thumbnails = {}
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(fetch_one, p): p for p in papers}
        for future in tqdm(as_completed(futures), total=len(futures), desc="æŠ“å–ç¼©ç•¥å›¾"):
            arxiv_id, thumb = future.result()
            if thumb:
                thumbnails[arxiv_id] = thumb
    
    # æ›´æ–°è®ºæ–‡æ•°æ®
    for paper in papers:
        arxiv_id = paper.get('arxiv_id', '')
        if arxiv_id in thumbnails:
            paper['thumbnail'] = thumbnails[arxiv_id]
    
    print(f"[INFO] æˆåŠŸè·å– {len(thumbnails)}/{len(papers)} ç¯‡è®ºæ–‡çš„é¢„è§ˆå›¾")
    return papers


def _result_to_minimal(r: arxiv.Result) -> Dict:
    arxiv_id = r.get_short_id() if hasattr(r, "get_short_id") else r.entry_id.split("/abs/")[-1]
    authors = [a.name for a in r.authors] if r.authors else []
    
    # è·å–åˆ†ç±»ä¿¡æ¯
    categories = list(r.categories) if r.categories else []
    primary_category = r.primary_category if hasattr(r, "primary_category") and r.primary_category else (categories[0] if categories else "")
    
    # è·å–æ›´å¤šå…ƒæ•°æ®
    summary = (r.summary or "").strip()
    
    # æå–ä»£ç é“¾æ¥
    code_links = extract_code_links(summary)
    
    # å‘å¸ƒå’Œæ›´æ–°æ—¥æœŸ
    published = r.published.strftime("%Y-%m-%d") if r.published else ""
    updated = r.updated.strftime("%Y-%m-%d") if r.updated else ""
    
    # è¯„è®ºä¿¡æ¯ï¼ˆé€šå¸¸åŒ…å«é¡µæ•°ã€ä¼šè®®ç­‰ï¼‰
    comment = (r.comment or "").strip() if hasattr(r, 'comment') and r.comment else ""
    
    # DOI å’ŒæœŸåˆŠå¼•ç”¨
    doi = r.doi if hasattr(r, 'doi') and r.doi else ""
    journal_ref = (r.journal_ref or "").strip() if hasattr(r, 'journal_ref') and r.journal_ref else ""
    
    # PDF URL
    pdf_url = r.pdf_url if hasattr(r, 'pdf_url') else f"https://arxiv.org/pdf/{arxiv_id.split('v')[0]}.pdf"
    
    return {
        "title": (r.title or "").strip().replace("\n", " "),
        "authors": authors,
        "arxiv_id": arxiv_id,
        "summary": summary,
        "categories": categories,
        "primary_category": primary_category,
        "published": published,
        "updated": updated,
        "comment": comment,
        "doi": doi,
        "journal_ref": journal_ref,
        "pdf_url": pdf_url,
        "code_links": code_links,
    }


def query_arxiv(tags: List[str], time_range: Tuple[str, str], max_results: int = 500, fetch_thumbnails: bool = False) -> Dict:
    start, end = time_range
    tag_clause = " OR ".join(f"cat:{t}" for t in tags)
    query = f"({tag_clause}) AND submittedDate:[{start} TO {end}]"

    client = arxiv.Client(page_size=200, delay_seconds=1.0, num_retries=3)

    search = arxiv.Search(
        query=query,
        max_results=max_results,
        sort_by=arxiv.SortCriterion.SubmittedDate,
        sort_order=arxiv.SortOrder.Descending,
    )

    seen = set()
    papers = []
    print(f"[INFO] æ­£åœ¨ä» arXiv æŠ“å–è®ºæ–‡...")
    for r in tqdm(client.results(search), desc="æŠ“å–è®ºæ–‡", unit="paper"):
        item = _result_to_minimal(r)
        if item["arxiv_id"] in seen:
            continue
        seen.add(item["arxiv_id"])
        papers.append(item)
    
    # å¯é€‰ï¼šæ‰¹é‡æŠ“å–ç¼©ç•¥å›¾
    if fetch_thumbnails and papers:
        papers = fetch_thumbnails_batch(papers)
    
    return {"count": len(papers), "papers": papers}


def get_today_arxiv(tags: List[str], max_results: int = 500, fetch_thumbnails: bool = False) -> Tuple[Dict, str]:
    start, end, label_date = get_UTC_range()
    return query_arxiv(tags, (start, end), max_results=max_results, fetch_thumbnails=fetch_thumbnails), label_date


if __name__ == "__main__":
    tags = load_tags('../tags.json')
    result, label_date = get_today_arxiv(tags, fetch_thumbnails=True)
    print(f'Tags: {tags}')
    print(f"Date: {label_date}, Found {result['count']} papers")
    
    # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
    if result['papers']:
        p = result['papers'][0]
        print(f"\nç¤ºä¾‹è®ºæ–‡:")
        print(f"  æ ‡é¢˜: {p['title'][:60]}...")
        print(f"  å‘å¸ƒ: {p['published']}")
        print(f"  ä»£ç : {p['code_links']}")
        print(f"  ç¼©ç•¥å›¾: {p.get('thumbnail', 'N/A')}")
